06/02/2019 01:15:02 {'input_path': 'data/word2vec_temp', 'output_path': 'save/mlp_2', 'gpu': True, 'seed': 20000125, 'display_per_batch': 5, 'optimizer': 'adagrad', 'lr': 0.01, 'lr_decay': 0, 'weight_decay': 0.0001, 'momentum': 0.985, 'num_epochs': 300, 'batch_size': 64, 'num_labels': 8, 'embedding_size': 300, 'type': 'mlp', 'mlp': {'max_length': 512, 'dropout': 0.9, 'hidden_size': 512, 'loss': 'cross_entropy'}}
06/02/2019 01:15:02 Loading Train Data
06/02/2019 01:15:02 load data from data/word2vec_temp/train_text.npy, data/word2vec_temp/train_label.npy, training: True
06/02/2019 01:15:29 loaded. total len: 2342
06/02/2019 01:15:29 Train: length: 2108, total batch: 33, batch size: 64
06/02/2019 01:15:29 Dev: length: 234, total batch: 4, batch size: 64
06/02/2019 01:15:29 Loading model mlp
06/02/2019 01:15:43 *** epoch: 1 ***
06/02/2019 01:15:43 *** training ***
06/02/2019 01:15:44 step: 5, epoch: 0, batch: 4, loss: 92.11314392089844, acc: 23.4375, f1: 15.490960534056079, r: 0.015361765063545002
06/02/2019 01:15:44 step: 10, epoch: 0, batch: 9, loss: 24.0312442779541, acc: 25.0, f1: 19.408915552532573, r: 0.06472990433713643
06/02/2019 01:15:44 step: 15, epoch: 0, batch: 14, loss: 3.8481295108795166, acc: 21.875, f1: 12.968954248366012, r: 0.0952021451335017
06/02/2019 01:15:44 step: 20, epoch: 0, batch: 19, loss: 2.1322147846221924, acc: 26.5625, f1: 15.673938579639431, r: 0.0743837583493143
06/02/2019 01:15:45 step: 25, epoch: 0, batch: 24, loss: 2.269329309463501, acc: 7.8125, f1: 2.452491011813046, r: -0.046884329356973946
06/02/2019 01:15:45 step: 30, epoch: 0, batch: 29, loss: 2.0274229049682617, acc: 23.4375, f1: 13.360641747738523, r: 0.01991311360938739
06/02/2019 01:15:45 *** evaluating ***
06/02/2019 01:15:45 step: 1, epoch: 0, acc: 20.085470085470085, f1: 6.771366948358098, r: 0.145998913555109
06/02/2019 01:15:45 *** epoch: 2 ***
06/02/2019 01:15:45 *** training ***
06/02/2019 01:15:45 step: 38, epoch: 1, batch: 4, loss: 2.524688243865967, acc: 23.4375, f1: 9.134336421296945, r: 0.006523356642982998
06/02/2019 01:15:45 step: 43, epoch: 1, batch: 9, loss: 2.1383378505706787, acc: 20.3125, f1: 9.445812807881774, r: 0.07738483181315692
06/02/2019 01:15:46 step: 48, epoch: 1, batch: 14, loss: 2.0320262908935547, acc: 18.75, f1: 8.180005438230763, r: 0.10153828433558607
06/02/2019 01:15:46 step: 53, epoch: 1, batch: 19, loss: 2.446882724761963, acc: 21.875, f1: 10.266506602641057, r: 0.0995620417636053
06/02/2019 01:15:46 step: 58, epoch: 1, batch: 24, loss: 1.9067271947860718, acc: 26.5625, f1: 11.043123543123542, r: 0.1503569102820189
06/02/2019 01:15:46 step: 63, epoch: 1, batch: 29, loss: 1.8366872072219849, acc: 48.4375, f1: 17.12121212121212, r: 0.1882595446889838
06/02/2019 01:15:46 *** evaluating ***
06/02/2019 01:15:46 step: 2, epoch: 1, acc: 46.15384615384615, f1: 10.514592933947773, r: 0.04035926192984717
06/02/2019 01:15:46 *** epoch: 3 ***
06/02/2019 01:15:46 *** training ***
06/02/2019 01:15:46 step: 71, epoch: 2, batch: 4, loss: 2.3259928226470947, acc: 32.8125, f1: 12.835497835497836, r: 0.036186804091202306
06/02/2019 01:15:47 step: 76, epoch: 2, batch: 9, loss: 1.8493409156799316, acc: 42.1875, f1: 13.90817901234568, r: 0.1644346082467689
06/02/2019 01:15:47 step: 81, epoch: 2, batch: 14, loss: 2.2711703777313232, acc: 53.125, f1: 35.35029004908523, r: 0.23111927424706755
06/02/2019 01:15:47 step: 86, epoch: 2, batch: 19, loss: 2.1766068935394287, acc: 37.5, f1: 12.70951270951271, r: 0.08517170052072813
06/02/2019 01:15:47 step: 91, epoch: 2, batch: 24, loss: 2.218993663787842, acc: 37.5, f1: 11.766667222379676, r: 0.13812335041639334
06/02/2019 01:15:47 step: 96, epoch: 2, batch: 29, loss: 2.1813957691192627, acc: 29.6875, f1: 10.484472049689442, r: 0.10610113516185313
06/02/2019 01:15:47 *** evaluating ***
06/02/2019 01:15:47 step: 3, epoch: 2, acc: 47.008547008547005, f1: 13.744328493647911, r: 0.1529968354042484
06/02/2019 01:15:47 *** epoch: 4 ***
06/02/2019 01:15:47 *** training ***
06/02/2019 01:15:48 step: 104, epoch: 3, batch: 4, loss: 1.967578649520874, acc: 45.3125, f1: 20.504201680672267, r: 0.13259145765978614
06/02/2019 01:15:48 step: 109, epoch: 3, batch: 9, loss: 2.648153305053711, acc: 40.625, f1: 12.304905239687846, r: 0.042975382762521774
06/02/2019 01:15:48 step: 114, epoch: 3, batch: 14, loss: 2.127486228942871, acc: 45.3125, f1: 18.583797155225724, r: 0.12409289434922174
06/02/2019 01:15:48 step: 119, epoch: 3, batch: 19, loss: 2.0633890628814697, acc: 45.3125, f1: 9.563492063492063, r: 0.03999316121257155
06/02/2019 01:15:48 step: 124, epoch: 3, batch: 24, loss: 1.9165585041046143, acc: 43.75, f1: 14.297385620915032, r: 0.09087205945325048
06/02/2019 01:15:48 step: 129, epoch: 3, batch: 29, loss: 2.259765386581421, acc: 48.4375, f1: 12.18266253869969, r: 0.025235228071283766
06/02/2019 01:15:48 *** evaluating ***
06/02/2019 01:15:48 step: 4, epoch: 3, acc: 20.51282051282051, f1: 5.314897698209719, r: 0.07189769886349198
06/02/2019 01:15:48 *** epoch: 5 ***
06/02/2019 01:15:48 *** training ***
06/02/2019 01:15:49 step: 137, epoch: 4, batch: 4, loss: 1.8365925550460815, acc: 40.625, f1: 12.10690045248869, r: 0.14041739316165494
06/02/2019 01:15:49 step: 142, epoch: 4, batch: 9, loss: 2.0729029178619385, acc: 46.875, f1: 21.615460999214502, r: 0.27474760362742856
06/02/2019 01:15:49 step: 147, epoch: 4, batch: 14, loss: 2.178800344467163, acc: 46.875, f1: 15.008025682182987, r: 0.1346019268243332
06/02/2019 01:15:49 step: 152, epoch: 4, batch: 19, loss: 2.123424768447876, acc: 46.875, f1: 21.02650290885585, r: 0.0600618579482953
06/02/2019 01:15:49 step: 157, epoch: 4, batch: 24, loss: 2.638976812362671, acc: 37.5, f1: 10.100982231541773, r: 0.0523571522543954
06/02/2019 01:15:50 step: 162, epoch: 4, batch: 29, loss: 3.2873198986053467, acc: 43.75, f1: 15.573071506806446, r: 0.06735149607231292
06/02/2019 01:15:50 *** evaluating ***
06/02/2019 01:15:50 step: 5, epoch: 4, acc: 47.43589743589743, f1: 10.98181631586804, r: 0.10901477245577434
06/02/2019 01:15:50 *** epoch: 6 ***
06/02/2019 01:15:50 *** training ***
06/02/2019 01:15:50 step: 170, epoch: 5, batch: 4, loss: 1.9719655513763428, acc: 35.9375, f1: 7.942295497780595, r: 0.09346198562564874
06/02/2019 01:15:50 step: 175, epoch: 5, batch: 9, loss: 2.1847646236419678, acc: 29.6875, f1: 13.37740730094855, r: 0.07927597818329502
06/02/2019 01:15:50 step: 180, epoch: 5, batch: 14, loss: 1.8812862634658813, acc: 48.4375, f1: 16.961451247165535, r: 0.13284664093352888
06/02/2019 01:15:50 step: 185, epoch: 5, batch: 19, loss: 3.2958924770355225, acc: 34.375, f1: 30.12826183104821, r: 0.17194549440170465
06/02/2019 01:15:51 step: 190, epoch: 5, batch: 24, loss: 2.067549705505371, acc: 46.875, f1: 13.420724094881397, r: 0.14709057108696375
06/02/2019 01:15:51 step: 195, epoch: 5, batch: 29, loss: 2.4580845832824707, acc: 43.75, f1: 25.501877640431864, r: 0.12293726202037611
06/02/2019 01:15:51 *** evaluating ***
06/02/2019 01:15:51 step: 6, epoch: 5, acc: 44.44444444444444, f1: 8.894675925925926, r: 0.11586020590044667
06/02/2019 01:15:51 *** epoch: 7 ***
06/02/2019 01:15:51 *** training ***
06/02/2019 01:15:51 step: 203, epoch: 6, batch: 4, loss: 2.42813777923584, acc: 48.4375, f1: 12.593755450898309, r: 0.14732869801942933
06/02/2019 01:15:51 step: 208, epoch: 6, batch: 9, loss: 2.4918017387390137, acc: 53.125, f1: 25.434088269454126, r: 0.12943849683262626
06/02/2019 01:15:51 step: 213, epoch: 6, batch: 14, loss: 1.9233980178833008, acc: 43.75, f1: 20.6436420722135, r: 0.13751657886724322
06/02/2019 01:15:52 step: 218, epoch: 6, batch: 19, loss: 1.9342817068099976, acc: 39.0625, f1: 9.200438116100766, r: 0.05318526304770919
06/02/2019 01:15:52 step: 223, epoch: 6, batch: 24, loss: 2.1008710861206055, acc: 40.625, f1: 10.541979949874687, r: 0.15455714553828515
06/02/2019 01:15:52 step: 228, epoch: 6, batch: 29, loss: 2.001708745956421, acc: 34.375, f1: 13.031358885017422, r: 0.05209076725421179
06/02/2019 01:15:52 *** evaluating ***
06/02/2019 01:15:52 step: 7, epoch: 6, acc: 44.871794871794876, f1: 7.859281437125748, r: 0.1265278604524205
06/02/2019 01:15:52 *** epoch: 8 ***
06/02/2019 01:15:52 *** training ***
06/02/2019 01:15:52 step: 236, epoch: 7, batch: 4, loss: 2.2099990844726562, acc: 45.3125, f1: 14.703049759229536, r: 0.08568645768391125
06/02/2019 01:15:52 step: 241, epoch: 7, batch: 9, loss: 2.360319137573242, acc: 59.375, f1: 20.6495501426377, r: 0.1739424331499732
06/02/2019 01:15:53 step: 246, epoch: 7, batch: 14, loss: 2.3162429332733154, acc: 39.0625, f1: 10.570824524312895, r: 0.04739381946589063
06/02/2019 01:15:53 step: 251, epoch: 7, batch: 19, loss: 1.9327070713043213, acc: 40.625, f1: 9.797482211275314, r: 0.0016766652516571959
06/02/2019 01:15:53 step: 256, epoch: 7, batch: 24, loss: 2.1696319580078125, acc: 40.625, f1: 11.391223155929039, r: 0.10630231740900518
06/02/2019 01:15:53 step: 261, epoch: 7, batch: 29, loss: 1.8403435945510864, acc: 28.125, f1: 13.80952380952381, r: 0.09214642153100325
06/02/2019 01:15:53 *** evaluating ***
06/02/2019 01:15:53 step: 8, epoch: 7, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.096812415492765
06/02/2019 01:15:53 *** epoch: 9 ***
06/02/2019 01:15:53 *** training ***
06/02/2019 01:15:53 step: 269, epoch: 8, batch: 4, loss: 1.8609082698822021, acc: 48.4375, f1: 24.0681291243089, r: 0.19063744077883227
06/02/2019 01:15:53 step: 274, epoch: 8, batch: 9, loss: 1.7872201204299927, acc: 39.0625, f1: 11.726907630522089, r: 0.21071407218198202
06/02/2019 01:15:54 step: 279, epoch: 8, batch: 14, loss: 2.871422529220581, acc: 35.9375, f1: 13.754208754208753, r: 0.045182304395088004
06/02/2019 01:15:54 step: 284, epoch: 8, batch: 19, loss: 1.8662550449371338, acc: 39.0625, f1: 15.124716553287978, r: 0.0986630974128063
06/02/2019 01:15:54 step: 289, epoch: 8, batch: 24, loss: 1.802722692489624, acc: 39.0625, f1: 11.011904761904761, r: 0.15466952681462554
06/02/2019 01:15:54 step: 294, epoch: 8, batch: 29, loss: 1.848658561706543, acc: 39.0625, f1: 14.859307359307362, r: 0.1840485726504589
06/02/2019 01:15:54 *** evaluating ***
06/02/2019 01:15:54 step: 9, epoch: 8, acc: 44.871794871794876, f1: 7.766272189349112, r: 0.10308684028954321
06/02/2019 01:15:54 *** epoch: 10 ***
06/02/2019 01:15:54 *** training ***
06/02/2019 01:15:54 step: 302, epoch: 9, batch: 4, loss: 1.7803165912628174, acc: 39.0625, f1: 8.116883116883118, r: 0.09820205337513165
06/02/2019 01:15:54 step: 307, epoch: 9, batch: 9, loss: 1.6774730682373047, acc: 45.3125, f1: 13.845538215286116, r: 0.07837279608293246
06/02/2019 01:15:55 step: 312, epoch: 9, batch: 14, loss: 1.8769348859786987, acc: 31.25, f1: 11.350409458962089, r: 0.18035262719655804
06/02/2019 01:15:55 step: 317, epoch: 9, batch: 19, loss: 2.008992910385132, acc: 54.6875, f1: 29.432365387421566, r: 0.13291326340862608
06/02/2019 01:15:55 step: 322, epoch: 9, batch: 24, loss: 2.0363128185272217, acc: 37.5, f1: 11.982574387637678, r: 0.10948283614712997
06/02/2019 01:15:55 step: 327, epoch: 9, batch: 29, loss: 1.8244446516036987, acc: 42.1875, f1: 10.655120481927712, r: 0.10691691026740471
06/02/2019 01:15:55 *** evaluating ***
06/02/2019 01:15:55 step: 10, epoch: 9, acc: 44.871794871794876, f1: 7.8125, r: 0.19694077792564033
06/02/2019 01:15:55 *** epoch: 11 ***
06/02/2019 01:15:55 *** training ***
06/02/2019 01:15:55 step: 335, epoch: 10, batch: 4, loss: 1.8870432376861572, acc: 37.5, f1: 10.495553643144003, r: 0.14936548989249382
06/02/2019 01:15:56 step: 340, epoch: 10, batch: 9, loss: 2.573065757751465, acc: 35.9375, f1: 12.794117647058826, r: 0.10663676182871039
06/02/2019 01:15:56 step: 345, epoch: 10, batch: 14, loss: 1.9863964319229126, acc: 35.9375, f1: 8.835227272727272, r: 0.10613317566099686
06/02/2019 01:15:56 step: 350, epoch: 10, batch: 19, loss: 1.7248284816741943, acc: 39.0625, f1: 17.397195711087544, r: 0.20802426223096168
06/02/2019 01:15:56 step: 355, epoch: 10, batch: 24, loss: 1.8327099084854126, acc: 39.0625, f1: 9.67432950191571, r: 0.1013856349945743
06/02/2019 01:15:56 step: 360, epoch: 10, batch: 29, loss: 1.9271256923675537, acc: 50.0, f1: 13.152173913043477, r: 0.16747148886108187
06/02/2019 01:15:56 *** evaluating ***
06/02/2019 01:15:56 step: 11, epoch: 10, acc: 44.871794871794876, f1: 7.766272189349112, r: 0.15597016636412003
06/02/2019 01:15:56 *** epoch: 12 ***
06/02/2019 01:15:56 *** training ***
06/02/2019 01:15:57 step: 368, epoch: 11, batch: 4, loss: 2.1096994876861572, acc: 40.625, f1: 10.998217468805704, r: 0.20525148882574992
06/02/2019 01:15:57 step: 373, epoch: 11, batch: 9, loss: 1.716814398765564, acc: 45.3125, f1: 9.006211180124224, r: 0.1316071296187418
06/02/2019 01:15:57 step: 378, epoch: 11, batch: 14, loss: 1.7675130367279053, acc: 39.0625, f1: 12.902494331065759, r: 0.16501295492021195
06/02/2019 01:15:57 step: 383, epoch: 11, batch: 19, loss: 1.7201931476593018, acc: 45.3125, f1: 18.506493506493506, r: 0.06199403820209218
06/02/2019 01:15:57 step: 388, epoch: 11, batch: 24, loss: 1.88340163230896, acc: 39.0625, f1: 10.278745644599303, r: 0.11866708520361119
06/02/2019 01:15:57 step: 393, epoch: 11, batch: 29, loss: 1.733657717704773, acc: 43.75, f1: 21.950710108604845, r: 0.1843645479379321
06/02/2019 01:15:57 *** evaluating ***
06/02/2019 01:15:57 step: 12, epoch: 11, acc: 44.871794871794876, f1: 7.7893175074183985, r: 0.16077238993991294
06/02/2019 01:15:57 *** epoch: 13 ***
06/02/2019 01:15:57 *** training ***
06/02/2019 01:15:58 step: 401, epoch: 12, batch: 4, loss: 1.8983086347579956, acc: 48.4375, f1: 29.011664181550927, r: 0.22832287896555956
06/02/2019 01:15:58 step: 406, epoch: 12, batch: 9, loss: 1.83396577835083, acc: 34.375, f1: 10.6430155210643, r: 0.09229525931068722
06/02/2019 01:15:58 step: 411, epoch: 12, batch: 14, loss: 1.5996958017349243, acc: 42.1875, f1: 14.935064935064934, r: 0.07869296839442236
06/02/2019 01:15:58 step: 416, epoch: 12, batch: 19, loss: 1.786098837852478, acc: 40.625, f1: 10.762195121951217, r: 0.126243015437287
06/02/2019 01:15:58 step: 421, epoch: 12, batch: 24, loss: 2.509732723236084, acc: 26.5625, f1: 6.625791139240506, r: 0.08649261569734017
06/02/2019 01:15:58 step: 426, epoch: 12, batch: 29, loss: 1.7973064184188843, acc: 40.625, f1: 8.441558441558442, r: 0.08910491161562073
06/02/2019 01:15:58 *** evaluating ***
06/02/2019 01:15:58 step: 13, epoch: 12, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.1627840250079225
06/02/2019 01:15:58 *** epoch: 14 ***
06/02/2019 01:15:58 *** training ***
06/02/2019 01:15:59 step: 434, epoch: 13, batch: 4, loss: 2.372260570526123, acc: 42.1875, f1: 7.670454545454546, r: 0.09889828733218566
06/02/2019 01:15:59 step: 439, epoch: 13, batch: 9, loss: 1.7474128007888794, acc: 39.0625, f1: 11.526610644257703, r: 0.13642098898467478
06/02/2019 01:15:59 step: 444, epoch: 13, batch: 14, loss: 1.9648133516311646, acc: 37.5, f1: 13.177710843373495, r: 0.17123266185941038
06/02/2019 01:15:59 step: 449, epoch: 13, batch: 19, loss: 1.7709987163543701, acc: 43.75, f1: 16.47509578544061, r: 0.16623150356500008
06/02/2019 01:15:59 step: 454, epoch: 13, batch: 24, loss: 1.841813087463379, acc: 40.625, f1: 11.622315592903828, r: 0.09742885936421551
06/02/2019 01:15:59 step: 459, epoch: 13, batch: 29, loss: 1.7633510828018188, acc: 35.9375, f1: 9.577042198993418, r: 0.14073359750717843
06/02/2019 01:15:59 *** evaluating ***
06/02/2019 01:16:00 step: 14, epoch: 13, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.21164059738131008
06/02/2019 01:16:00 *** epoch: 15 ***
06/02/2019 01:16:00 *** training ***
06/02/2019 01:16:00 step: 467, epoch: 14, batch: 4, loss: 1.8776015043258667, acc: 42.1875, f1: 13.136288998357964, r: 0.13282289118701984
06/02/2019 01:16:00 step: 472, epoch: 14, batch: 9, loss: 1.9437675476074219, acc: 31.25, f1: 6.884681583476763, r: 0.07010704241842969
06/02/2019 01:16:00 step: 477, epoch: 14, batch: 14, loss: 1.8986674547195435, acc: 26.5625, f1: 6.148282097649186, r: 0.06928705945644693
06/02/2019 01:16:00 step: 482, epoch: 14, batch: 19, loss: 1.6652591228485107, acc: 45.3125, f1: 13.429641519529161, r: 0.13749204031748113
06/02/2019 01:16:00 step: 487, epoch: 14, batch: 24, loss: 1.6802010536193848, acc: 39.0625, f1: 10.416666666666666, r: 0.12114422061524072
06/02/2019 01:16:01 step: 492, epoch: 14, batch: 29, loss: 1.7715065479278564, acc: 43.75, f1: 24.62855297157623, r: 0.23138328401856395
06/02/2019 01:16:01 *** evaluating ***
06/02/2019 01:16:01 step: 15, epoch: 14, acc: 44.871794871794876, f1: 7.8125, r: 0.21136974746459253
06/02/2019 01:16:01 *** epoch: 16 ***
06/02/2019 01:16:01 *** training ***
06/02/2019 01:16:01 step: 500, epoch: 15, batch: 4, loss: 1.8618930578231812, acc: 45.3125, f1: 11.17216117216117, r: 0.10451962735407601
06/02/2019 01:16:01 step: 505, epoch: 15, batch: 9, loss: 1.610800862312317, acc: 42.1875, f1: 9.87012987012987, r: 0.2200354654459161
06/02/2019 01:16:01 step: 510, epoch: 15, batch: 14, loss: 1.7243905067443848, acc: 45.3125, f1: 15.121564482029598, r: 0.12107429303797033
06/02/2019 01:16:02 step: 515, epoch: 15, batch: 19, loss: 1.779555320739746, acc: 43.75, f1: 20.30812324929972, r: 0.15439641213242672
06/02/2019 01:16:02 step: 520, epoch: 15, batch: 24, loss: 1.7478431463241577, acc: 31.25, f1: 18.958333333333332, r: 0.21010883508231365
06/02/2019 01:16:02 step: 525, epoch: 15, batch: 29, loss: 1.7162656784057617, acc: 43.75, f1: 15.655039454806314, r: 0.17789902811630687
06/02/2019 01:16:02 *** evaluating ***
06/02/2019 01:16:02 step: 16, epoch: 15, acc: 44.871794871794876, f1: 7.7893175074183985, r: 0.21480354181400663
06/02/2019 01:16:02 *** epoch: 17 ***
06/02/2019 01:16:02 *** training ***
06/02/2019 01:16:02 step: 533, epoch: 16, batch: 4, loss: 1.5858649015426636, acc: 48.4375, f1: 17.074829931972786, r: 0.26156440047535207
06/02/2019 01:16:02 step: 538, epoch: 16, batch: 9, loss: 2.039606809616089, acc: 26.5625, f1: 8.646616541353383, r: 0.028168886180339185
06/02/2019 01:16:02 step: 543, epoch: 16, batch: 14, loss: 1.5564064979553223, acc: 43.75, f1: 9.166666666666668, r: 0.18176466666734023
06/02/2019 01:16:03 step: 548, epoch: 16, batch: 19, loss: 1.7891621589660645, acc: 35.9375, f1: 10.657596371882086, r: 0.24034940587509643
06/02/2019 01:16:03 step: 553, epoch: 16, batch: 24, loss: 2.364969253540039, acc: 29.6875, f1: 14.51093951093951, r: 0.08961693332503658
06/02/2019 01:16:03 step: 558, epoch: 16, batch: 29, loss: 2.387054681777954, acc: 34.375, f1: 9.387254901960787, r: 0.05920963280402718
06/02/2019 01:16:03 *** evaluating ***
06/02/2019 01:16:03 step: 17, epoch: 16, acc: 46.15384615384615, f1: 9.607535321821036, r: 0.17321277776878588
06/02/2019 01:16:03 *** epoch: 18 ***
06/02/2019 01:16:03 *** training ***
06/02/2019 01:16:03 step: 566, epoch: 17, batch: 4, loss: 1.793413519859314, acc: 37.5, f1: 16.213151927437643, r: 0.16084390338367405
06/02/2019 01:16:03 step: 571, epoch: 17, batch: 9, loss: 1.4006565809249878, acc: 46.875, f1: 14.181672669067627, r: 0.21919909937216683
06/02/2019 01:16:04 step: 576, epoch: 17, batch: 14, loss: 1.6964970827102661, acc: 51.5625, f1: 26.05832076420312, r: 0.3061262590460605
06/02/2019 01:16:04 step: 581, epoch: 17, batch: 19, loss: 1.6759157180786133, acc: 40.625, f1: 11.522198731501057, r: 0.18444072877362774
06/02/2019 01:16:04 step: 586, epoch: 17, batch: 24, loss: 1.6595786809921265, acc: 45.3125, f1: 12.805788982259573, r: 0.19723565911098687
06/02/2019 01:16:04 step: 591, epoch: 17, batch: 29, loss: 1.6687132120132446, acc: 53.125, f1: 17.677304964539008, r: 0.11935737418847883
06/02/2019 01:16:04 *** evaluating ***
06/02/2019 01:16:04 step: 18, epoch: 17, acc: 44.871794871794876, f1: 7.7893175074183985, r: 0.17319813585692603
06/02/2019 01:16:04 *** epoch: 19 ***
06/02/2019 01:16:04 *** training ***
06/02/2019 01:16:04 step: 599, epoch: 18, batch: 4, loss: 1.6188759803771973, acc: 40.625, f1: 22.735313209996754, r: 0.2249417659992511
06/02/2019 01:16:04 step: 604, epoch: 18, batch: 9, loss: 1.6519237756729126, acc: 48.4375, f1: 27.806697612732094, r: 0.24881537956335548
06/02/2019 01:16:05 step: 609, epoch: 18, batch: 14, loss: 1.7019553184509277, acc: 42.1875, f1: 19.886363636363637, r: 0.11051239342888476
06/02/2019 01:16:05 step: 614, epoch: 18, batch: 19, loss: 1.7885710000991821, acc: 39.0625, f1: 11.963727329580989, r: 0.1628469771243189
06/02/2019 01:16:05 step: 619, epoch: 18, batch: 24, loss: 1.7719535827636719, acc: 34.375, f1: 11.876130198915009, r: 0.18642556778886762
06/02/2019 01:16:05 step: 624, epoch: 18, batch: 29, loss: 1.7961868047714233, acc: 42.1875, f1: 8.702153110047847, r: 0.162658411561656
06/02/2019 01:16:05 *** evaluating ***
06/02/2019 01:16:05 step: 19, epoch: 18, acc: 44.871794871794876, f1: 7.7893175074183985, r: 0.19054656843724724
06/02/2019 01:16:05 *** epoch: 20 ***
06/02/2019 01:16:05 *** training ***
06/02/2019 01:16:05 step: 632, epoch: 19, batch: 4, loss: 1.5707347393035889, acc: 34.375, f1: 12.422839506172838, r: 0.2427334962788352
06/02/2019 01:16:06 step: 637, epoch: 19, batch: 9, loss: 2.2953970432281494, acc: 43.75, f1: 9.164626682986537, r: 0.05067600815263396
06/02/2019 01:16:06 step: 642, epoch: 19, batch: 14, loss: 1.654296636581421, acc: 40.625, f1: 10.314685314685315, r: 0.141069469857533
06/02/2019 01:16:06 step: 647, epoch: 19, batch: 19, loss: 1.90309476852417, acc: 37.5, f1: 10.617283950617287, r: 0.1659412439538668
06/02/2019 01:16:06 step: 652, epoch: 19, batch: 24, loss: 2.4918651580810547, acc: 32.8125, f1: 8.18089430894309, r: 0.10136672293133735
06/02/2019 01:16:06 step: 657, epoch: 19, batch: 29, loss: 1.6982734203338623, acc: 45.3125, f1: 14.753787878787877, r: 0.19771571900608265
06/02/2019 01:16:06 *** evaluating ***
06/02/2019 01:16:06 step: 20, epoch: 19, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.21518324139380668
06/02/2019 01:16:06 *** epoch: 21 ***
06/02/2019 01:16:06 *** training ***
06/02/2019 01:16:07 step: 665, epoch: 20, batch: 4, loss: 1.8168399333953857, acc: 35.9375, f1: 19.136577708006282, r: 0.17653862462190334
06/02/2019 01:16:07 step: 670, epoch: 20, batch: 9, loss: 1.7185882329940796, acc: 39.0625, f1: 11.184473789515806, r: 0.16447167645156022
06/02/2019 01:16:07 step: 675, epoch: 20, batch: 14, loss: 1.7277557849884033, acc: 35.9375, f1: 6.686046511627906, r: 0.09829278715673728
06/02/2019 01:16:07 step: 680, epoch: 20, batch: 19, loss: 1.6542176008224487, acc: 45.3125, f1: 12.82051282051282, r: 0.1405055762767239
06/02/2019 01:16:07 step: 685, epoch: 20, batch: 24, loss: 1.6026158332824707, acc: 43.75, f1: 21.211958467597565, r: 0.1495067003392179
06/02/2019 01:16:07 step: 690, epoch: 20, batch: 29, loss: 1.8413257598876953, acc: 34.375, f1: 11.632144227080937, r: 0.11922264420613585
06/02/2019 01:16:08 *** evaluating ***
06/02/2019 01:16:08 step: 21, epoch: 20, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.2274737620539863
06/02/2019 01:16:08 *** epoch: 22 ***
06/02/2019 01:16:08 *** training ***
06/02/2019 01:16:08 step: 698, epoch: 21, batch: 4, loss: 1.6361966133117676, acc: 35.9375, f1: 11.310548315711825, r: 0.18572982479001035
06/02/2019 01:16:08 step: 703, epoch: 21, batch: 9, loss: 1.5666877031326294, acc: 48.4375, f1: 9.323308270676693, r: 0.1778440331528391
06/02/2019 01:16:08 step: 708, epoch: 21, batch: 14, loss: 1.743853211402893, acc: 34.375, f1: 16.676931046391157, r: 0.17942443869084446
06/02/2019 01:16:08 step: 713, epoch: 21, batch: 19, loss: 1.685401201248169, acc: 53.125, f1: 16.96886446886447, r: 0.1361764901661399
06/02/2019 01:16:08 step: 718, epoch: 21, batch: 24, loss: 1.5812891721725464, acc: 50.0, f1: 15.375180375180374, r: 0.18909008363042878
06/02/2019 01:16:08 step: 723, epoch: 21, batch: 29, loss: 1.810437798500061, acc: 37.5, f1: 9.426910299003321, r: 0.05751576886219016
06/02/2019 01:16:09 *** evaluating ***
06/02/2019 01:16:09 step: 22, epoch: 21, acc: 44.871794871794876, f1: 7.7893175074183985, r: 0.22053009058448864
06/02/2019 01:16:09 *** epoch: 23 ***
06/02/2019 01:16:09 *** training ***
06/02/2019 01:16:09 step: 731, epoch: 22, batch: 4, loss: 1.4528306722640991, acc: 50.0, f1: 18.450619148293566, r: 0.2587630925207791
06/02/2019 01:16:09 step: 736, epoch: 22, batch: 9, loss: 1.5155755281448364, acc: 48.4375, f1: 20.46535468119832, r: 0.18242023442208805
06/02/2019 01:16:09 step: 741, epoch: 22, batch: 14, loss: 1.6178292036056519, acc: 35.9375, f1: 14.572384137601532, r: 0.20783078268060684
06/02/2019 01:16:09 step: 746, epoch: 22, batch: 19, loss: 1.6708240509033203, acc: 39.0625, f1: 12.946511938108577, r: 0.1233983166672518
06/02/2019 01:16:09 step: 751, epoch: 22, batch: 24, loss: 1.6022748947143555, acc: 43.75, f1: 7.692307692307692, r: 0.2118477614007854
06/02/2019 01:16:10 step: 756, epoch: 22, batch: 29, loss: 1.6648584604263306, acc: 35.9375, f1: 9.832317073170731, r: 0.18845258892503794
06/02/2019 01:16:10 *** evaluating ***
06/02/2019 01:16:10 step: 23, epoch: 22, acc: 44.871794871794876, f1: 7.7893175074183985, r: 0.1991405654131287
06/02/2019 01:16:10 *** epoch: 24 ***
06/02/2019 01:16:10 *** training ***
06/02/2019 01:16:10 step: 764, epoch: 23, batch: 4, loss: 1.5873918533325195, acc: 42.1875, f1: 14.850156930241978, r: 0.17347508895251484
06/02/2019 01:16:10 step: 769, epoch: 23, batch: 9, loss: 1.600533366203308, acc: 43.75, f1: 12.239165329052968, r: 0.15459603354817308
06/02/2019 01:16:10 step: 774, epoch: 23, batch: 14, loss: 1.6884808540344238, acc: 28.125, f1: 5.625, r: 0.1680520865674136
06/02/2019 01:16:10 step: 779, epoch: 23, batch: 19, loss: 1.5417687892913818, acc: 48.4375, f1: 18.733062330623305, r: 0.1899426849079748
06/02/2019 01:16:10 step: 784, epoch: 23, batch: 24, loss: 1.5675196647644043, acc: 42.1875, f1: 11.492213680268087, r: 0.07414420516380162
06/02/2019 01:16:11 step: 789, epoch: 23, batch: 29, loss: 1.6093041896820068, acc: 32.8125, f1: 18.69047619047619, r: 0.19541797108351044
06/02/2019 01:16:11 *** evaluating ***
06/02/2019 01:16:11 step: 24, epoch: 23, acc: 44.871794871794876, f1: 7.7893175074183985, r: 0.23842611802716282
06/02/2019 01:16:11 *** epoch: 25 ***
06/02/2019 01:16:11 *** training ***
06/02/2019 01:16:11 step: 797, epoch: 24, batch: 4, loss: 1.721840739250183, acc: 28.125, f1: 5.625, r: 0.2139419439219569
06/02/2019 01:16:11 step: 802, epoch: 24, batch: 9, loss: 1.6819788217544556, acc: 34.375, f1: 17.36425339366516, r: 0.15110163251381314
06/02/2019 01:16:11 step: 807, epoch: 24, batch: 14, loss: 1.7327067852020264, acc: 37.5, f1: 10.386593661854985, r: 0.12965557163789038
06/02/2019 01:16:11 step: 812, epoch: 24, batch: 19, loss: 1.6148592233657837, acc: 37.5, f1: 10.893246187363834, r: 0.2275831976865804
06/02/2019 01:16:12 step: 817, epoch: 24, batch: 24, loss: 1.6223386526107788, acc: 46.875, f1: 16.743866085760146, r: 0.23518900594705924
06/02/2019 01:16:12 step: 822, epoch: 24, batch: 29, loss: 1.7712013721466064, acc: 37.5, f1: 14.432773109243696, r: 0.21163722078774128
06/02/2019 01:16:12 *** evaluating ***
06/02/2019 01:16:12 step: 25, epoch: 24, acc: 44.871794871794876, f1: 7.835820895522387, r: 0.2165893510089303
06/02/2019 01:16:12 *** epoch: 26 ***
06/02/2019 01:16:12 *** training ***
06/02/2019 01:16:12 step: 830, epoch: 25, batch: 4, loss: 1.5585134029388428, acc: 37.5, f1: 12.603623727972202, r: 0.2321740150707654
06/02/2019 01:16:12 step: 835, epoch: 25, batch: 9, loss: 1.66104257106781, acc: 46.875, f1: 13.368983957219251, r: 0.10502060205442915
06/02/2019 01:16:12 step: 840, epoch: 25, batch: 14, loss: 1.7502161264419556, acc: 40.625, f1: 13.679688742967066, r: 0.09215938899390788
06/02/2019 01:16:13 step: 845, epoch: 25, batch: 19, loss: 1.6291334629058838, acc: 43.75, f1: 13.181444991789817, r: 0.17413548544587104
06/02/2019 01:16:13 step: 850, epoch: 25, batch: 24, loss: 1.8818378448486328, acc: 45.3125, f1: 19.03453689167975, r: 0.15627223242340313
06/02/2019 01:16:13 step: 855, epoch: 25, batch: 29, loss: 1.6871212720870972, acc: 37.5, f1: 8.850787766450416, r: 0.19799993335878616
06/02/2019 01:16:13 *** evaluating ***
06/02/2019 01:16:13 step: 26, epoch: 25, acc: 44.871794871794876, f1: 7.7893175074183985, r: 0.2085930417658612
06/02/2019 01:16:13 *** epoch: 27 ***
06/02/2019 01:16:13 *** training ***
06/02/2019 01:16:13 step: 863, epoch: 26, batch: 4, loss: 1.6988874673843384, acc: 39.0625, f1: 19.687500000000004, r: 0.24578063388229146
06/02/2019 01:16:13 step: 868, epoch: 26, batch: 9, loss: 1.6856259107589722, acc: 46.875, f1: 15.962643678160921, r: 0.1892994466750723
06/02/2019 01:16:14 step: 873, epoch: 26, batch: 14, loss: 1.5026803016662598, acc: 40.625, f1: 11.791383219954648, r: 0.20302967061972366
06/02/2019 01:16:14 step: 878, epoch: 26, batch: 19, loss: 1.4525783061981201, acc: 40.625, f1: 11.638655462184875, r: 0.22288855054395604
06/02/2019 01:16:14 step: 883, epoch: 26, batch: 24, loss: 1.8158780336380005, acc: 46.875, f1: 14.920634920634921, r: 0.12980305753068835
06/02/2019 01:16:14 step: 888, epoch: 26, batch: 29, loss: 1.4908392429351807, acc: 50.0, f1: 10.090579710144928, r: 0.2102200169674436
06/02/2019 01:16:14 *** evaluating ***
06/02/2019 01:16:14 step: 27, epoch: 26, acc: 44.871794871794876, f1: 7.8125, r: 0.2623849345503796
06/02/2019 01:16:14 *** epoch: 28 ***
06/02/2019 01:16:14 *** training ***
06/02/2019 01:16:14 step: 896, epoch: 27, batch: 4, loss: 1.7391903400421143, acc: 32.8125, f1: 15.515873015873014, r: 0.13752713754497564
06/02/2019 01:16:14 step: 901, epoch: 27, batch: 9, loss: 1.3783907890319824, acc: 50.0, f1: 22.377622377622377, r: 0.25523114016678516
06/02/2019 01:16:15 step: 906, epoch: 27, batch: 14, loss: 1.745850682258606, acc: 35.9375, f1: 9.503676470588236, r: 0.12461726401298212
06/02/2019 01:16:15 step: 911, epoch: 27, batch: 19, loss: 1.7721832990646362, acc: 29.6875, f1: 8.969490425186628, r: 0.18007128453037768
06/02/2019 01:16:15 step: 916, epoch: 27, batch: 24, loss: 1.466084361076355, acc: 43.75, f1: 21.89108201420103, r: 0.29482619031562285
06/02/2019 01:16:15 step: 921, epoch: 27, batch: 29, loss: 1.5917952060699463, acc: 42.1875, f1: 8.571428571428571, r: 0.16380991783509938
06/02/2019 01:16:15 *** evaluating ***
06/02/2019 01:16:15 step: 28, epoch: 27, acc: 44.871794871794876, f1: 7.7893175074183985, r: 0.2502673232993896
06/02/2019 01:16:15 *** epoch: 29 ***
06/02/2019 01:16:15 *** training ***
06/02/2019 01:16:15 step: 929, epoch: 28, batch: 4, loss: 1.4988429546356201, acc: 43.75, f1: 17.961184473789515, r: 0.23942451258878272
06/02/2019 01:16:15 step: 934, epoch: 28, batch: 9, loss: 1.5491827726364136, acc: 45.3125, f1: 12.777777777777777, r: 0.2287774752104707
06/02/2019 01:16:16 step: 939, epoch: 28, batch: 14, loss: 1.6929105520248413, acc: 37.5, f1: 15.01937984496124, r: 0.22230329356288156
06/02/2019 01:16:16 step: 944, epoch: 28, batch: 19, loss: 1.5505650043487549, acc: 45.3125, f1: 25.264403618062154, r: 0.34379300834880006
06/02/2019 01:16:16 step: 949, epoch: 28, batch: 24, loss: 1.5478324890136719, acc: 46.875, f1: 13.152709359605911, r: 0.18402811975599306
06/02/2019 01:16:16 step: 954, epoch: 28, batch: 29, loss: 1.61360764503479, acc: 42.1875, f1: 9.803370786516853, r: 0.1955376064768967
06/02/2019 01:16:16 *** evaluating ***
06/02/2019 01:16:16 step: 29, epoch: 28, acc: 44.871794871794876, f1: 7.7893175074183985, r: 0.24743789559900692
06/02/2019 01:16:16 *** epoch: 30 ***
06/02/2019 01:16:16 *** training ***
06/02/2019 01:16:16 step: 962, epoch: 29, batch: 4, loss: 1.5852323770523071, acc: 42.1875, f1: 12.987012987012989, r: 0.1710975795030927
06/02/2019 01:16:17 step: 967, epoch: 29, batch: 9, loss: 1.6146453619003296, acc: 29.6875, f1: 9.126984126984127, r: 0.21401478520854764
06/02/2019 01:16:17 step: 972, epoch: 29, batch: 14, loss: 1.7015336751937866, acc: 40.625, f1: 10.462321109123435, r: 0.1180019850514623
06/02/2019 01:16:17 step: 977, epoch: 29, batch: 19, loss: 1.528687596321106, acc: 40.625, f1: 23.919567827130855, r: 0.24238243612235413
06/02/2019 01:16:17 step: 982, epoch: 29, batch: 24, loss: 1.4455292224884033, acc: 48.4375, f1: 23.876110502616523, r: 0.2738260961468963
06/02/2019 01:16:17 step: 987, epoch: 29, batch: 29, loss: 1.505051851272583, acc: 43.75, f1: 22.582123758594346, r: 0.2632928986209862
06/02/2019 01:16:17 *** evaluating ***
06/02/2019 01:16:17 step: 30, epoch: 29, acc: 44.871794871794876, f1: 7.8125, r: 0.2279142142339571
06/02/2019 01:16:17 *** epoch: 31 ***
06/02/2019 01:16:17 *** training ***
06/02/2019 01:16:17 step: 995, epoch: 30, batch: 4, loss: 1.7632390260696411, acc: 34.375, f1: 10.062192518332868, r: 0.10245605610854641
06/02/2019 01:16:18 step: 1000, epoch: 30, batch: 9, loss: 1.6115422248840332, acc: 43.75, f1: 19.444700957306, r: 0.23883806274940203
06/02/2019 01:16:18 step: 1005, epoch: 30, batch: 14, loss: 1.4730808734893799, acc: 43.75, f1: 21.395730706075536, r: 0.26286916145677536
06/02/2019 01:16:18 step: 1010, epoch: 30, batch: 19, loss: 1.748654842376709, acc: 34.375, f1: 9.775762156714537, r: 0.13509770599312199
06/02/2019 01:16:18 step: 1015, epoch: 30, batch: 24, loss: 1.7837803363800049, acc: 39.0625, f1: 10.738916256157635, r: 0.08741467652397938
06/02/2019 01:16:18 step: 1020, epoch: 30, batch: 29, loss: 1.6902869939804077, acc: 37.5, f1: 14.380570409982173, r: 0.20268631764823036
06/02/2019 01:16:18 *** evaluating ***
06/02/2019 01:16:18 step: 31, epoch: 30, acc: 44.871794871794876, f1: 7.7893175074183985, r: 0.22079253559031106
06/02/2019 01:16:18 *** epoch: 32 ***
06/02/2019 01:16:18 *** training ***
06/02/2019 01:16:19 step: 1028, epoch: 31, batch: 4, loss: 1.4684789180755615, acc: 37.5, f1: 21.770451770451768, r: 0.263379628804997
06/02/2019 01:16:19 step: 1033, epoch: 31, batch: 9, loss: 1.6001482009887695, acc: 35.9375, f1: 14.047270955165692, r: 0.20133200348564467
06/02/2019 01:16:19 step: 1038, epoch: 31, batch: 14, loss: 1.4821972846984863, acc: 37.5, f1: 8.769379844961241, r: 0.26352165142284684
06/02/2019 01:16:19 step: 1043, epoch: 31, batch: 19, loss: 1.6354926824569702, acc: 42.1875, f1: 7.5, r: 0.1277403098725657
06/02/2019 01:16:19 step: 1048, epoch: 31, batch: 24, loss: 1.9283148050308228, acc: 32.8125, f1: 11.651287309182047, r: 0.10871422296893309
06/02/2019 01:16:20 step: 1053, epoch: 31, batch: 29, loss: 1.4457157850265503, acc: 43.75, f1: 13.760504201680673, r: 0.26227174566866174
06/02/2019 01:16:20 *** evaluating ***
06/02/2019 01:16:20 step: 32, epoch: 31, acc: 44.871794871794876, f1: 7.8125, r: 0.23697072044867062
06/02/2019 01:16:20 *** epoch: 33 ***
06/02/2019 01:16:20 *** training ***
06/02/2019 01:16:20 step: 1061, epoch: 32, batch: 4, loss: 1.4505038261413574, acc: 40.625, f1: 17.346938775510207, r: 0.2645771841052338
06/02/2019 01:16:20 step: 1066, epoch: 32, batch: 9, loss: 1.5178762674331665, acc: 54.6875, f1: 20.32982890125747, r: 0.1803066090550231
06/02/2019 01:16:20 step: 1071, epoch: 32, batch: 14, loss: 1.5049394369125366, acc: 43.75, f1: 15.686274509803921, r: 0.22124034870345016
06/02/2019 01:16:20 step: 1076, epoch: 32, batch: 19, loss: 1.5305466651916504, acc: 46.875, f1: 13.070396698005046, r: 0.2107910768668248
06/02/2019 01:16:20 step: 1081, epoch: 32, batch: 24, loss: 1.6527197360992432, acc: 40.625, f1: 11.225956457408582, r: 0.15418054126267133
06/02/2019 01:16:21 step: 1086, epoch: 32, batch: 29, loss: 1.5422778129577637, acc: 40.625, f1: 10.227272727272728, r: 0.20542854312524197
06/02/2019 01:16:21 *** evaluating ***
06/02/2019 01:16:21 step: 33, epoch: 32, acc: 44.871794871794876, f1: 7.8125, r: 0.23916147976674468
06/02/2019 01:16:21 *** epoch: 34 ***
06/02/2019 01:16:21 *** training ***
06/02/2019 01:16:21 step: 1094, epoch: 33, batch: 4, loss: 1.9548054933547974, acc: 45.3125, f1: 9.444444444444445, r: 0.21280441549157364
06/02/2019 01:16:21 step: 1099, epoch: 33, batch: 9, loss: 1.5396915674209595, acc: 45.3125, f1: 8.90937019969278, r: 0.15444834273912939
06/02/2019 01:16:21 step: 1104, epoch: 33, batch: 14, loss: 1.4338743686676025, acc: 43.75, f1: 15.909090909090912, r: 0.21364284767488312
06/02/2019 01:16:21 step: 1109, epoch: 33, batch: 19, loss: 1.4991142749786377, acc: 37.5, f1: 11.027568922305765, r: 0.2897128130070686
06/02/2019 01:16:21 step: 1114, epoch: 33, batch: 24, loss: 1.459211826324463, acc: 46.875, f1: 14.778297682709448, r: 0.1822281883668558
06/02/2019 01:16:22 step: 1119, epoch: 33, batch: 29, loss: 1.4543683528900146, acc: 43.75, f1: 13.770053475935828, r: 0.2446111518271681
06/02/2019 01:16:22 *** evaluating ***
06/02/2019 01:16:22 step: 34, epoch: 33, acc: 44.871794871794876, f1: 7.7893175074183985, r: 0.24314994376539448
06/02/2019 01:16:22 *** epoch: 35 ***
06/02/2019 01:16:22 *** training ***
06/02/2019 01:16:22 step: 1127, epoch: 34, batch: 4, loss: 1.6831786632537842, acc: 34.375, f1: 8.732675061146843, r: 0.1520741252781815
06/02/2019 01:16:22 step: 1132, epoch: 34, batch: 9, loss: 1.4983264207839966, acc: 40.625, f1: 24.99084249084249, r: 0.37865513486032804
06/02/2019 01:16:22 step: 1137, epoch: 34, batch: 14, loss: 1.4896025657653809, acc: 43.75, f1: 14.899054433938156, r: 0.26191810186716047
06/02/2019 01:16:22 step: 1142, epoch: 34, batch: 19, loss: 1.5214399099349976, acc: 40.625, f1: 11.22549019607843, r: 0.2875450351994191
06/02/2019 01:16:22 step: 1147, epoch: 34, batch: 24, loss: 1.7741823196411133, acc: 42.1875, f1: 9.30944055944056, r: 0.1913628839929296
06/02/2019 01:16:23 step: 1152, epoch: 34, batch: 29, loss: 1.4694008827209473, acc: 43.75, f1: 25.513879683648856, r: 0.27438801383984435
06/02/2019 01:16:23 *** evaluating ***
06/02/2019 01:16:23 step: 35, epoch: 34, acc: 44.871794871794876, f1: 7.7893175074183985, r: 0.23322068687623645
06/02/2019 01:16:23 *** epoch: 36 ***
06/02/2019 01:16:23 *** training ***
06/02/2019 01:16:23 step: 1160, epoch: 35, batch: 4, loss: 1.4618475437164307, acc: 43.75, f1: 10.144927536231885, r: 0.19998856319287162
06/02/2019 01:16:23 step: 1165, epoch: 35, batch: 9, loss: 1.4851709604263306, acc: 48.4375, f1: 16.387895812053117, r: 0.29691619644979084
06/02/2019 01:16:23 step: 1170, epoch: 35, batch: 14, loss: 1.4625064134597778, acc: 42.1875, f1: 20.90108401084011, r: 0.24824379766903767
06/02/2019 01:16:23 step: 1175, epoch: 35, batch: 19, loss: 1.5309758186340332, acc: 34.375, f1: 7.394957983193276, r: 0.19103806774717522
06/02/2019 01:16:24 step: 1180, epoch: 35, batch: 24, loss: 1.3518162965774536, acc: 57.8125, f1: 19.626679162761636, r: 0.167873258080789
06/02/2019 01:16:24 step: 1185, epoch: 35, batch: 29, loss: 1.6180810928344727, acc: 39.0625, f1: 12.812499999999998, r: 0.18756286515870424
06/02/2019 01:16:24 *** evaluating ***
06/02/2019 01:16:24 step: 36, epoch: 35, acc: 44.871794871794876, f1: 7.8125, r: 0.252668793822119
06/02/2019 01:16:24 *** epoch: 37 ***
06/02/2019 01:16:24 *** training ***
06/02/2019 01:16:24 step: 1193, epoch: 36, batch: 4, loss: 1.7763481140136719, acc: 39.0625, f1: 7.022471910112358, r: 0.18324888374637915
06/02/2019 01:16:24 step: 1198, epoch: 36, batch: 9, loss: 1.4753873348236084, acc: 45.3125, f1: 11.728896103896105, r: 0.23960860623062288
06/02/2019 01:16:24 step: 1203, epoch: 36, batch: 14, loss: 1.6501723527908325, acc: 42.1875, f1: 7.417582417582418, r: 0.14355968237945274
06/02/2019 01:16:25 step: 1208, epoch: 36, batch: 19, loss: 1.7122620344161987, acc: 29.6875, f1: 7.158119658119658, r: 0.15551801083397665
06/02/2019 01:16:25 step: 1213, epoch: 36, batch: 24, loss: 1.581799864768982, acc: 43.75, f1: 11.931818181818182, r: 0.1488538463818453
06/02/2019 01:16:25 step: 1218, epoch: 36, batch: 29, loss: 1.9862288236618042, acc: 40.625, f1: 12.098765432098766, r: 0.09954234855964973
06/02/2019 01:16:25 *** evaluating ***
06/02/2019 01:16:25 step: 37, epoch: 36, acc: 44.871794871794876, f1: 7.7893175074183985, r: 0.26304237005294934
06/02/2019 01:16:25 *** epoch: 38 ***
06/02/2019 01:16:25 *** training ***
06/02/2019 01:16:25 step: 1226, epoch: 37, batch: 4, loss: 1.4733631610870361, acc: 43.75, f1: 15.800865800865799, r: 0.22686624305486702
06/02/2019 01:16:25 step: 1231, epoch: 37, batch: 9, loss: 1.4634851217269897, acc: 53.125, f1: 11.935483870967742, r: 0.1617364538009113
06/02/2019 01:16:26 step: 1236, epoch: 37, batch: 14, loss: 1.4364475011825562, acc: 50.0, f1: 24.646464646464647, r: 0.30271377157501544
06/02/2019 01:16:26 step: 1241, epoch: 37, batch: 19, loss: 1.566041350364685, acc: 43.75, f1: 12.386489479512736, r: 0.17850614501246703
06/02/2019 01:16:26 step: 1246, epoch: 37, batch: 24, loss: 1.5160832405090332, acc: 43.75, f1: 13.296833885069178, r: 0.17848388922841
06/02/2019 01:16:26 step: 1251, epoch: 37, batch: 29, loss: 1.464186191558838, acc: 48.4375, f1: 23.274478330658106, r: 0.3021223238212334
06/02/2019 01:16:26 *** evaluating ***
06/02/2019 01:16:26 step: 38, epoch: 37, acc: 44.871794871794876, f1: 7.7893175074183985, r: 0.23974289891278236
06/02/2019 01:16:26 *** epoch: 39 ***
06/02/2019 01:16:26 *** training ***
06/02/2019 01:16:26 step: 1259, epoch: 38, batch: 4, loss: 1.4915392398834229, acc: 42.1875, f1: 16.19047619047619, r: 0.2059771656877878
06/02/2019 01:16:26 step: 1264, epoch: 38, batch: 9, loss: 1.618041753768921, acc: 35.9375, f1: 14.076576576576578, r: 0.19514486686215707
06/02/2019 01:16:27 step: 1269, epoch: 38, batch: 14, loss: 1.6622546911239624, acc: 28.125, f1: 9.30451127819549, r: 0.15809628753506028
06/02/2019 01:16:27 step: 1274, epoch: 38, batch: 19, loss: 1.5168126821517944, acc: 40.625, f1: 12.797619047619047, r: 0.2225676163081277
06/02/2019 01:16:27 step: 1279, epoch: 38, batch: 24, loss: 1.5820058584213257, acc: 40.625, f1: 15.539215686274513, r: 0.20391919751278462
06/02/2019 01:16:27 step: 1284, epoch: 38, batch: 29, loss: 1.5153690576553345, acc: 40.625, f1: 8.253968253968253, r: 0.20789805246369086
06/02/2019 01:16:27 *** evaluating ***
06/02/2019 01:16:27 step: 39, epoch: 38, acc: 44.871794871794876, f1: 7.7893175074183985, r: 0.2609970624004134
06/02/2019 01:16:27 *** epoch: 40 ***
06/02/2019 01:16:27 *** training ***
06/02/2019 01:16:27 step: 1292, epoch: 39, batch: 4, loss: 1.595696210861206, acc: 40.625, f1: 13.850787766450418, r: 0.15799764864268878
06/02/2019 01:16:27 step: 1297, epoch: 39, batch: 9, loss: 1.4823005199432373, acc: 35.9375, f1: 9.432870370370368, r: 0.24665032031713727
06/02/2019 01:16:28 step: 1302, epoch: 39, batch: 14, loss: 1.4320892095565796, acc: 48.4375, f1: 19.94163471995984, r: 0.2872947772033722
06/02/2019 01:16:28 step: 1307, epoch: 39, batch: 19, loss: 1.6165542602539062, acc: 37.5, f1: 10.34013605442177, r: 0.1559684388840197
06/02/2019 01:16:28 step: 1312, epoch: 39, batch: 24, loss: 1.5059374570846558, acc: 40.625, f1: 20.331465919701216, r: 0.27266309921152876
06/02/2019 01:16:28 step: 1317, epoch: 39, batch: 29, loss: 1.4872832298278809, acc: 40.625, f1: 16.387309737759033, r: 0.20862638435005582
06/02/2019 01:16:28 *** evaluating ***
06/02/2019 01:16:28 step: 40, epoch: 39, acc: 44.871794871794876, f1: 7.906626506024097, r: 0.2399017657074877
06/02/2019 01:16:28 *** epoch: 41 ***
06/02/2019 01:16:28 *** training ***
06/02/2019 01:16:28 step: 1325, epoch: 40, batch: 4, loss: 1.566684365272522, acc: 40.625, f1: 8.253968253968253, r: 0.21237803652251394
06/02/2019 01:16:28 step: 1330, epoch: 40, batch: 9, loss: 1.492441177368164, acc: 42.1875, f1: 9.659090909090908, r: 0.22090599585590326
06/02/2019 01:16:29 step: 1335, epoch: 40, batch: 14, loss: 1.495054006576538, acc: 42.1875, f1: 13.456114152978262, r: 0.21882525938763883
06/02/2019 01:16:29 step: 1340, epoch: 40, batch: 19, loss: 1.508565902709961, acc: 43.75, f1: 17.523852116875375, r: 0.22020985116990993
06/02/2019 01:16:29 step: 1345, epoch: 40, batch: 24, loss: 1.593306064605713, acc: 39.0625, f1: 14.188712522045854, r: 0.14317961107436733
06/02/2019 01:16:29 step: 1350, epoch: 40, batch: 29, loss: 1.457930326461792, acc: 43.75, f1: 14.337393872277593, r: 0.1598921871222349
06/02/2019 01:16:29 *** evaluating ***
06/02/2019 01:16:29 step: 41, epoch: 40, acc: 44.871794871794876, f1: 7.8125, r: 0.23328501685868425
06/02/2019 01:16:29 *** epoch: 42 ***
06/02/2019 01:16:29 *** training ***
06/02/2019 01:16:29 step: 1358, epoch: 41, batch: 4, loss: 1.478435754776001, acc: 40.625, f1: 11.442809682012339, r: 0.18749109376241507
06/02/2019 01:16:29 step: 1363, epoch: 41, batch: 9, loss: 1.4251612424850464, acc: 45.3125, f1: 10.569561157796452, r: 0.21108044032785653
06/02/2019 01:16:30 step: 1368, epoch: 41, batch: 14, loss: 1.263242483139038, acc: 53.125, f1: 16.666666666666668, r: 0.2845872871020952
06/02/2019 01:16:30 step: 1373, epoch: 41, batch: 19, loss: 1.5816787481307983, acc: 40.625, f1: 16.585365853658534, r: 0.2596548748969749
06/02/2019 01:16:30 step: 1378, epoch: 41, batch: 24, loss: 1.5372425317764282, acc: 37.5, f1: 16.88291628050664, r: 0.26819769606329275
06/02/2019 01:16:30 step: 1383, epoch: 41, batch: 29, loss: 1.4850146770477295, acc: 40.625, f1: 14.241723466407011, r: 0.22607270202187663
06/02/2019 01:16:30 *** evaluating ***
06/02/2019 01:16:30 step: 42, epoch: 41, acc: 44.871794871794876, f1: 7.8125, r: 0.2598718752548952
06/02/2019 01:16:30 *** epoch: 43 ***
06/02/2019 01:16:30 *** training ***
06/02/2019 01:16:30 step: 1391, epoch: 42, batch: 4, loss: 1.5302056074142456, acc: 35.9375, f1: 6.686046511627906, r: 0.24282141903138182
06/02/2019 01:16:31 step: 1396, epoch: 42, batch: 9, loss: 1.5630978345870972, acc: 29.6875, f1: 11.584249084249084, r: 0.17406515769785727
06/02/2019 01:16:31 step: 1401, epoch: 42, batch: 14, loss: 1.8149396181106567, acc: 42.1875, f1: 15.350769350225821, r: 0.12456904233872508
06/02/2019 01:16:31 step: 1406, epoch: 42, batch: 19, loss: 1.8007488250732422, acc: 32.8125, f1: 7.764227642276422, r: 0.14618303061069918
06/02/2019 01:16:31 step: 1411, epoch: 42, batch: 24, loss: 1.5785130262374878, acc: 42.1875, f1: 8.47723704866562, r: 0.16977396794151012
06/02/2019 01:16:31 step: 1416, epoch: 42, batch: 29, loss: 1.5133247375488281, acc: 45.3125, f1: 17.620798319327733, r: 0.18338499850993267
06/02/2019 01:16:31 *** evaluating ***
06/02/2019 01:16:31 step: 43, epoch: 42, acc: 44.871794871794876, f1: 7.8125, r: 0.2550392688199812
06/02/2019 01:16:31 *** epoch: 44 ***
06/02/2019 01:16:31 *** training ***
06/02/2019 01:16:32 step: 1424, epoch: 43, batch: 4, loss: 1.9086759090423584, acc: 45.3125, f1: 14.074074074074073, r: 0.20359202683474287
06/02/2019 01:16:32 step: 1429, epoch: 43, batch: 9, loss: 1.6302478313446045, acc: 46.875, f1: 9.216589861751153, r: 0.16077987201228378
06/02/2019 01:16:32 step: 1434, epoch: 43, batch: 14, loss: 1.456507921218872, acc: 42.1875, f1: 12.829131652661067, r: 0.2323617267580837
06/02/2019 01:16:32 step: 1439, epoch: 43, batch: 19, loss: 1.4854322671890259, acc: 48.4375, f1: 16.131621187800967, r: 0.16745722222542053
06/02/2019 01:16:32 step: 1444, epoch: 43, batch: 24, loss: 1.719039797782898, acc: 37.5, f1: 7.973421926910299, r: 0.2286781579108517
06/02/2019 01:16:32 step: 1449, epoch: 43, batch: 29, loss: 1.5303068161010742, acc: 32.8125, f1: 11.248647186147187, r: 0.2802942625115171
06/02/2019 01:16:32 *** evaluating ***
06/02/2019 01:16:33 step: 44, epoch: 43, acc: 44.871794871794876, f1: 7.859281437125748, r: 0.2507250145402879
06/02/2019 01:16:33 *** epoch: 45 ***
06/02/2019 01:16:33 *** training ***
06/02/2019 01:16:33 step: 1457, epoch: 44, batch: 4, loss: 1.6528319120407104, acc: 26.5625, f1: 8.75, r: 0.18989134137872649
06/02/2019 01:16:33 step: 1462, epoch: 44, batch: 9, loss: 1.4535002708435059, acc: 48.4375, f1: 14.044289044289043, r: 0.2125491188744745
06/02/2019 01:16:33 step: 1467, epoch: 44, batch: 14, loss: 1.4369205236434937, acc: 51.5625, f1: 29.925467262928564, r: 0.18660121559640633
06/02/2019 01:16:33 step: 1472, epoch: 44, batch: 19, loss: 1.3979856967926025, acc: 43.75, f1: 11.875284997720017, r: 0.2446802661164745
06/02/2019 01:16:33 step: 1477, epoch: 44, batch: 24, loss: 1.5014311075210571, acc: 40.625, f1: 12.075986029474402, r: 0.15757980013054526
06/02/2019 01:16:33 step: 1482, epoch: 44, batch: 29, loss: 1.4151692390441895, acc: 46.875, f1: 14.217032967032967, r: 0.2621281858252358
06/02/2019 01:16:33 *** evaluating ***
06/02/2019 01:16:34 step: 45, epoch: 44, acc: 44.871794871794876, f1: 7.8125, r: 0.21184285814997272
06/02/2019 01:16:34 *** epoch: 46 ***
06/02/2019 01:16:34 *** training ***
06/02/2019 01:16:34 step: 1490, epoch: 45, batch: 4, loss: 1.6576790809631348, acc: 48.4375, f1: 18.437385907265423, r: 0.22456120003380892
06/02/2019 01:16:34 step: 1495, epoch: 45, batch: 9, loss: 1.4505212306976318, acc: 43.75, f1: 13.582251082251084, r: 0.23852014830872537
06/02/2019 01:16:34 step: 1500, epoch: 45, batch: 14, loss: 1.4931354522705078, acc: 43.75, f1: 13.464052287581701, r: 0.24384175460373309
06/02/2019 01:16:34 step: 1505, epoch: 45, batch: 19, loss: 1.4406921863555908, acc: 48.4375, f1: 21.084629656058226, r: 0.2049199780209525
06/02/2019 01:16:34 step: 1510, epoch: 45, batch: 24, loss: 1.5675935745239258, acc: 35.9375, f1: 11.262651402024224, r: 0.16781706616943534
06/02/2019 01:16:34 step: 1515, epoch: 45, batch: 29, loss: 1.5472288131713867, acc: 46.875, f1: 14.99169435215947, r: 0.15195824294423227
06/02/2019 01:16:34 *** evaluating ***
06/02/2019 01:16:35 step: 46, epoch: 45, acc: 44.871794871794876, f1: 7.8125, r: 0.2607839536691242
06/02/2019 01:16:35 *** epoch: 47 ***
06/02/2019 01:16:35 *** training ***
06/02/2019 01:16:35 step: 1523, epoch: 46, batch: 4, loss: 1.527480959892273, acc: 35.9375, f1: 11.654135338345863, r: 0.2054847690702391
06/02/2019 01:16:35 step: 1528, epoch: 46, batch: 9, loss: 1.576934814453125, acc: 31.25, f1: 8.571428571428571, r: 0.26066770546233187
06/02/2019 01:16:35 step: 1533, epoch: 46, batch: 14, loss: 1.3235790729522705, acc: 48.4375, f1: 15.05028735632184, r: 0.24261888216660657
06/02/2019 01:16:35 step: 1538, epoch: 46, batch: 19, loss: 1.4024040699005127, acc: 45.3125, f1: 23.99099883855981, r: 0.2658100690894722
06/02/2019 01:16:35 step: 1543, epoch: 46, batch: 24, loss: 1.344571590423584, acc: 48.4375, f1: 22.745889113026468, r: 0.3137186512630129
06/02/2019 01:16:35 step: 1548, epoch: 46, batch: 29, loss: 1.6915297508239746, acc: 37.5, f1: 9.542483660130719, r: 0.2857320783703138
06/02/2019 01:16:35 *** evaluating ***
06/02/2019 01:16:36 step: 47, epoch: 46, acc: 44.871794871794876, f1: 7.8125, r: 0.2633054931301326
06/02/2019 01:16:36 *** epoch: 48 ***
06/02/2019 01:16:36 *** training ***
06/02/2019 01:16:36 step: 1556, epoch: 47, batch: 4, loss: 1.8631378412246704, acc: 31.25, f1: 11.052489177489178, r: 0.24499692128619008
06/02/2019 01:16:36 step: 1561, epoch: 47, batch: 9, loss: 1.4910537004470825, acc: 43.75, f1: 17.207152903355436, r: 0.27249769309103833
06/02/2019 01:16:36 step: 1566, epoch: 47, batch: 14, loss: 1.5093669891357422, acc: 51.5625, f1: 16.098187526758956, r: 0.1646546343037415
06/02/2019 01:16:36 step: 1571, epoch: 47, batch: 19, loss: 1.6080752611160278, acc: 40.625, f1: 11.434108527131784, r: 0.21997521859607172
06/02/2019 01:16:36 step: 1576, epoch: 47, batch: 24, loss: 1.5485365390777588, acc: 42.1875, f1: 17.52568716424138, r: 0.19993313933589404
06/02/2019 01:16:36 step: 1581, epoch: 47, batch: 29, loss: 1.5201036930084229, acc: 35.9375, f1: 10.651848151848151, r: 0.17379401166452793
06/02/2019 01:16:36 *** evaluating ***
06/02/2019 01:16:37 step: 48, epoch: 47, acc: 44.871794871794876, f1: 7.930513595166164, r: 0.255289425419415
06/02/2019 01:16:37 *** epoch: 49 ***
06/02/2019 01:16:37 *** training ***
06/02/2019 01:16:37 step: 1589, epoch: 48, batch: 4, loss: 1.512589693069458, acc: 34.375, f1: 11.157323688969258, r: 0.22931649666329273
06/02/2019 01:16:37 step: 1594, epoch: 48, batch: 9, loss: 1.3181054592132568, acc: 48.4375, f1: 21.840053094408493, r: 0.2554215362970028
06/02/2019 01:16:37 step: 1599, epoch: 48, batch: 14, loss: 1.5206849575042725, acc: 37.5, f1: 9.485094850948508, r: 0.176898087366648
06/02/2019 01:16:37 step: 1604, epoch: 48, batch: 19, loss: 1.4674441814422607, acc: 42.1875, f1: 10.944112067707572, r: 0.21947348956295257
06/02/2019 01:16:37 step: 1609, epoch: 48, batch: 24, loss: 1.5503172874450684, acc: 37.5, f1: 11.634774391684385, r: 0.24352341898587337
06/02/2019 01:16:37 step: 1614, epoch: 48, batch: 29, loss: 1.5619821548461914, acc: 34.375, f1: 9.08974358974359, r: 0.17930066784907558
06/02/2019 01:16:37 *** evaluating ***
06/02/2019 01:16:38 step: 49, epoch: 48, acc: 44.871794871794876, f1: 7.7893175074183985, r: 0.2487857903689027
06/02/2019 01:16:38 *** epoch: 50 ***
06/02/2019 01:16:38 *** training ***
06/02/2019 01:16:38 step: 1622, epoch: 49, batch: 4, loss: 2.1176936626434326, acc: 35.9375, f1: 13.550644653718793, r: 0.15554547678251113
06/02/2019 01:16:38 step: 1627, epoch: 49, batch: 9, loss: 1.6610459089279175, acc: 31.25, f1: 8.661025385764063, r: 0.22239227908720388
06/02/2019 01:16:38 step: 1632, epoch: 49, batch: 14, loss: 1.4961317777633667, acc: 46.875, f1: 17.85477453580902, r: 0.213028230739199
06/02/2019 01:16:38 step: 1637, epoch: 49, batch: 19, loss: 1.443084955215454, acc: 34.375, f1: 11.661525369641526, r: 0.18087371847389586
06/02/2019 01:16:38 step: 1642, epoch: 49, batch: 24, loss: 1.382460117340088, acc: 46.875, f1: 23.703703703703706, r: 0.2107001453714606
06/02/2019 01:16:39 step: 1647, epoch: 49, batch: 29, loss: 1.2879157066345215, acc: 51.5625, f1: 23.23747680890538, r: 0.296624283982745
06/02/2019 01:16:39 *** evaluating ***
06/02/2019 01:16:39 step: 50, epoch: 49, acc: 44.871794871794876, f1: 7.8125, r: 0.24742243113093446
06/02/2019 01:16:39 *** epoch: 51 ***
06/02/2019 01:16:39 *** training ***
06/02/2019 01:16:39 step: 1655, epoch: 50, batch: 4, loss: 1.5044552087783813, acc: 34.375, f1: 6.395348837209303, r: 0.262728793061218
06/02/2019 01:16:39 step: 1660, epoch: 50, batch: 9, loss: 1.1948214769363403, acc: 43.75, f1: 20.010270930490783, r: 0.36051206158532634
06/02/2019 01:16:39 step: 1665, epoch: 50, batch: 14, loss: 1.604689598083496, acc: 42.1875, f1: 10.957792207792208, r: 0.19138598754031094
06/02/2019 01:16:39 step: 1670, epoch: 50, batch: 19, loss: 1.4116979837417603, acc: 43.75, f1: 24.523809523809526, r: 0.24898784763804482
06/02/2019 01:16:39 step: 1675, epoch: 50, batch: 24, loss: 1.4772415161132812, acc: 42.1875, f1: 9.89010989010989, r: 0.1660975797555416
06/02/2019 01:16:40 step: 1680, epoch: 50, batch: 29, loss: 1.5463453531265259, acc: 45.3125, f1: 13.482501434308663, r: 0.16114606314393148
06/02/2019 01:16:40 *** evaluating ***
06/02/2019 01:16:40 step: 51, epoch: 50, acc: 44.871794871794876, f1: 7.7893175074183985, r: 0.26241105591673874
06/02/2019 01:16:40 *** epoch: 52 ***
06/02/2019 01:16:40 *** training ***
06/02/2019 01:16:40 step: 1688, epoch: 51, batch: 4, loss: 1.4874624013900757, acc: 43.75, f1: 13.733393043737868, r: 0.22286524435390195
06/02/2019 01:16:40 step: 1693, epoch: 51, batch: 9, loss: 1.5798736810684204, acc: 34.375, f1: 11.826086956521738, r: 0.24504611247006786
06/02/2019 01:16:40 step: 1698, epoch: 51, batch: 14, loss: 1.2761659622192383, acc: 48.4375, f1: 22.541528239202655, r: 0.3132021380909054
06/02/2019 01:16:40 step: 1703, epoch: 51, batch: 19, loss: 1.3501358032226562, acc: 42.1875, f1: 12.019704433497537, r: 0.30843054859546654
06/02/2019 01:16:40 step: 1708, epoch: 51, batch: 24, loss: 1.6128759384155273, acc: 37.5, f1: 9.95983935742972, r: 0.20181172318633456
06/02/2019 01:16:41 step: 1713, epoch: 51, batch: 29, loss: 1.3958858251571655, acc: 39.0625, f1: 15.287433155080214, r: 0.3132340613343466
06/02/2019 01:16:41 *** evaluating ***
06/02/2019 01:16:41 step: 52, epoch: 51, acc: 44.871794871794876, f1: 7.7893175074183985, r: 0.2664414384868848
06/02/2019 01:16:41 *** epoch: 53 ***
06/02/2019 01:16:41 *** training ***
06/02/2019 01:16:41 step: 1721, epoch: 52, batch: 4, loss: 1.4760669469833374, acc: 40.625, f1: 27.57971014492754, r: 0.3817300599089766
06/02/2019 01:16:41 step: 1726, epoch: 52, batch: 9, loss: 1.6362327337265015, acc: 42.1875, f1: 11.616161616161616, r: 0.1976800065991516
06/02/2019 01:16:41 step: 1731, epoch: 52, batch: 14, loss: 1.7845243215560913, acc: 26.5625, f1: 5.3125, r: 0.1836949525032886
06/02/2019 01:16:41 step: 1736, epoch: 52, batch: 19, loss: 1.6071271896362305, acc: 37.5, f1: 11.983648881239244, r: 0.15608524044649966
06/02/2019 01:16:41 step: 1741, epoch: 52, batch: 24, loss: 1.4667621850967407, acc: 46.875, f1: 12.06097273513004, r: 0.18575302743605696
06/02/2019 01:16:42 step: 1746, epoch: 52, batch: 29, loss: 1.5125716924667358, acc: 39.0625, f1: 9.789972899728996, r: 0.21096815152190448
06/02/2019 01:16:42 *** evaluating ***
06/02/2019 01:16:42 step: 53, epoch: 52, acc: 44.871794871794876, f1: 7.7893175074183985, r: 0.25544094665888006
06/02/2019 01:16:42 *** epoch: 54 ***
06/02/2019 01:16:42 *** training ***
06/02/2019 01:16:42 step: 1754, epoch: 53, batch: 4, loss: 1.462980031967163, acc: 45.3125, f1: 10.793650793650796, r: 0.16542815891990625
06/02/2019 01:16:42 step: 1759, epoch: 53, batch: 9, loss: 1.4456707239151, acc: 43.75, f1: 12.624584717607974, r: 0.257862308688364
06/02/2019 01:16:42 step: 1764, epoch: 53, batch: 14, loss: 1.404801845550537, acc: 46.875, f1: 17.525703463203463, r: 0.23660281937245936
06/02/2019 01:16:42 step: 1769, epoch: 53, batch: 19, loss: 1.6307551860809326, acc: 42.1875, f1: 15.09327092660426, r: 0.1876702783732939
06/02/2019 01:16:42 step: 1774, epoch: 53, batch: 24, loss: 1.4646552801132202, acc: 32.8125, f1: 15.239197530864198, r: 0.29969608442374723
06/02/2019 01:16:43 step: 1779, epoch: 53, batch: 29, loss: 1.512954592704773, acc: 40.625, f1: 17.105044345898, r: 0.2785732387477661
06/02/2019 01:16:43 *** evaluating ***
06/02/2019 01:16:43 step: 54, epoch: 53, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.25887116886118133
06/02/2019 01:16:43 *** epoch: 55 ***
06/02/2019 01:16:43 *** training ***
06/02/2019 01:16:43 step: 1787, epoch: 54, batch: 4, loss: 1.6305861473083496, acc: 34.375, f1: 13.248299319727892, r: 0.148782020581091
06/02/2019 01:16:43 step: 1792, epoch: 54, batch: 9, loss: 1.402060627937317, acc: 48.4375, f1: 18.2224025974026, r: 0.2787464996299183
06/02/2019 01:16:43 step: 1797, epoch: 54, batch: 14, loss: 1.4888359308242798, acc: 40.625, f1: 12.428662016487001, r: 0.2675513163418686
06/02/2019 01:16:43 step: 1802, epoch: 54, batch: 19, loss: 1.340333104133606, acc: 48.4375, f1: 13.500784929356357, r: 0.25855358095513264
06/02/2019 01:16:43 step: 1807, epoch: 54, batch: 24, loss: 1.3383468389511108, acc: 48.4375, f1: 17.051184110007643, r: 0.20433416136947316
06/02/2019 01:16:44 step: 1812, epoch: 54, batch: 29, loss: 1.438396692276001, acc: 46.875, f1: 13.593073593073596, r: 0.24441785308610897
06/02/2019 01:16:44 *** evaluating ***
06/02/2019 01:16:44 step: 55, epoch: 54, acc: 44.871794871794876, f1: 7.7893175074183985, r: 0.2469775511222186
06/02/2019 01:16:44 *** epoch: 56 ***
06/02/2019 01:16:44 *** training ***
06/02/2019 01:16:44 step: 1820, epoch: 55, batch: 4, loss: 1.533831238746643, acc: 37.5, f1: 16.323529411764707, r: 0.2046178897365036
06/02/2019 01:16:44 step: 1825, epoch: 55, batch: 9, loss: 1.5804792642593384, acc: 39.0625, f1: 10.738916256157635, r: 0.1805647964685536
06/02/2019 01:16:44 step: 1830, epoch: 55, batch: 14, loss: 1.530707836151123, acc: 48.4375, f1: 8.24468085106383, r: 0.16920576907419405
06/02/2019 01:16:44 step: 1835, epoch: 55, batch: 19, loss: 1.4798208475112915, acc: 37.5, f1: 16.299596761781636, r: 0.2211116377793607
06/02/2019 01:16:44 step: 1840, epoch: 55, batch: 24, loss: 1.470079779624939, acc: 34.375, f1: 9.228395061728396, r: 0.18038666360624597
06/02/2019 01:16:45 step: 1845, epoch: 55, batch: 29, loss: 1.4883089065551758, acc: 43.75, f1: 7.608695652173914, r: 0.18882831882995468
06/02/2019 01:16:45 *** evaluating ***
06/02/2019 01:16:45 step: 56, epoch: 55, acc: 44.871794871794876, f1: 7.8125, r: 0.2394119937731701
06/02/2019 01:16:45 *** epoch: 57 ***
06/02/2019 01:16:45 *** training ***
06/02/2019 01:16:45 step: 1853, epoch: 56, batch: 4, loss: 1.5523992776870728, acc: 34.375, f1: 6.470588235294117, r: 0.24750520831572193
06/02/2019 01:16:45 step: 1858, epoch: 56, batch: 9, loss: 1.5707101821899414, acc: 34.375, f1: 10.416666666666666, r: 0.20526012828596385
06/02/2019 01:16:45 step: 1863, epoch: 56, batch: 14, loss: 1.4272937774658203, acc: 39.0625, f1: 18.686483739837396, r: 0.24323642563334144
06/02/2019 01:16:45 step: 1868, epoch: 56, batch: 19, loss: 1.5339974164962769, acc: 46.875, f1: 12.350108582758947, r: 0.17918619269251718
06/02/2019 01:16:45 step: 1873, epoch: 56, batch: 24, loss: 1.4054808616638184, acc: 46.875, f1: 16.33470695970696, r: 0.2471769543346827
06/02/2019 01:16:46 step: 1878, epoch: 56, batch: 29, loss: 1.4239795207977295, acc: 43.75, f1: 20.74163012204318, r: 0.2859177450087895
06/02/2019 01:16:46 *** evaluating ***
06/02/2019 01:16:46 step: 57, epoch: 56, acc: 44.871794871794876, f1: 7.8125, r: 0.26317068938128313
06/02/2019 01:16:46 *** epoch: 58 ***
06/02/2019 01:16:46 *** training ***
06/02/2019 01:16:46 step: 1886, epoch: 57, batch: 4, loss: 1.6075513362884521, acc: 39.0625, f1: 8.116883116883118, r: 0.21427609373814888
06/02/2019 01:16:46 step: 1891, epoch: 57, batch: 9, loss: 1.4644240140914917, acc: 42.1875, f1: 19.474206349206348, r: 0.21297896298089286
06/02/2019 01:16:46 step: 1896, epoch: 57, batch: 14, loss: 1.5475772619247437, acc: 39.0625, f1: 9.759136212624584, r: 0.13810858103107881
06/02/2019 01:16:46 step: 1901, epoch: 57, batch: 19, loss: 1.413838267326355, acc: 50.0, f1: 20.43083900226757, r: 0.23656447130207256
06/02/2019 01:16:46 step: 1906, epoch: 57, batch: 24, loss: 1.327268362045288, acc: 40.625, f1: 16.39455782312925, r: 0.22085382625107156
06/02/2019 01:16:46 step: 1911, epoch: 57, batch: 29, loss: 1.2771153450012207, acc: 43.75, f1: 11.802902979373567, r: 0.26129183647668575
06/02/2019 01:16:47 *** evaluating ***
06/02/2019 01:16:47 step: 58, epoch: 57, acc: 44.871794871794876, f1: 7.8125, r: 0.24937156004613387
06/02/2019 01:16:47 *** epoch: 59 ***
06/02/2019 01:16:47 *** training ***
06/02/2019 01:16:47 step: 1919, epoch: 58, batch: 4, loss: 1.3964130878448486, acc: 34.375, f1: 11.071428571428573, r: 0.20844129818632465
06/02/2019 01:16:47 step: 1924, epoch: 58, batch: 9, loss: 1.20584237575531, acc: 56.25, f1: 28.253968253968253, r: 0.406736644702331
06/02/2019 01:16:47 step: 1929, epoch: 58, batch: 14, loss: 1.4603469371795654, acc: 45.3125, f1: 14.010279126558196, r: 0.13182067402593842
06/02/2019 01:16:47 step: 1934, epoch: 58, batch: 19, loss: 1.5189943313598633, acc: 35.9375, f1: 12.795889440113395, r: 0.24587349627189067
06/02/2019 01:16:47 step: 1939, epoch: 58, batch: 24, loss: 1.5205421447753906, acc: 39.0625, f1: 8.116883116883116, r: 0.15950416019471003
06/02/2019 01:16:48 step: 1944, epoch: 58, batch: 29, loss: 1.4743750095367432, acc: 32.8125, f1: 8.754355400696864, r: 0.2902516231977484
06/02/2019 01:16:48 *** evaluating ***
06/02/2019 01:16:48 step: 59, epoch: 58, acc: 44.871794871794876, f1: 7.8125, r: 0.2493499236033384
06/02/2019 01:16:48 *** epoch: 60 ***
06/02/2019 01:16:48 *** training ***
06/02/2019 01:16:48 step: 1952, epoch: 59, batch: 4, loss: 1.4182722568511963, acc: 40.625, f1: 11.572128851540617, r: 0.23369034214591652
06/02/2019 01:16:48 step: 1957, epoch: 59, batch: 9, loss: 1.4028557538986206, acc: 46.875, f1: 26.140147115756875, r: 0.3011230579976395
06/02/2019 01:16:48 step: 1962, epoch: 59, batch: 14, loss: 1.3677794933319092, acc: 40.625, f1: 8.433908045977011, r: 0.15860417301624438
06/02/2019 01:16:48 step: 1967, epoch: 59, batch: 19, loss: 1.4373054504394531, acc: 46.875, f1: 11.11111111111111, r: 0.17918462286929365
06/02/2019 01:16:49 step: 1972, epoch: 59, batch: 24, loss: 1.4874054193496704, acc: 42.1875, f1: 19.93179234558545, r: 0.25871461438097143
06/02/2019 01:16:49 step: 1977, epoch: 59, batch: 29, loss: 1.2227473258972168, acc: 56.25, f1: 13.775510204081634, r: 0.2655317120202004
06/02/2019 01:16:49 *** evaluating ***
06/02/2019 01:16:49 step: 60, epoch: 59, acc: 44.871794871794876, f1: 7.8125, r: 0.2601382619342231
06/02/2019 01:16:49 *** epoch: 61 ***
06/02/2019 01:16:49 *** training ***
06/02/2019 01:16:49 step: 1985, epoch: 60, batch: 4, loss: 1.6589725017547607, acc: 35.9375, f1: 8.097094259390502, r: 0.14683043770546633
06/02/2019 01:16:49 step: 1990, epoch: 60, batch: 9, loss: 1.5423519611358643, acc: 39.0625, f1: 12.678571428571427, r: 0.19866015658096542
06/02/2019 01:16:49 step: 1995, epoch: 60, batch: 14, loss: 1.3856412172317505, acc: 40.625, f1: 13.17790530846485, r: 0.2341910218956675
06/02/2019 01:16:49 step: 2000, epoch: 60, batch: 19, loss: 1.4311041831970215, acc: 43.75, f1: 13.399778516057584, r: 0.19677835628464707
06/02/2019 01:16:50 step: 2005, epoch: 60, batch: 24, loss: 1.344892978668213, acc: 51.5625, f1: 18.80952380952381, r: 0.24408656666468379
06/02/2019 01:16:50 step: 2010, epoch: 60, batch: 29, loss: 1.5576061010360718, acc: 43.75, f1: 11.76470588235294, r: 0.18380498434832793
06/02/2019 01:16:50 *** evaluating ***
06/02/2019 01:16:50 step: 61, epoch: 60, acc: 44.871794871794876, f1: 7.8125, r: 0.25339336917501354
06/02/2019 01:16:50 *** epoch: 62 ***
06/02/2019 01:16:50 *** training ***
06/02/2019 01:16:50 step: 2018, epoch: 61, batch: 4, loss: 1.480395793914795, acc: 28.125, f1: 9.305555555555555, r: 0.22258717949145468
06/02/2019 01:16:50 step: 2023, epoch: 61, batch: 9, loss: 1.431302785873413, acc: 46.875, f1: 14.79328165374677, r: 0.2904889357622888
06/02/2019 01:16:50 step: 2028, epoch: 61, batch: 14, loss: 1.401352882385254, acc: 45.3125, f1: 19.376026272578, r: 0.2849075373146694
06/02/2019 01:16:50 step: 2033, epoch: 61, batch: 19, loss: 1.318664312362671, acc: 50.0, f1: 20.782576596530085, r: 0.27167319646873433
06/02/2019 01:16:51 step: 2038, epoch: 61, batch: 24, loss: 1.4175809621810913, acc: 43.75, f1: 14.682128643857913, r: 0.23607262379784588
06/02/2019 01:16:51 step: 2043, epoch: 61, batch: 29, loss: 1.6091104745864868, acc: 31.25, f1: 11.868686868686869, r: 0.2740550945733186
06/02/2019 01:16:51 *** evaluating ***
06/02/2019 01:16:51 step: 62, epoch: 61, acc: 44.871794871794876, f1: 7.8125, r: 0.25200220435004955
06/02/2019 01:16:51 *** epoch: 63 ***
06/02/2019 01:16:51 *** training ***
06/02/2019 01:16:51 step: 2051, epoch: 62, batch: 4, loss: 1.252047061920166, acc: 54.6875, f1: 24.11674347158218, r: 0.29175748981470695
06/02/2019 01:16:51 step: 2056, epoch: 62, batch: 9, loss: 1.3821252584457397, acc: 45.3125, f1: 24.196787148594378, r: 0.29540218035357246
06/02/2019 01:16:51 step: 2061, epoch: 62, batch: 14, loss: 1.376936435699463, acc: 51.5625, f1: 16.889534883720934, r: 0.21461433128709273
06/02/2019 01:16:52 step: 2066, epoch: 62, batch: 19, loss: 1.5172759294509888, acc: 42.1875, f1: 11.026736275905712, r: 0.18396198335228794
06/02/2019 01:16:52 step: 2071, epoch: 62, batch: 24, loss: 1.4414676427841187, acc: 37.5, f1: 19.145569620253163, r: 0.28031188415768393
06/02/2019 01:16:52 step: 2076, epoch: 62, batch: 29, loss: 1.4547202587127686, acc: 43.75, f1: 22.15050678906101, r: 0.23831289338111317
06/02/2019 01:16:52 *** evaluating ***
06/02/2019 01:16:52 step: 63, epoch: 62, acc: 44.871794871794876, f1: 7.8125, r: 0.25256027608818543
06/02/2019 01:16:52 *** epoch: 64 ***
06/02/2019 01:16:52 *** training ***
06/02/2019 01:16:52 step: 2084, epoch: 63, batch: 4, loss: 1.2538982629776, acc: 46.875, f1: 11.30298273155416, r: 0.31862635354083135
06/02/2019 01:16:52 step: 2089, epoch: 63, batch: 9, loss: 1.284615159034729, acc: 53.125, f1: 12.41883116883117, r: 0.24289122669674063
06/02/2019 01:16:52 step: 2094, epoch: 63, batch: 14, loss: 1.5513213872909546, acc: 40.625, f1: 11.642156862745098, r: 0.19765025163843986
06/02/2019 01:16:53 step: 2099, epoch: 63, batch: 19, loss: 1.5146424770355225, acc: 40.625, f1: 10.622577519379844, r: 0.16468557473666448
06/02/2019 01:16:53 step: 2104, epoch: 63, batch: 24, loss: 1.331115961074829, acc: 48.4375, f1: 19.6186556179469, r: 0.31383152160643635
06/02/2019 01:16:53 step: 2109, epoch: 63, batch: 29, loss: 1.549249291419983, acc: 40.625, f1: 10.978084415584416, r: 0.16513305583951723
06/02/2019 01:16:53 *** evaluating ***
06/02/2019 01:16:53 step: 64, epoch: 63, acc: 44.871794871794876, f1: 7.8125, r: 0.2525778548968369
06/02/2019 01:16:53 *** epoch: 65 ***
06/02/2019 01:16:53 *** training ***
06/02/2019 01:16:53 step: 2117, epoch: 64, batch: 4, loss: 1.505801796913147, acc: 37.5, f1: 15.906483075157777, r: 0.27679086746282416
06/02/2019 01:16:53 step: 2122, epoch: 64, batch: 9, loss: 1.4315587282180786, acc: 42.1875, f1: 8.47723704866562, r: 0.2193012517729538
06/02/2019 01:16:54 step: 2127, epoch: 64, batch: 14, loss: 1.6440433263778687, acc: 37.5, f1: 10.420531849103277, r: 0.15272962828723471
06/02/2019 01:16:54 step: 2132, epoch: 64, batch: 19, loss: 1.4268198013305664, acc: 45.3125, f1: 18.41269841269841, r: 0.26728182598117545
06/02/2019 01:16:54 step: 2137, epoch: 64, batch: 24, loss: 1.507541537284851, acc: 45.3125, f1: 10.555555555555555, r: 0.20258557841859598
06/02/2019 01:16:54 step: 2142, epoch: 64, batch: 29, loss: 1.4880404472351074, acc: 42.1875, f1: 13.623301985370952, r: 0.24391099805440328
06/02/2019 01:16:54 *** evaluating ***
06/02/2019 01:16:54 step: 65, epoch: 64, acc: 44.871794871794876, f1: 7.8125, r: 0.2559556718282652
06/02/2019 01:16:54 *** epoch: 66 ***
06/02/2019 01:16:54 *** training ***
06/02/2019 01:16:54 step: 2150, epoch: 65, batch: 4, loss: 1.3852035999298096, acc: 48.4375, f1: 12.073732718894007, r: 0.21460814263684644
06/02/2019 01:16:55 step: 2155, epoch: 65, batch: 9, loss: 1.5695668458938599, acc: 31.25, f1: 9.753231492361927, r: 0.23337278694755784
06/02/2019 01:16:55 step: 2160, epoch: 65, batch: 14, loss: 1.3734465837478638, acc: 51.5625, f1: 18.095238095238095, r: 0.23638533659285624
06/02/2019 01:16:55 step: 2165, epoch: 65, batch: 19, loss: 1.469464898109436, acc: 42.1875, f1: 8.865870786516854, r: 0.23227451822398756
06/02/2019 01:16:55 step: 2170, epoch: 65, batch: 24, loss: 1.4810965061187744, acc: 45.3125, f1: 19.843782572441107, r: 0.33911046191112937
06/02/2019 01:16:55 step: 2175, epoch: 65, batch: 29, loss: 1.5926611423492432, acc: 34.375, f1: 9.01462994836489, r: 0.2541908733575372
06/02/2019 01:16:55 *** evaluating ***
06/02/2019 01:16:55 step: 66, epoch: 65, acc: 44.871794871794876, f1: 7.835820895522387, r: 0.25382267714164897
06/02/2019 01:16:55 *** epoch: 67 ***
06/02/2019 01:16:55 *** training ***
06/02/2019 01:16:55 step: 2183, epoch: 66, batch: 4, loss: 1.429745078086853, acc: 45.3125, f1: 11.241883116883116, r: 0.19636407772485814
06/02/2019 01:16:56 step: 2188, epoch: 66, batch: 9, loss: 1.444955587387085, acc: 31.25, f1: 12.046783625730995, r: 0.22526380515471398
06/02/2019 01:16:56 step: 2193, epoch: 66, batch: 14, loss: 1.3262041807174683, acc: 50.0, f1: 22.03463203463204, r: 0.223948908884453
06/02/2019 01:16:56 step: 2198, epoch: 66, batch: 19, loss: 1.5136841535568237, acc: 48.4375, f1: 14.115168539325845, r: 0.17829694334886653
06/02/2019 01:16:56 step: 2203, epoch: 66, batch: 24, loss: 1.4719640016555786, acc: 43.75, f1: 17.766684534977216, r: 0.2764468142020258
06/02/2019 01:16:56 step: 2208, epoch: 66, batch: 29, loss: 1.4389560222625732, acc: 43.75, f1: 17.234219269102994, r: 0.19761566987165388
06/02/2019 01:16:56 *** evaluating ***
06/02/2019 01:16:56 step: 67, epoch: 66, acc: 44.871794871794876, f1: 7.835820895522387, r: 0.2523379962967138
06/02/2019 01:16:56 *** epoch: 68 ***
06/02/2019 01:16:56 *** training ***
06/02/2019 01:16:56 step: 2216, epoch: 67, batch: 4, loss: 1.343619465827942, acc: 50.0, f1: 13.500784929356357, r: 0.21181433489572737
06/02/2019 01:16:57 step: 2221, epoch: 67, batch: 9, loss: 1.3372803926467896, acc: 51.5625, f1: 23.998917748917748, r: 0.287220051885965
06/02/2019 01:16:57 step: 2226, epoch: 67, batch: 14, loss: 1.552233338356018, acc: 42.1875, f1: 10.838870431893689, r: 0.2027852450250552
06/02/2019 01:16:57 step: 2231, epoch: 67, batch: 19, loss: 1.2260565757751465, acc: 54.6875, f1: 21.60330948121646, r: 0.2678313952148683
06/02/2019 01:16:57 step: 2236, epoch: 67, batch: 24, loss: 1.4190400838851929, acc: 46.875, f1: 26.862527716186253, r: 0.31166935581896316
06/02/2019 01:16:57 step: 2241, epoch: 67, batch: 29, loss: 1.5608354806900024, acc: 39.0625, f1: 10.336134453781511, r: 0.23855920228443248
06/02/2019 01:16:57 *** evaluating ***
06/02/2019 01:16:57 step: 68, epoch: 67, acc: 44.871794871794876, f1: 7.766272189349112, r: 0.26263069826327634
06/02/2019 01:16:57 *** epoch: 69 ***
06/02/2019 01:16:57 *** training ***
06/02/2019 01:16:57 step: 2249, epoch: 68, batch: 4, loss: 1.3550807237625122, acc: 48.4375, f1: 16.687280972995257, r: 0.20730107675965406
06/02/2019 01:16:58 step: 2254, epoch: 68, batch: 9, loss: 1.3623405694961548, acc: 46.875, f1: 13.650793650793652, r: 0.18445484138591253
06/02/2019 01:16:58 step: 2259, epoch: 68, batch: 14, loss: 1.3816547393798828, acc: 46.875, f1: 21.86181434599156, r: 0.354400870041237
06/02/2019 01:16:58 step: 2264, epoch: 68, batch: 19, loss: 1.4929094314575195, acc: 46.875, f1: 14.502583979328165, r: 0.20178702934988518
06/02/2019 01:16:58 step: 2269, epoch: 68, batch: 24, loss: 1.3225677013397217, acc: 46.875, f1: 9.316770186335404, r: 0.2556890100502639
06/02/2019 01:16:58 step: 2274, epoch: 68, batch: 29, loss: 1.4353307485580444, acc: 39.0625, f1: 10.197934595524957, r: 0.2760281742629621
06/02/2019 01:16:58 *** evaluating ***
06/02/2019 01:16:58 step: 69, epoch: 68, acc: 44.871794871794876, f1: 7.8125, r: 0.2512218988455818
06/02/2019 01:16:58 *** epoch: 70 ***
06/02/2019 01:16:58 *** training ***
06/02/2019 01:16:59 step: 2282, epoch: 69, batch: 4, loss: 1.3455637693405151, acc: 48.4375, f1: 12.877747252747252, r: 0.2724447925475654
06/02/2019 01:16:59 step: 2287, epoch: 69, batch: 9, loss: 1.4273731708526611, acc: 39.0625, f1: 21.838624338624342, r: 0.3255125092364023
06/02/2019 01:16:59 step: 2292, epoch: 69, batch: 14, loss: 1.5370780229568481, acc: 34.375, f1: 14.233980305408878, r: 0.2752082827744514
06/02/2019 01:16:59 step: 2297, epoch: 69, batch: 19, loss: 1.1498831510543823, acc: 54.6875, f1: 29.335929493748715, r: 0.37170170476240794
06/02/2019 01:16:59 step: 2302, epoch: 69, batch: 24, loss: 1.2459651231765747, acc: 50.0, f1: 14.371715241280455, r: 0.2646125062403241
06/02/2019 01:16:59 step: 2307, epoch: 69, batch: 29, loss: 1.3085907697677612, acc: 53.125, f1: 14.138607388035307, r: 0.26462955778810765
06/02/2019 01:16:59 *** evaluating ***
06/02/2019 01:17:00 step: 70, epoch: 69, acc: 44.871794871794876, f1: 7.8125, r: 0.2527717181434514
06/02/2019 01:17:00 *** epoch: 71 ***
06/02/2019 01:17:00 *** training ***
06/02/2019 01:17:00 step: 2315, epoch: 70, batch: 4, loss: 1.4670146703720093, acc: 39.0625, f1: 11.976744186046512, r: 0.2826286104306537
06/02/2019 01:17:00 step: 2320, epoch: 70, batch: 9, loss: 1.3636345863342285, acc: 51.5625, f1: 16.780045351473923, r: 0.31359323324456767
06/02/2019 01:17:00 step: 2325, epoch: 70, batch: 14, loss: 1.5397804975509644, acc: 40.625, f1: 10.15769944341373, r: 0.2175621941546201
06/02/2019 01:17:00 step: 2330, epoch: 70, batch: 19, loss: 1.561133623123169, acc: 29.6875, f1: 11.82915258257724, r: 0.2639104351914199
06/02/2019 01:17:00 step: 2335, epoch: 70, batch: 24, loss: 1.5491148233413696, acc: 39.0625, f1: 14.44541231126597, r: 0.28125512967168786
06/02/2019 01:17:01 step: 2340, epoch: 70, batch: 29, loss: 1.4723182916641235, acc: 39.0625, f1: 10.30760130139012, r: 0.1917771110482835
06/02/2019 01:17:01 *** evaluating ***
06/02/2019 01:17:01 step: 71, epoch: 70, acc: 44.871794871794876, f1: 7.8125, r: 0.265253658474568
06/02/2019 01:17:01 *** epoch: 72 ***
06/02/2019 01:17:01 *** training ***
06/02/2019 01:17:01 step: 2348, epoch: 71, batch: 4, loss: 1.2187422513961792, acc: 46.875, f1: 19.99336870026525, r: 0.33094911873016514
06/02/2019 01:17:01 step: 2353, epoch: 71, batch: 9, loss: 1.6264259815216064, acc: 43.75, f1: 14.55128205128205, r: 0.21038721444782896
06/02/2019 01:17:01 step: 2358, epoch: 71, batch: 14, loss: 1.4412336349487305, acc: 42.1875, f1: 14.756671899529042, r: 0.17503883295238418
06/02/2019 01:17:01 step: 2363, epoch: 71, batch: 19, loss: 1.2534112930297852, acc: 43.75, f1: 25.187074829931973, r: 0.31744357973285275
06/02/2019 01:17:01 step: 2368, epoch: 71, batch: 24, loss: 1.4045113325119019, acc: 51.5625, f1: 18.65842490842491, r: 0.2599569674209512
06/02/2019 01:17:02 step: 2373, epoch: 71, batch: 29, loss: 1.3666760921478271, acc: 43.75, f1: 21.041666666666664, r: 0.2712118029164884
06/02/2019 01:17:02 *** evaluating ***
06/02/2019 01:17:02 step: 72, epoch: 71, acc: 44.871794871794876, f1: 7.8125, r: 0.2732015219314861
06/02/2019 01:17:02 *** epoch: 73 ***
06/02/2019 01:17:02 *** training ***
06/02/2019 01:17:02 step: 2381, epoch: 72, batch: 4, loss: 1.4518110752105713, acc: 37.5, f1: 8.32720588235294, r: 0.2628378261556976
06/02/2019 01:17:02 step: 2386, epoch: 72, batch: 9, loss: 1.5368022918701172, acc: 45.3125, f1: 16.35854341736695, r: 0.20023337640773545
06/02/2019 01:17:02 step: 2391, epoch: 72, batch: 14, loss: 1.3325750827789307, acc: 42.1875, f1: 9.743991640543365, r: 0.20539800771052036
06/02/2019 01:17:02 step: 2396, epoch: 72, batch: 19, loss: 1.4529218673706055, acc: 40.625, f1: 16.671075837742507, r: 0.25279923694054207
06/02/2019 01:17:03 step: 2401, epoch: 72, batch: 24, loss: 1.4595990180969238, acc: 37.5, f1: 11.213177066835604, r: 0.2885853688634746
06/02/2019 01:17:03 step: 2406, epoch: 72, batch: 29, loss: 1.265898585319519, acc: 48.4375, f1: 23.12925170068027, r: 0.22105643094291672
06/02/2019 01:17:03 *** evaluating ***
06/02/2019 01:17:03 step: 73, epoch: 72, acc: 44.871794871794876, f1: 7.954545454545454, r: 0.2874532954196479
06/02/2019 01:17:03 *** epoch: 74 ***
06/02/2019 01:17:03 *** training ***
06/02/2019 01:17:03 step: 2414, epoch: 73, batch: 4, loss: 1.4715142250061035, acc: 51.5625, f1: 14.90530303030303, r: 0.25480427308524367
06/02/2019 01:17:03 step: 2419, epoch: 73, batch: 9, loss: 1.4832432270050049, acc: 34.375, f1: 18.40659340659341, r: 0.246419435528079
06/02/2019 01:17:03 step: 2424, epoch: 73, batch: 14, loss: 1.6708251237869263, acc: 28.125, f1: 8.13598166539343, r: 0.18981430627790574
06/02/2019 01:17:03 step: 2429, epoch: 73, batch: 19, loss: 1.401244044303894, acc: 42.1875, f1: 27.55952380952381, r: 0.2789463427014552
06/02/2019 01:17:04 step: 2434, epoch: 73, batch: 24, loss: 1.6196457147598267, acc: 40.625, f1: 23.99350649350649, r: 0.32231937322454907
06/02/2019 01:17:04 step: 2439, epoch: 73, batch: 29, loss: 1.3531759977340698, acc: 50.0, f1: 14.816810344827585, r: 0.26436008299164193
06/02/2019 01:17:04 *** evaluating ***
06/02/2019 01:17:04 step: 74, epoch: 73, acc: 44.871794871794876, f1: 7.835820895522387, r: 0.27802554122139644
06/02/2019 01:17:04 *** epoch: 75 ***
06/02/2019 01:17:04 *** training ***
06/02/2019 01:17:04 step: 2447, epoch: 74, batch: 4, loss: 1.4022561311721802, acc: 40.625, f1: 21.399821109123433, r: 0.36198944879883743
06/02/2019 01:17:04 step: 2452, epoch: 74, batch: 9, loss: 1.5095101594924927, acc: 40.625, f1: 21.44736842105263, r: 0.24970134526078175
06/02/2019 01:17:04 step: 2457, epoch: 74, batch: 14, loss: 1.4861114025115967, acc: 50.0, f1: 20.312822098536383, r: 0.28659783506463754
06/02/2019 01:17:05 step: 2462, epoch: 74, batch: 19, loss: 1.482187032699585, acc: 43.75, f1: 16.15748709122203, r: 0.24575709723709846
06/02/2019 01:17:05 step: 2467, epoch: 74, batch: 24, loss: 1.491560935974121, acc: 34.375, f1: 12.76422764227642, r: 0.2204675739181076
06/02/2019 01:17:05 step: 2472, epoch: 74, batch: 29, loss: 1.3618817329406738, acc: 37.5, f1: 9.412883733074715, r: 0.22761345394856938
06/02/2019 01:17:05 *** evaluating ***
06/02/2019 01:17:05 step: 75, epoch: 74, acc: 44.871794871794876, f1: 7.766272189349112, r: 0.27445534129878585
06/02/2019 01:17:05 *** epoch: 76 ***
06/02/2019 01:17:05 *** training ***
06/02/2019 01:17:05 step: 2480, epoch: 75, batch: 4, loss: 1.4769386053085327, acc: 45.3125, f1: 16.420092133238835, r: 0.19593238610302635
06/02/2019 01:17:05 step: 2485, epoch: 75, batch: 9, loss: 1.4599047899246216, acc: 42.1875, f1: 16.94033935413246, r: 0.2527164800941679
06/02/2019 01:17:06 step: 2490, epoch: 75, batch: 14, loss: 1.222995400428772, acc: 53.125, f1: 26.558834424002963, r: 0.26151280533361865
06/02/2019 01:17:06 step: 2495, epoch: 75, batch: 19, loss: 1.5099637508392334, acc: 37.5, f1: 13.373983739837398, r: 0.25387933615492103
06/02/2019 01:17:06 step: 2500, epoch: 75, batch: 24, loss: 1.441977620124817, acc: 43.75, f1: 15.606242496998801, r: 0.2341607554648021
06/02/2019 01:17:06 step: 2505, epoch: 75, batch: 29, loss: 1.2492362260818481, acc: 39.0625, f1: 12.398360017407636, r: 0.331912427076379
06/02/2019 01:17:06 *** evaluating ***
06/02/2019 01:17:06 step: 76, epoch: 75, acc: 44.871794871794876, f1: 7.766272189349112, r: 0.2638473212487443
06/02/2019 01:17:06 *** epoch: 77 ***
06/02/2019 01:17:06 *** training ***
06/02/2019 01:17:06 step: 2513, epoch: 76, batch: 4, loss: 1.4719468355178833, acc: 39.0625, f1: 10.903679653679651, r: 0.22823414178819187
06/02/2019 01:17:07 step: 2518, epoch: 76, batch: 9, loss: 1.332676887512207, acc: 45.3125, f1: 21.862728831369946, r: 0.32489306684495645
06/02/2019 01:17:07 step: 2523, epoch: 76, batch: 14, loss: 1.5993571281433105, acc: 32.8125, f1: 12.745784916837547, r: 0.20250397092046363
06/02/2019 01:17:07 step: 2528, epoch: 76, batch: 19, loss: 1.4774436950683594, acc: 34.375, f1: 15.01082251082251, r: 0.31626763581624645
06/02/2019 01:17:07 step: 2533, epoch: 76, batch: 24, loss: 1.373313307762146, acc: 46.875, f1: 15.677668539325843, r: 0.28540297933191044
06/02/2019 01:17:07 step: 2538, epoch: 76, batch: 29, loss: 1.5438227653503418, acc: 45.3125, f1: 12.285124363776049, r: 0.10849797759480764
06/02/2019 01:17:07 *** evaluating ***
06/02/2019 01:17:07 step: 77, epoch: 76, acc: 44.871794871794876, f1: 7.8125, r: 0.23553405927481172
06/02/2019 01:17:07 *** epoch: 78 ***
06/02/2019 01:17:07 *** training ***
06/02/2019 01:17:07 step: 2546, epoch: 77, batch: 4, loss: 1.5091326236724854, acc: 46.875, f1: 12.9004329004329, r: 0.17103498536058243
06/02/2019 01:17:08 step: 2551, epoch: 77, batch: 9, loss: 1.4207274913787842, acc: 40.625, f1: 13.69047619047619, r: 0.26658863887001216
06/02/2019 01:17:08 step: 2556, epoch: 77, batch: 14, loss: 1.539806842803955, acc: 32.8125, f1: 11.210407239819004, r: 0.20326268698202787
06/02/2019 01:17:08 step: 2561, epoch: 77, batch: 19, loss: 1.347283959388733, acc: 46.875, f1: 16.330049261083744, r: 0.28281943136323995
06/02/2019 01:17:08 step: 2566, epoch: 77, batch: 24, loss: 1.2713896036148071, acc: 43.75, f1: 25.519989353624073, r: 0.3214645185702879
06/02/2019 01:17:08 step: 2571, epoch: 77, batch: 29, loss: 1.4184404611587524, acc: 43.75, f1: 24.829931972789115, r: 0.2922747537103982
06/02/2019 01:17:08 *** evaluating ***
06/02/2019 01:17:08 step: 78, epoch: 77, acc: 44.871794871794876, f1: 7.7893175074183985, r: 0.2653605186643348
06/02/2019 01:17:08 *** epoch: 79 ***
06/02/2019 01:17:08 *** training ***
06/02/2019 01:17:08 step: 2579, epoch: 78, batch: 4, loss: 1.5983147621154785, acc: 31.25, f1: 8.256578947368421, r: 0.22693337194182514
06/02/2019 01:17:09 step: 2584, epoch: 78, batch: 9, loss: 1.4391841888427734, acc: 40.625, f1: 11.963018825763925, r: 0.25646120843699316
06/02/2019 01:17:09 step: 2589, epoch: 78, batch: 14, loss: 1.362048625946045, acc: 43.75, f1: 18.764428848462465, r: 0.2561069328332369
06/02/2019 01:17:09 step: 2594, epoch: 78, batch: 19, loss: 1.5023095607757568, acc: 39.0625, f1: 15.6587025053924, r: 0.3196239543260559
06/02/2019 01:17:09 step: 2599, epoch: 78, batch: 24, loss: 1.4122204780578613, acc: 50.0, f1: 20.62937062937063, r: 0.2493396132637493
06/02/2019 01:17:09 step: 2604, epoch: 78, batch: 29, loss: 1.483441948890686, acc: 34.375, f1: 15.145330859616577, r: 0.1988517179272068
06/02/2019 01:17:09 *** evaluating ***
06/02/2019 01:17:09 step: 79, epoch: 78, acc: 44.871794871794876, f1: 7.8125, r: 0.26963079320173955
06/02/2019 01:17:09 *** epoch: 80 ***
06/02/2019 01:17:09 *** training ***
06/02/2019 01:17:09 step: 2612, epoch: 79, batch: 4, loss: 1.4194778203964233, acc: 40.625, f1: 14.415998957722625, r: 0.18075657661184022
06/02/2019 01:17:10 step: 2617, epoch: 79, batch: 9, loss: 1.4863338470458984, acc: 50.0, f1: 20.806834361651635, r: 0.18842313301862923
06/02/2019 01:17:10 step: 2622, epoch: 79, batch: 14, loss: 1.4043313264846802, acc: 39.0625, f1: 12.97531060617018, r: 0.2478483529292533
06/02/2019 01:17:10 step: 2627, epoch: 79, batch: 19, loss: 1.525344729423523, acc: 39.0625, f1: 11.858864027538726, r: 0.194719424188651
06/02/2019 01:17:10 step: 2632, epoch: 79, batch: 24, loss: 1.4294207096099854, acc: 43.75, f1: 15.917602996254681, r: 0.22657145838288045
06/02/2019 01:17:10 step: 2637, epoch: 79, batch: 29, loss: 1.5744434595108032, acc: 35.9375, f1: 6.609195402298851, r: 0.22640104642198014
06/02/2019 01:17:10 *** evaluating ***
06/02/2019 01:17:10 step: 80, epoch: 79, acc: 44.871794871794876, f1: 7.8125, r: 0.2737309246169529
06/02/2019 01:17:10 *** epoch: 81 ***
06/02/2019 01:17:10 *** training ***
06/02/2019 01:17:11 step: 2645, epoch: 80, batch: 4, loss: 1.3779442310333252, acc: 43.75, f1: 14.65909090909091, r: 0.3033579986610978
06/02/2019 01:17:11 step: 2650, epoch: 80, batch: 9, loss: 1.3074767589569092, acc: 54.6875, f1: 21.805555555555557, r: 0.23076450647840907
06/02/2019 01:17:11 step: 2655, epoch: 80, batch: 14, loss: 1.4785230159759521, acc: 46.875, f1: 14.788245462402767, r: 0.2725996550371531
06/02/2019 01:17:11 step: 2660, epoch: 80, batch: 19, loss: 1.3433620929718018, acc: 46.875, f1: 11.047027506654837, r: 0.2034339010354624
06/02/2019 01:17:11 step: 2665, epoch: 80, batch: 24, loss: 1.3939015865325928, acc: 31.25, f1: 15.290237307044027, r: 0.2853348774771048
06/02/2019 01:17:11 step: 2670, epoch: 80, batch: 29, loss: 1.4187395572662354, acc: 40.625, f1: 12.4755864290748, r: 0.23624418866729258
06/02/2019 01:17:11 *** evaluating ***
06/02/2019 01:17:11 step: 81, epoch: 80, acc: 44.871794871794876, f1: 7.8125, r: 0.2637197575315613
06/02/2019 01:17:11 *** epoch: 82 ***
06/02/2019 01:17:11 *** training ***
06/02/2019 01:17:12 step: 2678, epoch: 81, batch: 4, loss: 1.564924716949463, acc: 37.5, f1: 16.3510101010101, r: 0.2712255160588909
06/02/2019 01:17:12 step: 2683, epoch: 81, batch: 9, loss: 1.3178682327270508, acc: 45.3125, f1: 18.920565302144247, r: 0.2742889283951166
06/02/2019 01:17:12 step: 2688, epoch: 81, batch: 14, loss: 1.449992060661316, acc: 54.6875, f1: 24.208211143695017, r: 0.25970976505235227
06/02/2019 01:17:12 step: 2693, epoch: 81, batch: 19, loss: 1.4149253368377686, acc: 40.625, f1: 20.127712477396027, r: 0.33522919277863766
06/02/2019 01:17:12 step: 2698, epoch: 81, batch: 24, loss: 1.5348656177520752, acc: 35.9375, f1: 15.491967871485945, r: 0.23427221944983642
06/02/2019 01:17:12 step: 2703, epoch: 81, batch: 29, loss: 1.264283299446106, acc: 46.875, f1: 13.268921095008052, r: 0.2027620053258743
06/02/2019 01:17:13 *** evaluating ***
06/02/2019 01:17:13 step: 82, epoch: 81, acc: 44.871794871794876, f1: 7.835820895522387, r: 0.2606162213240676
06/02/2019 01:17:13 *** epoch: 83 ***
06/02/2019 01:17:13 *** training ***
06/02/2019 01:17:13 step: 2711, epoch: 82, batch: 4, loss: 1.3089942932128906, acc: 51.5625, f1: 16.795977011494255, r: 0.2705583019944971
06/02/2019 01:17:13 step: 2716, epoch: 82, batch: 9, loss: 1.35002601146698, acc: 45.3125, f1: 16.70155793573515, r: 0.27951904974041336
06/02/2019 01:17:13 step: 2721, epoch: 82, batch: 14, loss: 1.701409935951233, acc: 29.6875, f1: 5.72289156626506, r: 0.1778189116503996
06/02/2019 01:17:13 step: 2726, epoch: 82, batch: 19, loss: 1.3607642650604248, acc: 45.3125, f1: 12.94862772695285, r: 0.2350305324684802
06/02/2019 01:17:14 step: 2731, epoch: 82, batch: 24, loss: 1.5639476776123047, acc: 35.9375, f1: 8.023106546854942, r: 0.23318836551815683
06/02/2019 01:17:14 step: 2736, epoch: 82, batch: 29, loss: 1.3930954933166504, acc: 39.0625, f1: 19.12121212121212, r: 0.2785448459995992
06/02/2019 01:17:14 *** evaluating ***
06/02/2019 01:17:14 step: 83, epoch: 82, acc: 44.871794871794876, f1: 7.8125, r: 0.2625191152119974
06/02/2019 01:17:14 *** epoch: 84 ***
06/02/2019 01:17:14 *** training ***
06/02/2019 01:17:14 step: 2744, epoch: 83, batch: 4, loss: 1.2816057205200195, acc: 51.5625, f1: 21.03641456582633, r: 0.3435354987504481
06/02/2019 01:17:14 step: 2749, epoch: 83, batch: 9, loss: 1.6420530080795288, acc: 31.25, f1: 10.041152263374487, r: 0.25486148582028545
06/02/2019 01:17:14 step: 2754, epoch: 83, batch: 14, loss: 1.6779017448425293, acc: 35.9375, f1: 13.287435456110156, r: 0.15723170054075272
06/02/2019 01:17:15 step: 2759, epoch: 83, batch: 19, loss: 1.5576105117797852, acc: 39.0625, f1: 17.540584415584416, r: 0.23037457249945417
06/02/2019 01:17:15 step: 2764, epoch: 83, batch: 24, loss: 1.7080150842666626, acc: 39.0625, f1: 14.267533936651583, r: 0.17011923621448752
06/02/2019 01:17:15 step: 2769, epoch: 83, batch: 29, loss: 1.4972498416900635, acc: 45.3125, f1: 24.62163159837578, r: 0.2932856648995493
06/02/2019 01:17:15 *** evaluating ***
06/02/2019 01:17:15 step: 84, epoch: 83, acc: 44.871794871794876, f1: 7.8125, r: 0.2723413571170983
06/02/2019 01:17:15 *** epoch: 85 ***
06/02/2019 01:17:15 *** training ***
06/02/2019 01:17:15 step: 2777, epoch: 84, batch: 4, loss: 1.3691104650497437, acc: 45.3125, f1: 13.301820728291316, r: 0.25913396263308386
06/02/2019 01:17:15 step: 2782, epoch: 84, batch: 9, loss: 1.5950267314910889, acc: 32.8125, f1: 13.584871269081795, r: 0.2820871992679661
06/02/2019 01:17:16 step: 2787, epoch: 84, batch: 14, loss: 1.5561881065368652, acc: 23.4375, f1: 6.390977443609022, r: 0.1863488052808247
06/02/2019 01:17:16 step: 2792, epoch: 84, batch: 19, loss: 1.4863888025283813, acc: 31.25, f1: 11.184210526315788, r: 0.33326858532079406
06/02/2019 01:17:16 step: 2797, epoch: 84, batch: 24, loss: 1.4250006675720215, acc: 42.1875, f1: 16.286245400169452, r: 0.2275584617282519
06/02/2019 01:17:16 step: 2802, epoch: 84, batch: 29, loss: 1.2624701261520386, acc: 46.875, f1: 25.16260162601626, r: 0.27040328520415163
06/02/2019 01:17:16 *** evaluating ***
06/02/2019 01:17:16 step: 85, epoch: 84, acc: 44.871794871794876, f1: 7.8125, r: 0.2772019304700971
06/02/2019 01:17:16 *** epoch: 86 ***
06/02/2019 01:17:16 *** training ***
06/02/2019 01:17:16 step: 2810, epoch: 85, batch: 4, loss: 1.4299087524414062, acc: 39.0625, f1: 11.241830065359478, r: 0.23887123179815176
06/02/2019 01:17:17 step: 2815, epoch: 85, batch: 9, loss: 1.4617869853973389, acc: 35.9375, f1: 14.762658227848103, r: 0.3040274874921294
06/02/2019 01:17:17 step: 2820, epoch: 85, batch: 14, loss: 1.4252742528915405, acc: 45.3125, f1: 12.637362637362637, r: 0.22677209936432297
06/02/2019 01:17:17 step: 2825, epoch: 85, batch: 19, loss: 1.4276458024978638, acc: 40.625, f1: 15.304924630606124, r: 0.21490136091633205
06/02/2019 01:17:17 step: 2830, epoch: 85, batch: 24, loss: 1.5358251333236694, acc: 39.0625, f1: 10.49107142857143, r: 0.1414494816366797
06/02/2019 01:17:17 step: 2835, epoch: 85, batch: 29, loss: 1.4744240045547485, acc: 42.1875, f1: 21.0906862745098, r: 0.1444887491349623
06/02/2019 01:17:17 *** evaluating ***
06/02/2019 01:17:17 step: 86, epoch: 85, acc: 44.871794871794876, f1: 7.8125, r: 0.2743050532104162
06/02/2019 01:17:17 *** epoch: 87 ***
06/02/2019 01:17:17 *** training ***
06/02/2019 01:17:18 step: 2843, epoch: 86, batch: 4, loss: 1.3566107749938965, acc: 43.75, f1: 13.230561999551, r: 0.21877293198225542
06/02/2019 01:17:18 step: 2848, epoch: 86, batch: 9, loss: 1.3537609577178955, acc: 50.0, f1: 18.529684601113175, r: 0.22302308139968866
06/02/2019 01:17:18 step: 2853, epoch: 86, batch: 14, loss: 1.2902405261993408, acc: 45.3125, f1: 19.801375095492745, r: 0.3032103602774419
06/02/2019 01:17:18 step: 2858, epoch: 86, batch: 19, loss: 1.2923839092254639, acc: 56.25, f1: 24.22360248447205, r: 0.2600159234605878
06/02/2019 01:17:18 step: 2863, epoch: 86, batch: 24, loss: 1.4320133924484253, acc: 37.5, f1: 13.769590114526824, r: 0.23369810131935687
06/02/2019 01:17:18 step: 2868, epoch: 86, batch: 29, loss: 1.4510083198547363, acc: 45.3125, f1: 19.17070880926303, r: 0.22485035735550915
06/02/2019 01:17:18 *** evaluating ***
06/02/2019 01:17:18 step: 87, epoch: 86, acc: 44.871794871794876, f1: 7.8125, r: 0.26773726276873916
06/02/2019 01:17:18 *** epoch: 88 ***
06/02/2019 01:17:18 *** training ***
06/02/2019 01:17:19 step: 2876, epoch: 87, batch: 4, loss: 1.5078198909759521, acc: 40.625, f1: 9.456635318704283, r: 0.2499022929093412
06/02/2019 01:17:19 step: 2881, epoch: 87, batch: 9, loss: 1.3111079931259155, acc: 45.3125, f1: 18.979591836734695, r: 0.2879954922502328
06/02/2019 01:17:19 step: 2886, epoch: 87, batch: 14, loss: 1.414115071296692, acc: 40.625, f1: 17.31408126756964, r: 0.2694919204064837
06/02/2019 01:17:19 step: 2891, epoch: 87, batch: 19, loss: 1.519989252090454, acc: 34.375, f1: 8.730158730158731, r: 0.23568861619431253
06/02/2019 01:17:19 step: 2896, epoch: 87, batch: 24, loss: 1.349397897720337, acc: 35.9375, f1: 18.17687074829932, r: 0.2967437055360676
06/02/2019 01:17:19 step: 2901, epoch: 87, batch: 29, loss: 1.409867286682129, acc: 39.0625, f1: 23.899939722724532, r: 0.2912128548926134
06/02/2019 01:17:20 *** evaluating ***
06/02/2019 01:17:20 step: 88, epoch: 87, acc: 44.871794871794876, f1: 7.859281437125748, r: 0.27051677708379557
06/02/2019 01:17:20 *** epoch: 89 ***
06/02/2019 01:17:20 *** training ***
06/02/2019 01:17:20 step: 2909, epoch: 88, batch: 4, loss: 1.4002814292907715, acc: 45.3125, f1: 15.41244083840433, r: 0.18484057539144128
06/02/2019 01:17:20 step: 2914, epoch: 88, batch: 9, loss: 1.480154275894165, acc: 43.75, f1: 17.265946904501124, r: 0.2999659794166827
06/02/2019 01:17:20 step: 2919, epoch: 88, batch: 14, loss: 1.5478121042251587, acc: 39.0625, f1: 9.331550802139036, r: 0.26921335512394623
06/02/2019 01:17:20 step: 2924, epoch: 88, batch: 19, loss: 1.2685186862945557, acc: 43.75, f1: 32.15831787260359, r: 0.3697088837760199
06/02/2019 01:17:20 step: 2929, epoch: 88, batch: 24, loss: 1.398879051208496, acc: 46.875, f1: 13.949081505261281, r: 0.18007497807044154
06/02/2019 01:17:21 step: 2934, epoch: 88, batch: 29, loss: 1.4662150144577026, acc: 43.75, f1: 16.295219638242894, r: 0.25269001450934675
06/02/2019 01:17:21 *** evaluating ***
06/02/2019 01:17:21 step: 89, epoch: 88, acc: 44.871794871794876, f1: 7.882882882882883, r: 0.27548725831767296
06/02/2019 01:17:21 *** epoch: 90 ***
06/02/2019 01:17:21 *** training ***
06/02/2019 01:17:21 step: 2942, epoch: 89, batch: 4, loss: 1.4465599060058594, acc: 43.75, f1: 16.663159371492704, r: 0.2508743914175184
06/02/2019 01:17:21 step: 2947, epoch: 89, batch: 9, loss: 1.4462476968765259, acc: 40.625, f1: 12.775842044134727, r: 0.251811024692759
06/02/2019 01:17:21 step: 2952, epoch: 89, batch: 14, loss: 1.3319480419158936, acc: 40.625, f1: 12.959400627720969, r: 0.2619965067260593
06/02/2019 01:17:22 step: 2957, epoch: 89, batch: 19, loss: 1.3851616382598877, acc: 48.4375, f1: 10.235507246376812, r: 0.237284937543158
06/02/2019 01:17:22 step: 2962, epoch: 89, batch: 24, loss: 1.48194420337677, acc: 48.4375, f1: 19.052287581699346, r: 0.19714528738337517
06/02/2019 01:17:22 step: 2967, epoch: 89, batch: 29, loss: 1.317272663116455, acc: 42.1875, f1: 12.193362193362193, r: 0.25811669156721084
06/02/2019 01:17:22 *** evaluating ***
06/02/2019 01:17:22 step: 90, epoch: 89, acc: 44.871794871794876, f1: 8.003048780487806, r: 0.2752170804358974
06/02/2019 01:17:22 *** epoch: 91 ***
06/02/2019 01:17:22 *** training ***
06/02/2019 01:17:22 step: 2975, epoch: 90, batch: 4, loss: 1.514061450958252, acc: 45.3125, f1: 11.60477453580902, r: 0.2366439676785749
06/02/2019 01:17:22 step: 2980, epoch: 90, batch: 9, loss: 1.2796136140823364, acc: 50.0, f1: 29.042305129913395, r: 0.34173785841116766
06/02/2019 01:17:23 step: 2985, epoch: 90, batch: 14, loss: 1.363471269607544, acc: 46.875, f1: 18.98585910213817, r: 0.3333015555417386
06/02/2019 01:17:23 step: 2990, epoch: 90, batch: 19, loss: 1.3357125520706177, acc: 43.75, f1: 16.07172783643372, r: 0.1868649502511348
06/02/2019 01:17:23 step: 2995, epoch: 90, batch: 24, loss: 1.3998500108718872, acc: 42.1875, f1: 17.369614512471657, r: 0.22948217829283069
06/02/2019 01:17:23 step: 3000, epoch: 90, batch: 29, loss: 1.2648745775222778, acc: 46.875, f1: 14.963221060782036, r: 0.2568815700134408
06/02/2019 01:17:23 *** evaluating ***
06/02/2019 01:17:23 step: 91, epoch: 90, acc: 45.2991452991453, f1: 8.559437829396837, r: 0.2571853902636089
06/02/2019 01:17:23 *** epoch: 92 ***
06/02/2019 01:17:23 *** training ***
06/02/2019 01:17:23 step: 3008, epoch: 91, batch: 4, loss: 1.5336309671401978, acc: 37.5, f1: 12.98076923076923, r: 0.2620000233648768
06/02/2019 01:17:23 step: 3013, epoch: 91, batch: 9, loss: 1.6832858324050903, acc: 34.375, f1: 9.735576923076923, r: 0.25911407891664234
06/02/2019 01:17:24 step: 3018, epoch: 91, batch: 14, loss: 1.209513783454895, acc: 56.25, f1: 30.68555008210181, r: 0.2662938214023486
06/02/2019 01:17:24 step: 3023, epoch: 91, batch: 19, loss: 1.3117021322250366, acc: 46.875, f1: 14.700996677740862, r: 0.30470882748807543
06/02/2019 01:17:24 step: 3028, epoch: 91, batch: 24, loss: 1.3963658809661865, acc: 40.625, f1: 11.969727650096186, r: 0.2692419676804401
06/02/2019 01:17:24 step: 3033, epoch: 91, batch: 29, loss: 1.4054356813430786, acc: 50.0, f1: 27.394636015325673, r: 0.22740214904009726
06/02/2019 01:17:24 *** evaluating ***
06/02/2019 01:17:24 step: 92, epoch: 91, acc: 44.871794871794876, f1: 7.906626506024097, r: 0.2596093557116993
06/02/2019 01:17:24 *** epoch: 93 ***
06/02/2019 01:17:24 *** training ***
06/02/2019 01:17:25 step: 3041, epoch: 92, batch: 4, loss: 1.6422998905181885, acc: 34.375, f1: 8.902439024390244, r: 0.2664327683639421
06/02/2019 01:17:25 step: 3046, epoch: 92, batch: 9, loss: 1.5145899057388306, acc: 37.5, f1: 12.513550135501356, r: 0.1754453014502313
06/02/2019 01:17:25 step: 3051, epoch: 92, batch: 14, loss: 1.5517302751541138, acc: 40.625, f1: 10.497835497835498, r: 0.20255328964953057
06/02/2019 01:17:25 step: 3056, epoch: 92, batch: 19, loss: 1.5671807527542114, acc: 34.375, f1: 17.05465587044534, r: 0.29694367185248216
06/02/2019 01:17:26 step: 3061, epoch: 92, batch: 24, loss: 1.4836013317108154, acc: 37.5, f1: 12.433862433862435, r: 0.27883643269262515
06/02/2019 01:17:26 step: 3066, epoch: 92, batch: 29, loss: 1.4526842832565308, acc: 32.8125, f1: 12.887817235643324, r: 0.2657958908662029
06/02/2019 01:17:26 *** evaluating ***
06/02/2019 01:17:26 step: 93, epoch: 92, acc: 44.871794871794876, f1: 8.003048780487806, r: 0.2595626942963061
06/02/2019 01:17:26 *** epoch: 94 ***
06/02/2019 01:17:26 *** training ***
06/02/2019 01:17:26 step: 3074, epoch: 93, batch: 4, loss: 1.6047580242156982, acc: 29.6875, f1: 23.63375603864734, r: 0.32399603977083063
06/02/2019 01:17:26 step: 3079, epoch: 93, batch: 9, loss: 1.5873823165893555, acc: 40.625, f1: 18.916797488226063, r: 0.2503276867213219
06/02/2019 01:17:27 step: 3084, epoch: 93, batch: 14, loss: 1.4463781118392944, acc: 34.375, f1: 12.723214285714285, r: 0.3014546918960681
06/02/2019 01:17:27 step: 3089, epoch: 93, batch: 19, loss: 1.4059128761291504, acc: 39.0625, f1: 28.16117216117216, r: 0.40196325393302
06/02/2019 01:17:27 step: 3094, epoch: 93, batch: 24, loss: 1.2878484725952148, acc: 51.5625, f1: 16.051136363636363, r: 0.25279719248043264
06/02/2019 01:17:27 step: 3099, epoch: 93, batch: 29, loss: 1.4066625833511353, acc: 48.4375, f1: 16.971049783549784, r: 0.21303672089634576
06/02/2019 01:17:28 *** evaluating ***
06/02/2019 01:17:28 step: 94, epoch: 93, acc: 45.2991452991453, f1: 8.601851851851853, r: 0.2743727507933402
06/02/2019 01:17:28 *** epoch: 95 ***
06/02/2019 01:17:28 *** training ***
06/02/2019 01:17:28 step: 3107, epoch: 94, batch: 4, loss: 1.4314868450164795, acc: 39.0625, f1: 18.217649467649466, r: 0.18288374167083868
06/02/2019 01:17:28 step: 3112, epoch: 94, batch: 9, loss: 1.4508494138717651, acc: 40.625, f1: 10.58362369337979, r: 0.22770641244678097
06/02/2019 01:17:28 step: 3117, epoch: 94, batch: 14, loss: 1.468672513961792, acc: 35.9375, f1: 8.75550220088035, r: 0.15753164455167107
06/02/2019 01:17:29 step: 3122, epoch: 94, batch: 19, loss: 1.4442952871322632, acc: 35.9375, f1: 15.583333333333332, r: 0.3409775704279779
06/02/2019 01:17:29 step: 3127, epoch: 94, batch: 24, loss: 1.4078612327575684, acc: 39.0625, f1: 21.05820105820106, r: 0.26656061535285125
06/02/2019 01:17:29 step: 3132, epoch: 94, batch: 29, loss: 1.5521591901779175, acc: 34.375, f1: 9.191176470588237, r: 0.27637393349102335
06/02/2019 01:17:29 *** evaluating ***
06/02/2019 01:17:29 step: 95, epoch: 94, acc: 44.871794871794876, f1: 7.906626506024097, r: 0.27248250434047716
06/02/2019 01:17:29 *** epoch: 96 ***
06/02/2019 01:17:29 *** training ***
06/02/2019 01:17:30 step: 3140, epoch: 95, batch: 4, loss: 1.288853645324707, acc: 43.75, f1: 15.091036414565828, r: 0.25741792440259165
06/02/2019 01:17:30 step: 3145, epoch: 95, batch: 9, loss: 1.3579649925231934, acc: 42.1875, f1: 21.09494870072785, r: 0.18051489889252598
06/02/2019 01:17:30 step: 3150, epoch: 95, batch: 14, loss: 1.5827572345733643, acc: 32.8125, f1: 12.688334593096497, r: 0.28733337891923827
06/02/2019 01:17:30 step: 3155, epoch: 95, batch: 19, loss: 1.499530553817749, acc: 35.9375, f1: 13.375747059957584, r: 0.1577945435901548
06/02/2019 01:17:31 step: 3160, epoch: 95, batch: 24, loss: 1.5535703897476196, acc: 32.8125, f1: 12.865497076023392, r: 0.31005818247478917
06/02/2019 01:17:31 step: 3165, epoch: 95, batch: 29, loss: 1.2677502632141113, acc: 50.0, f1: 18.425082669268715, r: 0.33007820372409263
06/02/2019 01:17:31 *** evaluating ***
06/02/2019 01:17:31 step: 96, epoch: 95, acc: 44.871794871794876, f1: 7.859281437125748, r: 0.26079024866055617
06/02/2019 01:17:31 *** epoch: 97 ***
06/02/2019 01:17:31 *** training ***
06/02/2019 01:17:31 step: 3173, epoch: 96, batch: 4, loss: 1.5343281030654907, acc: 31.25, f1: 14.01315789473684, r: 0.30434021009571044
06/02/2019 01:17:31 step: 3178, epoch: 96, batch: 9, loss: 1.4368557929992676, acc: 43.75, f1: 14.479638009049772, r: 0.3115997098932973
06/02/2019 01:17:32 step: 3183, epoch: 96, batch: 14, loss: 1.2595371007919312, acc: 46.875, f1: 20.113378684807255, r: 0.2916387651699154
06/02/2019 01:17:32 step: 3188, epoch: 96, batch: 19, loss: 1.439704179763794, acc: 39.0625, f1: 8.447712418300654, r: 0.23336722842013283
06/02/2019 01:17:32 step: 3193, epoch: 96, batch: 24, loss: 1.2519338130950928, acc: 51.5625, f1: 19.421373200442968, r: 0.29601540174428786
06/02/2019 01:17:32 step: 3198, epoch: 96, batch: 29, loss: 1.418626308441162, acc: 43.75, f1: 12.251082251082252, r: 0.24215744139921566
06/02/2019 01:17:32 *** evaluating ***
06/02/2019 01:17:33 step: 97, epoch: 96, acc: 44.871794871794876, f1: 7.906626506024097, r: 0.2724105844532397
06/02/2019 01:17:33 *** epoch: 98 ***
06/02/2019 01:17:33 *** training ***
06/02/2019 01:17:33 step: 3206, epoch: 97, batch: 4, loss: 1.3541792631149292, acc: 50.0, f1: 20.86206896551724, r: 0.2690732078411318
06/02/2019 01:17:33 step: 3211, epoch: 97, batch: 9, loss: 1.2267653942108154, acc: 46.875, f1: 11.198501872659175, r: 0.3266413944388424
06/02/2019 01:17:33 step: 3216, epoch: 97, batch: 14, loss: 1.549270510673523, acc: 37.5, f1: 19.448621553884713, r: 0.25539590619922464
06/02/2019 01:17:34 step: 3221, epoch: 97, batch: 19, loss: 1.3642436265945435, acc: 48.4375, f1: 22.35905856595512, r: 0.24949934888446632
06/02/2019 01:17:34 step: 3226, epoch: 97, batch: 24, loss: 1.2379518747329712, acc: 48.4375, f1: 23.36356764928194, r: 0.31912618411379495
06/02/2019 01:17:34 step: 3231, epoch: 97, batch: 29, loss: 1.4019789695739746, acc: 40.625, f1: 13.537414965986393, r: 0.21498964729287856
06/02/2019 01:17:34 *** evaluating ***
06/02/2019 01:17:34 step: 98, epoch: 97, acc: 41.88034188034188, f1: 10.570909398242055, r: 0.26030384399148865
06/02/2019 01:17:34 *** epoch: 99 ***
06/02/2019 01:17:34 *** training ***
06/02/2019 01:17:34 step: 3239, epoch: 98, batch: 4, loss: 1.3444592952728271, acc: 50.0, f1: 15.081168831168831, r: 0.22164555237401
06/02/2019 01:17:35 step: 3244, epoch: 98, batch: 9, loss: 1.407137155532837, acc: 50.0, f1: 23.83255967593317, r: 0.3195335831812293
06/02/2019 01:17:35 step: 3249, epoch: 98, batch: 14, loss: 1.3805863857269287, acc: 40.625, f1: 16.08231707317073, r: 0.1668813096350666
06/02/2019 01:17:35 step: 3254, epoch: 98, batch: 19, loss: 1.519599199295044, acc: 40.625, f1: 15.147115756871854, r: 0.29984867928395625
06/02/2019 01:17:35 step: 3259, epoch: 98, batch: 24, loss: 1.6377346515655518, acc: 28.125, f1: 5.487804878048781, r: 0.16733511504870374
06/02/2019 01:17:36 step: 3264, epoch: 98, batch: 29, loss: 1.4574048519134521, acc: 40.625, f1: 14.465002868617324, r: 0.22521116517790724
06/02/2019 01:17:36 *** evaluating ***
06/02/2019 01:17:36 step: 99, epoch: 98, acc: 44.871794871794876, f1: 7.954545454545454, r: 0.2596198562299573
06/02/2019 01:17:36 *** epoch: 100 ***
06/02/2019 01:17:36 *** training ***
06/02/2019 01:17:36 step: 3272, epoch: 99, batch: 4, loss: 1.4851210117340088, acc: 37.5, f1: 11.236933797909408, r: 0.15542649446254309
06/02/2019 01:17:36 step: 3277, epoch: 99, batch: 9, loss: 1.2826600074768066, acc: 46.875, f1: 16.29093497864262, r: 0.24226618759204427
06/02/2019 01:17:37 step: 3282, epoch: 99, batch: 14, loss: 1.5031791925430298, acc: 34.375, f1: 10.547839506172838, r: 0.23310871289664994
06/02/2019 01:17:37 step: 3287, epoch: 99, batch: 19, loss: 1.475150465965271, acc: 46.875, f1: 16.379310344827587, r: 0.2577384558364932
06/02/2019 01:17:37 step: 3292, epoch: 99, batch: 24, loss: 1.5829923152923584, acc: 32.8125, f1: 8.37028824833703, r: 0.20758769075545813
06/02/2019 01:17:37 step: 3297, epoch: 99, batch: 29, loss: 1.3714134693145752, acc: 35.9375, f1: 13.800705467372135, r: 0.29136918244769594
06/02/2019 01:17:38 *** evaluating ***
06/02/2019 01:17:38 step: 100, epoch: 99, acc: 44.871794871794876, f1: 7.906626506024097, r: 0.26567335098077066
06/02/2019 01:17:38 *** epoch: 101 ***
06/02/2019 01:17:38 *** training ***
06/02/2019 01:17:38 step: 3305, epoch: 100, batch: 4, loss: 1.363722324371338, acc: 46.875, f1: 21.15740740740741, r: 0.2633473361873063
06/02/2019 01:17:38 step: 3310, epoch: 100, batch: 9, loss: 1.3825628757476807, acc: 45.3125, f1: 13.862541952429591, r: 0.27842498895444634
06/02/2019 01:17:38 step: 3315, epoch: 100, batch: 14, loss: 1.3100299835205078, acc: 37.5, f1: 10.278745644599303, r: 0.27792413993237297
06/02/2019 01:17:39 step: 3320, epoch: 100, batch: 19, loss: 1.3984925746917725, acc: 39.0625, f1: 11.5406162464986, r: 0.300949275587525
06/02/2019 01:17:39 step: 3325, epoch: 100, batch: 24, loss: 1.255612850189209, acc: 39.0625, f1: 17.570516684440733, r: 0.2941856102616062
06/02/2019 01:17:39 step: 3330, epoch: 100, batch: 29, loss: 1.2229180335998535, acc: 51.5625, f1: 18.738880167451594, r: 0.24923350774307534
06/02/2019 01:17:39 *** evaluating ***
06/02/2019 01:17:39 step: 101, epoch: 100, acc: 44.871794871794876, f1: 7.930513595166164, r: 0.26435561132340235
06/02/2019 01:17:39 *** epoch: 102 ***
06/02/2019 01:17:39 *** training ***
06/02/2019 01:17:39 step: 3338, epoch: 101, batch: 4, loss: 1.5895795822143555, acc: 32.8125, f1: 14.723980309423348, r: 0.2722617790266478
06/02/2019 01:17:40 step: 3343, epoch: 101, batch: 9, loss: 1.5387054681777954, acc: 37.5, f1: 16.383928571428573, r: 0.29141775973500206
06/02/2019 01:17:40 step: 3348, epoch: 101, batch: 14, loss: 1.1176451444625854, acc: 51.5625, f1: 20.434072793623358, r: 0.33529178783450847
06/02/2019 01:17:40 step: 3353, epoch: 101, batch: 19, loss: 1.5458768606185913, acc: 42.1875, f1: 20.321629589922274, r: 0.262495567510762
06/02/2019 01:17:40 step: 3358, epoch: 101, batch: 24, loss: 1.3317595720291138, acc: 40.625, f1: 14.91219743711438, r: 0.31084685776325716
06/02/2019 01:17:41 step: 3363, epoch: 101, batch: 29, loss: 1.4621003866195679, acc: 35.9375, f1: 12.313786274003272, r: 0.18614229297354334
06/02/2019 01:17:41 *** evaluating ***
06/02/2019 01:17:41 step: 102, epoch: 101, acc: 19.65811965811966, f1: 6.25, r: 0.2246884100118428
06/02/2019 01:17:41 *** epoch: 103 ***
06/02/2019 01:17:41 *** training ***
06/02/2019 01:17:41 step: 3371, epoch: 102, batch: 4, loss: 1.3395239114761353, acc: 43.75, f1: 14.54895982783357, r: 0.2501047150366026
06/02/2019 01:17:41 step: 3376, epoch: 102, batch: 9, loss: 1.4533488750457764, acc: 35.9375, f1: 20.186830713146502, r: 0.3211903462333345
06/02/2019 01:17:42 step: 3381, epoch: 102, batch: 14, loss: 1.393122673034668, acc: 37.5, f1: 18.30720092915215, r: 0.2586643290771041
06/02/2019 01:17:42 step: 3386, epoch: 102, batch: 19, loss: 1.3172866106033325, acc: 45.3125, f1: 18.45104895104895, r: 0.2687613765649397
06/02/2019 01:17:42 step: 3391, epoch: 102, batch: 24, loss: 1.3205931186676025, acc: 37.5, f1: 19.39309056956116, r: 0.3842032702912937
06/02/2019 01:17:42 step: 3396, epoch: 102, batch: 29, loss: 1.6648622751235962, acc: 28.125, f1: 6.349206349206349, r: 0.20017446978084075
06/02/2019 01:17:42 *** evaluating ***
06/02/2019 01:17:43 step: 103, epoch: 102, acc: 45.2991452991453, f1: 8.54652704135737, r: 0.26529995510523013
06/02/2019 01:17:43 *** epoch: 104 ***
06/02/2019 01:17:43 *** training ***
06/02/2019 01:17:43 step: 3404, epoch: 103, batch: 4, loss: 1.2469286918640137, acc: 51.5625, f1: 16.414835164835164, r: 0.2923623058051561
06/02/2019 01:17:43 step: 3409, epoch: 103, batch: 9, loss: 1.4818685054779053, acc: 46.875, f1: 22.19095719095719, r: 0.257227767955377
06/02/2019 01:17:43 step: 3414, epoch: 103, batch: 14, loss: 2.0370678901672363, acc: 31.25, f1: 8.730765033286044, r: 0.2213373653389374
06/02/2019 01:17:43 step: 3419, epoch: 103, batch: 19, loss: 1.4944233894348145, acc: 32.8125, f1: 16.202844774273345, r: 0.3142706303173193
06/02/2019 01:17:44 step: 3424, epoch: 103, batch: 24, loss: 1.3703484535217285, acc: 34.375, f1: 17.550733373518185, r: 0.37498575441804805
06/02/2019 01:17:44 step: 3429, epoch: 103, batch: 29, loss: 1.4381418228149414, acc: 37.5, f1: 8.60912343470483, r: 0.22766441834871132
06/02/2019 01:17:44 *** evaluating ***
06/02/2019 01:17:44 step: 104, epoch: 103, acc: 45.2991452991453, f1: 8.406626506024097, r: 0.26793865203095085
06/02/2019 01:17:44 *** epoch: 105 ***
06/02/2019 01:17:44 *** training ***
06/02/2019 01:17:44 step: 3437, epoch: 104, batch: 4, loss: 1.5186152458190918, acc: 35.9375, f1: 9.973867595818817, r: 0.2321299916465165
06/02/2019 01:17:45 step: 3442, epoch: 104, batch: 9, loss: 1.4529619216918945, acc: 42.1875, f1: 11.746323529411766, r: 0.3102157409210797
06/02/2019 01:17:45 step: 3447, epoch: 104, batch: 14, loss: 1.51506507396698, acc: 34.375, f1: 15.397522645421805, r: 0.2075112524191713
06/02/2019 01:17:45 step: 3452, epoch: 104, batch: 19, loss: 1.4295631647109985, acc: 42.1875, f1: 17.282717282717282, r: 0.1821001694774137
06/02/2019 01:17:45 step: 3457, epoch: 104, batch: 24, loss: 1.3681577444076538, acc: 40.625, f1: 17.721804511278197, r: 0.25080535836456036
06/02/2019 01:17:46 step: 3462, epoch: 104, batch: 29, loss: 1.4263336658477783, acc: 42.1875, f1: 11.345909955611923, r: 0.2859610082876582
06/02/2019 01:17:46 *** evaluating ***
06/02/2019 01:17:46 step: 105, epoch: 104, acc: 44.871794871794876, f1: 7.859281437125748, r: 0.2656232163834357
06/02/2019 01:17:46 *** epoch: 106 ***
06/02/2019 01:17:46 *** training ***
06/02/2019 01:17:46 step: 3470, epoch: 105, batch: 4, loss: 1.4277173280715942, acc: 40.625, f1: 13.032581453634087, r: 0.27117538919772194
06/02/2019 01:17:46 step: 3475, epoch: 105, batch: 9, loss: 1.4589076042175293, acc: 43.75, f1: 15.051679586563308, r: 0.27041777970705605
06/02/2019 01:17:47 step: 3480, epoch: 105, batch: 14, loss: 1.445574164390564, acc: 45.3125, f1: 10.902777777777779, r: 0.2396499831572828
06/02/2019 01:17:47 step: 3485, epoch: 105, batch: 19, loss: 1.4007387161254883, acc: 35.9375, f1: 12.619047619047619, r: 0.18454525689519277
06/02/2019 01:17:47 step: 3490, epoch: 105, batch: 24, loss: 1.2595049142837524, acc: 53.125, f1: 23.516414141414142, r: 0.31607303657876173
06/02/2019 01:17:47 step: 3495, epoch: 105, batch: 29, loss: 1.4013792276382446, acc: 31.25, f1: 16.125541125541126, r: 0.33176020705888115
06/02/2019 01:17:48 *** evaluating ***
06/02/2019 01:17:48 step: 106, epoch: 105, acc: 44.871794871794876, f1: 7.882882882882883, r: 0.273012267061326
06/02/2019 01:17:48 *** epoch: 107 ***
06/02/2019 01:17:48 *** training ***
06/02/2019 01:17:48 step: 3503, epoch: 106, batch: 4, loss: 1.2472565174102783, acc: 51.5625, f1: 11.631205673758863, r: 0.28562854162399104
06/02/2019 01:17:48 step: 3508, epoch: 106, batch: 9, loss: 1.5036853551864624, acc: 39.0625, f1: 19.14210128495843, r: 0.2578434425669975
06/02/2019 01:17:48 step: 3513, epoch: 106, batch: 14, loss: 1.5119414329528809, acc: 29.6875, f1: 16.301169590643276, r: 0.27827512718208286
06/02/2019 01:17:48 step: 3518, epoch: 106, batch: 19, loss: 1.22809898853302, acc: 46.875, f1: 13.410532752426816, r: 0.2829761862486697
06/02/2019 01:17:49 step: 3523, epoch: 106, batch: 24, loss: 1.3916362524032593, acc: 39.0625, f1: 19.185956790123456, r: 0.26114471032890185
06/02/2019 01:17:49 step: 3528, epoch: 106, batch: 29, loss: 1.2997393608093262, acc: 51.5625, f1: 22.987012987012985, r: 0.21722122052906032
06/02/2019 01:17:49 *** evaluating ***
06/02/2019 01:17:49 step: 107, epoch: 106, acc: 44.871794871794876, f1: 7.906626506024097, r: 0.2729133974196906
06/02/2019 01:17:49 *** epoch: 108 ***
06/02/2019 01:17:49 *** training ***
06/02/2019 01:17:49 step: 3536, epoch: 107, batch: 4, loss: 1.6367793083190918, acc: 37.5, f1: 9.09090909090909, r: 0.13693198511775795
06/02/2019 01:17:50 step: 3541, epoch: 107, batch: 9, loss: 1.4425110816955566, acc: 43.75, f1: 14.053030303030303, r: 0.2628788098633113
06/02/2019 01:17:50 step: 3546, epoch: 107, batch: 14, loss: 1.27940833568573, acc: 50.0, f1: 17.342904702455268, r: 0.2338514524271783
06/02/2019 01:17:50 step: 3551, epoch: 107, batch: 19, loss: 1.2179802656173706, acc: 53.125, f1: 24.523544088761483, r: 0.338625316852803
06/02/2019 01:17:50 step: 3556, epoch: 107, batch: 24, loss: 1.3712621927261353, acc: 46.875, f1: 24.38618108798832, r: 0.26442375976222154
06/02/2019 01:17:51 step: 3561, epoch: 107, batch: 29, loss: 1.2980220317840576, acc: 46.875, f1: 15.60514449682312, r: 0.28325384228908607
06/02/2019 01:17:51 *** evaluating ***
06/02/2019 01:17:51 step: 108, epoch: 107, acc: 44.871794871794876, f1: 7.930513595166164, r: 0.267693729176765
06/02/2019 01:17:51 *** epoch: 109 ***
06/02/2019 01:17:51 *** training ***
06/02/2019 01:17:51 step: 3569, epoch: 108, batch: 4, loss: 1.3498015403747559, acc: 48.4375, f1: 19.3283004258614, r: 0.260611763694032
06/02/2019 01:17:51 step: 3574, epoch: 108, batch: 9, loss: 1.4180949926376343, acc: 42.1875, f1: 14.011172650878533, r: 0.18623106030369274
06/02/2019 01:17:52 step: 3579, epoch: 108, batch: 14, loss: 1.4142311811447144, acc: 31.25, f1: 11.537414965986393, r: 0.21789339312485936
06/02/2019 01:17:52 step: 3584, epoch: 108, batch: 19, loss: 1.516477346420288, acc: 31.25, f1: 9.714285714285714, r: 0.22101910039330663
06/02/2019 01:17:52 step: 3589, epoch: 108, batch: 24, loss: 1.4375948905944824, acc: 50.0, f1: 21.003422806701494, r: 0.26389699801171984
06/02/2019 01:17:52 step: 3594, epoch: 108, batch: 29, loss: 1.2891862392425537, acc: 45.3125, f1: 25.242918313570485, r: 0.3370093090012221
06/02/2019 01:17:52 *** evaluating ***
06/02/2019 01:17:52 step: 109, epoch: 108, acc: 53.84615384615385, f1: 15.541666666666668, r: 0.26926888519824765
06/02/2019 01:17:52 *** epoch: 110 ***
06/02/2019 01:17:52 *** training ***
06/02/2019 01:17:53 step: 3602, epoch: 109, batch: 4, loss: 1.4313079118728638, acc: 37.5, f1: 17.7088948787062, r: 0.2695904594484817
06/02/2019 01:17:53 step: 3607, epoch: 109, batch: 9, loss: 1.3661351203918457, acc: 43.75, f1: 21.26010781671159, r: 0.24237036563430445
06/02/2019 01:17:53 step: 3612, epoch: 109, batch: 14, loss: 1.2594105005264282, acc: 53.125, f1: 33.76911976911977, r: 0.2946142303466068
06/02/2019 01:17:53 step: 3617, epoch: 109, batch: 19, loss: 1.2617177963256836, acc: 43.75, f1: 20.716858171840162, r: 0.25010101928864514
06/02/2019 01:17:54 step: 3622, epoch: 109, batch: 24, loss: 1.4279156923294067, acc: 39.0625, f1: 15.916802148865482, r: 0.24175408848978194
06/02/2019 01:17:54 step: 3627, epoch: 109, batch: 29, loss: 1.3895872831344604, acc: 46.875, f1: 21.333333333333332, r: 0.2858726227371515
06/02/2019 01:17:54 *** evaluating ***
06/02/2019 01:17:54 step: 110, epoch: 109, acc: 52.13675213675214, f1: 14.969995644004655, r: 0.2833622679894485
06/02/2019 01:17:54 *** epoch: 111 ***
06/02/2019 01:17:54 *** training ***
06/02/2019 01:17:54 step: 3635, epoch: 110, batch: 4, loss: 1.4130609035491943, acc: 46.875, f1: 19.15063168124393, r: 0.2623893233334292
06/02/2019 01:17:54 step: 3640, epoch: 110, batch: 9, loss: 1.3771111965179443, acc: 39.0625, f1: 14.27450100919489, r: 0.19751174232453178
06/02/2019 01:17:55 step: 3645, epoch: 110, batch: 14, loss: 1.394069790840149, acc: 45.3125, f1: 20.434078393262066, r: 0.32161981018979197
06/02/2019 01:17:55 step: 3650, epoch: 110, batch: 19, loss: 1.4014240503311157, acc: 45.3125, f1: 29.121622809330454, r: 0.2200421331556784
06/02/2019 01:17:55 step: 3655, epoch: 110, batch: 24, loss: 1.4440034627914429, acc: 39.0625, f1: 28.35213032581454, r: 0.3411460454873604
06/02/2019 01:17:56 step: 3660, epoch: 110, batch: 29, loss: 1.4872885942459106, acc: 42.1875, f1: 21.470588235294116, r: 0.30726727150685407
06/02/2019 01:17:56 *** evaluating ***
06/02/2019 01:17:56 step: 111, epoch: 110, acc: 53.84615384615385, f1: 15.878198677400803, r: 0.2746091930669687
06/02/2019 01:17:56 *** epoch: 112 ***
06/02/2019 01:17:56 *** training ***
06/02/2019 01:17:56 step: 3668, epoch: 111, batch: 4, loss: 1.3569077253341675, acc: 40.625, f1: 18.33037047571642, r: 0.29719653818139263
06/02/2019 01:17:56 step: 3673, epoch: 111, batch: 9, loss: 1.2828043699264526, acc: 51.5625, f1: 19.471153846153847, r: 0.2732401588092013
06/02/2019 01:17:57 step: 3678, epoch: 111, batch: 14, loss: 1.4161001443862915, acc: 40.625, f1: 18.438954529180094, r: 0.3200835401001547
06/02/2019 01:17:57 step: 3683, epoch: 111, batch: 19, loss: 1.339897632598877, acc: 50.0, f1: 22.001527883880826, r: 0.24482234352774024
06/02/2019 01:17:57 step: 3688, epoch: 111, batch: 24, loss: 1.4782021045684814, acc: 40.625, f1: 21.026312823980163, r: 0.32440348901165683
06/02/2019 01:17:57 step: 3693, epoch: 111, batch: 29, loss: 1.3140724897384644, acc: 46.875, f1: 27.363550071191263, r: 0.2952855664532335
06/02/2019 01:17:57 *** evaluating ***
06/02/2019 01:17:57 step: 112, epoch: 111, acc: 54.27350427350427, f1: 15.817326930772307, r: 0.26436543420941255
06/02/2019 01:17:57 *** epoch: 113 ***
06/02/2019 01:17:57 *** training ***
06/02/2019 01:17:58 step: 3701, epoch: 112, batch: 4, loss: 1.3597004413604736, acc: 46.875, f1: 21.10434434410504, r: 0.2515059827717901
06/02/2019 01:17:58 step: 3706, epoch: 112, batch: 9, loss: 1.4642152786254883, acc: 34.375, f1: 12.553418803418804, r: 0.20878276603693974
06/02/2019 01:17:58 step: 3711, epoch: 112, batch: 14, loss: 1.1377724409103394, acc: 50.0, f1: 33.69907326455674, r: 0.336003876301854
06/02/2019 01:17:58 step: 3716, epoch: 112, batch: 19, loss: 1.2941590547561646, acc: 43.75, f1: 19.03112760255617, r: 0.28831913487331834
06/02/2019 01:17:59 step: 3721, epoch: 112, batch: 24, loss: 1.388290524482727, acc: 51.5625, f1: 21.515972735484933, r: 0.24929774734903842
06/02/2019 01:17:59 step: 3726, epoch: 112, batch: 29, loss: 1.4124724864959717, acc: 37.5, f1: 23.35078545245038, r: 0.2252808717608974
06/02/2019 01:17:59 *** evaluating ***
06/02/2019 01:17:59 step: 113, epoch: 112, acc: 55.12820512820513, f1: 16.815698677400807, r: 0.27055336424751547
06/02/2019 01:17:59 *** epoch: 114 ***
06/02/2019 01:17:59 *** training ***
06/02/2019 01:17:59 step: 3734, epoch: 113, batch: 4, loss: 1.4185165166854858, acc: 48.4375, f1: 17.715403731315607, r: 0.23348317446373762
06/02/2019 01:18:00 step: 3739, epoch: 113, batch: 9, loss: 1.3657257556915283, acc: 46.875, f1: 27.947201607915893, r: 0.29554098771605986
06/02/2019 01:18:00 step: 3744, epoch: 113, batch: 14, loss: 1.40055251121521, acc: 46.875, f1: 17.76936026936027, r: 0.22996208858989628
06/02/2019 01:18:00 step: 3749, epoch: 113, batch: 19, loss: 1.4044549465179443, acc: 50.0, f1: 23.67075433277524, r: 0.279863611941337
06/02/2019 01:18:00 step: 3754, epoch: 113, batch: 24, loss: 1.4319924116134644, acc: 51.5625, f1: 30.442520442520447, r: 0.2619749679509196
06/02/2019 01:18:01 step: 3759, epoch: 113, batch: 29, loss: 1.2772464752197266, acc: 48.4375, f1: 20.044454722993112, r: 0.39444623764247266
06/02/2019 01:18:01 *** evaluating ***
06/02/2019 01:18:01 step: 114, epoch: 113, acc: 52.56410256410257, f1: 14.864864864864865, r: 0.2672385196428469
06/02/2019 01:18:01 *** epoch: 115 ***
06/02/2019 01:18:01 *** training ***
06/02/2019 01:18:01 step: 3767, epoch: 114, batch: 4, loss: 1.517494797706604, acc: 40.625, f1: 17.22222222222222, r: 0.19056743319955244
06/02/2019 01:18:01 step: 3772, epoch: 114, batch: 9, loss: 1.399706482887268, acc: 43.75, f1: 17.36393405600723, r: 0.24568799924879658
06/02/2019 01:18:01 step: 3777, epoch: 114, batch: 14, loss: 1.423598051071167, acc: 50.0, f1: 17.888100866824274, r: 0.25470104289902346
06/02/2019 01:18:02 step: 3782, epoch: 114, batch: 19, loss: 1.305835247039795, acc: 46.875, f1: 25.97129107616651, r: 0.3704516282605943
06/02/2019 01:18:02 step: 3787, epoch: 114, batch: 24, loss: 1.5338382720947266, acc: 34.375, f1: 17.28320029027576, r: 0.2027753748111671
06/02/2019 01:18:02 step: 3792, epoch: 114, batch: 29, loss: 1.2916780710220337, acc: 39.0625, f1: 22.64151963328144, r: 0.31898475436387896
06/02/2019 01:18:02 *** evaluating ***
06/02/2019 01:18:02 step: 115, epoch: 114, acc: 54.700854700854705, f1: 16.33458569434179, r: 0.27502327795280784
06/02/2019 01:18:02 *** epoch: 116 ***
06/02/2019 01:18:02 *** training ***
06/02/2019 01:18:03 step: 3800, epoch: 115, batch: 4, loss: 1.4206929206848145, acc: 37.5, f1: 18.05323335191096, r: 0.18600644730615715
06/02/2019 01:18:03 step: 3805, epoch: 115, batch: 9, loss: 1.437164545059204, acc: 42.1875, f1: 17.573351529061473, r: 0.2718405493732514
06/02/2019 01:18:03 step: 3810, epoch: 115, batch: 14, loss: 1.3027143478393555, acc: 42.1875, f1: 24.90068366592757, r: 0.43268652109870587
06/02/2019 01:18:03 step: 3815, epoch: 115, batch: 19, loss: 1.163932204246521, acc: 51.5625, f1: 35.5963480963481, r: 0.3349979149269481
06/02/2019 01:18:04 step: 3820, epoch: 115, batch: 24, loss: 1.3657609224319458, acc: 46.875, f1: 25.85395634176122, r: 0.2796620005540675
06/02/2019 01:18:04 step: 3825, epoch: 115, batch: 29, loss: 1.5035744905471802, acc: 32.8125, f1: 13.315396668316016, r: 0.27699974828907253
06/02/2019 01:18:04 *** evaluating ***
06/02/2019 01:18:04 step: 116, epoch: 115, acc: 54.700854700854705, f1: 16.05226550079491, r: 0.2682993161803011
06/02/2019 01:18:04 *** epoch: 117 ***
06/02/2019 01:18:04 *** training ***
06/02/2019 01:18:04 step: 3833, epoch: 116, batch: 4, loss: 1.2869991064071655, acc: 50.0, f1: 21.50782208451613, r: 0.32734488019806013
06/02/2019 01:18:05 step: 3838, epoch: 116, batch: 9, loss: 1.3814853429794312, acc: 39.0625, f1: 23.125, r: 0.2510143568355067
06/02/2019 01:18:05 step: 3843, epoch: 116, batch: 14, loss: 1.3257280588150024, acc: 48.4375, f1: 30.645616737260937, r: 0.33817575864214733
06/02/2019 01:18:05 step: 3848, epoch: 116, batch: 19, loss: 1.1851085424423218, acc: 50.0, f1: 39.3009768009768, r: 0.2938502374144871
06/02/2019 01:18:05 step: 3853, epoch: 116, batch: 24, loss: 1.2946441173553467, acc: 40.625, f1: 16.613756613756614, r: 0.20975232617300055
06/02/2019 01:18:06 step: 3858, epoch: 116, batch: 29, loss: 1.3489351272583008, acc: 51.5625, f1: 34.60249554367202, r: 0.2902707353527981
06/02/2019 01:18:06 *** evaluating ***
06/02/2019 01:18:06 step: 117, epoch: 116, acc: 54.27350427350427, f1: 16.051448946470817, r: 0.28316055896789716
06/02/2019 01:18:06 *** epoch: 118 ***
06/02/2019 01:18:06 *** training ***
06/02/2019 01:18:06 step: 3866, epoch: 117, batch: 4, loss: 1.5845797061920166, acc: 31.25, f1: 13.148584905660377, r: 0.18350911750219506
06/02/2019 01:18:06 step: 3871, epoch: 117, batch: 9, loss: 1.3369789123535156, acc: 40.625, f1: 16.26222509252205, r: 0.31987064048038305
06/02/2019 01:18:07 step: 3876, epoch: 117, batch: 14, loss: 1.5018163919448853, acc: 34.375, f1: 14.5516717325228, r: 0.25139958263288353
06/02/2019 01:18:07 step: 3881, epoch: 117, batch: 19, loss: 1.4001445770263672, acc: 40.625, f1: 17.522727272727273, r: 0.3025800485849831
06/02/2019 01:18:07 step: 3886, epoch: 117, batch: 24, loss: 1.523396372795105, acc: 45.3125, f1: 22.084064969271292, r: 0.2285232112300088
06/02/2019 01:18:07 step: 3891, epoch: 117, batch: 29, loss: 1.2702522277832031, acc: 35.9375, f1: 28.035800677310107, r: 0.2712596481174396
06/02/2019 01:18:07 *** evaluating ***
06/02/2019 01:18:07 step: 118, epoch: 117, acc: 54.27350427350427, f1: 16.064354656074194, r: 0.27093581810876294
06/02/2019 01:18:07 *** epoch: 119 ***
06/02/2019 01:18:07 *** training ***
06/02/2019 01:18:08 step: 3899, epoch: 118, batch: 4, loss: 1.530011534690857, acc: 46.875, f1: 22.57287114429971, r: 0.274644107022157
06/02/2019 01:18:08 step: 3904, epoch: 118, batch: 9, loss: 1.546501636505127, acc: 40.625, f1: 25.10615889098861, r: 0.2633900469205018
06/02/2019 01:18:08 step: 3909, epoch: 118, batch: 14, loss: 1.1299370527267456, acc: 56.25, f1: 25.47835892098187, r: 0.2599053966427706
06/02/2019 01:18:08 step: 3914, epoch: 118, batch: 19, loss: 1.2955769300460815, acc: 50.0, f1: 21.49659863945578, r: 0.30462319226416806
06/02/2019 01:18:09 step: 3919, epoch: 118, batch: 24, loss: 1.2121738195419312, acc: 45.3125, f1: 17.731769036116862, r: 0.2961149753857484
06/02/2019 01:18:09 step: 3924, epoch: 118, batch: 29, loss: 1.4937835931777954, acc: 40.625, f1: 16.3578442833762, r: 0.22506570566343576
06/02/2019 01:18:09 *** evaluating ***
06/02/2019 01:18:09 step: 119, epoch: 118, acc: 53.84615384615385, f1: 15.62081198265668, r: 0.27522595220655166
06/02/2019 01:18:09 *** epoch: 120 ***
06/02/2019 01:18:09 *** training ***
06/02/2019 01:18:09 step: 3932, epoch: 119, batch: 4, loss: 1.4717649221420288, acc: 39.0625, f1: 20.01923430494859, r: 0.24809933604084813
06/02/2019 01:18:10 step: 3937, epoch: 119, batch: 9, loss: 1.3696163892745972, acc: 50.0, f1: 24.718381976446494, r: 0.30578895693338976
06/02/2019 01:18:10 step: 3942, epoch: 119, batch: 14, loss: 1.3363475799560547, acc: 45.3125, f1: 19.808115254404825, r: 0.2768072130477366
06/02/2019 01:18:10 step: 3947, epoch: 119, batch: 19, loss: 1.297133207321167, acc: 46.875, f1: 33.90895228523303, r: 0.2698670313238524
06/02/2019 01:18:10 step: 3952, epoch: 119, batch: 24, loss: 1.2165021896362305, acc: 45.3125, f1: 17.3365572644152, r: 0.2854895089794417
06/02/2019 01:18:10 step: 3957, epoch: 119, batch: 29, loss: 1.3226767778396606, acc: 45.3125, f1: 29.579767749980512, r: 0.3207335732074867
06/02/2019 01:18:11 *** evaluating ***
06/02/2019 01:18:11 step: 120, epoch: 119, acc: 53.84615384615385, f1: 15.774495514428398, r: 0.26851868757571934
06/02/2019 01:18:11 *** epoch: 121 ***
06/02/2019 01:18:11 *** training ***
06/02/2019 01:18:11 step: 3965, epoch: 120, batch: 4, loss: 1.4338881969451904, acc: 45.3125, f1: 16.72939068100358, r: 0.2693688014122836
06/02/2019 01:18:11 step: 3970, epoch: 120, batch: 9, loss: 1.30490243434906, acc: 43.75, f1: 20.08730158730159, r: 0.2814065995107455
06/02/2019 01:18:11 step: 3975, epoch: 120, batch: 14, loss: 1.3291579484939575, acc: 48.4375, f1: 21.80627570177007, r: 0.281144369334096
06/02/2019 01:18:12 step: 3980, epoch: 120, batch: 19, loss: 1.419989824295044, acc: 40.625, f1: 17.727502527805864, r: 0.2829108463820773
06/02/2019 01:18:12 step: 3985, epoch: 120, batch: 24, loss: 1.2316728830337524, acc: 46.875, f1: 35.351473922902485, r: 0.3332929593199849
06/02/2019 01:18:12 step: 3990, epoch: 120, batch: 29, loss: 1.2401676177978516, acc: 43.75, f1: 19.351781116487, r: 0.32116013363258694
06/02/2019 01:18:12 *** evaluating ***
06/02/2019 01:18:12 step: 121, epoch: 120, acc: 55.55555555555556, f1: 16.784814986185996, r: 0.27506723098084934
06/02/2019 01:18:12 *** epoch: 122 ***
06/02/2019 01:18:12 *** training ***
06/02/2019 01:18:13 step: 3998, epoch: 121, batch: 4, loss: 1.450243592262268, acc: 37.5, f1: 17.281665702718335, r: 0.26829363067346407
06/02/2019 01:18:13 step: 4003, epoch: 121, batch: 9, loss: 1.2167916297912598, acc: 62.5, f1: 27.698771816418873, r: 0.254829476267248
06/02/2019 01:18:13 step: 4008, epoch: 121, batch: 14, loss: 1.3141733407974243, acc: 40.625, f1: 15.39313399778516, r: 0.34752555595664086
06/02/2019 01:18:13 step: 4013, epoch: 121, batch: 19, loss: 1.3657097816467285, acc: 50.0, f1: 22.5, r: 0.32213933781910875
06/02/2019 01:18:13 step: 4018, epoch: 121, batch: 24, loss: 1.4531172513961792, acc: 43.75, f1: 27.96470805617147, r: 0.2622014264137667
06/02/2019 01:18:14 step: 4023, epoch: 121, batch: 29, loss: 1.394039273262024, acc: 43.75, f1: 18.82725279106858, r: 0.2987297336971148
06/02/2019 01:18:14 *** evaluating ***
06/02/2019 01:18:14 step: 122, epoch: 121, acc: 55.55555555555556, f1: 16.69290354822589, r: 0.2763342467199466
06/02/2019 01:18:14 *** epoch: 123 ***
06/02/2019 01:18:14 *** training ***
06/02/2019 01:18:14 step: 4031, epoch: 122, batch: 4, loss: 1.279099464416504, acc: 46.875, f1: 21.281158740091783, r: 0.3097975557068049
06/02/2019 01:18:14 step: 4036, epoch: 122, batch: 9, loss: 1.2349190711975098, acc: 46.875, f1: 18.398268398268396, r: 0.3175602129344085
06/02/2019 01:18:15 step: 4041, epoch: 122, batch: 14, loss: 1.3754023313522339, acc: 45.3125, f1: 27.492305633614343, r: 0.29347998989529456
06/02/2019 01:18:15 step: 4046, epoch: 122, batch: 19, loss: 1.5434497594833374, acc: 39.0625, f1: 18.790218790218788, r: 0.29301709245813357
06/02/2019 01:18:15 step: 4051, epoch: 122, batch: 24, loss: 1.4059027433395386, acc: 46.875, f1: 20.1421731483291, r: 0.2441978442061249
06/02/2019 01:18:15 step: 4056, epoch: 122, batch: 29, loss: 1.1922699213027954, acc: 48.4375, f1: 32.00793650793651, r: 0.2863026222663817
06/02/2019 01:18:15 *** evaluating ***
06/02/2019 01:18:15 step: 123, epoch: 122, acc: 52.56410256410257, f1: 14.971550497866287, r: 0.28250051149078836
06/02/2019 01:18:15 *** epoch: 124 ***
06/02/2019 01:18:15 *** training ***
06/02/2019 01:18:16 step: 4064, epoch: 123, batch: 4, loss: 1.4350807666778564, acc: 37.5, f1: 25.88614393125671, r: 0.33335471097048786
06/02/2019 01:18:16 step: 4069, epoch: 123, batch: 9, loss: 1.4351989030838013, acc: 35.9375, f1: 15.86466165413534, r: 0.2239147487881347
06/02/2019 01:18:16 step: 4074, epoch: 123, batch: 14, loss: 2.272326707839966, acc: 48.4375, f1: 30.36630036630037, r: 0.2800420371379632
06/02/2019 01:18:17 step: 4079, epoch: 123, batch: 19, loss: 1.2580976486206055, acc: 51.5625, f1: 22.2269595235803, r: 0.2385269920430764
06/02/2019 01:18:17 step: 4084, epoch: 123, batch: 24, loss: 1.5069910287857056, acc: 32.8125, f1: 14.771622934888242, r: 0.20070112718043073
06/02/2019 01:18:17 step: 4089, epoch: 123, batch: 29, loss: 1.335233449935913, acc: 46.875, f1: 21.24470300940889, r: 0.2573079341054295
06/02/2019 01:18:17 *** evaluating ***
06/02/2019 01:18:17 step: 124, epoch: 123, acc: 55.98290598290598, f1: 17.196488351438134, r: 0.2752928978887272
06/02/2019 01:18:17 *** epoch: 125 ***
06/02/2019 01:18:17 *** training ***
06/02/2019 01:18:18 step: 4097, epoch: 124, batch: 4, loss: 1.355241060256958, acc: 46.875, f1: 16.32313829787234, r: 0.26893005056751657
06/02/2019 01:18:18 step: 4102, epoch: 124, batch: 9, loss: 1.4599521160125732, acc: 42.1875, f1: 22.383568812140236, r: 0.23176078502238903
06/02/2019 01:18:18 step: 4107, epoch: 124, batch: 14, loss: 1.201303243637085, acc: 59.375, f1: 26.103249475890983, r: 0.33363854525794434
06/02/2019 01:18:18 step: 4112, epoch: 124, batch: 19, loss: 1.2858176231384277, acc: 53.125, f1: 28.542568542568542, r: 0.2566888284107067
06/02/2019 01:18:18 step: 4117, epoch: 124, batch: 24, loss: 1.209071159362793, acc: 53.125, f1: 34.609440267335, r: 0.39252209054842896
06/02/2019 01:18:19 step: 4122, epoch: 124, batch: 29, loss: 1.3279459476470947, acc: 42.1875, f1: 16.228661275831087, r: 0.327618981100289
06/02/2019 01:18:19 *** evaluating ***
06/02/2019 01:18:19 step: 125, epoch: 124, acc: 54.27350427350427, f1: 15.912332214765101, r: 0.28517388911881353
06/02/2019 01:18:19 *** epoch: 126 ***
06/02/2019 01:18:19 *** training ***
06/02/2019 01:18:19 step: 4130, epoch: 125, batch: 4, loss: 1.3784475326538086, acc: 54.6875, f1: 23.992673992673993, r: 0.32304066866319825
06/02/2019 01:18:19 step: 4135, epoch: 125, batch: 9, loss: 1.1545522212982178, acc: 54.6875, f1: 23.479591836734695, r: 0.3211468950382101
06/02/2019 01:18:20 step: 4140, epoch: 125, batch: 14, loss: 1.2789034843444824, acc: 43.75, f1: 18.92644346715846, r: 0.23396505046139618
06/02/2019 01:18:20 step: 4145, epoch: 125, batch: 19, loss: 1.5402830839157104, acc: 35.9375, f1: 18.15217391304348, r: 0.2751678654547062
06/02/2019 01:18:20 step: 4150, epoch: 125, batch: 24, loss: 1.1343809366226196, acc: 60.9375, f1: 40.94251336898396, r: 0.38856211591451895
06/02/2019 01:18:20 step: 4155, epoch: 125, batch: 29, loss: 1.401852011680603, acc: 35.9375, f1: 20.59492231906025, r: 0.3433709903214696
06/02/2019 01:18:21 *** evaluating ***
06/02/2019 01:18:21 step: 126, epoch: 125, acc: 52.991452991452995, f1: 15.188218891599863, r: 0.2748546623206979
06/02/2019 01:18:21 *** epoch: 127 ***
06/02/2019 01:18:21 *** training ***
06/02/2019 01:18:21 step: 4163, epoch: 126, batch: 4, loss: 1.3856121301651, acc: 43.75, f1: 23.664086324184847, r: 0.3336908065578434
06/02/2019 01:18:21 step: 4168, epoch: 126, batch: 9, loss: 1.2260018587112427, acc: 50.0, f1: 29.642397875680548, r: 0.38872855965295233
06/02/2019 01:18:21 step: 4173, epoch: 126, batch: 14, loss: 1.5214288234710693, acc: 40.625, f1: 18.817371475953564, r: 0.24437422936857014
06/02/2019 01:18:22 step: 4178, epoch: 126, batch: 19, loss: 1.2851359844207764, acc: 51.5625, f1: 31.29329780692765, r: 0.3137210410618653
06/02/2019 01:18:22 step: 4183, epoch: 126, batch: 24, loss: 1.3720769882202148, acc: 46.875, f1: 19.07129742962056, r: 0.3261027528809897
06/02/2019 01:18:22 step: 4188, epoch: 126, batch: 29, loss: 1.2040785551071167, acc: 48.4375, f1: 28.599439775910362, r: 0.29003079375918117
06/02/2019 01:18:22 *** evaluating ***
06/02/2019 01:18:22 step: 127, epoch: 126, acc: 56.41025641025641, f1: 17.267616191904047, r: 0.27111880188103915
06/02/2019 01:18:22 *** epoch: 128 ***
06/02/2019 01:18:22 *** training ***
06/02/2019 01:18:23 step: 4196, epoch: 127, batch: 4, loss: 1.1933666467666626, acc: 56.25, f1: 30.75396825396825, r: 0.33282262901774634
06/02/2019 01:18:23 step: 4201, epoch: 127, batch: 9, loss: 1.2450758218765259, acc: 45.3125, f1: 18.814699792960667, r: 0.2797826471819575
06/02/2019 01:18:23 step: 4206, epoch: 127, batch: 14, loss: 1.2187501192092896, acc: 53.125, f1: 21.077294685990335, r: 0.28442774093957884
06/02/2019 01:18:23 step: 4211, epoch: 127, batch: 19, loss: 1.4372259378433228, acc: 35.9375, f1: 13.597711811997529, r: 0.22840278102595823
06/02/2019 01:18:23 step: 4216, epoch: 127, batch: 24, loss: 1.3558576107025146, acc: 45.3125, f1: 28.43323996265173, r: 0.36434196797787355
06/02/2019 01:18:24 step: 4221, epoch: 127, batch: 29, loss: 1.3005411624908447, acc: 45.3125, f1: 29.340145148968677, r: 0.3354977297094251
06/02/2019 01:18:24 *** evaluating ***
06/02/2019 01:18:24 step: 128, epoch: 127, acc: 53.84615384615385, f1: 15.514955671555224, r: 0.2725864497381534
06/02/2019 01:18:24 *** epoch: 129 ***
06/02/2019 01:18:24 *** training ***
06/02/2019 01:18:24 step: 4229, epoch: 128, batch: 4, loss: 1.55059814453125, acc: 34.375, f1: 17.0349519117992, r: 0.19437747333503086
06/02/2019 01:18:24 step: 4234, epoch: 128, batch: 9, loss: 1.2616205215454102, acc: 46.875, f1: 18.373764600179694, r: 0.2716066180302362
06/02/2019 01:18:25 step: 4239, epoch: 128, batch: 14, loss: 1.3494516611099243, acc: 48.4375, f1: 24.052478134110785, r: 0.23117320224394367
06/02/2019 01:18:25 step: 4244, epoch: 128, batch: 19, loss: 1.3303117752075195, acc: 48.4375, f1: 20.852596357692015, r: 0.2694251168870869
06/02/2019 01:18:25 step: 4249, epoch: 128, batch: 24, loss: 1.3056538105010986, acc: 50.0, f1: 25.449731427992294, r: 0.3209436814620225
06/02/2019 01:18:25 step: 4254, epoch: 128, batch: 29, loss: 1.258333683013916, acc: 43.75, f1: 32.076069294114404, r: 0.27488890277722644
06/02/2019 01:18:25 *** evaluating ***
06/02/2019 01:18:26 step: 129, epoch: 128, acc: 53.84615384615385, f1: 15.440065346130282, r: 0.2723685651452853
06/02/2019 01:18:26 *** epoch: 130 ***
06/02/2019 01:18:26 *** training ***
06/02/2019 01:18:26 step: 4262, epoch: 129, batch: 4, loss: 1.3397200107574463, acc: 40.625, f1: 24.304347826086957, r: 0.25864361643810146
06/02/2019 01:18:26 step: 4267, epoch: 129, batch: 9, loss: 1.2351864576339722, acc: 46.875, f1: 15.732758620689658, r: 0.31264981059634606
06/02/2019 01:18:26 step: 4272, epoch: 129, batch: 14, loss: 1.418302059173584, acc: 45.3125, f1: 21.193181818181817, r: 0.191405003422455
06/02/2019 01:18:27 step: 4277, epoch: 129, batch: 19, loss: 1.2223844528198242, acc: 51.5625, f1: 33.920841063698205, r: 0.3397284897159176
06/02/2019 01:18:27 step: 4282, epoch: 129, batch: 24, loss: 1.334563970565796, acc: 39.0625, f1: 16.483739837398375, r: 0.29946928062548817
06/02/2019 01:18:27 step: 4287, epoch: 129, batch: 29, loss: 1.263280987739563, acc: 56.25, f1: 25.486111111111114, r: 0.2668957793467731
06/02/2019 01:18:27 *** evaluating ***
06/02/2019 01:18:27 step: 130, epoch: 129, acc: 53.41880341880342, f1: 15.76029938344344, r: 0.2760231237488829
06/02/2019 01:18:27 *** epoch: 131 ***
06/02/2019 01:18:27 *** training ***
06/02/2019 01:18:28 step: 4295, epoch: 130, batch: 4, loss: 1.2595397233963013, acc: 45.3125, f1: 28.681159420289852, r: 0.3768725864582948
06/02/2019 01:18:28 step: 4300, epoch: 130, batch: 9, loss: 1.4411852359771729, acc: 37.5, f1: 17.48503912843333, r: 0.23468295465710654
06/02/2019 01:18:28 step: 4305, epoch: 130, batch: 14, loss: 1.1245396137237549, acc: 48.4375, f1: 22.960898007731192, r: 0.2861782896846431
06/02/2019 01:18:28 step: 4310, epoch: 130, batch: 19, loss: 1.3712959289550781, acc: 53.125, f1: 27.065708600052936, r: 0.3418916524318017
06/02/2019 01:18:29 step: 4315, epoch: 130, batch: 24, loss: 1.5184717178344727, acc: 34.375, f1: 14.485861839750472, r: 0.18127900156514812
06/02/2019 01:18:29 step: 4320, epoch: 130, batch: 29, loss: 1.29202401638031, acc: 46.875, f1: 31.657608695652172, r: 0.35411413618762144
06/02/2019 01:18:29 *** evaluating ***
06/02/2019 01:18:29 step: 131, epoch: 130, acc: 53.41880341880342, f1: 15.319357206949997, r: 0.27475201110092706
06/02/2019 01:18:29 *** epoch: 132 ***
06/02/2019 01:18:29 *** training ***
06/02/2019 01:18:29 step: 4328, epoch: 131, batch: 4, loss: 1.098354458808899, acc: 54.6875, f1: 24.528301886792455, r: 0.30171795060115325
06/02/2019 01:18:29 step: 4333, epoch: 131, batch: 9, loss: 1.1572444438934326, acc: 46.875, f1: 26.26305717011204, r: 0.37401301814995896
06/02/2019 01:18:30 step: 4338, epoch: 131, batch: 14, loss: 1.1360130310058594, acc: 67.1875, f1: 35.92098377812663, r: 0.22939046689737352
06/02/2019 01:18:30 step: 4343, epoch: 131, batch: 19, loss: 1.2192201614379883, acc: 51.5625, f1: 22.608154372860252, r: 0.3189499387034038
06/02/2019 01:18:30 step: 4348, epoch: 131, batch: 24, loss: 1.2412850856781006, acc: 51.5625, f1: 26.93405732198836, r: 0.2840390820081178
06/02/2019 01:18:30 step: 4353, epoch: 131, batch: 29, loss: 1.351349949836731, acc: 45.3125, f1: 18.69747899159664, r: 0.28179550022032473
06/02/2019 01:18:30 *** evaluating ***
06/02/2019 01:18:31 step: 132, epoch: 131, acc: 52.991452991452995, f1: 15.074364579315075, r: 0.27238936783646484
06/02/2019 01:18:31 *** epoch: 133 ***
06/02/2019 01:18:31 *** training ***
06/02/2019 01:18:31 step: 4361, epoch: 132, batch: 4, loss: 1.3734272718429565, acc: 42.1875, f1: 16.624830710316164, r: 0.2565861272780667
06/02/2019 01:18:31 step: 4366, epoch: 132, batch: 9, loss: 1.4680804014205933, acc: 35.9375, f1: 15.021008403361344, r: 0.24373660049728452
06/02/2019 01:18:31 step: 4371, epoch: 132, batch: 14, loss: 1.4839814901351929, acc: 43.75, f1: 22.081447963800905, r: 0.328368419147651
06/02/2019 01:18:31 step: 4376, epoch: 132, batch: 19, loss: 1.273943305015564, acc: 37.5, f1: 23.992035838725734, r: 0.23639723243183178
06/02/2019 01:18:32 step: 4381, epoch: 132, batch: 24, loss: 1.359249234199524, acc: 45.3125, f1: 23.48731884057971, r: 0.28255329504205795
06/02/2019 01:18:32 step: 4386, epoch: 132, batch: 29, loss: 1.2882949113845825, acc: 37.5, f1: 21.278511404561826, r: 0.2890705208093919
06/02/2019 01:18:32 *** evaluating ***
06/02/2019 01:18:32 step: 133, epoch: 132, acc: 55.12820512820513, f1: 16.25403551251009, r: 0.28225095941139666
06/02/2019 01:18:32 *** epoch: 134 ***
06/02/2019 01:18:32 *** training ***
06/02/2019 01:18:32 step: 4394, epoch: 133, batch: 4, loss: 1.4963092803955078, acc: 31.25, f1: 12.307692307692308, r: 0.18132540495430424
06/02/2019 01:18:33 step: 4399, epoch: 133, batch: 9, loss: 1.3490750789642334, acc: 45.3125, f1: 21.42087604272478, r: 0.2541368189176627
06/02/2019 01:18:33 step: 4404, epoch: 133, batch: 14, loss: 1.2660142183303833, acc: 48.4375, f1: 20.36969998642472, r: 0.26073369588486894
06/02/2019 01:18:33 step: 4409, epoch: 133, batch: 19, loss: 1.279887318611145, acc: 46.875, f1: 19.255744255744254, r: 0.2773553145869715
06/02/2019 01:18:33 step: 4414, epoch: 133, batch: 24, loss: 1.5336318016052246, acc: 32.8125, f1: 11.504629629629632, r: 0.26590803495753496
06/02/2019 01:18:33 step: 4419, epoch: 133, batch: 29, loss: 1.2499266862869263, acc: 48.4375, f1: 25.799618644867756, r: 0.35187253677877467
06/02/2019 01:18:34 *** evaluating ***
06/02/2019 01:18:34 step: 134, epoch: 133, acc: 54.27350427350427, f1: 15.688775510204083, r: 0.2754221218437699
06/02/2019 01:18:34 *** epoch: 135 ***
06/02/2019 01:18:34 *** training ***
06/02/2019 01:18:34 step: 4427, epoch: 134, batch: 4, loss: 1.3840570449829102, acc: 43.75, f1: 12.925170068027212, r: 0.23476655062229992
06/02/2019 01:18:34 step: 4432, epoch: 134, batch: 9, loss: 1.434654712677002, acc: 43.75, f1: 22.004229323308273, r: 0.2967426973247299
06/02/2019 01:18:34 step: 4437, epoch: 134, batch: 14, loss: 1.2952743768692017, acc: 42.1875, f1: 23.038277511961724, r: 0.3534100766502972
06/02/2019 01:18:35 step: 4442, epoch: 134, batch: 19, loss: 1.22056245803833, acc: 50.0, f1: 19.361772486772487, r: 0.2599009807469392
06/02/2019 01:18:35 step: 4447, epoch: 134, batch: 24, loss: 1.3876056671142578, acc: 37.5, f1: 14.668174962292607, r: 0.2289360175539006
06/02/2019 01:18:35 step: 4452, epoch: 134, batch: 29, loss: 1.5523536205291748, acc: 39.0625, f1: 20.0297619047619, r: 0.1926921208939206
06/02/2019 01:18:35 *** evaluating ***
06/02/2019 01:18:35 step: 135, epoch: 134, acc: 52.56410256410257, f1: 14.89093959731544, r: 0.2788912923397503
06/02/2019 01:18:35 *** epoch: 136 ***
06/02/2019 01:18:35 *** training ***
06/02/2019 01:18:35 step: 4460, epoch: 135, batch: 4, loss: 1.4186625480651855, acc: 42.1875, f1: 28.811893528563253, r: 0.3311778940672685
06/02/2019 01:18:36 step: 4465, epoch: 135, batch: 9, loss: 1.3936673402786255, acc: 39.0625, f1: 31.751046572475143, r: 0.36137209640466267
06/02/2019 01:18:36 step: 4470, epoch: 135, batch: 14, loss: 1.327471375465393, acc: 45.3125, f1: 23.065747575625995, r: 0.29058041930112793
06/02/2019 01:18:36 step: 4475, epoch: 135, batch: 19, loss: 1.2040276527404785, acc: 48.4375, f1: 14.583333333333334, r: 0.3658869634669407
06/02/2019 01:18:36 step: 4480, epoch: 135, batch: 24, loss: 1.341496467590332, acc: 40.625, f1: 21.281106134129647, r: 0.32616790607659
06/02/2019 01:18:37 step: 4485, epoch: 135, batch: 29, loss: 1.4119527339935303, acc: 40.625, f1: 15.887263593380613, r: 0.34481656127260873
06/02/2019 01:18:37 *** evaluating ***
06/02/2019 01:18:37 step: 136, epoch: 135, acc: 55.55555555555556, f1: 16.89601742758959, r: 0.2769671429503603
06/02/2019 01:18:37 *** epoch: 137 ***
06/02/2019 01:18:37 *** training ***
06/02/2019 01:18:37 step: 4493, epoch: 136, batch: 4, loss: 1.4006567001342773, acc: 35.9375, f1: 25.684819382298375, r: 0.2634997791241212
06/02/2019 01:18:37 step: 4498, epoch: 136, batch: 9, loss: 1.240004539489746, acc: 43.75, f1: 18.498599439775912, r: 0.2232374049520679
06/02/2019 01:18:37 step: 4503, epoch: 136, batch: 14, loss: 1.1617424488067627, acc: 50.0, f1: 21.095162147793726, r: 0.3147573233206558
06/02/2019 01:18:38 step: 4508, epoch: 136, batch: 19, loss: 1.3791371583938599, acc: 42.1875, f1: 16.503267973856207, r: 0.3042559026550822
06/02/2019 01:18:38 step: 4513, epoch: 136, batch: 24, loss: 1.4404690265655518, acc: 46.875, f1: 28.72549019607843, r: 0.3010269926791007
06/02/2019 01:18:38 step: 4518, epoch: 136, batch: 29, loss: 1.5709604024887085, acc: 37.5, f1: 21.458714896214897, r: 0.3244535400614785
06/02/2019 01:18:38 *** evaluating ***
06/02/2019 01:18:38 step: 137, epoch: 136, acc: 55.55555555555556, f1: 16.321071540143706, r: 0.27609289970734835
06/02/2019 01:18:38 *** epoch: 138 ***
06/02/2019 01:18:38 *** training ***
06/02/2019 01:18:39 step: 4526, epoch: 137, batch: 4, loss: 1.2993042469024658, acc: 40.625, f1: 16.125415282392026, r: 0.35811327120299175
06/02/2019 01:18:39 step: 4531, epoch: 137, batch: 9, loss: 1.4445641040802002, acc: 35.9375, f1: 16.32640999662276, r: 0.25681801453643593
06/02/2019 01:18:39 step: 4536, epoch: 137, batch: 14, loss: 0.9866865873336792, acc: 62.5, f1: 31.963587487781037, r: 0.34013803083510313
06/02/2019 01:18:39 step: 4541, epoch: 137, batch: 19, loss: 1.4995431900024414, acc: 37.5, f1: 13.632610939112485, r: 0.3271383304169373
06/02/2019 01:18:40 step: 4546, epoch: 137, batch: 24, loss: 1.430704116821289, acc: 43.75, f1: 19.411983348201012, r: 0.24595929830684427
06/02/2019 01:18:40 step: 4551, epoch: 137, batch: 29, loss: 1.4551143646240234, acc: 42.1875, f1: 20.71545284780579, r: 0.30181921045365057
06/02/2019 01:18:40 *** evaluating ***
06/02/2019 01:18:40 step: 138, epoch: 137, acc: 55.12820512820513, f1: 16.22666131298936, r: 0.27090349816852566
06/02/2019 01:18:40 *** epoch: 139 ***
06/02/2019 01:18:40 *** training ***
06/02/2019 01:18:40 step: 4559, epoch: 138, batch: 4, loss: 1.3345632553100586, acc: 48.4375, f1: 17.263986013986017, r: 0.28057423309993373
06/02/2019 01:18:41 step: 4564, epoch: 138, batch: 9, loss: 1.2763023376464844, acc: 51.5625, f1: 25.43266951161688, r: 0.2925050744431742
06/02/2019 01:18:41 step: 4569, epoch: 138, batch: 14, loss: 1.4269657135009766, acc: 34.375, f1: 11.777777777777779, r: 0.31955099479936844
06/02/2019 01:18:41 step: 4574, epoch: 138, batch: 19, loss: 1.2560489177703857, acc: 45.3125, f1: 28.774586757780042, r: 0.30653091792084664
06/02/2019 01:18:41 step: 4579, epoch: 138, batch: 24, loss: 1.2485777139663696, acc: 51.5625, f1: 23.91774891774892, r: 0.2825117509880317
06/02/2019 01:18:42 step: 4584, epoch: 138, batch: 29, loss: 1.4950368404388428, acc: 34.375, f1: 12.660256410256409, r: 0.1631115485917567
06/02/2019 01:18:42 *** evaluating ***
06/02/2019 01:18:42 step: 139, epoch: 138, acc: 55.98290598290598, f1: 16.642179654082202, r: 0.2692185780860542
06/02/2019 01:18:42 *** epoch: 140 ***
06/02/2019 01:18:42 *** training ***
06/02/2019 01:18:42 step: 4592, epoch: 139, batch: 4, loss: 1.2134262323379517, acc: 53.125, f1: 23.85170385170385, r: 0.29515897818178816
06/02/2019 01:18:42 step: 4597, epoch: 139, batch: 9, loss: 1.3965564966201782, acc: 40.625, f1: 24.25822730915648, r: 0.2954302246484667
06/02/2019 01:18:43 step: 4602, epoch: 139, batch: 14, loss: 1.2761605978012085, acc: 48.4375, f1: 27.601407226754127, r: 0.19594459407905787
06/02/2019 01:18:43 step: 4607, epoch: 139, batch: 19, loss: 1.207846999168396, acc: 48.4375, f1: 27.065850815850816, r: 0.36707751822336815
06/02/2019 01:18:43 step: 4612, epoch: 139, batch: 24, loss: 1.4269304275512695, acc: 39.0625, f1: 24.28307123034228, r: 0.3487130526021857
06/02/2019 01:18:43 step: 4617, epoch: 139, batch: 29, loss: 1.3791109323501587, acc: 46.875, f1: 25.83580368906456, r: 0.3588165387056345
06/02/2019 01:18:43 *** evaluating ***
06/02/2019 01:18:43 step: 140, epoch: 139, acc: 56.837606837606835, f1: 17.026266416510317, r: 0.27492867086178807
06/02/2019 01:18:43 *** epoch: 141 ***
06/02/2019 01:18:43 *** training ***
06/02/2019 01:18:44 step: 4625, epoch: 140, batch: 4, loss: 1.578376054763794, acc: 29.6875, f1: 11.900761648745519, r: 0.19710058139051703
06/02/2019 01:18:44 step: 4630, epoch: 140, batch: 9, loss: 1.208447813987732, acc: 51.5625, f1: 25.54031206815376, r: 0.30877398096993863
06/02/2019 01:18:44 step: 4635, epoch: 140, batch: 14, loss: 1.1238367557525635, acc: 53.125, f1: 38.26340326340326, r: 0.3799762661531334
06/02/2019 01:18:44 step: 4640, epoch: 140, batch: 19, loss: 1.4309778213500977, acc: 42.1875, f1: 27.366648945596317, r: 0.27885633864604953
06/02/2019 01:18:45 step: 4645, epoch: 140, batch: 24, loss: 1.2009302377700806, acc: 54.6875, f1: 51.182336182336186, r: 0.36701043878705525
06/02/2019 01:18:45 step: 4650, epoch: 140, batch: 29, loss: 1.1529682874679565, acc: 43.75, f1: 21.848739495798316, r: 0.3048305100950488
06/02/2019 01:18:45 *** evaluating ***
06/02/2019 01:18:45 step: 141, epoch: 140, acc: 56.41025641025641, f1: 16.805555555555557, r: 0.27396561345869785
06/02/2019 01:18:45 *** epoch: 142 ***
06/02/2019 01:18:45 *** training ***
06/02/2019 01:18:45 step: 4658, epoch: 141, batch: 4, loss: 1.3827112913131714, acc: 40.625, f1: 24.264989396568343, r: 0.34938082578599927
06/02/2019 01:18:45 step: 4663, epoch: 141, batch: 9, loss: 1.2364227771759033, acc: 45.3125, f1: 41.88159136962328, r: 0.4435498085285795
06/02/2019 01:18:46 step: 4668, epoch: 141, batch: 14, loss: 1.3094544410705566, acc: 40.625, f1: 18.929612088939155, r: 0.3139660331402466
06/02/2019 01:18:46 step: 4673, epoch: 141, batch: 19, loss: 1.4354188442230225, acc: 43.75, f1: 21.257988283850352, r: 0.3275121770442701
06/02/2019 01:18:46 step: 4678, epoch: 141, batch: 24, loss: 1.3321032524108887, acc: 45.3125, f1: 22.5811300129808, r: 0.2871780388773228
06/02/2019 01:18:46 step: 4683, epoch: 141, batch: 29, loss: 1.4368882179260254, acc: 42.1875, f1: 25.4945054945055, r: 0.30826135473054744
06/02/2019 01:18:46 *** evaluating ***
06/02/2019 01:18:46 step: 142, epoch: 141, acc: 55.98290598290598, f1: 17.1777818735025, r: 0.2594484825794274
06/02/2019 01:18:46 *** epoch: 143 ***
06/02/2019 01:18:46 *** training ***
06/02/2019 01:18:47 step: 4691, epoch: 142, batch: 4, loss: 1.0800085067749023, acc: 62.5, f1: 35.08403361344538, r: 0.38037422468605453
06/02/2019 01:18:47 step: 4696, epoch: 142, batch: 9, loss: 1.4205725193023682, acc: 35.9375, f1: 23.48107319037551, r: 0.30199733200700934
06/02/2019 01:18:47 step: 4701, epoch: 142, batch: 14, loss: 1.3281927108764648, acc: 39.0625, f1: 22.04700061842919, r: 0.34895381884969606
06/02/2019 01:18:47 step: 4706, epoch: 142, batch: 19, loss: 1.2845574617385864, acc: 45.3125, f1: 27.328198360089377, r: 0.2708857827785669
06/02/2019 01:18:48 step: 4711, epoch: 142, batch: 24, loss: 1.5702011585235596, acc: 35.9375, f1: 17.71085552865214, r: 0.3365156519113152
06/02/2019 01:18:48 step: 4716, epoch: 142, batch: 29, loss: 1.2345730066299438, acc: 50.0, f1: 27.383958234456575, r: 0.2928411941389584
06/02/2019 01:18:48 *** evaluating ***
06/02/2019 01:18:48 step: 143, epoch: 142, acc: 55.55555555555556, f1: 16.38290342875118, r: 0.27204232698261366
06/02/2019 01:18:48 *** epoch: 144 ***
06/02/2019 01:18:48 *** training ***
06/02/2019 01:18:48 step: 4724, epoch: 143, batch: 4, loss: 1.2886930704116821, acc: 40.625, f1: 26.98348500168521, r: 0.3065415758523482
06/02/2019 01:18:49 step: 4729, epoch: 143, batch: 9, loss: 1.408809781074524, acc: 35.9375, f1: 18.41114372170273, r: 0.24981154060691874
06/02/2019 01:18:49 step: 4734, epoch: 143, batch: 14, loss: 1.2422562837600708, acc: 50.0, f1: 19.975490196078432, r: 0.2866478784183371
06/02/2019 01:18:49 step: 4739, epoch: 143, batch: 19, loss: 1.150732159614563, acc: 53.125, f1: 31.761657199704757, r: 0.29785375691892196
06/02/2019 01:18:49 step: 4744, epoch: 143, batch: 24, loss: 1.479255199432373, acc: 35.9375, f1: 15.968992248062014, r: 0.3293663944922208
06/02/2019 01:18:50 step: 4749, epoch: 143, batch: 29, loss: 1.3348634243011475, acc: 53.125, f1: 27.084992604391857, r: 0.28314657484570827
06/02/2019 01:18:50 *** evaluating ***
06/02/2019 01:18:50 step: 144, epoch: 143, acc: 55.12820512820513, f1: 16.771778155706727, r: 0.2736760252531505
06/02/2019 01:18:50 *** epoch: 145 ***
06/02/2019 01:18:50 *** training ***
06/02/2019 01:18:50 step: 4757, epoch: 144, batch: 4, loss: 1.5812709331512451, acc: 42.1875, f1: 30.601386896221904, r: 0.340106113933237
06/02/2019 01:18:50 step: 4762, epoch: 144, batch: 9, loss: 1.409275770187378, acc: 43.75, f1: 26.519164513350564, r: 0.3009799033718507
06/02/2019 01:18:51 step: 4767, epoch: 144, batch: 14, loss: 1.2646961212158203, acc: 43.75, f1: 15.787139689578716, r: 0.24159868717900482
06/02/2019 01:18:51 step: 4772, epoch: 144, batch: 19, loss: 1.100767970085144, acc: 59.375, f1: 28.74441132637854, r: 0.3346596063991357
06/02/2019 01:18:51 step: 4777, epoch: 144, batch: 24, loss: 1.2986432313919067, acc: 43.75, f1: 22.167874396135268, r: 0.28789638109867927
06/02/2019 01:18:51 step: 4782, epoch: 144, batch: 29, loss: 1.4339319467544556, acc: 42.1875, f1: 23.58495670995671, r: 0.31096343998166676
06/02/2019 01:18:51 *** evaluating ***
06/02/2019 01:18:52 step: 145, epoch: 144, acc: 53.84615384615385, f1: 16.148545625146614, r: 0.2770435348118722
06/02/2019 01:18:52 *** epoch: 146 ***
06/02/2019 01:18:52 *** training ***
06/02/2019 01:18:52 step: 4790, epoch: 145, batch: 4, loss: 1.269516944885254, acc: 46.875, f1: 20.276397515527954, r: 0.262614885923119
06/02/2019 01:18:52 step: 4795, epoch: 145, batch: 9, loss: 1.366792917251587, acc: 34.375, f1: 17.731300654232985, r: 0.23625906674940753
06/02/2019 01:18:52 step: 4800, epoch: 145, batch: 14, loss: 1.3904234170913696, acc: 46.875, f1: 31.4625850340136, r: 0.3464696643020926
06/02/2019 01:18:52 step: 4805, epoch: 145, batch: 19, loss: 1.2668999433517456, acc: 45.3125, f1: 21.27248793330408, r: 0.3447419894553129
06/02/2019 01:18:53 step: 4810, epoch: 145, batch: 24, loss: 1.461673378944397, acc: 39.0625, f1: 19.841689318433502, r: 0.25919447027456594
06/02/2019 01:18:53 step: 4815, epoch: 145, batch: 29, loss: 1.1568026542663574, acc: 53.125, f1: 28.258547008547012, r: 0.26636116614019967
06/02/2019 01:18:53 *** evaluating ***
06/02/2019 01:18:53 step: 146, epoch: 145, acc: 51.28205128205128, f1: 14.032051282051281, r: 0.2778185844729623
06/02/2019 01:18:53 *** epoch: 147 ***
06/02/2019 01:18:53 *** training ***
06/02/2019 01:18:53 step: 4823, epoch: 146, batch: 4, loss: 1.304294228553772, acc: 42.1875, f1: 19.349631331714203, r: 0.30625959247676243
06/02/2019 01:18:54 step: 4828, epoch: 146, batch: 9, loss: 1.2412484884262085, acc: 48.4375, f1: 29.728715728715727, r: 0.33556027265047306
06/02/2019 01:18:54 step: 4833, epoch: 146, batch: 14, loss: 1.381613850593567, acc: 42.1875, f1: 17.410373240205175, r: 0.2754595782898024
06/02/2019 01:18:54 step: 4838, epoch: 146, batch: 19, loss: 1.440693974494934, acc: 45.3125, f1: 19.774436090225564, r: 0.22373370319151806
06/02/2019 01:18:54 step: 4843, epoch: 146, batch: 24, loss: 1.3145471811294556, acc: 42.1875, f1: 30.165631469979292, r: 0.302850444532533
06/02/2019 01:18:54 step: 4848, epoch: 146, batch: 29, loss: 1.2276127338409424, acc: 51.5625, f1: 40.417351356126865, r: 0.31975146316455
06/02/2019 01:18:55 *** evaluating ***
06/02/2019 01:18:55 step: 147, epoch: 146, acc: 53.84615384615385, f1: 15.59573970037453, r: 0.27775013766484635
06/02/2019 01:18:55 *** epoch: 148 ***
06/02/2019 01:18:55 *** training ***
06/02/2019 01:18:55 step: 4856, epoch: 147, batch: 4, loss: 1.2872874736785889, acc: 46.875, f1: 30.088731144631765, r: 0.3467378763377714
06/02/2019 01:18:55 step: 4861, epoch: 147, batch: 9, loss: 1.1592504978179932, acc: 50.0, f1: 38.0602834018983, r: 0.34728145757092654
06/02/2019 01:18:55 step: 4866, epoch: 147, batch: 14, loss: 1.5665143728256226, acc: 43.75, f1: 21.679624230644638, r: 0.24442472627203246
06/02/2019 01:18:56 step: 4871, epoch: 147, batch: 19, loss: 1.4986211061477661, acc: 31.25, f1: 20.20768833849329, r: 0.25927035156064304
06/02/2019 01:18:56 step: 4876, epoch: 147, batch: 24, loss: 1.2366917133331299, acc: 50.0, f1: 21.101130107215297, r: 0.2564765914228303
06/02/2019 01:18:56 step: 4881, epoch: 147, batch: 29, loss: 1.494201421737671, acc: 42.1875, f1: 20.285373428268443, r: 0.2913013274825919
06/02/2019 01:18:56 *** evaluating ***
06/02/2019 01:18:56 step: 148, epoch: 147, acc: 55.98290598290598, f1: 17.0198754789272, r: 0.2859409435747548
06/02/2019 01:18:56 *** epoch: 149 ***
06/02/2019 01:18:56 *** training ***
06/02/2019 01:18:57 step: 4889, epoch: 148, batch: 4, loss: 1.077778697013855, acc: 46.875, f1: 16.494676494676494, r: 0.28874343007263026
06/02/2019 01:18:57 step: 4894, epoch: 148, batch: 9, loss: 1.3468058109283447, acc: 45.3125, f1: 19.519255455712454, r: 0.25011287819013944
06/02/2019 01:18:57 step: 4899, epoch: 148, batch: 14, loss: 1.3276097774505615, acc: 42.1875, f1: 19.694811503371096, r: 0.2529821396880594
06/02/2019 01:18:57 step: 4904, epoch: 148, batch: 19, loss: 1.2411819696426392, acc: 40.625, f1: 21.98407243526194, r: 0.3257455597823651
06/02/2019 01:18:58 step: 4909, epoch: 148, batch: 24, loss: 1.1188185214996338, acc: 48.4375, f1: 26.107801972463623, r: 0.2992063484927599
06/02/2019 01:18:58 step: 4914, epoch: 148, batch: 29, loss: 1.3633850812911987, acc: 42.1875, f1: 23.271221532091094, r: 0.22277750360609902
06/02/2019 01:18:58 *** evaluating ***
06/02/2019 01:18:58 step: 149, epoch: 148, acc: 55.12820512820513, f1: 16.718618164319988, r: 0.28020170714733056
06/02/2019 01:18:58 *** epoch: 150 ***
06/02/2019 01:18:58 *** training ***
06/02/2019 01:18:58 step: 4922, epoch: 149, batch: 4, loss: 1.2642980813980103, acc: 50.0, f1: 37.33766233766233, r: 0.345624477319187
06/02/2019 01:18:58 step: 4927, epoch: 149, batch: 9, loss: 1.2923977375030518, acc: 48.4375, f1: 19.066584967320264, r: 0.3442517830199187
06/02/2019 01:18:59 step: 4932, epoch: 149, batch: 14, loss: 1.3355060815811157, acc: 59.375, f1: 26.604391371340526, r: 0.2741798544150288
06/02/2019 01:18:59 step: 4937, epoch: 149, batch: 19, loss: 1.4390192031860352, acc: 42.1875, f1: 17.95723073918563, r: 0.2546851787402953
06/02/2019 01:18:59 step: 4942, epoch: 149, batch: 24, loss: 1.3605831861495972, acc: 35.9375, f1: 16.88004666728071, r: 0.21875093154911512
06/02/2019 01:18:59 step: 4947, epoch: 149, batch: 29, loss: 1.2940804958343506, acc: 50.0, f1: 25.177981392113384, r: 0.2676228566699668
06/02/2019 01:18:59 *** evaluating ***
06/02/2019 01:18:59 step: 150, epoch: 149, acc: 54.27350427350427, f1: 16.190698677400803, r: 0.2905101056064642
06/02/2019 01:18:59 *** epoch: 151 ***
06/02/2019 01:18:59 *** training ***
06/02/2019 01:19:00 step: 4955, epoch: 150, batch: 4, loss: 1.3947645425796509, acc: 39.0625, f1: 25.218000479324516, r: 0.3008314098347967
06/02/2019 01:19:00 step: 4960, epoch: 150, batch: 9, loss: 1.3363449573516846, acc: 45.3125, f1: 19.685494223363285, r: 0.2753102793199867
06/02/2019 01:19:00 step: 4965, epoch: 150, batch: 14, loss: 1.2971112728118896, acc: 43.75, f1: 17.92908577157962, r: 0.20781965274576383
06/02/2019 01:19:00 step: 4970, epoch: 150, batch: 19, loss: 1.252584457397461, acc: 46.875, f1: 23.87011542205535, r: 0.2998340708716057
06/02/2019 01:19:01 step: 4975, epoch: 150, batch: 24, loss: 1.4659862518310547, acc: 35.9375, f1: 13.834901800327332, r: 0.16591313908844285
06/02/2019 01:19:01 step: 4980, epoch: 150, batch: 29, loss: 1.3389408588409424, acc: 45.3125, f1: 20.613851727982162, r: 0.30973507911206244
06/02/2019 01:19:01 *** evaluating ***
06/02/2019 01:19:01 step: 151, epoch: 150, acc: 53.41880341880342, f1: 15.6852984977985, r: 0.2870094039342944
06/02/2019 01:19:01 *** epoch: 152 ***
06/02/2019 01:19:01 *** training ***
06/02/2019 01:19:01 step: 4988, epoch: 151, batch: 4, loss: 1.2978347539901733, acc: 48.4375, f1: 29.381787802840435, r: 0.3299993599419402
06/02/2019 01:19:02 step: 4993, epoch: 151, batch: 9, loss: 1.4839977025985718, acc: 43.75, f1: 21.979166666666668, r: 0.35260601407258385
06/02/2019 01:19:02 step: 4998, epoch: 151, batch: 14, loss: 1.4046685695648193, acc: 42.1875, f1: 24.113475177304966, r: 0.33318952935670476
06/02/2019 01:19:02 step: 5003, epoch: 151, batch: 19, loss: 1.4413905143737793, acc: 34.375, f1: 21.370548219287716, r: 0.25646881063628574
06/02/2019 01:19:02 step: 5008, epoch: 151, batch: 24, loss: 1.3833520412445068, acc: 40.625, f1: 19.336734693877553, r: 0.28267736272389815
06/02/2019 01:19:03 step: 5013, epoch: 151, batch: 29, loss: 1.2290830612182617, acc: 48.4375, f1: 17.631347410759172, r: 0.26662329951447045
06/02/2019 01:19:03 *** evaluating ***
06/02/2019 01:19:03 step: 152, epoch: 151, acc: 55.98290598290598, f1: 16.894519129611226, r: 0.28119386703224186
06/02/2019 01:19:03 *** epoch: 153 ***
06/02/2019 01:19:03 *** training ***
06/02/2019 01:19:03 step: 5021, epoch: 152, batch: 4, loss: 1.1277053356170654, acc: 57.8125, f1: 33.214285714285715, r: 0.3623220459968745
06/02/2019 01:19:03 step: 5026, epoch: 152, batch: 9, loss: 1.259808897972107, acc: 43.75, f1: 21.809121111446693, r: 0.31983414537470695
06/02/2019 01:19:04 step: 5031, epoch: 152, batch: 14, loss: 1.2658231258392334, acc: 42.1875, f1: 23.30357142857143, r: 0.32015432063468463
06/02/2019 01:19:04 step: 5036, epoch: 152, batch: 19, loss: 1.4272840023040771, acc: 42.1875, f1: 20.83090379008746, r: 0.2894311281053589
06/02/2019 01:19:04 step: 5041, epoch: 152, batch: 24, loss: 1.342057466506958, acc: 50.0, f1: 17.635658914728683, r: 0.2614223879604481
06/02/2019 01:19:04 step: 5046, epoch: 152, batch: 29, loss: 1.3150956630706787, acc: 46.875, f1: 21.159232923938806, r: 0.29206052363736235
06/02/2019 01:19:04 *** evaluating ***
06/02/2019 01:19:04 step: 153, epoch: 152, acc: 53.41880341880342, f1: 15.73917204160493, r: 0.2831730075676003
06/02/2019 01:19:04 *** epoch: 154 ***
06/02/2019 01:19:04 *** training ***
06/02/2019 01:19:05 step: 5054, epoch: 153, batch: 4, loss: 1.2068264484405518, acc: 46.875, f1: 24.81477373558119, r: 0.3384187510281401
06/02/2019 01:19:05 step: 5059, epoch: 153, batch: 9, loss: 1.160276174545288, acc: 51.5625, f1: 35.64586754241927, r: 0.30615243946948545
06/02/2019 01:19:05 step: 5064, epoch: 153, batch: 14, loss: 1.2850627899169922, acc: 46.875, f1: 15.904550499445063, r: 0.21337675296927816
06/02/2019 01:19:05 step: 5069, epoch: 153, batch: 19, loss: 1.3812154531478882, acc: 45.3125, f1: 19.930426716141, r: 0.32416076407385147
06/02/2019 01:19:06 step: 5074, epoch: 153, batch: 24, loss: 1.273877739906311, acc: 46.875, f1: 25.940976003554226, r: 0.2872109690466741
06/02/2019 01:19:06 step: 5079, epoch: 153, batch: 29, loss: 1.3398816585540771, acc: 46.875, f1: 19.33193261976883, r: 0.26535134747389605
06/02/2019 01:19:06 *** evaluating ***
06/02/2019 01:19:06 step: 154, epoch: 153, acc: 55.55555555555556, f1: 16.732519157088124, r: 0.2767395952759015
06/02/2019 01:19:06 *** epoch: 155 ***
06/02/2019 01:19:06 *** training ***
06/02/2019 01:19:06 step: 5087, epoch: 154, batch: 4, loss: 1.2819103002548218, acc: 45.3125, f1: 41.642857142857146, r: 0.34722546248219793
06/02/2019 01:19:06 step: 5092, epoch: 154, batch: 9, loss: 1.2927380800247192, acc: 45.3125, f1: 22.365448504983387, r: 0.2504766286326435
06/02/2019 01:19:07 step: 5097, epoch: 154, batch: 14, loss: 1.2279207706451416, acc: 51.5625, f1: 32.119908600171755, r: 0.34415471019658833
06/02/2019 01:19:07 step: 5102, epoch: 154, batch: 19, loss: 1.3305484056472778, acc: 53.125, f1: 27.947085084033617, r: 0.34923322641707877
06/02/2019 01:19:07 step: 5107, epoch: 154, batch: 24, loss: 1.46461021900177, acc: 42.1875, f1: 15.82080200501253, r: 0.22828783140178027
06/02/2019 01:19:07 step: 5112, epoch: 154, batch: 29, loss: 1.3349858522415161, acc: 40.625, f1: 17.377491094637527, r: 0.30174057913462593
06/02/2019 01:19:07 *** evaluating ***
06/02/2019 01:19:08 step: 155, epoch: 154, acc: 56.837606837606835, f1: 17.37117371000466, r: 0.277182119071243
06/02/2019 01:19:08 *** epoch: 156 ***
06/02/2019 01:19:08 *** training ***
06/02/2019 01:19:08 step: 5120, epoch: 155, batch: 4, loss: 1.1474168300628662, acc: 54.6875, f1: 29.393184335044804, r: 0.403208697687257
06/02/2019 01:19:08 step: 5125, epoch: 155, batch: 9, loss: 1.3243114948272705, acc: 56.25, f1: 20.028287231329266, r: 0.24058543771703553
06/02/2019 01:19:08 step: 5130, epoch: 155, batch: 14, loss: 1.1603549718856812, acc: 48.4375, f1: 18.66406136922854, r: 0.30682142545824526
06/02/2019 01:19:08 step: 5135, epoch: 155, batch: 19, loss: 1.3904614448547363, acc: 46.875, f1: 17.688618096966554, r: 0.30530708298314546
06/02/2019 01:19:09 step: 5140, epoch: 155, batch: 24, loss: 1.3528592586517334, acc: 46.875, f1: 23.412698412698415, r: 0.28999443678535425
06/02/2019 01:19:09 step: 5145, epoch: 155, batch: 29, loss: 1.280365228652954, acc: 43.75, f1: 25.032938076416333, r: 0.2729417182040004
06/02/2019 01:19:09 *** evaluating ***
06/02/2019 01:19:09 step: 156, epoch: 155, acc: 55.55555555555556, f1: 16.92461949814891, r: 0.2849588265193626
06/02/2019 01:19:09 *** epoch: 157 ***
06/02/2019 01:19:09 *** training ***
06/02/2019 01:19:09 step: 5153, epoch: 156, batch: 4, loss: 1.211917757987976, acc: 50.0, f1: 23.398809523809522, r: 0.3306789876727778
06/02/2019 01:19:10 step: 5158, epoch: 156, batch: 9, loss: 1.0672881603240967, acc: 60.9375, f1: 41.75285657998424, r: 0.4405024985260501
06/02/2019 01:19:10 step: 5163, epoch: 156, batch: 14, loss: 1.4064899682998657, acc: 34.375, f1: 24.194682992401287, r: 0.24129623565093636
06/02/2019 01:19:10 step: 5168, epoch: 156, batch: 19, loss: 1.2195402383804321, acc: 54.6875, f1: 25.28818120654855, r: 0.35022876256394375
06/02/2019 01:19:10 step: 5173, epoch: 156, batch: 24, loss: 1.2539206743240356, acc: 54.6875, f1: 31.39717425431711, r: 0.35467025590012524
06/02/2019 01:19:11 step: 5178, epoch: 156, batch: 29, loss: 1.2796239852905273, acc: 51.5625, f1: 32.36111111111111, r: 0.37540134084392635
06/02/2019 01:19:11 *** evaluating ***
06/02/2019 01:19:11 step: 157, epoch: 156, acc: 56.837606837606835, f1: 17.338757889834937, r: 0.27603206560848387
06/02/2019 01:19:11 *** epoch: 158 ***
06/02/2019 01:19:11 *** training ***
06/02/2019 01:19:11 step: 5186, epoch: 157, batch: 4, loss: 1.2677640914916992, acc: 45.3125, f1: 29.63636363636364, r: 0.23713106026188474
06/02/2019 01:19:11 step: 5191, epoch: 157, batch: 9, loss: 1.4457374811172485, acc: 42.1875, f1: 16.58706631532719, r: 0.29002430644149213
06/02/2019 01:19:11 step: 5196, epoch: 157, batch: 14, loss: 1.2210458517074585, acc: 54.6875, f1: 25.657978130775028, r: 0.32932802207292183
06/02/2019 01:19:12 step: 5201, epoch: 157, batch: 19, loss: 1.3600016832351685, acc: 40.625, f1: 21.2157683586255, r: 0.31705000823528245
06/02/2019 01:19:12 step: 5206, epoch: 157, batch: 24, loss: 1.2704664468765259, acc: 42.1875, f1: 16.45021645021645, r: 0.24944580010030043
06/02/2019 01:19:12 step: 5211, epoch: 157, batch: 29, loss: 1.2567309141159058, acc: 51.5625, f1: 22.125292088572383, r: 0.31128721780151747
06/02/2019 01:19:12 *** evaluating ***
06/02/2019 01:19:12 step: 158, epoch: 157, acc: 56.837606837606835, f1: 17.55973715651135, r: 0.27364244851080727
06/02/2019 01:19:12 *** epoch: 159 ***
06/02/2019 01:19:12 *** training ***
06/02/2019 01:19:13 step: 5219, epoch: 158, batch: 4, loss: 1.2357094287872314, acc: 43.75, f1: 22.565682420105716, r: 0.3338772839666648
06/02/2019 01:19:13 step: 5224, epoch: 158, batch: 9, loss: 1.2103267908096313, acc: 53.125, f1: 24.70890811316343, r: 0.39716634791138405
06/02/2019 01:19:13 step: 5229, epoch: 158, batch: 14, loss: 1.3797950744628906, acc: 35.9375, f1: 16.570647397715067, r: 0.2516347273182527
06/02/2019 01:19:13 step: 5234, epoch: 158, batch: 19, loss: 1.3386799097061157, acc: 37.5, f1: 14.68217259151579, r: 0.28582996505613506
06/02/2019 01:19:13 step: 5239, epoch: 158, batch: 24, loss: 1.3213682174682617, acc: 53.125, f1: 25.257452574525747, r: 0.27915257061841775
06/02/2019 01:19:14 step: 5244, epoch: 158, batch: 29, loss: 1.3487595319747925, acc: 46.875, f1: 20.51575203252033, r: 0.2915808950589998
06/02/2019 01:19:14 *** evaluating ***
06/02/2019 01:19:14 step: 159, epoch: 158, acc: 54.700854700854705, f1: 16.545794332370953, r: 0.2790151834126461
06/02/2019 01:19:14 *** epoch: 160 ***
06/02/2019 01:19:14 *** training ***
06/02/2019 01:19:14 step: 5252, epoch: 159, batch: 4, loss: 1.2930904626846313, acc: 48.4375, f1: 27.58503401360544, r: 0.30852628127079057
06/02/2019 01:19:14 step: 5257, epoch: 159, batch: 9, loss: 1.553841233253479, acc: 35.9375, f1: 17.261904761904763, r: 0.24718978256896673
06/02/2019 01:19:15 step: 5262, epoch: 159, batch: 14, loss: 1.3012598752975464, acc: 48.4375, f1: 27.262971257932065, r: 0.29666749981268237
06/02/2019 01:19:15 step: 5267, epoch: 159, batch: 19, loss: 1.29204523563385, acc: 42.1875, f1: 24.048402255639097, r: 0.3623960271801119
06/02/2019 01:19:15 step: 5272, epoch: 159, batch: 24, loss: 1.2125698328018188, acc: 50.0, f1: 20.111317254174395, r: 0.2427135191291924
06/02/2019 01:19:15 step: 5277, epoch: 159, batch: 29, loss: 1.3291584253311157, acc: 53.125, f1: 29.709834190966262, r: 0.318985073908712
06/02/2019 01:19:16 *** evaluating ***
06/02/2019 01:19:16 step: 160, epoch: 159, acc: 56.41025641025641, f1: 17.284897049718946, r: 0.273237733549026
06/02/2019 01:19:16 *** epoch: 161 ***
06/02/2019 01:19:16 *** training ***
06/02/2019 01:19:16 step: 5285, epoch: 160, batch: 4, loss: 1.3371753692626953, acc: 43.75, f1: 23.75, r: 0.40473676870050795
06/02/2019 01:19:16 step: 5290, epoch: 160, batch: 9, loss: 1.1575686931610107, acc: 53.125, f1: 36.64251207729469, r: 0.3386741901944983
06/02/2019 01:19:16 step: 5295, epoch: 160, batch: 14, loss: 1.3892147541046143, acc: 43.75, f1: 25.831655679679997, r: 0.27173216516115123
06/02/2019 01:19:17 step: 5300, epoch: 160, batch: 19, loss: 1.3460943698883057, acc: 40.625, f1: 24.651483781918568, r: 0.3328554972952751
06/02/2019 01:19:17 step: 5305, epoch: 160, batch: 24, loss: 1.1055166721343994, acc: 53.125, f1: 28.43228200371058, r: 0.22576053785032663
06/02/2019 01:19:17 step: 5310, epoch: 160, batch: 29, loss: 1.2506455183029175, acc: 54.6875, f1: 30.45875797127362, r: 0.36036635215369617
06/02/2019 01:19:17 *** evaluating ***
06/02/2019 01:19:17 step: 161, epoch: 160, acc: 56.41025641025641, f1: 17.284897049718946, r: 0.2796044969451984
06/02/2019 01:19:17 *** epoch: 162 ***
06/02/2019 01:19:17 *** training ***
06/02/2019 01:19:18 step: 5318, epoch: 161, batch: 4, loss: 1.3416088819503784, acc: 42.1875, f1: 31.782106782106773, r: 0.3374025878765693
06/02/2019 01:19:18 step: 5323, epoch: 161, batch: 9, loss: 1.3405921459197998, acc: 45.3125, f1: 17.808067375886523, r: 0.25558328088746357
06/02/2019 01:19:18 step: 5328, epoch: 161, batch: 14, loss: 1.3559767007827759, acc: 39.0625, f1: 31.524644945697577, r: 0.37101942842243074
06/02/2019 01:19:18 step: 5333, epoch: 161, batch: 19, loss: 1.239733338356018, acc: 50.0, f1: 28.34140110047663, r: 0.28097151439953155
06/02/2019 01:19:18 step: 5338, epoch: 161, batch: 24, loss: 1.2967383861541748, acc: 45.3125, f1: 19.04761904761905, r: 0.2709540877895051
06/02/2019 01:19:19 step: 5343, epoch: 161, batch: 29, loss: 1.1970118284225464, acc: 56.25, f1: 29.766233766233764, r: 0.3335281074715877
06/02/2019 01:19:19 *** evaluating ***
06/02/2019 01:19:19 step: 162, epoch: 161, acc: 54.700854700854705, f1: 16.489504145107993, r: 0.2828910234417379
06/02/2019 01:19:19 *** epoch: 163 ***
06/02/2019 01:19:19 *** training ***
06/02/2019 01:19:19 step: 5351, epoch: 162, batch: 4, loss: 1.2526495456695557, acc: 45.3125, f1: 21.649141227355358, r: 0.36942435370125026
06/02/2019 01:19:19 step: 5356, epoch: 162, batch: 9, loss: 1.27069091796875, acc: 50.0, f1: 24.414068316507343, r: 0.29750865482582667
06/02/2019 01:19:20 step: 5361, epoch: 162, batch: 14, loss: 1.2150533199310303, acc: 45.3125, f1: 23.407544836116266, r: 0.2898303787027442
06/02/2019 01:19:20 step: 5366, epoch: 162, batch: 19, loss: 1.2491649389266968, acc: 46.875, f1: 25.952380952380956, r: 0.31060285858477416
06/02/2019 01:19:20 step: 5371, epoch: 162, batch: 24, loss: 1.2276891469955444, acc: 53.125, f1: 30.479024943310655, r: 0.3677283589258493
06/02/2019 01:19:20 step: 5376, epoch: 162, batch: 29, loss: 1.1087026596069336, acc: 57.8125, f1: 31.199835280023958, r: 0.32607910559135234
06/02/2019 01:19:20 *** evaluating ***
06/02/2019 01:19:21 step: 163, epoch: 162, acc: 54.700854700854705, f1: 16.53439153439153, r: 0.28983073522890684
06/02/2019 01:19:21 *** epoch: 164 ***
06/02/2019 01:19:21 *** training ***
06/02/2019 01:19:21 step: 5384, epoch: 163, batch: 4, loss: 1.0548176765441895, acc: 60.9375, f1: 32.86032743269585, r: 0.3738410709485592
06/02/2019 01:19:21 step: 5389, epoch: 163, batch: 9, loss: 1.6628155708312988, acc: 35.9375, f1: 25.14533085961657, r: 0.24695626717561447
06/02/2019 01:19:21 step: 5394, epoch: 163, batch: 14, loss: 1.1736280918121338, acc: 57.8125, f1: 35.180710762106116, r: 0.4377049927435441
06/02/2019 01:19:21 step: 5399, epoch: 163, batch: 19, loss: 1.3114540576934814, acc: 54.6875, f1: 26.288941452591818, r: 0.2786467984228298
06/02/2019 01:19:22 step: 5404, epoch: 163, batch: 24, loss: 1.3571786880493164, acc: 46.875, f1: 33.83522727272727, r: 0.3503096605811519
06/02/2019 01:19:22 step: 5409, epoch: 163, batch: 29, loss: 1.313643455505371, acc: 45.3125, f1: 21.047008547008545, r: 0.2376422618715712
06/02/2019 01:19:22 *** evaluating ***
06/02/2019 01:19:22 step: 164, epoch: 163, acc: 54.27350427350427, f1: 16.184626096327506, r: 0.28758256384893777
06/02/2019 01:19:22 *** epoch: 165 ***
06/02/2019 01:19:22 *** training ***
06/02/2019 01:19:22 step: 5417, epoch: 164, batch: 4, loss: 1.3065648078918457, acc: 35.9375, f1: 19.66684340756451, r: 0.3376379805946819
06/02/2019 01:19:23 step: 5422, epoch: 164, batch: 9, loss: 1.224984049797058, acc: 46.875, f1: 18.559051527587776, r: 0.36596416711027496
06/02/2019 01:19:23 step: 5427, epoch: 164, batch: 14, loss: 1.259581446647644, acc: 46.875, f1: 23.25391396868825, r: 0.24808533442391667
06/02/2019 01:19:23 step: 5432, epoch: 164, batch: 19, loss: 1.6136375665664673, acc: 39.0625, f1: 19.078282828282827, r: 0.2559043610921848
06/02/2019 01:19:23 step: 5437, epoch: 164, batch: 24, loss: 1.1577364206314087, acc: 56.25, f1: 30.069930069930066, r: 0.30721988191811195
06/02/2019 01:19:24 step: 5442, epoch: 164, batch: 29, loss: 1.342942476272583, acc: 46.875, f1: 20.42958459979737, r: 0.276831958967846
06/02/2019 01:19:24 *** evaluating ***
06/02/2019 01:19:24 step: 165, epoch: 164, acc: 57.26495726495726, f1: 17.536390414092885, r: 0.2779337557486049
06/02/2019 01:19:24 *** epoch: 166 ***
06/02/2019 01:19:24 *** training ***
06/02/2019 01:19:24 step: 5450, epoch: 165, batch: 4, loss: 1.046983003616333, acc: 57.8125, f1: 30.19453395393245, r: 0.3740317283299018
06/02/2019 01:19:24 step: 5455, epoch: 165, batch: 9, loss: 1.2100764513015747, acc: 46.875, f1: 22.119883040935672, r: 0.2172597757154953
06/02/2019 01:19:24 step: 5460, epoch: 165, batch: 14, loss: 1.3301094770431519, acc: 48.4375, f1: 26.855410599252966, r: 0.2775549335387417
06/02/2019 01:19:25 step: 5465, epoch: 165, batch: 19, loss: 1.1855268478393555, acc: 45.3125, f1: 16.318058076225046, r: 0.2548414174670737
06/02/2019 01:19:25 step: 5470, epoch: 165, batch: 24, loss: 1.100782036781311, acc: 54.6875, f1: 29.276437847866415, r: 0.2720335388542113
06/02/2019 01:19:25 step: 5475, epoch: 165, batch: 29, loss: 1.0599044561386108, acc: 51.5625, f1: 28.401360544217685, r: 0.3061236995954257
06/02/2019 01:19:25 *** evaluating ***
06/02/2019 01:19:25 step: 166, epoch: 165, acc: 54.700854700854705, f1: 16.333345873509288, r: 0.28468331243088724
06/02/2019 01:19:25 *** epoch: 167 ***
06/02/2019 01:19:25 *** training ***
06/02/2019 01:19:26 step: 5483, epoch: 166, batch: 4, loss: 1.320554494857788, acc: 46.875, f1: 21.297077642779314, r: 0.20830955727810035
06/02/2019 01:19:26 step: 5488, epoch: 166, batch: 9, loss: 1.3470869064331055, acc: 50.0, f1: 19.143819143819144, r: 0.24927221960657234
06/02/2019 01:19:26 step: 5493, epoch: 166, batch: 14, loss: 1.3629696369171143, acc: 45.3125, f1: 34.491341991342, r: 0.36294280272800217
06/02/2019 01:19:26 step: 5498, epoch: 166, batch: 19, loss: 1.1877208948135376, acc: 57.8125, f1: 35.08568168388451, r: 0.31306031844874566
06/02/2019 01:19:26 step: 5503, epoch: 166, batch: 24, loss: 1.3525265455245972, acc: 43.75, f1: 25.93253968253968, r: 0.32530937506318564
06/02/2019 01:19:27 step: 5508, epoch: 166, batch: 29, loss: 1.2811346054077148, acc: 42.1875, f1: 28.05206698063841, r: 0.3204219246868812
06/02/2019 01:19:27 *** evaluating ***
06/02/2019 01:19:27 step: 167, epoch: 166, acc: 55.55555555555556, f1: 16.732519157088124, r: 0.27804794245498216
06/02/2019 01:19:27 *** epoch: 168 ***
06/02/2019 01:19:27 *** training ***
06/02/2019 01:19:27 step: 5516, epoch: 167, batch: 4, loss: 1.064176082611084, acc: 57.8125, f1: 30.223791802739175, r: 0.34493337082505177
06/02/2019 01:19:27 step: 5521, epoch: 167, batch: 9, loss: 1.3685214519500732, acc: 46.875, f1: 25.00974658869396, r: 0.3881455000603142
06/02/2019 01:19:28 step: 5526, epoch: 167, batch: 14, loss: 1.3517801761627197, acc: 42.1875, f1: 19.30368106838695, r: 0.26428509370450537
06/02/2019 01:19:28 step: 5531, epoch: 167, batch: 19, loss: 1.3477200269699097, acc: 42.1875, f1: 30.030925284512616, r: 0.3217041886012692
06/02/2019 01:19:28 step: 5536, epoch: 167, batch: 24, loss: 1.2518105506896973, acc: 46.875, f1: 25.95878285847891, r: 0.3394454614883789
06/02/2019 01:19:28 step: 5541, epoch: 167, batch: 29, loss: 1.4545209407806396, acc: 46.875, f1: 23.42810838573551, r: 0.3310450492052942
06/02/2019 01:19:28 *** evaluating ***
06/02/2019 01:19:29 step: 168, epoch: 167, acc: 56.837606837606835, f1: 17.408461298713753, r: 0.2792019858686215
06/02/2019 01:19:29 *** epoch: 169 ***
06/02/2019 01:19:29 *** training ***
06/02/2019 01:19:29 step: 5549, epoch: 168, batch: 4, loss: 1.261765480041504, acc: 43.75, f1: 24.310134310134305, r: 0.3599754321797113
06/02/2019 01:19:29 step: 5554, epoch: 168, batch: 9, loss: 1.3104791641235352, acc: 40.625, f1: 15.59485350204155, r: 0.2782748495770403
06/02/2019 01:19:29 step: 5559, epoch: 168, batch: 14, loss: 1.4275588989257812, acc: 37.5, f1: 18.28081584179145, r: 0.29966055890549836
06/02/2019 01:19:30 step: 5564, epoch: 168, batch: 19, loss: 1.3087533712387085, acc: 42.1875, f1: 19.350566313481917, r: 0.24049897286507618
06/02/2019 01:19:30 step: 5569, epoch: 168, batch: 24, loss: 1.2836757898330688, acc: 45.3125, f1: 29.974839149350764, r: 0.3256293835756171
06/02/2019 01:19:30 step: 5574, epoch: 168, batch: 29, loss: 1.2950712442398071, acc: 43.75, f1: 24.06718804812117, r: 0.33616618494771344
06/02/2019 01:19:30 *** evaluating ***
06/02/2019 01:19:30 step: 169, epoch: 168, acc: 55.55555555555556, f1: 17.05550232685464, r: 0.27203178968231495
06/02/2019 01:19:30 *** epoch: 170 ***
06/02/2019 01:19:30 *** training ***
06/02/2019 01:19:31 step: 5582, epoch: 169, batch: 4, loss: 1.2994295358657837, acc: 45.3125, f1: 26.161870335380023, r: 0.29212304247723714
06/02/2019 01:19:31 step: 5587, epoch: 169, batch: 9, loss: 1.4739558696746826, acc: 48.4375, f1: 27.458307508252016, r: 0.221952156268916
06/02/2019 01:19:31 step: 5592, epoch: 169, batch: 14, loss: 1.2992689609527588, acc: 43.75, f1: 15.774436090225564, r: 0.22359940608142145
06/02/2019 01:19:31 step: 5597, epoch: 169, batch: 19, loss: 1.4437240362167358, acc: 35.9375, f1: 14.394230769230768, r: 0.2617158452416829
06/02/2019 01:19:31 step: 5602, epoch: 169, batch: 24, loss: 1.2680584192276, acc: 48.4375, f1: 22.60860484544695, r: 0.35832652762939765
06/02/2019 01:19:32 step: 5607, epoch: 169, batch: 29, loss: 1.4069098234176636, acc: 39.0625, f1: 19.03409090909091, r: 0.3392604356201381
06/02/2019 01:19:32 *** evaluating ***
06/02/2019 01:19:32 step: 170, epoch: 169, acc: 54.27350427350427, f1: 16.19423304583122, r: 0.28262707705826495
06/02/2019 01:19:32 *** epoch: 171 ***
06/02/2019 01:19:32 *** training ***
06/02/2019 01:19:32 step: 5615, epoch: 170, batch: 4, loss: 1.2713664770126343, acc: 50.0, f1: 18.585858585858585, r: 0.28824054838333113
06/02/2019 01:19:32 step: 5620, epoch: 170, batch: 9, loss: 1.265634536743164, acc: 43.75, f1: 32.21377912867274, r: 0.3161221786931438
06/02/2019 01:19:33 step: 5625, epoch: 170, batch: 14, loss: 1.135128140449524, acc: 57.8125, f1: 32.1360171975767, r: 0.3777811263054902
06/02/2019 01:19:33 step: 5630, epoch: 170, batch: 19, loss: 1.2278668880462646, acc: 46.875, f1: 34.405994281770674, r: 0.267342016720214
06/02/2019 01:19:33 step: 5635, epoch: 170, batch: 24, loss: 1.5093755722045898, acc: 31.25, f1: 13.712121212121211, r: 0.26821543725647784
06/02/2019 01:19:33 step: 5640, epoch: 170, batch: 29, loss: 1.2673139572143555, acc: 45.3125, f1: 18.17042606516291, r: 0.28559131126735027
06/02/2019 01:19:33 *** evaluating ***
06/02/2019 01:19:33 step: 171, epoch: 170, acc: 55.98290598290598, f1: 16.966430419174323, r: 0.282262077096072
06/02/2019 01:19:33 *** epoch: 172 ***
06/02/2019 01:19:33 *** training ***
06/02/2019 01:19:34 step: 5648, epoch: 171, batch: 4, loss: 1.3320990800857544, acc: 45.3125, f1: 21.078987150415724, r: 0.29004758851539997
06/02/2019 01:19:34 step: 5653, epoch: 171, batch: 9, loss: 1.335761547088623, acc: 48.4375, f1: 25.89311594202898, r: 0.24519718382828784
06/02/2019 01:19:34 step: 5658, epoch: 171, batch: 14, loss: 1.2993019819259644, acc: 46.875, f1: 29.084402991966023, r: 0.3007075115648828
06/02/2019 01:19:34 step: 5663, epoch: 171, batch: 19, loss: 1.203270673751831, acc: 54.6875, f1: 21.164835164835168, r: 0.2963853406087897
06/02/2019 01:19:35 step: 5668, epoch: 171, batch: 24, loss: 1.075561761856079, acc: 59.375, f1: 32.02947845804989, r: 0.3411728624722596
06/02/2019 01:19:35 step: 5673, epoch: 171, batch: 29, loss: 1.24812650680542, acc: 45.3125, f1: 27.915097822074564, r: 0.3319279974015525
06/02/2019 01:19:35 *** evaluating ***
06/02/2019 01:19:35 step: 172, epoch: 171, acc: 52.56410256410257, f1: 15.338443069535506, r: 0.28081655861071314
06/02/2019 01:19:35 *** epoch: 173 ***
06/02/2019 01:19:35 *** training ***
06/02/2019 01:19:35 step: 5681, epoch: 172, batch: 4, loss: 1.2799042463302612, acc: 54.6875, f1: 19.881644518272424, r: 0.2909685356172281
06/02/2019 01:19:35 step: 5686, epoch: 172, batch: 9, loss: 1.0500094890594482, acc: 60.9375, f1: 36.74443907156673, r: 0.263059516274625
06/02/2019 01:19:36 step: 5691, epoch: 172, batch: 14, loss: 1.2793101072311401, acc: 46.875, f1: 22.208472686733558, r: 0.322259575468653
06/02/2019 01:19:36 step: 5696, epoch: 172, batch: 19, loss: 1.1553676128387451, acc: 56.25, f1: 28.223172176660547, r: 0.37434564426385947
06/02/2019 01:19:36 step: 5701, epoch: 172, batch: 24, loss: 1.3519941568374634, acc: 37.5, f1: 17.377260981912144, r: 0.2986900821722392
06/02/2019 01:19:36 step: 5706, epoch: 172, batch: 29, loss: 1.1863547563552856, acc: 46.875, f1: 26.258326297025985, r: 0.2572997007603824
06/02/2019 01:19:36 *** evaluating ***
06/02/2019 01:19:37 step: 173, epoch: 172, acc: 54.700854700854705, f1: 16.4362578519205, r: 0.2834922227147475
06/02/2019 01:19:37 *** epoch: 174 ***
06/02/2019 01:19:37 *** training ***
06/02/2019 01:19:37 step: 5714, epoch: 173, batch: 4, loss: 1.3138028383255005, acc: 43.75, f1: 20.990886920806055, r: 0.3001603309539585
06/02/2019 01:19:37 step: 5719, epoch: 173, batch: 9, loss: 1.1163080930709839, acc: 51.5625, f1: 29.88233988233989, r: 0.3005741222724099
06/02/2019 01:19:37 step: 5724, epoch: 173, batch: 14, loss: 1.1934759616851807, acc: 53.125, f1: 32.7531012404962, r: 0.3242792796439134
06/02/2019 01:19:37 step: 5729, epoch: 173, batch: 19, loss: 1.2948689460754395, acc: 51.5625, f1: 27.692743764172338, r: 0.31812804476645457
06/02/2019 01:19:38 step: 5734, epoch: 173, batch: 24, loss: 1.0456249713897705, acc: 59.375, f1: 34.04214832786261, r: 0.3708600690788242
06/02/2019 01:19:38 step: 5739, epoch: 173, batch: 29, loss: 1.332929015159607, acc: 40.625, f1: 21.59333750004678, r: 0.27509244533100763
06/02/2019 01:19:38 *** evaluating ***
06/02/2019 01:19:38 step: 174, epoch: 173, acc: 57.692307692307686, f1: 17.8457742234722, r: 0.2777373692900815
06/02/2019 01:19:38 *** epoch: 175 ***
06/02/2019 01:19:38 *** training ***
06/02/2019 01:19:38 step: 5747, epoch: 174, batch: 4, loss: 1.3656005859375, acc: 46.875, f1: 20.385343822843822, r: 0.30215345066564353
06/02/2019 01:19:39 step: 5752, epoch: 174, batch: 9, loss: 1.216606855392456, acc: 50.0, f1: 38.424282756580894, r: 0.4409177951550194
06/02/2019 01:19:39 step: 5757, epoch: 174, batch: 14, loss: 1.1159899234771729, acc: 56.25, f1: 33.47128398397687, r: 0.393471507713676
06/02/2019 01:19:39 step: 5762, epoch: 174, batch: 19, loss: 1.2063127756118774, acc: 39.0625, f1: 25.407163393433418, r: 0.31206568486530223
06/02/2019 01:19:39 step: 5767, epoch: 174, batch: 24, loss: 1.429925560951233, acc: 39.0625, f1: 13.528622540250446, r: 0.2557214946422864
06/02/2019 01:19:40 step: 5772, epoch: 174, batch: 29, loss: 1.20050048828125, acc: 54.6875, f1: 36.101740205083665, r: 0.3990423563810874
06/02/2019 01:19:40 *** evaluating ***
06/02/2019 01:19:40 step: 175, epoch: 174, acc: 55.12820512820513, f1: 16.66083099906629, r: 0.2814837077962181
06/02/2019 01:19:40 *** epoch: 176 ***
06/02/2019 01:19:40 *** training ***
06/02/2019 01:19:40 step: 5780, epoch: 175, batch: 4, loss: 1.2117455005645752, acc: 54.6875, f1: 30.775226757369612, r: 0.3388485083395175
06/02/2019 01:19:40 step: 5785, epoch: 175, batch: 9, loss: 1.3369089365005493, acc: 42.1875, f1: 25.303164744158536, r: 0.2717510894067876
06/02/2019 01:19:40 step: 5790, epoch: 175, batch: 14, loss: 1.2216176986694336, acc: 48.4375, f1: 18.754882335709404, r: 0.353749372567816
06/02/2019 01:19:41 step: 5795, epoch: 175, batch: 19, loss: 1.4166078567504883, acc: 35.9375, f1: 14.175724637681158, r: 0.24722848933474037
06/02/2019 01:19:41 step: 5800, epoch: 175, batch: 24, loss: 1.2574219703674316, acc: 42.1875, f1: 28.745451669039713, r: 0.3294669831021278
06/02/2019 01:19:41 step: 5805, epoch: 175, batch: 29, loss: 1.384024739265442, acc: 42.1875, f1: 29.885352742495602, r: 0.30529969242820487
06/02/2019 01:19:41 *** evaluating ***
06/02/2019 01:19:41 step: 176, epoch: 175, acc: 55.98290598290598, f1: 16.79527962896537, r: 0.28100274331788433
06/02/2019 01:19:41 *** epoch: 177 ***
06/02/2019 01:19:41 *** training ***
06/02/2019 01:19:42 step: 5813, epoch: 176, batch: 4, loss: 1.2202258110046387, acc: 46.875, f1: 33.85990916025241, r: 0.3175776473140552
06/02/2019 01:19:42 step: 5818, epoch: 176, batch: 9, loss: 1.3169142007827759, acc: 43.75, f1: 18.91413956887103, r: 0.3548401217277513
06/02/2019 01:19:42 step: 5823, epoch: 176, batch: 14, loss: 1.3497130870819092, acc: 43.75, f1: 22.727298727298724, r: 0.30896179736351886
06/02/2019 01:19:42 step: 5828, epoch: 176, batch: 19, loss: 1.398645281791687, acc: 37.5, f1: 17.178891569135477, r: 0.2637067424651155
06/02/2019 01:19:43 step: 5833, epoch: 176, batch: 24, loss: 1.4944336414337158, acc: 39.0625, f1: 20.87054789182449, r: 0.24608923203973626
06/02/2019 01:19:43 step: 5838, epoch: 176, batch: 29, loss: 1.2026194334030151, acc: 50.0, f1: 23.28822833481218, r: 0.33151920301066445
06/02/2019 01:19:43 *** evaluating ***
06/02/2019 01:19:43 step: 177, epoch: 176, acc: 53.84615384615385, f1: 15.969333023919715, r: 0.2803943344265789
06/02/2019 01:19:43 *** epoch: 178 ***
06/02/2019 01:19:43 *** training ***
06/02/2019 01:19:43 step: 5846, epoch: 177, batch: 4, loss: 1.3248941898345947, acc: 43.75, f1: 25.78231292517007, r: 0.32232805483454424
06/02/2019 01:19:44 step: 5851, epoch: 177, batch: 9, loss: 1.1597636938095093, acc: 56.25, f1: 25.884353741496597, r: 0.337093860005063
06/02/2019 01:19:44 step: 5856, epoch: 177, batch: 14, loss: 1.1915535926818848, acc: 48.4375, f1: 21.501145323994535, r: 0.2907086568269171
06/02/2019 01:19:44 step: 5861, epoch: 177, batch: 19, loss: 1.270418643951416, acc: 51.5625, f1: 25.238355025589073, r: 0.30619874817539705
06/02/2019 01:19:44 step: 5866, epoch: 177, batch: 24, loss: 1.4564037322998047, acc: 34.375, f1: 17.534373838721663, r: 0.2583120068955172
06/02/2019 01:19:45 step: 5871, epoch: 177, batch: 29, loss: 1.1949018239974976, acc: 48.4375, f1: 33.65079365079365, r: 0.5025930018296794
06/02/2019 01:19:45 *** evaluating ***
06/02/2019 01:19:45 step: 178, epoch: 177, acc: 56.41025641025641, f1: 17.210054234459744, r: 0.27377908758073183
06/02/2019 01:19:45 *** epoch: 179 ***
06/02/2019 01:19:45 *** training ***
06/02/2019 01:19:45 step: 5879, epoch: 178, batch: 4, loss: 1.1898994445800781, acc: 51.5625, f1: 21.916231608922637, r: 0.3234778746164579
06/02/2019 01:19:45 step: 5884, epoch: 178, batch: 9, loss: 1.308713436126709, acc: 50.0, f1: 32.82111899133176, r: 0.36213316281892677
06/02/2019 01:19:46 step: 5889, epoch: 178, batch: 14, loss: 1.2572431564331055, acc: 50.0, f1: 26.430616879910847, r: 0.31667883320617557
06/02/2019 01:19:46 step: 5894, epoch: 178, batch: 19, loss: 1.3045403957366943, acc: 45.3125, f1: 19.86607142857143, r: 0.2758942375637153
06/02/2019 01:19:46 step: 5899, epoch: 178, batch: 24, loss: 1.2377365827560425, acc: 45.3125, f1: 19.329795597484278, r: 0.30473163990242336
06/02/2019 01:19:46 step: 5904, epoch: 178, batch: 29, loss: 1.418908715248108, acc: 43.75, f1: 28.232636928289107, r: 0.3033210377474754
06/02/2019 01:19:46 *** evaluating ***
06/02/2019 01:19:46 step: 179, epoch: 178, acc: 55.12820512820513, f1: 16.58660130718954, r: 0.27808344270148067
06/02/2019 01:19:46 *** epoch: 180 ***
06/02/2019 01:19:46 *** training ***
06/02/2019 01:19:47 step: 5912, epoch: 179, batch: 4, loss: 1.2620837688446045, acc: 48.4375, f1: 22.009499451954696, r: 0.28670042721270317
06/02/2019 01:19:47 step: 5917, epoch: 179, batch: 9, loss: 1.5363155603408813, acc: 42.1875, f1: 28.36106120961233, r: 0.30328047081855775
06/02/2019 01:19:47 step: 5922, epoch: 179, batch: 14, loss: 1.240503191947937, acc: 50.0, f1: 21.832982913210074, r: 0.3413098330033393
06/02/2019 01:19:47 step: 5927, epoch: 179, batch: 19, loss: 1.1917780637741089, acc: 48.4375, f1: 19.7718253968254, r: 0.3293693476809139
06/02/2019 01:19:48 step: 5932, epoch: 179, batch: 24, loss: 1.1628072261810303, acc: 45.3125, f1: 29.86111111111111, r: 0.3739299463283406
06/02/2019 01:19:48 step: 5937, epoch: 179, batch: 29, loss: 1.2827988862991333, acc: 46.875, f1: 32.36637512147716, r: 0.3155650567687998
06/02/2019 01:19:48 *** evaluating ***
06/02/2019 01:19:48 step: 180, epoch: 179, acc: 54.700854700854705, f1: 16.446903025731288, r: 0.2807291106393172
06/02/2019 01:19:48 *** epoch: 181 ***
06/02/2019 01:19:48 *** training ***
06/02/2019 01:19:48 step: 5945, epoch: 180, batch: 4, loss: 1.4073134660720825, acc: 37.5, f1: 18.8953488372093, r: 0.2602169393430179
06/02/2019 01:19:48 step: 5950, epoch: 180, batch: 9, loss: 1.2502880096435547, acc: 45.3125, f1: 21.05482456140351, r: 0.2785637951469836
06/02/2019 01:19:49 step: 5955, epoch: 180, batch: 14, loss: 1.1537089347839355, acc: 53.125, f1: 18.322401749053483, r: 0.27317584678938645
06/02/2019 01:19:49 step: 5960, epoch: 180, batch: 19, loss: 1.235871434211731, acc: 43.75, f1: 24.570215331084896, r: 0.3559497389336139
06/02/2019 01:19:49 step: 5965, epoch: 180, batch: 24, loss: 1.196436882019043, acc: 50.0, f1: 29.74400540357987, r: 0.31703280555691615
06/02/2019 01:19:49 step: 5970, epoch: 180, batch: 29, loss: 1.2027218341827393, acc: 48.4375, f1: 25.735250074872713, r: 0.33993861257958213
06/02/2019 01:19:49 *** evaluating ***
06/02/2019 01:19:50 step: 181, epoch: 180, acc: 55.98290598290598, f1: 17.109666459573265, r: 0.27861291318337067
06/02/2019 01:19:50 *** epoch: 182 ***
06/02/2019 01:19:50 *** training ***
06/02/2019 01:19:50 step: 5978, epoch: 181, batch: 4, loss: 1.1528950929641724, acc: 56.25, f1: 32.02171049298273, r: 0.3695773031163699
06/02/2019 01:19:50 step: 5983, epoch: 181, batch: 9, loss: 1.4489256143569946, acc: 39.0625, f1: 22.857142857142858, r: 0.30211253455595716
06/02/2019 01:19:50 step: 5988, epoch: 181, batch: 14, loss: 1.1179276704788208, acc: 53.125, f1: 32.783068783068785, r: 0.34493854246837985
06/02/2019 01:19:50 step: 5993, epoch: 181, batch: 19, loss: 1.277579426765442, acc: 48.4375, f1: 28.766233766233768, r: 0.37883092359427245
06/02/2019 01:19:51 step: 5998, epoch: 181, batch: 24, loss: 1.3897634744644165, acc: 45.3125, f1: 24.397759103641455, r: 0.3012628901878128
06/02/2019 01:19:51 step: 6003, epoch: 181, batch: 29, loss: 1.4007551670074463, acc: 42.1875, f1: 19.19538794538795, r: 0.22487590352354084
06/02/2019 01:19:51 *** evaluating ***
06/02/2019 01:19:51 step: 182, epoch: 181, acc: 55.98290598290598, f1: 17.071479563629453, r: 0.28330829517871964
06/02/2019 01:19:51 *** epoch: 183 ***
06/02/2019 01:19:51 *** training ***
06/02/2019 01:19:51 step: 6011, epoch: 182, batch: 4, loss: 1.4501196146011353, acc: 45.3125, f1: 29.96323529411764, r: 0.31249980305446484
06/02/2019 01:19:52 step: 6016, epoch: 182, batch: 9, loss: 1.1320433616638184, acc: 56.25, f1: 26.67082731134761, r: 0.37672051473817025
06/02/2019 01:19:52 step: 6021, epoch: 182, batch: 14, loss: 1.0276451110839844, acc: 62.5, f1: 26.22710622710623, r: 0.30077702638869247
06/02/2019 01:19:52 step: 6026, epoch: 182, batch: 19, loss: 1.3275978565216064, acc: 43.75, f1: 19.18588712080577, r: 0.2643279487178382
06/02/2019 01:19:52 step: 6031, epoch: 182, batch: 24, loss: 1.3933453559875488, acc: 43.75, f1: 21.959586466165415, r: 0.32042908088866606
06/02/2019 01:19:53 step: 6036, epoch: 182, batch: 29, loss: 1.2226210832595825, acc: 43.75, f1: 24.647108843537413, r: 0.35553065378058385
06/02/2019 01:19:53 *** evaluating ***
06/02/2019 01:19:53 step: 183, epoch: 182, acc: 54.27350427350427, f1: 16.2904771345493, r: 0.2809570422566193
06/02/2019 01:19:53 *** epoch: 184 ***
06/02/2019 01:19:53 *** training ***
06/02/2019 01:19:53 step: 6044, epoch: 183, batch: 4, loss: 1.1786402463912964, acc: 59.375, f1: 30.28803494642004, r: 0.3433617102150676
06/02/2019 01:19:53 step: 6049, epoch: 183, batch: 9, loss: 1.2613004446029663, acc: 51.5625, f1: 27.055421038471888, r: 0.2928021330482217
06/02/2019 01:19:53 step: 6054, epoch: 183, batch: 14, loss: 1.390703558921814, acc: 40.625, f1: 25.062024809923972, r: 0.354326292521761
06/02/2019 01:19:54 step: 6059, epoch: 183, batch: 19, loss: 1.2645152807235718, acc: 40.625, f1: 18.831168831168835, r: 0.2932276718083786
06/02/2019 01:19:54 step: 6064, epoch: 183, batch: 24, loss: 1.1816049814224243, acc: 60.9375, f1: 34.20484472858911, r: 0.3196073425558805
06/02/2019 01:19:54 step: 6069, epoch: 183, batch: 29, loss: 1.2893309593200684, acc: 48.4375, f1: 20.467447688989914, r: 0.2527734136551974
06/02/2019 01:19:54 *** evaluating ***
06/02/2019 01:19:54 step: 184, epoch: 183, acc: 55.98290598290598, f1: 16.958300363981643, r: 0.2752324684578439
06/02/2019 01:19:54 *** epoch: 185 ***
06/02/2019 01:19:54 *** training ***
06/02/2019 01:19:55 step: 6077, epoch: 184, batch: 4, loss: 1.3711371421813965, acc: 42.1875, f1: 24.22266139657444, r: 0.37381981165641276
06/02/2019 01:19:55 step: 6082, epoch: 184, batch: 9, loss: 1.2564502954483032, acc: 54.6875, f1: 23.84348807221148, r: 0.36986487627806247
06/02/2019 01:19:55 step: 6087, epoch: 184, batch: 14, loss: 1.342150330543518, acc: 43.75, f1: 23.335254721124286, r: 0.3352756311078723
06/02/2019 01:19:55 step: 6092, epoch: 184, batch: 19, loss: 1.2698253393173218, acc: 45.3125, f1: 20.272435897435898, r: 0.3505635208548627
06/02/2019 01:19:55 step: 6097, epoch: 184, batch: 24, loss: 1.3827706575393677, acc: 51.5625, f1: 34.58041958041958, r: 0.3299609131222184
06/02/2019 01:19:56 step: 6102, epoch: 184, batch: 29, loss: 1.3305994272232056, acc: 45.3125, f1: 17.09846866096866, r: 0.21867742436305682
06/02/2019 01:19:56 *** evaluating ***
06/02/2019 01:19:56 step: 185, epoch: 184, acc: 56.41025641025641, f1: 17.234949285469582, r: 0.2702723544259191
06/02/2019 01:19:56 *** epoch: 186 ***
06/02/2019 01:19:56 *** training ***
06/02/2019 01:19:56 step: 6110, epoch: 185, batch: 4, loss: 1.2930941581726074, acc: 46.875, f1: 20.658879904162923, r: 0.22657136607002265
06/02/2019 01:19:56 step: 6115, epoch: 185, batch: 9, loss: 1.3072643280029297, acc: 45.3125, f1: 36.77232296427343, r: 0.35556909875438564
06/02/2019 01:19:57 step: 6120, epoch: 185, batch: 14, loss: 1.1112995147705078, acc: 56.25, f1: 33.823473268431634, r: 0.4341068024387823
06/02/2019 01:19:57 step: 6125, epoch: 185, batch: 19, loss: 1.1183385848999023, acc: 48.4375, f1: 20.604195166438668, r: 0.2912170718543027
06/02/2019 01:19:57 step: 6130, epoch: 185, batch: 24, loss: 1.2916960716247559, acc: 43.75, f1: 30.15873015873015, r: 0.3197000927591274
06/02/2019 01:19:57 step: 6135, epoch: 185, batch: 29, loss: 1.1815372705459595, acc: 50.0, f1: 24.025974025974026, r: 0.32068618922554554
06/02/2019 01:19:58 *** evaluating ***
06/02/2019 01:19:58 step: 186, epoch: 185, acc: 55.55555555555556, f1: 16.73059188464118, r: 0.2752254760777985
06/02/2019 01:19:58 *** epoch: 187 ***
06/02/2019 01:19:58 *** training ***
06/02/2019 01:19:58 step: 6143, epoch: 186, batch: 4, loss: 1.1460648775100708, acc: 50.0, f1: 28.344988344988348, r: 0.3641824831184605
06/02/2019 01:19:58 step: 6148, epoch: 186, batch: 9, loss: 1.2050747871398926, acc: 46.875, f1: 26.401515151515152, r: 0.3121459311201248
06/02/2019 01:19:58 step: 6153, epoch: 186, batch: 14, loss: 1.4037171602249146, acc: 35.9375, f1: 15.579975579975578, r: 0.2417062327321714
06/02/2019 01:19:59 step: 6158, epoch: 186, batch: 19, loss: 1.200415849685669, acc: 48.4375, f1: 30.366357069143447, r: 0.3279744167762693
06/02/2019 01:19:59 step: 6163, epoch: 186, batch: 24, loss: 1.1030619144439697, acc: 57.8125, f1: 27.475315899141673, r: 0.4156967919763093
06/02/2019 01:19:59 step: 6168, epoch: 186, batch: 29, loss: 1.2771384716033936, acc: 53.125, f1: 20.486536107711135, r: 0.33283789007093256
06/02/2019 01:19:59 *** evaluating ***
06/02/2019 01:20:00 step: 187, epoch: 186, acc: 55.12820512820513, f1: 16.487460815047026, r: 0.2820962036786126
06/02/2019 01:20:00 *** epoch: 188 ***
06/02/2019 01:20:00 *** training ***
06/02/2019 01:20:00 step: 6176, epoch: 187, batch: 4, loss: 1.372440218925476, acc: 42.1875, f1: 24.462759462759465, r: 0.2767153954818913
06/02/2019 01:20:00 step: 6181, epoch: 187, batch: 9, loss: 1.242490530014038, acc: 43.75, f1: 21.28787878787879, r: 0.3256661626354931
06/02/2019 01:20:00 step: 6186, epoch: 187, batch: 14, loss: 1.1136528253555298, acc: 51.5625, f1: 24.017680884284957, r: 0.3596851282659238
06/02/2019 01:20:00 step: 6191, epoch: 187, batch: 19, loss: 1.3702095746994019, acc: 50.0, f1: 19.02597402597403, r: 0.2691522701392113
06/02/2019 01:20:01 step: 6196, epoch: 187, batch: 24, loss: 1.268730640411377, acc: 48.4375, f1: 24.48581560283688, r: 0.3721135172873574
06/02/2019 01:20:01 step: 6201, epoch: 187, batch: 29, loss: 1.144940733909607, acc: 51.5625, f1: 26.53729410296547, r: 0.2998685424702374
06/02/2019 01:20:01 *** evaluating ***
06/02/2019 01:20:01 step: 188, epoch: 187, acc: 55.55555555555556, f1: 16.947446307221995, r: 0.29194628559576885
06/02/2019 01:20:01 *** epoch: 189 ***
06/02/2019 01:20:01 *** training ***
06/02/2019 01:20:01 step: 6209, epoch: 188, batch: 4, loss: 1.3345667123794556, acc: 42.1875, f1: 18.23286052009456, r: 0.2899327392586756
06/02/2019 01:20:02 step: 6214, epoch: 188, batch: 9, loss: 1.4046558141708374, acc: 37.5, f1: 19.099355928624217, r: 0.2568080993855735
06/02/2019 01:20:02 step: 6219, epoch: 188, batch: 14, loss: 1.3925063610076904, acc: 42.1875, f1: 26.335470085470085, r: 0.32976333249953477
06/02/2019 01:20:02 step: 6224, epoch: 188, batch: 19, loss: 1.1537781953811646, acc: 56.25, f1: 21.37291897891232, r: 0.2075534131104042
06/02/2019 01:20:02 step: 6229, epoch: 188, batch: 24, loss: 1.5007541179656982, acc: 39.0625, f1: 12.875773651635722, r: 0.20760122996832747
06/02/2019 01:20:02 step: 6234, epoch: 188, batch: 29, loss: 1.4032435417175293, acc: 42.1875, f1: 17.977272727272727, r: 0.3301967996446482
06/02/2019 01:20:03 *** evaluating ***
06/02/2019 01:20:03 step: 189, epoch: 188, acc: 55.55555555555556, f1: 16.77406281661601, r: 0.2848091535559464
06/02/2019 01:20:03 *** epoch: 190 ***
06/02/2019 01:20:03 *** training ***
06/02/2019 01:20:03 step: 6242, epoch: 189, batch: 4, loss: 1.3769341707229614, acc: 45.3125, f1: 22.91801948051948, r: 0.33929336237787583
06/02/2019 01:20:03 step: 6247, epoch: 189, batch: 9, loss: 1.1160268783569336, acc: 54.6875, f1: 20.74486213830476, r: 0.3228069539291861
06/02/2019 01:20:03 step: 6252, epoch: 189, batch: 14, loss: 1.223404049873352, acc: 45.3125, f1: 19.392230576441104, r: 0.2549973868596371
06/02/2019 01:20:04 step: 6257, epoch: 189, batch: 19, loss: 1.3076151609420776, acc: 48.4375, f1: 32.90335334964291, r: 0.2966628499736847
06/02/2019 01:20:04 step: 6262, epoch: 189, batch: 24, loss: 1.4240849018096924, acc: 37.5, f1: 13.642120291056465, r: 0.23750840015154961
06/02/2019 01:20:04 step: 6267, epoch: 189, batch: 29, loss: 1.2395777702331543, acc: 50.0, f1: 19.809401924263543, r: 0.2967126138332562
06/02/2019 01:20:04 *** evaluating ***
06/02/2019 01:20:04 step: 190, epoch: 189, acc: 55.98290598290598, f1: 16.956729784401087, r: 0.2784345146593963
06/02/2019 01:20:04 *** epoch: 191 ***
06/02/2019 01:20:04 *** training ***
06/02/2019 01:20:04 step: 6275, epoch: 190, batch: 4, loss: 1.4687299728393555, acc: 48.4375, f1: 22.187503625472278, r: 0.26751741623853326
06/02/2019 01:20:05 step: 6280, epoch: 190, batch: 9, loss: 1.187225103378296, acc: 53.125, f1: 19.25825593395253, r: 0.28729505318525556
06/02/2019 01:20:05 step: 6285, epoch: 190, batch: 14, loss: 1.1699042320251465, acc: 56.25, f1: 29.858030168589174, r: 0.2671985114423193
06/02/2019 01:20:05 step: 6290, epoch: 190, batch: 19, loss: 1.456713318824768, acc: 39.0625, f1: 19.70460704607046, r: 0.27639439446584313
06/02/2019 01:20:05 step: 6295, epoch: 190, batch: 24, loss: 1.3193424940109253, acc: 51.5625, f1: 25.002616431187864, r: 0.3419297094141034
06/02/2019 01:20:05 step: 6300, epoch: 190, batch: 29, loss: 1.165517807006836, acc: 51.5625, f1: 32.89975223937488, r: 0.45683180240096405
06/02/2019 01:20:06 *** evaluating ***
06/02/2019 01:20:06 step: 191, epoch: 190, acc: 54.700854700854705, f1: 16.372622828786216, r: 0.281820897121417
06/02/2019 01:20:06 *** epoch: 192 ***
06/02/2019 01:20:06 *** training ***
06/02/2019 01:20:06 step: 6308, epoch: 191, batch: 4, loss: 1.361742377281189, acc: 42.1875, f1: 16.05017006802721, r: 0.28618480785723444
06/02/2019 01:20:06 step: 6313, epoch: 191, batch: 9, loss: 1.2220511436462402, acc: 53.125, f1: 30.2435259818213, r: 0.31203284379150226
06/02/2019 01:20:06 step: 6318, epoch: 191, batch: 14, loss: 1.3706682920455933, acc: 50.0, f1: 22.78450363196126, r: 0.2875516798086858
06/02/2019 01:20:07 step: 6323, epoch: 191, batch: 19, loss: 1.5366543531417847, acc: 32.8125, f1: 16.767434246425843, r: 0.2514588422215058
06/02/2019 01:20:07 step: 6328, epoch: 191, batch: 24, loss: 1.1887527704238892, acc: 46.875, f1: 30.388382303275918, r: 0.3715583367624297
06/02/2019 01:20:07 step: 6333, epoch: 191, batch: 29, loss: 1.154720664024353, acc: 40.625, f1: 28.680114212029107, r: 0.32104511834137534
06/02/2019 01:20:07 *** evaluating ***
06/02/2019 01:20:07 step: 192, epoch: 191, acc: 54.700854700854705, f1: 16.320959418250062, r: 0.2790080482810542
06/02/2019 01:20:07 *** epoch: 193 ***
06/02/2019 01:20:07 *** training ***
06/02/2019 01:20:08 step: 6341, epoch: 192, batch: 4, loss: 1.2752646207809448, acc: 48.4375, f1: 32.34661172161172, r: 0.3545458971443799
06/02/2019 01:20:08 step: 6346, epoch: 192, batch: 9, loss: 1.1974505186080933, acc: 57.8125, f1: 29.26456121157045, r: 0.2911262700065045
06/02/2019 01:20:08 step: 6351, epoch: 192, batch: 14, loss: 1.1132938861846924, acc: 46.875, f1: 23.715332286760855, r: 0.3364992209900576
06/02/2019 01:20:08 step: 6356, epoch: 192, batch: 19, loss: 1.3967286348342896, acc: 37.5, f1: 15.256712441073992, r: 0.26205006810516485
06/02/2019 01:20:09 step: 6361, epoch: 192, batch: 24, loss: 1.2667866945266724, acc: 51.5625, f1: 23.476523476523475, r: 0.28074850582906663
06/02/2019 01:20:09 step: 6366, epoch: 192, batch: 29, loss: 1.1194159984588623, acc: 53.125, f1: 26.750303284807764, r: 0.38441616795717604
06/02/2019 01:20:09 *** evaluating ***
06/02/2019 01:20:09 step: 193, epoch: 192, acc: 55.98290598290598, f1: 17.264951363957167, r: 0.28334892798506206
06/02/2019 01:20:09 *** epoch: 194 ***
06/02/2019 01:20:09 *** training ***
06/02/2019 01:20:09 step: 6374, epoch: 193, batch: 4, loss: 1.3937290906906128, acc: 42.1875, f1: 18.15075479396165, r: 0.25850833585807304
06/02/2019 01:20:10 step: 6379, epoch: 193, batch: 9, loss: 1.3520182371139526, acc: 48.4375, f1: 31.068236629309126, r: 0.32425776636638914
06/02/2019 01:20:10 step: 6384, epoch: 193, batch: 14, loss: 1.2650986909866333, acc: 53.125, f1: 25.416874066699847, r: 0.31474090679108074
06/02/2019 01:20:10 step: 6389, epoch: 193, batch: 19, loss: 1.2465527057647705, acc: 40.625, f1: 22.637622424856467, r: 0.2780264226660956
06/02/2019 01:20:10 step: 6394, epoch: 193, batch: 24, loss: 1.427773356437683, acc: 34.375, f1: 15.385835095137423, r: 0.25324939070359137
06/02/2019 01:20:10 step: 6399, epoch: 193, batch: 29, loss: 1.2804349660873413, acc: 46.875, f1: 25.232288037166086, r: 0.37178896600760725
06/02/2019 01:20:11 *** evaluating ***
06/02/2019 01:20:11 step: 194, epoch: 193, acc: 54.700854700854705, f1: 16.4362578519205, r: 0.2842026986830551
06/02/2019 01:20:11 *** epoch: 195 ***
06/02/2019 01:20:11 *** training ***
06/02/2019 01:20:11 step: 6407, epoch: 194, batch: 4, loss: 1.3762887716293335, acc: 39.0625, f1: 18.168290043290042, r: 0.3172796235238601
06/02/2019 01:20:11 step: 6412, epoch: 194, batch: 9, loss: 1.1883959770202637, acc: 51.5625, f1: 38.90614216701174, r: 0.3969630589285819
06/02/2019 01:20:11 step: 6417, epoch: 194, batch: 14, loss: 1.2767257690429688, acc: 51.5625, f1: 21.77729677729678, r: 0.30331943195791217
06/02/2019 01:20:12 step: 6422, epoch: 194, batch: 19, loss: 1.2118736505508423, acc: 48.4375, f1: 26.816351540616246, r: 0.3610946043850687
06/02/2019 01:20:12 step: 6427, epoch: 194, batch: 24, loss: 1.0990660190582275, acc: 50.0, f1: 35.2891156462585, r: 0.3821921546137348
06/02/2019 01:20:12 step: 6432, epoch: 194, batch: 29, loss: 1.311890721321106, acc: 46.875, f1: 26.522366522366518, r: 0.3104195671025357
06/02/2019 01:20:12 *** evaluating ***
06/02/2019 01:20:12 step: 195, epoch: 194, acc: 53.84615384615385, f1: 16.085258598057237, r: 0.28859722681547645
06/02/2019 01:20:12 *** epoch: 196 ***
06/02/2019 01:20:12 *** training ***
06/02/2019 01:20:12 step: 6440, epoch: 195, batch: 4, loss: 1.4517416954040527, acc: 35.9375, f1: 21.23684210526316, r: 0.31271678219641535
06/02/2019 01:20:13 step: 6445, epoch: 195, batch: 9, loss: 1.3172262907028198, acc: 46.875, f1: 19.909366948840635, r: 0.2153841273735822
06/02/2019 01:20:13 step: 6450, epoch: 195, batch: 14, loss: 1.316507339477539, acc: 46.875, f1: 21.647449677545435, r: 0.27721125830264864
06/02/2019 01:20:13 step: 6455, epoch: 195, batch: 19, loss: 1.2749438285827637, acc: 39.0625, f1: 22.986825266237034, r: 0.3316385144849376
06/02/2019 01:20:13 step: 6460, epoch: 195, batch: 24, loss: 1.3299763202667236, acc: 45.3125, f1: 20.70847851335656, r: 0.31380669449877047
06/02/2019 01:20:14 step: 6465, epoch: 195, batch: 29, loss: 1.1936101913452148, acc: 48.4375, f1: 19.16695892759723, r: 0.259288232151203
06/02/2019 01:20:14 *** evaluating ***
06/02/2019 01:20:14 step: 196, epoch: 195, acc: 56.41025641025641, f1: 17.20078686967842, r: 0.27592745704289573
06/02/2019 01:20:14 *** epoch: 197 ***
06/02/2019 01:20:14 *** training ***
06/02/2019 01:20:14 step: 6473, epoch: 196, batch: 4, loss: 1.195017695426941, acc: 56.25, f1: 27.748085592236073, r: 0.3163884870418578
06/02/2019 01:20:14 step: 6478, epoch: 196, batch: 9, loss: 1.281524658203125, acc: 53.125, f1: 24.83533095777994, r: 0.27236713485544106
06/02/2019 01:20:15 step: 6483, epoch: 196, batch: 14, loss: 1.204038143157959, acc: 46.875, f1: 18.421206453121346, r: 0.3248267236276567
06/02/2019 01:20:15 step: 6488, epoch: 196, batch: 19, loss: 0.9730668663978577, acc: 60.9375, f1: 40.16123499142367, r: 0.42574017494822497
06/02/2019 01:20:15 step: 6493, epoch: 196, batch: 24, loss: 1.113645076751709, acc: 56.25, f1: 29.388888888888893, r: 0.3077609920831734
06/02/2019 01:20:15 step: 6498, epoch: 196, batch: 29, loss: 1.4911527633666992, acc: 43.75, f1: 23.198051948051948, r: 0.3069613039365252
06/02/2019 01:20:15 *** evaluating ***
06/02/2019 01:20:16 step: 197, epoch: 196, acc: 55.55555555555556, f1: 16.86520737327189, r: 0.27765047663333303
06/02/2019 01:20:16 *** epoch: 198 ***
06/02/2019 01:20:16 *** training ***
06/02/2019 01:20:16 step: 6506, epoch: 197, batch: 4, loss: 1.3059475421905518, acc: 56.25, f1: 23.0484693877551, r: 0.23868388638916324
06/02/2019 01:20:16 step: 6511, epoch: 197, batch: 9, loss: 1.1626757383346558, acc: 53.125, f1: 27.813040109660886, r: 0.3493931119824566
06/02/2019 01:20:16 step: 6516, epoch: 197, batch: 14, loss: 1.3395296335220337, acc: 37.5, f1: 20.75502137613939, r: 0.26331619294242664
06/02/2019 01:20:16 step: 6521, epoch: 197, batch: 19, loss: 1.4659550189971924, acc: 35.9375, f1: 26.659479409479413, r: 0.4000339514770085
06/02/2019 01:20:17 step: 6526, epoch: 197, batch: 24, loss: 1.2987356185913086, acc: 53.125, f1: 27.01569264069264, r: 0.4054894063301366
06/02/2019 01:20:17 step: 6531, epoch: 197, batch: 29, loss: 1.1484794616699219, acc: 54.6875, f1: 33.40336134453782, r: 0.31542050902132696
06/02/2019 01:20:17 *** evaluating ***
06/02/2019 01:20:17 step: 198, epoch: 197, acc: 55.12820512820513, f1: 16.639396935244687, r: 0.2833086847214772
06/02/2019 01:20:17 *** epoch: 199 ***
06/02/2019 01:20:17 *** training ***
06/02/2019 01:20:17 step: 6539, epoch: 198, batch: 4, loss: 1.2095102071762085, acc: 50.0, f1: 23.120300751879704, r: 0.2335613923448132
06/02/2019 01:20:18 step: 6544, epoch: 198, batch: 9, loss: 1.1746337413787842, acc: 53.125, f1: 42.085214527074996, r: 0.38436587777331016
06/02/2019 01:20:18 step: 6549, epoch: 198, batch: 14, loss: 1.1798179149627686, acc: 54.6875, f1: 34.6041336168585, r: 0.3934275745948511
06/02/2019 01:20:18 step: 6554, epoch: 198, batch: 19, loss: 1.3080837726593018, acc: 43.75, f1: 22.286585365853657, r: 0.3453617395042616
06/02/2019 01:20:18 step: 6559, epoch: 198, batch: 24, loss: 1.0337951183319092, acc: 59.375, f1: 38.183605668180896, r: 0.3830141115746402
06/02/2019 01:20:19 step: 6564, epoch: 198, batch: 29, loss: 1.0630569458007812, acc: 60.9375, f1: 37.81249999999999, r: 0.44560356521444017
06/02/2019 01:20:19 *** evaluating ***
06/02/2019 01:20:19 step: 199, epoch: 198, acc: 56.837606837606835, f1: 17.44793901780445, r: 0.2728526640153846
06/02/2019 01:20:19 *** epoch: 200 ***
06/02/2019 01:20:19 *** training ***
06/02/2019 01:20:19 step: 6572, epoch: 199, batch: 4, loss: 1.2929607629776, acc: 45.3125, f1: 20.440115440115438, r: 0.33018230181629227
06/02/2019 01:20:19 step: 6577, epoch: 199, batch: 9, loss: 1.1803240776062012, acc: 50.0, f1: 19.8520990264746, r: 0.29850549535832865
06/02/2019 01:20:19 step: 6582, epoch: 199, batch: 14, loss: 1.245505452156067, acc: 45.3125, f1: 24.03682077534261, r: 0.2114583192340262
06/02/2019 01:20:20 step: 6587, epoch: 199, batch: 19, loss: 1.3017935752868652, acc: 53.125, f1: 24.761904761904763, r: 0.27829748516548847
06/02/2019 01:20:20 step: 6592, epoch: 199, batch: 24, loss: 1.3197969198226929, acc: 43.75, f1: 22.613636363636363, r: 0.3209058201414514
06/02/2019 01:20:20 step: 6597, epoch: 199, batch: 29, loss: 1.178965449333191, acc: 45.3125, f1: 27.14332414888152, r: 0.381676165498642
06/02/2019 01:20:20 *** evaluating ***
06/02/2019 01:20:20 step: 200, epoch: 199, acc: 55.55555555555556, f1: 17.06314290235797, r: 0.2780207627692024
06/02/2019 01:20:20 *** epoch: 201 ***
06/02/2019 01:20:20 *** training ***
06/02/2019 01:20:21 step: 6605, epoch: 200, batch: 4, loss: 1.298699975013733, acc: 51.5625, f1: 24.872405441867272, r: 0.337008364256012
06/02/2019 01:20:21 step: 6610, epoch: 200, batch: 9, loss: 1.1893075704574585, acc: 54.6875, f1: 20.820153061224488, r: 0.3014805520657885
06/02/2019 01:20:21 step: 6615, epoch: 200, batch: 14, loss: 1.3618309497833252, acc: 50.0, f1: 28.37134853941577, r: 0.29239458258808576
06/02/2019 01:20:21 step: 6620, epoch: 200, batch: 19, loss: 1.3614805936813354, acc: 43.75, f1: 32.491373360938574, r: 0.29515738402829506
06/02/2019 01:20:22 step: 6625, epoch: 200, batch: 24, loss: 1.3398126363754272, acc: 39.0625, f1: 28.32835573253692, r: 0.34706661591126525
06/02/2019 01:20:22 step: 6630, epoch: 200, batch: 29, loss: 1.3945404291152954, acc: 50.0, f1: 22.816430817610062, r: 0.3419105072825312
06/02/2019 01:20:22 *** evaluating ***
06/02/2019 01:20:22 step: 201, epoch: 200, acc: 55.98290598290598, f1: 17.237298491503765, r: 0.27165628161777733
06/02/2019 01:20:22 *** epoch: 202 ***
06/02/2019 01:20:22 *** training ***
06/02/2019 01:20:22 step: 6638, epoch: 201, batch: 4, loss: 1.2977205514907837, acc: 37.5, f1: 18.37614011527055, r: 0.3116523404092452
06/02/2019 01:20:22 step: 6643, epoch: 201, batch: 9, loss: 1.3521595001220703, acc: 40.625, f1: 15.661801088570012, r: 0.3810723273334448
06/02/2019 01:20:23 step: 6648, epoch: 201, batch: 14, loss: 1.20590078830719, acc: 50.0, f1: 26.0012077294686, r: 0.3013167165893333
06/02/2019 01:20:23 step: 6653, epoch: 201, batch: 19, loss: 1.2472385168075562, acc: 51.5625, f1: 29.48915415222692, r: 0.2997171955644656
06/02/2019 01:20:23 step: 6658, epoch: 201, batch: 24, loss: 1.2654448747634888, acc: 46.875, f1: 25.8483393357343, r: 0.40002926994767196
06/02/2019 01:20:23 step: 6663, epoch: 201, batch: 29, loss: 1.2220538854599, acc: 53.125, f1: 25.235735009671178, r: 0.38002440716278046
06/02/2019 01:20:23 *** evaluating ***
06/02/2019 01:20:24 step: 202, epoch: 201, acc: 54.700854700854705, f1: 16.499795751633982, r: 0.2825798487868943
06/02/2019 01:20:24 *** epoch: 203 ***
06/02/2019 01:20:24 *** training ***
06/02/2019 01:20:24 step: 6671, epoch: 202, batch: 4, loss: 1.2626193761825562, acc: 48.4375, f1: 20.071982281284605, r: 0.2720111477186418
06/02/2019 01:20:24 step: 6676, epoch: 202, batch: 9, loss: 1.213510274887085, acc: 48.4375, f1: 25.870231133389026, r: 0.27630619077850455
06/02/2019 01:20:24 step: 6681, epoch: 202, batch: 14, loss: 1.266666054725647, acc: 45.3125, f1: 19.8607061872368, r: 0.28701800243928555
06/02/2019 01:20:24 step: 6686, epoch: 202, batch: 19, loss: 1.2146422863006592, acc: 46.875, f1: 29.781600513307833, r: 0.2589085086061239
06/02/2019 01:20:25 step: 6691, epoch: 202, batch: 24, loss: 1.2587512731552124, acc: 42.1875, f1: 23.58712121212121, r: 0.338019111717198
06/02/2019 01:20:25 step: 6696, epoch: 202, batch: 29, loss: 1.2433648109436035, acc: 48.4375, f1: 38.30455259026688, r: 0.35864152140825567
06/02/2019 01:20:25 *** evaluating ***
06/02/2019 01:20:25 step: 203, epoch: 202, acc: 55.55555555555556, f1: 16.943461123245715, r: 0.28288130116021615
06/02/2019 01:20:25 *** epoch: 204 ***
06/02/2019 01:20:25 *** training ***
06/02/2019 01:20:25 step: 6704, epoch: 203, batch: 4, loss: 1.3115789890289307, acc: 48.4375, f1: 26.006657982341867, r: 0.30638175781386445
06/02/2019 01:20:26 step: 6709, epoch: 203, batch: 9, loss: 1.3100258111953735, acc: 46.875, f1: 20.665374677002585, r: 0.25905570614836576
06/02/2019 01:20:26 step: 6714, epoch: 203, batch: 14, loss: 1.0547813177108765, acc: 51.5625, f1: 41.46484504733067, r: 0.3942576743596157
06/02/2019 01:20:26 step: 6719, epoch: 203, batch: 19, loss: 1.0785367488861084, acc: 56.25, f1: 34.58646616541353, r: 0.4312327268054933
06/02/2019 01:20:26 step: 6724, epoch: 203, batch: 24, loss: 1.2595608234405518, acc: 51.5625, f1: 27.282075620946056, r: 0.2759604896485012
06/02/2019 01:20:27 step: 6729, epoch: 203, batch: 29, loss: 1.3110307455062866, acc: 51.5625, f1: 24.520764194677238, r: 0.3591915566853515
06/02/2019 01:20:27 *** evaluating ***
06/02/2019 01:20:27 step: 204, epoch: 203, acc: 55.98290598290598, f1: 17.023581171599076, r: 0.2818690840487147
06/02/2019 01:20:27 *** epoch: 205 ***
06/02/2019 01:20:27 *** training ***
06/02/2019 01:20:27 step: 6737, epoch: 204, batch: 4, loss: 1.347726583480835, acc: 45.3125, f1: 20.358688930117502, r: 0.24045850614993947
06/02/2019 01:20:27 step: 6742, epoch: 204, batch: 9, loss: 1.1964406967163086, acc: 48.4375, f1: 25.197905249056145, r: 0.3584789372654555
06/02/2019 01:20:27 step: 6747, epoch: 204, batch: 14, loss: 1.1824162006378174, acc: 50.0, f1: 28.548085901027083, r: 0.35799373600653905
06/02/2019 01:20:28 step: 6752, epoch: 204, batch: 19, loss: 1.2065389156341553, acc: 50.0, f1: 23.261904761904763, r: 0.2668280064135108
06/02/2019 01:20:28 step: 6757, epoch: 204, batch: 24, loss: 1.2578493356704712, acc: 46.875, f1: 27.774980121918897, r: 0.2828341099839933
06/02/2019 01:20:28 step: 6762, epoch: 204, batch: 29, loss: 1.1758084297180176, acc: 48.4375, f1: 20.402329027665953, r: 0.3054720216488447
06/02/2019 01:20:28 *** evaluating ***
06/02/2019 01:20:28 step: 205, epoch: 204, acc: 55.55555555555556, f1: 16.87027219496756, r: 0.2825214446153281
06/02/2019 01:20:28 *** epoch: 206 ***
06/02/2019 01:20:28 *** training ***
06/02/2019 01:20:29 step: 6770, epoch: 205, batch: 4, loss: 1.0965148210525513, acc: 54.6875, f1: 28.37894285262706, r: 0.3514900252254157
06/02/2019 01:20:29 step: 6775, epoch: 205, batch: 9, loss: 1.3612949848175049, acc: 50.0, f1: 27.767857142857146, r: 0.27534630537491195
06/02/2019 01:20:29 step: 6780, epoch: 205, batch: 14, loss: 1.3801050186157227, acc: 39.0625, f1: 20.23852657004831, r: 0.29705352000494273
06/02/2019 01:20:29 step: 6785, epoch: 205, batch: 19, loss: 1.2783176898956299, acc: 45.3125, f1: 18.05909863945578, r: 0.2657991418430681
06/02/2019 01:20:30 step: 6790, epoch: 205, batch: 24, loss: 1.1102428436279297, acc: 56.25, f1: 36.23815154757212, r: 0.3597752849156422
06/02/2019 01:20:30 step: 6795, epoch: 205, batch: 29, loss: 1.2171722650527954, acc: 53.125, f1: 28.75086266390614, r: 0.38113424946958335
06/02/2019 01:20:30 *** evaluating ***
06/02/2019 01:20:30 step: 206, epoch: 205, acc: 56.837606837606835, f1: 17.394168875425976, r: 0.2810393453770838
06/02/2019 01:20:30 *** epoch: 207 ***
06/02/2019 01:20:30 *** training ***
06/02/2019 01:20:30 step: 6803, epoch: 206, batch: 4, loss: 1.3128538131713867, acc: 40.625, f1: 19.919992288413344, r: 0.22433903500965963
06/02/2019 01:20:31 step: 6808, epoch: 206, batch: 9, loss: 1.160282850265503, acc: 54.6875, f1: 28.035214085634248, r: 0.3754578807955088
06/02/2019 01:20:31 step: 6813, epoch: 206, batch: 14, loss: 1.3081411123275757, acc: 39.0625, f1: 34.689685829664626, r: 0.34596223815810423
06/02/2019 01:20:31 step: 6818, epoch: 206, batch: 19, loss: 1.2881605625152588, acc: 45.3125, f1: 20.54711246200608, r: 0.3161599208153369
06/02/2019 01:20:31 step: 6823, epoch: 206, batch: 24, loss: 1.1947021484375, acc: 46.875, f1: 19.494066024678272, r: 0.35440676149410283
06/02/2019 01:20:32 step: 6828, epoch: 206, batch: 29, loss: 1.374330997467041, acc: 48.4375, f1: 26.498063340168603, r: 0.2841013121372443
06/02/2019 01:20:32 *** evaluating ***
06/02/2019 01:20:32 step: 207, epoch: 206, acc: 55.98290598290598, f1: 16.97765127556399, r: 0.2843623474466555
06/02/2019 01:20:32 *** epoch: 208 ***
06/02/2019 01:20:32 *** training ***
06/02/2019 01:20:32 step: 6836, epoch: 207, batch: 4, loss: 1.062127947807312, acc: 57.8125, f1: 32.11193568336425, r: 0.3427382540998851
06/02/2019 01:20:32 step: 6841, epoch: 207, batch: 9, loss: 1.4258062839508057, acc: 43.75, f1: 22.328042328042326, r: 0.3274414960463477
06/02/2019 01:20:33 step: 6846, epoch: 207, batch: 14, loss: 1.3413065671920776, acc: 45.3125, f1: 21.51185770750988, r: 0.33229182820637604
06/02/2019 01:20:33 step: 6851, epoch: 207, batch: 19, loss: 1.3319581747055054, acc: 43.75, f1: 23.784059078176725, r: 0.2653210109668786
06/02/2019 01:20:33 step: 6856, epoch: 207, batch: 24, loss: 1.1127442121505737, acc: 54.6875, f1: 30.182072829131652, r: 0.38256965887262806
06/02/2019 01:20:33 step: 6861, epoch: 207, batch: 29, loss: 1.1712884902954102, acc: 46.875, f1: 18.659394792399723, r: 0.3241375746797421
06/02/2019 01:20:33 *** evaluating ***
06/02/2019 01:20:34 step: 208, epoch: 207, acc: 55.12820512820513, f1: 16.657871090935565, r: 0.28091307776654045
06/02/2019 01:20:34 *** epoch: 209 ***
06/02/2019 01:20:34 *** training ***
06/02/2019 01:20:34 step: 6869, epoch: 208, batch: 4, loss: 1.192413330078125, acc: 51.5625, f1: 26.228926353149955, r: 0.3108333064890184
06/02/2019 01:20:34 step: 6874, epoch: 208, batch: 9, loss: 1.4764484167099, acc: 40.625, f1: 24.430132708821237, r: 0.30158426858218673
06/02/2019 01:20:34 step: 6879, epoch: 208, batch: 14, loss: 1.1633265018463135, acc: 51.5625, f1: 21.972789115646258, r: 0.29435546288887793
06/02/2019 01:20:34 step: 6884, epoch: 208, batch: 19, loss: 1.1293190717697144, acc: 60.9375, f1: 29.327646038172357, r: 0.34994973052594247
06/02/2019 01:20:35 step: 6889, epoch: 208, batch: 24, loss: 1.273975133895874, acc: 48.4375, f1: 17.763157894736842, r: 0.2548944865156054
06/02/2019 01:20:35 step: 6894, epoch: 208, batch: 29, loss: 1.1448302268981934, acc: 56.25, f1: 24.82570806100218, r: 0.28378348163649375
06/02/2019 01:20:35 *** evaluating ***
06/02/2019 01:20:35 step: 209, epoch: 208, acc: 54.700854700854705, f1: 16.363211951447244, r: 0.2872770021960068
06/02/2019 01:20:35 *** epoch: 210 ***
06/02/2019 01:20:35 *** training ***
06/02/2019 01:20:35 step: 6902, epoch: 209, batch: 4, loss: 1.2171971797943115, acc: 46.875, f1: 19.8685540950455, r: 0.3180273950370658
06/02/2019 01:20:36 step: 6907, epoch: 209, batch: 9, loss: 1.4441919326782227, acc: 46.875, f1: 25.243055555555554, r: 0.3805218370582143
06/02/2019 01:20:36 step: 6912, epoch: 209, batch: 14, loss: 1.3916692733764648, acc: 42.1875, f1: 20.952165286404416, r: 0.30660146966293306
06/02/2019 01:20:36 step: 6917, epoch: 209, batch: 19, loss: 1.1601063013076782, acc: 46.875, f1: 23.424908424908423, r: 0.3447910937732602
06/02/2019 01:20:36 step: 6922, epoch: 209, batch: 24, loss: 1.2431797981262207, acc: 48.4375, f1: 19.526143790849673, r: 0.34136098251966246
06/02/2019 01:20:36 step: 6927, epoch: 209, batch: 29, loss: 1.2950730323791504, acc: 46.875, f1: 17.9609634551495, r: 0.3212893180064538
06/02/2019 01:20:37 *** evaluating ***
06/02/2019 01:20:37 step: 210, epoch: 209, acc: 55.55555555555556, f1: 16.894069551137452, r: 0.2922377257961464
06/02/2019 01:20:37 *** epoch: 211 ***
06/02/2019 01:20:37 *** training ***
06/02/2019 01:20:37 step: 6935, epoch: 210, batch: 4, loss: 1.2189815044403076, acc: 50.0, f1: 29.317460317460313, r: 0.3811360819875174
06/02/2019 01:20:37 step: 6940, epoch: 210, batch: 9, loss: 1.3138842582702637, acc: 40.625, f1: 22.152854852632878, r: 0.35970704461838593
06/02/2019 01:20:37 step: 6945, epoch: 210, batch: 14, loss: 1.241180658340454, acc: 40.625, f1: 17.627541540585018, r: 0.32479100557663804
06/02/2019 01:20:38 step: 6950, epoch: 210, batch: 19, loss: 1.1687921285629272, acc: 46.875, f1: 35.314420010072176, r: 0.3603994815113163
06/02/2019 01:20:38 step: 6955, epoch: 210, batch: 24, loss: 1.2922343015670776, acc: 43.75, f1: 24.64880464880465, r: 0.2853733537085552
06/02/2019 01:20:38 step: 6960, epoch: 210, batch: 29, loss: 1.4266140460968018, acc: 45.3125, f1: 19.290090601566014, r: 0.32230128081273957
06/02/2019 01:20:38 *** evaluating ***
06/02/2019 01:20:38 step: 211, epoch: 210, acc: 56.837606837606835, f1: 17.43517280988497, r: 0.28393554962224044
06/02/2019 01:20:38 *** epoch: 212 ***
06/02/2019 01:20:38 *** training ***
06/02/2019 01:20:39 step: 6968, epoch: 211, batch: 4, loss: 1.22420334815979, acc: 48.4375, f1: 23.399749373433583, r: 0.327975215386137
06/02/2019 01:20:39 step: 6973, epoch: 211, batch: 9, loss: 1.3385874032974243, acc: 45.3125, f1: 16.065671247357294, r: 0.29284989585244076
06/02/2019 01:20:39 step: 6978, epoch: 211, batch: 14, loss: 1.384317398071289, acc: 45.3125, f1: 22.703818369453042, r: 0.27795685500639644
06/02/2019 01:20:39 step: 6983, epoch: 211, batch: 19, loss: 1.2066775560379028, acc: 42.1875, f1: 25.855706600250887, r: 0.26814820369224346
06/02/2019 01:20:39 step: 6988, epoch: 211, batch: 24, loss: 1.4047874212265015, acc: 40.625, f1: 25.630952380952376, r: 0.25049285286713946
06/02/2019 01:20:40 step: 6993, epoch: 211, batch: 29, loss: 1.3474011421203613, acc: 42.1875, f1: 17.291280148423006, r: 0.26860981439602705
06/02/2019 01:20:40 *** evaluating ***
06/02/2019 01:20:40 step: 212, epoch: 211, acc: 55.55555555555556, f1: 16.91728899035237, r: 0.2806265007968286
06/02/2019 01:20:40 *** epoch: 213 ***
06/02/2019 01:20:40 *** training ***
06/02/2019 01:20:40 step: 7001, epoch: 212, batch: 4, loss: 1.254225492477417, acc: 56.25, f1: 32.910756501182036, r: 0.3140735421936667
06/02/2019 01:20:40 step: 7006, epoch: 212, batch: 9, loss: 1.3068403005599976, acc: 45.3125, f1: 17.665302782324062, r: 0.27976152274271426
06/02/2019 01:20:41 step: 7011, epoch: 212, batch: 14, loss: 1.3171213865280151, acc: 46.875, f1: 24.85268720641061, r: 0.4421400990355159
06/02/2019 01:20:41 step: 7016, epoch: 212, batch: 19, loss: 1.2713525295257568, acc: 46.875, f1: 27.335858585858585, r: 0.4468122182839288
06/02/2019 01:20:41 step: 7021, epoch: 212, batch: 24, loss: 1.2602577209472656, acc: 51.5625, f1: 21.646341463414632, r: 0.3211567348944325
06/02/2019 01:20:41 step: 7026, epoch: 212, batch: 29, loss: 1.258998155593872, acc: 48.4375, f1: 24.33058058058058, r: 0.2972990519290408
06/02/2019 01:20:41 *** evaluating ***
06/02/2019 01:20:41 step: 213, epoch: 212, acc: 54.700854700854705, f1: 16.07245307761659, r: 0.2905438447926213
06/02/2019 01:20:41 *** epoch: 214 ***
06/02/2019 01:20:41 *** training ***
06/02/2019 01:20:42 step: 7034, epoch: 213, batch: 4, loss: 1.1421711444854736, acc: 51.5625, f1: 25.98003848003848, r: 0.3218577365765112
06/02/2019 01:20:42 step: 7039, epoch: 213, batch: 9, loss: 1.3534595966339111, acc: 43.75, f1: 26.53516295025729, r: 0.33362669980856013
06/02/2019 01:20:42 step: 7044, epoch: 213, batch: 14, loss: 1.2245266437530518, acc: 50.0, f1: 24.96905148126014, r: 0.3742258704017555
06/02/2019 01:20:42 step: 7049, epoch: 213, batch: 19, loss: 1.3009194135665894, acc: 46.875, f1: 28.715277777777775, r: 0.36945051396515727
06/02/2019 01:20:43 step: 7054, epoch: 213, batch: 24, loss: 1.0871785879135132, acc: 50.0, f1: 30.48554652213189, r: 0.30994802939364535
06/02/2019 01:20:43 step: 7059, epoch: 213, batch: 29, loss: 1.434651255607605, acc: 45.3125, f1: 24.35930735930736, r: 0.3209978683369981
06/02/2019 01:20:43 *** evaluating ***
06/02/2019 01:20:43 step: 214, epoch: 213, acc: 55.55555555555556, f1: 16.79932771230558, r: 0.2798577168467095
06/02/2019 01:20:43 *** epoch: 215 ***
06/02/2019 01:20:43 *** training ***
06/02/2019 01:20:43 step: 7067, epoch: 214, batch: 4, loss: 1.1228079795837402, acc: 54.6875, f1: 23.75553395961559, r: 0.2658262799372907
06/02/2019 01:20:43 step: 7072, epoch: 214, batch: 9, loss: 1.297253966331482, acc: 46.875, f1: 26.410511503679206, r: 0.36777108273763287
06/02/2019 01:20:44 step: 7077, epoch: 214, batch: 14, loss: 1.2314257621765137, acc: 56.25, f1: 22.368421052631575, r: 0.2658819767740758
06/02/2019 01:20:44 step: 7082, epoch: 214, batch: 19, loss: 1.3151283264160156, acc: 37.5, f1: 19.23324902048306, r: 0.38124023213706687
06/02/2019 01:20:44 step: 7087, epoch: 214, batch: 24, loss: 1.1783051490783691, acc: 48.4375, f1: 24.297040169133194, r: 0.37267141861105885
06/02/2019 01:20:45 step: 7092, epoch: 214, batch: 29, loss: 1.1887537240982056, acc: 50.0, f1: 27.440371893438225, r: 0.3337573435675111
06/02/2019 01:20:45 *** evaluating ***
06/02/2019 01:20:45 step: 215, epoch: 214, acc: 56.41025641025641, f1: 17.21728076209786, r: 0.2780632433572162
06/02/2019 01:20:45 *** epoch: 216 ***
06/02/2019 01:20:45 *** training ***
06/02/2019 01:20:45 step: 7100, epoch: 215, batch: 4, loss: 1.3789767026901245, acc: 42.1875, f1: 19.235819327731097, r: 0.2939949618183841
06/02/2019 01:20:45 step: 7105, epoch: 215, batch: 9, loss: 1.2260706424713135, acc: 51.5625, f1: 27.39003676004563, r: 0.3111957884013338
06/02/2019 01:20:46 step: 7110, epoch: 215, batch: 14, loss: 1.3882200717926025, acc: 39.0625, f1: 15.042983502285828, r: 0.30831144276907374
06/02/2019 01:20:46 step: 7115, epoch: 215, batch: 19, loss: 1.177667260169983, acc: 48.4375, f1: 24.960317460317462, r: 0.32221663635726033
06/02/2019 01:20:46 step: 7120, epoch: 215, batch: 24, loss: 1.3230562210083008, acc: 48.4375, f1: 28.47136222910217, r: 0.33883983843722876
06/02/2019 01:20:46 step: 7125, epoch: 215, batch: 29, loss: 1.2515660524368286, acc: 42.1875, f1: 29.60359408033827, r: 0.3227862915828765
06/02/2019 01:20:46 *** evaluating ***
06/02/2019 01:20:47 step: 216, epoch: 215, acc: 56.837606837606835, f1: 16.88424798651433, r: 0.2807934694906786
06/02/2019 01:20:47 *** epoch: 217 ***
06/02/2019 01:20:47 *** training ***
06/02/2019 01:20:47 step: 7133, epoch: 216, batch: 4, loss: 1.172475814819336, acc: 53.125, f1: 26.738095238095237, r: 0.3705782318322466
06/02/2019 01:20:47 step: 7138, epoch: 216, batch: 9, loss: 1.183584213256836, acc: 48.4375, f1: 27.965032166712838, r: 0.33964063802853156
06/02/2019 01:20:47 step: 7143, epoch: 216, batch: 14, loss: 1.2319414615631104, acc: 51.5625, f1: 23.340764877379524, r: 0.3088887741546367
06/02/2019 01:20:47 step: 7148, epoch: 216, batch: 19, loss: 1.3372488021850586, acc: 43.75, f1: 16.568627450980394, r: 0.2632124437814387
06/02/2019 01:20:48 step: 7153, epoch: 216, batch: 24, loss: 1.2376430034637451, acc: 46.875, f1: 21.966818642350557, r: 0.3085449125534554
06/02/2019 01:20:48 step: 7158, epoch: 216, batch: 29, loss: 1.209829330444336, acc: 45.3125, f1: 22.106216628527843, r: 0.3187880782420501
06/02/2019 01:20:48 *** evaluating ***
06/02/2019 01:20:48 step: 217, epoch: 216, acc: 56.41025641025641, f1: 17.17068645640074, r: 0.27509847640106133
06/02/2019 01:20:48 *** epoch: 218 ***
06/02/2019 01:20:48 *** training ***
06/02/2019 01:20:48 step: 7166, epoch: 217, batch: 4, loss: 1.248980164527893, acc: 50.0, f1: 22.22951680672269, r: 0.34440966254623717
06/02/2019 01:20:49 step: 7171, epoch: 217, batch: 9, loss: 1.3706997632980347, acc: 43.75, f1: 28.455988455988457, r: 0.3030428611026962
06/02/2019 01:20:49 step: 7176, epoch: 217, batch: 14, loss: 1.2324488162994385, acc: 48.4375, f1: 23.564917467356494, r: 0.44270584667819046
06/02/2019 01:20:49 step: 7181, epoch: 217, batch: 19, loss: 1.257743239402771, acc: 45.3125, f1: 16.785060177917323, r: 0.23815141754821723
06/02/2019 01:20:49 step: 7186, epoch: 217, batch: 24, loss: 1.334838628768921, acc: 42.1875, f1: 23.412087912087912, r: 0.21988650786259908
06/02/2019 01:20:49 step: 7191, epoch: 217, batch: 29, loss: 1.271643877029419, acc: 46.875, f1: 29.61451247165533, r: 0.3471112655619011
06/02/2019 01:20:50 *** evaluating ***
06/02/2019 01:20:50 step: 218, epoch: 217, acc: 54.700854700854705, f1: 16.426508792624972, r: 0.2797557788438352
06/02/2019 01:20:50 *** epoch: 219 ***
06/02/2019 01:20:50 *** training ***
06/02/2019 01:20:50 step: 7199, epoch: 218, batch: 4, loss: 1.3675885200500488, acc: 45.3125, f1: 22.990366878458698, r: 0.3291782301025984
06/02/2019 01:20:50 step: 7204, epoch: 218, batch: 9, loss: 1.2562912702560425, acc: 53.125, f1: 21.22060470324748, r: 0.3085907507128366
06/02/2019 01:20:50 step: 7209, epoch: 218, batch: 14, loss: 1.2500089406967163, acc: 48.4375, f1: 33.788080449571126, r: 0.3044819025195179
06/02/2019 01:20:51 step: 7214, epoch: 218, batch: 19, loss: 1.239216923713684, acc: 43.75, f1: 16.837121212121215, r: 0.3085766864257277
06/02/2019 01:20:51 step: 7219, epoch: 218, batch: 24, loss: 1.328373908996582, acc: 43.75, f1: 23.086883876357557, r: 0.2679255514971008
06/02/2019 01:20:51 step: 7224, epoch: 218, batch: 29, loss: 1.1297812461853027, acc: 56.25, f1: 31.537146878455818, r: 0.2818586857640648
06/02/2019 01:20:51 *** evaluating ***
06/02/2019 01:20:51 step: 219, epoch: 218, acc: 56.837606837606835, f1: 17.36136469131982, r: 0.27990311398858164
06/02/2019 01:20:51 *** epoch: 220 ***
06/02/2019 01:20:51 *** training ***
06/02/2019 01:20:52 step: 7232, epoch: 219, batch: 4, loss: 1.193248987197876, acc: 56.25, f1: 29.445786319101686, r: 0.3120289625845091
06/02/2019 01:20:52 step: 7237, epoch: 219, batch: 9, loss: 1.1832263469696045, acc: 45.3125, f1: 24.69869825947053, r: 0.33689428905108704
06/02/2019 01:20:52 step: 7242, epoch: 219, batch: 14, loss: 1.293006181716919, acc: 45.3125, f1: 22.738843965259058, r: 0.34145533024300007
06/02/2019 01:20:52 step: 7247, epoch: 219, batch: 19, loss: 1.3850102424621582, acc: 46.875, f1: 26.85076380728555, r: 0.32428317911436827
06/02/2019 01:20:52 step: 7252, epoch: 219, batch: 24, loss: 1.1575983762741089, acc: 56.25, f1: 22.32142857142857, r: 0.3002071849835582
06/02/2019 01:20:53 step: 7257, epoch: 219, batch: 29, loss: 1.1173911094665527, acc: 51.5625, f1: 30.74929965994837, r: 0.31494953163788775
06/02/2019 01:20:53 *** evaluating ***
06/02/2019 01:20:53 step: 220, epoch: 219, acc: 55.55555555555556, f1: 16.894069551137452, r: 0.28602634790107
06/02/2019 01:20:53 *** epoch: 221 ***
06/02/2019 01:20:53 *** training ***
06/02/2019 01:20:53 step: 7265, epoch: 220, batch: 4, loss: 1.2623002529144287, acc: 48.4375, f1: 25.019114132801224, r: 0.3128686467907548
06/02/2019 01:20:53 step: 7270, epoch: 220, batch: 9, loss: 1.2682445049285889, acc: 43.75, f1: 29.37706972893528, r: 0.37661364208284825
06/02/2019 01:20:54 step: 7275, epoch: 220, batch: 14, loss: 1.1607475280761719, acc: 45.3125, f1: 30.11773940345369, r: 0.322230282759541
06/02/2019 01:20:54 step: 7280, epoch: 220, batch: 19, loss: 1.2316268682479858, acc: 48.4375, f1: 27.541182170542633, r: 0.3521881135287108
06/02/2019 01:20:54 step: 7285, epoch: 220, batch: 24, loss: 1.2275975942611694, acc: 51.5625, f1: 22.942523640661943, r: 0.3618730462669216
06/02/2019 01:20:54 step: 7290, epoch: 220, batch: 29, loss: 1.1883583068847656, acc: 57.8125, f1: 24.483213907494257, r: 0.3076837163852508
06/02/2019 01:20:54 *** evaluating ***
06/02/2019 01:20:54 step: 221, epoch: 220, acc: 56.41025641025641, f1: 17.210054234459744, r: 0.27885923309288463
06/02/2019 01:20:54 *** epoch: 222 ***
06/02/2019 01:20:54 *** training ***
06/02/2019 01:20:55 step: 7298, epoch: 221, batch: 4, loss: 1.1192982196807861, acc: 53.125, f1: 22.61904761904762, r: 0.29383232701024214
06/02/2019 01:20:55 step: 7303, epoch: 221, batch: 9, loss: 1.3194602727890015, acc: 48.4375, f1: 25.203005115089518, r: 0.29324370946161915
06/02/2019 01:20:55 step: 7308, epoch: 221, batch: 14, loss: 1.319292664527893, acc: 51.5625, f1: 28.624543850367267, r: 0.31671350898132616
06/02/2019 01:20:55 step: 7313, epoch: 221, batch: 19, loss: 1.317421793937683, acc: 39.0625, f1: 25.322830886740665, r: 0.3217504012505145
06/02/2019 01:20:56 step: 7318, epoch: 221, batch: 24, loss: 1.3741276264190674, acc: 39.0625, f1: 19.849909285832098, r: 0.25649039383603467
06/02/2019 01:20:56 step: 7323, epoch: 221, batch: 29, loss: 1.2213771343231201, acc: 48.4375, f1: 21.64247517188694, r: 0.261255545128518
06/02/2019 01:20:56 *** evaluating ***
06/02/2019 01:20:56 step: 222, epoch: 221, acc: 57.26495726495726, f1: 17.555480021489693, r: 0.2795674519768097
06/02/2019 01:20:56 *** epoch: 223 ***
06/02/2019 01:20:56 *** training ***
06/02/2019 01:20:56 step: 7331, epoch: 222, batch: 4, loss: 1.262054681777954, acc: 54.6875, f1: 26.461898493491294, r: 0.29320989772719636
06/02/2019 01:20:57 step: 7336, epoch: 222, batch: 9, loss: 1.3383525609970093, acc: 43.75, f1: 32.606757277809905, r: 0.2808588396918171
06/02/2019 01:20:57 step: 7341, epoch: 222, batch: 14, loss: 1.133931279182434, acc: 53.125, f1: 32.640582655826556, r: 0.44303765627503394
06/02/2019 01:20:57 step: 7346, epoch: 222, batch: 19, loss: 1.0810697078704834, acc: 51.5625, f1: 27.823581123270568, r: 0.31064974029480436
06/02/2019 01:20:57 step: 7351, epoch: 222, batch: 24, loss: 1.293794870376587, acc: 50.0, f1: 27.975372145584913, r: 0.27606101698856467
06/02/2019 01:20:57 step: 7356, epoch: 222, batch: 29, loss: 1.1346852779388428, acc: 50.0, f1: 25.593965151638205, r: 0.3113202962738715
06/02/2019 01:20:58 *** evaluating ***
06/02/2019 01:20:58 step: 223, epoch: 222, acc: 55.55555555555556, f1: 16.75200115381842, r: 0.2751921642209009
06/02/2019 01:20:58 *** epoch: 224 ***
06/02/2019 01:20:58 *** training ***
06/02/2019 01:20:58 step: 7364, epoch: 223, batch: 4, loss: 1.205183744430542, acc: 51.5625, f1: 19.813327980730627, r: 0.3397124692584375
06/02/2019 01:20:58 step: 7369, epoch: 223, batch: 9, loss: 1.1122630834579468, acc: 46.875, f1: 20.66584209441352, r: 0.31840060360831085
06/02/2019 01:20:58 step: 7374, epoch: 223, batch: 14, loss: 1.201494812965393, acc: 48.4375, f1: 25.12752883342787, r: 0.3344871313108109
06/02/2019 01:20:59 step: 7379, epoch: 223, batch: 19, loss: 1.2783684730529785, acc: 45.3125, f1: 28.64334130781499, r: 0.30143720900847826
06/02/2019 01:20:59 step: 7384, epoch: 223, batch: 24, loss: 1.112227201461792, acc: 51.5625, f1: 20.18995589414595, r: 0.35847687756115276
06/02/2019 01:20:59 step: 7389, epoch: 223, batch: 29, loss: 1.2558008432388306, acc: 48.4375, f1: 21.71289129686963, r: 0.2600457378818331
06/02/2019 01:20:59 *** evaluating ***
06/02/2019 01:20:59 step: 224, epoch: 223, acc: 53.84615384615385, f1: 15.932909749265558, r: 0.2795761877143222
06/02/2019 01:20:59 *** epoch: 225 ***
06/02/2019 01:20:59 *** training ***
06/02/2019 01:21:00 step: 7397, epoch: 224, batch: 4, loss: 1.1581155061721802, acc: 57.8125, f1: 21.98295506334883, r: 0.2337718956861941
06/02/2019 01:21:00 step: 7402, epoch: 224, batch: 9, loss: 1.056414008140564, acc: 48.4375, f1: 25.82843174700176, r: 0.2749513135959787
06/02/2019 01:21:00 step: 7407, epoch: 224, batch: 14, loss: 1.1739784479141235, acc: 45.3125, f1: 28.307449062924988, r: 0.31094330765255385
06/02/2019 01:21:00 step: 7412, epoch: 224, batch: 19, loss: 1.1910439729690552, acc: 53.125, f1: 25.34514398644833, r: 0.31323571144733736
06/02/2019 01:21:01 step: 7417, epoch: 224, batch: 24, loss: 1.2544313669204712, acc: 45.3125, f1: 15.381493506493504, r: 0.26980657201654373
06/02/2019 01:21:01 step: 7422, epoch: 224, batch: 29, loss: 1.177700161933899, acc: 54.6875, f1: 24.31372549019608, r: 0.3133376926190416
06/02/2019 01:21:01 *** evaluating ***
06/02/2019 01:21:01 step: 225, epoch: 224, acc: 54.27350427350427, f1: 16.2904771345493, r: 0.28264947754817576
06/02/2019 01:21:01 *** epoch: 226 ***
06/02/2019 01:21:01 *** training ***
06/02/2019 01:21:01 step: 7430, epoch: 225, batch: 4, loss: 1.189256191253662, acc: 53.125, f1: 38.297453127592505, r: 0.42926882416306084
06/02/2019 01:21:01 step: 7435, epoch: 225, batch: 9, loss: 1.3387857675552368, acc: 43.75, f1: 35.27389903329753, r: 0.3399783479441543
06/02/2019 01:21:02 step: 7440, epoch: 225, batch: 14, loss: 1.298675537109375, acc: 48.4375, f1: 22.091876208897485, r: 0.3330281005950242
06/02/2019 01:21:02 step: 7445, epoch: 225, batch: 19, loss: 1.3171241283416748, acc: 45.3125, f1: 25.075648988692468, r: 0.25585948095804606
06/02/2019 01:21:02 step: 7450, epoch: 225, batch: 24, loss: 1.2579271793365479, acc: 50.0, f1: 23.894398284642186, r: 0.3797770471208861
06/02/2019 01:21:02 step: 7455, epoch: 225, batch: 29, loss: 1.5667387247085571, acc: 37.5, f1: 16.73202614379085, r: 0.23203747631936003
06/02/2019 01:21:02 *** evaluating ***
06/02/2019 01:21:03 step: 226, epoch: 225, acc: 55.98290598290598, f1: 17.041082923135175, r: 0.2856152843878734
06/02/2019 01:21:03 *** epoch: 227 ***
06/02/2019 01:21:03 *** training ***
06/02/2019 01:21:03 step: 7463, epoch: 226, batch: 4, loss: 1.2578654289245605, acc: 45.3125, f1: 18.82056791371756, r: 0.2775622197936681
06/02/2019 01:21:03 step: 7468, epoch: 226, batch: 9, loss: 1.2548081874847412, acc: 53.125, f1: 28.22922691343744, r: 0.3347319783549749
06/02/2019 01:21:03 step: 7473, epoch: 226, batch: 14, loss: 1.3233425617218018, acc: 50.0, f1: 18.7241054613936, r: 0.2829506502931761
06/02/2019 01:21:03 step: 7478, epoch: 226, batch: 19, loss: 1.2560011148452759, acc: 43.75, f1: 21.60697015750207, r: 0.28115308903462144
06/02/2019 01:21:04 step: 7483, epoch: 226, batch: 24, loss: 1.1186052560806274, acc: 50.0, f1: 27.077485380116954, r: 0.294802017772489
06/02/2019 01:21:04 step: 7488, epoch: 226, batch: 29, loss: 1.264068603515625, acc: 50.0, f1: 27.59680649827529, r: 0.3348712750804281
06/02/2019 01:21:04 *** evaluating ***
06/02/2019 01:21:04 step: 227, epoch: 226, acc: 54.27350427350427, f1: 16.121630743844484, r: 0.28383553675627504
06/02/2019 01:21:04 *** epoch: 228 ***
06/02/2019 01:21:04 *** training ***
06/02/2019 01:21:04 step: 7496, epoch: 227, batch: 4, loss: 1.1799241304397583, acc: 50.0, f1: 26.09390609390609, r: 0.33464457722979957
06/02/2019 01:21:04 step: 7501, epoch: 227, batch: 9, loss: 1.2347017526626587, acc: 45.3125, f1: 25.02164502164502, r: 0.31798821988575565
06/02/2019 01:21:05 step: 7506, epoch: 227, batch: 14, loss: 1.2394582033157349, acc: 48.4375, f1: 27.428413225755417, r: 0.34040972511086837
06/02/2019 01:21:05 step: 7511, epoch: 227, batch: 19, loss: 1.3448410034179688, acc: 43.75, f1: 26.472535355605658, r: 0.3835286418473443
06/02/2019 01:21:05 step: 7516, epoch: 227, batch: 24, loss: 1.0960696935653687, acc: 50.0, f1: 20.801435406698566, r: 0.3921797208349481
06/02/2019 01:21:05 step: 7521, epoch: 227, batch: 29, loss: 1.1761611700057983, acc: 48.4375, f1: 27.832749261320693, r: 0.3781216202900168
06/02/2019 01:21:06 *** evaluating ***
06/02/2019 01:21:06 step: 228, epoch: 227, acc: 53.84615384615385, f1: 16.004416905534292, r: 0.2857106128387824
06/02/2019 01:21:06 *** epoch: 229 ***
06/02/2019 01:21:06 *** training ***
06/02/2019 01:21:06 step: 7529, epoch: 228, batch: 4, loss: 1.240671157836914, acc: 54.6875, f1: 22.40021008403361, r: 0.2707522894661987
06/02/2019 01:21:06 step: 7534, epoch: 228, batch: 9, loss: 1.307265281677246, acc: 45.3125, f1: 22.030112391319715, r: 0.327791227302622
06/02/2019 01:21:06 step: 7539, epoch: 228, batch: 14, loss: 1.2836551666259766, acc: 42.1875, f1: 16.719553385108537, r: 0.3028448193969624
06/02/2019 01:21:07 step: 7544, epoch: 228, batch: 19, loss: 1.198858618736267, acc: 48.4375, f1: 37.309711723852665, r: 0.29004818514483094
06/02/2019 01:21:07 step: 7549, epoch: 228, batch: 24, loss: 1.0521105527877808, acc: 59.375, f1: 37.30555555555556, r: 0.4462556804111981
06/02/2019 01:21:07 step: 7554, epoch: 228, batch: 29, loss: 1.1629292964935303, acc: 51.5625, f1: 21.78053830227743, r: 0.2664299812138653
06/02/2019 01:21:07 *** evaluating ***
06/02/2019 01:21:07 step: 229, epoch: 228, acc: 55.55555555555556, f1: 16.87932240523704, r: 0.28714218638976696
06/02/2019 01:21:07 *** epoch: 230 ***
06/02/2019 01:21:07 *** training ***
06/02/2019 01:21:08 step: 7562, epoch: 229, batch: 4, loss: 1.2133972644805908, acc: 42.1875, f1: 24.640224640224638, r: 0.3108647624776645
06/02/2019 01:21:08 step: 7567, epoch: 229, batch: 9, loss: 1.2922147512435913, acc: 56.25, f1: 24.688644688644686, r: 0.3110144589829913
06/02/2019 01:21:08 step: 7572, epoch: 229, batch: 14, loss: 1.1153026819229126, acc: 64.0625, f1: 35.62339221402846, r: 0.3545371034343464
06/02/2019 01:21:08 step: 7577, epoch: 229, batch: 19, loss: 1.2413499355316162, acc: 50.0, f1: 25.768147684605758, r: 0.3344127342701682
06/02/2019 01:21:08 step: 7582, epoch: 229, batch: 24, loss: 1.3849507570266724, acc: 43.75, f1: 16.84465400790111, r: 0.3113601085203415
06/02/2019 01:21:09 step: 7587, epoch: 229, batch: 29, loss: 1.1174259185791016, acc: 51.5625, f1: 41.732804232804234, r: 0.3321205511247102
06/02/2019 01:21:09 *** evaluating ***
06/02/2019 01:21:09 step: 230, epoch: 229, acc: 54.700854700854705, f1: 16.6029771345493, r: 0.2873789566659898
06/02/2019 01:21:09 *** epoch: 231 ***
06/02/2019 01:21:09 *** training ***
06/02/2019 01:21:09 step: 7595, epoch: 230, batch: 4, loss: 1.3082785606384277, acc: 51.5625, f1: 33.412583279604554, r: 0.3925229417162762
06/02/2019 01:21:09 step: 7600, epoch: 230, batch: 9, loss: 1.1735409498214722, acc: 50.0, f1: 27.174734969234187, r: 0.21171150638666653
06/02/2019 01:21:10 step: 7605, epoch: 230, batch: 14, loss: 1.3956791162490845, acc: 43.75, f1: 15.64935064935065, r: 0.2923873073623839
06/02/2019 01:21:10 step: 7610, epoch: 230, batch: 19, loss: 1.264540195465088, acc: 42.1875, f1: 25.359346396365858, r: 0.3576316689967623
06/02/2019 01:21:10 step: 7615, epoch: 230, batch: 24, loss: 1.465535044670105, acc: 42.1875, f1: 18.69071815718157, r: 0.27910236733583743
06/02/2019 01:21:10 step: 7620, epoch: 230, batch: 29, loss: 1.236875057220459, acc: 40.625, f1: 18.952807603877787, r: 0.2525348571332609
06/02/2019 01:21:10 *** evaluating ***
06/02/2019 01:21:10 step: 231, epoch: 230, acc: 54.700854700854705, f1: 16.314135410261667, r: 0.28111613423289766
06/02/2019 01:21:10 *** epoch: 232 ***
06/02/2019 01:21:10 *** training ***
06/02/2019 01:21:11 step: 7628, epoch: 231, batch: 4, loss: 1.4776828289031982, acc: 42.1875, f1: 20.162337662337663, r: 0.24925711152284857
06/02/2019 01:21:11 step: 7633, epoch: 231, batch: 9, loss: 1.351173758506775, acc: 50.0, f1: 27.786692340607622, r: 0.30464772775757787
06/02/2019 01:21:11 step: 7638, epoch: 231, batch: 14, loss: 1.2610312700271606, acc: 45.3125, f1: 22.00431147799569, r: 0.2800821584282144
06/02/2019 01:21:11 step: 7643, epoch: 231, batch: 19, loss: 1.1882834434509277, acc: 51.5625, f1: 21.749084249084248, r: 0.23833858943003874
06/02/2019 01:21:12 step: 7648, epoch: 231, batch: 24, loss: 1.1227974891662598, acc: 54.6875, f1: 25.002717002717006, r: 0.3733005070095186
06/02/2019 01:21:12 step: 7653, epoch: 231, batch: 29, loss: 0.9775424003601074, acc: 62.5, f1: 23.014280153748846, r: 0.3419532782552456
06/02/2019 01:21:12 *** evaluating ***
06/02/2019 01:21:12 step: 232, epoch: 231, acc: 55.55555555555556, f1: 16.796754288904175, r: 0.27575527604547817
06/02/2019 01:21:12 *** epoch: 233 ***
06/02/2019 01:21:12 *** training ***
06/02/2019 01:21:12 step: 7661, epoch: 232, batch: 4, loss: 1.4233896732330322, acc: 37.5, f1: 18.221308766421547, r: 0.2650058825532274
06/02/2019 01:21:12 step: 7666, epoch: 232, batch: 9, loss: 1.201407551765442, acc: 48.4375, f1: 25.690362269309635, r: 0.2938227732259174
06/02/2019 01:21:13 step: 7671, epoch: 232, batch: 14, loss: 1.3129019737243652, acc: 46.875, f1: 18.520188285025913, r: 0.27795933755464913
06/02/2019 01:21:13 step: 7676, epoch: 232, batch: 19, loss: 1.4185236692428589, acc: 35.9375, f1: 17.469426795108284, r: 0.20149409673752325
06/02/2019 01:21:13 step: 7681, epoch: 232, batch: 24, loss: 1.171633243560791, acc: 56.25, f1: 22.27934102934103, r: 0.2797432500071272
06/02/2019 01:21:13 step: 7686, epoch: 232, batch: 29, loss: 1.131583571434021, acc: 53.125, f1: 34.837092731829564, r: 0.2708809658588323
06/02/2019 01:21:13 *** evaluating ***
06/02/2019 01:21:14 step: 233, epoch: 232, acc: 55.98290598290598, f1: 16.97765127556399, r: 0.276486004464591
06/02/2019 01:21:14 *** epoch: 234 ***
06/02/2019 01:21:14 *** training ***
06/02/2019 01:21:14 step: 7694, epoch: 233, batch: 4, loss: 1.1579655408859253, acc: 50.0, f1: 28.89374366767984, r: 0.40212424911833655
06/02/2019 01:21:14 step: 7699, epoch: 233, batch: 9, loss: 1.231071949005127, acc: 53.125, f1: 34.50290885585004, r: 0.3728011367753096
06/02/2019 01:21:14 step: 7704, epoch: 233, batch: 14, loss: 1.0633834600448608, acc: 56.25, f1: 29.888676337743767, r: 0.3891897522706913
06/02/2019 01:21:14 step: 7709, epoch: 233, batch: 19, loss: 1.3311197757720947, acc: 42.1875, f1: 25.113871635610767, r: 0.31735374094711516
06/02/2019 01:21:15 step: 7714, epoch: 233, batch: 24, loss: 1.29033362865448, acc: 40.625, f1: 25.027056277056275, r: 0.32434188720510426
06/02/2019 01:21:15 step: 7719, epoch: 233, batch: 29, loss: 1.369057059288025, acc: 46.875, f1: 18.969979296066253, r: 0.3047651975200662
06/02/2019 01:21:15 *** evaluating ***
06/02/2019 01:21:15 step: 234, epoch: 233, acc: 54.700854700854705, f1: 16.44886962158658, r: 0.28497818327131774
06/02/2019 01:21:15 *** epoch: 235 ***
06/02/2019 01:21:15 *** training ***
06/02/2019 01:21:15 step: 7727, epoch: 234, batch: 4, loss: 1.1723941564559937, acc: 48.4375, f1: 37.63626391205978, r: 0.39345786470926475
06/02/2019 01:21:16 step: 7732, epoch: 234, batch: 9, loss: 1.2946877479553223, acc: 50.0, f1: 20.540411286465808, r: 0.30795665106142356
06/02/2019 01:21:16 step: 7737, epoch: 234, batch: 14, loss: 1.3130816221237183, acc: 50.0, f1: 24.912536443148685, r: 0.31390506745431057
06/02/2019 01:21:16 step: 7742, epoch: 234, batch: 19, loss: 1.3373688459396362, acc: 43.75, f1: 26.450926524455937, r: 0.35021999913320645
06/02/2019 01:21:16 step: 7747, epoch: 234, batch: 24, loss: 1.3733057975769043, acc: 40.625, f1: 18.43500797448166, r: 0.28618096075107324
06/02/2019 01:21:17 step: 7752, epoch: 234, batch: 29, loss: 1.421118974685669, acc: 42.1875, f1: 21.981292517006803, r: 0.22680747144763405
06/02/2019 01:21:17 *** evaluating ***
06/02/2019 01:21:17 step: 235, epoch: 234, acc: 54.27350427350427, f1: 16.229513485611044, r: 0.2872935049208516
06/02/2019 01:21:17 *** epoch: 236 ***
06/02/2019 01:21:17 *** training ***
06/02/2019 01:21:17 step: 7760, epoch: 235, batch: 4, loss: 1.3544714450836182, acc: 53.125, f1: 30.72094152976506, r: 0.3941797791293032
06/02/2019 01:21:17 step: 7765, epoch: 235, batch: 9, loss: 1.2172861099243164, acc: 46.875, f1: 28.67898147401253, r: 0.2512718812399823
06/02/2019 01:21:18 step: 7770, epoch: 235, batch: 14, loss: 1.2426629066467285, acc: 43.75, f1: 19.76322458029775, r: 0.3034631663799549
06/02/2019 01:21:18 step: 7775, epoch: 235, batch: 19, loss: 1.1661779880523682, acc: 50.0, f1: 25.536598420982205, r: 0.3362990433064157
06/02/2019 01:21:18 step: 7780, epoch: 235, batch: 24, loss: 1.053815484046936, acc: 48.4375, f1: 30.701754385964907, r: 0.29036561610911354
06/02/2019 01:21:18 step: 7785, epoch: 235, batch: 29, loss: 1.1072255373001099, acc: 56.25, f1: 26.043650135237996, r: 0.21900311543873555
06/02/2019 01:21:18 *** evaluating ***
06/02/2019 01:21:19 step: 236, epoch: 235, acc: 55.98290598290598, f1: 17.071244563004235, r: 0.27823350247907136
06/02/2019 01:21:19 *** epoch: 237 ***
06/02/2019 01:21:19 *** training ***
06/02/2019 01:21:19 step: 7793, epoch: 236, batch: 4, loss: 1.2013765573501587, acc: 45.3125, f1: 36.26477541371159, r: 0.32351549599131896
06/02/2019 01:21:19 step: 7798, epoch: 236, batch: 9, loss: 1.3101673126220703, acc: 43.75, f1: 19.20595188429982, r: 0.31416542549618953
06/02/2019 01:21:19 step: 7803, epoch: 236, batch: 14, loss: 1.1536930799484253, acc: 54.6875, f1: 35.503710575139145, r: 0.34187121861453756
06/02/2019 01:21:19 step: 7808, epoch: 236, batch: 19, loss: 1.267534613609314, acc: 43.75, f1: 17.02928607340372, r: 0.2765248028634356
06/02/2019 01:21:20 step: 7813, epoch: 236, batch: 24, loss: 1.275557518005371, acc: 48.4375, f1: 31.184668989547042, r: 0.3525508551646639
06/02/2019 01:21:20 step: 7818, epoch: 236, batch: 29, loss: 1.2898885011672974, acc: 42.1875, f1: 23.933747412008284, r: 0.2446642991516171
06/02/2019 01:21:20 *** evaluating ***
06/02/2019 01:21:20 step: 237, epoch: 236, acc: 55.98290598290598, f1: 17.297118576033167, r: 0.27851602656238156
06/02/2019 01:21:20 *** epoch: 238 ***
06/02/2019 01:21:20 *** training ***
06/02/2019 01:21:20 step: 7826, epoch: 237, batch: 4, loss: 1.123831033706665, acc: 51.5625, f1: 31.551394342092014, r: 0.31765405154651494
06/02/2019 01:21:21 step: 7831, epoch: 237, batch: 9, loss: 1.22442626953125, acc: 56.25, f1: 22.977272727272723, r: 0.3476529652265335
06/02/2019 01:21:21 step: 7836, epoch: 237, batch: 14, loss: 1.274690866470337, acc: 42.1875, f1: 31.153559249786873, r: 0.36142771429124676
06/02/2019 01:21:21 step: 7841, epoch: 237, batch: 19, loss: 1.0380380153656006, acc: 57.8125, f1: 36.00282485875706, r: 0.4004676895090935
06/02/2019 01:21:21 step: 7846, epoch: 237, batch: 24, loss: 1.3146125078201294, acc: 43.75, f1: 20.940050661412887, r: 0.25831601211912275
06/02/2019 01:21:21 step: 7851, epoch: 237, batch: 29, loss: 1.261364459991455, acc: 42.1875, f1: 15.833333333333332, r: 0.3291571402263589
06/02/2019 01:21:22 *** evaluating ***
06/02/2019 01:21:22 step: 238, epoch: 237, acc: 56.837606837606835, f1: 17.408461298713753, r: 0.2819678380590134
06/02/2019 01:21:22 *** epoch: 239 ***
06/02/2019 01:21:22 *** training ***
06/02/2019 01:21:22 step: 7859, epoch: 238, batch: 4, loss: 1.3452988862991333, acc: 48.4375, f1: 37.02658303464755, r: 0.3403490069767326
06/02/2019 01:21:22 step: 7864, epoch: 238, batch: 9, loss: 1.3567293882369995, acc: 46.875, f1: 25.263421385870366, r: 0.26350565249937785
06/02/2019 01:21:22 step: 7869, epoch: 238, batch: 14, loss: 1.132406234741211, acc: 54.6875, f1: 28.7202380952381, r: 0.37185901885921324
06/02/2019 01:21:23 step: 7874, epoch: 238, batch: 19, loss: 1.0726104974746704, acc: 53.125, f1: 28.52813852813853, r: 0.349876985235188
06/02/2019 01:21:23 step: 7879, epoch: 238, batch: 24, loss: 1.2294470071792603, acc: 54.6875, f1: 25.471903982542283, r: 0.3693110659772294
06/02/2019 01:21:23 step: 7884, epoch: 238, batch: 29, loss: 1.110187292098999, acc: 53.125, f1: 38.36394872472578, r: 0.3319013059609661
06/02/2019 01:21:23 *** evaluating ***
06/02/2019 01:21:23 step: 239, epoch: 238, acc: 55.12820512820513, f1: 16.535977635523864, r: 0.285504222792654
06/02/2019 01:21:23 *** epoch: 240 ***
06/02/2019 01:21:23 *** training ***
06/02/2019 01:21:23 step: 7892, epoch: 239, batch: 4, loss: 1.2535123825073242, acc: 42.1875, f1: 22.20614035087719, r: 0.31885273698014566
06/02/2019 01:21:24 step: 7897, epoch: 239, batch: 9, loss: 1.0087288618087769, acc: 56.25, f1: 29.100260946950847, r: 0.23031011328153736
06/02/2019 01:21:24 step: 7902, epoch: 239, batch: 14, loss: 1.050376296043396, acc: 56.25, f1: 34.35374149659864, r: 0.3649011813831522
06/02/2019 01:21:24 step: 7907, epoch: 239, batch: 19, loss: 1.200049638748169, acc: 50.0, f1: 28.298845670672453, r: 0.32762958934225156
06/02/2019 01:21:24 step: 7912, epoch: 239, batch: 24, loss: 1.3591538667678833, acc: 50.0, f1: 26.424233661075768, r: 0.33068455078023273
06/02/2019 01:21:24 step: 7917, epoch: 239, batch: 29, loss: 1.2087230682373047, acc: 48.4375, f1: 26.263026487190622, r: 0.34768225796979274
06/02/2019 01:21:25 *** evaluating ***
06/02/2019 01:21:25 step: 240, epoch: 239, acc: 57.692307692307686, f1: 17.821437468298203, r: 0.2829684522716913
06/02/2019 01:21:25 *** epoch: 241 ***
06/02/2019 01:21:25 *** training ***
06/02/2019 01:21:25 step: 7925, epoch: 240, batch: 4, loss: 1.0092182159423828, acc: 59.375, f1: 46.111111111111114, r: 0.32386955716199184
06/02/2019 01:21:25 step: 7930, epoch: 240, batch: 9, loss: 1.1738300323486328, acc: 59.375, f1: 34.4419807834442, r: 0.3424451003674489
06/02/2019 01:21:25 step: 7935, epoch: 240, batch: 14, loss: 1.2940101623535156, acc: 50.0, f1: 32.861440030557674, r: 0.41545955663184353
06/02/2019 01:21:26 step: 7940, epoch: 240, batch: 19, loss: 1.2487857341766357, acc: 54.6875, f1: 27.90580847723705, r: 0.3287034113730718
06/02/2019 01:21:26 step: 7945, epoch: 240, batch: 24, loss: 1.2767231464385986, acc: 45.3125, f1: 22.720959595959595, r: 0.30426223463540275
06/02/2019 01:21:26 step: 7950, epoch: 240, batch: 29, loss: 1.4362767934799194, acc: 56.25, f1: 22.346256684491976, r: 0.27632069879771026
06/02/2019 01:21:26 *** evaluating ***
06/02/2019 01:21:26 step: 241, epoch: 240, acc: 56.837606837606835, f1: 17.394168875425976, r: 0.28049380988900263
06/02/2019 01:21:26 *** epoch: 242 ***
06/02/2019 01:21:26 *** training ***
06/02/2019 01:21:26 step: 7958, epoch: 241, batch: 4, loss: 1.0208855867385864, acc: 59.375, f1: 34.86091616260557, r: 0.3323306403752985
06/02/2019 01:21:27 step: 7963, epoch: 241, batch: 9, loss: 1.079614520072937, acc: 57.8125, f1: 26.87668690958165, r: 0.292888086124126
06/02/2019 01:21:27 step: 7968, epoch: 241, batch: 14, loss: 1.2506790161132812, acc: 54.6875, f1: 26.36904761904762, r: 0.3184453569921993
06/02/2019 01:21:27 step: 7973, epoch: 241, batch: 19, loss: 1.2428258657455444, acc: 51.5625, f1: 32.77186761229315, r: 0.3486823141883798
06/02/2019 01:21:27 step: 7978, epoch: 241, batch: 24, loss: 1.2629835605621338, acc: 50.0, f1: 28.46128927761581, r: 0.34396997341275526
06/02/2019 01:21:28 step: 7983, epoch: 241, batch: 29, loss: 0.9808836579322815, acc: 57.8125, f1: 32.091248156821926, r: 0.3581908845948342
06/02/2019 01:21:28 *** evaluating ***
06/02/2019 01:21:28 step: 242, epoch: 241, acc: 57.26495726495726, f1: 17.728821156432094, r: 0.2840633357721072
06/02/2019 01:21:28 *** epoch: 243 ***
06/02/2019 01:21:28 *** training ***
06/02/2019 01:21:28 step: 7991, epoch: 242, batch: 4, loss: 1.3268746137619019, acc: 42.1875, f1: 18.460174577195854, r: 0.3190642426678299
06/02/2019 01:21:28 step: 7996, epoch: 242, batch: 9, loss: 1.4388433694839478, acc: 42.1875, f1: 17.78070175438597, r: 0.2511269036353901
06/02/2019 01:21:28 step: 8001, epoch: 242, batch: 14, loss: 1.2118010520935059, acc: 51.5625, f1: 22.48759495221759, r: 0.3374885156279404
06/02/2019 01:21:29 step: 8006, epoch: 242, batch: 19, loss: 0.9741765856742859, acc: 64.0625, f1: 41.228070175438596, r: 0.46562503055880344
06/02/2019 01:21:29 step: 8011, epoch: 242, batch: 24, loss: 1.1605255603790283, acc: 51.5625, f1: 22.886869033327617, r: 0.30618071116717027
06/02/2019 01:21:29 step: 8016, epoch: 242, batch: 29, loss: 1.3085001707077026, acc: 48.4375, f1: 33.16883116883117, r: 0.3220457424010527
06/02/2019 01:21:29 *** evaluating ***
06/02/2019 01:21:29 step: 243, epoch: 242, acc: 56.837606837606835, f1: 17.348840852130323, r: 0.28107018778751824
06/02/2019 01:21:29 *** epoch: 244 ***
06/02/2019 01:21:29 *** training ***
06/02/2019 01:21:30 step: 8024, epoch: 243, batch: 4, loss: 1.3414851427078247, acc: 45.3125, f1: 23.56617647058824, r: 0.36983281794032735
06/02/2019 01:21:30 step: 8029, epoch: 243, batch: 9, loss: 1.379936695098877, acc: 45.3125, f1: 22.571353065539114, r: 0.3253796379709196
06/02/2019 01:21:30 step: 8034, epoch: 243, batch: 14, loss: 1.201931357383728, acc: 53.125, f1: 27.232394934258288, r: 0.3024543454538579
06/02/2019 01:21:30 step: 8039, epoch: 243, batch: 19, loss: 1.3311688899993896, acc: 46.875, f1: 26.518263852851298, r: 0.3269445602918824
06/02/2019 01:21:31 step: 8044, epoch: 243, batch: 24, loss: 1.2192398309707642, acc: 46.875, f1: 26.191015413257503, r: 0.3624289312447942
06/02/2019 01:21:31 step: 8049, epoch: 243, batch: 29, loss: 1.1792654991149902, acc: 53.125, f1: 25.83333333333333, r: 0.3223152522691492
06/02/2019 01:21:31 *** evaluating ***
06/02/2019 01:21:31 step: 244, epoch: 243, acc: 56.41025641025641, f1: 17.246468479865065, r: 0.2809667760329887
06/02/2019 01:21:31 *** epoch: 245 ***
06/02/2019 01:21:31 *** training ***
06/02/2019 01:21:31 step: 8057, epoch: 244, batch: 4, loss: 1.2651197910308838, acc: 45.3125, f1: 20.685463659147874, r: 0.363951551376894
06/02/2019 01:21:31 step: 8062, epoch: 244, batch: 9, loss: 1.0558069944381714, acc: 56.25, f1: 45.25436969045992, r: 0.42649672616895756
06/02/2019 01:21:32 step: 8067, epoch: 244, batch: 14, loss: 1.2740683555603027, acc: 48.4375, f1: 26.711073318216176, r: 0.2643848030251402
06/02/2019 01:21:32 step: 8072, epoch: 244, batch: 19, loss: 1.0013030767440796, acc: 57.8125, f1: 28.926282051282055, r: 0.3555320701140872
06/02/2019 01:21:32 step: 8077, epoch: 244, batch: 24, loss: 1.447208046913147, acc: 59.375, f1: 32.73878830529077, r: 0.3267972798823256
06/02/2019 01:21:32 step: 8082, epoch: 244, batch: 29, loss: 1.3348275423049927, acc: 46.875, f1: 24.203811959566437, r: 0.33310530360533047
06/02/2019 01:21:33 *** evaluating ***
06/02/2019 01:21:33 step: 245, epoch: 244, acc: 55.98290598290598, f1: 17.009170653907496, r: 0.2882032352186594
06/02/2019 01:21:33 *** epoch: 246 ***
06/02/2019 01:21:33 *** training ***
06/02/2019 01:21:33 step: 8090, epoch: 245, batch: 4, loss: 1.3839513063430786, acc: 39.0625, f1: 15.886414695387868, r: 0.3087182526222953
06/02/2019 01:21:33 step: 8095, epoch: 245, batch: 9, loss: 1.154513955116272, acc: 46.875, f1: 21.269437469714987, r: 0.33987056979355196
06/02/2019 01:21:33 step: 8100, epoch: 245, batch: 14, loss: 1.2380056381225586, acc: 45.3125, f1: 18.384795778776624, r: 0.3009059620839655
06/02/2019 01:21:34 step: 8105, epoch: 245, batch: 19, loss: 1.011945128440857, acc: 60.9375, f1: 36.12268518518518, r: 0.4039117519750365
06/02/2019 01:21:34 step: 8110, epoch: 245, batch: 24, loss: 1.1592330932617188, acc: 46.875, f1: 18.1236136317806, r: 0.3059575512967628
06/02/2019 01:21:34 step: 8115, epoch: 245, batch: 29, loss: 1.1415839195251465, acc: 51.5625, f1: 19.01577310703983, r: 0.3104760684192348
06/02/2019 01:21:34 *** evaluating ***
06/02/2019 01:21:34 step: 246, epoch: 245, acc: 56.41025641025641, f1: 17.323513366066557, r: 0.2854376096399639
06/02/2019 01:21:34 *** epoch: 247 ***
06/02/2019 01:21:34 *** training ***
06/02/2019 01:21:35 step: 8123, epoch: 246, batch: 4, loss: 1.3074134588241577, acc: 40.625, f1: 22.351398601398603, r: 0.3031266168934195
06/02/2019 01:21:35 step: 8128, epoch: 246, batch: 9, loss: 1.2692171335220337, acc: 48.4375, f1: 21.296296296296298, r: 0.34097178125717553
06/02/2019 01:21:35 step: 8133, epoch: 246, batch: 14, loss: 1.082358479499817, acc: 56.25, f1: 30.354166666666664, r: 0.42134125866906175
06/02/2019 01:21:35 step: 8138, epoch: 246, batch: 19, loss: 1.2045955657958984, acc: 59.375, f1: 25.9141156462585, r: 0.3342127700494908
06/02/2019 01:21:36 step: 8143, epoch: 246, batch: 24, loss: 1.1434952020645142, acc: 51.5625, f1: 32.34024034141958, r: 0.35855738297477946
06/02/2019 01:21:36 step: 8148, epoch: 246, batch: 29, loss: 1.2311618328094482, acc: 51.5625, f1: 19.683517611896512, r: 0.2085550826922536
06/02/2019 01:21:36 *** evaluating ***
06/02/2019 01:21:36 step: 247, epoch: 246, acc: 55.12820512820513, f1: 16.565567403431963, r: 0.2864498814204173
06/02/2019 01:21:36 *** epoch: 248 ***
06/02/2019 01:21:36 *** training ***
06/02/2019 01:21:36 step: 8156, epoch: 247, batch: 4, loss: 1.2417573928833008, acc: 50.0, f1: 39.75378787878788, r: 0.43867698755219703
06/02/2019 01:21:37 step: 8161, epoch: 247, batch: 9, loss: 1.2841414213180542, acc: 45.3125, f1: 28.22463768115942, r: 0.3499904927842768
06/02/2019 01:21:37 step: 8166, epoch: 247, batch: 14, loss: 1.160236120223999, acc: 51.5625, f1: 24.693515704154002, r: 0.4330872232018602
06/02/2019 01:21:37 step: 8171, epoch: 247, batch: 19, loss: 1.5159682035446167, acc: 42.1875, f1: 18.14516129032258, r: 0.24095554191504773
06/02/2019 01:21:37 step: 8176, epoch: 247, batch: 24, loss: 1.2649972438812256, acc: 48.4375, f1: 35.188515837923475, r: 0.37554626500490257
06/02/2019 01:21:37 step: 8181, epoch: 247, batch: 29, loss: 1.3791801929473877, acc: 42.1875, f1: 27.735665694849367, r: 0.3046840923285119
06/02/2019 01:21:38 *** evaluating ***
06/02/2019 01:21:38 step: 248, epoch: 247, acc: 53.41880341880342, f1: 15.556763724324266, r: 0.2840305787578957
06/02/2019 01:21:38 *** epoch: 249 ***
06/02/2019 01:21:38 *** training ***
06/02/2019 01:21:38 step: 8189, epoch: 248, batch: 4, loss: 1.188490629196167, acc: 53.125, f1: 31.765802027429935, r: 0.39301005060526384
06/02/2019 01:21:38 step: 8194, epoch: 248, batch: 9, loss: 1.0552088022232056, acc: 60.9375, f1: 28.70452528837622, r: 0.3788061543725829
06/02/2019 01:21:38 step: 8199, epoch: 248, batch: 14, loss: 1.153555154800415, acc: 43.75, f1: 32.97317813765182, r: 0.346698771982802
06/02/2019 01:21:39 step: 8204, epoch: 248, batch: 19, loss: 1.1639513969421387, acc: 51.5625, f1: 30.29113903734733, r: 0.35700870948029284
06/02/2019 01:21:39 step: 8209, epoch: 248, batch: 24, loss: 1.1543747186660767, acc: 54.6875, f1: 34.068134763786944, r: 0.24730426010099915
06/02/2019 01:21:39 step: 8214, epoch: 248, batch: 29, loss: 1.4303081035614014, acc: 34.375, f1: 17.83407161314138, r: 0.2645114258480967
06/02/2019 01:21:39 *** evaluating ***
06/02/2019 01:21:39 step: 249, epoch: 248, acc: 56.41025641025641, f1: 17.261744893296182, r: 0.28187866415543306
06/02/2019 01:21:39 *** epoch: 250 ***
06/02/2019 01:21:39 *** training ***
06/02/2019 01:21:39 step: 8222, epoch: 249, batch: 4, loss: 1.2867724895477295, acc: 42.1875, f1: 20.755910755910755, r: 0.27841881867852114
06/02/2019 01:21:40 step: 8227, epoch: 249, batch: 9, loss: 1.0359355211257935, acc: 60.9375, f1: 26.122589616430602, r: 0.28812277498488365
06/02/2019 01:21:40 step: 8232, epoch: 249, batch: 14, loss: 1.2114583253860474, acc: 51.5625, f1: 24.194025436261462, r: 0.3021749121066372
06/02/2019 01:21:40 step: 8237, epoch: 249, batch: 19, loss: 1.372536063194275, acc: 48.4375, f1: 36.36834043962413, r: 0.3959113545373066
06/02/2019 01:21:40 step: 8242, epoch: 249, batch: 24, loss: 1.0460290908813477, acc: 50.0, f1: 34.11580016622033, r: 0.41292568509661604
06/02/2019 01:21:41 step: 8247, epoch: 249, batch: 29, loss: 1.110095500946045, acc: 62.5, f1: 30.893939393939394, r: 0.3431917437427108
06/02/2019 01:21:41 *** evaluating ***
06/02/2019 01:21:41 step: 250, epoch: 249, acc: 55.55555555555556, f1: 16.82667530994247, r: 0.2829467843071266
06/02/2019 01:21:41 *** epoch: 251 ***
06/02/2019 01:21:41 *** training ***
06/02/2019 01:21:41 step: 8255, epoch: 250, batch: 4, loss: 1.327813744544983, acc: 45.3125, f1: 19.83901515151515, r: 0.2883499161661774
06/02/2019 01:21:41 step: 8260, epoch: 250, batch: 9, loss: 1.2670295238494873, acc: 46.875, f1: 17.18785600364548, r: 0.27548295409877593
06/02/2019 01:21:42 step: 8265, epoch: 250, batch: 14, loss: 1.319373607635498, acc: 50.0, f1: 25.49762329174094, r: 0.3189249126292543
06/02/2019 01:21:42 step: 8270, epoch: 250, batch: 19, loss: 1.1732596158981323, acc: 57.8125, f1: 23.905677655677653, r: 0.2788120716065251
06/02/2019 01:21:42 step: 8275, epoch: 250, batch: 24, loss: 1.3937057256698608, acc: 40.625, f1: 24.561609509344702, r: 0.2683021628220127
06/02/2019 01:21:42 step: 8280, epoch: 250, batch: 29, loss: 1.145482063293457, acc: 56.25, f1: 35.22727272727273, r: 0.35864579508658434
06/02/2019 01:21:42 *** evaluating ***
06/02/2019 01:21:42 step: 251, epoch: 250, acc: 55.98290598290598, f1: 17.009170653907496, r: 0.27990031124524445
06/02/2019 01:21:42 *** epoch: 252 ***
06/02/2019 01:21:42 *** training ***
06/02/2019 01:21:43 step: 8288, epoch: 251, batch: 4, loss: 1.4587876796722412, acc: 32.8125, f1: 18.838652482269502, r: 0.2768447711333828
06/02/2019 01:21:43 step: 8293, epoch: 251, batch: 9, loss: 1.301782488822937, acc: 50.0, f1: 23.23627125268712, r: 0.34489330414249914
06/02/2019 01:21:43 step: 8298, epoch: 251, batch: 14, loss: 1.2495958805084229, acc: 48.4375, f1: 20.063953488372093, r: 0.3200050763563451
06/02/2019 01:21:43 step: 8303, epoch: 251, batch: 19, loss: 1.2439666986465454, acc: 43.75, f1: 25.64240790655885, r: 0.31558702422926865
06/02/2019 01:21:44 step: 8308, epoch: 251, batch: 24, loss: 1.2164808511734009, acc: 57.8125, f1: 25.16208597603946, r: 0.2935011354158757
06/02/2019 01:21:44 step: 8313, epoch: 251, batch: 29, loss: 1.1249914169311523, acc: 51.5625, f1: 29.41281775868242, r: 0.31294829678464714
06/02/2019 01:21:44 *** evaluating ***
06/02/2019 01:21:44 step: 252, epoch: 251, acc: 55.98290598290598, f1: 17.174836601307188, r: 0.28552287134413107
06/02/2019 01:21:44 *** epoch: 253 ***
06/02/2019 01:21:44 *** training ***
06/02/2019 01:21:44 step: 8321, epoch: 252, batch: 4, loss: 1.4390465021133423, acc: 39.0625, f1: 20.650219298245613, r: 0.28217799415940714
06/02/2019 01:21:44 step: 8326, epoch: 252, batch: 9, loss: 1.3497815132141113, acc: 40.625, f1: 19.051394342092014, r: 0.279934495194707
06/02/2019 01:21:45 step: 8331, epoch: 252, batch: 14, loss: 1.1614415645599365, acc: 57.8125, f1: 39.02992776057791, r: 0.4422228991742765
06/02/2019 01:21:45 step: 8336, epoch: 252, batch: 19, loss: 1.2964775562286377, acc: 53.125, f1: 26.170327899051305, r: 0.3522018054054382
06/02/2019 01:21:45 step: 8341, epoch: 252, batch: 24, loss: 1.133223056793213, acc: 48.4375, f1: 21.988795518207287, r: 0.3242553736794097
06/02/2019 01:21:45 step: 8346, epoch: 252, batch: 29, loss: 1.308885097503662, acc: 45.3125, f1: 32.116192985758204, r: 0.26163061855068154
06/02/2019 01:21:45 *** evaluating ***
06/02/2019 01:21:46 step: 253, epoch: 252, acc: 54.27350427350427, f1: 16.234752521698333, r: 0.2860332891335146
06/02/2019 01:21:46 *** epoch: 254 ***
06/02/2019 01:21:46 *** training ***
06/02/2019 01:21:46 step: 8354, epoch: 253, batch: 4, loss: 1.4038747549057007, acc: 43.75, f1: 20.362620103473766, r: 0.29591349383712917
06/02/2019 01:21:46 step: 8359, epoch: 253, batch: 9, loss: 1.3478772640228271, acc: 43.75, f1: 20.918367346938773, r: 0.24701223470636294
06/02/2019 01:21:46 step: 8364, epoch: 253, batch: 14, loss: 1.264438509941101, acc: 42.1875, f1: 17.51270325203252, r: 0.3547719075009678
06/02/2019 01:21:46 step: 8369, epoch: 253, batch: 19, loss: 1.2127877473831177, acc: 48.4375, f1: 26.618075801749274, r: 0.2819352864651808
06/02/2019 01:21:47 step: 8374, epoch: 253, batch: 24, loss: 1.224433183670044, acc: 48.4375, f1: 23.23717948717949, r: 0.29500498389080515
06/02/2019 01:21:47 step: 8379, epoch: 253, batch: 29, loss: 1.1510847806930542, acc: 56.25, f1: 34.87873410233659, r: 0.3844615842020583
06/02/2019 01:21:47 *** evaluating ***
06/02/2019 01:21:47 step: 254, epoch: 253, acc: 55.98290598290598, f1: 17.15686274509804, r: 0.28329608181720733
06/02/2019 01:21:47 *** epoch: 255 ***
06/02/2019 01:21:47 *** training ***
06/02/2019 01:21:47 step: 8387, epoch: 254, batch: 4, loss: 1.1471128463745117, acc: 46.875, f1: 18.707482993197278, r: 0.3756020811427457
06/02/2019 01:21:48 step: 8392, epoch: 254, batch: 9, loss: 1.2689563035964966, acc: 46.875, f1: 20.532452181388354, r: 0.35255902819916113
06/02/2019 01:21:48 step: 8397, epoch: 254, batch: 14, loss: 1.4939711093902588, acc: 34.375, f1: 13.173076923076923, r: 0.2639782677876201
06/02/2019 01:21:48 step: 8402, epoch: 254, batch: 19, loss: 1.1203213930130005, acc: 48.4375, f1: 22.535300382794293, r: 0.3018328435267917
06/02/2019 01:21:48 step: 8407, epoch: 254, batch: 24, loss: 1.2672184705734253, acc: 46.875, f1: 20.079051383399214, r: 0.2045518213728314
06/02/2019 01:21:48 step: 8412, epoch: 254, batch: 29, loss: 1.3155484199523926, acc: 45.3125, f1: 18.185064935064933, r: 0.3016746914199497
06/02/2019 01:21:49 *** evaluating ***
06/02/2019 01:21:49 step: 255, epoch: 254, acc: 56.837606837606835, f1: 17.442156638450395, r: 0.2888783770307026
06/02/2019 01:21:49 *** epoch: 256 ***
06/02/2019 01:21:49 *** training ***
06/02/2019 01:21:49 step: 8420, epoch: 255, batch: 4, loss: 1.2752102613449097, acc: 48.4375, f1: 27.871148459383754, r: 0.3295337920021028
06/02/2019 01:21:49 step: 8425, epoch: 255, batch: 9, loss: 1.4224560260772705, acc: 39.0625, f1: 22.609368950832366, r: 0.3448137021393589
06/02/2019 01:21:49 step: 8430, epoch: 255, batch: 14, loss: 1.2352542877197266, acc: 50.0, f1: 25.33899923605806, r: 0.3417292065110932
06/02/2019 01:21:50 step: 8435, epoch: 255, batch: 19, loss: 1.3524359464645386, acc: 43.75, f1: 21.632848261255727, r: 0.30593351720699835
06/02/2019 01:21:50 step: 8440, epoch: 255, batch: 24, loss: 1.091347575187683, acc: 51.5625, f1: 28.74716802034548, r: 0.3625764072006591
06/02/2019 01:21:50 step: 8445, epoch: 255, batch: 29, loss: 1.2794336080551147, acc: 43.75, f1: 26.694733837590984, r: 0.3660397585887991
06/02/2019 01:21:50 *** evaluating ***
06/02/2019 01:21:50 step: 256, epoch: 255, acc: 55.12820512820513, f1: 16.753246753246753, r: 0.2890650328051205
06/02/2019 01:21:50 *** epoch: 257 ***
06/02/2019 01:21:50 *** training ***
06/02/2019 01:21:51 step: 8453, epoch: 256, batch: 4, loss: 1.1474521160125732, acc: 60.9375, f1: 29.420757967269594, r: 0.2992820639242838
06/02/2019 01:21:51 step: 8458, epoch: 256, batch: 9, loss: 1.056388020515442, acc: 50.0, f1: 38.115613395116505, r: 0.3060825873104113
06/02/2019 01:21:51 step: 8463, epoch: 256, batch: 14, loss: 1.246741771697998, acc: 46.875, f1: 24.147727272727273, r: 0.43131341330840123
06/02/2019 01:21:51 step: 8468, epoch: 256, batch: 19, loss: 1.1394301652908325, acc: 56.25, f1: 36.58067700620892, r: 0.3808317664369925
06/02/2019 01:21:52 step: 8473, epoch: 256, batch: 24, loss: 1.4593209028244019, acc: 40.625, f1: 24.803002360102653, r: 0.2910553388903187
06/02/2019 01:21:52 step: 8478, epoch: 256, batch: 29, loss: 0.9190622568130493, acc: 56.25, f1: 36.625677033551455, r: 0.4235779370237664
06/02/2019 01:21:52 *** evaluating ***
06/02/2019 01:21:52 step: 257, epoch: 256, acc: 56.41025641025641, f1: 17.31558020135071, r: 0.293607084224755
06/02/2019 01:21:52 *** epoch: 258 ***
06/02/2019 01:21:52 *** training ***
06/02/2019 01:21:52 step: 8486, epoch: 257, batch: 4, loss: 1.1283241510391235, acc: 59.375, f1: 28.163265306122447, r: 0.36994553827601684
06/02/2019 01:21:53 step: 8491, epoch: 257, batch: 9, loss: 1.287261962890625, acc: 51.5625, f1: 17.61979823455233, r: 0.2575881875957799
06/02/2019 01:21:53 step: 8496, epoch: 257, batch: 14, loss: 0.9802637100219727, acc: 60.9375, f1: 28.356928356928357, r: 0.3966225986234113
06/02/2019 01:21:53 step: 8501, epoch: 257, batch: 19, loss: 1.206466555595398, acc: 43.75, f1: 22.365242817187898, r: 0.3305168545096941
06/02/2019 01:21:53 step: 8506, epoch: 257, batch: 24, loss: 1.3635013103485107, acc: 39.0625, f1: 20.41223404255319, r: 0.3326731413170917
06/02/2019 01:21:54 step: 8511, epoch: 257, batch: 29, loss: 1.238208532333374, acc: 46.875, f1: 30.929919137466307, r: 0.35508588606045643
06/02/2019 01:21:54 *** evaluating ***
06/02/2019 01:21:54 step: 258, epoch: 257, acc: 53.84615384615385, f1: 16.14240547167376, r: 0.2899199248384293
06/02/2019 01:21:54 *** epoch: 259 ***
06/02/2019 01:21:54 *** training ***
06/02/2019 01:21:54 step: 8519, epoch: 258, batch: 4, loss: 1.317144513130188, acc: 50.0, f1: 29.738174339503242, r: 0.3835652907188278
06/02/2019 01:21:54 step: 8524, epoch: 258, batch: 9, loss: 1.205228328704834, acc: 46.875, f1: 19.91269841269841, r: 0.39011428067468246
06/02/2019 01:21:54 step: 8529, epoch: 258, batch: 14, loss: 1.2321091890335083, acc: 46.875, f1: 36.837584247389984, r: 0.3570497816510885
06/02/2019 01:21:55 step: 8534, epoch: 258, batch: 19, loss: 1.1632922887802124, acc: 54.6875, f1: 27.140096618357486, r: 0.30626813816525467
06/02/2019 01:21:55 step: 8539, epoch: 258, batch: 24, loss: 1.3276478052139282, acc: 40.625, f1: 14.521713938858847, r: 0.40477658603589056
06/02/2019 01:21:55 step: 8544, epoch: 258, batch: 29, loss: 1.2687183618545532, acc: 42.1875, f1: 15.367965367965366, r: 0.29971591817335125
06/02/2019 01:21:55 *** evaluating ***
06/02/2019 01:21:55 step: 259, epoch: 258, acc: 54.700854700854705, f1: 16.426508792624972, r: 0.29074686533037447
06/02/2019 01:21:55 *** epoch: 260 ***
06/02/2019 01:21:55 *** training ***
06/02/2019 01:21:56 step: 8552, epoch: 259, batch: 4, loss: 1.2091002464294434, acc: 50.0, f1: 28.759926803702456, r: 0.2317276640813811
06/02/2019 01:21:56 step: 8557, epoch: 259, batch: 9, loss: 1.1965359449386597, acc: 51.5625, f1: 20.471554739920858, r: 0.3092692387561943
06/02/2019 01:21:56 step: 8562, epoch: 259, batch: 14, loss: 1.1230671405792236, acc: 51.5625, f1: 22.308488612836438, r: 0.31998690756320464
06/02/2019 01:21:56 step: 8567, epoch: 259, batch: 19, loss: 1.1437381505966187, acc: 46.875, f1: 25.60556789765715, r: 0.3432598992921374
06/02/2019 01:21:56 step: 8572, epoch: 259, batch: 24, loss: 1.463984727859497, acc: 35.9375, f1: 18.250915750915752, r: 0.33369639138693724
06/02/2019 01:21:57 step: 8577, epoch: 259, batch: 29, loss: 1.2744503021240234, acc: 46.875, f1: 28.328267477203646, r: 0.2987035925456431
06/02/2019 01:21:57 *** evaluating ***
06/02/2019 01:21:57 step: 260, epoch: 259, acc: 53.84615384615385, f1: 16.30675772242037, r: 0.28282099816722295
06/02/2019 01:21:57 *** epoch: 261 ***
06/02/2019 01:21:57 *** training ***
06/02/2019 01:21:57 step: 8585, epoch: 260, batch: 4, loss: 1.1014844179153442, acc: 57.8125, f1: 38.32988267770876, r: 0.3376330375377065
06/02/2019 01:21:57 step: 8590, epoch: 260, batch: 9, loss: 1.2870198488235474, acc: 50.0, f1: 24.925273139558858, r: 0.35001466805411785
06/02/2019 01:21:58 step: 8595, epoch: 260, batch: 14, loss: 1.396217942237854, acc: 39.0625, f1: 17.636729405346426, r: 0.3100974270314177
06/02/2019 01:21:58 step: 8600, epoch: 260, batch: 19, loss: 1.160528540611267, acc: 48.4375, f1: 21.122448979591837, r: 0.27648055279651285
06/02/2019 01:21:58 step: 8605, epoch: 260, batch: 24, loss: 1.3060990571975708, acc: 40.625, f1: 22.863475177304966, r: 0.31373785538257537
06/02/2019 01:21:58 step: 8610, epoch: 260, batch: 29, loss: 1.1916930675506592, acc: 46.875, f1: 33.69417332150251, r: 0.34919078835343625
06/02/2019 01:21:58 *** evaluating ***
06/02/2019 01:21:59 step: 261, epoch: 260, acc: 54.700854700854705, f1: 16.68848624493786, r: 0.28875000861768485
06/02/2019 01:21:59 *** epoch: 262 ***
06/02/2019 01:21:59 *** training ***
06/02/2019 01:21:59 step: 8618, epoch: 261, batch: 4, loss: 1.2781246900558472, acc: 45.3125, f1: 24.65686274509804, r: 0.3897240976558938
06/02/2019 01:21:59 step: 8623, epoch: 261, batch: 9, loss: 1.2455946207046509, acc: 51.5625, f1: 34.13690476190476, r: 0.3597786677231719
06/02/2019 01:21:59 step: 8628, epoch: 261, batch: 14, loss: 1.2496178150177002, acc: 53.125, f1: 34.51923076923077, r: 0.40185135787668247
06/02/2019 01:21:59 step: 8633, epoch: 261, batch: 19, loss: 1.2404167652130127, acc: 50.0, f1: 28.462196048632215, r: 0.3909414655431194
06/02/2019 01:22:00 step: 8638, epoch: 261, batch: 24, loss: 1.2209174633026123, acc: 56.25, f1: 24.880682431702844, r: 0.3541621271023884
06/02/2019 01:22:00 step: 8643, epoch: 261, batch: 29, loss: 1.1469274759292603, acc: 53.125, f1: 33.64252723210669, r: 0.38478721774226504
06/02/2019 01:22:00 *** evaluating ***
06/02/2019 01:22:00 step: 262, epoch: 261, acc: 58.54700854700855, f1: 18.217243006072113, r: 0.2855231156916427
06/02/2019 01:22:00 *** epoch: 263 ***
06/02/2019 01:22:00 *** training ***
06/02/2019 01:22:00 step: 8651, epoch: 262, batch: 4, loss: 1.1744128465652466, acc: 51.5625, f1: 26.11510800865601, r: 0.2968196425163412
06/02/2019 01:22:01 step: 8656, epoch: 262, batch: 9, loss: 1.333385944366455, acc: 39.0625, f1: 18.245192307692307, r: 0.2986117375828339
06/02/2019 01:22:01 step: 8661, epoch: 262, batch: 14, loss: 1.3089392185211182, acc: 42.1875, f1: 38.84575569358178, r: 0.4029726851720468
06/02/2019 01:22:01 step: 8666, epoch: 262, batch: 19, loss: 1.1323100328445435, acc: 51.5625, f1: 32.77777777777777, r: 0.2957776774425769
06/02/2019 01:22:01 step: 8671, epoch: 262, batch: 24, loss: 1.4434560537338257, acc: 43.75, f1: 19.068699241113034, r: 0.26985258449852517
06/02/2019 01:22:02 step: 8676, epoch: 262, batch: 29, loss: 1.1592148542404175, acc: 53.125, f1: 29.23502853735412, r: 0.3482817357530436
06/02/2019 01:22:02 *** evaluating ***
06/02/2019 01:22:02 step: 263, epoch: 262, acc: 56.837606837606835, f1: 17.45879120879121, r: 0.28400222420138915
06/02/2019 01:22:02 *** epoch: 264 ***
06/02/2019 01:22:02 *** training ***
06/02/2019 01:22:02 step: 8684, epoch: 263, batch: 4, loss: 0.925429105758667, acc: 67.1875, f1: 40.03391345496609, r: 0.3895175469437734
06/02/2019 01:22:02 step: 8689, epoch: 263, batch: 9, loss: 1.0362234115600586, acc: 56.25, f1: 22.69320843091335, r: 0.4447526979449804
06/02/2019 01:22:02 step: 8694, epoch: 263, batch: 14, loss: 1.2913471460342407, acc: 48.4375, f1: 21.51521073545697, r: 0.31250566581749895
06/02/2019 01:22:03 step: 8699, epoch: 263, batch: 19, loss: 1.2857600450515747, acc: 45.3125, f1: 18.363973552652794, r: 0.2775336788540992
06/02/2019 01:22:03 step: 8704, epoch: 263, batch: 24, loss: 1.0332739353179932, acc: 51.5625, f1: 34.99898461298133, r: 0.3793223920086872
06/02/2019 01:22:03 step: 8709, epoch: 263, batch: 29, loss: 1.1511527299880981, acc: 56.25, f1: 53.06713989943803, r: 0.3581308025920423
06/02/2019 01:22:03 *** evaluating ***
06/02/2019 01:22:03 step: 264, epoch: 263, acc: 58.119658119658126, f1: 18.003238591473885, r: 0.2826835028569512
06/02/2019 01:22:03 *** epoch: 265 ***
06/02/2019 01:22:03 *** training ***
06/02/2019 01:22:04 step: 8717, epoch: 264, batch: 4, loss: 1.2953112125396729, acc: 42.1875, f1: 26.010118333047505, r: 0.4145872397036829
06/02/2019 01:22:04 step: 8722, epoch: 264, batch: 9, loss: 1.1110237836837769, acc: 62.5, f1: 24.90192873488068, r: 0.366628358071998
06/02/2019 01:22:04 step: 8727, epoch: 264, batch: 14, loss: 1.173259973526001, acc: 46.875, f1: 24.3577694235589, r: 0.3261428377172729
06/02/2019 01:22:04 step: 8732, epoch: 264, batch: 19, loss: 1.1825952529907227, acc: 56.25, f1: 38.125, r: 0.3168618943710265
06/02/2019 01:22:04 step: 8737, epoch: 264, batch: 24, loss: 1.3183960914611816, acc: 39.0625, f1: 19.511470985155196, r: 0.19735928569202243
06/02/2019 01:22:05 step: 8742, epoch: 264, batch: 29, loss: 1.2125039100646973, acc: 51.5625, f1: 27.548519407763106, r: 0.30653116088710225
06/02/2019 01:22:05 *** evaluating ***
06/02/2019 01:22:05 step: 265, epoch: 264, acc: 56.837606837606835, f1: 17.45879120879121, r: 0.2859462818017633
06/02/2019 01:22:05 *** epoch: 266 ***
06/02/2019 01:22:05 *** training ***
06/02/2019 01:22:05 step: 8750, epoch: 265, batch: 4, loss: 1.4214141368865967, acc: 39.0625, f1: 17.232602500459645, r: 0.3038999211264089
06/02/2019 01:22:05 step: 8755, epoch: 265, batch: 9, loss: 1.2163172960281372, acc: 50.0, f1: 19.337606837606835, r: 0.3861441496798139
06/02/2019 01:22:06 step: 8760, epoch: 265, batch: 14, loss: 1.3056352138519287, acc: 42.1875, f1: 31.829004329004327, r: 0.3920695500541132
06/02/2019 01:22:06 step: 8765, epoch: 265, batch: 19, loss: 1.1501493453979492, acc: 62.5, f1: 46.92439986820005, r: 0.3853311864008368
06/02/2019 01:22:06 step: 8770, epoch: 265, batch: 24, loss: 1.3082860708236694, acc: 45.3125, f1: 23.96815869204825, r: 0.2559965987963357
06/02/2019 01:22:06 step: 8775, epoch: 265, batch: 29, loss: 1.248909831047058, acc: 50.0, f1: 23.559782608695652, r: 0.35264159714753995
06/02/2019 01:22:07 *** evaluating ***
06/02/2019 01:22:07 step: 266, epoch: 265, acc: 54.700854700854705, f1: 16.363211951447244, r: 0.2856859026659256
06/02/2019 01:22:07 *** epoch: 267 ***
06/02/2019 01:22:07 *** training ***
06/02/2019 01:22:07 step: 8783, epoch: 266, batch: 4, loss: 1.2922250032424927, acc: 43.75, f1: 24.526397515527947, r: 0.3380967714433587
06/02/2019 01:22:07 step: 8788, epoch: 266, batch: 9, loss: 1.0689747333526611, acc: 50.0, f1: 20.841063698206554, r: 0.35345885361075274
06/02/2019 01:22:07 step: 8793, epoch: 266, batch: 14, loss: 1.2028565406799316, acc: 53.125, f1: 36.14494893564662, r: 0.332927162338647
06/02/2019 01:22:08 step: 8798, epoch: 266, batch: 19, loss: 1.0538305044174194, acc: 64.0625, f1: 41.450447700447704, r: 0.3247996825470607
06/02/2019 01:22:08 step: 8803, epoch: 266, batch: 24, loss: 1.1209827661514282, acc: 56.25, f1: 32.3191094619666, r: 0.39014577480922963
06/02/2019 01:22:08 step: 8808, epoch: 266, batch: 29, loss: 1.1744078397750854, acc: 57.8125, f1: 32.15527823240589, r: 0.41624430997534834
06/02/2019 01:22:08 *** evaluating ***
06/02/2019 01:22:08 step: 267, epoch: 266, acc: 57.26495726495726, f1: 17.686891123061336, r: 0.2886147584408041
06/02/2019 01:22:08 *** epoch: 268 ***
06/02/2019 01:22:08 *** training ***
06/02/2019 01:22:09 step: 8816, epoch: 267, batch: 4, loss: 1.2889231443405151, acc: 45.3125, f1: 25.278973377969077, r: 0.23101977763317966
06/02/2019 01:22:09 step: 8821, epoch: 267, batch: 9, loss: 1.3032732009887695, acc: 37.5, f1: 18.91156462585034, r: 0.30333489504965
06/02/2019 01:22:09 step: 8826, epoch: 267, batch: 14, loss: 1.0822744369506836, acc: 50.0, f1: 27.579719387755098, r: 0.37857542963970164
06/02/2019 01:22:09 step: 8831, epoch: 267, batch: 19, loss: 1.1285792589187622, acc: 48.4375, f1: 27.93222552178766, r: 0.36214554854851266
06/02/2019 01:22:09 step: 8836, epoch: 267, batch: 24, loss: 1.1708966493606567, acc: 53.125, f1: 27.617666021921337, r: 0.396873193813141
06/02/2019 01:22:10 step: 8841, epoch: 267, batch: 29, loss: 1.0839545726776123, acc: 48.4375, f1: 21.634066681236494, r: 0.33676108714134106
06/02/2019 01:22:10 *** evaluating ***
06/02/2019 01:22:10 step: 268, epoch: 267, acc: 56.41025641025641, f1: 17.194724605170535, r: 0.2846340746136462
06/02/2019 01:22:10 *** epoch: 269 ***
06/02/2019 01:22:10 *** training ***
06/02/2019 01:22:10 step: 8849, epoch: 268, batch: 4, loss: 1.2270302772521973, acc: 48.4375, f1: 25.329280648429588, r: 0.334894146841333
06/02/2019 01:22:10 step: 8854, epoch: 268, batch: 9, loss: 1.1509389877319336, acc: 51.5625, f1: 36.872396712822244, r: 0.32387156511113135
06/02/2019 01:22:11 step: 8859, epoch: 268, batch: 14, loss: 1.1890310049057007, acc: 48.4375, f1: 26.029337083479824, r: 0.3343035515447963
06/02/2019 01:22:11 step: 8864, epoch: 268, batch: 19, loss: 1.1667065620422363, acc: 48.4375, f1: 22.254376058723885, r: 0.2446947014963876
06/02/2019 01:22:11 step: 8869, epoch: 268, batch: 24, loss: 1.1083604097366333, acc: 59.375, f1: 32.14317083882302, r: 0.3567240795480191
06/02/2019 01:22:11 step: 8874, epoch: 268, batch: 29, loss: 1.2024893760681152, acc: 51.5625, f1: 35.0359911406423, r: 0.3522708838982029
06/02/2019 01:22:11 *** evaluating ***
06/02/2019 01:22:11 step: 269, epoch: 268, acc: 55.55555555555556, f1: 16.91534569104495, r: 0.2861123197418063
06/02/2019 01:22:11 *** epoch: 270 ***
06/02/2019 01:22:11 *** training ***
06/02/2019 01:22:12 step: 8882, epoch: 269, batch: 4, loss: 1.3394463062286377, acc: 35.9375, f1: 19.09313725490196, r: 0.33598959628562225
06/02/2019 01:22:12 step: 8887, epoch: 269, batch: 9, loss: 1.1827117204666138, acc: 50.0, f1: 20.246400885935767, r: 0.2499810933661669
06/02/2019 01:22:12 step: 8892, epoch: 269, batch: 14, loss: 1.342897891998291, acc: 50.0, f1: 24.85974133349082, r: 0.2991985556535381
06/02/2019 01:22:12 step: 8897, epoch: 269, batch: 19, loss: 1.1923549175262451, acc: 51.5625, f1: 30.67092918782256, r: 0.25208244537321856
06/02/2019 01:22:13 step: 8902, epoch: 269, batch: 24, loss: 1.2033429145812988, acc: 53.125, f1: 25.324675324675322, r: 0.30438017657206456
06/02/2019 01:22:13 step: 8907, epoch: 269, batch: 29, loss: 1.3405214548110962, acc: 42.1875, f1: 21.725966982537695, r: 0.29049968477045107
06/02/2019 01:22:13 *** evaluating ***
06/02/2019 01:22:13 step: 270, epoch: 269, acc: 57.26495726495726, f1: 17.708114085258902, r: 0.2885647951968602
06/02/2019 01:22:13 *** epoch: 271 ***
06/02/2019 01:22:13 *** training ***
06/02/2019 01:22:13 step: 8915, epoch: 270, batch: 4, loss: 1.2333476543426514, acc: 48.4375, f1: 21.614934345831355, r: 0.3194631314295612
06/02/2019 01:22:13 step: 8920, epoch: 270, batch: 9, loss: 1.3184340000152588, acc: 46.875, f1: 18.206099456099455, r: 0.38553172956438675
06/02/2019 01:22:14 step: 8925, epoch: 270, batch: 14, loss: 1.1803662776947021, acc: 50.0, f1: 29.555555555555557, r: 0.31227956753935315
06/02/2019 01:22:14 step: 8930, epoch: 270, batch: 19, loss: 1.166976809501648, acc: 54.6875, f1: 20.740740740740744, r: 0.30295501753943477
06/02/2019 01:22:14 step: 8935, epoch: 270, batch: 24, loss: 1.1011654138565063, acc: 57.8125, f1: 28.122109158186863, r: 0.4262850510284109
06/02/2019 01:22:14 step: 8940, epoch: 270, batch: 29, loss: 1.0153425931930542, acc: 57.8125, f1: 47.399455586963995, r: 0.38237730656034574
06/02/2019 01:22:14 *** evaluating ***
06/02/2019 01:22:15 step: 271, epoch: 270, acc: 55.55555555555556, f1: 16.91534569104495, r: 0.2816240835872461
06/02/2019 01:22:15 *** epoch: 272 ***
06/02/2019 01:22:15 *** training ***
06/02/2019 01:22:15 step: 8948, epoch: 271, batch: 4, loss: 1.0700676441192627, acc: 62.5, f1: 33.33108715184187, r: 0.3141439590784357
06/02/2019 01:22:15 step: 8953, epoch: 271, batch: 9, loss: 1.0824861526489258, acc: 53.125, f1: 28.801169590643276, r: 0.2753940596145593
06/02/2019 01:22:15 step: 8958, epoch: 271, batch: 14, loss: 1.2655854225158691, acc: 45.3125, f1: 27.89322250639386, r: 0.31798433753766947
06/02/2019 01:22:15 step: 8963, epoch: 271, batch: 19, loss: 1.27227783203125, acc: 48.4375, f1: 30.19607843137255, r: 0.2696161698337092
06/02/2019 01:22:16 step: 8968, epoch: 271, batch: 24, loss: 1.407938003540039, acc: 45.3125, f1: 23.555555555555554, r: 0.343729850911291
06/02/2019 01:22:16 step: 8973, epoch: 271, batch: 29, loss: 1.1831824779510498, acc: 50.0, f1: 26.597234638536264, r: 0.2974857069798927
06/02/2019 01:22:16 *** evaluating ***
06/02/2019 01:22:16 step: 272, epoch: 271, acc: 55.98290598290598, f1: 17.047116433511945, r: 0.2824910697851225
06/02/2019 01:22:16 *** epoch: 273 ***
06/02/2019 01:22:16 *** training ***
06/02/2019 01:22:16 step: 8981, epoch: 272, batch: 4, loss: 1.0982444286346436, acc: 57.8125, f1: 33.67918719211823, r: 0.33843409785851364
06/02/2019 01:22:17 step: 8986, epoch: 272, batch: 9, loss: 1.1961321830749512, acc: 51.5625, f1: 30.726423902894496, r: 0.3222609784438578
06/02/2019 01:22:17 step: 8991, epoch: 272, batch: 14, loss: 1.239912986755371, acc: 46.875, f1: 30.04201680672269, r: 0.3238162774021104
06/02/2019 01:22:17 step: 8996, epoch: 272, batch: 19, loss: 1.077022671699524, acc: 56.25, f1: 33.34693877551021, r: 0.44581901744064933
06/02/2019 01:22:18 step: 9001, epoch: 272, batch: 24, loss: 0.9937006831169128, acc: 56.25, f1: 22.166982323232325, r: 0.35597290903015083
06/02/2019 01:22:18 step: 9006, epoch: 272, batch: 29, loss: 1.1167534589767456, acc: 48.4375, f1: 28.987882761467667, r: 0.28492886864990613
06/02/2019 01:22:18 *** evaluating ***
06/02/2019 01:22:18 step: 273, epoch: 272, acc: 55.98290598290598, f1: 17.24366119731288, r: 0.28247670060184193
06/02/2019 01:22:18 *** epoch: 274 ***
06/02/2019 01:22:18 *** training ***
06/02/2019 01:22:18 step: 9014, epoch: 273, batch: 4, loss: 1.0032892227172852, acc: 54.6875, f1: 46.09609609609609, r: 0.37528925151324993
06/02/2019 01:22:19 step: 9019, epoch: 273, batch: 9, loss: 1.053939938545227, acc: 54.6875, f1: 29.272579393644772, r: 0.35796890194244707
06/02/2019 01:22:19 step: 9024, epoch: 273, batch: 14, loss: 1.112451195716858, acc: 54.6875, f1: 33.53454172366621, r: 0.393260544691751
06/02/2019 01:22:19 step: 9029, epoch: 273, batch: 19, loss: 1.118909478187561, acc: 59.375, f1: 29.363017934446507, r: 0.3593689177924735
06/02/2019 01:22:19 step: 9034, epoch: 273, batch: 24, loss: 1.2364274263381958, acc: 46.875, f1: 31.01125338296803, r: 0.2509263545600166
06/02/2019 01:22:19 step: 9039, epoch: 273, batch: 29, loss: 1.3820375204086304, acc: 45.3125, f1: 23.94782913165266, r: 0.3260147119496952
06/02/2019 01:22:20 *** evaluating ***
06/02/2019 01:22:20 step: 274, epoch: 273, acc: 54.27350427350427, f1: 16.320432711942146, r: 0.2894127633192562
06/02/2019 01:22:20 *** epoch: 275 ***
06/02/2019 01:22:20 *** training ***
06/02/2019 01:22:20 step: 9047, epoch: 274, batch: 4, loss: 1.3105381727218628, acc: 51.5625, f1: 21.505102040816325, r: 0.362095715340764
06/02/2019 01:22:20 step: 9052, epoch: 274, batch: 9, loss: 1.3161014318466187, acc: 34.375, f1: 21.12863327149041, r: 0.3678152403181507
06/02/2019 01:22:20 step: 9057, epoch: 274, batch: 14, loss: 1.3576656579971313, acc: 42.1875, f1: 23.04319793681496, r: 0.2838368950789382
06/02/2019 01:22:21 step: 9062, epoch: 274, batch: 19, loss: 1.1103861331939697, acc: 56.25, f1: 28.37837837837838, r: 0.34596735820335406
06/02/2019 01:22:21 step: 9067, epoch: 274, batch: 24, loss: 1.1165745258331299, acc: 51.5625, f1: 31.663290316126453, r: 0.36799714950850715
06/02/2019 01:22:21 step: 9072, epoch: 274, batch: 29, loss: 1.239512324333191, acc: 56.25, f1: 36.22009569377991, r: 0.41208987962947535
06/02/2019 01:22:21 *** evaluating ***
06/02/2019 01:22:21 step: 275, epoch: 274, acc: 54.27350427350427, f1: 16.294570253766608, r: 0.2933933674453921
06/02/2019 01:22:21 *** epoch: 276 ***
06/02/2019 01:22:21 *** training ***
06/02/2019 01:22:22 step: 9080, epoch: 275, batch: 4, loss: 1.289165735244751, acc: 43.75, f1: 25.838016860698517, r: 0.2719185861476312
06/02/2019 01:22:22 step: 9085, epoch: 275, batch: 9, loss: 1.1500828266143799, acc: 51.5625, f1: 27.440582673140813, r: 0.33604873130772206
06/02/2019 01:22:22 step: 9090, epoch: 275, batch: 14, loss: 1.1860963106155396, acc: 56.25, f1: 28.000000000000004, r: 0.34939637415904595
06/02/2019 01:22:22 step: 9095, epoch: 275, batch: 19, loss: 1.1470531225204468, acc: 51.5625, f1: 16.10377358490566, r: 0.2798966215448174
06/02/2019 01:22:22 step: 9100, epoch: 275, batch: 24, loss: 1.1862229108810425, acc: 53.125, f1: 33.91956782713085, r: 0.38585411617381227
06/02/2019 01:22:23 step: 9105, epoch: 275, batch: 29, loss: 1.127034306526184, acc: 51.5625, f1: 35.43020297549915, r: 0.37374613004841545
06/02/2019 01:22:23 *** evaluating ***
06/02/2019 01:22:23 step: 276, epoch: 275, acc: 54.27350427350427, f1: 16.366684958234252, r: 0.2947838528289994
06/02/2019 01:22:23 *** epoch: 277 ***
06/02/2019 01:22:23 *** training ***
06/02/2019 01:22:23 step: 9113, epoch: 276, batch: 4, loss: 0.9212541580200195, acc: 67.1875, f1: 28.638273491214672, r: 0.3690726563527226
06/02/2019 01:22:23 step: 9118, epoch: 276, batch: 9, loss: 1.0583763122558594, acc: 59.375, f1: 26.929499072356215, r: 0.3865381248549207
06/02/2019 01:22:24 step: 9123, epoch: 276, batch: 14, loss: 1.3309704065322876, acc: 45.3125, f1: 27.495567375886527, r: 0.297106523766609
06/02/2019 01:22:24 step: 9128, epoch: 276, batch: 19, loss: 1.104561686515808, acc: 48.4375, f1: 19.881965944272448, r: 0.2962322122541085
06/02/2019 01:22:24 step: 9133, epoch: 276, batch: 24, loss: 1.414469838142395, acc: 39.0625, f1: 20.0973165679048, r: 0.32454167278269413
06/02/2019 01:22:24 step: 9138, epoch: 276, batch: 29, loss: 1.1251397132873535, acc: 56.25, f1: 26.833062770562766, r: 0.29296883641543064
06/02/2019 01:22:24 *** evaluating ***
06/02/2019 01:22:25 step: 277, epoch: 276, acc: 56.837606837606835, f1: 17.412165848336063, r: 0.2909813446904642
06/02/2019 01:22:25 *** epoch: 278 ***
06/02/2019 01:22:25 *** training ***
06/02/2019 01:22:25 step: 9146, epoch: 277, batch: 4, loss: 1.3032259941101074, acc: 51.5625, f1: 21.0188261351052, r: 0.18509533079333443
06/02/2019 01:22:25 step: 9151, epoch: 277, batch: 9, loss: 1.1871610879898071, acc: 50.0, f1: 29.068152454780368, r: 0.33794107755358055
06/02/2019 01:22:25 step: 9156, epoch: 277, batch: 14, loss: 1.0285120010375977, acc: 59.375, f1: 26.821163247907858, r: 0.41079553499277616
06/02/2019 01:22:26 step: 9161, epoch: 277, batch: 19, loss: 1.1647965908050537, acc: 46.875, f1: 24.950128700128698, r: 0.37934381183720034
06/02/2019 01:22:26 step: 9166, epoch: 277, batch: 24, loss: 1.1902236938476562, acc: 54.6875, f1: 22.818538647342997, r: 0.3466783363166433
06/02/2019 01:22:26 step: 9171, epoch: 277, batch: 29, loss: 1.019086480140686, acc: 60.9375, f1: 23.925908667287977, r: 0.2966208092809696
06/02/2019 01:22:26 *** evaluating ***
06/02/2019 01:22:26 step: 278, epoch: 277, acc: 53.84615384615385, f1: 16.159610181304647, r: 0.29017959123940407
06/02/2019 01:22:26 *** epoch: 279 ***
06/02/2019 01:22:26 *** training ***
06/02/2019 01:22:26 step: 9179, epoch: 278, batch: 4, loss: 1.1238089799880981, acc: 53.125, f1: 25.3901895206243, r: 0.27185232036791246
06/02/2019 01:22:27 step: 9184, epoch: 278, batch: 9, loss: 1.066549301147461, acc: 59.375, f1: 30.510759657765096, r: 0.33095881243484154
06/02/2019 01:22:27 step: 9189, epoch: 278, batch: 14, loss: 1.228088617324829, acc: 45.3125, f1: 18.268258536205956, r: 0.31595794639780955
06/02/2019 01:22:27 step: 9194, epoch: 278, batch: 19, loss: 1.2986938953399658, acc: 46.875, f1: 32.14285714285714, r: 0.3693973726713689
06/02/2019 01:22:27 step: 9199, epoch: 278, batch: 24, loss: 1.1177226305007935, acc: 54.6875, f1: 39.61279461279461, r: 0.3228014011477926
06/02/2019 01:22:28 step: 9204, epoch: 278, batch: 29, loss: 1.1260613203048706, acc: 46.875, f1: 20.133547008547012, r: 0.3368909050515222
06/02/2019 01:22:28 *** evaluating ***
06/02/2019 01:22:28 step: 279, epoch: 278, acc: 55.55555555555556, f1: 16.866692520607117, r: 0.29578406557315073
06/02/2019 01:22:28 *** epoch: 280 ***
06/02/2019 01:22:28 *** training ***
06/02/2019 01:22:28 step: 9212, epoch: 279, batch: 4, loss: 1.2194232940673828, acc: 57.8125, f1: 40.05575137650609, r: 0.42219806936030635
06/02/2019 01:22:28 step: 9217, epoch: 279, batch: 9, loss: 1.1056573390960693, acc: 51.5625, f1: 32.85714285714286, r: 0.3073274665837199
06/02/2019 01:22:28 step: 9222, epoch: 279, batch: 14, loss: 1.4039216041564941, acc: 39.0625, f1: 21.259309925520597, r: 0.36086061892517235
06/02/2019 01:22:29 step: 9227, epoch: 279, batch: 19, loss: 1.1355347633361816, acc: 54.6875, f1: 24.090839343256096, r: 0.31842639963570785
06/02/2019 01:22:29 step: 9232, epoch: 279, batch: 24, loss: 1.2473288774490356, acc: 50.0, f1: 26.90699468193074, r: 0.24893368513601374
06/02/2019 01:22:29 step: 9237, epoch: 279, batch: 29, loss: 1.0567386150360107, acc: 59.375, f1: 27.19771779679614, r: 0.3650376946982866
06/02/2019 01:22:29 *** evaluating ***
06/02/2019 01:22:29 step: 280, epoch: 279, acc: 54.700854700854705, f1: 16.515343707704762, r: 0.29200360776826734
06/02/2019 01:22:29 *** epoch: 281 ***
06/02/2019 01:22:29 *** training ***
06/02/2019 01:22:30 step: 9245, epoch: 280, batch: 4, loss: 1.276350736618042, acc: 40.625, f1: 16.825131390348783, r: 0.27645111365515124
06/02/2019 01:22:30 step: 9250, epoch: 280, batch: 9, loss: 1.2992933988571167, acc: 42.1875, f1: 16.76829268292683, r: 0.28257846422507227
06/02/2019 01:22:30 step: 9255, epoch: 280, batch: 14, loss: 1.1202116012573242, acc: 54.6875, f1: 23.187339970110887, r: 0.33654282998810503
06/02/2019 01:22:30 step: 9260, epoch: 280, batch: 19, loss: 1.2709252834320068, acc: 40.625, f1: 24.38163035082162, r: 0.32398153254405965
06/02/2019 01:22:31 step: 9265, epoch: 280, batch: 24, loss: 1.211148738861084, acc: 48.4375, f1: 17.881944444444443, r: 0.2970405096917696
06/02/2019 01:22:31 step: 9270, epoch: 280, batch: 29, loss: 1.2660905122756958, acc: 48.4375, f1: 31.035859465737513, r: 0.3731858384906188
06/02/2019 01:22:31 *** evaluating ***
06/02/2019 01:22:31 step: 281, epoch: 280, acc: 55.98290598290598, f1: 17.180312821702934, r: 0.28649168886268067
06/02/2019 01:22:31 *** epoch: 282 ***
06/02/2019 01:22:31 *** training ***
06/02/2019 01:22:31 step: 9278, epoch: 281, batch: 4, loss: 1.147960901260376, acc: 48.4375, f1: 19.878706199460918, r: 0.3188720333724292
06/02/2019 01:22:31 step: 9283, epoch: 281, batch: 9, loss: 1.135983943939209, acc: 51.5625, f1: 24.432464809823305, r: 0.36367718382957703
06/02/2019 01:22:32 step: 9288, epoch: 281, batch: 14, loss: 1.0092374086380005, acc: 54.6875, f1: 28.74985271591846, r: 0.3178779047723978
06/02/2019 01:22:32 step: 9293, epoch: 281, batch: 19, loss: 1.173597812652588, acc: 51.5625, f1: 22.212073640645066, r: 0.322758615376605
06/02/2019 01:22:32 step: 9298, epoch: 281, batch: 24, loss: 1.122607707977295, acc: 57.8125, f1: 30.940454893943265, r: 0.31997006943032924
06/02/2019 01:22:32 step: 9303, epoch: 281, batch: 29, loss: 1.2639185190200806, acc: 48.4375, f1: 32.05799491513777, r: 0.3281113768477095
06/02/2019 01:22:33 *** evaluating ***
06/02/2019 01:22:33 step: 282, epoch: 281, acc: 57.692307692307686, f1: 18.085385643375332, r: 0.2852690764434728
06/02/2019 01:22:33 *** epoch: 283 ***
06/02/2019 01:22:33 *** training ***
06/02/2019 01:22:33 step: 9311, epoch: 282, batch: 4, loss: 1.1395862102508545, acc: 56.25, f1: 34.56722847959377, r: 0.41838325287288824
06/02/2019 01:22:33 step: 9316, epoch: 282, batch: 9, loss: 1.1107733249664307, acc: 51.5625, f1: 34.531658817373106, r: 0.38499633957021884
06/02/2019 01:22:33 step: 9321, epoch: 282, batch: 14, loss: 1.0506315231323242, acc: 54.6875, f1: 26.147898206721738, r: 0.2969064927564352
06/02/2019 01:22:34 step: 9326, epoch: 282, batch: 19, loss: 1.0244516134262085, acc: 57.8125, f1: 31.08333333333333, r: 0.3416093685638195
06/02/2019 01:22:34 step: 9331, epoch: 282, batch: 24, loss: 1.1232478618621826, acc: 56.25, f1: 31.0108604845447, r: 0.3741948248395704
06/02/2019 01:22:34 step: 9336, epoch: 282, batch: 29, loss: 1.3010896444320679, acc: 45.3125, f1: 27.53968253968254, r: 0.2638031962499526
06/02/2019 01:22:34 *** evaluating ***
06/02/2019 01:22:34 step: 283, epoch: 282, acc: 58.119658119658126, f1: 18.101255122480104, r: 0.2807383150340092
06/02/2019 01:22:34 *** epoch: 284 ***
06/02/2019 01:22:34 *** training ***
06/02/2019 01:22:35 step: 9344, epoch: 283, batch: 4, loss: 1.190733790397644, acc: 53.125, f1: 23.82644309868029, r: 0.286489039693664
06/02/2019 01:22:35 step: 9349, epoch: 283, batch: 9, loss: 1.36313796043396, acc: 42.1875, f1: 18.510036910323855, r: 0.32954273102418025
06/02/2019 01:22:35 step: 9354, epoch: 283, batch: 14, loss: 1.1597470045089722, acc: 51.5625, f1: 22.53401360544218, r: 0.3758698607363019
06/02/2019 01:22:35 step: 9359, epoch: 283, batch: 19, loss: 1.0661089420318604, acc: 59.375, f1: 27.713417281806347, r: 0.34826114614704684
06/02/2019 01:22:36 step: 9364, epoch: 283, batch: 24, loss: 1.196513295173645, acc: 50.0, f1: 19.51121794871795, r: 0.3470186006203557
06/02/2019 01:22:36 step: 9369, epoch: 283, batch: 29, loss: 1.038955569267273, acc: 56.25, f1: 33.79454926624738, r: 0.39407788882330147
06/02/2019 01:22:36 *** evaluating ***
06/02/2019 01:22:36 step: 284, epoch: 283, acc: 54.700854700854705, f1: 16.689424200429336, r: 0.2855848204707354
06/02/2019 01:22:36 *** epoch: 285 ***
06/02/2019 01:22:36 *** training ***
06/02/2019 01:22:36 step: 9377, epoch: 284, batch: 4, loss: 1.1858047246932983, acc: 51.5625, f1: 32.655141843971634, r: 0.3747242161316635
06/02/2019 01:22:37 step: 9382, epoch: 284, batch: 9, loss: 1.0748313665390015, acc: 56.25, f1: 24.697332421340633, r: 0.3175692318723437
06/02/2019 01:22:37 step: 9387, epoch: 284, batch: 14, loss: 1.1801657676696777, acc: 43.75, f1: 33.391941391941394, r: 0.3307861631007018
06/02/2019 01:22:37 step: 9392, epoch: 284, batch: 19, loss: 1.3196712732315063, acc: 39.0625, f1: 17.732807408677758, r: 0.23774127694698066
06/02/2019 01:22:37 step: 9397, epoch: 284, batch: 24, loss: 1.2932144403457642, acc: 45.3125, f1: 24.965229805655337, r: 0.3630877084692447
06/02/2019 01:22:38 step: 9402, epoch: 284, batch: 29, loss: 1.308009147644043, acc: 50.0, f1: 30.774910248594455, r: 0.3177655092644881
06/02/2019 01:22:38 *** evaluating ***
06/02/2019 01:22:38 step: 285, epoch: 284, acc: 56.41025641025641, f1: 17.208276946560574, r: 0.28888539079372116
06/02/2019 01:22:38 *** epoch: 286 ***
06/02/2019 01:22:38 *** training ***
06/02/2019 01:22:38 step: 9410, epoch: 285, batch: 4, loss: 1.1420941352844238, acc: 56.25, f1: 40.76819407008086, r: 0.3627140196148464
06/02/2019 01:22:38 step: 9415, epoch: 285, batch: 9, loss: 1.2309118509292603, acc: 46.875, f1: 34.78882041382042, r: 0.4744398527409813
06/02/2019 01:22:38 step: 9420, epoch: 285, batch: 14, loss: 1.3462964296340942, acc: 37.5, f1: 16.436877076411964, r: 0.2831246843753552
06/02/2019 01:22:39 step: 9425, epoch: 285, batch: 19, loss: 1.1961623430252075, acc: 46.875, f1: 37.38636363636363, r: 0.37969840340643823
06/02/2019 01:22:39 step: 9430, epoch: 285, batch: 24, loss: 1.088486671447754, acc: 59.375, f1: 42.16804168475901, r: 0.4284810947312183
06/02/2019 01:22:39 step: 9435, epoch: 285, batch: 29, loss: 1.3953193426132202, acc: 42.1875, f1: 19.926739926739927, r: 0.26715148934842536
06/02/2019 01:22:39 *** evaluating ***
06/02/2019 01:22:39 step: 286, epoch: 285, acc: 56.837606837606835, f1: 17.535190176699615, r: 0.2918283349474258
06/02/2019 01:22:39 *** epoch: 287 ***
06/02/2019 01:22:39 *** training ***
06/02/2019 01:22:40 step: 9443, epoch: 286, batch: 4, loss: 1.233834981918335, acc: 42.1875, f1: 20.895390070921987, r: 0.2594334669111635
06/02/2019 01:22:40 step: 9448, epoch: 286, batch: 9, loss: 1.2052549123764038, acc: 48.4375, f1: 24.404761904761905, r: 0.3616595398557513
06/02/2019 01:22:40 step: 9453, epoch: 286, batch: 14, loss: 1.1357147693634033, acc: 54.6875, f1: 30.588420679421695, r: 0.3375255840141515
06/02/2019 01:22:40 step: 9458, epoch: 286, batch: 19, loss: 1.2075088024139404, acc: 50.0, f1: 21.709020146520146, r: 0.38052091264527405
06/02/2019 01:22:40 step: 9463, epoch: 286, batch: 24, loss: 1.2983506917953491, acc: 50.0, f1: 19.02777777777778, r: 0.3137323526498982
06/02/2019 01:22:41 step: 9468, epoch: 286, batch: 29, loss: 1.112526297569275, acc: 60.9375, f1: 29.207548403976975, r: 0.370627712639787
06/02/2019 01:22:41 *** evaluating ***
06/02/2019 01:22:41 step: 287, epoch: 286, acc: 58.119658119658126, f1: 18.087394915106714, r: 0.2948009874368465
06/02/2019 01:22:41 *** epoch: 288 ***
06/02/2019 01:22:41 *** training ***
06/02/2019 01:22:41 step: 9476, epoch: 287, batch: 4, loss: 1.3866132497787476, acc: 40.625, f1: 24.50599287333981, r: 0.30934419355033405
06/02/2019 01:22:41 step: 9481, epoch: 287, batch: 9, loss: 1.1792937517166138, acc: 60.9375, f1: 29.75155279503106, r: 0.25950501400576215
06/02/2019 01:22:42 step: 9486, epoch: 287, batch: 14, loss: 1.2043431997299194, acc: 59.375, f1: 24.43452380952381, r: 0.30408960193131934
06/02/2019 01:22:42 step: 9491, epoch: 287, batch: 19, loss: 1.1262555122375488, acc: 51.5625, f1: 28.844246031746035, r: 0.35307041405289036
06/02/2019 01:22:42 step: 9496, epoch: 287, batch: 24, loss: 1.255009412765503, acc: 54.6875, f1: 31.396198830409354, r: 0.37648434568213973
06/02/2019 01:22:42 step: 9501, epoch: 287, batch: 29, loss: 1.160839557647705, acc: 50.0, f1: 36.17256426380076, r: 0.378544769616614
06/02/2019 01:22:42 *** evaluating ***
06/02/2019 01:22:43 step: 288, epoch: 287, acc: 55.98290598290598, f1: 16.98084601689169, r: 0.28830400202517814
06/02/2019 01:22:43 *** epoch: 289 ***
06/02/2019 01:22:43 *** training ***
06/02/2019 01:22:43 step: 9509, epoch: 288, batch: 4, loss: 1.195993423461914, acc: 56.25, f1: 31.15390465129141, r: 0.3707183586108055
06/02/2019 01:22:43 step: 9514, epoch: 288, batch: 9, loss: 1.3867655992507935, acc: 37.5, f1: 28.363787375415285, r: 0.3780760567903527
06/02/2019 01:22:43 step: 9519, epoch: 288, batch: 14, loss: 1.2506427764892578, acc: 40.625, f1: 19.879374566874564, r: 0.3139445020608389
06/02/2019 01:22:44 step: 9524, epoch: 288, batch: 19, loss: 0.9962862730026245, acc: 62.5, f1: 33.25409500258813, r: 0.33611632970253724
06/02/2019 01:22:44 step: 9529, epoch: 288, batch: 24, loss: 1.2478530406951904, acc: 45.3125, f1: 27.312271062271066, r: 0.2573519919338044
06/02/2019 01:22:44 step: 9534, epoch: 288, batch: 29, loss: 1.2642091512680054, acc: 43.75, f1: 22.12040832730488, r: 0.32004744708123567
06/02/2019 01:22:44 *** evaluating ***
06/02/2019 01:22:44 step: 289, epoch: 288, acc: 57.26495726495726, f1: 17.680200429148538, r: 0.2969199192670211
06/02/2019 01:22:44 *** epoch: 290 ***
06/02/2019 01:22:44 *** training ***
06/02/2019 01:22:44 step: 9542, epoch: 289, batch: 4, loss: 1.1793018579483032, acc: 51.5625, f1: 26.778914464336456, r: 0.31914132450635907
06/02/2019 01:22:45 step: 9547, epoch: 289, batch: 9, loss: 1.0727742910385132, acc: 59.375, f1: 26.48568861506867, r: 0.4028244933349403
06/02/2019 01:22:45 step: 9552, epoch: 289, batch: 14, loss: 1.2911007404327393, acc: 45.3125, f1: 33.566608461566446, r: 0.37387569956138866
06/02/2019 01:22:45 step: 9557, epoch: 289, batch: 19, loss: 1.1894031763076782, acc: 50.0, f1: 24.258421317244846, r: 0.3831799218156174
06/02/2019 01:22:45 step: 9562, epoch: 289, batch: 24, loss: 1.2093825340270996, acc: 50.0, f1: 32.741674375578164, r: 0.3536450156170934
06/02/2019 01:22:46 step: 9567, epoch: 289, batch: 29, loss: 1.0619724988937378, acc: 54.6875, f1: 33.11757338121072, r: 0.33258189877838723
06/02/2019 01:22:46 *** evaluating ***
06/02/2019 01:22:46 step: 290, epoch: 289, acc: 57.26495726495726, f1: 17.70989808461025, r: 0.29361298641703687
06/02/2019 01:22:46 *** epoch: 291 ***
06/02/2019 01:22:46 *** training ***
06/02/2019 01:22:46 step: 9575, epoch: 290, batch: 4, loss: 1.2584478855133057, acc: 43.75, f1: 18.954361241595286, r: 0.19316061021682174
06/02/2019 01:22:46 step: 9580, epoch: 290, batch: 9, loss: 1.0960462093353271, acc: 46.875, f1: 24.245959726903465, r: 0.3314779564430369
06/02/2019 01:22:46 step: 9585, epoch: 290, batch: 14, loss: 1.3836257457733154, acc: 45.3125, f1: 25.307017543859644, r: 0.31420531807486096
06/02/2019 01:22:47 step: 9590, epoch: 290, batch: 19, loss: 1.159521222114563, acc: 54.6875, f1: 22.693096377306905, r: 0.355823569156688
06/02/2019 01:22:47 step: 9595, epoch: 290, batch: 24, loss: 1.4243007898330688, acc: 43.75, f1: 16.954787234042552, r: 0.33436294566508645
06/02/2019 01:22:47 step: 9600, epoch: 290, batch: 29, loss: 1.1808158159255981, acc: 53.125, f1: 28.564593301435405, r: 0.3733984208944598
06/02/2019 01:22:47 *** evaluating ***
06/02/2019 01:22:47 step: 291, epoch: 290, acc: 56.41025641025641, f1: 17.270240879256892, r: 0.2903971024246623
06/02/2019 01:22:47 *** epoch: 292 ***
06/02/2019 01:22:47 *** training ***
06/02/2019 01:22:48 step: 9608, epoch: 291, batch: 4, loss: 1.3590763807296753, acc: 45.3125, f1: 29.504349816849818, r: 0.3479887021446477
06/02/2019 01:22:48 step: 9613, epoch: 291, batch: 9, loss: 1.145393967628479, acc: 57.8125, f1: 28.69047619047619, r: 0.29679972236965696
06/02/2019 01:22:48 step: 9618, epoch: 291, batch: 14, loss: 1.2708455324172974, acc: 42.1875, f1: 24.808956287614826, r: 0.38338591789490506
06/02/2019 01:22:49 step: 9623, epoch: 291, batch: 19, loss: 1.1601240634918213, acc: 56.25, f1: 30.95314042682464, r: 0.30887767299610674
06/02/2019 01:22:49 step: 9628, epoch: 291, batch: 24, loss: 1.1498908996582031, acc: 59.375, f1: 30.93591691995947, r: 0.3234578658442002
06/02/2019 01:22:49 step: 9633, epoch: 291, batch: 29, loss: 1.0123534202575684, acc: 60.9375, f1: 34.7860378823834, r: 0.34196024300182937
06/02/2019 01:22:49 *** evaluating ***
06/02/2019 01:22:49 step: 292, epoch: 291, acc: 56.41025641025641, f1: 17.270240879256892, r: 0.28772418957624973
06/02/2019 01:22:49 *** epoch: 293 ***
06/02/2019 01:22:49 *** training ***
06/02/2019 01:22:50 step: 9641, epoch: 292, batch: 4, loss: 1.1579039096832275, acc: 51.5625, f1: 43.84528514963297, r: 0.38024309582879906
06/02/2019 01:22:50 step: 9646, epoch: 292, batch: 9, loss: 1.2470778226852417, acc: 43.75, f1: 17.011494252873565, r: 0.2559430859106976
06/02/2019 01:22:50 step: 9651, epoch: 292, batch: 14, loss: 1.4503569602966309, acc: 32.8125, f1: 25.369352869352866, r: 0.3229023733604436
06/02/2019 01:22:50 step: 9656, epoch: 292, batch: 19, loss: 1.3532265424728394, acc: 48.4375, f1: 28.771773462175936, r: 0.3167306363072268
06/02/2019 01:22:50 step: 9661, epoch: 292, batch: 24, loss: 0.9996110200881958, acc: 60.9375, f1: 28.958333333333336, r: 0.39773017027397356
06/02/2019 01:22:51 step: 9666, epoch: 292, batch: 29, loss: 1.231992483139038, acc: 48.4375, f1: 27.074992191271264, r: 0.35207727428442603
06/02/2019 01:22:51 *** evaluating ***
06/02/2019 01:22:51 step: 293, epoch: 292, acc: 55.98290598290598, f1: 17.219668843744554, r: 0.2904769234461422
06/02/2019 01:22:51 *** epoch: 294 ***
06/02/2019 01:22:51 *** training ***
06/02/2019 01:22:51 step: 9674, epoch: 293, batch: 4, loss: 1.3072727918624878, acc: 40.625, f1: 23.63369963369964, r: 0.3428788153158991
06/02/2019 01:22:51 step: 9679, epoch: 293, batch: 9, loss: 1.164027214050293, acc: 56.25, f1: 23.498774509803926, r: 0.3201985019041024
06/02/2019 01:22:52 step: 9684, epoch: 293, batch: 14, loss: 1.0138895511627197, acc: 62.5, f1: 38.44224924012159, r: 0.3114230588299774
06/02/2019 01:22:52 step: 9689, epoch: 293, batch: 19, loss: 1.1419117450714111, acc: 50.0, f1: 21.083476272155515, r: 0.39094492128211555
06/02/2019 01:22:52 step: 9694, epoch: 293, batch: 24, loss: 1.1555222272872925, acc: 54.6875, f1: 23.721465826728984, r: 0.30880118752362706
06/02/2019 01:22:52 step: 9699, epoch: 293, batch: 29, loss: 1.1783303022384644, acc: 56.25, f1: 38.75974025974026, r: 0.35316491932533967
06/02/2019 01:22:52 *** evaluating ***
06/02/2019 01:22:53 step: 294, epoch: 293, acc: 55.55555555555556, f1: 16.916197513488157, r: 0.28121287030677955
06/02/2019 01:22:53 *** epoch: 295 ***
06/02/2019 01:22:53 *** training ***
06/02/2019 01:22:53 step: 9707, epoch: 294, batch: 4, loss: 1.1758348941802979, acc: 54.6875, f1: 23.2858933046903, r: 0.33355393801507927
06/02/2019 01:22:53 step: 9712, epoch: 294, batch: 9, loss: 1.26374351978302, acc: 53.125, f1: 35.833333333333336, r: 0.4002965121308087
06/02/2019 01:22:53 step: 9717, epoch: 294, batch: 14, loss: 1.1292873620986938, acc: 48.4375, f1: 29.446138211382113, r: 0.3668663392559753
06/02/2019 01:22:53 step: 9722, epoch: 294, batch: 19, loss: 1.067387580871582, acc: 59.375, f1: 33.92938127632005, r: 0.34981006769760126
06/02/2019 01:22:54 step: 9727, epoch: 294, batch: 24, loss: 1.290459394454956, acc: 46.875, f1: 27.483739837398375, r: 0.2986099118032802
06/02/2019 01:22:54 step: 9732, epoch: 294, batch: 29, loss: 1.3821779489517212, acc: 40.625, f1: 18.384103641456583, r: 0.3286253990489757
06/02/2019 01:22:54 *** evaluating ***
06/02/2019 01:22:54 step: 295, epoch: 294, acc: 55.98290598290598, f1: 17.071479563629453, r: 0.28324396947654457
06/02/2019 01:22:54 *** epoch: 296 ***
06/02/2019 01:22:54 *** training ***
06/02/2019 01:22:54 step: 9740, epoch: 295, batch: 4, loss: 1.2248563766479492, acc: 53.125, f1: 25.093557758031437, r: 0.2777318950472944
06/02/2019 01:22:55 step: 9745, epoch: 295, batch: 9, loss: 1.4570960998535156, acc: 39.0625, f1: 23.35036443092709, r: 0.2990483843370043
06/02/2019 01:22:55 step: 9750, epoch: 295, batch: 14, loss: 1.278805136680603, acc: 43.75, f1: 20.803440113981065, r: 0.3141495356312743
06/02/2019 01:22:55 step: 9755, epoch: 295, batch: 19, loss: 1.282762885093689, acc: 48.4375, f1: 33.36461857470261, r: 0.30549178525779297
06/02/2019 01:22:55 step: 9760, epoch: 295, batch: 24, loss: 1.3006706237792969, acc: 39.0625, f1: 17.556334839088763, r: 0.2960534468747717
06/02/2019 01:22:55 step: 9765, epoch: 295, batch: 29, loss: 1.2521342039108276, acc: 46.875, f1: 22.483558640847644, r: 0.3435040828712939
06/02/2019 01:22:56 *** evaluating ***
06/02/2019 01:22:56 step: 296, epoch: 295, acc: 56.837606837606835, f1: 17.56739871260886, r: 0.2849948607843286
06/02/2019 01:22:56 *** epoch: 297 ***
06/02/2019 01:22:56 *** training ***
06/02/2019 01:22:56 step: 9773, epoch: 296, batch: 4, loss: 1.3115129470825195, acc: 43.75, f1: 28.07523009203681, r: 0.3364141489726219
06/02/2019 01:22:56 step: 9778, epoch: 296, batch: 9, loss: 1.2045960426330566, acc: 48.4375, f1: 20.689915458937197, r: 0.26477167209648667
06/02/2019 01:22:56 step: 9783, epoch: 296, batch: 14, loss: 1.3365819454193115, acc: 46.875, f1: 29.20488888592377, r: 0.29697323368164685
06/02/2019 01:22:57 step: 9788, epoch: 296, batch: 19, loss: 1.0693211555480957, acc: 54.6875, f1: 44.583333333333336, r: 0.35036777592230683
06/02/2019 01:22:57 step: 9793, epoch: 296, batch: 24, loss: 1.0916744470596313, acc: 48.4375, f1: 38.27814100468563, r: 0.34269453659549226
06/02/2019 01:22:57 step: 9798, epoch: 296, batch: 29, loss: 1.2665135860443115, acc: 50.0, f1: 19.225146198830412, r: 0.30422203768405837
06/02/2019 01:22:57 *** evaluating ***
06/02/2019 01:22:57 step: 297, epoch: 296, acc: 54.700854700854705, f1: 16.426508792624972, r: 0.28780970364823383
06/02/2019 01:22:57 *** epoch: 298 ***
06/02/2019 01:22:57 *** training ***
06/02/2019 01:22:58 step: 9806, epoch: 297, batch: 4, loss: 1.2457813024520874, acc: 39.0625, f1: 21.61965811965812, r: 0.3680219934036056
06/02/2019 01:22:58 step: 9811, epoch: 297, batch: 9, loss: 1.3687083721160889, acc: 40.625, f1: 19.843304843304843, r: 0.2531635985494074
06/02/2019 01:22:58 step: 9816, epoch: 297, batch: 14, loss: 1.2121760845184326, acc: 54.6875, f1: 26.147875816993466, r: 0.3594746297479781
06/02/2019 01:22:58 step: 9821, epoch: 297, batch: 19, loss: 1.2228142023086548, acc: 50.0, f1: 25.616515004270106, r: 0.25238018914352345
06/02/2019 01:22:59 step: 9826, epoch: 297, batch: 24, loss: 1.2270913124084473, acc: 54.6875, f1: 29.280303030303028, r: 0.2937607031986512
06/02/2019 01:22:59 step: 9831, epoch: 297, batch: 29, loss: 1.2391760349273682, acc: 50.0, f1: 31.80952380952381, r: 0.3708149695672019
06/02/2019 01:22:59 *** evaluating ***
06/02/2019 01:22:59 step: 298, epoch: 297, acc: 56.41025641025641, f1: 17.199594517127228, r: 0.286468453158863
06/02/2019 01:22:59 *** epoch: 299 ***
06/02/2019 01:22:59 *** training ***
06/02/2019 01:22:59 step: 9839, epoch: 298, batch: 4, loss: 1.1904205083847046, acc: 57.8125, f1: 31.9980264833206, r: 0.2902777268963211
06/02/2019 01:23:00 step: 9844, epoch: 298, batch: 9, loss: 1.1221727132797241, acc: 54.6875, f1: 28.55201277128187, r: 0.30931006441723063
06/02/2019 01:23:00 step: 9849, epoch: 298, batch: 14, loss: 1.1376863718032837, acc: 50.0, f1: 29.743217054263564, r: 0.38625725278678485
06/02/2019 01:23:00 step: 9854, epoch: 298, batch: 19, loss: 1.2857720851898193, acc: 51.5625, f1: 23.712933449241767, r: 0.2978164552545696
06/02/2019 01:23:00 step: 9859, epoch: 298, batch: 24, loss: 1.0822734832763672, acc: 56.25, f1: 22.16010733452594, r: 0.3747324787958763
06/02/2019 01:23:01 step: 9864, epoch: 298, batch: 29, loss: 1.179357647895813, acc: 46.875, f1: 17.508012820512818, r: 0.22726450000836812
06/02/2019 01:23:01 *** evaluating ***
06/02/2019 01:23:01 step: 299, epoch: 298, acc: 57.26495726495726, f1: 17.608528880866427, r: 0.286331071877093
06/02/2019 01:23:01 *** epoch: 300 ***
06/02/2019 01:23:01 *** training ***
06/02/2019 01:23:01 step: 9872, epoch: 299, batch: 4, loss: 1.2296347618103027, acc: 53.125, f1: 34.330512692882024, r: 0.4006865803414317
06/02/2019 01:23:01 step: 9877, epoch: 299, batch: 9, loss: 1.3183873891830444, acc: 51.5625, f1: 29.049465027874692, r: 0.35034611968583007
06/02/2019 01:23:02 step: 9882, epoch: 299, batch: 14, loss: 1.1878776550292969, acc: 48.4375, f1: 33.486997635933804, r: 0.378998295255541
06/02/2019 01:23:02 step: 9887, epoch: 299, batch: 19, loss: 1.1493427753448486, acc: 46.875, f1: 22.524509803921568, r: 0.3841337723338406
06/02/2019 01:23:02 step: 9892, epoch: 299, batch: 24, loss: 1.1496208906173706, acc: 57.8125, f1: 37.467261904761905, r: 0.3751809665141708
06/02/2019 01:23:02 step: 9897, epoch: 299, batch: 29, loss: 1.1678352355957031, acc: 53.125, f1: 24.406893192059435, r: 0.4153518904011993
06/02/2019 01:23:03 *** evaluating ***
06/02/2019 01:23:03 step: 300, epoch: 299, acc: 57.26495726495726, f1: 17.73150941074425, r: 0.28839031622686134
06/02/2019 01:23:03 
*** Best acc model ***
epoch: 262
acc: 58.54700854700855
f1: 18.217243006072113
corr: 0.2855231156916427
06/02/2019 01:23:03 Loading Test Data
06/02/2019 01:23:03 load data from data/word2vec_temp/test_text.npy, data/word2vec_temp/test_label.npy, training: False
06/02/2019 01:23:31 loaded. total len: 2228
06/02/2019 01:23:31 Test: length: 2228, total batch: 35, batch size: 64
06/02/2019 01:23:31 
*** Test Result ***
acc: 57.26495726495726
f1: 17.73150941074425
corr: 0.28839031622686134
