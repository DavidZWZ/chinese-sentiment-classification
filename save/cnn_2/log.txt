06/02/2019 01:16:39 {'input_path': 'data/word2vec_temp', 'output_path': 'save/cnn_2', 'gpu': True, 'seed': 20000125, 'display_per_batch': 5, 'optimizer': 'adagrad', 'lr': 0.01, 'lr_decay': 0, 'weight_decay': 0.0001, 'momentum': 0.985, 'num_epochs': 300, 'batch_size': 64, 'num_labels': 8, 'embedding_size': 300, 'type': 'cnn', 'cnn': {'max_length': 512, 'conv_1': {'size': 512, 'kernel_size': 3, 'dropout': 0.5}, 'max_pool_1': {'kernel_size': 2, 'stride': 2}, 'fc': {'hidden_size': 2000, 'dropout': 0.5}, 'loss': 'l1'}}
06/02/2019 01:16:39 Loading Train Data
06/02/2019 01:16:39 load data from data/word2vec_temp/train_text.npy, data/word2vec_temp/train_label.npy, training: True
06/02/2019 01:17:08 loaded. total len: 2342
06/02/2019 01:17:08 Train: length: 2108, total batch: 33, batch size: 64
06/02/2019 01:17:08 Dev: length: 234, total batch: 4, batch size: 64
06/02/2019 01:17:08 Loading model cnn
06/02/2019 01:17:23 *** epoch: 1 ***
06/02/2019 01:17:23 *** training ***
06/02/2019 01:17:25 step: 5, epoch: 0, batch: 4, loss: 0.16433367133140564, acc: 37.5, f1: 7.792207792207792, r: nan
06/02/2019 01:17:25 step: 10, epoch: 0, batch: 9, loss: 0.17080579698085785, acc: 35.9375, f1: 7.55336617405583, r: nan
06/02/2019 01:17:26 step: 15, epoch: 0, batch: 14, loss: 0.1720346063375473, acc: 31.25, f1: 5.952380952380952, r: nan
06/02/2019 01:17:27 step: 20, epoch: 0, batch: 19, loss: 0.15139350295066833, acc: 43.75, f1: 7.608695652173914, r: nan
06/02/2019 01:17:28 step: 25, epoch: 0, batch: 24, loss: 0.1471453607082367, acc: 53.125, f1: 9.912536443148689, r: nan
06/02/2019 01:17:28 step: 30, epoch: 0, batch: 29, loss: 0.16233187913894653, acc: 39.0625, f1: 8.025682182985554, r: nan
06/02/2019 01:17:29 *** evaluating ***
06/02/2019 01:17:29 step: 1, epoch: 0, acc: 44.871794871794876, f1: 7.743362831858408, r: nan
06/02/2019 01:17:29 *** epoch: 2 ***
06/02/2019 01:17:29 *** training ***
06/02/2019 01:17:30 step: 38, epoch: 1, batch: 4, loss: 0.1653296947479248, acc: 35.9375, f1: 6.609195402298851, r: nan
06/02/2019 01:17:30 step: 43, epoch: 1, batch: 9, loss: 0.17088685929775238, acc: 35.9375, f1: 7.55336617405583, r: -0.06377071659208985
06/02/2019 01:17:31 step: 48, epoch: 1, batch: 14, loss: 0.14722084999084473, acc: 45.3125, f1: 8.90937019969278, r: nan
06/02/2019 01:17:32 step: 53, epoch: 1, batch: 19, loss: 0.14890995621681213, acc: 48.4375, f1: 9.323308270676693, r: nan
06/02/2019 01:17:33 step: 58, epoch: 1, batch: 24, loss: 0.16372068226337433, acc: 31.25, f1: 6.802721088435374, r: nan
06/02/2019 01:17:33 step: 63, epoch: 1, batch: 29, loss: 0.18111193180084229, acc: 34.375, f1: 7.308970099667775, r: nan
06/02/2019 01:17:34 *** evaluating ***
06/02/2019 01:17:34 step: 2, epoch: 1, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.03900742098483025
06/02/2019 01:17:34 *** epoch: 3 ***
06/02/2019 01:17:34 *** training ***
06/02/2019 01:17:35 step: 71, epoch: 2, batch: 4, loss: 0.15202441811561584, acc: 43.75, f1: 8.695652173913045, r: -0.017970642669835998
06/02/2019 01:17:35 step: 76, epoch: 2, batch: 9, loss: 0.17801687121391296, acc: 28.125, f1: 6.2717770034843205, r: nan
06/02/2019 01:17:36 step: 81, epoch: 2, batch: 14, loss: 0.17511020600795746, acc: 34.375, f1: 6.395348837209303, r: nan
06/02/2019 01:17:37 step: 86, epoch: 2, batch: 19, loss: 0.1637161523103714, acc: 35.9375, f1: 7.55336617405583, r: 0.04357978104691619
06/02/2019 01:17:38 step: 91, epoch: 2, batch: 24, loss: 0.15072430670261383, acc: 42.1875, f1: 8.47723704866562, r: 0.01049944800175026
06/02/2019 01:17:38 step: 96, epoch: 2, batch: 29, loss: 0.15977837145328522, acc: 42.1875, f1: 8.47723704866562, r: -0.06615168343382029
06/02/2019 01:17:39 *** evaluating ***
06/02/2019 01:17:39 step: 3, epoch: 2, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.039487800991478944
06/02/2019 01:17:39 *** epoch: 4 ***
06/02/2019 01:17:39 *** training ***
06/02/2019 01:17:40 step: 104, epoch: 3, batch: 4, loss: 0.1593596190214157, acc: 40.625, f1: 8.253968253968253, r: 0.05731027404751451
06/02/2019 01:17:40 step: 109, epoch: 3, batch: 9, loss: 0.16561459004878998, acc: 39.0625, f1: 7.02247191011236, r: 0.028849374444339146
06/02/2019 01:17:41 step: 114, epoch: 3, batch: 14, loss: 0.16188570857048035, acc: 37.5, f1: 9.09090909090909, r: nan
06/02/2019 01:17:42 step: 119, epoch: 3, batch: 19, loss: 0.1664051115512848, acc: 39.0625, f1: 8.025682182985554, r: nan
06/02/2019 01:17:42 step: 124, epoch: 3, batch: 24, loss: 0.16429656744003296, acc: 39.0625, f1: 7.02247191011236, r: nan
06/02/2019 01:17:43 step: 129, epoch: 3, batch: 29, loss: 0.1650037169456482, acc: 32.8125, f1: 6.176470588235294, r: nan
06/02/2019 01:17:43 *** evaluating ***
06/02/2019 01:17:44 step: 4, epoch: 3, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.04006216420076195
06/02/2019 01:17:44 *** epoch: 5 ***
06/02/2019 01:17:44 *** training ***
06/02/2019 01:17:44 step: 137, epoch: 4, batch: 4, loss: 0.17035894095897675, acc: 39.0625, f1: 8.025682182985554, r: nan
06/02/2019 01:17:45 step: 142, epoch: 4, batch: 9, loss: 0.16641360521316528, acc: 31.25, f1: 6.802721088435374, r: 0.03404747487259989
06/02/2019 01:17:46 step: 147, epoch: 4, batch: 14, loss: 0.16592960059642792, acc: 42.1875, f1: 9.89010989010989, r: 0.06333673997769056
06/02/2019 01:17:46 step: 152, epoch: 4, batch: 19, loss: 0.20327118039131165, acc: 20.3125, f1: 4.220779220779221, r: -0.02305733099340309
06/02/2019 01:17:47 step: 157, epoch: 4, batch: 24, loss: 0.21495890617370605, acc: 12.5, f1: 3.1746031746031744, r: 0.016791543935927154
06/02/2019 01:17:48 step: 162, epoch: 4, batch: 29, loss: 0.23384100198745728, acc: 3.125, f1: 0.7575757575757576, r: 0.011332544783554277
06/02/2019 01:17:48 *** evaluating ***
06/02/2019 01:17:48 step: 5, epoch: 4, acc: 4.700854700854701, f1: 2.3986617264834376, r: -0.026653837724477658
06/02/2019 01:17:48 *** epoch: 6 ***
06/02/2019 01:17:48 *** training ***
06/02/2019 01:17:49 step: 170, epoch: 5, batch: 4, loss: 0.22047767043113708, acc: 14.0625, f1: 3.0821917808219177, r: nan
06/02/2019 01:17:50 step: 175, epoch: 5, batch: 9, loss: 0.21865040063858032, acc: 15.625, f1: 3.861003861003861, r: nan
06/02/2019 01:17:51 step: 180, epoch: 5, batch: 14, loss: 0.22326800227165222, acc: 12.5, f1: 3.1746031746031744, r: -0.0002842754455361536
06/02/2019 01:17:51 step: 185, epoch: 5, batch: 19, loss: 0.15315312147140503, acc: 34.375, f1: 7.308970099667775, r: 0.056245881039444556
06/02/2019 01:17:52 step: 190, epoch: 5, batch: 24, loss: 0.20155082643032074, acc: 26.5625, f1: 5.996472663139329, r: nan
06/02/2019 01:17:53 step: 195, epoch: 5, batch: 29, loss: 0.19894899427890778, acc: 28.125, f1: 5.487804878048781, r: nan
06/02/2019 01:17:53 *** evaluating ***
06/02/2019 01:17:53 step: 6, epoch: 5, acc: 14.957264957264957, f1: 3.2527881040892193, r: 0.02567770678363274
06/02/2019 01:17:53 *** epoch: 7 ***
06/02/2019 01:17:53 *** training ***
06/02/2019 01:17:54 step: 203, epoch: 6, batch: 4, loss: 0.19804665446281433, acc: 31.25, f1: 6.802721088435374, r: nan
06/02/2019 01:17:55 step: 208, epoch: 6, batch: 9, loss: 0.21477657556533813, acc: 15.625, f1: 3.861003861003861, r: nan
06/02/2019 01:17:55 step: 213, epoch: 6, batch: 14, loss: 0.20888152718544006, acc: 20.3125, f1: 4.823747680890538, r: nan
06/02/2019 01:17:56 step: 218, epoch: 6, batch: 19, loss: 0.20797276496887207, acc: 17.1875, f1: 4.190476190476191, r: nan
06/02/2019 01:17:57 step: 223, epoch: 6, batch: 24, loss: 0.21457448601722717, acc: 14.0625, f1: 3.522504892367906, r: nan
06/02/2019 01:17:57 step: 228, epoch: 6, batch: 29, loss: 0.200008824467659, acc: 26.5625, f1: 5.246913580246913, r: nan
06/02/2019 01:17:58 *** evaluating ***
06/02/2019 01:17:58 step: 7, epoch: 6, acc: 14.957264957264957, f1: 3.2527881040892193, r: 0.02568057920466036
06/02/2019 01:17:58 *** epoch: 8 ***
06/02/2019 01:17:58 *** training ***
06/02/2019 01:17:59 step: 236, epoch: 7, batch: 4, loss: 0.20798559486865997, acc: 20.3125, f1: 4.220779220779221, r: nan
06/02/2019 01:17:59 step: 241, epoch: 7, batch: 9, loss: 0.19940732419490814, acc: 28.125, f1: 7.317073170731707, r: nan
06/02/2019 01:18:00 step: 246, epoch: 7, batch: 14, loss: 0.1945776492357254, acc: 26.5625, f1: 5.996472663139329, r: 0.011225426493338353
06/02/2019 01:18:01 step: 251, epoch: 7, batch: 19, loss: 0.21438643336296082, acc: 20.3125, f1: 4.823747680890538, r: nan
06/02/2019 01:18:02 step: 256, epoch: 7, batch: 24, loss: 0.20684215426445007, acc: 17.1875, f1: 3.6666666666666665, r: nan
06/02/2019 01:18:02 step: 261, epoch: 7, batch: 29, loss: 0.2065512090921402, acc: 18.75, f1: 3.9473684210526314, r: -0.0804537909137458
06/02/2019 01:18:03 *** evaluating ***
06/02/2019 01:18:03 step: 8, epoch: 7, acc: 14.957264957264957, f1: 3.2527881040892193, r: 0.025859279942711616
06/02/2019 01:18:03 *** epoch: 9 ***
06/02/2019 01:18:03 *** training ***
06/02/2019 01:18:04 step: 269, epoch: 8, batch: 4, loss: 0.2019132524728775, acc: 23.4375, f1: 5.4249547920434, r: -0.02201924319011404
06/02/2019 01:18:04 step: 274, epoch: 8, batch: 9, loss: 0.2051350325345993, acc: 15.625, f1: 3.3783783783783785, r: -0.00688647745621813
06/02/2019 01:18:05 step: 279, epoch: 8, batch: 14, loss: 0.20806878805160522, acc: 25.0, f1: 5.714285714285714, r: 0.029554751893411585
06/02/2019 01:18:06 step: 284, epoch: 8, batch: 19, loss: 0.20766332745552063, acc: 18.75, f1: 3.9473684210526314, r: -0.048560261782838746
06/02/2019 01:18:06 step: 289, epoch: 8, batch: 24, loss: 0.21467596292495728, acc: 15.625, f1: 3.3783783783783785, r: 0.11305077489255272
06/02/2019 01:18:07 step: 294, epoch: 8, batch: 29, loss: 0.2063184529542923, acc: 23.4375, f1: 5.4249547920434, r: -0.0489123326011186
06/02/2019 01:18:07 *** evaluating ***
06/02/2019 01:18:08 step: 9, epoch: 8, acc: 14.957264957264957, f1: 3.2527881040892193, r: 0.039607855156737065
06/02/2019 01:18:08 *** epoch: 10 ***
06/02/2019 01:18:08 *** training ***
06/02/2019 01:18:08 step: 302, epoch: 9, batch: 4, loss: 0.19158446788787842, acc: 28.125, f1: 9.543817527010805, r: 0.035280388623816175
06/02/2019 01:18:09 step: 307, epoch: 9, batch: 9, loss: 0.17528092861175537, acc: 31.25, f1: 6.802721088435374, r: nan
06/02/2019 01:18:10 step: 312, epoch: 9, batch: 14, loss: 0.16186100244522095, acc: 42.1875, f1: 8.47723704866562, r: nan
06/02/2019 01:18:10 step: 317, epoch: 9, batch: 19, loss: 0.15842583775520325, acc: 39.0625, f1: 7.02247191011236, r: nan
06/02/2019 01:18:11 step: 322, epoch: 9, batch: 24, loss: 0.1666754186153412, acc: 34.375, f1: 6.395348837209303, r: nan
06/02/2019 01:18:12 step: 327, epoch: 9, batch: 29, loss: 0.16400812566280365, acc: 39.0625, f1: 7.02247191011236, r: nan
06/02/2019 01:18:12 *** evaluating ***
06/02/2019 01:18:12 step: 10, epoch: 9, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.03901415237667307
06/02/2019 01:18:12 *** epoch: 11 ***
06/02/2019 01:18:12 *** training ***
06/02/2019 01:18:13 step: 335, epoch: 10, batch: 4, loss: 0.15524855256080627, acc: 39.0625, f1: 7.02247191011236, r: 0.027752709315911075
06/02/2019 01:18:14 step: 340, epoch: 10, batch: 9, loss: 0.15290361642837524, acc: 39.0625, f1: 7.02247191011236, r: nan
06/02/2019 01:18:15 step: 345, epoch: 10, batch: 14, loss: 0.149770587682724, acc: 42.1875, f1: 8.47723704866562, r: nan
06/02/2019 01:18:15 step: 350, epoch: 10, batch: 19, loss: 0.1599375456571579, acc: 40.625, f1: 8.253968253968253, r: nan
06/02/2019 01:18:16 step: 355, epoch: 10, batch: 24, loss: 0.1708722561597824, acc: 42.1875, f1: 7.417582417582418, r: 0.03506150702539347
06/02/2019 01:18:17 step: 360, epoch: 10, batch: 29, loss: 0.16751550137996674, acc: 37.5, f1: 7.792207792207792, r: 0.0048177061407552076
06/02/2019 01:18:17 *** evaluating ***
06/02/2019 01:18:17 step: 11, epoch: 10, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.03926526948903406
06/02/2019 01:18:17 *** epoch: 12 ***
06/02/2019 01:18:17 *** training ***
06/02/2019 01:18:18 step: 368, epoch: 11, batch: 4, loss: 0.1647711843252182, acc: 40.625, f1: 7.222222222222221, r: -0.08284932084228373
06/02/2019 01:18:19 step: 373, epoch: 11, batch: 9, loss: 0.16247588396072388, acc: 34.375, f1: 7.308970099667775, r: 0.03670810649847078
06/02/2019 01:18:19 step: 378, epoch: 11, batch: 14, loss: 0.14389514923095703, acc: 50.0, f1: 9.523809523809524, r: 0.05872454137410819
06/02/2019 01:18:20 step: 383, epoch: 11, batch: 19, loss: 0.15884026885032654, acc: 45.3125, f1: 7.795698924731183, r: 0.012306573852027594
06/02/2019 01:18:21 step: 388, epoch: 11, batch: 24, loss: 0.17269018292427063, acc: 29.6875, f1: 6.540447504302925, r: 0.03147143207816952
06/02/2019 01:18:21 step: 393, epoch: 11, batch: 29, loss: 0.17246362566947937, acc: 37.5, f1: 6.8181818181818175, r: 0.060816615286013107
06/02/2019 01:18:22 *** evaluating ***
06/02/2019 01:18:22 step: 12, epoch: 11, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.04081589650870579
06/02/2019 01:18:22 *** epoch: 13 ***
06/02/2019 01:18:22 *** training ***
06/02/2019 01:18:23 step: 401, epoch: 12, batch: 4, loss: 0.17742013931274414, acc: 32.8125, f1: 6.176470588235294, r: 0.04400662569258565
06/02/2019 01:18:23 step: 406, epoch: 12, batch: 9, loss: 0.1569010466337204, acc: 42.1875, f1: 7.417582417582418, r: 0.042331765638655636
06/02/2019 01:18:24 step: 411, epoch: 12, batch: 14, loss: 0.1490112543106079, acc: 46.875, f1: 9.118541033434651, r: 0.04043271293392493
06/02/2019 01:18:25 step: 416, epoch: 12, batch: 19, loss: 0.17726832628250122, acc: 29.6875, f1: 5.72289156626506, r: -0.013128260693667078
06/02/2019 01:18:26 step: 421, epoch: 12, batch: 24, loss: 0.1829620748758316, acc: 28.125, f1: 6.2717770034843205, r: 0.03379146204682765
06/02/2019 01:18:26 step: 426, epoch: 12, batch: 29, loss: 0.16413459181785583, acc: 39.0625, f1: 8.025682182985554, r: 0.004969198135681363
06/02/2019 01:18:27 *** evaluating ***
06/02/2019 01:18:27 step: 13, epoch: 12, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.04012845675350451
06/02/2019 01:18:27 *** epoch: 14 ***
06/02/2019 01:18:27 *** training ***
06/02/2019 01:18:28 step: 434, epoch: 13, batch: 4, loss: 0.16637057065963745, acc: 35.9375, f1: 6.609195402298851, r: 0.021483600510038458
06/02/2019 01:18:28 step: 439, epoch: 13, batch: 9, loss: 0.16738371551036835, acc: 34.375, f1: 6.395348837209303, r: 0.02059661924435277
06/02/2019 01:18:29 step: 444, epoch: 13, batch: 14, loss: 0.15114320814609528, acc: 42.1875, f1: 7.417582417582418, r: 0.04721153200782717
06/02/2019 01:18:30 step: 449, epoch: 13, batch: 19, loss: 0.22549355030059814, acc: 6.25, f1: 1.680672268907563, r: 0.022953451841584618
06/02/2019 01:18:30 step: 454, epoch: 13, batch: 24, loss: 0.15354999899864197, acc: 45.3125, f1: 8.90937019969278, r: 0.07113421799844567
06/02/2019 01:18:31 step: 459, epoch: 13, batch: 29, loss: 0.18751950562000275, acc: 28.125, f1: 6.2717770034843205, r: 0.0053572064746835665
06/02/2019 01:18:31 *** evaluating ***
06/02/2019 01:18:32 step: 14, epoch: 13, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.04084665281830609
06/02/2019 01:18:32 *** epoch: 15 ***
06/02/2019 01:18:32 *** training ***
06/02/2019 01:18:32 step: 467, epoch: 14, batch: 4, loss: 0.16542571783065796, acc: 37.5, f1: 6.8181818181818175, r: 0.07319486849514614
06/02/2019 01:18:33 step: 472, epoch: 14, batch: 9, loss: 0.16087748110294342, acc: 39.0625, f1: 8.025682182985554, r: 0.011801593522034111
06/02/2019 01:18:34 step: 477, epoch: 14, batch: 14, loss: 0.17376355826854706, acc: 29.6875, f1: 6.540447504302925, r: -0.03367282392595635
06/02/2019 01:18:35 step: 482, epoch: 14, batch: 19, loss: 0.16077670454978943, acc: 42.1875, f1: 8.47723704866562, r: 0.05261385969875962
06/02/2019 01:18:35 step: 487, epoch: 14, batch: 24, loss: 0.16815125942230225, acc: 35.9375, f1: 7.55336617405583, r: 0.09087978449706244
06/02/2019 01:18:36 step: 492, epoch: 14, batch: 29, loss: 0.1597277820110321, acc: 40.625, f1: 8.850574712643677, r: 0.04852172232081122
06/02/2019 01:18:36 *** evaluating ***
06/02/2019 01:18:37 step: 15, epoch: 14, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.03982852648093368
06/02/2019 01:18:37 *** epoch: 16 ***
06/02/2019 01:18:37 *** training ***
06/02/2019 01:18:37 step: 500, epoch: 15, batch: 4, loss: 0.15820066630840302, acc: 40.625, f1: 7.222222222222221, r: -0.003091212241150423
06/02/2019 01:18:38 step: 505, epoch: 15, batch: 9, loss: 0.17461736500263214, acc: 37.5, f1: 6.8181818181818175, r: 0.007540153152072196
06/02/2019 01:18:39 step: 510, epoch: 15, batch: 14, loss: 0.15523086488246918, acc: 40.625, f1: 7.222222222222221, r: -0.07502359088952648
06/02/2019 01:18:39 step: 515, epoch: 15, batch: 19, loss: 0.170235276222229, acc: 34.375, f1: 6.395348837209303, r: 0.02264244695432987
06/02/2019 01:18:40 step: 520, epoch: 15, batch: 24, loss: 0.1629665195941925, acc: 39.0625, f1: 7.02247191011236, r: -0.02905306175267758
06/02/2019 01:18:41 step: 525, epoch: 15, batch: 29, loss: 0.13978800177574158, acc: 48.4375, f1: 9.323308270676693, r: 0.0778863568699849
06/02/2019 01:18:41 *** evaluating ***
06/02/2019 01:18:41 step: 16, epoch: 15, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.03992473675636472
06/02/2019 01:18:41 *** epoch: 17 ***
06/02/2019 01:18:41 *** training ***
06/02/2019 01:18:42 step: 533, epoch: 16, batch: 4, loss: 0.17818723618984222, acc: 34.375, f1: 6.395348837209303, r: -0.01270043281903798
06/02/2019 01:18:43 step: 538, epoch: 16, batch: 9, loss: 0.162983238697052, acc: 43.75, f1: 8.695652173913045, r: 0.007457658355506902
06/02/2019 01:18:44 step: 543, epoch: 16, batch: 14, loss: 0.16579441726207733, acc: 40.625, f1: 7.222222222222221, r: -0.003750842961170631
06/02/2019 01:18:44 step: 548, epoch: 16, batch: 19, loss: 0.1580420732498169, acc: 43.75, f1: 8.695652173913045, r: -0.012958434216281985
06/02/2019 01:18:45 step: 553, epoch: 16, batch: 24, loss: 0.16394703090190887, acc: 39.0625, f1: 7.02247191011236, r: -0.06406256348573394
06/02/2019 01:18:46 step: 558, epoch: 16, batch: 29, loss: 0.15367555618286133, acc: 39.0625, f1: 8.025682182985552, r: 0.017504955148205242
06/02/2019 01:18:46 *** evaluating ***
06/02/2019 01:18:46 step: 17, epoch: 16, acc: 41.02564102564102, f1: 9.539187678549247, r: 0.0603005075284865
06/02/2019 01:18:46 *** epoch: 18 ***
06/02/2019 01:18:46 *** training ***
06/02/2019 01:18:47 step: 566, epoch: 17, batch: 4, loss: 0.2024318277835846, acc: 18.75, f1: 6.4117199391172, r: 0.09489775417273542
06/02/2019 01:18:48 step: 571, epoch: 17, batch: 9, loss: 0.1658238172531128, acc: 32.8125, f1: 6.25, r: -0.02495748707221803
06/02/2019 01:18:48 step: 576, epoch: 17, batch: 14, loss: 0.1679350584745407, acc: 37.5, f1: 7.792207792207792, r: 0.014728060378723696
06/02/2019 01:18:49 step: 581, epoch: 17, batch: 19, loss: 0.13767428696155548, acc: 50.0, f1: 9.523809523809524, r: 0.07523779496637653
06/02/2019 01:18:50 step: 586, epoch: 17, batch: 24, loss: 0.1437700241804123, acc: 48.4375, f1: 10.87719298245614, r: 0.03609064648756255
06/02/2019 01:18:50 step: 591, epoch: 17, batch: 29, loss: 0.15395228564739227, acc: 42.1875, f1: 10.227272727272728, r: -0.01734164035981784
06/02/2019 01:18:51 *** evaluating ***
06/02/2019 01:18:51 step: 18, epoch: 17, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.039995298355893154
06/02/2019 01:18:51 *** epoch: 19 ***
06/02/2019 01:18:51 *** training ***
06/02/2019 01:18:52 step: 599, epoch: 18, batch: 4, loss: 0.15142737329006195, acc: 43.75, f1: 7.608695652173914, r: -0.059738877432599535
06/02/2019 01:18:52 step: 604, epoch: 18, batch: 9, loss: 0.16854646801948547, acc: 35.9375, f1: 7.55336617405583, r: -0.054643065300845464
06/02/2019 01:18:53 step: 609, epoch: 18, batch: 14, loss: 0.1441824585199356, acc: 46.875, f1: 7.9787234042553195, r: 0.041087018938232966
06/02/2019 01:18:54 step: 614, epoch: 18, batch: 19, loss: 0.15950854122638702, acc: 37.5, f1: 6.8181818181818175, r: -0.06241714899386603
06/02/2019 01:18:55 step: 619, epoch: 18, batch: 24, loss: 0.15883134305477142, acc: 43.75, f1: 7.608695652173914, r: -0.003982811917736871
06/02/2019 01:18:55 step: 624, epoch: 18, batch: 29, loss: 0.17130769789218903, acc: 31.25, f1: 5.952380952380952, r: 0.00314665634721886
06/02/2019 01:18:56 *** evaluating ***
06/02/2019 01:18:56 step: 19, epoch: 18, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.04319084996861382
06/02/2019 01:18:56 *** epoch: 20 ***
06/02/2019 01:18:56 *** training ***
06/02/2019 01:18:57 step: 632, epoch: 19, batch: 4, loss: 0.16034777462482452, acc: 37.5, f1: 6.8181818181818175, r: 0.03366866406281007
06/02/2019 01:18:57 step: 637, epoch: 19, batch: 9, loss: 0.15606093406677246, acc: 45.3125, f1: 12.46031746031746, r: 0.07554883926887773
06/02/2019 01:18:58 step: 642, epoch: 19, batch: 14, loss: 0.17612889409065247, acc: 31.25, f1: 9.187940140845068, r: 0.00397835350469853
06/02/2019 01:18:59 step: 647, epoch: 19, batch: 19, loss: 0.16245326399803162, acc: 37.5, f1: 8.899233296823658, r: 0.10889910120043052
06/02/2019 01:19:00 step: 652, epoch: 19, batch: 24, loss: 0.1586776077747345, acc: 31.25, f1: 8.333333333333336, r: 0.008191031680253539
06/02/2019 01:19:00 step: 657, epoch: 19, batch: 29, loss: 0.20929886400699615, acc: 17.1875, f1: 4.190476190476191, r: 0.017279384992487844
06/02/2019 01:19:01 *** evaluating ***
06/02/2019 01:19:01 step: 20, epoch: 19, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.1291586726138475
06/02/2019 01:19:01 *** epoch: 21 ***
06/02/2019 01:19:01 *** training ***
06/02/2019 01:19:02 step: 665, epoch: 20, batch: 4, loss: 0.14328180253505707, acc: 35.9375, f1: 8.393665158371041, r: 0.17723382874061508
06/02/2019 01:19:02 step: 670, epoch: 20, batch: 9, loss: 0.12165872752666473, acc: 34.375, f1: 8.804418453541262, r: 0.1574591258156941
06/02/2019 01:19:03 step: 675, epoch: 20, batch: 14, loss: 0.13862308859825134, acc: 28.125, f1: 10.183982683982684, r: 0.25195694560018184
06/02/2019 01:19:04 step: 680, epoch: 20, batch: 19, loss: 0.13026154041290283, acc: 39.0625, f1: 10.317047817047815, r: 0.17596575844116796
06/02/2019 01:19:04 step: 685, epoch: 20, batch: 24, loss: 0.1296692043542862, acc: 32.8125, f1: 6.176470588235294, r: 0.21082518986634546
06/02/2019 01:19:05 step: 690, epoch: 20, batch: 29, loss: 0.12189881503582001, acc: 39.0625, f1: 7.440476190476191, r: 0.17783751819002655
06/02/2019 01:19:06 *** evaluating ***
06/02/2019 01:19:06 step: 21, epoch: 20, acc: 43.58974358974359, f1: 8.200659781985083, r: 0.24271402191237426
06/02/2019 01:19:06 *** epoch: 22 ***
06/02/2019 01:19:06 *** training ***
06/02/2019 01:19:07 step: 698, epoch: 21, batch: 4, loss: 0.11883145570755005, acc: 40.625, f1: 8.253968253968253, r: 0.22619634349394202
06/02/2019 01:19:07 step: 703, epoch: 21, batch: 9, loss: 0.11526395380496979, acc: 43.75, f1: 7.608695652173914, r: 0.23055301847342982
06/02/2019 01:19:08 step: 708, epoch: 21, batch: 14, loss: 0.12633740901947021, acc: 43.75, f1: 14.503401360544219, r: 0.29510777610672434
06/02/2019 01:19:09 step: 713, epoch: 21, batch: 19, loss: 0.12238620221614838, acc: 43.75, f1: 13.860544217687073, r: 0.1816177257523998
06/02/2019 01:19:09 step: 718, epoch: 21, batch: 24, loss: 0.12858466804027557, acc: 35.9375, f1: 13.434873949579831, r: 0.25012260552921084
06/02/2019 01:19:10 step: 723, epoch: 21, batch: 29, loss: 0.11851509660482407, acc: 51.5625, f1: 22.115565593826464, r: 0.34083836878239987
06/02/2019 01:19:11 *** evaluating ***
06/02/2019 01:19:11 step: 22, epoch: 21, acc: 52.56410256410257, f1: 14.699173695988982, r: 0.3571542104132231
06/02/2019 01:19:11 *** epoch: 23 ***
06/02/2019 01:19:11 *** training ***
06/02/2019 01:19:12 step: 731, epoch: 22, batch: 4, loss: 0.13599908351898193, acc: 32.8125, f1: 9.583333333333332, r: 0.2079423139600177
06/02/2019 01:19:12 step: 736, epoch: 22, batch: 9, loss: 0.11474739015102386, acc: 43.75, f1: 15.0, r: 0.3369343366692533
06/02/2019 01:19:13 step: 741, epoch: 22, batch: 14, loss: 0.1034834012389183, acc: 54.6875, f1: 23.544382190998732, r: 0.43714581005379294
06/02/2019 01:19:14 step: 746, epoch: 22, batch: 19, loss: 0.11541826277971268, acc: 45.3125, f1: 16.113924586325098, r: 0.38573367982627077
06/02/2019 01:19:14 step: 751, epoch: 22, batch: 24, loss: 0.14395834505558014, acc: 32.8125, f1: 14.75650542458602, r: 0.2731156230928567
06/02/2019 01:19:15 step: 756, epoch: 22, batch: 29, loss: 0.10947126150131226, acc: 46.875, f1: 23.34433040078201, r: 0.39526079027959843
06/02/2019 01:19:15 *** evaluating ***
06/02/2019 01:19:16 step: 23, epoch: 22, acc: 57.692307692307686, f1: 19.82839478529134, r: 0.35339498836802
06/02/2019 01:19:16 *** epoch: 24 ***
06/02/2019 01:19:16 *** training ***
06/02/2019 01:19:16 step: 764, epoch: 23, batch: 4, loss: 0.1015864759683609, acc: 57.8125, f1: 26.73654244306418, r: 0.4555776891952951
06/02/2019 01:19:17 step: 769, epoch: 23, batch: 9, loss: 0.12176409363746643, acc: 39.0625, f1: 10.2650290885585, r: 0.2817737560205459
06/02/2019 01:19:18 step: 774, epoch: 23, batch: 14, loss: 0.10442059487104416, acc: 45.3125, f1: 18.403078403078403, r: 0.43870225865474727
06/02/2019 01:19:18 step: 779, epoch: 23, batch: 19, loss: 0.1069488674402237, acc: 56.25, f1: 20.554445554445554, r: 0.3433497021832367
06/02/2019 01:19:19 step: 784, epoch: 23, batch: 24, loss: 0.1056244820356369, acc: 53.125, f1: 32.580059050647286, r: 0.43522649754757287
06/02/2019 01:19:20 step: 789, epoch: 23, batch: 29, loss: 0.10426704585552216, acc: 48.4375, f1: 19.4047619047619, r: 0.45573774052927596
06/02/2019 01:19:20 *** evaluating ***
06/02/2019 01:19:21 step: 24, epoch: 23, acc: 55.12820512820513, f1: 19.49028628427068, r: 0.35837947836900375
06/02/2019 01:19:21 *** epoch: 25 ***
06/02/2019 01:19:21 *** training ***
06/02/2019 01:19:21 step: 797, epoch: 24, batch: 4, loss: 0.10330002009868622, acc: 48.4375, f1: 46.04794298671849, r: 0.5284517807713346
06/02/2019 01:19:22 step: 802, epoch: 24, batch: 9, loss: 0.09600938856601715, acc: 51.5625, f1: 19.826797385620917, r: 0.4723835327700655
06/02/2019 01:19:23 step: 807, epoch: 24, batch: 14, loss: 0.08937734365463257, acc: 59.375, f1: 31.060555369688856, r: 0.5828454280986937
06/02/2019 01:19:24 step: 812, epoch: 24, batch: 19, loss: 0.0998348742723465, acc: 65.625, f1: 32.143462469733656, r: 0.4133391596179021
06/02/2019 01:19:24 step: 817, epoch: 24, batch: 24, loss: 0.10175485908985138, acc: 60.9375, f1: 31.746351474612343, r: 0.4411272488860043
06/02/2019 01:19:25 step: 822, epoch: 24, batch: 29, loss: 0.08737660944461823, acc: 64.0625, f1: 34.17299291742653, r: 0.5477148513987028
06/02/2019 01:19:25 *** evaluating ***
06/02/2019 01:19:26 step: 25, epoch: 24, acc: 55.98290598290598, f1: 18.375421395084317, r: 0.37007619625607646
06/02/2019 01:19:26 *** epoch: 26 ***
06/02/2019 01:19:26 *** training ***
06/02/2019 01:19:26 step: 830, epoch: 25, batch: 4, loss: 0.10130517184734344, acc: 57.8125, f1: 34.633699633699635, r: 0.5286598432662231
06/02/2019 01:19:27 step: 835, epoch: 25, batch: 9, loss: 0.08637507259845734, acc: 64.0625, f1: 32.43589743589743, r: 0.6354516920690217
06/02/2019 01:19:28 step: 840, epoch: 25, batch: 14, loss: 0.09214237332344055, acc: 57.8125, f1: 42.867317867317865, r: 0.47119889203012505
06/02/2019 01:19:28 step: 845, epoch: 25, batch: 19, loss: 0.08765658736228943, acc: 62.5, f1: 40.02450980392157, r: 0.5606055378295629
06/02/2019 01:19:29 step: 850, epoch: 25, batch: 24, loss: 0.09303167462348938, acc: 64.0625, f1: 31.965695203400124, r: 0.5626474065474292
06/02/2019 01:19:30 step: 855, epoch: 25, batch: 29, loss: 0.095160573720932, acc: 64.0625, f1: 43.161283161283166, r: 0.48790637234574996
06/02/2019 01:19:30 *** evaluating ***
06/02/2019 01:19:30 step: 26, epoch: 25, acc: 48.717948717948715, f1: 15.491601088032523, r: 0.3210723407185285
06/02/2019 01:19:30 *** epoch: 27 ***
06/02/2019 01:19:30 *** training ***
06/02/2019 01:19:31 step: 863, epoch: 26, batch: 4, loss: 0.08534335345029831, acc: 68.75, f1: 52.46258503401361, r: 0.6054337939761276
06/02/2019 01:19:32 step: 868, epoch: 26, batch: 9, loss: 0.08122822642326355, acc: 62.5, f1: 40.382980115122976, r: 0.5660287667421734
06/02/2019 01:19:32 step: 873, epoch: 26, batch: 14, loss: 0.0933634340763092, acc: 59.375, f1: 33.54921497584541, r: 0.5552436241026485
06/02/2019 01:19:33 step: 878, epoch: 26, batch: 19, loss: 0.07937823235988617, acc: 62.5, f1: 29.46239084298785, r: 0.6241750837350437
06/02/2019 01:19:34 step: 883, epoch: 26, batch: 24, loss: 0.09353473037481308, acc: 59.375, f1: 35.47646735911475, r: 0.5213401365444446
06/02/2019 01:19:35 step: 888, epoch: 26, batch: 29, loss: 0.08574825525283813, acc: 60.9375, f1: 33.46675501917438, r: 0.648828436845462
06/02/2019 01:19:35 *** evaluating ***
06/02/2019 01:19:35 step: 27, epoch: 26, acc: 54.700854700854705, f1: 23.143292200601167, r: 0.3690843459918944
06/02/2019 01:19:35 *** epoch: 28 ***
06/02/2019 01:19:35 *** training ***
06/02/2019 01:19:36 step: 896, epoch: 27, batch: 4, loss: 0.07602660357952118, acc: 75.0, f1: 45.530400756793945, r: 0.6629819221295187
06/02/2019 01:19:37 step: 901, epoch: 27, batch: 9, loss: 0.07984449714422226, acc: 70.3125, f1: 55.69845329436123, r: 0.6246826539242394
06/02/2019 01:19:37 step: 906, epoch: 27, batch: 14, loss: 0.07376506924629211, acc: 68.75, f1: 38.1452142260413, r: 0.6594304735839294
06/02/2019 01:19:38 step: 911, epoch: 27, batch: 19, loss: 0.07432285696268082, acc: 60.9375, f1: 42.62679571445628, r: 0.678443579551767
06/02/2019 01:19:39 step: 916, epoch: 27, batch: 24, loss: 0.07913143932819366, acc: 57.8125, f1: 48.34190966266438, r: 0.719225166825179
06/02/2019 01:19:40 step: 921, epoch: 27, batch: 29, loss: 0.08765703439712524, acc: 60.9375, f1: 43.6676418819276, r: 0.6001814951210088
06/02/2019 01:19:40 *** evaluating ***
06/02/2019 01:19:40 step: 28, epoch: 27, acc: 60.256410256410255, f1: 27.986438318017264, r: 0.3453714980569898
06/02/2019 01:19:40 *** epoch: 29 ***
06/02/2019 01:19:40 *** training ***
06/02/2019 01:19:41 step: 929, epoch: 28, batch: 4, loss: 0.08108467608690262, acc: 64.0625, f1: 50.401806526806524, r: 0.6671674128402834
06/02/2019 01:19:42 step: 934, epoch: 28, batch: 9, loss: 0.06885933130979538, acc: 76.5625, f1: 69.71646577441726, r: 0.6968629908294176
06/02/2019 01:19:42 step: 939, epoch: 28, batch: 14, loss: 0.08290332555770874, acc: 64.0625, f1: 42.89940212998622, r: 0.6339275851398174
06/02/2019 01:19:43 step: 944, epoch: 28, batch: 19, loss: 0.0718170553445816, acc: 59.375, f1: 35.08838383838384, r: 0.7161111621497916
06/02/2019 01:19:44 step: 949, epoch: 28, batch: 24, loss: 0.08562175184488297, acc: 73.4375, f1: 57.27693602693602, r: 0.6928842282334866
06/02/2019 01:19:44 step: 954, epoch: 28, batch: 29, loss: 0.07495374232530594, acc: 67.1875, f1: 43.22144522144522, r: 0.6996427015910638
06/02/2019 01:19:45 *** evaluating ***
06/02/2019 01:19:45 step: 29, epoch: 28, acc: 57.26495726495726, f1: 25.763899596535865, r: 0.36457408056611174
06/02/2019 01:19:45 *** epoch: 30 ***
06/02/2019 01:19:45 *** training ***
06/02/2019 01:19:46 step: 962, epoch: 29, batch: 4, loss: 0.06393133848905563, acc: 81.25, f1: 74.71350078492935, r: 0.7777148146251889
06/02/2019 01:19:47 step: 967, epoch: 29, batch: 9, loss: 0.06975249201059341, acc: 78.125, f1: 59.59561602418745, r: 0.6684376497457968
06/02/2019 01:19:47 step: 972, epoch: 29, batch: 14, loss: 0.062294457107782364, acc: 76.5625, f1: 61.29099281221924, r: 0.767878528904808
06/02/2019 01:19:48 step: 977, epoch: 29, batch: 19, loss: 0.07409551739692688, acc: 70.3125, f1: 48.69521926788343, r: 0.6655567864811521
06/02/2019 01:19:49 step: 982, epoch: 29, batch: 24, loss: 0.06475082039833069, acc: 71.875, f1: 51.80288461538461, r: 0.7589414818957628
06/02/2019 01:19:49 step: 987, epoch: 29, batch: 29, loss: 0.06794167309999466, acc: 68.75, f1: 62.52765752765753, r: 0.7807934491176942
06/02/2019 01:19:50 *** evaluating ***
06/02/2019 01:19:50 step: 30, epoch: 29, acc: 56.837606837606835, f1: 24.06016731016731, r: 0.3710309197205399
06/02/2019 01:19:50 *** epoch: 31 ***
06/02/2019 01:19:50 *** training ***
06/02/2019 01:19:51 step: 995, epoch: 30, batch: 4, loss: 0.061482399702072144, acc: 78.125, f1: 62.98210144272763, r: 0.7995168581331178
06/02/2019 01:19:51 step: 1000, epoch: 30, batch: 9, loss: 0.0719783678650856, acc: 68.75, f1: 59.70933828076684, r: 0.6689779117798813
06/02/2019 01:19:52 step: 1005, epoch: 30, batch: 14, loss: 0.060242973268032074, acc: 75.0, f1: 69.14721869609087, r: 0.8415227709146803
06/02/2019 01:19:53 step: 1010, epoch: 30, batch: 19, loss: 0.07465998828411102, acc: 73.4375, f1: 68.71496924128503, r: 0.7461571514771577
06/02/2019 01:19:54 step: 1015, epoch: 30, batch: 24, loss: 0.06687711179256439, acc: 73.4375, f1: 49.33506144393242, r: 0.6937755936140851
06/02/2019 01:19:54 step: 1020, epoch: 30, batch: 29, loss: 0.06758926808834076, acc: 70.3125, f1: 47.13033518492142, r: 0.7229846551271292
06/02/2019 01:19:55 *** evaluating ***
06/02/2019 01:19:55 step: 31, epoch: 30, acc: 59.401709401709404, f1: 25.187893764475344, r: 0.37933119608140214
06/02/2019 01:19:55 *** epoch: 32 ***
06/02/2019 01:19:55 *** training ***
06/02/2019 01:19:56 step: 1028, epoch: 31, batch: 4, loss: 0.05878903344273567, acc: 82.8125, f1: 75.6436220587164, r: 0.7205708211115922
06/02/2019 01:19:56 step: 1033, epoch: 31, batch: 9, loss: 0.06366884708404541, acc: 79.6875, f1: 73.32228995494302, r: 0.7584869638064429
06/02/2019 01:19:57 step: 1038, epoch: 31, batch: 14, loss: 0.06306001543998718, acc: 62.5, f1: 47.688501964099736, r: 0.6974332326791763
06/02/2019 01:19:58 step: 1043, epoch: 31, batch: 19, loss: 0.07083334028720856, acc: 68.75, f1: 57.69390863602189, r: 0.7447211301890627
06/02/2019 01:19:58 step: 1048, epoch: 31, batch: 24, loss: 0.06198694929480553, acc: 76.5625, f1: 45.579858737753476, r: 0.7143339491440877
06/02/2019 01:19:59 step: 1053, epoch: 31, batch: 29, loss: 0.06339062750339508, acc: 70.3125, f1: 59.37039514774174, r: 0.8042954663242063
06/02/2019 01:19:59 *** evaluating ***
06/02/2019 01:20:00 step: 32, epoch: 31, acc: 55.55555555555556, f1: 23.95370457705394, r: 0.3671865793596158
06/02/2019 01:20:00 *** epoch: 33 ***
06/02/2019 01:20:00 *** training ***
06/02/2019 01:20:00 step: 1061, epoch: 32, batch: 4, loss: 0.05353569984436035, acc: 82.8125, f1: 71.01628530199959, r: 0.8153476831182073
06/02/2019 01:20:01 step: 1066, epoch: 32, batch: 9, loss: 0.06709061563014984, acc: 73.4375, f1: 60.767276422764226, r: 0.7803275773123638
06/02/2019 01:20:02 step: 1071, epoch: 32, batch: 14, loss: 0.06781023740768433, acc: 82.8125, f1: 66.11340048840049, r: 0.7133739728250905
06/02/2019 01:20:03 step: 1076, epoch: 32, batch: 19, loss: 0.06674431264400482, acc: 67.1875, f1: 57.491628741628745, r: 0.8036257696839023
06/02/2019 01:20:03 step: 1081, epoch: 32, batch: 24, loss: 0.06506592035293579, acc: 70.3125, f1: 50.9984491675157, r: 0.7740918444225054
06/02/2019 01:20:04 step: 1086, epoch: 32, batch: 29, loss: 0.05829976499080658, acc: 82.8125, f1: 69.12174303683739, r: 0.8007191168283188
06/02/2019 01:20:04 *** evaluating ***
06/02/2019 01:20:05 step: 33, epoch: 32, acc: 58.119658119658126, f1: 26.426251280081924, r: 0.39184492216423794
06/02/2019 01:20:05 *** epoch: 34 ***
06/02/2019 01:20:05 *** training ***
06/02/2019 01:20:05 step: 1094, epoch: 33, batch: 4, loss: 0.06310972571372986, acc: 78.125, f1: 62.812692592136074, r: 0.7450416652419108
06/02/2019 01:20:06 step: 1099, epoch: 33, batch: 9, loss: 0.058938443660736084, acc: 81.25, f1: 65.07288629737609, r: 0.8135494109143748
06/02/2019 01:20:07 step: 1104, epoch: 33, batch: 14, loss: 0.06253638118505478, acc: 76.5625, f1: 63.17280961348758, r: 0.7512579819802413
06/02/2019 01:20:07 step: 1109, epoch: 33, batch: 19, loss: 0.06434686481952667, acc: 75.0, f1: 49.583333333333336, r: 0.7664779977695415
06/02/2019 01:20:08 step: 1114, epoch: 33, batch: 24, loss: 0.06275808066129684, acc: 73.4375, f1: 57.52067493664133, r: 0.7857586514476532
06/02/2019 01:20:09 step: 1119, epoch: 33, batch: 29, loss: 0.05527900531888008, acc: 75.0, f1: 68.42871414299985, r: 0.8257885948285887
06/02/2019 01:20:09 *** evaluating ***
06/02/2019 01:20:09 step: 34, epoch: 33, acc: 55.55555555555556, f1: 27.642017851032918, r: 0.3712886065152992
06/02/2019 01:20:10 *** epoch: 35 ***
06/02/2019 01:20:10 *** training ***
06/02/2019 01:20:10 step: 1127, epoch: 34, batch: 4, loss: 0.055056631565093994, acc: 78.125, f1: 65.9573886418313, r: 0.8175947142686011
06/02/2019 01:20:11 step: 1132, epoch: 34, batch: 9, loss: 0.05213089659810066, acc: 89.0625, f1: 93.73881932021466, r: 0.8899471253327428
06/02/2019 01:20:12 step: 1137, epoch: 34, batch: 14, loss: 0.06011170521378517, acc: 81.25, f1: 75.48567827100436, r: 0.8323788004799074
06/02/2019 01:20:13 step: 1142, epoch: 34, batch: 19, loss: 0.05187182128429413, acc: 81.25, f1: 64.77202562354366, r: 0.8394801917440818
06/02/2019 01:20:13 step: 1147, epoch: 34, batch: 24, loss: 0.05485277250409126, acc: 75.0, f1: 54.07671859802937, r: 0.7902729113380695
06/02/2019 01:20:14 step: 1152, epoch: 34, batch: 29, loss: 0.06372968852519989, acc: 79.6875, f1: 63.3531746031746, r: 0.7046799535677803
06/02/2019 01:20:14 *** evaluating ***
06/02/2019 01:20:14 step: 35, epoch: 34, acc: 56.41025641025641, f1: 24.905656275814092, r: 0.37479456635625297
06/02/2019 01:20:14 *** epoch: 36 ***
06/02/2019 01:20:14 *** training ***
06/02/2019 01:20:15 step: 1160, epoch: 35, batch: 4, loss: 0.04494938254356384, acc: 82.8125, f1: 65.06850369458128, r: 0.894056130111828
06/02/2019 01:20:16 step: 1165, epoch: 35, batch: 9, loss: 0.05966254696249962, acc: 79.6875, f1: 68.77550131860477, r: 0.7966456661221667
06/02/2019 01:20:17 step: 1170, epoch: 35, batch: 14, loss: 0.053147245198488235, acc: 84.375, f1: 67.23665223665223, r: 0.8323899278747631
06/02/2019 01:20:17 step: 1175, epoch: 35, batch: 19, loss: 0.05180802941322327, acc: 78.125, f1: 67.21683795469406, r: 0.8459884881188859
06/02/2019 01:20:18 step: 1180, epoch: 35, batch: 24, loss: 0.05697048455476761, acc: 81.25, f1: 68.28265516733009, r: 0.7962085615768402
06/02/2019 01:20:19 step: 1185, epoch: 35, batch: 29, loss: 0.05696342885494232, acc: 81.25, f1: 74.56525080383875, r: 0.8691576755895438
06/02/2019 01:20:19 *** evaluating ***
06/02/2019 01:20:19 step: 36, epoch: 35, acc: 55.98290598290598, f1: 25.498064559942048, r: 0.3757457331878513
06/02/2019 01:20:19 *** epoch: 37 ***
06/02/2019 01:20:19 *** training ***
06/02/2019 01:20:20 step: 1193, epoch: 36, batch: 4, loss: 0.055006738752126694, acc: 81.25, f1: 72.38994006415295, r: 0.7773449972055103
06/02/2019 01:20:21 step: 1198, epoch: 36, batch: 9, loss: 0.048000968992710114, acc: 87.5, f1: 76.55105902009926, r: 0.8974864807254878
06/02/2019 01:20:22 step: 1203, epoch: 36, batch: 14, loss: 0.05459534004330635, acc: 79.6875, f1: 62.01130089717045, r: 0.8269098804285606
06/02/2019 01:20:22 step: 1208, epoch: 36, batch: 19, loss: 0.05507505685091019, acc: 81.25, f1: 69.04932786511733, r: 0.8050827938695655
06/02/2019 01:20:23 step: 1213, epoch: 36, batch: 24, loss: 0.055590514093637466, acc: 87.5, f1: 70.08970512769905, r: 0.7804480971696686
06/02/2019 01:20:24 step: 1218, epoch: 36, batch: 29, loss: 0.0487937331199646, acc: 81.25, f1: 70.64368965671375, r: 0.8910351714774272
06/02/2019 01:20:24 *** evaluating ***
06/02/2019 01:20:24 step: 37, epoch: 36, acc: 55.12820512820513, f1: 22.368080131445904, r: 0.3730639331238336
06/02/2019 01:20:24 *** epoch: 38 ***
06/02/2019 01:20:24 *** training ***
06/02/2019 01:20:25 step: 1226, epoch: 37, batch: 4, loss: 0.047979846596717834, acc: 81.25, f1: 60.10734463276837, r: 0.848761971877792
06/02/2019 01:20:26 step: 1231, epoch: 37, batch: 9, loss: 0.049801625311374664, acc: 85.9375, f1: 83.04631876060446, r: 0.8447972810990899
06/02/2019 01:20:26 step: 1236, epoch: 37, batch: 14, loss: 0.05290229246020317, acc: 75.0, f1: 47.716658341658345, r: 0.8259302066589757
06/02/2019 01:20:27 step: 1241, epoch: 37, batch: 19, loss: 0.06170889735221863, acc: 82.8125, f1: 65.25304938028724, r: 0.7688146645333758
06/02/2019 01:20:28 step: 1246, epoch: 37, batch: 24, loss: 0.051990047097206116, acc: 87.5, f1: 68.24749484232242, r: 0.847227822807178
06/02/2019 01:20:29 step: 1251, epoch: 37, batch: 29, loss: 0.0545109361410141, acc: 81.25, f1: 64.2687074829932, r: 0.7747793314152884
06/02/2019 01:20:29 *** evaluating ***
06/02/2019 01:20:29 step: 38, epoch: 37, acc: 55.98290598290598, f1: 23.6460969496913, r: 0.3758745042665089
06/02/2019 01:20:29 *** epoch: 39 ***
06/02/2019 01:20:29 *** training ***
06/02/2019 01:20:30 step: 1259, epoch: 38, batch: 4, loss: 0.05694842338562012, acc: 76.5625, f1: 70.72146807440926, r: 0.8093084466341026
06/02/2019 01:20:31 step: 1264, epoch: 38, batch: 9, loss: 0.05532757565379143, acc: 73.4375, f1: 62.68910913647756, r: 0.7955563467248615
06/02/2019 01:20:31 step: 1269, epoch: 38, batch: 14, loss: 0.05401768162846565, acc: 79.6875, f1: 64.83174058645757, r: 0.7874076959555165
06/02/2019 01:20:32 step: 1274, epoch: 38, batch: 19, loss: 0.04351276904344559, acc: 76.5625, f1: 51.87500000000001, r: 0.8835541275408684
06/02/2019 01:20:33 step: 1279, epoch: 38, batch: 24, loss: 0.04752953350543976, acc: 84.375, f1: 70.17511474033213, r: 0.9205059541884174
06/02/2019 01:20:34 step: 1284, epoch: 38, batch: 29, loss: 0.04706309363245964, acc: 82.8125, f1: 71.42094017094016, r: 0.8998846765693789
06/02/2019 01:20:34 *** evaluating ***
06/02/2019 01:20:34 step: 39, epoch: 38, acc: 57.692307692307686, f1: 26.710164835164836, r: 0.3845030287590831
06/02/2019 01:20:34 *** epoch: 40 ***
06/02/2019 01:20:34 *** training ***
06/02/2019 01:20:35 step: 1292, epoch: 39, batch: 4, loss: 0.04238738492131233, acc: 82.8125, f1: 68.30440091309656, r: 0.8975159480924874
06/02/2019 01:20:36 step: 1297, epoch: 39, batch: 9, loss: 0.05337260663509369, acc: 85.9375, f1: 69.13620763620763, r: 0.8106213743107817
06/02/2019 01:20:37 step: 1302, epoch: 39, batch: 14, loss: 0.053762756288051605, acc: 84.375, f1: 71.71842650103521, r: 0.8380002071218919
06/02/2019 01:20:37 step: 1307, epoch: 39, batch: 19, loss: 0.04668477550148964, acc: 79.6875, f1: 64.30037210610536, r: 0.9018419609225508
06/02/2019 01:20:38 step: 1312, epoch: 39, batch: 24, loss: 0.05898432433605194, acc: 75.0, f1: 41.44342056138208, r: 0.7368682526171977
06/02/2019 01:20:39 step: 1317, epoch: 39, batch: 29, loss: 0.04759015515446663, acc: 85.9375, f1: 72.52518493736292, r: 0.8549040570826563
06/02/2019 01:20:39 *** evaluating ***
06/02/2019 01:20:39 step: 40, epoch: 39, acc: 58.119658119658126, f1: 27.68003807969742, r: 0.3844117335660148
06/02/2019 01:20:39 *** epoch: 41 ***
06/02/2019 01:20:39 *** training ***
06/02/2019 01:20:40 step: 1325, epoch: 40, batch: 4, loss: 0.04732353240251541, acc: 79.6875, f1: 78.34197052947053, r: 0.8925883196910334
06/02/2019 01:20:41 step: 1330, epoch: 40, batch: 9, loss: 0.04890016093850136, acc: 73.4375, f1: 66.72004186289898, r: 0.8088128060870574
06/02/2019 01:20:42 step: 1335, epoch: 40, batch: 14, loss: 0.048652924597263336, acc: 82.8125, f1: 78.98064353946708, r: 0.8619635553548659
06/02/2019 01:20:42 step: 1340, epoch: 40, batch: 19, loss: 0.05042530968785286, acc: 85.9375, f1: 85.28758305647841, r: 0.9031822594372255
06/02/2019 01:20:43 step: 1345, epoch: 40, batch: 24, loss: 0.04824160411953926, acc: 78.125, f1: 62.52256144393241, r: 0.8738334638241698
06/02/2019 01:20:44 step: 1350, epoch: 40, batch: 29, loss: 0.047749314457178116, acc: 85.9375, f1: 78.11949954807096, r: 0.874509600030887
06/02/2019 01:20:44 *** evaluating ***
06/02/2019 01:20:44 step: 41, epoch: 40, acc: 56.837606837606835, f1: 25.97837011473375, r: 0.3684091094309511
06/02/2019 01:20:44 *** epoch: 42 ***
06/02/2019 01:20:44 *** training ***
06/02/2019 01:20:45 step: 1358, epoch: 41, batch: 4, loss: 0.04971935600042343, acc: 81.25, f1: 73.66841491841491, r: 0.8477008039228077
06/02/2019 01:20:46 step: 1363, epoch: 41, batch: 9, loss: 0.045311905443668365, acc: 79.6875, f1: 65.54987863127398, r: 0.8489258147513383
06/02/2019 01:20:46 step: 1368, epoch: 41, batch: 14, loss: 0.04399218410253525, acc: 87.5, f1: 63.03552425040573, r: 0.8788798750549973
06/02/2019 01:20:47 step: 1373, epoch: 41, batch: 19, loss: 0.047633737325668335, acc: 79.6875, f1: 69.3546088745152, r: 0.8399053248207219
06/02/2019 01:20:48 step: 1378, epoch: 41, batch: 24, loss: 0.04457765817642212, acc: 85.9375, f1: 78.16199955015745, r: 0.8994410867931468
06/02/2019 01:20:49 step: 1383, epoch: 41, batch: 29, loss: 0.05149058252573013, acc: 78.125, f1: 67.56704980842912, r: 0.8466050546996705
06/02/2019 01:20:49 *** evaluating ***
06/02/2019 01:20:49 step: 42, epoch: 41, acc: 57.26495726495726, f1: 24.18404139021669, r: 0.3785161483669826
06/02/2019 01:20:49 *** epoch: 43 ***
06/02/2019 01:20:49 *** training ***
06/02/2019 01:20:50 step: 1391, epoch: 42, batch: 4, loss: 0.04232433810830116, acc: 81.25, f1: 79.22452148684698, r: 0.9282801890039315
06/02/2019 01:20:51 step: 1396, epoch: 42, batch: 9, loss: 0.04133036360144615, acc: 90.625, f1: 76.34231029810297, r: 0.9146910363762747
06/02/2019 01:20:51 step: 1401, epoch: 42, batch: 14, loss: 0.051060788333415985, acc: 82.8125, f1: 80.24166643715516, r: 0.8479569852154079
06/02/2019 01:20:52 step: 1406, epoch: 42, batch: 19, loss: 0.042753349989652634, acc: 85.9375, f1: 85.09924195142248, r: 0.8957815936980235
06/02/2019 01:20:53 step: 1411, epoch: 42, batch: 24, loss: 0.05108298733830452, acc: 81.25, f1: 63.757974481658685, r: 0.8694069235520172
06/02/2019 01:20:54 step: 1416, epoch: 42, batch: 29, loss: 0.048660214990377426, acc: 89.0625, f1: 77.12578616352201, r: 0.8683738650891065
06/02/2019 01:20:54 *** evaluating ***
06/02/2019 01:20:54 step: 43, epoch: 42, acc: 57.26495726495726, f1: 28.026459621075645, r: 0.38741784091176235
06/02/2019 01:20:54 *** epoch: 44 ***
06/02/2019 01:20:54 *** training ***
06/02/2019 01:20:55 step: 1424, epoch: 43, batch: 4, loss: 0.04772769287228584, acc: 78.125, f1: 64.75484660267269, r: 0.8169751730260654
06/02/2019 01:20:56 step: 1429, epoch: 43, batch: 9, loss: 0.039363712072372437, acc: 84.375, f1: 71.95670995670996, r: 0.9307056345337543
06/02/2019 01:20:56 step: 1434, epoch: 43, batch: 14, loss: 0.04449133574962616, acc: 82.8125, f1: 70.48294416580862, r: 0.9129449337178717
06/02/2019 01:20:57 step: 1439, epoch: 43, batch: 19, loss: 0.0494113489985466, acc: 71.875, f1: 49.52974170809332, r: 0.8654193582793394
06/02/2019 01:20:58 step: 1444, epoch: 43, batch: 24, loss: 0.044256895780563354, acc: 79.6875, f1: 71.64757730015083, r: 0.9105625466683918
06/02/2019 01:20:59 step: 1449, epoch: 43, batch: 29, loss: 0.04077999293804169, acc: 85.9375, f1: 73.74525474525474, r: 0.8468071609453003
06/02/2019 01:20:59 *** evaluating ***
06/02/2019 01:20:59 step: 44, epoch: 43, acc: 58.54700854700855, f1: 27.921765905354334, r: 0.3809911047246551
06/02/2019 01:20:59 *** epoch: 45 ***
06/02/2019 01:20:59 *** training ***
06/02/2019 01:21:00 step: 1457, epoch: 44, batch: 4, loss: 0.04257252812385559, acc: 85.9375, f1: 80.05144365438484, r: 0.8953754249141408
06/02/2019 01:21:01 step: 1462, epoch: 44, batch: 9, loss: 0.041882485151290894, acc: 76.5625, f1: 75.69989106753813, r: 0.9190041801148057
06/02/2019 01:21:01 step: 1467, epoch: 44, batch: 14, loss: 0.045878853648900986, acc: 79.6875, f1: 74.30551188438203, r: 0.9058805061224682
06/02/2019 01:21:02 step: 1472, epoch: 44, batch: 19, loss: 0.04988384246826172, acc: 84.375, f1: 68.35280599903241, r: 0.8677780546758664
06/02/2019 01:21:03 step: 1477, epoch: 44, batch: 24, loss: 0.03986423462629318, acc: 85.9375, f1: 79.10039878020171, r: 0.9017188394625901
06/02/2019 01:21:04 step: 1482, epoch: 44, batch: 29, loss: 0.04740332439541817, acc: 81.25, f1: 68.33442180424937, r: 0.8388312502683379
06/02/2019 01:21:04 *** evaluating ***
06/02/2019 01:21:04 step: 45, epoch: 44, acc: 57.26495726495726, f1: 26.99223697999546, r: 0.37134676803396227
06/02/2019 01:21:04 *** epoch: 46 ***
06/02/2019 01:21:04 *** training ***
06/02/2019 01:21:05 step: 1490, epoch: 45, batch: 4, loss: 0.03817204385995865, acc: 81.25, f1: 66.86063936063935, r: 0.9177646620822635
06/02/2019 01:21:06 step: 1495, epoch: 45, batch: 9, loss: 0.04851499944925308, acc: 79.6875, f1: 61.94970459021361, r: 0.8435643794820423
06/02/2019 01:21:06 step: 1500, epoch: 45, batch: 14, loss: 0.03702547401189804, acc: 90.625, f1: 88.55054302422724, r: 0.910947882268878
06/02/2019 01:21:07 step: 1505, epoch: 45, batch: 19, loss: 0.045523326843976974, acc: 85.9375, f1: 73.85531135531136, r: 0.9033420859121478
06/02/2019 01:21:08 step: 1510, epoch: 45, batch: 24, loss: 0.04491489380598068, acc: 79.6875, f1: 75.83615825578889, r: 0.9142034445489009
06/02/2019 01:21:09 step: 1515, epoch: 45, batch: 29, loss: 0.039213139563798904, acc: 90.625, f1: 57.19219505984212, r: 0.9097824377756163
06/02/2019 01:21:09 *** evaluating ***
06/02/2019 01:21:09 step: 46, epoch: 45, acc: 55.12820512820513, f1: 25.37671709766994, r: 0.3822523211150473
06/02/2019 01:21:09 *** epoch: 47 ***
06/02/2019 01:21:09 *** training ***
06/02/2019 01:21:10 step: 1523, epoch: 46, batch: 4, loss: 0.048728641122579575, acc: 78.125, f1: 65.76073232323232, r: 0.8526885106593985
06/02/2019 01:21:11 step: 1528, epoch: 46, batch: 9, loss: 0.04518784210085869, acc: 87.5, f1: 82.04650188521157, r: 0.8190733365303243
06/02/2019 01:21:11 step: 1533, epoch: 46, batch: 14, loss: 0.04397325962781906, acc: 87.5, f1: 79.18383699633699, r: 0.9020519803093898
06/02/2019 01:21:12 step: 1538, epoch: 46, batch: 19, loss: 0.04102567583322525, acc: 90.625, f1: 85.02314814814815, r: 0.9116429700139742
06/02/2019 01:21:13 step: 1543, epoch: 46, batch: 24, loss: 0.041245970875024796, acc: 84.375, f1: 66.72588156141578, r: 0.9021944705777624
06/02/2019 01:21:14 step: 1548, epoch: 46, batch: 29, loss: 0.051307789981365204, acc: 85.9375, f1: 86.40434419381788, r: 0.8617898111997182
06/02/2019 01:21:14 *** evaluating ***
06/02/2019 01:21:14 step: 47, epoch: 46, acc: 58.97435897435898, f1: 27.27415370674707, r: 0.38669188140218835
06/02/2019 01:21:14 *** epoch: 48 ***
06/02/2019 01:21:14 *** training ***
06/02/2019 01:21:15 step: 1556, epoch: 47, batch: 4, loss: 0.04232075437903404, acc: 81.25, f1: 81.39829902801601, r: 0.9057243081037099
06/02/2019 01:21:16 step: 1561, epoch: 47, batch: 9, loss: 0.04033474996685982, acc: 90.625, f1: 77.33273094183912, r: 0.8667649636344585
06/02/2019 01:21:16 step: 1566, epoch: 47, batch: 14, loss: 0.044922299683094025, acc: 71.875, f1: 70.10032642089092, r: 0.8720719624234564
06/02/2019 01:21:17 step: 1571, epoch: 47, batch: 19, loss: 0.0491710864007473, acc: 84.375, f1: 80.632215007215, r: 0.8700017877443226
06/02/2019 01:21:18 step: 1576, epoch: 47, batch: 24, loss: 0.040919333696365356, acc: 89.0625, f1: 75.1841070806588, r: 0.8586782191912598
06/02/2019 01:21:18 step: 1581, epoch: 47, batch: 29, loss: 0.04144113138318062, acc: 95.3125, f1: 93.64658330175571, r: 0.8975472812695388
06/02/2019 01:21:19 *** evaluating ***
06/02/2019 01:21:19 step: 48, epoch: 47, acc: 55.12820512820513, f1: 26.702987165635722, r: 0.3851943530644988
06/02/2019 01:21:19 *** epoch: 49 ***
06/02/2019 01:21:19 *** training ***
06/02/2019 01:21:20 step: 1589, epoch: 48, batch: 4, loss: 0.041090477257966995, acc: 85.9375, f1: 79.71959310039804, r: 0.9225125491813879
06/02/2019 01:21:20 step: 1594, epoch: 48, batch: 9, loss: 0.040912043303251266, acc: 75.0, f1: 58.561329795075935, r: 0.9156425404356763
06/02/2019 01:21:21 step: 1599, epoch: 48, batch: 14, loss: 0.0397343747317791, acc: 85.9375, f1: 79.70779220779221, r: 0.8961324514499729
06/02/2019 01:21:22 step: 1604, epoch: 48, batch: 19, loss: 0.04402179643511772, acc: 79.6875, f1: 77.82084785133566, r: 0.9074819633950987
06/02/2019 01:21:23 step: 1609, epoch: 48, batch: 24, loss: 0.05121295899152756, acc: 82.8125, f1: 75.46130952380952, r: 0.8614973668764974
06/02/2019 01:21:23 step: 1614, epoch: 48, batch: 29, loss: 0.043644051998853683, acc: 85.9375, f1: 70.51761762827337, r: 0.8453846741300313
06/02/2019 01:21:24 *** evaluating ***
06/02/2019 01:21:24 step: 49, epoch: 48, acc: 59.82905982905983, f1: 29.055015478076985, r: 0.37924477881945673
06/02/2019 01:21:24 *** epoch: 50 ***
06/02/2019 01:21:24 *** training ***
06/02/2019 01:21:25 step: 1622, epoch: 49, batch: 4, loss: 0.04361964389681816, acc: 89.0625, f1: 76.30886000768614, r: 0.8646662732213641
06/02/2019 01:21:25 step: 1627, epoch: 49, batch: 9, loss: 0.04587755352258682, acc: 82.8125, f1: 74.7367216117216, r: 0.907068373561025
06/02/2019 01:21:26 step: 1632, epoch: 49, batch: 14, loss: 0.037198521196842194, acc: 84.375, f1: 71.51163273612254, r: 0.9306090774397557
06/02/2019 01:21:27 step: 1637, epoch: 49, batch: 19, loss: 0.05164844170212746, acc: 71.875, f1: 62.092609771181195, r: 0.8695235427658786
06/02/2019 01:21:28 step: 1642, epoch: 49, batch: 24, loss: 0.04987373948097229, acc: 89.0625, f1: 89.8089430894309, r: 0.8669289516205174
06/02/2019 01:21:28 step: 1647, epoch: 49, batch: 29, loss: 0.04468822851777077, acc: 75.0, f1: 63.34456424079066, r: 0.854679398275859
06/02/2019 01:21:29 *** evaluating ***
06/02/2019 01:21:29 step: 50, epoch: 49, acc: 58.54700854700855, f1: 26.459028810292313, r: 0.3843955943949066
06/02/2019 01:21:29 *** epoch: 51 ***
06/02/2019 01:21:29 *** training ***
06/02/2019 01:21:30 step: 1655, epoch: 50, batch: 4, loss: 0.04242183268070221, acc: 85.9375, f1: 61.96479885057471, r: 0.8991364126606504
06/02/2019 01:21:30 step: 1660, epoch: 50, batch: 9, loss: 0.03849906846880913, acc: 90.625, f1: 83.0255991285403, r: 0.915436697584638
06/02/2019 01:21:31 step: 1665, epoch: 50, batch: 14, loss: 0.04617580026388168, acc: 79.6875, f1: 75.77745383867833, r: 0.7941524628418682
06/02/2019 01:21:32 step: 1670, epoch: 50, batch: 19, loss: 0.05527973175048828, acc: 75.0, f1: 66.81180164373443, r: 0.8963058241136519
06/02/2019 01:21:33 step: 1675, epoch: 50, batch: 24, loss: 0.038622792810201645, acc: 82.8125, f1: 74.60618085618084, r: 0.9315248688949244
06/02/2019 01:21:33 step: 1680, epoch: 50, batch: 29, loss: 0.04176147282123566, acc: 85.9375, f1: 77.2937062937063, r: 0.8212990932621664
06/02/2019 01:21:34 *** evaluating ***
06/02/2019 01:21:34 step: 51, epoch: 50, acc: 58.97435897435898, f1: 28.134284144164656, r: 0.39304402281947437
06/02/2019 01:21:34 *** epoch: 52 ***
06/02/2019 01:21:34 *** training ***
06/02/2019 01:21:35 step: 1688, epoch: 51, batch: 4, loss: 0.03253839537501335, acc: 82.8125, f1: 72.81172379533034, r: 0.9526766837855737
06/02/2019 01:21:35 step: 1693, epoch: 51, batch: 9, loss: 0.03442076966166496, acc: 93.75, f1: 80.16433566433567, r: 0.9369946479566864
06/02/2019 01:21:36 step: 1698, epoch: 51, batch: 14, loss: 0.040272366255521774, acc: 89.0625, f1: 89.0035332314744, r: 0.9005945207014993
06/02/2019 01:21:37 step: 1703, epoch: 51, batch: 19, loss: 0.04453960061073303, acc: 75.0, f1: 63.69355500821019, r: 0.8313118274440279
06/02/2019 01:21:38 step: 1708, epoch: 51, batch: 24, loss: 0.04246477037668228, acc: 85.9375, f1: 84.34358919382504, r: 0.8955880795164053
06/02/2019 01:21:38 step: 1713, epoch: 51, batch: 29, loss: 0.04078562557697296, acc: 85.9375, f1: 77.4889604964793, r: 0.8981493609903879
06/02/2019 01:21:39 *** evaluating ***
06/02/2019 01:21:39 step: 52, epoch: 51, acc: 60.68376068376068, f1: 28.68365559558682, r: 0.3912307084851349
06/02/2019 01:21:39 *** epoch: 53 ***
06/02/2019 01:21:39 *** training ***
06/02/2019 01:21:40 step: 1721, epoch: 52, batch: 4, loss: 0.040063656866550446, acc: 87.5, f1: 84.76292466011893, r: 0.8912820937220453
06/02/2019 01:21:40 step: 1726, epoch: 52, batch: 9, loss: 0.03915642201900482, acc: 90.625, f1: 86.23376623376625, r: 0.9309634160715212
06/02/2019 01:21:41 step: 1731, epoch: 52, batch: 14, loss: 0.04400118440389633, acc: 85.9375, f1: 83.88965201465201, r: 0.8564755537632616
06/02/2019 01:21:42 step: 1736, epoch: 52, batch: 19, loss: 0.04222104698419571, acc: 76.5625, f1: 69.44929253752782, r: 0.8899366572272146
06/02/2019 01:21:42 step: 1741, epoch: 52, batch: 24, loss: 0.03863102197647095, acc: 95.3125, f1: 95.08701824491298, r: 0.9041239678140527
06/02/2019 01:21:43 step: 1746, epoch: 52, batch: 29, loss: 0.03552791103720665, acc: 84.375, f1: 63.254280441780445, r: 0.9318397335209571
06/02/2019 01:21:44 *** evaluating ***
06/02/2019 01:21:44 step: 53, epoch: 52, acc: 58.54700854700855, f1: 28.88803684283815, r: 0.39484564396151134
06/02/2019 01:21:44 *** epoch: 54 ***
06/02/2019 01:21:44 *** training ***
06/02/2019 01:21:45 step: 1754, epoch: 53, batch: 4, loss: 0.03784278780221939, acc: 79.6875, f1: 68.58585858585859, r: 0.8970529027872176
06/02/2019 01:21:45 step: 1759, epoch: 53, batch: 9, loss: 0.039829958230257034, acc: 87.5, f1: 84.96162997218592, r: 0.920319229314694
06/02/2019 01:21:46 step: 1764, epoch: 53, batch: 14, loss: 0.044395387172698975, acc: 73.4375, f1: 68.08779282183538, r: 0.9122345240405021
06/02/2019 01:21:47 step: 1769, epoch: 53, batch: 19, loss: 0.03912459686398506, acc: 84.375, f1: 66.53702582728008, r: 0.8919171053724994
06/02/2019 01:21:47 step: 1774, epoch: 53, batch: 24, loss: 0.04392503947019577, acc: 81.25, f1: 78.16026834922182, r: 0.8950469431218189
06/02/2019 01:21:48 step: 1779, epoch: 53, batch: 29, loss: 0.03864186257123947, acc: 79.6875, f1: 68.61471861471861, r: 0.8939982530803027
06/02/2019 01:21:49 *** evaluating ***
06/02/2019 01:21:49 step: 54, epoch: 53, acc: 58.54700854700855, f1: 28.58567040165808, r: 0.395380528944609
06/02/2019 01:21:49 *** epoch: 55 ***
06/02/2019 01:21:49 *** training ***
06/02/2019 01:21:50 step: 1787, epoch: 54, batch: 4, loss: 0.03862667828798294, acc: 87.5, f1: 75.73361767493545, r: 0.9038296176171008
06/02/2019 01:21:50 step: 1792, epoch: 54, batch: 9, loss: 0.03797410428524017, acc: 78.125, f1: 65.82028265851795, r: 0.9248235156271541
06/02/2019 01:21:51 step: 1797, epoch: 54, batch: 14, loss: 0.035811565816402435, acc: 92.1875, f1: 87.28515560813698, r: 0.8950468366509066
06/02/2019 01:21:52 step: 1802, epoch: 54, batch: 19, loss: 0.04265553131699562, acc: 85.9375, f1: 82.65175821607309, r: 0.9047234644950211
06/02/2019 01:21:52 step: 1807, epoch: 54, batch: 24, loss: 0.11586660146713257, acc: 51.5625, f1: 30.38961038961039, r: 0.6143106265941294
06/02/2019 01:21:53 step: 1812, epoch: 54, batch: 29, loss: 0.04179166629910469, acc: 81.25, f1: 73.65996471157405, r: 0.8979141679151257
06/02/2019 01:21:53 *** evaluating ***
06/02/2019 01:21:54 step: 55, epoch: 54, acc: 57.692307692307686, f1: 26.773062601377344, r: 0.39363117285074894
06/02/2019 01:21:54 *** epoch: 56 ***
06/02/2019 01:21:54 *** training ***
06/02/2019 01:21:54 step: 1820, epoch: 55, batch: 4, loss: 0.039865244179964066, acc: 82.8125, f1: 66.92052557724199, r: 0.9093604556814563
06/02/2019 01:21:55 step: 1825, epoch: 55, batch: 9, loss: 0.03778772056102753, acc: 81.25, f1: 77.95284780578898, r: 0.9365240491701405
06/02/2019 01:21:56 step: 1830, epoch: 55, batch: 14, loss: 0.03533255681395531, acc: 79.6875, f1: 75.47369297369298, r: 0.9504791929084131
06/02/2019 01:21:57 step: 1835, epoch: 55, batch: 19, loss: 0.042159851640462875, acc: 81.25, f1: 64.04405118690832, r: 0.8586496272812529
06/02/2019 01:21:57 step: 1840, epoch: 55, batch: 24, loss: 0.04326572269201279, acc: 82.8125, f1: 81.69415292353823, r: 0.8498253009469579
06/02/2019 01:21:58 step: 1845, epoch: 55, batch: 29, loss: 0.035632114857435226, acc: 85.9375, f1: 71.7873816193144, r: 0.9165499030779611
06/02/2019 01:21:58 *** evaluating ***
06/02/2019 01:21:59 step: 56, epoch: 55, acc: 58.54700854700855, f1: 27.805382287089603, r: 0.3953715370998447
06/02/2019 01:21:59 *** epoch: 57 ***
06/02/2019 01:21:59 *** training ***
06/02/2019 01:21:59 step: 1853, epoch: 56, batch: 4, loss: 0.031031528487801552, acc: 90.625, f1: 92.75252525252526, r: 0.962893288385357
06/02/2019 01:22:00 step: 1858, epoch: 56, batch: 9, loss: 0.03812165558338165, acc: 87.5, f1: 73.23415071770334, r: 0.8430517574589889
06/02/2019 01:22:01 step: 1863, epoch: 56, batch: 14, loss: 0.036559101194143295, acc: 89.0625, f1: 81.16168032411227, r: 0.9397082081234647
06/02/2019 01:22:01 step: 1868, epoch: 56, batch: 19, loss: 0.04212353378534317, acc: 89.0625, f1: 88.63061608824322, r: 0.8690404668167986
06/02/2019 01:22:02 step: 1873, epoch: 56, batch: 24, loss: 0.038386404514312744, acc: 82.8125, f1: 69.0117180577707, r: 0.9161223981969653
06/02/2019 01:22:03 step: 1878, epoch: 56, batch: 29, loss: 0.040136415511369705, acc: 85.9375, f1: 73.84920634920636, r: 0.8527885766152415
06/02/2019 01:22:03 *** evaluating ***
06/02/2019 01:22:04 step: 57, epoch: 56, acc: 58.97435897435898, f1: 27.603985460071755, r: 0.4012358998414383
06/02/2019 01:22:04 *** epoch: 58 ***
06/02/2019 01:22:04 *** training ***
06/02/2019 01:22:04 step: 1886, epoch: 57, batch: 4, loss: 0.03754710033535957, acc: 85.9375, f1: 71.26190476190476, r: 0.9201484665827342
06/02/2019 01:22:05 step: 1891, epoch: 57, batch: 9, loss: 0.04200317710638046, acc: 87.5, f1: 82.04248366013071, r: 0.8757981746400277
06/02/2019 01:22:06 step: 1896, epoch: 57, batch: 14, loss: 0.03943949565291405, acc: 84.375, f1: 68.96167557932263, r: 0.87368055604875
06/02/2019 01:22:06 step: 1901, epoch: 57, batch: 19, loss: 0.037348393350839615, acc: 87.5, f1: 71.0567367220593, r: 0.8880109373870326
06/02/2019 01:22:07 step: 1906, epoch: 57, batch: 24, loss: 0.039434999227523804, acc: 87.5, f1: 71.60533910533911, r: 0.8916514413730248
06/02/2019 01:22:08 step: 1911, epoch: 57, batch: 29, loss: 0.032914455980062485, acc: 89.0625, f1: 90.39571052301518, r: 0.9365358185896732
06/02/2019 01:22:08 *** evaluating ***
06/02/2019 01:22:08 step: 58, epoch: 57, acc: 60.256410256410255, f1: 30.12577264275057, r: 0.39490512510457515
06/02/2019 01:22:08 *** epoch: 59 ***
06/02/2019 01:22:08 *** training ***
06/02/2019 01:22:09 step: 1919, epoch: 58, batch: 4, loss: 0.034213658422231674, acc: 87.5, f1: 75.87377899877899, r: 0.9459634092829969
06/02/2019 01:22:10 step: 1924, epoch: 58, batch: 9, loss: 0.03818261995911598, acc: 82.8125, f1: 82.56823242117359, r: 0.9320614862373929
06/02/2019 01:22:11 step: 1929, epoch: 58, batch: 14, loss: 0.03713643550872803, acc: 87.5, f1: 85.23237768520787, r: 0.9124257900871142
06/02/2019 01:22:11 step: 1934, epoch: 58, batch: 19, loss: 0.04170731455087662, acc: 87.5, f1: 82.14285714285712, r: 0.9088182668860164
06/02/2019 01:22:12 step: 1939, epoch: 58, batch: 24, loss: 0.03858647122979164, acc: 76.5625, f1: 63.61852433281003, r: 0.9170795774560621
06/02/2019 01:22:13 step: 1944, epoch: 58, batch: 29, loss: 0.03909200057387352, acc: 85.9375, f1: 86.7903418335004, r: 0.9509809182580109
06/02/2019 01:22:13 *** evaluating ***
06/02/2019 01:22:13 step: 59, epoch: 58, acc: 58.54700854700855, f1: 27.281671997157076, r: 0.3998994391900001
06/02/2019 01:22:13 *** epoch: 60 ***
06/02/2019 01:22:13 *** training ***
06/02/2019 01:22:14 step: 1952, epoch: 59, batch: 4, loss: 0.03399302810430527, acc: 89.0625, f1: 81.0643274853801, r: 0.9361887082556379
06/02/2019 01:22:15 step: 1957, epoch: 59, batch: 9, loss: 0.04318735748529434, acc: 81.25, f1: 68.66025641025641, r: 0.8253128725694183
06/02/2019 01:22:16 step: 1962, epoch: 59, batch: 14, loss: 0.04087817296385765, acc: 82.8125, f1: 79.56291327719899, r: 0.91738555534207
06/02/2019 01:22:16 step: 1967, epoch: 59, batch: 19, loss: 0.03344482555985451, acc: 85.9375, f1: 84.5578231292517, r: 0.9371638149940232
06/02/2019 01:22:17 step: 1972, epoch: 59, batch: 24, loss: 0.0324697382748127, acc: 89.0625, f1: 79.38657407407408, r: 0.9489729853546187
06/02/2019 01:22:18 step: 1977, epoch: 59, batch: 29, loss: 0.03384681046009064, acc: 85.9375, f1: 80.59001670843776, r: 0.9490390467357526
06/02/2019 01:22:18 *** evaluating ***
06/02/2019 01:22:18 step: 60, epoch: 59, acc: 58.97435897435898, f1: 26.319489683350657, r: 0.39809918848092785
06/02/2019 01:22:18 *** epoch: 61 ***
06/02/2019 01:22:18 *** training ***
06/02/2019 01:22:19 step: 1985, epoch: 60, batch: 4, loss: 0.04199139401316643, acc: 81.25, f1: 63.122668422207596, r: 0.8875145643294787
06/02/2019 01:22:20 step: 1990, epoch: 60, batch: 9, loss: 0.0336146354675293, acc: 81.25, f1: 75.32616892911011, r: 0.9209745682108886
06/02/2019 01:22:21 step: 1995, epoch: 60, batch: 14, loss: 0.03919084742665291, acc: 87.5, f1: 82.92934717746749, r: 0.9025540330994447
06/02/2019 01:22:21 step: 2000, epoch: 60, batch: 19, loss: 0.03633728623390198, acc: 89.0625, f1: 86.73417537746806, r: 0.9290293169049089
06/02/2019 01:22:22 step: 2005, epoch: 60, batch: 24, loss: 0.033367738127708435, acc: 93.75, f1: 91.26183806856075, r: 0.9580691973713065
06/02/2019 01:22:23 step: 2010, epoch: 60, batch: 29, loss: 0.03538511320948601, acc: 87.5, f1: 85.0990675990676, r: 0.9318511636344385
06/02/2019 01:22:23 *** evaluating ***
06/02/2019 01:22:24 step: 61, epoch: 60, acc: 58.54700854700855, f1: 25.19462804220691, r: 0.39207552963429737
06/02/2019 01:22:24 *** epoch: 62 ***
06/02/2019 01:22:24 *** training ***
06/02/2019 01:22:24 step: 2018, epoch: 61, batch: 4, loss: 0.04108457267284393, acc: 81.25, f1: 58.67115705931496, r: 0.8105316276512092
06/02/2019 01:22:25 step: 2023, epoch: 61, batch: 9, loss: 0.03855973482131958, acc: 87.5, f1: 86.90310846560847, r: 0.919415452858006
06/02/2019 01:22:26 step: 2028, epoch: 61, batch: 14, loss: 0.03858666121959686, acc: 79.6875, f1: 71.29331490943369, r: 0.939852569741372
06/02/2019 01:22:26 step: 2033, epoch: 61, batch: 19, loss: 0.03282308951020241, acc: 87.5, f1: 85.90325018896448, r: 0.9359561424247513
06/02/2019 01:22:27 step: 2038, epoch: 61, batch: 24, loss: 0.035826675593853, acc: 87.5, f1: 84.02813498558179, r: 0.9319655290910718
06/02/2019 01:22:28 step: 2043, epoch: 61, batch: 29, loss: 0.03716595843434334, acc: 81.25, f1: 77.18813131313131, r: 0.9375486354368407
06/02/2019 01:22:28 *** evaluating ***
06/02/2019 01:22:28 step: 62, epoch: 61, acc: 57.692307692307686, f1: 26.892725142831736, r: 0.40179074027011913
06/02/2019 01:22:28 *** epoch: 63 ***
06/02/2019 01:22:28 *** training ***
06/02/2019 01:22:29 step: 2051, epoch: 62, batch: 4, loss: 0.03951354697346687, acc: 84.375, f1: 62.03968253968255, r: 0.8784639590633133
06/02/2019 01:22:30 step: 2056, epoch: 62, batch: 9, loss: 0.040002092719078064, acc: 78.125, f1: 62.679776363986896, r: 0.9123454526932768
06/02/2019 01:22:31 step: 2061, epoch: 62, batch: 14, loss: 0.04038320854306221, acc: 87.5, f1: 83.26839826839827, r: 0.9075456496627912
06/02/2019 01:22:31 step: 2066, epoch: 62, batch: 19, loss: 0.034325648099184036, acc: 92.1875, f1: 77.68959818649259, r: 0.9397087606750048
06/02/2019 01:22:32 step: 2071, epoch: 62, batch: 24, loss: 0.04147697985172272, acc: 79.6875, f1: 69.14772727272727, r: 0.8961082464719471
06/02/2019 01:22:33 step: 2076, epoch: 62, batch: 29, loss: 0.037658918648958206, acc: 85.9375, f1: 75.31655844155844, r: 0.9017939236017706
06/02/2019 01:22:33 *** evaluating ***
06/02/2019 01:22:33 step: 63, epoch: 62, acc: 57.692307692307686, f1: 25.17619553033298, r: 0.39482453019527686
06/02/2019 01:22:33 *** epoch: 64 ***
06/02/2019 01:22:33 *** training ***
06/02/2019 01:22:34 step: 2084, epoch: 63, batch: 4, loss: 0.03695356100797653, acc: 79.6875, f1: 58.62095141700405, r: 0.9032143566689518
06/02/2019 01:22:35 step: 2089, epoch: 63, batch: 9, loss: 0.040122490376234055, acc: 90.625, f1: 79.25983227299017, r: 0.9100236571775743
06/02/2019 01:22:35 step: 2094, epoch: 63, batch: 14, loss: 0.03494023159146309, acc: 90.625, f1: 86.82182495236155, r: 0.8834516034066892
06/02/2019 01:22:36 step: 2099, epoch: 63, batch: 19, loss: 0.03529108315706253, acc: 81.25, f1: 77.81993052484856, r: 0.938086615884044
06/02/2019 01:22:37 step: 2104, epoch: 63, batch: 24, loss: 0.041738808155059814, acc: 79.6875, f1: 76.81400175172227, r: 0.8997018860973273
06/02/2019 01:22:38 step: 2109, epoch: 63, batch: 29, loss: 0.03645314648747444, acc: 84.375, f1: 79.57279730073847, r: 0.9344148900867185
06/02/2019 01:22:38 *** evaluating ***
06/02/2019 01:22:38 step: 64, epoch: 63, acc: 59.401709401709404, f1: 30.029702839300366, r: 0.3999643650004933
06/02/2019 01:22:38 *** epoch: 65 ***
06/02/2019 01:22:38 *** training ***
06/02/2019 01:22:39 step: 2117, epoch: 64, batch: 4, loss: 0.03936856612563133, acc: 89.0625, f1: 63.28726127595807, r: 0.8834066644098311
06/02/2019 01:22:40 step: 2122, epoch: 64, batch: 9, loss: 0.03646831586956978, acc: 89.0625, f1: 83.25637325637327, r: 0.9321823881477478
06/02/2019 01:22:40 step: 2127, epoch: 64, batch: 14, loss: 0.03800276294350624, acc: 79.6875, f1: 64.54600954600956, r: 0.9299893143291995
06/02/2019 01:22:41 step: 2132, epoch: 64, batch: 19, loss: 0.03888874500989914, acc: 82.8125, f1: 80.44209956709956, r: 0.907809702833149
06/02/2019 01:22:42 step: 2137, epoch: 64, batch: 24, loss: 0.03925388306379318, acc: 90.625, f1: 81.93275634008393, r: 0.8651155892536402
06/02/2019 01:22:43 step: 2142, epoch: 64, batch: 29, loss: 0.037763532251119614, acc: 84.375, f1: 75.75062636262146, r: 0.9282575259168118
06/02/2019 01:22:43 *** evaluating ***
06/02/2019 01:22:43 step: 65, epoch: 64, acc: 58.97435897435898, f1: 26.329174411212097, r: 0.3959458763178744
06/02/2019 01:22:43 *** epoch: 66 ***
06/02/2019 01:22:43 *** training ***
06/02/2019 01:22:44 step: 2150, epoch: 65, batch: 4, loss: 0.04389912262558937, acc: 84.375, f1: 88.55285762679055, r: 0.9184818408397132
06/02/2019 01:22:45 step: 2155, epoch: 65, batch: 9, loss: 0.03216002508997917, acc: 89.0625, f1: 69.82142857142857, r: 0.9471213470578115
06/02/2019 01:22:45 step: 2160, epoch: 65, batch: 14, loss: 0.03526320308446884, acc: 90.625, f1: 83.65842759391147, r: 0.9136370653597436
06/02/2019 01:22:46 step: 2165, epoch: 65, batch: 19, loss: 0.03642922639846802, acc: 85.9375, f1: 60.03052503052503, r: 0.8846736907890536
06/02/2019 01:22:47 step: 2170, epoch: 65, batch: 24, loss: 0.03875862807035446, acc: 93.75, f1: 80.65874604990736, r: 0.9336358503736455
06/02/2019 01:22:48 step: 2175, epoch: 65, batch: 29, loss: 0.03449953719973564, acc: 85.9375, f1: 81.55722288915565, r: 0.8926230930415266
06/02/2019 01:22:48 *** evaluating ***
06/02/2019 01:22:48 step: 66, epoch: 65, acc: 57.692307692307686, f1: 25.618530020703933, r: 0.3974372543548026
06/02/2019 01:22:48 *** epoch: 67 ***
06/02/2019 01:22:48 *** training ***
06/02/2019 01:22:49 step: 2183, epoch: 66, batch: 4, loss: 0.03552774339914322, acc: 85.9375, f1: 71.97792162414804, r: 0.9006812840482287
06/02/2019 01:22:49 step: 2188, epoch: 66, batch: 9, loss: 0.03239677846431732, acc: 90.625, f1: 87.43734335839599, r: 0.9511036637024453
06/02/2019 01:22:50 step: 2193, epoch: 66, batch: 14, loss: 0.038682036101818085, acc: 81.25, f1: 61.72608521746452, r: 0.8975974266875808
06/02/2019 01:22:51 step: 2198, epoch: 66, batch: 19, loss: 0.036790765821933746, acc: 73.4375, f1: 68.65266032417196, r: 0.9369042760328965
06/02/2019 01:22:51 step: 2203, epoch: 66, batch: 24, loss: 0.02976987697184086, acc: 84.375, f1: 54.065180102915946, r: 0.9545990842253872
06/02/2019 01:22:52 step: 2208, epoch: 66, batch: 29, loss: 0.034339018166065216, acc: 87.5, f1: 81.29438178780283, r: 0.9261924540770679
06/02/2019 01:22:53 *** evaluating ***
06/02/2019 01:22:53 step: 67, epoch: 66, acc: 59.401709401709404, f1: 28.74707161981258, r: 0.40034890697016845
06/02/2019 01:22:53 *** epoch: 68 ***
06/02/2019 01:22:53 *** training ***
06/02/2019 01:22:53 step: 2216, epoch: 67, batch: 4, loss: 0.038259267807006836, acc: 92.1875, f1: 86.86702008130578, r: 0.9065385824420883
06/02/2019 01:22:54 step: 2221, epoch: 67, batch: 9, loss: 0.031035233289003372, acc: 87.5, f1: 84.51762523191094, r: 0.9184066256327357
06/02/2019 01:22:55 step: 2226, epoch: 67, batch: 14, loss: 0.03671064227819443, acc: 85.9375, f1: 78.79046123944083, r: 0.8846581187981934
06/02/2019 01:22:56 step: 2231, epoch: 67, batch: 19, loss: 0.03286974877119064, acc: 89.0625, f1: 88.29200497403261, r: 0.9344571474089851
06/02/2019 01:22:57 step: 2236, epoch: 67, batch: 24, loss: 0.03377027064561844, acc: 87.5, f1: 77.22156209129479, r: 0.9326435749619901
06/02/2019 01:22:57 step: 2241, epoch: 67, batch: 29, loss: 0.03162137418985367, acc: 93.75, f1: 95.77859338728905, r: 0.9509168091724195
06/02/2019 01:22:58 *** evaluating ***
06/02/2019 01:22:58 step: 68, epoch: 67, acc: 59.82905982905983, f1: 26.95424598070962, r: 0.40218109738246177
06/02/2019 01:22:58 *** epoch: 69 ***
06/02/2019 01:22:58 *** training ***
06/02/2019 01:22:59 step: 2249, epoch: 68, batch: 4, loss: 0.03345407173037529, acc: 82.8125, f1: 73.1747333548804, r: 0.9271214393215926
06/02/2019 01:22:59 step: 2254, epoch: 68, batch: 9, loss: 0.04061928391456604, acc: 85.9375, f1: 79.57516339869281, r: 0.8125539358588135
06/02/2019 01:23:00 step: 2259, epoch: 68, batch: 14, loss: 0.03770697861909866, acc: 90.625, f1: 69.32572209211554, r: 0.9141036526416404
06/02/2019 01:23:01 step: 2264, epoch: 68, batch: 19, loss: 0.03957067430019379, acc: 84.375, f1: 69.8953823953824, r: 0.8973352785179038
06/02/2019 01:23:01 step: 2269, epoch: 68, batch: 24, loss: 0.03203243762254715, acc: 87.5, f1: 77.60782949570135, r: 0.9587182179727377
06/02/2019 01:23:02 step: 2274, epoch: 68, batch: 29, loss: 0.0364542193710804, acc: 85.9375, f1: 77.72108843537414, r: 0.8863696934525409
06/02/2019 01:23:03 *** evaluating ***
06/02/2019 01:23:03 step: 69, epoch: 68, acc: 58.54700854700855, f1: 25.2782027743778, r: 0.40980957836248455
06/02/2019 01:23:03 *** epoch: 70 ***
06/02/2019 01:23:03 *** training ***
06/02/2019 01:23:03 step: 2282, epoch: 69, batch: 4, loss: 0.03517608717083931, acc: 87.5, f1: 65.52729044834308, r: 0.9266507373821221
06/02/2019 01:23:04 step: 2287, epoch: 69, batch: 9, loss: 0.03818553686141968, acc: 89.0625, f1: 84.69076402321083, r: 0.931714476808276
06/02/2019 01:23:04 step: 2292, epoch: 69, batch: 14, loss: 0.03337262570858002, acc: 81.25, f1: 75.01805706031058, r: 0.9351289498956471
06/02/2019 01:23:05 step: 2297, epoch: 69, batch: 19, loss: 0.036288123577833176, acc: 84.375, f1: 80.31109424201529, r: 0.939926796253948
06/02/2019 01:23:05 step: 2302, epoch: 69, batch: 24, loss: 0.03331834077835083, acc: 90.625, f1: 93.88846991788168, r: 0.9369444161976459
06/02/2019 01:23:06 step: 2307, epoch: 69, batch: 29, loss: 0.032625697553157806, acc: 90.625, f1: 78.3990820604093, r: 0.9100206516262747
06/02/2019 01:23:06 *** evaluating ***
06/02/2019 01:23:06 step: 70, epoch: 69, acc: 59.401709401709404, f1: 27.588314842413205, r: 0.3911930072915979
06/02/2019 01:23:06 *** epoch: 71 ***
06/02/2019 01:23:06 *** training ***
06/02/2019 01:23:07 step: 2315, epoch: 70, batch: 4, loss: 0.034942638128995895, acc: 85.9375, f1: 78.01447833775418, r: 0.9446688687337449
06/02/2019 01:23:07 step: 2320, epoch: 70, batch: 9, loss: 0.03233727067708969, acc: 81.25, f1: 74.8994755244755, r: 0.9462434713614727
06/02/2019 01:23:08 step: 2325, epoch: 70, batch: 14, loss: 0.03711461275815964, acc: 81.25, f1: 76.9779426922284, r: 0.9314216449834208
06/02/2019 01:23:08 step: 2330, epoch: 70, batch: 19, loss: 0.038750335574150085, acc: 82.8125, f1: 66.27136752136752, r: 0.920545157911835
06/02/2019 01:23:09 step: 2335, epoch: 70, batch: 24, loss: 0.03550024703145027, acc: 87.5, f1: 58.47846921852473, r: 0.8618918864011753
06/02/2019 01:23:09 step: 2340, epoch: 70, batch: 29, loss: 0.03753606230020523, acc: 82.8125, f1: 71.96160161677405, r: 0.9223420280089965
06/02/2019 01:23:09 *** evaluating ***
06/02/2019 01:23:09 step: 71, epoch: 70, acc: 59.401709401709404, f1: 27.489674617648163, r: 0.40918798772644316
06/02/2019 01:23:09 *** epoch: 72 ***
06/02/2019 01:23:09 *** training ***
06/02/2019 01:23:10 step: 2348, epoch: 71, batch: 4, loss: 0.03955412656068802, acc: 82.8125, f1: 66.63003663003664, r: 0.8490484656783079
06/02/2019 01:23:10 step: 2353, epoch: 71, batch: 9, loss: 0.03504566103219986, acc: 82.8125, f1: 72.5046992481203, r: 0.943811810614074
06/02/2019 01:23:11 step: 2358, epoch: 71, batch: 14, loss: 0.03483381122350693, acc: 87.5, f1: 75.03947033358799, r: 0.870520793916415
06/02/2019 01:23:11 step: 2363, epoch: 71, batch: 19, loss: 0.03337976336479187, acc: 87.5, f1: 65.48389819777994, r: 0.93179012529942
06/02/2019 01:23:12 step: 2368, epoch: 71, batch: 24, loss: 0.03532929718494415, acc: 89.0625, f1: 78.96720831503441, r: 0.9172531449335237
06/02/2019 01:23:12 step: 2373, epoch: 71, batch: 29, loss: 0.031868189573287964, acc: 85.9375, f1: 69.25305087069793, r: 0.948143275974464
06/02/2019 01:23:13 *** evaluating ***
06/02/2019 01:23:13 step: 72, epoch: 71, acc: 59.401709401709404, f1: 28.72418933625981, r: 0.4037426127392439
06/02/2019 01:23:13 *** epoch: 73 ***
06/02/2019 01:23:13 *** training ***
06/02/2019 01:23:13 step: 2381, epoch: 72, batch: 4, loss: 0.03998717665672302, acc: 81.25, f1: 69.53800366300366, r: 0.8408261424546478
06/02/2019 01:23:14 step: 2386, epoch: 72, batch: 9, loss: 0.03438475728034973, acc: 89.0625, f1: 77.1910350678733, r: 0.9212870421807993
06/02/2019 01:23:14 step: 2391, epoch: 72, batch: 14, loss: 0.0386037714779377, acc: 85.9375, f1: 82.26331194416302, r: 0.9266052582842714
06/02/2019 01:23:15 step: 2396, epoch: 72, batch: 19, loss: 0.03892388939857483, acc: 87.5, f1: 69.0879416282642, r: 0.866427772371787
06/02/2019 01:23:15 step: 2401, epoch: 72, batch: 24, loss: 0.03288029879331589, acc: 82.8125, f1: 82.27725096365245, r: 0.9622238416206763
06/02/2019 01:23:16 step: 2406, epoch: 72, batch: 29, loss: 0.034226223826408386, acc: 89.0625, f1: 83.07539682539682, r: 0.9421263408937971
06/02/2019 01:23:16 *** evaluating ***
06/02/2019 01:23:16 step: 73, epoch: 72, acc: 57.26495726495726, f1: 24.978829649882282, r: 0.4061902915021147
06/02/2019 01:23:16 *** epoch: 74 ***
06/02/2019 01:23:16 *** training ***
06/02/2019 01:23:17 step: 2414, epoch: 73, batch: 4, loss: 0.03146471455693245, acc: 85.9375, f1: 85.65160926030491, r: 0.94561953676226
06/02/2019 01:23:17 step: 2419, epoch: 73, batch: 9, loss: 0.03887089341878891, acc: 79.6875, f1: 69.6868824110672, r: 0.8619101021834186
06/02/2019 01:23:17 step: 2424, epoch: 73, batch: 14, loss: 0.031953416764736176, acc: 85.9375, f1: 62.372432301677584, r: 0.9188588430569185
06/02/2019 01:23:18 step: 2429, epoch: 73, batch: 19, loss: 0.03456737473607063, acc: 84.375, f1: 75.09607351712616, r: 0.8553308586380777
06/02/2019 01:23:18 step: 2434, epoch: 73, batch: 24, loss: 0.040176115930080414, acc: 81.25, f1: 55.59654631083203, r: 0.9055321784392016
06/02/2019 01:23:19 step: 2439, epoch: 73, batch: 29, loss: 0.031069451943039894, acc: 85.9375, f1: 72.3656204906205, r: 0.9628767257871601
06/02/2019 01:23:19 *** evaluating ***
06/02/2019 01:23:19 step: 74, epoch: 73, acc: 59.82905982905983, f1: 28.408229825794862, r: 0.4056593774586762
06/02/2019 01:23:19 *** epoch: 75 ***
06/02/2019 01:23:19 *** training ***
06/02/2019 01:23:20 step: 2447, epoch: 74, batch: 4, loss: 0.030386358499526978, acc: 93.75, f1: 89.98138761754572, r: 0.9578533499433853
06/02/2019 01:23:21 step: 2452, epoch: 74, batch: 9, loss: 0.032715946435928345, acc: 90.625, f1: 85.30045351473923, r: 0.9417477614675459
06/02/2019 01:23:21 step: 2457, epoch: 74, batch: 14, loss: 0.037967994809150696, acc: 78.125, f1: 66.44546425796425, r: 0.916689370603797
06/02/2019 01:23:22 step: 2462, epoch: 74, batch: 19, loss: 0.03977569565176964, acc: 87.5, f1: 84.32678432678433, r: 0.8495546925957876
06/02/2019 01:23:22 step: 2467, epoch: 74, batch: 24, loss: 0.03347980976104736, acc: 85.9375, f1: 80.375481000481, r: 0.9415916922464997
06/02/2019 01:23:23 step: 2472, epoch: 74, batch: 29, loss: 0.03569863736629486, acc: 82.8125, f1: 83.29772613693154, r: 0.9316580907182532
06/02/2019 01:23:23 *** evaluating ***
06/02/2019 01:23:23 step: 75, epoch: 74, acc: 59.82905982905983, f1: 27.232911725696106, r: 0.41532978123692027
06/02/2019 01:23:23 *** epoch: 76 ***
06/02/2019 01:23:23 *** training ***
06/02/2019 01:23:23 step: 2480, epoch: 75, batch: 4, loss: 0.031998056918382645, acc: 85.9375, f1: 74.56027993244983, r: 0.928681471431106
06/02/2019 01:23:24 step: 2485, epoch: 75, batch: 9, loss: 0.035978034138679504, acc: 84.375, f1: 70.76131272378736, r: 0.8853696585988444
06/02/2019 01:23:24 step: 2490, epoch: 75, batch: 14, loss: 0.03331219404935837, acc: 89.0625, f1: 81.78704157477742, r: 0.9367353501827429
06/02/2019 01:23:25 step: 2495, epoch: 75, batch: 19, loss: 0.03526188060641289, acc: 87.5, f1: 69.73171565276827, r: 0.9171247894530179
06/02/2019 01:23:25 step: 2500, epoch: 75, batch: 24, loss: 0.03353169932961464, acc: 89.0625, f1: 85.3219696969697, r: 0.9240956387376917
06/02/2019 01:23:26 step: 2505, epoch: 75, batch: 29, loss: 0.03656639903783798, acc: 82.8125, f1: 78.11493558776168, r: 0.8886118770375493
06/02/2019 01:23:26 *** evaluating ***
06/02/2019 01:23:26 step: 76, epoch: 75, acc: 60.256410256410255, f1: 27.85379303236446, r: 0.41237565651799
06/02/2019 01:23:26 *** epoch: 77 ***
06/02/2019 01:23:26 *** training ***
06/02/2019 01:23:27 step: 2513, epoch: 76, batch: 4, loss: 0.03690387308597565, acc: 75.0, f1: 60.03157673325741, r: 0.915870367404917
06/02/2019 01:23:27 step: 2518, epoch: 76, batch: 9, loss: 0.04074358195066452, acc: 82.8125, f1: 80.97222222222223, r: 0.8639862772213972
06/02/2019 01:23:28 step: 2523, epoch: 76, batch: 14, loss: 0.02944907546043396, acc: 87.5, f1: 77.2539482375548, r: 0.924255538788411
06/02/2019 01:23:28 step: 2528, epoch: 76, batch: 19, loss: 0.029835399240255356, acc: 87.5, f1: 80.86118072960178, r: 0.9537118136408629
06/02/2019 01:23:29 step: 2533, epoch: 76, batch: 24, loss: 0.03497260808944702, acc: 87.5, f1: 72.95698924731182, r: 0.9199660199595676
06/02/2019 01:23:29 step: 2538, epoch: 76, batch: 29, loss: 0.03610577806830406, acc: 89.0625, f1: 73.56085526315789, r: 0.898499748537695
06/02/2019 01:23:29 *** evaluating ***
06/02/2019 01:23:30 step: 77, epoch: 76, acc: 60.68376068376068, f1: 27.19427796657311, r: 0.40313499210063447
06/02/2019 01:23:30 *** epoch: 78 ***
06/02/2019 01:23:30 *** training ***
06/02/2019 01:23:30 step: 2546, epoch: 77, batch: 4, loss: 0.02638578787446022, acc: 85.9375, f1: 82.9287270463741, r: 0.9563595488295326
06/02/2019 01:23:31 step: 2551, epoch: 77, batch: 9, loss: 0.03163889795541763, acc: 89.0625, f1: 85.35487501004741, r: 0.928219046735607
06/02/2019 01:23:31 step: 2556, epoch: 77, batch: 14, loss: 0.0289728082716465, acc: 90.625, f1: 87.4829931972789, r: 0.9186345760742975
06/02/2019 01:23:31 step: 2561, epoch: 77, batch: 19, loss: 0.032057639211416245, acc: 87.5, f1: 73.53553921568627, r: 0.924900178698105
06/02/2019 01:23:32 step: 2566, epoch: 77, batch: 24, loss: 0.039111100137233734, acc: 87.5, f1: 66.50556680161944, r: 0.9285418681679407
06/02/2019 01:23:32 step: 2571, epoch: 77, batch: 29, loss: 0.03121315687894821, acc: 82.8125, f1: 72.13450292397661, r: 0.9580720380544852
06/02/2019 01:23:33 *** evaluating ***
06/02/2019 01:23:33 step: 78, epoch: 77, acc: 58.97435897435898, f1: 26.726175802503917, r: 0.4038360697858555
06/02/2019 01:23:33 *** epoch: 79 ***
06/02/2019 01:23:33 *** training ***
06/02/2019 01:23:33 step: 2579, epoch: 78, batch: 4, loss: 0.03373618796467781, acc: 87.5, f1: 78.94472864300451, r: 0.9472186647362683
06/02/2019 01:23:34 step: 2584, epoch: 78, batch: 9, loss: 0.02987920679152012, acc: 84.375, f1: 72.26001010483769, r: 0.9563637986289656
06/02/2019 01:23:34 step: 2589, epoch: 78, batch: 14, loss: 0.04006397724151611, acc: 85.9375, f1: 71.85222763347764, r: 0.8822035762474375
06/02/2019 01:23:35 step: 2594, epoch: 78, batch: 19, loss: 0.035375338047742844, acc: 85.9375, f1: 70.15482054890923, r: 0.90677300631987
06/02/2019 01:23:35 step: 2599, epoch: 78, batch: 24, loss: 0.03153068199753761, acc: 90.625, f1: 79.89177489177489, r: 0.9238211229705525
06/02/2019 01:23:36 step: 2604, epoch: 78, batch: 29, loss: 0.034291353076696396, acc: 90.625, f1: 81.73397843986079, r: 0.8615295770925209
06/02/2019 01:23:36 *** evaluating ***
06/02/2019 01:23:36 step: 79, epoch: 78, acc: 60.256410256410255, f1: 28.3452964519141, r: 0.40827283000476444
06/02/2019 01:23:36 *** epoch: 80 ***
06/02/2019 01:23:36 *** training ***
06/02/2019 01:23:37 step: 2612, epoch: 79, batch: 4, loss: 0.03204847127199173, acc: 85.9375, f1: 75.33763750869014, r: 0.9332536032098783
06/02/2019 01:23:37 step: 2617, epoch: 79, batch: 9, loss: 0.037995073944330215, acc: 84.375, f1: 70.3339517625232, r: 0.8882362846701407
06/02/2019 01:23:38 step: 2622, epoch: 79, batch: 14, loss: 0.03574111685156822, acc: 90.625, f1: 80.20626844632395, r: 0.9328353294312112
06/02/2019 01:23:38 step: 2627, epoch: 79, batch: 19, loss: 0.03408189117908478, acc: 93.75, f1: 86.21582692363093, r: 0.9015645499217239
06/02/2019 01:23:39 step: 2632, epoch: 79, batch: 24, loss: 0.03597097471356392, acc: 90.625, f1: 91.284175269302, r: 0.9257857527113814
06/02/2019 01:23:39 step: 2637, epoch: 79, batch: 29, loss: 0.0328596867620945, acc: 89.0625, f1: 77.70957113062376, r: 0.9412950299522532
06/02/2019 01:23:39 *** evaluating ***
06/02/2019 01:23:39 step: 80, epoch: 79, acc: 59.401709401709404, f1: 27.01907444248236, r: 0.40487565242638346
06/02/2019 01:23:39 *** epoch: 81 ***
06/02/2019 01:23:39 *** training ***
06/02/2019 01:23:40 step: 2645, epoch: 80, batch: 4, loss: 0.0381026454269886, acc: 76.5625, f1: 65.80691786283892, r: 0.8936652863325868
06/02/2019 01:23:40 step: 2650, epoch: 80, batch: 9, loss: 0.03794265538454056, acc: 87.5, f1: 81.1614774114774, r: 0.8822966299016993
06/02/2019 01:23:41 step: 2655, epoch: 80, batch: 14, loss: 0.02827085554599762, acc: 90.625, f1: 90.2545866831581, r: 0.8697617849178617
06/02/2019 01:23:41 step: 2660, epoch: 80, batch: 19, loss: 0.03347107395529747, acc: 81.25, f1: 71.59306240701589, r: 0.9566758767554873
06/02/2019 01:23:42 step: 2665, epoch: 80, batch: 24, loss: 0.04126136749982834, acc: 84.375, f1: 65.23235735735736, r: 0.8505832299488457
06/02/2019 01:23:42 step: 2670, epoch: 80, batch: 29, loss: 0.030638892203569412, acc: 87.5, f1: 73.26424800689506, r: 0.9612107669148344
06/02/2019 01:23:43 *** evaluating ***
06/02/2019 01:23:43 step: 81, epoch: 80, acc: 59.401709401709404, f1: 27.154022052583066, r: 0.4059124483598897
06/02/2019 01:23:43 *** epoch: 82 ***
06/02/2019 01:23:43 *** training ***
06/02/2019 01:23:43 step: 2678, epoch: 81, batch: 4, loss: 0.03298259899020195, acc: 82.8125, f1: 67.90040318186772, r: 0.9346460326553568
06/02/2019 01:23:44 step: 2683, epoch: 81, batch: 9, loss: 0.03244105353951454, acc: 82.8125, f1: 76.34956355701037, r: 0.9588201862752076
06/02/2019 01:23:44 step: 2688, epoch: 81, batch: 14, loss: 0.03435763716697693, acc: 87.5, f1: 68.26072606599895, r: 0.902828862809016
06/02/2019 01:23:45 step: 2693, epoch: 81, batch: 19, loss: 0.033115044236183167, acc: 82.8125, f1: 63.662059188374975, r: 0.9300612312888386
06/02/2019 01:23:45 step: 2698, epoch: 81, batch: 24, loss: 0.03239065781235695, acc: 81.25, f1: 70.84633853541418, r: 0.9349716650326859
06/02/2019 01:23:45 step: 2703, epoch: 81, batch: 29, loss: 0.03480757400393486, acc: 76.5625, f1: 74.2642144577129, r: 0.9583944478822856
06/02/2019 01:23:46 *** evaluating ***
06/02/2019 01:23:46 step: 82, epoch: 81, acc: 60.68376068376068, f1: 27.342249399759904, r: 0.4049714851150003
06/02/2019 01:23:46 *** epoch: 83 ***
06/02/2019 01:23:46 *** training ***
06/02/2019 01:23:46 step: 2711, epoch: 82, batch: 4, loss: 0.029273293912410736, acc: 92.1875, f1: 88.80586080586082, r: 0.9329300642408912
06/02/2019 01:23:47 step: 2716, epoch: 82, batch: 9, loss: 0.03536631166934967, acc: 85.9375, f1: 76.05937191463508, r: 0.8784220287788338
06/02/2019 01:23:47 step: 2721, epoch: 82, batch: 14, loss: 0.035175248980522156, acc: 85.9375, f1: 84.01116427432217, r: 0.9124369709155602
06/02/2019 01:23:48 step: 2726, epoch: 82, batch: 19, loss: 0.03356573358178139, acc: 85.9375, f1: 72.65152437786708, r: 0.9571623855946324
06/02/2019 01:23:48 step: 2731, epoch: 82, batch: 24, loss: 0.03313767910003662, acc: 84.375, f1: 80.13377225718423, r: 0.9312506805333298
06/02/2019 01:23:49 step: 2736, epoch: 82, batch: 29, loss: 0.034700386226177216, acc: 85.9375, f1: 76.73611111111111, r: 0.9154901520921653
06/02/2019 01:23:49 *** evaluating ***
06/02/2019 01:23:49 step: 83, epoch: 82, acc: 61.111111111111114, f1: 28.22019860221574, r: 0.4138229018155904
06/02/2019 01:23:49 *** epoch: 84 ***
06/02/2019 01:23:49 *** training ***
06/02/2019 01:23:49 step: 2744, epoch: 83, batch: 4, loss: 0.03890072554349899, acc: 82.8125, f1: 75.18165887044474, r: 0.8968828603050061
06/02/2019 01:23:50 step: 2749, epoch: 83, batch: 9, loss: 0.03790769726037979, acc: 87.5, f1: 65.96382783882784, r: 0.9161142344658599
06/02/2019 01:23:50 step: 2754, epoch: 83, batch: 14, loss: 0.023322906345129013, acc: 95.3125, f1: 91.1654411764706, r: 0.9443362416757204
06/02/2019 01:23:51 step: 2759, epoch: 83, batch: 19, loss: 0.03313077241182327, acc: 84.375, f1: 76.37502703916365, r: 0.9296813498488521
06/02/2019 01:23:51 step: 2764, epoch: 83, batch: 24, loss: 0.03415759280323982, acc: 89.0625, f1: 90.88034188034189, r: 0.9516545132747617
06/02/2019 01:23:52 step: 2769, epoch: 83, batch: 29, loss: 0.03383767977356911, acc: 79.6875, f1: 66.01209135691894, r: 0.9328242527585657
06/02/2019 01:23:52 *** evaluating ***
06/02/2019 01:23:52 step: 84, epoch: 83, acc: 58.119658119658126, f1: 27.095700922529854, r: 0.3967590908022729
06/02/2019 01:23:52 *** epoch: 85 ***
06/02/2019 01:23:52 *** training ***
06/02/2019 01:23:53 step: 2777, epoch: 84, batch: 4, loss: 0.03242339566349983, acc: 87.5, f1: 76.14808139164757, r: 0.9528066132658816
06/02/2019 01:23:53 step: 2782, epoch: 84, batch: 9, loss: 0.03182218223810196, acc: 84.375, f1: 70.23919933494402, r: 0.9255101819551806
06/02/2019 01:23:54 step: 2787, epoch: 84, batch: 14, loss: 0.03412800282239914, acc: 81.25, f1: 59.24683528834607, r: 0.9212585057757966
06/02/2019 01:23:54 step: 2792, epoch: 84, batch: 19, loss: 0.02886774018406868, acc: 85.9375, f1: 79.31519274376416, r: 0.9570257551446545
06/02/2019 01:23:55 step: 2797, epoch: 84, batch: 24, loss: 0.031359344720840454, acc: 93.75, f1: 77.87671672442562, r: 0.9265179378919242
06/02/2019 01:23:55 step: 2802, epoch: 84, batch: 29, loss: 0.03216690570116043, acc: 89.0625, f1: 65.63717440999388, r: 0.9184272558190381
06/02/2019 01:23:55 *** evaluating ***
06/02/2019 01:23:56 step: 85, epoch: 84, acc: 59.82905982905983, f1: 27.47124465474849, r: 0.40678181275492115
06/02/2019 01:23:56 *** epoch: 86 ***
06/02/2019 01:23:56 *** training ***
06/02/2019 01:23:56 step: 2810, epoch: 85, batch: 4, loss: 0.0338154099881649, acc: 90.625, f1: 78.23799136879632, r: 0.9146031385946927
06/02/2019 01:23:57 step: 2815, epoch: 85, batch: 9, loss: 0.030161112546920776, acc: 84.375, f1: 68.0621693121693, r: 0.9479656836245768
06/02/2019 01:23:57 step: 2820, epoch: 85, batch: 14, loss: 0.03373420238494873, acc: 87.5, f1: 76.77777522443851, r: 0.9516717979641047
06/02/2019 01:23:57 step: 2825, epoch: 85, batch: 19, loss: 0.03047270141541958, acc: 89.0625, f1: 75.06235431235432, r: 0.9599017927913717
06/02/2019 01:23:58 step: 2830, epoch: 85, batch: 24, loss: 0.023510903120040894, acc: 90.625, f1: 84.34983801509225, r: 0.9751568078426165
06/02/2019 01:23:58 step: 2835, epoch: 85, batch: 29, loss: 0.03632541373372078, acc: 87.5, f1: 82.71085858585859, r: 0.9458621742061767
06/02/2019 01:23:59 *** evaluating ***
06/02/2019 01:23:59 step: 86, epoch: 85, acc: 60.68376068376068, f1: 27.396981789598485, r: 0.4014236115347553
06/02/2019 01:23:59 *** epoch: 87 ***
06/02/2019 01:23:59 *** training ***
06/02/2019 01:23:59 step: 2843, epoch: 86, batch: 4, loss: 0.03420364484190941, acc: 81.25, f1: 68.63519228227831, r: 0.9455599265605061
06/02/2019 01:24:00 step: 2848, epoch: 86, batch: 9, loss: 0.03254583477973938, acc: 84.375, f1: 77.20729473778253, r: 0.9430160619226976
06/02/2019 01:24:00 step: 2853, epoch: 86, batch: 14, loss: 0.03320062905550003, acc: 89.0625, f1: 78.29208754208754, r: 0.9318741587748899
06/02/2019 01:24:01 step: 2858, epoch: 86, batch: 19, loss: 0.03702400624752045, acc: 82.8125, f1: 82.90636446886448, r: 0.9268225873705899
06/02/2019 01:24:01 step: 2863, epoch: 86, batch: 24, loss: 0.03318951651453972, acc: 95.3125, f1: 93.10977001654969, r: 0.9280038598888032
06/02/2019 01:24:02 step: 2868, epoch: 86, batch: 29, loss: 0.0344536267220974, acc: 87.5, f1: 86.98552497922246, r: 0.9378085186003328
06/02/2019 01:24:02 *** evaluating ***
06/02/2019 01:24:02 step: 87, epoch: 86, acc: 57.26495726495726, f1: 25.741437590917204, r: 0.40003421620367746
06/02/2019 01:24:02 *** epoch: 88 ***
06/02/2019 01:24:02 *** training ***
06/02/2019 01:24:02 step: 2876, epoch: 87, batch: 4, loss: 0.03419763222336769, acc: 85.9375, f1: 71.69175070960785, r: 0.9295381108856644
06/02/2019 01:24:03 step: 2881, epoch: 87, batch: 9, loss: 0.03355416655540466, acc: 85.9375, f1: 80.87263613579402, r: 0.8976632137679077
06/02/2019 01:24:03 step: 2886, epoch: 87, batch: 14, loss: 0.03233947604894638, acc: 82.8125, f1: 68.20485138499845, r: 0.9333260794495807
06/02/2019 01:24:04 step: 2891, epoch: 87, batch: 19, loss: 0.034537214785814285, acc: 89.0625, f1: 90.43126294445428, r: 0.9370472942486001
06/02/2019 01:24:04 step: 2896, epoch: 87, batch: 24, loss: 0.03021281398832798, acc: 90.625, f1: 76.63219133807368, r: 0.9175904802810139
06/02/2019 01:24:05 step: 2901, epoch: 87, batch: 29, loss: 0.031941723078489304, acc: 89.0625, f1: 84.57090528519099, r: 0.9196213992538272
06/02/2019 01:24:05 *** evaluating ***
06/02/2019 01:24:05 step: 88, epoch: 87, acc: 61.111111111111114, f1: 27.6192650832741, r: 0.4104361692258589
06/02/2019 01:24:05 *** epoch: 89 ***
06/02/2019 01:24:05 *** training ***
06/02/2019 01:24:06 step: 2909, epoch: 88, batch: 4, loss: 0.036718107759952545, acc: 89.0625, f1: 75.98074453015053, r: 0.8925247573246339
06/02/2019 01:24:06 step: 2914, epoch: 88, batch: 9, loss: 0.026344848796725273, acc: 84.375, f1: 82.22466068210747, r: 0.9710127598870669
06/02/2019 01:24:07 step: 2919, epoch: 88, batch: 14, loss: 0.03082907944917679, acc: 87.5, f1: 74.90725586470266, r: 0.9579680772821583
06/02/2019 01:24:07 step: 2924, epoch: 88, batch: 19, loss: 0.028068281710147858, acc: 90.625, f1: 86.40151515151516, r: 0.9636584194353559
06/02/2019 01:24:08 step: 2929, epoch: 88, batch: 24, loss: 0.03133934363722801, acc: 82.8125, f1: 78.49582042837726, r: 0.9425924250144783
06/02/2019 01:24:08 step: 2934, epoch: 88, batch: 29, loss: 0.02737078070640564, acc: 87.5, f1: 82.78337403337403, r: 0.9695944138039836
06/02/2019 01:24:08 *** evaluating ***
06/02/2019 01:24:09 step: 89, epoch: 88, acc: 60.68376068376068, f1: 27.763025526496566, r: 0.4107347199249487
06/02/2019 01:24:09 *** epoch: 90 ***
06/02/2019 01:24:09 *** training ***
06/02/2019 01:24:09 step: 2942, epoch: 89, batch: 4, loss: 0.03836679086089134, acc: 92.1875, f1: 80.02976190476191, r: 0.9086992742597205
06/02/2019 01:24:09 step: 2947, epoch: 89, batch: 9, loss: 0.0329875610768795, acc: 89.0625, f1: 86.71296296296296, r: 0.952679712379089
06/02/2019 01:24:10 step: 2952, epoch: 89, batch: 14, loss: 0.033126432448625565, acc: 93.75, f1: 90.56491729884343, r: 0.9517660468505591
06/02/2019 01:24:10 step: 2957, epoch: 89, batch: 19, loss: 0.03009062446653843, acc: 98.4375, f1: 94.04761904761905, r: 0.9556474525792835
06/02/2019 01:24:11 step: 2962, epoch: 89, batch: 24, loss: 0.03188897669315338, acc: 82.8125, f1: 80.1281522710094, r: 0.9175482688364768
06/02/2019 01:24:11 step: 2967, epoch: 89, batch: 29, loss: 0.029983529821038246, acc: 76.5625, f1: 71.03650492784415, r: 0.9366244203943952
06/02/2019 01:24:12 *** evaluating ***
06/02/2019 01:24:12 step: 90, epoch: 89, acc: 60.256410256410255, f1: 28.78298909925416, r: 0.4129542912716026
06/02/2019 01:24:12 *** epoch: 91 ***
06/02/2019 01:24:12 *** training ***
06/02/2019 01:24:12 step: 2975, epoch: 90, batch: 4, loss: 0.030948996543884277, acc: 93.75, f1: 80.05933498206679, r: 0.9418116747080434
06/02/2019 01:24:13 step: 2980, epoch: 90, batch: 9, loss: 0.03351868689060211, acc: 82.8125, f1: 57.32623857623857, r: 0.9321498199216038
06/02/2019 01:24:13 step: 2985, epoch: 90, batch: 14, loss: 0.02968618832528591, acc: 87.5, f1: 72.9874213836478, r: 0.9651207306718578
06/02/2019 01:24:14 step: 2990, epoch: 90, batch: 19, loss: 0.035008858889341354, acc: 90.625, f1: 85.30555555555554, r: 0.9395491739764265
06/02/2019 01:24:14 step: 2995, epoch: 90, batch: 24, loss: 0.0320914089679718, acc: 85.9375, f1: 71.78301899046579, r: 0.9052932444219871
06/02/2019 01:24:14 step: 3000, epoch: 90, batch: 29, loss: 0.03277771547436714, acc: 89.0625, f1: 75.34798534798534, r: 0.9438305936542516
06/02/2019 01:24:15 *** evaluating ***
06/02/2019 01:24:15 step: 91, epoch: 90, acc: 58.97435897435898, f1: 28.01272128284512, r: 0.4110297974465637
06/02/2019 01:24:15 *** epoch: 92 ***
06/02/2019 01:24:15 *** training ***
06/02/2019 01:24:15 step: 3008, epoch: 91, batch: 4, loss: 0.029546203091740608, acc: 85.9375, f1: 69.20604919083621, r: 0.9468530223612174
06/02/2019 01:24:16 step: 3013, epoch: 91, batch: 9, loss: 0.03497500717639923, acc: 93.75, f1: 93.37419904440905, r: 0.9125418078417804
06/02/2019 01:24:16 step: 3018, epoch: 91, batch: 14, loss: 0.030413659289479256, acc: 89.0625, f1: 76.72892845712957, r: 0.9291979115183537
06/02/2019 01:24:17 step: 3023, epoch: 91, batch: 19, loss: 0.03628787770867348, acc: 89.0625, f1: 77.37179487179488, r: 0.9078112973676794
06/02/2019 01:24:17 step: 3028, epoch: 91, batch: 24, loss: 0.03362712636590004, acc: 82.8125, f1: 81.13095238095238, r: 0.9368380342686502
06/02/2019 01:24:18 step: 3033, epoch: 91, batch: 29, loss: 0.03598630428314209, acc: 85.9375, f1: 75.8344017094017, r: 0.9149152993687935
06/02/2019 01:24:18 *** evaluating ***
06/02/2019 01:24:18 step: 92, epoch: 91, acc: 61.53846153846154, f1: 27.885907951697426, r: 0.40941380686079293
06/02/2019 01:24:18 *** epoch: 93 ***
06/02/2019 01:24:18 *** training ***
06/02/2019 01:24:19 step: 3041, epoch: 92, batch: 4, loss: 0.03333570808172226, acc: 95.3125, f1: 93.09780515979976, r: 0.9211496739167812
06/02/2019 01:24:19 step: 3046, epoch: 92, batch: 9, loss: 0.03316027671098709, acc: 89.0625, f1: 82.9684265010352, r: 0.9400198258815315
06/02/2019 01:24:20 step: 3051, epoch: 92, batch: 14, loss: 0.030963724479079247, acc: 76.5625, f1: 59.47582972582972, r: 0.9621176377231005
06/02/2019 01:24:20 step: 3056, epoch: 92, batch: 19, loss: 0.029985349625349045, acc: 85.9375, f1: 77.78308293014176, r: 0.9564888366387739
06/02/2019 01:24:21 step: 3061, epoch: 92, batch: 24, loss: 0.03022514097392559, acc: 90.625, f1: 88.1544366380432, r: 0.9140106724875919
06/02/2019 01:24:21 step: 3066, epoch: 92, batch: 29, loss: 0.030166542157530785, acc: 92.1875, f1: 81.28275233538392, r: 0.9110954440093806
06/02/2019 01:24:21 *** evaluating ***
06/02/2019 01:24:22 step: 93, epoch: 92, acc: 59.401709401709404, f1: 26.713189773760458, r: 0.4093264186581787
06/02/2019 01:24:22 *** epoch: 94 ***
06/02/2019 01:24:22 *** training ***
06/02/2019 01:24:22 step: 3074, epoch: 93, batch: 4, loss: 0.03321779891848564, acc: 93.75, f1: 81.90810415075121, r: 0.9267951727314148
06/02/2019 01:24:23 step: 3079, epoch: 93, batch: 9, loss: 0.03051748499274254, acc: 84.375, f1: 78.84244720595508, r: 0.9479876874080435
06/02/2019 01:24:23 step: 3084, epoch: 93, batch: 14, loss: 0.030496593564748764, acc: 93.75, f1: 77.77777777777779, r: 0.9198705425954112
06/02/2019 01:24:23 step: 3089, epoch: 93, batch: 19, loss: 0.026903783902525902, acc: 87.5, f1: 80.85491947903598, r: 0.9543486554120557
06/02/2019 01:24:24 step: 3094, epoch: 93, batch: 24, loss: 0.03454991430044174, acc: 81.25, f1: 72.83517573696146, r: 0.9309557432976903
06/02/2019 01:24:24 step: 3099, epoch: 93, batch: 29, loss: 0.028977634385228157, acc: 89.0625, f1: 84.09498207885304, r: 0.9565687230593634
06/02/2019 01:24:25 *** evaluating ***
06/02/2019 01:24:25 step: 94, epoch: 93, acc: 61.965811965811966, f1: 29.453116523289157, r: 0.4074770692248998
06/02/2019 01:24:25 *** epoch: 95 ***
06/02/2019 01:24:25 *** training ***
06/02/2019 01:24:25 step: 3107, epoch: 94, batch: 4, loss: 0.034622181206941605, acc: 85.9375, f1: 67.24168375484166, r: 0.932625683382917
06/02/2019 01:24:26 step: 3112, epoch: 94, batch: 9, loss: 0.03472432494163513, acc: 87.5, f1: 83.1513461072035, r: 0.9117427786949045
06/02/2019 01:24:26 step: 3117, epoch: 94, batch: 14, loss: 0.026143847033381462, acc: 85.9375, f1: 79.21615761689291, r: 0.977512863738372
06/02/2019 01:24:27 step: 3122, epoch: 94, batch: 19, loss: 0.03268285095691681, acc: 89.0625, f1: 85.0367819362009, r: 0.9388597833918684
06/02/2019 01:24:27 step: 3127, epoch: 94, batch: 24, loss: 0.033834315836429596, acc: 84.375, f1: 80.40378100160707, r: 0.9454599647938238
06/02/2019 01:24:28 step: 3132, epoch: 94, batch: 29, loss: 0.03076370619237423, acc: 89.0625, f1: 88.66267977210227, r: 0.9564659646391407
06/02/2019 01:24:28 *** evaluating ***
06/02/2019 01:24:28 step: 95, epoch: 94, acc: 58.97435897435898, f1: 24.767133588901004, r: 0.4009392042684165
06/02/2019 01:24:28 *** epoch: 96 ***
06/02/2019 01:24:28 *** training ***
06/02/2019 01:24:29 step: 3140, epoch: 95, batch: 4, loss: 0.03279465436935425, acc: 89.0625, f1: 80.63114134542707, r: 0.9292446576188065
06/02/2019 01:24:29 step: 3145, epoch: 95, batch: 9, loss: 0.03165831416845322, acc: 84.375, f1: 75.99161255411255, r: 0.928511275031988
06/02/2019 01:24:30 step: 3150, epoch: 95, batch: 14, loss: 0.028932347893714905, acc: 92.1875, f1: 68.95986398511972, r: 0.9687700448947717
06/02/2019 01:24:30 step: 3155, epoch: 95, batch: 19, loss: 0.030650010332465172, acc: 79.6875, f1: 65.42393592205622, r: 0.9398264732186407
06/02/2019 01:24:31 step: 3160, epoch: 95, batch: 24, loss: 0.029034465551376343, acc: 84.375, f1: 79.99128430049483, r: 0.9566053897736206
06/02/2019 01:24:31 step: 3165, epoch: 95, batch: 29, loss: 0.03172394633293152, acc: 87.5, f1: 82.71457199032064, r: 0.9438015769408126
06/02/2019 01:24:31 *** evaluating ***
06/02/2019 01:24:32 step: 96, epoch: 95, acc: 59.82905982905983, f1: 25.97040000355796, r: 0.42058070228990085
06/02/2019 01:24:32 *** epoch: 97 ***
06/02/2019 01:24:32 *** training ***
06/02/2019 01:24:32 step: 3173, epoch: 96, batch: 4, loss: 0.031997159123420715, acc: 82.8125, f1: 58.18165672065927, r: 0.9348082501462587
06/02/2019 01:24:33 step: 3178, epoch: 96, batch: 9, loss: 0.02949371375143528, acc: 84.375, f1: 71.00072408211943, r: 0.9637691784268145
06/02/2019 01:24:33 step: 3183, epoch: 96, batch: 14, loss: 0.0316721573472023, acc: 84.375, f1: 67.02870978900035, r: 0.9319763369163078
06/02/2019 01:24:34 step: 3188, epoch: 96, batch: 19, loss: 0.028745707124471664, acc: 93.75, f1: 80.32310781743632, r: 0.9630424994362907
06/02/2019 01:24:34 step: 3193, epoch: 96, batch: 24, loss: 0.030442047864198685, acc: 95.3125, f1: 94.72534946993152, r: 0.9567124385888637
06/02/2019 01:24:35 step: 3198, epoch: 96, batch: 29, loss: 0.02766738273203373, acc: 87.5, f1: 82.18749999999999, r: 0.9671749079433815
06/02/2019 01:24:35 *** evaluating ***
06/02/2019 01:24:35 step: 97, epoch: 96, acc: 58.54700854700855, f1: 24.453015667975624, r: 0.40883949962975263
06/02/2019 01:24:35 *** epoch: 98 ***
06/02/2019 01:24:35 *** training ***
06/02/2019 01:24:35 step: 3206, epoch: 97, batch: 4, loss: 0.03303585946559906, acc: 82.8125, f1: 74.93295869072266, r: 0.869111878965867
06/02/2019 01:24:36 step: 3211, epoch: 97, batch: 9, loss: 0.03110949695110321, acc: 87.5, f1: 83.41489842690322, r: 0.9124284006439243
06/02/2019 01:24:36 step: 3216, epoch: 97, batch: 14, loss: 0.02744002267718315, acc: 89.0625, f1: 75.89093273875882, r: 0.9550311680721997
06/02/2019 01:24:37 step: 3221, epoch: 97, batch: 19, loss: 0.029413461685180664, acc: 78.125, f1: 68.30438378825477, r: 0.9615692595062699
06/02/2019 01:24:37 step: 3226, epoch: 97, batch: 24, loss: 0.02973790280520916, acc: 89.0625, f1: 71.65178571428572, r: 0.9530619914881023
06/02/2019 01:24:38 step: 3231, epoch: 97, batch: 29, loss: 0.030640246346592903, acc: 85.9375, f1: 78.67424242424242, r: 0.9512655440797715
06/02/2019 01:24:38 *** evaluating ***
06/02/2019 01:24:38 step: 98, epoch: 97, acc: 61.111111111111114, f1: 27.29006763164195, r: 0.398404050559555
06/02/2019 01:24:38 *** epoch: 99 ***
06/02/2019 01:24:38 *** training ***
06/02/2019 01:24:39 step: 3239, epoch: 98, batch: 4, loss: 0.029726430773735046, acc: 93.75, f1: 88.36304048698045, r: 0.9505926452381765
06/02/2019 01:24:39 step: 3244, epoch: 98, batch: 9, loss: 0.027223026379942894, acc: 89.0625, f1: 75.87301587301587, r: 0.9442205653914763
06/02/2019 01:24:40 step: 3249, epoch: 98, batch: 14, loss: 0.027362724766135216, acc: 85.9375, f1: 56.65860019897288, r: 0.9203683219201706
06/02/2019 01:24:40 step: 3254, epoch: 98, batch: 19, loss: 0.02937936596572399, acc: 92.1875, f1: 88.93476893476893, r: 0.9336468119527018
06/02/2019 01:24:40 step: 3259, epoch: 98, batch: 24, loss: 0.03233505040407181, acc: 84.375, f1: 67.64135608963196, r: 0.9229287245095749
06/02/2019 01:24:41 step: 3264, epoch: 98, batch: 29, loss: 0.03146103397011757, acc: 84.375, f1: 79.92973433522214, r: 0.9625505571199303
06/02/2019 01:24:41 *** evaluating ***
06/02/2019 01:24:41 step: 99, epoch: 98, acc: 60.68376068376068, f1: 28.364745747696674, r: 0.4041252924646948
06/02/2019 01:24:41 *** epoch: 100 ***
06/02/2019 01:24:41 *** training ***
06/02/2019 01:24:42 step: 3272, epoch: 99, batch: 4, loss: 0.03396701440215111, acc: 93.75, f1: 85.59027777777779, r: 0.8293202734560358
06/02/2019 01:24:42 step: 3277, epoch: 99, batch: 9, loss: 0.03421688824892044, acc: 78.125, f1: 67.86502446828533, r: 0.9376224215921849
06/02/2019 01:24:43 step: 3282, epoch: 99, batch: 14, loss: 0.03241487964987755, acc: 82.8125, f1: 72.24047420386594, r: 0.9516583013996788
06/02/2019 01:24:43 step: 3287, epoch: 99, batch: 19, loss: 0.029518654569983482, acc: 90.625, f1: 85.50250626566415, r: 0.9539768369884571
06/02/2019 01:24:44 step: 3292, epoch: 99, batch: 24, loss: 0.0353793129324913, acc: 82.8125, f1: 62.95289855072463, r: 0.9082185329994804
06/02/2019 01:24:44 step: 3297, epoch: 99, batch: 29, loss: 0.028592143207788467, acc: 90.625, f1: 89.77248666336688, r: 0.9614655081900522
06/02/2019 01:24:45 *** evaluating ***
06/02/2019 01:24:45 step: 100, epoch: 99, acc: 59.401709401709404, f1: 26.220475779149453, r: 0.4047095740470632
06/02/2019 01:24:45 *** epoch: 101 ***
06/02/2019 01:24:45 *** training ***
06/02/2019 01:24:45 step: 3305, epoch: 100, batch: 4, loss: 0.026252033188939095, acc: 89.0625, f1: 87.065065260554, r: 0.9528712375602308
06/02/2019 01:24:46 step: 3310, epoch: 100, batch: 9, loss: 0.027710767462849617, acc: 92.1875, f1: 92.08238851095993, r: 0.9589291606526804
06/02/2019 01:24:46 step: 3315, epoch: 100, batch: 14, loss: 0.02883824333548546, acc: 78.125, f1: 71.21964263134004, r: 0.9456305762758546
06/02/2019 01:24:47 step: 3320, epoch: 100, batch: 19, loss: 0.034049201756715775, acc: 85.9375, f1: 79.85714285714286, r: 0.9205893118943821
06/02/2019 01:24:47 step: 3325, epoch: 100, batch: 24, loss: 0.032226189970970154, acc: 75.0, f1: 78.51839283904502, r: 0.949143956013092
06/02/2019 01:24:48 step: 3330, epoch: 100, batch: 29, loss: 0.02813143841922283, acc: 82.8125, f1: 70.69597069597069, r: 0.9614557658000304
06/02/2019 01:24:48 *** evaluating ***
06/02/2019 01:24:48 step: 101, epoch: 100, acc: 58.97435897435898, f1: 25.905767655645217, r: 0.4030260374624835
06/02/2019 01:24:48 *** epoch: 102 ***
06/02/2019 01:24:48 *** training ***
06/02/2019 01:24:48 step: 3338, epoch: 101, batch: 4, loss: 0.03158038109540939, acc: 82.8125, f1: 79.85121485121485, r: 0.9440070843256779
06/02/2019 01:24:49 step: 3343, epoch: 101, batch: 9, loss: 0.03153286874294281, acc: 84.375, f1: 62.317558602388324, r: 0.8802385836692619
06/02/2019 01:24:49 step: 3348, epoch: 101, batch: 14, loss: 0.02819366753101349, acc: 96.875, f1: 98.36309523809524, r: 0.9694965465085232
06/02/2019 01:24:50 step: 3353, epoch: 101, batch: 19, loss: 0.030312340706586838, acc: 85.9375, f1: 72.0327589077589, r: 0.947352369622976
06/02/2019 01:24:50 step: 3358, epoch: 101, batch: 24, loss: 0.031836967915296555, acc: 87.5, f1: 84.45279393808805, r: 0.9521616414904075
06/02/2019 01:24:51 step: 3363, epoch: 101, batch: 29, loss: 0.03241796791553497, acc: 82.8125, f1: 65.49479166666666, r: 0.9123652466673772
06/02/2019 01:24:51 *** evaluating ***
06/02/2019 01:24:51 step: 102, epoch: 101, acc: 59.401709401709404, f1: 25.85083372315806, r: 0.4068783109117416
06/02/2019 01:24:51 *** epoch: 103 ***
06/02/2019 01:24:51 *** training ***
06/02/2019 01:24:52 step: 3371, epoch: 102, batch: 4, loss: 0.028324458748102188, acc: 84.375, f1: 79.41501238176429, r: 0.9234391138874145
06/02/2019 01:24:52 step: 3376, epoch: 102, batch: 9, loss: 0.035542428493499756, acc: 95.3125, f1: 88.47330127019362, r: 0.9102582425436914
06/02/2019 01:24:53 step: 3381, epoch: 102, batch: 14, loss: 0.03145204484462738, acc: 92.1875, f1: 91.71319102353586, r: 0.9432661568315099
06/02/2019 01:24:53 step: 3386, epoch: 102, batch: 19, loss: 0.035047635436058044, acc: 85.9375, f1: 88.97546897546896, r: 0.9268999322618148
06/02/2019 01:24:54 step: 3391, epoch: 102, batch: 24, loss: 0.029976077377796173, acc: 84.375, f1: 82.36381673881674, r: 0.9619777611695185
06/02/2019 01:24:54 step: 3396, epoch: 102, batch: 29, loss: 0.03167881444096565, acc: 87.5, f1: 86.01133846773381, r: 0.9547387151499135
06/02/2019 01:24:54 *** evaluating ***
06/02/2019 01:24:55 step: 103, epoch: 102, acc: 58.119658119658126, f1: 24.84176798230599, r: 0.4064421667293507
06/02/2019 01:24:55 *** epoch: 104 ***
06/02/2019 01:24:55 *** training ***
06/02/2019 01:24:55 step: 3404, epoch: 103, batch: 4, loss: 0.03373710438609123, acc: 87.5, f1: 87.59048821548822, r: 0.9099354947118694
06/02/2019 01:24:56 step: 3409, epoch: 103, batch: 9, loss: 0.027336280792951584, acc: 90.625, f1: 75.81356199954583, r: 0.920971528213839
06/02/2019 01:24:56 step: 3414, epoch: 103, batch: 14, loss: 0.028980784118175507, acc: 95.3125, f1: 90.58503401360545, r: 0.9547030026110598
06/02/2019 01:24:57 step: 3419, epoch: 103, batch: 19, loss: 0.03157581761479378, acc: 87.5, f1: 83.22983176244045, r: 0.9332121700432958
06/02/2019 01:24:57 step: 3424, epoch: 103, batch: 24, loss: 0.03448030352592468, acc: 85.9375, f1: 70.97670647206252, r: 0.912573880248784
06/02/2019 01:24:58 step: 3429, epoch: 103, batch: 29, loss: 0.025480220094323158, acc: 90.625, f1: 80.77655677655677, r: 0.9683434620863807
06/02/2019 01:24:58 *** evaluating ***
06/02/2019 01:24:58 step: 104, epoch: 103, acc: 58.97435897435898, f1: 24.940377837722192, r: 0.40386162348613286
06/02/2019 01:24:58 *** epoch: 105 ***
06/02/2019 01:24:58 *** training ***
06/02/2019 01:24:59 step: 3437, epoch: 104, batch: 4, loss: 0.03542633727192879, acc: 87.5, f1: 72.07695578231292, r: 0.8880984197512624
06/02/2019 01:24:59 step: 3442, epoch: 104, batch: 9, loss: 0.02601931057870388, acc: 89.0625, f1: 86.96924603174604, r: 0.9719537125618447
06/02/2019 01:25:00 step: 3447, epoch: 104, batch: 14, loss: 0.03214738145470619, acc: 89.0625, f1: 76.16170381195955, r: 0.8979181097120265
06/02/2019 01:25:00 step: 3452, epoch: 104, batch: 19, loss: 0.0289141908288002, acc: 85.9375, f1: 79.41246373302123, r: 0.9460244373277107
06/02/2019 01:25:00 step: 3457, epoch: 104, batch: 24, loss: 0.030531732365489006, acc: 82.8125, f1: 77.15736575044167, r: 0.9498167035181314
06/02/2019 01:25:01 step: 3462, epoch: 104, batch: 29, loss: 0.03575492277741432, acc: 87.5, f1: 83.58659735654416, r: 0.9343535142313188
06/02/2019 01:25:01 *** evaluating ***
06/02/2019 01:25:01 step: 105, epoch: 104, acc: 59.401709401709404, f1: 24.87896059324631, r: 0.4127788648676015
06/02/2019 01:25:01 *** epoch: 106 ***
06/02/2019 01:25:01 *** training ***
06/02/2019 01:25:02 step: 3470, epoch: 105, batch: 4, loss: 0.03046317771077156, acc: 85.9375, f1: 64.81962481962482, r: 0.943208980977567
06/02/2019 01:25:02 step: 3475, epoch: 105, batch: 9, loss: 0.026914171874523163, acc: 92.1875, f1: 75.03425918619638, r: 0.9703824833848593
06/02/2019 01:25:03 step: 3480, epoch: 105, batch: 14, loss: 0.02632940374314785, acc: 87.5, f1: 79.1522406812788, r: 0.962432664737754
06/02/2019 01:25:03 step: 3485, epoch: 105, batch: 19, loss: 0.027499739080667496, acc: 87.5, f1: 82.72828362114078, r: 0.9088137748680395
06/02/2019 01:25:04 step: 3490, epoch: 105, batch: 24, loss: 0.027311459183692932, acc: 95.3125, f1: 96.91665459287164, r: 0.9257186502423655
06/02/2019 01:25:04 step: 3495, epoch: 105, batch: 29, loss: 0.028970472514629364, acc: 87.5, f1: 79.68468819888699, r: 0.9628262327446635
06/02/2019 01:25:04 *** evaluating ***
06/02/2019 01:25:05 step: 106, epoch: 105, acc: 58.97435897435898, f1: 24.569321313324245, r: 0.40959321756536743
06/02/2019 01:25:05 *** epoch: 107 ***
06/02/2019 01:25:05 *** training ***
06/02/2019 01:25:05 step: 3503, epoch: 106, batch: 4, loss: 0.027553467079997063, acc: 87.5, f1: 80.15919942749211, r: 0.9584893160346903
06/02/2019 01:25:06 step: 3508, epoch: 106, batch: 9, loss: 0.031126003712415695, acc: 93.75, f1: 84.178089198036, r: 0.9300087626608703
06/02/2019 01:25:06 step: 3513, epoch: 106, batch: 14, loss: 0.027704348787665367, acc: 87.5, f1: 71.36431387041142, r: 0.968209462753956
06/02/2019 01:25:07 step: 3518, epoch: 106, batch: 19, loss: 0.030132662504911423, acc: 85.9375, f1: 74.46147837125281, r: 0.9499197336429066
06/02/2019 01:25:07 step: 3523, epoch: 106, batch: 24, loss: 0.028781836852431297, acc: 85.9375, f1: 87.97319441387239, r: 0.9456392463663978
06/02/2019 01:25:08 step: 3528, epoch: 106, batch: 29, loss: 0.030467882752418518, acc: 87.5, f1: 87.22575669944091, r: 0.9338072063099999
06/02/2019 01:25:08 *** evaluating ***
06/02/2019 01:25:08 step: 107, epoch: 106, acc: 58.97435897435898, f1: 25.556019756387403, r: 0.402025416751799
06/02/2019 01:25:08 *** epoch: 108 ***
06/02/2019 01:25:08 *** training ***
06/02/2019 01:25:08 step: 3536, epoch: 107, batch: 4, loss: 0.02689000964164734, acc: 85.9375, f1: 71.87282135076252, r: 0.9332999421683381
06/02/2019 01:25:09 step: 3541, epoch: 107, batch: 9, loss: 0.03076137974858284, acc: 87.5, f1: 73.43478586403114, r: 0.9080134107718303
06/02/2019 01:25:09 step: 3546, epoch: 107, batch: 14, loss: 0.028804482892155647, acc: 81.25, f1: 75.58008658008657, r: 0.9465582207497659
06/02/2019 01:25:10 step: 3551, epoch: 107, batch: 19, loss: 0.02580748125910759, acc: 96.875, f1: 93.335900397895, r: 0.9491106226182098
06/02/2019 01:25:10 step: 3556, epoch: 107, batch: 24, loss: 0.027528665959835052, acc: 90.625, f1: 88.14244567527307, r: 0.9531632440750955
06/02/2019 01:25:11 step: 3561, epoch: 107, batch: 29, loss: 0.02733096480369568, acc: 81.25, f1: 70.61331189563948, r: 0.9589307279673313
06/02/2019 01:25:11 *** evaluating ***
06/02/2019 01:25:11 step: 108, epoch: 107, acc: 61.53846153846154, f1: 28.683455307565474, r: 0.4045993678870341
06/02/2019 01:25:11 *** epoch: 109 ***
06/02/2019 01:25:11 *** training ***
06/02/2019 01:25:12 step: 3569, epoch: 108, batch: 4, loss: 0.027063598856329918, acc: 87.5, f1: 78.8945123503947, r: 0.937577567803535
06/02/2019 01:25:12 step: 3574, epoch: 108, batch: 9, loss: 0.03090045601129532, acc: 81.25, f1: 66.4678810474967, r: 0.9384084259452314
06/02/2019 01:25:13 step: 3579, epoch: 108, batch: 14, loss: 0.029604468494653702, acc: 90.625, f1: 88.99790746338061, r: 0.9692835648098508
06/02/2019 01:25:13 step: 3584, epoch: 108, batch: 19, loss: 0.027659909799695015, acc: 92.1875, f1: 83.04136409399567, r: 0.9502760149275937
06/02/2019 01:25:14 step: 3589, epoch: 108, batch: 24, loss: 0.03637387603521347, acc: 89.0625, f1: 76.7375700280112, r: 0.9228332370062237
06/02/2019 01:25:14 step: 3594, epoch: 108, batch: 29, loss: 0.029112424701452255, acc: 93.75, f1: 79.5527842586666, r: 0.9510511959462542
06/02/2019 01:25:14 *** evaluating ***
06/02/2019 01:25:14 step: 109, epoch: 108, acc: 58.97435897435898, f1: 24.298023336816442, r: 0.40266364132909815
06/02/2019 01:25:14 *** epoch: 110 ***
06/02/2019 01:25:14 *** training ***
06/02/2019 01:25:15 step: 3602, epoch: 109, batch: 4, loss: 0.03248133882880211, acc: 89.0625, f1: 86.03488574917145, r: 0.918675083220634
06/02/2019 01:25:15 step: 3607, epoch: 109, batch: 9, loss: 0.025478431954979897, acc: 90.625, f1: 76.01682778374507, r: 0.9642429790408976
06/02/2019 01:25:16 step: 3612, epoch: 109, batch: 14, loss: 0.02893340028822422, acc: 90.625, f1: 75.79103305490112, r: 0.9141320165346709
06/02/2019 01:25:16 step: 3617, epoch: 109, batch: 19, loss: 0.03390601649880409, acc: 96.875, f1: 97.73443223443225, r: 0.9385280265315343
06/02/2019 01:25:17 step: 3622, epoch: 109, batch: 24, loss: 0.030616920441389084, acc: 92.1875, f1: 85.54382332643202, r: 0.9553920400621656
06/02/2019 01:25:17 step: 3627, epoch: 109, batch: 29, loss: 0.030457118526101112, acc: 85.9375, f1: 73.92705860447796, r: 0.9148479608829851
06/02/2019 01:25:17 *** evaluating ***
06/02/2019 01:25:18 step: 110, epoch: 109, acc: 58.54700854700855, f1: 24.380346243938845, r: 0.4063109633712201
06/02/2019 01:25:18 *** epoch: 111 ***
06/02/2019 01:25:18 *** training ***
06/02/2019 01:25:18 step: 3635, epoch: 110, batch: 4, loss: 0.027261557057499886, acc: 96.875, f1: 95.92630095516468, r: 0.9631336915351308
06/02/2019 01:25:19 step: 3640, epoch: 110, batch: 9, loss: 0.026189306750893593, acc: 90.625, f1: 86.71505545538054, r: 0.9572086059311373
06/02/2019 01:25:19 step: 3645, epoch: 110, batch: 14, loss: 0.026384495198726654, acc: 92.1875, f1: 76.63522012578615, r: 0.9456667378941441
06/02/2019 01:25:20 step: 3650, epoch: 110, batch: 19, loss: 0.02833344042301178, acc: 82.8125, f1: 75.16682913860333, r: 0.9616429879854497
06/02/2019 01:25:20 step: 3655, epoch: 110, batch: 24, loss: 0.029140833765268326, acc: 89.0625, f1: 87.21805859229794, r: 0.9676999598579281
06/02/2019 01:25:21 step: 3660, epoch: 110, batch: 29, loss: 0.025101561099290848, acc: 87.5, f1: 71.4073645652593, r: 0.971693871175482
06/02/2019 01:25:21 *** evaluating ***
06/02/2019 01:25:21 step: 111, epoch: 110, acc: 58.54700854700855, f1: 26.106432000717906, r: 0.4019732363759514
06/02/2019 01:25:21 *** epoch: 112 ***
06/02/2019 01:25:21 *** training ***
06/02/2019 01:25:21 step: 3668, epoch: 111, batch: 4, loss: 0.024920916184782982, acc: 90.625, f1: 87.67543859649123, r: 0.9608140801348845
06/02/2019 01:25:22 step: 3673, epoch: 111, batch: 9, loss: 0.03199867159128189, acc: 92.1875, f1: 86.91813686180166, r: 0.9170227524254266
06/02/2019 01:25:22 step: 3678, epoch: 111, batch: 14, loss: 0.02885323204100132, acc: 82.8125, f1: 72.23600771987869, r: 0.9632329360048109
06/02/2019 01:25:23 step: 3683, epoch: 111, batch: 19, loss: 0.02307187393307686, acc: 89.0625, f1: 74.94047619047619, r: 0.9786555219506978
06/02/2019 01:25:23 step: 3688, epoch: 111, batch: 24, loss: 0.029464494436979294, acc: 82.8125, f1: 67.33003838266995, r: 0.9220648828956687
06/02/2019 01:25:24 step: 3693, epoch: 111, batch: 29, loss: 0.02873964048922062, acc: 82.8125, f1: 78.17640692640693, r: 0.9660443256062194
06/02/2019 01:25:24 *** evaluating ***
06/02/2019 01:25:24 step: 112, epoch: 111, acc: 61.53846153846154, f1: 28.1442706957861, r: 0.40867694822650436
06/02/2019 01:25:24 *** epoch: 113 ***
06/02/2019 01:25:24 *** training ***
06/02/2019 01:25:25 step: 3701, epoch: 112, batch: 4, loss: 0.029885759577155113, acc: 90.625, f1: 75.20050125313283, r: 0.9529442377768701
06/02/2019 01:25:25 step: 3706, epoch: 112, batch: 9, loss: 0.029325557872653008, acc: 84.375, f1: 72.9038789428815, r: 0.9538313760339229
06/02/2019 01:25:26 step: 3711, epoch: 112, batch: 14, loss: 0.02760639414191246, acc: 84.375, f1: 79.35028739376565, r: 0.9441869929916578
06/02/2019 01:25:26 step: 3716, epoch: 112, batch: 19, loss: 0.030037812888622284, acc: 87.5, f1: 75.32043571237637, r: 0.9624685039988424
06/02/2019 01:25:26 step: 3721, epoch: 112, batch: 24, loss: 0.02710082195699215, acc: 92.1875, f1: 86.79035250463821, r: 0.9650857052058661
06/02/2019 01:25:27 step: 3726, epoch: 112, batch: 29, loss: 0.028140675276517868, acc: 87.5, f1: 81.81909373919059, r: 0.9418538985155513
06/02/2019 01:25:27 *** evaluating ***
06/02/2019 01:25:27 step: 113, epoch: 112, acc: 60.256410256410255, f1: 27.718314196820348, r: 0.4039209889174472
06/02/2019 01:25:27 *** epoch: 114 ***
06/02/2019 01:25:27 *** training ***
06/02/2019 01:25:28 step: 3734, epoch: 113, batch: 4, loss: 0.027832023799419403, acc: 87.5, f1: 71.51862026862025, r: 0.9606006098596402
06/02/2019 01:25:28 step: 3739, epoch: 113, batch: 9, loss: 0.026853276416659355, acc: 82.8125, f1: 64.26940622306756, r: 0.9570035922925493
06/02/2019 01:25:29 step: 3744, epoch: 113, batch: 14, loss: 0.02799973264336586, acc: 90.625, f1: 87.98181114551085, r: 0.9678437659260788
06/02/2019 01:25:29 step: 3749, epoch: 113, batch: 19, loss: 0.030125681310892105, acc: 90.625, f1: 86.70068027210883, r: 0.9627642218143038
06/02/2019 01:25:30 step: 3754, epoch: 113, batch: 24, loss: 0.03008030727505684, acc: 75.0, f1: 72.77145408938888, r: 0.9597795645286602
06/02/2019 01:25:30 step: 3759, epoch: 113, batch: 29, loss: 0.029362691566348076, acc: 89.0625, f1: 70.68506493506493, r: 0.9504721531056954
06/02/2019 01:25:30 *** evaluating ***
06/02/2019 01:25:30 step: 114, epoch: 113, acc: 60.68376068376068, f1: 27.218137254901965, r: 0.40896265934844106
06/02/2019 01:25:30 *** epoch: 115 ***
06/02/2019 01:25:30 *** training ***
06/02/2019 01:25:31 step: 3767, epoch: 114, batch: 4, loss: 0.02672417089343071, acc: 85.9375, f1: 63.74050477824063, r: 0.9643752897089277
06/02/2019 01:25:31 step: 3772, epoch: 114, batch: 9, loss: 0.02767828106880188, acc: 90.625, f1: 89.35907842157842, r: 0.9734967262192844
06/02/2019 01:25:32 step: 3777, epoch: 114, batch: 14, loss: 0.027791578322649002, acc: 85.9375, f1: 66.83662773285415, r: 0.9507385193207154
06/02/2019 01:25:32 step: 3782, epoch: 114, batch: 19, loss: 0.02585851028561592, acc: 92.1875, f1: 84.64316635745207, r: 0.9518480449354079
06/02/2019 01:25:33 step: 3787, epoch: 114, batch: 24, loss: 0.028403665870428085, acc: 90.625, f1: 84.70521541950113, r: 0.9431329814549463
06/02/2019 01:25:33 step: 3792, epoch: 114, batch: 29, loss: 0.02880699560046196, acc: 93.75, f1: 92.51526251526252, r: 0.9076452992017933
06/02/2019 01:25:34 *** evaluating ***
06/02/2019 01:25:34 step: 115, epoch: 114, acc: 58.119658119658126, f1: 25.144087938205583, r: 0.4134375188934142
06/02/2019 01:25:34 *** epoch: 116 ***
06/02/2019 01:25:34 *** training ***
06/02/2019 01:25:34 step: 3800, epoch: 115, batch: 4, loss: 0.02539859153330326, acc: 90.625, f1: 72.33549783549785, r: 0.9519513563244526
06/02/2019 01:25:35 step: 3805, epoch: 115, batch: 9, loss: 0.03038112260401249, acc: 82.8125, f1: 74.36660561660563, r: 0.9455457564758862
06/02/2019 01:25:35 step: 3810, epoch: 115, batch: 14, loss: 0.026760032400488853, acc: 89.0625, f1: 75.62641062641063, r: 0.9496219774473407
06/02/2019 01:25:36 step: 3815, epoch: 115, batch: 19, loss: 0.03039816953241825, acc: 85.9375, f1: 71.56278245563959, r: 0.9522952975113179
06/02/2019 01:25:36 step: 3820, epoch: 115, batch: 24, loss: 0.032469093799591064, acc: 85.9375, f1: 83.34502507296625, r: 0.9483806581617072
06/02/2019 01:25:37 step: 3825, epoch: 115, batch: 29, loss: 0.027037709951400757, acc: 81.25, f1: 55.17676767676767, r: 0.959849367161495
06/02/2019 01:25:37 *** evaluating ***
06/02/2019 01:25:37 step: 116, epoch: 115, acc: 60.256410256410255, f1: 27.373857036706568, r: 0.4088411185129916
06/02/2019 01:25:37 *** epoch: 117 ***
06/02/2019 01:25:37 *** training ***
06/02/2019 01:25:38 step: 3833, epoch: 116, batch: 4, loss: 0.02716975286602974, acc: 89.0625, f1: 70.0944945317099, r: 0.9507281508389364
06/02/2019 01:25:38 step: 3838, epoch: 116, batch: 9, loss: 0.025715133175253868, acc: 93.75, f1: 92.02465834818776, r: 0.9525904646782395
06/02/2019 01:25:39 step: 3843, epoch: 116, batch: 14, loss: 0.030425582081079483, acc: 85.9375, f1: 72.8249666137865, r: 0.946123416387174
06/02/2019 01:25:39 step: 3848, epoch: 116, batch: 19, loss: 0.03198964521288872, acc: 85.9375, f1: 84.21150278293136, r: 0.9030120336214698
06/02/2019 01:25:39 step: 3853, epoch: 116, batch: 24, loss: 0.03373587504029274, acc: 87.5, f1: 81.81300106249397, r: 0.9165652360919707
06/02/2019 01:25:40 step: 3858, epoch: 116, batch: 29, loss: 0.030825670808553696, acc: 87.5, f1: 64.38897016296397, r: 0.952567815434868
06/02/2019 01:25:40 *** evaluating ***
06/02/2019 01:25:40 step: 117, epoch: 116, acc: 59.82905982905983, f1: 25.609168477718637, r: 0.41007318163558926
06/02/2019 01:25:40 *** epoch: 118 ***
06/02/2019 01:25:40 *** training ***
06/02/2019 01:25:41 step: 3866, epoch: 117, batch: 4, loss: 0.027939265593886375, acc: 90.625, f1: 78.0689181146847, r: 0.9627858202897417
06/02/2019 01:25:41 step: 3871, epoch: 117, batch: 9, loss: 0.02865065261721611, acc: 81.25, f1: 70.66035975518734, r: 0.9485195959139587
06/02/2019 01:25:42 step: 3876, epoch: 117, batch: 14, loss: 0.03071204200387001, acc: 82.8125, f1: 69.76689976689975, r: 0.9486791729451285
06/02/2019 01:25:42 step: 3881, epoch: 117, batch: 19, loss: 0.0298948772251606, acc: 87.5, f1: 89.41175558312655, r: 0.9173704065484098
06/02/2019 01:25:43 step: 3886, epoch: 117, batch: 24, loss: 0.027521144598722458, acc: 89.0625, f1: 87.22314722314722, r: 0.9423436603176836
06/02/2019 01:25:43 step: 3891, epoch: 117, batch: 29, loss: 0.031946975737810135, acc: 82.8125, f1: 72.96281918564527, r: 0.9388054816738058
06/02/2019 01:25:43 *** evaluating ***
06/02/2019 01:25:44 step: 118, epoch: 117, acc: 59.401709401709404, f1: 24.7276370871536, r: 0.414547503826657
06/02/2019 01:25:44 *** epoch: 119 ***
06/02/2019 01:25:44 *** training ***
06/02/2019 01:25:44 step: 3899, epoch: 118, batch: 4, loss: 0.02796526812016964, acc: 90.625, f1: 87.3471557682084, r: 0.9527217527476862
06/02/2019 01:25:45 step: 3904, epoch: 118, batch: 9, loss: 0.0276805367320776, acc: 87.5, f1: 74.48400345930138, r: 0.9614063334930718
06/02/2019 01:25:45 step: 3909, epoch: 118, batch: 14, loss: 0.027111615985631943, acc: 81.25, f1: 63.890091390091385, r: 0.954878034142388
06/02/2019 01:25:46 step: 3914, epoch: 118, batch: 19, loss: 0.029514137655496597, acc: 90.625, f1: 80.57981492192019, r: 0.9387635187876162
06/02/2019 01:25:46 step: 3919, epoch: 118, batch: 24, loss: 0.028569985181093216, acc: 89.0625, f1: 89.14981750275868, r: 0.9613997293181709
06/02/2019 01:25:47 step: 3924, epoch: 118, batch: 29, loss: 0.026830501854419708, acc: 87.5, f1: 78.65079365079366, r: 0.9091919414092509
06/02/2019 01:25:47 *** evaluating ***
06/02/2019 01:25:47 step: 119, epoch: 118, acc: 60.68376068376068, f1: 27.187896189126704, r: 0.41223082230560076
06/02/2019 01:25:47 *** epoch: 120 ***
06/02/2019 01:25:47 *** training ***
06/02/2019 01:25:47 step: 3932, epoch: 119, batch: 4, loss: 0.025633124634623528, acc: 92.1875, f1: 78.94891289849272, r: 0.8997076977471274
06/02/2019 01:25:48 step: 3937, epoch: 119, batch: 9, loss: 0.028800737112760544, acc: 85.9375, f1: 78.64994802494803, r: 0.9666894300680098
06/02/2019 01:25:48 step: 3942, epoch: 119, batch: 14, loss: 0.027221210300922394, acc: 90.625, f1: 86.79946335036473, r: 0.9598319437521861
06/02/2019 01:25:49 step: 3947, epoch: 119, batch: 19, loss: 0.024433374404907227, acc: 93.75, f1: 90.2793909430972, r: 0.9827072495125833
06/02/2019 01:25:49 step: 3952, epoch: 119, batch: 24, loss: 0.029338689520955086, acc: 92.1875, f1: 94.38466295609153, r: 0.953261337506126
06/02/2019 01:25:50 step: 3957, epoch: 119, batch: 29, loss: 0.030569937080144882, acc: 90.625, f1: 78.7091503267974, r: 0.9316839904976543
06/02/2019 01:25:50 *** evaluating ***
06/02/2019 01:25:50 step: 120, epoch: 119, acc: 60.68376068376068, f1: 28.722651222651223, r: 0.4098042885832925
06/02/2019 01:25:50 *** epoch: 121 ***
06/02/2019 01:25:50 *** training ***
06/02/2019 01:25:51 step: 3965, epoch: 120, batch: 4, loss: 0.027332909405231476, acc: 81.25, f1: 74.29958183990442, r: 0.9519266978066676
06/02/2019 01:25:51 step: 3970, epoch: 120, batch: 9, loss: 0.026698037981987, acc: 92.1875, f1: 91.78587266392144, r: 0.9376196313695855
06/02/2019 01:25:51 step: 3975, epoch: 120, batch: 14, loss: 0.028249327093362808, acc: 89.0625, f1: 83.98597476000266, r: 0.9319786372047265
06/02/2019 01:25:52 step: 3980, epoch: 120, batch: 19, loss: 0.028107043355703354, acc: 92.1875, f1: 82.28258310816344, r: 0.9664310435724418
06/02/2019 01:25:52 step: 3985, epoch: 120, batch: 24, loss: 0.030434265732765198, acc: 95.3125, f1: 91.38613825314593, r: 0.9520002412818231
06/02/2019 01:25:53 step: 3990, epoch: 120, batch: 29, loss: 0.030114125460386276, acc: 87.5, f1: 63.42035083560508, r: 0.9299940777156493
06/02/2019 01:25:53 *** evaluating ***
06/02/2019 01:25:53 step: 121, epoch: 120, acc: 60.256410256410255, f1: 26.3603758834022, r: 0.4128124715259507
06/02/2019 01:25:53 *** epoch: 122 ***
06/02/2019 01:25:53 *** training ***
06/02/2019 01:25:54 step: 3998, epoch: 121, batch: 4, loss: 0.026620885357260704, acc: 90.625, f1: 83.91698058533758, r: 0.9557512445324257
06/02/2019 01:25:54 step: 4003, epoch: 121, batch: 9, loss: 0.024851854890584946, acc: 90.625, f1: 93.96504978803117, r: 0.9674501947763328
06/02/2019 01:25:55 step: 4008, epoch: 121, batch: 14, loss: 0.030054330825805664, acc: 89.0625, f1: 75.65771093313467, r: 0.9383413168232205
06/02/2019 01:25:55 step: 4013, epoch: 121, batch: 19, loss: 0.025983735918998718, acc: 87.5, f1: 65.71762796027501, r: 0.9649768317628841
06/02/2019 01:25:56 step: 4018, epoch: 121, batch: 24, loss: 0.029839394614100456, acc: 95.3125, f1: 97.19774011299435, r: 0.9506180649057937
06/02/2019 01:25:56 step: 4023, epoch: 121, batch: 29, loss: 0.026210153475403786, acc: 82.8125, f1: 57.907454916749614, r: 0.9553407125962662
06/02/2019 01:25:57 *** evaluating ***
06/02/2019 01:25:57 step: 122, epoch: 121, acc: 59.82905982905983, f1: 27.452287430336153, r: 0.40647193953195104
06/02/2019 01:25:57 *** epoch: 123 ***
06/02/2019 01:25:57 *** training ***
06/02/2019 01:25:57 step: 4031, epoch: 122, batch: 4, loss: 0.025138622149825096, acc: 92.1875, f1: 79.86002254288852, r: 0.9700536062842343
06/02/2019 01:25:58 step: 4036, epoch: 122, batch: 9, loss: 0.028186628594994545, acc: 92.1875, f1: 86.34762989203779, r: 0.9492318949893388
06/02/2019 01:25:58 step: 4041, epoch: 122, batch: 14, loss: 0.02759496122598648, acc: 89.0625, f1: 80.51141405178672, r: 0.9099568003030687
06/02/2019 01:25:59 step: 4046, epoch: 122, batch: 19, loss: 0.028310580179095268, acc: 85.9375, f1: 83.66431371340208, r: 0.9592080237479882
06/02/2019 01:25:59 step: 4051, epoch: 122, batch: 24, loss: 0.029353799298405647, acc: 92.1875, f1: 90.45321637426899, r: 0.9695110203127242
06/02/2019 01:26:00 step: 4056, epoch: 122, batch: 29, loss: 0.029482219368219376, acc: 90.625, f1: 80.54483914778032, r: 0.9368252999753757
06/02/2019 01:26:00 *** evaluating ***
06/02/2019 01:26:00 step: 123, epoch: 122, acc: 60.256410256410255, f1: 27.00216081541964, r: 0.4151117935759058
06/02/2019 01:26:00 *** epoch: 124 ***
06/02/2019 01:26:00 *** training ***
06/02/2019 01:26:00 step: 4064, epoch: 123, batch: 4, loss: 0.025938572362065315, acc: 87.5, f1: 85.23459383753502, r: 0.9656662658284407
06/02/2019 01:26:01 step: 4069, epoch: 123, batch: 9, loss: 0.026011213660240173, acc: 89.0625, f1: 74.18148028442147, r: 0.961144521603488
06/02/2019 01:26:01 step: 4074, epoch: 123, batch: 14, loss: 0.027359280735254288, acc: 90.625, f1: 83.63718820861678, r: 0.9667347080639158
06/02/2019 01:26:02 step: 4079, epoch: 123, batch: 19, loss: 0.025131450966000557, acc: 93.75, f1: 90.49775048203958, r: 0.9550976688531119
06/02/2019 01:26:02 step: 4084, epoch: 123, batch: 24, loss: 0.030640047043561935, acc: 82.8125, f1: 66.11495662094626, r: 0.9185987042002164
06/02/2019 01:26:03 step: 4089, epoch: 123, batch: 29, loss: 0.025985462591052055, acc: 95.3125, f1: 92.87394957983194, r: 0.9626771665702354
06/02/2019 01:26:03 *** evaluating ***
06/02/2019 01:26:03 step: 124, epoch: 123, acc: 61.111111111111114, f1: 30.48448815009875, r: 0.4130645456123811
06/02/2019 01:26:03 *** epoch: 125 ***
06/02/2019 01:26:03 *** training ***
06/02/2019 01:26:04 step: 4097, epoch: 124, batch: 4, loss: 0.029899075627326965, acc: 85.9375, f1: 69.56634334683115, r: 0.9434706901910836
06/02/2019 01:26:04 step: 4102, epoch: 124, batch: 9, loss: 0.029046237468719482, acc: 78.125, f1: 69.86430236430236, r: 0.9552878523258049
06/02/2019 01:26:05 step: 4107, epoch: 124, batch: 14, loss: 0.02852281555533409, acc: 84.375, f1: 72.66939511185035, r: 0.9542708982859025
06/02/2019 01:26:05 step: 4112, epoch: 124, batch: 19, loss: 0.025887154042720795, acc: 95.3125, f1: 93.7590286629408, r: 0.9686854666608918
06/02/2019 01:26:06 step: 4117, epoch: 124, batch: 24, loss: 0.025651905685663223, acc: 98.4375, f1: 99.22727711774365, r: 0.9458107194899574
06/02/2019 01:26:06 step: 4122, epoch: 124, batch: 29, loss: 0.028161177411675453, acc: 85.9375, f1: 76.85578056359157, r: 0.9221335517132931
06/02/2019 01:26:06 *** evaluating ***
06/02/2019 01:26:07 step: 125, epoch: 124, acc: 60.68376068376068, f1: 27.232756215766667, r: 0.41781841630393457
06/02/2019 01:26:07 *** epoch: 126 ***
06/02/2019 01:26:07 *** training ***
06/02/2019 01:26:07 step: 4130, epoch: 125, batch: 4, loss: 0.029943330213427544, acc: 84.375, f1: 79.43803106526892, r: 0.9475522207246363
06/02/2019 01:26:08 step: 4135, epoch: 125, batch: 9, loss: 0.02646719105541706, acc: 82.8125, f1: 78.20146276595744, r: 0.9671754155253296
06/02/2019 01:26:08 step: 4140, epoch: 125, batch: 14, loss: 0.033045873045921326, acc: 85.9375, f1: 79.73725722172928, r: 0.9249586147214989
06/02/2019 01:26:09 step: 4145, epoch: 125, batch: 19, loss: 0.020415915176272392, acc: 90.625, f1: 79.05520687426656, r: 0.9667541644709601
06/02/2019 01:26:09 step: 4150, epoch: 125, batch: 24, loss: 0.027950510382652283, acc: 92.1875, f1: 88.49237567987568, r: 0.9660948428758119
06/02/2019 01:26:09 step: 4155, epoch: 125, batch: 29, loss: 0.026525279507040977, acc: 89.0625, f1: 84.08163265306122, r: 0.9635438675849508
06/02/2019 01:26:10 *** evaluating ***
06/02/2019 01:26:10 step: 126, epoch: 125, acc: 61.53846153846154, f1: 28.851687354696065, r: 0.4012536880289125
06/02/2019 01:26:10 *** epoch: 127 ***
06/02/2019 01:26:10 *** training ***
06/02/2019 01:26:10 step: 4163, epoch: 126, batch: 4, loss: 0.03049379587173462, acc: 85.9375, f1: 76.41941391941391, r: 0.9259083193636992
06/02/2019 01:26:11 step: 4168, epoch: 126, batch: 9, loss: 0.022693496197462082, acc: 87.5, f1: 82.66036007670951, r: 0.9760436179563858
06/02/2019 01:26:11 step: 4173, epoch: 126, batch: 14, loss: 0.02596944570541382, acc: 87.5, f1: 88.2487083894903, r: 0.9777225716770045
06/02/2019 01:26:12 step: 4178, epoch: 126, batch: 19, loss: 0.02978295460343361, acc: 87.5, f1: 65.65286831812256, r: 0.9373627835731078
06/02/2019 01:26:12 step: 4183, epoch: 126, batch: 24, loss: 0.029800234362483025, acc: 82.8125, f1: 69.44940476190476, r: 0.9433931233210695
06/02/2019 01:26:13 step: 4188, epoch: 126, batch: 29, loss: 0.0302840918302536, acc: 87.5, f1: 70.23863636363636, r: 0.9450054650761457
06/02/2019 01:26:13 *** evaluating ***
06/02/2019 01:26:13 step: 127, epoch: 126, acc: 59.82905982905983, f1: 27.241583461341527, r: 0.40571344228014744
06/02/2019 01:26:13 *** epoch: 128 ***
06/02/2019 01:26:13 *** training ***
06/02/2019 01:26:14 step: 4196, epoch: 127, batch: 4, loss: 0.026256324723362923, acc: 90.625, f1: 91.56565656565657, r: 0.9600228160710613
06/02/2019 01:26:14 step: 4201, epoch: 127, batch: 9, loss: 0.033270955085754395, acc: 79.6875, f1: 73.9085744187785, r: 0.8983886790939459
06/02/2019 01:26:15 step: 4206, epoch: 127, batch: 14, loss: 0.026183275505900383, acc: 84.375, f1: 74.18248418248419, r: 0.9621999614936109
06/02/2019 01:26:15 step: 4211, epoch: 127, batch: 19, loss: 0.029252581298351288, acc: 85.9375, f1: 69.1426994058573, r: 0.9689147470192473
06/02/2019 01:26:16 step: 4216, epoch: 127, batch: 24, loss: 0.025736624374985695, acc: 84.375, f1: 81.40589569160997, r: 0.9451849952178197
06/02/2019 01:26:16 step: 4221, epoch: 127, batch: 29, loss: 0.029178626835346222, acc: 95.3125, f1: 92.48283752860411, r: 0.9589048477405941
06/02/2019 01:26:16 *** evaluating ***
06/02/2019 01:26:17 step: 128, epoch: 127, acc: 61.965811965811966, f1: 31.650758612112917, r: 0.4108664202516326
06/02/2019 01:26:17 *** epoch: 129 ***
06/02/2019 01:26:17 *** training ***
06/02/2019 01:26:17 step: 4229, epoch: 128, batch: 4, loss: 0.03062240406870842, acc: 89.0625, f1: 76.06060606060605, r: 0.9566261903764576
06/02/2019 01:26:17 step: 4234, epoch: 128, batch: 9, loss: 0.025464054197072983, acc: 90.625, f1: 82.43589743589743, r: 0.9668399235941719
06/02/2019 01:26:18 step: 4239, epoch: 128, batch: 14, loss: 0.02762550115585327, acc: 93.75, f1: 92.94870186880722, r: 0.9557640470261614
06/02/2019 01:26:18 step: 4244, epoch: 128, batch: 19, loss: 0.03177478909492493, acc: 85.9375, f1: 84.51902173913044, r: 0.9123819479646468
06/02/2019 01:26:19 step: 4249, epoch: 128, batch: 24, loss: 0.02396094612777233, acc: 89.0625, f1: 81.99168556311413, r: 0.9509531176793348
06/02/2019 01:26:19 step: 4254, epoch: 128, batch: 29, loss: 0.0218636654317379, acc: 93.75, f1: 85.35714285714285, r: 0.9647902650177186
06/02/2019 01:26:20 *** evaluating ***
06/02/2019 01:26:20 step: 129, epoch: 128, acc: 59.401709401709404, f1: 26.40751135308097, r: 0.41385369458956467
06/02/2019 01:26:20 *** epoch: 130 ***
06/02/2019 01:26:20 *** training ***
06/02/2019 01:26:20 step: 4262, epoch: 129, batch: 4, loss: 0.029425160959362984, acc: 85.9375, f1: 75.50351650675678, r: 0.948874705472325
06/02/2019 01:26:21 step: 4267, epoch: 129, batch: 9, loss: 0.028399843722581863, acc: 87.5, f1: 78.06890120650694, r: 0.9493947918651452
06/02/2019 01:26:21 step: 4272, epoch: 129, batch: 14, loss: 0.026145098730921745, acc: 87.5, f1: 86.49422799422798, r: 0.9676985947287701
06/02/2019 01:26:22 step: 4277, epoch: 129, batch: 19, loss: 0.02661675028502941, acc: 85.9375, f1: 64.55357142857143, r: 0.9430876613056772
06/02/2019 01:26:22 step: 4282, epoch: 129, batch: 24, loss: 0.030154887586832047, acc: 89.0625, f1: 86.58780296461455, r: 0.935939372823063
06/02/2019 01:26:23 step: 4287, epoch: 129, batch: 29, loss: 0.028315093368291855, acc: 96.875, f1: 95.87526205450733, r: 0.9645406842199541
06/02/2019 01:26:23 *** evaluating ***
06/02/2019 01:26:23 step: 130, epoch: 129, acc: 59.82905982905983, f1: 27.135349809561077, r: 0.4152396593805149
06/02/2019 01:26:23 *** epoch: 131 ***
06/02/2019 01:26:23 *** training ***
06/02/2019 01:26:24 step: 4295, epoch: 130, batch: 4, loss: 0.024311918765306473, acc: 87.5, f1: 67.80969634230503, r: 0.9639025151851168
06/02/2019 01:26:24 step: 4300, epoch: 130, batch: 9, loss: 0.027246056124567986, acc: 89.0625, f1: 75.62606777870965, r: 0.9486253300316
06/02/2019 01:26:25 step: 4305, epoch: 130, batch: 14, loss: 0.026999520137906075, acc: 93.75, f1: 87.39429896344791, r: 0.9598549225020935
06/02/2019 01:26:25 step: 4310, epoch: 130, batch: 19, loss: 0.031348008662462234, acc: 90.625, f1: 82.12757566794834, r: 0.9306947720958019
06/02/2019 01:26:26 step: 4315, epoch: 130, batch: 24, loss: 0.02792125567793846, acc: 89.0625, f1: 79.29712323800995, r: 0.9467809034200855
06/02/2019 01:26:26 step: 4320, epoch: 130, batch: 29, loss: 0.024203140288591385, acc: 84.375, f1: 70.19918025237175, r: 0.9575172818668901
06/02/2019 01:26:26 *** evaluating ***
06/02/2019 01:26:27 step: 131, epoch: 130, acc: 61.53846153846154, f1: 30.127500180824036, r: 0.4084104170879864
06/02/2019 01:26:27 *** epoch: 132 ***
06/02/2019 01:26:27 *** training ***
06/02/2019 01:26:27 step: 4328, epoch: 131, batch: 4, loss: 0.02914709411561489, acc: 89.0625, f1: 75.89348151848152, r: 0.9413501393371465
06/02/2019 01:26:28 step: 4333, epoch: 131, batch: 9, loss: 0.02532820962369442, acc: 87.5, f1: 82.15577139946888, r: 0.9522298614920258
06/02/2019 01:26:28 step: 4338, epoch: 131, batch: 14, loss: 0.029971705749630928, acc: 87.5, f1: 82.0, r: 0.959195616500929
06/02/2019 01:26:29 step: 4343, epoch: 131, batch: 19, loss: 0.024354606866836548, acc: 93.75, f1: 90.94614127702363, r: 0.9776180188091995
06/02/2019 01:26:29 step: 4348, epoch: 131, batch: 24, loss: 0.026028616353869438, acc: 89.0625, f1: 72.83618769511382, r: 0.9627344730906678
06/02/2019 01:26:29 step: 4353, epoch: 131, batch: 29, loss: 0.028603896498680115, acc: 93.75, f1: 77.58823529411765, r: 0.9393543441687774
06/02/2019 01:26:30 *** evaluating ***
06/02/2019 01:26:30 step: 132, epoch: 131, acc: 60.68376068376068, f1: 26.7326499782025, r: 0.4099958591368007
06/02/2019 01:26:30 *** epoch: 133 ***
06/02/2019 01:26:30 *** training ***
06/02/2019 01:26:30 step: 4361, epoch: 132, batch: 4, loss: 0.029041102156043053, acc: 82.8125, f1: 71.25167458287662, r: 0.933272344922477
06/02/2019 01:26:31 step: 4366, epoch: 132, batch: 9, loss: 0.02782408893108368, acc: 89.0625, f1: 87.25246656281139, r: 0.9599683317511634
06/02/2019 01:26:31 step: 4371, epoch: 132, batch: 14, loss: 0.029201485216617584, acc: 93.75, f1: 87.12148652564473, r: 0.95014418738612
06/02/2019 01:26:32 step: 4376, epoch: 132, batch: 19, loss: 0.02635328471660614, acc: 85.9375, f1: 79.9908424908425, r: 0.9627518946828207
06/02/2019 01:26:32 step: 4381, epoch: 132, batch: 24, loss: 0.028166284784674644, acc: 92.1875, f1: 89.03678196630766, r: 0.9579140614774317
06/02/2019 01:26:33 step: 4386, epoch: 132, batch: 29, loss: 0.02489946037530899, acc: 87.5, f1: 77.1494708994709, r: 0.970143037172967
06/02/2019 01:26:33 *** evaluating ***
06/02/2019 01:26:33 step: 133, epoch: 132, acc: 61.111111111111114, f1: 26.40032396480944, r: 0.40607847813412373
06/02/2019 01:26:33 *** epoch: 134 ***
06/02/2019 01:26:33 *** training ***
06/02/2019 01:26:34 step: 4394, epoch: 133, batch: 4, loss: 0.029851414263248444, acc: 85.9375, f1: 65.53099017384731, r: 0.9387809432732037
06/02/2019 01:26:34 step: 4399, epoch: 133, batch: 9, loss: 0.027489881962537766, acc: 84.375, f1: 69.91942839768926, r: 0.9484645806260827
06/02/2019 01:26:35 step: 4404, epoch: 133, batch: 14, loss: 0.024232767522335052, acc: 93.75, f1: 80.41541144759792, r: 0.9720412148846839
06/02/2019 01:26:35 step: 4409, epoch: 133, batch: 19, loss: 0.02926076203584671, acc: 87.5, f1: 74.41073271413829, r: 0.9460093228682687
06/02/2019 01:26:36 step: 4414, epoch: 133, batch: 24, loss: 0.027029627934098244, acc: 90.625, f1: 87.47868712702471, r: 0.9601205788387278
06/02/2019 01:26:36 step: 4419, epoch: 133, batch: 29, loss: 0.025328254327178, acc: 89.0625, f1: 74.43449828473415, r: 0.9658688031564772
06/02/2019 01:26:36 *** evaluating ***
06/02/2019 01:26:37 step: 134, epoch: 133, acc: 60.256410256410255, f1: 26.287592468924316, r: 0.40774704400259354
06/02/2019 01:26:37 *** epoch: 135 ***
06/02/2019 01:26:37 *** training ***
06/02/2019 01:26:37 step: 4427, epoch: 134, batch: 4, loss: 0.02632368914783001, acc: 93.75, f1: 89.07243445777281, r: 0.9675176541544447
06/02/2019 01:26:38 step: 4432, epoch: 134, batch: 9, loss: 0.026725992560386658, acc: 92.1875, f1: 91.07405462184875, r: 0.9602043849046221
06/02/2019 01:26:38 step: 4437, epoch: 134, batch: 14, loss: 0.030444983392953873, acc: 95.3125, f1: 93.02436125965538, r: 0.9506320235573162
06/02/2019 01:26:39 step: 4442, epoch: 134, batch: 19, loss: 0.026099342852830887, acc: 85.9375, f1: 85.18187830687832, r: 0.9757198777855405
06/02/2019 01:26:39 step: 4447, epoch: 134, batch: 24, loss: 0.02626330405473709, acc: 87.5, f1: 77.55816119551268, r: 0.9656436914616321
06/02/2019 01:26:40 step: 4452, epoch: 134, batch: 29, loss: 0.02457505464553833, acc: 87.5, f1: 83.33626939553929, r: 0.9737585275574103
06/02/2019 01:26:40 *** evaluating ***
06/02/2019 01:26:40 step: 135, epoch: 134, acc: 60.256410256410255, f1: 27.734316852737905, r: 0.40444796493161816
06/02/2019 01:26:40 *** epoch: 136 ***
06/02/2019 01:26:40 *** training ***
06/02/2019 01:26:40 step: 4460, epoch: 135, batch: 4, loss: 0.02722487598657608, acc: 89.0625, f1: 82.49050477824062, r: 0.9603034383813673
06/02/2019 01:26:41 step: 4465, epoch: 135, batch: 9, loss: 0.021790295839309692, acc: 84.375, f1: 82.61007169666286, r: 0.9800715234623285
06/02/2019 01:26:41 step: 4470, epoch: 135, batch: 14, loss: 0.026174454018473625, acc: 87.5, f1: 81.44133625990443, r: 0.9529592751071589
06/02/2019 01:26:42 step: 4475, epoch: 135, batch: 19, loss: 0.028233658522367477, acc: 87.5, f1: 79.52306680247857, r: 0.964706303009595
06/02/2019 01:26:42 step: 4480, epoch: 135, batch: 24, loss: 0.02378018945455551, acc: 95.3125, f1: 94.64121132323896, r: 0.9364569092639694
06/02/2019 01:26:43 step: 4485, epoch: 135, batch: 29, loss: 0.027477383613586426, acc: 90.625, f1: 71.97278911564625, r: 0.9491863356782783
06/02/2019 01:26:43 *** evaluating ***
06/02/2019 01:26:43 step: 136, epoch: 135, acc: 59.82905982905983, f1: 25.85530045351474, r: 0.40587847949041356
06/02/2019 01:26:43 *** epoch: 137 ***
06/02/2019 01:26:43 *** training ***
06/02/2019 01:26:44 step: 4493, epoch: 136, batch: 4, loss: 0.029418785125017166, acc: 87.5, f1: 82.77841781874041, r: 0.9528757501768877
06/02/2019 01:26:44 step: 4498, epoch: 136, batch: 9, loss: 0.02634574845433235, acc: 90.625, f1: 89.05998763141619, r: 0.9566321908774924
06/02/2019 01:26:45 step: 4503, epoch: 136, batch: 14, loss: 0.02907857485115528, acc: 89.0625, f1: 84.61018617973505, r: 0.9603606903786699
06/02/2019 01:26:45 step: 4508, epoch: 136, batch: 19, loss: 0.02557417005300522, acc: 85.9375, f1: 81.76252319109462, r: 0.9276657183938182
06/02/2019 01:26:46 step: 4513, epoch: 136, batch: 24, loss: 0.022609686478972435, acc: 92.1875, f1: 90.8236221123218, r: 0.9812862016614797
06/02/2019 01:26:46 step: 4518, epoch: 136, batch: 29, loss: 0.02809971012175083, acc: 87.5, f1: 84.77981029810297, r: 0.9685722269512204
06/02/2019 01:26:47 *** evaluating ***
06/02/2019 01:26:47 step: 137, epoch: 136, acc: 60.256410256410255, f1: 26.85647766713057, r: 0.4025706995795992
06/02/2019 01:26:47 *** epoch: 138 ***
06/02/2019 01:26:47 *** training ***
06/02/2019 01:26:47 step: 4526, epoch: 137, batch: 4, loss: 0.024798616766929626, acc: 93.75, f1: 88.94557823129252, r: 0.9273315526930064
06/02/2019 01:26:48 step: 4531, epoch: 137, batch: 9, loss: 0.024022333323955536, acc: 90.625, f1: 78.78654970760233, r: 0.9738973656539442
06/02/2019 01:26:48 step: 4536, epoch: 137, batch: 14, loss: 0.024366751313209534, acc: 89.0625, f1: 86.95699228307923, r: 0.9634556159394122
06/02/2019 01:26:49 step: 4541, epoch: 137, batch: 19, loss: 0.030224930495023727, acc: 87.5, f1: 85.26105536975103, r: 0.9602917494547847
06/02/2019 01:26:49 step: 4546, epoch: 137, batch: 24, loss: 0.03201117366552353, acc: 87.5, f1: 82.90850578960335, r: 0.9499805759906591
06/02/2019 01:26:50 step: 4551, epoch: 137, batch: 29, loss: 0.023290889337658882, acc: 89.0625, f1: 82.573309146585, r: 0.97706856771573
06/02/2019 01:26:50 *** evaluating ***
06/02/2019 01:26:50 step: 138, epoch: 137, acc: 60.68376068376068, f1: 26.046074984331373, r: 0.4046605331923044
06/02/2019 01:26:50 *** epoch: 139 ***
06/02/2019 01:26:50 *** training ***
06/02/2019 01:26:50 step: 4559, epoch: 138, batch: 4, loss: 0.027644148096442223, acc: 89.0625, f1: 85.28519099947671, r: 0.9500355183112563
06/02/2019 01:26:51 step: 4564, epoch: 138, batch: 9, loss: 0.026706736534833908, acc: 85.9375, f1: 71.14010833535453, r: 0.9551258117743723
06/02/2019 01:26:52 step: 4569, epoch: 138, batch: 14, loss: 0.026046939194202423, acc: 87.5, f1: 63.805825322218766, r: 0.9633827975551763
06/02/2019 01:26:52 step: 4574, epoch: 138, batch: 19, loss: 0.02929881401360035, acc: 85.9375, f1: 71.8763102725367, r: 0.9534938684112086
06/02/2019 01:26:53 step: 4579, epoch: 138, batch: 24, loss: 0.024030404165387154, acc: 92.1875, f1: 79.31763285024155, r: 0.9628096751358843
06/02/2019 01:26:53 step: 4584, epoch: 138, batch: 29, loss: 0.027505189180374146, acc: 81.25, f1: 65.65367965367966, r: 0.9633837318084417
06/02/2019 01:26:53 *** evaluating ***
06/02/2019 01:26:53 step: 139, epoch: 138, acc: 61.53846153846154, f1: 28.662762684885447, r: 0.4073180050495066
06/02/2019 01:26:53 *** epoch: 140 ***
06/02/2019 01:26:53 *** training ***
06/02/2019 01:26:54 step: 4592, epoch: 139, batch: 4, loss: 0.026555266231298447, acc: 89.0625, f1: 83.05288567039719, r: 0.951277704925882
06/02/2019 01:26:54 step: 4597, epoch: 139, batch: 9, loss: 0.02812185510993004, acc: 82.8125, f1: 79.04121068717342, r: 0.9578602187499882
06/02/2019 01:26:55 step: 4602, epoch: 139, batch: 14, loss: 0.027076803147792816, acc: 85.9375, f1: 71.07830056939963, r: 0.9564724858335109
06/02/2019 01:26:55 step: 4607, epoch: 139, batch: 19, loss: 0.027060816064476967, acc: 90.625, f1: 88.04192651014918, r: 0.963436010704221
06/02/2019 01:26:56 step: 4612, epoch: 139, batch: 24, loss: 0.027527518570423126, acc: 89.0625, f1: 84.5325616412573, r: 0.9687613822746688
06/02/2019 01:26:56 step: 4617, epoch: 139, batch: 29, loss: 0.026192737743258476, acc: 92.1875, f1: 94.07447189402076, r: 0.940130055365386
06/02/2019 01:26:57 *** evaluating ***
06/02/2019 01:26:57 step: 140, epoch: 139, acc: 61.53846153846154, f1: 28.77536922280755, r: 0.4075028943008002
06/02/2019 01:26:57 *** epoch: 141 ***
06/02/2019 01:26:57 *** training ***
06/02/2019 01:26:57 step: 4625, epoch: 140, batch: 4, loss: 0.03085297718644142, acc: 81.25, f1: 78.13664596273291, r: 0.9184683764369648
06/02/2019 01:26:58 step: 4630, epoch: 140, batch: 9, loss: 0.023802049458026886, acc: 90.625, f1: 92.68133595963785, r: 0.9415303680968339
06/02/2019 01:26:58 step: 4635, epoch: 140, batch: 14, loss: 0.025391988456249237, acc: 90.625, f1: 89.26651305683563, r: 0.9652171722524302
06/02/2019 01:26:59 step: 4640, epoch: 140, batch: 19, loss: 0.025187784805893898, acc: 93.75, f1: 91.05566564582958, r: 0.954430557325032
06/02/2019 01:26:59 step: 4645, epoch: 140, batch: 24, loss: 0.02601686492562294, acc: 90.625, f1: 86.57456828885401, r: 0.9443766670874593
06/02/2019 01:27:00 step: 4650, epoch: 140, batch: 29, loss: 0.028878390789031982, acc: 92.1875, f1: 86.1590296495957, r: 0.914880649995896
06/02/2019 01:27:00 *** evaluating ***
06/02/2019 01:27:00 step: 141, epoch: 140, acc: 60.68376068376068, f1: 27.196196807895888, r: 0.4112398414604619
06/02/2019 01:27:00 *** epoch: 142 ***
06/02/2019 01:27:00 *** training ***
06/02/2019 01:27:01 step: 4658, epoch: 141, batch: 4, loss: 0.029745545238256454, acc: 85.9375, f1: 78.56857428285998, r: 0.9286061450181391
06/02/2019 01:27:01 step: 4663, epoch: 141, batch: 9, loss: 0.029712039977312088, acc: 93.75, f1: 93.95031055900621, r: 0.9250917265269192
06/02/2019 01:27:02 step: 4668, epoch: 141, batch: 14, loss: 0.024423174560070038, acc: 85.9375, f1: 64.87240829346092, r: 0.9679684911612951
06/02/2019 01:27:02 step: 4673, epoch: 141, batch: 19, loss: 0.028626583516597748, acc: 81.25, f1: 81.56565656565657, r: 0.963348587108215
06/02/2019 01:27:03 step: 4678, epoch: 141, batch: 24, loss: 0.025731224566698074, acc: 78.125, f1: 77.23099263860134, r: 0.9696578124041108
06/02/2019 01:27:03 step: 4683, epoch: 141, batch: 29, loss: 0.026346296072006226, acc: 90.625, f1: 85.83290612895875, r: 0.972201986494559
06/02/2019 01:27:03 *** evaluating ***
06/02/2019 01:27:03 step: 142, epoch: 141, acc: 61.111111111111114, f1: 27.49685842716533, r: 0.41253563031738816
06/02/2019 01:27:03 *** epoch: 143 ***
06/02/2019 01:27:03 *** training ***
06/02/2019 01:27:04 step: 4691, epoch: 142, batch: 4, loss: 0.026044171303510666, acc: 92.1875, f1: 92.32789269553976, r: 0.9698134972086341
06/02/2019 01:27:04 step: 4696, epoch: 142, batch: 9, loss: 0.026716185733675957, acc: 92.1875, f1: 87.64484126984127, r: 0.9645676743421334
06/02/2019 01:27:05 step: 4701, epoch: 142, batch: 14, loss: 0.024270547553896904, acc: 93.75, f1: 86.83608058608058, r: 0.9764631959200621
06/02/2019 01:27:05 step: 4706, epoch: 142, batch: 19, loss: 0.028890013694763184, acc: 98.4375, f1: 98.36601307189542, r: 0.9340906205212461
06/02/2019 01:27:06 step: 4711, epoch: 142, batch: 24, loss: 0.024968376383185387, acc: 87.5, f1: 67.72614482291901, r: 0.9651778145079454
06/02/2019 01:27:06 step: 4716, epoch: 142, batch: 29, loss: 0.023566612973809242, acc: 95.3125, f1: 95.91797091797092, r: 0.9734578623267689
06/02/2019 01:27:07 *** evaluating ***
06/02/2019 01:27:07 step: 143, epoch: 142, acc: 61.111111111111114, f1: 27.018621744983168, r: 0.4057981594372098
06/02/2019 01:27:07 *** epoch: 144 ***
06/02/2019 01:27:07 *** training ***
06/02/2019 01:27:07 step: 4724, epoch: 143, batch: 4, loss: 0.025221914052963257, acc: 92.1875, f1: 78.82682132682133, r: 0.9596888880563288
06/02/2019 01:27:08 step: 4729, epoch: 143, batch: 9, loss: 0.02660497836768627, acc: 95.3125, f1: 81.30656185919344, r: 0.9523489399353127
06/02/2019 01:27:08 step: 4734, epoch: 143, batch: 14, loss: 0.028211507946252823, acc: 89.0625, f1: 85.14264203705196, r: 0.9467176014863539
06/02/2019 01:27:09 step: 4739, epoch: 143, batch: 19, loss: 0.02797985076904297, acc: 90.625, f1: 72.04509775938347, r: 0.9255567302521044
06/02/2019 01:27:09 step: 4744, epoch: 143, batch: 24, loss: 0.02501371130347252, acc: 89.0625, f1: 87.09536928636872, r: 0.9680551950852722
06/02/2019 01:27:10 step: 4749, epoch: 143, batch: 29, loss: 0.028950748965144157, acc: 90.625, f1: 92.41050126344244, r: 0.9486446083717797
06/02/2019 01:27:10 *** evaluating ***
06/02/2019 01:27:10 step: 144, epoch: 143, acc: 60.256410256410255, f1: 25.71338383838384, r: 0.4128656091155979
06/02/2019 01:27:10 *** epoch: 145 ***
06/02/2019 01:27:10 *** training ***
06/02/2019 01:27:11 step: 4757, epoch: 144, batch: 4, loss: 0.023541852831840515, acc: 87.5, f1: 70.43894830659536, r: 0.974487057560212
06/02/2019 01:27:11 step: 4762, epoch: 144, batch: 9, loss: 0.026193665340542793, acc: 85.9375, f1: 71.76190617947584, r: 0.970183496242556
06/02/2019 01:27:12 step: 4767, epoch: 144, batch: 14, loss: 0.026828454807400703, acc: 95.3125, f1: 92.39163614163614, r: 0.9506155832931098
06/02/2019 01:27:12 step: 4772, epoch: 144, batch: 19, loss: 0.029662080109119415, acc: 90.625, f1: 90.40572252913451, r: 0.9539261949700959
06/02/2019 01:27:13 step: 4777, epoch: 144, batch: 24, loss: 0.027592528611421585, acc: 92.1875, f1: 86.44757688589377, r: 0.9622948016544169
06/02/2019 01:27:13 step: 4782, epoch: 144, batch: 29, loss: 0.029477480798959732, acc: 89.0625, f1: 89.20697097388826, r: 0.9607519121258556
06/02/2019 01:27:13 *** evaluating ***
06/02/2019 01:27:14 step: 145, epoch: 144, acc: 62.39316239316239, f1: 28.77204701392943, r: 0.41427202384397066
06/02/2019 01:27:14 *** epoch: 146 ***
06/02/2019 01:27:14 *** training ***
06/02/2019 01:27:14 step: 4790, epoch: 145, batch: 4, loss: 0.025669187307357788, acc: 92.1875, f1: 90.3278063655422, r: 0.9594235303803088
06/02/2019 01:27:15 step: 4795, epoch: 145, batch: 9, loss: 0.02527420036494732, acc: 82.8125, f1: 75.37896765471517, r: 0.9687296978222177
06/02/2019 01:27:15 step: 4800, epoch: 145, batch: 14, loss: 0.027434779331088066, acc: 93.75, f1: 82.97110448749794, r: 0.9626692370758836
06/02/2019 01:27:15 step: 4805, epoch: 145, batch: 19, loss: 0.027876632288098335, acc: 93.75, f1: 93.0952380952381, r: 0.9636102218956626
06/02/2019 01:27:16 step: 4810, epoch: 145, batch: 24, loss: 0.02716899663209915, acc: 87.5, f1: 80.30254206945936, r: 0.9677288790829246
06/02/2019 01:27:16 step: 4815, epoch: 145, batch: 29, loss: 0.026773586869239807, acc: 89.0625, f1: 88.95152505446623, r: 0.9629769945212084
06/02/2019 01:27:17 *** evaluating ***
06/02/2019 01:27:17 step: 146, epoch: 145, acc: 59.82905982905983, f1: 25.80823167021394, r: 0.41435317714279174
06/02/2019 01:27:17 *** epoch: 147 ***
06/02/2019 01:27:17 *** training ***
06/02/2019 01:27:17 step: 4823, epoch: 146, batch: 4, loss: 0.02880029007792473, acc: 84.375, f1: 73.59364357662905, r: 0.9643667517404317
06/02/2019 01:27:18 step: 4828, epoch: 146, batch: 9, loss: 0.024706561118364334, acc: 95.3125, f1: 93.37573990635215, r: 0.960457652657914
06/02/2019 01:27:18 step: 4833, epoch: 146, batch: 14, loss: 0.027382945641875267, acc: 90.625, f1: 90.96442429521699, r: 0.9675495267454546
06/02/2019 01:27:19 step: 4838, epoch: 146, batch: 19, loss: 0.024395117536187172, acc: 84.375, f1: 74.45274126118679, r: 0.9632219990110051
06/02/2019 01:27:19 step: 4843, epoch: 146, batch: 24, loss: 0.020836733281612396, acc: 93.75, f1: 83.11115432258809, r: 0.977602522909686
06/02/2019 01:27:20 step: 4848, epoch: 146, batch: 29, loss: 0.024129170924425125, acc: 92.1875, f1: 86.51440648723258, r: 0.9718277650298166
06/02/2019 01:27:20 *** evaluating ***
06/02/2019 01:27:20 step: 147, epoch: 146, acc: 60.256410256410255, f1: 27.1062602124183, r: 0.412135140655449
06/02/2019 01:27:20 *** epoch: 148 ***
06/02/2019 01:27:20 *** training ***
06/02/2019 01:27:21 step: 4856, epoch: 147, batch: 4, loss: 0.026614321395754814, acc: 85.9375, f1: 74.01018099547511, r: 0.9681940741827879
06/02/2019 01:27:21 step: 4861, epoch: 147, batch: 9, loss: 0.023830251768231392, acc: 90.625, f1: 79.5992730095991, r: 0.9636066532428578
06/02/2019 01:27:22 step: 4866, epoch: 147, batch: 14, loss: 0.028532247990369797, acc: 85.9375, f1: 86.92838921793593, r: 0.9256497865172664
06/02/2019 01:27:22 step: 4871, epoch: 147, batch: 19, loss: 0.025004256516695023, acc: 93.75, f1: 91.66373386961622, r: 0.9706190740953571
06/02/2019 01:27:23 step: 4876, epoch: 147, batch: 24, loss: 0.02468542754650116, acc: 92.1875, f1: 87.09840754483612, r: 0.9602677897893427
06/02/2019 01:27:23 step: 4881, epoch: 147, batch: 29, loss: 0.022769439965486526, acc: 96.875, f1: 80.812324929972, r: 0.9496474526491442
06/02/2019 01:27:24 *** evaluating ***
06/02/2019 01:27:24 step: 148, epoch: 147, acc: 60.256410256410255, f1: 27.292073363067665, r: 0.4131392300752736
06/02/2019 01:27:24 *** epoch: 149 ***
06/02/2019 01:27:24 *** training ***
06/02/2019 01:27:24 step: 4889, epoch: 148, batch: 4, loss: 0.024660633876919746, acc: 89.0625, f1: 87.07509988122233, r: 0.9393397482400615
06/02/2019 01:27:25 step: 4894, epoch: 148, batch: 9, loss: 0.026115350425243378, acc: 84.375, f1: 81.45021645021644, r: 0.968317221660081
06/02/2019 01:27:25 step: 4899, epoch: 148, batch: 14, loss: 0.02225623093545437, acc: 84.375, f1: 79.52081931673769, r: 0.9671449273224745
06/02/2019 01:27:26 step: 4904, epoch: 148, batch: 19, loss: 0.025534173473715782, acc: 81.25, f1: 69.27239622405601, r: 0.9727156528158014
06/02/2019 01:27:26 step: 4909, epoch: 148, batch: 24, loss: 0.02546425350010395, acc: 85.9375, f1: 88.06151116573112, r: 0.9740499766531607
06/02/2019 01:27:27 step: 4914, epoch: 148, batch: 29, loss: 0.029575716704130173, acc: 89.0625, f1: 72.4247976641793, r: 0.9206087475315455
06/02/2019 01:27:27 *** evaluating ***
06/02/2019 01:27:27 step: 149, epoch: 148, acc: 58.97435897435898, f1: 25.56997157837494, r: 0.4062700169914514
06/02/2019 01:27:27 *** epoch: 150 ***
06/02/2019 01:27:27 *** training ***
06/02/2019 01:27:28 step: 4922, epoch: 149, batch: 4, loss: 0.025760142132639885, acc: 87.5, f1: 85.98899876502851, r: 0.9344539539821137
06/02/2019 01:27:28 step: 4927, epoch: 149, batch: 9, loss: 0.029633454978466034, acc: 87.5, f1: 84.12215950209871, r: 0.9457354764853407
06/02/2019 01:27:28 step: 4932, epoch: 149, batch: 14, loss: 0.025579020380973816, acc: 92.1875, f1: 81.81678921568627, r: 0.9693420932489283
06/02/2019 01:27:29 step: 4937, epoch: 149, batch: 19, loss: 0.027651645243167877, acc: 82.8125, f1: 78.75293140512018, r: 0.9649196807704623
06/02/2019 01:27:29 step: 4942, epoch: 149, batch: 24, loss: 0.025748716667294502, acc: 93.75, f1: 92.51472626472625, r: 0.9579353159869265
06/02/2019 01:27:30 step: 4947, epoch: 149, batch: 29, loss: 0.023641880601644516, acc: 95.3125, f1: 95.56576402321083, r: 0.979274734888288
06/02/2019 01:27:30 *** evaluating ***
06/02/2019 01:27:30 step: 150, epoch: 149, acc: 60.256410256410255, f1: 27.098048026191446, r: 0.41071047132337635
06/02/2019 01:27:30 *** epoch: 151 ***
06/02/2019 01:27:30 *** training ***
06/02/2019 01:27:31 step: 4955, epoch: 150, batch: 4, loss: 0.02562752738595009, acc: 84.375, f1: 70.98830084359324, r: 0.9721795698991437
06/02/2019 01:27:31 step: 4960, epoch: 150, batch: 9, loss: 0.027002453804016113, acc: 89.0625, f1: 88.68561332847047, r: 0.9579058372894115
06/02/2019 01:27:32 step: 4965, epoch: 150, batch: 14, loss: 0.02551799826323986, acc: 84.375, f1: 75.95535714285715, r: 0.9656352662339904
06/02/2019 01:27:32 step: 4970, epoch: 150, batch: 19, loss: 0.024929888546466827, acc: 92.1875, f1: 85.59203391501529, r: 0.9557115849968394
06/02/2019 01:27:33 step: 4975, epoch: 150, batch: 24, loss: 0.022517496719956398, acc: 90.625, f1: 77.60841170323928, r: 0.97517376952943
06/02/2019 01:27:33 step: 4980, epoch: 150, batch: 29, loss: 0.02330135926604271, acc: 92.1875, f1: 83.41617933723197, r: 0.9650589483586689
06/02/2019 01:27:34 *** evaluating ***
06/02/2019 01:27:34 step: 151, epoch: 150, acc: 61.53846153846154, f1: 27.731147795086407, r: 0.40496312656058736
06/02/2019 01:27:34 *** epoch: 152 ***
06/02/2019 01:27:34 *** training ***
06/02/2019 01:27:34 step: 4988, epoch: 151, batch: 4, loss: 0.02280363440513611, acc: 87.5, f1: 76.25225528808014, r: 0.9485535231366407
06/02/2019 01:27:35 step: 4993, epoch: 151, batch: 9, loss: 0.02275659516453743, acc: 90.625, f1: 77.70691717702587, r: 0.9792087292018654
06/02/2019 01:27:35 step: 4998, epoch: 151, batch: 14, loss: 0.026930419728159904, acc: 93.75, f1: 91.74513496547392, r: 0.9423331531850946
06/02/2019 01:27:36 step: 5003, epoch: 151, batch: 19, loss: 0.024473855271935463, acc: 84.375, f1: 77.8674527227159, r: 0.9747182234078131
06/02/2019 01:27:36 step: 5008, epoch: 151, batch: 24, loss: 0.023455796763300896, acc: 90.625, f1: 90.55565903023529, r: 0.9488789077787279
06/02/2019 01:27:37 step: 5013, epoch: 151, batch: 29, loss: 0.025487959384918213, acc: 89.0625, f1: 82.99943475452196, r: 0.9712571288813219
06/02/2019 01:27:37 *** evaluating ***
06/02/2019 01:27:37 step: 152, epoch: 151, acc: 61.53846153846154, f1: 28.464004370076474, r: 0.41045214836993954
06/02/2019 01:27:37 *** epoch: 153 ***
06/02/2019 01:27:37 *** training ***
06/02/2019 01:27:37 step: 5021, epoch: 152, batch: 4, loss: 0.02748669870197773, acc: 92.1875, f1: 77.96204895001479, r: 0.9661657252854097
06/02/2019 01:27:38 step: 5026, epoch: 152, batch: 9, loss: 0.026125896722078323, acc: 89.0625, f1: 85.21180026563538, r: 0.9545372088812257
06/02/2019 01:27:38 step: 5031, epoch: 152, batch: 14, loss: 0.024528587237000465, acc: 95.3125, f1: 90.34335839598998, r: 0.9438079629472821
06/02/2019 01:27:39 step: 5036, epoch: 152, batch: 19, loss: 0.025465738028287888, acc: 90.625, f1: 75.35512106940678, r: 0.957088614745391
06/02/2019 01:27:39 step: 5041, epoch: 152, batch: 24, loss: 0.02926154062151909, acc: 87.5, f1: 73.53540100250626, r: 0.9681343988552126
06/02/2019 01:27:40 step: 5046, epoch: 152, batch: 29, loss: 0.025879710912704468, acc: 92.1875, f1: 89.08232118758434, r: 0.9712457997544334
06/02/2019 01:27:40 *** evaluating ***
06/02/2019 01:27:40 step: 153, epoch: 152, acc: 61.965811965811966, f1: 28.74589222341719, r: 0.41033509017546727
06/02/2019 01:27:40 *** epoch: 154 ***
06/02/2019 01:27:40 *** training ***
06/02/2019 01:27:41 step: 5054, epoch: 153, batch: 4, loss: 0.025720279663801193, acc: 84.375, f1: 69.7107107971988, r: 0.942795647190974
06/02/2019 01:27:41 step: 5059, epoch: 153, batch: 9, loss: 0.024410855025053024, acc: 87.5, f1: 90.80247275284191, r: 0.9641911294272562
06/02/2019 01:27:42 step: 5064, epoch: 153, batch: 14, loss: 0.022399047389626503, acc: 90.625, f1: 91.6759403036577, r: 0.9849334848784593
06/02/2019 01:27:42 step: 5069, epoch: 153, batch: 19, loss: 0.02351955883204937, acc: 92.1875, f1: 83.0715811965812, r: 0.9747727074215419
06/02/2019 01:27:43 step: 5074, epoch: 153, batch: 24, loss: 0.02686406672000885, acc: 90.625, f1: 67.33532639452768, r: 0.9647415025310725
06/02/2019 01:27:43 step: 5079, epoch: 153, batch: 29, loss: 0.028220105916261673, acc: 79.6875, f1: 74.20807453416148, r: 0.9546575368395451
06/02/2019 01:27:44 *** evaluating ***
06/02/2019 01:27:44 step: 154, epoch: 153, acc: 60.256410256410255, f1: 26.25700674744631, r: 0.41161026654992766
06/02/2019 01:27:44 *** epoch: 155 ***
06/02/2019 01:27:44 *** training ***
06/02/2019 01:27:44 step: 5087, epoch: 154, batch: 4, loss: 0.031807973980903625, acc: 84.375, f1: 73.04855944607496, r: 0.9331216000173688
06/02/2019 01:27:45 step: 5092, epoch: 154, batch: 9, loss: 0.025540055707097054, acc: 92.1875, f1: 90.74646074646074, r: 0.953374816574238
06/02/2019 01:27:45 step: 5097, epoch: 154, batch: 14, loss: 0.027713162824511528, acc: 87.5, f1: 84.99400871459694, r: 0.9522405432261271
06/02/2019 01:27:46 step: 5102, epoch: 154, batch: 19, loss: 0.026544759050011635, acc: 92.1875, f1: 90.70111175374332, r: 0.9634621586195515
06/02/2019 01:27:46 step: 5107, epoch: 154, batch: 24, loss: 0.026709774509072304, acc: 90.625, f1: 84.93982208267923, r: 0.965323645728412
06/02/2019 01:27:47 step: 5112, epoch: 154, batch: 29, loss: 0.026149995625019073, acc: 92.1875, f1: 78.3475303164744, r: 0.9501061075287673
06/02/2019 01:27:47 *** evaluating ***
06/02/2019 01:27:47 step: 155, epoch: 154, acc: 58.119658119658126, f1: 22.731863214288087, r: 0.41251019378649184
06/02/2019 01:27:47 *** epoch: 156 ***
06/02/2019 01:27:47 *** training ***
06/02/2019 01:27:47 step: 5120, epoch: 155, batch: 4, loss: 0.02303682267665863, acc: 96.875, f1: 91.73400673400674, r: 0.9421297787705043
06/02/2019 01:27:48 step: 5125, epoch: 155, batch: 9, loss: 0.027601582929491997, acc: 82.8125, f1: 73.45378304076678, r: 0.9352919317748536
06/02/2019 01:27:48 step: 5130, epoch: 155, batch: 14, loss: 0.023204762488603592, acc: 95.3125, f1: 77.12585034013605, r: 0.9628726475863597
06/02/2019 01:27:49 step: 5135, epoch: 155, batch: 19, loss: 0.022350015118718147, acc: 87.5, f1: 87.10212352783937, r: 0.9831950459289673
06/02/2019 01:27:49 step: 5140, epoch: 155, batch: 24, loss: 0.02592754364013672, acc: 92.1875, f1: 88.80594343000358, r: 0.9570546862824134
06/02/2019 01:27:50 step: 5145, epoch: 155, batch: 29, loss: 0.026076652109622955, acc: 84.375, f1: 78.53383458646618, r: 0.957989260253685
06/02/2019 01:27:50 *** evaluating ***
06/02/2019 01:27:50 step: 156, epoch: 155, acc: 60.256410256410255, f1: 25.672032645716858, r: 0.41240562416495813
06/02/2019 01:27:50 *** epoch: 157 ***
06/02/2019 01:27:50 *** training ***
06/02/2019 01:27:51 step: 5153, epoch: 156, batch: 4, loss: 0.023924188688397408, acc: 82.8125, f1: 65.41181497703236, r: 0.9667035440869384
06/02/2019 01:27:51 step: 5158, epoch: 156, batch: 9, loss: 0.023453010246157646, acc: 92.1875, f1: 84.19371443545973, r: 0.9662797144624544
06/02/2019 01:27:52 step: 5163, epoch: 156, batch: 14, loss: 0.02368101477622986, acc: 93.75, f1: 88.89789485534166, r: 0.9820031077206688
06/02/2019 01:27:52 step: 5168, epoch: 156, batch: 19, loss: 0.02583581954240799, acc: 81.25, f1: 72.53096041889145, r: 0.971493436948069
06/02/2019 01:27:53 step: 5173, epoch: 156, batch: 24, loss: 0.02705490216612816, acc: 93.75, f1: 85.69307885097359, r: 0.9637130617205463
06/02/2019 01:27:53 step: 5178, epoch: 156, batch: 29, loss: 0.022633645683526993, acc: 87.5, f1: 87.86027078541613, r: 0.9648779788152191
06/02/2019 01:27:53 *** evaluating ***
06/02/2019 01:27:54 step: 157, epoch: 156, acc: 59.401709401709404, f1: 24.707859689386787, r: 0.410148894403081
06/02/2019 01:27:54 *** epoch: 158 ***
06/02/2019 01:27:54 *** training ***
06/02/2019 01:27:54 step: 5186, epoch: 157, batch: 4, loss: 0.025726402178406715, acc: 93.75, f1: 81.97916666666667, r: 0.9634373481731643
06/02/2019 01:27:55 step: 5191, epoch: 157, batch: 9, loss: 0.02603439800441265, acc: 93.75, f1: 92.76523744706594, r: 0.9577355731385938
06/02/2019 01:27:55 step: 5196, epoch: 157, batch: 14, loss: 0.030195312574505806, acc: 84.375, f1: 76.30713713792983, r: 0.9421608780968986
06/02/2019 01:27:56 step: 5201, epoch: 157, batch: 19, loss: 0.024223100394010544, acc: 89.0625, f1: 85.09593265618408, r: 0.9710834238572414
06/02/2019 01:27:56 step: 5206, epoch: 157, batch: 24, loss: 0.028394512832164764, acc: 82.8125, f1: 77.01745718050066, r: 0.9558105018394847
06/02/2019 01:27:57 step: 5211, epoch: 157, batch: 29, loss: 0.026026302948594093, acc: 92.1875, f1: 94.82924482924483, r: 0.9592060457939091
06/02/2019 01:27:57 *** evaluating ***
06/02/2019 01:27:57 step: 158, epoch: 157, acc: 60.256410256410255, f1: 25.98490513463505, r: 0.4073799316971263
06/02/2019 01:27:57 *** epoch: 159 ***
06/02/2019 01:27:57 *** training ***
06/02/2019 01:27:58 step: 5219, epoch: 158, batch: 4, loss: 0.02633039467036724, acc: 92.1875, f1: 72.87414965986396, r: 0.9471656499198942
06/02/2019 01:27:58 step: 5224, epoch: 158, batch: 9, loss: 0.02693396806716919, acc: 95.3125, f1: 84.63659836748293, r: 0.964766265986522
06/02/2019 01:27:59 step: 5229, epoch: 158, batch: 14, loss: 0.02669190615415573, acc: 85.9375, f1: 79.33505639097744, r: 0.9511394284911231
06/02/2019 01:27:59 step: 5234, epoch: 158, batch: 19, loss: 0.02371065691113472, acc: 90.625, f1: 83.50461133069828, r: 0.9680632873356421
06/02/2019 01:28:00 step: 5239, epoch: 158, batch: 24, loss: 0.02590680494904518, acc: 90.625, f1: 83.7002812002812, r: 0.9562218971224448
06/02/2019 01:28:00 step: 5244, epoch: 158, batch: 29, loss: 0.024911247193813324, acc: 92.1875, f1: 91.23573014111305, r: 0.9705447067067592
06/02/2019 01:28:00 *** evaluating ***
06/02/2019 01:28:00 step: 159, epoch: 158, acc: 59.82905982905983, f1: 27.28005115089514, r: 0.40499728362950704
06/02/2019 01:28:00 *** epoch: 160 ***
06/02/2019 01:28:00 *** training ***
06/02/2019 01:28:01 step: 5252, epoch: 159, batch: 4, loss: 0.02256530150771141, acc: 90.625, f1: 78.37656739811911, r: 0.9656381594569061
06/02/2019 01:28:01 step: 5257, epoch: 159, batch: 9, loss: 0.024068057537078857, acc: 92.1875, f1: 93.68582806082806, r: 0.9766499682187766
06/02/2019 01:28:02 step: 5262, epoch: 159, batch: 14, loss: 0.025255724787712097, acc: 96.875, f1: 95.15358648607005, r: 0.956992089131124
06/02/2019 01:28:02 step: 5267, epoch: 159, batch: 19, loss: 0.02502000890672207, acc: 90.625, f1: 89.55266955266956, r: 0.9723813179107115
06/02/2019 01:28:03 step: 5272, epoch: 159, batch: 24, loss: 0.024106400087475777, acc: 90.625, f1: 80.34624106052678, r: 0.9608011907544571
06/02/2019 01:28:03 step: 5277, epoch: 159, batch: 29, loss: 0.024137558415532112, acc: 87.5, f1: 73.4363686995266, r: 0.976041818547171
06/02/2019 01:28:04 *** evaluating ***
06/02/2019 01:28:04 step: 160, epoch: 159, acc: 60.256410256410255, f1: 25.743898958184673, r: 0.40759618797345554
06/02/2019 01:28:04 *** epoch: 161 ***
06/02/2019 01:28:04 *** training ***
06/02/2019 01:28:04 step: 5285, epoch: 160, batch: 4, loss: 0.022046539932489395, acc: 98.4375, f1: 97.26415094339622, r: 0.9741845633865918
06/02/2019 01:28:05 step: 5290, epoch: 160, batch: 9, loss: 0.024653349071741104, acc: 92.1875, f1: 81.23940677966101, r: 0.9627180946779391
06/02/2019 01:28:05 step: 5295, epoch: 160, batch: 14, loss: 0.023963937535881996, acc: 92.1875, f1: 86.57764912720783, r: 0.9511691579358421
06/02/2019 01:28:06 step: 5300, epoch: 160, batch: 19, loss: 0.026187803596258163, acc: 92.1875, f1: 94.47456401915605, r: 0.9674658772358129
06/02/2019 01:28:06 step: 5305, epoch: 160, batch: 24, loss: 0.026202350854873657, acc: 87.5, f1: 86.34890798424631, r: 0.9498968565860848
06/02/2019 01:28:07 step: 5310, epoch: 160, batch: 29, loss: 0.023083092644810677, acc: 82.8125, f1: 69.71815072182719, r: 0.976142944782389
06/02/2019 01:28:07 *** evaluating ***
06/02/2019 01:28:07 step: 161, epoch: 160, acc: 59.82905982905983, f1: 24.7961189516129, r: 0.40822067016041447
06/02/2019 01:28:07 *** epoch: 162 ***
06/02/2019 01:28:07 *** training ***
06/02/2019 01:28:08 step: 5318, epoch: 161, batch: 4, loss: 0.0258565004914999, acc: 90.625, f1: 85.01423395445133, r: 0.9635775840246745
06/02/2019 01:28:08 step: 5323, epoch: 161, batch: 9, loss: 0.0281387846916914, acc: 87.5, f1: 82.30367585630745, r: 0.9290347023218627
06/02/2019 01:28:09 step: 5328, epoch: 161, batch: 14, loss: 0.025147274136543274, acc: 85.9375, f1: 80.69778669043374, r: 0.9710778284987319
06/02/2019 01:28:09 step: 5333, epoch: 161, batch: 19, loss: 0.02333507314324379, acc: 90.625, f1: 88.72449877680346, r: 0.9688064003869359
06/02/2019 01:28:10 step: 5338, epoch: 161, batch: 24, loss: 0.027443572878837585, acc: 87.5, f1: 75.37909081342733, r: 0.9604605725878538
06/02/2019 01:28:10 step: 5343, epoch: 161, batch: 29, loss: 0.025916127488017082, acc: 92.1875, f1: 90.12808149741612, r: 0.9471757953155402
06/02/2019 01:28:11 *** evaluating ***
06/02/2019 01:28:11 step: 162, epoch: 161, acc: 59.401709401709404, f1: 24.678533351875032, r: 0.41309318909254295
06/02/2019 01:28:11 *** epoch: 163 ***
06/02/2019 01:28:11 *** training ***
06/02/2019 01:28:11 step: 5351, epoch: 162, batch: 4, loss: 0.02426563948392868, acc: 90.625, f1: 87.84220861802015, r: 0.9627417909201256
06/02/2019 01:28:12 step: 5356, epoch: 162, batch: 9, loss: 0.0250899288803339, acc: 85.9375, f1: 72.94745484400657, r: 0.9599101085262395
06/02/2019 01:28:12 step: 5361, epoch: 162, batch: 14, loss: 0.026499908417463303, acc: 87.5, f1: 73.46428571428572, r: 0.9549969880887467
06/02/2019 01:28:13 step: 5366, epoch: 162, batch: 19, loss: 0.023986956104636192, acc: 87.5, f1: 78.23469387755102, r: 0.9589689771703087
06/02/2019 01:28:13 step: 5371, epoch: 162, batch: 24, loss: 0.021257467567920685, acc: 92.1875, f1: 86.13636363636363, r: 0.9747917526144462
06/02/2019 01:28:14 step: 5376, epoch: 162, batch: 29, loss: 0.024372074753046036, acc: 92.1875, f1: 88.79794245926969, r: 0.9668752880992466
06/02/2019 01:28:14 *** evaluating ***
06/02/2019 01:28:14 step: 163, epoch: 162, acc: 60.68376068376068, f1: 27.686291411255098, r: 0.41215804600155126
06/02/2019 01:28:14 *** epoch: 164 ***
06/02/2019 01:28:14 *** training ***
06/02/2019 01:28:15 step: 5384, epoch: 163, batch: 4, loss: 0.025296147912740707, acc: 87.5, f1: 85.52139037433155, r: 0.9677884746001719
06/02/2019 01:28:15 step: 5389, epoch: 163, batch: 9, loss: 0.023546487092971802, acc: 89.0625, f1: 85.5264422153042, r: 0.9693558660926285
06/02/2019 01:28:16 step: 5394, epoch: 163, batch: 14, loss: 0.024804890155792236, acc: 90.625, f1: 89.9702380952381, r: 0.9713812875916539
06/02/2019 01:28:16 step: 5399, epoch: 163, batch: 19, loss: 0.026172636076807976, acc: 93.75, f1: 73.7444260000651, r: 0.933050834256356
06/02/2019 01:28:17 step: 5404, epoch: 163, batch: 24, loss: 0.02646561525762081, acc: 87.5, f1: 68.75, r: 0.9678504400877871
06/02/2019 01:28:17 step: 5409, epoch: 163, batch: 29, loss: 0.024221230298280716, acc: 90.625, f1: 79.32298786739575, r: 0.9722908722913198
06/02/2019 01:28:17 *** evaluating ***
06/02/2019 01:28:17 step: 164, epoch: 163, acc: 61.111111111111114, f1: 27.678941574610867, r: 0.4060941946942999
06/02/2019 01:28:17 *** epoch: 165 ***
06/02/2019 01:28:17 *** training ***
06/02/2019 01:28:18 step: 5417, epoch: 164, batch: 4, loss: 0.029251620173454285, acc: 93.75, f1: 76.44920183982684, r: 0.9327148362287668
06/02/2019 01:28:18 step: 5422, epoch: 164, batch: 9, loss: 0.0260771531611681, acc: 89.0625, f1: 86.00172022832122, r: 0.9637580183241479
06/02/2019 01:28:19 step: 5427, epoch: 164, batch: 14, loss: 0.024033039808273315, acc: 93.75, f1: 92.1517611218608, r: 0.9319640646198022
06/02/2019 01:28:19 step: 5432, epoch: 164, batch: 19, loss: 0.021038614213466644, acc: 95.3125, f1: 82.80323118258292, r: 0.971896903521951
06/02/2019 01:28:20 step: 5437, epoch: 164, batch: 24, loss: 0.024883726611733437, acc: 93.75, f1: 82.50257184575563, r: 0.9709210787854751
06/02/2019 01:28:20 step: 5442, epoch: 164, batch: 29, loss: 0.022412724792957306, acc: 93.75, f1: 92.83946488294315, r: 0.9635898745139992
06/02/2019 01:28:21 *** evaluating ***
06/02/2019 01:28:21 step: 165, epoch: 164, acc: 60.68376068376068, f1: 26.954776172868737, r: 0.4118353387206416
06/02/2019 01:28:21 *** epoch: 166 ***
06/02/2019 01:28:21 *** training ***
06/02/2019 01:28:21 step: 5450, epoch: 165, batch: 4, loss: 0.025691796094179153, acc: 95.3125, f1: 93.93887945670627, r: 0.94454046044061
06/02/2019 01:28:22 step: 5455, epoch: 165, batch: 9, loss: 0.02723468467593193, acc: 87.5, f1: 80.12766829219727, r: 0.9567796368562476
06/02/2019 01:28:22 step: 5460, epoch: 165, batch: 14, loss: 0.022382885217666626, acc: 93.75, f1: 71.92758515846971, r: 0.9285781118004734
06/02/2019 01:28:23 step: 5465, epoch: 165, batch: 19, loss: 0.02495461329817772, acc: 93.75, f1: 80.66262285012284, r: 0.9691918690495284
06/02/2019 01:28:23 step: 5470, epoch: 165, batch: 24, loss: 0.02715485170483589, acc: 89.0625, f1: 87.99092970521541, r: 0.9553472977109326
06/02/2019 01:28:24 step: 5475, epoch: 165, batch: 29, loss: 0.022126128897070885, acc: 98.4375, f1: 96.84210526315789, r: 0.9818920034916148
06/02/2019 01:28:24 *** evaluating ***
06/02/2019 01:28:24 step: 166, epoch: 165, acc: 60.256410256410255, f1: 27.235230047730045, r: 0.4181502556247157
06/02/2019 01:28:24 *** epoch: 167 ***
06/02/2019 01:28:24 *** training ***
06/02/2019 01:28:25 step: 5483, epoch: 166, batch: 4, loss: 0.023191209882497787, acc: 92.1875, f1: 85.13603003925584, r: 0.9681141984929541
06/02/2019 01:28:25 step: 5488, epoch: 166, batch: 9, loss: 0.02961377054452896, acc: 95.3125, f1: 91.26587301587301, r: 0.9615029597236808
06/02/2019 01:28:26 step: 5493, epoch: 166, batch: 14, loss: 0.020681457594037056, acc: 93.75, f1: 92.31611550092674, r: 0.9732206425091465
06/02/2019 01:28:26 step: 5498, epoch: 166, batch: 19, loss: 0.022847848013043404, acc: 85.9375, f1: 61.22086247086247, r: 0.965541233367711
06/02/2019 01:28:27 step: 5503, epoch: 166, batch: 24, loss: 0.023498166352510452, acc: 87.5, f1: 80.11750154607297, r: 0.9510162517714387
06/02/2019 01:28:27 step: 5508, epoch: 166, batch: 29, loss: 0.022746019065380096, acc: 89.0625, f1: 78.5110628951034, r: 0.9742720233564504
06/02/2019 01:28:27 *** evaluating ***
06/02/2019 01:28:28 step: 167, epoch: 166, acc: 61.111111111111114, f1: 27.49032799032799, r: 0.4132242349540538
06/02/2019 01:28:28 *** epoch: 168 ***
06/02/2019 01:28:28 *** training ***
06/02/2019 01:28:28 step: 5516, epoch: 167, batch: 4, loss: 0.02332734875380993, acc: 93.75, f1: 79.39099449303532, r: 0.963504961138377
06/02/2019 01:28:29 step: 5521, epoch: 167, batch: 9, loss: 0.02621838077902794, acc: 93.75, f1: 79.2989417989418, r: 0.9617442629064887
06/02/2019 01:28:29 step: 5526, epoch: 167, batch: 14, loss: 0.024410545825958252, acc: 85.9375, f1: 72.70643867626626, r: 0.9695485219460931
06/02/2019 01:28:30 step: 5531, epoch: 167, batch: 19, loss: 0.02317831665277481, acc: 95.3125, f1: 85.041928721174, r: 0.9691140871996888
06/02/2019 01:28:30 step: 5536, epoch: 167, batch: 24, loss: 0.026250366121530533, acc: 85.9375, f1: 73.26105214470033, r: 0.9549487632341962
06/02/2019 01:28:31 step: 5541, epoch: 167, batch: 29, loss: 0.029048550873994827, acc: 95.3125, f1: 84.75274725274726, r: 0.9509437304765622
06/02/2019 01:28:31 *** evaluating ***
06/02/2019 01:28:31 step: 168, epoch: 167, acc: 60.256410256410255, f1: 27.045177045177045, r: 0.41385438018591747
06/02/2019 01:28:31 *** epoch: 169 ***
06/02/2019 01:28:31 *** training ***
06/02/2019 01:28:31 step: 5549, epoch: 168, batch: 4, loss: 0.02602066844701767, acc: 92.1875, f1: 92.50450937950939, r: 0.9715632887056564
06/02/2019 01:28:32 step: 5554, epoch: 168, batch: 9, loss: 0.02437490038573742, acc: 87.5, f1: 85.75663466967815, r: 0.9706287394893447
06/02/2019 01:28:32 step: 5559, epoch: 168, batch: 14, loss: 0.026857491582632065, acc: 89.0625, f1: 64.00141888150608, r: 0.9499827581430943
06/02/2019 01:28:33 step: 5564, epoch: 168, batch: 19, loss: 0.0240215752273798, acc: 92.1875, f1: 77.63873734618414, r: 0.9760751892780626
06/02/2019 01:28:33 step: 5569, epoch: 168, batch: 24, loss: 0.024647457525134087, acc: 84.375, f1: 75.44911539139443, r: 0.9542453239357321
06/02/2019 01:28:34 step: 5574, epoch: 168, batch: 29, loss: 0.025917774066329002, acc: 89.0625, f1: 82.39316239316238, r: 0.9669171403373689
06/02/2019 01:28:34 *** evaluating ***
06/02/2019 01:28:34 step: 169, epoch: 168, acc: 61.965811965811966, f1: 28.27413123369006, r: 0.414970445064779
06/02/2019 01:28:34 *** epoch: 170 ***
06/02/2019 01:28:34 *** training ***
06/02/2019 01:28:35 step: 5582, epoch: 169, batch: 4, loss: 0.024419739842414856, acc: 89.0625, f1: 74.30603022708286, r: 0.9765974123795376
06/02/2019 01:28:35 step: 5587, epoch: 169, batch: 9, loss: 0.02830810472369194, acc: 82.8125, f1: 75.20909645909646, r: 0.9663248834957555
06/02/2019 01:28:36 step: 5592, epoch: 169, batch: 14, loss: 0.024183442816138268, acc: 90.625, f1: 77.43090679754724, r: 0.9490847879978527
06/02/2019 01:28:36 step: 5597, epoch: 169, batch: 19, loss: 0.02138473466038704, acc: 92.1875, f1: 90.83760683760683, r: 0.973755731004895
06/02/2019 01:28:37 step: 5602, epoch: 169, batch: 24, loss: 0.028317635878920555, acc: 89.0625, f1: 89.65962539311458, r: 0.94551514782465
06/02/2019 01:28:37 step: 5607, epoch: 169, batch: 29, loss: 0.028642825782299042, acc: 87.5, f1: 87.77489177489177, r: 0.9511090499006342
06/02/2019 01:28:37 *** evaluating ***
06/02/2019 01:28:37 step: 170, epoch: 169, acc: 61.965811965811966, f1: 28.218465096089673, r: 0.4091594035085674
06/02/2019 01:28:37 *** epoch: 171 ***
06/02/2019 01:28:37 *** training ***
06/02/2019 01:28:38 step: 5615, epoch: 170, batch: 4, loss: 0.026712283492088318, acc: 87.5, f1: 83.37259903961585, r: 0.968882557023566
06/02/2019 01:28:38 step: 5620, epoch: 170, batch: 9, loss: 0.02339194528758526, acc: 93.75, f1: 80.46823089700997, r: 0.9798240040929609
06/02/2019 01:28:39 step: 5625, epoch: 170, batch: 14, loss: 0.02780085988342762, acc: 89.0625, f1: 68.46560846560847, r: 0.9607799312420731
06/02/2019 01:28:39 step: 5630, epoch: 170, batch: 19, loss: 0.021903986111283302, acc: 85.9375, f1: 76.71535503833641, r: 0.9648008555104485
06/02/2019 01:28:40 step: 5635, epoch: 170, batch: 24, loss: 0.024528123438358307, acc: 90.625, f1: 73.70336899942163, r: 0.9363822817290239
06/02/2019 01:28:40 step: 5640, epoch: 170, batch: 29, loss: 0.02589445188641548, acc: 92.1875, f1: 87.06593748610555, r: 0.9701125269251577
06/02/2019 01:28:41 *** evaluating ***
06/02/2019 01:28:41 step: 171, epoch: 170, acc: 61.111111111111114, f1: 28.055982539404255, r: 0.41488443728104335
06/02/2019 01:28:41 *** epoch: 172 ***
06/02/2019 01:28:41 *** training ***
06/02/2019 01:28:41 step: 5648, epoch: 171, batch: 4, loss: 0.026271158829331398, acc: 82.8125, f1: 67.05043859649122, r: 0.9551325499392816
06/02/2019 01:28:42 step: 5653, epoch: 171, batch: 9, loss: 0.021811578422784805, acc: 89.0625, f1: 85.57004018316046, r: 0.9825709517887047
06/02/2019 01:28:42 step: 5658, epoch: 171, batch: 14, loss: 0.02456805109977722, acc: 89.0625, f1: 90.12570419335788, r: 0.9618977322755373
06/02/2019 01:28:43 step: 5663, epoch: 171, batch: 19, loss: 0.02363661304116249, acc: 93.75, f1: 88.43949496382466, r: 0.9420015456850165
06/02/2019 01:28:43 step: 5668, epoch: 171, batch: 24, loss: 0.025062480941414833, acc: 89.0625, f1: 84.04473886328725, r: 0.9674220704784368
06/02/2019 01:28:44 step: 5673, epoch: 171, batch: 29, loss: 0.025485213845968246, acc: 84.375, f1: 74.76766892527762, r: 0.9763158327887882
06/02/2019 01:28:44 *** evaluating ***
06/02/2019 01:28:44 step: 172, epoch: 171, acc: 61.53846153846154, f1: 28.66096737364401, r: 0.4115855094454706
06/02/2019 01:28:44 *** epoch: 173 ***
06/02/2019 01:28:44 *** training ***
06/02/2019 01:28:45 step: 5681, epoch: 172, batch: 4, loss: 0.023494455963373184, acc: 95.3125, f1: 94.71825396825398, r: 0.9735516961131373
06/02/2019 01:28:45 step: 5686, epoch: 172, batch: 9, loss: 0.02841768227517605, acc: 87.5, f1: 83.35497835497836, r: 0.9480809339502687
06/02/2019 01:28:45 step: 5691, epoch: 172, batch: 14, loss: 0.02529355138540268, acc: 96.875, f1: 81.53443195460002, r: 0.9499399228368034
06/02/2019 01:28:46 step: 5696, epoch: 172, batch: 19, loss: 0.02334962785243988, acc: 89.0625, f1: 78.26519916142558, r: 0.967380317445957
06/02/2019 01:28:47 step: 5701, epoch: 172, batch: 24, loss: 0.023575447499752045, acc: 82.8125, f1: 68.71088603425561, r: 0.9740262793515088
06/02/2019 01:28:47 step: 5706, epoch: 172, batch: 29, loss: 0.02686295658349991, acc: 89.0625, f1: 83.50490196078432, r: 0.8945984192366823
06/02/2019 01:28:47 *** evaluating ***
06/02/2019 01:28:47 step: 173, epoch: 172, acc: 61.111111111111114, f1: 28.14185200670255, r: 0.410814250214771
06/02/2019 01:28:47 *** epoch: 174 ***
06/02/2019 01:28:47 *** training ***
06/02/2019 01:28:48 step: 5714, epoch: 173, batch: 4, loss: 0.026761863380670547, acc: 89.0625, f1: 74.82444638694639, r: 0.9556039396213304
06/02/2019 01:28:48 step: 5719, epoch: 173, batch: 9, loss: 0.02552376314997673, acc: 93.75, f1: 94.66036414565826, r: 0.9611816260987139
06/02/2019 01:28:49 step: 5724, epoch: 173, batch: 14, loss: 0.027397606521844864, acc: 79.6875, f1: 69.61857769423558, r: 0.9644499209230967
06/02/2019 01:28:49 step: 5729, epoch: 173, batch: 19, loss: 0.027163546532392502, acc: 87.5, f1: 82.39286402820237, r: 0.9531122305634769
06/02/2019 01:35:06 step: 5734, epoch: 173, batch: 24, loss: 0.02209911122918129, acc: 90.625, f1: 89.45102124262544, r: 0.9595467399521572
06/02/2019 01:35:07 step: 5739, epoch: 173, batch: 29, loss: 0.022396540269255638, acc: 90.625, f1: 83.49712192569336, r: 0.968508121703688
06/02/2019 01:35:07 *** evaluating ***
06/02/2019 01:35:07 step: 174, epoch: 173, acc: 60.68376068376068, f1: 28.478398853398858, r: 0.4113179383837748
06/02/2019 01:35:07 *** epoch: 175 ***
06/02/2019 01:35:07 *** training ***
06/02/2019 01:35:08 step: 5747, epoch: 174, batch: 4, loss: 0.021567195653915405, acc: 90.625, f1: 64.10756995239754, r: 0.9714858283463499
06/02/2019 01:35:08 step: 5752, epoch: 174, batch: 9, loss: 0.02438575029373169, acc: 90.625, f1: 84.921657944046, r: 0.9718741279909894
06/02/2019 01:35:09 step: 5757, epoch: 174, batch: 14, loss: 0.024593772366642952, acc: 93.75, f1: 76.1288169342339, r: 0.9563538924415571
06/02/2019 01:35:09 step: 5762, epoch: 174, batch: 19, loss: 0.02684587985277176, acc: 93.75, f1: 96.79871443016033, r: 0.9741516420164589
06/02/2019 01:35:10 step: 5767, epoch: 174, batch: 24, loss: 0.020194033160805702, acc: 93.75, f1: 88.7816944959802, r: 0.9747532006354916
06/02/2019 01:35:10 step: 5772, epoch: 174, batch: 29, loss: 0.0211349930614233, acc: 92.1875, f1: 91.66010263754625, r: 0.9562935501175746
06/02/2019 01:35:10 *** evaluating ***
06/02/2019 01:35:11 step: 175, epoch: 174, acc: 61.111111111111114, f1: 28.85704026211545, r: 0.4217836218135906
06/02/2019 01:35:11 *** epoch: 176 ***
06/02/2019 01:35:11 *** training ***
06/02/2019 01:35:11 step: 5780, epoch: 175, batch: 4, loss: 0.021925397217273712, acc: 90.625, f1: 85.22366522366522, r: 0.9809014406253783
06/02/2019 01:35:12 step: 5785, epoch: 175, batch: 9, loss: 0.02741590142250061, acc: 81.25, f1: 71.5914934871176, r: 0.918092087597951
06/02/2019 01:35:12 step: 5790, epoch: 175, batch: 14, loss: 0.026365892961621284, acc: 92.1875, f1: 92.51700680272108, r: 0.9673310702590058
06/02/2019 01:35:12 step: 5795, epoch: 175, batch: 19, loss: 0.022758496925234795, acc: 96.875, f1: 93.54359925788496, r: 0.9756711396816412
06/02/2019 01:35:13 step: 5800, epoch: 175, batch: 24, loss: 0.025463495403528214, acc: 87.5, f1: 80.69941019886214, r: 0.9470452139274474
06/02/2019 01:35:13 step: 5805, epoch: 175, batch: 29, loss: 0.024900875985622406, acc: 82.8125, f1: 76.1525974025974, r: 0.961387917406038
06/02/2019 01:35:14 *** evaluating ***
06/02/2019 01:35:14 step: 176, epoch: 175, acc: 61.53846153846154, f1: 28.994087164762465, r: 0.41150323681801826
06/02/2019 01:35:14 *** epoch: 177 ***
06/02/2019 01:35:14 *** training ***
06/02/2019 01:35:14 step: 5813, epoch: 176, batch: 4, loss: 0.02692662551999092, acc: 87.5, f1: 82.6763336913713, r: 0.9442242898466845
06/02/2019 01:35:15 step: 5818, epoch: 176, batch: 9, loss: 0.023026403039693832, acc: 89.0625, f1: 82.8125, r: 0.9829165177827554
06/02/2019 01:35:15 step: 5823, epoch: 176, batch: 14, loss: 0.027187548577785492, acc: 84.375, f1: 82.65949736537974, r: 0.9487671615862103
06/02/2019 01:35:16 step: 5828, epoch: 176, batch: 19, loss: 0.024353642016649246, acc: 87.5, f1: 86.95877766311203, r: 0.9621073140419949
06/02/2019 01:35:16 step: 5833, epoch: 176, batch: 24, loss: 0.024385463446378708, acc: 82.8125, f1: 78.62543575587054, r: 0.9654200305684995
06/02/2019 01:35:17 step: 5838, epoch: 176, batch: 29, loss: 0.02314206212759018, acc: 90.625, f1: 87.96146044624746, r: 0.967546410188424
06/02/2019 01:35:17 *** evaluating ***
06/02/2019 01:35:17 step: 177, epoch: 176, acc: 59.82905982905983, f1: 25.68968066186722, r: 0.4099583301249965
06/02/2019 01:35:17 *** epoch: 178 ***
06/02/2019 01:35:17 *** training ***
06/02/2019 01:35:17 step: 5846, epoch: 177, batch: 4, loss: 0.021774357184767723, acc: 89.0625, f1: 75.64565179472011, r: 0.9753467605844341
06/02/2019 01:35:18 step: 5851, epoch: 177, batch: 9, loss: 0.025342898443341255, acc: 93.75, f1: 75.49973835688122, r: 0.9396347785660961
06/02/2019 01:35:18 step: 5856, epoch: 177, batch: 14, loss: 0.026220709085464478, acc: 92.1875, f1: 91.25, r: 0.969313138905572
06/02/2019 01:35:19 step: 5861, epoch: 177, batch: 19, loss: 0.024675574153661728, acc: 90.625, f1: 76.3334704511175, r: 0.953214215233575
06/02/2019 01:35:19 step: 5866, epoch: 177, batch: 24, loss: 0.021130884066224098, acc: 87.5, f1: 82.17199467199467, r: 0.9746659052135234
06/02/2019 01:35:20 step: 5871, epoch: 177, batch: 29, loss: 0.02386505715548992, acc: 92.1875, f1: 85.38070666863237, r: 0.9758482272954878
06/02/2019 01:35:20 *** evaluating ***
06/02/2019 01:35:20 step: 178, epoch: 177, acc: 59.82905982905983, f1: 27.22147092548725, r: 0.4164142280442089
06/02/2019 01:35:20 *** epoch: 179 ***
06/02/2019 01:35:20 *** training ***
06/02/2019 01:35:20 step: 5879, epoch: 178, batch: 4, loss: 0.02430230937898159, acc: 81.25, f1: 62.708333333333336, r: 0.9589501104229528
06/02/2019 01:35:21 step: 5884, epoch: 178, batch: 9, loss: 0.024447638541460037, acc: 95.3125, f1: 94.64734733351754, r: 0.9694806484665376
06/02/2019 01:35:21 step: 5889, epoch: 178, batch: 14, loss: 0.023836655542254448, acc: 87.5, f1: 81.09815354713314, r: 0.9543158889903683
06/02/2019 01:35:22 step: 5894, epoch: 178, batch: 19, loss: 0.023448660969734192, acc: 89.0625, f1: 84.69185455178663, r: 0.9781696636445631
06/02/2019 01:35:22 step: 5899, epoch: 178, batch: 24, loss: 0.0244758240878582, acc: 93.75, f1: 82.00096899224806, r: 0.9683401123889059
06/02/2019 01:35:23 step: 5904, epoch: 178, batch: 29, loss: 0.027146100997924805, acc: 92.1875, f1: 87.2704081632653, r: 0.9580784476441235
06/02/2019 01:35:23 *** evaluating ***
06/02/2019 01:35:23 step: 179, epoch: 178, acc: 61.111111111111114, f1: 26.53246488313808, r: 0.4160013147995486
06/02/2019 01:35:23 *** epoch: 180 ***
06/02/2019 01:35:23 *** training ***
06/02/2019 01:35:24 step: 5912, epoch: 179, batch: 4, loss: 0.025650665163993835, acc: 87.5, f1: 72.32426303854875, r: 0.9590439633740184
06/02/2019 01:35:24 step: 5917, epoch: 179, batch: 9, loss: 0.026073047891259193, acc: 89.0625, f1: 75.61208010335918, r: 0.9612892073796364
06/02/2019 01:35:25 step: 5922, epoch: 179, batch: 14, loss: 0.027404669672250748, acc: 92.1875, f1: 65.90277777777777, r: 0.9208678801275789
06/02/2019 01:35:25 step: 5927, epoch: 179, batch: 19, loss: 0.025194711983203888, acc: 89.0625, f1: 68.3556098744886, r: 0.956940756323756
06/02/2019 01:35:26 step: 5932, epoch: 179, batch: 24, loss: 0.021535946056246758, acc: 89.0625, f1: 81.51934546023216, r: 0.9734886206798625
06/02/2019 01:35:26 step: 5937, epoch: 179, batch: 29, loss: 0.02345692366361618, acc: 85.9375, f1: 80.20545591974162, r: 0.97443935762056
06/02/2019 01:35:26 *** evaluating ***
06/02/2019 01:35:26 step: 180, epoch: 179, acc: 61.965811965811966, f1: 28.799939327645014, r: 0.4136313875453619
06/02/2019 01:35:26 *** epoch: 181 ***
06/02/2019 01:35:26 *** training ***
06/02/2019 01:35:27 step: 5945, epoch: 180, batch: 4, loss: 0.023234901949763298, acc: 95.3125, f1: 78.95833333333333, r: 0.9564624683462948
06/02/2019 01:35:27 step: 5950, epoch: 180, batch: 9, loss: 0.024892190471291542, acc: 90.625, f1: 93.2855582855583, r: 0.9705504831361023
06/02/2019 01:35:28 step: 5955, epoch: 180, batch: 14, loss: 0.024905327707529068, acc: 89.0625, f1: 84.49350539811067, r: 0.9636182141209793
06/02/2019 01:35:28 step: 5960, epoch: 180, batch: 19, loss: 0.0227032657712698, acc: 92.1875, f1: 76.80157222333503, r: 0.9684615928736289
06/02/2019 01:35:29 step: 5965, epoch: 180, batch: 24, loss: 0.02298557758331299, acc: 87.5, f1: 81.81563534504711, r: 0.9722791826266024
06/02/2019 01:35:29 step: 5970, epoch: 180, batch: 29, loss: 0.023946553468704224, acc: 85.9375, f1: 78.08862675504984, r: 0.9683868456409833
06/02/2019 01:35:30 *** evaluating ***
06/02/2019 01:35:30 step: 181, epoch: 180, acc: 61.111111111111114, f1: 29.394474449522225, r: 0.41113641346092067
06/02/2019 01:35:30 *** epoch: 182 ***
06/02/2019 01:35:30 *** training ***
06/02/2019 01:35:30 step: 5978, epoch: 181, batch: 4, loss: 0.023750698193907738, acc: 87.5, f1: 78.17327688651218, r: 0.9627253736712929
06/02/2019 01:35:31 step: 5983, epoch: 181, batch: 9, loss: 0.025112435221672058, acc: 89.0625, f1: 84.58983593437375, r: 0.9533160580580305
06/02/2019 01:35:31 step: 5988, epoch: 181, batch: 14, loss: 0.021797889843583107, acc: 95.3125, f1: 77.58354350567464, r: 0.9596914928838264
06/02/2019 01:35:32 step: 5993, epoch: 181, batch: 19, loss: 0.019726410508155823, acc: 90.625, f1: 80.34632034632034, r: 0.9800959486453116
06/02/2019 01:35:32 step: 5998, epoch: 181, batch: 24, loss: 0.02539774402976036, acc: 84.375, f1: 68.6164320800784, r: 0.9229324821744324
06/02/2019 01:35:32 step: 6003, epoch: 181, batch: 29, loss: 0.02351979911327362, acc: 90.625, f1: 89.78346487336353, r: 0.93434235074606
06/02/2019 01:35:33 *** evaluating ***
06/02/2019 01:35:33 step: 182, epoch: 181, acc: 61.111111111111114, f1: 27.419759520056942, r: 0.4109365995864458
06/02/2019 01:35:33 *** epoch: 183 ***
06/02/2019 01:35:33 *** training ***
06/02/2019 01:35:33 step: 6011, epoch: 182, batch: 4, loss: 0.0258895605802536, acc: 90.625, f1: 84.34891905480141, r: 0.9675580326555718
06/02/2019 01:35:34 step: 6016, epoch: 182, batch: 9, loss: 0.021231824532151222, acc: 85.9375, f1: 81.28183530306173, r: 0.9715510595203688
06/02/2019 01:35:34 step: 6021, epoch: 182, batch: 14, loss: 0.024561692029237747, acc: 85.9375, f1: 77.24158653846153, r: 0.9648120607336864
06/02/2019 01:35:35 step: 6026, epoch: 182, batch: 19, loss: 0.027661584317684174, acc: 92.1875, f1: 84.70899470899471, r: 0.9448503259659153
06/02/2019 01:35:35 step: 6031, epoch: 182, batch: 24, loss: 0.027370044961571693, acc: 85.9375, f1: 77.56519902861368, r: 0.9697112667943655
06/02/2019 01:35:36 step: 6036, epoch: 182, batch: 29, loss: 0.02429875358939171, acc: 82.8125, f1: 68.23934837092732, r: 0.9554062477554787
06/02/2019 01:35:36 *** evaluating ***
06/02/2019 01:35:36 step: 183, epoch: 182, acc: 61.965811965811966, f1: 28.653116643789563, r: 0.41614067562345936
06/02/2019 01:35:36 *** epoch: 184 ***
06/02/2019 01:35:36 *** training ***
06/02/2019 01:35:37 step: 6044, epoch: 183, batch: 4, loss: 0.027135411277413368, acc: 85.9375, f1: 71.82990620490621, r: 0.9641597791899794
06/02/2019 01:35:37 step: 6049, epoch: 183, batch: 9, loss: 0.024317249655723572, acc: 89.0625, f1: 86.52886624869383, r: 0.9666413299649057
06/02/2019 01:35:37 step: 6054, epoch: 183, batch: 14, loss: 0.02345118299126625, acc: 93.75, f1: 95.03841829085458, r: 0.9698572245280949
06/02/2019 01:35:38 step: 6059, epoch: 183, batch: 19, loss: 0.024029618129134178, acc: 89.0625, f1: 79.23469387755102, r: 0.9738053547513197
06/02/2019 01:35:38 step: 6064, epoch: 183, batch: 24, loss: 0.024072865024209023, acc: 89.0625, f1: 71.78264340562095, r: 0.9408523275874855
06/02/2019 01:35:39 step: 6069, epoch: 183, batch: 29, loss: 0.025172213092446327, acc: 92.1875, f1: 89.493006993007, r: 0.9581157048457247
06/02/2019 01:35:39 *** evaluating ***
06/02/2019 01:35:39 step: 184, epoch: 183, acc: 61.965811965811966, f1: 28.69779862426921, r: 0.4102107767613759
06/02/2019 01:35:39 *** epoch: 185 ***
06/02/2019 01:35:39 *** training ***
06/02/2019 01:35:40 step: 6077, epoch: 184, batch: 4, loss: 0.0250164195895195, acc: 90.625, f1: 88.21878247384247, r: 0.9645580546266842
06/02/2019 01:35:40 step: 6082, epoch: 184, batch: 9, loss: 0.023980125784873962, acc: 93.75, f1: 93.95858770858771, r: 0.9784997754969719
06/02/2019 01:35:40 step: 6087, epoch: 184, batch: 14, loss: 0.024644115939736366, acc: 95.3125, f1: 97.57869249394673, r: 0.9792703098103733
06/02/2019 01:35:41 step: 6092, epoch: 184, batch: 19, loss: 0.02340962365269661, acc: 92.1875, f1: 91.77437641723355, r: 0.9680190707649333
06/02/2019 01:35:41 step: 6097, epoch: 184, batch: 24, loss: 0.026783989742398262, acc: 89.0625, f1: 84.66007719702301, r: 0.950736586069636
06/02/2019 01:35:42 step: 6102, epoch: 184, batch: 29, loss: 0.025316715240478516, acc: 90.625, f1: 66.57962713387242, r: 0.9443876553565683
06/02/2019 01:35:42 *** evaluating ***
06/02/2019 01:35:42 step: 185, epoch: 184, acc: 62.39316239316239, f1: 29.338976241150156, r: 0.4163096592450207
06/02/2019 01:35:42 *** epoch: 186 ***
06/02/2019 01:35:42 *** training ***
06/02/2019 01:35:43 step: 6110, epoch: 185, batch: 4, loss: 0.027073465287685394, acc: 84.375, f1: 79.3840261514442, r: 0.957424700823048
06/02/2019 01:35:43 step: 6115, epoch: 185, batch: 9, loss: 0.02638746052980423, acc: 89.0625, f1: 66.15758145363408, r: 0.9510888527010738
06/02/2019 01:35:44 step: 6120, epoch: 185, batch: 14, loss: 0.021580681204795837, acc: 85.9375, f1: 73.26912218216566, r: 0.9741366265507715
06/02/2019 01:35:44 step: 6125, epoch: 185, batch: 19, loss: 0.02483757585287094, acc: 85.9375, f1: 60.77097437137331, r: 0.9669622568930551
06/02/2019 01:35:45 step: 6130, epoch: 185, batch: 24, loss: 0.02365771494805813, acc: 95.3125, f1: 83.1140350877193, r: 0.9698708623240859
06/02/2019 01:35:45 step: 6135, epoch: 185, batch: 29, loss: 0.023128505796194077, acc: 90.625, f1: 75.54374389051807, r: 0.970940191246848
06/02/2019 01:35:45 *** evaluating ***
06/02/2019 01:35:46 step: 186, epoch: 185, acc: 63.67521367521367, f1: 30.716657678995528, r: 0.41575339931530014
06/02/2019 01:35:46 *** epoch: 187 ***
06/02/2019 01:35:46 *** training ***
06/02/2019 01:35:46 step: 6143, epoch: 186, batch: 4, loss: 0.025822177529335022, acc: 89.0625, f1: 85.66123188405797, r: 0.9632564270223649
06/02/2019 01:35:46 step: 6148, epoch: 186, batch: 9, loss: 0.022875552996993065, acc: 90.625, f1: 85.54500891265597, r: 0.9759109753420316
06/02/2019 01:35:47 step: 6153, epoch: 186, batch: 14, loss: 0.02692551724612713, acc: 84.375, f1: 70.69298245614036, r: 0.9564351138558783
06/02/2019 01:35:47 step: 6158, epoch: 186, batch: 19, loss: 0.02127239480614662, acc: 93.75, f1: 87.9625183406696, r: 0.9715666791710218
06/02/2019 01:35:48 step: 6163, epoch: 186, batch: 24, loss: 0.024885324761271477, acc: 90.625, f1: 79.44444444444446, r: 0.9438042864242051
06/02/2019 01:35:48 step: 6168, epoch: 186, batch: 29, loss: 0.026448294520378113, acc: 89.0625, f1: 90.99749572133294, r: 0.9630982541433966
06/02/2019 01:35:48 *** evaluating ***
06/02/2019 01:35:48 step: 187, epoch: 186, acc: 62.39316239316239, f1: 28.174956028923198, r: 0.41438891045520077
06/02/2019 01:35:48 *** epoch: 188 ***
06/02/2019 01:35:48 *** training ***
06/02/2019 01:35:49 step: 6176, epoch: 187, batch: 4, loss: 0.021132534369826317, acc: 93.75, f1: 76.93452380952381, r: 0.9717975458339296
06/02/2019 01:35:49 step: 6181, epoch: 187, batch: 9, loss: 0.02501763589680195, acc: 89.0625, f1: 80.93828633148175, r: 0.9702219118908655
06/02/2019 01:35:50 step: 6186, epoch: 187, batch: 14, loss: 0.0239491518586874, acc: 87.5, f1: 76.66666666666666, r: 0.9682708383368142
06/02/2019 01:35:50 step: 6191, epoch: 187, batch: 19, loss: 0.022455986589193344, acc: 93.75, f1: 87.5994271685761, r: 0.9718519599342765
06/02/2019 01:35:51 step: 6196, epoch: 187, batch: 24, loss: 0.02311892993748188, acc: 92.1875, f1: 87.43506493506493, r: 0.9823809301832809
06/02/2019 01:35:51 step: 6201, epoch: 187, batch: 29, loss: 0.023270856589078903, acc: 89.0625, f1: 74.31599832915622, r: 0.9454951029075314
06/02/2019 01:35:51 *** evaluating ***
06/02/2019 01:35:51 step: 188, epoch: 187, acc: 63.24786324786324, f1: 29.24447285452978, r: 0.4139954523478332
06/02/2019 01:35:51 *** epoch: 189 ***
06/02/2019 01:35:51 *** training ***
06/02/2019 01:35:52 step: 6209, epoch: 188, batch: 4, loss: 0.026127545163035393, acc: 90.625, f1: 73.39863184079603, r: 0.9607220386978773
06/02/2019 01:35:52 step: 6214, epoch: 188, batch: 9, loss: 0.029579676687717438, acc: 85.9375, f1: 80.43987275761368, r: 0.9563938862029179
06/02/2019 01:35:53 step: 6219, epoch: 188, batch: 14, loss: 0.028797758743166924, acc: 93.75, f1: 71.1752217237955, r: 0.9550305397457335
06/02/2019 01:35:53 step: 6224, epoch: 188, batch: 19, loss: 0.02497662790119648, acc: 87.5, f1: 74.1273803879736, r: 0.9563746678386703
06/02/2019 01:35:54 step: 6229, epoch: 188, batch: 24, loss: 0.023093897849321365, acc: 84.375, f1: 79.15802611367127, r: 0.9718424490051454
06/02/2019 01:35:54 step: 6234, epoch: 188, batch: 29, loss: 0.022669345140457153, acc: 89.0625, f1: 76.71445221445221, r: 0.9665082391082295
06/02/2019 01:35:54 *** evaluating ***
06/02/2019 01:35:55 step: 189, epoch: 188, acc: 61.111111111111114, f1: 27.939006943798212, r: 0.41543279822063434
06/02/2019 01:35:55 *** epoch: 190 ***
06/02/2019 01:35:55 *** training ***
06/02/2019 01:35:55 step: 6242, epoch: 189, batch: 4, loss: 0.024889536201953888, acc: 85.9375, f1: 79.14228628514343, r: 0.9671147701220726
06/02/2019 01:35:56 step: 6247, epoch: 189, batch: 9, loss: 0.021831750869750977, acc: 85.9375, f1: 79.50319521748094, r: 0.9549652690367842
06/02/2019 01:35:56 step: 6252, epoch: 189, batch: 14, loss: 0.02132819965481758, acc: 93.75, f1: 94.12790697674419, r: 0.9772953063822762
06/02/2019 01:35:56 step: 6257, epoch: 189, batch: 19, loss: 0.02177954837679863, acc: 89.0625, f1: 87.01814058956916, r: 0.9706817477506751
06/02/2019 01:35:57 step: 6262, epoch: 189, batch: 24, loss: 0.03121364861726761, acc: 87.5, f1: 71.4908379291941, r: 0.9145314081187443
06/02/2019 01:35:57 step: 6267, epoch: 189, batch: 29, loss: 0.02111883834004402, acc: 89.0625, f1: 70.23931039720513, r: 0.9768230542496891
06/02/2019 01:35:58 *** evaluating ***
06/02/2019 01:35:58 step: 190, epoch: 189, acc: 61.53846153846154, f1: 27.822260763437235, r: 0.4192862682324439
06/02/2019 01:35:58 *** epoch: 191 ***
06/02/2019 01:35:58 *** training ***
06/02/2019 01:35:58 step: 6275, epoch: 190, batch: 4, loss: 0.02311493456363678, acc: 89.0625, f1: 86.59677128427128, r: 0.967257749019804
06/02/2019 01:35:59 step: 6280, epoch: 190, batch: 9, loss: 0.022123662754893303, acc: 84.375, f1: 81.7622213567746, r: 0.9744371918096384
06/02/2019 01:35:59 step: 6285, epoch: 190, batch: 14, loss: 0.025061629712581635, acc: 93.75, f1: 90.49853915925343, r: 0.9675231402820532
06/02/2019 01:35:59 step: 6290, epoch: 190, batch: 19, loss: 0.023114662617444992, acc: 85.9375, f1: 61.5426161888426, r: 0.959405853358003
06/02/2019 01:36:00 step: 6295, epoch: 190, batch: 24, loss: 0.026616999879479408, acc: 95.3125, f1: 93.00794358778303, r: 0.9258426255487139
06/02/2019 01:36:00 step: 6300, epoch: 190, batch: 29, loss: 0.02244236320257187, acc: 92.1875, f1: 75.0925925925926, r: 0.9746292353085635
06/02/2019 01:36:00 *** evaluating ***
06/02/2019 01:36:01 step: 191, epoch: 190, acc: 61.111111111111114, f1: 27.66930720038378, r: 0.4148655635264916
06/02/2019 01:36:01 *** epoch: 192 ***
06/02/2019 01:36:01 *** training ***
06/02/2019 01:36:01 step: 6308, epoch: 191, batch: 4, loss: 0.022349292412400246, acc: 89.0625, f1: 76.30799592564298, r: 0.9634203173989535
06/02/2019 01:36:02 step: 6313, epoch: 191, batch: 9, loss: 0.024369515478610992, acc: 87.5, f1: 82.07081590400135, r: 0.9502005699707041
06/02/2019 01:36:02 step: 6318, epoch: 191, batch: 14, loss: 0.027562106028199196, acc: 89.0625, f1: 81.12499999999999, r: 0.94781732718346
06/02/2019 01:36:02 step: 6323, epoch: 191, batch: 19, loss: 0.023092567920684814, acc: 92.1875, f1: 89.69607843137256, r: 0.9706752142401672
06/02/2019 01:36:03 step: 6328, epoch: 191, batch: 24, loss: 0.022256135940551758, acc: 90.625, f1: 88.16846303738886, r: 0.9746086294843233
06/02/2019 01:36:03 step: 6333, epoch: 191, batch: 29, loss: 0.023431764915585518, acc: 90.625, f1: 78.01190476190476, r: 0.9654698734852066
06/02/2019 01:36:04 *** evaluating ***
06/02/2019 01:36:04 step: 192, epoch: 191, acc: 60.256410256410255, f1: 26.40933219992632, r: 0.41184447714726896
06/02/2019 01:36:04 *** epoch: 193 ***
06/02/2019 01:36:04 *** training ***
06/02/2019 01:36:04 step: 6341, epoch: 192, batch: 4, loss: 0.02440538816154003, acc: 89.0625, f1: 88.47519439624702, r: 0.9768931706859649
06/02/2019 01:36:05 step: 6346, epoch: 192, batch: 9, loss: 0.021538231521844864, acc: 93.75, f1: 68.56229707792207, r: 0.9762072722433844
06/02/2019 01:36:05 step: 6351, epoch: 192, batch: 14, loss: 0.02204265259206295, acc: 84.375, f1: 68.63888888888889, r: 0.9756181905343222
06/02/2019 01:36:06 step: 6356, epoch: 192, batch: 19, loss: 0.028065836057066917, acc: 87.5, f1: 86.67719508752117, r: 0.9595706214923011
06/02/2019 01:36:06 step: 6361, epoch: 192, batch: 24, loss: 0.021365510299801826, acc: 93.75, f1: 92.86483253588517, r: 0.9809424993579384
06/02/2019 01:36:07 step: 6366, epoch: 192, batch: 29, loss: 0.022977245971560478, acc: 87.5, f1: 85.01764342100478, r: 0.9752376924811215
06/02/2019 01:36:07 *** evaluating ***
06/02/2019 01:36:07 step: 193, epoch: 192, acc: 62.39316239316239, f1: 29.533848504436737, r: 0.4094049378220959
06/02/2019 01:36:07 *** epoch: 194 ***
06/02/2019 01:36:07 *** training ***
06/02/2019 01:36:08 step: 6374, epoch: 193, batch: 4, loss: 0.021709058433771133, acc: 85.9375, f1: 89.1364734299517, r: 0.9792265646504124
06/02/2019 01:36:08 step: 6379, epoch: 193, batch: 9, loss: 0.026615113019943237, acc: 92.1875, f1: 90.74974670719351, r: 0.9551412374739385
06/02/2019 01:36:08 step: 6384, epoch: 193, batch: 14, loss: 0.024170834571123123, acc: 93.75, f1: 90.24753714194709, r: 0.9641597586197697
06/02/2019 01:36:09 step: 6389, epoch: 193, batch: 19, loss: 0.022576572373509407, acc: 89.0625, f1: 76.22508652583842, r: 0.9661407869100536
06/02/2019 01:36:09 step: 6394, epoch: 193, batch: 24, loss: 0.02446809969842434, acc: 90.625, f1: 87.08139759969029, r: 0.9751952838380442
06/02/2019 01:36:10 step: 6399, epoch: 193, batch: 29, loss: 0.023102039471268654, acc: 81.25, f1: 72.7878045157457, r: 0.977965659861483
06/02/2019 01:36:10 *** evaluating ***
06/02/2019 01:36:10 step: 194, epoch: 193, acc: 60.68376068376068, f1: 27.744876906182046, r: 0.4114640250957476
06/02/2019 01:36:10 *** epoch: 195 ***
06/02/2019 01:36:10 *** training ***
06/02/2019 01:36:11 step: 6407, epoch: 194, batch: 4, loss: 0.0221988745033741, acc: 95.3125, f1: 94.25387176147055, r: 0.981051330539385
06/02/2019 01:36:11 step: 6412, epoch: 194, batch: 9, loss: 0.022396773099899292, acc: 93.75, f1: 92.10670781036913, r: 0.9766086297513569
06/02/2019 01:36:12 step: 6417, epoch: 194, batch: 14, loss: 0.02669183909893036, acc: 84.375, f1: 68.32563309876795, r: 0.950063154124371
06/02/2019 01:36:12 step: 6422, epoch: 194, batch: 19, loss: 0.02730240486562252, acc: 90.625, f1: 73.66980638409208, r: 0.9480889461396961
06/02/2019 01:36:13 step: 6427, epoch: 194, batch: 24, loss: 0.025567365810275078, acc: 92.1875, f1: 79.65750915750915, r: 0.9553668060314254
06/02/2019 01:36:13 step: 6432, epoch: 194, batch: 29, loss: 0.02401832677423954, acc: 93.75, f1: 83.77976190476191, r: 0.9577850634701365
06/02/2019 01:36:13 *** evaluating ***
06/02/2019 01:36:13 step: 195, epoch: 194, acc: 61.965811965811966, f1: 29.19740973312402, r: 0.41192779273609986
06/02/2019 01:36:13 *** epoch: 196 ***
06/02/2019 01:36:13 *** training ***
06/02/2019 01:36:14 step: 6440, epoch: 195, batch: 4, loss: 0.022479413077235222, acc: 90.625, f1: 76.4634571777429, r: 0.9632494078779735
06/02/2019 01:36:14 step: 6445, epoch: 195, batch: 9, loss: 0.025485863909125328, acc: 90.625, f1: 83.79802162410857, r: 0.9555542513911792
06/02/2019 01:36:15 step: 6450, epoch: 195, batch: 14, loss: 0.026477038860321045, acc: 89.0625, f1: 56.60603282318741, r: 0.9390621373909218
06/02/2019 01:36:15 step: 6455, epoch: 195, batch: 19, loss: 0.025162996724247932, acc: 92.1875, f1: 80.15873015873017, r: 0.9603750286953252
06/02/2019 01:36:16 step: 6460, epoch: 195, batch: 24, loss: 0.026931170374155045, acc: 84.375, f1: 67.83689107827038, r: 0.9557040485080868
06/02/2019 01:36:16 step: 6465, epoch: 195, batch: 29, loss: 0.025326456874608994, acc: 93.75, f1: 94.97739534685347, r: 0.9719471401234691
06/02/2019 01:36:17 *** evaluating ***
06/02/2019 01:36:17 step: 196, epoch: 195, acc: 61.111111111111114, f1: 29.674195694242865, r: 0.40953420509756777
06/02/2019 01:36:17 *** epoch: 197 ***
06/02/2019 01:36:17 *** training ***
06/02/2019 01:36:17 step: 6473, epoch: 196, batch: 4, loss: 0.024534156545996666, acc: 85.9375, f1: 82.85327575650157, r: 0.9622284703083572
06/02/2019 01:36:18 step: 6478, epoch: 196, batch: 9, loss: 0.021862290799617767, acc: 93.75, f1: 92.24489795918367, r: 0.9656608804221052
06/02/2019 01:36:18 step: 6483, epoch: 196, batch: 14, loss: 0.021887898445129395, acc: 87.5, f1: 78.55516194331985, r: 0.9733346812357168
06/02/2019 01:36:19 step: 6488, epoch: 196, batch: 19, loss: 0.0260658897459507, acc: 85.9375, f1: 83.63215488215488, r: 0.9612194483665139
06/02/2019 01:36:19 step: 6493, epoch: 196, batch: 24, loss: 0.02309183031320572, acc: 90.625, f1: 90.21554834054835, r: 0.9844619301551869
06/02/2019 01:36:19 step: 6498, epoch: 196, batch: 29, loss: 0.0237415861338377, acc: 89.0625, f1: 85.95238095238095, r: 0.9364268389656019
06/02/2019 01:36:20 *** evaluating ***
06/02/2019 01:36:20 step: 197, epoch: 196, acc: 61.53846153846154, f1: 28.822920785431428, r: 0.4126670323807494
06/02/2019 01:36:20 *** epoch: 198 ***
06/02/2019 01:36:20 *** training ***
06/02/2019 01:36:20 step: 6506, epoch: 197, batch: 4, loss: 0.02313351072371006, acc: 93.75, f1: 82.48309763600847, r: 0.9688268929601664
06/02/2019 01:36:21 step: 6511, epoch: 197, batch: 9, loss: 0.022951586171984673, acc: 92.1875, f1: 74.49888226527571, r: 0.96978223431361
06/02/2019 01:36:21 step: 6516, epoch: 197, batch: 14, loss: 0.02408650889992714, acc: 85.9375, f1: 81.5327380952381, r: 0.9792107309477119
06/02/2019 01:36:22 step: 6521, epoch: 197, batch: 19, loss: 0.025064874440431595, acc: 92.1875, f1: 81.49968320661598, r: 0.9631593990434586
06/02/2019 01:36:22 step: 6526, epoch: 197, batch: 24, loss: 0.02234286069869995, acc: 89.0625, f1: 73.1609623015873, r: 0.9747117704104776
06/02/2019 01:36:23 step: 6531, epoch: 197, batch: 29, loss: 0.023613840341567993, acc: 90.625, f1: 87.82738095238095, r: 0.9753767295111917
06/02/2019 01:36:23 *** evaluating ***
06/02/2019 01:36:23 step: 198, epoch: 197, acc: 61.965811965811966, f1: 29.191981802509403, r: 0.4122747799365987
06/02/2019 01:36:23 *** epoch: 199 ***
06/02/2019 01:36:23 *** training ***
06/02/2019 01:36:24 step: 6539, epoch: 198, batch: 4, loss: 0.02402583137154579, acc: 82.8125, f1: 73.12083454071283, r: 0.9690896299168605
06/02/2019 01:36:24 step: 6544, epoch: 198, batch: 9, loss: 0.02279348112642765, acc: 87.5, f1: 76.69222830832739, r: 0.9719643370347876
06/02/2019 01:36:25 step: 6549, epoch: 198, batch: 14, loss: 0.022526806220412254, acc: 95.3125, f1: 91.156462585034, r: 0.9783930554211271
06/02/2019 01:36:25 step: 6554, epoch: 198, batch: 19, loss: 0.023181697353720665, acc: 90.625, f1: 85.68678535243886, r: 0.9594482972388079
06/02/2019 01:36:25 step: 6559, epoch: 198, batch: 24, loss: 0.02304726466536522, acc: 89.0625, f1: 84.33574287347871, r: 0.9815964272856901
06/02/2019 01:36:26 step: 6564, epoch: 198, batch: 29, loss: 0.02275899052619934, acc: 89.0625, f1: 88.68399740848719, r: 0.9675673140520553
06/02/2019 01:36:26 *** evaluating ***
06/02/2019 01:36:26 step: 199, epoch: 198, acc: 62.39316239316239, f1: 29.658350992764515, r: 0.4081599648519561
06/02/2019 01:36:26 *** epoch: 200 ***
06/02/2019 01:36:26 *** training ***
06/02/2019 01:36:27 step: 6572, epoch: 199, batch: 4, loss: 0.023608798161149025, acc: 89.0625, f1: 72.13509316770187, r: 0.9705818976993714
06/02/2019 01:36:27 step: 6577, epoch: 199, batch: 9, loss: 0.021332236006855965, acc: 92.1875, f1: 90.98484848484848, r: 0.9707908973274484
06/02/2019 01:36:28 step: 6582, epoch: 199, batch: 14, loss: 0.022852828726172447, acc: 85.9375, f1: 59.75825471698113, r: 0.9728218636290481
06/02/2019 01:36:28 step: 6587, epoch: 199, batch: 19, loss: 0.021856063976883888, acc: 89.0625, f1: 74.31908369408369, r: 0.9824960781216795
06/02/2019 01:36:29 step: 6592, epoch: 199, batch: 24, loss: 0.024556022137403488, acc: 89.0625, f1: 82.0199938949939, r: 0.9763268659220697
06/02/2019 01:36:29 step: 6597, epoch: 199, batch: 29, loss: 0.023358047008514404, acc: 96.875, f1: 96.67457680746718, r: 0.9773463001776267
06/02/2019 01:36:29 *** evaluating ***
06/02/2019 01:36:30 step: 200, epoch: 199, acc: 61.53846153846154, f1: 27.187583016631827, r: 0.4141522962969141
06/02/2019 01:36:30 *** epoch: 201 ***
06/02/2019 01:36:30 *** training ***
06/02/2019 01:36:30 step: 6605, epoch: 200, batch: 4, loss: 0.02229190804064274, acc: 93.75, f1: 95.29993815708102, r: 0.9647680333422193
06/02/2019 01:36:30 step: 6610, epoch: 200, batch: 9, loss: 0.022261884063482285, acc: 87.5, f1: 64.12771560236997, r: 0.9613191546332007
06/02/2019 01:36:31 step: 6615, epoch: 200, batch: 14, loss: 0.023850591853260994, acc: 89.0625, f1: 81.13404452690168, r: 0.9638232271801593
06/02/2019 01:36:31 step: 6620, epoch: 200, batch: 19, loss: 0.02455606311559677, acc: 93.75, f1: 91.00484768687534, r: 0.9300896024804278
06/02/2019 01:36:32 step: 6625, epoch: 200, batch: 24, loss: 0.023095302283763885, acc: 93.75, f1: 90.5765490478034, r: 0.969677253678516
06/02/2019 01:36:32 step: 6630, epoch: 200, batch: 29, loss: 0.022722551599144936, acc: 89.0625, f1: 80.47140359640359, r: 0.9768496035608902
06/02/2019 01:36:32 *** evaluating ***
06/02/2019 01:36:33 step: 201, epoch: 200, acc: 61.53846153846154, f1: 27.354739628303303, r: 0.4118018628463896
06/02/2019 01:36:33 *** epoch: 202 ***
06/02/2019 01:36:33 *** training ***
06/02/2019 01:36:33 step: 6638, epoch: 201, batch: 4, loss: 0.02263958752155304, acc: 84.375, f1: 61.716269841269835, r: 0.9547733116629189
06/02/2019 01:36:34 step: 6643, epoch: 201, batch: 9, loss: 0.022816693410277367, acc: 93.75, f1: 89.06822344322343, r: 0.9650389126463826
06/02/2019 01:36:34 step: 6648, epoch: 201, batch: 14, loss: 0.020913150161504745, acc: 89.0625, f1: 75.30873753009976, r: 0.9774581123198987
06/02/2019 01:36:34 step: 6653, epoch: 201, batch: 19, loss: 0.021869422867894173, acc: 89.0625, f1: 80.82885576045827, r: 0.9705106628750577
06/02/2019 01:36:35 step: 6658, epoch: 201, batch: 24, loss: 0.02288898453116417, acc: 85.9375, f1: 76.100768321513, r: 0.9699467562963011
06/02/2019 01:36:35 step: 6663, epoch: 201, batch: 29, loss: 0.023202337324619293, acc: 84.375, f1: 68.97839248945974, r: 0.971580742599554
06/02/2019 01:36:35 *** evaluating ***
06/02/2019 01:36:36 step: 202, epoch: 201, acc: 62.39316239316239, f1: 29.199922320304427, r: 0.40766720145873625
06/02/2019 01:36:36 *** epoch: 203 ***
06/02/2019 01:36:36 *** training ***
06/02/2019 01:36:36 step: 6671, epoch: 202, batch: 4, loss: 0.023941267281770706, acc: 90.625, f1: 91.11691086691086, r: 0.9738052536652192
06/02/2019 01:36:37 step: 6676, epoch: 202, batch: 9, loss: 0.021652963012456894, acc: 90.625, f1: 80.02013532240971, r: 0.9716196715917024
06/02/2019 01:36:37 step: 6681, epoch: 202, batch: 14, loss: 0.020582763478159904, acc: 90.625, f1: 83.09427512157542, r: 0.9816482300664815
06/02/2019 01:36:37 step: 6686, epoch: 202, batch: 19, loss: 0.025035787373781204, acc: 89.0625, f1: 72.75944170771757, r: 0.9586792502300904
06/02/2019 01:36:38 step: 6691, epoch: 202, batch: 24, loss: 0.023291783407330513, acc: 81.25, f1: 76.75925925925927, r: 0.9668519034239307
06/02/2019 01:36:38 step: 6696, epoch: 202, batch: 29, loss: 0.01980089396238327, acc: 89.0625, f1: 82.54245283018868, r: 0.9771666616997103
06/02/2019 01:36:39 *** evaluating ***
06/02/2019 01:36:39 step: 203, epoch: 202, acc: 61.53846153846154, f1: 28.252892773358262, r: 0.4089787040939531
06/02/2019 01:36:39 *** epoch: 204 ***
06/02/2019 01:36:39 *** training ***
06/02/2019 01:36:39 step: 6704, epoch: 203, batch: 4, loss: 0.024767626076936722, acc: 90.625, f1: 92.73211768851304, r: 0.9528217744781111
06/02/2019 01:36:40 step: 6709, epoch: 203, batch: 9, loss: 0.022203095257282257, acc: 89.0625, f1: 76.59819476189445, r: 0.9775233383298395
06/02/2019 01:36:40 step: 6714, epoch: 203, batch: 14, loss: 0.023915167897939682, acc: 89.0625, f1: 81.0511332679525, r: 0.9708961804932636
06/02/2019 01:36:41 step: 6719, epoch: 203, batch: 19, loss: 0.020858922973275185, acc: 89.0625, f1: 86.3484376801568, r: 0.9716089429786902
06/02/2019 01:36:41 step: 6724, epoch: 203, batch: 24, loss: 0.022334005683660507, acc: 89.0625, f1: 69.62887989203779, r: 0.9782846089701849
06/02/2019 01:36:42 step: 6729, epoch: 203, batch: 29, loss: 0.022128785029053688, acc: 89.0625, f1: 92.34280661866869, r: 0.9642642245508165
06/02/2019 01:36:42 *** evaluating ***
06/02/2019 01:36:42 step: 204, epoch: 203, acc: 61.53846153846154, f1: 27.42234904501985, r: 0.41280516875828294
06/02/2019 01:36:42 *** epoch: 205 ***
06/02/2019 01:36:42 *** training ***
06/02/2019 01:36:43 step: 6737, epoch: 204, batch: 4, loss: 0.028307685628533363, acc: 96.875, f1: 98.25826465852042, r: 0.9670714950214475
06/02/2019 01:36:43 step: 6742, epoch: 204, batch: 9, loss: 0.023460693657398224, acc: 95.3125, f1: 95.6415108141045, r: 0.9679142078400735
06/02/2019 01:36:44 step: 6747, epoch: 204, batch: 14, loss: 0.022032909095287323, acc: 89.0625, f1: 70.9000089397461, r: 0.9729598444234272
06/02/2019 01:36:44 step: 6752, epoch: 204, batch: 19, loss: 0.021223194897174835, acc: 89.0625, f1: 79.23404210168916, r: 0.974183262970789
06/02/2019 01:36:44 step: 6757, epoch: 204, batch: 24, loss: 0.025657784193754196, acc: 85.9375, f1: 70.60195106470681, r: 0.9736537484869281
06/02/2019 01:36:45 step: 6762, epoch: 204, batch: 29, loss: 0.025510583072900772, acc: 96.875, f1: 98.08018068887634, r: 0.957554450497555
06/02/2019 01:36:45 *** evaluating ***
06/02/2019 01:36:45 step: 205, epoch: 204, acc: 62.82051282051282, f1: 29.338419058129606, r: 0.4134330143512588
06/02/2019 01:36:45 *** epoch: 206 ***
06/02/2019 01:36:45 *** training ***
06/02/2019 01:36:46 step: 6770, epoch: 205, batch: 4, loss: 0.02170773223042488, acc: 90.625, f1: 79.8883656026513, r: 0.9739282606664506
06/02/2019 01:36:46 step: 6775, epoch: 205, batch: 9, loss: 0.019721243530511856, acc: 90.625, f1: 86.68177797434764, r: 0.9818299191191353
06/02/2019 01:36:47 step: 6780, epoch: 205, batch: 14, loss: 0.023422667756676674, acc: 93.75, f1: 70.39613960273819, r: 0.9540337576427795
06/02/2019 01:36:47 step: 6785, epoch: 205, batch: 19, loss: 0.025324132293462753, acc: 90.625, f1: 84.15633922742029, r: 0.9559306676167689
06/02/2019 01:36:47 step: 6790, epoch: 205, batch: 24, loss: 0.018798725679516792, acc: 89.0625, f1: 86.2012987012987, r: 0.9883036238533481
06/02/2019 01:36:48 step: 6795, epoch: 205, batch: 29, loss: 0.02436654642224312, acc: 81.25, f1: 80.16388373531231, r: 0.9494667869870188
06/02/2019 01:36:48 *** evaluating ***
06/02/2019 01:36:48 step: 206, epoch: 205, acc: 62.82051282051282, f1: 30.223886637145576, r: 0.4069174417332114
06/02/2019 01:36:48 *** epoch: 207 ***
06/02/2019 01:36:48 *** training ***
06/02/2019 01:36:49 step: 6803, epoch: 206, batch: 4, loss: 0.02393307536840439, acc: 90.625, f1: 83.59846991058171, r: 0.9774334361810928
06/02/2019 01:36:49 step: 6808, epoch: 206, batch: 9, loss: 0.023949075490236282, acc: 90.625, f1: 82.59833024118738, r: 0.9717198658086306
06/02/2019 01:36:50 step: 6813, epoch: 206, batch: 14, loss: 0.022574540227651596, acc: 96.875, f1: 91.92012288786484, r: 0.9728504464347731
06/02/2019 01:36:50 step: 6818, epoch: 206, batch: 19, loss: 0.022529518231749535, acc: 89.0625, f1: 83.06671398776662, r: 0.9688316730673702
06/02/2019 01:36:51 step: 6823, epoch: 206, batch: 24, loss: 0.021296638995409012, acc: 87.5, f1: 86.46265120201619, r: 0.9796102901533144
06/02/2019 01:36:51 step: 6828, epoch: 206, batch: 29, loss: 0.0220942422747612, acc: 84.375, f1: 71.83255269320843, r: 0.9781949388620298
06/02/2019 01:36:52 *** evaluating ***
06/02/2019 01:36:52 step: 207, epoch: 206, acc: 62.39316239316239, f1: 30.434049775943834, r: 0.406108322288019
06/02/2019 01:36:52 *** epoch: 208 ***
06/02/2019 01:36:52 *** training ***
06/02/2019 01:36:52 step: 6836, epoch: 207, batch: 4, loss: 0.020691819489002228, acc: 89.0625, f1: 82.19487688237687, r: 0.9819463418884706
06/02/2019 01:36:53 step: 6841, epoch: 207, batch: 9, loss: 0.022783616557717323, acc: 84.375, f1: 78.17741469057258, r: 0.9783620123232293
06/02/2019 01:36:53 step: 6846, epoch: 207, batch: 14, loss: 0.020656349137425423, acc: 87.5, f1: 83.21496212121212, r: 0.9844341987192002
06/02/2019 01:36:53 step: 6851, epoch: 207, batch: 19, loss: 0.0220072902739048, acc: 84.375, f1: 61.08723513328778, r: 0.9763180478920568
06/02/2019 01:36:54 step: 6856, epoch: 207, batch: 24, loss: 0.02892235480248928, acc: 89.0625, f1: 84.10635341669825, r: 0.9553695995163274
06/02/2019 01:36:54 step: 6861, epoch: 207, batch: 29, loss: 0.0207134447991848, acc: 90.625, f1: 83.43707343707342, r: 0.965305159476497
06/02/2019 01:36:55 *** evaluating ***
06/02/2019 01:36:55 step: 208, epoch: 207, acc: 62.39316239316239, f1: 30.63084157839494, r: 0.4092760974859513
06/02/2019 01:36:55 *** epoch: 209 ***
06/02/2019 01:36:55 *** training ***
06/02/2019 01:36:55 step: 6869, epoch: 208, batch: 4, loss: 0.02131754718720913, acc: 89.0625, f1: 64.48092280390418, r: 0.9714062754432012
06/02/2019 01:36:56 step: 6874, epoch: 208, batch: 9, loss: 0.024321746081113815, acc: 92.1875, f1: 92.8438107785934, r: 0.9528738952264938
06/02/2019 01:36:56 step: 6879, epoch: 208, batch: 14, loss: 0.02480924315750599, acc: 87.5, f1: 82.0444557133198, r: 0.9577783083558279
06/02/2019 01:36:57 step: 6884, epoch: 208, batch: 19, loss: 0.021729346364736557, acc: 89.0625, f1: 80.01642036124794, r: 0.97455354738916
06/02/2019 01:36:57 step: 6889, epoch: 208, batch: 24, loss: 0.022610925137996674, acc: 90.625, f1: 88.54256854256855, r: 0.9745218694880566
06/02/2019 01:36:57 step: 6894, epoch: 208, batch: 29, loss: 0.020598504692316055, acc: 87.5, f1: 61.998556998557, r: 0.9626528946568573
06/02/2019 01:36:58 *** evaluating ***
06/02/2019 01:36:58 step: 209, epoch: 208, acc: 62.39316239316239, f1: 29.714960756262144, r: 0.4105755465577388
06/02/2019 01:36:58 *** epoch: 210 ***
06/02/2019 01:36:58 *** training ***
06/02/2019 01:36:58 step: 6902, epoch: 209, batch: 4, loss: 0.022751912474632263, acc: 90.625, f1: 89.15607344632768, r: 0.9756221799315845
06/02/2019 01:36:59 step: 6907, epoch: 209, batch: 9, loss: 0.023053664714097977, acc: 89.0625, f1: 75.3110599078341, r: 0.9722634869818254
06/02/2019 01:36:59 step: 6912, epoch: 209, batch: 14, loss: 0.022656027227640152, acc: 89.0625, f1: 70.510678721174, r: 0.9707238938419852
06/02/2019 01:37:00 step: 6917, epoch: 209, batch: 19, loss: 0.02381473407149315, acc: 90.625, f1: 84.63718820861679, r: 0.9606708032716536
06/02/2019 01:37:00 step: 6922, epoch: 209, batch: 24, loss: 0.027932796627283096, acc: 78.125, f1: 64.10162183995857, r: 0.9299776525443777
06/02/2019 01:37:01 step: 6927, epoch: 209, batch: 29, loss: 0.02211333066225052, acc: 90.625, f1: 78.64298966339783, r: 0.9724780274197488
06/02/2019 01:37:01 *** evaluating ***
06/02/2019 01:37:01 step: 210, epoch: 209, acc: 60.68376068376068, f1: 26.842969723432986, r: 0.409251601361982
06/02/2019 01:37:01 *** epoch: 211 ***
06/02/2019 01:37:01 *** training ***
06/02/2019 01:37:01 step: 6935, epoch: 210, batch: 4, loss: 0.024061817675828934, acc: 87.5, f1: 74.66108452950557, r: 0.968387523865027
06/02/2019 01:37:02 step: 6940, epoch: 210, batch: 9, loss: 0.025286640971899033, acc: 87.5, f1: 78.45641139900879, r: 0.9511119002734815
06/02/2019 01:37:02 step: 6945, epoch: 210, batch: 14, loss: 0.023378049954771996, acc: 84.375, f1: 69.42176870748298, r: 0.9739268153031588
06/02/2019 01:37:03 step: 6950, epoch: 210, batch: 19, loss: 0.02385156974196434, acc: 93.75, f1: 80.91627762215997, r: 0.9664820909871306
06/02/2019 01:37:03 step: 6955, epoch: 210, batch: 24, loss: 0.01976863294839859, acc: 95.3125, f1: 97.35719492311785, r: 0.9719950230829898
06/02/2019 01:37:04 step: 6960, epoch: 210, batch: 29, loss: 0.023587707430124283, acc: 82.8125, f1: 71.77147337429595, r: 0.9679642712092401
06/02/2019 01:37:04 *** evaluating ***
06/02/2019 01:37:04 step: 211, epoch: 210, acc: 62.39316239316239, f1: 29.811086596800884, r: 0.408780902418493
06/02/2019 01:37:04 *** epoch: 212 ***
06/02/2019 01:37:04 *** training ***
06/02/2019 01:37:05 step: 6968, epoch: 211, batch: 4, loss: 0.024631956592202187, acc: 95.3125, f1: 89.68522770761577, r: 0.9548401953513738
06/02/2019 01:37:05 step: 6973, epoch: 211, batch: 9, loss: 0.019965900108218193, acc: 92.1875, f1: 92.49986124986125, r: 0.9806032787794827
06/02/2019 01:37:05 step: 6978, epoch: 211, batch: 14, loss: 0.022415820509195328, acc: 81.25, f1: 73.25387022397892, r: 0.9767445029280069
06/02/2019 01:37:06 step: 6983, epoch: 211, batch: 19, loss: 0.021526092663407326, acc: 93.75, f1: 78.203231292517, r: 0.978903758150041
06/02/2019 01:37:06 step: 6988, epoch: 211, batch: 24, loss: 0.02461317554116249, acc: 93.75, f1: 95.28846153846153, r: 0.9752041422636171
06/02/2019 01:37:07 step: 6993, epoch: 211, batch: 29, loss: 0.02352128177881241, acc: 84.375, f1: 72.61715296198055, r: 0.97618734141171
06/02/2019 01:37:07 *** evaluating ***
06/02/2019 01:37:07 step: 212, epoch: 211, acc: 61.111111111111114, f1: 27.432476386583893, r: 0.40392484400886475
06/02/2019 01:37:07 *** epoch: 213 ***
06/02/2019 01:37:07 *** training ***
06/02/2019 01:37:08 step: 7001, epoch: 212, batch: 4, loss: 0.024173101410269737, acc: 90.625, f1: 80.3697025566354, r: 0.9706902169464815
06/02/2019 01:37:08 step: 7006, epoch: 212, batch: 9, loss: 0.02310721017420292, acc: 93.75, f1: 95.62937062937063, r: 0.9734947818107316
06/02/2019 01:37:08 step: 7011, epoch: 212, batch: 14, loss: 0.027021287009119987, acc: 89.0625, f1: 70.39682539682539, r: 0.9008137005839784
06/02/2019 01:37:09 step: 7016, epoch: 212, batch: 19, loss: 0.019994888454675674, acc: 93.75, f1: 87.90750915750915, r: 0.9786354689436824
06/02/2019 01:37:09 step: 7021, epoch: 212, batch: 24, loss: 0.022075127810239792, acc: 90.625, f1: 90.06631161236425, r: 0.959531636393489
06/02/2019 01:37:10 step: 7026, epoch: 212, batch: 29, loss: 0.020218970254063606, acc: 95.3125, f1: 93.24665350010972, r: 0.9716713627134416
06/02/2019 01:37:10 *** evaluating ***
06/02/2019 01:37:10 step: 213, epoch: 212, acc: 61.965811965811966, f1: 27.95999183381693, r: 0.4080291777434868
06/02/2019 01:37:10 *** epoch: 214 ***
06/02/2019 01:37:10 *** training ***
06/02/2019 01:37:11 step: 7034, epoch: 213, batch: 4, loss: 0.023573996499180794, acc: 93.75, f1: 86.49364093463473, r: 0.8958886588411855
06/02/2019 01:37:11 step: 7039, epoch: 213, batch: 9, loss: 0.022595683112740517, acc: 92.1875, f1: 77.83531509141265, r: 0.9796925382906579
06/02/2019 01:37:12 step: 7044, epoch: 213, batch: 14, loss: 0.02297365665435791, acc: 89.0625, f1: 84.81786680189316, r: 0.9720754301103478
06/02/2019 01:37:12 step: 7049, epoch: 213, batch: 19, loss: 0.023937081918120384, acc: 90.625, f1: 85.03573945434411, r: 0.9641627424727786
06/02/2019 01:37:13 step: 7054, epoch: 213, batch: 24, loss: 0.026805557310581207, acc: 90.625, f1: 89.58873327759197, r: 0.9632513821389936
06/02/2019 01:37:13 step: 7059, epoch: 213, batch: 29, loss: 0.023182908073067665, acc: 87.5, f1: 76.2855963922475, r: 0.966821426995855
06/02/2019 01:37:13 *** evaluating ***
06/02/2019 01:37:13 step: 214, epoch: 213, acc: 62.39316239316239, f1: 30.10748413671952, r: 0.4028142711473041
06/02/2019 01:37:13 *** epoch: 215 ***
06/02/2019 01:37:13 *** training ***
06/02/2019 01:37:14 step: 7067, epoch: 214, batch: 4, loss: 0.020613646134734154, acc: 93.75, f1: 87.2752904989747, r: 0.9834015967552806
06/02/2019 01:37:14 step: 7072, epoch: 214, batch: 9, loss: 0.0240524522960186, acc: 92.1875, f1: 82.41249251272836, r: 0.9758457134582914
06/02/2019 01:37:15 step: 7077, epoch: 214, batch: 14, loss: 0.024925164878368378, acc: 81.25, f1: 73.89324960753532, r: 0.9566960555913608
06/02/2019 01:37:15 step: 7082, epoch: 214, batch: 19, loss: 0.023505723103880882, acc: 90.625, f1: 88.19850966702472, r: 0.9794886325709098
06/02/2019 01:37:16 step: 7087, epoch: 214, batch: 24, loss: 0.021915370598435402, acc: 95.3125, f1: 88.89104554865423, r: 0.9793631533806959
06/02/2019 01:37:16 step: 7092, epoch: 214, batch: 29, loss: 0.021936701610684395, acc: 89.0625, f1: 85.67264962665227, r: 0.966793728739795
06/02/2019 01:37:16 *** evaluating ***
06/02/2019 01:37:17 step: 215, epoch: 214, acc: 61.53846153846154, f1: 28.654768497347206, r: 0.4059498539160277
06/02/2019 01:37:17 *** epoch: 216 ***
06/02/2019 01:37:17 *** training ***
06/02/2019 01:37:17 step: 7100, epoch: 215, batch: 4, loss: 0.02381100505590439, acc: 89.0625, f1: 84.9505708716235, r: 0.9650929974865334
06/02/2019 01:37:17 step: 7105, epoch: 215, batch: 9, loss: 0.0178501196205616, acc: 93.75, f1: 89.00891481399955, r: 0.9824660090772075
06/02/2019 01:37:18 step: 7110, epoch: 215, batch: 14, loss: 0.023275716230273247, acc: 96.875, f1: 90.52410901467506, r: 0.9563181030211972
06/02/2019 01:37:18 step: 7115, epoch: 215, batch: 19, loss: 0.021606190130114555, acc: 87.5, f1: 66.87524822986427, r: 0.9681248244424571
06/02/2019 01:37:19 step: 7120, epoch: 215, batch: 24, loss: 0.02311404049396515, acc: 84.375, f1: 59.14387889543169, r: 0.9691353619369938
06/02/2019 01:37:19 step: 7125, epoch: 215, batch: 29, loss: 0.02279566414654255, acc: 84.375, f1: 78.16542944929168, r: 0.978681438931438
06/02/2019 01:37:19 *** evaluating ***
06/02/2019 01:37:20 step: 216, epoch: 215, acc: 61.53846153846154, f1: 27.183011963577506, r: 0.4053776886460686
06/02/2019 01:37:20 *** epoch: 217 ***
06/02/2019 01:37:20 *** training ***
06/02/2019 01:37:20 step: 7133, epoch: 216, batch: 4, loss: 0.022806856781244278, acc: 87.5, f1: 87.22151360544218, r: 0.98027941978067
06/02/2019 01:37:21 step: 7138, epoch: 216, batch: 9, loss: 0.02117512933909893, acc: 85.9375, f1: 85.16504329004329, r: 0.9834490621926955
06/02/2019 01:37:21 step: 7143, epoch: 216, batch: 14, loss: 0.02284291200339794, acc: 89.0625, f1: 88.08697089947091, r: 0.9795080047316271
06/02/2019 01:37:22 step: 7148, epoch: 216, batch: 19, loss: 0.02337563782930374, acc: 95.3125, f1: 95.29761904761904, r: 0.9634752842051804
06/02/2019 01:37:22 step: 7153, epoch: 216, batch: 24, loss: 0.02281852439045906, acc: 93.75, f1: 95.0349892238512, r: 0.9612776190323871
06/02/2019 01:37:23 step: 7158, epoch: 216, batch: 29, loss: 0.02301393821835518, acc: 95.3125, f1: 93.86973180076629, r: 0.9480324640730307
06/02/2019 01:37:23 *** evaluating ***
06/02/2019 01:37:23 step: 217, epoch: 216, acc: 61.111111111111114, f1: 26.240738060210784, r: 0.4044053720213503
06/02/2019 01:37:23 *** epoch: 218 ***
06/02/2019 01:37:23 *** training ***
06/02/2019 01:37:23 step: 7166, epoch: 217, batch: 4, loss: 0.02149176225066185, acc: 90.625, f1: 86.95926899187769, r: 0.9800469883652421
06/02/2019 01:37:24 step: 7171, epoch: 217, batch: 9, loss: 0.023224879056215286, acc: 90.625, f1: 85.56690872480345, r: 0.9574974446508796
06/02/2019 01:37:24 step: 7176, epoch: 217, batch: 14, loss: 0.025063607841730118, acc: 90.625, f1: 84.95685980539143, r: 0.9731560565739656
06/02/2019 01:37:25 step: 7181, epoch: 217, batch: 19, loss: 0.021846063435077667, acc: 89.0625, f1: 84.70539876789877, r: 0.9793274553849272
06/02/2019 01:37:25 step: 7186, epoch: 217, batch: 24, loss: 0.020237084478139877, acc: 87.5, f1: 87.87746495825765, r: 0.9857066713027729
06/02/2019 01:37:26 step: 7191, epoch: 217, batch: 29, loss: 0.020985838025808334, acc: 90.625, f1: 67.47931905259492, r: 0.976566120316158
06/02/2019 01:37:26 *** evaluating ***
06/02/2019 01:37:26 step: 218, epoch: 217, acc: 61.53846153846154, f1: 27.57934952978056, r: 0.40733075071944214
06/02/2019 01:37:26 *** epoch: 219 ***
06/02/2019 01:37:26 *** training ***
06/02/2019 01:37:26 step: 7199, epoch: 218, batch: 4, loss: 0.02258443459868431, acc: 89.0625, f1: 84.01798153953325, r: 0.974629752768476
06/02/2019 01:37:27 step: 7204, epoch: 218, batch: 9, loss: 0.022963887080550194, acc: 90.625, f1: 78.65178571428572, r: 0.9777680833778414
06/02/2019 01:37:27 step: 7209, epoch: 218, batch: 14, loss: 0.021789003163576126, acc: 85.9375, f1: 66.38773867034737, r: 0.9582033713370997
06/02/2019 01:37:28 step: 7214, epoch: 218, batch: 19, loss: 0.024133915081620216, acc: 92.1875, f1: 89.10412273322655, r: 0.9617953200820399
06/02/2019 01:37:28 step: 7219, epoch: 218, batch: 24, loss: 0.022420885041356087, acc: 92.1875, f1: 87.0112781954887, r: 0.9211420096370209
06/02/2019 01:37:29 step: 7224, epoch: 218, batch: 29, loss: 0.024900540709495544, acc: 92.1875, f1: 93.81613756613756, r: 0.9715318848691261
06/02/2019 01:37:29 *** evaluating ***
06/02/2019 01:37:29 step: 219, epoch: 218, acc: 61.111111111111114, f1: 26.446907008392174, r: 0.4099658416940645
06/02/2019 01:37:29 *** epoch: 220 ***
06/02/2019 01:37:29 *** training ***
06/02/2019 01:37:30 step: 7232, epoch: 219, batch: 4, loss: 0.02066897414624691, acc: 92.1875, f1: 79.8376894617496, r: 0.9757336444386464
06/02/2019 01:37:30 step: 7237, epoch: 219, batch: 9, loss: 0.02197698876261711, acc: 96.875, f1: 95.2832512315271, r: 0.9747884931859921
06/02/2019 01:37:31 step: 7242, epoch: 219, batch: 14, loss: 0.026190001517534256, acc: 93.75, f1: 89.85053957059435, r: 0.9643723698410127
06/02/2019 01:37:31 step: 7247, epoch: 219, batch: 19, loss: 0.026679104194045067, acc: 79.6875, f1: 60.17298402081012, r: 0.9264120446586486
06/02/2019 01:37:32 step: 7252, epoch: 219, batch: 24, loss: 0.021453550085425377, acc: 93.75, f1: 77.70845481049564, r: 0.9675318328308445
06/02/2019 01:37:32 step: 7257, epoch: 219, batch: 29, loss: 0.024689897894859314, acc: 92.1875, f1: 90.68778162457762, r: 0.967944381653477
06/02/2019 01:37:32 *** evaluating ***
06/02/2019 01:37:32 step: 220, epoch: 219, acc: 59.82905982905983, f1: 24.20535897820324, r: 0.4071068946675771
06/02/2019 01:37:32 *** epoch: 221 ***
06/02/2019 01:37:32 *** training ***
06/02/2019 01:37:33 step: 7265, epoch: 220, batch: 4, loss: 0.021913396194577217, acc: 89.0625, f1: 80.60096153846155, r: 0.9751097132835818
06/02/2019 01:37:33 step: 7270, epoch: 220, batch: 9, loss: 0.0234246626496315, acc: 85.9375, f1: 79.55621376674009, r: 0.9748347489626922
06/02/2019 01:37:34 step: 7275, epoch: 220, batch: 14, loss: 0.02464250847697258, acc: 93.75, f1: 88.91493055555556, r: 0.9387602570009969
06/02/2019 01:37:34 step: 7280, epoch: 220, batch: 19, loss: 0.019938265904784203, acc: 90.625, f1: 88.8863287250384, r: 0.9774079351978519
06/02/2019 01:37:35 step: 7285, epoch: 220, batch: 24, loss: 0.023299288004636765, acc: 90.625, f1: 85.90232683982684, r: 0.9738298500095859
06/02/2019 01:37:35 step: 7290, epoch: 220, batch: 29, loss: 0.02334611676633358, acc: 89.0625, f1: 85.13612412177986, r: 0.9695307186284352
06/02/2019 01:37:35 *** evaluating ***
06/02/2019 01:37:36 step: 221, epoch: 220, acc: 61.965811965811966, f1: 27.789949483704124, r: 0.4038875238941305
06/02/2019 01:37:36 *** epoch: 222 ***
06/02/2019 01:37:36 *** training ***
06/02/2019 01:37:36 step: 7298, epoch: 221, batch: 4, loss: 0.020361486822366714, acc: 92.1875, f1: 91.18107769423558, r: 0.9766775333362224
06/02/2019 01:37:37 step: 7303, epoch: 221, batch: 9, loss: 0.021937306970357895, acc: 90.625, f1: 87.91821710707909, r: 0.9713688212043956
06/02/2019 01:37:37 step: 7308, epoch: 221, batch: 14, loss: 0.020852159708738327, acc: 92.1875, f1: 81.76470588235294, r: 0.9743780499793616
06/02/2019 01:37:38 step: 7313, epoch: 221, batch: 19, loss: 0.02421167865395546, acc: 90.625, f1: 76.3421052631579, r: 0.9686673812246367
06/02/2019 01:37:38 step: 7318, epoch: 221, batch: 24, loss: 0.022088076919317245, acc: 79.6875, f1: 58.06811815635344, r: 0.974202525303055
06/02/2019 01:37:39 step: 7323, epoch: 221, batch: 29, loss: 0.01993105746805668, acc: 85.9375, f1: 78.4366902986854, r: 0.983585172278903
06/02/2019 01:37:39 *** evaluating ***
06/02/2019 01:37:39 step: 222, epoch: 221, acc: 61.53846153846154, f1: 26.115430178145637, r: 0.4070829474121279
06/02/2019 01:37:39 *** epoch: 223 ***
06/02/2019 01:37:39 *** training ***
06/02/2019 01:37:39 step: 7331, epoch: 222, batch: 4, loss: 0.023708097636699677, acc: 90.625, f1: 90.81410341914545, r: 0.9567950879996439
06/02/2019 01:37:40 step: 7336, epoch: 222, batch: 9, loss: 0.022745557129383087, acc: 90.625, f1: 89.2021587571939, r: 0.9654933167798766
06/02/2019 01:37:40 step: 7341, epoch: 222, batch: 14, loss: 0.021037166938185692, acc: 92.1875, f1: 84.22169512966478, r: 0.9798956391932729
06/02/2019 01:37:41 step: 7346, epoch: 222, batch: 19, loss: 0.020749472081661224, acc: 95.3125, f1: 96.45933014354067, r: 0.9809058391264127
06/02/2019 01:37:41 step: 7351, epoch: 222, batch: 24, loss: 0.023127445951104164, acc: 93.75, f1: 89.52233627690018, r: 0.9612397507462723
06/02/2019 01:37:42 step: 7356, epoch: 222, batch: 29, loss: 0.02504732459783554, acc: 87.5, f1: 86.03585140349846, r: 0.9651270205461064
06/02/2019 01:37:42 *** evaluating ***
06/02/2019 01:37:42 step: 223, epoch: 222, acc: 60.68376068376068, f1: 26.577641245776462, r: 0.40380207432109455
06/02/2019 01:37:42 *** epoch: 224 ***
06/02/2019 01:37:42 *** training ***
06/02/2019 01:37:43 step: 7364, epoch: 223, batch: 4, loss: 0.023298753425478935, acc: 85.9375, f1: 80.75159495236221, r: 0.9742454239573776
06/02/2019 01:37:43 step: 7369, epoch: 223, batch: 9, loss: 0.021388748660683632, acc: 82.8125, f1: 73.10451173222913, r: 0.9725862282442853
06/02/2019 01:37:44 step: 7374, epoch: 223, batch: 14, loss: 0.02595340646803379, acc: 89.0625, f1: 86.0451113512338, r: 0.9355980871727397
06/02/2019 01:37:44 step: 7379, epoch: 223, batch: 19, loss: 0.019686738029122353, acc: 92.1875, f1: 80.14095468695953, r: 0.9764115091698317
06/02/2019 01:37:44 step: 7384, epoch: 223, batch: 24, loss: 0.02068118378520012, acc: 90.625, f1: 86.07230071515785, r: 0.9833350389916324
06/02/2019 01:37:45 step: 7389, epoch: 223, batch: 29, loss: 0.01951478235423565, acc: 93.75, f1: 91.57366071428571, r: 0.9851261072805474
06/02/2019 01:37:45 *** evaluating ***
06/02/2019 01:37:45 step: 224, epoch: 223, acc: 59.82905982905983, f1: 25.933043672014268, r: 0.4021101273092324
06/02/2019 01:37:45 *** epoch: 225 ***
06/02/2019 01:37:45 *** training ***
06/02/2019 01:37:46 step: 7397, epoch: 224, batch: 4, loss: 0.019909923896193504, acc: 92.1875, f1: 93.91799908915229, r: 0.9658181137136026
06/02/2019 01:37:46 step: 7402, epoch: 224, batch: 9, loss: 0.021421559154987335, acc: 93.75, f1: 91.00577731092436, r: 0.9773201122844435
06/02/2019 01:37:47 step: 7407, epoch: 224, batch: 14, loss: 0.02053230255842209, acc: 89.0625, f1: 83.63896754874197, r: 0.9682466602464591
06/02/2019 01:37:47 step: 7412, epoch: 224, batch: 19, loss: 0.01955428346991539, acc: 90.625, f1: 79.46434517863088, r: 0.9775777220142079
06/02/2019 01:37:48 step: 7417, epoch: 224, batch: 24, loss: 0.021902848035097122, acc: 92.1875, f1: 88.16724941724942, r: 0.976877999021842
06/02/2019 01:37:48 step: 7422, epoch: 224, batch: 29, loss: 0.021931130439043045, acc: 92.1875, f1: 92.77504105090311, r: 0.9760955734102447
06/02/2019 01:37:48 *** evaluating ***
06/02/2019 01:37:49 step: 225, epoch: 224, acc: 61.111111111111114, f1: 29.394350975022693, r: 0.4069852099738701
06/02/2019 01:37:49 *** epoch: 226 ***
06/02/2019 01:37:49 *** training ***
06/02/2019 01:37:49 step: 7430, epoch: 225, batch: 4, loss: 0.0213114395737648, acc: 89.0625, f1: 82.57805227354099, r: 0.9616251471358014
06/02/2019 01:37:50 step: 7435, epoch: 225, batch: 9, loss: 0.021853826940059662, acc: 84.375, f1: 81.11658310089169, r: 0.9679113478206207
06/02/2019 01:37:50 step: 7440, epoch: 225, batch: 14, loss: 0.023036610335111618, acc: 87.5, f1: 75.90866370471633, r: 0.9678518057882055
06/02/2019 01:37:50 step: 7445, epoch: 225, batch: 19, loss: 0.021207282319664955, acc: 89.0625, f1: 86.82266009852216, r: 0.960861009352062
06/02/2019 01:37:51 step: 7450, epoch: 225, batch: 24, loss: 0.02197691984474659, acc: 92.1875, f1: 87.32413419913419, r: 0.9785279239294051
06/02/2019 01:37:51 step: 7455, epoch: 225, batch: 29, loss: 0.02030467987060547, acc: 93.75, f1: 89.53816666755732, r: 0.9688623895708344
06/02/2019 01:37:52 *** evaluating ***
06/02/2019 01:37:52 step: 226, epoch: 225, acc: 61.111111111111114, f1: 28.434117911672086, r: 0.4029658240308427
06/02/2019 01:37:52 *** epoch: 227 ***
06/02/2019 01:37:52 *** training ***
06/02/2019 01:37:52 step: 7463, epoch: 226, batch: 4, loss: 0.018314722925424576, acc: 92.1875, f1: 87.98400673400673, r: 0.9829834818359702
06/02/2019 01:37:53 step: 7468, epoch: 226, batch: 9, loss: 0.022824861109256744, acc: 92.1875, f1: 89.91071428571429, r: 0.9674994447028176
06/02/2019 01:37:53 step: 7473, epoch: 226, batch: 14, loss: 0.022674281150102615, acc: 87.5, f1: 85.28305028305027, r: 0.9709280820954271
06/02/2019 01:37:54 step: 7478, epoch: 226, batch: 19, loss: 0.020988183096051216, acc: 93.75, f1: 93.0722003086949, r: 0.9745012096993305
06/02/2019 01:37:54 step: 7483, epoch: 226, batch: 24, loss: 0.020991547033190727, acc: 93.75, f1: 88.08972030157427, r: 0.9801022484087472
06/02/2019 01:37:55 step: 7488, epoch: 226, batch: 29, loss: 0.02304386906325817, acc: 93.75, f1: 93.01770050125313, r: 0.9786696775833925
06/02/2019 01:37:55 *** evaluating ***
06/02/2019 01:37:55 step: 227, epoch: 226, acc: 61.53846153846154, f1: 28.479427716243, r: 0.4060258613462432
06/02/2019 01:37:55 *** epoch: 228 ***
06/02/2019 01:37:55 *** training ***
06/02/2019 01:37:56 step: 7496, epoch: 227, batch: 4, loss: 0.02102855034172535, acc: 90.625, f1: 87.58387445887446, r: 0.9707453589035868
06/02/2019 01:37:56 step: 7501, epoch: 227, batch: 9, loss: 0.02262260392308235, acc: 92.1875, f1: 93.18687343358395, r: 0.9746315889133618
06/02/2019 01:37:57 step: 7506, epoch: 227, batch: 14, loss: 0.023494645953178406, acc: 93.75, f1: 88.35748792270532, r: 0.9591041454641784
06/02/2019 01:37:57 step: 7511, epoch: 227, batch: 19, loss: 0.023220395669341087, acc: 87.5, f1: 89.9774653451124, r: 0.9804650401461226
06/02/2019 01:37:57 step: 7516, epoch: 227, batch: 24, loss: 0.022656414657831192, acc: 93.75, f1: 95.18398268398268, r: 0.9628488044473634
06/02/2019 01:37:58 step: 7521, epoch: 227, batch: 29, loss: 0.0248115137219429, acc: 95.3125, f1: 94.68426501035196, r: 0.9580551358206015
06/02/2019 01:37:58 *** evaluating ***
06/02/2019 01:37:58 step: 228, epoch: 227, acc: 61.111111111111114, f1: 27.67713572207461, r: 0.4003749178584601
06/02/2019 01:37:58 *** epoch: 229 ***
06/02/2019 01:37:58 *** training ***
06/02/2019 01:37:59 step: 7529, epoch: 228, batch: 4, loss: 0.021816786378622055, acc: 90.625, f1: 85.00164643718652, r: 0.9802449924364838
06/02/2019 01:37:59 step: 7534, epoch: 228, batch: 9, loss: 0.02289053425192833, acc: 89.0625, f1: 65.93188557474272, r: 0.9679594348036659
06/02/2019 01:38:00 step: 7539, epoch: 228, batch: 14, loss: 0.021632228046655655, acc: 90.625, f1: 82.37955236075537, r: 0.9779739105681143
06/02/2019 01:38:00 step: 7544, epoch: 228, batch: 19, loss: 0.01978035271167755, acc: 87.5, f1: 77.69728116710876, r: 0.9687910149826883
06/02/2019 01:38:01 step: 7549, epoch: 228, batch: 24, loss: 0.020625300705432892, acc: 90.625, f1: 76.9221117047204, r: 0.9782641832040808
06/02/2019 01:38:01 step: 7554, epoch: 228, batch: 29, loss: 0.022315308451652527, acc: 93.75, f1: 88.0009179200554, r: 0.9633188957153119
06/02/2019 01:38:01 *** evaluating ***
06/02/2019 01:38:02 step: 229, epoch: 228, acc: 61.965811965811966, f1: 28.951015295553113, r: 0.40677140674932516
06/02/2019 01:38:02 *** epoch: 230 ***
06/02/2019 01:38:02 *** training ***
06/02/2019 01:38:02 step: 7562, epoch: 229, batch: 4, loss: 0.023936694487929344, acc: 92.1875, f1: 91.54481538992408, r: 0.971967515371471
06/02/2019 01:38:03 step: 7567, epoch: 229, batch: 9, loss: 0.019659316167235374, acc: 92.1875, f1: 87.94017763845349, r: 0.9773330056005151
06/02/2019 01:38:03 step: 7572, epoch: 229, batch: 14, loss: 0.02107970230281353, acc: 89.0625, f1: 83.74917146345717, r: 0.9752105560665478
06/02/2019 01:38:04 step: 7577, epoch: 229, batch: 19, loss: 0.021186769008636475, acc: 93.75, f1: 91.73280423280424, r: 0.9821327563052209
06/02/2019 01:38:04 step: 7582, epoch: 229, batch: 24, loss: 0.02279612049460411, acc: 92.1875, f1: 89.65986394557824, r: 0.9709950056521841
06/02/2019 01:38:05 step: 7587, epoch: 229, batch: 29, loss: 0.02476051077246666, acc: 85.9375, f1: 67.81946124051387, r: 0.9337929416708609
06/02/2019 01:38:05 *** evaluating ***
06/02/2019 01:38:05 step: 230, epoch: 229, acc: 61.53846153846154, f1: 29.432733680522244, r: 0.41132977656535485
06/02/2019 01:38:05 *** epoch: 231 ***
06/02/2019 01:38:05 *** training ***
06/02/2019 01:38:05 step: 7595, epoch: 230, batch: 4, loss: 0.023819388821721077, acc: 93.75, f1: 92.63585434173669, r: 0.927209249820868
06/02/2019 01:38:06 step: 7600, epoch: 230, batch: 9, loss: 0.020102903246879578, acc: 90.625, f1: 93.88381321520856, r: 0.9848817967569956
06/02/2019 01:38:06 step: 7605, epoch: 230, batch: 14, loss: 0.022525552660226822, acc: 92.1875, f1: 90.68190411373261, r: 0.9746590487954451
06/02/2019 01:38:07 step: 7610, epoch: 230, batch: 19, loss: 0.023541301488876343, acc: 87.5, f1: 80.30012018020739, r: 0.9683939171548319
06/02/2019 01:38:07 step: 7615, epoch: 230, batch: 24, loss: 0.022612322121858597, acc: 92.1875, f1: 91.5988514173998, r: 0.9752209045026341
06/02/2019 01:38:08 step: 7620, epoch: 230, batch: 29, loss: 0.023678550496697426, acc: 89.0625, f1: 89.51213260423788, r: 0.9633762389072899
06/02/2019 01:38:08 *** evaluating ***
06/02/2019 01:38:08 step: 231, epoch: 230, acc: 61.965811965811966, f1: 29.052080731768225, r: 0.4043654443161811
06/02/2019 01:38:08 *** epoch: 232 ***
06/02/2019 01:38:08 *** training ***
06/02/2019 01:38:09 step: 7628, epoch: 231, batch: 4, loss: 0.023718729615211487, acc: 84.375, f1: 73.83622098725073, r: 0.9630537120712542
06/02/2019 01:38:09 step: 7633, epoch: 231, batch: 9, loss: 0.02046261355280876, acc: 93.75, f1: 85.26785714285714, r: 0.9771938539567451
06/02/2019 01:38:09 step: 7638, epoch: 231, batch: 14, loss: 0.023013886064291, acc: 95.3125, f1: 95.2640103660512, r: 0.9743010046519768
06/02/2019 01:38:10 step: 7643, epoch: 231, batch: 19, loss: 0.022937534376978874, acc: 90.625, f1: 92.0831608005521, r: 0.9572184539133307
06/02/2019 01:38:10 step: 7648, epoch: 231, batch: 24, loss: 0.023815467953681946, acc: 90.625, f1: 89.71353398182667, r: 0.9794045916412402
06/02/2019 01:38:11 step: 7653, epoch: 231, batch: 29, loss: 0.019119547680020332, acc: 89.0625, f1: 72.35684010489878, r: 0.9821524814343447
06/02/2019 01:38:11 *** evaluating ***
06/02/2019 01:38:11 step: 232, epoch: 231, acc: 61.965811965811966, f1: 29.093763590592797, r: 0.40490778294687196
06/02/2019 01:38:11 *** epoch: 233 ***
06/02/2019 01:38:11 *** training ***
06/02/2019 01:38:12 step: 7661, epoch: 232, batch: 4, loss: 0.02040349319577217, acc: 93.75, f1: 83.53992527862208, r: 0.9771387407751643
06/02/2019 01:38:12 step: 7666, epoch: 232, batch: 9, loss: 0.02131098508834839, acc: 92.1875, f1: 89.0436806301468, r: 0.968829909720091
06/02/2019 01:38:13 step: 7671, epoch: 232, batch: 14, loss: 0.022371582686901093, acc: 90.625, f1: 73.14402684150583, r: 0.9710227229664193
06/02/2019 01:38:13 step: 7676, epoch: 232, batch: 19, loss: 0.02307293191552162, acc: 84.375, f1: 80.59229659229659, r: 0.945135715910199
06/02/2019 01:38:14 step: 7681, epoch: 232, batch: 24, loss: 0.02412320487201214, acc: 90.625, f1: 76.93448845444568, r: 0.9711496232472594
06/02/2019 01:38:14 step: 7686, epoch: 232, batch: 29, loss: 0.022153882309794426, acc: 89.0625, f1: 76.3144375949254, r: 0.9754051081664191
06/02/2019 01:38:14 *** evaluating ***
06/02/2019 01:38:15 step: 233, epoch: 232, acc: 62.82051282051282, f1: 29.77550714762991, r: 0.4075227165009693
06/02/2019 01:38:15 *** epoch: 234 ***
06/02/2019 01:38:15 *** training ***
06/02/2019 01:38:15 step: 7694, epoch: 233, batch: 4, loss: 0.021096333861351013, acc: 93.75, f1: 88.24361628709455, r: 0.9551367824919244
06/02/2019 01:38:16 step: 7699, epoch: 233, batch: 9, loss: 0.021517343819141388, acc: 89.0625, f1: 87.68628430393136, r: 0.9813098994397017
06/02/2019 01:38:16 step: 7704, epoch: 233, batch: 14, loss: 0.022884665057063103, acc: 90.625, f1: 87.5549876007542, r: 0.9705410499369116
06/02/2019 01:38:16 step: 7709, epoch: 233, batch: 19, loss: 0.021208301186561584, acc: 89.0625, f1: 85.58339305621914, r: 0.9726246888369852
06/02/2019 01:38:17 step: 7714, epoch: 233, batch: 24, loss: 0.021299416199326515, acc: 87.5, f1: 78.8296903460838, r: 0.9776487442973415
06/02/2019 01:38:17 step: 7719, epoch: 233, batch: 29, loss: 0.022286437451839447, acc: 89.0625, f1: 88.27259750053868, r: 0.9680009016418818
06/02/2019 01:38:18 *** evaluating ***
06/02/2019 01:38:18 step: 234, epoch: 233, acc: 62.39316239316239, f1: 28.23106579273562, r: 0.41120153960529815
06/02/2019 01:38:18 *** epoch: 235 ***
06/02/2019 01:38:18 *** training ***
06/02/2019 01:38:18 step: 7727, epoch: 234, batch: 4, loss: 0.02252521924674511, acc: 100.0, f1: 100.0, r: 0.9449635005111456
06/02/2019 01:38:19 step: 7732, epoch: 234, batch: 9, loss: 0.022483911365270615, acc: 87.5, f1: 70.06802721088434, r: 0.9793342390728607
06/02/2019 01:38:19 step: 7737, epoch: 234, batch: 14, loss: 0.024121999740600586, acc: 81.25, f1: 70.37329978862236, r: 0.9664650026929719
06/02/2019 01:38:20 step: 7742, epoch: 234, batch: 19, loss: 0.02534777857363224, acc: 90.625, f1: 84.3630831643002, r: 0.9668590735082309
06/02/2019 01:38:20 step: 7747, epoch: 234, batch: 24, loss: 0.020532306283712387, acc: 85.9375, f1: 82.39734299516908, r: 0.9843244883918171
06/02/2019 01:38:20 step: 7752, epoch: 234, batch: 29, loss: 0.021144267171621323, acc: 89.0625, f1: 84.70532507006672, r: 0.9822086893819181
06/02/2019 01:38:21 *** evaluating ***
06/02/2019 01:38:21 step: 235, epoch: 234, acc: 61.965811965811966, f1: 27.25252713281765, r: 0.40677894805668213
06/02/2019 01:38:21 *** epoch: 236 ***
06/02/2019 01:38:21 *** training ***
06/02/2019 01:38:21 step: 7760, epoch: 235, batch: 4, loss: 0.02293016016483307, acc: 87.5, f1: 70.43650793650794, r: 0.963227499182778
06/02/2019 01:38:22 step: 7765, epoch: 235, batch: 9, loss: 0.02321750484406948, acc: 87.5, f1: 67.4131546558017, r: 0.9620405610972377
06/02/2019 01:38:22 step: 7770, epoch: 235, batch: 14, loss: 0.02219511941075325, acc: 90.625, f1: 71.20926899187769, r: 0.9685757547614716
06/02/2019 01:38:23 step: 7775, epoch: 235, batch: 19, loss: 0.021577470004558563, acc: 96.875, f1: 95.89989842090681, r: 0.9811904540542958
06/02/2019 01:38:23 step: 7780, epoch: 235, batch: 24, loss: 0.02316945232450962, acc: 90.625, f1: 87.37103174603175, r: 0.9784641878695897
06/02/2019 01:38:24 step: 7785, epoch: 235, batch: 29, loss: 0.022833362221717834, acc: 90.625, f1: 86.83295540438397, r: 0.9694317431741605
06/02/2019 01:38:24 *** evaluating ***
06/02/2019 01:38:24 step: 236, epoch: 235, acc: 61.53846153846154, f1: 27.321781425748593, r: 0.40609855795712524
06/02/2019 01:38:24 *** epoch: 237 ***
06/02/2019 01:38:24 *** training ***
06/02/2019 01:38:25 step: 7793, epoch: 236, batch: 4, loss: 0.019879018887877464, acc: 92.1875, f1: 94.58805744520032, r: 0.9563713645398237
06/02/2019 01:38:25 step: 7798, epoch: 236, batch: 9, loss: 0.01979954168200493, acc: 87.5, f1: 80.90484515484515, r: 0.9766271776907199
06/02/2019 01:38:26 step: 7803, epoch: 236, batch: 14, loss: 0.02264387160539627, acc: 87.5, f1: 87.15544871794872, r: 0.9464027282119576
06/02/2019 01:38:26 step: 7808, epoch: 236, batch: 19, loss: 0.02163335308432579, acc: 90.625, f1: 88.39285714285715, r: 0.979853389726486
06/02/2019 01:38:26 step: 7813, epoch: 236, batch: 24, loss: 0.022589221596717834, acc: 87.5, f1: 69.45367261737229, r: 0.9667860076687721
06/02/2019 01:38:27 step: 7818, epoch: 236, batch: 29, loss: 0.02090846374630928, acc: 92.1875, f1: 80.89889277389277, r: 0.9684232526725504
06/02/2019 01:38:27 *** evaluating ***
06/02/2019 01:38:27 step: 237, epoch: 236, acc: 62.39316239316239, f1: 30.000347353795632, r: 0.409430781310739
06/02/2019 01:38:27 *** epoch: 238 ***
06/02/2019 01:38:27 *** training ***
06/02/2019 01:38:28 step: 7826, epoch: 237, batch: 4, loss: 0.018234893679618835, acc: 89.0625, f1: 87.26190476190476, r: 0.9863327433054309
06/02/2019 01:38:28 step: 7831, epoch: 237, batch: 9, loss: 0.023911669850349426, acc: 89.0625, f1: 87.18186575329432, r: 0.9568882768515541
06/02/2019 01:38:29 step: 7836, epoch: 237, batch: 14, loss: 0.023875508457422256, acc: 90.625, f1: 85.73640604890605, r: 0.9645229247437545
06/02/2019 01:38:29 step: 7841, epoch: 237, batch: 19, loss: 0.020355461165308952, acc: 95.3125, f1: 81.34502923976609, r: 0.9775662835752531
06/02/2019 01:38:29 step: 7846, epoch: 237, batch: 24, loss: 0.024462323635816574, acc: 84.375, f1: 73.955296404276, r: 0.9651847399239849
06/02/2019 01:38:30 step: 7851, epoch: 237, batch: 29, loss: 0.018109602853655815, acc: 87.5, f1: 75.62027588813304, r: 0.9851291701687551
06/02/2019 01:38:30 *** evaluating ***
06/02/2019 01:38:30 step: 238, epoch: 237, acc: 60.256410256410255, f1: 26.87000249875063, r: 0.40789618999826194
06/02/2019 01:38:30 *** epoch: 239 ***
06/02/2019 01:38:30 *** training ***
06/02/2019 01:38:31 step: 7859, epoch: 238, batch: 4, loss: 0.02072063274681568, acc: 87.5, f1: 75.04761904761905, r: 0.9808915533784942
06/02/2019 01:38:31 step: 7864, epoch: 238, batch: 9, loss: 0.022013358771800995, acc: 90.625, f1: 79.99656593406593, r: 0.9788951326252081
06/02/2019 01:38:32 step: 7869, epoch: 238, batch: 14, loss: 0.020734485238790512, acc: 87.5, f1: 84.96031566910416, r: 0.9832093429886684
06/02/2019 01:38:32 step: 7874, epoch: 238, batch: 19, loss: 0.024205848574638367, acc: 93.75, f1: 82.1267802246063, r: 0.9731933902463505
06/02/2019 01:38:33 step: 7879, epoch: 238, batch: 24, loss: 0.023190181702375412, acc: 92.1875, f1: 89.4291843162811, r: 0.9492361189078855
06/02/2019 01:38:33 step: 7884, epoch: 238, batch: 29, loss: 0.019900377839803696, acc: 92.1875, f1: 91.65209790209789, r: 0.980972690641297
06/02/2019 01:38:34 *** evaluating ***
06/02/2019 01:38:34 step: 239, epoch: 238, acc: 60.68376068376068, f1: 28.210033361512043, r: 0.4095581042234515
06/02/2019 01:38:34 *** epoch: 240 ***
06/02/2019 01:38:34 *** training ***
06/02/2019 01:38:34 step: 7892, epoch: 239, batch: 4, loss: 0.021051451563835144, acc: 90.625, f1: 88.22420634920634, r: 0.979335864394792
06/02/2019 01:38:35 step: 7897, epoch: 239, batch: 9, loss: 0.022086940705776215, acc: 90.625, f1: 85.78162578162578, r: 0.9769551165444477
06/02/2019 01:38:35 step: 7902, epoch: 239, batch: 14, loss: 0.01952706277370453, acc: 90.625, f1: 79.73856209150327, r: 0.9827294339091944
06/02/2019 01:38:36 step: 7907, epoch: 239, batch: 19, loss: 0.02064730040729046, acc: 89.0625, f1: 82.51893939393939, r: 0.9746136069900454
06/02/2019 01:38:36 step: 7912, epoch: 239, batch: 24, loss: 0.02212042175233364, acc: 92.1875, f1: 90.6322272498743, r: 0.9809067807126168
06/02/2019 01:38:37 step: 7917, epoch: 239, batch: 29, loss: 0.020380618050694466, acc: 92.1875, f1: 89.0435606060606, r: 0.9842046210723305
06/02/2019 01:38:37 *** evaluating ***
06/02/2019 01:38:37 step: 240, epoch: 239, acc: 61.53846153846154, f1: 29.243669285956024, r: 0.40941692314780426
06/02/2019 01:38:37 *** epoch: 241 ***
06/02/2019 01:38:37 *** training ***
06/02/2019 01:38:37 step: 7925, epoch: 240, batch: 4, loss: 0.022865300998091698, acc: 82.8125, f1: 69.11129616643291, r: 0.9741564083421399
06/02/2019 01:38:38 step: 7930, epoch: 240, batch: 9, loss: 0.020900003612041473, acc: 93.75, f1: 90.51093643198907, r: 0.9807150520739248
06/02/2019 01:38:38 step: 7935, epoch: 240, batch: 14, loss: 0.024367954581975937, acc: 89.0625, f1: 74.49847274472778, r: 0.9639149543539225
06/02/2019 01:38:39 step: 7940, epoch: 240, batch: 19, loss: 0.024943094700574875, acc: 90.625, f1: 77.4760539667372, r: 0.9683343379496084
06/02/2019 01:38:39 step: 7945, epoch: 240, batch: 24, loss: 0.018330872058868408, acc: 95.3125, f1: 91.18689581095596, r: 0.9738054910090236
06/02/2019 01:38:40 step: 7950, epoch: 240, batch: 29, loss: 0.022469721734523773, acc: 87.5, f1: 82.93775190326915, r: 0.9670927760988062
06/02/2019 01:38:40 *** evaluating ***
06/02/2019 01:38:40 step: 241, epoch: 240, acc: 62.39316239316239, f1: 31.284732938051018, r: 0.41407618573718813
06/02/2019 01:38:40 *** epoch: 242 ***
06/02/2019 01:38:40 *** training ***
06/02/2019 01:38:40 step: 7958, epoch: 241, batch: 4, loss: 0.01991768181324005, acc: 93.75, f1: 93.48748473748473, r: 0.9773148635335281
06/02/2019 01:38:41 step: 7963, epoch: 241, batch: 9, loss: 0.0235312320291996, acc: 93.75, f1: 88.40781104108311, r: 0.9648425655744473
06/02/2019 01:38:41 step: 7968, epoch: 241, batch: 14, loss: 0.01953885331749916, acc: 89.0625, f1: 83.68146305646306, r: 0.9811264008480829
06/02/2019 01:38:42 step: 7973, epoch: 241, batch: 19, loss: 0.020308416336774826, acc: 89.0625, f1: 84.70767832379708, r: 0.9824216086517292
06/02/2019 01:38:42 step: 7978, epoch: 241, batch: 24, loss: 0.02627650462090969, acc: 87.5, f1: 87.79003112632218, r: 0.9544948636791428
06/02/2019 01:38:43 step: 7983, epoch: 241, batch: 29, loss: 0.023082023486495018, acc: 93.75, f1: 79.3385015367774, r: 0.9741887036435992
06/02/2019 01:38:43 *** evaluating ***
06/02/2019 01:38:43 step: 242, epoch: 241, acc: 60.68376068376068, f1: 28.41552503158727, r: 0.4114126038827944
06/02/2019 01:38:43 *** epoch: 243 ***
06/02/2019 01:38:43 *** training ***
06/02/2019 01:38:44 step: 7991, epoch: 242, batch: 4, loss: 0.022231098264455795, acc: 87.5, f1: 87.28384107266096, r: 0.9689230321706028
06/02/2019 01:38:44 step: 7996, epoch: 242, batch: 9, loss: 0.021583564579486847, acc: 92.1875, f1: 89.77338524508338, r: 0.9595357089665252
06/02/2019 01:38:45 step: 8001, epoch: 242, batch: 14, loss: 0.020351259037852287, acc: 92.1875, f1: 75.12825289464632, r: 0.980655765240662
06/02/2019 01:38:45 step: 8006, epoch: 242, batch: 19, loss: 0.018850158900022507, acc: 93.75, f1: 96.03452574590095, r: 0.9804541891173142
06/02/2019 01:38:46 step: 8011, epoch: 242, batch: 24, loss: 0.020256545394659042, acc: 90.625, f1: 92.65082743343612, r: 0.9768052065663522
06/02/2019 01:38:46 step: 8016, epoch: 242, batch: 29, loss: 0.022742796689271927, acc: 93.75, f1: 80.62934362934364, r: 0.9751722599765433
06/02/2019 01:38:46 *** evaluating ***
06/02/2019 01:38:47 step: 243, epoch: 242, acc: 61.111111111111114, f1: 26.894154972987426, r: 0.4078666805335437
06/02/2019 01:38:47 *** epoch: 244 ***
06/02/2019 01:38:47 *** training ***
06/02/2019 01:38:47 step: 8024, epoch: 243, batch: 4, loss: 0.022952305153012276, acc: 89.0625, f1: 82.89565826330532, r: 0.9770849786850277
06/02/2019 01:38:47 step: 8029, epoch: 243, batch: 9, loss: 0.018010400235652924, acc: 95.3125, f1: 82.9349785891368, r: 0.9809609658554467
06/02/2019 01:38:48 step: 8034, epoch: 243, batch: 14, loss: 0.018417978659272194, acc: 92.1875, f1: 94.3052370873387, r: 0.9852099360266355
06/02/2019 01:38:48 step: 8039, epoch: 243, batch: 19, loss: 0.022228654474020004, acc: 90.625, f1: 80.23809523809524, r: 0.971568189840598
06/02/2019 01:38:49 step: 8044, epoch: 243, batch: 24, loss: 0.02247229963541031, acc: 92.1875, f1: 85.79383183116353, r: 0.9801376426117502
06/02/2019 01:38:49 step: 8049, epoch: 243, batch: 29, loss: 0.023145927116274834, acc: 87.5, f1: 63.15634387726383, r: 0.9575404847916779
06/02/2019 01:38:49 *** evaluating ***
06/02/2019 01:38:50 step: 244, epoch: 243, acc: 61.53846153846154, f1: 27.688697947863222, r: 0.4131657739460058
06/02/2019 01:38:50 *** epoch: 245 ***
06/02/2019 01:38:50 *** training ***
06/02/2019 01:38:50 step: 8057, epoch: 244, batch: 4, loss: 0.022850964218378067, acc: 93.75, f1: 80.88235294117648, r: 0.9766052951375099
06/02/2019 01:38:50 step: 8062, epoch: 244, batch: 9, loss: 0.02118498459458351, acc: 87.5, f1: 76.52236652236653, r: 0.9800625772723931
06/02/2019 01:38:51 step: 8067, epoch: 244, batch: 14, loss: 0.023470070213079453, acc: 90.625, f1: 74.99092970521542, r: 0.9707033222059089
06/02/2019 01:38:51 step: 8072, epoch: 244, batch: 19, loss: 0.021343909204006195, acc: 92.1875, f1: 90.71905197485692, r: 0.9724936253216947
06/02/2019 01:38:52 step: 8077, epoch: 244, batch: 24, loss: 0.020879939198493958, acc: 89.0625, f1: 89.31065045355327, r: 0.9808014737235672
06/02/2019 01:38:52 step: 8082, epoch: 244, batch: 29, loss: 0.02358388528227806, acc: 85.9375, f1: 70.78149920255183, r: 0.9649394723939664
06/02/2019 01:38:52 *** evaluating ***
06/02/2019 01:38:53 step: 245, epoch: 244, acc: 61.965811965811966, f1: 28.686621716703687, r: 0.41040844488539935
06/02/2019 01:38:53 *** epoch: 246 ***
06/02/2019 01:38:53 *** training ***
06/02/2019 01:38:53 step: 8090, epoch: 245, batch: 4, loss: 0.025363251566886902, acc: 84.375, f1: 78.71865585681374, r: 0.9748699241409245
06/02/2019 01:38:54 step: 8095, epoch: 245, batch: 9, loss: 0.02546606957912445, acc: 90.625, f1: 80.26575854700855, r: 0.9558540192082287
06/02/2019 01:38:54 step: 8100, epoch: 245, batch: 14, loss: 0.021677052602171898, acc: 92.1875, f1: 87.46182757810665, r: 0.9806109697681986
06/02/2019 01:38:55 step: 8105, epoch: 245, batch: 19, loss: 0.020745547488331795, acc: 92.1875, f1: 79.5756667120985, r: 0.9775169499769342
06/02/2019 01:38:55 step: 8110, epoch: 245, batch: 24, loss: 0.019353119656443596, acc: 89.0625, f1: 84.48601662887378, r: 0.9681988548385333
06/02/2019 01:38:56 step: 8115, epoch: 245, batch: 29, loss: 0.022412804886698723, acc: 89.0625, f1: 88.95937018385996, r: 0.9728606734321101
06/02/2019 01:38:56 *** evaluating ***
06/02/2019 01:38:56 step: 246, epoch: 245, acc: 61.111111111111114, f1: 27.980858293358295, r: 0.4129447067275195
06/02/2019 01:38:56 *** epoch: 247 ***
06/02/2019 01:38:56 *** training ***
06/02/2019 01:38:56 step: 8123, epoch: 246, batch: 4, loss: 0.022260477766394615, acc: 95.3125, f1: 93.96703296703296, r: 0.976270042868091
06/02/2019 01:38:57 step: 8128, epoch: 246, batch: 9, loss: 0.02127465046942234, acc: 93.75, f1: 90.32266689181583, r: 0.9770917050551416
06/02/2019 01:38:57 step: 8133, epoch: 246, batch: 14, loss: 0.024219214916229248, acc: 96.875, f1: 95.87301587301589, r: 0.9329327208943825
06/02/2019 01:38:58 step: 8138, epoch: 246, batch: 19, loss: 0.02160865068435669, acc: 87.5, f1: 82.90445105010589, r: 0.9476708927774051
06/02/2019 01:38:58 step: 8143, epoch: 246, batch: 24, loss: 0.02046220935881138, acc: 89.0625, f1: 81.73484848484848, r: 0.973456811568982
06/02/2019 01:38:59 step: 8148, epoch: 246, batch: 29, loss: 0.02203955687582493, acc: 90.625, f1: 88.57816711590296, r: 0.9756299757408835
06/02/2019 01:38:59 *** evaluating ***
06/02/2019 01:38:59 step: 247, epoch: 246, acc: 61.53846153846154, f1: 28.21363453260005, r: 0.4129156652807671
06/02/2019 01:38:59 *** epoch: 248 ***
06/02/2019 01:38:59 *** training ***
06/02/2019 01:39:00 step: 8156, epoch: 247, batch: 4, loss: 0.022427599877119064, acc: 85.9375, f1: 81.7296000525814, r: 0.9618967205929924
06/02/2019 01:39:00 step: 8161, epoch: 247, batch: 9, loss: 0.021015778183937073, acc: 92.1875, f1: 77.38442113442113, r: 0.9710598336833777
06/02/2019 01:39:00 step: 8166, epoch: 247, batch: 14, loss: 0.021817157045006752, acc: 92.1875, f1: 80.28121775025801, r: 0.9787326915709385
06/02/2019 01:39:01 step: 8171, epoch: 247, batch: 19, loss: 0.024022884666919708, acc: 87.5, f1: 84.11952540213409, r: 0.9675887781340949
06/02/2019 01:39:01 step: 8176, epoch: 247, batch: 24, loss: 0.02246786840260029, acc: 89.0625, f1: 83.91318369453045, r: 0.9772596063550051
06/02/2019 01:39:02 step: 8181, epoch: 247, batch: 29, loss: 0.02228887379169464, acc: 89.0625, f1: 76.40450861195542, r: 0.9715426550914394
06/02/2019 01:39:02 *** evaluating ***
06/02/2019 01:39:02 step: 248, epoch: 247, acc: 61.53846153846154, f1: 27.743032853994524, r: 0.4063751063055778
06/02/2019 01:39:02 *** epoch: 249 ***
06/02/2019 01:39:02 *** training ***
06/02/2019 01:39:03 step: 8189, epoch: 248, batch: 4, loss: 0.018199842423200607, acc: 95.3125, f1: 92.90255467038149, r: 0.984128560837439
06/02/2019 01:39:03 step: 8194, epoch: 248, batch: 9, loss: 0.019494986161589622, acc: 92.1875, f1: 85.28187498775735, r: 0.9813177830896899
06/02/2019 01:39:04 step: 8199, epoch: 248, batch: 14, loss: 0.020172860473394394, acc: 92.1875, f1: 91.54574592074592, r: 0.9732062869338651
06/02/2019 01:39:04 step: 8204, epoch: 248, batch: 19, loss: 0.023329850286245346, acc: 84.375, f1: 72.17831813576494, r: 0.979033360741526
06/02/2019 01:39:05 step: 8209, epoch: 248, batch: 24, loss: 0.018387962132692337, acc: 84.375, f1: 72.27559205500383, r: 0.9805418830686026
06/02/2019 01:39:05 step: 8214, epoch: 248, batch: 29, loss: 0.02062063477933407, acc: 98.4375, f1: 97.68964189449363, r: 0.9748715207160146
06/02/2019 01:39:05 *** evaluating ***
06/02/2019 01:39:05 step: 249, epoch: 248, acc: 62.39316239316239, f1: 29.231608965610278, r: 0.40462559188195946
06/02/2019 01:39:05 *** epoch: 250 ***
06/02/2019 01:39:05 *** training ***
06/02/2019 01:39:06 step: 8222, epoch: 249, batch: 4, loss: 0.022109203040599823, acc: 92.1875, f1: 86.57998233849628, r: 0.9610591118884839
06/02/2019 01:39:06 step: 8227, epoch: 249, batch: 9, loss: 0.02328544482588768, acc: 84.375, f1: 75.674078363154, r: 0.9572196490326823
06/02/2019 01:39:07 step: 8232, epoch: 249, batch: 14, loss: 0.024378644302487373, acc: 93.75, f1: 87.13135495744193, r: 0.9512480605420066
06/02/2019 01:39:07 step: 8237, epoch: 249, batch: 19, loss: 0.02106487564742565, acc: 95.3125, f1: 93.96367521367522, r: 0.9760171049990405
06/02/2019 01:39:08 step: 8242, epoch: 249, batch: 24, loss: 0.022562280297279358, acc: 84.375, f1: 65.6813753169252, r: 0.9733228263540379
06/02/2019 01:39:08 step: 8247, epoch: 249, batch: 29, loss: 0.020827703177928925, acc: 89.0625, f1: 90.46986037698112, r: 0.9750871512267049
06/02/2019 01:39:08 *** evaluating ***
06/02/2019 01:39:09 step: 250, epoch: 249, acc: 63.24786324786324, f1: 30.130493162170364, r: 0.4111300997400086
06/02/2019 01:39:09 *** epoch: 251 ***
06/02/2019 01:39:09 *** training ***
06/02/2019 01:39:09 step: 8255, epoch: 250, batch: 4, loss: 0.02273636683821678, acc: 93.75, f1: 92.55978943917232, r: 0.974505199744464
06/02/2019 01:39:09 step: 8260, epoch: 250, batch: 9, loss: 0.02145889773964882, acc: 90.625, f1: 73.89235764235764, r: 0.9799361914928727
06/02/2019 01:39:10 step: 8265, epoch: 250, batch: 14, loss: 0.02039216458797455, acc: 92.1875, f1: 88.88435374149661, r: 0.9507380757613416
06/02/2019 01:39:10 step: 8270, epoch: 250, batch: 19, loss: 0.020851850509643555, acc: 95.3125, f1: 91.25425170068027, r: 0.9666764894795241
06/02/2019 01:39:11 step: 8275, epoch: 250, batch: 24, loss: 0.01996205374598503, acc: 93.75, f1: 86.89101271399407, r: 0.9732833172388626
06/02/2019 01:39:11 step: 8280, epoch: 250, batch: 29, loss: 0.020068027079105377, acc: 92.1875, f1: 90.55948832588176, r: 0.9840766647309537
06/02/2019 01:39:12 *** evaluating ***
06/02/2019 01:39:12 step: 251, epoch: 250, acc: 62.39316239316239, f1: 29.18213829096537, r: 0.41341698274354655
06/02/2019 01:39:12 *** epoch: 252 ***
06/02/2019 01:39:12 *** training ***
06/02/2019 01:39:12 step: 8288, epoch: 251, batch: 4, loss: 0.022505389526486397, acc: 95.3125, f1: 90.87635054021608, r: 0.9762185634068032
06/02/2019 01:39:13 step: 8293, epoch: 251, batch: 9, loss: 0.020884953439235687, acc: 89.0625, f1: 68.53466581727452, r: 0.9701075496479938
06/02/2019 01:39:13 step: 8298, epoch: 251, batch: 14, loss: 0.02066752314567566, acc: 84.375, f1: 76.88150813150814, r: 0.983423251163517
06/02/2019 01:39:14 step: 8303, epoch: 251, batch: 19, loss: 0.02061428315937519, acc: 93.75, f1: 79.27771108443376, r: 0.9726595823102424
06/02/2019 01:39:14 step: 8308, epoch: 251, batch: 24, loss: 0.023079408332705498, acc: 87.5, f1: 80.54365079365078, r: 0.9747951688256167
06/02/2019 01:39:14 step: 8313, epoch: 251, batch: 29, loss: 0.021992208436131477, acc: 92.1875, f1: 88.47939175670267, r: 0.9709955022585158
06/02/2019 01:39:15 *** evaluating ***
06/02/2019 01:39:15 step: 252, epoch: 251, acc: 62.39316239316239, f1: 29.609243762745162, r: 0.41261814920017276
06/02/2019 01:39:15 *** epoch: 253 ***
06/02/2019 01:39:15 *** training ***
06/02/2019 01:39:15 step: 8321, epoch: 252, batch: 4, loss: 0.019267307594418526, acc: 87.5, f1: 75.85822510822511, r: 0.9781156055940119
06/02/2019 01:39:16 step: 8326, epoch: 252, batch: 9, loss: 0.023506971076130867, acc: 92.1875, f1: 76.17063492063491, r: 0.9605478847727585
06/02/2019 01:39:16 step: 8331, epoch: 252, batch: 14, loss: 0.024371299892663956, acc: 93.75, f1: 93.43624629338915, r: 0.9661952660381008
06/02/2019 01:39:17 step: 8336, epoch: 252, batch: 19, loss: 0.01948585733771324, acc: 95.3125, f1: 95.6184889414703, r: 0.9695455494198415
06/02/2019 01:39:17 step: 8341, epoch: 252, batch: 24, loss: 0.02212602086365223, acc: 89.0625, f1: 79.04761904761905, r: 0.9723341526567787
06/02/2019 01:39:18 step: 8346, epoch: 252, batch: 29, loss: 0.02087811566889286, acc: 89.0625, f1: 79.1434134906231, r: 0.9758425748967717
06/02/2019 01:39:18 *** evaluating ***
06/02/2019 01:39:18 step: 253, epoch: 252, acc: 61.111111111111114, f1: 28.5201181329662, r: 0.41002668172088164
06/02/2019 01:39:18 *** epoch: 254 ***
06/02/2019 01:39:18 *** training ***
06/02/2019 01:39:18 step: 8354, epoch: 253, batch: 4, loss: 0.021957341581583023, acc: 93.75, f1: 92.80979437229438, r: 0.9437763297476176
06/02/2019 01:39:19 step: 8359, epoch: 253, batch: 9, loss: 0.022176362574100494, acc: 96.875, f1: 97.42714106919364, r: 0.9712458543612017
06/02/2019 01:39:19 step: 8364, epoch: 253, batch: 14, loss: 0.021999843418598175, acc: 82.8125, f1: 77.80982905982906, r: 0.9788190509493964
06/02/2019 01:39:20 step: 8369, epoch: 253, batch: 19, loss: 0.019358647987246513, acc: 90.625, f1: 66.53305028305027, r: 0.9800257084638297
06/02/2019 01:39:20 step: 8374, epoch: 253, batch: 24, loss: 0.024297986179590225, acc: 87.5, f1: 80.8675197351668, r: 0.9726470438016699
06/02/2019 01:39:21 step: 8379, epoch: 253, batch: 29, loss: 0.023563556373119354, acc: 95.3125, f1: 93.92361111111111, r: 0.9767980947727932
06/02/2019 01:39:21 *** evaluating ***
06/02/2019 01:39:21 step: 254, epoch: 253, acc: 62.82051282051282, f1: 29.978177317438604, r: 0.40673752228347904
06/02/2019 01:39:21 *** epoch: 255 ***
06/02/2019 01:39:21 *** training ***
06/02/2019 01:39:22 step: 8387, epoch: 254, batch: 4, loss: 0.02114802971482277, acc: 93.75, f1: 80.44422757174871, r: 0.9751744505001224
06/02/2019 01:39:22 step: 8392, epoch: 254, batch: 9, loss: 0.020478444173932076, acc: 93.75, f1: 93.87120833769261, r: 0.9818972054946784
06/02/2019 01:39:22 step: 8397, epoch: 254, batch: 14, loss: 0.02238907292485237, acc: 92.1875, f1: 89.85638699924417, r: 0.9419948434641512
06/02/2019 01:39:23 step: 8402, epoch: 254, batch: 19, loss: 0.021760784089565277, acc: 84.375, f1: 78.63555698959426, r: 0.9792149591040588
06/02/2019 01:39:23 step: 8407, epoch: 254, batch: 24, loss: 0.023988813161849976, acc: 92.1875, f1: 87.82263540624962, r: 0.9755967669155212
06/02/2019 01:39:24 step: 8412, epoch: 254, batch: 29, loss: 0.02176923304796219, acc: 84.375, f1: 63.633241758241766, r: 0.973885943348995
06/02/2019 01:39:24 *** evaluating ***
06/02/2019 01:39:24 step: 255, epoch: 254, acc: 63.67521367521367, f1: 31.02859906996127, r: 0.4120640917315747
06/02/2019 01:39:24 *** epoch: 256 ***
06/02/2019 01:39:24 *** training ***
06/02/2019 01:39:25 step: 8420, epoch: 255, batch: 4, loss: 0.022078964859247208, acc: 92.1875, f1: 89.81934879214845, r: 0.9708175139333306
06/02/2019 01:39:25 step: 8425, epoch: 255, batch: 9, loss: 0.022711386904120445, acc: 92.1875, f1: 80.60609438870307, r: 0.9745254371048202
06/02/2019 01:39:26 step: 8430, epoch: 255, batch: 14, loss: 0.021782279014587402, acc: 93.75, f1: 83.82575757575759, r: 0.9741754744820634
06/02/2019 01:39:26 step: 8435, epoch: 255, batch: 19, loss: 0.020712580531835556, acc: 84.375, f1: 70.81378803515027, r: 0.9798095255250409
06/02/2019 01:39:27 step: 8440, epoch: 255, batch: 24, loss: 0.0214284285902977, acc: 92.1875, f1: 89.2897842897843, r: 0.9797650070957612
06/02/2019 01:39:27 step: 8445, epoch: 255, batch: 29, loss: 0.022684918716549873, acc: 90.625, f1: 89.51467011249619, r: 0.9772627591044742
06/02/2019 01:39:27 *** evaluating ***
06/02/2019 01:39:28 step: 256, epoch: 255, acc: 60.68376068376068, f1: 27.998061096465353, r: 0.4070017276591427
06/02/2019 01:39:28 *** epoch: 257 ***
06/02/2019 01:39:28 *** training ***
06/02/2019 01:39:28 step: 8453, epoch: 256, batch: 4, loss: 0.02047741413116455, acc: 89.0625, f1: 78.51958525345621, r: 0.9794893572537676
06/02/2019 01:39:28 step: 8458, epoch: 256, batch: 9, loss: 0.0184987373650074, acc: 87.5, f1: 83.12179487179488, r: 0.9884804841710582
06/02/2019 01:39:29 step: 8463, epoch: 256, batch: 14, loss: 0.020990438759326935, acc: 93.75, f1: 95.03599848427436, r: 0.9749127946952443
06/02/2019 01:39:29 step: 8468, epoch: 256, batch: 19, loss: 0.02578430436551571, acc: 90.625, f1: 88.8809523809524, r: 0.9349587193390343
06/02/2019 01:39:30 step: 8473, epoch: 256, batch: 24, loss: 0.019275054335594177, acc: 92.1875, f1: 91.07240150064601, r: 0.9741517058619494
06/02/2019 01:39:30 step: 8478, epoch: 256, batch: 29, loss: 0.019753586500883102, acc: 92.1875, f1: 77.61904761904762, r: 0.9831299684853753
06/02/2019 01:39:30 *** evaluating ***
06/02/2019 01:39:31 step: 257, epoch: 256, acc: 62.39316239316239, f1: 29.39061326258251, r: 0.40719323736988144
06/02/2019 01:39:31 *** epoch: 258 ***
06/02/2019 01:39:31 *** training ***
06/02/2019 01:39:31 step: 8486, epoch: 257, batch: 4, loss: 0.016296613961458206, acc: 96.875, f1: 93.86105188343994, r: 0.9888883233021746
06/02/2019 01:39:32 step: 8491, epoch: 257, batch: 9, loss: 0.019119929522275925, acc: 90.625, f1: 83.02297522782698, r: 0.9732780774610956
06/02/2019 01:39:32 step: 8496, epoch: 257, batch: 14, loss: 0.023198653012514114, acc: 95.3125, f1: 83.14190420645915, r: 0.9697845691152053
06/02/2019 01:39:32 step: 8501, epoch: 257, batch: 19, loss: 0.02191922441124916, acc: 89.0625, f1: 68.71710526315789, r: 0.9651342081961943
06/02/2019 01:39:33 step: 8506, epoch: 257, batch: 24, loss: 0.026473453268408775, acc: 84.375, f1: 63.44964267758385, r: 0.9551677636796684
06/02/2019 01:39:33 step: 8511, epoch: 257, batch: 29, loss: 0.022240925580263138, acc: 93.75, f1: 90.89207438264043, r: 0.9632674249209882
06/02/2019 01:39:34 *** evaluating ***
06/02/2019 01:39:34 step: 258, epoch: 257, acc: 61.965811965811966, f1: 29.273274852227228, r: 0.4088277642456412
06/02/2019 01:39:34 *** epoch: 259 ***
06/02/2019 01:39:34 *** training ***
06/02/2019 01:39:34 step: 8519, epoch: 258, batch: 4, loss: 0.024311067536473274, acc: 82.8125, f1: 72.82535723084504, r: 0.9705647932546924
06/02/2019 01:39:35 step: 8524, epoch: 258, batch: 9, loss: 0.02162785455584526, acc: 90.625, f1: 83.38926681783825, r: 0.9747864928734741
06/02/2019 01:39:35 step: 8529, epoch: 258, batch: 14, loss: 0.020998714491724968, acc: 96.875, f1: 84.29193899782135, r: 0.9782863183957795
06/02/2019 01:39:36 step: 8534, epoch: 258, batch: 19, loss: 0.019120020791888237, acc: 89.0625, f1: 90.16498553444366, r: 0.9755747348032808
06/02/2019 01:39:36 step: 8539, epoch: 258, batch: 24, loss: 0.020656384527683258, acc: 95.3125, f1: 95.39952246099173, r: 0.964416624330271
06/02/2019 01:39:37 step: 8544, epoch: 258, batch: 29, loss: 0.02288222312927246, acc: 89.0625, f1: 86.9672054998142, r: 0.975654077431629
06/02/2019 01:39:37 *** evaluating ***
06/02/2019 01:39:37 step: 259, epoch: 258, acc: 62.39316239316239, f1: 30.045113379636323, r: 0.41029446957427385
06/02/2019 01:39:37 *** epoch: 260 ***
06/02/2019 01:39:37 *** training ***
06/02/2019 01:39:38 step: 8552, epoch: 259, batch: 4, loss: 0.025310076773166656, acc: 87.5, f1: 83.83257127987808, r: 0.9531399504860942
06/02/2019 01:39:38 step: 8557, epoch: 259, batch: 9, loss: 0.021137859672307968, acc: 93.75, f1: 91.7207792207792, r: 0.9701052718921976
06/02/2019 01:39:39 step: 8562, epoch: 259, batch: 14, loss: 0.02211049199104309, acc: 90.625, f1: 87.05026455026454, r: 0.9769784000613616
06/02/2019 01:39:39 step: 8567, epoch: 259, batch: 19, loss: 0.02299698442220688, acc: 95.3125, f1: 94.68597889083064, r: 0.9708396345954382
06/02/2019 01:39:40 step: 8572, epoch: 259, batch: 24, loss: 0.019424399361014366, acc: 92.1875, f1: 81.61904761904762, r: 0.9792690476403398
06/02/2019 01:39:40 step: 8577, epoch: 259, batch: 29, loss: 0.02232692763209343, acc: 85.9375, f1: 72.16468253968253, r: 0.9377809136149673
06/02/2019 01:39:40 *** evaluating ***
06/02/2019 01:39:40 step: 260, epoch: 259, acc: 63.24786324786324, f1: 29.83930958747135, r: 0.4095864016908644
06/02/2019 01:39:40 *** epoch: 261 ***
06/02/2019 01:39:40 *** training ***
06/02/2019 01:39:41 step: 8585, epoch: 260, batch: 4, loss: 0.02328859269618988, acc: 92.1875, f1: 79.83530961791831, r: 0.9661649040866527
06/02/2019 01:39:41 step: 8590, epoch: 260, batch: 9, loss: 0.022987084463238716, acc: 85.9375, f1: 84.55512741227027, r: 0.9724297654102712
06/02/2019 01:39:42 step: 8595, epoch: 260, batch: 14, loss: 0.026666145771741867, acc: 85.9375, f1: 80.43161881977672, r: 0.9620291584275589
06/02/2019 01:39:42 step: 8600, epoch: 260, batch: 19, loss: 0.022744130343198776, acc: 98.4375, f1: 99.27319712509085, r: 0.9607046898267394
06/02/2019 01:39:43 step: 8605, epoch: 260, batch: 24, loss: 0.018998198211193085, acc: 90.625, f1: 82.77380216155727, r: 0.9719115916986855
06/02/2019 01:39:43 step: 8610, epoch: 260, batch: 29, loss: 0.02216271311044693, acc: 81.25, f1: 65.55398475179545, r: 0.9780644491678888
06/02/2019 01:39:43 *** evaluating ***
06/02/2019 01:39:43 step: 261, epoch: 260, acc: 61.53846153846154, f1: 28.62316801851686, r: 0.4103390706498046
06/02/2019 01:39:43 *** epoch: 262 ***
06/02/2019 01:39:43 *** training ***
06/02/2019 01:39:44 step: 8618, epoch: 261, batch: 4, loss: 0.021488238126039505, acc: 96.875, f1: 96.40716698578746, r: 0.9442124827421945
06/02/2019 01:39:44 step: 8623, epoch: 261, batch: 9, loss: 0.02088659256696701, acc: 95.3125, f1: 93.74030313879936, r: 0.955008902806622
06/02/2019 01:39:45 step: 8628, epoch: 261, batch: 14, loss: 0.019596513360738754, acc: 95.3125, f1: 97.14285714285714, r: 0.9490773934182001
06/02/2019 01:39:45 step: 8633, epoch: 261, batch: 19, loss: 0.02103998139500618, acc: 92.1875, f1: 85.77097505668935, r: 0.9786285883825095
06/02/2019 01:39:46 step: 8638, epoch: 261, batch: 24, loss: 0.022581471130251884, acc: 92.1875, f1: 92.38568470866608, r: 0.9265742379218622
06/02/2019 01:39:46 step: 8643, epoch: 261, batch: 29, loss: 0.023847287520766258, acc: 81.25, f1: 64.04551467051466, r: 0.9726065049425753
06/02/2019 01:39:47 *** evaluating ***
06/02/2019 01:39:47 step: 262, epoch: 261, acc: 61.53846153846154, f1: 28.014170547953753, r: 0.4105073215536252
06/02/2019 01:39:47 *** epoch: 263 ***
06/02/2019 01:39:47 *** training ***
06/02/2019 01:39:47 step: 8651, epoch: 262, batch: 4, loss: 0.01870006136596203, acc: 93.75, f1: 89.37180613872343, r: 0.9786459067101403
06/02/2019 01:39:48 step: 8656, epoch: 262, batch: 9, loss: 0.02288050949573517, acc: 90.625, f1: 73.69047619047619, r: 0.9738824583673968
06/02/2019 01:39:48 step: 8661, epoch: 262, batch: 14, loss: 0.024684153497219086, acc: 84.375, f1: 70.71446496912336, r: 0.93474697616342
06/02/2019 01:39:49 step: 8666, epoch: 262, batch: 19, loss: 0.020358307287096977, acc: 95.3125, f1: 95.98214285714286, r: 0.9777385070400176
06/02/2019 01:39:49 step: 8671, epoch: 262, batch: 24, loss: 0.02105765789747238, acc: 90.625, f1: 80.72278911564625, r: 0.9784236214573971
06/02/2019 01:39:50 step: 8676, epoch: 262, batch: 29, loss: 0.021711865440011024, acc: 87.5, f1: 84.09946967348338, r: 0.9743120248119828
06/02/2019 01:39:50 *** evaluating ***
06/02/2019 01:39:50 step: 263, epoch: 262, acc: 62.82051282051282, f1: 29.968219098831383, r: 0.4088515733763839
06/02/2019 01:39:50 *** epoch: 264 ***
06/02/2019 01:39:50 *** training ***
06/02/2019 01:39:50 step: 8684, epoch: 263, batch: 4, loss: 0.022661225870251656, acc: 87.5, f1: 84.9931585386391, r: 0.9803439726194494
06/02/2019 01:39:51 step: 8689, epoch: 263, batch: 9, loss: 0.021175963804125786, acc: 93.75, f1: 79.85347985347985, r: 0.9732310033091054
06/02/2019 01:39:51 step: 8694, epoch: 263, batch: 14, loss: 0.0188897717744112, acc: 95.3125, f1: 94.77754237288136, r: 0.9569337701314511
06/02/2019 01:39:52 step: 8699, epoch: 263, batch: 19, loss: 0.020321477204561234, acc: 90.625, f1: 84.63317499791663, r: 0.9752406735932674
06/02/2019 01:39:52 step: 8704, epoch: 263, batch: 24, loss: 0.02148056961596012, acc: 98.4375, f1: 99.01258162127728, r: 0.9807700007197081
06/02/2019 01:39:53 step: 8709, epoch: 263, batch: 29, loss: 0.018306702375411987, acc: 100.0, f1: 100.0, r: 0.9870819220637579
06/02/2019 01:39:53 *** evaluating ***
06/02/2019 01:39:53 step: 264, epoch: 263, acc: 63.67521367521367, f1: 30.513141426783484, r: 0.40538085400872836
06/02/2019 01:39:53 *** epoch: 265 ***
06/02/2019 01:39:53 *** training ***
06/02/2019 01:39:53 step: 8717, epoch: 264, batch: 4, loss: 0.022548453882336617, acc: 90.625, f1: 70.46387520525451, r: 0.9679121339048044
06/02/2019 01:39:54 step: 8722, epoch: 264, batch: 9, loss: 0.024898415431380272, acc: 90.625, f1: 85.27354288223854, r: 0.9363410665650095
06/02/2019 01:39:54 step: 8727, epoch: 264, batch: 14, loss: 0.022450458258390427, acc: 89.0625, f1: 78.13262864954595, r: 0.9572808123821885
06/02/2019 01:39:55 step: 8732, epoch: 264, batch: 19, loss: 0.01736147329211235, acc: 87.5, f1: 73.7497982243745, r: 0.9800662835868739
06/02/2019 01:39:55 step: 8737, epoch: 264, batch: 24, loss: 0.021812565624713898, acc: 89.0625, f1: 76.22735507246377, r: 0.9750482883283336
06/02/2019 01:39:56 step: 8742, epoch: 264, batch: 29, loss: 0.022670630365610123, acc: 90.625, f1: 90.5908517253055, r: 0.9713576051624329
06/02/2019 01:39:56 *** evaluating ***
06/02/2019 01:39:56 step: 265, epoch: 264, acc: 61.53846153846154, f1: 28.130067065441732, r: 0.4050481958608405
06/02/2019 01:39:56 *** epoch: 266 ***
06/02/2019 01:39:56 *** training ***
06/02/2019 01:39:57 step: 8750, epoch: 265, batch: 4, loss: 0.021820498630404472, acc: 93.75, f1: 81.64424860853433, r: 0.9759402258601625
06/02/2019 01:39:57 step: 8755, epoch: 265, batch: 9, loss: 0.022451013326644897, acc: 93.75, f1: 90.40909912352038, r: 0.9755551692599777
06/02/2019 01:39:57 step: 8760, epoch: 265, batch: 14, loss: 0.02250005677342415, acc: 85.9375, f1: 71.44403215831787, r: 0.97559286678805
06/02/2019 01:39:58 step: 8765, epoch: 265, batch: 19, loss: 0.01946944184601307, acc: 89.0625, f1: 87.10208502345964, r: 0.9770003591304932
06/02/2019 01:39:58 step: 8770, epoch: 265, batch: 24, loss: 0.020805129781365395, acc: 90.625, f1: 85.57005494505493, r: 0.9802093997271543
06/02/2019 01:39:59 step: 8775, epoch: 265, batch: 29, loss: 0.022127464413642883, acc: 89.0625, f1: 89.52564193502624, r: 0.9777468976381508
06/02/2019 01:39:59 *** evaluating ***
06/02/2019 01:39:59 step: 266, epoch: 265, acc: 63.24786324786324, f1: 31.522364025176174, r: 0.4139859536845173
06/02/2019 01:39:59 *** epoch: 267 ***
06/02/2019 01:39:59 *** training ***
06/02/2019 01:40:00 step: 8783, epoch: 266, batch: 4, loss: 0.021209029480814934, acc: 92.1875, f1: 77.02313693839118, r: 0.97415443664073
06/02/2019 01:40:00 step: 8788, epoch: 266, batch: 9, loss: 0.021832318976521492, acc: 90.625, f1: 86.55969634230503, r: 0.982890900038172
06/02/2019 01:40:01 step: 8793, epoch: 266, batch: 14, loss: 0.022452857345342636, acc: 93.75, f1: 88.8525641025641, r: 0.9782373810883791
06/02/2019 01:40:01 step: 8798, epoch: 266, batch: 19, loss: 0.020161665976047516, acc: 93.75, f1: 95.6306990881459, r: 0.980861834230531
06/02/2019 01:40:01 step: 8803, epoch: 266, batch: 24, loss: 0.01945769414305687, acc: 96.875, f1: 97.72048611111111, r: 0.9823298623013812
06/02/2019 01:40:02 step: 8808, epoch: 266, batch: 29, loss: 0.021849777549505234, acc: 93.75, f1: 77.4962207105064, r: 0.9784103256524991
06/02/2019 01:40:02 *** evaluating ***
06/02/2019 01:40:02 step: 267, epoch: 266, acc: 61.965811965811966, f1: 29.438479335366495, r: 0.4089886604628815
06/02/2019 01:40:02 *** epoch: 268 ***
06/02/2019 01:40:02 *** training ***
06/02/2019 01:40:03 step: 8816, epoch: 267, batch: 4, loss: 0.022399622946977615, acc: 92.1875, f1: 90.91387890014893, r: 0.979353276035894
06/02/2019 01:40:03 step: 8821, epoch: 267, batch: 9, loss: 0.017529839649796486, acc: 87.5, f1: 76.03260869565219, r: 0.9858365287362896
06/02/2019 01:40:04 step: 8826, epoch: 267, batch: 14, loss: 0.018567649647593498, acc: 93.75, f1: 90.8921845574388, r: 0.9823851226574896
06/02/2019 01:40:04 step: 8831, epoch: 267, batch: 19, loss: 0.0212208554148674, acc: 89.0625, f1: 68.09656538430124, r: 0.9742740453612333
06/02/2019 01:40:04 step: 8836, epoch: 267, batch: 24, loss: 0.01957700215280056, acc: 95.3125, f1: 94.29516250944823, r: 0.9693738390607767
06/02/2019 01:40:05 step: 8841, epoch: 267, batch: 29, loss: 0.020648259669542313, acc: 90.625, f1: 86.1598124098124, r: 0.9850969659593318
06/02/2019 01:40:05 *** evaluating ***
06/02/2019 01:40:05 step: 268, epoch: 267, acc: 63.24786324786324, f1: 30.456680726765416, r: 0.4072908176796305
06/02/2019 01:40:05 *** epoch: 269 ***
06/02/2019 01:40:05 *** training ***
06/02/2019 01:40:06 step: 8849, epoch: 268, batch: 4, loss: 0.02190295234322548, acc: 87.5, f1: 78.74380836792488, r: 0.9600578268080132
06/02/2019 01:40:06 step: 8854, epoch: 268, batch: 9, loss: 0.023836374282836914, acc: 92.1875, f1: 90.81270930327534, r: 0.9000192730605772
06/02/2019 01:40:07 step: 8859, epoch: 268, batch: 14, loss: 0.02081795036792755, acc: 89.0625, f1: 74.39991925286043, r: 0.972300119896961
06/02/2019 01:40:07 step: 8864, epoch: 268, batch: 19, loss: 0.021013131365180016, acc: 81.25, f1: 71.06646434727026, r: 0.9721284066391382
06/02/2019 01:40:08 step: 8869, epoch: 268, batch: 24, loss: 0.021787520498037338, acc: 87.5, f1: 84.40552503052503, r: 0.981233479517641
06/02/2019 01:40:08 step: 8874, epoch: 268, batch: 29, loss: 0.020770058035850525, acc: 92.1875, f1: 93.06612475498673, r: 0.9763344496994596
06/02/2019 01:40:08 *** evaluating ***
06/02/2019 01:40:09 step: 269, epoch: 268, acc: 61.965811965811966, f1: 28.8296800815817, r: 0.40196552668198116
06/02/2019 01:40:09 *** epoch: 270 ***
06/02/2019 01:40:09 *** training ***
06/02/2019 01:40:09 step: 8882, epoch: 269, batch: 4, loss: 0.023109443485736847, acc: 87.5, f1: 87.43537414965986, r: 0.9702811331569913
06/02/2019 01:40:09 step: 8887, epoch: 269, batch: 9, loss: 0.02246594987809658, acc: 89.0625, f1: 87.95292207792207, r: 0.9783252178851545
06/02/2019 01:40:10 step: 8892, epoch: 269, batch: 14, loss: 0.019235173240303993, acc: 89.0625, f1: 72.32001854062324, r: 0.9792135924018572
06/02/2019 01:40:10 step: 8897, epoch: 269, batch: 19, loss: 0.019739078357815742, acc: 87.5, f1: 70.9297052154195, r: 0.9693421422290888
06/02/2019 01:40:11 step: 8902, epoch: 269, batch: 24, loss: 0.02012445777654648, acc: 93.75, f1: 91.34920634920634, r: 0.9767117075640894
06/02/2019 01:40:11 step: 8907, epoch: 269, batch: 29, loss: 0.01861466094851494, acc: 87.5, f1: 87.44969758238224, r: 0.9781960265470778
06/02/2019 01:40:11 *** evaluating ***
06/02/2019 01:40:12 step: 270, epoch: 269, acc: 61.965811965811966, f1: 29.389461311368002, r: 0.40525175253567536
06/02/2019 01:40:12 *** epoch: 271 ***
06/02/2019 01:40:12 *** training ***
06/02/2019 01:40:12 step: 8915, epoch: 270, batch: 4, loss: 0.02048858441412449, acc: 93.75, f1: 88.30357142857143, r: 0.9796349187326336
06/02/2019 01:40:13 step: 8920, epoch: 270, batch: 9, loss: 0.021780170500278473, acc: 87.5, f1: 86.33744213865918, r: 0.9723372062255793
06/02/2019 01:40:13 step: 8925, epoch: 270, batch: 14, loss: 0.021500101312994957, acc: 90.625, f1: 86.43830128205127, r: 0.975849899813433
06/02/2019 01:40:13 step: 8930, epoch: 270, batch: 19, loss: 0.0213240385055542, acc: 93.75, f1: 90.98290598290598, r: 0.9761568213809669
06/02/2019 01:40:14 step: 8935, epoch: 270, batch: 24, loss: 0.021136928349733353, acc: 92.1875, f1: 87.65159652377697, r: 0.9732562356543596
06/02/2019 01:40:14 step: 8940, epoch: 270, batch: 29, loss: 0.01883860118687153, acc: 84.375, f1: 83.45899470899471, r: 0.9643714546958502
06/02/2019 01:40:14 *** evaluating ***
06/02/2019 01:40:15 step: 271, epoch: 270, acc: 61.965811965811966, f1: 29.692023897886067, r: 0.4072347211594338
06/02/2019 01:40:15 *** epoch: 272 ***
06/02/2019 01:40:15 *** training ***
06/02/2019 01:40:15 step: 8948, epoch: 271, batch: 4, loss: 0.01961965300142765, acc: 95.3125, f1: 79.93010752688173, r: 0.9777161568509143
06/02/2019 01:40:16 step: 8953, epoch: 271, batch: 9, loss: 0.01787918247282505, acc: 87.5, f1: 79.24695459579179, r: 0.9813924579671734
06/02/2019 01:40:16 step: 8958, epoch: 271, batch: 14, loss: 0.021206490695476532, acc: 95.3125, f1: 95.59875832053251, r: 0.9832381029086588
06/02/2019 01:40:16 step: 8963, epoch: 271, batch: 19, loss: 0.023603662848472595, acc: 95.3125, f1: 84.15178571428572, r: 0.9742813473867588
06/02/2019 01:40:17 step: 8968, epoch: 271, batch: 24, loss: 0.021342776715755463, acc: 90.625, f1: 77.92002734107997, r: 0.9779479174108023
06/02/2019 01:40:17 step: 8973, epoch: 271, batch: 29, loss: 0.019261518493294716, acc: 95.3125, f1: 93.96367521367522, r: 0.9829546461898973
06/02/2019 01:40:18 *** evaluating ***
06/02/2019 01:40:18 step: 272, epoch: 271, acc: 61.965811965811966, f1: 29.235724218119262, r: 0.4117240648737328
06/02/2019 01:40:18 *** epoch: 273 ***
06/02/2019 01:40:18 *** training ***
06/02/2019 01:40:18 step: 8981, epoch: 272, batch: 4, loss: 0.022814519703388214, acc: 87.5, f1: 76.32493655460445, r: 0.9760252610117734
06/02/2019 01:40:19 step: 8986, epoch: 272, batch: 9, loss: 0.020694047212600708, acc: 89.0625, f1: 88.32955404383975, r: 0.9763741114450486
06/02/2019 01:40:19 step: 8991, epoch: 272, batch: 14, loss: 0.02265569567680359, acc: 90.625, f1: 85.92083210095633, r: 0.9630547205840196
06/02/2019 01:40:20 step: 8996, epoch: 272, batch: 19, loss: 0.01983596384525299, acc: 95.3125, f1: 71.47186147186147, r: 0.9773447239282261
06/02/2019 01:40:20 step: 9001, epoch: 272, batch: 24, loss: 0.01971239037811756, acc: 87.5, f1: 86.09929078014184, r: 0.9787685832533701
06/02/2019 01:40:21 step: 9006, epoch: 272, batch: 29, loss: 0.019603004679083824, acc: 95.3125, f1: 94.16666666666667, r: 0.9796997017603644
06/02/2019 01:40:21 *** evaluating ***
06/02/2019 01:40:21 step: 273, epoch: 272, acc: 62.82051282051282, f1: 29.576079507167357, r: 0.40765648938109444
06/02/2019 01:40:21 *** epoch: 274 ***
06/02/2019 01:40:21 *** training ***
06/02/2019 01:40:21 step: 9014, epoch: 273, batch: 4, loss: 0.025785274803638458, acc: 92.1875, f1: 80.39424860853433, r: 0.9693316926749043
06/02/2019 01:40:22 step: 9019, epoch: 273, batch: 9, loss: 0.02053486742079258, acc: 96.875, f1: 96.98595979137676, r: 0.9719973077565262
06/02/2019 01:40:22 step: 9024, epoch: 273, batch: 14, loss: 0.020654164254665375, acc: 92.1875, f1: 91.28166160081054, r: 0.9828943919659229
06/02/2019 01:40:23 step: 9029, epoch: 273, batch: 19, loss: 0.021220309659838676, acc: 93.75, f1: 88.36175398675398, r: 0.9821054077564992
06/02/2019 01:40:23 step: 9034, epoch: 273, batch: 24, loss: 0.021108156070113182, acc: 89.0625, f1: 84.94047619047619, r: 0.9760045623177324
06/02/2019 01:40:24 step: 9039, epoch: 273, batch: 29, loss: 0.022651202976703644, acc: 92.1875, f1: 78.59998043893333, r: 0.9738150609416047
06/02/2019 01:40:24 *** evaluating ***
06/02/2019 01:40:24 step: 274, epoch: 273, acc: 61.965811965811966, f1: 28.399228066645332, r: 0.40848572647861814
06/02/2019 01:40:24 *** epoch: 275 ***
06/02/2019 01:40:24 *** training ***
06/02/2019 01:40:24 step: 9047, epoch: 274, batch: 4, loss: 0.018966523930430412, acc: 89.0625, f1: 87.53816359079518, r: 0.9871569245541274
06/02/2019 01:40:25 step: 9052, epoch: 274, batch: 9, loss: 0.020268455147743225, acc: 95.3125, f1: 95.5829420970266, r: 0.9791065487999386
06/02/2019 01:40:25 step: 9057, epoch: 274, batch: 14, loss: 0.02055278979241848, acc: 90.625, f1: 89.72789115646258, r: 0.9750894179633509
06/02/2019 01:40:26 step: 9062, epoch: 274, batch: 19, loss: 0.023052603006362915, acc: 93.75, f1: 93.25334224598932, r: 0.9782268355408039
06/02/2019 01:40:26 step: 9067, epoch: 274, batch: 24, loss: 0.02085012011229992, acc: 93.75, f1: 83.94322344322345, r: 0.9790901390785738
06/02/2019 01:40:27 step: 9072, epoch: 274, batch: 29, loss: 0.01775379665195942, acc: 96.875, f1: 95.6484295845998, r: 0.9778993282509627
06/02/2019 01:40:27 *** evaluating ***
06/02/2019 01:40:27 step: 275, epoch: 274, acc: 62.82051282051282, f1: 30.333234856442576, r: 0.4056545560009974
06/02/2019 01:40:27 *** epoch: 276 ***
06/02/2019 01:40:27 *** training ***
06/02/2019 01:40:28 step: 9080, epoch: 275, batch: 4, loss: 0.019948728382587433, acc: 90.625, f1: 91.76046176046175, r: 0.9862058832797186
06/02/2019 01:40:28 step: 9085, epoch: 275, batch: 9, loss: 0.022552339360117912, acc: 93.75, f1: 82.15151515151516, r: 0.9767867538464321
06/02/2019 01:40:29 step: 9090, epoch: 275, batch: 14, loss: 0.020430592820048332, acc: 92.1875, f1: 78.77497563352827, r: 0.9786319844536454
06/02/2019 01:40:29 step: 9095, epoch: 275, batch: 19, loss: 0.02024860866367817, acc: 89.0625, f1: 75.65226848315083, r: 0.978586131933713
06/02/2019 01:40:30 step: 9100, epoch: 275, batch: 24, loss: 0.02252398431301117, acc: 93.75, f1: 89.16316881362472, r: 0.9784947641665123
06/02/2019 01:40:30 step: 9105, epoch: 275, batch: 29, loss: 0.021671563386917114, acc: 95.3125, f1: 80.51470588235294, r: 0.9771685667032252
06/02/2019 01:40:30 *** evaluating ***
06/02/2019 01:40:30 step: 276, epoch: 275, acc: 60.68376068376068, f1: 27.32881480221906, r: 0.40843510204687133
06/02/2019 01:40:30 *** epoch: 277 ***
06/02/2019 01:40:30 *** training ***
06/02/2019 01:40:31 step: 9113, epoch: 276, batch: 4, loss: 0.01919604279100895, acc: 92.1875, f1: 88.94208965637537, r: 0.9715197572545573
06/02/2019 01:40:31 step: 9118, epoch: 276, batch: 9, loss: 0.018313687294721603, acc: 95.3125, f1: 91.66241496598639, r: 0.976672741486531
06/02/2019 01:40:32 step: 9123, epoch: 276, batch: 14, loss: 0.019631560891866684, acc: 90.625, f1: 81.21247412008282, r: 0.9777429146519161
06/02/2019 01:40:32 step: 9128, epoch: 276, batch: 19, loss: 0.020824624225497246, acc: 95.3125, f1: 92.13007265638845, r: 0.9731479529756971
06/02/2019 01:40:33 step: 9133, epoch: 276, batch: 24, loss: 0.021536728367209435, acc: 89.0625, f1: 73.98843976950047, r: 0.9751333038374295
06/02/2019 01:40:33 step: 9138, epoch: 276, batch: 29, loss: 0.019374733790755272, acc: 85.9375, f1: 79.79043954199234, r: 0.9755510249900138
06/02/2019 01:40:33 *** evaluating ***
06/02/2019 01:40:33 step: 277, epoch: 276, acc: 62.82051282051282, f1: 30.147394957121588, r: 0.40853898929550025
06/02/2019 01:40:33 *** epoch: 278 ***
06/02/2019 01:40:33 *** training ***
06/02/2019 01:40:34 step: 9146, epoch: 277, batch: 4, loss: 0.016966689378023148, acc: 93.75, f1: 80.83642547928262, r: 0.9828072941415703
06/02/2019 01:40:34 step: 9151, epoch: 277, batch: 9, loss: 0.02153797820210457, acc: 90.625, f1: 77.78246120351383, r: 0.9815127863077192
06/02/2019 01:40:35 step: 9156, epoch: 277, batch: 14, loss: 0.01935540698468685, acc: 92.1875, f1: 76.2797619047619, r: 0.9811474363323603
06/02/2019 01:40:35 step: 9161, epoch: 277, batch: 19, loss: 0.020897455513477325, acc: 92.1875, f1: 82.77791489556195, r: 0.9694100081256052
06/02/2019 01:40:36 step: 9166, epoch: 277, batch: 24, loss: 0.020780015736818314, acc: 85.9375, f1: 85.32362630586826, r: 0.9770489632018518
06/02/2019 01:40:36 step: 9171, epoch: 277, batch: 29, loss: 0.018777921795845032, acc: 93.75, f1: 90.91700473292765, r: 0.9841887595599228
06/02/2019 01:40:36 *** evaluating ***
06/02/2019 01:40:36 step: 278, epoch: 277, acc: 62.39316239316239, f1: 30.250893866566784, r: 0.4110187664214746
06/02/2019 01:40:36 *** epoch: 279 ***
06/02/2019 01:40:36 *** training ***
06/02/2019 01:40:37 step: 9179, epoch: 278, batch: 4, loss: 0.023607630282640457, acc: 93.75, f1: 81.36525341130604, r: 0.9680900265038334
06/02/2019 01:40:37 step: 9184, epoch: 278, batch: 9, loss: 0.021037081256508827, acc: 90.625, f1: 86.01279717284216, r: 0.9785564774342036
06/02/2019 01:40:38 step: 9189, epoch: 278, batch: 14, loss: 0.018141116946935654, acc: 93.75, f1: 90.92233163661734, r: 0.9777055063310045
06/02/2019 01:40:38 step: 9194, epoch: 278, batch: 19, loss: 0.023319337517023087, acc: 85.9375, f1: 85.35019439624702, r: 0.9756297757210655
06/02/2019 01:40:39 step: 9199, epoch: 278, batch: 24, loss: 0.021571312099695206, acc: 93.75, f1: 92.17171717171718, r: 0.9758615988676882
06/02/2019 01:40:39 step: 9204, epoch: 278, batch: 29, loss: 0.021937640383839607, acc: 95.3125, f1: 95.83091436865023, r: 0.9786859463974305
06/02/2019 01:40:39 *** evaluating ***
06/02/2019 01:40:40 step: 279, epoch: 278, acc: 61.111111111111114, f1: 28.030108648658807, r: 0.41108683600289303
06/02/2019 01:40:40 *** epoch: 280 ***
06/02/2019 01:40:40 *** training ***
06/02/2019 01:40:40 step: 9212, epoch: 279, batch: 4, loss: 0.023061666637659073, acc: 84.375, f1: 81.02283337577457, r: 0.9701788182603248
06/02/2019 01:40:41 step: 9217, epoch: 279, batch: 9, loss: 0.01900225691497326, acc: 89.0625, f1: 79.1802581023482, r: 0.9809671260377268
06/02/2019 01:40:41 step: 9222, epoch: 279, batch: 14, loss: 0.02379433810710907, acc: 84.375, f1: 68.50318662818663, r: 0.950015803809327
06/02/2019 01:40:42 step: 9227, epoch: 279, batch: 19, loss: 0.018968068063259125, acc: 90.625, f1: 67.39376523859282, r: 0.9766188666527617
06/02/2019 01:40:42 step: 9232, epoch: 279, batch: 24, loss: 0.01986592821776867, acc: 93.75, f1: 90.86447178552442, r: 0.9790906394275465
06/02/2019 01:40:43 step: 9237, epoch: 279, batch: 29, loss: 0.018987273797392845, acc: 93.75, f1: 93.86363389018881, r: 0.9751435190073341
06/02/2019 01:40:43 *** evaluating ***
06/02/2019 01:40:43 step: 280, epoch: 279, acc: 63.24786324786324, f1: 29.95539875414297, r: 0.4079746129729999
06/02/2019 01:40:43 *** epoch: 281 ***
06/02/2019 01:40:43 *** training ***
06/02/2019 01:40:43 step: 9245, epoch: 280, batch: 4, loss: 0.021915121003985405, acc: 89.0625, f1: 87.44751082251082, r: 0.9761724836747355
06/02/2019 01:40:44 step: 9250, epoch: 280, batch: 9, loss: 0.021542910486459732, acc: 89.0625, f1: 86.10248700954524, r: 0.9718845304291702
06/02/2019 01:40:44 step: 9255, epoch: 280, batch: 14, loss: 0.021583568304777145, acc: 93.75, f1: 92.27844984147505, r: 0.9767207514816807
06/02/2019 01:40:45 step: 9260, epoch: 280, batch: 19, loss: 0.0196633692830801, acc: 90.625, f1: 84.16613957410921, r: 0.9786966094737273
06/02/2019 01:40:45 step: 9265, epoch: 280, batch: 24, loss: 0.021352147683501244, acc: 89.0625, f1: 69.00857654889914, r: 0.936214093322828
06/02/2019 01:40:46 step: 9270, epoch: 280, batch: 29, loss: 0.02290288731455803, acc: 90.625, f1: 71.74358974358974, r: 0.9512644490915677
06/02/2019 01:40:46 *** evaluating ***
06/02/2019 01:40:46 step: 281, epoch: 280, acc: 61.965811965811966, f1: 29.747359644246806, r: 0.40736564339187853
06/02/2019 01:40:46 *** epoch: 282 ***
06/02/2019 01:40:46 *** training ***
06/02/2019 01:40:46 step: 9278, epoch: 281, batch: 4, loss: 0.021456053480505943, acc: 87.5, f1: 78.27146302321107, r: 0.9798748348556374
06/02/2019 01:40:47 step: 9283, epoch: 281, batch: 9, loss: 0.018447216600179672, acc: 89.0625, f1: 91.92786309926271, r: 0.9761873182960544
06/02/2019 01:40:47 step: 9288, epoch: 281, batch: 14, loss: 0.018902279436588287, acc: 89.0625, f1: 84.09377156659765, r: 0.985342839256501
06/02/2019 01:40:48 step: 9293, epoch: 281, batch: 19, loss: 0.0205831415951252, acc: 90.625, f1: 82.65604262886872, r: 0.9782939860708009
06/02/2019 01:40:48 step: 9298, epoch: 281, batch: 24, loss: 0.020193947479128838, acc: 93.75, f1: 88.74551971326164, r: 0.9751625517096634
06/02/2019 01:40:49 step: 9303, epoch: 281, batch: 29, loss: 0.01961110159754753, acc: 85.9375, f1: 82.84941520467837, r: 0.984640327630351
06/02/2019 01:40:49 *** evaluating ***
06/02/2019 01:40:49 step: 282, epoch: 281, acc: 61.53846153846154, f1: 28.067690241603284, r: 0.40503906148231505
06/02/2019 01:40:49 *** epoch: 283 ***
06/02/2019 01:40:49 *** training ***
06/02/2019 01:40:50 step: 9311, epoch: 282, batch: 4, loss: 0.02221827395260334, acc: 89.0625, f1: 88.87260536398468, r: 0.9785110809356427
06/02/2019 01:40:50 step: 9316, epoch: 282, batch: 9, loss: 0.02140560932457447, acc: 85.9375, f1: 69.64471931577194, r: 0.9734830813133822
06/02/2019 01:40:51 step: 9321, epoch: 282, batch: 14, loss: 0.022910945117473602, acc: 93.75, f1: 83.29580745341613, r: 0.9619171896887151
06/02/2019 01:40:51 step: 9326, epoch: 282, batch: 19, loss: 0.019968725740909576, acc: 93.75, f1: 92.11638361638362, r: 0.9821417572032287
06/02/2019 01:40:51 step: 9331, epoch: 282, batch: 24, loss: 0.0187858734279871, acc: 90.625, f1: 89.60567277758561, r: 0.9654283288888684
06/02/2019 01:40:52 step: 9336, epoch: 282, batch: 29, loss: 0.02450825273990631, acc: 92.1875, f1: 81.64229024943312, r: 0.9722959787213
06/02/2019 01:40:52 *** evaluating ***
06/02/2019 01:40:52 step: 283, epoch: 282, acc: 60.256410256410255, f1: 26.22824934119587, r: 0.4062429194126309
06/02/2019 01:40:52 *** epoch: 284 ***
06/02/2019 01:40:52 *** training ***
06/02/2019 01:40:53 step: 9344, epoch: 283, batch: 4, loss: 0.020010052248835564, acc: 85.9375, f1: 73.89917100443417, r: 0.9705405874972328
06/02/2019 01:40:53 step: 9349, epoch: 283, batch: 9, loss: 0.019246134907007217, acc: 90.625, f1: 84.88783072413759, r: 0.9809907811708738
06/02/2019 01:40:54 step: 9354, epoch: 283, batch: 14, loss: 0.023119010031223297, acc: 87.5, f1: 65.21428571428571, r: 0.9723655957808
06/02/2019 01:40:54 step: 9359, epoch: 283, batch: 19, loss: 0.021161025390028954, acc: 93.75, f1: 93.22205572205571, r: 0.9610851374757273
06/02/2019 01:40:55 step: 9364, epoch: 283, batch: 24, loss: 0.023424264043569565, acc: 90.625, f1: 75.38368377654092, r: 0.973352458715016
06/02/2019 01:40:55 step: 9369, epoch: 283, batch: 29, loss: 0.023383479565382004, acc: 93.75, f1: 91.92343604108311, r: 0.9686329132996301
06/02/2019 01:40:55 *** evaluating ***
06/02/2019 01:40:55 step: 284, epoch: 283, acc: 61.53846153846154, f1: 29.155141333203062, r: 0.4086307488044989
06/02/2019 01:40:55 *** epoch: 285 ***
06/02/2019 01:40:55 *** training ***
06/02/2019 01:40:56 step: 9377, epoch: 284, batch: 4, loss: 0.019539250060915947, acc: 92.1875, f1: 81.78062678062679, r: 0.9725134841445886
06/02/2019 01:40:56 step: 9382, epoch: 284, batch: 9, loss: 0.023406561464071274, acc: 89.0625, f1: 85.98523169951741, r: 0.943347220886374
06/02/2019 01:40:57 step: 9387, epoch: 284, batch: 14, loss: 0.023232929408550262, acc: 90.625, f1: 86.8287037037037, r: 0.9752462706893268
06/02/2019 01:40:57 step: 9392, epoch: 284, batch: 19, loss: 0.019427461549639702, acc: 89.0625, f1: 87.47252747252746, r: 0.9767140661058669
06/02/2019 01:40:58 step: 9397, epoch: 284, batch: 24, loss: 0.020473381504416466, acc: 90.625, f1: 83.52693862690967, r: 0.9793039446934588
06/02/2019 01:40:58 step: 9402, epoch: 284, batch: 29, loss: 0.020938072353601456, acc: 85.9375, f1: 67.30085390799675, r: 0.9582198395091847
06/02/2019 01:40:58 *** evaluating ***
06/02/2019 01:40:58 step: 285, epoch: 284, acc: 62.82051282051282, f1: 31.255070738654865, r: 0.40877502881599065
06/02/2019 01:40:58 *** epoch: 286 ***
06/02/2019 01:40:58 *** training ***
06/02/2019 01:40:59 step: 9410, epoch: 285, batch: 4, loss: 0.021329080685973167, acc: 92.1875, f1: 91.1451247165533, r: 0.9705127880784417
06/02/2019 01:40:59 step: 9415, epoch: 285, batch: 9, loss: 0.02116662636399269, acc: 92.1875, f1: 86.39631610219845, r: 0.9651241523183001
06/02/2019 01:41:00 step: 9420, epoch: 285, batch: 14, loss: 0.0201889518648386, acc: 93.75, f1: 91.9654113639076, r: 0.9800615227471675
06/02/2019 01:41:00 step: 9425, epoch: 285, batch: 19, loss: 0.018469206988811493, acc: 85.9375, f1: 80.48148148148148, r: 0.9846217868478198
06/02/2019 01:41:01 step: 9430, epoch: 285, batch: 24, loss: 0.022361380979418755, acc: 93.75, f1: 84.65229771620749, r: 0.9671902473973379
06/02/2019 01:41:01 step: 9435, epoch: 285, batch: 29, loss: 0.01910828799009323, acc: 92.1875, f1: 76.8162393162393, r: 0.9862259460200404
06/02/2019 01:41:02 *** evaluating ***
06/02/2019 01:41:02 step: 286, epoch: 285, acc: 60.68376068376068, f1: 27.471687962650343, r: 0.41068885074855904
06/02/2019 01:41:02 *** epoch: 287 ***
06/02/2019 01:41:02 *** training ***
06/02/2019 01:41:02 step: 9443, epoch: 286, batch: 4, loss: 0.021322375163435936, acc: 92.1875, f1: 84.43102240896359, r: 0.9673346384892207
06/02/2019 01:41:03 step: 9448, epoch: 286, batch: 9, loss: 0.020732402801513672, acc: 90.625, f1: 87.23281926406928, r: 0.9801752910878577
06/02/2019 01:41:03 step: 9453, epoch: 286, batch: 14, loss: 0.019802231341600418, acc: 87.5, f1: 78.16899585921324, r: 0.9847614186523092
06/02/2019 01:41:04 step: 9458, epoch: 286, batch: 19, loss: 0.020566977560520172, acc: 89.0625, f1: 74.04930450906339, r: 0.9800984216384401
06/02/2019 01:41:04 step: 9463, epoch: 286, batch: 24, loss: 0.022595861926674843, acc: 84.375, f1: 78.44723619659682, r: 0.9682392595263073
06/02/2019 01:41:04 step: 9468, epoch: 286, batch: 29, loss: 0.019304189831018448, acc: 95.3125, f1: 93.71428571428572, r: 0.9798993461359322
06/02/2019 01:41:05 *** evaluating ***
06/02/2019 01:41:05 step: 287, epoch: 286, acc: 63.24786324786324, f1: 30.086595900762102, r: 0.4080589075165566
06/02/2019 01:41:05 *** epoch: 288 ***
06/02/2019 01:41:05 *** training ***
06/02/2019 01:41:05 step: 9476, epoch: 287, batch: 4, loss: 0.018060648813843727, acc: 90.625, f1: 84.7687851364322, r: 0.9836128926250344
06/02/2019 01:41:06 step: 9481, epoch: 287, batch: 9, loss: 0.023357795551419258, acc: 89.0625, f1: 87.12192569335426, r: 0.9726781949774681
06/02/2019 01:41:06 step: 9486, epoch: 287, batch: 14, loss: 0.02074907347559929, acc: 92.1875, f1: 85.92019695280564, r: 0.9457934190832733
06/02/2019 01:41:07 step: 9491, epoch: 287, batch: 19, loss: 0.022387711331248283, acc: 87.5, f1: 82.47027972027972, r: 0.965601530321244
06/02/2019 01:41:07 step: 9496, epoch: 287, batch: 24, loss: 0.01984897069633007, acc: 92.1875, f1: 76.9835107956226, r: 0.9812100296847684
06/02/2019 01:41:07 step: 9501, epoch: 287, batch: 29, loss: 0.022536706179380417, acc: 92.1875, f1: 91.19609685979655, r: 0.9760225043737627
06/02/2019 01:41:08 *** evaluating ***
06/02/2019 01:41:08 step: 288, epoch: 287, acc: 62.82051282051282, f1: 30.701759071324286, r: 0.41011786571730136
06/02/2019 01:41:08 *** epoch: 289 ***
06/02/2019 01:41:08 *** training ***
06/02/2019 01:41:08 step: 9509, epoch: 288, batch: 4, loss: 0.021155929192900658, acc: 89.0625, f1: 78.57905982905983, r: 0.9664595273144754
06/02/2019 01:41:09 step: 9514, epoch: 288, batch: 9, loss: 0.020955119282007217, acc: 87.5, f1: 75.73332148976739, r: 0.980298661578773
06/02/2019 01:41:09 step: 9519, epoch: 288, batch: 14, loss: 0.02149019204080105, acc: 93.75, f1: 80.90924775707384, r: 0.9723791259998815
06/02/2019 01:41:10 step: 9524, epoch: 288, batch: 19, loss: 0.0220649391412735, acc: 95.3125, f1: 96.04759466510619, r: 0.9729822025721572
06/02/2019 01:41:10 step: 9529, epoch: 288, batch: 24, loss: 0.020284444093704224, acc: 95.3125, f1: 94.34486838213546, r: 0.9782528736857982
06/02/2019 01:41:11 step: 9534, epoch: 288, batch: 29, loss: 0.022843584418296814, acc: 89.0625, f1: 78.19991789819376, r: 0.9782538971572192
06/02/2019 01:41:11 *** evaluating ***
06/02/2019 01:41:11 step: 289, epoch: 288, acc: 61.111111111111114, f1: 26.634941502117286, r: 0.40958449101357386
06/02/2019 01:41:11 *** epoch: 290 ***
06/02/2019 01:41:11 *** training ***
06/02/2019 01:41:11 step: 9542, epoch: 289, batch: 4, loss: 0.018552126362919807, acc: 85.9375, f1: 81.85580798923698, r: 0.9825651565441806
06/02/2019 01:41:12 step: 9547, epoch: 289, batch: 9, loss: 0.022760694846510887, acc: 92.1875, f1: 87.88377871084639, r: 0.9216162588812342
06/02/2019 01:41:12 step: 9552, epoch: 289, batch: 14, loss: 0.021059976890683174, acc: 90.625, f1: 90.86538461538461, r: 0.9786361937842769
06/02/2019 01:41:13 step: 9557, epoch: 289, batch: 19, loss: 0.02147153578698635, acc: 93.75, f1: 92.59938590820944, r: 0.9741369431097464
06/02/2019 01:41:13 step: 9562, epoch: 289, batch: 24, loss: 0.020703958347439766, acc: 100.0, f1: 100.0, r: 0.9799176282104458
06/02/2019 01:41:14 step: 9567, epoch: 289, batch: 29, loss: 0.021331414580345154, acc: 89.0625, f1: 65.8484446410143, r: 0.9765929726461295
06/02/2019 01:41:14 *** evaluating ***
06/02/2019 01:41:14 step: 290, epoch: 289, acc: 60.68376068376068, f1: 26.31870233090508, r: 0.4103839647782485
06/02/2019 01:41:14 *** epoch: 291 ***
06/02/2019 01:41:14 *** training ***
06/02/2019 01:41:15 step: 9575, epoch: 290, batch: 4, loss: 0.019880905747413635, acc: 93.75, f1: 95.8503540305011, r: 0.9746741678088847
06/02/2019 01:41:15 step: 9580, epoch: 290, batch: 9, loss: 0.020898107439279556, acc: 89.0625, f1: 73.21106212083656, r: 0.9740999878965179
06/02/2019 01:41:15 step: 9585, epoch: 290, batch: 14, loss: 0.020945018157362938, acc: 87.5, f1: 82.6767955339384, r: 0.9826749406815953
06/02/2019 01:41:16 step: 9590, epoch: 290, batch: 19, loss: 0.022278957068920135, acc: 85.9375, f1: 67.2821051497522, r: 0.9789454149270201
06/02/2019 01:41:16 step: 9595, epoch: 290, batch: 24, loss: 0.02242423966526985, acc: 92.1875, f1: 86.78312629399586, r: 0.973608601867839
06/02/2019 01:41:17 step: 9600, epoch: 290, batch: 29, loss: 0.021374598145484924, acc: 96.875, f1: 93.9268680445151, r: 0.9839855486073967
06/02/2019 01:41:17 *** evaluating ***
06/02/2019 01:41:17 step: 291, epoch: 290, acc: 61.53846153846154, f1: 29.155727085414583, r: 0.4092235798339101
06/02/2019 01:41:17 *** epoch: 292 ***
06/02/2019 01:41:17 *** training ***
06/02/2019 01:41:18 step: 9608, epoch: 291, batch: 4, loss: 0.022832581773400307, acc: 90.625, f1: 83.83117135743187, r: 0.9637360464431759
06/02/2019 01:41:18 step: 9613, epoch: 291, batch: 9, loss: 0.021814225241541862, acc: 87.5, f1: 87.8670634920635, r: 0.9760144373996305
06/02/2019 01:41:18 step: 9618, epoch: 291, batch: 14, loss: 0.019954713061451912, acc: 92.1875, f1: 86.48347382431233, r: 0.9823539393538154
06/02/2019 01:41:19 step: 9623, epoch: 291, batch: 19, loss: 0.021897803992033005, acc: 93.75, f1: 91.70748299319727, r: 0.9586692319921276
06/02/2019 01:41:19 step: 9628, epoch: 291, batch: 24, loss: 0.020654890686273575, acc: 85.9375, f1: 83.49540573805278, r: 0.9775243265416419
06/02/2019 01:41:20 step: 9633, epoch: 291, batch: 29, loss: 0.02096695452928543, acc: 87.5, f1: 72.23850223850224, r: 0.9746485086021877
06/02/2019 01:41:20 *** evaluating ***
06/02/2019 01:41:20 step: 292, epoch: 291, acc: 61.965811965811966, f1: 28.874384959976496, r: 0.4067750479184099
06/02/2019 01:41:20 *** epoch: 293 ***
06/02/2019 01:41:20 *** training ***
06/02/2019 01:41:21 step: 9641, epoch: 292, batch: 4, loss: 0.01965026743710041, acc: 87.5, f1: 89.5787155441887, r: 0.9850352873648716
06/02/2019 01:41:21 step: 9646, epoch: 292, batch: 9, loss: 0.019915174692869186, acc: 87.5, f1: 84.53788081836862, r: 0.9811552414395673
06/02/2019 01:41:22 step: 9651, epoch: 292, batch: 14, loss: 0.02029665932059288, acc: 89.0625, f1: 85.44146825396825, r: 0.9813418794630457
06/02/2019 01:41:22 step: 9656, epoch: 292, batch: 19, loss: 0.019648414105176926, acc: 95.3125, f1: 94.38709677419355, r: 0.9778687818246978
06/02/2019 01:41:23 step: 9661, epoch: 292, batch: 24, loss: 0.02086244896054268, acc: 100.0, f1: 100.0, r: 0.9699705306887049
06/02/2019 01:41:23 step: 9666, epoch: 292, batch: 29, loss: 0.020496848970651627, acc: 82.8125, f1: 66.20609872889509, r: 0.978107601668576
06/02/2019 01:41:23 *** evaluating ***
06/02/2019 01:41:23 step: 293, epoch: 292, acc: 63.24786324786324, f1: 29.959654014422384, r: 0.41013883618020996
06/02/2019 01:41:23 *** epoch: 294 ***
06/02/2019 01:41:23 *** training ***
06/02/2019 01:41:24 step: 9674, epoch: 293, batch: 4, loss: 0.021015221253037453, acc: 89.0625, f1: 82.88394145537002, r: 0.9600635718984558
06/02/2019 01:41:24 step: 9679, epoch: 293, batch: 9, loss: 0.018524812534451485, acc: 85.9375, f1: 66.62073649683325, r: 0.9702559996997329
06/02/2019 01:41:25 step: 9684, epoch: 293, batch: 14, loss: 0.019947584718465805, acc: 87.5, f1: 71.12885154061624, r: 0.9756521266887008
06/02/2019 01:41:25 step: 9689, epoch: 293, batch: 19, loss: 0.020458247512578964, acc: 95.3125, f1: 86.4333722969024, r: 0.9772121114111656
06/02/2019 01:41:26 step: 9694, epoch: 293, batch: 24, loss: 0.019745387136936188, acc: 92.1875, f1: 94.97495968084203, r: 0.9513306932157517
06/02/2019 01:41:26 step: 9699, epoch: 293, batch: 29, loss: 0.023392710834741592, acc: 89.0625, f1: 89.78372202510133, r: 0.9780914413671404
06/02/2019 01:41:26 *** evaluating ***
06/02/2019 01:41:27 step: 294, epoch: 293, acc: 62.39316239316239, f1: 28.27195824092871, r: 0.41270557351405257
06/02/2019 01:41:27 *** epoch: 295 ***
06/02/2019 01:41:27 *** training ***
06/02/2019 01:41:27 step: 9707, epoch: 294, batch: 4, loss: 0.019835617393255234, acc: 89.0625, f1: 88.46946912736388, r: 0.9816549809647342
06/02/2019 01:41:27 step: 9712, epoch: 294, batch: 9, loss: 0.023828672245144844, acc: 92.1875, f1: 92.55841467021591, r: 0.9504251672975379
06/02/2019 01:41:28 step: 9717, epoch: 294, batch: 14, loss: 0.019194843247532845, acc: 90.625, f1: 86.07711901965087, r: 0.9693660089501963
06/02/2019 01:41:28 step: 9722, epoch: 294, batch: 19, loss: 0.020509323105216026, acc: 98.4375, f1: 98.93081761006289, r: 0.9869989518064339
06/02/2019 01:41:29 step: 9727, epoch: 294, batch: 24, loss: 0.018472755327820778, acc: 92.1875, f1: 81.27795031055899, r: 0.9829807609135549
06/02/2019 01:41:29 step: 9732, epoch: 294, batch: 29, loss: 0.02267255075275898, acc: 95.3125, f1: 95.4811507936508, r: 0.9745894259508293
06/02/2019 01:41:29 *** evaluating ***
06/02/2019 01:41:30 step: 295, epoch: 294, acc: 62.82051282051282, f1: 29.887534256084415, r: 0.4119290945758761
06/02/2019 01:41:30 *** epoch: 296 ***
06/02/2019 01:41:30 *** training ***
06/02/2019 01:41:30 step: 9740, epoch: 295, batch: 4, loss: 0.01751224882900715, acc: 93.75, f1: 93.11514572384138, r: 0.9879018140939884
06/02/2019 01:41:31 step: 9745, epoch: 295, batch: 9, loss: 0.01892985589802265, acc: 90.625, f1: 78.81176999101527, r: 0.9844142870328445
06/02/2019 01:41:31 step: 9750, epoch: 295, batch: 14, loss: 0.018540985882282257, acc: 87.5, f1: 84.94561235865584, r: 0.9868198007087372
06/02/2019 01:41:32 step: 9755, epoch: 295, batch: 19, loss: 0.024917319416999817, acc: 90.625, f1: 83.36936949544709, r: 0.9654705846279246
06/02/2019 01:41:32 step: 9760, epoch: 295, batch: 24, loss: 0.01860656961798668, acc: 93.75, f1: 88.80494505494505, r: 0.9838895636650147
06/02/2019 01:41:33 step: 9765, epoch: 295, batch: 29, loss: 0.01969902589917183, acc: 93.75, f1: 76.90476190476191, r: 0.9678187846124389
06/02/2019 01:41:33 *** evaluating ***
06/02/2019 01:41:33 step: 296, epoch: 295, acc: 61.111111111111114, f1: 26.900653059914635, r: 0.40499650110584867
06/02/2019 01:41:33 *** epoch: 297 ***
06/02/2019 01:41:33 *** training ***
06/02/2019 01:41:33 step: 9773, epoch: 296, batch: 4, loss: 0.021827906370162964, acc: 95.3125, f1: 92.4582560296846, r: 0.9739674360619531
06/02/2019 01:41:34 step: 9778, epoch: 296, batch: 9, loss: 0.022227391600608826, acc: 96.875, f1: 95.60096653918166, r: 0.9495280635039671
06/02/2019 01:41:34 step: 9783, epoch: 296, batch: 14, loss: 0.018699437379837036, acc: 92.1875, f1: 90.48581538471244, r: 0.9856921114610315
06/02/2019 01:41:35 step: 9788, epoch: 296, batch: 19, loss: 0.02310987375676632, acc: 87.5, f1: 85.08297258297259, r: 0.9757110919894655
06/02/2019 01:41:35 step: 9793, epoch: 296, batch: 24, loss: 0.021457336843013763, acc: 95.3125, f1: 88.84920634920636, r: 0.9793211911857809
06/02/2019 01:41:36 step: 9798, epoch: 296, batch: 29, loss: 0.017461085692048073, acc: 85.9375, f1: 67.33341488891189, r: 0.9838965256583575
06/02/2019 01:41:36 *** evaluating ***
06/02/2019 01:41:36 step: 297, epoch: 296, acc: 61.111111111111114, f1: 27.44376322265878, r: 0.4086898506644034
06/02/2019 01:41:36 *** epoch: 298 ***
06/02/2019 01:41:36 *** training ***
06/02/2019 01:41:37 step: 9806, epoch: 297, batch: 4, loss: 0.020687047392129898, acc: 90.625, f1: 86.17188970449841, r: 0.9803839677977632
06/02/2019 01:41:37 step: 9811, epoch: 297, batch: 9, loss: 0.022265680134296417, acc: 95.3125, f1: 93.12998405103669, r: 0.9732890814344828
06/02/2019 01:41:38 step: 9816, epoch: 297, batch: 14, loss: 0.019195491448044777, acc: 89.0625, f1: 77.9672886103594, r: 0.9627322981415036
06/02/2019 01:41:38 step: 9821, epoch: 297, batch: 19, loss: 0.01875573769211769, acc: 93.75, f1: 89.72692902297531, r: 0.9569402236122506
06/02/2019 01:41:38 step: 9826, epoch: 297, batch: 24, loss: 0.01940351165831089, acc: 90.625, f1: 85.38919413919415, r: 0.9833142729757396
06/02/2019 01:41:39 step: 9831, epoch: 297, batch: 29, loss: 0.020974835380911827, acc: 87.5, f1: 86.78189865689865, r: 0.9708617698843297
06/02/2019 01:41:39 *** evaluating ***
06/02/2019 01:41:39 step: 298, epoch: 297, acc: 61.965811965811966, f1: 28.802642104272536, r: 0.41054425495821556
06/02/2019 01:41:39 *** epoch: 299 ***
06/02/2019 01:41:39 *** training ***
06/02/2019 01:41:40 step: 9839, epoch: 298, batch: 4, loss: 0.020227840170264244, acc: 84.375, f1: 76.20970502365851, r: 0.9832194146874769
06/02/2019 01:41:40 step: 9844, epoch: 298, batch: 9, loss: 0.019599247723817825, acc: 93.75, f1: 84.49167791273054, r: 0.9760817168092066
06/02/2019 01:41:41 step: 9849, epoch: 298, batch: 14, loss: 0.021487031131982803, acc: 87.5, f1: 80.1992497748301, r: 0.9740378307229113
06/02/2019 01:41:41 step: 9854, epoch: 298, batch: 19, loss: 0.01773475855588913, acc: 87.5, f1: 78.24111652236653, r: 0.984880418550946
06/02/2019 01:41:42 step: 9859, epoch: 298, batch: 24, loss: 0.01856370083987713, acc: 93.75, f1: 95.14285714285714, r: 0.9724519925597821
06/02/2019 01:41:42 step: 9864, epoch: 298, batch: 29, loss: 0.02258799411356449, acc: 87.5, f1: 83.53693013975271, r: 0.9764066643297513
06/02/2019 01:41:42 *** evaluating ***
06/02/2019 01:41:43 step: 299, epoch: 298, acc: 60.256410256410255, f1: 26.564469365221242, r: 0.412848081516534
06/02/2019 01:41:43 *** epoch: 300 ***
06/02/2019 01:41:43 *** training ***
06/02/2019 01:41:43 step: 9872, epoch: 299, batch: 4, loss: 0.02253871224820614, acc: 90.625, f1: 66.5151515151515, r: 0.9707988618489525
06/02/2019 01:41:44 step: 9877, epoch: 299, batch: 9, loss: 0.020621808245778084, acc: 92.1875, f1: 80.24458874458874, r: 0.9803394787349841
06/02/2019 01:41:44 step: 9882, epoch: 299, batch: 14, loss: 0.01976366713643074, acc: 90.625, f1: 77.19966541395112, r: 0.981521136733813
06/02/2019 01:41:44 step: 9887, epoch: 299, batch: 19, loss: 0.019137833267450333, acc: 93.75, f1: 94.89441609977324, r: 0.9838879649755804
06/02/2019 01:41:45 step: 9892, epoch: 299, batch: 24, loss: 0.021122224628925323, acc: 93.75, f1: 83.95484697723504, r: 0.9804732935854592
06/02/2019 01:41:45 step: 9897, epoch: 299, batch: 29, loss: 0.01883181743323803, acc: 92.1875, f1: 90.97576316269603, r: 0.98623803187468
06/02/2019 01:41:46 *** evaluating ***
06/02/2019 01:41:46 step: 300, epoch: 299, acc: 61.111111111111114, f1: 27.560797665369652, r: 0.4074174289851735
06/02/2019 01:41:46 
*** Best acc model ***
epoch: 186
acc: 63.67521367521367
f1: 30.716657678995528
corr: 0.41575339931530014
06/02/2019 01:41:46 Loading Test Data
06/02/2019 01:41:46 load data from data/word2vec_temp/test_text.npy, data/word2vec_temp/test_label.npy, training: False
06/02/2019 01:42:14 loaded. total len: 2228
06/02/2019 01:42:14 Test: length: 2228, total batch: 35, batch size: 64
06/02/2019 01:42:15 
*** Test Result ***
acc: 61.111111111111114
f1: 27.560797665369652
corr: 0.4074174289851735
