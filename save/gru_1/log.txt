06/02/2019 12:20:00 {'input_path': 'data/word2vec_temp', 'output_path': 'save/gru_1', 'gpu': True, 'seed': 20000125, 'display_per_batch': 5, 'optimizer': 'adagrad', 'lr': 0.01, 'lr_decay': 0, 'weight_decay': 0.0001, 'momentum': 0.985, 'num_epochs': 300, 'batch_size': 64, 'num_labels': 8, 'embedding_size': 300, 'type': 'rnn', 'rnn': {'type': 'gru', 'bidirectional': False, 'rnn_hidden_size': 256, 'mlp_hidden_size': 512, 'dropout': 0.5, 'p_coefficient': 1, 'num_layers': 1, 'param_da': 350, 'param_r': 30, 'loss': 'cross_entropy'}}
06/02/2019 12:20:00 Loading Train Data
06/02/2019 12:20:00 load data from data/word2vec_temp/train_text.npy, data/word2vec_temp/train_label.npy, training: True
06/02/2019 12:20:23 loaded. total len: 2342
06/02/2019 12:20:23 Train: length: 2108, total batch: 33, batch size: 64
06/02/2019 12:20:23 Dev: length: 234, total batch: 4, batch size: 64
06/02/2019 12:20:23 Loading model rnn
06/02/2019 12:20:32 *** epoch: 1 ***
06/02/2019 12:20:32 *** training ***
06/02/2019 12:20:33 step: 5, epoch: 0, batch: 4, loss: 23.831356048583984, acc: 34.375, f1: 7.308970099667775, r: 0.04040605122319625
06/02/2019 12:20:34 step: 10, epoch: 0, batch: 9, loss: 7.244394779205322, acc: 10.9375, f1: 8.463203463203465, r: 0.029868131859589593
06/02/2019 12:20:35 step: 15, epoch: 0, batch: 14, loss: 6.354485511779785, acc: 28.125, f1: 6.349206349206349, r: 0.054850278321515275
06/02/2019 12:20:35 step: 20, epoch: 0, batch: 19, loss: 6.419156074523926, acc: 31.25, f1: 6.097560975609756, r: 0.0577948615214446
06/02/2019 12:20:36 step: 25, epoch: 0, batch: 24, loss: 5.974903106689453, acc: 39.0625, f1: 13.759512713114272, r: 0.056218747955263215
06/02/2019 12:20:37 step: 30, epoch: 0, batch: 29, loss: 5.57267951965332, acc: 35.9375, f1: 15.794317727090837, r: 0.04768319854357185
06/02/2019 12:20:37 *** evaluating ***
06/02/2019 12:20:37 step: 1, epoch: 0, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.3070482832244155
06/02/2019 12:20:37 *** epoch: 2 ***
06/02/2019 12:20:37 *** training ***
06/02/2019 12:20:38 step: 38, epoch: 1, batch: 4, loss: 4.9570770263671875, acc: 39.0625, f1: 8.45905172413793, r: 0.1938666035990602
06/02/2019 12:20:38 step: 43, epoch: 1, batch: 9, loss: 4.512800693511963, acc: 54.6875, f1: 11.904761904761903, r: 0.09228718481139611
06/02/2019 12:20:39 step: 48, epoch: 1, batch: 14, loss: 4.887853622436523, acc: 34.375, f1: 12.257006151742992, r: 0.2073956249163948
06/02/2019 12:20:40 step: 53, epoch: 1, batch: 19, loss: 4.302131652832031, acc: 34.375, f1: 7.394957983193276, r: 0.17594606215594663
06/02/2019 12:20:40 step: 58, epoch: 1, batch: 24, loss: 4.835278034210205, acc: 32.8125, f1: 11.933899433899434, r: 0.1262715654185894
06/02/2019 12:20:41 step: 63, epoch: 1, batch: 29, loss: 4.57281494140625, acc: 40.625, f1: 14.14356787491116, r: 0.16207778880151158
06/02/2019 12:20:41 *** evaluating ***
06/02/2019 12:20:41 step: 2, epoch: 1, acc: 44.871794871794876, f1: 8.981191222570533, r: 0.3189654033820747
06/02/2019 12:20:41 *** epoch: 3 ***
06/02/2019 12:20:41 *** training ***
06/02/2019 12:20:42 step: 71, epoch: 2, batch: 4, loss: 4.037895202636719, acc: 40.625, f1: 12.908496732026142, r: 0.10730887194774923
06/02/2019 12:20:43 step: 76, epoch: 2, batch: 9, loss: 3.841785430908203, acc: 51.5625, f1: 21.544084391692845, r: 0.2739964728784156
06/02/2019 12:20:43 step: 81, epoch: 2, batch: 14, loss: 3.8186750411987305, acc: 43.75, f1: 8.79120879120879, r: 0.20901178541965565
06/02/2019 12:20:44 step: 86, epoch: 2, batch: 19, loss: 4.526240348815918, acc: 37.5, f1: 17.462962962962965, r: 0.29776470971254343
06/02/2019 12:20:45 step: 91, epoch: 2, batch: 24, loss: 3.858720302581787, acc: 48.4375, f1: 23.650121065375306, r: 0.29397572813917694
06/02/2019 12:20:45 step: 96, epoch: 2, batch: 29, loss: 3.553957223892212, acc: 35.9375, f1: 15.992866782340467, r: 0.22963249618991777
06/02/2019 12:20:46 *** evaluating ***
06/02/2019 12:20:46 step: 3, epoch: 2, acc: 54.27350427350427, f1: 15.961038961038959, r: 0.31373369717161514
06/02/2019 12:20:46 *** epoch: 4 ***
06/02/2019 12:20:46 *** training ***
06/02/2019 12:20:46 step: 104, epoch: 3, batch: 4, loss: 3.3590903282165527, acc: 46.875, f1: 17.890176794286386, r: 0.2380663522829483
06/02/2019 12:20:47 step: 109, epoch: 3, batch: 9, loss: 3.5096611976623535, acc: 42.1875, f1: 17.453938096977918, r: 0.3152172249528764
06/02/2019 12:20:48 step: 114, epoch: 3, batch: 14, loss: 3.6115522384643555, acc: 42.1875, f1: 15.340909090909092, r: 0.2270594829384399
06/02/2019 12:20:48 step: 119, epoch: 3, batch: 19, loss: 3.558979034423828, acc: 34.375, f1: 8.693609022556393, r: 0.23643852145664074
06/02/2019 12:20:49 step: 124, epoch: 3, batch: 24, loss: 3.3855934143066406, acc: 42.1875, f1: 13.84698275862069, r: 0.30064521908055575
06/02/2019 12:20:50 step: 129, epoch: 3, batch: 29, loss: 3.3052470684051514, acc: 42.1875, f1: 12.658730158730158, r: 0.24019830245461793
06/02/2019 12:20:50 *** evaluating ***
06/02/2019 12:20:50 step: 4, epoch: 3, acc: 54.700854700854705, f1: 17.302402092912637, r: 0.3513686995523761
06/02/2019 12:20:50 *** epoch: 5 ***
06/02/2019 12:20:50 *** training ***
06/02/2019 12:20:51 step: 137, epoch: 4, batch: 4, loss: 3.465085029602051, acc: 46.875, f1: 20.502523590081196, r: 0.30673148295475594
06/02/2019 12:20:51 step: 142, epoch: 4, batch: 9, loss: 3.2231500148773193, acc: 46.875, f1: 16.362889983579638, r: 0.2593250202396913
06/02/2019 12:20:52 step: 147, epoch: 4, batch: 14, loss: 3.0853028297424316, acc: 46.875, f1: 14.06926406926407, r: 0.3041698553314435
06/02/2019 12:20:53 step: 152, epoch: 4, batch: 19, loss: 3.0942187309265137, acc: 48.4375, f1: 18.335118602736795, r: 0.3197020134064516
06/02/2019 12:20:53 step: 157, epoch: 4, batch: 24, loss: 3.5536293983459473, acc: 45.3125, f1: 15.436507936507937, r: 0.26868136047993196
06/02/2019 12:20:54 step: 162, epoch: 4, batch: 29, loss: 3.3618292808532715, acc: 37.5, f1: 11.853701527614572, r: 0.2710820272965028
06/02/2019 12:20:54 *** evaluating ***
06/02/2019 12:20:55 step: 5, epoch: 4, acc: 48.29059829059829, f1: 15.996651295564337, r: 0.34942728503524373
06/02/2019 12:20:55 *** epoch: 6 ***
06/02/2019 12:20:55 *** training ***
06/02/2019 12:20:55 step: 170, epoch: 5, batch: 4, loss: 3.0719172954559326, acc: 42.1875, f1: 16.922043010752688, r: 0.23302261495566542
06/02/2019 12:20:56 step: 175, epoch: 5, batch: 9, loss: 3.0995466709136963, acc: 34.375, f1: 12.841065235883006, r: 0.3063689078582551
06/02/2019 12:20:57 step: 180, epoch: 5, batch: 14, loss: 3.1462242603302, acc: 50.0, f1: 18.803418803418808, r: 0.26161161139831285
06/02/2019 12:20:57 step: 185, epoch: 5, batch: 19, loss: 2.9785404205322266, acc: 59.375, f1: 25.879917184265004, r: 0.3184000974057951
06/02/2019 12:20:58 step: 190, epoch: 5, batch: 24, loss: 2.956982135772705, acc: 48.4375, f1: 26.390804597701145, r: 0.31183815913202745
06/02/2019 12:20:58 step: 195, epoch: 5, batch: 29, loss: 2.936141014099121, acc: 43.75, f1: 15.576319460590563, r: 0.28865720544766965
06/02/2019 12:20:59 *** evaluating ***
06/02/2019 12:20:59 step: 6, epoch: 5, acc: 53.41880341880342, f1: 18.930008281114496, r: 0.3526372322603061
06/02/2019 12:20:59 *** epoch: 7 ***
06/02/2019 12:20:59 *** training ***
06/02/2019 12:21:00 step: 203, epoch: 6, batch: 4, loss: 2.820829153060913, acc: 50.0, f1: 17.651515151515156, r: 0.242366572353822
06/02/2019 12:21:00 step: 208, epoch: 6, batch: 9, loss: 2.7825655937194824, acc: 51.5625, f1: 28.551216751556307, r: 0.2281855730230563
06/02/2019 12:21:01 step: 213, epoch: 6, batch: 14, loss: 2.9190781116485596, acc: 56.25, f1: 22.99107142857143, r: 0.3405228094957728
06/02/2019 12:21:01 step: 218, epoch: 6, batch: 19, loss: 3.0389058589935303, acc: 50.0, f1: 22.270264037818517, r: 0.23838627664808998
06/02/2019 12:21:02 step: 223, epoch: 6, batch: 24, loss: 2.8220267295837402, acc: 43.75, f1: 28.698099929627023, r: 0.3554379307707818
06/02/2019 12:21:03 step: 228, epoch: 6, batch: 29, loss: 2.835671901702881, acc: 54.6875, f1: 22.84652808491818, r: 0.24676853845113655
06/02/2019 12:21:03 *** evaluating ***
06/02/2019 12:21:03 step: 7, epoch: 6, acc: 54.700854700854705, f1: 21.861332721115765, r: 0.36701862145299546
06/02/2019 12:21:03 *** epoch: 8 ***
06/02/2019 12:21:03 *** training ***
06/02/2019 12:21:04 step: 236, epoch: 7, batch: 4, loss: 2.886622428894043, acc: 42.1875, f1: 20.262018683071314, r: 0.221678804001367
06/02/2019 12:21:05 step: 241, epoch: 7, batch: 9, loss: 2.7456307411193848, acc: 48.4375, f1: 16.053921568627455, r: 0.26345286207125046
06/02/2019 12:21:05 step: 246, epoch: 7, batch: 14, loss: 2.5675461292266846, acc: 56.25, f1: 21.673669467787114, r: 0.23775603058800565
06/02/2019 12:21:06 step: 251, epoch: 7, batch: 19, loss: 2.6045737266540527, acc: 53.125, f1: 26.11607142857143, r: 0.37625294045617996
06/02/2019 12:21:06 step: 256, epoch: 7, batch: 24, loss: 3.0068106651306152, acc: 37.5, f1: 20.086491327300152, r: 0.2424602246206241
06/02/2019 12:21:07 step: 261, epoch: 7, batch: 29, loss: 2.6665987968444824, acc: 65.625, f1: 30.555699593812115, r: 0.3480258793971501
06/02/2019 12:21:07 *** evaluating ***
06/02/2019 12:21:08 step: 8, epoch: 7, acc: 57.692307692307686, f1: 24.497412610623893, r: 0.3703506760552741
06/02/2019 12:21:08 *** epoch: 9 ***
06/02/2019 12:21:08 *** training ***
06/02/2019 12:21:08 step: 269, epoch: 8, batch: 4, loss: 2.9004290103912354, acc: 56.25, f1: 23.833107955826005, r: 0.37067280056281116
06/02/2019 12:21:09 step: 274, epoch: 8, batch: 9, loss: 2.9117648601531982, acc: 54.6875, f1: 20.385366385505144, r: 0.38314421735111687
06/02/2019 12:21:09 step: 279, epoch: 8, batch: 14, loss: 2.878541946411133, acc: 57.8125, f1: 20.022968426501038, r: 0.4017324493966859
06/02/2019 12:21:10 step: 284, epoch: 8, batch: 19, loss: 2.4970924854278564, acc: 56.25, f1: 28.302617521367523, r: 0.31047133519228276
06/02/2019 12:21:11 step: 289, epoch: 8, batch: 24, loss: 2.650379180908203, acc: 48.4375, f1: 26.157599157599158, r: 0.3484503192744187
06/02/2019 12:21:11 step: 294, epoch: 8, batch: 29, loss: 2.899529457092285, acc: 40.625, f1: 23.158482142857146, r: 0.3193037556374557
06/02/2019 12:21:12 *** evaluating ***
06/02/2019 12:21:12 step: 9, epoch: 8, acc: 58.119658119658126, f1: 24.60126118991048, r: 0.38087823132888543
06/02/2019 12:21:12 *** epoch: 10 ***
06/02/2019 12:21:12 *** training ***
06/02/2019 12:21:13 step: 302, epoch: 9, batch: 4, loss: 2.826897621154785, acc: 42.1875, f1: 23.645165122356854, r: 0.3581889913025906
06/02/2019 12:21:13 step: 307, epoch: 9, batch: 9, loss: 2.620940685272217, acc: 46.875, f1: 27.83706816059757, r: 0.3180133037213184
06/02/2019 12:21:14 step: 312, epoch: 9, batch: 14, loss: 2.566619873046875, acc: 51.5625, f1: 26.662301284150026, r: 0.3592909225191663
06/02/2019 12:21:14 step: 317, epoch: 9, batch: 19, loss: 2.8267440795898438, acc: 34.375, f1: 21.69447863296792, r: 0.24017183568411885
06/02/2019 12:21:15 step: 322, epoch: 9, batch: 24, loss: 2.68641996383667, acc: 53.125, f1: 24.541204609697758, r: 0.27565674527119743
06/02/2019 12:21:16 step: 327, epoch: 9, batch: 29, loss: 2.5269722938537598, acc: 51.5625, f1: 29.31034482758621, r: 0.3419057685448428
06/02/2019 12:21:16 *** evaluating ***
06/02/2019 12:21:16 step: 10, epoch: 9, acc: 56.41025641025641, f1: 17.990747853969737, r: 0.39500850232115015
06/02/2019 12:21:16 *** epoch: 11 ***
06/02/2019 12:21:16 *** training ***
06/02/2019 12:21:17 step: 335, epoch: 10, batch: 4, loss: 2.3179640769958496, acc: 53.125, f1: 18.073593073593074, r: 0.2740870457076645
06/02/2019 12:21:17 step: 340, epoch: 10, batch: 9, loss: 2.3381638526916504, acc: 62.5, f1: 29.204545454545457, r: 0.459291805545426
06/02/2019 12:21:18 step: 345, epoch: 10, batch: 14, loss: 2.3947982788085938, acc: 51.5625, f1: 22.772657450076803, r: 0.3475190338702958
06/02/2019 12:21:19 step: 350, epoch: 10, batch: 19, loss: 2.433528423309326, acc: 50.0, f1: 21.273225708193724, r: 0.24879344356018174
06/02/2019 12:21:19 step: 355, epoch: 10, batch: 24, loss: 2.2353954315185547, acc: 62.5, f1: 40.60846560846561, r: 0.4011830591611204
06/02/2019 12:21:20 step: 360, epoch: 10, batch: 29, loss: 2.6168503761291504, acc: 51.5625, f1: 28.790366721401206, r: 0.3522407931009813
06/02/2019 12:21:20 *** evaluating ***
06/02/2019 12:21:21 step: 11, epoch: 10, acc: 55.55555555555556, f1: 21.420115499163703, r: 0.39000108499734576
06/02/2019 12:21:21 *** epoch: 12 ***
06/02/2019 12:21:21 *** training ***
06/02/2019 12:21:21 step: 368, epoch: 11, batch: 4, loss: 2.270098924636841, acc: 62.5, f1: 42.45470236941452, r: 0.35146148432947294
06/02/2019 12:21:22 step: 373, epoch: 11, batch: 9, loss: 2.0659103393554688, acc: 64.0625, f1: 49.952380952380956, r: 0.363412500821917
06/02/2019 12:21:22 step: 378, epoch: 11, batch: 14, loss: 2.4147958755493164, acc: 53.125, f1: 31.945067154346845, r: 0.3956921859003393
06/02/2019 12:21:23 step: 383, epoch: 11, batch: 19, loss: 2.382108211517334, acc: 54.6875, f1: 27.495139041749212, r: 0.3248609822869308
06/02/2019 12:21:24 step: 388, epoch: 11, batch: 24, loss: 2.492265224456787, acc: 54.6875, f1: 31.335922787193976, r: 0.38271199190362026
06/02/2019 12:21:24 step: 393, epoch: 11, batch: 29, loss: 2.529956340789795, acc: 56.25, f1: 29.126984126984123, r: 0.28994816244423116
06/02/2019 12:21:25 *** evaluating ***
06/02/2019 12:21:25 step: 12, epoch: 11, acc: 52.13675213675214, f1: 25.552302527508218, r: 0.3669716934513309
06/02/2019 12:21:25 *** epoch: 13 ***
06/02/2019 12:21:25 *** training ***
06/02/2019 12:21:25 step: 401, epoch: 12, batch: 4, loss: 2.4349710941314697, acc: 39.0625, f1: 18.038302277432713, r: 0.32130361281756775
06/02/2019 12:21:26 step: 406, epoch: 12, batch: 9, loss: 2.041442394256592, acc: 60.9375, f1: 40.19607843137255, r: 0.4631300874482321
06/02/2019 12:21:27 step: 411, epoch: 12, batch: 14, loss: 2.0354976654052734, acc: 57.8125, f1: 23.544842254519672, r: 0.4541361739731126
06/02/2019 12:21:27 step: 416, epoch: 12, batch: 19, loss: 2.4394874572753906, acc: 39.0625, f1: 17.719221835075494, r: 0.3526153978020024
06/02/2019 12:21:28 step: 421, epoch: 12, batch: 24, loss: 2.2964208126068115, acc: 59.375, f1: 38.89051749037011, r: 0.4976981541051824
06/02/2019 12:21:29 step: 426, epoch: 12, batch: 29, loss: 2.514333724975586, acc: 43.75, f1: 25.30964713820751, r: 0.3079643429155169
06/02/2019 12:21:29 *** evaluating ***
06/02/2019 12:21:29 step: 13, epoch: 12, acc: 56.41025641025641, f1: 26.87215440985984, r: 0.3898809578178012
06/02/2019 12:21:29 *** epoch: 14 ***
06/02/2019 12:21:29 *** training ***
06/02/2019 12:21:30 step: 434, epoch: 13, batch: 4, loss: 2.626725912094116, acc: 48.4375, f1: 32.581583357980875, r: 0.27565553624085165
06/02/2019 12:21:30 step: 439, epoch: 13, batch: 9, loss: 2.433713912963867, acc: 56.25, f1: 32.72360407953629, r: 0.4231086826051492
06/02/2019 12:21:31 step: 444, epoch: 13, batch: 14, loss: 2.081038475036621, acc: 57.8125, f1: 31.07834101382489, r: 0.4436340812039651
06/02/2019 12:21:32 step: 449, epoch: 13, batch: 19, loss: 2.0183346271514893, acc: 59.375, f1: 24.538390215903576, r: 0.4235861375924335
06/02/2019 12:21:32 step: 454, epoch: 13, batch: 24, loss: 2.3900513648986816, acc: 45.3125, f1: 21.794212956414874, r: 0.281642970015582
06/02/2019 12:21:33 step: 459, epoch: 13, batch: 29, loss: 1.947709560394287, acc: 60.9375, f1: 26.36363636363636, r: 0.44403054258086233
06/02/2019 12:21:33 *** evaluating ***
06/02/2019 12:21:34 step: 14, epoch: 13, acc: 54.27350427350427, f1: 25.24437843900137, r: 0.38596912947669404
06/02/2019 12:21:34 *** epoch: 15 ***
06/02/2019 12:21:34 *** training ***
06/02/2019 12:21:34 step: 467, epoch: 14, batch: 4, loss: 1.9053833484649658, acc: 56.25, f1: 40.79388353581902, r: 0.46819054716811764
06/02/2019 12:21:35 step: 472, epoch: 14, batch: 9, loss: 2.1893579959869385, acc: 53.125, f1: 40.7569573283859, r: 0.4067379966320466
06/02/2019 12:21:35 step: 477, epoch: 14, batch: 14, loss: 2.2305281162261963, acc: 48.4375, f1: 20.03052503052503, r: 0.35127026626071867
06/02/2019 12:21:36 step: 482, epoch: 14, batch: 19, loss: 2.0315074920654297, acc: 54.6875, f1: 30.721726190476197, r: 0.42918999971797284
06/02/2019 12:21:37 step: 487, epoch: 14, batch: 24, loss: 2.288074016571045, acc: 59.375, f1: 33.02289026426958, r: 0.49213611767528526
06/02/2019 12:21:37 step: 492, epoch: 14, batch: 29, loss: 2.055586814880371, acc: 57.8125, f1: 27.711939699644617, r: 0.3292582270055358
06/02/2019 12:21:38 *** evaluating ***
06/02/2019 12:21:38 step: 15, epoch: 14, acc: 54.27350427350427, f1: 24.65600828136915, r: 0.3749625748826711
06/02/2019 12:21:38 *** epoch: 16 ***
06/02/2019 12:21:38 *** training ***
06/02/2019 12:21:39 step: 500, epoch: 15, batch: 4, loss: 2.3215367794036865, acc: 54.6875, f1: 34.39249429815468, r: 0.5449657686108969
06/02/2019 12:21:39 step: 505, epoch: 15, batch: 9, loss: 1.8708125352859497, acc: 53.125, f1: 35.165351565727505, r: 0.47054426219454715
06/02/2019 12:21:40 step: 510, epoch: 15, batch: 14, loss: 1.957773208618164, acc: 56.25, f1: 39.54576824142042, r: 0.46054676109501924
06/02/2019 12:21:41 step: 515, epoch: 15, batch: 19, loss: 1.9964505434036255, acc: 62.5, f1: 36.276614010989015, r: 0.4992823128672455
06/02/2019 12:21:41 step: 520, epoch: 15, batch: 24, loss: 1.7561070919036865, acc: 57.8125, f1: 49.56212370005473, r: 0.5189824867770403
06/02/2019 12:21:42 step: 525, epoch: 15, batch: 29, loss: 1.9930214881896973, acc: 62.5, f1: 55.20408163265304, r: 0.35375034597135613
06/02/2019 12:21:42 *** evaluating ***
06/02/2019 12:21:42 step: 16, epoch: 15, acc: 58.97435897435898, f1: 28.472802618378463, r: 0.3955468255884309
06/02/2019 12:21:42 *** epoch: 17 ***
06/02/2019 12:21:42 *** training ***
06/02/2019 12:21:43 step: 533, epoch: 16, batch: 4, loss: 1.6417728662490845, acc: 60.9375, f1: 45.151515151515156, r: 0.5208788770202893
06/02/2019 12:21:43 step: 538, epoch: 16, batch: 9, loss: 1.9039838314056396, acc: 57.8125, f1: 57.24396862570165, r: 0.45387752980722923
06/02/2019 12:21:44 step: 543, epoch: 16, batch: 14, loss: 1.930788278579712, acc: 54.6875, f1: 39.359034666785426, r: 0.3859241466035921
06/02/2019 12:21:45 step: 548, epoch: 16, batch: 19, loss: 1.7655878067016602, acc: 65.625, f1: 37.58434547908232, r: 0.38786266774812456
06/02/2019 12:21:45 step: 553, epoch: 16, batch: 24, loss: 1.6870496273040771, acc: 59.375, f1: 34.52271608988027, r: 0.2702805707715446
06/02/2019 12:21:46 step: 558, epoch: 16, batch: 29, loss: 2.0211849212646484, acc: 56.25, f1: 32.66047662259096, r: 0.472745116055358
06/02/2019 12:21:46 *** evaluating ***
06/02/2019 12:21:46 step: 17, epoch: 16, acc: 58.54700854700855, f1: 29.63360416612113, r: 0.40241841474676127
06/02/2019 12:21:46 *** epoch: 18 ***
06/02/2019 12:21:46 *** training ***
06/02/2019 12:21:47 step: 566, epoch: 17, batch: 4, loss: 1.7507914304733276, acc: 57.8125, f1: 36.63392857142857, r: 0.5298790887233443
06/02/2019 12:21:48 step: 571, epoch: 17, batch: 9, loss: 1.666161298751831, acc: 67.1875, f1: 47.976190476190474, r: 0.567970478557118
06/02/2019 12:21:48 step: 576, epoch: 17, batch: 14, loss: 1.6493308544158936, acc: 57.8125, f1: 28.043478260869563, r: 0.459049404402609
06/02/2019 12:21:49 step: 581, epoch: 17, batch: 19, loss: 1.8042175769805908, acc: 59.375, f1: 40.4920814479638, r: 0.40591254394820325
06/02/2019 12:21:50 step: 586, epoch: 17, batch: 24, loss: 1.8599190711975098, acc: 50.0, f1: 31.55270655270655, r: 0.3835956631503385
06/02/2019 12:21:50 step: 591, epoch: 17, batch: 29, loss: 1.7692506313323975, acc: 57.8125, f1: 32.597928060112935, r: 0.4161441778971536
06/02/2019 12:21:50 *** evaluating ***
06/02/2019 12:21:51 step: 18, epoch: 17, acc: 59.401709401709404, f1: 30.336909467344253, r: 0.3939985880372612
06/02/2019 12:21:51 *** epoch: 19 ***
06/02/2019 12:21:51 *** training ***
06/02/2019 12:21:51 step: 599, epoch: 18, batch: 4, loss: 1.491126298904419, acc: 60.9375, f1: 38.30994548571902, r: 0.44924222324350094
06/02/2019 12:21:52 step: 604, epoch: 18, batch: 9, loss: 1.6997448205947876, acc: 53.125, f1: 34.01970267793504, r: 0.39335202498065786
06/02/2019 12:21:53 step: 609, epoch: 18, batch: 14, loss: 1.7835521697998047, acc: 51.5625, f1: 29.22161172161172, r: 0.3679962234332014
06/02/2019 12:21:53 step: 614, epoch: 18, batch: 19, loss: 1.4906396865844727, acc: 65.625, f1: 36.701044053985235, r: 0.5454450680469194
06/02/2019 12:21:54 step: 619, epoch: 18, batch: 24, loss: 1.998894214630127, acc: 68.75, f1: 53.68480725623582, r: 0.4834909681700057
06/02/2019 12:21:54 step: 624, epoch: 18, batch: 29, loss: 1.7501747608184814, acc: 62.5, f1: 41.10564825720029, r: 0.42205481835625774
06/02/2019 12:21:55 *** evaluating ***
06/02/2019 12:21:55 step: 19, epoch: 18, acc: 55.55555555555556, f1: 24.994229504105984, r: 0.41246998437370547
06/02/2019 12:21:55 *** epoch: 20 ***
06/02/2019 12:21:55 *** training ***
06/02/2019 12:21:56 step: 632, epoch: 19, batch: 4, loss: 1.5794100761413574, acc: 67.1875, f1: 47.84451274856392, r: 0.44921447740478554
06/02/2019 12:21:56 step: 637, epoch: 19, batch: 9, loss: 1.3009394407272339, acc: 68.75, f1: 44.72479733761361, r: 0.5166315803268956
06/02/2019 12:21:57 step: 642, epoch: 19, batch: 14, loss: 1.3447988033294678, acc: 71.875, f1: 54.196976732188, r: 0.5604308336684419
06/02/2019 12:21:57 step: 647, epoch: 19, batch: 19, loss: 1.610328197479248, acc: 57.8125, f1: 36.30797773654916, r: 0.35609974394159194
06/02/2019 12:21:58 step: 652, epoch: 19, batch: 24, loss: 1.3181824684143066, acc: 64.0625, f1: 40.51566645551608, r: 0.48057557883507607
06/02/2019 12:21:59 step: 657, epoch: 19, batch: 29, loss: 1.5289226770401, acc: 59.375, f1: 45.602453102453104, r: 0.5144084986352365
06/02/2019 12:21:59 *** evaluating ***
06/02/2019 12:21:59 step: 20, epoch: 19, acc: 56.837606837606835, f1: 26.757445639900578, r: 0.4053871511214844
06/02/2019 12:21:59 *** epoch: 21 ***
06/02/2019 12:21:59 *** training ***
06/02/2019 12:22:00 step: 665, epoch: 20, batch: 4, loss: 1.3524513244628906, acc: 64.0625, f1: 49.36589354742789, r: 0.5348639876269066
06/02/2019 12:22:01 step: 670, epoch: 20, batch: 9, loss: 1.2592272758483887, acc: 65.625, f1: 33.07900432900433, r: 0.5642458362687383
06/02/2019 12:22:01 step: 675, epoch: 20, batch: 14, loss: 1.363889217376709, acc: 56.25, f1: 27.474903584392013, r: 0.5025378593131596
06/02/2019 12:22:02 step: 680, epoch: 20, batch: 19, loss: 1.4783992767333984, acc: 57.8125, f1: 28.848609883092635, r: 0.40004013281987777
06/02/2019 12:22:03 step: 685, epoch: 20, batch: 24, loss: 1.1983206272125244, acc: 78.125, f1: 55.48642439243634, r: 0.5386989142603091
06/02/2019 12:22:03 step: 690, epoch: 20, batch: 29, loss: 1.466334581375122, acc: 54.6875, f1: 26.613813628899834, r: 0.4985186916963861
06/02/2019 12:22:04 *** evaluating ***
06/02/2019 12:22:04 step: 21, epoch: 20, acc: 54.700854700854705, f1: 28.49158589917661, r: 0.36122969494192747
06/02/2019 12:22:04 *** epoch: 22 ***
06/02/2019 12:22:04 *** training ***
06/02/2019 12:22:04 step: 698, epoch: 21, batch: 4, loss: 1.2607672214508057, acc: 62.5, f1: 41.38227513227514, r: 0.49156687588029313
06/02/2019 12:22:05 step: 703, epoch: 21, batch: 9, loss: 1.2740994691848755, acc: 62.5, f1: 41.46214896214896, r: 0.5495850393255642
06/02/2019 12:22:06 step: 708, epoch: 21, batch: 14, loss: 1.178863286972046, acc: 67.1875, f1: 53.67541612567403, r: 0.5053986312884473
06/02/2019 12:22:06 step: 713, epoch: 21, batch: 19, loss: 1.3087941408157349, acc: 64.0625, f1: 50.77463916708768, r: 0.48477794581333766
06/02/2019 12:22:07 step: 718, epoch: 21, batch: 24, loss: 1.2447096109390259, acc: 62.5, f1: 36.69642857142858, r: 0.5536143880006011
06/02/2019 12:22:08 step: 723, epoch: 21, batch: 29, loss: 1.3169327974319458, acc: 62.5, f1: 45.13305322128851, r: 0.561937086642197
06/02/2019 12:22:08 *** evaluating ***
06/02/2019 12:22:08 step: 22, epoch: 21, acc: 58.119658119658126, f1: 28.848917829927913, r: 0.4017070616628058
06/02/2019 12:22:08 *** epoch: 23 ***
06/02/2019 12:22:08 *** training ***
06/02/2019 12:22:09 step: 731, epoch: 22, batch: 4, loss: 1.1032131910324097, acc: 67.1875, f1: 55.601015160622445, r: 0.47654440241752616
06/02/2019 12:22:09 step: 736, epoch: 22, batch: 9, loss: 1.1462478637695312, acc: 60.9375, f1: 44.8242271311032, r: 0.5134519092161933
06/02/2019 12:22:10 step: 741, epoch: 22, batch: 14, loss: 1.1933199167251587, acc: 59.375, f1: 44.491223062651635, r: 0.4592114868007713
06/02/2019 12:22:11 step: 746, epoch: 22, batch: 19, loss: 1.2687054872512817, acc: 62.5, f1: 43.30357142857143, r: 0.5080272175680909
06/02/2019 12:22:11 step: 751, epoch: 22, batch: 24, loss: 1.2505849599838257, acc: 65.625, f1: 49.418575977399506, r: 0.479535090583608
06/02/2019 12:22:12 step: 756, epoch: 22, batch: 29, loss: 1.0871549844741821, acc: 70.3125, f1: 42.97291993720565, r: 0.5086301703636749
06/02/2019 12:22:12 *** evaluating ***
06/02/2019 12:22:12 step: 23, epoch: 22, acc: 58.119658119658126, f1: 25.449790645497806, r: 0.4012396750643574
06/02/2019 12:22:12 *** epoch: 24 ***
06/02/2019 12:22:12 *** training ***
06/02/2019 12:22:13 step: 764, epoch: 23, batch: 4, loss: 1.025283694267273, acc: 65.625, f1: 57.30458221024259, r: 0.6340824545520944
06/02/2019 12:22:14 step: 769, epoch: 23, batch: 9, loss: 0.8567514419555664, acc: 70.3125, f1: 57.0215035166609, r: 0.5709462351138516
06/02/2019 12:22:14 step: 774, epoch: 23, batch: 14, loss: 1.0295448303222656, acc: 75.0, f1: 52.61183261183261, r: 0.5224400075090689
06/02/2019 12:22:15 step: 779, epoch: 23, batch: 19, loss: 1.0446498394012451, acc: 70.3125, f1: 50.105670726788745, r: 0.4535350820976143
06/02/2019 12:22:16 step: 784, epoch: 23, batch: 24, loss: 1.0834996700286865, acc: 68.75, f1: 54.665421724245256, r: 0.5805880860471708
06/02/2019 12:22:16 step: 789, epoch: 23, batch: 29, loss: 1.0342117547988892, acc: 67.1875, f1: 46.969086021505376, r: 0.5664386936918788
06/02/2019 12:22:16 *** evaluating ***
06/02/2019 12:22:17 step: 24, epoch: 23, acc: 49.14529914529914, f1: 26.746221913965734, r: 0.36012438372390904
06/02/2019 12:22:17 *** epoch: 25 ***
06/02/2019 12:22:17 *** training ***
06/02/2019 12:22:17 step: 797, epoch: 24, batch: 4, loss: 1.035327434539795, acc: 73.4375, f1: 68.8706876284516, r: 0.540588250297439
06/02/2019 12:22:18 step: 802, epoch: 24, batch: 9, loss: 1.2060989141464233, acc: 57.8125, f1: 31.059366260979164, r: 0.46880774873952274
06/02/2019 12:22:19 step: 807, epoch: 24, batch: 14, loss: 0.9463167786598206, acc: 76.5625, f1: 49.68734968734969, r: 0.45738485368308396
06/02/2019 12:22:19 step: 812, epoch: 24, batch: 19, loss: 1.1134214401245117, acc: 65.625, f1: 50.111009159922205, r: 0.47604215115123366
06/02/2019 12:22:20 step: 817, epoch: 24, batch: 24, loss: 0.9652950763702393, acc: 62.5, f1: 51.621710153145116, r: 0.5118581450189384
06/02/2019 12:22:20 step: 822, epoch: 24, batch: 29, loss: 0.8495281338691711, acc: 70.3125, f1: 53.721488595438174, r: 0.5591214743365683
06/02/2019 12:22:21 *** evaluating ***
06/02/2019 12:22:21 step: 25, epoch: 24, acc: 60.256410256410255, f1: 31.48534798534798, r: 0.3820642382899007
06/02/2019 12:22:21 *** epoch: 26 ***
06/02/2019 12:22:21 *** training ***
06/02/2019 12:22:21 step: 830, epoch: 25, batch: 4, loss: 0.7892391085624695, acc: 82.8125, f1: 72.65825854959981, r: 0.5684677965732334
06/02/2019 12:22:22 step: 835, epoch: 25, batch: 9, loss: 0.8758313059806824, acc: 67.1875, f1: 43.553884711779446, r: 0.5364499137561204
06/02/2019 12:22:23 step: 840, epoch: 25, batch: 14, loss: 1.182679295539856, acc: 59.375, f1: 45.488534746166565, r: 0.5867046178825674
06/02/2019 12:22:23 step: 845, epoch: 25, batch: 19, loss: 1.3667771816253662, acc: 67.1875, f1: 51.42100459064744, r: 0.5991303160590363
06/02/2019 12:22:24 step: 850, epoch: 25, batch: 24, loss: 0.8888849020004272, acc: 79.6875, f1: 78.6399398937439, r: 0.6239066179413468
06/02/2019 12:22:24 step: 855, epoch: 25, batch: 29, loss: 0.8440471887588501, acc: 75.0, f1: 59.34336627732855, r: 0.535964855789541
06/02/2019 12:22:25 *** evaluating ***
06/02/2019 12:22:25 step: 26, epoch: 25, acc: 54.27350427350427, f1: 30.214978734715576, r: 0.3725816017884448
06/02/2019 12:22:25 *** epoch: 27 ***
06/02/2019 12:22:25 *** training ***
06/02/2019 12:22:26 step: 863, epoch: 26, batch: 4, loss: 0.7778339982032776, acc: 79.6875, f1: 53.504362196856135, r: 0.496526001104905
06/02/2019 12:22:26 step: 868, epoch: 26, batch: 9, loss: 1.0841624736785889, acc: 79.6875, f1: 56.21160870315231, r: 0.6086611365140163
06/02/2019 12:22:27 step: 873, epoch: 26, batch: 14, loss: 0.8194833993911743, acc: 76.5625, f1: 59.61038961038961, r: 0.5855610167880821
06/02/2019 12:22:27 step: 878, epoch: 26, batch: 19, loss: 1.0890300273895264, acc: 64.0625, f1: 49.26662356447795, r: 0.5255804476161968
06/02/2019 12:22:28 step: 883, epoch: 26, batch: 24, loss: 0.854295015335083, acc: 68.75, f1: 57.65457812627625, r: 0.6362365338437409
06/02/2019 12:22:29 step: 888, epoch: 26, batch: 29, loss: 0.8438102006912231, acc: 73.4375, f1: 44.80848861283644, r: 0.6271456909743818
06/02/2019 12:22:29 *** evaluating ***
06/02/2019 12:22:29 step: 27, epoch: 26, acc: 53.41880341880342, f1: 28.431844281107622, r: 0.36433330603044284
06/02/2019 12:22:29 *** epoch: 28 ***
06/02/2019 12:22:29 *** training ***
06/02/2019 12:22:30 step: 896, epoch: 27, batch: 4, loss: 0.7796116471290588, acc: 78.125, f1: 58.118131868131876, r: 0.6239615516612658
06/02/2019 12:22:30 step: 901, epoch: 27, batch: 9, loss: 0.765272855758667, acc: 73.4375, f1: 56.80245389593386, r: 0.556297487395571
06/02/2019 12:22:31 step: 906, epoch: 27, batch: 14, loss: 0.8754135370254517, acc: 70.3125, f1: 60.924409237379166, r: 0.5832583903676158
06/02/2019 12:22:32 step: 911, epoch: 27, batch: 19, loss: 0.8811054229736328, acc: 75.0, f1: 57.66670671632824, r: 0.5236474247323717
06/02/2019 12:22:32 step: 916, epoch: 27, batch: 24, loss: 0.9098722338676453, acc: 68.75, f1: 54.98208902464222, r: 0.5156580273624017
06/02/2019 12:22:33 step: 921, epoch: 27, batch: 29, loss: 0.7698972225189209, acc: 82.8125, f1: 70.49953314659196, r: 0.5981768528724519
06/02/2019 12:22:33 *** evaluating ***
06/02/2019 12:22:33 step: 28, epoch: 27, acc: 57.26495726495726, f1: 26.657494387467494, r: 0.3849967103752081
06/02/2019 12:22:33 *** epoch: 29 ***
06/02/2019 12:22:33 *** training ***
06/02/2019 12:22:34 step: 929, epoch: 28, batch: 4, loss: 0.7979983687400818, acc: 75.0, f1: 57.71784575899325, r: 0.5250163828412644
06/02/2019 12:22:35 step: 934, epoch: 28, batch: 9, loss: 0.7525041699409485, acc: 79.6875, f1: 50.381944444444436, r: 0.5900002677144872
06/02/2019 12:22:35 step: 939, epoch: 28, batch: 14, loss: 0.8131771683692932, acc: 76.5625, f1: 76.93171608265949, r: 0.5110244132107793
06/02/2019 12:22:36 step: 944, epoch: 28, batch: 19, loss: 0.7729246616363525, acc: 76.5625, f1: 64.38054009482582, r: 0.6273986526274076
06/02/2019 12:22:36 step: 949, epoch: 28, batch: 24, loss: 1.1763255596160889, acc: 71.875, f1: 53.75696767001115, r: 0.5445005004004752
06/02/2019 12:22:37 step: 954, epoch: 28, batch: 29, loss: 0.9457168579101562, acc: 65.625, f1: 62.322269567167524, r: 0.5597584911979415
06/02/2019 12:22:37 *** evaluating ***
06/02/2019 12:22:37 step: 29, epoch: 28, acc: 57.26495726495726, f1: 31.289318789318788, r: 0.3841788613142735
06/02/2019 12:22:37 *** epoch: 30 ***
06/02/2019 12:22:37 *** training ***
06/02/2019 12:22:38 step: 962, epoch: 29, batch: 4, loss: 0.6516176462173462, acc: 81.25, f1: 71.68357354692759, r: 0.5836462965243352
06/02/2019 12:22:39 step: 967, epoch: 29, batch: 9, loss: 1.2074724435806274, acc: 71.875, f1: 71.65707671957672, r: 0.6799804215537479
06/02/2019 12:22:39 step: 972, epoch: 29, batch: 14, loss: 0.958834171295166, acc: 71.875, f1: 49.42031926406926, r: 0.6132056914529924
06/02/2019 12:22:40 step: 977, epoch: 29, batch: 19, loss: 0.9229748249053955, acc: 75.0, f1: 48.88775174101261, r: 0.5465517797031376
06/02/2019 12:22:41 step: 982, epoch: 29, batch: 24, loss: 0.6168015003204346, acc: 84.375, f1: 68.77446363160648, r: 0.6305272781161643
06/02/2019 12:22:41 step: 987, epoch: 29, batch: 29, loss: 0.860292911529541, acc: 68.75, f1: 55.05113933685363, r: 0.539343087061086
06/02/2019 12:22:42 *** evaluating ***
06/02/2019 12:22:42 step: 30, epoch: 29, acc: 55.55555555555556, f1: 27.995848516813705, r: 0.3786977508383476
06/02/2019 12:22:42 *** epoch: 31 ***
06/02/2019 12:22:42 *** training ***
06/02/2019 12:22:42 step: 995, epoch: 30, batch: 4, loss: 0.6284260153770447, acc: 79.6875, f1: 62.518955059277644, r: 0.6509081999561703
06/02/2019 12:22:43 step: 1000, epoch: 30, batch: 9, loss: 0.9699791073799133, acc: 59.375, f1: 50.82560296846012, r: 0.5182574392643958
06/02/2019 12:22:44 step: 1005, epoch: 30, batch: 14, loss: 1.0508532524108887, acc: 78.125, f1: 63.02130800486063, r: 0.6044287075845651
06/02/2019 12:22:44 step: 1010, epoch: 30, batch: 19, loss: 0.8150532245635986, acc: 73.4375, f1: 61.383492179863154, r: 0.6437499487645683
06/02/2019 12:22:45 step: 1015, epoch: 30, batch: 24, loss: 0.6782394051551819, acc: 78.125, f1: 53.92151326933936, r: 0.6312091285587509
06/02/2019 12:22:45 step: 1020, epoch: 30, batch: 29, loss: 0.9057832360267639, acc: 64.0625, f1: 65.53909858257684, r: 0.6550162066631156
06/02/2019 12:22:46 *** evaluating ***
06/02/2019 12:22:46 step: 31, epoch: 30, acc: 57.26495726495726, f1: 32.17790570175438, r: 0.37700606711587603
06/02/2019 12:22:46 *** epoch: 32 ***
06/02/2019 12:22:46 *** training ***
06/02/2019 12:22:47 step: 1028, epoch: 31, batch: 4, loss: 0.5403872132301331, acc: 84.375, f1: 81.35642135642135, r: 0.6445951449585743
06/02/2019 12:22:47 step: 1033, epoch: 31, batch: 9, loss: 0.7030760049819946, acc: 84.375, f1: 76.64559400923618, r: 0.5970060184972495
06/02/2019 12:22:48 step: 1038, epoch: 31, batch: 14, loss: 1.136643409729004, acc: 73.4375, f1: 45.69776119402985, r: 0.5787319103291166
06/02/2019 12:22:48 step: 1043, epoch: 31, batch: 19, loss: 0.7502751350402832, acc: 68.75, f1: 51.145833333333336, r: 0.6541173115846421
06/02/2019 12:22:49 step: 1048, epoch: 31, batch: 24, loss: 0.7247985601425171, acc: 75.0, f1: 57.97077922077922, r: 0.5680011023120856
06/02/2019 12:22:50 step: 1053, epoch: 31, batch: 29, loss: 0.6832962036132812, acc: 76.5625, f1: 70.10971055088702, r: 0.5843015773631031
06/02/2019 12:22:50 *** evaluating ***
06/02/2019 12:22:50 step: 32, epoch: 31, acc: 55.98290598290598, f1: 29.81918346065715, r: 0.370061825316899
06/02/2019 12:22:50 *** epoch: 33 ***
06/02/2019 12:22:50 *** training ***
06/02/2019 12:22:51 step: 1061, epoch: 32, batch: 4, loss: 0.674485981464386, acc: 78.125, f1: 68.33620887968715, r: 0.7003503277987639
06/02/2019 12:22:51 step: 1066, epoch: 32, batch: 9, loss: 0.7074653506278992, acc: 76.5625, f1: 57.82967032967032, r: 0.6251837476036705
06/02/2019 12:22:52 step: 1071, epoch: 32, batch: 14, loss: 0.9182941913604736, acc: 70.3125, f1: 56.01239619873161, r: 0.5677760481968451
06/02/2019 12:22:53 step: 1076, epoch: 32, batch: 19, loss: 0.6540009379386902, acc: 76.5625, f1: 68.22813165670308, r: 0.584336011113247
06/02/2019 12:22:53 step: 1081, epoch: 32, batch: 24, loss: 0.6006413698196411, acc: 85.9375, f1: 63.17916002126528, r: 0.6601081621662288
06/02/2019 12:22:54 step: 1086, epoch: 32, batch: 29, loss: 0.9385191798210144, acc: 84.375, f1: 78.72834373240988, r: 0.6127506121513224
06/02/2019 12:22:54 *** evaluating ***
06/02/2019 12:22:54 step: 33, epoch: 32, acc: 55.98290598290598, f1: 32.11139972583634, r: 0.38028512530760883
06/02/2019 12:22:54 *** epoch: 34 ***
06/02/2019 12:22:54 *** training ***
06/02/2019 12:22:55 step: 1094, epoch: 33, batch: 4, loss: 0.7750940322875977, acc: 75.0, f1: 56.8476871965244, r: 0.7505633794627682
06/02/2019 12:22:55 step: 1099, epoch: 33, batch: 9, loss: 0.6637841463088989, acc: 82.8125, f1: 80.62210391402938, r: 0.6756595377645177
06/02/2019 12:22:56 step: 1104, epoch: 33, batch: 14, loss: 0.592585563659668, acc: 81.25, f1: 66.61903891991578, r: 0.6137071214171015
06/02/2019 12:22:56 step: 1109, epoch: 33, batch: 19, loss: 1.0233304500579834, acc: 79.6875, f1: 80.30280830280829, r: 0.551169918832077
06/02/2019 12:22:57 step: 1114, epoch: 33, batch: 24, loss: 0.7689536809921265, acc: 75.0, f1: 64.89800759013282, r: 0.5827880211440085
06/02/2019 12:22:58 step: 1119, epoch: 33, batch: 29, loss: 0.7133351564407349, acc: 78.125, f1: 64.82077704903791, r: 0.6769916099744517
06/02/2019 12:22:58 *** evaluating ***
06/02/2019 12:22:58 step: 34, epoch: 33, acc: 57.692307692307686, f1: 27.368279569892472, r: 0.3749909530514479
06/02/2019 12:22:58 *** epoch: 35 ***
06/02/2019 12:22:58 *** training ***
06/02/2019 12:22:59 step: 1127, epoch: 34, batch: 4, loss: 0.5140510201454163, acc: 87.5, f1: 85.87267742918063, r: 0.6598331850121408
06/02/2019 12:22:59 step: 1132, epoch: 34, batch: 9, loss: 0.4759514033794403, acc: 85.9375, f1: 82.13436671388617, r: 0.5254198446390246
06/02/2019 12:23:00 step: 1137, epoch: 34, batch: 14, loss: 0.7795454859733582, acc: 70.3125, f1: 47.46212121212121, r: 0.6272964001598625
06/02/2019 12:23:01 step: 1142, epoch: 34, batch: 19, loss: 0.7242334485054016, acc: 82.8125, f1: 72.19887955182072, r: 0.6901907638771678
06/02/2019 12:23:01 step: 1147, epoch: 34, batch: 24, loss: 0.6041088700294495, acc: 84.375, f1: 60.795502311895746, r: 0.6300107847235322
06/02/2019 12:23:02 step: 1152, epoch: 34, batch: 29, loss: 1.1107897758483887, acc: 76.5625, f1: 74.9793453672764, r: 0.6503369650889281
06/02/2019 12:23:02 *** evaluating ***
06/02/2019 12:23:02 step: 35, epoch: 34, acc: 56.837606837606835, f1: 26.47871354692445, r: 0.3696092693666365
06/02/2019 12:23:02 *** epoch: 36 ***
06/02/2019 12:23:02 *** training ***
06/02/2019 12:23:03 step: 1160, epoch: 35, batch: 4, loss: 0.6718651652336121, acc: 84.375, f1: 69.3401735564207, r: 0.6966782290307547
06/02/2019 12:23:03 step: 1165, epoch: 35, batch: 9, loss: 0.48496919870376587, acc: 89.0625, f1: 74.50230666979436, r: 0.6031807405588346
06/02/2019 12:23:04 step: 1170, epoch: 35, batch: 14, loss: 0.5534833669662476, acc: 81.25, f1: 67.06625733297271, r: 0.504741869665301
06/02/2019 12:23:05 step: 1175, epoch: 35, batch: 19, loss: 0.5694476366043091, acc: 89.0625, f1: 89.05036630036629, r: 0.7815833402573295
06/02/2019 12:23:05 step: 1180, epoch: 35, batch: 24, loss: 0.5639206767082214, acc: 82.8125, f1: 71.5633423180593, r: 0.6368214593103759
06/02/2019 12:23:06 step: 1185, epoch: 35, batch: 29, loss: 0.5030621290206909, acc: 84.375, f1: 82.36796536796537, r: 0.6659692726744755
06/02/2019 12:23:06 *** evaluating ***
06/02/2019 12:23:06 step: 36, epoch: 35, acc: 55.98290598290598, f1: 31.97454230062926, r: 0.3607067966146417
06/02/2019 12:23:06 *** epoch: 37 ***
06/02/2019 12:23:06 *** training ***
06/02/2019 12:23:07 step: 1193, epoch: 36, batch: 4, loss: 0.5151168704032898, acc: 90.625, f1: 84.70388155262106, r: 0.6221270548689758
06/02/2019 12:23:08 step: 1198, epoch: 36, batch: 9, loss: 0.5852410793304443, acc: 89.0625, f1: 93.16585054289972, r: 0.6099925461619855
06/02/2019 12:23:08 step: 1203, epoch: 36, batch: 14, loss: 0.9179621338844299, acc: 85.9375, f1: 69.28113553113553, r: 0.6747662677425353
06/02/2019 12:23:09 step: 1208, epoch: 36, batch: 19, loss: 0.5515444278717041, acc: 82.8125, f1: 81.092555105713, r: 0.7837662222941915
06/02/2019 12:23:09 step: 1213, epoch: 36, batch: 24, loss: 0.6188297271728516, acc: 85.9375, f1: 87.69693568080665, r: 0.6723461845511575
06/02/2019 12:23:10 step: 1218, epoch: 36, batch: 29, loss: 0.9429484605789185, acc: 87.5, f1: 80.70638884924598, r: 0.6152644592692657
06/02/2019 12:23:10 *** evaluating ***
06/02/2019 12:23:10 step: 37, epoch: 36, acc: 55.55555555555556, f1: 28.98319823536408, r: 0.3506893479267825
06/02/2019 12:23:10 *** epoch: 38 ***
06/02/2019 12:23:10 *** training ***
06/02/2019 12:23:11 step: 1226, epoch: 37, batch: 4, loss: 0.5224719047546387, acc: 89.0625, f1: 87.68792576932111, r: 0.7282027384309752
06/02/2019 12:23:12 step: 1231, epoch: 37, batch: 9, loss: 0.4677026867866516, acc: 87.5, f1: 73.4692513368984, r: 0.6703263017439935
06/02/2019 12:23:12 step: 1236, epoch: 37, batch: 14, loss: 0.6053805351257324, acc: 87.5, f1: 87.16189140452816, r: 0.7437559673987988
06/02/2019 12:23:13 step: 1241, epoch: 37, batch: 19, loss: 0.5908445715904236, acc: 82.8125, f1: 64.94084547825459, r: 0.6291978809036459
06/02/2019 12:23:13 step: 1246, epoch: 37, batch: 24, loss: 0.5215920805931091, acc: 87.5, f1: 79.38304881701107, r: 0.5300707020719077
06/02/2019 12:23:14 step: 1251, epoch: 37, batch: 29, loss: 0.34466803073883057, acc: 93.75, f1: 90.62639992244623, r: 0.6953363215224027
06/02/2019 12:23:14 *** evaluating ***
06/02/2019 12:23:14 step: 38, epoch: 37, acc: 55.55555555555556, f1: 28.334500147633896, r: 0.3588827112787559
06/02/2019 12:23:14 *** epoch: 39 ***
06/02/2019 12:23:14 *** training ***
06/02/2019 12:23:15 step: 1259, epoch: 38, batch: 4, loss: 0.613777220249176, acc: 84.375, f1: 83.06689451558037, r: 0.7146096573823979
06/02/2019 12:23:16 step: 1264, epoch: 38, batch: 9, loss: 0.6029858589172363, acc: 81.25, f1: 67.30519480519482, r: 0.6846234150041961
06/02/2019 12:23:16 step: 1269, epoch: 38, batch: 14, loss: 0.6413651704788208, acc: 79.6875, f1: 77.78183414128114, r: 0.6865253206923141
06/02/2019 12:23:17 step: 1274, epoch: 38, batch: 19, loss: 0.46277573704719543, acc: 89.0625, f1: 93.68942979490646, r: 0.693829610960033
06/02/2019 12:23:17 step: 1279, epoch: 38, batch: 24, loss: 0.550195038318634, acc: 79.6875, f1: 60.447761194029844, r: 0.6510405087636749
06/02/2019 12:23:18 step: 1284, epoch: 38, batch: 29, loss: 0.5081887245178223, acc: 90.625, f1: 83.07738095238095, r: 0.66624640206892
06/02/2019 12:23:18 *** evaluating ***
06/02/2019 12:23:18 step: 39, epoch: 38, acc: 58.97435897435898, f1: 32.57851516566656, r: 0.3550580001798672
06/02/2019 12:23:18 *** epoch: 40 ***
06/02/2019 12:23:18 *** training ***
06/02/2019 12:23:19 step: 1292, epoch: 39, batch: 4, loss: 0.4961540699005127, acc: 87.5, f1: 84.18768533054248, r: 0.656293624979922
06/02/2019 12:23:20 step: 1297, epoch: 39, batch: 9, loss: 0.4663972556591034, acc: 93.75, f1: 89.15343915343915, r: 0.6734006651380763
06/02/2019 12:23:20 step: 1302, epoch: 39, batch: 14, loss: 0.5931273698806763, acc: 79.6875, f1: 77.84250063661828, r: 0.7239760842625972
06/02/2019 12:23:21 step: 1307, epoch: 39, batch: 19, loss: 0.5055534243583679, acc: 84.375, f1: 85.65003779289493, r: 0.6075516019819867
06/02/2019 12:23:21 step: 1312, epoch: 39, batch: 24, loss: 0.4628711938858032, acc: 92.1875, f1: 83.89603352009367, r: 0.6525413503994663
06/02/2019 12:23:22 step: 1317, epoch: 39, batch: 29, loss: 0.586582601070404, acc: 78.125, f1: 81.4904143475572, r: 0.6758425357642234
06/02/2019 12:23:22 *** evaluating ***
06/02/2019 12:23:22 step: 40, epoch: 39, acc: 57.26495726495726, f1: 29.46979491097138, r: 0.35008939182442617
06/02/2019 12:23:22 *** epoch: 41 ***
06/02/2019 12:23:22 *** training ***
06/02/2019 12:23:23 step: 1325, epoch: 40, batch: 4, loss: 0.31699398159980774, acc: 93.75, f1: 88.42432476943345, r: 0.7410123663838651
06/02/2019 12:23:24 step: 1330, epoch: 40, batch: 9, loss: 0.4162130355834961, acc: 89.0625, f1: 82.71010598596806, r: 0.6278400981535448
06/02/2019 12:23:24 step: 1335, epoch: 40, batch: 14, loss: 0.6655247211456299, acc: 89.0625, f1: 89.22308053656246, r: 0.5952193885284958
06/02/2019 12:23:25 step: 1340, epoch: 40, batch: 19, loss: 0.5913828611373901, acc: 85.9375, f1: 82.60026507309115, r: 0.7588362238882326
06/02/2019 12:23:25 step: 1345, epoch: 40, batch: 24, loss: 0.4360305666923523, acc: 89.0625, f1: 72.67298929141035, r: 0.7416400121557722
06/02/2019 12:23:26 step: 1350, epoch: 40, batch: 29, loss: 0.4662822484970093, acc: 85.9375, f1: 69.36026936026937, r: 0.5695455228423272
06/02/2019 12:23:26 *** evaluating ***
06/02/2019 12:23:26 step: 41, epoch: 40, acc: 56.41025641025641, f1: 30.376134420553093, r: 0.3543388013977411
06/02/2019 12:23:26 *** epoch: 42 ***
06/02/2019 12:23:26 *** training ***
06/02/2019 12:23:27 step: 1358, epoch: 41, batch: 4, loss: 0.4494493007659912, acc: 90.625, f1: 87.20179291607863, r: 0.6610059352206833
06/02/2019 12:23:28 step: 1363, epoch: 41, batch: 9, loss: 0.44773221015930176, acc: 89.0625, f1: 86.78746650175222, r: 0.6831655985709103
06/02/2019 12:23:28 step: 1368, epoch: 41, batch: 14, loss: 0.3588889241218567, acc: 96.875, f1: 95.12155318450718, r: 0.6803004701294003
06/02/2019 12:23:29 step: 1373, epoch: 41, batch: 19, loss: 0.4221879243850708, acc: 87.5, f1: 81.16832187420422, r: 0.6615131610850531
06/02/2019 12:23:30 step: 1378, epoch: 41, batch: 24, loss: 0.6646084189414978, acc: 76.5625, f1: 59.247645294156925, r: 0.6069357925874854
06/02/2019 12:23:30 step: 1383, epoch: 41, batch: 29, loss: 0.4691517651081085, acc: 87.5, f1: 86.0709706959707, r: 0.7261043527631093
06/02/2019 12:23:30 *** evaluating ***
06/02/2019 12:23:31 step: 42, epoch: 41, acc: 54.27350427350427, f1: 28.750322247236305, r: 0.35029108336449855
06/02/2019 12:23:31 *** epoch: 43 ***
06/02/2019 12:23:31 *** training ***
06/02/2019 12:23:31 step: 1391, epoch: 42, batch: 4, loss: 0.4494912028312683, acc: 90.625, f1: 88.93989523021781, r: 0.6957668287198179
06/02/2019 12:23:32 step: 1396, epoch: 42, batch: 9, loss: 0.4220676124095917, acc: 87.5, f1: 87.65322481193735, r: 0.6778267588117896
06/02/2019 12:23:32 step: 1401, epoch: 42, batch: 14, loss: 0.40861162543296814, acc: 95.3125, f1: 95.48759942848612, r: 0.6625166010551047
06/02/2019 12:23:33 step: 1406, epoch: 42, batch: 19, loss: 0.3829880654811859, acc: 92.1875, f1: 87.71825396825396, r: 0.8057093502941207
06/02/2019 12:23:34 step: 1411, epoch: 42, batch: 24, loss: 0.3749270737171173, acc: 93.75, f1: 89.22834712037472, r: 0.7547061772013702
06/02/2019 12:23:34 step: 1416, epoch: 42, batch: 29, loss: 0.4889502227306366, acc: 85.9375, f1: 87.82760250502186, r: 0.6909509078466979
06/02/2019 12:23:35 *** evaluating ***
06/02/2019 12:23:35 step: 43, epoch: 42, acc: 55.12820512820513, f1: 28.750043980678658, r: 0.3558981223602412
06/02/2019 12:23:35 *** epoch: 44 ***
06/02/2019 12:23:35 *** training ***
06/02/2019 12:23:35 step: 1424, epoch: 43, batch: 4, loss: 0.3774321675300598, acc: 92.1875, f1: 80.03072196620583, r: 0.671370377337467
06/02/2019 12:23:36 step: 1429, epoch: 43, batch: 9, loss: 0.40389183163642883, acc: 93.75, f1: 94.40686006203248, r: 0.5969059973050974
06/02/2019 12:23:37 step: 1434, epoch: 43, batch: 14, loss: 0.4449697732925415, acc: 89.0625, f1: 88.17460317460316, r: 0.6400770688818701
06/02/2019 12:23:37 step: 1439, epoch: 43, batch: 19, loss: 0.4108535647392273, acc: 90.625, f1: 85.29142231605286, r: 0.636460152789496
06/02/2019 12:23:38 step: 1444, epoch: 43, batch: 24, loss: 0.39957714080810547, acc: 85.9375, f1: 89.99372909698997, r: 0.7785337110979027
06/02/2019 12:23:38 step: 1449, epoch: 43, batch: 29, loss: 0.46238136291503906, acc: 89.0625, f1: 85.80061156752886, r: 0.7265280876836786
06/02/2019 12:23:39 *** evaluating ***
06/02/2019 12:23:39 step: 44, epoch: 43, acc: 51.28205128205128, f1: 29.89460196972652, r: 0.3481152604808032
06/02/2019 12:23:39 *** epoch: 45 ***
06/02/2019 12:23:39 *** training ***
06/02/2019 12:23:40 step: 1457, epoch: 44, batch: 4, loss: 0.4333416521549225, acc: 87.5, f1: 78.14968623108159, r: 0.7463369342939947
06/02/2019 12:23:40 step: 1462, epoch: 44, batch: 9, loss: 0.30129823088645935, acc: 95.3125, f1: 95.72964669738863, r: 0.6954153418927936
06/02/2019 12:23:41 step: 1467, epoch: 44, batch: 14, loss: 0.36488091945648193, acc: 92.1875, f1: 88.57183899200705, r: 0.6485175328877011
06/02/2019 12:23:41 step: 1472, epoch: 44, batch: 19, loss: 0.34789884090423584, acc: 95.3125, f1: 95.48001018589254, r: 0.6791110478547492
06/02/2019 12:23:42 step: 1477, epoch: 44, batch: 24, loss: 0.7052408456802368, acc: 93.75, f1: 91.86674669867946, r: 0.635292777498179
06/02/2019 12:23:42 step: 1482, epoch: 44, batch: 29, loss: 0.42159682512283325, acc: 93.75, f1: 86.93903318903318, r: 0.757605376461586
06/02/2019 12:23:43 *** evaluating ***
06/02/2019 12:23:43 step: 45, epoch: 44, acc: 54.27350427350427, f1: 27.98014098834531, r: 0.3372702964890926
06/02/2019 12:23:43 *** epoch: 46 ***
06/02/2019 12:23:43 *** training ***
06/02/2019 12:23:44 step: 1490, epoch: 45, batch: 4, loss: 0.3394850790500641, acc: 98.4375, f1: 96.85131195335276, r: 0.6435624671671885
06/02/2019 12:23:44 step: 1495, epoch: 45, batch: 9, loss: 0.3537684679031372, acc: 93.75, f1: 81.64721279003962, r: 0.7213587707475156
06/02/2019 12:23:45 step: 1500, epoch: 45, batch: 14, loss: 0.5023280382156372, acc: 87.5, f1: 83.45903578822833, r: 0.6994027822491983
06/02/2019 12:23:45 step: 1505, epoch: 45, batch: 19, loss: 0.3493982255458832, acc: 93.75, f1: 95.87301587301587, r: 0.6773495678493128
06/02/2019 12:23:46 step: 1510, epoch: 45, batch: 24, loss: 0.4383784532546997, acc: 90.625, f1: 82.59318638445006, r: 0.6515221894197509
06/02/2019 12:23:46 step: 1515, epoch: 45, batch: 29, loss: 0.3688211441040039, acc: 90.625, f1: 86.08086358086359, r: 0.6181839641251399
06/02/2019 12:23:47 *** evaluating ***
06/02/2019 12:23:47 step: 46, epoch: 45, acc: 54.700854700854705, f1: 26.46055530236609, r: 0.33514231866290384
06/02/2019 12:23:47 *** epoch: 47 ***
06/02/2019 12:23:47 *** training ***
06/02/2019 12:23:47 step: 1523, epoch: 46, batch: 4, loss: 0.3010605275630951, acc: 95.3125, f1: 96.37896825396825, r: 0.7817510042301294
06/02/2019 12:23:48 step: 1528, epoch: 46, batch: 9, loss: 0.34172749519348145, acc: 89.0625, f1: 91.04418569935811, r: 0.6500390661603604
06/02/2019 12:23:49 step: 1533, epoch: 46, batch: 14, loss: 0.41819995641708374, acc: 87.5, f1: 72.05921836583859, r: 0.6349492917970293
06/02/2019 12:23:49 step: 1538, epoch: 46, batch: 19, loss: 0.36382511258125305, acc: 89.0625, f1: 78.43654703260616, r: 0.6372556844834992
06/02/2019 12:23:50 step: 1543, epoch: 46, batch: 24, loss: 0.7498972415924072, acc: 89.0625, f1: 76.33620530172254, r: 0.5829589379291537
06/02/2019 12:23:50 step: 1548, epoch: 46, batch: 29, loss: 0.597602367401123, acc: 89.0625, f1: 76.3931623931624, r: 0.5407067214634449
06/02/2019 12:23:51 *** evaluating ***
06/02/2019 12:23:51 step: 47, epoch: 46, acc: 54.27350427350427, f1: 26.52363799503338, r: 0.3421263675290304
06/02/2019 12:23:51 *** epoch: 48 ***
06/02/2019 12:23:51 *** training ***
06/02/2019 12:23:52 step: 1556, epoch: 47, batch: 4, loss: 0.35762399435043335, acc: 93.75, f1: 81.20915032679738, r: 0.7695409715738836
06/02/2019 12:23:52 step: 1561, epoch: 47, batch: 9, loss: 0.8250186443328857, acc: 92.1875, f1: 89.31104818322864, r: 0.6590456234056605
06/02/2019 12:23:53 step: 1566, epoch: 47, batch: 14, loss: 0.3290362060070038, acc: 93.75, f1: 80.69205271242623, r: 0.6779477683117371
06/02/2019 12:23:53 step: 1571, epoch: 47, batch: 19, loss: 0.3532596230506897, acc: 96.875, f1: 91.0952380952381, r: 0.6222943332544958
06/02/2019 12:23:54 step: 1576, epoch: 47, batch: 24, loss: 0.31813177466392517, acc: 95.3125, f1: 83.51278954539823, r: 0.7214294554800457
06/02/2019 12:23:55 step: 1581, epoch: 47, batch: 29, loss: 0.29975059628486633, acc: 98.4375, f1: 98.01587301587303, r: 0.801897738277235
06/02/2019 12:23:55 *** evaluating ***
06/02/2019 12:23:55 step: 48, epoch: 47, acc: 55.98290598290598, f1: 29.56752425472778, r: 0.33099596450554286
06/02/2019 12:23:55 *** epoch: 49 ***
06/02/2019 12:23:55 *** training ***
06/02/2019 12:23:56 step: 1589, epoch: 48, batch: 4, loss: 0.40925875306129456, acc: 96.875, f1: 92.63888888888889, r: 0.658545877506068
06/02/2019 12:23:56 step: 1594, epoch: 48, batch: 9, loss: 0.34791916608810425, acc: 90.625, f1: 90.22284775979357, r: 0.6154573569359346
06/02/2019 12:23:57 step: 1599, epoch: 48, batch: 14, loss: 0.29349228739738464, acc: 95.3125, f1: 92.3342266199409, r: 0.6523519101985564
06/02/2019 12:23:58 step: 1604, epoch: 48, batch: 19, loss: 0.5033843517303467, acc: 84.375, f1: 83.99107142857143, r: 0.7497979463564358
06/02/2019 12:23:58 step: 1609, epoch: 48, batch: 24, loss: 0.2933429181575775, acc: 95.3125, f1: 93.32446007902398, r: 0.5993233937037887
06/02/2019 12:23:59 step: 1614, epoch: 48, batch: 29, loss: 0.3216322064399719, acc: 92.1875, f1: 90.2070888684161, r: 0.7146029665187389
06/02/2019 12:23:59 *** evaluating ***
06/02/2019 12:23:59 step: 49, epoch: 48, acc: 54.700854700854705, f1: 26.245987995492115, r: 0.33632389683529684
06/02/2019 12:23:59 *** epoch: 50 ***
06/02/2019 12:23:59 *** training ***
06/02/2019 12:24:00 step: 1622, epoch: 49, batch: 4, loss: 0.4893791377544403, acc: 89.0625, f1: 88.53019995877138, r: 0.7314761567775951
06/02/2019 12:24:01 step: 1627, epoch: 49, batch: 9, loss: 0.3380458652973175, acc: 95.3125, f1: 97.50649350649351, r: 0.6499877707087539
06/02/2019 12:24:01 step: 1632, epoch: 49, batch: 14, loss: 0.39315831661224365, acc: 93.75, f1: 94.96638075520065, r: 0.6660423968374719
06/02/2019 12:24:02 step: 1637, epoch: 49, batch: 19, loss: 0.2751106321811676, acc: 96.875, f1: 97.9709274292354, r: 0.6838024255860966
06/02/2019 12:24:02 step: 1642, epoch: 49, batch: 24, loss: 0.3334764540195465, acc: 96.875, f1: 95.73807951531921, r: 0.6287157915445285
06/02/2019 12:24:03 step: 1647, epoch: 49, batch: 29, loss: 0.44109874963760376, acc: 93.75, f1: 91.10582841195087, r: 0.6597026509943812
06/02/2019 12:24:03 *** evaluating ***
06/02/2019 12:24:03 step: 50, epoch: 49, acc: 52.13675213675214, f1: 26.3005148005148, r: 0.32042397516254284
06/02/2019 12:24:03 *** epoch: 51 ***
06/02/2019 12:24:03 *** training ***
06/02/2019 12:24:04 step: 1655, epoch: 50, batch: 4, loss: 0.2067810595035553, acc: 100.0, f1: 100.0, r: 0.6853938176249419
06/02/2019 12:24:04 step: 1660, epoch: 50, batch: 9, loss: 0.2933892011642456, acc: 95.3125, f1: 83.8147095959596, r: 0.7479222328665213
06/02/2019 12:24:05 step: 1665, epoch: 50, batch: 14, loss: 0.28146180510520935, acc: 93.75, f1: 81.79090504671899, r: 0.5612849035433932
06/02/2019 12:24:05 step: 1670, epoch: 50, batch: 19, loss: 0.2754763960838318, acc: 96.875, f1: 98.69791666666667, r: 0.7174918686192877
06/02/2019 12:24:06 step: 1675, epoch: 50, batch: 24, loss: 0.273344486951828, acc: 93.75, f1: 81.5585879236548, r: 0.7097155721075393
06/02/2019 12:24:07 step: 1680, epoch: 50, batch: 29, loss: 0.7039956450462341, acc: 89.0625, f1: 85.01346149810755, r: 0.7942705860337482
06/02/2019 12:24:07 *** evaluating ***
06/02/2019 12:24:07 step: 51, epoch: 50, acc: 55.98290598290598, f1: 31.312166901285, r: 0.34622757636403967
06/02/2019 12:24:07 *** epoch: 52 ***
06/02/2019 12:24:07 *** training ***
06/02/2019 12:24:08 step: 1688, epoch: 51, batch: 4, loss: 0.6480289697647095, acc: 87.5, f1: 82.41851822496984, r: 0.6109782019188057
06/02/2019 12:24:08 step: 1693, epoch: 51, batch: 9, loss: 0.3904561400413513, acc: 87.5, f1: 82.145473574045, r: 0.7143284069600203
06/02/2019 12:24:09 step: 1698, epoch: 51, batch: 14, loss: 0.3407209515571594, acc: 93.75, f1: 84.943879649762, r: 0.6931666894295208
06/02/2019 12:24:10 step: 1703, epoch: 51, batch: 19, loss: 0.29087692499160767, acc: 95.3125, f1: 97.53383458646617, r: 0.7105432500847547
06/02/2019 12:24:10 step: 1708, epoch: 51, batch: 24, loss: 0.22280481457710266, acc: 96.875, f1: 97.85210380448476, r: 0.5990970197148787
06/02/2019 12:24:11 step: 1713, epoch: 51, batch: 29, loss: 0.26634860038757324, acc: 96.875, f1: 97.66440704378994, r: 0.6979633268770442
06/02/2019 12:24:11 *** evaluating ***
06/02/2019 12:24:11 step: 52, epoch: 51, acc: 53.41880341880342, f1: 28.700725399274663, r: 0.33046409376698604
06/02/2019 12:24:11 *** epoch: 53 ***
06/02/2019 12:24:11 *** training ***
06/02/2019 12:24:12 step: 1721, epoch: 52, batch: 4, loss: 0.2094329297542572, acc: 98.4375, f1: 96.76470588235294, r: 0.827913015648433
06/02/2019 12:24:13 step: 1726, epoch: 52, batch: 9, loss: 0.24405770003795624, acc: 100.0, f1: 100.0, r: 0.7480447560328474
06/02/2019 12:24:13 step: 1731, epoch: 52, batch: 14, loss: 0.34315305948257446, acc: 92.1875, f1: 85.26925921883904, r: 0.6417107131740013
06/02/2019 12:24:14 step: 1736, epoch: 52, batch: 19, loss: 0.5505735278129578, acc: 96.875, f1: 97.62926986202567, r: 0.6785046498315209
06/02/2019 12:24:14 step: 1741, epoch: 52, batch: 24, loss: 0.315979540348053, acc: 95.3125, f1: 92.78002395926926, r: 0.7832557908997008
06/02/2019 12:24:15 step: 1746, epoch: 52, batch: 29, loss: 0.27700287103652954, acc: 96.875, f1: 94.19566234982057, r: 0.7319833891841211
06/02/2019 12:24:15 *** evaluating ***
06/02/2019 12:24:15 step: 53, epoch: 52, acc: 54.27350427350427, f1: 26.250065723749934, r: 0.32864386581668154
06/02/2019 12:24:15 *** epoch: 54 ***
06/02/2019 12:24:15 *** training ***
06/02/2019 12:24:16 step: 1754, epoch: 53, batch: 4, loss: 0.2909497618675232, acc: 95.3125, f1: 92.76094178824209, r: 0.6771441011711256
06/02/2019 12:24:16 step: 1759, epoch: 53, batch: 9, loss: 0.35055816173553467, acc: 93.75, f1: 93.90500487884208, r: 0.741878639571227
06/02/2019 12:24:17 step: 1764, epoch: 53, batch: 14, loss: 0.21724198758602142, acc: 100.0, f1: 100.0, r: 0.7158637141786477
06/02/2019 12:24:17 step: 1769, epoch: 53, batch: 19, loss: 0.40891343355178833, acc: 89.0625, f1: 87.49374331672468, r: 0.6856124907738746
06/02/2019 12:24:18 step: 1774, epoch: 53, batch: 24, loss: 0.3755122423171997, acc: 92.1875, f1: 77.79448621553884, r: 0.6929002144403671
06/02/2019 12:24:19 step: 1779, epoch: 53, batch: 29, loss: 0.28086018562316895, acc: 96.875, f1: 93.80952380952381, r: 0.6569428889075573
06/02/2019 12:24:19 *** evaluating ***
06/02/2019 12:24:19 step: 54, epoch: 53, acc: 53.41880341880342, f1: 27.066608281029538, r: 0.3299882624866205
06/02/2019 12:24:19 *** epoch: 55 ***
06/02/2019 12:24:19 *** training ***
06/02/2019 12:24:20 step: 1787, epoch: 54, batch: 4, loss: 0.23803767561912537, acc: 93.75, f1: 93.82846245749472, r: 0.67509677787486
06/02/2019 12:24:20 step: 1792, epoch: 54, batch: 9, loss: 0.22168859839439392, acc: 92.1875, f1: 91.57560568086883, r: 0.6731925477542571
06/02/2019 12:24:21 step: 1797, epoch: 54, batch: 14, loss: 0.2571021318435669, acc: 93.75, f1: 92.53787878787878, r: 0.7359898722047784
06/02/2019 12:24:21 step: 1802, epoch: 54, batch: 19, loss: 0.25288188457489014, acc: 95.3125, f1: 97.07562174667439, r: 0.8107193265238023
06/02/2019 12:24:22 step: 1807, epoch: 54, batch: 24, loss: 0.3596848249435425, acc: 90.625, f1: 91.83858998144713, r: 0.6452613180632197
06/02/2019 12:24:23 step: 1812, epoch: 54, batch: 29, loss: 0.2409009337425232, acc: 95.3125, f1: 97.3847662634848, r: 0.745751784563917
06/02/2019 12:24:23 *** evaluating ***
06/02/2019 12:24:23 step: 55, epoch: 54, acc: 52.56410256410257, f1: 26.00785847441133, r: 0.32902718672101905
06/02/2019 12:24:23 *** epoch: 56 ***
06/02/2019 12:24:23 *** training ***
06/02/2019 12:24:24 step: 1820, epoch: 55, batch: 4, loss: 0.24884405732154846, acc: 96.875, f1: 94.76981490270528, r: 0.6518538724246135
06/02/2019 12:24:25 step: 1825, epoch: 55, batch: 9, loss: 0.2966516315937042, acc: 93.75, f1: 95.115589198036, r: 0.79474095315117
06/02/2019 12:24:25 step: 1830, epoch: 55, batch: 14, loss: 0.3576779365539551, acc: 93.75, f1: 92.63045657134327, r: 0.6654263625803343
06/02/2019 12:24:26 step: 1835, epoch: 55, batch: 19, loss: 0.25824299454689026, acc: 96.875, f1: 96.75274725274726, r: 0.7439169203958762
06/02/2019 12:24:26 step: 1840, epoch: 55, batch: 24, loss: 0.2981272339820862, acc: 95.3125, f1: 93.86567463956057, r: 0.6833258572985123
06/02/2019 12:24:27 step: 1845, epoch: 55, batch: 29, loss: 0.522532045841217, acc: 95.3125, f1: 81.86803393784851, r: 0.6187536112582558
06/02/2019 12:24:27 *** evaluating ***
06/02/2019 12:24:27 step: 56, epoch: 55, acc: 52.991452991452995, f1: 28.945950512745455, r: 0.3206552828818988
06/02/2019 12:24:27 *** epoch: 57 ***
06/02/2019 12:24:27 *** training ***
06/02/2019 12:24:28 step: 1853, epoch: 56, batch: 4, loss: 0.5619601011276245, acc: 95.3125, f1: 95.20361990950227, r: 0.7017803981969882
06/02/2019 12:24:29 step: 1858, epoch: 56, batch: 9, loss: 0.3559989035129547, acc: 92.1875, f1: 89.92681607010876, r: 0.6928685978759407
06/02/2019 12:24:29 step: 1863, epoch: 56, batch: 14, loss: 0.28544455766677856, acc: 93.75, f1: 93.12030287771528, r: 0.7171457796785112
06/02/2019 12:24:30 step: 1868, epoch: 56, batch: 19, loss: 0.26513904333114624, acc: 95.3125, f1: 90.82103082103083, r: 0.557903100587843
06/02/2019 12:24:31 step: 1873, epoch: 56, batch: 24, loss: 0.22233732044696808, acc: 95.3125, f1: 97.04703996983409, r: 0.788398127893781
06/02/2019 12:24:31 step: 1878, epoch: 56, batch: 29, loss: 0.3084942698478699, acc: 90.625, f1: 86.62603041913387, r: 0.5557609701989586
06/02/2019 12:24:32 *** evaluating ***
06/02/2019 12:24:32 step: 57, epoch: 56, acc: 53.41880341880342, f1: 28.594313694470042, r: 0.32145427682038924
06/02/2019 12:24:32 *** epoch: 58 ***
06/02/2019 12:24:32 *** training ***
06/02/2019 12:24:32 step: 1886, epoch: 57, batch: 4, loss: 0.2592008709907532, acc: 93.75, f1: 95.51994301994301, r: 0.7101512612822923
06/02/2019 12:24:33 step: 1891, epoch: 57, batch: 9, loss: 0.33472561836242676, acc: 95.3125, f1: 95.53107344632768, r: 0.7544812488616959
06/02/2019 12:24:34 step: 1896, epoch: 57, batch: 14, loss: 0.21943369507789612, acc: 98.4375, f1: 98.9329064959317, r: 0.6554629034817454
06/02/2019 12:24:34 step: 1901, epoch: 57, batch: 19, loss: 0.2491314709186554, acc: 93.75, f1: 90.21736443158852, r: 0.655600468324258
06/02/2019 12:24:35 step: 1906, epoch: 57, batch: 24, loss: 0.25260651111602783, acc: 93.75, f1: 81.41654641654641, r: 0.7601732118852023
06/02/2019 12:24:36 step: 1911, epoch: 57, batch: 29, loss: 0.25274956226348877, acc: 95.3125, f1: 95.16819985569987, r: 0.7268683186949835
06/02/2019 12:24:36 *** evaluating ***
06/02/2019 12:24:36 step: 58, epoch: 57, acc: 55.55555555555556, f1: 26.994031796663375, r: 0.3237759172000975
06/02/2019 12:24:36 *** epoch: 59 ***
06/02/2019 12:24:36 *** training ***
06/02/2019 12:24:37 step: 1919, epoch: 58, batch: 4, loss: 0.2890458405017853, acc: 93.75, f1: 95.14599686028257, r: 0.6337294481994795
06/02/2019 12:24:38 step: 1924, epoch: 58, batch: 9, loss: 0.2565881311893463, acc: 93.75, f1: 81.80359323216464, r: 0.5828259751582042
06/02/2019 12:24:38 step: 1929, epoch: 58, batch: 14, loss: 0.22631260752677917, acc: 92.1875, f1: 81.74554928354321, r: 0.7281438908934091
06/02/2019 12:24:39 step: 1934, epoch: 58, batch: 19, loss: 0.4165785014629364, acc: 95.3125, f1: 94.94994879672298, r: 0.7810679965365072
06/02/2019 12:24:39 step: 1939, epoch: 58, batch: 24, loss: 0.280317097902298, acc: 95.3125, f1: 96.57616262879422, r: 0.7092126442179829
06/02/2019 12:24:40 step: 1944, epoch: 58, batch: 29, loss: 0.1694701761007309, acc: 98.4375, f1: 99.21142369991475, r: 0.7974803799924521
06/02/2019 12:24:40 *** evaluating ***
06/02/2019 12:24:41 step: 59, epoch: 58, acc: 52.991452991452995, f1: 26.27446781796168, r: 0.32048602012193345
06/02/2019 12:24:41 *** epoch: 60 ***
06/02/2019 12:24:41 *** training ***
06/02/2019 12:24:41 step: 1952, epoch: 59, batch: 4, loss: 0.5033052563667297, acc: 98.4375, f1: 99.23215898825654, r: 0.7456267959486824
06/02/2019 12:24:42 step: 1957, epoch: 59, batch: 9, loss: 0.25287190079689026, acc: 93.75, f1: 97.06876456876456, r: 0.7725686566333354
06/02/2019 12:24:42 step: 1962, epoch: 59, batch: 14, loss: 0.4121130108833313, acc: 100.0, f1: 100.0, r: 0.8295077041786874
06/02/2019 12:24:43 step: 1967, epoch: 59, batch: 19, loss: 0.2024892270565033, acc: 98.4375, f1: 98.40975351179434, r: 0.6835462402707072
06/02/2019 12:24:44 step: 1972, epoch: 59, batch: 24, loss: 0.2673214375972748, acc: 95.3125, f1: 89.07196969696969, r: 0.7535376258786909
06/02/2019 12:24:44 step: 1977, epoch: 59, batch: 29, loss: 0.22391408681869507, acc: 96.875, f1: 97.76334776334777, r: 0.7281054144127878
06/02/2019 12:24:45 *** evaluating ***
06/02/2019 12:24:45 step: 60, epoch: 59, acc: 54.700854700854705, f1: 26.035487464180164, r: 0.31474937681362364
06/02/2019 12:24:45 *** epoch: 61 ***
06/02/2019 12:24:45 *** training ***
06/02/2019 12:24:46 step: 1985, epoch: 60, batch: 4, loss: 0.17642998695373535, acc: 95.3125, f1: 93.39176133961669, r: 0.6609945589114695
06/02/2019 12:24:46 step: 1990, epoch: 60, batch: 9, loss: 0.3187843859195709, acc: 100.0, f1: 100.0, r: 0.7647135972489825
06/02/2019 12:24:47 step: 1995, epoch: 60, batch: 14, loss: 0.2860153317451477, acc: 96.875, f1: 92.35742705570291, r: 0.7987729001542557
06/02/2019 12:24:47 step: 2000, epoch: 60, batch: 19, loss: 0.21617251634597778, acc: 93.75, f1: 88.89649661306515, r: 0.7330993153054708
06/02/2019 12:24:48 step: 2005, epoch: 60, batch: 24, loss: 0.19752183556556702, acc: 96.875, f1: 97.96626984126985, r: 0.6591677076332663
06/02/2019 12:24:49 step: 2010, epoch: 60, batch: 29, loss: 0.2364465594291687, acc: 93.75, f1: 82.41750208855471, r: 0.7843695384944718
06/02/2019 12:24:49 *** evaluating ***
06/02/2019 12:24:49 step: 61, epoch: 60, acc: 52.56410256410257, f1: 25.658627173696214, r: 0.3131208517551113
06/02/2019 12:24:49 *** epoch: 62 ***
06/02/2019 12:24:49 *** training ***
06/02/2019 12:24:50 step: 2018, epoch: 61, batch: 4, loss: 0.22534406185150146, acc: 96.875, f1: 95.3658536585366, r: 0.6790555786184383
06/02/2019 12:24:50 step: 2023, epoch: 61, batch: 9, loss: 0.1497545838356018, acc: 96.875, f1: 94.0455486542443, r: 0.6687633851730133
06/02/2019 12:24:51 step: 2028, epoch: 61, batch: 14, loss: 0.22105425596237183, acc: 95.3125, f1: 77.5604089589052, r: 0.6645007321556589
06/02/2019 12:24:52 step: 2033, epoch: 61, batch: 19, loss: 0.1648108959197998, acc: 100.0, f1: 100.0, r: 0.7661453448493708
06/02/2019 12:24:52 step: 2038, epoch: 61, batch: 24, loss: 0.21924448013305664, acc: 98.4375, f1: 98.17056766209308, r: 0.7106317041402724
06/02/2019 12:24:53 step: 2043, epoch: 61, batch: 29, loss: 0.1815379410982132, acc: 100.0, f1: 100.0, r: 0.6501101485001405
06/02/2019 12:24:53 *** evaluating ***
06/02/2019 12:24:53 step: 62, epoch: 61, acc: 50.85470085470085, f1: 25.221210466728788, r: 0.29959800546052245
06/02/2019 12:24:53 *** epoch: 63 ***
06/02/2019 12:24:53 *** training ***
06/02/2019 12:24:54 step: 2051, epoch: 62, batch: 4, loss: 0.24855944514274597, acc: 95.3125, f1: 93.73015873015873, r: 0.8488930468123794
06/02/2019 12:24:55 step: 2056, epoch: 62, batch: 9, loss: 0.16305425763130188, acc: 98.4375, f1: 99.07493061979648, r: 0.7005059670320124
06/02/2019 12:24:55 step: 2061, epoch: 62, batch: 14, loss: 0.19705024361610413, acc: 96.875, f1: 96.6691804927099, r: 0.7864951400253425
06/02/2019 12:24:56 step: 2066, epoch: 62, batch: 19, loss: 0.22476553916931152, acc: 93.75, f1: 93.41062801932367, r: 0.6472942787450771
06/02/2019 12:24:56 step: 2071, epoch: 62, batch: 24, loss: 0.2342681884765625, acc: 100.0, f1: 100.0, r: 0.7474271985830063
06/02/2019 12:24:57 step: 2076, epoch: 62, batch: 29, loss: 0.2426912933588028, acc: 96.875, f1: 94.04928404928405, r: 0.8405752474398176
06/02/2019 12:24:57 *** evaluating ***
06/02/2019 12:24:58 step: 63, epoch: 62, acc: 53.84615384615385, f1: 25.863981034260398, r: 0.30520422886655724
06/02/2019 12:24:58 *** epoch: 64 ***
06/02/2019 12:24:58 *** training ***
06/02/2019 12:24:58 step: 2084, epoch: 63, batch: 4, loss: 0.23571279644966125, acc: 96.875, f1: 96.71010952643606, r: 0.6635898016271057
06/02/2019 12:24:59 step: 2089, epoch: 63, batch: 9, loss: 0.25327393412590027, acc: 93.75, f1: 90.82099996030786, r: 0.6918631947815345
06/02/2019 12:24:59 step: 2094, epoch: 63, batch: 14, loss: 0.18054622411727905, acc: 96.875, f1: 96.99450089475665, r: 0.8479087490008125
06/02/2019 12:25:00 step: 2099, epoch: 63, batch: 19, loss: 0.3988247215747833, acc: 98.4375, f1: 99.25925925925925, r: 0.714512139037778
06/02/2019 12:25:00 step: 2104, epoch: 63, batch: 24, loss: 0.3317432105541229, acc: 92.1875, f1: 88.3027279737806, r: 0.7574594229069977
06/02/2019 12:25:01 step: 2109, epoch: 63, batch: 29, loss: 0.24651511013507843, acc: 93.75, f1: 87.85347985347987, r: 0.649349032845879
06/02/2019 12:25:01 *** evaluating ***
06/02/2019 12:25:02 step: 64, epoch: 63, acc: 53.84615384615385, f1: 26.473352903874748, r: 0.30224297796378885
06/02/2019 12:25:02 *** epoch: 65 ***
06/02/2019 12:25:02 *** training ***
06/02/2019 12:25:02 step: 2117, epoch: 64, batch: 4, loss: 0.17234991490840912, acc: 100.0, f1: 100.0, r: 0.6425752030939561
06/02/2019 12:25:03 step: 2122, epoch: 64, batch: 9, loss: 0.31482750177383423, acc: 95.3125, f1: 94.21910430839003, r: 0.7602780554845017
06/02/2019 12:25:03 step: 2127, epoch: 64, batch: 14, loss: 0.2091524600982666, acc: 98.4375, f1: 98.94736842105263, r: 0.7958244517150865
06/02/2019 12:25:04 step: 2132, epoch: 64, batch: 19, loss: 0.5117001533508301, acc: 92.1875, f1: 89.3703790726817, r: 0.7785030767354075
06/02/2019 12:25:05 step: 2137, epoch: 64, batch: 24, loss: 0.2614111006259918, acc: 98.4375, f1: 97.64172335600907, r: 0.6606873053842652
06/02/2019 12:25:05 step: 2142, epoch: 64, batch: 29, loss: 0.1843874752521515, acc: 95.3125, f1: 95.18655947227376, r: 0.7203512293797366
06/02/2019 12:25:06 *** evaluating ***
06/02/2019 12:25:06 step: 65, epoch: 64, acc: 52.991452991452995, f1: 27.529098292806758, r: 0.31420946839170893
06/02/2019 12:25:06 *** epoch: 66 ***
06/02/2019 12:25:06 *** training ***
06/02/2019 12:25:06 step: 2150, epoch: 65, batch: 4, loss: 0.16994613409042358, acc: 98.4375, f1: 98.81342701014833, r: 0.6751322619793501
06/02/2019 12:25:07 step: 2155, epoch: 65, batch: 9, loss: 0.20816567540168762, acc: 98.4375, f1: 99.20343779478041, r: 0.6530501708087645
06/02/2019 12:25:08 step: 2160, epoch: 65, batch: 14, loss: 0.2376829832792282, acc: 96.875, f1: 83.72553266170287, r: 0.6294519358010915
06/02/2019 12:25:08 step: 2165, epoch: 65, batch: 19, loss: 0.2213478684425354, acc: 95.3125, f1: 96.34953161592506, r: 0.8071610558641994
06/02/2019 12:25:09 step: 2170, epoch: 65, batch: 24, loss: 0.19332990050315857, acc: 96.875, f1: 94.50218270655613, r: 0.7985936918132035
06/02/2019 12:25:10 step: 2175, epoch: 65, batch: 29, loss: 0.15659621357917786, acc: 96.875, f1: 83.9221710189452, r: 0.7816554280261884
06/02/2019 12:25:10 *** evaluating ***
06/02/2019 12:25:10 step: 66, epoch: 65, acc: 52.991452991452995, f1: 25.86353501254101, r: 0.29710354792150084
06/02/2019 12:25:10 *** epoch: 67 ***
06/02/2019 12:25:10 *** training ***
06/02/2019 12:25:11 step: 2183, epoch: 66, batch: 4, loss: 0.20418569445610046, acc: 96.875, f1: 95.8801336911936, r: 0.7356231205500045
06/02/2019 12:25:11 step: 2188, epoch: 66, batch: 9, loss: 0.16267989575862885, acc: 98.4375, f1: 94.44444444444444, r: 0.7214659286366882
06/02/2019 12:25:12 step: 2193, epoch: 66, batch: 14, loss: 0.13258042931556702, acc: 100.0, f1: 100.0, r: 0.6940044345563583
06/02/2019 12:25:12 step: 2198, epoch: 66, batch: 19, loss: 0.2172688990831375, acc: 100.0, f1: 100.0, r: 0.707775851109341
06/02/2019 12:25:13 step: 2203, epoch: 66, batch: 24, loss: 0.3097444474697113, acc: 92.1875, f1: 91.15143369175627, r: 0.7359095626367097
06/02/2019 12:25:14 step: 2208, epoch: 66, batch: 29, loss: 0.18250443041324615, acc: 98.4375, f1: 98.26839826839827, r: 0.7781867570197579
06/02/2019 12:25:14 *** evaluating ***
06/02/2019 12:25:14 step: 67, epoch: 66, acc: 54.700854700854705, f1: 28.12782262782263, r: 0.31728757769646204
06/02/2019 12:25:14 *** epoch: 68 ***
06/02/2019 12:25:14 *** training ***
06/02/2019 12:25:15 step: 2216, epoch: 67, batch: 4, loss: 0.16353315114974976, acc: 100.0, f1: 100.0, r: 0.6855166272923363
06/02/2019 12:25:15 step: 2221, epoch: 67, batch: 9, loss: 0.18116223812103271, acc: 96.875, f1: 84.34167215273206, r: 0.7664271018649654
06/02/2019 12:25:16 step: 2226, epoch: 67, batch: 14, loss: 0.18880581855773926, acc: 96.875, f1: 98.49498327759196, r: 0.7836491290970953
06/02/2019 12:25:17 step: 2231, epoch: 67, batch: 19, loss: 0.1392512023448944, acc: 100.0, f1: 100.0, r: 0.6886862999458161
06/02/2019 12:25:17 step: 2236, epoch: 67, batch: 24, loss: 0.14691303670406342, acc: 98.4375, f1: 97.47899159663865, r: 0.7217610793561984
06/02/2019 12:25:18 step: 2241, epoch: 67, batch: 29, loss: 0.14855501055717468, acc: 100.0, f1: 100.0, r: 0.6504175995996566
06/02/2019 12:25:18 *** evaluating ***
06/02/2019 12:25:18 step: 68, epoch: 67, acc: 54.700854700854705, f1: 27.46755083077953, r: 0.30963200926137535
06/02/2019 12:25:18 *** epoch: 69 ***
06/02/2019 12:25:18 *** training ***
06/02/2019 12:25:19 step: 2249, epoch: 68, batch: 4, loss: 0.4023488163948059, acc: 100.0, f1: 100.0, r: 0.553823533237298
06/02/2019 12:25:19 step: 2254, epoch: 68, batch: 9, loss: 0.1533719152212143, acc: 96.875, f1: 96.69642857142857, r: 0.7474002924763405
06/02/2019 12:25:20 step: 2259, epoch: 68, batch: 14, loss: 0.17310552299022675, acc: 98.4375, f1: 99.08733679807327, r: 0.6811264548004686
06/02/2019 12:25:21 step: 2264, epoch: 68, batch: 19, loss: 0.16287429630756378, acc: 100.0, f1: 100.0, r: 0.8134778977845168
06/02/2019 12:25:21 step: 2269, epoch: 68, batch: 24, loss: 0.21827904880046844, acc: 93.75, f1: 94.86419806522821, r: 0.6590218358079701
06/02/2019 12:25:22 step: 2274, epoch: 68, batch: 29, loss: 0.15143655240535736, acc: 98.4375, f1: 99.08700322234158, r: 0.7480576957051147
06/02/2019 12:25:22 *** evaluating ***
06/02/2019 12:25:22 step: 69, epoch: 68, acc: 53.84615384615385, f1: 28.309867617843587, r: 0.31058628744019035
06/02/2019 12:25:22 *** epoch: 70 ***
06/02/2019 12:25:22 *** training ***
06/02/2019 12:25:23 step: 2282, epoch: 69, batch: 4, loss: 0.16333432495594025, acc: 96.875, f1: 95.67346938775509, r: 0.7556252422519243
06/02/2019 12:25:24 step: 2287, epoch: 69, batch: 9, loss: 0.1729792356491089, acc: 98.4375, f1: 98.7878787878788, r: 0.7318435955154047
06/02/2019 12:25:24 step: 2292, epoch: 69, batch: 14, loss: 0.15646429359912872, acc: 100.0, f1: 100.0, r: 0.8200256039509879
06/02/2019 12:25:25 step: 2297, epoch: 69, batch: 19, loss: 0.15288390219211578, acc: 98.4375, f1: 98.93081761006289, r: 0.7929224846224298
06/02/2019 12:25:26 step: 2302, epoch: 69, batch: 24, loss: 0.17991679906845093, acc: 96.875, f1: 97.4881588999236, r: 0.6240631001204553
06/02/2019 12:25:26 step: 2307, epoch: 69, batch: 29, loss: 0.3811655342578888, acc: 98.4375, f1: 99.28193499622071, r: 0.754221942139667
06/02/2019 12:25:26 *** evaluating ***
06/02/2019 12:25:27 step: 70, epoch: 69, acc: 52.991452991452995, f1: 25.448466866810517, r: 0.31826312119701383
06/02/2019 12:25:27 *** epoch: 71 ***
06/02/2019 12:25:27 *** training ***
06/02/2019 12:25:27 step: 2315, epoch: 70, batch: 4, loss: 0.14783282577991486, acc: 100.0, f1: 100.0, r: 0.730310587068102
06/02/2019 12:25:28 step: 2320, epoch: 70, batch: 9, loss: 0.1824294477701187, acc: 100.0, f1: 100.0, r: 0.694250788328943
06/02/2019 12:25:29 step: 2325, epoch: 70, batch: 14, loss: 0.16962459683418274, acc: 100.0, f1: 100.0, r: 0.7992149057881266
06/02/2019 12:25:29 step: 2330, epoch: 70, batch: 19, loss: 0.19558365643024445, acc: 98.4375, f1: 97.0, r: 0.7790415802384495
06/02/2019 12:25:30 step: 2335, epoch: 70, batch: 24, loss: 0.16015052795410156, acc: 100.0, f1: 100.0, r: 0.7531269209324615
06/02/2019 12:25:30 step: 2340, epoch: 70, batch: 29, loss: 0.18849602341651917, acc: 96.875, f1: 96.34198001544941, r: 0.7297918943063504
06/02/2019 12:25:31 *** evaluating ***
06/02/2019 12:25:31 step: 71, epoch: 70, acc: 54.700854700854705, f1: 25.345301733295393, r: 0.312029603795256
06/02/2019 12:25:31 *** epoch: 72 ***
06/02/2019 12:25:31 *** training ***
06/02/2019 12:25:32 step: 2348, epoch: 71, batch: 4, loss: 0.14243412017822266, acc: 98.4375, f1: 99.12698412698413, r: 0.7992646775723189
06/02/2019 12:25:32 step: 2353, epoch: 71, batch: 9, loss: 0.2859216034412384, acc: 100.0, f1: 100.0, r: 0.850382976916457
06/02/2019 12:25:33 step: 2358, epoch: 71, batch: 14, loss: 0.40422341227531433, acc: 98.4375, f1: 96.65024630541872, r: 0.7070541791937135
06/02/2019 12:25:33 step: 2363, epoch: 71, batch: 19, loss: 0.14334453642368317, acc: 100.0, f1: 100.0, r: 0.6927366155799947
06/02/2019 12:25:34 step: 2368, epoch: 71, batch: 24, loss: 0.18902122974395752, acc: 96.875, f1: 98.33024118738403, r: 0.7288112449317101
06/02/2019 12:25:34 step: 2373, epoch: 71, batch: 29, loss: 0.241773784160614, acc: 98.4375, f1: 98.14921920185078, r: 0.7239721494299132
06/02/2019 12:25:35 *** evaluating ***
06/02/2019 12:25:35 step: 72, epoch: 71, acc: 55.12820512820513, f1: 27.504626395664566, r: 0.313622368466387
06/02/2019 12:25:35 *** epoch: 73 ***
06/02/2019 12:25:35 *** training ***
06/02/2019 12:25:35 step: 2381, epoch: 72, batch: 4, loss: 0.3957919776439667, acc: 98.4375, f1: 99.22222222222223, r: 0.7967567184859282
06/02/2019 12:25:36 step: 2386, epoch: 72, batch: 9, loss: 0.1413639783859253, acc: 96.875, f1: 98.08111278699513, r: 0.7236389356973308
06/02/2019 12:25:37 step: 2391, epoch: 72, batch: 14, loss: 0.1754073053598404, acc: 100.0, f1: 100.0, r: 0.8029672312855684
06/02/2019 12:25:37 step: 2396, epoch: 72, batch: 19, loss: 0.1668805181980133, acc: 98.4375, f1: 97.0, r: 0.7961980506320698
06/02/2019 12:25:38 step: 2401, epoch: 72, batch: 24, loss: 0.11488886177539825, acc: 100.0, f1: 100.0, r: 0.7431628115942515
06/02/2019 12:25:38 step: 2406, epoch: 72, batch: 29, loss: 0.14225007593631744, acc: 98.4375, f1: 99.20343779478041, r: 0.6462022565705384
06/02/2019 12:25:39 *** evaluating ***
06/02/2019 12:25:39 step: 73, epoch: 72, acc: 51.70940170940172, f1: 24.598598956831925, r: 0.29957841688433456
06/02/2019 12:25:39 *** epoch: 74 ***
06/02/2019 12:25:39 *** training ***
06/02/2019 12:25:39 step: 2414, epoch: 73, batch: 4, loss: 0.1888829469680786, acc: 96.875, f1: 96.4360726945154, r: 0.7399417421836391
06/02/2019 12:25:40 step: 2419, epoch: 73, batch: 9, loss: 0.12944546341896057, acc: 98.4375, f1: 94.7454844006568, r: 0.686819285030925
06/02/2019 12:25:41 step: 2424, epoch: 73, batch: 14, loss: 0.18420648574829102, acc: 93.75, f1: 91.27604953186348, r: 0.6856957612202781
06/02/2019 12:25:41 step: 2429, epoch: 73, batch: 19, loss: 0.14039058983325958, acc: 98.4375, f1: 97.87644787644788, r: 0.7457137882858864
06/02/2019 12:25:42 step: 2434, epoch: 73, batch: 24, loss: 0.1437482237815857, acc: 98.4375, f1: 98.14471243042672, r: 0.6464335652062231
06/02/2019 12:25:43 step: 2439, epoch: 73, batch: 29, loss: 0.19876091182231903, acc: 100.0, f1: 100.0, r: 0.7861402085063509
06/02/2019 12:25:43 *** evaluating ***
06/02/2019 12:25:43 step: 74, epoch: 73, acc: 55.55555555555556, f1: 29.374823988259752, r: 0.31445627250963093
06/02/2019 12:25:43 *** epoch: 75 ***
06/02/2019 12:25:43 *** training ***
06/02/2019 12:25:44 step: 2447, epoch: 74, batch: 4, loss: 0.13429495692253113, acc: 98.4375, f1: 98.99874843554444, r: 0.7634860833421876
06/02/2019 12:25:44 step: 2452, epoch: 74, batch: 9, loss: 0.14160725474357605, acc: 98.4375, f1: 98.04639804639805, r: 0.6796048813444721
06/02/2019 12:25:45 step: 2457, epoch: 74, batch: 14, loss: 0.13582253456115723, acc: 96.875, f1: 98.15167825371908, r: 0.6989958790408342
06/02/2019 12:25:46 step: 2462, epoch: 74, batch: 19, loss: 0.16841751337051392, acc: 98.4375, f1: 97.73242630385487, r: 0.6720747711194379
06/02/2019 12:25:46 step: 2467, epoch: 74, batch: 24, loss: 0.2003101408481598, acc: 95.3125, f1: 91.62533830736595, r: 0.729244064878088
06/02/2019 12:25:47 step: 2472, epoch: 74, batch: 29, loss: 0.15998074412345886, acc: 100.0, f1: 100.0, r: 0.8085520046108792
06/02/2019 12:25:47 *** evaluating ***
06/02/2019 12:25:47 step: 75, epoch: 74, acc: 54.27350427350427, f1: 26.21726123483672, r: 0.30487293937945026
06/02/2019 12:25:47 *** epoch: 76 ***
06/02/2019 12:25:47 *** training ***
06/02/2019 12:25:48 step: 2480, epoch: 75, batch: 4, loss: 0.18581172823905945, acc: 100.0, f1: 100.0, r: 0.7669102675667817
06/02/2019 12:25:49 step: 2485, epoch: 75, batch: 9, loss: 0.1436222344636917, acc: 98.4375, f1: 98.10874704491727, r: 0.7067726710304282
06/02/2019 12:25:49 step: 2490, epoch: 75, batch: 14, loss: 0.14531201124191284, acc: 100.0, f1: 100.0, r: 0.805701254580818
06/02/2019 12:25:50 step: 2495, epoch: 75, batch: 19, loss: 0.16713789105415344, acc: 100.0, f1: 100.0, r: 0.7793140651195523
06/02/2019 12:25:50 step: 2500, epoch: 75, batch: 24, loss: 0.43594998121261597, acc: 96.875, f1: 94.5411033085281, r: 0.7755612882227438
06/02/2019 12:25:51 step: 2505, epoch: 75, batch: 29, loss: 0.09817429631948471, acc: 100.0, f1: 100.0, r: 0.7555899898429335
06/02/2019 12:25:51 *** evaluating ***
06/02/2019 12:25:51 step: 76, epoch: 75, acc: 52.991452991452995, f1: 25.207555057201287, r: 0.3063289473078783
06/02/2019 12:25:51 *** epoch: 77 ***
06/02/2019 12:25:51 *** training ***
06/02/2019 12:25:52 step: 2513, epoch: 76, batch: 4, loss: 0.4321547746658325, acc: 98.4375, f1: 98.60742705570291, r: 0.7905300724303845
06/02/2019 12:25:52 step: 2518, epoch: 76, batch: 9, loss: 0.17694447934627533, acc: 96.875, f1: 98.67037724180581, r: 0.7107173884395276
06/02/2019 12:25:53 step: 2523, epoch: 76, batch: 14, loss: 0.1254548281431198, acc: 98.4375, f1: 98.8421052631579, r: 0.8218873099136884
06/02/2019 12:25:54 step: 2528, epoch: 76, batch: 19, loss: 0.13628461956977844, acc: 98.4375, f1: 97.97979797979798, r: 0.6931283192724641
06/02/2019 12:25:54 step: 2533, epoch: 76, batch: 24, loss: 0.13923531770706177, acc: 100.0, f1: 100.0, r: 0.7664980384717129
06/02/2019 12:25:55 step: 2538, epoch: 76, batch: 29, loss: 0.15865837037563324, acc: 98.4375, f1: 97.77777777777779, r: 0.7580978289461234
06/02/2019 12:25:55 *** evaluating ***
06/02/2019 12:25:55 step: 77, epoch: 76, acc: 54.27350427350427, f1: 26.572931049231208, r: 0.3022762425387244
06/02/2019 12:25:55 *** epoch: 78 ***
06/02/2019 12:25:55 *** training ***
06/02/2019 12:25:56 step: 2546, epoch: 77, batch: 4, loss: 0.15052109956741333, acc: 100.0, f1: 100.0, r: 0.7162402676955573
06/02/2019 12:25:56 step: 2551, epoch: 77, batch: 9, loss: 0.13116636872291565, acc: 100.0, f1: 100.0, r: 0.8541072696734615
06/02/2019 12:25:57 step: 2556, epoch: 77, batch: 14, loss: 0.12349668145179749, acc: 98.4375, f1: 99.19289749798224, r: 0.8060183459329749
06/02/2019 12:25:58 step: 2561, epoch: 77, batch: 19, loss: 0.15160834789276123, acc: 100.0, f1: 100.0, r: 0.6665861349712893
06/02/2019 12:25:58 step: 2566, epoch: 77, batch: 24, loss: 0.17388945817947388, acc: 98.4375, f1: 98.40848806366047, r: 0.6614659280045962
06/02/2019 12:25:59 step: 2571, epoch: 77, batch: 29, loss: 0.12863299250602722, acc: 98.4375, f1: 95.76719576719577, r: 0.6775002851014931
06/02/2019 12:25:59 *** evaluating ***
06/02/2019 12:25:59 step: 78, epoch: 77, acc: 52.13675213675214, f1: 25.82075090019015, r: 0.29274119357461725
06/02/2019 12:25:59 *** epoch: 79 ***
06/02/2019 12:25:59 *** training ***
06/02/2019 12:26:00 step: 2579, epoch: 78, batch: 4, loss: 0.12319085747003555, acc: 100.0, f1: 100.0, r: 0.8201353398921459
06/02/2019 12:26:01 step: 2584, epoch: 78, batch: 9, loss: 0.13583053648471832, acc: 98.4375, f1: 99.23784738358583, r: 0.6783575912417866
06/02/2019 12:26:01 step: 2589, epoch: 78, batch: 14, loss: 0.18325646221637726, acc: 95.3125, f1: 89.50113378684807, r: 0.6899450067794931
06/02/2019 12:26:02 step: 2594, epoch: 78, batch: 19, loss: 0.13493502140045166, acc: 96.875, f1: 91.32714904143474, r: 0.6386256033848147
06/02/2019 12:26:02 step: 2599, epoch: 78, batch: 24, loss: 0.294751912355423, acc: 98.4375, f1: 94.04761904761905, r: 0.7872790089421574
06/02/2019 12:26:03 step: 2604, epoch: 78, batch: 29, loss: 0.14498484134674072, acc: 96.875, f1: 83.06763285024155, r: 0.7563405185377616
06/02/2019 12:26:03 *** evaluating ***
06/02/2019 12:26:03 step: 79, epoch: 78, acc: 53.41880341880342, f1: 25.67764587298288, r: 0.3011269390736811
06/02/2019 12:26:03 *** epoch: 80 ***
06/02/2019 12:26:03 *** training ***
06/02/2019 12:26:04 step: 2612, epoch: 79, batch: 4, loss: 0.14884807169437408, acc: 98.4375, f1: 96.53846153846153, r: 0.7858033160028612
06/02/2019 12:26:05 step: 2617, epoch: 79, batch: 9, loss: 0.12658849358558655, acc: 100.0, f1: 100.0, r: 0.6723623046098677
06/02/2019 12:26:05 step: 2622, epoch: 79, batch: 14, loss: 0.20232893526554108, acc: 100.0, f1: 100.0, r: 0.6914641955032933
06/02/2019 12:26:06 step: 2627, epoch: 79, batch: 19, loss: 0.17744015157222748, acc: 100.0, f1: 100.0, r: 0.7878073095930218
06/02/2019 12:26:07 step: 2632, epoch: 79, batch: 24, loss: 0.19930477440357208, acc: 95.3125, f1: 93.5982814178303, r: 0.6415389580113321
06/02/2019 12:26:07 step: 2637, epoch: 79, batch: 29, loss: 0.13151662051677704, acc: 98.4375, f1: 99.23521913913127, r: 0.6753774438968028
06/02/2019 12:26:07 *** evaluating ***
06/02/2019 12:26:08 step: 80, epoch: 79, acc: 52.56410256410257, f1: 28.28239920676896, r: 0.2951960506320405
06/02/2019 12:26:08 *** epoch: 81 ***
06/02/2019 12:26:08 *** training ***
06/02/2019 12:26:08 step: 2645, epoch: 80, batch: 4, loss: 0.12209676951169968, acc: 100.0, f1: 100.0, r: 0.8050666898126673
06/02/2019 12:26:09 step: 2650, epoch: 80, batch: 9, loss: 0.1934778094291687, acc: 100.0, f1: 100.0, r: 0.8002539385029501
06/02/2019 12:26:09 step: 2655, epoch: 80, batch: 14, loss: 0.3142002522945404, acc: 98.4375, f1: 98.54312354312354, r: 0.7696321220638989
06/02/2019 12:26:10 step: 2660, epoch: 80, batch: 19, loss: 0.15816602110862732, acc: 98.4375, f1: 98.47939175670268, r: 0.6976109431633787
06/02/2019 12:26:10 step: 2665, epoch: 80, batch: 24, loss: 0.19504980742931366, acc: 100.0, f1: 100.0, r: 0.73561307864043
06/02/2019 12:26:11 step: 2670, epoch: 80, batch: 29, loss: 0.09950520098209381, acc: 98.4375, f1: 98.89012208657047, r: 0.7292569989074383
06/02/2019 12:26:12 *** evaluating ***
06/02/2019 12:26:12 step: 81, epoch: 80, acc: 53.41880341880342, f1: 26.492520013162213, r: 0.2993696036243473
06/02/2019 12:26:12 *** epoch: 82 ***
06/02/2019 12:26:12 *** training ***
06/02/2019 12:26:12 step: 2678, epoch: 81, batch: 4, loss: 0.14742392301559448, acc: 98.4375, f1: 99.13288969550312, r: 0.5773129027597262
06/02/2019 12:26:13 step: 2683, epoch: 81, batch: 9, loss: 0.08481359481811523, acc: 100.0, f1: 100.0, r: 0.7108757802618698
06/02/2019 12:26:14 step: 2688, epoch: 81, batch: 14, loss: 0.1483493447303772, acc: 98.4375, f1: 98.51851851851852, r: 0.6806711941483155
06/02/2019 12:26:14 step: 2693, epoch: 81, batch: 19, loss: 0.2889055609703064, acc: 98.4375, f1: 98.36363636363636, r: 0.8238365882167301
06/02/2019 12:26:15 step: 2698, epoch: 81, batch: 24, loss: 0.12372057884931564, acc: 98.4375, f1: 99.15824915824916, r: 0.7303328023611187
06/02/2019 12:26:16 step: 2703, epoch: 81, batch: 29, loss: 0.10664375126361847, acc: 100.0, f1: 100.0, r: 0.6521685806401514
06/02/2019 12:26:16 *** evaluating ***
06/02/2019 12:26:16 step: 82, epoch: 81, acc: 52.13675213675214, f1: 27.021583995179878, r: 0.2928334808344915
06/02/2019 12:26:16 *** epoch: 83 ***
06/02/2019 12:26:16 *** training ***
06/02/2019 12:26:17 step: 2711, epoch: 82, batch: 4, loss: 0.08997426927089691, acc: 100.0, f1: 100.0, r: 0.732312077495698
06/02/2019 12:26:17 step: 2716, epoch: 82, batch: 9, loss: 0.18051910400390625, acc: 96.875, f1: 96.86111111111111, r: 0.8276558001822405
06/02/2019 12:26:18 step: 2721, epoch: 82, batch: 14, loss: 0.13815869390964508, acc: 96.875, f1: 96.08465608465609, r: 0.7519735026428469
06/02/2019 12:26:19 step: 2726, epoch: 82, batch: 19, loss: 0.33478936553001404, acc: 100.0, f1: 100.0, r: 0.821416637437981
06/02/2019 12:26:19 step: 2731, epoch: 82, batch: 24, loss: 0.43508848547935486, acc: 98.4375, f1: 96.53846153846153, r: 0.7639838473303443
06/02/2019 12:26:20 step: 2736, epoch: 82, batch: 29, loss: 0.11795756220817566, acc: 100.0, f1: 100.0, r: 0.6267711567319272
06/02/2019 12:26:20 *** evaluating ***
06/02/2019 12:26:20 step: 83, epoch: 82, acc: 52.13675213675214, f1: 30.865451586367428, r: 0.3005366628420787
06/02/2019 12:26:20 *** epoch: 84 ***
06/02/2019 12:26:20 *** training ***
06/02/2019 12:26:21 step: 2744, epoch: 83, batch: 4, loss: 0.12722191214561462, acc: 98.4375, f1: 96.3718820861678, r: 0.7397790641304572
06/02/2019 12:26:22 step: 2749, epoch: 83, batch: 9, loss: 0.16359061002731323, acc: 96.875, f1: 95.15651874785001, r: 0.7751891530601861
06/02/2019 12:26:22 step: 2754, epoch: 83, batch: 14, loss: 0.10915479809045792, acc: 100.0, f1: 100.0, r: 0.6704966877415264
06/02/2019 12:26:23 step: 2759, epoch: 83, batch: 19, loss: 0.1599830687046051, acc: 96.875, f1: 94.2262838436882, r: 0.6177679005877321
06/02/2019 12:26:23 step: 2764, epoch: 83, batch: 24, loss: 0.15987098217010498, acc: 98.4375, f1: 99.17287014061208, r: 0.690052080407151
06/02/2019 12:26:24 step: 2769, epoch: 83, batch: 29, loss: 0.15497523546218872, acc: 98.4375, f1: 98.76750700280112, r: 0.7514351184497021
06/02/2019 12:26:24 *** evaluating ***
06/02/2019 12:26:24 step: 84, epoch: 83, acc: 54.700854700854705, f1: 30.09668138344609, r: 0.2966209029586336
06/02/2019 12:26:24 *** epoch: 85 ***
06/02/2019 12:26:24 *** training ***
06/02/2019 12:26:25 step: 2777, epoch: 84, batch: 4, loss: 0.11873269081115723, acc: 100.0, f1: 100.0, r: 0.6898880955412802
06/02/2019 12:26:26 step: 2782, epoch: 84, batch: 9, loss: 0.15613678097724915, acc: 96.875, f1: 94.57792207792208, r: 0.7648238459520889
06/02/2019 12:26:26 step: 2787, epoch: 84, batch: 14, loss: 0.1850677728652954, acc: 98.4375, f1: 98.98838004101161, r: 0.7143657929841329
06/02/2019 12:26:27 step: 2792, epoch: 84, batch: 19, loss: 0.35539957880973816, acc: 98.4375, f1: 98.36363636363636, r: 0.7665544689744169
06/02/2019 12:26:27 step: 2797, epoch: 84, batch: 24, loss: 0.16490286588668823, acc: 96.875, f1: 85.26785714285714, r: 0.7304099190375479
06/02/2019 12:26:28 step: 2802, epoch: 84, batch: 29, loss: 0.0911002904176712, acc: 100.0, f1: 100.0, r: 0.7290627547954096
06/02/2019 12:26:28 *** evaluating ***
06/02/2019 12:26:28 step: 85, epoch: 84, acc: 53.84615384615385, f1: 25.80723341923563, r: 0.30150403254068275
06/02/2019 12:26:28 *** epoch: 86 ***
06/02/2019 12:26:28 *** training ***
06/02/2019 12:26:29 step: 2810, epoch: 85, batch: 4, loss: 0.12426787614822388, acc: 100.0, f1: 100.0, r: 0.6874493447542376
06/02/2019 12:26:30 step: 2815, epoch: 85, batch: 9, loss: 0.15386927127838135, acc: 98.4375, f1: 97.94871794871796, r: 0.664747994853693
06/02/2019 12:26:30 step: 2820, epoch: 85, batch: 14, loss: 0.10402711480855942, acc: 100.0, f1: 100.0, r: 0.6643469621525669
06/02/2019 12:26:31 step: 2825, epoch: 85, batch: 19, loss: 0.14801040291786194, acc: 98.4375, f1: 95.09803921568627, r: 0.806987212628921
06/02/2019 12:26:31 step: 2830, epoch: 85, batch: 24, loss: 0.10988026857376099, acc: 100.0, f1: 100.0, r: 0.7283569673670268
06/02/2019 12:26:32 step: 2835, epoch: 85, batch: 29, loss: 0.16849806904792786, acc: 100.0, f1: 100.0, r: 0.8329788470117827
06/02/2019 12:26:32 *** evaluating ***
06/02/2019 12:26:33 step: 86, epoch: 85, acc: 51.70940170940172, f1: 28.723159468296767, r: 0.29668009885881097
06/02/2019 12:26:33 *** epoch: 87 ***
06/02/2019 12:26:33 *** training ***
06/02/2019 12:26:33 step: 2843, epoch: 86, batch: 4, loss: 0.06788232177495956, acc: 100.0, f1: 100.0, r: 0.6983110765389201
06/02/2019 12:26:34 step: 2848, epoch: 86, batch: 9, loss: 0.1816844344139099, acc: 98.4375, f1: 99.2918961447679, r: 0.8108290673754142
06/02/2019 12:26:34 step: 2853, epoch: 86, batch: 14, loss: 0.0776437446475029, acc: 100.0, f1: 100.0, r: 0.7224014972206497
06/02/2019 12:26:35 step: 2858, epoch: 86, batch: 19, loss: 0.09748801589012146, acc: 100.0, f1: 100.0, r: 0.8363705975421696
06/02/2019 12:26:35 step: 2863, epoch: 86, batch: 24, loss: 0.15952825546264648, acc: 98.4375, f1: 99.20079920079921, r: 0.6975749048035386
06/02/2019 12:26:36 step: 2868, epoch: 86, batch: 29, loss: 0.11477343738079071, acc: 98.4375, f1: 99.20879120879121, r: 0.6625436410559578
06/02/2019 12:26:36 *** evaluating ***
06/02/2019 12:26:37 step: 87, epoch: 86, acc: 53.84615384615385, f1: 28.37118942986899, r: 0.2946924586688777
06/02/2019 12:26:37 *** epoch: 88 ***
06/02/2019 12:26:37 *** training ***
06/02/2019 12:26:37 step: 2876, epoch: 87, batch: 4, loss: 0.1097087562084198, acc: 100.0, f1: 100.0, r: 0.8581414707107037
06/02/2019 12:26:38 step: 2881, epoch: 87, batch: 9, loss: 0.112724170088768, acc: 100.0, f1: 100.0, r: 0.7563906097549346
06/02/2019 12:26:38 step: 2886, epoch: 87, batch: 14, loss: 0.09305839985609055, acc: 100.0, f1: 100.0, r: 0.6839749056700486
06/02/2019 12:26:39 step: 2891, epoch: 87, batch: 19, loss: 0.15224888920783997, acc: 98.4375, f1: 98.9737274220033, r: 0.7884457389330485
06/02/2019 12:26:39 step: 2896, epoch: 87, batch: 24, loss: 0.09175840020179749, acc: 100.0, f1: 100.0, r: 0.6629287072112549
06/02/2019 12:26:40 step: 2901, epoch: 87, batch: 29, loss: 0.1213981881737709, acc: 98.4375, f1: 95.55555555555556, r: 0.7263192128649931
06/02/2019 12:26:40 *** evaluating ***
06/02/2019 12:26:40 step: 88, epoch: 87, acc: 53.84615384615385, f1: 26.696573350790064, r: 0.2905813089807765
06/02/2019 12:26:40 *** epoch: 89 ***
06/02/2019 12:26:40 *** training ***
06/02/2019 12:26:41 step: 2909, epoch: 88, batch: 4, loss: 0.10876969248056412, acc: 100.0, f1: 100.0, r: 0.7774468671386408
06/02/2019 12:26:42 step: 2914, epoch: 88, batch: 9, loss: 0.07756181061267853, acc: 100.0, f1: 100.0, r: 0.7425262702961339
06/02/2019 12:26:42 step: 2919, epoch: 88, batch: 14, loss: 0.09963440150022507, acc: 100.0, f1: 100.0, r: 0.8551134251551198
06/02/2019 12:26:43 step: 2924, epoch: 88, batch: 19, loss: 0.4390285015106201, acc: 100.0, f1: 100.0, r: 0.8120209341169871
06/02/2019 12:26:43 step: 2929, epoch: 88, batch: 24, loss: 0.13088920712471008, acc: 100.0, f1: 100.0, r: 0.7474132502400107
06/02/2019 12:26:44 step: 2934, epoch: 88, batch: 29, loss: 0.09288572520017624, acc: 100.0, f1: 100.0, r: 0.753741779919997
06/02/2019 12:26:44 *** evaluating ***
06/02/2019 12:26:44 step: 89, epoch: 88, acc: 53.84615384615385, f1: 29.75311703572573, r: 0.2951025116851414
06/02/2019 12:26:44 *** epoch: 90 ***
06/02/2019 12:26:44 *** training ***
06/02/2019 12:26:45 step: 2942, epoch: 89, batch: 4, loss: 0.18905454874038696, acc: 95.3125, f1: 90.55648652422846, r: 0.6468104759920745
06/02/2019 12:26:46 step: 2947, epoch: 89, batch: 9, loss: 0.13041025400161743, acc: 100.0, f1: 100.0, r: 0.7323866713177299
06/02/2019 12:26:46 step: 2952, epoch: 89, batch: 14, loss: 0.0906238704919815, acc: 100.0, f1: 100.0, r: 0.7590115372074662
06/02/2019 12:26:47 step: 2957, epoch: 89, batch: 19, loss: 0.12166544795036316, acc: 100.0, f1: 100.0, r: 0.7585507830706475
06/02/2019 12:26:47 step: 2962, epoch: 89, batch: 24, loss: 0.20440548658370972, acc: 100.0, f1: 100.0, r: 0.6863627678195963
06/02/2019 12:26:48 step: 2967, epoch: 89, batch: 29, loss: 0.12825912237167358, acc: 98.4375, f1: 97.49835418038182, r: 0.6800310167433401
06/02/2019 12:26:48 *** evaluating ***
06/02/2019 12:26:49 step: 90, epoch: 89, acc: 54.27350427350427, f1: 29.314697450886705, r: 0.30606615257891545
06/02/2019 12:26:49 *** epoch: 91 ***
06/02/2019 12:26:49 *** training ***
06/02/2019 12:26:49 step: 2975, epoch: 90, batch: 4, loss: 0.16533204913139343, acc: 98.4375, f1: 98.65896815049356, r: 0.7031433109153195
06/02/2019 12:26:50 step: 2980, epoch: 90, batch: 9, loss: 0.11351903527975082, acc: 96.875, f1: 93.97523753979246, r: 0.7762370672592237
06/02/2019 12:26:50 step: 2985, epoch: 90, batch: 14, loss: 0.14736509323120117, acc: 100.0, f1: 100.0, r: 0.8252754241613697
06/02/2019 12:26:51 step: 2990, epoch: 90, batch: 19, loss: 0.07502684742212296, acc: 100.0, f1: 100.0, r: 0.7949151341422686
06/02/2019 12:26:52 step: 2995, epoch: 90, batch: 24, loss: 0.13811978697776794, acc: 98.4375, f1: 98.38383838383838, r: 0.753671843441611
06/02/2019 12:26:52 step: 3000, epoch: 90, batch: 29, loss: 0.14679452776908875, acc: 96.875, f1: 97.79569892473118, r: 0.5837717899536109
06/02/2019 12:26:53 *** evaluating ***
06/02/2019 12:26:53 step: 91, epoch: 90, acc: 50.427350427350426, f1: 26.828991554095786, r: 0.2962482477664638
06/02/2019 12:26:53 *** epoch: 92 ***
06/02/2019 12:26:53 *** training ***
06/02/2019 12:26:54 step: 3008, epoch: 91, batch: 4, loss: 0.33258819580078125, acc: 100.0, f1: 100.0, r: 0.6517540425084105
06/02/2019 12:26:54 step: 3013, epoch: 91, batch: 9, loss: 0.10083991289138794, acc: 100.0, f1: 100.0, r: 0.687529667357033
06/02/2019 12:26:55 step: 3018, epoch: 91, batch: 14, loss: 0.10719559341669083, acc: 100.0, f1: 100.0, r: 0.6655150009787643
06/02/2019 12:26:55 step: 3023, epoch: 91, batch: 19, loss: 0.1070803701877594, acc: 98.4375, f1: 98.38383838383838, r: 0.6943291164150597
06/02/2019 12:26:56 step: 3028, epoch: 91, batch: 24, loss: 0.1427769958972931, acc: 95.3125, f1: 94.82680860805861, r: 0.8167066516092919
06/02/2019 12:26:57 step: 3033, epoch: 91, batch: 29, loss: 0.1549939215183258, acc: 96.875, f1: 95.02314814814814, r: 0.7923519632902325
06/02/2019 12:26:57 *** evaluating ***
06/02/2019 12:26:57 step: 92, epoch: 91, acc: 51.70940170940172, f1: 25.579954559295437, r: 0.3029907176274541
06/02/2019 12:26:57 *** epoch: 93 ***
06/02/2019 12:26:57 *** training ***
06/02/2019 12:26:57 step: 3041, epoch: 92, batch: 4, loss: 0.13048973679542542, acc: 98.4375, f1: 99.20343779478041, r: 0.7145804695298036
06/02/2019 12:26:58 step: 3046, epoch: 92, batch: 9, loss: 0.326619952917099, acc: 98.4375, f1: 98.80261248185775, r: 0.8113637624343897
06/02/2019 12:26:59 step: 3051, epoch: 92, batch: 14, loss: 0.09887494146823883, acc: 100.0, f1: 100.0, r: 0.8065601878435372
06/02/2019 12:26:59 step: 3056, epoch: 92, batch: 19, loss: 0.17430008947849274, acc: 100.0, f1: 100.0, r: 0.7842998222133934
06/02/2019 12:27:00 step: 3061, epoch: 92, batch: 24, loss: 0.11486676335334778, acc: 100.0, f1: 100.0, r: 0.7319110220203646
06/02/2019 12:27:01 step: 3066, epoch: 92, batch: 29, loss: 0.09686373174190521, acc: 100.0, f1: 100.0, r: 0.7258189942622668
06/02/2019 12:27:01 *** evaluating ***
06/02/2019 12:27:01 step: 93, epoch: 92, acc: 53.84615384615385, f1: 27.024100928279033, r: 0.2948173677886943
06/02/2019 12:27:01 *** epoch: 94 ***
06/02/2019 12:27:01 *** training ***
06/02/2019 12:27:02 step: 3074, epoch: 93, batch: 4, loss: 0.11002655327320099, acc: 100.0, f1: 100.0, r: 0.8146954310520408
06/02/2019 12:27:03 step: 3079, epoch: 93, batch: 9, loss: 0.1232965737581253, acc: 98.4375, f1: 96.57142857142857, r: 0.6814286951974792
06/02/2019 12:27:03 step: 3084, epoch: 93, batch: 14, loss: 0.17923706769943237, acc: 100.0, f1: 100.0, r: 0.7559473143352693
06/02/2019 12:27:04 step: 3089, epoch: 93, batch: 19, loss: 0.11824333667755127, acc: 98.4375, f1: 99.19078742608154, r: 0.6801091163028404
06/02/2019 12:27:04 step: 3094, epoch: 93, batch: 24, loss: 0.11441519856452942, acc: 100.0, f1: 100.0, r: 0.7867780547440814
06/02/2019 12:27:05 step: 3099, epoch: 93, batch: 29, loss: 0.1403893232345581, acc: 100.0, f1: 100.0, r: 0.8404748342581939
06/02/2019 12:27:05 *** evaluating ***
06/02/2019 12:27:05 step: 94, epoch: 93, acc: 48.717948717948715, f1: 23.854359203303392, r: 0.27249678851984205
06/02/2019 12:27:05 *** epoch: 95 ***
06/02/2019 12:27:05 *** training ***
06/02/2019 12:27:06 step: 3107, epoch: 94, batch: 4, loss: 0.1385638415813446, acc: 100.0, f1: 100.0, r: 0.6689805347846292
06/02/2019 12:27:07 step: 3112, epoch: 94, batch: 9, loss: 0.1050821840763092, acc: 100.0, f1: 100.0, r: 0.6985662411625326
06/02/2019 12:27:07 step: 3117, epoch: 94, batch: 14, loss: 0.07722160220146179, acc: 100.0, f1: 100.0, r: 0.745601190785387
06/02/2019 12:27:08 step: 3122, epoch: 94, batch: 19, loss: 0.13709041476249695, acc: 98.4375, f1: 96.73469387755101, r: 0.7392916082541674
06/02/2019 12:27:09 step: 3127, epoch: 94, batch: 24, loss: 0.09739808738231659, acc: 96.875, f1: 97.03970070102794, r: 0.6827519827309358
06/02/2019 12:27:09 step: 3132, epoch: 94, batch: 29, loss: 0.1269921064376831, acc: 98.4375, f1: 99.21866751135043, r: 0.6901031608626256
06/02/2019 12:27:09 *** evaluating ***
06/02/2019 12:27:10 step: 95, epoch: 94, acc: 52.56410256410257, f1: 27.48699438626623, r: 0.2904423755392589
06/02/2019 12:27:10 *** epoch: 96 ***
06/02/2019 12:27:10 *** training ***
06/02/2019 12:27:10 step: 3140, epoch: 95, batch: 4, loss: 0.11899587512016296, acc: 98.4375, f1: 97.95918367346938, r: 0.7853098019719634
06/02/2019 12:27:11 step: 3145, epoch: 95, batch: 9, loss: 0.10166031122207642, acc: 100.0, f1: 100.0, r: 0.8092778554575162
06/02/2019 12:27:11 step: 3150, epoch: 95, batch: 14, loss: 0.0836862102150917, acc: 100.0, f1: 100.0, r: 0.7576806200981883
06/02/2019 12:27:12 step: 3155, epoch: 95, batch: 19, loss: 0.08842073380947113, acc: 100.0, f1: 100.0, r: 0.772333742045784
06/02/2019 12:27:12 step: 3160, epoch: 95, batch: 24, loss: 0.12069933116436005, acc: 96.875, f1: 96.04855275443511, r: 0.6642962967791048
06/02/2019 12:27:13 step: 3165, epoch: 95, batch: 29, loss: 0.09428553283214569, acc: 98.4375, f1: 96.3718820861678, r: 0.697736623156528
06/02/2019 12:27:13 *** evaluating ***
06/02/2019 12:27:14 step: 96, epoch: 95, acc: 49.572649572649574, f1: 25.96942788051005, r: 0.2817815023265549
06/02/2019 12:27:14 *** epoch: 97 ***
06/02/2019 12:27:14 *** training ***
06/02/2019 12:27:14 step: 3173, epoch: 96, batch: 4, loss: 0.12216722220182419, acc: 100.0, f1: 100.0, r: 0.7756238457611562
06/02/2019 12:27:15 step: 3178, epoch: 96, batch: 9, loss: 0.1019381433725357, acc: 100.0, f1: 100.0, r: 0.7267020379061905
06/02/2019 12:27:15 step: 3183, epoch: 96, batch: 14, loss: 0.09458815306425095, acc: 98.4375, f1: 99.25490196078431, r: 0.8197307293607331
06/02/2019 12:27:16 step: 3188, epoch: 96, batch: 19, loss: 0.116373211145401, acc: 100.0, f1: 100.0, r: 0.7442758884821998
06/02/2019 12:27:16 step: 3193, epoch: 96, batch: 24, loss: 0.08775424212217331, acc: 100.0, f1: 100.0, r: 0.7600130925130456
06/02/2019 12:27:17 step: 3198, epoch: 96, batch: 29, loss: 0.20755983889102936, acc: 100.0, f1: 100.0, r: 0.7128753052700949
06/02/2019 12:27:17 *** evaluating ***
06/02/2019 12:27:17 step: 97, epoch: 96, acc: 47.863247863247864, f1: 25.078991178573585, r: 0.286931457126303
06/02/2019 12:27:17 *** epoch: 98 ***
06/02/2019 12:27:17 *** training ***
06/02/2019 12:27:18 step: 3206, epoch: 97, batch: 4, loss: 0.11775065213441849, acc: 98.4375, f1: 97.57236227824464, r: 0.7182737352857463
06/02/2019 12:27:19 step: 3211, epoch: 97, batch: 9, loss: 0.0991855263710022, acc: 98.4375, f1: 98.80745341614906, r: 0.7200188376217014
06/02/2019 12:27:19 step: 3216, epoch: 97, batch: 14, loss: 0.09208429604768753, acc: 100.0, f1: 100.0, r: 0.7400695440950896
06/02/2019 12:27:20 step: 3221, epoch: 97, batch: 19, loss: 0.10686320066452026, acc: 98.4375, f1: 99.20181405895691, r: 0.6784041202901538
06/02/2019 12:27:21 step: 3226, epoch: 97, batch: 24, loss: 0.0945548564195633, acc: 100.0, f1: 100.0, r: 0.6739554927907212
06/02/2019 12:27:21 step: 3231, epoch: 97, batch: 29, loss: 0.0871514230966568, acc: 100.0, f1: 100.0, r: 0.7520924327233168
06/02/2019 12:27:21 *** evaluating ***
06/02/2019 12:27:22 step: 98, epoch: 97, acc: 53.41880341880342, f1: 24.7983690251174, r: 0.28969535633499344
06/02/2019 12:27:22 *** epoch: 99 ***
06/02/2019 12:27:22 *** training ***
06/02/2019 12:27:22 step: 3239, epoch: 98, batch: 4, loss: 0.11252003908157349, acc: 100.0, f1: 100.0, r: 0.647883201150899
06/02/2019 12:27:23 step: 3244, epoch: 98, batch: 9, loss: 0.09670969843864441, acc: 100.0, f1: 100.0, r: 0.6644404421536309
06/02/2019 12:27:23 step: 3249, epoch: 98, batch: 14, loss: 0.07226130366325378, acc: 98.4375, f1: 97.95186891961085, r: 0.6958863302285458
06/02/2019 12:27:24 step: 3254, epoch: 98, batch: 19, loss: 0.10248871147632599, acc: 100.0, f1: 100.0, r: 0.6394870812731108
06/02/2019 12:27:25 step: 3259, epoch: 98, batch: 24, loss: 0.13694734871387482, acc: 98.4375, f1: 99.1484593837535, r: 0.7398165176147249
06/02/2019 12:27:25 step: 3264, epoch: 98, batch: 29, loss: 0.0897870808839798, acc: 100.0, f1: 100.0, r: 0.736606151055506
06/02/2019 12:27:26 *** evaluating ***
06/02/2019 12:27:26 step: 99, epoch: 98, acc: 49.14529914529914, f1: 24.02493474142587, r: 0.27273079552616253
06/02/2019 12:27:26 *** epoch: 100 ***
06/02/2019 12:27:26 *** training ***
06/02/2019 12:27:26 step: 3272, epoch: 99, batch: 4, loss: 0.10793742537498474, acc: 100.0, f1: 100.0, r: 0.6698474710385188
06/02/2019 12:27:27 step: 3277, epoch: 99, batch: 9, loss: 0.16387939453125, acc: 96.875, f1: 97.23261234889142, r: 0.7859553869939876
06/02/2019 12:27:28 step: 3282, epoch: 99, batch: 14, loss: 0.42658647894859314, acc: 100.0, f1: 100.0, r: 0.7928074763194568
06/02/2019 12:27:28 step: 3287, epoch: 99, batch: 19, loss: 0.09128723293542862, acc: 100.0, f1: 100.0, r: 0.6110452572876411
06/02/2019 12:27:29 step: 3292, epoch: 99, batch: 24, loss: 0.18071703612804413, acc: 100.0, f1: 100.0, r: 0.7888770392381944
06/02/2019 12:27:29 step: 3297, epoch: 99, batch: 29, loss: 0.13982874155044556, acc: 98.4375, f1: 98.60681114551085, r: 0.8042014843105866
06/02/2019 12:27:30 *** evaluating ***
06/02/2019 12:27:30 step: 100, epoch: 99, acc: 51.70940170940172, f1: 24.717652900468, r: 0.2818560130820659
06/02/2019 12:27:30 *** epoch: 101 ***
06/02/2019 12:27:30 *** training ***
06/02/2019 12:27:31 step: 3305, epoch: 100, batch: 4, loss: 0.08684781193733215, acc: 100.0, f1: 100.0, r: 0.7651496693882447
06/02/2019 12:27:31 step: 3310, epoch: 100, batch: 9, loss: 0.13129329681396484, acc: 100.0, f1: 100.0, r: 0.6956556118489073
06/02/2019 12:27:32 step: 3315, epoch: 100, batch: 14, loss: 0.09434572607278824, acc: 98.4375, f1: 99.12462006079028, r: 0.7024865382804594
06/02/2019 12:27:32 step: 3320, epoch: 100, batch: 19, loss: 0.10979357361793518, acc: 100.0, f1: 100.0, r: 0.7202853347199911
06/02/2019 12:27:33 step: 3325, epoch: 100, batch: 24, loss: 0.09093198925256729, acc: 98.4375, f1: 99.1951219512195, r: 0.8291814308444271
06/02/2019 12:27:34 step: 3330, epoch: 100, batch: 29, loss: 0.07873336970806122, acc: 100.0, f1: 100.0, r: 0.8137191182385606
06/02/2019 12:27:34 *** evaluating ***
06/02/2019 12:27:34 step: 101, epoch: 100, acc: 51.28205128205128, f1: 26.226473932594995, r: 0.28119367309945204
06/02/2019 12:27:34 *** epoch: 102 ***
06/02/2019 12:27:34 *** training ***
06/02/2019 12:27:35 step: 3338, epoch: 101, batch: 4, loss: 0.08234089612960815, acc: 100.0, f1: 100.0, r: 0.7548239852083729
06/02/2019 12:27:35 step: 3343, epoch: 101, batch: 9, loss: 0.11727657169103622, acc: 100.0, f1: 100.0, r: 0.6800648379244896
06/02/2019 12:27:36 step: 3348, epoch: 101, batch: 14, loss: 0.08214884996414185, acc: 100.0, f1: 100.0, r: 0.6640214899115838
06/02/2019 12:27:36 step: 3353, epoch: 101, batch: 19, loss: 0.1361694633960724, acc: 100.0, f1: 100.0, r: 0.7599139222611919
06/02/2019 12:27:37 step: 3358, epoch: 101, batch: 24, loss: 0.10066553205251694, acc: 98.4375, f1: 98.49498327759196, r: 0.7673662921051607
06/02/2019 12:27:37 step: 3363, epoch: 101, batch: 29, loss: 0.11187206208705902, acc: 98.4375, f1: 97.27272727272728, r: 0.8619719048461754
06/02/2019 12:27:38 *** evaluating ***
06/02/2019 12:27:38 step: 102, epoch: 101, acc: 49.572649572649574, f1: 27.295059658437616, r: 0.2853013920760466
06/02/2019 12:27:38 *** epoch: 103 ***
06/02/2019 12:27:38 *** training ***
06/02/2019 12:27:39 step: 3371, epoch: 102, batch: 4, loss: 0.09984773397445679, acc: 98.4375, f1: 95.40229885057472, r: 0.7418357211295954
06/02/2019 12:27:39 step: 3376, epoch: 102, batch: 9, loss: 0.0873599573969841, acc: 98.4375, f1: 98.00453514739228, r: 0.7225004736153544
06/02/2019 12:27:40 step: 3381, epoch: 102, batch: 14, loss: 0.08688473701477051, acc: 100.0, f1: 100.0, r: 0.6593489341132033
06/02/2019 12:27:40 step: 3386, epoch: 102, batch: 19, loss: 0.06367602199316025, acc: 100.0, f1: 100.0, r: 0.7979553529935234
06/02/2019 12:27:41 step: 3391, epoch: 102, batch: 24, loss: 0.11371386051177979, acc: 98.4375, f1: 98.86178861788618, r: 0.8256050717437722
06/02/2019 12:27:41 step: 3396, epoch: 102, batch: 29, loss: 0.11588679254055023, acc: 98.4375, f1: 99.09634551495017, r: 0.6923923829393546
06/02/2019 12:27:42 *** evaluating ***
06/02/2019 12:27:42 step: 103, epoch: 102, acc: 52.991452991452995, f1: 25.691368582366813, r: 0.2831189674109072
06/02/2019 12:27:42 *** epoch: 104 ***
06/02/2019 12:27:42 *** training ***
06/02/2019 12:27:42 step: 3404, epoch: 103, batch: 4, loss: 0.14269810914993286, acc: 98.4375, f1: 97.33806566104703, r: 0.6861858110435748
06/02/2019 12:27:43 step: 3409, epoch: 103, batch: 9, loss: 0.11087112873792648, acc: 98.4375, f1: 84.61538461538461, r: 0.67287598775373
06/02/2019 12:27:44 step: 3414, epoch: 103, batch: 14, loss: 0.08527594804763794, acc: 100.0, f1: 100.0, r: 0.6972038251383424
06/02/2019 12:27:44 step: 3419, epoch: 103, batch: 19, loss: 0.10679690539836884, acc: 98.4375, f1: 98.66666666666667, r: 0.7246483423335437
06/02/2019 12:27:45 step: 3424, epoch: 103, batch: 24, loss: 0.12193875759840012, acc: 98.4375, f1: 98.97129583540733, r: 0.7256128026563053
06/02/2019 12:27:45 step: 3429, epoch: 103, batch: 29, loss: 0.10191087424755096, acc: 98.4375, f1: 99.12698412698413, r: 0.8056243156130469
06/02/2019 12:27:46 *** evaluating ***
06/02/2019 12:27:46 step: 104, epoch: 103, acc: 53.41880341880342, f1: 26.1484410677959, r: 0.2772621423236512
06/02/2019 12:27:46 *** epoch: 105 ***
06/02/2019 12:27:46 *** training ***
06/02/2019 12:27:47 step: 3437, epoch: 104, batch: 4, loss: 0.08762124925851822, acc: 98.4375, f1: 98.01587301587303, r: 0.7850332521364367
06/02/2019 12:27:47 step: 3442, epoch: 104, batch: 9, loss: 0.0873410552740097, acc: 100.0, f1: 100.0, r: 0.6905777056063874
06/02/2019 12:27:48 step: 3447, epoch: 104, batch: 14, loss: 0.12980180978775024, acc: 100.0, f1: 100.0, r: 0.6857085529891793
06/02/2019 12:27:48 step: 3452, epoch: 104, batch: 19, loss: 0.0889931470155716, acc: 98.4375, f1: 97.83549783549783, r: 0.8075343182333824
06/02/2019 12:27:49 step: 3457, epoch: 104, batch: 24, loss: 0.07945043593645096, acc: 100.0, f1: 100.0, r: 0.7311118792984355
06/02/2019 12:27:50 step: 3462, epoch: 104, batch: 29, loss: 0.07274553924798965, acc: 100.0, f1: 100.0, r: 0.7521591279870496
06/02/2019 12:27:50 *** evaluating ***
06/02/2019 12:27:50 step: 105, epoch: 104, acc: 50.85470085470085, f1: 27.41607954595998, r: 0.28204711615461514
06/02/2019 12:27:50 *** epoch: 106 ***
06/02/2019 12:27:50 *** training ***
06/02/2019 12:27:51 step: 3470, epoch: 105, batch: 4, loss: 0.07370427250862122, acc: 100.0, f1: 100.0, r: 0.8109286264338131
06/02/2019 12:27:51 step: 3475, epoch: 105, batch: 9, loss: 0.07918038219213486, acc: 100.0, f1: 100.0, r: 0.7221774640868749
06/02/2019 12:27:52 step: 3480, epoch: 105, batch: 14, loss: 0.0947522521018982, acc: 100.0, f1: 100.0, r: 0.7436905009967093
06/02/2019 12:27:53 step: 3485, epoch: 105, batch: 19, loss: 0.14790748059749603, acc: 100.0, f1: 100.0, r: 0.6680154746316621
06/02/2019 12:27:53 step: 3490, epoch: 105, batch: 24, loss: 0.0990540161728859, acc: 100.0, f1: 100.0, r: 0.5752259362865872
06/02/2019 12:27:54 step: 3495, epoch: 105, batch: 29, loss: 0.14700329303741455, acc: 100.0, f1: 100.0, r: 0.6237709380902328
06/02/2019 12:27:54 *** evaluating ***
06/02/2019 12:27:54 step: 106, epoch: 105, acc: 53.41880341880342, f1: 27.715984086623624, r: 0.2889056607118354
06/02/2019 12:27:54 *** epoch: 107 ***
06/02/2019 12:27:54 *** training ***
06/02/2019 12:27:55 step: 3503, epoch: 106, batch: 4, loss: 0.13708719611167908, acc: 98.4375, f1: 99.06910132474042, r: 0.6922820995050388
06/02/2019 12:27:55 step: 3508, epoch: 106, batch: 9, loss: 0.10374218225479126, acc: 100.0, f1: 100.0, r: 0.8116390861149569
06/02/2019 12:27:56 step: 3513, epoch: 106, batch: 14, loss: 0.3695656955242157, acc: 98.4375, f1: 96.65024630541872, r: 0.6271716087851867
06/02/2019 12:27:56 step: 3518, epoch: 106, batch: 19, loss: 0.11346933990716934, acc: 96.875, f1: 97.59027777777777, r: 0.8089479743156772
06/02/2019 12:27:57 step: 3523, epoch: 106, batch: 24, loss: 0.09730957448482513, acc: 100.0, f1: 100.0, r: 0.703796618123979
06/02/2019 12:27:58 step: 3528, epoch: 106, batch: 29, loss: 0.09926681220531464, acc: 100.0, f1: 100.0, r: 0.6972849330500098
06/02/2019 12:27:58 *** evaluating ***
06/02/2019 12:27:58 step: 107, epoch: 106, acc: 52.13675213675214, f1: 23.916080547613166, r: 0.26992902566255134
06/02/2019 12:27:58 *** epoch: 108 ***
06/02/2019 12:27:58 *** training ***
06/02/2019 12:27:59 step: 3536, epoch: 107, batch: 4, loss: 0.09469807893037796, acc: 100.0, f1: 100.0, r: 0.6166515999550317
06/02/2019 12:28:00 step: 3541, epoch: 107, batch: 9, loss: 0.14864987134933472, acc: 100.0, f1: 100.0, r: 0.8140786346651077
06/02/2019 12:28:00 step: 3546, epoch: 107, batch: 14, loss: 0.11390098929405212, acc: 100.0, f1: 100.0, r: 0.6690553792748591
06/02/2019 12:28:01 step: 3551, epoch: 107, batch: 19, loss: 0.1315295249223709, acc: 100.0, f1: 100.0, r: 0.7567888189435343
06/02/2019 12:28:01 step: 3556, epoch: 107, batch: 24, loss: 0.08856774866580963, acc: 100.0, f1: 100.0, r: 0.7045831809510005
06/02/2019 12:28:02 step: 3561, epoch: 107, batch: 29, loss: 0.398651123046875, acc: 96.875, f1: 93.95711500974659, r: 0.6750151852794799
06/02/2019 12:28:02 *** evaluating ***
06/02/2019 12:28:02 step: 108, epoch: 107, acc: 52.56410256410257, f1: 24.208200012547838, r: 0.27457877929164193
06/02/2019 12:28:02 *** epoch: 109 ***
06/02/2019 12:28:02 *** training ***
06/02/2019 12:28:03 step: 3569, epoch: 108, batch: 4, loss: 0.13714808225631714, acc: 98.4375, f1: 99.03044993182851, r: 0.6589649557765973
06/02/2019 12:28:04 step: 3574, epoch: 108, batch: 9, loss: 0.10226592421531677, acc: 100.0, f1: 100.0, r: 0.7826013671376039
06/02/2019 12:28:04 step: 3579, epoch: 108, batch: 14, loss: 0.11168855428695679, acc: 100.0, f1: 100.0, r: 0.684733985754316
06/02/2019 12:28:05 step: 3584, epoch: 108, batch: 19, loss: 0.11454343795776367, acc: 100.0, f1: 100.0, r: 0.6823530010755328
06/02/2019 12:28:05 step: 3589, epoch: 108, batch: 24, loss: 0.11232024431228638, acc: 100.0, f1: 100.0, r: 0.6659752436116194
06/02/2019 12:28:06 step: 3594, epoch: 108, batch: 29, loss: 0.11554938554763794, acc: 98.4375, f1: 97.37373737373738, r: 0.6778294634594983
06/02/2019 12:28:06 *** evaluating ***
06/02/2019 12:28:07 step: 109, epoch: 108, acc: 52.56410256410257, f1: 24.21349921349921, r: 0.27316965462711096
06/02/2019 12:28:07 *** epoch: 110 ***
06/02/2019 12:28:07 *** training ***
06/02/2019 12:28:07 step: 3602, epoch: 109, batch: 4, loss: 0.09684601426124573, acc: 100.0, f1: 100.0, r: 0.8107282431576498
06/02/2019 12:28:08 step: 3607, epoch: 109, batch: 9, loss: 0.11349757760763168, acc: 100.0, f1: 100.0, r: 0.7842039989977848
06/02/2019 12:28:08 step: 3612, epoch: 109, batch: 14, loss: 0.11722448468208313, acc: 98.4375, f1: 99.22067268252665, r: 0.786479658157533
06/02/2019 12:28:09 step: 3617, epoch: 109, batch: 19, loss: 0.10232323408126831, acc: 100.0, f1: 100.0, r: 0.7044618733191023
06/02/2019 12:28:09 step: 3622, epoch: 109, batch: 24, loss: 0.09826375544071198, acc: 100.0, f1: 100.0, r: 0.6840004223891692
06/02/2019 12:28:10 step: 3627, epoch: 109, batch: 29, loss: 0.07162528485059738, acc: 100.0, f1: 100.0, r: 0.6984891737226824
06/02/2019 12:28:10 *** evaluating ***
06/02/2019 12:28:11 step: 110, epoch: 109, acc: 52.991452991452995, f1: 25.569437546075896, r: 0.2740215601925508
06/02/2019 12:28:11 *** epoch: 111 ***
06/02/2019 12:28:11 *** training ***
06/02/2019 12:28:11 step: 3635, epoch: 110, batch: 4, loss: 0.086886465549469, acc: 98.4375, f1: 99.15902964959568, r: 0.6901872973914025
06/02/2019 12:28:12 step: 3640, epoch: 110, batch: 9, loss: 0.0751895159482956, acc: 100.0, f1: 100.0, r: 0.7536158716052739
06/02/2019 12:28:12 step: 3645, epoch: 110, batch: 14, loss: 0.07370668649673462, acc: 100.0, f1: 100.0, r: 0.6752565895468985
06/02/2019 12:28:13 step: 3650, epoch: 110, batch: 19, loss: 0.2555099129676819, acc: 98.4375, f1: 96.82539682539682, r: 0.778938000669072
06/02/2019 12:28:14 step: 3655, epoch: 110, batch: 24, loss: 0.07853613793849945, acc: 100.0, f1: 100.0, r: 0.665928633360967
06/02/2019 12:28:14 step: 3660, epoch: 110, batch: 29, loss: 0.12654545903205872, acc: 98.4375, f1: 98.20868786386028, r: 0.7058137585910541
06/02/2019 12:28:15 *** evaluating ***
06/02/2019 12:28:15 step: 111, epoch: 110, acc: 50.85470085470085, f1: 28.708742326319413, r: 0.2651247355067752
06/02/2019 12:28:15 *** epoch: 112 ***
06/02/2019 12:28:15 *** training ***
06/02/2019 12:28:16 step: 3668, epoch: 111, batch: 4, loss: 0.07533755153417587, acc: 100.0, f1: 100.0, r: 0.6804699063284575
06/02/2019 12:28:16 step: 3673, epoch: 111, batch: 9, loss: 0.05880942940711975, acc: 100.0, f1: 100.0, r: 0.6576807302256604
06/02/2019 12:28:17 step: 3678, epoch: 111, batch: 14, loss: 0.09759711474180222, acc: 100.0, f1: 100.0, r: 0.7534508792112922
06/02/2019 12:28:17 step: 3683, epoch: 111, batch: 19, loss: 0.1444859504699707, acc: 100.0, f1: 100.0, r: 0.7915541887181315
06/02/2019 12:28:18 step: 3688, epoch: 111, batch: 24, loss: 0.21511468291282654, acc: 100.0, f1: 100.0, r: 0.7031705279015416
06/02/2019 12:28:18 step: 3693, epoch: 111, batch: 29, loss: 0.3234666585922241, acc: 100.0, f1: 100.0, r: 0.6576972002488792
06/02/2019 12:28:19 *** evaluating ***
06/02/2019 12:28:19 step: 112, epoch: 111, acc: 54.700854700854705, f1: 26.696471812869916, r: 0.27290227473111783
06/02/2019 12:28:19 *** epoch: 113 ***
06/02/2019 12:28:19 *** training ***
06/02/2019 12:28:19 step: 3701, epoch: 112, batch: 4, loss: 0.07538648694753647, acc: 98.4375, f1: 98.01587301587301, r: 0.744522226919644
06/02/2019 12:28:20 step: 3706, epoch: 112, batch: 9, loss: 0.13982467353343964, acc: 98.4375, f1: 94.94655004859086, r: 0.6783686661114492
06/02/2019 12:28:21 step: 3711, epoch: 112, batch: 14, loss: 0.111448273062706, acc: 100.0, f1: 100.0, r: 0.6953930087699606
06/02/2019 12:28:21 step: 3716, epoch: 112, batch: 19, loss: 0.06841078400611877, acc: 100.0, f1: 100.0, r: 0.7302391548460846
06/02/2019 12:28:22 step: 3721, epoch: 112, batch: 24, loss: 0.09435577690601349, acc: 100.0, f1: 100.0, r: 0.7127833027060609
06/02/2019 12:28:22 step: 3726, epoch: 112, batch: 29, loss: 0.06930366158485413, acc: 100.0, f1: 100.0, r: 0.7576917173893338
06/02/2019 12:28:23 *** evaluating ***
06/02/2019 12:28:23 step: 113, epoch: 112, acc: 52.13675213675214, f1: 24.60560273060273, r: 0.2668434404302866
06/02/2019 12:28:23 *** epoch: 114 ***
06/02/2019 12:28:23 *** training ***
06/02/2019 12:28:24 step: 3734, epoch: 113, batch: 4, loss: 0.0847400650382042, acc: 100.0, f1: 100.0, r: 0.751023822630876
06/02/2019 12:28:24 step: 3739, epoch: 113, batch: 9, loss: 0.08978226780891418, acc: 100.0, f1: 100.0, r: 0.6944377959427103
06/02/2019 12:28:25 step: 3744, epoch: 113, batch: 14, loss: 0.09630598872900009, acc: 100.0, f1: 100.0, r: 0.7185932139652409
06/02/2019 12:28:25 step: 3749, epoch: 113, batch: 19, loss: 0.10090580582618713, acc: 100.0, f1: 100.0, r: 0.7731926080288309
06/02/2019 12:28:26 step: 3754, epoch: 113, batch: 24, loss: 0.08264946937561035, acc: 100.0, f1: 100.0, r: 0.688270715543297
06/02/2019 12:28:27 step: 3759, epoch: 113, batch: 29, loss: 0.10142415761947632, acc: 100.0, f1: 100.0, r: 0.7238435600460853
06/02/2019 12:28:27 *** evaluating ***
06/02/2019 12:28:27 step: 114, epoch: 113, acc: 53.84615384615385, f1: 27.356696393074103, r: 0.27321076029851704
06/02/2019 12:28:27 *** epoch: 115 ***
06/02/2019 12:28:27 *** training ***
06/02/2019 12:28:28 step: 3767, epoch: 114, batch: 4, loss: 0.08922100812196732, acc: 98.4375, f1: 99.29193899782135, r: 0.7991363180275571
06/02/2019 12:28:28 step: 3772, epoch: 114, batch: 9, loss: 0.05913862586021423, acc: 100.0, f1: 100.0, r: 0.7677314683618821
06/02/2019 12:28:29 step: 3777, epoch: 114, batch: 14, loss: 0.09059220552444458, acc: 100.0, f1: 100.0, r: 0.7691632287030019
06/02/2019 12:28:30 step: 3782, epoch: 114, batch: 19, loss: 0.29910972714424133, acc: 98.4375, f1: 97.27891156462584, r: 0.6433151035277308
06/02/2019 12:28:30 step: 3787, epoch: 114, batch: 24, loss: 0.07369278371334076, acc: 100.0, f1: 100.0, r: 0.8225454625428111
06/02/2019 12:28:31 step: 3792, epoch: 114, batch: 29, loss: 0.09466877579689026, acc: 100.0, f1: 100.0, r: 0.8260175695294242
06/02/2019 12:28:31 *** evaluating ***
06/02/2019 12:28:31 step: 115, epoch: 114, acc: 51.28205128205128, f1: 23.822632605397356, r: 0.272968931282345
06/02/2019 12:28:31 *** epoch: 116 ***
06/02/2019 12:28:31 *** training ***
06/02/2019 12:28:32 step: 3800, epoch: 115, batch: 4, loss: 0.08664838969707489, acc: 100.0, f1: 100.0, r: 0.7919203226159237
06/02/2019 12:28:32 step: 3805, epoch: 115, batch: 9, loss: 0.08096229285001755, acc: 100.0, f1: 100.0, r: 0.6667868644133509
06/02/2019 12:28:33 step: 3810, epoch: 115, batch: 14, loss: 0.07174384593963623, acc: 100.0, f1: 100.0, r: 0.6847937280015917
06/02/2019 12:28:34 step: 3815, epoch: 115, batch: 19, loss: 0.06333906948566437, acc: 100.0, f1: 100.0, r: 0.7491610945302299
06/02/2019 12:28:34 step: 3820, epoch: 115, batch: 24, loss: 0.10082954168319702, acc: 100.0, f1: 100.0, r: 0.7162964385132753
06/02/2019 12:28:35 step: 3825, epoch: 115, batch: 29, loss: 0.07311822474002838, acc: 100.0, f1: 100.0, r: 0.7548402358907929
06/02/2019 12:28:35 *** evaluating ***
06/02/2019 12:28:35 step: 116, epoch: 115, acc: 53.41880341880342, f1: 24.91847522249339, r: 0.28182692461510167
06/02/2019 12:28:35 *** epoch: 117 ***
06/02/2019 12:28:35 *** training ***
06/02/2019 12:28:36 step: 3833, epoch: 116, batch: 4, loss: 0.07726090401411057, acc: 100.0, f1: 100.0, r: 0.7780991698127522
06/02/2019 12:28:37 step: 3838, epoch: 116, batch: 9, loss: 0.04849925637245178, acc: 100.0, f1: 100.0, r: 0.6944165987818179
06/02/2019 12:28:37 step: 3843, epoch: 116, batch: 14, loss: 0.08281571418046951, acc: 100.0, f1: 100.0, r: 0.7083633480077602
06/02/2019 12:28:38 step: 3848, epoch: 116, batch: 19, loss: 0.0973007082939148, acc: 100.0, f1: 100.0, r: 0.6968558069425325
06/02/2019 12:28:38 step: 3853, epoch: 116, batch: 24, loss: 0.10744407773017883, acc: 100.0, f1: 100.0, r: 0.6895793174232858
06/02/2019 12:28:39 step: 3858, epoch: 116, batch: 29, loss: 0.18726392090320587, acc: 98.4375, f1: 99.02834008097166, r: 0.7242085255569342
06/02/2019 12:28:39 *** evaluating ***
06/02/2019 12:28:39 step: 117, epoch: 116, acc: 52.56410256410257, f1: 23.214477882054407, r: 0.2691645143666686
06/02/2019 12:28:39 *** epoch: 118 ***
06/02/2019 12:28:39 *** training ***
06/02/2019 12:28:40 step: 3866, epoch: 117, batch: 4, loss: 0.32607340812683105, acc: 98.4375, f1: 99.2158439730572, r: 0.7477602548255046
06/02/2019 12:28:41 step: 3871, epoch: 117, batch: 9, loss: 0.06049027293920517, acc: 100.0, f1: 100.0, r: 0.7206171002348432
06/02/2019 12:28:41 step: 3876, epoch: 117, batch: 14, loss: 0.05278324335813522, acc: 100.0, f1: 100.0, r: 0.7796008613776427
06/02/2019 12:28:42 step: 3881, epoch: 117, batch: 19, loss: 0.06910324841737747, acc: 100.0, f1: 100.0, r: 0.790893259338977
06/02/2019 12:28:42 step: 3886, epoch: 117, batch: 24, loss: 0.13153088092803955, acc: 98.4375, f1: 98.97400820793433, r: 0.7577574002899519
06/02/2019 12:28:43 step: 3891, epoch: 117, batch: 29, loss: 0.15622101724147797, acc: 100.0, f1: 100.0, r: 0.789483846433695
06/02/2019 12:28:43 *** evaluating ***
06/02/2019 12:28:43 step: 118, epoch: 117, acc: 50.0, f1: 23.26600801575972, r: 0.26081967626787905
06/02/2019 12:28:43 *** epoch: 119 ***
06/02/2019 12:28:43 *** training ***
06/02/2019 12:28:44 step: 3899, epoch: 118, batch: 4, loss: 0.09437529742717743, acc: 100.0, f1: 100.0, r: 0.8003674729720478
06/02/2019 12:28:45 step: 3904, epoch: 118, batch: 9, loss: 0.13815411925315857, acc: 98.4375, f1: 97.65523230568823, r: 0.6497906900231912
06/02/2019 12:28:45 step: 3909, epoch: 118, batch: 14, loss: 0.11330798268318176, acc: 98.4375, f1: 97.73242630385488, r: 0.6659435545813646
06/02/2019 12:28:46 step: 3914, epoch: 118, batch: 19, loss: 0.10742965340614319, acc: 100.0, f1: 100.0, r: 0.7757511346271247
06/02/2019 12:28:46 step: 3919, epoch: 118, batch: 24, loss: 0.13218101859092712, acc: 100.0, f1: 100.0, r: 0.6862591754278797
06/02/2019 12:28:47 step: 3924, epoch: 118, batch: 29, loss: 0.10860368609428406, acc: 100.0, f1: 100.0, r: 0.6822826456510748
06/02/2019 12:28:47 *** evaluating ***
06/02/2019 12:28:47 step: 119, epoch: 118, acc: 50.85470085470085, f1: 23.041541164658632, r: 0.26022122062973757
06/02/2019 12:28:47 *** epoch: 120 ***
06/02/2019 12:28:47 *** training ***
06/02/2019 12:28:48 step: 3932, epoch: 119, batch: 4, loss: 0.10970648378133774, acc: 100.0, f1: 100.0, r: 0.6451412385640611
06/02/2019 12:28:48 step: 3937, epoch: 119, batch: 9, loss: 0.09643074870109558, acc: 100.0, f1: 100.0, r: 0.7356637253681793
06/02/2019 12:28:49 step: 3942, epoch: 119, batch: 14, loss: 0.21368055045604706, acc: 100.0, f1: 100.0, r: 0.7035668344620255
06/02/2019 12:28:50 step: 3947, epoch: 119, batch: 19, loss: 0.11654533445835114, acc: 98.4375, f1: 99.09988385598142, r: 0.8040451889289146
06/02/2019 12:28:50 step: 3952, epoch: 119, batch: 24, loss: 0.08140864968299866, acc: 100.0, f1: 100.0, r: 0.8255097667533778
06/02/2019 12:28:51 step: 3957, epoch: 119, batch: 29, loss: 0.09499169141054153, acc: 100.0, f1: 100.0, r: 0.6511492531394883
06/02/2019 12:28:51 *** evaluating ***
06/02/2019 12:28:52 step: 120, epoch: 119, acc: 51.70940170940172, f1: 23.14716433160633, r: 0.2632397241142252
06/02/2019 12:28:52 *** epoch: 121 ***
06/02/2019 12:28:52 *** training ***
06/02/2019 12:28:52 step: 3965, epoch: 120, batch: 4, loss: 0.0665719285607338, acc: 100.0, f1: 100.0, r: 0.7503279770375391
06/02/2019 12:28:53 step: 3970, epoch: 120, batch: 9, loss: 0.10945221781730652, acc: 100.0, f1: 100.0, r: 0.7224817380957375
06/02/2019 12:28:54 step: 3975, epoch: 120, batch: 14, loss: 0.08492859452962875, acc: 100.0, f1: 100.0, r: 0.6496452710711252
06/02/2019 12:28:54 step: 3980, epoch: 120, batch: 19, loss: 0.08170159161090851, acc: 100.0, f1: 100.0, r: 0.6971598004000062
06/02/2019 12:28:55 step: 3985, epoch: 120, batch: 24, loss: 0.3575615882873535, acc: 98.4375, f1: 99.2348736534783, r: 0.6562770214807727
06/02/2019 12:28:56 step: 3990, epoch: 120, batch: 29, loss: 0.10251464694738388, acc: 100.0, f1: 100.0, r: 0.8229004666300133
06/02/2019 12:28:56 *** evaluating ***
06/02/2019 12:28:56 step: 121, epoch: 120, acc: 52.56410256410257, f1: 24.660641707141796, r: 0.2638132250741386
06/02/2019 12:28:56 *** epoch: 122 ***
06/02/2019 12:28:56 *** training ***
06/02/2019 12:28:57 step: 3998, epoch: 121, batch: 4, loss: 0.06956923007965088, acc: 100.0, f1: 100.0, r: 0.6791494996885359
06/02/2019 12:28:57 step: 4003, epoch: 121, batch: 9, loss: 0.10618139058351517, acc: 98.4375, f1: 98.52579852579852, r: 0.7387044411473241
06/02/2019 12:28:58 step: 4008, epoch: 121, batch: 14, loss: 0.08523571491241455, acc: 98.4375, f1: 99.14468995010691, r: 0.7307603121387894
06/02/2019 12:28:58 step: 4013, epoch: 121, batch: 19, loss: 0.11443488299846649, acc: 100.0, f1: 100.0, r: 0.7912811758435763
06/02/2019 12:28:59 step: 4018, epoch: 121, batch: 24, loss: 0.0749935433268547, acc: 100.0, f1: 100.0, r: 0.7222928270239091
06/02/2019 12:29:00 step: 4023, epoch: 121, batch: 29, loss: 0.0911601334810257, acc: 100.0, f1: 100.0, r: 0.7276379631395941
06/02/2019 12:29:00 *** evaluating ***
06/02/2019 12:29:00 step: 122, epoch: 121, acc: 54.27350427350427, f1: 25.68984476287552, r: 0.2741909983329431
06/02/2019 12:29:00 *** epoch: 123 ***
06/02/2019 12:29:00 *** training ***
06/02/2019 12:29:01 step: 4031, epoch: 122, batch: 4, loss: 0.11639104783535004, acc: 100.0, f1: 100.0, r: 0.7972381092394811
06/02/2019 12:29:01 step: 4036, epoch: 122, batch: 9, loss: 0.06946345418691635, acc: 100.0, f1: 100.0, r: 0.7267657749647434
06/02/2019 12:29:02 step: 4041, epoch: 122, batch: 14, loss: 0.10213232040405273, acc: 98.4375, f1: 98.36907278767744, r: 0.6881933463864857
06/02/2019 12:29:02 step: 4046, epoch: 122, batch: 19, loss: 0.08286575973033905, acc: 100.0, f1: 100.0, r: 0.7753056951200539
06/02/2019 12:29:03 step: 4051, epoch: 122, batch: 24, loss: 0.09965959936380386, acc: 100.0, f1: 100.0, r: 0.7092890364692964
06/02/2019 12:29:04 step: 4056, epoch: 122, batch: 29, loss: 0.08192866295576096, acc: 100.0, f1: 100.0, r: 0.6822036402667994
06/02/2019 12:29:04 *** evaluating ***
06/02/2019 12:29:04 step: 123, epoch: 122, acc: 53.41880341880342, f1: 24.790067695757156, r: 0.2780649950173321
06/02/2019 12:29:04 *** epoch: 124 ***
06/02/2019 12:29:04 *** training ***
06/02/2019 12:29:05 step: 4064, epoch: 123, batch: 4, loss: 0.08296184241771698, acc: 98.4375, f1: 97.93650793650795, r: 0.8224753126919423
06/02/2019 12:29:05 step: 4069, epoch: 123, batch: 9, loss: 0.0873628556728363, acc: 100.0, f1: 100.0, r: 0.7079584288250764
06/02/2019 12:29:06 step: 4074, epoch: 123, batch: 14, loss: 0.09768042713403702, acc: 98.4375, f1: 99.1186839012926, r: 0.8229176890821578
06/02/2019 12:29:06 step: 4079, epoch: 123, batch: 19, loss: 0.18848934769630432, acc: 100.0, f1: 100.0, r: 0.7628190905317549
06/02/2019 12:29:07 step: 4084, epoch: 123, batch: 24, loss: 0.15169286727905273, acc: 100.0, f1: 100.0, r: 0.5988753731976708
06/02/2019 12:29:08 step: 4089, epoch: 123, batch: 29, loss: 0.07543420791625977, acc: 100.0, f1: 100.0, r: 0.6686368880776513
06/02/2019 12:29:08 *** evaluating ***
06/02/2019 12:29:08 step: 124, epoch: 123, acc: 54.700854700854705, f1: 26.977046560465855, r: 0.27474537569484336
06/02/2019 12:29:08 *** epoch: 125 ***
06/02/2019 12:29:08 *** training ***
06/02/2019 12:29:09 step: 4097, epoch: 124, batch: 4, loss: 0.0909893810749054, acc: 98.4375, f1: 99.21142369991475, r: 0.7657402568882826
06/02/2019 12:29:09 step: 4102, epoch: 124, batch: 9, loss: 0.07560619711875916, acc: 100.0, f1: 100.0, r: 0.7128875355156832
06/02/2019 12:29:10 step: 4107, epoch: 124, batch: 14, loss: 0.20877361297607422, acc: 100.0, f1: 100.0, r: 0.7233141472675593
06/02/2019 12:29:10 step: 4112, epoch: 124, batch: 19, loss: 0.07959446310997009, acc: 100.0, f1: 100.0, r: 0.7522332683632911
06/02/2019 12:29:11 step: 4117, epoch: 124, batch: 24, loss: 0.10956033319234848, acc: 98.4375, f1: 98.36907278767744, r: 0.6588158872597151
06/02/2019 12:29:12 step: 4122, epoch: 124, batch: 29, loss: 0.09859274327754974, acc: 98.4375, f1: 97.27272727272727, r: 0.7699043923232499
06/02/2019 12:29:12 *** evaluating ***
06/02/2019 12:29:12 step: 125, epoch: 124, acc: 52.991452991452995, f1: 25.720047783008926, r: 0.27043948906080073
06/02/2019 12:29:12 *** epoch: 126 ***
06/02/2019 12:29:12 *** training ***
06/02/2019 12:29:13 step: 4130, epoch: 125, batch: 4, loss: 0.0884910523891449, acc: 98.4375, f1: 98.14315663372267, r: 0.7279121033336989
06/02/2019 12:29:14 step: 4135, epoch: 125, batch: 9, loss: 0.06297944486141205, acc: 100.0, f1: 100.0, r: 0.82423922784322
06/02/2019 12:29:14 step: 4140, epoch: 125, batch: 14, loss: 0.08025019615888596, acc: 100.0, f1: 100.0, r: 0.6801778066042002
06/02/2019 12:29:15 step: 4145, epoch: 125, batch: 19, loss: 0.09094037860631943, acc: 98.4375, f1: 96.82539682539682, r: 0.8015876314430661
06/02/2019 12:29:15 step: 4150, epoch: 125, batch: 24, loss: 0.19911445677280426, acc: 100.0, f1: 100.0, r: 0.7685750260617812
06/02/2019 12:29:16 step: 4155, epoch: 125, batch: 29, loss: 0.05586282163858414, acc: 100.0, f1: 100.0, r: 0.7456358122121363
06/02/2019 12:29:16 *** evaluating ***
06/02/2019 12:29:16 step: 126, epoch: 125, acc: 54.700854700854705, f1: 27.143732492997195, r: 0.2748517168019844
06/02/2019 12:29:16 *** epoch: 127 ***
06/02/2019 12:29:16 *** training ***
06/02/2019 12:29:17 step: 4163, epoch: 126, batch: 4, loss: 0.12087266892194748, acc: 98.4375, f1: 98.99874843554444, r: 0.8243032207023421
06/02/2019 12:29:18 step: 4168, epoch: 126, batch: 9, loss: 0.07410416752099991, acc: 98.4375, f1: 99.32229495571814, r: 0.6795646895474363
06/02/2019 12:29:18 step: 4173, epoch: 126, batch: 14, loss: 0.1116466075181961, acc: 100.0, f1: 100.0, r: 0.7285944548615976
06/02/2019 12:29:19 step: 4178, epoch: 126, batch: 19, loss: 0.05719715729355812, acc: 100.0, f1: 100.0, r: 0.8410394400734025
06/02/2019 12:29:20 step: 4183, epoch: 126, batch: 24, loss: 0.09176450967788696, acc: 100.0, f1: 100.0, r: 0.6872037677362383
06/02/2019 12:29:20 step: 4188, epoch: 126, batch: 29, loss: 0.05130155012011528, acc: 100.0, f1: 100.0, r: 0.686381667665668
06/02/2019 12:29:20 *** evaluating ***
06/02/2019 12:29:21 step: 127, epoch: 126, acc: 52.13675213675214, f1: 23.695089866200767, r: 0.269649018012701
06/02/2019 12:29:21 *** epoch: 128 ***
06/02/2019 12:29:21 *** training ***
06/02/2019 12:29:21 step: 4196, epoch: 127, batch: 4, loss: 0.10976392775774002, acc: 98.4375, f1: 97.57236227824463, r: 0.6808409186080937
06/02/2019 12:29:22 step: 4201, epoch: 127, batch: 9, loss: 0.07966166734695435, acc: 100.0, f1: 100.0, r: 0.7140702282977567
06/02/2019 12:29:22 step: 4206, epoch: 127, batch: 14, loss: 0.06227470934391022, acc: 100.0, f1: 100.0, r: 0.7063151355143146
06/02/2019 12:29:23 step: 4211, epoch: 127, batch: 19, loss: 0.08599916845560074, acc: 100.0, f1: 100.0, r: 0.7536104217707191
06/02/2019 12:29:23 step: 4216, epoch: 127, batch: 24, loss: 0.10009191185235977, acc: 100.0, f1: 100.0, r: 0.7007226985504291
06/02/2019 12:29:24 step: 4221, epoch: 127, batch: 29, loss: 0.054885610938072205, acc: 100.0, f1: 100.0, r: 0.6339690277767442
06/02/2019 12:29:24 *** evaluating ***
06/02/2019 12:29:25 step: 128, epoch: 127, acc: 52.56410256410257, f1: 25.099074379593084, r: 0.27730134227722464
06/02/2019 12:29:25 *** epoch: 129 ***
06/02/2019 12:29:25 *** training ***
06/02/2019 12:29:25 step: 4229, epoch: 128, batch: 4, loss: 0.06186797469854355, acc: 100.0, f1: 100.0, r: 0.8296121774285377
06/02/2019 12:29:26 step: 4234, epoch: 128, batch: 9, loss: 0.08933062851428986, acc: 100.0, f1: 100.0, r: 0.7046292742264859
06/02/2019 12:29:27 step: 4239, epoch: 128, batch: 14, loss: 0.09840013831853867, acc: 100.0, f1: 100.0, r: 0.7932615048841754
06/02/2019 12:29:27 step: 4244, epoch: 128, batch: 19, loss: 0.14002835750579834, acc: 96.875, f1: 95.54982694832319, r: 0.7642137789629069
06/02/2019 12:29:28 step: 4249, epoch: 128, batch: 24, loss: 0.06923496723175049, acc: 100.0, f1: 100.0, r: 0.71064997744052
06/02/2019 12:29:28 step: 4254, epoch: 128, batch: 29, loss: 0.07655662298202515, acc: 100.0, f1: 100.0, r: 0.7069396223410707
06/02/2019 12:29:28 *** evaluating ***
06/02/2019 12:29:29 step: 129, epoch: 128, acc: 50.0, f1: 26.367908617335374, r: 0.2618764601426984
06/02/2019 12:29:29 *** epoch: 130 ***
06/02/2019 12:29:29 *** training ***
06/02/2019 12:29:29 step: 4262, epoch: 129, batch: 4, loss: 0.10459032654762268, acc: 98.4375, f1: 99.3331164606376, r: 0.7358292170192375
06/02/2019 12:29:30 step: 4267, epoch: 129, batch: 9, loss: 0.33955031633377075, acc: 98.4375, f1: 99.2093441150045, r: 0.6445695084805447
06/02/2019 12:29:30 step: 4272, epoch: 129, batch: 14, loss: 0.12251836061477661, acc: 98.4375, f1: 99.08479908479907, r: 0.6952436823342779
06/02/2019 12:29:31 step: 4277, epoch: 129, batch: 19, loss: 0.18406368792057037, acc: 98.4375, f1: 99.24489795918367, r: 0.8298825266088297
06/02/2019 12:29:32 step: 4282, epoch: 129, batch: 24, loss: 0.10059945285320282, acc: 100.0, f1: 100.0, r: 0.807436555833855
06/02/2019 12:29:32 step: 4287, epoch: 129, batch: 29, loss: 0.07390190660953522, acc: 100.0, f1: 100.0, r: 0.7927116021468392
06/02/2019 12:29:32 *** evaluating ***
06/02/2019 12:29:33 step: 130, epoch: 129, acc: 52.56410256410257, f1: 25.039068849413677, r: 0.27748155363792576
06/02/2019 12:29:33 *** epoch: 131 ***
06/02/2019 12:29:33 *** training ***
06/02/2019 12:29:33 step: 4295, epoch: 130, batch: 4, loss: 0.09608036279678345, acc: 100.0, f1: 100.0, r: 0.7499892434630445
06/02/2019 12:29:34 step: 4300, epoch: 130, batch: 9, loss: 0.07174984365701675, acc: 100.0, f1: 100.0, r: 0.8028429660114514
06/02/2019 12:29:34 step: 4305, epoch: 130, batch: 14, loss: 0.10484708100557327, acc: 100.0, f1: 100.0, r: 0.7086120473684357
06/02/2019 12:29:35 step: 4310, epoch: 130, batch: 19, loss: 0.08411788940429688, acc: 98.4375, f1: 97.71428571428571, r: 0.7593594738307515
06/02/2019 12:29:36 step: 4315, epoch: 130, batch: 24, loss: 0.08333031088113785, acc: 100.0, f1: 100.0, r: 0.6566573799424652
06/02/2019 12:29:36 step: 4320, epoch: 130, batch: 29, loss: 0.0877077579498291, acc: 98.4375, f1: 99.05018611218071, r: 0.7093829630752574
06/02/2019 12:29:36 *** evaluating ***
06/02/2019 12:29:37 step: 131, epoch: 130, acc: 54.27350427350427, f1: 26.28257528870197, r: 0.28581470602457143
06/02/2019 12:29:37 *** epoch: 132 ***
06/02/2019 12:29:37 *** training ***
06/02/2019 12:29:37 step: 4328, epoch: 131, batch: 4, loss: 0.04503006860613823, acc: 100.0, f1: 100.0, r: 0.7258478108426778
06/02/2019 12:29:38 step: 4333, epoch: 131, batch: 9, loss: 0.09076131135225296, acc: 100.0, f1: 100.0, r: 0.6752368577812908
06/02/2019 12:29:38 step: 4338, epoch: 131, batch: 14, loss: 0.07535777986049652, acc: 100.0, f1: 100.0, r: 0.7319608270620391
06/02/2019 12:29:39 step: 4343, epoch: 131, batch: 19, loss: 0.11890868097543716, acc: 100.0, f1: 100.0, r: 0.798920974983645
06/02/2019 12:29:39 step: 4348, epoch: 131, batch: 24, loss: 0.09196139872074127, acc: 100.0, f1: 100.0, r: 0.6971664092285114
06/02/2019 12:29:40 step: 4353, epoch: 131, batch: 29, loss: 0.09840402752161026, acc: 98.4375, f1: 99.2158439730572, r: 0.6986539035859737
06/02/2019 12:29:41 *** evaluating ***
06/02/2019 12:29:41 step: 132, epoch: 131, acc: 52.56410256410257, f1: 24.286899729527438, r: 0.28334758011149574
06/02/2019 12:29:41 *** epoch: 133 ***
06/02/2019 12:29:41 *** training ***
06/02/2019 12:29:41 step: 4361, epoch: 132, batch: 4, loss: 0.09491686522960663, acc: 100.0, f1: 100.0, r: 0.7382668953180906
06/02/2019 12:29:42 step: 4366, epoch: 132, batch: 9, loss: 0.0733879804611206, acc: 98.4375, f1: 99.26525841195625, r: 0.7439797483810584
06/02/2019 12:29:43 step: 4371, epoch: 132, batch: 14, loss: 0.05859585478901863, acc: 98.4375, f1: 97.48148148148147, r: 0.6850884925849096
06/02/2019 12:29:43 step: 4376, epoch: 132, batch: 19, loss: 0.06646385788917542, acc: 100.0, f1: 100.0, r: 0.7254925117059822
06/02/2019 12:29:44 step: 4381, epoch: 132, batch: 24, loss: 0.08860629796981812, acc: 100.0, f1: 100.0, r: 0.6824747597899197
06/02/2019 12:29:44 step: 4386, epoch: 132, batch: 29, loss: 0.05823959782719612, acc: 100.0, f1: 100.0, r: 0.7526550146799191
06/02/2019 12:29:45 *** evaluating ***
06/02/2019 12:29:45 step: 133, epoch: 132, acc: 55.55555555555556, f1: 29.826062714167556, r: 0.2840058601523096
06/02/2019 12:29:45 *** epoch: 134 ***
06/02/2019 12:29:45 *** training ***
06/02/2019 12:29:46 step: 4394, epoch: 133, batch: 4, loss: 0.07942794263362885, acc: 100.0, f1: 100.0, r: 0.6997514585091371
06/02/2019 12:29:46 step: 4399, epoch: 133, batch: 9, loss: 0.06948947161436081, acc: 100.0, f1: 100.0, r: 0.7063915118065117
06/02/2019 12:29:47 step: 4404, epoch: 133, batch: 14, loss: 0.08428086340427399, acc: 100.0, f1: 100.0, r: 0.8173091526665932
06/02/2019 12:29:48 step: 4409, epoch: 133, batch: 19, loss: 0.06068951636552811, acc: 100.0, f1: 100.0, r: 0.755722445599907
06/02/2019 12:29:48 step: 4414, epoch: 133, batch: 24, loss: 0.0590401329100132, acc: 100.0, f1: 100.0, r: 0.8372185371628211
06/02/2019 12:29:49 step: 4419, epoch: 133, batch: 29, loss: 0.079903244972229, acc: 100.0, f1: 100.0, r: 0.6688452755354108
06/02/2019 12:29:49 *** evaluating ***
06/02/2019 12:29:49 step: 134, epoch: 133, acc: 50.85470085470085, f1: 22.88674858496494, r: 0.26903204300094524
06/02/2019 12:29:49 *** epoch: 135 ***
06/02/2019 12:29:49 *** training ***
06/02/2019 12:29:50 step: 4427, epoch: 134, batch: 4, loss: 0.08271971344947815, acc: 100.0, f1: 100.0, r: 0.7544800037727947
06/02/2019 12:29:50 step: 4432, epoch: 134, batch: 9, loss: 0.13291020691394806, acc: 100.0, f1: 100.0, r: 0.709704837165501
06/02/2019 12:29:51 step: 4437, epoch: 134, batch: 14, loss: 0.0773247629404068, acc: 98.4375, f1: 99.09677419354838, r: 0.7461863385421124
06/02/2019 12:29:51 step: 4442, epoch: 134, batch: 19, loss: 0.061184581369161606, acc: 100.0, f1: 100.0, r: 0.7047365784414364
06/02/2019 12:29:52 step: 4447, epoch: 134, batch: 24, loss: 0.11008325219154358, acc: 98.4375, f1: 94.94655004859086, r: 0.6792339419178086
06/02/2019 12:29:53 step: 4452, epoch: 134, batch: 29, loss: 0.07184268534183502, acc: 98.4375, f1: 98.95657511124752, r: 0.6993070561552598
06/02/2019 12:29:53 *** evaluating ***
06/02/2019 12:29:53 step: 135, epoch: 134, acc: 50.85470085470085, f1: 24.047015765765764, r: 0.25930640774817215
06/02/2019 12:29:53 *** epoch: 136 ***
06/02/2019 12:29:53 *** training ***
06/02/2019 12:29:54 step: 4460, epoch: 135, batch: 4, loss: 0.04721910506486893, acc: 100.0, f1: 100.0, r: 0.6094683000352293
06/02/2019 12:29:54 step: 4465, epoch: 135, batch: 9, loss: 0.08271560817956924, acc: 98.4375, f1: 97.92358803986711, r: 0.7953164506387481
06/02/2019 12:29:55 step: 4470, epoch: 135, batch: 14, loss: 0.0445147268474102, acc: 100.0, f1: 100.0, r: 0.7342800472039372
06/02/2019 12:29:56 step: 4475, epoch: 135, batch: 19, loss: 0.053912870585918427, acc: 100.0, f1: 100.0, r: 0.8005998009025687
06/02/2019 12:29:56 step: 4480, epoch: 135, batch: 24, loss: 0.28913167119026184, acc: 100.0, f1: 100.0, r: 0.7420940613659647
06/02/2019 12:29:57 step: 4485, epoch: 135, batch: 29, loss: 0.05467483401298523, acc: 100.0, f1: 100.0, r: 0.7105541714515515
06/02/2019 12:29:57 *** evaluating ***
06/02/2019 12:29:57 step: 136, epoch: 135, acc: 51.70940170940172, f1: 21.68445551473074, r: 0.2630708922247433
06/02/2019 12:29:57 *** epoch: 137 ***
06/02/2019 12:29:57 *** training ***
06/02/2019 12:29:58 step: 4493, epoch: 136, batch: 4, loss: 0.2932284474372864, acc: 100.0, f1: 100.0, r: 0.7292315143215853
06/02/2019 12:29:58 step: 4498, epoch: 136, batch: 9, loss: 0.04308970272541046, acc: 100.0, f1: 100.0, r: 0.6787371880327382
06/02/2019 12:29:59 step: 4503, epoch: 136, batch: 14, loss: 0.07536599040031433, acc: 100.0, f1: 100.0, r: 0.7802113031702654
06/02/2019 12:30:00 step: 4508, epoch: 136, batch: 19, loss: 0.07648900151252747, acc: 100.0, f1: 100.0, r: 0.7301998088279562
06/02/2019 12:30:00 step: 4513, epoch: 136, batch: 24, loss: 0.07583842426538467, acc: 100.0, f1: 100.0, r: 0.7171285970199796
06/02/2019 12:30:01 step: 4518, epoch: 136, batch: 29, loss: 0.08600999414920807, acc: 100.0, f1: 100.0, r: 0.6589380701604449
06/02/2019 12:30:01 *** evaluating ***
06/02/2019 12:30:01 step: 137, epoch: 136, acc: 52.13675213675214, f1: 26.292867614335446, r: 0.2668865930522494
06/02/2019 12:30:01 *** epoch: 138 ***
06/02/2019 12:30:01 *** training ***
06/02/2019 12:30:02 step: 4526, epoch: 137, batch: 4, loss: 0.10011252760887146, acc: 100.0, f1: 100.0, r: 0.6657211265049231
06/02/2019 12:30:02 step: 4531, epoch: 137, batch: 9, loss: 0.10365693271160126, acc: 98.4375, f1: 99.28698752228165, r: 0.6692665895370142
06/02/2019 12:30:03 step: 4536, epoch: 137, batch: 14, loss: 0.08969391882419586, acc: 100.0, f1: 100.0, r: 0.8138963008424114
06/02/2019 12:30:03 step: 4541, epoch: 137, batch: 19, loss: 0.053151343017816544, acc: 100.0, f1: 100.0, r: 0.7975695068876391
06/02/2019 12:30:04 step: 4546, epoch: 137, batch: 24, loss: 0.08046694099903107, acc: 100.0, f1: 100.0, r: 0.7604515502458467
06/02/2019 12:30:05 step: 4551, epoch: 137, batch: 29, loss: 0.06356222182512283, acc: 100.0, f1: 100.0, r: 0.7367551025559387
06/02/2019 12:30:05 *** evaluating ***
06/02/2019 12:30:05 step: 138, epoch: 137, acc: 52.13675213675214, f1: 22.942137865431167, r: 0.2566800226483798
06/02/2019 12:30:05 *** epoch: 139 ***
06/02/2019 12:30:05 *** training ***
06/02/2019 12:30:06 step: 4559, epoch: 138, batch: 4, loss: 0.061877816915512085, acc: 98.4375, f1: 97.94871794871796, r: 0.7117853915200629
06/02/2019 12:30:07 step: 4564, epoch: 138, batch: 9, loss: 0.0620933473110199, acc: 100.0, f1: 100.0, r: 0.7036037594817507
06/02/2019 12:30:07 step: 4569, epoch: 138, batch: 14, loss: 0.06893537938594818, acc: 100.0, f1: 100.0, r: 0.7169682995805899
06/02/2019 12:30:08 step: 4574, epoch: 138, batch: 19, loss: 0.06320036947727203, acc: 98.4375, f1: 98.9254718280755, r: 0.7425810149784124
06/02/2019 12:30:08 step: 4579, epoch: 138, batch: 24, loss: 0.2442951500415802, acc: 100.0, f1: 100.0, r: 0.8577957907557351
06/02/2019 12:30:09 step: 4584, epoch: 138, batch: 29, loss: 0.06391115486621857, acc: 100.0, f1: 100.0, r: 0.7480137997931146
06/02/2019 12:30:09 *** evaluating ***
06/02/2019 12:30:10 step: 139, epoch: 138, acc: 52.991452991452995, f1: 26.214503340862034, r: 0.2687668503215261
06/02/2019 12:30:10 *** epoch: 140 ***
06/02/2019 12:30:10 *** training ***
06/02/2019 12:30:10 step: 4592, epoch: 139, batch: 4, loss: 0.07194595038890839, acc: 100.0, f1: 100.0, r: 0.7744801211685414
06/02/2019 12:30:11 step: 4597, epoch: 139, batch: 9, loss: 0.0830768495798111, acc: 98.4375, f1: 98.36601307189542, r: 0.819978281021936
06/02/2019 12:30:11 step: 4602, epoch: 139, batch: 14, loss: 0.05128232389688492, acc: 100.0, f1: 100.0, r: 0.7855615942541501
06/02/2019 12:30:12 step: 4607, epoch: 139, batch: 19, loss: 0.07385460287332535, acc: 100.0, f1: 100.0, r: 0.8303815148363926
06/02/2019 12:30:13 step: 4612, epoch: 139, batch: 24, loss: 0.05629098415374756, acc: 100.0, f1: 100.0, r: 0.6387208856235194
06/02/2019 12:30:13 step: 4617, epoch: 139, batch: 29, loss: 0.07050485908985138, acc: 98.4375, f1: 97.81105990783409, r: 0.8305196900447881
06/02/2019 12:30:14 *** evaluating ***
06/02/2019 12:30:14 step: 140, epoch: 139, acc: 51.70940170940172, f1: 26.06912479553989, r: 0.26135564975439995
06/02/2019 12:30:14 *** epoch: 141 ***
06/02/2019 12:30:14 *** training ***
06/02/2019 12:30:15 step: 4625, epoch: 140, batch: 4, loss: 0.07173344492912292, acc: 100.0, f1: 100.0, r: 0.7871381295781839
06/02/2019 12:30:15 step: 4630, epoch: 140, batch: 9, loss: 0.051337920129299164, acc: 100.0, f1: 100.0, r: 0.769980680379925
06/02/2019 12:30:16 step: 4635, epoch: 140, batch: 14, loss: 0.0760919451713562, acc: 100.0, f1: 100.0, r: 0.7767529478467264
06/02/2019 12:30:16 step: 4640, epoch: 140, batch: 19, loss: 0.05160228908061981, acc: 100.0, f1: 100.0, r: 0.7731743455008667
06/02/2019 12:30:17 step: 4645, epoch: 140, batch: 24, loss: 0.0802069753408432, acc: 100.0, f1: 100.0, r: 0.7623271607488087
06/02/2019 12:30:18 step: 4650, epoch: 140, batch: 29, loss: 0.07940147817134857, acc: 98.4375, f1: 98.75607385811468, r: 0.6728399484998075
06/02/2019 12:30:18 *** evaluating ***
06/02/2019 12:30:18 step: 141, epoch: 140, acc: 53.41880341880342, f1: 24.788216791811923, r: 0.2594109482118156
06/02/2019 12:30:18 *** epoch: 142 ***
06/02/2019 12:30:18 *** training ***
06/02/2019 12:30:19 step: 4658, epoch: 141, batch: 4, loss: 0.08364211022853851, acc: 100.0, f1: 100.0, r: 0.7337665152039172
06/02/2019 12:30:20 step: 4663, epoch: 141, batch: 9, loss: 0.06691697239875793, acc: 100.0, f1: 100.0, r: 0.7495270339803615
06/02/2019 12:30:20 step: 4668, epoch: 141, batch: 14, loss: 0.05813770741224289, acc: 100.0, f1: 100.0, r: 0.7417393477603509
06/02/2019 12:30:21 step: 4673, epoch: 141, batch: 19, loss: 0.07298204302787781, acc: 100.0, f1: 100.0, r: 0.7544006214099532
06/02/2019 12:30:21 step: 4678, epoch: 141, batch: 24, loss: 0.30459287762641907, acc: 100.0, f1: 100.0, r: 0.7018379992722004
06/02/2019 12:30:22 step: 4683, epoch: 141, batch: 29, loss: 0.08962367475032806, acc: 98.4375, f1: 99.15902964959568, r: 0.625471996930388
06/02/2019 12:30:22 *** evaluating ***
06/02/2019 12:30:23 step: 142, epoch: 141, acc: 52.56410256410257, f1: 25.03586390087972, r: 0.26107949057821783
06/02/2019 12:30:23 *** epoch: 143 ***
06/02/2019 12:30:23 *** training ***
06/02/2019 12:30:23 step: 4691, epoch: 142, batch: 4, loss: 0.17292296886444092, acc: 100.0, f1: 100.0, r: 0.6595802656712688
06/02/2019 12:30:24 step: 4696, epoch: 142, batch: 9, loss: 0.061736591160297394, acc: 100.0, f1: 100.0, r: 0.4681743093103224
06/02/2019 12:30:25 step: 4701, epoch: 142, batch: 14, loss: 0.0662422627210617, acc: 100.0, f1: 100.0, r: 0.7820790918465796
06/02/2019 12:30:25 step: 4706, epoch: 142, batch: 19, loss: 0.24550020694732666, acc: 100.0, f1: 100.0, r: 0.7203264379435237
06/02/2019 12:30:26 step: 4711, epoch: 142, batch: 24, loss: 0.2954576015472412, acc: 100.0, f1: 100.0, r: 0.6645809493826578
06/02/2019 12:30:26 step: 4716, epoch: 142, batch: 29, loss: 0.07255680859088898, acc: 98.4375, f1: 98.26839826839827, r: 0.8336387609230101
06/02/2019 12:30:27 *** evaluating ***
06/02/2019 12:30:27 step: 143, epoch: 142, acc: 50.85470085470085, f1: 26.55036488672159, r: 0.2706219056493397
06/02/2019 12:30:27 *** epoch: 144 ***
06/02/2019 12:30:27 *** training ***
06/02/2019 12:30:28 step: 4724, epoch: 143, batch: 4, loss: 0.13667064905166626, acc: 100.0, f1: 100.0, r: 0.8409515785903862
06/02/2019 12:30:28 step: 4729, epoch: 143, batch: 9, loss: 0.0811055451631546, acc: 98.4375, f1: 98.36363636363636, r: 0.80832579130669
06/02/2019 12:30:29 step: 4734, epoch: 143, batch: 14, loss: 0.1230013370513916, acc: 100.0, f1: 100.0, r: 0.6771884094017949
06/02/2019 12:30:29 step: 4739, epoch: 143, batch: 19, loss: 0.05631674826145172, acc: 100.0, f1: 100.0, r: 0.7665160232350786
06/02/2019 12:30:30 step: 4744, epoch: 143, batch: 24, loss: 0.0692148208618164, acc: 98.4375, f1: 96.37188208616782, r: 0.7378399120662319
06/02/2019 12:30:31 step: 4749, epoch: 143, batch: 29, loss: 0.10845459997653961, acc: 100.0, f1: 100.0, r: 0.755712048918379
06/02/2019 12:30:31 *** evaluating ***
06/02/2019 12:30:31 step: 144, epoch: 143, acc: 52.13675213675214, f1: 25.27665043290043, r: 0.26007762256700245
06/02/2019 12:30:31 *** epoch: 145 ***
06/02/2019 12:30:31 *** training ***
06/02/2019 12:30:32 step: 4757, epoch: 144, batch: 4, loss: 0.07409678399562836, acc: 100.0, f1: 100.0, r: 0.8485527960426548
06/02/2019 12:30:32 step: 4762, epoch: 144, batch: 9, loss: 0.07989908754825592, acc: 98.4375, f1: 97.87581699346406, r: 0.7918880536554567
06/02/2019 12:30:33 step: 4767, epoch: 144, batch: 14, loss: 0.08211959153413773, acc: 100.0, f1: 100.0, r: 0.7244783944860277
06/02/2019 12:30:33 step: 4772, epoch: 144, batch: 19, loss: 0.0799863338470459, acc: 100.0, f1: 100.0, r: 0.8196005626336188
06/02/2019 12:30:34 step: 4777, epoch: 144, batch: 24, loss: 0.06856133043766022, acc: 100.0, f1: 100.0, r: 0.8353410782140506
06/02/2019 12:30:35 step: 4782, epoch: 144, batch: 29, loss: 0.1274326592683792, acc: 98.4375, f1: 97.88359788359789, r: 0.6786586568778818
06/02/2019 12:30:35 *** evaluating ***
06/02/2019 12:30:35 step: 145, epoch: 144, acc: 52.13675213675214, f1: 24.998468512619457, r: 0.261651570603011
06/02/2019 12:30:35 *** epoch: 146 ***
06/02/2019 12:30:35 *** training ***
06/02/2019 12:30:36 step: 4790, epoch: 145, batch: 4, loss: 0.09586000442504883, acc: 100.0, f1: 100.0, r: 0.7383894714461191
06/02/2019 12:30:36 step: 4795, epoch: 145, batch: 9, loss: 0.28513020277023315, acc: 100.0, f1: 100.0, r: 0.8074272100241439
06/02/2019 12:30:37 step: 4800, epoch: 145, batch: 14, loss: 0.054035723209381104, acc: 100.0, f1: 100.0, r: 0.7482346043522703
06/02/2019 12:30:38 step: 4805, epoch: 145, batch: 19, loss: 0.05075617879629135, acc: 100.0, f1: 100.0, r: 0.6960123616734057
06/02/2019 12:30:38 step: 4810, epoch: 145, batch: 24, loss: 0.06599020212888718, acc: 100.0, f1: 100.0, r: 0.7799459794743848
06/02/2019 12:30:39 step: 4815, epoch: 145, batch: 29, loss: 0.07259967923164368, acc: 100.0, f1: 100.0, r: 0.7123405802846882
06/02/2019 12:30:39 *** evaluating ***
06/02/2019 12:30:39 step: 146, epoch: 145, acc: 51.70940170940172, f1: 24.76359924023759, r: 0.2671315316119509
06/02/2019 12:30:39 *** epoch: 147 ***
06/02/2019 12:30:39 *** training ***
06/02/2019 12:30:40 step: 4823, epoch: 146, batch: 4, loss: 0.05750969797372818, acc: 100.0, f1: 100.0, r: 0.6087208732246042
06/02/2019 12:30:41 step: 4828, epoch: 146, batch: 9, loss: 0.07481151819229126, acc: 100.0, f1: 100.0, r: 0.8222430424419157
06/02/2019 12:30:41 step: 4833, epoch: 146, batch: 14, loss: 0.08137013018131256, acc: 100.0, f1: 100.0, r: 0.7041650846781713
06/02/2019 12:30:42 step: 4838, epoch: 146, batch: 19, loss: 0.05625063180923462, acc: 100.0, f1: 100.0, r: 0.7992878970260963
06/02/2019 12:30:42 step: 4843, epoch: 146, batch: 24, loss: 0.08583775162696838, acc: 100.0, f1: 100.0, r: 0.8000615457263995
06/02/2019 12:30:43 step: 4848, epoch: 146, batch: 29, loss: 0.07615108788013458, acc: 100.0, f1: 100.0, r: 0.8079659393736871
06/02/2019 12:30:43 *** evaluating ***
06/02/2019 12:30:43 step: 147, epoch: 146, acc: 53.41880341880342, f1: 25.175846938901326, r: 0.2636847246210874
06/02/2019 12:30:43 *** epoch: 148 ***
06/02/2019 12:30:43 *** training ***
06/02/2019 12:30:44 step: 4856, epoch: 147, batch: 4, loss: 0.06954588741064072, acc: 100.0, f1: 100.0, r: 0.6277780078453153
06/02/2019 12:30:45 step: 4861, epoch: 147, batch: 9, loss: 0.050198011100292206, acc: 100.0, f1: 100.0, r: 0.7264549902710624
06/02/2019 12:30:45 step: 4866, epoch: 147, batch: 14, loss: 0.08615408837795258, acc: 100.0, f1: 100.0, r: 0.6577358804289533
06/02/2019 12:30:46 step: 4871, epoch: 147, batch: 19, loss: 0.05079931020736694, acc: 100.0, f1: 100.0, r: 0.8193406154192406
06/02/2019 12:30:46 step: 4876, epoch: 147, batch: 24, loss: 0.07824333012104034, acc: 98.4375, f1: 98.92156862745098, r: 0.8085848581325533
06/02/2019 12:30:47 step: 4881, epoch: 147, batch: 29, loss: 0.0941951796412468, acc: 100.0, f1: 100.0, r: 0.8198964906087901
06/02/2019 12:30:47 *** evaluating ***
06/02/2019 12:30:48 step: 148, epoch: 147, acc: 51.28205128205128, f1: 24.652654482594897, r: 0.2621953484718311
06/02/2019 12:30:48 *** epoch: 149 ***
06/02/2019 12:30:48 *** training ***
06/02/2019 12:30:48 step: 4889, epoch: 148, batch: 4, loss: 0.04733575880527496, acc: 100.0, f1: 100.0, r: 0.75104189276682
06/02/2019 12:30:49 step: 4894, epoch: 148, batch: 9, loss: 0.03945700824260712, acc: 100.0, f1: 100.0, r: 0.7858298490543765
06/02/2019 12:30:49 step: 4899, epoch: 148, batch: 14, loss: 0.051641300320625305, acc: 100.0, f1: 100.0, r: 0.760418886697968
06/02/2019 12:30:50 step: 4904, epoch: 148, batch: 19, loss: 0.061025336384773254, acc: 100.0, f1: 100.0, r: 0.7176685681231323
06/02/2019 12:30:51 step: 4909, epoch: 148, batch: 24, loss: 0.045947760343551636, acc: 100.0, f1: 100.0, r: 0.7300343030411647
06/02/2019 12:30:51 step: 4914, epoch: 148, batch: 29, loss: 0.03889923170208931, acc: 100.0, f1: 100.0, r: 0.822779292256343
06/02/2019 12:30:52 *** evaluating ***
06/02/2019 12:30:52 step: 149, epoch: 148, acc: 50.85470085470085, f1: 23.97390635911562, r: 0.2650322464035044
06/02/2019 12:30:52 *** epoch: 150 ***
06/02/2019 12:30:52 *** training ***
06/02/2019 12:30:52 step: 4922, epoch: 149, batch: 4, loss: 0.06284359097480774, acc: 100.0, f1: 100.0, r: 0.688827884640248
06/02/2019 12:30:53 step: 4927, epoch: 149, batch: 9, loss: 0.0667315274477005, acc: 100.0, f1: 100.0, r: 0.781770461596297
06/02/2019 12:30:54 step: 4932, epoch: 149, batch: 14, loss: 0.06225363165140152, acc: 100.0, f1: 100.0, r: 0.809606584995296
06/02/2019 12:30:54 step: 4937, epoch: 149, batch: 19, loss: 0.25886666774749756, acc: 100.0, f1: 100.0, r: 0.7830718436187983
06/02/2019 12:30:55 step: 4942, epoch: 149, batch: 24, loss: 0.06171758472919464, acc: 100.0, f1: 100.0, r: 0.735014003429478
06/02/2019 12:30:56 step: 4947, epoch: 149, batch: 29, loss: 0.04310062527656555, acc: 100.0, f1: 100.0, r: 0.7709244074604347
06/02/2019 12:30:56 *** evaluating ***
06/02/2019 12:30:56 step: 150, epoch: 149, acc: 53.84615384615385, f1: 25.62614110565511, r: 0.2640560461539084
06/02/2019 12:30:56 *** epoch: 151 ***
06/02/2019 12:30:56 *** training ***
06/02/2019 12:30:57 step: 4955, epoch: 150, batch: 4, loss: 0.06658419221639633, acc: 100.0, f1: 100.0, r: 0.6701571862149776
06/02/2019 12:30:57 step: 4960, epoch: 150, batch: 9, loss: 0.17613452672958374, acc: 98.4375, f1: 94.74548440065682, r: 0.752676873041887
06/02/2019 12:30:58 step: 4965, epoch: 150, batch: 14, loss: 0.06028437614440918, acc: 100.0, f1: 100.0, r: 0.7973575575463592
06/02/2019 12:30:59 step: 4970, epoch: 150, batch: 19, loss: 0.07405929267406464, acc: 98.4375, f1: 98.56887298747765, r: 0.6941703758648337
06/02/2019 12:30:59 step: 4975, epoch: 150, batch: 24, loss: 0.06158515065908432, acc: 100.0, f1: 100.0, r: 0.7175007230271949
06/02/2019 12:31:00 step: 4980, epoch: 150, batch: 29, loss: 0.057804208248853683, acc: 100.0, f1: 100.0, r: 0.729664224708453
06/02/2019 12:31:00 *** evaluating ***
06/02/2019 12:31:00 step: 151, epoch: 150, acc: 52.991452991452995, f1: 27.07489186534351, r: 0.2680622643065255
06/02/2019 12:31:00 *** epoch: 152 ***
06/02/2019 12:31:00 *** training ***
06/02/2019 12:31:01 step: 4988, epoch: 151, batch: 4, loss: 0.21692009270191193, acc: 98.4375, f1: 98.97857852177614, r: 0.7041169828912736
06/02/2019 12:31:01 step: 4993, epoch: 151, batch: 9, loss: 0.0675688236951828, acc: 100.0, f1: 100.0, r: 0.7248842363027641
06/02/2019 12:31:02 step: 4998, epoch: 151, batch: 14, loss: 0.06789590418338776, acc: 100.0, f1: 100.0, r: 0.6337976555187979
06/02/2019 12:31:03 step: 5003, epoch: 151, batch: 19, loss: 0.06274878978729248, acc: 100.0, f1: 100.0, r: 0.7804639998064885
06/02/2019 12:31:03 step: 5008, epoch: 151, batch: 24, loss: 0.03235868364572525, acc: 100.0, f1: 100.0, r: 0.7872743265850346
06/02/2019 12:31:04 step: 5013, epoch: 151, batch: 29, loss: 0.06802055239677429, acc: 98.4375, f1: 99.37611408199643, r: 0.7985043042288756
06/02/2019 12:31:04 *** evaluating ***
06/02/2019 12:31:04 step: 152, epoch: 151, acc: 51.28205128205128, f1: 25.7986160768788, r: 0.27481475129668415
06/02/2019 12:31:04 *** epoch: 153 ***
06/02/2019 12:31:04 *** training ***
06/02/2019 12:31:05 step: 5021, epoch: 152, batch: 4, loss: 0.1111820787191391, acc: 98.4375, f1: 96.79442508710801, r: 0.6805295799943596
06/02/2019 12:31:05 step: 5026, epoch: 152, batch: 9, loss: 0.05907772481441498, acc: 100.0, f1: 100.0, r: 0.7222828730472638
06/02/2019 12:31:06 step: 5031, epoch: 152, batch: 14, loss: 0.10598885267972946, acc: 100.0, f1: 100.0, r: 0.7043035817622291
06/02/2019 12:31:06 step: 5036, epoch: 152, batch: 19, loss: 0.09439209848642349, acc: 100.0, f1: 100.0, r: 0.6159657878884137
06/02/2019 12:31:07 step: 5041, epoch: 152, batch: 24, loss: 0.07380059361457825, acc: 100.0, f1: 100.0, r: 0.8096629620856699
06/02/2019 12:31:08 step: 5046, epoch: 152, batch: 29, loss: 0.054660577327013016, acc: 100.0, f1: 100.0, r: 0.767748363174732
06/02/2019 12:31:08 *** evaluating ***
06/02/2019 12:31:08 step: 153, epoch: 152, acc: 51.28205128205128, f1: 25.54534399292464, r: 0.27110915237793587
06/02/2019 12:31:08 *** epoch: 154 ***
06/02/2019 12:31:08 *** training ***
06/02/2019 12:31:09 step: 5054, epoch: 153, batch: 4, loss: 0.04833037778735161, acc: 100.0, f1: 100.0, r: 0.8071879787164302
06/02/2019 12:31:09 step: 5059, epoch: 153, batch: 9, loss: 0.05672454088926315, acc: 100.0, f1: 100.0, r: 0.7399958483283953
06/02/2019 12:31:10 step: 5064, epoch: 153, batch: 14, loss: 0.08530937880277634, acc: 100.0, f1: 100.0, r: 0.7319485138401566
06/02/2019 12:31:10 step: 5069, epoch: 153, batch: 19, loss: 0.06739892065525055, acc: 100.0, f1: 100.0, r: 0.7760539716015259
06/02/2019 12:31:11 step: 5074, epoch: 153, batch: 24, loss: 0.06855815649032593, acc: 100.0, f1: 100.0, r: 0.6854757234252599
06/02/2019 12:31:12 step: 5079, epoch: 153, batch: 29, loss: 0.03395332023501396, acc: 100.0, f1: 100.0, r: 0.7588430260092756
06/02/2019 12:31:12 *** evaluating ***
06/02/2019 12:31:12 step: 154, epoch: 153, acc: 51.70940170940172, f1: 25.827352423566534, r: 0.26929239935389915
06/02/2019 12:31:12 *** epoch: 155 ***
06/02/2019 12:31:12 *** training ***
06/02/2019 12:31:13 step: 5087, epoch: 154, batch: 4, loss: 0.061678268015384674, acc: 100.0, f1: 100.0, r: 0.6788140871586259
06/02/2019 12:31:13 step: 5092, epoch: 154, batch: 9, loss: 0.05725817382335663, acc: 100.0, f1: 100.0, r: 0.8025075800536806
06/02/2019 12:31:14 step: 5097, epoch: 154, batch: 14, loss: 0.04607070982456207, acc: 100.0, f1: 100.0, r: 0.6941440718403525
06/02/2019 12:31:15 step: 5102, epoch: 154, batch: 19, loss: 0.08486650139093399, acc: 100.0, f1: 100.0, r: 0.8188801270518008
06/02/2019 12:31:15 step: 5107, epoch: 154, batch: 24, loss: 0.06009863317012787, acc: 100.0, f1: 100.0, r: 0.7764908742367599
06/02/2019 12:31:16 step: 5112, epoch: 154, batch: 29, loss: 0.04431974142789841, acc: 100.0, f1: 100.0, r: 0.7696841900318824
06/02/2019 12:31:16 *** evaluating ***
06/02/2019 12:31:16 step: 155, epoch: 154, acc: 51.70940170940172, f1: 25.14043630979115, r: 0.26144169141686185
06/02/2019 12:31:16 *** epoch: 156 ***
06/02/2019 12:31:16 *** training ***
06/02/2019 12:31:17 step: 5120, epoch: 155, batch: 4, loss: 0.03180525451898575, acc: 100.0, f1: 100.0, r: 0.7542669621838414
06/02/2019 12:31:17 step: 5125, epoch: 155, batch: 9, loss: 0.33908501267433167, acc: 100.0, f1: 100.0, r: 0.6520893337350756
06/02/2019 12:31:18 step: 5130, epoch: 155, batch: 14, loss: 0.04711785539984703, acc: 100.0, f1: 100.0, r: 0.8247712953682663
06/02/2019 12:31:19 step: 5135, epoch: 155, batch: 19, loss: 0.06285515427589417, acc: 100.0, f1: 100.0, r: 0.783634455486401
06/02/2019 12:31:19 step: 5140, epoch: 155, batch: 24, loss: 0.06226179376244545, acc: 100.0, f1: 100.0, r: 0.8277476664734602
06/02/2019 12:31:20 step: 5145, epoch: 155, batch: 29, loss: 0.051382891833782196, acc: 100.0, f1: 100.0, r: 0.7238184341846109
06/02/2019 12:31:20 *** evaluating ***
06/02/2019 12:31:20 step: 156, epoch: 155, acc: 52.13675213675214, f1: 24.980719516518096, r: 0.2583306403555564
06/02/2019 12:31:20 *** epoch: 157 ***
06/02/2019 12:31:20 *** training ***
06/02/2019 12:31:21 step: 5153, epoch: 156, batch: 4, loss: 0.059895265847444534, acc: 100.0, f1: 100.0, r: 0.7068614584544942
06/02/2019 12:31:22 step: 5158, epoch: 156, batch: 9, loss: 0.07871754467487335, acc: 100.0, f1: 100.0, r: 0.7860564509706928
06/02/2019 12:31:22 step: 5163, epoch: 156, batch: 14, loss: 0.06057015806436539, acc: 100.0, f1: 100.0, r: 0.6562987183309654
06/02/2019 12:31:23 step: 5168, epoch: 156, batch: 19, loss: 0.05285147577524185, acc: 100.0, f1: 100.0, r: 0.821522819055117
06/02/2019 12:31:23 step: 5173, epoch: 156, batch: 24, loss: 0.03541206941008568, acc: 100.0, f1: 100.0, r: 0.764097766147878
06/02/2019 12:31:24 step: 5178, epoch: 156, batch: 29, loss: 0.0725940614938736, acc: 98.4375, f1: 99.24759924759925, r: 0.661924944317395
06/02/2019 12:31:24 *** evaluating ***
06/02/2019 12:31:24 step: 157, epoch: 156, acc: 52.13675213675214, f1: 24.95054682780791, r: 0.25153180590892393
06/02/2019 12:31:24 *** epoch: 158 ***
06/02/2019 12:31:24 *** training ***
06/02/2019 12:31:25 step: 5186, epoch: 157, batch: 4, loss: 0.07868095487356186, acc: 98.4375, f1: 98.44322344322345, r: 0.7805466231505302
06/02/2019 12:31:26 step: 5191, epoch: 157, batch: 9, loss: 0.06701917946338654, acc: 100.0, f1: 100.0, r: 0.6677858998904297
06/02/2019 12:31:26 step: 5196, epoch: 157, batch: 14, loss: 0.04550030454993248, acc: 100.0, f1: 100.0, r: 0.7417670853479708
06/02/2019 12:31:27 step: 5201, epoch: 157, batch: 19, loss: 0.07675452530384064, acc: 100.0, f1: 100.0, r: 0.7741820644334851
06/02/2019 12:31:27 step: 5206, epoch: 157, batch: 24, loss: 0.04818788170814514, acc: 100.0, f1: 100.0, r: 0.8193619949892725
06/02/2019 12:31:28 step: 5211, epoch: 157, batch: 29, loss: 0.05843159928917885, acc: 100.0, f1: 100.0, r: 0.7304553931219402
06/02/2019 12:31:28 *** evaluating ***
06/02/2019 12:31:28 step: 158, epoch: 157, acc: 53.41880341880342, f1: 27.028961935685082, r: 0.2670161232304
06/02/2019 12:31:28 *** epoch: 159 ***
06/02/2019 12:31:28 *** training ***
06/02/2019 12:31:29 step: 5219, epoch: 158, batch: 4, loss: 0.22558489441871643, acc: 100.0, f1: 100.0, r: 0.7448221207227529
06/02/2019 12:31:30 step: 5224, epoch: 158, batch: 9, loss: 0.05641276761889458, acc: 100.0, f1: 100.0, r: 0.6825553725207676
06/02/2019 12:31:30 step: 5229, epoch: 158, batch: 14, loss: 0.04380248114466667, acc: 100.0, f1: 100.0, r: 0.6732364181945009
06/02/2019 12:31:31 step: 5234, epoch: 158, batch: 19, loss: 0.07359811663627625, acc: 100.0, f1: 100.0, r: 0.8079520147711671
06/02/2019 12:31:31 step: 5239, epoch: 158, batch: 24, loss: 0.11918874830007553, acc: 98.4375, f1: 99.20079920079921, r: 0.7747146567351945
06/02/2019 12:31:32 step: 5244, epoch: 158, batch: 29, loss: 0.04393872246146202, acc: 100.0, f1: 100.0, r: 0.8254621176697081
06/02/2019 12:31:32 *** evaluating ***
06/02/2019 12:31:32 step: 159, epoch: 158, acc: 50.427350427350426, f1: 22.075921160587754, r: 0.26061719716014475
06/02/2019 12:31:32 *** epoch: 160 ***
06/02/2019 12:31:32 *** training ***
06/02/2019 12:31:33 step: 5252, epoch: 159, batch: 4, loss: 0.061483025550842285, acc: 100.0, f1: 100.0, r: 0.7377717332635451
06/02/2019 12:31:34 step: 5257, epoch: 159, batch: 9, loss: 0.058245278894901276, acc: 100.0, f1: 100.0, r: 0.7197407833207727
06/02/2019 12:31:34 step: 5262, epoch: 159, batch: 14, loss: 0.05821637809276581, acc: 98.4375, f1: 99.20694459329118, r: 0.7057596370922257
06/02/2019 12:31:35 step: 5267, epoch: 159, batch: 19, loss: 0.06603812426328659, acc: 100.0, f1: 100.0, r: 0.8204743288672828
06/02/2019 12:31:35 step: 5272, epoch: 159, batch: 24, loss: 0.06108133867383003, acc: 100.0, f1: 100.0, r: 0.7498354619400822
06/02/2019 12:31:36 step: 5277, epoch: 159, batch: 29, loss: 0.11701050400733948, acc: 100.0, f1: 100.0, r: 0.8174386269135301
06/02/2019 12:31:36 *** evaluating ***
06/02/2019 12:31:37 step: 160, epoch: 159, acc: 52.13675213675214, f1: 22.907823661828974, r: 0.2629060890084207
06/02/2019 12:31:37 *** epoch: 161 ***
06/02/2019 12:31:37 *** training ***
06/02/2019 12:31:37 step: 5285, epoch: 160, batch: 4, loss: 0.03912368416786194, acc: 100.0, f1: 100.0, r: 0.6877957454433282
06/02/2019 12:31:38 step: 5290, epoch: 160, batch: 9, loss: 0.1617657095193863, acc: 100.0, f1: 100.0, r: 0.7646677362321701
06/02/2019 12:31:38 step: 5295, epoch: 160, batch: 14, loss: 0.08563889563083649, acc: 100.0, f1: 100.0, r: 0.681613923316504
06/02/2019 12:31:39 step: 5300, epoch: 160, batch: 19, loss: 0.06547575443983078, acc: 100.0, f1: 100.0, r: 0.819715722460108
06/02/2019 12:31:40 step: 5305, epoch: 160, batch: 24, loss: 0.05975927412509918, acc: 100.0, f1: 100.0, r: 0.7318339687431431
06/02/2019 12:31:40 step: 5310, epoch: 160, batch: 29, loss: 0.06651919335126877, acc: 100.0, f1: 100.0, r: 0.7501849171004332
06/02/2019 12:31:41 *** evaluating ***
06/02/2019 12:31:41 step: 161, epoch: 160, acc: 51.70940170940172, f1: 27.147489582074748, r: 0.27362697282577914
06/02/2019 12:31:41 *** epoch: 162 ***
06/02/2019 12:31:41 *** training ***
06/02/2019 12:31:42 step: 5318, epoch: 161, batch: 4, loss: 0.2164517641067505, acc: 100.0, f1: 100.0, r: 0.7677901027721373
06/02/2019 12:31:42 step: 5323, epoch: 161, batch: 9, loss: 0.07390859723091125, acc: 100.0, f1: 100.0, r: 0.7318028573599846
06/02/2019 12:31:43 step: 5328, epoch: 161, batch: 14, loss: 0.0486786812543869, acc: 100.0, f1: 100.0, r: 0.7428774645616755
06/02/2019 12:31:43 step: 5333, epoch: 161, batch: 19, loss: 0.05382896587252617, acc: 100.0, f1: 100.0, r: 0.8129107051102523
06/02/2019 12:31:44 step: 5338, epoch: 161, batch: 24, loss: 0.04799702391028404, acc: 100.0, f1: 100.0, r: 0.7105116492152427
06/02/2019 12:31:44 step: 5343, epoch: 161, batch: 29, loss: 0.04595515504479408, acc: 100.0, f1: 100.0, r: 0.6945372685673273
06/02/2019 12:31:45 *** evaluating ***
06/02/2019 12:31:45 step: 162, epoch: 161, acc: 49.572649572649574, f1: 23.456259941623266, r: 0.25752967594753967
06/02/2019 12:31:45 *** epoch: 163 ***
06/02/2019 12:31:45 *** training ***
06/02/2019 12:31:46 step: 5351, epoch: 162, batch: 4, loss: 0.07736870646476746, acc: 100.0, f1: 100.0, r: 0.8178520834854518
06/02/2019 12:31:46 step: 5356, epoch: 162, batch: 9, loss: 0.06633399426937103, acc: 98.4375, f1: 97.92008757525998, r: 0.6926215034384644
06/02/2019 12:31:47 step: 5361, epoch: 162, batch: 14, loss: 0.05577853322029114, acc: 100.0, f1: 100.0, r: 0.7407685952110552
06/02/2019 12:31:47 step: 5366, epoch: 162, batch: 19, loss: 0.09015654027462006, acc: 100.0, f1: 100.0, r: 0.6543598104875118
06/02/2019 12:31:48 step: 5371, epoch: 162, batch: 24, loss: 0.0776425451040268, acc: 100.0, f1: 100.0, r: 0.82225058861763
06/02/2019 12:31:48 step: 5376, epoch: 162, batch: 29, loss: 0.0688624233007431, acc: 100.0, f1: 100.0, r: 0.6590557419470393
06/02/2019 12:31:49 *** evaluating ***
06/02/2019 12:31:49 step: 163, epoch: 162, acc: 52.13675213675214, f1: 26.443939076211464, r: 0.2691442815934367
06/02/2019 12:31:49 *** epoch: 164 ***
06/02/2019 12:31:49 *** training ***
06/02/2019 12:31:50 step: 5384, epoch: 163, batch: 4, loss: 0.06375614553689957, acc: 100.0, f1: 100.0, r: 0.6813830163919468
06/02/2019 12:31:50 step: 5389, epoch: 163, batch: 9, loss: 0.08124646544456482, acc: 100.0, f1: 100.0, r: 0.6883793280062934
06/02/2019 12:31:51 step: 5394, epoch: 163, batch: 14, loss: 0.08867831528186798, acc: 100.0, f1: 100.0, r: 0.6793489459396158
06/02/2019 12:31:51 step: 5399, epoch: 163, batch: 19, loss: 0.06115519255399704, acc: 100.0, f1: 100.0, r: 0.5542187403118891
06/02/2019 12:31:52 step: 5404, epoch: 163, batch: 24, loss: 0.06790190935134888, acc: 100.0, f1: 100.0, r: 0.7046164308675207
06/02/2019 12:31:53 step: 5409, epoch: 163, batch: 29, loss: 0.03988727927207947, acc: 100.0, f1: 100.0, r: 0.7990179070613753
06/02/2019 12:31:53 *** evaluating ***
06/02/2019 12:31:53 step: 164, epoch: 163, acc: 51.70940170940172, f1: 24.573798133657704, r: 0.2645067810380139
06/02/2019 12:31:53 *** epoch: 165 ***
06/02/2019 12:31:53 *** training ***
06/02/2019 12:31:54 step: 5417, epoch: 164, batch: 4, loss: 0.06240129843354225, acc: 100.0, f1: 100.0, r: 0.832557921799294
06/02/2019 12:31:54 step: 5422, epoch: 164, batch: 9, loss: 0.0313883051276207, acc: 100.0, f1: 100.0, r: 0.7078026246698667
06/02/2019 12:31:55 step: 5427, epoch: 164, batch: 14, loss: 0.07621380686759949, acc: 100.0, f1: 100.0, r: 0.7009611818683725
06/02/2019 12:31:56 step: 5432, epoch: 164, batch: 19, loss: 0.07965435087680817, acc: 100.0, f1: 100.0, r: 0.7652703266205956
06/02/2019 12:31:56 step: 5437, epoch: 164, batch: 24, loss: 0.058917101472616196, acc: 100.0, f1: 100.0, r: 0.635812649468006
06/02/2019 12:31:57 step: 5442, epoch: 164, batch: 29, loss: 0.05513554811477661, acc: 100.0, f1: 100.0, r: 0.7986330169420817
06/02/2019 12:31:57 *** evaluating ***
06/02/2019 12:31:58 step: 165, epoch: 164, acc: 53.84615384615385, f1: 27.20538611163611, r: 0.26396389295536243
06/02/2019 12:31:58 *** epoch: 166 ***
06/02/2019 12:31:58 *** training ***
06/02/2019 12:31:58 step: 5450, epoch: 165, batch: 4, loss: 0.04723428934812546, acc: 100.0, f1: 100.0, r: 0.7532298467791476
06/02/2019 12:31:59 step: 5455, epoch: 165, batch: 9, loss: 0.03967103362083435, acc: 100.0, f1: 100.0, r: 0.7859481327757762
06/02/2019 12:32:00 step: 5460, epoch: 165, batch: 14, loss: 0.07296524941921234, acc: 98.4375, f1: 97.87581699346404, r: 0.8464805175584685
06/02/2019 12:32:00 step: 5465, epoch: 165, batch: 19, loss: 0.07567145675420761, acc: 100.0, f1: 100.0, r: 0.7986948508339813
06/02/2019 12:32:01 step: 5470, epoch: 165, batch: 24, loss: 0.2372676432132721, acc: 100.0, f1: 100.0, r: 0.7019577201988764
06/02/2019 12:32:01 step: 5475, epoch: 165, batch: 29, loss: 0.10085715353488922, acc: 100.0, f1: 100.0, r: 0.8149199201951784
06/02/2019 12:32:02 *** evaluating ***
06/02/2019 12:32:02 step: 166, epoch: 165, acc: 51.70940170940172, f1: 25.541171477079793, r: 0.26564962631058575
06/02/2019 12:32:02 *** epoch: 167 ***
06/02/2019 12:32:02 *** training ***
06/02/2019 12:32:03 step: 5483, epoch: 166, batch: 4, loss: 0.0407695509493351, acc: 100.0, f1: 100.0, r: 0.7640093842601394
06/02/2019 12:32:03 step: 5488, epoch: 166, batch: 9, loss: 0.04042816907167435, acc: 100.0, f1: 100.0, r: 0.7085143631093404
06/02/2019 12:32:04 step: 5493, epoch: 166, batch: 14, loss: 0.04549715667963028, acc: 100.0, f1: 100.0, r: 0.8553082995500241
06/02/2019 12:32:05 step: 5498, epoch: 166, batch: 19, loss: 0.09409422427415848, acc: 100.0, f1: 100.0, r: 0.6679280872552703
06/02/2019 12:32:05 step: 5503, epoch: 166, batch: 24, loss: 0.06687039136886597, acc: 98.4375, f1: 97.68964189449363, r: 0.6728290461273749
06/02/2019 12:32:06 step: 5508, epoch: 166, batch: 29, loss: 0.22910767793655396, acc: 100.0, f1: 100.0, r: 0.7307632497952437
06/02/2019 12:32:06 *** evaluating ***
06/02/2019 12:32:06 step: 167, epoch: 166, acc: 52.991452991452995, f1: 26.27204564626866, r: 0.26591714459708776
06/02/2019 12:32:06 *** epoch: 168 ***
06/02/2019 12:32:06 *** training ***
06/02/2019 12:32:07 step: 5516, epoch: 167, batch: 4, loss: 0.0622769370675087, acc: 100.0, f1: 100.0, r: 0.7263631295418727
06/02/2019 12:32:08 step: 5521, epoch: 167, batch: 9, loss: 0.046638332307338715, acc: 100.0, f1: 100.0, r: 0.6583453369010162
06/02/2019 12:32:08 step: 5526, epoch: 167, batch: 14, loss: 0.07468674331903458, acc: 98.4375, f1: 98.36363636363636, r: 0.8086152529719692
06/02/2019 12:32:09 step: 5531, epoch: 167, batch: 19, loss: 0.05537945777177811, acc: 100.0, f1: 100.0, r: 0.7257210872382723
06/02/2019 12:32:09 step: 5536, epoch: 167, batch: 24, loss: 0.04947515204548836, acc: 100.0, f1: 100.0, r: 0.7222431633959837
06/02/2019 12:32:10 step: 5541, epoch: 167, batch: 29, loss: 0.06470371037721634, acc: 100.0, f1: 100.0, r: 0.6648725909271502
06/02/2019 12:32:10 *** evaluating ***
06/02/2019 12:32:11 step: 168, epoch: 167, acc: 52.991452991452995, f1: 26.576577727472866, r: 0.26571040679642793
06/02/2019 12:32:11 *** epoch: 169 ***
06/02/2019 12:32:11 *** training ***
06/02/2019 12:32:11 step: 5549, epoch: 168, batch: 4, loss: 0.0704181045293808, acc: 100.0, f1: 100.0, r: 0.6951080325934818
06/02/2019 12:32:12 step: 5554, epoch: 168, batch: 9, loss: 0.06573139131069183, acc: 100.0, f1: 100.0, r: 0.8154855661277713
06/02/2019 12:32:13 step: 5559, epoch: 168, batch: 14, loss: 0.06276575475931168, acc: 100.0, f1: 100.0, r: 0.7461631217421341
06/02/2019 12:32:13 step: 5564, epoch: 168, batch: 19, loss: 0.24187496304512024, acc: 98.4375, f1: 99.05314009661835, r: 0.692483597983629
06/02/2019 12:32:14 step: 5569, epoch: 168, batch: 24, loss: 0.05817583203315735, acc: 100.0, f1: 100.0, r: 0.7552185341444341
06/02/2019 12:32:14 step: 5574, epoch: 168, batch: 29, loss: 0.045898981392383575, acc: 100.0, f1: 100.0, r: 0.7687552669963863
06/02/2019 12:32:15 *** evaluating ***
06/02/2019 12:32:15 step: 169, epoch: 168, acc: 53.41880341880342, f1: 26.133523008523007, r: 0.26737952169905815
06/02/2019 12:32:15 *** epoch: 170 ***
06/02/2019 12:32:15 *** training ***
06/02/2019 12:32:16 step: 5582, epoch: 169, batch: 4, loss: 0.2019067108631134, acc: 100.0, f1: 100.0, r: 0.8157075002034284
06/02/2019 12:32:16 step: 5587, epoch: 169, batch: 9, loss: 0.22937826812267303, acc: 98.4375, f1: 94.82993197278911, r: 0.7043889565616818
06/02/2019 12:32:17 step: 5592, epoch: 169, batch: 14, loss: 0.2706983685493469, acc: 100.0, f1: 100.0, r: 0.6606886755394263
06/02/2019 12:32:17 step: 5597, epoch: 169, batch: 19, loss: 0.06667099893093109, acc: 100.0, f1: 100.0, r: 0.6815786977514354
06/02/2019 12:32:18 step: 5602, epoch: 169, batch: 24, loss: 0.046758830547332764, acc: 100.0, f1: 100.0, r: 0.74657495819158
06/02/2019 12:32:19 step: 5607, epoch: 169, batch: 29, loss: 0.04373949021100998, acc: 100.0, f1: 100.0, r: 0.8259571316137369
06/02/2019 12:32:19 *** evaluating ***
06/02/2019 12:32:19 step: 170, epoch: 169, acc: 52.991452991452995, f1: 25.91720779220779, r: 0.2683232560689936
06/02/2019 12:32:19 *** epoch: 171 ***
06/02/2019 12:32:19 *** training ***
06/02/2019 12:32:20 step: 5615, epoch: 170, batch: 4, loss: 0.045831311494112015, acc: 100.0, f1: 100.0, r: 0.8443853976152504
06/02/2019 12:32:21 step: 5620, epoch: 170, batch: 9, loss: 0.03928148373961449, acc: 100.0, f1: 100.0, r: 0.8313019962535317
06/02/2019 12:32:21 step: 5625, epoch: 170, batch: 14, loss: 0.08832304924726486, acc: 100.0, f1: 100.0, r: 0.8010582372931636
06/02/2019 12:32:22 step: 5630, epoch: 170, batch: 19, loss: 0.04037492349743843, acc: 100.0, f1: 100.0, r: 0.6817250225044436
06/02/2019 12:32:22 step: 5635, epoch: 170, batch: 24, loss: 0.07482466101646423, acc: 98.4375, f1: 97.26415094339622, r: 0.814931200547139
06/02/2019 12:32:23 step: 5640, epoch: 170, batch: 29, loss: 0.06883803009986877, acc: 100.0, f1: 100.0, r: 0.6311083306902389
06/02/2019 12:32:24 *** evaluating ***
06/02/2019 12:32:24 step: 171, epoch: 170, acc: 52.991452991452995, f1: 24.3533676652265, r: 0.2537346112291908
06/02/2019 12:32:24 *** epoch: 172 ***
06/02/2019 12:32:24 *** training ***
06/02/2019 12:32:24 step: 5648, epoch: 171, batch: 4, loss: 0.040486615151166916, acc: 100.0, f1: 100.0, r: 0.7103133039863708
06/02/2019 12:32:25 step: 5653, epoch: 171, batch: 9, loss: 0.029705705121159554, acc: 100.0, f1: 100.0, r: 0.7117459209290056
06/02/2019 12:32:26 step: 5658, epoch: 171, batch: 14, loss: 0.08026200532913208, acc: 98.4375, f1: 98.58823529411765, r: 0.7175184366880838
06/02/2019 12:32:26 step: 5663, epoch: 171, batch: 19, loss: 0.05685923621058464, acc: 100.0, f1: 100.0, r: 0.7747223006844921
06/02/2019 12:32:27 step: 5668, epoch: 171, batch: 24, loss: 0.04117883741855621, acc: 100.0, f1: 100.0, r: 0.7267012757392092
06/02/2019 12:32:28 step: 5673, epoch: 171, batch: 29, loss: 0.05943162366747856, acc: 100.0, f1: 100.0, r: 0.6314302164611372
06/02/2019 12:32:28 *** evaluating ***
06/02/2019 12:32:28 step: 172, epoch: 171, acc: 50.85470085470085, f1: 25.882228980055068, r: 0.25794998120862506
06/02/2019 12:32:28 *** epoch: 173 ***
06/02/2019 12:32:28 *** training ***
06/02/2019 12:32:29 step: 5681, epoch: 172, batch: 4, loss: 0.0589621439576149, acc: 98.4375, f1: 96.33986928104575, r: 0.6798763897337607
06/02/2019 12:32:30 step: 5686, epoch: 172, batch: 9, loss: 0.06194417551159859, acc: 100.0, f1: 100.0, r: 0.7971823063273213
06/02/2019 12:32:30 step: 5691, epoch: 172, batch: 14, loss: 0.06065648794174194, acc: 100.0, f1: 100.0, r: 0.8158706033181775
06/02/2019 12:32:31 step: 5696, epoch: 172, batch: 19, loss: 0.09198303520679474, acc: 98.4375, f1: 99.07067742724381, r: 0.5581283221317621
06/02/2019 12:32:31 step: 5701, epoch: 172, batch: 24, loss: 0.08597394824028015, acc: 100.0, f1: 100.0, r: 0.7154658032437374
06/02/2019 12:32:32 step: 5706, epoch: 172, batch: 29, loss: 0.08407635986804962, acc: 98.4375, f1: 96.39097744360903, r: 0.7388589288041971
06/02/2019 12:32:32 *** evaluating ***
06/02/2019 12:32:32 step: 173, epoch: 172, acc: 52.13675213675214, f1: 26.711249803746227, r: 0.2579388809724088
06/02/2019 12:32:32 *** epoch: 174 ***
06/02/2019 12:32:32 *** training ***
06/02/2019 12:32:33 step: 5714, epoch: 173, batch: 4, loss: 0.06272916495800018, acc: 100.0, f1: 100.0, r: 0.7052485635320319
06/02/2019 12:32:33 step: 5719, epoch: 173, batch: 9, loss: 0.05123733729124069, acc: 100.0, f1: 100.0, r: 0.7988113331243283
06/02/2019 12:32:34 step: 5724, epoch: 173, batch: 14, loss: 0.06145305931568146, acc: 100.0, f1: 100.0, r: 0.7858006393933121
06/02/2019 12:32:35 step: 5729, epoch: 173, batch: 19, loss: 0.07517386972904205, acc: 100.0, f1: 100.0, r: 0.6876205161614393
06/02/2019 12:32:35 step: 5734, epoch: 173, batch: 24, loss: 0.06236875057220459, acc: 100.0, f1: 100.0, r: 0.7807577293525544
06/02/2019 12:32:36 step: 5739, epoch: 173, batch: 29, loss: 0.03364536166191101, acc: 100.0, f1: 100.0, r: 0.7375661191788863
06/02/2019 12:32:36 *** evaluating ***
06/02/2019 12:32:37 step: 174, epoch: 173, acc: 53.84615384615385, f1: 25.373978270476556, r: 0.2676858038902911
06/02/2019 12:32:37 *** epoch: 175 ***
06/02/2019 12:32:37 *** training ***
06/02/2019 12:32:37 step: 5747, epoch: 174, batch: 4, loss: 0.05969725176692009, acc: 100.0, f1: 100.0, r: 0.8054472861909489
06/02/2019 12:32:38 step: 5752, epoch: 174, batch: 9, loss: 0.04736977070569992, acc: 100.0, f1: 100.0, r: 0.7068070900445752
06/02/2019 12:32:38 step: 5757, epoch: 174, batch: 14, loss: 0.05084896460175514, acc: 100.0, f1: 100.0, r: 0.829207016750262
06/02/2019 12:32:39 step: 5762, epoch: 174, batch: 19, loss: 0.0537363700568676, acc: 100.0, f1: 100.0, r: 0.7873154881850846
06/02/2019 12:32:40 step: 5767, epoch: 174, batch: 24, loss: 0.2822861075401306, acc: 100.0, f1: 100.0, r: 0.7387542015062973
06/02/2019 12:32:41 step: 5772, epoch: 174, batch: 29, loss: 0.0633632093667984, acc: 100.0, f1: 100.0, r: 0.6818266571817159
06/02/2019 12:32:41 *** evaluating ***
06/02/2019 12:32:41 step: 175, epoch: 174, acc: 52.56410256410257, f1: 26.442104449478464, r: 0.272020033980343
06/02/2019 12:32:41 *** epoch: 176 ***
06/02/2019 12:32:41 *** training ***
06/02/2019 12:32:42 step: 5780, epoch: 175, batch: 4, loss: 0.04982513189315796, acc: 100.0, f1: 100.0, r: 0.818731692740158
06/02/2019 12:32:43 step: 5785, epoch: 175, batch: 9, loss: 0.05159125477075577, acc: 100.0, f1: 100.0, r: 0.8375236888938937
06/02/2019 12:32:43 step: 5790, epoch: 175, batch: 14, loss: 0.05876944214105606, acc: 100.0, f1: 100.0, r: 0.781071189492824
06/02/2019 12:32:44 step: 5795, epoch: 175, batch: 19, loss: 0.0755375325679779, acc: 100.0, f1: 100.0, r: 0.6980150424027883
06/02/2019 12:32:44 step: 5800, epoch: 175, batch: 24, loss: 0.03333628922700882, acc: 100.0, f1: 100.0, r: 0.834049982127137
06/02/2019 12:32:45 step: 5805, epoch: 175, batch: 29, loss: 0.06785750389099121, acc: 100.0, f1: 100.0, r: 0.8209762425548224
06/02/2019 12:32:45 *** evaluating ***
06/02/2019 12:32:45 step: 176, epoch: 175, acc: 52.13675213675214, f1: 24.69443161245557, r: 0.2695582177273842
06/02/2019 12:32:45 *** epoch: 177 ***
06/02/2019 12:32:45 *** training ***
06/02/2019 12:32:46 step: 5813, epoch: 176, batch: 4, loss: 0.05809301882982254, acc: 100.0, f1: 100.0, r: 0.7381343595681035
06/02/2019 12:32:46 step: 5818, epoch: 176, batch: 9, loss: 0.21558554470539093, acc: 100.0, f1: 100.0, r: 0.7276366350977325
06/02/2019 12:32:47 step: 5823, epoch: 176, batch: 14, loss: 0.06588760018348694, acc: 100.0, f1: 100.0, r: 0.816847655026726
06/02/2019 12:32:48 step: 5828, epoch: 176, batch: 19, loss: 0.057905785739421844, acc: 100.0, f1: 100.0, r: 0.8404245463732011
06/02/2019 12:32:48 step: 5833, epoch: 176, batch: 24, loss: 0.07685306668281555, acc: 100.0, f1: 100.0, r: 0.7883478570278687
06/02/2019 12:32:49 step: 5838, epoch: 176, batch: 29, loss: 0.08138106763362885, acc: 100.0, f1: 100.0, r: 0.8176010856598422
06/02/2019 12:32:49 *** evaluating ***
06/02/2019 12:32:50 step: 177, epoch: 176, acc: 50.85470085470085, f1: 24.620867006224397, r: 0.25786544346932905
06/02/2019 12:32:50 *** epoch: 178 ***
06/02/2019 12:32:50 *** training ***
06/02/2019 12:32:50 step: 5846, epoch: 177, batch: 4, loss: 0.0504293330013752, acc: 100.0, f1: 100.0, r: 0.7095143427922094
06/02/2019 12:32:51 step: 5851, epoch: 177, batch: 9, loss: 0.11734014749526978, acc: 98.4375, f1: 99.32229495571814, r: 0.6935000557942078
06/02/2019 12:32:51 step: 5856, epoch: 177, batch: 14, loss: 0.09493871033191681, acc: 100.0, f1: 100.0, r: 0.6206495663350476
06/02/2019 12:32:52 step: 5861, epoch: 177, batch: 19, loss: 0.08887103945016861, acc: 100.0, f1: 100.0, r: 0.8289346863940639
06/02/2019 12:32:53 step: 5866, epoch: 177, batch: 24, loss: 0.051719941198825836, acc: 100.0, f1: 100.0, r: 0.7234575125184997
06/02/2019 12:32:53 step: 5871, epoch: 177, batch: 29, loss: 0.061381712555885315, acc: 100.0, f1: 100.0, r: 0.8061090867769363
06/02/2019 12:32:54 *** evaluating ***
06/02/2019 12:32:54 step: 178, epoch: 177, acc: 53.84615384615385, f1: 25.52003302003302, r: 0.2666839379019772
06/02/2019 12:32:54 *** epoch: 179 ***
06/02/2019 12:32:54 *** training ***
06/02/2019 12:32:54 step: 5879, epoch: 178, batch: 4, loss: 0.05480417609214783, acc: 100.0, f1: 100.0, r: 0.7208399230097526
06/02/2019 12:32:55 step: 5884, epoch: 178, batch: 9, loss: 0.033245690166950226, acc: 100.0, f1: 100.0, r: 0.6253610824225914
06/02/2019 12:32:55 step: 5889, epoch: 178, batch: 14, loss: 0.03689325973391533, acc: 100.0, f1: 100.0, r: 0.7973300676941175
06/02/2019 12:32:56 step: 5894, epoch: 178, batch: 19, loss: 0.06726393848657608, acc: 100.0, f1: 100.0, r: 0.7789280063060793
06/02/2019 12:32:57 step: 5899, epoch: 178, batch: 24, loss: 0.061622217297554016, acc: 100.0, f1: 100.0, r: 0.8567148168529348
06/02/2019 12:32:57 step: 5904, epoch: 178, batch: 29, loss: 0.053055644035339355, acc: 100.0, f1: 100.0, r: 0.8295806896254042
06/02/2019 12:32:58 *** evaluating ***
06/02/2019 12:32:58 step: 179, epoch: 178, acc: 55.12820512820513, f1: 27.59901471224341, r: 0.2691504414500489
06/02/2019 12:32:58 *** epoch: 180 ***
06/02/2019 12:32:58 *** training ***
06/02/2019 12:32:58 step: 5912, epoch: 179, batch: 4, loss: 0.047195836901664734, acc: 100.0, f1: 100.0, r: 0.7471939708536425
06/02/2019 12:32:59 step: 5917, epoch: 179, batch: 9, loss: 0.05124707147479057, acc: 100.0, f1: 100.0, r: 0.7465252398699684
06/02/2019 12:33:00 step: 5922, epoch: 179, batch: 14, loss: 0.04401955008506775, acc: 100.0, f1: 100.0, r: 0.7754021505793607
06/02/2019 12:33:00 step: 5927, epoch: 179, batch: 19, loss: 0.07962299883365631, acc: 98.4375, f1: 99.26962872793669, r: 0.7394794936431718
06/02/2019 12:33:01 step: 5932, epoch: 179, batch: 24, loss: 0.04778232425451279, acc: 100.0, f1: 100.0, r: 0.689217737331201
06/02/2019 12:33:01 step: 5937, epoch: 179, batch: 29, loss: 0.051597580313682556, acc: 100.0, f1: 100.0, r: 0.8211909324406635
06/02/2019 12:33:02 *** evaluating ***
06/02/2019 12:33:02 step: 180, epoch: 179, acc: 52.991452991452995, f1: 26.04950081621865, r: 0.2720798920317926
06/02/2019 12:33:02 *** epoch: 181 ***
06/02/2019 12:33:02 *** training ***
06/02/2019 12:33:03 step: 5945, epoch: 180, batch: 4, loss: 0.03834584355354309, acc: 100.0, f1: 100.0, r: 0.7498301625120594
06/02/2019 12:33:03 step: 5950, epoch: 180, batch: 9, loss: 0.04291318356990814, acc: 100.0, f1: 100.0, r: 0.7605805386057551
06/02/2019 12:33:04 step: 5955, epoch: 180, batch: 14, loss: 0.06698407232761383, acc: 100.0, f1: 100.0, r: 0.6223035933073204
06/02/2019 12:33:04 step: 5960, epoch: 180, batch: 19, loss: 0.051680393517017365, acc: 100.0, f1: 100.0, r: 0.6533381342349899
06/02/2019 12:33:05 step: 5965, epoch: 180, batch: 24, loss: 0.047616131603717804, acc: 100.0, f1: 100.0, r: 0.7043055118660974
06/02/2019 12:33:05 step: 5970, epoch: 180, batch: 29, loss: 0.056838348507881165, acc: 100.0, f1: 100.0, r: 0.7753217431527475
06/02/2019 12:33:06 *** evaluating ***
06/02/2019 12:33:06 step: 181, epoch: 180, acc: 52.991452991452995, f1: 26.171597863045616, r: 0.28111462914196617
06/02/2019 12:33:06 *** epoch: 182 ***
06/02/2019 12:33:06 *** training ***
06/02/2019 12:33:06 step: 5978, epoch: 181, batch: 4, loss: 0.058357689529657364, acc: 100.0, f1: 100.0, r: 0.6884640968282335
06/02/2019 12:33:07 step: 5983, epoch: 181, batch: 9, loss: 0.05917028710246086, acc: 100.0, f1: 100.0, r: 0.7806642877784795
06/02/2019 12:33:07 step: 5988, epoch: 181, batch: 14, loss: 0.05446971207857132, acc: 100.0, f1: 100.0, r: 0.7206765784301903
06/02/2019 12:33:08 step: 5993, epoch: 181, batch: 19, loss: 0.06541724503040314, acc: 100.0, f1: 100.0, r: 0.783208024597246
06/02/2019 12:33:09 step: 5998, epoch: 181, batch: 24, loss: 0.07130414247512817, acc: 100.0, f1: 100.0, r: 0.8263113729840083
06/02/2019 12:33:09 step: 6003, epoch: 181, batch: 29, loss: 0.08082950115203857, acc: 100.0, f1: 100.0, r: 0.7087094980419368
06/02/2019 12:33:10 *** evaluating ***
06/02/2019 12:33:10 step: 182, epoch: 181, acc: 54.700854700854705, f1: 24.779716626479555, r: 0.26292340608478515
06/02/2019 12:33:10 *** epoch: 183 ***
06/02/2019 12:33:10 *** training ***
06/02/2019 12:33:10 step: 6011, epoch: 182, batch: 4, loss: 0.06540985405445099, acc: 98.4375, f1: 96.8733153638814, r: 0.7087374132155262
06/02/2019 12:33:11 step: 6016, epoch: 182, batch: 9, loss: 0.03888031095266342, acc: 100.0, f1: 100.0, r: 0.7097220483707355
06/02/2019 12:33:12 step: 6021, epoch: 182, batch: 14, loss: 0.049676984548568726, acc: 100.0, f1: 100.0, r: 0.8023755626496762
06/02/2019 12:33:12 step: 6026, epoch: 182, batch: 19, loss: 0.051885783672332764, acc: 100.0, f1: 100.0, r: 0.5922182574229419
06/02/2019 12:33:13 step: 6031, epoch: 182, batch: 24, loss: 0.14135532081127167, acc: 100.0, f1: 100.0, r: 0.8286227507139023
06/02/2019 12:33:14 step: 6036, epoch: 182, batch: 29, loss: 0.09875942766666412, acc: 100.0, f1: 100.0, r: 0.7051644764365916
06/02/2019 12:33:14 *** evaluating ***
06/02/2019 12:33:14 step: 183, epoch: 182, acc: 52.991452991452995, f1: 24.138285233486474, r: 0.2705663249859764
06/02/2019 12:33:14 *** epoch: 184 ***
06/02/2019 12:33:14 *** training ***
06/02/2019 12:33:15 step: 6044, epoch: 183, batch: 4, loss: 0.08628338575363159, acc: 100.0, f1: 100.0, r: 0.7881325697363697
06/02/2019 12:33:15 step: 6049, epoch: 183, batch: 9, loss: 0.08101599663496017, acc: 98.4375, f1: 98.38383838383838, r: 0.6668110240524062
06/02/2019 12:33:16 step: 6054, epoch: 183, batch: 14, loss: 0.061903297901153564, acc: 100.0, f1: 100.0, r: 0.783919141647534
06/02/2019 12:33:16 step: 6059, epoch: 183, batch: 19, loss: 0.05517382547259331, acc: 100.0, f1: 100.0, r: 0.7636065683762008
06/02/2019 12:33:17 step: 6064, epoch: 183, batch: 24, loss: 0.04377081245183945, acc: 100.0, f1: 100.0, r: 0.6924054596458636
06/02/2019 12:33:18 step: 6069, epoch: 183, batch: 29, loss: 0.0626571848988533, acc: 100.0, f1: 100.0, r: 0.7644870006240858
06/02/2019 12:33:18 *** evaluating ***
06/02/2019 12:33:18 step: 184, epoch: 183, acc: 54.27350427350427, f1: 26.567423230974637, r: 0.27994541739062573
06/02/2019 12:33:18 *** epoch: 185 ***
06/02/2019 12:33:18 *** training ***
06/02/2019 12:33:19 step: 6077, epoch: 184, batch: 4, loss: 0.050660885870456696, acc: 100.0, f1: 100.0, r: 0.6567265678212796
06/02/2019 12:33:20 step: 6082, epoch: 184, batch: 9, loss: 0.06591209769248962, acc: 100.0, f1: 100.0, r: 0.801517727512475
06/02/2019 12:33:20 step: 6087, epoch: 184, batch: 14, loss: 0.10243453085422516, acc: 100.0, f1: 100.0, r: 0.7415671884125593
06/02/2019 12:33:21 step: 6092, epoch: 184, batch: 19, loss: 0.04366796091198921, acc: 100.0, f1: 100.0, r: 0.6894752528523138
06/02/2019 12:33:21 step: 6097, epoch: 184, batch: 24, loss: 0.045605357736349106, acc: 100.0, f1: 100.0, r: 0.7171130931028313
06/02/2019 12:33:22 step: 6102, epoch: 184, batch: 29, loss: 0.042104654014110565, acc: 100.0, f1: 100.0, r: 0.708705882584546
06/02/2019 12:33:22 *** evaluating ***
06/02/2019 12:33:23 step: 185, epoch: 184, acc: 54.700854700854705, f1: 28.182855339105338, r: 0.27956535259733006
06/02/2019 12:33:23 *** epoch: 186 ***
06/02/2019 12:33:23 *** training ***
06/02/2019 12:33:23 step: 6110, epoch: 185, batch: 4, loss: 0.06938177347183228, acc: 100.0, f1: 100.0, r: 0.6602707383338821
06/02/2019 12:33:24 step: 6115, epoch: 185, batch: 9, loss: 0.057900670915842056, acc: 100.0, f1: 100.0, r: 0.8259001011481714
06/02/2019 12:33:24 step: 6120, epoch: 185, batch: 14, loss: 0.06630115211009979, acc: 100.0, f1: 100.0, r: 0.853819168573224
06/02/2019 12:33:25 step: 6125, epoch: 185, batch: 19, loss: 0.062206752598285675, acc: 100.0, f1: 100.0, r: 0.7362445372639225
06/02/2019 12:33:26 step: 6130, epoch: 185, batch: 24, loss: 0.0492706224322319, acc: 100.0, f1: 100.0, r: 0.7762083000172412
06/02/2019 12:33:26 step: 6135, epoch: 185, batch: 29, loss: 0.09395703673362732, acc: 100.0, f1: 100.0, r: 0.718131368714612
06/02/2019 12:33:26 *** evaluating ***
06/02/2019 12:33:27 step: 186, epoch: 185, acc: 53.41880341880342, f1: 25.91818172214116, r: 0.27458733842465854
06/02/2019 12:33:27 *** epoch: 187 ***
06/02/2019 12:33:27 *** training ***
06/02/2019 12:33:27 step: 6143, epoch: 186, batch: 4, loss: 0.07661264389753342, acc: 98.4375, f1: 98.37526205450735, r: 0.7792086049473737
06/02/2019 12:33:28 step: 6148, epoch: 186, batch: 9, loss: 0.05852213501930237, acc: 100.0, f1: 100.0, r: 0.7775216316417308
06/02/2019 12:33:28 step: 6153, epoch: 186, batch: 14, loss: 0.08988192677497864, acc: 98.4375, f1: 98.39924670433145, r: 0.8021004937769026
06/02/2019 12:33:29 step: 6158, epoch: 186, batch: 19, loss: 0.07316596806049347, acc: 100.0, f1: 100.0, r: 0.7336524387253662
06/02/2019 12:33:30 step: 6163, epoch: 186, batch: 24, loss: 0.06711259484291077, acc: 100.0, f1: 100.0, r: 0.764314731209595
06/02/2019 12:33:30 step: 6168, epoch: 186, batch: 29, loss: 0.15366533398628235, acc: 100.0, f1: 100.0, r: 0.7723570331121569
06/02/2019 12:33:31 *** evaluating ***
06/02/2019 12:33:31 step: 187, epoch: 186, acc: 52.13675213675214, f1: 25.777700411763337, r: 0.2705864212052815
06/02/2019 12:33:31 *** epoch: 188 ***
06/02/2019 12:33:31 *** training ***
06/02/2019 12:33:31 step: 6176, epoch: 187, batch: 4, loss: 0.06197284907102585, acc: 100.0, f1: 100.0, r: 0.6243127338100547
06/02/2019 12:33:32 step: 6181, epoch: 187, batch: 9, loss: 0.09783472865819931, acc: 100.0, f1: 100.0, r: 0.7297446053332697
06/02/2019 12:33:33 step: 6186, epoch: 187, batch: 14, loss: 0.057624902576208115, acc: 100.0, f1: 100.0, r: 0.8216035181106006
06/02/2019 12:33:33 step: 6191, epoch: 187, batch: 19, loss: 0.11635193973779678, acc: 100.0, f1: 100.0, r: 0.8186525204563007
06/02/2019 12:33:34 step: 6196, epoch: 187, batch: 24, loss: 0.06003360077738762, acc: 100.0, f1: 100.0, r: 0.6597101465128943
06/02/2019 12:33:35 step: 6201, epoch: 187, batch: 29, loss: 0.04653068631887436, acc: 100.0, f1: 100.0, r: 0.793646991068601
06/02/2019 12:33:35 *** evaluating ***
06/02/2019 12:33:35 step: 188, epoch: 187, acc: 52.56410256410257, f1: 25.95313299693348, r: 0.2598180211875756
06/02/2019 12:33:35 *** epoch: 189 ***
06/02/2019 12:33:35 *** training ***
06/02/2019 12:33:36 step: 6209, epoch: 188, batch: 4, loss: 0.041757065802812576, acc: 100.0, f1: 100.0, r: 0.6550445977788282
06/02/2019 12:33:37 step: 6214, epoch: 188, batch: 9, loss: 0.08044841885566711, acc: 100.0, f1: 100.0, r: 0.8277516597124845
06/02/2019 12:33:37 step: 6219, epoch: 188, batch: 14, loss: 0.07601581513881683, acc: 100.0, f1: 100.0, r: 0.7483149475570404
06/02/2019 12:33:38 step: 6224, epoch: 188, batch: 19, loss: 0.09535586833953857, acc: 100.0, f1: 100.0, r: 0.7773951362950526
06/02/2019 12:33:39 step: 6229, epoch: 188, batch: 24, loss: 0.06904610246419907, acc: 100.0, f1: 100.0, r: 0.6684489635976663
06/02/2019 12:33:39 step: 6234, epoch: 188, batch: 29, loss: 0.04650971293449402, acc: 100.0, f1: 100.0, r: 0.7148221966918028
06/02/2019 12:33:40 *** evaluating ***
06/02/2019 12:33:40 step: 189, epoch: 188, acc: 51.70940170940172, f1: 24.04949267521075, r: 0.26296357241735485
06/02/2019 12:33:40 *** epoch: 190 ***
06/02/2019 12:33:40 *** training ***
06/02/2019 12:33:41 step: 6242, epoch: 189, batch: 4, loss: 0.04280849173665047, acc: 100.0, f1: 100.0, r: 0.750038350861737
06/02/2019 12:33:41 step: 6247, epoch: 189, batch: 9, loss: 0.07373321056365967, acc: 100.0, f1: 100.0, r: 0.7271405034326752
06/02/2019 12:33:42 step: 6252, epoch: 189, batch: 14, loss: 0.057155542075634, acc: 100.0, f1: 100.0, r: 0.7882117288294139
06/02/2019 12:33:42 step: 6257, epoch: 189, batch: 19, loss: 0.08263812959194183, acc: 98.4375, f1: 99.0, r: 0.7922640588156238
06/02/2019 12:33:43 step: 6262, epoch: 189, batch: 24, loss: 0.07495944201946259, acc: 100.0, f1: 100.0, r: 0.7289187033597744
06/02/2019 12:33:44 step: 6267, epoch: 189, batch: 29, loss: 0.06574515253305435, acc: 100.0, f1: 100.0, r: 0.5779086178853354
06/02/2019 12:33:44 *** evaluating ***
06/02/2019 12:33:44 step: 190, epoch: 189, acc: 52.991452991452995, f1: 26.100611867516754, r: 0.2581280550719021
06/02/2019 12:33:44 *** epoch: 191 ***
06/02/2019 12:33:44 *** training ***
06/02/2019 12:33:45 step: 6275, epoch: 190, batch: 4, loss: 0.2748880684375763, acc: 100.0, f1: 100.0, r: 0.6400951002778603
06/02/2019 12:33:46 step: 6280, epoch: 190, batch: 9, loss: 0.057561129331588745, acc: 100.0, f1: 100.0, r: 0.6434836334803966
06/02/2019 12:33:46 step: 6285, epoch: 190, batch: 14, loss: 0.08343575894832611, acc: 100.0, f1: 100.0, r: 0.6835950829527587
06/02/2019 12:33:47 step: 6290, epoch: 190, batch: 19, loss: 0.06265166401863098, acc: 100.0, f1: 100.0, r: 0.6734936377265004
06/02/2019 12:33:47 step: 6295, epoch: 190, batch: 24, loss: 0.2952826917171478, acc: 100.0, f1: 100.0, r: 0.7643665319859365
06/02/2019 12:33:48 step: 6300, epoch: 190, batch: 29, loss: 0.05968477576971054, acc: 100.0, f1: 100.0, r: 0.7594309358481757
06/02/2019 12:33:48 *** evaluating ***
06/02/2019 12:33:49 step: 191, epoch: 190, acc: 53.84615384615385, f1: 25.695596220944516, r: 0.2817862489748174
06/02/2019 12:33:49 *** epoch: 192 ***
06/02/2019 12:33:49 *** training ***
06/02/2019 12:33:49 step: 6308, epoch: 191, batch: 4, loss: 0.08310945332050323, acc: 100.0, f1: 100.0, r: 0.7527376334731438
06/02/2019 12:33:50 step: 6313, epoch: 191, batch: 9, loss: 0.07706749439239502, acc: 100.0, f1: 100.0, r: 0.7401329982870039
06/02/2019 12:33:50 step: 6318, epoch: 191, batch: 14, loss: 0.06415639817714691, acc: 100.0, f1: 100.0, r: 0.6777948543362288
06/02/2019 12:33:51 step: 6323, epoch: 191, batch: 19, loss: 0.03925454616546631, acc: 100.0, f1: 100.0, r: 0.7076147277729553
06/02/2019 12:33:52 step: 6328, epoch: 191, batch: 24, loss: 0.0795029029250145, acc: 98.4375, f1: 98.78335949764521, r: 0.7681300671979722
06/02/2019 12:33:52 step: 6333, epoch: 191, batch: 29, loss: 0.03846125304698944, acc: 100.0, f1: 100.0, r: 0.8347124809052084
06/02/2019 12:33:53 *** evaluating ***
06/02/2019 12:33:53 step: 192, epoch: 191, acc: 53.84615384615385, f1: 25.93724572082724, r: 0.2767332207018512
06/02/2019 12:33:53 *** epoch: 193 ***
06/02/2019 12:33:53 *** training ***
06/02/2019 12:33:53 step: 6341, epoch: 192, batch: 4, loss: 0.0689917504787445, acc: 100.0, f1: 100.0, r: 0.8210905707079816
06/02/2019 12:33:54 step: 6346, epoch: 192, batch: 9, loss: 0.08019287884235382, acc: 100.0, f1: 100.0, r: 0.6953226378365378
06/02/2019 12:33:55 step: 6351, epoch: 192, batch: 14, loss: 0.04182738810777664, acc: 100.0, f1: 100.0, r: 0.5678170387754061
06/02/2019 12:33:55 step: 6356, epoch: 192, batch: 19, loss: 0.05842433124780655, acc: 100.0, f1: 100.0, r: 0.7120886556985854
06/02/2019 12:33:56 step: 6361, epoch: 192, batch: 24, loss: 0.06691879034042358, acc: 100.0, f1: 100.0, r: 0.7954481717335983
06/02/2019 12:33:56 step: 6366, epoch: 192, batch: 29, loss: 0.05018739029765129, acc: 100.0, f1: 100.0, r: 0.6906291050219927
06/02/2019 12:33:57 *** evaluating ***
06/02/2019 12:33:57 step: 193, epoch: 192, acc: 52.13675213675214, f1: 24.08810560022645, r: 0.2607543431428584
06/02/2019 12:33:57 *** epoch: 194 ***
06/02/2019 12:33:57 *** training ***
06/02/2019 12:33:58 step: 6374, epoch: 193, batch: 4, loss: 0.05606166273355484, acc: 100.0, f1: 100.0, r: 0.7451505641021646
06/02/2019 12:33:58 step: 6379, epoch: 193, batch: 9, loss: 0.057673532515764236, acc: 100.0, f1: 100.0, r: 0.8321306998331408
06/02/2019 12:33:59 step: 6384, epoch: 193, batch: 14, loss: 0.2385176122188568, acc: 100.0, f1: 100.0, r: 0.66860015488331
06/02/2019 12:33:59 step: 6389, epoch: 193, batch: 19, loss: 0.06149310618638992, acc: 100.0, f1: 100.0, r: 0.7281905266365605
06/02/2019 12:34:00 step: 6394, epoch: 193, batch: 24, loss: 0.05013667047023773, acc: 100.0, f1: 100.0, r: 0.6542092285896797
06/02/2019 12:34:01 step: 6399, epoch: 193, batch: 29, loss: 0.0515497624874115, acc: 100.0, f1: 100.0, r: 0.8170040183354069
06/02/2019 12:34:01 *** evaluating ***
06/02/2019 12:34:01 step: 194, epoch: 193, acc: 53.41880341880342, f1: 25.53334165834166, r: 0.2685187284824711
06/02/2019 12:34:01 *** epoch: 195 ***
06/02/2019 12:34:01 *** training ***
06/02/2019 12:34:02 step: 6407, epoch: 194, batch: 4, loss: 0.06035266071557999, acc: 100.0, f1: 100.0, r: 0.8555057450428515
06/02/2019 12:34:03 step: 6412, epoch: 194, batch: 9, loss: 0.06516913324594498, acc: 100.0, f1: 100.0, r: 0.6296730824795858
06/02/2019 12:34:03 step: 6417, epoch: 194, batch: 14, loss: 0.05215220898389816, acc: 100.0, f1: 100.0, r: 0.6309640474704181
06/02/2019 12:34:04 step: 6422, epoch: 194, batch: 19, loss: 0.07124632596969604, acc: 98.4375, f1: 98.0952380952381, r: 0.6954418898927269
06/02/2019 12:34:04 step: 6427, epoch: 194, batch: 24, loss: 0.08628126978874207, acc: 100.0, f1: 100.0, r: 0.6680070019467833
06/02/2019 12:34:05 step: 6432, epoch: 194, batch: 29, loss: 0.05405275523662567, acc: 100.0, f1: 100.0, r: 0.7999269653691592
06/02/2019 12:34:05 *** evaluating ***
06/02/2019 12:34:05 step: 195, epoch: 194, acc: 54.27350427350427, f1: 26.304334725810175, r: 0.27204781744531137
06/02/2019 12:34:05 *** epoch: 196 ***
06/02/2019 12:34:05 *** training ***
06/02/2019 12:34:06 step: 6440, epoch: 195, batch: 4, loss: 0.06836511194705963, acc: 100.0, f1: 100.0, r: 0.7320724319210644
06/02/2019 12:34:07 step: 6445, epoch: 195, batch: 9, loss: 0.059124790132045746, acc: 100.0, f1: 100.0, r: 0.7731692481391923
06/02/2019 12:34:07 step: 6450, epoch: 195, batch: 14, loss: 0.05507078766822815, acc: 100.0, f1: 100.0, r: 0.6772903569642073
06/02/2019 12:34:08 step: 6455, epoch: 195, batch: 19, loss: 0.06546597182750702, acc: 98.4375, f1: 97.03703703703704, r: 0.8273733886368563
06/02/2019 12:34:09 step: 6460, epoch: 195, batch: 24, loss: 0.22583360970020294, acc: 100.0, f1: 100.0, r: 0.8084669589706193
06/02/2019 12:34:09 step: 6465, epoch: 195, batch: 29, loss: 0.06698259711265564, acc: 100.0, f1: 100.0, r: 0.6894777613003097
06/02/2019 12:34:10 *** evaluating ***
06/02/2019 12:34:10 step: 196, epoch: 195, acc: 52.13675213675214, f1: 23.7470374221034, r: 0.25943817000111724
06/02/2019 12:34:10 *** epoch: 197 ***
06/02/2019 12:34:10 *** training ***
06/02/2019 12:34:10 step: 6473, epoch: 196, batch: 4, loss: 0.09040762484073639, acc: 100.0, f1: 100.0, r: 0.8132643160328783
06/02/2019 12:34:11 step: 6478, epoch: 196, batch: 9, loss: 0.06500368565320969, acc: 100.0, f1: 100.0, r: 0.6969381294523453
06/02/2019 12:34:12 step: 6483, epoch: 196, batch: 14, loss: 0.05969661846756935, acc: 100.0, f1: 100.0, r: 0.6700620977668803
06/02/2019 12:34:12 step: 6488, epoch: 196, batch: 19, loss: 0.056261464953422546, acc: 100.0, f1: 100.0, r: 0.7272454179761568
06/02/2019 12:34:13 step: 6493, epoch: 196, batch: 24, loss: 0.10018375515937805, acc: 100.0, f1: 100.0, r: 0.665985479430004
06/02/2019 12:34:13 step: 6498, epoch: 196, batch: 29, loss: 0.06505820155143738, acc: 100.0, f1: 100.0, r: 0.7625088840533075
06/02/2019 12:34:14 *** evaluating ***
06/02/2019 12:34:14 step: 197, epoch: 196, acc: 54.27350427350427, f1: 26.36510361562472, r: 0.26566033222829805
06/02/2019 12:34:14 *** epoch: 198 ***
06/02/2019 12:34:14 *** training ***
06/02/2019 12:34:14 step: 6506, epoch: 197, batch: 4, loss: 0.05687937140464783, acc: 100.0, f1: 100.0, r: 0.8065188853528416
06/02/2019 12:34:15 step: 6511, epoch: 197, batch: 9, loss: 0.04167740046977997, acc: 100.0, f1: 100.0, r: 0.7096778886467875
06/02/2019 12:34:16 step: 6516, epoch: 197, batch: 14, loss: 0.08475205302238464, acc: 100.0, f1: 100.0, r: 0.7579366642290435
06/02/2019 12:34:16 step: 6521, epoch: 197, batch: 19, loss: 0.09229442477226257, acc: 100.0, f1: 100.0, r: 0.7484792751240686
06/02/2019 12:34:17 step: 6526, epoch: 197, batch: 24, loss: 0.16269215941429138, acc: 100.0, f1: 100.0, r: 0.6827336521113827
06/02/2019 12:34:17 step: 6531, epoch: 197, batch: 29, loss: 0.053721584379673004, acc: 100.0, f1: 100.0, r: 0.6863364780773878
06/02/2019 12:34:18 *** evaluating ***
06/02/2019 12:34:18 step: 198, epoch: 197, acc: 51.70940170940172, f1: 25.16160058881578, r: 0.2606164534633735
06/02/2019 12:34:18 *** epoch: 199 ***
06/02/2019 12:34:18 *** training ***
06/02/2019 12:34:19 step: 6539, epoch: 198, batch: 4, loss: 0.07597893476486206, acc: 98.4375, f1: 99.0599876314162, r: 0.7132778875885797
06/02/2019 12:34:19 step: 6544, epoch: 198, batch: 9, loss: 0.060450535267591476, acc: 98.4375, f1: 98.90070921985816, r: 0.8072571752744399
06/02/2019 12:34:20 step: 6549, epoch: 198, batch: 14, loss: 0.07222726941108704, acc: 100.0, f1: 100.0, r: 0.6689563766836587
06/02/2019 12:34:20 step: 6554, epoch: 198, batch: 19, loss: 0.0513419546186924, acc: 100.0, f1: 100.0, r: 0.8006580274194651
06/02/2019 12:34:21 step: 6559, epoch: 198, batch: 24, loss: 0.25310102105140686, acc: 100.0, f1: 100.0, r: 0.6455339546748116
06/02/2019 12:34:22 step: 6564, epoch: 198, batch: 29, loss: 0.045715924352407455, acc: 100.0, f1: 100.0, r: 0.7721577542057739
06/02/2019 12:34:22 *** evaluating ***
06/02/2019 12:34:22 step: 199, epoch: 198, acc: 51.70940170940172, f1: 22.980204365366376, r: 0.2632040518644538
06/02/2019 12:34:22 *** epoch: 200 ***
06/02/2019 12:34:22 *** training ***
06/02/2019 12:34:23 step: 6572, epoch: 199, batch: 4, loss: 0.0741448849439621, acc: 100.0, f1: 100.0, r: 0.7318912962142228
06/02/2019 12:34:23 step: 6577, epoch: 199, batch: 9, loss: 0.0696747899055481, acc: 100.0, f1: 100.0, r: 0.641061924727989
06/02/2019 12:34:24 step: 6582, epoch: 199, batch: 14, loss: 0.12388919293880463, acc: 100.0, f1: 100.0, r: 0.8235529487995251
06/02/2019 12:34:25 step: 6587, epoch: 199, batch: 19, loss: 0.055255211889743805, acc: 100.0, f1: 100.0, r: 0.7311303294777936
06/02/2019 12:34:25 step: 6592, epoch: 199, batch: 24, loss: 0.05818437412381172, acc: 100.0, f1: 100.0, r: 0.7945821733332121
06/02/2019 12:34:26 step: 6597, epoch: 199, batch: 29, loss: 0.06065366417169571, acc: 100.0, f1: 100.0, r: 0.7840346901931688
06/02/2019 12:34:26 *** evaluating ***
06/02/2019 12:34:26 step: 200, epoch: 199, acc: 52.13675213675214, f1: 24.382696831326967, r: 0.2597405986445049
06/02/2019 12:34:26 *** epoch: 201 ***
06/02/2019 12:34:26 *** training ***
06/02/2019 12:34:27 step: 6605, epoch: 200, batch: 4, loss: 0.2332959920167923, acc: 100.0, f1: 100.0, r: 0.692326872686756
06/02/2019 12:34:27 step: 6610, epoch: 200, batch: 9, loss: 0.07557478547096252, acc: 98.4375, f1: 97.27891156462584, r: 0.6954679843699455
06/02/2019 12:34:28 step: 6615, epoch: 200, batch: 14, loss: 0.0838177502155304, acc: 100.0, f1: 100.0, r: 0.7662664424597903
06/02/2019 12:34:29 step: 6620, epoch: 200, batch: 19, loss: 0.0689924955368042, acc: 100.0, f1: 100.0, r: 0.6886252263859983
06/02/2019 12:34:29 step: 6625, epoch: 200, batch: 24, loss: 0.06639087200164795, acc: 100.0, f1: 100.0, r: 0.7492799660283974
06/02/2019 12:34:30 step: 6630, epoch: 200, batch: 29, loss: 0.07498039305210114, acc: 100.0, f1: 100.0, r: 0.6735110874277016
06/02/2019 12:34:30 *** evaluating ***
06/02/2019 12:34:30 step: 201, epoch: 200, acc: 53.41880341880342, f1: 25.50724183039341, r: 0.2657247380809852
06/02/2019 12:34:30 *** epoch: 202 ***
06/02/2019 12:34:30 *** training ***
06/02/2019 12:34:31 step: 6638, epoch: 201, batch: 4, loss: 0.05171176418662071, acc: 100.0, f1: 100.0, r: 0.8090384443914549
06/02/2019 12:34:31 step: 6643, epoch: 201, batch: 9, loss: 0.08056782186031342, acc: 100.0, f1: 100.0, r: 0.7085908169296162
06/02/2019 12:34:32 step: 6648, epoch: 201, batch: 14, loss: 0.07316355407238007, acc: 100.0, f1: 100.0, r: 0.6868589356391219
06/02/2019 12:34:33 step: 6653, epoch: 201, batch: 19, loss: 0.04019790515303612, acc: 100.0, f1: 100.0, r: 0.6575890502010906
06/02/2019 12:34:33 step: 6658, epoch: 201, batch: 24, loss: 0.04114965721964836, acc: 100.0, f1: 100.0, r: 0.6271202047594334
06/02/2019 12:34:34 step: 6663, epoch: 201, batch: 29, loss: 0.05273333191871643, acc: 100.0, f1: 100.0, r: 0.7108505555473074
06/02/2019 12:34:34 *** evaluating ***
06/02/2019 12:34:34 step: 202, epoch: 201, acc: 52.13675213675214, f1: 25.95838476459178, r: 0.25349365563177806
06/02/2019 12:34:34 *** epoch: 203 ***
06/02/2019 12:34:34 *** training ***
06/02/2019 12:34:35 step: 6671, epoch: 202, batch: 4, loss: 0.05182259529829025, acc: 100.0, f1: 100.0, r: 0.7931926402608667
06/02/2019 12:34:35 step: 6676, epoch: 202, batch: 9, loss: 0.0684603601694107, acc: 100.0, f1: 100.0, r: 0.6430191045358968
06/02/2019 12:34:36 step: 6681, epoch: 202, batch: 14, loss: 0.08127310872077942, acc: 100.0, f1: 100.0, r: 0.686306663724121
06/02/2019 12:34:37 step: 6686, epoch: 202, batch: 19, loss: 0.07369235903024673, acc: 100.0, f1: 100.0, r: 0.7013172836452151
06/02/2019 12:34:37 step: 6691, epoch: 202, batch: 24, loss: 0.05300223082304001, acc: 100.0, f1: 100.0, r: 0.7477376028526372
06/02/2019 12:34:38 step: 6696, epoch: 202, batch: 29, loss: 0.04403674229979515, acc: 100.0, f1: 100.0, r: 0.7588418706277588
06/02/2019 12:34:38 *** evaluating ***
06/02/2019 12:34:38 step: 203, epoch: 202, acc: 50.427350427350426, f1: 25.234519864283744, r: 0.25922122345790477
06/02/2019 12:34:38 *** epoch: 204 ***
06/02/2019 12:34:38 *** training ***
06/02/2019 12:34:39 step: 6704, epoch: 203, batch: 4, loss: 0.04920715093612671, acc: 100.0, f1: 100.0, r: 0.7608139275348405
06/02/2019 12:34:40 step: 6709, epoch: 203, batch: 9, loss: 0.04219285026192665, acc: 100.0, f1: 100.0, r: 0.8372042558301608
06/02/2019 12:34:40 step: 6714, epoch: 203, batch: 14, loss: 0.04354606941342354, acc: 100.0, f1: 100.0, r: 0.7266103820957147
06/02/2019 12:34:41 step: 6719, epoch: 203, batch: 19, loss: 0.04252920672297478, acc: 100.0, f1: 100.0, r: 0.804249495617393
06/02/2019 12:34:41 step: 6724, epoch: 203, batch: 24, loss: 0.061578162014484406, acc: 100.0, f1: 100.0, r: 0.790215860195552
06/02/2019 12:34:42 step: 6729, epoch: 203, batch: 29, loss: 0.06931094825267792, acc: 100.0, f1: 100.0, r: 0.7112294594741253
06/02/2019 12:34:42 *** evaluating ***
06/02/2019 12:34:42 step: 204, epoch: 203, acc: 52.13675213675214, f1: 25.53766798960312, r: 0.2605745323933063
06/02/2019 12:34:42 *** epoch: 205 ***
06/02/2019 12:34:42 *** training ***
06/02/2019 12:34:43 step: 6737, epoch: 204, batch: 4, loss: 0.06117965281009674, acc: 100.0, f1: 100.0, r: 0.5800862939825113
06/02/2019 12:34:44 step: 6742, epoch: 204, batch: 9, loss: 0.2101011574268341, acc: 100.0, f1: 100.0, r: 0.6298363333122252
06/02/2019 12:34:44 step: 6747, epoch: 204, batch: 14, loss: 0.2267991304397583, acc: 100.0, f1: 100.0, r: 0.698304759429683
06/02/2019 12:34:45 step: 6752, epoch: 204, batch: 19, loss: 0.04563765972852707, acc: 100.0, f1: 100.0, r: 0.8071593069329356
06/02/2019 12:34:45 step: 6757, epoch: 204, batch: 24, loss: 0.062233466655015945, acc: 100.0, f1: 100.0, r: 0.7448015043126273
06/02/2019 12:34:46 step: 6762, epoch: 204, batch: 29, loss: 0.0495578832924366, acc: 100.0, f1: 100.0, r: 0.6668891864440452
06/02/2019 12:34:46 *** evaluating ***
06/02/2019 12:34:47 step: 205, epoch: 204, acc: 50.85470085470085, f1: 25.563688013431673, r: 0.2405613935484811
06/02/2019 12:34:47 *** epoch: 206 ***
06/02/2019 12:34:47 *** training ***
06/02/2019 12:34:47 step: 6770, epoch: 205, batch: 4, loss: 0.058977238833904266, acc: 100.0, f1: 100.0, r: 0.8263715408071622
06/02/2019 12:34:48 step: 6775, epoch: 205, batch: 9, loss: 0.03762826323509216, acc: 100.0, f1: 100.0, r: 0.6618686634487959
06/02/2019 12:34:48 step: 6780, epoch: 205, batch: 14, loss: 0.0437106117606163, acc: 100.0, f1: 100.0, r: 0.7711850282572157
06/02/2019 12:34:49 step: 6785, epoch: 205, batch: 19, loss: 0.04954509064555168, acc: 100.0, f1: 100.0, r: 0.7449862816141585
06/02/2019 12:34:49 step: 6790, epoch: 205, batch: 24, loss: 0.05943868309259415, acc: 100.0, f1: 100.0, r: 0.8065414290643673
06/02/2019 12:34:50 step: 6795, epoch: 205, batch: 29, loss: 0.05464499816298485, acc: 100.0, f1: 100.0, r: 0.8248403878732005
06/02/2019 12:34:50 *** evaluating ***
06/02/2019 12:34:51 step: 206, epoch: 205, acc: 53.84615384615385, f1: 26.99127123626609, r: 0.2637843125268709
06/02/2019 12:34:51 *** epoch: 207 ***
06/02/2019 12:34:51 *** training ***
06/02/2019 12:34:51 step: 6803, epoch: 206, batch: 4, loss: 0.17220865190029144, acc: 100.0, f1: 100.0, r: 0.7937396189322079
06/02/2019 12:34:52 step: 6808, epoch: 206, batch: 9, loss: 0.0327175036072731, acc: 100.0, f1: 100.0, r: 0.7002938951210109
06/02/2019 12:34:52 step: 6813, epoch: 206, batch: 14, loss: 0.05404455214738846, acc: 100.0, f1: 100.0, r: 0.7756771698704703
06/02/2019 12:34:53 step: 6818, epoch: 206, batch: 19, loss: 0.05862404778599739, acc: 100.0, f1: 100.0, r: 0.815290015483501
06/02/2019 12:34:54 step: 6823, epoch: 206, batch: 24, loss: 0.038168661296367645, acc: 100.0, f1: 100.0, r: 0.8158874472312261
06/02/2019 12:34:54 step: 6828, epoch: 206, batch: 29, loss: 0.0542435348033905, acc: 100.0, f1: 100.0, r: 0.8138007616553675
06/02/2019 12:34:54 *** evaluating ***
06/02/2019 12:34:55 step: 207, epoch: 206, acc: 52.991452991452995, f1: 25.33958033737963, r: 0.25997154599106587
06/02/2019 12:34:55 *** epoch: 208 ***
06/02/2019 12:34:55 *** training ***
06/02/2019 12:34:55 step: 6836, epoch: 207, batch: 4, loss: 0.07417073845863342, acc: 100.0, f1: 100.0, r: 0.7362822149145429
06/02/2019 12:34:56 step: 6841, epoch: 207, batch: 9, loss: 0.023837661370635033, acc: 100.0, f1: 100.0, r: 0.7391024795438798
06/02/2019 12:34:56 step: 6846, epoch: 207, batch: 14, loss: 0.0866285040974617, acc: 100.0, f1: 100.0, r: 0.8702278625386713
06/02/2019 12:34:57 step: 6851, epoch: 207, batch: 19, loss: 0.22240500152111053, acc: 100.0, f1: 100.0, r: 0.7815212539722675
06/02/2019 12:34:58 step: 6856, epoch: 207, batch: 24, loss: 0.03357801213860512, acc: 100.0, f1: 100.0, r: 0.6850800658999238
06/02/2019 12:34:58 step: 6861, epoch: 207, batch: 29, loss: 0.08751463890075684, acc: 100.0, f1: 100.0, r: 0.6771785788634549
06/02/2019 12:34:59 *** evaluating ***
06/02/2019 12:34:59 step: 208, epoch: 207, acc: 52.13675213675214, f1: 25.790488327871504, r: 0.25652830909160546
06/02/2019 12:34:59 *** epoch: 209 ***
06/02/2019 12:34:59 *** training ***
06/02/2019 12:34:59 step: 6869, epoch: 208, batch: 4, loss: 0.08523452281951904, acc: 100.0, f1: 100.0, r: 0.7094799328829173
06/02/2019 12:35:00 step: 6874, epoch: 208, batch: 9, loss: 0.05368683114647865, acc: 100.0, f1: 100.0, r: 0.8389166639770866
06/02/2019 12:35:01 step: 6879, epoch: 208, batch: 14, loss: 0.0406583771109581, acc: 100.0, f1: 100.0, r: 0.6746321011788546
06/02/2019 12:35:01 step: 6884, epoch: 208, batch: 19, loss: 0.08019565045833588, acc: 100.0, f1: 100.0, r: 0.6948720736090225
06/02/2019 12:35:02 step: 6889, epoch: 208, batch: 24, loss: 0.03101515769958496, acc: 100.0, f1: 100.0, r: 0.6747947657550675
06/02/2019 12:35:02 step: 6894, epoch: 208, batch: 29, loss: 0.04458330199122429, acc: 100.0, f1: 100.0, r: 0.7920858577651971
06/02/2019 12:35:02 *** evaluating ***
06/02/2019 12:35:03 step: 209, epoch: 208, acc: 51.28205128205128, f1: 26.080974160655114, r: 0.256428802727012
06/02/2019 12:35:03 *** epoch: 210 ***
06/02/2019 12:35:03 *** training ***
06/02/2019 12:35:03 step: 6902, epoch: 209, batch: 4, loss: 0.06476502120494843, acc: 100.0, f1: 100.0, r: 0.777461063694557
06/02/2019 12:35:04 step: 6907, epoch: 209, batch: 9, loss: 0.10379692912101746, acc: 100.0, f1: 100.0, r: 0.6834403679994452
06/02/2019 12:35:04 step: 6912, epoch: 209, batch: 14, loss: 0.09800577163696289, acc: 100.0, f1: 100.0, r: 0.8389437498819033
06/02/2019 12:35:05 step: 6917, epoch: 209, batch: 19, loss: 0.07509763538837433, acc: 98.4375, f1: 97.38095238095238, r: 0.8069928920843676
06/02/2019 12:35:06 step: 6922, epoch: 209, batch: 24, loss: 0.08163908123970032, acc: 100.0, f1: 100.0, r: 0.6905435304291311
06/02/2019 12:35:06 step: 6927, epoch: 209, batch: 29, loss: 0.2270716428756714, acc: 98.4375, f1: 99.41352973267867, r: 0.8224443666931065
06/02/2019 12:35:07 *** evaluating ***
06/02/2019 12:35:07 step: 210, epoch: 209, acc: 52.56410256410257, f1: 25.833321607969495, r: 0.2584494160027724
06/02/2019 12:35:07 *** epoch: 211 ***
06/02/2019 12:35:07 *** training ***
06/02/2019 12:35:07 step: 6935, epoch: 210, batch: 4, loss: 0.08540742844343185, acc: 100.0, f1: 100.0, r: 0.8195471078261024
06/02/2019 12:35:08 step: 6940, epoch: 210, batch: 9, loss: 0.09939124435186386, acc: 100.0, f1: 100.0, r: 0.6469246951644249
06/02/2019 12:35:08 step: 6945, epoch: 210, batch: 14, loss: 0.07307233661413193, acc: 98.4375, f1: 94.55782312925169, r: 0.7272949139284006
06/02/2019 12:35:09 step: 6950, epoch: 210, batch: 19, loss: 0.060245051980018616, acc: 100.0, f1: 100.0, r: 0.6667223343763097
06/02/2019 12:35:10 step: 6955, epoch: 210, batch: 24, loss: 0.2847658395767212, acc: 100.0, f1: 100.0, r: 0.7686484164289071
06/02/2019 12:35:10 step: 6960, epoch: 210, batch: 29, loss: 0.04707562178373337, acc: 100.0, f1: 100.0, r: 0.6928349335007028
06/02/2019 12:35:10 *** evaluating ***
06/02/2019 12:35:11 step: 211, epoch: 210, acc: 55.55555555555556, f1: 27.46714378659727, r: 0.2644104795934787
06/02/2019 12:35:11 *** epoch: 212 ***
06/02/2019 12:35:11 *** training ***
06/02/2019 12:35:11 step: 6968, epoch: 211, batch: 4, loss: 0.0746031254529953, acc: 100.0, f1: 100.0, r: 0.7663726079248027
06/02/2019 12:35:12 step: 6973, epoch: 211, batch: 9, loss: 0.04191170632839203, acc: 100.0, f1: 100.0, r: 0.6301852521626804
06/02/2019 12:35:12 step: 6978, epoch: 211, batch: 14, loss: 0.10255315154790878, acc: 100.0, f1: 100.0, r: 0.7717243775976971
06/02/2019 12:35:13 step: 6983, epoch: 211, batch: 19, loss: 0.058837439864873886, acc: 100.0, f1: 100.0, r: 0.7411785130207832
06/02/2019 12:35:13 step: 6988, epoch: 211, batch: 24, loss: 0.08404660224914551, acc: 100.0, f1: 100.0, r: 0.7702993538141054
06/02/2019 12:35:14 step: 6993, epoch: 211, batch: 29, loss: 0.04634898528456688, acc: 100.0, f1: 100.0, r: 0.6723786288480175
06/02/2019 12:35:14 *** evaluating ***
06/02/2019 12:35:15 step: 212, epoch: 211, acc: 54.27350427350427, f1: 25.56825065162325, r: 0.27092885024864893
06/02/2019 12:35:15 *** epoch: 213 ***
06/02/2019 12:35:15 *** training ***
06/02/2019 12:35:15 step: 7001, epoch: 212, batch: 4, loss: 0.057235926389694214, acc: 100.0, f1: 100.0, r: 0.6724368667909495
06/02/2019 12:35:16 step: 7006, epoch: 212, batch: 9, loss: 0.10027776658535004, acc: 100.0, f1: 100.0, r: 0.7265419605375317
06/02/2019 12:35:16 step: 7011, epoch: 212, batch: 14, loss: 0.06820183247327805, acc: 100.0, f1: 100.0, r: 0.8325154474336885
06/02/2019 12:35:17 step: 7016, epoch: 212, batch: 19, loss: 0.04843200743198395, acc: 100.0, f1: 100.0, r: 0.714439093749691
06/02/2019 12:35:18 step: 7021, epoch: 212, batch: 24, loss: 0.05772034451365471, acc: 100.0, f1: 100.0, r: 0.8450918832483805
06/02/2019 12:35:18 step: 7026, epoch: 212, batch: 29, loss: 0.09545157849788666, acc: 100.0, f1: 100.0, r: 0.6644041831936778
06/02/2019 12:35:18 *** evaluating ***
06/02/2019 12:35:19 step: 213, epoch: 212, acc: 51.28205128205128, f1: 24.35074849636037, r: 0.26734839590925474
06/02/2019 12:35:19 *** epoch: 214 ***
06/02/2019 12:35:19 *** training ***
06/02/2019 12:35:19 step: 7034, epoch: 213, batch: 4, loss: 0.10702298581600189, acc: 100.0, f1: 100.0, r: 0.80355252339401
06/02/2019 12:35:20 step: 7039, epoch: 213, batch: 9, loss: 0.06513388454914093, acc: 100.0, f1: 100.0, r: 0.7817665143596974
06/02/2019 12:35:20 step: 7044, epoch: 213, batch: 14, loss: 0.059792157262563705, acc: 100.0, f1: 100.0, r: 0.7598804667981129
06/02/2019 12:35:21 step: 7049, epoch: 213, batch: 19, loss: 0.076788030564785, acc: 100.0, f1: 100.0, r: 0.7447812265809624
06/02/2019 12:35:22 step: 7054, epoch: 213, batch: 24, loss: 0.048304930329322815, acc: 100.0, f1: 100.0, r: 0.722422870385031
06/02/2019 12:35:22 step: 7059, epoch: 213, batch: 29, loss: 0.07046808302402496, acc: 100.0, f1: 100.0, r: 0.7274936583073474
06/02/2019 12:35:23 *** evaluating ***
06/02/2019 12:35:23 step: 214, epoch: 213, acc: 54.700854700854705, f1: 27.035948285948287, r: 0.2871773687006202
06/02/2019 12:35:23 *** epoch: 215 ***
06/02/2019 12:35:23 *** training ***
06/02/2019 12:35:24 step: 7067, epoch: 214, batch: 4, loss: 0.08135734498500824, acc: 100.0, f1: 100.0, r: 0.772133988049842
06/02/2019 12:35:24 step: 7072, epoch: 214, batch: 9, loss: 0.06276639550924301, acc: 100.0, f1: 100.0, r: 0.7086593367407472
06/02/2019 12:35:25 step: 7077, epoch: 214, batch: 14, loss: 0.07804376631975174, acc: 100.0, f1: 100.0, r: 0.6039941935283673
06/02/2019 12:35:25 step: 7082, epoch: 214, batch: 19, loss: 0.06264251470565796, acc: 100.0, f1: 100.0, r: 0.7023106002477025
06/02/2019 12:35:26 step: 7087, epoch: 214, batch: 24, loss: 0.04929981753230095, acc: 100.0, f1: 100.0, r: 0.6786353416821177
06/02/2019 12:35:27 step: 7092, epoch: 214, batch: 29, loss: 0.05686711519956589, acc: 100.0, f1: 100.0, r: 0.7291547559489261
06/02/2019 12:35:27 *** evaluating ***
06/02/2019 12:35:27 step: 215, epoch: 214, acc: 53.41880341880342, f1: 27.706193101835307, r: 0.26331080959311887
06/02/2019 12:35:27 *** epoch: 216 ***
06/02/2019 12:35:27 *** training ***
06/02/2019 12:35:28 step: 7100, epoch: 215, batch: 4, loss: 0.038313135504722595, acc: 100.0, f1: 100.0, r: 0.7062454383724259
06/02/2019 12:35:28 step: 7105, epoch: 215, batch: 9, loss: 0.04056720435619354, acc: 100.0, f1: 100.0, r: 0.7599887074083945
06/02/2019 12:35:29 step: 7110, epoch: 215, batch: 14, loss: 0.07923731207847595, acc: 100.0, f1: 100.0, r: 0.8218608581943825
06/02/2019 12:35:30 step: 7115, epoch: 215, batch: 19, loss: 0.0566360205411911, acc: 100.0, f1: 100.0, r: 0.7541658372655599
06/02/2019 12:35:30 step: 7120, epoch: 215, batch: 24, loss: 0.057402364909648895, acc: 100.0, f1: 100.0, r: 0.6932649351524465
06/02/2019 12:35:31 step: 7125, epoch: 215, batch: 29, loss: 0.04228369891643524, acc: 100.0, f1: 100.0, r: 0.7503492815281807
06/02/2019 12:35:31 *** evaluating ***
06/02/2019 12:35:31 step: 216, epoch: 215, acc: 54.700854700854705, f1: 27.374672259235062, r: 0.26867556742488213
06/02/2019 12:35:31 *** epoch: 217 ***
06/02/2019 12:35:31 *** training ***
06/02/2019 12:35:32 step: 7133, epoch: 216, batch: 4, loss: 0.11041309684515, acc: 96.875, f1: 94.99915739804517, r: 0.76453894748812
06/02/2019 12:35:32 step: 7138, epoch: 216, batch: 9, loss: 0.0659523606300354, acc: 100.0, f1: 100.0, r: 0.7556881170898163
06/02/2019 12:35:33 step: 7143, epoch: 216, batch: 14, loss: 0.17949432134628296, acc: 100.0, f1: 100.0, r: 0.6985078521182134
06/02/2019 12:35:34 step: 7148, epoch: 216, batch: 19, loss: 0.039981402456760406, acc: 100.0, f1: 100.0, r: 0.6722182993389294
06/02/2019 12:35:34 step: 7153, epoch: 216, batch: 24, loss: 0.04983609914779663, acc: 100.0, f1: 100.0, r: 0.7040800455033109
06/02/2019 12:35:35 step: 7158, epoch: 216, batch: 29, loss: 0.057429663836956024, acc: 98.4375, f1: 95.57823129251702, r: 0.7793750001635293
06/02/2019 12:35:35 *** evaluating ***
06/02/2019 12:35:35 step: 217, epoch: 216, acc: 54.27350427350427, f1: 28.847727376814213, r: 0.27918006288460295
06/02/2019 12:35:35 *** epoch: 218 ***
06/02/2019 12:35:35 *** training ***
06/02/2019 12:35:36 step: 7166, epoch: 217, batch: 4, loss: 0.08208908885717392, acc: 100.0, f1: 100.0, r: 0.8227811375181454
06/02/2019 12:35:36 step: 7171, epoch: 217, batch: 9, loss: 0.045567020773887634, acc: 100.0, f1: 100.0, r: 0.7308298819781933
06/02/2019 12:35:37 step: 7176, epoch: 217, batch: 14, loss: 0.1663403958082199, acc: 100.0, f1: 100.0, r: 0.6568297533874821
06/02/2019 12:35:38 step: 7181, epoch: 217, batch: 19, loss: 0.06115826591849327, acc: 100.0, f1: 100.0, r: 0.7478358167475747
06/02/2019 12:35:38 step: 7186, epoch: 217, batch: 24, loss: 0.2079358696937561, acc: 100.0, f1: 100.0, r: 0.6454963903647261
06/02/2019 12:35:39 step: 7191, epoch: 217, batch: 29, loss: 0.04945949465036392, acc: 100.0, f1: 100.0, r: 0.7272836243806858
06/02/2019 12:35:39 *** evaluating ***
06/02/2019 12:35:39 step: 218, epoch: 217, acc: 52.56410256410257, f1: 26.944353315836644, r: 0.2693190132302854
06/02/2019 12:35:39 *** epoch: 219 ***
06/02/2019 12:35:39 *** training ***
06/02/2019 12:35:40 step: 7199, epoch: 218, batch: 4, loss: 0.06269937753677368, acc: 100.0, f1: 100.0, r: 0.8121385656834645
06/02/2019 12:35:41 step: 7204, epoch: 218, batch: 9, loss: 0.03654605522751808, acc: 100.0, f1: 100.0, r: 0.7633394569800935
06/02/2019 12:35:41 step: 7209, epoch: 218, batch: 14, loss: 0.08842921257019043, acc: 100.0, f1: 100.0, r: 0.7966867366894449
06/02/2019 12:35:42 step: 7214, epoch: 218, batch: 19, loss: 0.06937916576862335, acc: 98.4375, f1: 97.16216216216216, r: 0.8072283755716622
06/02/2019 12:35:42 step: 7219, epoch: 218, batch: 24, loss: 0.04037085548043251, acc: 100.0, f1: 100.0, r: 0.7461063498653893
06/02/2019 12:35:43 step: 7224, epoch: 218, batch: 29, loss: 0.04506552219390869, acc: 100.0, f1: 100.0, r: 0.6935544331538022
06/02/2019 12:35:43 *** evaluating ***
06/02/2019 12:35:43 step: 219, epoch: 218, acc: 52.13675213675214, f1: 26.627811742248365, r: 0.26982133059801494
06/02/2019 12:35:43 *** epoch: 220 ***
06/02/2019 12:35:43 *** training ***
06/02/2019 12:35:44 step: 7232, epoch: 219, batch: 4, loss: 0.056342922151088715, acc: 100.0, f1: 100.0, r: 0.7169771011858598
06/02/2019 12:35:45 step: 7237, epoch: 219, batch: 9, loss: 0.04835965111851692, acc: 100.0, f1: 100.0, r: 0.7581784719152234
06/02/2019 12:35:45 step: 7242, epoch: 219, batch: 14, loss: 0.05614914000034332, acc: 100.0, f1: 100.0, r: 0.7168165320174671
06/02/2019 12:35:46 step: 7247, epoch: 219, batch: 19, loss: 0.07421115040779114, acc: 100.0, f1: 100.0, r: 0.8372341339784642
06/02/2019 12:35:46 step: 7252, epoch: 219, batch: 24, loss: 0.04001229628920555, acc: 100.0, f1: 100.0, r: 0.7854641720541247
06/02/2019 12:35:47 step: 7257, epoch: 219, batch: 29, loss: 0.055621471256017685, acc: 100.0, f1: 100.0, r: 0.7526138191687392
06/02/2019 12:35:47 *** evaluating ***
06/02/2019 12:35:47 step: 220, epoch: 219, acc: 53.84615384615385, f1: 29.0994114403822, r: 0.26762483480435617
06/02/2019 12:35:47 *** epoch: 221 ***
06/02/2019 12:35:47 *** training ***
06/02/2019 12:35:48 step: 7265, epoch: 220, batch: 4, loss: 0.046394120901823044, acc: 98.4375, f1: 98.26839826839827, r: 0.7897704395165789
06/02/2019 12:35:49 step: 7270, epoch: 220, batch: 9, loss: 0.06514821946620941, acc: 100.0, f1: 100.0, r: 0.7338584882767103
06/02/2019 12:35:49 step: 7275, epoch: 220, batch: 14, loss: 0.05532044172286987, acc: 100.0, f1: 100.0, r: 0.8117869072512954
06/02/2019 12:35:50 step: 7280, epoch: 220, batch: 19, loss: 0.08682461082935333, acc: 100.0, f1: 100.0, r: 0.7500182708407882
06/02/2019 12:35:50 step: 7285, epoch: 220, batch: 24, loss: 0.03424074500799179, acc: 100.0, f1: 100.0, r: 0.7504853761731016
06/02/2019 12:35:51 step: 7290, epoch: 220, batch: 29, loss: 0.06439047306776047, acc: 100.0, f1: 100.0, r: 0.7085507167936027
06/02/2019 12:35:51 *** evaluating ***
06/02/2019 12:35:52 step: 221, epoch: 220, acc: 52.56410256410257, f1: 27.77511560120256, r: 0.2729586908429764
06/02/2019 12:35:52 *** epoch: 222 ***
06/02/2019 12:35:52 *** training ***
06/02/2019 12:35:52 step: 7298, epoch: 221, batch: 4, loss: 0.1849990338087082, acc: 100.0, f1: 100.0, r: 0.6504729304034996
06/02/2019 12:35:53 step: 7303, epoch: 221, batch: 9, loss: 0.052682071924209595, acc: 100.0, f1: 100.0, r: 0.710881752879453
06/02/2019 12:35:53 step: 7308, epoch: 221, batch: 14, loss: 0.07877110689878464, acc: 100.0, f1: 100.0, r: 0.801684715984093
06/02/2019 12:35:54 step: 7313, epoch: 221, batch: 19, loss: 0.03885480388998985, acc: 100.0, f1: 100.0, r: 0.7788898511694624
06/02/2019 12:35:55 step: 7318, epoch: 221, batch: 24, loss: 0.047573789954185486, acc: 100.0, f1: 100.0, r: 0.802414860666085
06/02/2019 12:35:55 step: 7323, epoch: 221, batch: 29, loss: 0.09679433703422546, acc: 98.4375, f1: 98.26839826839827, r: 0.6565438032222058
06/02/2019 12:35:55 *** evaluating ***
06/02/2019 12:35:56 step: 222, epoch: 221, acc: 52.56410256410257, f1: 27.553633399287303, r: 0.2669284439723032
06/02/2019 12:35:56 *** epoch: 223 ***
06/02/2019 12:35:56 *** training ***
06/02/2019 12:35:56 step: 7331, epoch: 222, batch: 4, loss: 0.061329472810029984, acc: 100.0, f1: 100.0, r: 0.7488499016633099
06/02/2019 12:35:57 step: 7336, epoch: 222, batch: 9, loss: 0.06611926853656769, acc: 100.0, f1: 100.0, r: 0.8025539652889833
06/02/2019 12:35:57 step: 7341, epoch: 222, batch: 14, loss: 0.05275799334049225, acc: 100.0, f1: 100.0, r: 0.6584766473173477
06/02/2019 12:35:58 step: 7346, epoch: 222, batch: 19, loss: 0.041359420865774155, acc: 100.0, f1: 100.0, r: 0.6942216299122821
06/02/2019 12:35:59 step: 7351, epoch: 222, batch: 24, loss: 0.1696491241455078, acc: 100.0, f1: 100.0, r: 0.7992169220665184
06/02/2019 12:35:59 step: 7356, epoch: 222, batch: 29, loss: 0.04708462581038475, acc: 100.0, f1: 100.0, r: 0.6700463911614768
06/02/2019 12:36:00 *** evaluating ***
06/02/2019 12:36:00 step: 223, epoch: 222, acc: 52.991452991452995, f1: 27.757094520498356, r: 0.26687316160906316
06/02/2019 12:36:00 *** epoch: 224 ***
06/02/2019 12:36:00 *** training ***
06/02/2019 12:36:00 step: 7364, epoch: 223, batch: 4, loss: 0.04223392531275749, acc: 100.0, f1: 100.0, r: 0.6625349153080509
06/02/2019 12:36:01 step: 7369, epoch: 223, batch: 9, loss: 0.06689892709255219, acc: 100.0, f1: 100.0, r: 0.6912495970144504
06/02/2019 12:36:02 step: 7374, epoch: 223, batch: 14, loss: 0.06960831582546234, acc: 100.0, f1: 100.0, r: 0.8307188574250488
06/02/2019 12:36:02 step: 7379, epoch: 223, batch: 19, loss: 0.179338276386261, acc: 98.4375, f1: 94.55782312925169, r: 0.6674235142059839
06/02/2019 12:36:03 step: 7384, epoch: 223, batch: 24, loss: 0.05710370093584061, acc: 100.0, f1: 100.0, r: 0.6140013732652286
06/02/2019 12:36:03 step: 7389, epoch: 223, batch: 29, loss: 0.0511566624045372, acc: 98.4375, f1: 99.08733679807327, r: 0.705225454467631
06/02/2019 12:36:04 *** evaluating ***
06/02/2019 12:36:04 step: 224, epoch: 223, acc: 54.27350427350427, f1: 29.134921076958932, r: 0.2762699582182131
06/02/2019 12:36:04 *** epoch: 225 ***
06/02/2019 12:36:04 *** training ***
06/02/2019 12:36:05 step: 7397, epoch: 224, batch: 4, loss: 0.06999754905700684, acc: 100.0, f1: 100.0, r: 0.7985039046845878
06/02/2019 12:36:05 step: 7402, epoch: 224, batch: 9, loss: 0.06950691342353821, acc: 100.0, f1: 100.0, r: 0.6943374583900563
06/02/2019 12:36:06 step: 7407, epoch: 224, batch: 14, loss: 0.20540320873260498, acc: 100.0, f1: 100.0, r: 0.7508682307718678
06/02/2019 12:36:06 step: 7412, epoch: 224, batch: 19, loss: 0.03976481035351753, acc: 100.0, f1: 100.0, r: 0.581708018161867
06/02/2019 12:36:07 step: 7417, epoch: 224, batch: 24, loss: 0.04728595167398453, acc: 100.0, f1: 100.0, r: 0.7173350703260638
06/02/2019 12:36:07 step: 7422, epoch: 224, batch: 29, loss: 0.2098139524459839, acc: 100.0, f1: 100.0, r: 0.759181784520005
06/02/2019 12:36:08 *** evaluating ***
06/02/2019 12:36:08 step: 225, epoch: 224, acc: 54.27350427350427, f1: 28.288681294195996, r: 0.2782245838887907
06/02/2019 12:36:08 *** epoch: 226 ***
06/02/2019 12:36:08 *** training ***
06/02/2019 12:36:08 step: 7430, epoch: 225, batch: 4, loss: 0.044546473771333694, acc: 100.0, f1: 100.0, r: 0.698828801532687
06/02/2019 12:36:09 step: 7435, epoch: 225, batch: 9, loss: 0.04286566749215126, acc: 100.0, f1: 100.0, r: 0.8297190308715188
06/02/2019 12:36:10 step: 7440, epoch: 225, batch: 14, loss: 0.04959928244352341, acc: 100.0, f1: 100.0, r: 0.7920296598235222
06/02/2019 12:36:10 step: 7445, epoch: 225, batch: 19, loss: 0.053644560277462006, acc: 100.0, f1: 100.0, r: 0.708273733752698
06/02/2019 12:36:11 step: 7450, epoch: 225, batch: 24, loss: 0.06663261353969574, acc: 98.4375, f1: 98.06076276664513, r: 0.7094645589765054
06/02/2019 12:36:11 step: 7455, epoch: 225, batch: 29, loss: 0.04602004215121269, acc: 100.0, f1: 100.0, r: 0.7052878200260115
06/02/2019 12:36:12 *** evaluating ***
06/02/2019 12:36:12 step: 226, epoch: 225, acc: 52.56410256410257, f1: 27.295090589413263, r: 0.26405901882391875
06/02/2019 12:36:12 *** epoch: 227 ***
06/02/2019 12:36:12 *** training ***
06/02/2019 12:36:12 step: 7463, epoch: 226, batch: 4, loss: 0.038899097591638565, acc: 100.0, f1: 100.0, r: 0.7866051613443867
06/02/2019 12:36:13 step: 7468, epoch: 226, batch: 9, loss: 0.08242754638195038, acc: 100.0, f1: 100.0, r: 0.6923506184202163
06/02/2019 12:36:14 step: 7473, epoch: 226, batch: 14, loss: 0.040077537298202515, acc: 100.0, f1: 100.0, r: 0.7054822512504741
06/02/2019 12:36:14 step: 7478, epoch: 226, batch: 19, loss: 0.033876918256282806, acc: 100.0, f1: 100.0, r: 0.8427298685305143
06/02/2019 12:36:15 step: 7483, epoch: 226, batch: 24, loss: 0.04206106811761856, acc: 100.0, f1: 100.0, r: 0.7541836898648588
06/02/2019 12:36:15 step: 7488, epoch: 226, batch: 29, loss: 0.0411299392580986, acc: 100.0, f1: 100.0, r: 0.6270709919160734
06/02/2019 12:36:16 *** evaluating ***
06/02/2019 12:36:16 step: 227, epoch: 226, acc: 52.991452991452995, f1: 27.155047001058087, r: 0.26913959079351124
06/02/2019 12:36:16 *** epoch: 228 ***
06/02/2019 12:36:16 *** training ***
06/02/2019 12:36:17 step: 7496, epoch: 227, batch: 4, loss: 0.031356796622276306, acc: 100.0, f1: 100.0, r: 0.8433556118456896
06/02/2019 12:36:17 step: 7501, epoch: 227, batch: 9, loss: 0.04724322259426117, acc: 100.0, f1: 100.0, r: 0.706727860519176
06/02/2019 12:36:18 step: 7506, epoch: 227, batch: 14, loss: 0.028942879289388657, acc: 100.0, f1: 100.0, r: 0.7431883795728169
06/02/2019 12:36:19 step: 7511, epoch: 227, batch: 19, loss: 0.03730745613574982, acc: 100.0, f1: 100.0, r: 0.7268284407171245
06/02/2019 12:36:19 step: 7516, epoch: 227, batch: 24, loss: 0.048048876225948334, acc: 100.0, f1: 100.0, r: 0.8349809943512793
06/02/2019 12:36:20 step: 7521, epoch: 227, batch: 29, loss: 0.04497460275888443, acc: 100.0, f1: 100.0, r: 0.7122682877404501
06/02/2019 12:36:20 *** evaluating ***
06/02/2019 12:36:20 step: 228, epoch: 227, acc: 53.84615384615385, f1: 27.034919039519316, r: 0.2705550487768125
06/02/2019 12:36:20 *** epoch: 229 ***
06/02/2019 12:36:20 *** training ***
06/02/2019 12:36:21 step: 7529, epoch: 228, batch: 4, loss: 0.05054178833961487, acc: 100.0, f1: 100.0, r: 0.7121229813417149
06/02/2019 12:36:21 step: 7534, epoch: 228, batch: 9, loss: 0.0781160369515419, acc: 100.0, f1: 100.0, r: 0.714451516144498
06/02/2019 12:36:22 step: 7539, epoch: 228, batch: 14, loss: 0.06129851192235947, acc: 100.0, f1: 100.0, r: 0.6768871845823788
06/02/2019 12:36:22 step: 7544, epoch: 228, batch: 19, loss: 0.1791297346353531, acc: 100.0, f1: 100.0, r: 0.7921440059881436
06/02/2019 12:36:23 step: 7549, epoch: 228, batch: 24, loss: 0.07532563805580139, acc: 100.0, f1: 100.0, r: 0.7236339302873946
06/02/2019 12:36:24 step: 7554, epoch: 228, batch: 29, loss: 0.05678598955273628, acc: 100.0, f1: 100.0, r: 0.6970474179718626
06/02/2019 12:36:24 *** evaluating ***
06/02/2019 12:36:24 step: 229, epoch: 228, acc: 51.28205128205128, f1: 27.439458689458686, r: 0.2686147185958578
06/02/2019 12:36:24 *** epoch: 230 ***
06/02/2019 12:36:24 *** training ***
06/02/2019 12:36:25 step: 7562, epoch: 229, batch: 4, loss: 0.0686381459236145, acc: 98.4375, f1: 98.8795518207283, r: 0.7162720613978466
06/02/2019 12:36:25 step: 7567, epoch: 229, batch: 9, loss: 0.05198124796152115, acc: 100.0, f1: 100.0, r: 0.6599129953575306
06/02/2019 12:36:26 step: 7572, epoch: 229, batch: 14, loss: 0.06064234673976898, acc: 100.0, f1: 100.0, r: 0.8022386951454249
06/02/2019 12:36:26 step: 7577, epoch: 229, batch: 19, loss: 0.21683743596076965, acc: 100.0, f1: 100.0, r: 0.728512167017905
06/02/2019 12:36:27 step: 7582, epoch: 229, batch: 24, loss: 0.08789415657520294, acc: 100.0, f1: 100.0, r: 0.8036462574990364
06/02/2019 12:36:27 step: 7587, epoch: 229, batch: 29, loss: 0.2823887765407562, acc: 100.0, f1: 100.0, r: 0.7324592406728575
06/02/2019 12:36:28 *** evaluating ***
06/02/2019 12:36:28 step: 230, epoch: 229, acc: 52.56410256410257, f1: 27.117957984356337, r: 0.26924866284686977
06/02/2019 12:36:28 *** epoch: 231 ***
06/02/2019 12:36:28 *** training ***
06/02/2019 12:36:29 step: 7595, epoch: 230, batch: 4, loss: 0.05744317173957825, acc: 100.0, f1: 100.0, r: 0.7970548204058108
06/02/2019 12:36:29 step: 7600, epoch: 230, batch: 9, loss: 0.03834015130996704, acc: 100.0, f1: 100.0, r: 0.694534714032489
06/02/2019 12:36:30 step: 7605, epoch: 230, batch: 14, loss: 0.06757005304098129, acc: 98.4375, f1: 94.74548440065682, r: 0.6792301598618352
06/02/2019 12:36:30 step: 7610, epoch: 230, batch: 19, loss: 0.18764445185661316, acc: 100.0, f1: 100.0, r: 0.7351298268036656
06/02/2019 12:36:31 step: 7615, epoch: 230, batch: 24, loss: 0.07497818768024445, acc: 100.0, f1: 100.0, r: 0.7135153257219323
06/02/2019 12:36:31 step: 7620, epoch: 230, batch: 29, loss: 0.22237983345985413, acc: 100.0, f1: 100.0, r: 0.6383766193210313
06/02/2019 12:36:32 *** evaluating ***
06/02/2019 12:36:32 step: 231, epoch: 230, acc: 51.28205128205128, f1: 26.491088172599042, r: 0.263112498016688
06/02/2019 12:36:32 *** epoch: 232 ***
06/02/2019 12:36:32 *** training ***
06/02/2019 12:36:32 step: 7628, epoch: 231, batch: 4, loss: 0.07519901543855667, acc: 100.0, f1: 100.0, r: 0.6756963700622253
06/02/2019 12:36:33 step: 7633, epoch: 231, batch: 9, loss: 0.06864947080612183, acc: 100.0, f1: 100.0, r: 0.6923415555674348
06/02/2019 12:36:34 step: 7638, epoch: 231, batch: 14, loss: 0.05777216702699661, acc: 100.0, f1: 100.0, r: 0.755530331138174
06/02/2019 12:36:34 step: 7643, epoch: 231, batch: 19, loss: 0.060496825724840164, acc: 100.0, f1: 100.0, r: 0.6568480882814947
06/02/2019 12:36:35 step: 7648, epoch: 231, batch: 24, loss: 0.04924160614609718, acc: 100.0, f1: 100.0, r: 0.75127777412191
06/02/2019 12:36:35 step: 7653, epoch: 231, batch: 29, loss: 0.054500170052051544, acc: 100.0, f1: 100.0, r: 0.7359886328279174
06/02/2019 12:36:36 *** evaluating ***
06/02/2019 12:36:36 step: 232, epoch: 231, acc: 52.56410256410257, f1: 26.692042168284164, r: 0.26764668210430465
06/02/2019 12:36:36 *** epoch: 233 ***
06/02/2019 12:36:36 *** training ***
06/02/2019 12:36:37 step: 7661, epoch: 232, batch: 4, loss: 0.057164065539836884, acc: 100.0, f1: 100.0, r: 0.6563632571350467
06/02/2019 12:36:37 step: 7666, epoch: 232, batch: 9, loss: 0.042975589632987976, acc: 100.0, f1: 100.0, r: 0.7728527791681383
06/02/2019 12:36:38 step: 7671, epoch: 232, batch: 14, loss: 0.08619669079780579, acc: 100.0, f1: 100.0, r: 0.8121673964544588
06/02/2019 12:36:38 step: 7676, epoch: 232, batch: 19, loss: 0.062014251947402954, acc: 100.0, f1: 100.0, r: 0.687689183389784
06/02/2019 12:36:39 step: 7681, epoch: 232, batch: 24, loss: 0.053450845181941986, acc: 100.0, f1: 100.0, r: 0.7881958578637348
06/02/2019 12:36:39 step: 7686, epoch: 232, batch: 29, loss: 0.04411420226097107, acc: 98.4375, f1: 95.55555555555556, r: 0.7069582689636551
06/02/2019 12:36:40 *** evaluating ***
06/02/2019 12:36:40 step: 233, epoch: 232, acc: 52.991452991452995, f1: 28.347727346436148, r: 0.27170647421029986
06/02/2019 12:36:40 *** epoch: 234 ***
06/02/2019 12:36:40 *** training ***
06/02/2019 12:36:40 step: 7694, epoch: 233, batch: 4, loss: 0.039199087768793106, acc: 100.0, f1: 100.0, r: 0.7994062929203478
06/02/2019 12:36:41 step: 7699, epoch: 233, batch: 9, loss: 0.046461138874292374, acc: 100.0, f1: 100.0, r: 0.7309173472290403
06/02/2019 12:36:42 step: 7704, epoch: 233, batch: 14, loss: 0.03953427076339722, acc: 100.0, f1: 100.0, r: 0.806414756763129
06/02/2019 12:36:42 step: 7709, epoch: 233, batch: 19, loss: 0.02881646156311035, acc: 100.0, f1: 100.0, r: 0.7369677031777704
06/02/2019 12:36:43 step: 7714, epoch: 233, batch: 24, loss: 0.04891294240951538, acc: 100.0, f1: 100.0, r: 0.8145845113767741
06/02/2019 12:36:44 step: 7719, epoch: 233, batch: 29, loss: 0.07244721055030823, acc: 100.0, f1: 100.0, r: 0.7278579393369949
06/02/2019 12:36:44 *** evaluating ***
06/02/2019 12:36:44 step: 234, epoch: 233, acc: 52.13675213675214, f1: 22.71264086565257, r: 0.26879317635118655
06/02/2019 12:36:44 *** epoch: 235 ***
06/02/2019 12:36:44 *** training ***
06/02/2019 12:36:45 step: 7727, epoch: 234, batch: 4, loss: 0.06497889757156372, acc: 100.0, f1: 100.0, r: 0.6343729874459992
06/02/2019 12:36:45 step: 7732, epoch: 234, batch: 9, loss: 0.04525510594248772, acc: 100.0, f1: 100.0, r: 0.8429871281051313
06/02/2019 12:36:46 step: 7737, epoch: 234, batch: 14, loss: 0.037473082542419434, acc: 100.0, f1: 100.0, r: 0.7690539410745959
06/02/2019 12:36:47 step: 7742, epoch: 234, batch: 19, loss: 0.0455683171749115, acc: 100.0, f1: 100.0, r: 0.8166636658502793
06/02/2019 12:36:47 step: 7747, epoch: 234, batch: 24, loss: 0.14872056245803833, acc: 100.0, f1: 100.0, r: 0.7440183433726767
06/02/2019 12:36:48 step: 7752, epoch: 234, batch: 29, loss: 0.05284497141838074, acc: 100.0, f1: 100.0, r: 0.6835425790333233
06/02/2019 12:36:48 *** evaluating ***
06/02/2019 12:36:48 step: 235, epoch: 234, acc: 53.41880341880342, f1: 28.12964248890011, r: 0.26843815713344
06/02/2019 12:36:48 *** epoch: 236 ***
06/02/2019 12:36:48 *** training ***
06/02/2019 12:36:49 step: 7760, epoch: 235, batch: 4, loss: 0.05210335552692413, acc: 98.4375, f1: 98.69918699186992, r: 0.6862246923499057
06/02/2019 12:36:50 step: 7765, epoch: 235, batch: 9, loss: 0.22479644417762756, acc: 98.4375, f1: 99.2918961447679, r: 0.7406465189015751
06/02/2019 12:36:50 step: 7770, epoch: 235, batch: 14, loss: 0.06846138089895248, acc: 100.0, f1: 100.0, r: 0.7058368209888959
06/02/2019 12:36:51 step: 7775, epoch: 235, batch: 19, loss: 0.046027958393096924, acc: 100.0, f1: 100.0, r: 0.7638155337878677
06/02/2019 12:36:52 step: 7780, epoch: 235, batch: 24, loss: 0.0767257809638977, acc: 98.4375, f1: 97.68964189449363, r: 0.6440582150204949
06/02/2019 12:36:52 step: 7785, epoch: 235, batch: 29, loss: 0.05307112634181976, acc: 100.0, f1: 100.0, r: 0.7899887121023801
06/02/2019 12:36:53 *** evaluating ***
06/02/2019 12:36:53 step: 236, epoch: 235, acc: 53.41880341880342, f1: 27.709926960401344, r: 0.2748304075149745
06/02/2019 12:36:53 *** epoch: 237 ***
06/02/2019 12:36:53 *** training ***
06/02/2019 12:36:54 step: 7793, epoch: 236, batch: 4, loss: 0.06374094635248184, acc: 100.0, f1: 100.0, r: 0.7844669924114311
06/02/2019 12:36:54 step: 7798, epoch: 236, batch: 9, loss: 0.029990611597895622, acc: 100.0, f1: 100.0, r: 0.7124614996548698
06/02/2019 12:36:55 step: 7803, epoch: 236, batch: 14, loss: 0.06298962980508804, acc: 100.0, f1: 100.0, r: 0.7067284968929233
06/02/2019 12:36:55 step: 7808, epoch: 236, batch: 19, loss: 0.05006757751107216, acc: 100.0, f1: 100.0, r: 0.775968246311211
06/02/2019 12:36:56 step: 7813, epoch: 236, batch: 24, loss: 0.048716939985752106, acc: 100.0, f1: 100.0, r: 0.7843864983043591
06/02/2019 12:36:56 step: 7818, epoch: 236, batch: 29, loss: 0.06575527042150497, acc: 100.0, f1: 100.0, r: 0.6444342781106409
06/02/2019 12:36:57 *** evaluating ***
06/02/2019 12:36:57 step: 237, epoch: 236, acc: 52.56410256410257, f1: 28.086744536812937, r: 0.2678512527174253
06/02/2019 12:36:57 *** epoch: 238 ***
06/02/2019 12:36:57 *** training ***
06/02/2019 12:36:57 step: 7826, epoch: 237, batch: 4, loss: 0.06573918461799622, acc: 100.0, f1: 100.0, r: 0.6742596041396525
06/02/2019 12:36:58 step: 7831, epoch: 237, batch: 9, loss: 0.037157490849494934, acc: 100.0, f1: 100.0, r: 0.8233305014671342
06/02/2019 12:36:59 step: 7836, epoch: 237, batch: 14, loss: 0.043366946280002594, acc: 100.0, f1: 100.0, r: 0.7467648195524923
06/02/2019 12:36:59 step: 7841, epoch: 237, batch: 19, loss: 0.04107622057199478, acc: 100.0, f1: 100.0, r: 0.679623235955627
06/02/2019 12:37:00 step: 7846, epoch: 237, batch: 24, loss: 0.05536096543073654, acc: 100.0, f1: 100.0, r: 0.8046635165903444
06/02/2019 12:37:00 step: 7851, epoch: 237, batch: 29, loss: 0.03727620840072632, acc: 100.0, f1: 100.0, r: 0.7424888369170313
06/02/2019 12:37:01 *** evaluating ***
06/02/2019 12:37:01 step: 238, epoch: 237, acc: 53.84615384615385, f1: 27.059098386355746, r: 0.27490083508038177
06/02/2019 12:37:01 *** epoch: 239 ***
06/02/2019 12:37:01 *** training ***
06/02/2019 12:37:02 step: 7859, epoch: 238, batch: 4, loss: 0.029967334121465683, acc: 100.0, f1: 100.0, r: 0.7937932304099542
06/02/2019 12:37:02 step: 7864, epoch: 238, batch: 9, loss: 0.024415459483861923, acc: 100.0, f1: 100.0, r: 0.7188195245638638
06/02/2019 12:37:03 step: 7869, epoch: 238, batch: 14, loss: 0.04484810680150986, acc: 100.0, f1: 100.0, r: 0.7630509680928859
06/02/2019 12:37:03 step: 7874, epoch: 238, batch: 19, loss: 0.07064448297023773, acc: 100.0, f1: 100.0, r: 0.7242424691839003
06/02/2019 12:37:04 step: 7879, epoch: 238, batch: 24, loss: 0.051646243780851364, acc: 100.0, f1: 100.0, r: 0.7253712382748309
06/02/2019 12:37:04 step: 7884, epoch: 238, batch: 29, loss: 0.032359108328819275, acc: 100.0, f1: 100.0, r: 0.6999188798744234
06/02/2019 12:37:05 *** evaluating ***
06/02/2019 12:37:05 step: 239, epoch: 238, acc: 51.70940170940172, f1: 27.51908080305293, r: 0.27497595631131105
06/02/2019 12:37:05 *** epoch: 240 ***
06/02/2019 12:37:05 *** training ***
06/02/2019 12:37:06 step: 7892, epoch: 239, batch: 4, loss: 0.1521039605140686, acc: 100.0, f1: 100.0, r: 0.8176521644127203
06/02/2019 12:37:06 step: 7897, epoch: 239, batch: 9, loss: 0.07133140414953232, acc: 98.4375, f1: 98.47939175670268, r: 0.6878580077740177
06/02/2019 12:37:07 step: 7902, epoch: 239, batch: 14, loss: 0.06811700761318207, acc: 100.0, f1: 100.0, r: 0.7829646757866543
06/02/2019 12:37:07 step: 7907, epoch: 239, batch: 19, loss: 0.04688573628664017, acc: 100.0, f1: 100.0, r: 0.7160724809564446
06/02/2019 12:37:08 step: 7912, epoch: 239, batch: 24, loss: 0.04419534653425217, acc: 100.0, f1: 100.0, r: 0.7531484893637854
06/02/2019 12:37:08 step: 7917, epoch: 239, batch: 29, loss: 0.036915864795446396, acc: 100.0, f1: 100.0, r: 0.7678927725722667
06/02/2019 12:37:09 *** evaluating ***
06/02/2019 12:37:09 step: 240, epoch: 239, acc: 52.991452991452995, f1: 27.118269530034233, r: 0.2794433821236621
06/02/2019 12:37:09 *** epoch: 241 ***
06/02/2019 12:37:09 *** training ***
06/02/2019 12:37:10 step: 7925, epoch: 240, batch: 4, loss: 0.039045318961143494, acc: 100.0, f1: 100.0, r: 0.6359830938901283
06/02/2019 12:37:10 step: 7930, epoch: 240, batch: 9, loss: 0.0339684784412384, acc: 100.0, f1: 100.0, r: 0.7169340779306672
06/02/2019 12:37:11 step: 7935, epoch: 240, batch: 14, loss: 0.04015815258026123, acc: 100.0, f1: 100.0, r: 0.656183586190524
06/02/2019 12:37:11 step: 7940, epoch: 240, batch: 19, loss: 0.03868839889764786, acc: 100.0, f1: 100.0, r: 0.8064008772238516
06/02/2019 12:37:12 step: 7945, epoch: 240, batch: 24, loss: 0.04172396659851074, acc: 100.0, f1: 100.0, r: 0.8537513861008124
06/02/2019 12:37:13 step: 7950, epoch: 240, batch: 29, loss: 0.04582810401916504, acc: 100.0, f1: 100.0, r: 0.7565275872038494
06/02/2019 12:37:13 *** evaluating ***
06/02/2019 12:37:13 step: 241, epoch: 240, acc: 54.27350427350427, f1: 28.0508921999052, r: 0.2856971638739093
06/02/2019 12:37:13 *** epoch: 242 ***
06/02/2019 12:37:13 *** training ***
06/02/2019 12:37:14 step: 7958, epoch: 241, batch: 4, loss: 0.05180543661117554, acc: 100.0, f1: 100.0, r: 0.7978212673403723
06/02/2019 12:37:14 step: 7963, epoch: 241, batch: 9, loss: 0.09591750800609589, acc: 100.0, f1: 100.0, r: 0.7919485886356437
06/02/2019 12:37:15 step: 7968, epoch: 241, batch: 14, loss: 0.06728418171405792, acc: 100.0, f1: 100.0, r: 0.703515110202779
06/02/2019 12:37:16 step: 7973, epoch: 241, batch: 19, loss: 0.0614536814391613, acc: 100.0, f1: 100.0, r: 0.7986760250206867
06/02/2019 12:37:16 step: 7978, epoch: 241, batch: 24, loss: 0.044108327478170395, acc: 100.0, f1: 100.0, r: 0.7606789847294532
06/02/2019 12:37:17 step: 7983, epoch: 241, batch: 29, loss: 0.060574013739824295, acc: 100.0, f1: 100.0, r: 0.8363021034217368
06/02/2019 12:37:17 *** evaluating ***
06/02/2019 12:37:18 step: 242, epoch: 241, acc: 51.28205128205128, f1: 27.966000433286204, r: 0.27232971628643016
06/02/2019 12:37:18 *** epoch: 243 ***
06/02/2019 12:37:18 *** training ***
06/02/2019 12:37:18 step: 7991, epoch: 242, batch: 4, loss: 0.06776367127895355, acc: 100.0, f1: 100.0, r: 0.7123740609816703
06/02/2019 12:37:18 step: 7996, epoch: 242, batch: 9, loss: 0.05449530482292175, acc: 98.4375, f1: 97.89173789173789, r: 0.6479647681829809
06/02/2019 12:37:19 step: 8001, epoch: 242, batch: 14, loss: 0.056535761803388596, acc: 100.0, f1: 100.0, r: 0.8421444031632862
06/02/2019 12:37:20 step: 8006, epoch: 242, batch: 19, loss: 0.05795707553625107, acc: 100.0, f1: 100.0, r: 0.7867044995107753
06/02/2019 12:37:20 step: 8011, epoch: 242, batch: 24, loss: 0.07126197218894958, acc: 100.0, f1: 100.0, r: 0.7104178979181193
06/02/2019 12:37:21 step: 8016, epoch: 242, batch: 29, loss: 0.03747355937957764, acc: 100.0, f1: 100.0, r: 0.7324989785805589
06/02/2019 12:37:21 *** evaluating ***
06/02/2019 12:37:21 step: 243, epoch: 242, acc: 52.56410256410257, f1: 27.918101784398065, r: 0.2784791181862739
06/02/2019 12:37:21 *** epoch: 244 ***
06/02/2019 12:37:21 *** training ***
06/02/2019 12:37:22 step: 8024, epoch: 243, batch: 4, loss: 0.05622337386012077, acc: 100.0, f1: 100.0, r: 0.6171892411835967
06/02/2019 12:37:23 step: 8029, epoch: 243, batch: 9, loss: 0.0439123697578907, acc: 100.0, f1: 100.0, r: 0.7565342857457866
06/02/2019 12:37:23 step: 8034, epoch: 243, batch: 14, loss: 0.05153664946556091, acc: 100.0, f1: 100.0, r: 0.7171314979921827
06/02/2019 12:37:24 step: 8039, epoch: 243, batch: 19, loss: 0.14903077483177185, acc: 100.0, f1: 100.0, r: 0.6766560037131297
06/02/2019 12:37:25 step: 8044, epoch: 243, batch: 24, loss: 0.07408390194177628, acc: 98.4375, f1: 97.43008314436885, r: 0.6856891139208345
06/02/2019 12:37:25 step: 8049, epoch: 243, batch: 29, loss: 0.08598325401544571, acc: 100.0, f1: 100.0, r: 0.7040737405234958
06/02/2019 12:37:25 *** evaluating ***
06/02/2019 12:37:26 step: 244, epoch: 243, acc: 52.13675213675214, f1: 27.012280144178874, r: 0.2789459105377104
06/02/2019 12:37:26 *** epoch: 245 ***
06/02/2019 12:37:26 *** training ***
06/02/2019 12:37:26 step: 8057, epoch: 244, batch: 4, loss: 0.051429107785224915, acc: 100.0, f1: 100.0, r: 0.62846241580286
06/02/2019 12:37:27 step: 8062, epoch: 244, batch: 9, loss: 0.040051452815532684, acc: 100.0, f1: 100.0, r: 0.6302992928002306
06/02/2019 12:37:28 step: 8067, epoch: 244, batch: 14, loss: 0.0659143477678299, acc: 100.0, f1: 100.0, r: 0.7456073180641278
06/02/2019 12:37:28 step: 8072, epoch: 244, batch: 19, loss: 0.04989800974726677, acc: 100.0, f1: 100.0, r: 0.808140895136525
06/02/2019 12:37:29 step: 8077, epoch: 244, batch: 24, loss: 0.04535055160522461, acc: 100.0, f1: 100.0, r: 0.7701435940536444
06/02/2019 12:37:29 step: 8082, epoch: 244, batch: 29, loss: 0.04168729484081268, acc: 100.0, f1: 100.0, r: 0.7653539689303045
06/02/2019 12:37:30 *** evaluating ***
06/02/2019 12:37:30 step: 245, epoch: 244, acc: 51.70940170940172, f1: 27.174532144524644, r: 0.2681336826112417
06/02/2019 12:37:30 *** epoch: 246 ***
06/02/2019 12:37:30 *** training ***
06/02/2019 12:37:31 step: 8090, epoch: 245, batch: 4, loss: 0.03829541802406311, acc: 100.0, f1: 100.0, r: 0.6916052028266783
06/02/2019 12:37:31 step: 8095, epoch: 245, batch: 9, loss: 0.048211656510829926, acc: 100.0, f1: 100.0, r: 0.6683165012853199
06/02/2019 12:37:32 step: 8100, epoch: 245, batch: 14, loss: 0.050228673964738846, acc: 100.0, f1: 100.0, r: 0.6340363661744108
06/02/2019 12:37:32 step: 8105, epoch: 245, batch: 19, loss: 0.040352676063776016, acc: 100.0, f1: 100.0, r: 0.6038307442815938
06/02/2019 12:37:33 step: 8110, epoch: 245, batch: 24, loss: 0.041173674166202545, acc: 100.0, f1: 100.0, r: 0.662625505568198
06/02/2019 12:37:34 step: 8115, epoch: 245, batch: 29, loss: 0.057251978665590286, acc: 98.4375, f1: 97.95186891961085, r: 0.6873248049888276
06/02/2019 12:37:34 *** evaluating ***
06/02/2019 12:37:34 step: 246, epoch: 245, acc: 53.41880341880342, f1: 28.552601455561984, r: 0.27187032179417875
06/02/2019 12:37:34 *** epoch: 247 ***
06/02/2019 12:37:34 *** training ***
06/02/2019 12:37:35 step: 8123, epoch: 246, batch: 4, loss: 0.047111816704273224, acc: 100.0, f1: 100.0, r: 0.728715923946052
06/02/2019 12:37:35 step: 8128, epoch: 246, batch: 9, loss: 0.060622066259384155, acc: 100.0, f1: 100.0, r: 0.8229982709865703
06/02/2019 12:37:36 step: 8133, epoch: 246, batch: 14, loss: 0.06961802393198013, acc: 100.0, f1: 100.0, r: 0.8164865419718497
06/02/2019 12:37:37 step: 8138, epoch: 246, batch: 19, loss: 0.04493136703968048, acc: 100.0, f1: 100.0, r: 0.8349948098282599
06/02/2019 12:37:37 step: 8143, epoch: 246, batch: 24, loss: 0.0461929515004158, acc: 100.0, f1: 100.0, r: 0.7814557820696819
06/02/2019 12:37:38 step: 8148, epoch: 246, batch: 29, loss: 0.05481071025133133, acc: 98.4375, f1: 97.87581699346406, r: 0.804218428412845
06/02/2019 12:37:38 *** evaluating ***
06/02/2019 12:37:38 step: 247, epoch: 246, acc: 52.13675213675214, f1: 26.87554652811383, r: 0.2729865041319368
06/02/2019 12:37:38 *** epoch: 248 ***
06/02/2019 12:37:38 *** training ***
06/02/2019 12:37:39 step: 8156, epoch: 247, batch: 4, loss: 0.045523349195718765, acc: 100.0, f1: 100.0, r: 0.7603296094836607
06/02/2019 12:37:40 step: 8161, epoch: 247, batch: 9, loss: 0.21229109168052673, acc: 100.0, f1: 100.0, r: 0.7976291138064183
06/02/2019 12:37:40 step: 8166, epoch: 247, batch: 14, loss: 0.07856046408414841, acc: 100.0, f1: 100.0, r: 0.5948302523748953
06/02/2019 12:37:41 step: 8171, epoch: 247, batch: 19, loss: 0.03607207536697388, acc: 100.0, f1: 100.0, r: 0.6806097079552852
06/02/2019 12:37:41 step: 8176, epoch: 247, batch: 24, loss: 0.0351126492023468, acc: 100.0, f1: 100.0, r: 0.8246126244456954
06/02/2019 12:37:42 step: 8181, epoch: 247, batch: 29, loss: 0.04921424016356468, acc: 100.0, f1: 100.0, r: 0.759589805488988
06/02/2019 12:37:42 *** evaluating ***
06/02/2019 12:37:42 step: 248, epoch: 247, acc: 52.56410256410257, f1: 27.474794023181115, r: 0.2756830841878526
06/02/2019 12:37:42 *** epoch: 249 ***
06/02/2019 12:37:42 *** training ***
06/02/2019 12:37:43 step: 8189, epoch: 248, batch: 4, loss: 0.06324761360883713, acc: 100.0, f1: 100.0, r: 0.7182981247880712
06/02/2019 12:37:44 step: 8194, epoch: 248, batch: 9, loss: 0.035519711673259735, acc: 100.0, f1: 100.0, r: 0.6989110572387032
06/02/2019 12:37:44 step: 8199, epoch: 248, batch: 14, loss: 0.07606429606676102, acc: 100.0, f1: 100.0, r: 0.7804458685103209
06/02/2019 12:37:45 step: 8204, epoch: 248, batch: 19, loss: 0.32303953170776367, acc: 98.4375, f1: 98.32915622389308, r: 0.5997833434028568
06/02/2019 12:37:46 step: 8209, epoch: 248, batch: 24, loss: 0.05473069101572037, acc: 100.0, f1: 100.0, r: 0.703004321157747
06/02/2019 12:37:46 step: 8214, epoch: 248, batch: 29, loss: 0.050467830151319504, acc: 100.0, f1: 100.0, r: 0.7026803978300854
06/02/2019 12:37:47 *** evaluating ***
06/02/2019 12:37:47 step: 249, epoch: 248, acc: 52.56410256410257, f1: 27.647267063154914, r: 0.2816771946099131
06/02/2019 12:37:47 *** epoch: 250 ***
06/02/2019 12:37:47 *** training ***
06/02/2019 12:37:47 step: 8222, epoch: 249, batch: 4, loss: 0.04506528005003929, acc: 100.0, f1: 100.0, r: 0.6806112078975785
06/02/2019 12:37:48 step: 8227, epoch: 249, batch: 9, loss: 0.05124561861157417, acc: 100.0, f1: 100.0, r: 0.7491913952967023
06/02/2019 12:37:49 step: 8232, epoch: 249, batch: 14, loss: 0.05792172625660896, acc: 100.0, f1: 100.0, r: 0.7060908857397603
06/02/2019 12:37:49 step: 8237, epoch: 249, batch: 19, loss: 0.040815699845552444, acc: 100.0, f1: 100.0, r: 0.7006567356523415
06/02/2019 12:37:50 step: 8242, epoch: 249, batch: 24, loss: 0.06264838576316833, acc: 98.4375, f1: 98.90903729913018, r: 0.7096844187926084
06/02/2019 12:37:51 step: 8247, epoch: 249, batch: 29, loss: 0.07043187320232391, acc: 100.0, f1: 100.0, r: 0.786422047724704
06/02/2019 12:37:51 *** evaluating ***
06/02/2019 12:37:51 step: 250, epoch: 249, acc: 52.56410256410257, f1: 25.851724631092992, r: 0.2853551747647783
06/02/2019 12:37:51 *** epoch: 251 ***
06/02/2019 12:37:51 *** training ***
06/02/2019 12:37:52 step: 8255, epoch: 250, batch: 4, loss: 0.0722324475646019, acc: 100.0, f1: 100.0, r: 0.71615813041688
06/02/2019 12:37:53 step: 8260, epoch: 250, batch: 9, loss: 0.22261205315589905, acc: 100.0, f1: 100.0, r: 0.7445583628140375
06/02/2019 12:37:53 step: 8265, epoch: 250, batch: 14, loss: 0.04881978780031204, acc: 100.0, f1: 100.0, r: 0.7118454235797176
06/02/2019 12:37:54 step: 8270, epoch: 250, batch: 19, loss: 0.03383073955774307, acc: 100.0, f1: 100.0, r: 0.7588494967961881
06/02/2019 12:37:54 step: 8275, epoch: 250, batch: 24, loss: 0.05968789756298065, acc: 100.0, f1: 100.0, r: 0.7344422402899432
06/02/2019 12:37:55 step: 8280, epoch: 250, batch: 29, loss: 0.05093647912144661, acc: 100.0, f1: 100.0, r: 0.7267487248711986
06/02/2019 12:37:55 *** evaluating ***
06/02/2019 12:37:56 step: 251, epoch: 250, acc: 52.991452991452995, f1: 27.6026102610261, r: 0.27280149514679086
06/02/2019 12:37:56 *** epoch: 252 ***
06/02/2019 12:37:56 *** training ***
06/02/2019 12:37:56 step: 8288, epoch: 251, batch: 4, loss: 0.05626076087355614, acc: 100.0, f1: 100.0, r: 0.8132583921930259
06/02/2019 12:37:57 step: 8293, epoch: 251, batch: 9, loss: 0.06740628182888031, acc: 100.0, f1: 100.0, r: 0.735245753848065
06/02/2019 12:37:58 step: 8298, epoch: 251, batch: 14, loss: 0.030751150101423264, acc: 100.0, f1: 100.0, r: 0.7936031890923672
06/02/2019 12:37:58 step: 8303, epoch: 251, batch: 19, loss: 0.06917579472064972, acc: 100.0, f1: 100.0, r: 0.7549566533541356
06/02/2019 12:37:59 step: 8308, epoch: 251, batch: 24, loss: 0.11816459149122238, acc: 100.0, f1: 100.0, r: 0.7910013163193175
06/02/2019 12:38:00 step: 8313, epoch: 251, batch: 29, loss: 0.10398227721452713, acc: 98.4375, f1: 97.33806566104703, r: 0.6925283194075785
06/02/2019 12:38:00 *** evaluating ***
06/02/2019 12:38:00 step: 252, epoch: 251, acc: 54.700854700854705, f1: 27.78750796802981, r: 0.2853385840805535
06/02/2019 12:38:00 *** epoch: 253 ***
06/02/2019 12:38:00 *** training ***
06/02/2019 12:38:01 step: 8321, epoch: 252, batch: 4, loss: 0.044027674943208694, acc: 100.0, f1: 100.0, r: 0.7574257838442965
06/02/2019 12:38:02 step: 8326, epoch: 252, batch: 9, loss: 0.06215279549360275, acc: 100.0, f1: 100.0, r: 0.7223034691728809
06/02/2019 12:38:02 step: 8331, epoch: 252, batch: 14, loss: 0.05066312849521637, acc: 100.0, f1: 100.0, r: 0.806592944262044
06/02/2019 12:38:03 step: 8336, epoch: 252, batch: 19, loss: 0.08648326247930527, acc: 100.0, f1: 100.0, r: 0.6714398745634825
06/02/2019 12:38:04 step: 8341, epoch: 252, batch: 24, loss: 0.05178193002939224, acc: 100.0, f1: 100.0, r: 0.7491858763743532
06/02/2019 12:38:04 step: 8346, epoch: 252, batch: 29, loss: 0.0363227054476738, acc: 100.0, f1: 100.0, r: 0.7848574231671817
06/02/2019 12:38:05 *** evaluating ***
06/02/2019 12:38:05 step: 253, epoch: 252, acc: 53.41880341880342, f1: 28.427397134253376, r: 0.2751275092740647
06/02/2019 12:38:05 *** epoch: 254 ***
06/02/2019 12:38:05 *** training ***
06/02/2019 12:38:05 step: 8354, epoch: 253, batch: 4, loss: 0.06082029268145561, acc: 100.0, f1: 100.0, r: 0.7107645822862382
06/02/2019 12:38:06 step: 8359, epoch: 253, batch: 9, loss: 0.055504605174064636, acc: 100.0, f1: 100.0, r: 0.7934474176151268
06/02/2019 12:38:07 step: 8364, epoch: 253, batch: 14, loss: 0.12475363165140152, acc: 100.0, f1: 100.0, r: 0.7051896060830855
06/02/2019 12:38:07 step: 8369, epoch: 253, batch: 19, loss: 0.03299073129892349, acc: 100.0, f1: 100.0, r: 0.7706237503077679
06/02/2019 12:38:08 step: 8374, epoch: 253, batch: 24, loss: 0.0727292001247406, acc: 100.0, f1: 100.0, r: 0.6990231032000014
06/02/2019 12:38:09 step: 8379, epoch: 253, batch: 29, loss: 0.11119751632213593, acc: 100.0, f1: 100.0, r: 0.7245306908711101
06/02/2019 12:38:09 *** evaluating ***
06/02/2019 12:38:09 step: 254, epoch: 253, acc: 52.991452991452995, f1: 27.006353576534124, r: 0.2727502749740403
06/02/2019 12:38:09 *** epoch: 255 ***
06/02/2019 12:38:09 *** training ***
06/02/2019 12:38:10 step: 8387, epoch: 254, batch: 4, loss: 0.07803425937891006, acc: 100.0, f1: 100.0, r: 0.7229487376444788
06/02/2019 12:38:10 step: 8392, epoch: 254, batch: 9, loss: 0.2787085175514221, acc: 100.0, f1: 100.0, r: 0.7387545051923922
06/02/2019 12:38:11 step: 8397, epoch: 254, batch: 14, loss: 0.1055116206407547, acc: 100.0, f1: 100.0, r: 0.810958091532557
06/02/2019 12:38:11 step: 8402, epoch: 254, batch: 19, loss: 0.051921695470809937, acc: 100.0, f1: 100.0, r: 0.7810150835038913
06/02/2019 12:38:12 step: 8407, epoch: 254, batch: 24, loss: 0.06065893545746803, acc: 100.0, f1: 100.0, r: 0.7269307789699688
06/02/2019 12:38:13 step: 8412, epoch: 254, batch: 29, loss: 0.04920896142721176, acc: 100.0, f1: 100.0, r: 0.8400483435918582
06/02/2019 12:38:13 *** evaluating ***
06/02/2019 12:38:13 step: 255, epoch: 254, acc: 53.84615384615385, f1: 25.86945516485611, r: 0.2670702078494247
06/02/2019 12:38:13 *** epoch: 256 ***
06/02/2019 12:38:13 *** training ***
06/02/2019 12:38:14 step: 8420, epoch: 255, batch: 4, loss: 0.04798222705721855, acc: 100.0, f1: 100.0, r: 0.7579764913512007
06/02/2019 12:38:14 step: 8425, epoch: 255, batch: 9, loss: 0.05016613379120827, acc: 100.0, f1: 100.0, r: 0.6739256023485403
06/02/2019 12:38:15 step: 8430, epoch: 255, batch: 14, loss: 0.04674842581152916, acc: 100.0, f1: 100.0, r: 0.7902002456545658
06/02/2019 12:38:16 step: 8435, epoch: 255, batch: 19, loss: 0.06824995577335358, acc: 98.4375, f1: 97.57236227824464, r: 0.725236411008861
06/02/2019 12:38:16 step: 8440, epoch: 255, batch: 24, loss: 0.023139169439673424, acc: 100.0, f1: 100.0, r: 0.6523248297605693
06/02/2019 12:38:17 step: 8445, epoch: 255, batch: 29, loss: 0.05684956535696983, acc: 100.0, f1: 100.0, r: 0.752170753439885
06/02/2019 12:38:17 *** evaluating ***
06/02/2019 12:38:17 step: 256, epoch: 255, acc: 53.41880341880342, f1: 27.101876628077946, r: 0.2664078315183191
06/02/2019 12:38:17 *** epoch: 257 ***
06/02/2019 12:38:17 *** training ***
06/02/2019 12:38:18 step: 8453, epoch: 256, batch: 4, loss: 0.052082911133766174, acc: 100.0, f1: 100.0, r: 0.709198350101703
06/02/2019 12:38:19 step: 8458, epoch: 256, batch: 9, loss: 0.051252689212560654, acc: 100.0, f1: 100.0, r: 0.8106202893724137
06/02/2019 12:38:19 step: 8463, epoch: 256, batch: 14, loss: 0.036815691739320755, acc: 100.0, f1: 100.0, r: 0.712361721173768
06/02/2019 12:38:20 step: 8468, epoch: 256, batch: 19, loss: 0.07201676815748215, acc: 100.0, f1: 100.0, r: 0.7332639715650279
06/02/2019 12:38:20 step: 8473, epoch: 256, batch: 24, loss: 0.0738615170121193, acc: 100.0, f1: 100.0, r: 0.8542767426945559
06/02/2019 12:38:21 step: 8478, epoch: 256, batch: 29, loss: 0.2188596874475479, acc: 100.0, f1: 100.0, r: 0.7711337151348482
06/02/2019 12:38:21 *** evaluating ***
06/02/2019 12:38:22 step: 257, epoch: 256, acc: 51.28205128205128, f1: 25.642958649974894, r: 0.25933811117822314
06/02/2019 12:38:22 *** epoch: 258 ***
06/02/2019 12:38:22 *** training ***
06/02/2019 12:38:22 step: 8486, epoch: 257, batch: 4, loss: 0.050830136984586716, acc: 100.0, f1: 100.0, r: 0.6775582341520109
06/02/2019 12:38:23 step: 8491, epoch: 257, batch: 9, loss: 0.07204791903495789, acc: 100.0, f1: 100.0, r: 0.709199101704359
06/02/2019 12:38:23 step: 8496, epoch: 257, batch: 14, loss: 0.07685437798500061, acc: 100.0, f1: 100.0, r: 0.7578724468814265
06/02/2019 12:38:24 step: 8501, epoch: 257, batch: 19, loss: 0.06834616512060165, acc: 100.0, f1: 100.0, r: 0.7877975333250441
06/02/2019 12:38:25 step: 8506, epoch: 257, batch: 24, loss: 0.04941333457827568, acc: 100.0, f1: 100.0, r: 0.7086964085707679
06/02/2019 12:38:25 step: 8511, epoch: 257, batch: 29, loss: 0.05765680968761444, acc: 100.0, f1: 100.0, r: 0.7957028824954768
06/02/2019 12:38:26 *** evaluating ***
06/02/2019 12:38:26 step: 258, epoch: 257, acc: 52.56410256410257, f1: 27.62730061167813, r: 0.2614557488063632
06/02/2019 12:38:26 *** epoch: 259 ***
06/02/2019 12:38:26 *** training ***
06/02/2019 12:38:26 step: 8519, epoch: 258, batch: 4, loss: 0.05402024835348129, acc: 100.0, f1: 100.0, r: 0.7977280067170572
06/02/2019 12:38:27 step: 8524, epoch: 258, batch: 9, loss: 0.03695180267095566, acc: 100.0, f1: 100.0, r: 0.7429916276335966
06/02/2019 12:38:27 step: 8529, epoch: 258, batch: 14, loss: 0.06132669001817703, acc: 100.0, f1: 100.0, r: 0.6871244614169807
06/02/2019 12:38:28 step: 8534, epoch: 258, batch: 19, loss: 0.040029868483543396, acc: 100.0, f1: 100.0, r: 0.6976243741071119
06/02/2019 12:38:29 step: 8539, epoch: 258, batch: 24, loss: 0.05893067270517349, acc: 100.0, f1: 100.0, r: 0.6343641368154546
06/02/2019 12:38:29 step: 8544, epoch: 258, batch: 29, loss: 0.05784149467945099, acc: 100.0, f1: 100.0, r: 0.7859741756731881
06/02/2019 12:38:30 *** evaluating ***
06/02/2019 12:38:30 step: 259, epoch: 258, acc: 51.70940170940172, f1: 26.066392094636452, r: 0.2586274610375727
06/02/2019 12:38:30 *** epoch: 260 ***
06/02/2019 12:38:30 *** training ***
06/02/2019 12:38:30 step: 8552, epoch: 259, batch: 4, loss: 0.24224665760993958, acc: 100.0, f1: 100.0, r: 0.6926822766452614
06/02/2019 12:38:31 step: 8557, epoch: 259, batch: 9, loss: 0.060494884848594666, acc: 98.4375, f1: 98.32967032967032, r: 0.6927158733241258
06/02/2019 12:38:32 step: 8562, epoch: 259, batch: 14, loss: 0.06969746202230453, acc: 100.0, f1: 100.0, r: 0.7743709097055806
06/02/2019 12:38:32 step: 8567, epoch: 259, batch: 19, loss: 0.04360726475715637, acc: 100.0, f1: 100.0, r: 0.7328185035721303
06/02/2019 12:38:33 step: 8572, epoch: 259, batch: 24, loss: 0.056731950491666794, acc: 100.0, f1: 100.0, r: 0.6943587827673364
06/02/2019 12:38:33 step: 8577, epoch: 259, batch: 29, loss: 0.0513395257294178, acc: 100.0, f1: 100.0, r: 0.718797442346994
06/02/2019 12:38:34 *** evaluating ***
06/02/2019 12:38:34 step: 260, epoch: 259, acc: 51.70940170940172, f1: 26.203577186893668, r: 0.2551748999768154
06/02/2019 12:38:34 *** epoch: 261 ***
06/02/2019 12:38:34 *** training ***
06/02/2019 12:38:34 step: 8585, epoch: 260, batch: 4, loss: 0.05145929753780365, acc: 100.0, f1: 100.0, r: 0.8428963204735054
06/02/2019 12:38:35 step: 8590, epoch: 260, batch: 9, loss: 0.040226005017757416, acc: 100.0, f1: 100.0, r: 0.7249856660396501
06/02/2019 12:38:36 step: 8595, epoch: 260, batch: 14, loss: 0.05429933965206146, acc: 98.4375, f1: 85.32818532818533, r: 0.6196079939788454
06/02/2019 12:38:36 step: 8600, epoch: 260, batch: 19, loss: 0.06992097944021225, acc: 100.0, f1: 100.0, r: 0.7474383219116671
06/02/2019 12:38:37 step: 8605, epoch: 260, batch: 24, loss: 0.04445952922105789, acc: 100.0, f1: 100.0, r: 0.7965347668781209
06/02/2019 12:38:37 step: 8610, epoch: 260, batch: 29, loss: 0.044139664620161057, acc: 100.0, f1: 100.0, r: 0.7106916240048291
06/02/2019 12:38:38 *** evaluating ***
06/02/2019 12:38:38 step: 261, epoch: 260, acc: 54.27350427350427, f1: 27.915530655672292, r: 0.27385983980556117
06/02/2019 12:38:38 *** epoch: 262 ***
06/02/2019 12:38:38 *** training ***
06/02/2019 12:38:39 step: 8618, epoch: 261, batch: 4, loss: 0.04992854595184326, acc: 100.0, f1: 100.0, r: 0.8205252753865584
06/02/2019 12:38:39 step: 8623, epoch: 261, batch: 9, loss: 0.08296501636505127, acc: 100.0, f1: 100.0, r: 0.7007167785163214
06/02/2019 12:38:40 step: 8628, epoch: 261, batch: 14, loss: 0.03230497986078262, acc: 100.0, f1: 100.0, r: 0.7705570478506678
06/02/2019 12:38:40 step: 8633, epoch: 261, batch: 19, loss: 0.03660239279270172, acc: 100.0, f1: 100.0, r: 0.7648490939706638
06/02/2019 12:38:41 step: 8638, epoch: 261, batch: 24, loss: 0.0411505401134491, acc: 100.0, f1: 100.0, r: 0.802264264633638
06/02/2019 12:38:41 step: 8643, epoch: 261, batch: 29, loss: 0.059967998415231705, acc: 100.0, f1: 100.0, r: 0.7500218893885838
06/02/2019 12:38:42 *** evaluating ***
06/02/2019 12:38:42 step: 262, epoch: 261, acc: 52.991452991452995, f1: 27.319230882391533, r: 0.2700171316114856
06/02/2019 12:38:42 *** epoch: 263 ***
06/02/2019 12:38:42 *** training ***
06/02/2019 12:38:42 step: 8651, epoch: 262, batch: 4, loss: 0.06382947415113449, acc: 100.0, f1: 100.0, r: 0.8119995309492221
06/02/2019 12:38:43 step: 8656, epoch: 262, batch: 9, loss: 0.035367246717214584, acc: 100.0, f1: 100.0, r: 0.6882788286977124
06/02/2019 12:38:44 step: 8661, epoch: 262, batch: 14, loss: 0.06542624533176422, acc: 98.4375, f1: 98.58823529411765, r: 0.7211557635380293
06/02/2019 12:38:44 step: 8666, epoch: 262, batch: 19, loss: 0.04394175112247467, acc: 100.0, f1: 100.0, r: 0.7285957879466782
06/02/2019 12:38:45 step: 8671, epoch: 262, batch: 24, loss: 0.044609688222408295, acc: 100.0, f1: 100.0, r: 0.7757238056547338
06/02/2019 12:38:46 step: 8676, epoch: 262, batch: 29, loss: 0.04365506023168564, acc: 100.0, f1: 100.0, r: 0.7833478711157401
06/02/2019 12:38:46 *** evaluating ***
06/02/2019 12:38:46 step: 263, epoch: 262, acc: 55.98290598290598, f1: 28.414481805522477, r: 0.2688205017034414
06/02/2019 12:38:46 *** epoch: 264 ***
06/02/2019 12:38:46 *** training ***
06/02/2019 12:38:47 step: 8684, epoch: 263, batch: 4, loss: 0.051080815494060516, acc: 100.0, f1: 100.0, r: 0.7782089570550235
06/02/2019 12:38:47 step: 8689, epoch: 263, batch: 9, loss: 0.044040385633707047, acc: 100.0, f1: 100.0, r: 0.7935764395357312
06/02/2019 12:38:48 step: 8694, epoch: 263, batch: 14, loss: 0.03759724646806717, acc: 100.0, f1: 100.0, r: 0.7528010513072163
06/02/2019 12:38:49 step: 8699, epoch: 263, batch: 19, loss: 0.06510981917381287, acc: 100.0, f1: 100.0, r: 0.7376971832517336
06/02/2019 12:38:49 step: 8704, epoch: 263, batch: 24, loss: 0.09126710146665573, acc: 100.0, f1: 100.0, r: 0.7123028662557674
06/02/2019 12:38:50 step: 8709, epoch: 263, batch: 29, loss: 0.1801484078168869, acc: 100.0, f1: 100.0, r: 0.666128530670327
06/02/2019 12:38:50 *** evaluating ***
06/02/2019 12:38:50 step: 264, epoch: 263, acc: 52.991452991452995, f1: 26.90021164846358, r: 0.2671561049175407
06/02/2019 12:38:50 *** epoch: 265 ***
06/02/2019 12:38:50 *** training ***
06/02/2019 12:38:51 step: 8717, epoch: 264, batch: 4, loss: 0.04407591372728348, acc: 100.0, f1: 100.0, r: 0.6847552168298485
06/02/2019 12:38:51 step: 8722, epoch: 264, batch: 9, loss: 0.053536199033260345, acc: 100.0, f1: 100.0, r: 0.5964247080981399
06/02/2019 12:38:52 step: 8727, epoch: 264, batch: 14, loss: 0.05930984765291214, acc: 100.0, f1: 100.0, r: 0.7495819371606247
06/02/2019 12:38:53 step: 8732, epoch: 264, batch: 19, loss: 0.13983570039272308, acc: 98.4375, f1: 97.61075161772025, r: 0.6951192112459104
06/02/2019 12:38:53 step: 8737, epoch: 264, batch: 24, loss: 0.053617775440216064, acc: 100.0, f1: 100.0, r: 0.8072669078325373
06/02/2019 12:38:54 step: 8742, epoch: 264, batch: 29, loss: 0.047044217586517334, acc: 100.0, f1: 100.0, r: 0.7505126186272444
06/02/2019 12:38:54 *** evaluating ***
06/02/2019 12:38:54 step: 265, epoch: 264, acc: 51.70940170940172, f1: 27.003628992958262, r: 0.2655202291389235
06/02/2019 12:38:54 *** epoch: 266 ***
06/02/2019 12:38:54 *** training ***
06/02/2019 12:38:55 step: 8750, epoch: 265, batch: 4, loss: 0.18924541771411896, acc: 100.0, f1: 100.0, r: 0.8151471842641614
06/02/2019 12:38:55 step: 8755, epoch: 265, batch: 9, loss: 0.040609538555145264, acc: 100.0, f1: 100.0, r: 0.718867136686008
06/02/2019 12:38:56 step: 8760, epoch: 265, batch: 14, loss: 0.1253712773323059, acc: 98.4375, f1: 93.93939393939394, r: 0.7226865665887608
06/02/2019 12:38:57 step: 8765, epoch: 265, batch: 19, loss: 0.08066301792860031, acc: 100.0, f1: 100.0, r: 0.7784138365245955
06/02/2019 12:38:57 step: 8770, epoch: 265, batch: 24, loss: 0.05433021858334541, acc: 100.0, f1: 100.0, r: 0.7134322461087268
06/02/2019 12:38:58 step: 8775, epoch: 265, batch: 29, loss: 0.047644779086112976, acc: 100.0, f1: 100.0, r: 0.843335542067024
06/02/2019 12:38:58 *** evaluating ***
06/02/2019 12:38:58 step: 266, epoch: 265, acc: 51.70940170940172, f1: 27.42905857995372, r: 0.2638153152548402
06/02/2019 12:38:58 *** epoch: 267 ***
06/02/2019 12:38:58 *** training ***
06/02/2019 12:38:59 step: 8783, epoch: 266, batch: 4, loss: 0.027519721537828445, acc: 100.0, f1: 100.0, r: 0.6604806096635253
06/02/2019 12:39:00 step: 8788, epoch: 266, batch: 9, loss: 0.04288005456328392, acc: 100.0, f1: 100.0, r: 0.6806435323056246
06/02/2019 12:39:00 step: 8793, epoch: 266, batch: 14, loss: 0.04008854925632477, acc: 100.0, f1: 100.0, r: 0.6686911082473602
06/02/2019 12:39:01 step: 8798, epoch: 266, batch: 19, loss: 0.06651565432548523, acc: 100.0, f1: 100.0, r: 0.7951112053516874
06/02/2019 12:39:01 step: 8803, epoch: 266, batch: 24, loss: 0.19528323411941528, acc: 100.0, f1: 100.0, r: 0.7953749270373044
06/02/2019 12:39:02 step: 8808, epoch: 266, batch: 29, loss: 0.04367708042263985, acc: 100.0, f1: 100.0, r: 0.723163518276329
06/02/2019 12:39:02 *** evaluating ***
06/02/2019 12:39:02 step: 267, epoch: 266, acc: 55.12820512820513, f1: 28.61712633771457, r: 0.28212239051465027
06/02/2019 12:39:02 *** epoch: 268 ***
06/02/2019 12:39:02 *** training ***
06/02/2019 12:39:03 step: 8816, epoch: 267, batch: 4, loss: 0.05899980664253235, acc: 98.4375, f1: 95.33333333333334, r: 0.7415171534364031
06/02/2019 12:39:04 step: 8821, epoch: 267, batch: 9, loss: 0.057567376643419266, acc: 100.0, f1: 100.0, r: 0.7906180020767789
06/02/2019 12:39:04 step: 8826, epoch: 267, batch: 14, loss: 0.06560197472572327, acc: 100.0, f1: 100.0, r: 0.7212490250120914
06/02/2019 12:39:05 step: 8831, epoch: 267, batch: 19, loss: 0.07876762002706528, acc: 98.4375, f1: 97.43008314436885, r: 0.6779495113698543
06/02/2019 12:39:05 step: 8836, epoch: 267, batch: 24, loss: 0.0727720856666565, acc: 100.0, f1: 100.0, r: 0.6951959597440925
06/02/2019 12:39:06 step: 8841, epoch: 267, batch: 29, loss: 0.22111175954341888, acc: 100.0, f1: 100.0, r: 0.7060743188751427
06/02/2019 12:39:06 *** evaluating ***
06/02/2019 12:39:06 step: 268, epoch: 267, acc: 52.56410256410257, f1: 27.291509330060737, r: 0.2690467580680616
06/02/2019 12:39:06 *** epoch: 269 ***
06/02/2019 12:39:06 *** training ***
06/02/2019 12:39:07 step: 8849, epoch: 268, batch: 4, loss: 0.06091276928782463, acc: 100.0, f1: 100.0, r: 0.7704821095014092
06/02/2019 12:39:08 step: 8854, epoch: 268, batch: 9, loss: 0.053530335426330566, acc: 100.0, f1: 100.0, r: 0.7185653620117253
06/02/2019 12:39:08 step: 8859, epoch: 268, batch: 14, loss: 0.047937773168087006, acc: 100.0, f1: 100.0, r: 0.7863444900068922
06/02/2019 12:39:09 step: 8864, epoch: 268, batch: 19, loss: 0.047961123287677765, acc: 100.0, f1: 100.0, r: 0.7887066465563511
06/02/2019 12:39:09 step: 8869, epoch: 268, batch: 24, loss: 0.04698978364467621, acc: 100.0, f1: 100.0, r: 0.7121962593627832
06/02/2019 12:39:10 step: 8874, epoch: 268, batch: 29, loss: 0.07872951030731201, acc: 100.0, f1: 100.0, r: 0.6664660188218849
06/02/2019 12:39:10 *** evaluating ***
06/02/2019 12:39:11 step: 269, epoch: 268, acc: 53.41880341880342, f1: 28.086865369091797, r: 0.2707325948202112
06/02/2019 12:39:11 *** epoch: 270 ***
06/02/2019 12:39:11 *** training ***
06/02/2019 12:39:11 step: 8882, epoch: 269, batch: 4, loss: 0.03974948823451996, acc: 100.0, f1: 100.0, r: 0.6019695759029836
06/02/2019 12:39:12 step: 8887, epoch: 269, batch: 9, loss: 0.04486658796668053, acc: 100.0, f1: 100.0, r: 0.7321407094239684
06/02/2019 12:39:13 step: 8892, epoch: 269, batch: 14, loss: 0.053955771028995514, acc: 100.0, f1: 100.0, r: 0.6957635755561279
06/02/2019 12:39:13 step: 8897, epoch: 269, batch: 19, loss: 0.04987160488963127, acc: 100.0, f1: 100.0, r: 0.8346421383985749
06/02/2019 12:39:14 step: 8902, epoch: 269, batch: 24, loss: 0.03354518488049507, acc: 100.0, f1: 100.0, r: 0.6953169682002173
06/02/2019 12:39:15 step: 8907, epoch: 269, batch: 29, loss: 0.048729307949543, acc: 100.0, f1: 100.0, r: 0.7389079529500561
06/02/2019 12:39:15 *** evaluating ***
06/02/2019 12:39:15 step: 270, epoch: 269, acc: 52.991452991452995, f1: 27.58978719351446, r: 0.2732551887811537
06/02/2019 12:39:15 *** epoch: 271 ***
06/02/2019 12:39:15 *** training ***
06/02/2019 12:39:16 step: 8915, epoch: 270, batch: 4, loss: 0.0489632673561573, acc: 100.0, f1: 100.0, r: 0.7053801021917391
06/02/2019 12:39:17 step: 8920, epoch: 270, batch: 9, loss: 0.030375316739082336, acc: 100.0, f1: 100.0, r: 0.6997933059196975
06/02/2019 12:39:17 step: 8925, epoch: 270, batch: 14, loss: 0.03610735386610031, acc: 100.0, f1: 100.0, r: 0.7178692368925855
06/02/2019 12:39:18 step: 8930, epoch: 270, batch: 19, loss: 0.1449728012084961, acc: 100.0, f1: 100.0, r: 0.6897738420323701
06/02/2019 12:39:18 step: 8935, epoch: 270, batch: 24, loss: 0.04427309334278107, acc: 100.0, f1: 100.0, r: 0.7910302579112894
06/02/2019 12:39:19 step: 8940, epoch: 270, batch: 29, loss: 0.06200599670410156, acc: 100.0, f1: 100.0, r: 0.8196144232884548
06/02/2019 12:39:19 *** evaluating ***
06/02/2019 12:39:20 step: 271, epoch: 270, acc: 54.27350427350427, f1: 27.446428571428573, r: 0.27946010990273273
06/02/2019 12:39:20 *** epoch: 272 ***
06/02/2019 12:39:20 *** training ***
06/02/2019 12:39:20 step: 8948, epoch: 271, batch: 4, loss: 0.164515882730484, acc: 100.0, f1: 100.0, r: 0.7894151611665715
06/02/2019 12:39:21 step: 8953, epoch: 271, batch: 9, loss: 0.04555938020348549, acc: 100.0, f1: 100.0, r: 0.6876324770098285
06/02/2019 12:39:21 step: 8958, epoch: 271, batch: 14, loss: 0.03889366611838341, acc: 100.0, f1: 100.0, r: 0.709018055818761
06/02/2019 12:39:22 step: 8963, epoch: 271, batch: 19, loss: 0.06050626188516617, acc: 100.0, f1: 100.0, r: 0.7423412772716392
06/02/2019 12:39:23 step: 8968, epoch: 271, batch: 24, loss: 0.03447621688246727, acc: 100.0, f1: 100.0, r: 0.8258159919231742
06/02/2019 12:39:23 step: 8973, epoch: 271, batch: 29, loss: 0.22190459072589874, acc: 100.0, f1: 100.0, r: 0.6701981725705182
06/02/2019 12:39:24 *** evaluating ***
06/02/2019 12:39:24 step: 272, epoch: 271, acc: 54.27350427350427, f1: 27.94578798667584, r: 0.281231041882874
06/02/2019 12:39:24 *** epoch: 273 ***
06/02/2019 12:39:24 *** training ***
06/02/2019 12:39:24 step: 8981, epoch: 272, batch: 4, loss: 0.026195630431175232, acc: 100.0, f1: 100.0, r: 0.8528897468159768
06/02/2019 12:39:25 step: 8986, epoch: 272, batch: 9, loss: 0.03113575652241707, acc: 100.0, f1: 100.0, r: 0.6740033401439447
06/02/2019 12:39:26 step: 8991, epoch: 272, batch: 14, loss: 0.030711829662322998, acc: 100.0, f1: 100.0, r: 0.7686168796459246
06/02/2019 12:39:26 step: 8996, epoch: 272, batch: 19, loss: 0.04426860064268112, acc: 100.0, f1: 100.0, r: 0.681202203479651
06/02/2019 12:39:27 step: 9001, epoch: 272, batch: 24, loss: 0.20429690182209015, acc: 100.0, f1: 100.0, r: 0.8178439903651779
06/02/2019 12:39:27 step: 9006, epoch: 272, batch: 29, loss: 0.05512476712465286, acc: 98.4375, f1: 98.22082679225535, r: 0.699303057829634
06/02/2019 12:39:28 *** evaluating ***
06/02/2019 12:39:28 step: 273, epoch: 272, acc: 53.84615384615385, f1: 27.826878629210594, r: 0.2686575921880717
06/02/2019 12:39:28 *** epoch: 274 ***
06/02/2019 12:39:28 *** training ***
06/02/2019 12:39:28 step: 9014, epoch: 273, batch: 4, loss: 0.14770983159542084, acc: 100.0, f1: 100.0, r: 0.6834449858820634
06/02/2019 12:39:29 step: 9019, epoch: 273, batch: 9, loss: 0.05148036405444145, acc: 100.0, f1: 100.0, r: 0.6684293295462654
06/02/2019 12:39:29 step: 9024, epoch: 273, batch: 14, loss: 0.042841557413339615, acc: 100.0, f1: 100.0, r: 0.7277202894987768
06/02/2019 12:39:30 step: 9029, epoch: 273, batch: 19, loss: 0.05686313658952713, acc: 98.4375, f1: 97.97979797979798, r: 0.7065332027923525
06/02/2019 12:39:31 step: 9034, epoch: 273, batch: 24, loss: 0.0627814307808876, acc: 100.0, f1: 100.0, r: 0.6826124887169756
06/02/2019 12:39:31 step: 9039, epoch: 273, batch: 29, loss: 0.04394017159938812, acc: 100.0, f1: 100.0, r: 0.6592303869125868
06/02/2019 12:39:32 *** evaluating ***
06/02/2019 12:39:32 step: 274, epoch: 273, acc: 53.84615384615385, f1: 27.388025898337435, r: 0.2768917932054171
06/02/2019 12:39:32 *** epoch: 275 ***
06/02/2019 12:39:32 *** training ***
06/02/2019 12:39:32 step: 9047, epoch: 274, batch: 4, loss: 0.1850147843360901, acc: 100.0, f1: 100.0, r: 0.7505290486493171
06/02/2019 12:39:33 step: 9052, epoch: 274, batch: 9, loss: 0.3114536702632904, acc: 100.0, f1: 100.0, r: 0.7421974076918827
06/02/2019 12:39:33 step: 9057, epoch: 274, batch: 14, loss: 0.03151511028409004, acc: 100.0, f1: 100.0, r: 0.6495923454806439
06/02/2019 12:39:34 step: 9062, epoch: 274, batch: 19, loss: 0.051128290593624115, acc: 100.0, f1: 100.0, r: 0.8144967991322295
06/02/2019 12:39:35 step: 9067, epoch: 274, batch: 24, loss: 0.029376979917287827, acc: 100.0, f1: 100.0, r: 0.8452995737486488
06/02/2019 12:39:35 step: 9072, epoch: 274, batch: 29, loss: 0.053646694868803024, acc: 100.0, f1: 100.0, r: 0.7427960526962882
06/02/2019 12:39:36 *** evaluating ***
06/02/2019 12:39:36 step: 275, epoch: 274, acc: 53.41880341880342, f1: 25.787869121052253, r: 0.2709038435251242
06/02/2019 12:39:36 *** epoch: 276 ***
06/02/2019 12:39:36 *** training ***
06/02/2019 12:39:36 step: 9080, epoch: 275, batch: 4, loss: 0.058475300669670105, acc: 100.0, f1: 100.0, r: 0.7807894888972163
06/02/2019 12:39:37 step: 9085, epoch: 275, batch: 9, loss: 0.07836319506168365, acc: 100.0, f1: 100.0, r: 0.8282504989389867
06/02/2019 12:39:38 step: 9090, epoch: 275, batch: 14, loss: 0.045051611959934235, acc: 100.0, f1: 100.0, r: 0.7871216914214301
06/02/2019 12:39:38 step: 9095, epoch: 275, batch: 19, loss: 0.15645819902420044, acc: 100.0, f1: 100.0, r: 0.7745159517296021
06/02/2019 12:39:39 step: 9100, epoch: 275, batch: 24, loss: 0.036714740097522736, acc: 100.0, f1: 100.0, r: 0.7295813554322795
06/02/2019 12:39:39 step: 9105, epoch: 275, batch: 29, loss: 0.05752123147249222, acc: 98.4375, f1: 98.32967032967032, r: 0.7356457513023742
06/02/2019 12:39:40 *** evaluating ***
06/02/2019 12:39:40 step: 276, epoch: 275, acc: 52.991452991452995, f1: 27.20588235294118, r: 0.27161870901915536
06/02/2019 12:39:40 *** epoch: 277 ***
06/02/2019 12:39:40 *** training ***
06/02/2019 12:39:40 step: 9113, epoch: 276, batch: 4, loss: 0.05800781399011612, acc: 100.0, f1: 100.0, r: 0.7360362830614386
06/02/2019 12:39:41 step: 9118, epoch: 276, batch: 9, loss: 0.081952303647995, acc: 98.4375, f1: 98.95657511124752, r: 0.7306080967827823
06/02/2019 12:39:42 step: 9123, epoch: 276, batch: 14, loss: 0.09572144597768784, acc: 98.4375, f1: 97.67080745341616, r: 0.7480786061680753
06/02/2019 12:39:42 step: 9128, epoch: 276, batch: 19, loss: 0.04512908309698105, acc: 100.0, f1: 100.0, r: 0.7804440956061611
06/02/2019 12:39:43 step: 9133, epoch: 276, batch: 24, loss: 0.1412443220615387, acc: 100.0, f1: 100.0, r: 0.7376251063042142
06/02/2019 12:39:43 step: 9138, epoch: 276, batch: 29, loss: 0.06562189757823944, acc: 100.0, f1: 100.0, r: 0.7741246499484582
06/02/2019 12:39:44 *** evaluating ***
06/02/2019 12:39:44 step: 277, epoch: 276, acc: 53.41880341880342, f1: 26.771361754325202, r: 0.27033165164638523
06/02/2019 12:39:44 *** epoch: 278 ***
06/02/2019 12:39:44 *** training ***
06/02/2019 12:39:45 step: 9146, epoch: 277, batch: 4, loss: 0.031750909984111786, acc: 100.0, f1: 100.0, r: 0.6160326780966194
06/02/2019 12:39:45 step: 9151, epoch: 277, batch: 9, loss: 0.030851779505610466, acc: 100.0, f1: 100.0, r: 0.748984295364968
06/02/2019 12:39:46 step: 9156, epoch: 277, batch: 14, loss: 0.033019792288541794, acc: 100.0, f1: 100.0, r: 0.716276797131758
06/02/2019 12:39:46 step: 9161, epoch: 277, batch: 19, loss: 0.0679815337061882, acc: 100.0, f1: 100.0, r: 0.7974418573764485
06/02/2019 12:39:47 step: 9166, epoch: 277, batch: 24, loss: 0.07443734258413315, acc: 98.4375, f1: 97.92008757526, r: 0.6241982649202976
06/02/2019 12:39:48 step: 9171, epoch: 277, batch: 29, loss: 0.05135226249694824, acc: 100.0, f1: 100.0, r: 0.6850417763680599
06/02/2019 12:39:48 *** evaluating ***
06/02/2019 12:39:48 step: 278, epoch: 277, acc: 52.991452991452995, f1: 27.175781420590223, r: 0.2846569478862036
06/02/2019 12:39:48 *** epoch: 279 ***
06/02/2019 12:39:48 *** training ***
06/02/2019 12:39:49 step: 9179, epoch: 278, batch: 4, loss: 0.045788586139678955, acc: 100.0, f1: 100.0, r: 0.7023184692085346
06/02/2019 12:39:49 step: 9184, epoch: 278, batch: 9, loss: 0.039889462292194366, acc: 100.0, f1: 100.0, r: 0.7417197716640119
06/02/2019 12:39:50 step: 9189, epoch: 278, batch: 14, loss: 0.04577377066016197, acc: 100.0, f1: 100.0, r: 0.7844848779825965
06/02/2019 12:39:51 step: 9194, epoch: 278, batch: 19, loss: 0.04106950759887695, acc: 100.0, f1: 100.0, r: 0.8368704626648101
06/02/2019 12:39:51 step: 9199, epoch: 278, batch: 24, loss: 0.03396369889378548, acc: 100.0, f1: 100.0, r: 0.6931476779578968
06/02/2019 12:39:52 step: 9204, epoch: 278, batch: 29, loss: 0.061062995344400406, acc: 100.0, f1: 100.0, r: 0.5646392750098156
06/02/2019 12:39:52 *** evaluating ***
06/02/2019 12:39:52 step: 279, epoch: 278, acc: 54.700854700854705, f1: 27.461276886223352, r: 0.28448039104431916
06/02/2019 12:39:52 *** epoch: 280 ***
06/02/2019 12:39:52 *** training ***
06/02/2019 12:39:53 step: 9212, epoch: 279, batch: 4, loss: 0.06687942147254944, acc: 100.0, f1: 100.0, r: 0.6985737552852647
06/02/2019 12:39:53 step: 9217, epoch: 279, batch: 9, loss: 0.03590059280395508, acc: 100.0, f1: 100.0, r: 0.8019441027070509
06/02/2019 12:39:54 step: 9222, epoch: 279, batch: 14, loss: 0.04777994751930237, acc: 100.0, f1: 100.0, r: 0.7219868608927936
06/02/2019 12:39:54 step: 9227, epoch: 279, batch: 19, loss: 0.17481736838817596, acc: 100.0, f1: 100.0, r: 0.7573751364922299
06/02/2019 12:39:55 step: 9232, epoch: 279, batch: 24, loss: 0.09654644131660461, acc: 100.0, f1: 100.0, r: 0.7123138733826123
06/02/2019 12:39:56 step: 9237, epoch: 279, batch: 29, loss: 0.0647883266210556, acc: 100.0, f1: 100.0, r: 0.6024599650077351
06/02/2019 12:39:56 *** evaluating ***
06/02/2019 12:39:56 step: 280, epoch: 279, acc: 52.56410256410257, f1: 25.819558271035426, r: 0.27286355697280923
06/02/2019 12:39:56 *** epoch: 281 ***
06/02/2019 12:39:56 *** training ***
06/02/2019 12:39:57 step: 9245, epoch: 280, batch: 4, loss: 0.04520326480269432, acc: 100.0, f1: 100.0, r: 0.7704610847009143
06/02/2019 12:39:57 step: 9250, epoch: 280, batch: 9, loss: 0.1714475452899933, acc: 100.0, f1: 100.0, r: 0.7473388486328497
06/02/2019 12:39:58 step: 9255, epoch: 280, batch: 14, loss: 0.03525691479444504, acc: 100.0, f1: 100.0, r: 0.7531678406482545
06/02/2019 12:39:58 step: 9260, epoch: 280, batch: 19, loss: 0.03959406167268753, acc: 100.0, f1: 100.0, r: 0.6007082819960392
06/02/2019 12:39:59 step: 9265, epoch: 280, batch: 24, loss: 0.03852183744311333, acc: 100.0, f1: 100.0, r: 0.6702324145595554
06/02/2019 12:40:00 step: 9270, epoch: 280, batch: 29, loss: 0.0440993532538414, acc: 100.0, f1: 100.0, r: 0.7809081809342255
06/02/2019 12:40:00 *** evaluating ***
06/02/2019 12:40:00 step: 281, epoch: 280, acc: 52.13675213675214, f1: 25.009149875913984, r: 0.2707719917590542
06/02/2019 12:40:00 *** epoch: 282 ***
06/02/2019 12:40:00 *** training ***
06/02/2019 12:40:01 step: 9278, epoch: 281, batch: 4, loss: 0.0650240033864975, acc: 100.0, f1: 100.0, r: 0.7743628976669767
06/02/2019 12:40:01 step: 9283, epoch: 281, batch: 9, loss: 0.05184607952833176, acc: 100.0, f1: 100.0, r: 0.8101968645271875
06/02/2019 12:40:02 step: 9288, epoch: 281, batch: 14, loss: 0.03419571369886398, acc: 100.0, f1: 100.0, r: 0.773477701594085
06/02/2019 12:40:03 step: 9293, epoch: 281, batch: 19, loss: 0.04599590227007866, acc: 100.0, f1: 100.0, r: 0.7238661745972291
06/02/2019 12:40:03 step: 9298, epoch: 281, batch: 24, loss: 0.048990823328495026, acc: 100.0, f1: 100.0, r: 0.7126533793456922
06/02/2019 12:40:04 step: 9303, epoch: 281, batch: 29, loss: 0.033271607011556625, acc: 100.0, f1: 100.0, r: 0.7530088862544695
06/02/2019 12:40:04 *** evaluating ***
06/02/2019 12:40:04 step: 282, epoch: 281, acc: 53.41880341880342, f1: 26.374251148257148, r: 0.2580837819908909
06/02/2019 12:40:04 *** epoch: 283 ***
06/02/2019 12:40:04 *** training ***
06/02/2019 12:40:05 step: 9311, epoch: 282, batch: 4, loss: 0.048736583441495895, acc: 100.0, f1: 100.0, r: 0.8021295473049419
06/02/2019 12:40:05 step: 9316, epoch: 282, batch: 9, loss: 0.023689834401011467, acc: 100.0, f1: 100.0, r: 0.8335266898722894
06/02/2019 12:40:06 step: 9321, epoch: 282, batch: 14, loss: 0.05199267715215683, acc: 100.0, f1: 100.0, r: 0.7777538553223752
06/02/2019 12:40:07 step: 9326, epoch: 282, batch: 19, loss: 0.03887343034148216, acc: 100.0, f1: 100.0, r: 0.7741674761822965
06/02/2019 12:40:07 step: 9331, epoch: 282, batch: 24, loss: 0.05494895949959755, acc: 100.0, f1: 100.0, r: 0.8090311902439514
06/02/2019 12:40:08 step: 9336, epoch: 282, batch: 29, loss: 0.03727428987622261, acc: 100.0, f1: 100.0, r: 0.8069380595335928
06/02/2019 12:40:08 *** evaluating ***
06/02/2019 12:40:08 step: 283, epoch: 282, acc: 52.13675213675214, f1: 24.203000259622247, r: 0.27361525622835847
06/02/2019 12:40:08 *** epoch: 284 ***
06/02/2019 12:40:08 *** training ***
06/02/2019 12:40:09 step: 9344, epoch: 283, batch: 4, loss: 0.04964638128876686, acc: 100.0, f1: 100.0, r: 0.7594577882544844
06/02/2019 12:40:10 step: 9349, epoch: 283, batch: 9, loss: 0.18727268278598785, acc: 100.0, f1: 100.0, r: 0.7380518342895456
06/02/2019 12:40:10 step: 9354, epoch: 283, batch: 14, loss: 0.07145754247903824, acc: 100.0, f1: 100.0, r: 0.7760824721043511
06/02/2019 12:40:11 step: 9359, epoch: 283, batch: 19, loss: 0.0433024987578392, acc: 100.0, f1: 100.0, r: 0.6634651986020496
06/02/2019 12:40:11 step: 9364, epoch: 283, batch: 24, loss: 0.030575908720493317, acc: 100.0, f1: 100.0, r: 0.8491526968864922
06/02/2019 12:40:12 step: 9369, epoch: 283, batch: 29, loss: 0.057156700640916824, acc: 100.0, f1: 100.0, r: 0.7398844954147202
06/02/2019 12:40:12 *** evaluating ***
06/02/2019 12:40:12 step: 284, epoch: 283, acc: 54.27350427350427, f1: 26.58169052596297, r: 0.2764882257789628
06/02/2019 12:40:12 *** epoch: 285 ***
06/02/2019 12:40:12 *** training ***
06/02/2019 12:40:13 step: 9377, epoch: 284, batch: 4, loss: 0.03807394951581955, acc: 100.0, f1: 100.0, r: 0.707715772820066
06/02/2019 12:40:14 step: 9382, epoch: 284, batch: 9, loss: 0.05262855067849159, acc: 100.0, f1: 100.0, r: 0.8603773691873269
06/02/2019 12:40:14 step: 9387, epoch: 284, batch: 14, loss: 0.2489844411611557, acc: 98.4375, f1: 97.38775510204081, r: 0.6398668595760655
06/02/2019 12:40:15 step: 9392, epoch: 284, batch: 19, loss: 0.021209971979260445, acc: 100.0, f1: 100.0, r: 0.6928289891338959
06/02/2019 12:40:16 step: 9397, epoch: 284, batch: 24, loss: 0.053216978907585144, acc: 100.0, f1: 100.0, r: 0.7410799417467262
06/02/2019 12:40:16 step: 9402, epoch: 284, batch: 29, loss: 0.056459300220012665, acc: 100.0, f1: 100.0, r: 0.7859906935449213
06/02/2019 12:40:17 *** evaluating ***
06/02/2019 12:40:17 step: 285, epoch: 284, acc: 55.12820512820513, f1: 27.662925009018725, r: 0.2690800848078198
06/02/2019 12:40:17 *** epoch: 286 ***
06/02/2019 12:40:17 *** training ***
06/02/2019 12:40:17 step: 9410, epoch: 285, batch: 4, loss: 0.056379228830337524, acc: 100.0, f1: 100.0, r: 0.740581183500515
06/02/2019 12:40:18 step: 9415, epoch: 285, batch: 9, loss: 0.06024860590696335, acc: 98.4375, f1: 99.01392826328116, r: 0.7140482722290226
06/02/2019 12:40:19 step: 9420, epoch: 285, batch: 14, loss: 0.051915526390075684, acc: 100.0, f1: 100.0, r: 0.77665302121485
06/02/2019 12:40:19 step: 9425, epoch: 285, batch: 19, loss: 0.04087736830115318, acc: 100.0, f1: 100.0, r: 0.7730976406878599
06/02/2019 12:40:20 step: 9430, epoch: 285, batch: 24, loss: 0.058119408786296844, acc: 98.4375, f1: 98.58585858585859, r: 0.8366967280641009
06/02/2019 12:40:20 step: 9435, epoch: 285, batch: 29, loss: 0.04850906506180763, acc: 100.0, f1: 100.0, r: 0.6444576583025469
06/02/2019 12:40:21 *** evaluating ***
06/02/2019 12:40:21 step: 286, epoch: 285, acc: 52.991452991452995, f1: 26.40612085381927, r: 0.28290456367054345
06/02/2019 12:40:21 *** epoch: 287 ***
06/02/2019 12:40:21 *** training ***
06/02/2019 12:40:21 step: 9443, epoch: 286, batch: 4, loss: 0.043780617415905, acc: 100.0, f1: 100.0, r: 0.853794746345094
06/02/2019 12:40:22 step: 9448, epoch: 286, batch: 9, loss: 0.048041388392448425, acc: 100.0, f1: 100.0, r: 0.7971158550757049
06/02/2019 12:40:22 step: 9453, epoch: 286, batch: 14, loss: 0.045558225363492966, acc: 100.0, f1: 100.0, r: 0.6311038460529236
06/02/2019 12:40:23 step: 9458, epoch: 286, batch: 19, loss: 0.047209154814481735, acc: 100.0, f1: 100.0, r: 0.6938687939136067
06/02/2019 12:40:24 step: 9463, epoch: 286, batch: 24, loss: 0.22868958115577698, acc: 100.0, f1: 100.0, r: 0.7925922736231298
06/02/2019 12:40:24 step: 9468, epoch: 286, batch: 29, loss: 0.08220751583576202, acc: 100.0, f1: 100.0, r: 0.7722355851157027
06/02/2019 12:40:25 *** evaluating ***
06/02/2019 12:40:25 step: 287, epoch: 286, acc: 54.700854700854705, f1: 27.59506719513165, r: 0.2899165783956035
06/02/2019 12:40:25 *** epoch: 288 ***
06/02/2019 12:40:25 *** training ***
06/02/2019 12:40:25 step: 9476, epoch: 287, batch: 4, loss: 0.04542966187000275, acc: 100.0, f1: 100.0, r: 0.6732324558212398
06/02/2019 12:40:26 step: 9481, epoch: 287, batch: 9, loss: 0.10290373861789703, acc: 98.4375, f1: 98.70370370370371, r: 0.823917265557232
06/02/2019 12:40:26 step: 9486, epoch: 287, batch: 14, loss: 0.0917552039027214, acc: 100.0, f1: 100.0, r: 0.815877133674497
06/02/2019 12:40:27 step: 9491, epoch: 287, batch: 19, loss: 0.07892981171607971, acc: 100.0, f1: 100.0, r: 0.7624806197994987
06/02/2019 12:40:28 step: 9496, epoch: 287, batch: 24, loss: 0.06500089913606644, acc: 100.0, f1: 100.0, r: 0.6752997147336985
06/02/2019 12:40:28 step: 9501, epoch: 287, batch: 29, loss: 0.0695018470287323, acc: 100.0, f1: 100.0, r: 0.6865517653066653
06/02/2019 12:40:29 *** evaluating ***
06/02/2019 12:40:29 step: 288, epoch: 287, acc: 54.27350427350427, f1: 26.956850944754173, r: 0.27878333597565264
06/02/2019 12:40:29 *** epoch: 289 ***
06/02/2019 12:40:29 *** training ***
06/02/2019 12:40:29 step: 9509, epoch: 288, batch: 4, loss: 0.07222428917884827, acc: 100.0, f1: 100.0, r: 0.7219656150357144
06/02/2019 12:40:30 step: 9514, epoch: 288, batch: 9, loss: 0.06560948491096497, acc: 100.0, f1: 100.0, r: 0.7882647153213935
06/02/2019 12:40:31 step: 9519, epoch: 288, batch: 14, loss: 0.04914303123950958, acc: 100.0, f1: 100.0, r: 0.7188758251554724
06/02/2019 12:40:31 step: 9524, epoch: 288, batch: 19, loss: 0.033541202545166016, acc: 100.0, f1: 100.0, r: 0.6938888042319674
06/02/2019 12:40:32 step: 9529, epoch: 288, batch: 24, loss: 0.047631409019231796, acc: 100.0, f1: 100.0, r: 0.69363757141052
06/02/2019 12:40:32 step: 9534, epoch: 288, batch: 29, loss: 0.09799814224243164, acc: 98.4375, f1: 98.42118665648077, r: 0.776483685907784
06/02/2019 12:40:33 *** evaluating ***
06/02/2019 12:40:33 step: 289, epoch: 288, acc: 55.55555555555556, f1: 28.692164153475698, r: 0.285200559356341
06/02/2019 12:40:33 *** epoch: 290 ***
06/02/2019 12:40:33 *** training ***
06/02/2019 12:40:33 step: 9542, epoch: 289, batch: 4, loss: 0.04850148409605026, acc: 100.0, f1: 100.0, r: 0.7618864615269832
06/02/2019 12:40:34 step: 9547, epoch: 289, batch: 9, loss: 0.05510052666068077, acc: 98.4375, f1: 96.76470588235294, r: 0.7642892594018719
06/02/2019 12:40:35 step: 9552, epoch: 289, batch: 14, loss: 0.0479854978621006, acc: 100.0, f1: 100.0, r: 0.6698322892939532
06/02/2019 12:40:35 step: 9557, epoch: 289, batch: 19, loss: 0.034957122057676315, acc: 100.0, f1: 100.0, r: 0.7137721049134207
06/02/2019 12:40:36 step: 9562, epoch: 289, batch: 24, loss: 0.0328652560710907, acc: 100.0, f1: 100.0, r: 0.8049100962477784
06/02/2019 12:40:36 step: 9567, epoch: 289, batch: 29, loss: 0.05609727278351784, acc: 98.4375, f1: 99.2292490118577, r: 0.6859902923351981
06/02/2019 12:40:37 *** evaluating ***
06/02/2019 12:40:37 step: 290, epoch: 289, acc: 53.41880341880342, f1: 25.803908126631736, r: 0.2774680104555547
06/02/2019 12:40:37 *** epoch: 291 ***
06/02/2019 12:40:37 *** training ***
06/02/2019 12:40:37 step: 9575, epoch: 290, batch: 4, loss: 0.057928409427404404, acc: 100.0, f1: 100.0, r: 0.8209070000972023
06/02/2019 12:40:38 step: 9580, epoch: 290, batch: 9, loss: 0.05133724585175514, acc: 100.0, f1: 100.0, r: 0.6827022947655533
06/02/2019 12:40:39 step: 9585, epoch: 290, batch: 14, loss: 0.03716270625591278, acc: 100.0, f1: 100.0, r: 0.654385224109654
06/02/2019 12:40:39 step: 9590, epoch: 290, batch: 19, loss: 0.03107760287821293, acc: 100.0, f1: 100.0, r: 0.6659858716792634
06/02/2019 12:40:40 step: 9595, epoch: 290, batch: 24, loss: 0.08540207147598267, acc: 100.0, f1: 100.0, r: 0.7598665018885543
06/02/2019 12:40:40 step: 9600, epoch: 290, batch: 29, loss: 0.1958010196685791, acc: 100.0, f1: 100.0, r: 0.7561608722733619
06/02/2019 12:40:41 *** evaluating ***
06/02/2019 12:40:41 step: 291, epoch: 290, acc: 52.991452991452995, f1: 25.157492947985904, r: 0.2685382486514147
06/02/2019 12:40:41 *** epoch: 292 ***
06/02/2019 12:40:41 *** training ***
06/02/2019 12:40:41 step: 9608, epoch: 291, batch: 4, loss: 0.05846557021141052, acc: 100.0, f1: 100.0, r: 0.6456985718823288
06/02/2019 12:40:42 step: 9613, epoch: 291, batch: 9, loss: 0.06842145323753357, acc: 100.0, f1: 100.0, r: 0.741186950712676
06/02/2019 12:40:43 step: 9618, epoch: 291, batch: 14, loss: 0.09155946969985962, acc: 100.0, f1: 100.0, r: 0.7975477060148266
06/02/2019 12:40:43 step: 9623, epoch: 291, batch: 19, loss: 0.0633603185415268, acc: 100.0, f1: 100.0, r: 0.8279648928121353
06/02/2019 12:40:44 step: 9628, epoch: 291, batch: 24, loss: 0.0594358891248703, acc: 100.0, f1: 100.0, r: 0.7646824013826874
06/02/2019 12:40:44 step: 9633, epoch: 291, batch: 29, loss: 0.05101087689399719, acc: 100.0, f1: 100.0, r: 0.5910668861918296
06/02/2019 12:40:45 *** evaluating ***
06/02/2019 12:40:45 step: 292, epoch: 291, acc: 55.12820512820513, f1: 29.176132702334023, r: 0.27627862400783354
06/02/2019 12:40:45 *** epoch: 293 ***
06/02/2019 12:40:45 *** training ***
06/02/2019 12:40:46 step: 9641, epoch: 292, batch: 4, loss: 0.2147570550441742, acc: 100.0, f1: 100.0, r: 0.6958280143548927
06/02/2019 12:40:46 step: 9646, epoch: 292, batch: 9, loss: 0.18769991397857666, acc: 100.0, f1: 100.0, r: 0.7876918226521302
06/02/2019 12:40:47 step: 9651, epoch: 292, batch: 14, loss: 0.06836235523223877, acc: 100.0, f1: 100.0, r: 0.8181855307449251
06/02/2019 12:40:48 step: 9656, epoch: 292, batch: 19, loss: 0.046778880059719086, acc: 100.0, f1: 100.0, r: 0.7173963632134124
06/02/2019 12:40:48 step: 9661, epoch: 292, batch: 24, loss: 0.05601942539215088, acc: 100.0, f1: 100.0, r: 0.6373187773111657
06/02/2019 12:40:49 step: 9666, epoch: 292, batch: 29, loss: 0.04636646807193756, acc: 100.0, f1: 100.0, r: 0.7549785150290825
06/02/2019 12:40:49 *** evaluating ***
06/02/2019 12:40:49 step: 293, epoch: 292, acc: 52.13675213675214, f1: 25.508941290191288, r: 0.2655953101439475
06/02/2019 12:40:49 *** epoch: 294 ***
06/02/2019 12:40:49 *** training ***
06/02/2019 12:40:50 step: 9674, epoch: 293, batch: 4, loss: 0.04372032359242439, acc: 100.0, f1: 100.0, r: 0.7288752708543379
06/02/2019 12:40:50 step: 9679, epoch: 293, batch: 9, loss: 0.05858888477087021, acc: 100.0, f1: 100.0, r: 0.834393118757031
06/02/2019 12:40:51 step: 9684, epoch: 293, batch: 14, loss: 0.19065268337726593, acc: 100.0, f1: 100.0, r: 0.7948498548203603
06/02/2019 12:40:52 step: 9689, epoch: 293, batch: 19, loss: 0.05741610378026962, acc: 100.0, f1: 100.0, r: 0.6853380387694202
06/02/2019 12:40:52 step: 9694, epoch: 293, batch: 24, loss: 0.07250355929136276, acc: 100.0, f1: 100.0, r: 0.6943978730456799
06/02/2019 12:40:53 step: 9699, epoch: 293, batch: 29, loss: 0.06318420171737671, acc: 100.0, f1: 100.0, r: 0.6956409804024147
06/02/2019 12:40:53 *** evaluating ***
06/02/2019 12:40:53 step: 294, epoch: 293, acc: 53.84615384615385, f1: 25.565210668366007, r: 0.27291676998629594
06/02/2019 12:40:53 *** epoch: 295 ***
06/02/2019 12:40:53 *** training ***
06/02/2019 12:40:54 step: 9707, epoch: 294, batch: 4, loss: 0.048677120357751846, acc: 100.0, f1: 100.0, r: 0.8262384262936131
06/02/2019 12:40:54 step: 9712, epoch: 294, batch: 9, loss: 0.03701861947774887, acc: 100.0, f1: 100.0, r: 0.7224489778743428
06/02/2019 12:40:55 step: 9717, epoch: 294, batch: 14, loss: 0.05743889510631561, acc: 98.4375, f1: 97.79158040027606, r: 0.766279172993146
06/02/2019 12:40:56 step: 9722, epoch: 294, batch: 19, loss: 0.05112215131521225, acc: 100.0, f1: 100.0, r: 0.634129686919924
06/02/2019 12:40:56 step: 9727, epoch: 294, batch: 24, loss: 0.04669975861907005, acc: 100.0, f1: 100.0, r: 0.77381513711321
06/02/2019 12:40:57 step: 9732, epoch: 294, batch: 29, loss: 0.05894414708018303, acc: 100.0, f1: 100.0, r: 0.8259499071487886
06/02/2019 12:40:57 *** evaluating ***
06/02/2019 12:40:57 step: 295, epoch: 294, acc: 52.56410256410257, f1: 24.680856777630968, r: 0.2733210273658382
06/02/2019 12:40:57 *** epoch: 296 ***
06/02/2019 12:40:57 *** training ***
06/02/2019 12:40:58 step: 9740, epoch: 295, batch: 4, loss: 0.06377583742141724, acc: 98.4375, f1: 94.61697722567288, r: 0.6515238778879034
06/02/2019 12:40:58 step: 9745, epoch: 295, batch: 9, loss: 0.07941729575395584, acc: 100.0, f1: 100.0, r: 0.718137105537648
06/02/2019 12:40:59 step: 9750, epoch: 295, batch: 14, loss: 0.09588328748941422, acc: 98.4375, f1: 96.84210526315789, r: 0.7713686210797353
06/02/2019 12:41:00 step: 9755, epoch: 295, batch: 19, loss: 0.06921767443418503, acc: 100.0, f1: 100.0, r: 0.7319843292946095
06/02/2019 12:41:00 step: 9760, epoch: 295, batch: 24, loss: 0.05771956592798233, acc: 100.0, f1: 100.0, r: 0.7032322047377673
06/02/2019 12:41:01 step: 9765, epoch: 295, batch: 29, loss: 0.06327619403600693, acc: 100.0, f1: 100.0, r: 0.7750676398489548
06/02/2019 12:41:01 *** evaluating ***
06/02/2019 12:41:02 step: 296, epoch: 295, acc: 55.98290598290598, f1: 28.676508887846307, r: 0.29145944534775986
06/02/2019 12:41:02 *** epoch: 297 ***
06/02/2019 12:41:02 *** training ***
06/02/2019 12:41:02 step: 9773, epoch: 296, batch: 4, loss: 0.04701804369688034, acc: 100.0, f1: 100.0, r: 0.660759003412018
06/02/2019 12:41:03 step: 9778, epoch: 296, batch: 9, loss: 0.05678476765751839, acc: 100.0, f1: 100.0, r: 0.7323183874781216
06/02/2019 12:41:04 step: 9783, epoch: 296, batch: 14, loss: 0.09492432326078415, acc: 100.0, f1: 100.0, r: 0.8247162655448788
06/02/2019 12:41:04 step: 9788, epoch: 296, batch: 19, loss: 0.05779116228222847, acc: 100.0, f1: 100.0, r: 0.7919860332900531
06/02/2019 12:41:05 step: 9793, epoch: 296, batch: 24, loss: 0.04713110625743866, acc: 100.0, f1: 100.0, r: 0.6293199898356272
06/02/2019 12:41:05 step: 9798, epoch: 296, batch: 29, loss: 0.06916755437850952, acc: 100.0, f1: 100.0, r: 0.7938546971076543
06/02/2019 12:41:06 *** evaluating ***
06/02/2019 12:41:06 step: 297, epoch: 296, acc: 55.12820512820513, f1: 27.805036344755973, r: 0.26478437456584425
06/02/2019 12:41:06 *** epoch: 298 ***
06/02/2019 12:41:06 *** training ***
06/02/2019 12:41:07 step: 9806, epoch: 297, batch: 4, loss: 0.06763873994350433, acc: 100.0, f1: 100.0, r: 0.7940059189755014
06/02/2019 12:41:07 step: 9811, epoch: 297, batch: 9, loss: 0.04722200334072113, acc: 100.0, f1: 100.0, r: 0.6856317497439823
06/02/2019 12:41:08 step: 9816, epoch: 297, batch: 14, loss: 0.04498997703194618, acc: 100.0, f1: 100.0, r: 0.7893393135206688
06/02/2019 12:41:08 step: 9821, epoch: 297, batch: 19, loss: 0.05579282343387604, acc: 100.0, f1: 100.0, r: 0.7749652466234125
06/02/2019 12:41:09 step: 9826, epoch: 297, batch: 24, loss: 0.03695455193519592, acc: 100.0, f1: 100.0, r: 0.752676547534454
06/02/2019 12:41:10 step: 9831, epoch: 297, batch: 29, loss: 0.05482970550656319, acc: 100.0, f1: 100.0, r: 0.7495256722628879
06/02/2019 12:41:10 *** evaluating ***
06/02/2019 12:41:10 step: 298, epoch: 297, acc: 52.56410256410257, f1: 26.296589804262606, r: 0.27138372460186566
06/02/2019 12:41:10 *** epoch: 299 ***
06/02/2019 12:41:10 *** training ***
06/02/2019 12:41:11 step: 9839, epoch: 298, batch: 4, loss: 0.0512358658015728, acc: 100.0, f1: 100.0, r: 0.697568785538236
06/02/2019 12:41:11 step: 9844, epoch: 298, batch: 9, loss: 0.05798790976405144, acc: 100.0, f1: 100.0, r: 0.6508968546852366
06/02/2019 12:41:12 step: 9849, epoch: 298, batch: 14, loss: 0.058457791805267334, acc: 100.0, f1: 100.0, r: 0.7762696017312364
06/02/2019 12:41:13 step: 9854, epoch: 298, batch: 19, loss: 0.05321677029132843, acc: 100.0, f1: 100.0, r: 0.7773739582581435
06/02/2019 12:41:13 step: 9859, epoch: 298, batch: 24, loss: 0.18660074472427368, acc: 100.0, f1: 100.0, r: 0.6859740634584496
06/02/2019 12:41:14 step: 9864, epoch: 298, batch: 29, loss: 0.06606899201869965, acc: 100.0, f1: 100.0, r: 0.6891341944863479
06/02/2019 12:41:14 *** evaluating ***
06/02/2019 12:41:14 step: 299, epoch: 298, acc: 52.991452991452995, f1: 25.91424191237849, r: 0.26886316359529094
06/02/2019 12:41:14 *** epoch: 300 ***
06/02/2019 12:41:14 *** training ***
06/02/2019 12:41:15 step: 9872, epoch: 299, batch: 4, loss: 0.03144899755716324, acc: 100.0, f1: 100.0, r: 0.6831343202999095
06/02/2019 12:41:16 step: 9877, epoch: 299, batch: 9, loss: 0.06844273209571838, acc: 98.4375, f1: 99.07493061979649, r: 0.7436277323786126
06/02/2019 12:41:16 step: 9882, epoch: 299, batch: 14, loss: 0.056534819304943085, acc: 100.0, f1: 100.0, r: 0.7100839742041961
06/02/2019 12:41:17 step: 9887, epoch: 299, batch: 19, loss: 0.06945951282978058, acc: 100.0, f1: 100.0, r: 0.6525418356839093
06/02/2019 12:41:17 step: 9892, epoch: 299, batch: 24, loss: 0.06230388954281807, acc: 100.0, f1: 100.0, r: 0.672007509804399
06/02/2019 12:41:18 step: 9897, epoch: 299, batch: 29, loss: 0.04730655997991562, acc: 100.0, f1: 100.0, r: 0.8034050352903611
06/02/2019 12:41:18 *** evaluating ***
06/02/2019 12:41:19 step: 300, epoch: 299, acc: 51.28205128205128, f1: 25.256813905821524, r: 0.25698980466313587
06/02/2019 12:41:19 
*** Best acc model ***
epoch: 25
acc: 60.256410256410255
f1: 31.48534798534798
corr: 0.3820642382899007
06/02/2019 12:41:19 Loading Test Data
06/02/2019 12:41:19 load data from data/word2vec_temp/test_text.npy, data/word2vec_temp/test_label.npy, training: False
06/02/2019 12:41:39 loaded. total len: 2228
06/02/2019 12:41:39 Test: length: 2228, total batch: 35, batch size: 64
06/02/2019 12:41:40 
*** Test Result ***
acc: 51.28205128205128
f1: 25.256813905821524
corr: 0.25698980466313587
