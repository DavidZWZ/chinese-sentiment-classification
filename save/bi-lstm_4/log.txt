06/02/2019 10:03:52 {'input_path': 'data/word2vec_temp', 'output_path': 'save/bi-lstm_4', 'gpu': True, 'seed': 20000125, 'display_per_batch': 5, 'optimizer': 'adagrad', 'lr': 0.01, 'lr_decay': 0, 'weight_decay': 0.0001, 'momentum': 0.985, 'num_epochs': 300, 'batch_size': 64, 'num_labels': 8, 'embedding_size': 300, 'type': 'rnn', 'rnn': {'type': 'lstm', 'bidirectional': True, 'rnn_hidden_size': 512, 'mlp_hidden_size': 1024, 'dropout': 0.5, 'p_coefficient': 0.3, 'num_layers': 1, 'param_da': 350, 'param_r': 30, 'loss': 'cross_entropy'}}
06/02/2019 10:03:52 Loading Train Data
06/02/2019 10:03:52 load data from data/word2vec_temp/train_text.npy, data/word2vec_temp/train_label.npy, training: True
06/02/2019 10:04:15 loaded. total len: 2342
06/02/2019 10:04:15 Train: length: 2108, total batch: 33, batch size: 64
06/02/2019 10:04:15 Dev: length: 234, total batch: 4, batch size: 64
06/02/2019 10:04:15 Loading model rnn
06/02/2019 10:04:26 *** epoch: 1 ***
06/02/2019 10:04:26 *** training ***
06/02/2019 10:04:29 step: 5, epoch: 0, batch: 4, loss: 12.875097274780273, acc: 17.1875, f1: 3.6666666666666665, r: 0.07074832036790339
06/02/2019 10:04:31 step: 10, epoch: 0, batch: 9, loss: 2.1379640102386475, acc: 31.25, f1: 6.41025641025641, r: -0.00067478209824461
06/02/2019 10:04:33 step: 15, epoch: 0, batch: 14, loss: 1.9748643636703491, acc: 32.8125, f1: 10.0, r: 0.11958678136178086
06/02/2019 10:04:36 step: 20, epoch: 0, batch: 19, loss: 1.8097237348556519, acc: 35.9375, f1: 9.655603065069219, r: -0.018003813908069245
06/02/2019 10:04:38 step: 25, epoch: 0, batch: 24, loss: 1.8294864892959595, acc: 37.5, f1: 9.598056089905842, r: 0.04547108997702446
06/02/2019 10:04:40 step: 30, epoch: 0, batch: 29, loss: 2.046025037765503, acc: 28.125, f1: 7.305194805194805, r: -0.0008643745508797734
06/02/2019 10:04:41 *** evaluating ***
06/02/2019 10:04:42 step: 1, epoch: 0, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.020404097243582907
06/02/2019 10:04:42 *** epoch: 2 ***
06/02/2019 10:04:42 *** training ***
06/02/2019 10:04:44 step: 38, epoch: 1, batch: 4, loss: 1.6529080867767334, acc: 39.0625, f1: 11.488812392426851, r: 0.04993737985768322
06/02/2019 10:04:46 step: 43, epoch: 1, batch: 9, loss: 2.0860300064086914, acc: 39.0625, f1: 11.574074074074074, r: 0.04869090306340298
06/02/2019 10:04:49 step: 48, epoch: 1, batch: 14, loss: 1.7183105945587158, acc: 28.125, f1: 11.152882205513784, r: 0.07276810859592484
06/02/2019 10:04:51 step: 53, epoch: 1, batch: 19, loss: 1.950116515159607, acc: 29.6875, f1: 8.469387755102042, r: 0.002342347415615018
06/02/2019 10:04:53 step: 58, epoch: 1, batch: 24, loss: 1.609390377998352, acc: 42.1875, f1: 9.12063953488372, r: 0.04579902369659061
06/02/2019 10:04:55 step: 63, epoch: 1, batch: 29, loss: 1.7563124895095825, acc: 35.9375, f1: 6.609195402298851, r: 0.09256311211729698
06/02/2019 10:04:57 *** evaluating ***
06/02/2019 10:04:57 step: 2, epoch: 1, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.044603746062482706
06/02/2019 10:04:57 *** epoch: 3 ***
06/02/2019 10:04:57 *** training ***
06/02/2019 10:04:59 step: 71, epoch: 2, batch: 4, loss: 1.8148378133773804, acc: 25.0, f1: 5.714285714285714, r: -0.07185498269698463
06/02/2019 10:05:02 step: 76, epoch: 2, batch: 9, loss: 1.8547776937484741, acc: 29.6875, f1: 8.015873015873016, r: -0.02326746088572008
06/02/2019 10:05:04 step: 81, epoch: 2, batch: 14, loss: 1.5674419403076172, acc: 48.4375, f1: 11.597542242703533, r: 0.06231092696410222
06/02/2019 10:05:06 step: 86, epoch: 2, batch: 19, loss: 1.686980128288269, acc: 35.9375, f1: 7.55336617405583, r: 0.04868124476868614
06/02/2019 10:05:09 step: 91, epoch: 2, batch: 24, loss: 1.86874258518219, acc: 25.0, f1: 6.476244343891404, r: -0.028914136270763342
06/02/2019 10:05:11 step: 96, epoch: 2, batch: 29, loss: 1.7183363437652588, acc: 43.75, f1: 8.79120879120879, r: 0.03278250153980051
06/02/2019 10:05:12 *** evaluating ***
06/02/2019 10:05:12 step: 3, epoch: 2, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.10716903108030948
06/02/2019 10:05:12 *** epoch: 4 ***
06/02/2019 10:05:12 *** training ***
06/02/2019 10:05:15 step: 104, epoch: 3, batch: 4, loss: 1.7192999124526978, acc: 28.125, f1: 8.112244897959183, r: 0.151573624613803
06/02/2019 10:05:17 step: 109, epoch: 3, batch: 9, loss: 1.9096457958221436, acc: 31.25, f1: 5.952380952380952, r: -0.011292653382217126
06/02/2019 10:05:19 step: 114, epoch: 3, batch: 14, loss: 1.7465695142745972, acc: 40.625, f1: 7.222222222222221, r: -0.010199600184474227
06/02/2019 10:05:22 step: 119, epoch: 3, batch: 19, loss: 1.913204312324524, acc: 32.8125, f1: 7.0588235294117645, r: 0.03109369323595082
06/02/2019 10:05:23 step: 124, epoch: 3, batch: 24, loss: 1.6422008275985718, acc: 45.3125, f1: 8.90937019969278, r: 0.0691555770638646
06/02/2019 10:05:26 step: 129, epoch: 3, batch: 29, loss: 1.7490012645721436, acc: 31.25, f1: 6.802721088435374, r: 0.08155004320257084
06/02/2019 10:05:27 *** evaluating ***
06/02/2019 10:05:28 step: 4, epoch: 3, acc: 44.01709401709402, f1: 7.640949554896142, r: 0.08616382084223215
06/02/2019 10:05:28 *** epoch: 5 ***
06/02/2019 10:05:28 *** training ***
06/02/2019 10:05:30 step: 137, epoch: 4, batch: 4, loss: 1.7427231073379517, acc: 42.1875, f1: 7.5, r: 0.06926240223158947
06/02/2019 10:05:32 step: 142, epoch: 4, batch: 9, loss: 1.4264965057373047, acc: 50.0, f1: 11.11111111111111, r: 0.13196772278317803
06/02/2019 10:05:33 step: 147, epoch: 4, batch: 14, loss: 1.6626044511795044, acc: 39.0625, f1: 8.025682182985554, r: 0.17769253729805698
06/02/2019 10:05:35 step: 152, epoch: 4, batch: 19, loss: 1.5752164125442505, acc: 37.5, f1: 10.454545454545453, r: 0.1183292314629293
06/02/2019 10:05:38 step: 157, epoch: 4, batch: 24, loss: 1.7574427127838135, acc: 37.5, f1: 6.8181818181818175, r: 0.0832813130842994
06/02/2019 10:05:40 step: 162, epoch: 4, batch: 29, loss: 1.6037582159042358, acc: 42.1875, f1: 8.571428571428571, r: 0.054282826442395085
06/02/2019 10:05:42 *** evaluating ***
06/02/2019 10:05:42 step: 5, epoch: 4, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.18901738947780827
06/02/2019 10:05:42 *** epoch: 6 ***
06/02/2019 10:05:42 *** training ***
06/02/2019 10:05:45 step: 170, epoch: 5, batch: 4, loss: 1.6270873546600342, acc: 34.375, f1: 9.183673469387756, r: 0.054657235037547776
06/02/2019 10:05:47 step: 175, epoch: 5, batch: 9, loss: 1.5182530879974365, acc: 42.1875, f1: 8.47723704866562, r: 0.16091722510837286
06/02/2019 10:05:49 step: 180, epoch: 5, batch: 14, loss: 1.6592345237731934, acc: 40.625, f1: 8.911064425770308, r: 0.10486716653391746
06/02/2019 10:05:51 step: 185, epoch: 5, batch: 19, loss: 1.7063939571380615, acc: 42.1875, f1: 8.667736757624397, r: 0.1218925964129303
06/02/2019 10:05:54 step: 190, epoch: 5, batch: 24, loss: 1.612136721611023, acc: 43.75, f1: 8.791208791208792, r: 0.029797302013472575
06/02/2019 10:05:56 step: 195, epoch: 5, batch: 29, loss: 1.762986421585083, acc: 31.25, f1: 6.802721088435374, r: 0.14152125584815622
06/02/2019 10:05:57 *** evaluating ***
06/02/2019 10:05:58 step: 6, epoch: 5, acc: 47.43589743589743, f1: 11.07253086419753, r: 0.1886343352886801
06/02/2019 10:05:58 *** epoch: 7 ***
06/02/2019 10:05:58 *** training ***
06/02/2019 10:06:00 step: 203, epoch: 6, batch: 4, loss: 1.6012994050979614, acc: 56.25, f1: 14.166666666666666, r: 0.09704820927539512
06/02/2019 10:06:02 step: 208, epoch: 6, batch: 9, loss: 1.6095327138900757, acc: 42.1875, f1: 11.16636528028933, r: 0.1981215931800568
06/02/2019 10:06:05 step: 213, epoch: 6, batch: 14, loss: 1.6181623935699463, acc: 35.9375, f1: 7.55336617405583, r: 0.1484562288581544
06/02/2019 10:06:07 step: 218, epoch: 6, batch: 19, loss: 1.7550276517868042, acc: 40.625, f1: 10.392156862745097, r: 0.1225554169602431
06/02/2019 10:06:08 step: 223, epoch: 6, batch: 24, loss: 1.7328460216522217, acc: 35.9375, f1: 11.73469387755102, r: 0.13354622411303074
06/02/2019 10:06:11 step: 228, epoch: 6, batch: 29, loss: 1.6740740537643433, acc: 34.375, f1: 11.968641114982578, r: 0.20111703524083963
06/02/2019 10:06:12 *** evaluating ***
06/02/2019 10:06:13 step: 7, epoch: 6, acc: 45.72649572649573, f1: 9.817813765182185, r: 0.22641868737884951
06/02/2019 10:06:13 *** epoch: 8 ***
06/02/2019 10:06:13 *** training ***
06/02/2019 10:06:15 step: 236, epoch: 7, batch: 4, loss: 1.7772363424301147, acc: 35.9375, f1: 14.14333175130517, r: 0.21884630361318075
06/02/2019 10:06:18 step: 241, epoch: 7, batch: 9, loss: 1.5942169427871704, acc: 39.0625, f1: 10.361067503924648, r: 0.19597915870126192
06/02/2019 10:06:20 step: 246, epoch: 7, batch: 14, loss: 1.8394547700881958, acc: 32.8125, f1: 7.0588235294117645, r: 0.10509272669206267
06/02/2019 10:06:22 step: 251, epoch: 7, batch: 19, loss: 1.4824678897857666, acc: 45.3125, f1: 8.90937019969278, r: 0.08945953127025613
06/02/2019 10:06:24 step: 256, epoch: 7, batch: 24, loss: 1.8412384986877441, acc: 34.375, f1: 9.609868043602983, r: 0.19484580504653942
06/02/2019 10:06:27 step: 261, epoch: 7, batch: 29, loss: 1.55113685131073, acc: 37.5, f1: 9.545957918050942, r: 0.12533650269002222
06/02/2019 10:06:28 *** evaluating ***
06/02/2019 10:06:29 step: 8, epoch: 7, acc: 48.717948717948715, f1: 12.285445086231617, r: 0.2574543688701564
06/02/2019 10:06:29 *** epoch: 9 ***
06/02/2019 10:06:29 *** training ***
06/02/2019 10:06:31 step: 269, epoch: 8, batch: 4, loss: 1.58568274974823, acc: 53.125, f1: 20.072463768115938, r: 0.2309570777263893
06/02/2019 10:06:34 step: 274, epoch: 8, batch: 9, loss: 1.5963554382324219, acc: 37.5, f1: 12.077922077922079, r: 0.1484348313321419
06/02/2019 10:06:36 step: 279, epoch: 8, batch: 14, loss: 1.5185290575027466, acc: 48.4375, f1: 16.535646516671186, r: 0.08928074255995892
06/02/2019 10:06:38 step: 284, epoch: 8, batch: 19, loss: 1.6124639511108398, acc: 35.9375, f1: 9.499999999999998, r: 0.20022456808433245
06/02/2019 10:06:40 step: 289, epoch: 8, batch: 24, loss: 1.8517005443572998, acc: 29.6875, f1: 14.426129426129425, r: 0.12224138618074297
06/02/2019 10:06:42 step: 294, epoch: 8, batch: 29, loss: 1.5551350116729736, acc: 48.4375, f1: 19.000000000000004, r: 0.22586172403639432
06/02/2019 10:06:44 *** evaluating ***
06/02/2019 10:06:44 step: 9, epoch: 8, acc: 40.17094017094017, f1: 12.199440820130475, r: 0.16379262099841913
06/02/2019 10:06:44 *** epoch: 10 ***
06/02/2019 10:06:44 *** training ***
06/02/2019 10:06:47 step: 302, epoch: 9, batch: 4, loss: 1.550662875175476, acc: 54.6875, f1: 23.435763321346844, r: 0.2693896653536399
06/02/2019 10:06:49 step: 307, epoch: 9, batch: 9, loss: 1.6312055587768555, acc: 42.1875, f1: 10.482374768089056, r: 0.1728342375094219
06/02/2019 10:06:51 step: 312, epoch: 9, batch: 14, loss: 1.499180793762207, acc: 37.5, f1: 9.30232558139535, r: 0.1928573308168253
06/02/2019 10:06:54 step: 317, epoch: 9, batch: 19, loss: 1.4599672555923462, acc: 53.125, f1: 15.105908584169454, r: 0.22952964468418932
06/02/2019 10:06:56 step: 322, epoch: 9, batch: 24, loss: 1.469764232635498, acc: 50.0, f1: 17.03477443609022, r: 0.2506847677092475
06/02/2019 10:06:58 step: 327, epoch: 9, batch: 29, loss: 1.6350986957550049, acc: 42.1875, f1: 9.300595238095239, r: 0.24827217621085573
06/02/2019 10:06:59 *** evaluating ***
06/02/2019 10:07:00 step: 10, epoch: 9, acc: 45.2991452991453, f1: 8.266272189349113, r: 0.23582842454680636
06/02/2019 10:07:00 *** epoch: 11 ***
06/02/2019 10:07:00 *** training ***
06/02/2019 10:07:02 step: 335, epoch: 10, batch: 4, loss: 1.6217539310455322, acc: 43.75, f1: 13.28976034858388, r: 0.19535844629560611
06/02/2019 10:07:04 step: 340, epoch: 10, batch: 9, loss: 1.4647427797317505, acc: 50.0, f1: 17.573696145124718, r: 0.2584451013781801
06/02/2019 10:07:06 step: 345, epoch: 10, batch: 14, loss: 1.4621365070343018, acc: 46.875, f1: 17.728303442589155, r: 0.25642793936456576
06/02/2019 10:07:09 step: 350, epoch: 10, batch: 19, loss: 1.6405514478683472, acc: 45.3125, f1: 14.583333333333334, r: 0.1983843322233616
06/02/2019 10:07:11 step: 355, epoch: 10, batch: 24, loss: 1.513710856437683, acc: 51.5625, f1: 14.766233766233766, r: 0.2532584544804322
06/02/2019 10:07:13 step: 360, epoch: 10, batch: 29, loss: 1.3990756273269653, acc: 57.8125, f1: 21.871779014636157, r: 0.2478016046706925
06/02/2019 10:07:14 *** evaluating ***
06/02/2019 10:07:15 step: 11, epoch: 10, acc: 52.56410256410257, f1: 14.897996804719183, r: 0.2531765288286641
06/02/2019 10:07:15 *** epoch: 12 ***
06/02/2019 10:07:15 *** training ***
06/02/2019 10:07:18 step: 368, epoch: 11, batch: 4, loss: 1.6060807704925537, acc: 43.75, f1: 13.860544217687073, r: 0.2389712027825402
06/02/2019 10:07:20 step: 373, epoch: 11, batch: 9, loss: 1.4752498865127563, acc: 45.3125, f1: 11.969777306468716, r: 0.2631281453415385
06/02/2019 10:07:22 step: 378, epoch: 11, batch: 14, loss: 1.4657652378082275, acc: 50.0, f1: 18.27956989247312, r: 0.34631072611183933
06/02/2019 10:07:24 step: 383, epoch: 11, batch: 19, loss: 1.6643834114074707, acc: 39.0625, f1: 17.251461988304094, r: 0.21781509757642203
06/02/2019 10:07:26 step: 388, epoch: 11, batch: 24, loss: 1.582262635231018, acc: 45.3125, f1: 15.77639751552795, r: 0.2456379987735718
06/02/2019 10:07:29 step: 393, epoch: 11, batch: 29, loss: 1.635711669921875, acc: 35.9375, f1: 16.3907753254103, r: 0.21079169114291263
06/02/2019 10:07:30 *** evaluating ***
06/02/2019 10:07:31 step: 12, epoch: 11, acc: 52.56410256410257, f1: 15.323529411764708, r: 0.2330262889226824
06/02/2019 10:07:31 *** epoch: 13 ***
06/02/2019 10:07:31 *** training ***
06/02/2019 10:07:33 step: 401, epoch: 12, batch: 4, loss: 1.6758660078048706, acc: 42.1875, f1: 12.02731092436975, r: 0.23980067824132
06/02/2019 10:07:35 step: 406, epoch: 12, batch: 9, loss: 1.596153974533081, acc: 42.1875, f1: 14.384920634920634, r: 0.2947948065772181
06/02/2019 10:07:37 step: 411, epoch: 12, batch: 14, loss: 1.518356442451477, acc: 37.5, f1: 12.496795967190703, r: 0.27855244729993767
06/02/2019 10:07:40 step: 416, epoch: 12, batch: 19, loss: 1.619870662689209, acc: 46.875, f1: 16.93127608825283, r: 0.23625361723103322
06/02/2019 10:07:42 step: 421, epoch: 12, batch: 24, loss: 1.5096513032913208, acc: 46.875, f1: 15.324675324675328, r: 0.21933324331502502
06/02/2019 10:07:45 step: 426, epoch: 12, batch: 29, loss: 1.434404730796814, acc: 48.4375, f1: 18.429189857761287, r: 0.2893646864056102
06/02/2019 10:07:46 *** evaluating ***
06/02/2019 10:07:46 step: 13, epoch: 12, acc: 55.55555555555556, f1: 17.141808842414235, r: 0.24231781374965847
06/02/2019 10:07:46 *** epoch: 14 ***
06/02/2019 10:07:46 *** training ***
06/02/2019 10:07:49 step: 434, epoch: 13, batch: 4, loss: 1.5026695728302002, acc: 42.1875, f1: 14.831315301240114, r: 0.22093035554780388
06/02/2019 10:07:51 step: 439, epoch: 13, batch: 9, loss: 1.6513296365737915, acc: 42.1875, f1: 14.270833333333332, r: 0.25856502526985287
06/02/2019 10:07:53 step: 444, epoch: 13, batch: 14, loss: 1.5464426279067993, acc: 42.1875, f1: 15.041035353535353, r: 0.2435179345513776
06/02/2019 10:07:56 step: 449, epoch: 13, batch: 19, loss: 1.4805024862289429, acc: 39.0625, f1: 13.006756756756758, r: 0.3117977219282908
06/02/2019 10:07:58 step: 454, epoch: 13, batch: 24, loss: 1.402616024017334, acc: 48.4375, f1: 28.074814969976252, r: 0.2978286146004713
06/02/2019 10:08:01 step: 459, epoch: 13, batch: 29, loss: 1.3819078207015991, acc: 56.25, f1: 16.994492525570415, r: 0.2899372348786734
06/02/2019 10:08:02 *** evaluating ***
06/02/2019 10:08:02 step: 14, epoch: 13, acc: 55.55555555555556, f1: 16.16521739130435, r: 0.26791119191804375
06/02/2019 10:08:02 *** epoch: 15 ***
06/02/2019 10:08:02 *** training ***
06/02/2019 10:08:05 step: 467, epoch: 14, batch: 4, loss: 1.4083232879638672, acc: 51.5625, f1: 17.966405740378345, r: 0.27626211215877755
06/02/2019 10:08:07 step: 472, epoch: 14, batch: 9, loss: 1.5466063022613525, acc: 43.75, f1: 16.143380429094716, r: 0.23840274966846273
06/02/2019 10:08:10 step: 477, epoch: 14, batch: 14, loss: 1.4567766189575195, acc: 48.4375, f1: 17.64346764346764, r: 0.33243046320543634
06/02/2019 10:08:12 step: 482, epoch: 14, batch: 19, loss: 1.2718994617462158, acc: 56.25, f1: 19.523809523809526, r: 0.2724562092728379
06/02/2019 10:08:14 step: 487, epoch: 14, batch: 24, loss: 1.507763147354126, acc: 45.3125, f1: 14.469092721834498, r: 0.2948925995997711
06/02/2019 10:08:16 step: 492, epoch: 14, batch: 29, loss: 1.3378551006317139, acc: 50.0, f1: 17.83359497645212, r: 0.2941665215558642
06/02/2019 10:08:17 *** evaluating ***
06/02/2019 10:08:18 step: 15, epoch: 14, acc: 56.837606837606835, f1: 16.733183188156193, r: 0.2999060382893701
06/02/2019 10:08:18 *** epoch: 16 ***
06/02/2019 10:08:18 *** training ***
06/02/2019 10:08:20 step: 500, epoch: 15, batch: 4, loss: 1.4089975357055664, acc: 48.4375, f1: 20.728291316526608, r: 0.30297120064408606
06/02/2019 10:08:23 step: 505, epoch: 15, batch: 9, loss: 1.3109923601150513, acc: 57.8125, f1: 24.590854860462706, r: 0.3729426514297703
06/02/2019 10:08:25 step: 510, epoch: 15, batch: 14, loss: 1.3658907413482666, acc: 60.9375, f1: 17.63888888888889, r: 0.21545982920248885
06/02/2019 10:08:27 step: 515, epoch: 15, batch: 19, loss: 1.3296127319335938, acc: 50.0, f1: 21.531921531921533, r: 0.2830813125691016
06/02/2019 10:08:29 step: 520, epoch: 15, batch: 24, loss: 1.5846548080444336, acc: 39.0625, f1: 12.5, r: 0.2470394509661076
06/02/2019 10:08:31 step: 525, epoch: 15, batch: 29, loss: 1.4008558988571167, acc: 46.875, f1: 13.428571428571429, r: 0.2655264172825142
06/02/2019 10:08:33 *** evaluating ***
06/02/2019 10:08:33 step: 16, epoch: 15, acc: 56.41025641025641, f1: 16.76781238811314, r: 0.29645361201438797
06/02/2019 10:08:33 *** epoch: 17 ***
06/02/2019 10:08:33 *** training ***
06/02/2019 10:08:35 step: 533, epoch: 16, batch: 4, loss: 1.2823361158370972, acc: 59.375, f1: 26.61303276932962, r: 0.2921962412979126
06/02/2019 10:08:38 step: 538, epoch: 16, batch: 9, loss: 1.57024085521698, acc: 48.4375, f1: 25.229984301412873, r: 0.19876009803577824
06/02/2019 10:08:41 step: 543, epoch: 16, batch: 14, loss: 1.4013539552688599, acc: 50.0, f1: 16.706788637457873, r: 0.30615868783601735
06/02/2019 10:08:43 step: 548, epoch: 16, batch: 19, loss: 1.5456182956695557, acc: 40.625, f1: 13.025210084033612, r: 0.22968181644123586
06/02/2019 10:08:45 step: 553, epoch: 16, batch: 24, loss: 1.5141092538833618, acc: 48.4375, f1: 14.888392857142858, r: 0.2590873909800942
06/02/2019 10:08:47 step: 558, epoch: 16, batch: 29, loss: 1.50762140750885, acc: 46.875, f1: 20.93347265761059, r: 0.2600169006486007
06/02/2019 10:08:48 *** evaluating ***
06/02/2019 10:08:49 step: 17, epoch: 16, acc: 56.41025641025641, f1: 18.408581813883387, r: 0.2773826205087629
06/02/2019 10:08:49 *** epoch: 18 ***
06/02/2019 10:08:49 *** training ***
06/02/2019 10:08:51 step: 566, epoch: 17, batch: 4, loss: 1.3200379610061646, acc: 54.6875, f1: 22.07001522070015, r: 0.325418194414349
06/02/2019 10:08:53 step: 571, epoch: 17, batch: 9, loss: 1.2671432495117188, acc: 56.25, f1: 29.060150375939852, r: 0.2474574436228825
06/02/2019 10:08:56 step: 576, epoch: 17, batch: 14, loss: 1.3363925218582153, acc: 48.4375, f1: 15.436705362078495, r: 0.3617923160127696
06/02/2019 10:08:58 step: 581, epoch: 17, batch: 19, loss: 1.247840166091919, acc: 57.8125, f1: 24.687381583933306, r: 0.3043785406783152
06/02/2019 10:09:01 step: 586, epoch: 17, batch: 24, loss: 1.2953343391418457, acc: 53.125, f1: 36.800804828973845, r: 0.41968742720773256
06/02/2019 10:09:03 step: 591, epoch: 17, batch: 29, loss: 1.2523388862609863, acc: 54.6875, f1: 18.58210784313726, r: 0.33345732683900053
06/02/2019 10:09:04 *** evaluating ***
06/02/2019 10:09:05 step: 18, epoch: 17, acc: 56.837606837606835, f1: 19.41818665956597, r: 0.30412517092601565
06/02/2019 10:09:05 *** epoch: 19 ***
06/02/2019 10:09:05 *** training ***
06/02/2019 10:09:06 step: 599, epoch: 18, batch: 4, loss: 1.3479686975479126, acc: 48.4375, f1: 24.641350210970465, r: 0.2932139438353302
06/02/2019 10:09:08 step: 604, epoch: 18, batch: 9, loss: 1.3663291931152344, acc: 56.25, f1: 26.341050254093734, r: 0.2819305261095615
06/02/2019 10:09:11 step: 609, epoch: 18, batch: 14, loss: 1.402611494064331, acc: 50.0, f1: 25.389204545454547, r: 0.2972674696317164
06/02/2019 10:09:13 step: 614, epoch: 18, batch: 19, loss: 1.041214108467102, acc: 60.9375, f1: 24.041666666666668, r: 0.4534976265312539
06/02/2019 10:09:16 step: 619, epoch: 18, batch: 24, loss: 1.3711299896240234, acc: 51.5625, f1: 18.791602662570405, r: 0.32079270803647375
06/02/2019 10:09:18 step: 624, epoch: 18, batch: 29, loss: 1.3046351671218872, acc: 48.4375, f1: 18.911564625850342, r: 0.2712901683933776
06/02/2019 10:09:19 *** evaluating ***
06/02/2019 10:09:20 step: 19, epoch: 18, acc: 54.27350427350427, f1: 16.76518883415435, r: 0.28492795761917533
06/02/2019 10:09:20 *** epoch: 20 ***
06/02/2019 10:09:20 *** training ***
06/02/2019 10:09:22 step: 632, epoch: 19, batch: 4, loss: 1.2456295490264893, acc: 51.5625, f1: 26.638023630504836, r: 0.35756871228435666
06/02/2019 10:09:24 step: 637, epoch: 19, batch: 9, loss: 1.4581838846206665, acc: 43.75, f1: 24.034530771372875, r: 0.3450180337500112
06/02/2019 10:09:27 step: 642, epoch: 19, batch: 14, loss: 1.1075881719589233, acc: 60.9375, f1: 21.46671025389561, r: 0.3684153549009104
06/02/2019 10:09:29 step: 647, epoch: 19, batch: 19, loss: 1.0658025741577148, acc: 60.9375, f1: 17.0, r: 0.36126269918195497
06/02/2019 10:09:32 step: 652, epoch: 19, batch: 24, loss: 1.395896315574646, acc: 53.125, f1: 18.389610389610393, r: 0.2517030983295619
06/02/2019 10:09:34 step: 657, epoch: 19, batch: 29, loss: 1.2558904886245728, acc: 51.5625, f1: 26.070619609243366, r: 0.27676894607039176
06/02/2019 10:09:35 *** evaluating ***
06/02/2019 10:09:36 step: 20, epoch: 19, acc: 57.26495726495726, f1: 18.04807877349413, r: 0.3029589913167666
06/02/2019 10:09:36 *** epoch: 21 ***
06/02/2019 10:09:36 *** training ***
06/02/2019 10:09:38 step: 665, epoch: 20, batch: 4, loss: 1.222162127494812, acc: 59.375, f1: 27.979797979797983, r: 0.33859380308498366
06/02/2019 10:09:40 step: 670, epoch: 20, batch: 9, loss: 1.2404024600982666, acc: 53.125, f1: 23.849206349206348, r: 0.38019612458407753
06/02/2019 10:09:43 step: 675, epoch: 20, batch: 14, loss: 1.2968428134918213, acc: 45.3125, f1: 22.69276617102704, r: 0.3975431119198899
06/02/2019 10:09:45 step: 680, epoch: 20, batch: 19, loss: 1.3505795001983643, acc: 54.6875, f1: 16.558441558441558, r: 0.3092028835678319
06/02/2019 10:09:47 step: 685, epoch: 20, batch: 24, loss: 1.3042926788330078, acc: 51.5625, f1: 23.19040845356635, r: 0.36718791102625903
06/02/2019 10:09:50 step: 690, epoch: 20, batch: 29, loss: 1.084683895111084, acc: 56.25, f1: 24.702472668574366, r: 0.43414661226574947
06/02/2019 10:09:50 *** evaluating ***
06/02/2019 10:09:51 step: 21, epoch: 20, acc: 57.692307692307686, f1: 17.948717948717952, r: 0.29473016261997687
06/02/2019 10:09:51 *** epoch: 22 ***
06/02/2019 10:09:51 *** training ***
06/02/2019 10:09:54 step: 698, epoch: 21, batch: 4, loss: 1.6425479650497437, acc: 39.0625, f1: 21.986062717770036, r: 0.24985667442264364
06/02/2019 10:09:56 step: 703, epoch: 21, batch: 9, loss: 1.3712430000305176, acc: 45.3125, f1: 19.56256304082391, r: 0.3573651988938486
06/02/2019 10:09:58 step: 708, epoch: 21, batch: 14, loss: 1.2060773372650146, acc: 53.125, f1: 28.952380952380945, r: 0.2792273047610866
06/02/2019 10:10:00 step: 713, epoch: 21, batch: 19, loss: 1.3290174007415771, acc: 54.6875, f1: 12.939958592132502, r: 0.28910742200959644
06/02/2019 10:10:03 step: 718, epoch: 21, batch: 24, loss: 1.229661226272583, acc: 56.25, f1: 23.249345935371345, r: 0.3239030506802936
06/02/2019 10:10:05 step: 723, epoch: 21, batch: 29, loss: 1.3473505973815918, acc: 48.4375, f1: 25.3344298245614, r: 0.38480116061221
06/02/2019 10:10:06 *** evaluating ***
06/02/2019 10:10:07 step: 22, epoch: 21, acc: 55.12820512820513, f1: 20.507676479898702, r: 0.3062898447224021
06/02/2019 10:10:07 *** epoch: 23 ***
06/02/2019 10:10:07 *** training ***
06/02/2019 10:10:09 step: 731, epoch: 22, batch: 4, loss: 1.3008333444595337, acc: 53.125, f1: 24.819624819624824, r: 0.40607372239436773
06/02/2019 10:10:11 step: 736, epoch: 22, batch: 9, loss: 1.5084110498428345, acc: 50.0, f1: 20.87135589114108, r: 0.2555055759012031
06/02/2019 10:10:13 step: 741, epoch: 22, batch: 14, loss: 1.5058828592300415, acc: 46.875, f1: 20.959623673078458, r: 0.3575719793284339
06/02/2019 10:10:16 step: 746, epoch: 22, batch: 19, loss: 1.4550870656967163, acc: 40.625, f1: 22.91711287213536, r: 0.32251205144320655
06/02/2019 10:10:18 step: 751, epoch: 22, batch: 24, loss: 1.2021794319152832, acc: 45.3125, f1: 20.672268907563023, r: 0.40326244363233804
06/02/2019 10:10:20 step: 756, epoch: 22, batch: 29, loss: 1.022482991218567, acc: 73.4375, f1: 41.791601848527854, r: 0.3480199655189804
06/02/2019 10:10:22 *** evaluating ***
06/02/2019 10:10:22 step: 23, epoch: 22, acc: 54.700854700854705, f1: 19.224497935435437, r: 0.3257962405702334
06/02/2019 10:10:22 *** epoch: 24 ***
06/02/2019 10:10:22 *** training ***
06/02/2019 10:10:24 step: 764, epoch: 23, batch: 4, loss: 1.206305980682373, acc: 51.5625, f1: 27.488876529477196, r: 0.4253780991053024
06/02/2019 10:10:27 step: 769, epoch: 23, batch: 9, loss: 1.2981094121932983, acc: 56.25, f1: 27.770146520146515, r: 0.37478285047646076
06/02/2019 10:10:29 step: 774, epoch: 23, batch: 14, loss: 1.1496700048446655, acc: 57.8125, f1: 23.401360544217688, r: 0.34388651314659335
06/02/2019 10:10:32 step: 779, epoch: 23, batch: 19, loss: 1.405943751335144, acc: 50.0, f1: 34.15870621752974, r: 0.32093319113994856
06/02/2019 10:10:34 step: 784, epoch: 23, batch: 24, loss: 1.1833999156951904, acc: 51.5625, f1: 34.029102149126636, r: 0.3945793152005801
06/02/2019 10:10:36 step: 789, epoch: 23, batch: 29, loss: 1.308901309967041, acc: 46.875, f1: 17.071759259259263, r: 0.38716071635524013
06/02/2019 10:10:37 *** evaluating ***
06/02/2019 10:10:38 step: 24, epoch: 23, acc: 55.12820512820513, f1: 19.00093909089411, r: 0.286044252665383
06/02/2019 10:10:38 *** epoch: 25 ***
06/02/2019 10:10:38 *** training ***
06/02/2019 10:10:40 step: 797, epoch: 24, batch: 4, loss: 1.2674199342727661, acc: 54.6875, f1: 27.628968253968257, r: 0.3227557046900996
06/02/2019 10:10:43 step: 802, epoch: 24, batch: 9, loss: 1.166555643081665, acc: 53.125, f1: 20.334776334776333, r: 0.22373807543880428
06/02/2019 10:10:45 step: 807, epoch: 24, batch: 14, loss: 1.347681999206543, acc: 51.5625, f1: 30.913853446748185, r: 0.3655994396367265
06/02/2019 10:10:47 step: 812, epoch: 24, batch: 19, loss: 1.1436638832092285, acc: 60.9375, f1: 30.4219353259865, r: 0.3805821122457317
06/02/2019 10:10:50 step: 817, epoch: 24, batch: 24, loss: 1.179256558418274, acc: 59.375, f1: 31.678921568627448, r: 0.3955424235886385
06/02/2019 10:10:52 step: 822, epoch: 24, batch: 29, loss: 1.215972661972046, acc: 51.5625, f1: 27.906593406593405, r: 0.3948797194516482
06/02/2019 10:10:53 *** evaluating ***
06/02/2019 10:10:54 step: 25, epoch: 24, acc: 55.12820512820513, f1: 23.25295684938787, r: 0.30136217745664756
06/02/2019 10:10:54 *** epoch: 26 ***
06/02/2019 10:10:54 *** training ***
06/02/2019 10:10:56 step: 830, epoch: 25, batch: 4, loss: 1.496750831604004, acc: 43.75, f1: 20.30169172932331, r: 0.3717930109668179
06/02/2019 10:10:58 step: 835, epoch: 25, batch: 9, loss: 1.188149094581604, acc: 50.0, f1: 20.372503840245777, r: 0.36459740147256636
06/02/2019 10:11:01 step: 840, epoch: 25, batch: 14, loss: 1.1414798498153687, acc: 54.6875, f1: 28.75233426704015, r: 0.44813166514920305
06/02/2019 10:11:03 step: 845, epoch: 25, batch: 19, loss: 1.1894320249557495, acc: 65.625, f1: 46.966848940533154, r: 0.4037326385876803
06/02/2019 10:11:05 step: 850, epoch: 25, batch: 24, loss: 1.086574673652649, acc: 54.6875, f1: 25.210084033613445, r: 0.4060212694282387
06/02/2019 10:11:08 step: 855, epoch: 25, batch: 29, loss: 1.1546039581298828, acc: 57.8125, f1: 36.14468864468864, r: 0.32180322152377006
06/02/2019 10:11:09 *** evaluating ***
06/02/2019 10:11:10 step: 26, epoch: 25, acc: 55.55555555555556, f1: 23.762529448090664, r: 0.31480139027930065
06/02/2019 10:11:10 *** epoch: 27 ***
06/02/2019 10:11:10 *** training ***
06/02/2019 10:11:12 step: 863, epoch: 26, batch: 4, loss: 1.179625153541565, acc: 62.5, f1: 47.47665168581939, r: 0.5032023435892135
06/02/2019 10:11:14 step: 868, epoch: 26, batch: 9, loss: 0.9786520004272461, acc: 60.9375, f1: 41.96641556811048, r: 0.38838681916671525
06/02/2019 10:11:17 step: 873, epoch: 26, batch: 14, loss: 1.1430495977401733, acc: 50.0, f1: 28.186813186813186, r: 0.3879853052358205
06/02/2019 10:11:20 step: 878, epoch: 26, batch: 19, loss: 1.082079291343689, acc: 60.9375, f1: 24.402097197015525, r: 0.39815987881159587
06/02/2019 10:11:22 step: 883, epoch: 26, batch: 24, loss: 1.211276888847351, acc: 54.6875, f1: 31.682696619137296, r: 0.3963952301677521
06/02/2019 10:11:24 step: 888, epoch: 26, batch: 29, loss: 1.0264970064163208, acc: 62.5, f1: 28.91456582633053, r: 0.36996636128522997
06/02/2019 10:11:25 *** evaluating ***
06/02/2019 10:11:26 step: 27, epoch: 26, acc: 50.85470085470085, f1: 22.50403817170509, r: 0.27082601550134194
06/02/2019 10:11:26 *** epoch: 28 ***
06/02/2019 10:11:26 *** training ***
06/02/2019 10:11:28 step: 896, epoch: 27, batch: 4, loss: 1.1034996509552002, acc: 56.25, f1: 33.99446958270488, r: 0.44100230745980007
06/02/2019 10:11:30 step: 901, epoch: 27, batch: 9, loss: 0.9829228520393372, acc: 62.5, f1: 30.940138293079468, r: 0.412186174899979
06/02/2019 10:11:32 step: 906, epoch: 27, batch: 14, loss: 1.111158013343811, acc: 56.25, f1: 24.232456140350877, r: 0.40325571139217464
06/02/2019 10:11:35 step: 911, epoch: 27, batch: 19, loss: 1.2669540643692017, acc: 48.4375, f1: 28.4688995215311, r: 0.47806254250333213
06/02/2019 10:11:37 step: 916, epoch: 27, batch: 24, loss: 0.9719895720481873, acc: 62.5, f1: 34.23414820473644, r: 0.4457618495525759
06/02/2019 10:11:39 step: 921, epoch: 27, batch: 29, loss: 1.136020541191101, acc: 56.25, f1: 34.57284973678416, r: 0.4736640959352564
06/02/2019 10:11:40 *** evaluating ***
06/02/2019 10:11:41 step: 28, epoch: 27, acc: 52.991452991452995, f1: 23.093613634036373, r: 0.29594563819823616
06/02/2019 10:11:41 *** epoch: 29 ***
06/02/2019 10:11:41 *** training ***
06/02/2019 10:11:42 step: 929, epoch: 28, batch: 4, loss: 1.1631691455841064, acc: 57.8125, f1: 35.929621848739494, r: 0.37387827676139945
06/02/2019 10:11:45 step: 934, epoch: 28, batch: 9, loss: 1.0646027326583862, acc: 60.9375, f1: 36.54298082869512, r: 0.4603872529009143
06/02/2019 10:11:47 step: 939, epoch: 28, batch: 14, loss: 0.8648551106452942, acc: 70.3125, f1: 45.62240766752045, r: 0.4271761704523207
06/02/2019 10:11:50 step: 944, epoch: 28, batch: 19, loss: 1.029392957687378, acc: 60.9375, f1: 37.55219797016386, r: 0.45378509143902857
06/02/2019 10:11:52 step: 949, epoch: 28, batch: 24, loss: 0.7874166965484619, acc: 78.125, f1: 55.380571412273305, r: 0.5243995692855974
06/02/2019 10:11:54 step: 954, epoch: 28, batch: 29, loss: 0.9356756806373596, acc: 62.5, f1: 30.090742590742593, r: 0.4156497032598996
06/02/2019 10:11:56 *** evaluating ***
06/02/2019 10:11:56 step: 29, epoch: 28, acc: 49.14529914529914, f1: 25.53137522761971, r: 0.3181225922593855
06/02/2019 10:11:56 *** epoch: 30 ***
06/02/2019 10:11:56 *** training ***
06/02/2019 10:11:59 step: 962, epoch: 29, batch: 4, loss: 0.999697744846344, acc: 64.0625, f1: 40.591443596834424, r: 0.4838242178314872
06/02/2019 10:12:01 step: 967, epoch: 29, batch: 9, loss: 1.114383578300476, acc: 57.8125, f1: 38.14574314574315, r: 0.4211598701145651
06/02/2019 10:12:03 step: 972, epoch: 29, batch: 14, loss: 1.2150460481643677, acc: 54.6875, f1: 35.71919896991054, r: 0.4742790933245053
06/02/2019 10:12:06 step: 977, epoch: 29, batch: 19, loss: 1.2422566413879395, acc: 56.25, f1: 42.710836921363246, r: 0.3891672299466127
06/02/2019 10:12:08 step: 982, epoch: 29, batch: 24, loss: 1.0413813591003418, acc: 53.125, f1: 28.982940043040887, r: 0.4067433340334914
06/02/2019 10:12:10 step: 987, epoch: 29, batch: 29, loss: 1.0659433603286743, acc: 67.1875, f1: 49.93301717439648, r: 0.5091719171055649
06/02/2019 10:12:12 *** evaluating ***
06/02/2019 10:12:12 step: 30, epoch: 29, acc: 54.700854700854705, f1: 24.199768833232383, r: 0.3031843224503939
06/02/2019 10:12:12 *** epoch: 31 ***
06/02/2019 10:12:12 *** training ***
06/02/2019 10:12:15 step: 995, epoch: 30, batch: 4, loss: 1.0027295351028442, acc: 62.5, f1: 46.35891590678825, r: 0.4799449549566343
06/02/2019 10:12:17 step: 1000, epoch: 30, batch: 9, loss: 1.0675077438354492, acc: 62.5, f1: 45.887445887445885, r: 0.3695093609563426
06/02/2019 10:12:19 step: 1005, epoch: 30, batch: 14, loss: 1.201810598373413, acc: 57.8125, f1: 38.03332708645677, r: 0.4678269758766928
06/02/2019 10:12:21 step: 1010, epoch: 30, batch: 19, loss: 0.9996477365493774, acc: 68.75, f1: 44.807256235827666, r: 0.42431322377675507
06/02/2019 10:12:23 step: 1015, epoch: 30, batch: 24, loss: 1.1036485433578491, acc: 46.875, f1: 29.71928225384447, r: 0.37622412734439176
06/02/2019 10:12:25 step: 1020, epoch: 30, batch: 29, loss: 1.0469523668289185, acc: 57.8125, f1: 40.979592774103686, r: 0.4986320174482462
06/02/2019 10:12:27 *** evaluating ***
06/02/2019 10:12:27 step: 31, epoch: 30, acc: 52.13675213675214, f1: 26.236211085795034, r: 0.34000052933504066
06/02/2019 10:12:27 *** epoch: 32 ***
06/02/2019 10:12:27 *** training ***
06/02/2019 10:12:30 step: 1028, epoch: 31, batch: 4, loss: 1.120219349861145, acc: 59.375, f1: 36.18968055058281, r: 0.49850417892722027
06/02/2019 10:12:32 step: 1033, epoch: 31, batch: 9, loss: 1.0853075981140137, acc: 62.5, f1: 37.62235449735449, r: 0.4312250131014736
06/02/2019 10:12:35 step: 1038, epoch: 31, batch: 14, loss: 1.1325249671936035, acc: 62.5, f1: 46.709802240578334, r: 0.40033212064148044
06/02/2019 10:12:37 step: 1043, epoch: 31, batch: 19, loss: 1.1010445356369019, acc: 57.8125, f1: 41.24542124542125, r: 0.51665173935803
06/02/2019 10:12:39 step: 1048, epoch: 31, batch: 24, loss: 0.8576599359512329, acc: 68.75, f1: 47.315493646138805, r: 0.43875597158572743
06/02/2019 10:12:41 step: 1053, epoch: 31, batch: 29, loss: 1.055031180381775, acc: 67.1875, f1: 33.76867308318921, r: 0.3916657555201526
06/02/2019 10:12:42 *** evaluating ***
06/02/2019 10:12:43 step: 32, epoch: 31, acc: 51.28205128205128, f1: 25.835490367525065, r: 0.2970755405524566
06/02/2019 10:12:43 *** epoch: 33 ***
06/02/2019 10:12:43 *** training ***
06/02/2019 10:12:46 step: 1061, epoch: 32, batch: 4, loss: 0.89244145154953, acc: 68.75, f1: 46.92639954532537, r: 0.503602303326188
06/02/2019 10:12:48 step: 1066, epoch: 32, batch: 9, loss: 1.085030436515808, acc: 57.8125, f1: 31.285842293906814, r: 0.46540679492768666
06/02/2019 10:12:50 step: 1071, epoch: 32, batch: 14, loss: 1.1225539445877075, acc: 60.9375, f1: 39.41113385406864, r: 0.4401769141983792
06/02/2019 10:12:53 step: 1076, epoch: 32, batch: 19, loss: 1.6406525373458862, acc: 39.0625, f1: 28.998488284202566, r: 0.3096553441588192
06/02/2019 10:12:55 step: 1081, epoch: 32, batch: 24, loss: 0.865098237991333, acc: 70.3125, f1: 52.37012987012988, r: 0.5613409196281912
06/02/2019 10:12:57 step: 1086, epoch: 32, batch: 29, loss: 1.0257807970046997, acc: 56.25, f1: 35.780261310873556, r: 0.4297007353018296
06/02/2019 10:12:58 *** evaluating ***
06/02/2019 10:12:58 step: 33, epoch: 32, acc: 54.27350427350427, f1: 27.587771761454437, r: 0.3172132893711658
06/02/2019 10:12:58 *** epoch: 34 ***
06/02/2019 10:12:58 *** training ***
06/02/2019 10:13:01 step: 1094, epoch: 33, batch: 4, loss: 0.7621294260025024, acc: 76.5625, f1: 46.7436974789916, r: 0.46610950031380155
06/02/2019 10:13:03 step: 1099, epoch: 33, batch: 9, loss: 1.0606224536895752, acc: 60.9375, f1: 50.153190918277126, r: 0.5425825896527745
06/02/2019 10:13:06 step: 1104, epoch: 33, batch: 14, loss: 0.8692614436149597, acc: 68.75, f1: 51.608670586878816, r: 0.47272733758130103
06/02/2019 10:13:08 step: 1109, epoch: 33, batch: 19, loss: 0.8151541352272034, acc: 70.3125, f1: 46.108204288004515, r: 0.5096791287350427
06/02/2019 10:13:10 step: 1114, epoch: 33, batch: 24, loss: 1.0110623836517334, acc: 59.375, f1: 24.698108908635223, r: 0.40477445900894976
06/02/2019 10:13:13 step: 1119, epoch: 33, batch: 29, loss: 0.9609301090240479, acc: 65.625, f1: 46.09043617446979, r: 0.46521921852507053
06/02/2019 10:13:14 *** evaluating ***
06/02/2019 10:13:15 step: 34, epoch: 33, acc: 51.28205128205128, f1: 22.283307340917002, r: 0.2789712600745658
06/02/2019 10:13:15 *** epoch: 35 ***
06/02/2019 10:13:15 *** training ***
06/02/2019 10:13:17 step: 1127, epoch: 34, batch: 4, loss: 1.0859441757202148, acc: 54.6875, f1: 35.52359949125447, r: 0.38674232057499647
06/02/2019 10:13:19 step: 1132, epoch: 34, batch: 9, loss: 0.7652974128723145, acc: 75.0, f1: 50.627250900360146, r: 0.5124719295775172
06/02/2019 10:13:22 step: 1137, epoch: 34, batch: 14, loss: 0.8771278262138367, acc: 68.75, f1: 49.83258928571428, r: 0.4844394195279623
06/02/2019 10:13:24 step: 1142, epoch: 34, batch: 19, loss: 0.857546329498291, acc: 67.1875, f1: 53.40173432010167, r: 0.4854723247540389
06/02/2019 10:13:26 step: 1147, epoch: 34, batch: 24, loss: 0.9575438499450684, acc: 59.375, f1: 24.73498451058058, r: 0.39498113227917303
06/02/2019 10:13:29 step: 1152, epoch: 34, batch: 29, loss: 1.203677773475647, acc: 64.0625, f1: 39.72342141984999, r: 0.39594153033391055
06/02/2019 10:13:30 *** evaluating ***
06/02/2019 10:13:30 step: 35, epoch: 34, acc: 53.84615384615385, f1: 22.943111108146017, r: 0.31808603357876464
06/02/2019 10:13:30 *** epoch: 36 ***
06/02/2019 10:13:30 *** training ***
06/02/2019 10:13:33 step: 1160, epoch: 35, batch: 4, loss: 1.2301257848739624, acc: 60.9375, f1: 45.77400133895443, r: 0.44717556957958127
06/02/2019 10:13:35 step: 1165, epoch: 35, batch: 9, loss: 0.8184951543807983, acc: 76.5625, f1: 52.790096082779, r: 0.5684683745317383
06/02/2019 10:13:37 step: 1170, epoch: 35, batch: 14, loss: 0.8005274534225464, acc: 73.4375, f1: 51.9980986950134, r: 0.48580052221455156
06/02/2019 10:13:39 step: 1175, epoch: 35, batch: 19, loss: 0.8778396844863892, acc: 62.5, f1: 37.30014691902169, r: 0.4959516399757219
06/02/2019 10:13:42 step: 1180, epoch: 35, batch: 24, loss: 0.8986826539039612, acc: 68.75, f1: 46.904761904761905, r: 0.550255082376345
06/02/2019 10:13:44 step: 1185, epoch: 35, batch: 29, loss: 1.0016119480133057, acc: 59.375, f1: 45.620748299319736, r: 0.47244142771888487
06/02/2019 10:13:45 *** evaluating ***
06/02/2019 10:13:46 step: 36, epoch: 35, acc: 47.863247863247864, f1: 23.496903996658858, r: 0.3005532389412036
06/02/2019 10:13:46 *** epoch: 37 ***
06/02/2019 10:13:46 *** training ***
06/02/2019 10:13:49 step: 1193, epoch: 36, batch: 4, loss: 0.9761049151420593, acc: 64.0625, f1: 45.77470835077326, r: 0.39959331764146366
06/02/2019 10:13:51 step: 1198, epoch: 36, batch: 9, loss: 0.8797564506530762, acc: 64.0625, f1: 36.60445548603443, r: 0.4912827943667517
06/02/2019 10:13:53 step: 1203, epoch: 36, batch: 14, loss: 0.8024967312812805, acc: 71.875, f1: 39.30935880099166, r: 0.4893766297996334
06/02/2019 10:13:55 step: 1208, epoch: 36, batch: 19, loss: 0.8686215281486511, acc: 68.75, f1: 43.41013824884793, r: 0.5269977864416053
06/02/2019 10:13:58 step: 1213, epoch: 36, batch: 24, loss: 0.8705444931983948, acc: 70.3125, f1: 52.17687074829932, r: 0.4538024677169984
06/02/2019 10:14:00 step: 1218, epoch: 36, batch: 29, loss: 0.8014267086982727, acc: 62.5, f1: 45.14500537056928, r: 0.4817564474009996
06/02/2019 10:14:01 *** evaluating ***
06/02/2019 10:14:02 step: 37, epoch: 36, acc: 58.119658119658126, f1: 23.15816397994166, r: 0.3295768368380554
06/02/2019 10:14:02 *** epoch: 38 ***
06/02/2019 10:14:02 *** training ***
06/02/2019 10:14:04 step: 1226, epoch: 37, batch: 4, loss: 0.8308485746383667, acc: 70.3125, f1: 46.942640692640694, r: 0.5377074048252032
06/02/2019 10:14:06 step: 1231, epoch: 37, batch: 9, loss: 0.7444494962692261, acc: 71.875, f1: 34.04802744425386, r: 0.5448519043599512
06/02/2019 10:14:09 step: 1236, epoch: 37, batch: 14, loss: 0.7539210319519043, acc: 75.0, f1: 61.544056218978824, r: 0.4919701259688938
06/02/2019 10:14:11 step: 1241, epoch: 37, batch: 19, loss: 1.0022896528244019, acc: 68.75, f1: 52.07792207792208, r: 0.4406944049211491
06/02/2019 10:14:14 step: 1246, epoch: 37, batch: 24, loss: 0.8523401021957397, acc: 71.875, f1: 39.14240677212375, r: 0.4130526781038312
06/02/2019 10:14:16 step: 1251, epoch: 37, batch: 29, loss: 0.7387597560882568, acc: 79.6875, f1: 68.13424670567527, r: 0.566457998577013
06/02/2019 10:14:17 *** evaluating ***
06/02/2019 10:14:18 step: 38, epoch: 37, acc: 51.70940170940172, f1: 27.154610443919058, r: 0.3201142078416538
06/02/2019 10:14:18 *** epoch: 39 ***
06/02/2019 10:14:18 *** training ***
06/02/2019 10:14:20 step: 1259, epoch: 38, batch: 4, loss: 0.7765887379646301, acc: 67.1875, f1: 38.19962686567165, r: 0.4625812300904445
06/02/2019 10:14:22 step: 1264, epoch: 38, batch: 9, loss: 0.7040356397628784, acc: 78.125, f1: 57.06288967199783, r: 0.5071716650558431
06/02/2019 10:14:24 step: 1269, epoch: 38, batch: 14, loss: 0.7169170379638672, acc: 78.125, f1: 62.02189966895849, r: 0.5877563765450653
06/02/2019 10:14:27 step: 1274, epoch: 38, batch: 19, loss: 0.596997082233429, acc: 79.6875, f1: 62.623895712131, r: 0.5404378902209935
06/02/2019 10:14:29 step: 1279, epoch: 38, batch: 24, loss: 0.8752830624580383, acc: 70.3125, f1: 38.10601163542341, r: 0.4863612384127883
06/02/2019 10:14:31 step: 1284, epoch: 38, batch: 29, loss: 0.9160510897636414, acc: 67.1875, f1: 41.244939271255056, r: 0.562694708357278
06/02/2019 10:14:33 *** evaluating ***
06/02/2019 10:14:33 step: 39, epoch: 38, acc: 48.29059829059829, f1: 24.823267880153264, r: 0.2974852605092831
06/02/2019 10:14:33 *** epoch: 40 ***
06/02/2019 10:14:33 *** training ***
06/02/2019 10:14:36 step: 1292, epoch: 39, batch: 4, loss: 0.9345600605010986, acc: 67.1875, f1: 47.41790293539232, r: 0.5078910281836304
06/02/2019 10:14:38 step: 1297, epoch: 39, batch: 9, loss: 0.8379402756690979, acc: 65.625, f1: 40.42578384010609, r: 0.4746977287164415
06/02/2019 10:14:40 step: 1302, epoch: 39, batch: 14, loss: 0.8929643034934998, acc: 73.4375, f1: 53.884624147782034, r: 0.5043185283911482
06/02/2019 10:14:43 step: 1307, epoch: 39, batch: 19, loss: 0.7704646587371826, acc: 73.4375, f1: 44.11218724778047, r: 0.5742274852437942
06/02/2019 10:14:45 step: 1312, epoch: 39, batch: 24, loss: 0.6853016018867493, acc: 81.25, f1: 54.230169340463455, r: 0.5275654108969472
06/02/2019 10:14:47 step: 1317, epoch: 39, batch: 29, loss: 1.040713906288147, acc: 57.8125, f1: 32.981397687280044, r: 0.5085159712107293
06/02/2019 10:14:48 *** evaluating ***
06/02/2019 10:14:49 step: 40, epoch: 39, acc: 52.56410256410257, f1: 26.55088758571796, r: 0.28551062006189637
06/02/2019 10:14:49 *** epoch: 41 ***
06/02/2019 10:14:49 *** training ***
06/02/2019 10:14:51 step: 1325, epoch: 40, batch: 4, loss: 0.7222875356674194, acc: 73.4375, f1: 53.92739336135563, r: 0.5022551724962261
06/02/2019 10:14:54 step: 1330, epoch: 40, batch: 9, loss: 0.7838513255119324, acc: 75.0, f1: 59.529359529359525, r: 0.5282044696522863
06/02/2019 10:14:56 step: 1335, epoch: 40, batch: 14, loss: 0.6693692803382874, acc: 76.5625, f1: 58.93073593073592, r: 0.49750590912209014
06/02/2019 10:14:58 step: 1340, epoch: 40, batch: 19, loss: 0.7223483920097351, acc: 71.875, f1: 54.8601617680565, r: 0.5822848446236458
06/02/2019 10:15:00 step: 1345, epoch: 40, batch: 24, loss: 0.8556815385818481, acc: 67.1875, f1: 51.36743492538641, r: 0.5103271529261714
06/02/2019 10:15:03 step: 1350, epoch: 40, batch: 29, loss: 0.6760863661766052, acc: 78.125, f1: 56.448631156077965, r: 0.5679874265705744
06/02/2019 10:15:04 *** evaluating ***
06/02/2019 10:15:05 step: 41, epoch: 40, acc: 42.73504273504273, f1: 26.601964919095977, r: 0.2993178737414948
06/02/2019 10:15:05 *** epoch: 42 ***
06/02/2019 10:15:05 *** training ***
06/02/2019 10:15:07 step: 1358, epoch: 41, batch: 4, loss: 0.725418210029602, acc: 73.4375, f1: 46.04946458394734, r: 0.5457094286749593
06/02/2019 10:15:09 step: 1363, epoch: 41, batch: 9, loss: 0.7428140640258789, acc: 70.3125, f1: 49.67156093206512, r: 0.5304089494773606
06/02/2019 10:15:11 step: 1368, epoch: 41, batch: 14, loss: 0.7204923629760742, acc: 75.0, f1: 43.29937304075234, r: 0.5720623405633926
06/02/2019 10:15:13 step: 1373, epoch: 41, batch: 19, loss: 0.6260312795639038, acc: 79.6875, f1: 55.45546558704453, r: 0.5943093748157297
06/02/2019 10:15:16 step: 1378, epoch: 41, batch: 24, loss: 0.9897000193595886, acc: 70.3125, f1: 49.42054655870445, r: 0.49621751148342125
06/02/2019 10:15:18 step: 1383, epoch: 41, batch: 29, loss: 0.8184653520584106, acc: 73.4375, f1: 48.79469880057287, r: 0.5534503760918491
06/02/2019 10:15:20 *** evaluating ***
06/02/2019 10:15:20 step: 42, epoch: 41, acc: 50.427350427350426, f1: 25.215613567598695, r: 0.34389879972531534
06/02/2019 10:15:20 *** epoch: 43 ***
06/02/2019 10:15:20 *** training ***
06/02/2019 10:15:22 step: 1391, epoch: 42, batch: 4, loss: 0.6087758541107178, acc: 78.125, f1: 52.93317972350231, r: 0.5691466196664109
06/02/2019 10:15:25 step: 1396, epoch: 42, batch: 9, loss: 0.9293609857559204, acc: 64.0625, f1: 39.69312692138779, r: 0.5249965185538729
06/02/2019 10:15:27 step: 1401, epoch: 42, batch: 14, loss: 0.9215461015701294, acc: 64.0625, f1: 39.576242722794454, r: 0.5646862312654496
06/02/2019 10:15:29 step: 1406, epoch: 42, batch: 19, loss: 0.9185903072357178, acc: 64.0625, f1: 39.74673202614379, r: 0.5193768333738494
06/02/2019 10:15:32 step: 1411, epoch: 42, batch: 24, loss: 0.7778658866882324, acc: 70.3125, f1: 54.95192307692308, r: 0.6112414584706594
06/02/2019 10:15:34 step: 1416, epoch: 42, batch: 29, loss: 0.7535626292228699, acc: 73.4375, f1: 51.62645703442668, r: 0.5889825624358492
06/02/2019 10:15:35 *** evaluating ***
06/02/2019 10:15:36 step: 43, epoch: 42, acc: 51.70940170940172, f1: 26.84453152059026, r: 0.3453055590445815
06/02/2019 10:15:36 *** epoch: 44 ***
06/02/2019 10:15:36 *** training ***
06/02/2019 10:15:38 step: 1424, epoch: 43, batch: 4, loss: 0.6237788796424866, acc: 78.125, f1: 52.042794651821026, r: 0.5810647055275447
06/02/2019 10:15:41 step: 1429, epoch: 43, batch: 9, loss: 0.49401140213012695, acc: 79.6875, f1: 64.24380348865473, r: 0.6436262276166289
06/02/2019 10:15:43 step: 1434, epoch: 43, batch: 14, loss: 0.6497885584831238, acc: 78.125, f1: 65.89873894221721, r: 0.5853044035177274
06/02/2019 10:15:45 step: 1439, epoch: 43, batch: 19, loss: 0.5898867249488831, acc: 79.6875, f1: 57.24422015182884, r: 0.5815212648525223
06/02/2019 10:15:47 step: 1444, epoch: 43, batch: 24, loss: 0.7486283183097839, acc: 68.75, f1: 48.15913409898373, r: 0.5410409822325088
06/02/2019 10:15:50 step: 1449, epoch: 43, batch: 29, loss: 0.6002439260482788, acc: 79.6875, f1: 68.88219768781829, r: 0.6511671485690058
06/02/2019 10:15:51 *** evaluating ***
06/02/2019 10:15:51 step: 44, epoch: 43, acc: 45.72649572649573, f1: 25.06864817122218, r: 0.3351105293056046
06/02/2019 10:15:51 *** epoch: 45 ***
06/02/2019 10:15:51 *** training ***
06/02/2019 10:15:53 step: 1457, epoch: 44, batch: 4, loss: 0.651298999786377, acc: 75.0, f1: 62.229591836734684, r: 0.5189838007590536
06/02/2019 10:15:55 step: 1462, epoch: 44, batch: 9, loss: 0.3658146858215332, acc: 87.5, f1: 85.66425120772946, r: 0.6327070006419081
06/02/2019 10:15:57 step: 1467, epoch: 44, batch: 14, loss: 0.8871679902076721, acc: 65.625, f1: 48.94411144411145, r: 0.48511192270120623
06/02/2019 10:16:00 step: 1472, epoch: 44, batch: 19, loss: 0.6540673971176147, acc: 81.25, f1: 69.07891361672874, r: 0.5535447242334934
06/02/2019 10:16:02 step: 1477, epoch: 44, batch: 24, loss: 0.7566983699798584, acc: 71.875, f1: 48.23412698412698, r: 0.4792398870951855
06/02/2019 10:16:05 step: 1482, epoch: 44, batch: 29, loss: 0.7703161239624023, acc: 70.3125, f1: 50.633222986164164, r: 0.5325635131951956
06/02/2019 10:16:06 *** evaluating ***
06/02/2019 10:16:07 step: 45, epoch: 44, acc: 46.58119658119658, f1: 26.111022884120704, r: 0.2981646869383224
06/02/2019 10:16:07 *** epoch: 46 ***
06/02/2019 10:16:07 *** training ***
06/02/2019 10:16:09 step: 1490, epoch: 45, batch: 4, loss: 0.4192996919155121, acc: 87.5, f1: 76.67030706544384, r: 0.6034422744290107
06/02/2019 10:16:11 step: 1495, epoch: 45, batch: 9, loss: 0.6485431790351868, acc: 75.0, f1: 57.01493097238895, r: 0.6014543304522282
06/02/2019 10:16:13 step: 1500, epoch: 45, batch: 14, loss: 0.5279366970062256, acc: 84.375, f1: 53.311941103916126, r: 0.5877381690810657
06/02/2019 10:16:15 step: 1505, epoch: 45, batch: 19, loss: 0.5332709550857544, acc: 82.8125, f1: 58.40277777777778, r: 0.5984952483921169
06/02/2019 10:16:18 step: 1510, epoch: 45, batch: 24, loss: 0.6767420768737793, acc: 73.4375, f1: 44.301948051948045, r: 0.6030526433488868
06/02/2019 10:16:20 step: 1515, epoch: 45, batch: 29, loss: 0.6590821743011475, acc: 75.0, f1: 55.28846153846154, r: 0.592901003009228
06/02/2019 10:16:22 *** evaluating ***
06/02/2019 10:16:22 step: 46, epoch: 45, acc: 35.04273504273504, f1: 19.829347552011214, r: 0.25576880292611337
06/02/2019 10:16:22 *** epoch: 47 ***
06/02/2019 10:16:22 *** training ***
06/02/2019 10:16:25 step: 1523, epoch: 46, batch: 4, loss: 0.493440181016922, acc: 82.8125, f1: 56.3004595777705, r: 0.544207043209008
06/02/2019 10:16:27 step: 1528, epoch: 46, batch: 9, loss: 0.8724656105041504, acc: 70.3125, f1: 56.81493110064538, r: 0.5250923546595961
06/02/2019 10:16:30 step: 1533, epoch: 46, batch: 14, loss: 0.6849356293678284, acc: 73.4375, f1: 52.83613445378151, r: 0.6311991860850356
06/02/2019 10:16:32 step: 1538, epoch: 46, batch: 19, loss: 0.5400435328483582, acc: 85.9375, f1: 76.2174335807738, r: 0.6117583654059474
06/02/2019 10:16:34 step: 1543, epoch: 46, batch: 24, loss: 0.5867425203323364, acc: 75.0, f1: 43.977272727272734, r: 0.566369348597251
06/02/2019 10:16:36 step: 1548, epoch: 46, batch: 29, loss: 0.5349984765052795, acc: 81.25, f1: 69.33194563856586, r: 0.5634817389241017
06/02/2019 10:16:38 *** evaluating ***
06/02/2019 10:16:38 step: 47, epoch: 46, acc: 44.871794871794876, f1: 26.22760126063276, r: 0.2717818865598268
06/02/2019 10:16:38 *** epoch: 48 ***
06/02/2019 10:16:38 *** training ***
06/02/2019 10:16:40 step: 1556, epoch: 47, batch: 4, loss: 0.37227627635002136, acc: 84.375, f1: 78.16043083900227, r: 0.6428671053604331
06/02/2019 10:16:42 step: 1561, epoch: 47, batch: 9, loss: 0.5536521673202515, acc: 78.125, f1: 58.832071818341845, r: 0.6194936006674534
06/02/2019 10:16:45 step: 1566, epoch: 47, batch: 14, loss: 0.477798193693161, acc: 78.125, f1: 63.79282622139765, r: 0.6211380621037105
06/02/2019 10:16:47 step: 1571, epoch: 47, batch: 19, loss: 0.4882213771343231, acc: 85.9375, f1: 85.10759011158947, r: 0.6829471290937392
06/02/2019 10:16:50 step: 1576, epoch: 47, batch: 24, loss: 0.6657602787017822, acc: 73.4375, f1: 44.43521168097439, r: 0.5314780809746583
06/02/2019 10:16:52 step: 1581, epoch: 47, batch: 29, loss: 0.48518770933151245, acc: 85.9375, f1: 59.78476057346682, r: 0.6359904921131243
06/02/2019 10:16:54 *** evaluating ***
06/02/2019 10:16:54 step: 48, epoch: 47, acc: 52.13675213675214, f1: 24.918299365333855, r: 0.2896630339074769
06/02/2019 10:16:54 *** epoch: 49 ***
06/02/2019 10:16:54 *** training ***
06/02/2019 10:16:57 step: 1589, epoch: 48, batch: 4, loss: 0.48384368419647217, acc: 82.8125, f1: 66.26421197849768, r: 0.6388433373539718
06/02/2019 10:16:59 step: 1594, epoch: 48, batch: 9, loss: 0.4797784090042114, acc: 84.375, f1: 77.0419892094769, r: 0.6236661294723934
06/02/2019 10:17:01 step: 1599, epoch: 48, batch: 14, loss: 0.35339483618736267, acc: 90.625, f1: 74.6577841662981, r: 0.5696722715052017
06/02/2019 10:17:03 step: 1604, epoch: 48, batch: 19, loss: 0.5327969789505005, acc: 81.25, f1: 66.07997963484206, r: 0.5720534344206736
06/02/2019 10:17:06 step: 1609, epoch: 48, batch: 24, loss: 0.5272173881530762, acc: 82.8125, f1: 59.40051020408163, r: 0.5230416884833693
06/02/2019 10:17:08 step: 1614, epoch: 48, batch: 29, loss: 0.36205005645751953, acc: 89.0625, f1: 76.5066407515387, r: 0.6772349764139659
06/02/2019 10:17:10 *** evaluating ***
06/02/2019 10:17:10 step: 49, epoch: 48, acc: 48.717948717948715, f1: 26.680703957223216, r: 0.2785025959418384
06/02/2019 10:17:10 *** epoch: 50 ***
06/02/2019 10:17:10 *** training ***
06/02/2019 10:17:13 step: 1622, epoch: 49, batch: 4, loss: 0.6171579360961914, acc: 79.6875, f1: 61.22294372294371, r: 0.5280905245290434
06/02/2019 10:17:15 step: 1627, epoch: 49, batch: 9, loss: 0.6773126125335693, acc: 75.0, f1: 64.66918498168499, r: 0.6315511674560389
06/02/2019 10:17:17 step: 1632, epoch: 49, batch: 14, loss: 0.6539407968521118, acc: 79.6875, f1: 70.47018807523008, r: 0.5843442474930411
06/02/2019 10:17:19 step: 1637, epoch: 49, batch: 19, loss: 0.5187135934829712, acc: 81.25, f1: 66.71930310585772, r: 0.5988872126757236
06/02/2019 10:17:22 step: 1642, epoch: 49, batch: 24, loss: 0.6599807143211365, acc: 76.5625, f1: 52.501948147680125, r: 0.529939421050403
06/02/2019 10:17:24 step: 1647, epoch: 49, batch: 29, loss: 0.739206075668335, acc: 73.4375, f1: 39.45755933952528, r: 0.4762200258642501
06/02/2019 10:17:25 *** evaluating ***
06/02/2019 10:17:26 step: 50, epoch: 49, acc: 41.88034188034188, f1: 24.013514711095358, r: 0.2773857304128774
06/02/2019 10:17:26 *** epoch: 51 ***
06/02/2019 10:17:26 *** training ***
06/02/2019 10:17:28 step: 1655, epoch: 50, batch: 4, loss: 0.5683783888816833, acc: 79.6875, f1: 50.61296583850932, r: 0.6185828052405264
06/02/2019 10:17:31 step: 1660, epoch: 50, batch: 9, loss: 0.458981454372406, acc: 78.125, f1: 68.1388846265323, r: 0.6617135164398593
06/02/2019 10:17:32 step: 1665, epoch: 50, batch: 14, loss: 0.6744473576545715, acc: 76.5625, f1: 59.86339458413927, r: 0.5749896247096377
06/02/2019 10:17:34 step: 1670, epoch: 50, batch: 19, loss: 0.3568374514579773, acc: 87.5, f1: 59.59415584415585, r: 0.7410234900492854
06/02/2019 10:17:37 step: 1675, epoch: 50, batch: 24, loss: 0.6468057036399841, acc: 79.6875, f1: 66.36861122748219, r: 0.7126577598941047
06/02/2019 10:17:39 step: 1680, epoch: 50, batch: 29, loss: 0.6170631647109985, acc: 81.25, f1: 58.88881547068411, r: 0.5471435413762097
06/02/2019 10:17:40 *** evaluating ***
06/02/2019 10:17:41 step: 51, epoch: 50, acc: 44.871794871794876, f1: 26.29998291182502, r: 0.2817058192267332
06/02/2019 10:17:41 *** epoch: 52 ***
06/02/2019 10:17:41 *** training ***
06/02/2019 10:17:43 step: 1688, epoch: 51, batch: 4, loss: 0.6145800352096558, acc: 75.0, f1: 67.22222222222221, r: 0.6489484077057938
06/02/2019 10:17:45 step: 1693, epoch: 51, batch: 9, loss: 0.4680922329425812, acc: 82.8125, f1: 63.22077922077922, r: 0.6527198835596594
06/02/2019 10:17:48 step: 1698, epoch: 51, batch: 14, loss: 0.588128924369812, acc: 84.375, f1: 67.5297619047619, r: 0.7331979850823598
06/02/2019 10:17:51 step: 1703, epoch: 51, batch: 19, loss: 0.584272027015686, acc: 76.5625, f1: 61.75482108047898, r: 0.6271293116984257
06/02/2019 10:17:53 step: 1708, epoch: 51, batch: 24, loss: 0.4871182441711426, acc: 82.8125, f1: 62.66317016317016, r: 0.6113908364317724
06/02/2019 10:17:55 step: 1713, epoch: 51, batch: 29, loss: 0.5911690592765808, acc: 76.5625, f1: 46.350250626566414, r: 0.6172860167531175
06/02/2019 10:17:56 *** evaluating ***
06/02/2019 10:17:57 step: 52, epoch: 51, acc: 47.863247863247864, f1: 23.773537000439177, r: 0.2726964038870754
06/02/2019 10:17:57 *** epoch: 53 ***
06/02/2019 10:17:57 *** training ***
06/02/2019 10:17:59 step: 1721, epoch: 52, batch: 4, loss: 0.4233851134777069, acc: 85.9375, f1: 61.72902097902097, r: 0.5918457487988961
06/02/2019 10:18:01 step: 1726, epoch: 52, batch: 9, loss: 0.37122613191604614, acc: 85.9375, f1: 48.4963768115942, r: 0.5978583942236044
06/02/2019 10:18:04 step: 1731, epoch: 52, batch: 14, loss: 0.4149315655231476, acc: 84.375, f1: 62.651515151515156, r: 0.6343827928637708
06/02/2019 10:18:06 step: 1736, epoch: 52, batch: 19, loss: 0.4935038685798645, acc: 84.375, f1: 69.76499721134282, r: 0.6208108062326013
06/02/2019 10:18:08 step: 1741, epoch: 52, batch: 24, loss: 0.43967264890670776, acc: 84.375, f1: 60.247564935064936, r: 0.6797365627839269
06/02/2019 10:18:10 step: 1746, epoch: 52, batch: 29, loss: 0.21294301748275757, acc: 93.75, f1: 65.91431556948798, r: 0.6341670267730827
06/02/2019 10:18:11 *** evaluating ***
06/02/2019 10:18:12 step: 53, epoch: 52, acc: 51.28205128205128, f1: 27.177017902241595, r: 0.271050268533553
06/02/2019 10:18:12 *** epoch: 54 ***
06/02/2019 10:18:12 *** training ***
06/02/2019 10:18:14 step: 1754, epoch: 53, batch: 4, loss: 0.3499769866466522, acc: 85.9375, f1: 55.26432606941081, r: 0.5920273644344693
06/02/2019 10:18:16 step: 1759, epoch: 53, batch: 9, loss: 0.3885287344455719, acc: 82.8125, f1: 61.32823129251701, r: 0.6670742207456014
06/02/2019 10:18:18 step: 1764, epoch: 53, batch: 14, loss: 0.43250972032546997, acc: 79.6875, f1: 59.7849025974026, r: 0.6132165632111504
06/02/2019 10:18:20 step: 1769, epoch: 53, batch: 19, loss: 0.43851831555366516, acc: 81.25, f1: 70.02198613494295, r: 0.561472616341294
06/02/2019 10:18:23 step: 1774, epoch: 53, batch: 24, loss: 0.447770357131958, acc: 84.375, f1: 50.97943722943723, r: 0.5934960327250336
06/02/2019 10:18:25 step: 1779, epoch: 53, batch: 29, loss: 0.42915308475494385, acc: 87.5, f1: 78.40224820552436, r: 0.6461122632146339
06/02/2019 10:18:26 *** evaluating ***
06/02/2019 10:18:27 step: 54, epoch: 53, acc: 44.44444444444444, f1: 24.80942678061922, r: 0.2750044377156308
06/02/2019 10:18:27 *** epoch: 55 ***
06/02/2019 10:18:27 *** training ***
06/02/2019 10:18:29 step: 1787, epoch: 54, batch: 4, loss: 0.5892235636711121, acc: 76.5625, f1: 71.97098194116356, r: 0.5626379334123389
06/02/2019 10:18:31 step: 1792, epoch: 54, batch: 9, loss: 0.33201995491981506, acc: 90.625, f1: 74.08163952079687, r: 0.6682369746006291
06/02/2019 10:18:34 step: 1797, epoch: 54, batch: 14, loss: 0.5521516799926758, acc: 85.9375, f1: 71.4901696171046, r: 0.6066361612500986
06/02/2019 10:18:36 step: 1802, epoch: 54, batch: 19, loss: 0.43567049503326416, acc: 81.25, f1: 55.340401393532325, r: 0.5529966899470579
06/02/2019 10:18:39 step: 1807, epoch: 54, batch: 24, loss: 0.4562646150588989, acc: 82.8125, f1: 72.54005427307536, r: 0.6212167568504517
06/02/2019 10:18:41 step: 1812, epoch: 54, batch: 29, loss: 0.4612891376018524, acc: 81.25, f1: 64.52049664390863, r: 0.5362738570691351
06/02/2019 10:18:42 *** evaluating ***
06/02/2019 10:18:43 step: 55, epoch: 54, acc: 45.2991452991453, f1: 27.851979863624948, r: 0.2696757080814636
06/02/2019 10:18:43 *** epoch: 56 ***
06/02/2019 10:18:43 *** training ***
06/02/2019 10:18:45 step: 1820, epoch: 55, batch: 4, loss: 0.6186403632164001, acc: 79.6875, f1: 52.439692982456144, r: 0.6033681141284177
06/02/2019 10:18:47 step: 1825, epoch: 55, batch: 9, loss: 0.5735838413238525, acc: 76.5625, f1: 53.46732610754499, r: 0.6047773659586297
06/02/2019 10:18:49 step: 1830, epoch: 55, batch: 14, loss: 0.5603451728820801, acc: 81.25, f1: 79.42379616643291, r: 0.6436367496031374
06/02/2019 10:18:52 step: 1835, epoch: 55, batch: 19, loss: 0.5205749869346619, acc: 82.8125, f1: 58.628952569169954, r: 0.5516433412910285
06/02/2019 10:18:54 step: 1840, epoch: 55, batch: 24, loss: 0.6755857467651367, acc: 71.875, f1: 60.89826839826841, r: 0.5954676401626997
06/02/2019 10:18:57 step: 1845, epoch: 55, batch: 29, loss: 0.4720417559146881, acc: 84.375, f1: 63.89972176610108, r: 0.6753429339098695
06/02/2019 10:18:57 *** evaluating ***
06/02/2019 10:18:58 step: 56, epoch: 55, acc: 52.13675213675214, f1: 28.28559661732544, r: 0.2693755351476547
06/02/2019 10:18:58 *** epoch: 57 ***
06/02/2019 10:18:58 *** training ***
06/02/2019 10:19:01 step: 1853, epoch: 56, batch: 4, loss: 0.42352795600891113, acc: 87.5, f1: 70.75366988900824, r: 0.6809154931051986
06/02/2019 10:19:03 step: 1858, epoch: 56, batch: 9, loss: 0.27806565165519714, acc: 92.1875, f1: 87.04047275475847, r: 0.6309830316422722
06/02/2019 10:19:05 step: 1863, epoch: 56, batch: 14, loss: 0.5072964429855347, acc: 81.25, f1: 62.115800865800864, r: 0.6291003714514414
06/02/2019 10:19:07 step: 1868, epoch: 56, batch: 19, loss: 0.47389546036720276, acc: 79.6875, f1: 55.88166873449132, r: 0.6345246947474398
06/02/2019 10:19:10 step: 1873, epoch: 56, batch: 24, loss: 0.2692180275917053, acc: 93.75, f1: 82.62680510800811, r: 0.591762820655989
06/02/2019 10:19:12 step: 1878, epoch: 56, batch: 29, loss: 0.4899463355541229, acc: 78.125, f1: 67.46392496392497, r: 0.6724581453057676
06/02/2019 10:19:13 *** evaluating ***
06/02/2019 10:19:14 step: 57, epoch: 56, acc: 50.85470085470085, f1: 27.239985508760405, r: 0.3106026667257339
06/02/2019 10:19:14 *** epoch: 58 ***
06/02/2019 10:19:14 *** training ***
06/02/2019 10:19:16 step: 1886, epoch: 57, batch: 4, loss: 0.24024567008018494, acc: 96.875, f1: 79.53917050691244, r: 0.6711622673299745
06/02/2019 10:19:18 step: 1891, epoch: 57, batch: 9, loss: 0.4343104958534241, acc: 84.375, f1: 58.506493506493506, r: 0.5564984251395034
06/02/2019 10:19:20 step: 1896, epoch: 57, batch: 14, loss: 0.5092253684997559, acc: 79.6875, f1: 61.744422994423, r: 0.646899582311181
06/02/2019 10:19:22 step: 1901, epoch: 57, batch: 19, loss: 0.5257735848426819, acc: 79.6875, f1: 60.069022289766984, r: 0.5813700835383319
06/02/2019 10:19:25 step: 1906, epoch: 57, batch: 24, loss: 0.5008341670036316, acc: 78.125, f1: 70.76530612244898, r: 0.6087812990242468
06/02/2019 10:19:28 step: 1911, epoch: 57, batch: 29, loss: 0.6748424768447876, acc: 79.6875, f1: 58.45358345358345, r: 0.6486942886563338
06/02/2019 10:19:29 *** evaluating ***
06/02/2019 10:19:29 step: 58, epoch: 57, acc: 56.41025641025641, f1: 31.119976803524118, r: 0.30101983914374064
06/02/2019 10:19:29 *** epoch: 59 ***
06/02/2019 10:19:29 *** training ***
06/02/2019 10:19:31 step: 1919, epoch: 58, batch: 4, loss: 0.3361584544181824, acc: 89.0625, f1: 73.34131823461092, r: 0.7041895296881083
06/02/2019 10:19:34 step: 1924, epoch: 58, batch: 9, loss: 0.36585336923599243, acc: 90.625, f1: 76.47208298524089, r: 0.7086214336243226
06/02/2019 10:19:36 step: 1929, epoch: 58, batch: 14, loss: 0.32465696334838867, acc: 81.25, f1: 66.6979916635089, r: 0.6142748734182775
06/02/2019 10:19:38 step: 1934, epoch: 58, batch: 19, loss: 0.4059363603591919, acc: 82.8125, f1: 61.734131521867376, r: 0.6542692739737825
06/02/2019 10:19:40 step: 1939, epoch: 58, batch: 24, loss: 0.48643338680267334, acc: 81.25, f1: 76.99606158252774, r: 0.6453425024973641
06/02/2019 10:19:42 step: 1944, epoch: 58, batch: 29, loss: 0.38044604659080505, acc: 87.5, f1: 66.96974925235794, r: 0.7202064155093351
06/02/2019 10:19:44 *** evaluating ***
06/02/2019 10:19:44 step: 59, epoch: 58, acc: 43.162393162393165, f1: 26.082340011313022, r: 0.2996911083480671
06/02/2019 10:19:44 *** epoch: 60 ***
06/02/2019 10:19:44 *** training ***
06/02/2019 10:19:47 step: 1952, epoch: 59, batch: 4, loss: 0.3013180196285248, acc: 89.0625, f1: 82.71654842435242, r: 0.6646560501909099
06/02/2019 10:19:49 step: 1957, epoch: 59, batch: 9, loss: 0.2721741497516632, acc: 90.625, f1: 78.31168010970968, r: 0.5697578094933865
06/02/2019 10:19:51 step: 1962, epoch: 59, batch: 14, loss: 0.3185507655143738, acc: 87.5, f1: 73.53208960351819, r: 0.6028149447683385
06/02/2019 10:19:53 step: 1967, epoch: 59, batch: 19, loss: 0.4198145568370819, acc: 85.9375, f1: 59.307060755336614, r: 0.593683558877939
06/02/2019 10:19:56 step: 1972, epoch: 59, batch: 24, loss: 0.29625365138053894, acc: 89.0625, f1: 81.65160926030491, r: 0.6934297618735732
06/02/2019 10:19:58 step: 1977, epoch: 59, batch: 29, loss: 0.29481828212738037, acc: 92.1875, f1: 79.08747563352826, r: 0.6229145392260077
06/02/2019 10:19:59 *** evaluating ***
06/02/2019 10:20:00 step: 60, epoch: 59, acc: 48.29059829059829, f1: 25.705897018275493, r: 0.27993500182868747
06/02/2019 10:20:00 *** epoch: 61 ***
06/02/2019 10:20:00 *** training ***
06/02/2019 10:20:02 step: 1985, epoch: 60, batch: 4, loss: 0.21930721402168274, acc: 95.3125, f1: 96.53451544552247, r: 0.6416199290962279
06/02/2019 10:20:04 step: 1990, epoch: 60, batch: 9, loss: 0.1853644698858261, acc: 95.3125, f1: 88.29663090532655, r: 0.6731822153763459
06/02/2019 10:20:07 step: 1995, epoch: 60, batch: 14, loss: 0.24016639590263367, acc: 93.75, f1: 86.25923096511332, r: 0.6477603867132679
06/02/2019 10:20:10 step: 2000, epoch: 60, batch: 19, loss: 0.27563971281051636, acc: 89.0625, f1: 59.80821262071263, r: 0.6571545505317737
06/02/2019 10:20:12 step: 2005, epoch: 60, batch: 24, loss: 0.3015478849411011, acc: 85.9375, f1: 77.18676376097665, r: 0.6199880959306843
06/02/2019 10:20:14 step: 2010, epoch: 60, batch: 29, loss: 0.5243842005729675, acc: 84.375, f1: 69.61031003310414, r: 0.624991763969639
06/02/2019 10:20:15 *** evaluating ***
06/02/2019 10:20:15 step: 61, epoch: 60, acc: 52.13675213675214, f1: 26.09120151145549, r: 0.2633223320864222
06/02/2019 10:20:15 *** epoch: 62 ***
06/02/2019 10:20:15 *** training ***
06/02/2019 10:20:18 step: 2018, epoch: 61, batch: 4, loss: 0.19627365469932556, acc: 95.3125, f1: 91.83863755292326, r: 0.6852254426065292
06/02/2019 10:20:20 step: 2023, epoch: 61, batch: 9, loss: 0.19341304898262024, acc: 93.75, f1: 78.41712051389472, r: 0.7803563318084319
06/02/2019 10:20:22 step: 2028, epoch: 61, batch: 14, loss: 0.391941100358963, acc: 93.75, f1: 81.96253746253745, r: 0.6760427568985833
06/02/2019 10:20:25 step: 2033, epoch: 61, batch: 19, loss: 0.2694631814956665, acc: 90.625, f1: 91.0878040916497, r: 0.684000175962149
06/02/2019 10:20:27 step: 2038, epoch: 61, batch: 24, loss: 0.38600605726242065, acc: 82.8125, f1: 66.77470330237358, r: 0.6503250482471945
06/02/2019 10:20:29 step: 2043, epoch: 61, batch: 29, loss: 0.2323734313249588, acc: 92.1875, f1: 69.44444444444444, r: 0.6196583116012848
06/02/2019 10:20:30 *** evaluating ***
06/02/2019 10:20:31 step: 62, epoch: 61, acc: 50.427350427350426, f1: 28.0587101971153, r: 0.26389497164853154
06/02/2019 10:20:31 *** epoch: 63 ***
06/02/2019 10:20:31 *** training ***
06/02/2019 10:20:33 step: 2051, epoch: 62, batch: 4, loss: 0.20388373732566833, acc: 93.75, f1: 87.07816459917301, r: 0.6675516246926009
06/02/2019 10:20:35 step: 2056, epoch: 62, batch: 9, loss: 0.2018589973449707, acc: 96.875, f1: 93.66883116883116, r: 0.7187209210167134
06/02/2019 10:20:37 step: 2061, epoch: 62, batch: 14, loss: 0.2640446424484253, acc: 89.0625, f1: 73.51190476190477, r: 0.6606102335908632
06/02/2019 10:20:39 step: 2066, epoch: 62, batch: 19, loss: 0.4255923330783844, acc: 87.5, f1: 68.89339826839827, r: 0.6038031642404373
06/02/2019 10:20:42 step: 2071, epoch: 62, batch: 24, loss: 0.1970132291316986, acc: 92.1875, f1: 66.00185528756958, r: 0.5392653704787379
06/02/2019 10:20:44 step: 2076, epoch: 62, batch: 29, loss: 0.2502371668815613, acc: 92.1875, f1: 70.81043956043955, r: 0.6727783785331146
06/02/2019 10:20:45 *** evaluating ***
06/02/2019 10:20:46 step: 63, epoch: 62, acc: 44.44444444444444, f1: 24.5518435978658, r: 0.2537170874111325
06/02/2019 10:20:46 *** epoch: 64 ***
06/02/2019 10:20:46 *** training ***
06/02/2019 10:20:48 step: 2084, epoch: 63, batch: 4, loss: 0.27232152223587036, acc: 87.5, f1: 72.29131652661064, r: 0.645020694371487
06/02/2019 10:20:51 step: 2089, epoch: 63, batch: 9, loss: 0.11498522758483887, acc: 98.4375, f1: 86.90476190476191, r: 0.8362200626357735
06/02/2019 10:20:53 step: 2094, epoch: 63, batch: 14, loss: 0.22684699296951294, acc: 90.625, f1: 79.19984387197503, r: 0.6453433607581569
06/02/2019 10:20:55 step: 2099, epoch: 63, batch: 19, loss: 0.22405336797237396, acc: 93.75, f1: 83.90109890109889, r: 0.5912152089653792
06/02/2019 10:20:57 step: 2104, epoch: 63, batch: 24, loss: 0.28736618161201477, acc: 93.75, f1: 78.37301587301587, r: 0.7318788763834703
06/02/2019 10:20:59 step: 2109, epoch: 63, batch: 29, loss: 0.23120571672916412, acc: 92.1875, f1: 83.74030313879938, r: 0.6169188568101851
06/02/2019 10:21:01 *** evaluating ***
06/02/2019 10:21:01 step: 64, epoch: 63, acc: 50.427350427350426, f1: 27.48075451123737, r: 0.2954201348572533
06/02/2019 10:21:01 *** epoch: 65 ***
06/02/2019 10:21:01 *** training ***
06/02/2019 10:21:03 step: 2117, epoch: 64, batch: 4, loss: 0.1665888875722885, acc: 98.4375, f1: 97.74891774891775, r: 0.6909038154255638
06/02/2019 10:21:05 step: 2122, epoch: 64, batch: 9, loss: 0.14842623472213745, acc: 96.875, f1: 92.35294117647058, r: 0.7499533748228275
06/02/2019 10:21:08 step: 2127, epoch: 64, batch: 14, loss: 0.1733035147190094, acc: 95.3125, f1: 87.75132275132276, r: 0.7832983021425857
06/02/2019 10:21:10 step: 2132, epoch: 64, batch: 19, loss: 0.2378605306148529, acc: 93.75, f1: 87.0204081632653, r: 0.6853679445038211
06/02/2019 10:21:13 step: 2137, epoch: 64, batch: 24, loss: 0.18713867664337158, acc: 93.75, f1: 80.578231292517, r: 0.7093821364768028
06/02/2019 10:21:15 step: 2142, epoch: 64, batch: 29, loss: 0.1632910966873169, acc: 96.875, f1: 93.76961707470181, r: 0.7025532690129256
06/02/2019 10:21:16 *** evaluating ***
06/02/2019 10:21:17 step: 65, epoch: 64, acc: 49.572649572649574, f1: 28.09273420674113, r: 0.2835814550163614
06/02/2019 10:21:17 *** epoch: 66 ***
06/02/2019 10:21:17 *** training ***
06/02/2019 10:21:19 step: 2150, epoch: 65, batch: 4, loss: 0.18317565321922302, acc: 95.3125, f1: 69.62121212121212, r: 0.7088174858118664
06/02/2019 10:21:22 step: 2155, epoch: 65, batch: 9, loss: 0.14247924089431763, acc: 95.3125, f1: 93.84365634365635, r: 0.7577081352436801
06/02/2019 10:21:24 step: 2160, epoch: 65, batch: 14, loss: 0.113080233335495, acc: 98.4375, f1: 99.12121212121212, r: 0.7344841381135145
06/02/2019 10:21:26 step: 2165, epoch: 65, batch: 19, loss: 0.18776154518127441, acc: 95.3125, f1: 83.18786605551311, r: 0.7681468039160269
06/02/2019 10:21:28 step: 2170, epoch: 65, batch: 24, loss: 0.10173793137073517, acc: 96.875, f1: 92.47899159663865, r: 0.7531317837350404
06/02/2019 10:21:31 step: 2175, epoch: 65, batch: 29, loss: 0.14709287881851196, acc: 95.3125, f1: 95.9448223733938, r: 0.648845126460104
06/02/2019 10:21:32 *** evaluating ***
06/02/2019 10:21:32 step: 66, epoch: 65, acc: 45.72649572649573, f1: 25.951979899394818, r: 0.26293700007205173
06/02/2019 10:21:32 *** epoch: 67 ***
06/02/2019 10:21:32 *** training ***
06/02/2019 10:21:35 step: 2183, epoch: 66, batch: 4, loss: 0.20634067058563232, acc: 95.3125, f1: 92.26289682539684, r: 0.7069794716692663
06/02/2019 10:21:37 step: 2188, epoch: 66, batch: 9, loss: 0.16511908173561096, acc: 95.3125, f1: 78.69832826747721, r: 0.7178436574660699
06/02/2019 10:21:40 step: 2193, epoch: 66, batch: 14, loss: 0.25040724873542786, acc: 92.1875, f1: 94.56406166932483, r: 0.7375039266085599
06/02/2019 10:21:42 step: 2198, epoch: 66, batch: 19, loss: 0.23773255944252014, acc: 90.625, f1: 84.47712418300654, r: 0.6042232998066175
06/02/2019 10:21:44 step: 2203, epoch: 66, batch: 24, loss: 0.2525367736816406, acc: 92.1875, f1: 89.35399159663866, r: 0.7474040604310693
06/02/2019 10:21:46 step: 2208, epoch: 66, batch: 29, loss: 0.17210035026073456, acc: 93.75, f1: 76.27483627483628, r: 0.5279657485036247
06/02/2019 10:21:47 *** evaluating ***
06/02/2019 10:21:48 step: 67, epoch: 66, acc: 52.13675213675214, f1: 28.43927166010805, r: 0.26688936050235973
06/02/2019 10:21:48 *** epoch: 68 ***
06/02/2019 10:21:48 *** training ***
06/02/2019 10:21:50 step: 2216, epoch: 67, batch: 4, loss: 0.2174074947834015, acc: 92.1875, f1: 77.39101995313176, r: 0.7647391241676196
06/02/2019 10:21:52 step: 2221, epoch: 67, batch: 9, loss: 0.11032544821500778, acc: 98.4375, f1: 96.19047619047619, r: 0.6810229806029233
06/02/2019 10:21:54 step: 2226, epoch: 67, batch: 14, loss: 0.1498599350452423, acc: 96.875, f1: 82.07792207792208, r: 0.7079828807409371
06/02/2019 10:21:57 step: 2231, epoch: 67, batch: 19, loss: 0.11260794848203659, acc: 98.4375, f1: 95.17543859649122, r: 0.7449595884811858
06/02/2019 10:21:59 step: 2236, epoch: 67, batch: 24, loss: 0.17561498284339905, acc: 93.75, f1: 91.21160347075505, r: 0.6534901543753635
06/02/2019 10:22:01 step: 2241, epoch: 67, batch: 29, loss: 0.25311315059661865, acc: 92.1875, f1: 79.71967473709954, r: 0.729098480092949
06/02/2019 10:22:03 *** evaluating ***
06/02/2019 10:22:04 step: 68, epoch: 67, acc: 49.572649572649574, f1: 27.627269993545735, r: 0.28562600405818284
06/02/2019 10:22:04 *** epoch: 69 ***
06/02/2019 10:22:04 *** training ***
06/02/2019 10:22:06 step: 2249, epoch: 68, batch: 4, loss: 0.18801814317703247, acc: 96.875, f1: 92.6190476190476, r: 0.7480322466361264
06/02/2019 10:22:08 step: 2254, epoch: 68, batch: 9, loss: 0.07096192985773087, acc: 98.4375, f1: 93.19727891156462, r: 0.6401710273649778
06/02/2019 10:22:11 step: 2259, epoch: 68, batch: 14, loss: 0.08270885795354843, acc: 98.4375, f1: 84.61538461538461, r: 0.6114425929190206
06/02/2019 10:22:13 step: 2264, epoch: 68, batch: 19, loss: 0.1082579642534256, acc: 98.4375, f1: 96.1111111111111, r: 0.7414555866821103
06/02/2019 10:22:15 step: 2269, epoch: 68, batch: 24, loss: 0.09766978025436401, acc: 98.4375, f1: 97.3809523809524, r: 0.7899431878093534
06/02/2019 10:22:18 step: 2274, epoch: 68, batch: 29, loss: 0.10067159682512283, acc: 100.0, f1: 100.0, r: 0.7134776106146878
06/02/2019 10:22:19 *** evaluating ***
06/02/2019 10:22:19 step: 69, epoch: 68, acc: 49.572649572649574, f1: 28.08192913970539, r: 0.2904086928643275
06/02/2019 10:22:19 *** epoch: 70 ***
06/02/2019 10:22:19 *** training ***
06/02/2019 10:22:21 step: 2282, epoch: 69, batch: 4, loss: 0.11169055849313736, acc: 96.875, f1: 93.51473922902495, r: 0.7093613844151941
06/02/2019 10:22:24 step: 2287, epoch: 69, batch: 9, loss: 0.1322871595621109, acc: 93.75, f1: 79.7137960875407, r: 0.7480065382348676
06/02/2019 10:22:26 step: 2292, epoch: 69, batch: 14, loss: 0.09143497049808502, acc: 98.4375, f1: 86.76470588235294, r: 0.8106173251164673
06/02/2019 10:22:28 step: 2297, epoch: 69, batch: 19, loss: 0.13373765349388123, acc: 95.3125, f1: 87.1861471861472, r: 0.6573900907589649
06/02/2019 10:22:30 step: 2302, epoch: 69, batch: 24, loss: 0.11225221306085587, acc: 96.875, f1: 91.85913185913186, r: 0.7901416881411772
06/02/2019 10:22:33 step: 2307, epoch: 69, batch: 29, loss: 0.16055816411972046, acc: 95.3125, f1: 86.45833333333333, r: 0.7021688869157812
06/02/2019 10:22:34 *** evaluating ***
06/02/2019 10:22:34 step: 70, epoch: 69, acc: 48.717948717948715, f1: 26.455342902711322, r: 0.2575251762661291
06/02/2019 10:22:34 *** epoch: 71 ***
06/02/2019 10:22:34 *** training ***
06/02/2019 10:22:36 step: 2315, epoch: 70, batch: 4, loss: 0.14279869198799133, acc: 95.3125, f1: 95.60646392944528, r: 0.6235703851793957
06/02/2019 10:22:39 step: 2320, epoch: 70, batch: 9, loss: 0.09298048168420792, acc: 98.4375, f1: 91.66666666666666, r: 0.6480791408708272
06/02/2019 10:22:41 step: 2325, epoch: 70, batch: 14, loss: 0.16153225302696228, acc: 95.3125, f1: 93.72116370471633, r: 0.8294691630792133
06/02/2019 10:22:44 step: 2330, epoch: 70, batch: 19, loss: 0.08770182728767395, acc: 98.4375, f1: 83.6734693877551, r: 0.706610261078161
06/02/2019 10:22:46 step: 2335, epoch: 70, batch: 24, loss: 0.0914536640048027, acc: 98.4375, f1: 93.65079365079364, r: 0.7037079361739084
06/02/2019 10:22:48 step: 2340, epoch: 70, batch: 29, loss: 0.13817909359931946, acc: 93.75, f1: 78.73543123543124, r: 0.7192973365204297
06/02/2019 10:22:49 *** evaluating ***
06/02/2019 10:22:50 step: 71, epoch: 70, acc: 49.14529914529914, f1: 25.78192034134657, r: 0.24922034607960328
06/02/2019 10:22:50 *** epoch: 72 ***
06/02/2019 10:22:50 *** training ***
06/02/2019 10:22:53 step: 2348, epoch: 71, batch: 4, loss: 0.11493054777383804, acc: 96.875, f1: 95.76826682089839, r: 0.6853884005935077
06/02/2019 10:22:55 step: 2353, epoch: 71, batch: 9, loss: 0.06664280593395233, acc: 100.0, f1: 100.0, r: 0.6940923376314365
06/02/2019 10:22:57 step: 2358, epoch: 71, batch: 14, loss: 0.08474641293287277, acc: 98.4375, f1: 94.04761904761905, r: 0.7588841316356928
06/02/2019 10:23:00 step: 2363, epoch: 71, batch: 19, loss: 0.12506072223186493, acc: 96.875, f1: 83.24594795183032, r: 0.6397647035483555
06/02/2019 10:23:02 step: 2368, epoch: 71, batch: 24, loss: 0.05717644840478897, acc: 100.0, f1: 100.0, r: 0.7282989762573641
06/02/2019 10:23:04 step: 2373, epoch: 71, batch: 29, loss: 0.11195869743824005, acc: 96.875, f1: 94.79306874264857, r: 0.6748731554788997
06/02/2019 10:23:05 *** evaluating ***
06/02/2019 10:23:06 step: 72, epoch: 71, acc: 48.29059829059829, f1: 27.52239622933739, r: 0.27914219580533217
06/02/2019 10:23:06 *** epoch: 73 ***
06/02/2019 10:23:06 *** training ***
06/02/2019 10:23:08 step: 2381, epoch: 72, batch: 4, loss: 0.13892129063606262, acc: 95.3125, f1: 80.16384778012684, r: 0.7460527045121979
06/02/2019 10:23:10 step: 2386, epoch: 72, batch: 9, loss: 0.08699667453765869, acc: 98.4375, f1: 94.28571428571428, r: 0.657841776867473
06/02/2019 10:23:12 step: 2391, epoch: 72, batch: 14, loss: 0.15141665935516357, acc: 93.75, f1: 86.47794601711652, r: 0.6708837133323582
06/02/2019 10:23:15 step: 2396, epoch: 72, batch: 19, loss: 0.05942203849554062, acc: 98.4375, f1: 93.19727891156461, r: 0.6386399112832543
06/02/2019 10:23:17 step: 2401, epoch: 72, batch: 24, loss: 0.1487939953804016, acc: 95.3125, f1: 94.53514739229026, r: 0.7762034456443634
06/02/2019 10:23:19 step: 2406, epoch: 72, batch: 29, loss: 0.03880980238318443, acc: 100.0, f1: 100.0, r: 0.7224748837517161
06/02/2019 10:23:21 *** evaluating ***
06/02/2019 10:23:21 step: 73, epoch: 72, acc: 50.427350427350426, f1: 27.691881239675354, r: 0.25921752213678745
06/02/2019 10:23:21 *** epoch: 74 ***
06/02/2019 10:23:21 *** training ***
06/02/2019 10:23:24 step: 2414, epoch: 73, batch: 4, loss: 0.2052982896566391, acc: 92.1875, f1: 83.42490842490842, r: 0.6669630945581955
06/02/2019 10:23:26 step: 2419, epoch: 73, batch: 9, loss: 0.12711229920387268, acc: 93.75, f1: 87.79158040027608, r: 0.6856991113900779
06/02/2019 10:23:28 step: 2424, epoch: 73, batch: 14, loss: 0.22604809701442719, acc: 95.3125, f1: 80.29478458049887, r: 0.5135023874748187
06/02/2019 10:23:31 step: 2429, epoch: 73, batch: 19, loss: 0.13129110634326935, acc: 96.875, f1: 98.0952380952381, r: 0.7086125276864333
06/02/2019 10:23:33 step: 2434, epoch: 73, batch: 24, loss: 0.15925204753875732, acc: 95.3125, f1: 96.58333333333333, r: 0.7890064004748705
06/02/2019 10:23:35 step: 2439, epoch: 73, batch: 29, loss: 0.1567651629447937, acc: 93.75, f1: 76.63784193389456, r: 0.7123258517691812
06/02/2019 10:23:36 *** evaluating ***
06/02/2019 10:23:37 step: 74, epoch: 73, acc: 48.717948717948715, f1: 29.148301868848936, r: 0.27664323235941246
06/02/2019 10:23:37 *** epoch: 75 ***
06/02/2019 10:23:37 *** training ***
06/02/2019 10:23:39 step: 2447, epoch: 74, batch: 4, loss: 0.06082506477832794, acc: 96.875, f1: 81.29251700680273, r: 0.6921157915360467
06/02/2019 10:23:41 step: 2452, epoch: 74, batch: 9, loss: 0.13414229452610016, acc: 96.875, f1: 95.21549058494871, r: 0.7172454861312005
06/02/2019 10:23:44 step: 2457, epoch: 74, batch: 14, loss: 0.04701998084783554, acc: 100.0, f1: 100.0, r: 0.7303419868267917
06/02/2019 10:23:46 step: 2462, epoch: 74, batch: 19, loss: 0.07986129820346832, acc: 96.875, f1: 98.13759278897136, r: 0.6753332041409313
06/02/2019 10:23:48 step: 2467, epoch: 74, batch: 24, loss: 0.2090294510126114, acc: 95.3125, f1: 91.95991440054281, r: 0.6912257795496489
06/02/2019 10:23:51 step: 2472, epoch: 74, batch: 29, loss: 0.06360454857349396, acc: 100.0, f1: 100.0, r: 0.7939933420526013
06/02/2019 10:23:52 *** evaluating ***
06/02/2019 10:23:52 step: 75, epoch: 74, acc: 50.85470085470085, f1: 27.94597233283903, r: 0.28149976259293563
06/02/2019 10:23:52 *** epoch: 76 ***
06/02/2019 10:23:52 *** training ***
06/02/2019 10:23:55 step: 2480, epoch: 75, batch: 4, loss: 0.08634374290704727, acc: 96.875, f1: 92.62841530054644, r: 0.7801158578045881
06/02/2019 10:23:57 step: 2485, epoch: 75, batch: 9, loss: 0.09423141181468964, acc: 98.4375, f1: 99.19073845116331, r: 0.7103715902583653
06/02/2019 10:23:59 step: 2490, epoch: 75, batch: 14, loss: 0.1017765924334526, acc: 98.4375, f1: 91.66666666666666, r: 0.7372809787105337
06/02/2019 10:24:01 step: 2495, epoch: 75, batch: 19, loss: 0.05171777307987213, acc: 100.0, f1: 100.0, r: 0.7388199036099047
06/02/2019 10:24:04 step: 2500, epoch: 75, batch: 24, loss: 0.10620181262493134, acc: 96.875, f1: 90.65759637188208, r: 0.6331194514273337
06/02/2019 10:24:06 step: 2505, epoch: 75, batch: 29, loss: 0.06473774462938309, acc: 96.875, f1: 81.82539682539682, r: 0.7140037614466904
06/02/2019 10:24:07 *** evaluating ***
06/02/2019 10:24:08 step: 76, epoch: 75, acc: 50.427350427350426, f1: 28.621034418979214, r: 0.2576029239341379
06/02/2019 10:24:08 *** epoch: 77 ***
06/02/2019 10:24:08 *** training ***
06/02/2019 10:24:10 step: 2513, epoch: 76, batch: 4, loss: 0.054732851684093475, acc: 100.0, f1: 100.0, r: 0.7444195369578895
06/02/2019 10:24:12 step: 2518, epoch: 76, batch: 9, loss: 0.10067521780729294, acc: 98.4375, f1: 98.79336349924586, r: 0.692268526808747
06/02/2019 10:24:14 step: 2523, epoch: 76, batch: 14, loss: 0.0780545026063919, acc: 98.4375, f1: 99.19289749798224, r: 0.781082022963317
06/02/2019 10:24:17 step: 2528, epoch: 76, batch: 19, loss: 0.060106612741947174, acc: 98.4375, f1: 94.04761904761905, r: 0.7487089884103694
06/02/2019 10:24:19 step: 2533, epoch: 76, batch: 24, loss: 0.26114946603775024, acc: 95.3125, f1: 86.36363636363636, r: 0.6741907937868501
06/02/2019 10:24:21 step: 2538, epoch: 76, batch: 29, loss: 0.12841381132602692, acc: 93.75, f1: 89.08237207926649, r: 0.7960408200629001
06/02/2019 10:24:23 *** evaluating ***
06/02/2019 10:24:23 step: 77, epoch: 76, acc: 52.56410256410257, f1: 27.82702148536329, r: 0.27900674235907524
06/02/2019 10:24:23 *** epoch: 78 ***
06/02/2019 10:24:23 *** training ***
06/02/2019 10:24:26 step: 2546, epoch: 77, batch: 4, loss: 0.04862745851278305, acc: 98.4375, f1: 99.37611408199643, r: 0.7786986761276365
06/02/2019 10:24:28 step: 2551, epoch: 77, batch: 9, loss: 0.12124279141426086, acc: 96.875, f1: 91.81818181818183, r: 0.7208277190931424
06/02/2019 10:24:30 step: 2556, epoch: 77, batch: 14, loss: 0.12658321857452393, acc: 96.875, f1: 97.37394957983193, r: 0.6783243218366235
06/02/2019 10:24:32 step: 2561, epoch: 77, batch: 19, loss: 0.07664288580417633, acc: 96.875, f1: 96.22536193964766, r: 0.7257177800640655
06/02/2019 10:24:34 step: 2566, epoch: 77, batch: 24, loss: 0.13838635385036469, acc: 95.3125, f1: 94.15244937433388, r: 0.6360104126441316
06/02/2019 10:24:37 step: 2571, epoch: 77, batch: 29, loss: 0.03766504302620888, acc: 100.0, f1: 100.0, r: 0.7681187316121939
06/02/2019 10:24:38 *** evaluating ***
06/02/2019 10:24:39 step: 78, epoch: 77, acc: 48.29059829059829, f1: 27.898596680813537, r: 0.28049689519215326
06/02/2019 10:24:39 *** epoch: 79 ***
06/02/2019 10:24:39 *** training ***
06/02/2019 10:24:41 step: 2579, epoch: 78, batch: 4, loss: 0.11285002529621124, acc: 96.875, f1: 94.03985507246378, r: 0.7217186183744906
06/02/2019 10:24:44 step: 2584, epoch: 78, batch: 9, loss: 0.08320453017950058, acc: 96.875, f1: 94.30300807043285, r: 0.7029326966071402
06/02/2019 10:24:46 step: 2589, epoch: 78, batch: 14, loss: 0.16659769415855408, acc: 95.3125, f1: 92.08558671465589, r: 0.7118051070846139
06/02/2019 10:24:48 step: 2594, epoch: 78, batch: 19, loss: 0.08570098131895065, acc: 96.875, f1: 93.88522588522588, r: 0.686496624696311
06/02/2019 10:24:50 step: 2599, epoch: 78, batch: 24, loss: 0.019231244921684265, acc: 100.0, f1: 100.0, r: 0.6733329402202713
06/02/2019 10:24:53 step: 2604, epoch: 78, batch: 29, loss: 0.07834205031394958, acc: 98.4375, f1: 97.22222222222221, r: 0.8091940781661622
06/02/2019 10:24:54 *** evaluating ***
06/02/2019 10:24:55 step: 79, epoch: 78, acc: 50.427350427350426, f1: 29.01124647204859, r: 0.27115050907858707
06/02/2019 10:24:55 *** epoch: 80 ***
06/02/2019 10:24:55 *** training ***
06/02/2019 10:24:57 step: 2612, epoch: 79, batch: 4, loss: 0.02011188119649887, acc: 100.0, f1: 100.0, r: 0.6757755827787075
06/02/2019 10:24:59 step: 2617, epoch: 79, batch: 9, loss: 0.04153309762477875, acc: 100.0, f1: 100.0, r: 0.6588190599565076
06/02/2019 10:25:01 step: 2622, epoch: 79, batch: 14, loss: 0.21163006126880646, acc: 93.75, f1: 85.4471916971917, r: 0.6962811463592807
06/02/2019 10:25:03 step: 2627, epoch: 79, batch: 19, loss: 0.08457157760858536, acc: 98.4375, f1: 93.19727891156462, r: 0.6420424911840195
06/02/2019 10:25:05 step: 2632, epoch: 79, batch: 24, loss: 0.03487448766827583, acc: 100.0, f1: 100.0, r: 0.7748989229507589
06/02/2019 10:25:08 step: 2637, epoch: 79, batch: 29, loss: 0.05607743561267853, acc: 98.4375, f1: 99.15966386554622, r: 0.7753468409175218
06/02/2019 10:25:09 *** evaluating ***
06/02/2019 10:25:10 step: 80, epoch: 79, acc: 45.72649572649573, f1: 26.843685855633836, r: 0.26497163939572344
06/02/2019 10:25:10 *** epoch: 81 ***
06/02/2019 10:25:10 *** training ***
06/02/2019 10:25:12 step: 2645, epoch: 80, batch: 4, loss: 0.1460210084915161, acc: 95.3125, f1: 85.60606060606061, r: 0.6284861900811181
06/02/2019 10:25:14 step: 2650, epoch: 80, batch: 9, loss: 0.1203671470284462, acc: 95.3125, f1: 91.13650075414782, r: 0.7838903384437796
06/02/2019 10:25:17 step: 2655, epoch: 80, batch: 14, loss: 0.15892991423606873, acc: 93.75, f1: 87.41883116883116, r: 0.7554506125183694
06/02/2019 10:25:19 step: 2660, epoch: 80, batch: 19, loss: 0.2975601851940155, acc: 90.625, f1: 85.16217013121039, r: 0.669972525793514
06/02/2019 10:25:22 step: 2665, epoch: 80, batch: 24, loss: 0.11162234842777252, acc: 95.3125, f1: 80.08205634049907, r: 0.6604057348625456
06/02/2019 10:25:24 step: 2670, epoch: 80, batch: 29, loss: 0.07781698554754257, acc: 98.4375, f1: 92.38095238095238, r: 0.6860999098048853
06/02/2019 10:25:25 *** evaluating ***
06/02/2019 10:25:26 step: 81, epoch: 80, acc: 45.72649572649573, f1: 26.049891444216588, r: 0.2456925110455712
06/02/2019 10:25:26 *** epoch: 82 ***
06/02/2019 10:25:26 *** training ***
06/02/2019 10:25:28 step: 2678, epoch: 81, batch: 4, loss: 0.05271614342927933, acc: 98.4375, f1: 97.00680272108845, r: 0.7307970019616188
06/02/2019 10:25:30 step: 2683, epoch: 81, batch: 9, loss: 0.03995276242494583, acc: 100.0, f1: 100.0, r: 0.7533062257292524
06/02/2019 10:25:32 step: 2688, epoch: 81, batch: 14, loss: 0.146528959274292, acc: 95.3125, f1: 90.52816552816553, r: 0.6348195709601288
06/02/2019 10:25:35 step: 2693, epoch: 81, batch: 19, loss: 0.17681041359901428, acc: 93.75, f1: 84.92689029634845, r: 0.7212494976680861
06/02/2019 10:25:37 step: 2698, epoch: 81, batch: 24, loss: 0.08964214473962784, acc: 98.4375, f1: 94.44444444444444, r: 0.7746152211751948
06/02/2019 10:25:39 step: 2703, epoch: 81, batch: 29, loss: 0.13621783256530762, acc: 93.75, f1: 88.33236938500096, r: 0.6574007506278807
06/02/2019 10:25:41 *** evaluating ***
06/02/2019 10:25:41 step: 82, epoch: 81, acc: 50.427350427350426, f1: 29.225167100217597, r: 0.2486546549451124
06/02/2019 10:25:41 *** epoch: 83 ***
06/02/2019 10:25:41 *** training ***
06/02/2019 10:25:44 step: 2711, epoch: 82, batch: 4, loss: 0.028126373887062073, acc: 100.0, f1: 100.0, r: 0.7484825920413747
06/02/2019 10:25:46 step: 2716, epoch: 82, batch: 9, loss: 0.06921323388814926, acc: 100.0, f1: 100.0, r: 0.8102736007702293
06/02/2019 10:25:47 step: 2721, epoch: 82, batch: 14, loss: 0.07863073796033859, acc: 96.875, f1: 98.35847382431233, r: 0.6788203513148197
06/02/2019 10:25:49 step: 2726, epoch: 82, batch: 19, loss: 0.06537468731403351, acc: 98.4375, f1: 99.28070175438597, r: 0.8096513721213902
06/02/2019 10:25:52 step: 2731, epoch: 82, batch: 24, loss: 0.11006259173154831, acc: 95.3125, f1: 80.11363636363636, r: 0.772673934709266
06/02/2019 10:25:54 step: 2736, epoch: 82, batch: 29, loss: 0.041465431451797485, acc: 98.4375, f1: 98.63945578231294, r: 0.6954426507982839
06/02/2019 10:25:55 *** evaluating ***
06/02/2019 10:25:56 step: 83, epoch: 82, acc: 48.717948717948715, f1: 28.040719492925376, r: 0.2667792228425647
06/02/2019 10:25:56 *** epoch: 84 ***
06/02/2019 10:25:56 *** training ***
06/02/2019 10:25:58 step: 2744, epoch: 83, batch: 4, loss: 0.05847999453544617, acc: 96.875, f1: 94.37500000000001, r: 0.7409995698073361
06/02/2019 10:26:01 step: 2749, epoch: 83, batch: 9, loss: 0.15417121350765228, acc: 93.75, f1: 75.55995839681238, r: 0.6405073808748977
06/02/2019 10:26:03 step: 2754, epoch: 83, batch: 14, loss: 0.0775010883808136, acc: 96.875, f1: 93.91243250377512, r: 0.7602841754265272
06/02/2019 10:26:06 step: 2759, epoch: 83, batch: 19, loss: 0.08105416595935822, acc: 100.0, f1: 100.0, r: 0.7750021071104005
06/02/2019 10:26:08 step: 2764, epoch: 83, batch: 24, loss: 0.11891919374465942, acc: 95.3125, f1: 91.28364389233955, r: 0.7094068943432009
06/02/2019 10:26:10 step: 2769, epoch: 83, batch: 29, loss: 0.3884594440460205, acc: 87.5, f1: 72.05357142857143, r: 0.6413354037098995
06/02/2019 10:26:11 *** evaluating ***
06/02/2019 10:26:12 step: 84, epoch: 83, acc: 49.14529914529914, f1: 24.422538374781887, r: 0.216083891777375
06/02/2019 10:26:12 *** epoch: 85 ***
06/02/2019 10:26:12 *** training ***
06/02/2019 10:26:14 step: 2777, epoch: 84, batch: 4, loss: 0.051737263798713684, acc: 100.0, f1: 100.0, r: 0.663697068382392
06/02/2019 10:26:17 step: 2782, epoch: 84, batch: 9, loss: 0.1438644826412201, acc: 95.3125, f1: 90.78480090107998, r: 0.756267997873703
06/02/2019 10:26:19 step: 2787, epoch: 84, batch: 14, loss: 0.10606597363948822, acc: 96.875, f1: 92.20833333333334, r: 0.7667055680095558
06/02/2019 10:26:21 step: 2792, epoch: 84, batch: 19, loss: 0.06737641245126724, acc: 98.4375, f1: 87.09677419354838, r: 0.795156386243542
06/02/2019 10:26:23 step: 2797, epoch: 84, batch: 24, loss: 0.14016737043857574, acc: 95.3125, f1: 93.19727891156462, r: 0.6611898836518437
06/02/2019 10:26:25 step: 2802, epoch: 84, batch: 29, loss: 0.3246136009693146, acc: 92.1875, f1: 88.86033927402562, r: 0.7412812541049574
06/02/2019 10:26:26 *** evaluating ***
06/02/2019 10:26:27 step: 85, epoch: 84, acc: 38.034188034188034, f1: 24.096741215653843, r: 0.2256008524715311
06/02/2019 10:26:27 *** epoch: 86 ***
06/02/2019 10:26:27 *** training ***
06/02/2019 10:26:29 step: 2810, epoch: 85, batch: 4, loss: 0.7001855969429016, acc: 78.125, f1: 68.52809598682241, r: 0.6615367304028332
06/02/2019 10:26:31 step: 2815, epoch: 85, batch: 9, loss: 0.5060362815856934, acc: 81.25, f1: 60.325817240263234, r: 0.5183537640207252
06/02/2019 10:26:34 step: 2820, epoch: 85, batch: 14, loss: 0.3339276909828186, acc: 87.5, f1: 84.3203974046098, r: 0.5469072495515962
06/02/2019 10:26:36 step: 2825, epoch: 85, batch: 19, loss: 0.529374897480011, acc: 81.25, f1: 67.91115458040923, r: 0.7051382456663889
06/02/2019 10:26:38 step: 2830, epoch: 85, batch: 24, loss: 0.2340359091758728, acc: 93.75, f1: 78.48340061977605, r: 0.6446549571715612
06/02/2019 10:26:41 step: 2835, epoch: 85, batch: 29, loss: 0.5161442160606384, acc: 78.125, f1: 63.49252136752137, r: 0.6546685403020832
06/02/2019 10:26:42 *** evaluating ***
06/02/2019 10:26:42 step: 86, epoch: 85, acc: 48.717948717948715, f1: 26.73990054916306, r: 0.25160285261625837
06/02/2019 10:26:42 *** epoch: 87 ***
06/02/2019 10:26:42 *** training ***
06/02/2019 10:26:45 step: 2843, epoch: 86, batch: 4, loss: 0.41985785961151123, acc: 84.375, f1: 74.68719670077462, r: 0.6073225481540754
06/02/2019 10:26:47 step: 2848, epoch: 86, batch: 9, loss: 0.5460253953933716, acc: 82.8125, f1: 66.72740524781341, r: 0.5644835181461905
06/02/2019 10:26:49 step: 2853, epoch: 86, batch: 14, loss: 0.38368725776672363, acc: 85.9375, f1: 87.10884353741496, r: 0.5922773574499881
06/02/2019 10:26:51 step: 2858, epoch: 86, batch: 19, loss: 0.3622564375400543, acc: 85.9375, f1: 85.55288461538461, r: 0.7397468472909675
06/02/2019 10:26:54 step: 2863, epoch: 86, batch: 24, loss: 0.43673011660575867, acc: 85.9375, f1: 83.98031135531136, r: 0.6565557939459785
06/02/2019 10:26:55 step: 2868, epoch: 86, batch: 29, loss: 0.2738751769065857, acc: 87.5, f1: 89.09031734992496, r: 0.6984548167987137
06/02/2019 10:26:57 *** evaluating ***
06/02/2019 10:26:58 step: 87, epoch: 86, acc: 47.863247863247864, f1: 31.86325731924343, r: 0.31407688449118554
06/02/2019 10:26:58 *** epoch: 88 ***
06/02/2019 10:26:58 *** training ***
06/02/2019 10:27:00 step: 2876, epoch: 87, batch: 4, loss: 0.24760107696056366, acc: 90.625, f1: 78.40380405035577, r: 0.6451527065317023
06/02/2019 10:27:02 step: 2881, epoch: 87, batch: 9, loss: 0.19326210021972656, acc: 95.3125, f1: 83.21765953344902, r: 0.6288398222840811
06/02/2019 10:27:04 step: 2886, epoch: 87, batch: 14, loss: 0.26187413930892944, acc: 95.3125, f1: 81.91187888198759, r: 0.6588750260547946
06/02/2019 10:27:07 step: 2891, epoch: 87, batch: 19, loss: 0.2742776870727539, acc: 92.1875, f1: 76.6738851644512, r: 0.6000264433624031
06/02/2019 10:27:09 step: 2896, epoch: 87, batch: 24, loss: 0.40146803855895996, acc: 90.625, f1: 68.10431010010842, r: 0.6999879865281738
06/02/2019 10:27:11 step: 2901, epoch: 87, batch: 29, loss: 0.33749258518218994, acc: 89.0625, f1: 87.99963924963924, r: 0.7812923126479691
06/02/2019 10:27:12 *** evaluating ***
06/02/2019 10:27:13 step: 88, epoch: 87, acc: 49.14529914529914, f1: 29.44753985270485, r: 0.2692789860479799
06/02/2019 10:27:13 *** epoch: 89 ***
06/02/2019 10:27:13 *** training ***
06/02/2019 10:27:15 step: 2909, epoch: 88, batch: 4, loss: 0.15064825117588043, acc: 95.3125, f1: 91.53235240456335, r: 0.6013766766004626
06/02/2019 10:27:18 step: 2914, epoch: 88, batch: 9, loss: 0.19554677605628967, acc: 93.75, f1: 93.0531975099038, r: 0.7188481593723856
06/02/2019 10:27:20 step: 2919, epoch: 88, batch: 14, loss: 0.17046892642974854, acc: 95.3125, f1: 94.95726495726495, r: 0.7246928010736544
06/02/2019 10:27:22 step: 2924, epoch: 88, batch: 19, loss: 0.2651427984237671, acc: 90.625, f1: 76.20386904761904, r: 0.6584718892458423
06/02/2019 10:27:25 step: 2929, epoch: 88, batch: 24, loss: 0.23643314838409424, acc: 92.1875, f1: 86.72302753935408, r: 0.6891405947717916
06/02/2019 10:27:27 step: 2934, epoch: 88, batch: 29, loss: 0.5744050145149231, acc: 84.375, f1: 79.06746031746033, r: 0.650599237143183
06/02/2019 10:27:28 *** evaluating ***
06/02/2019 10:27:29 step: 89, epoch: 88, acc: 44.871794871794876, f1: 24.193040970049616, r: 0.2809321711502939
06/02/2019 10:27:29 *** epoch: 90 ***
06/02/2019 10:27:29 *** training ***
06/02/2019 10:27:31 step: 2942, epoch: 89, batch: 4, loss: 0.4940747618675232, acc: 85.9375, f1: 73.70941558441558, r: 0.6117194256040839
06/02/2019 10:27:33 step: 2947, epoch: 89, batch: 9, loss: 0.5907580852508545, acc: 81.25, f1: 69.10535117056857, r: 0.6163536813122154
06/02/2019 10:27:35 step: 2952, epoch: 89, batch: 14, loss: 0.5753737092018127, acc: 79.6875, f1: 72.576726342711, r: 0.6488876765078158
06/02/2019 10:27:37 step: 2957, epoch: 89, batch: 19, loss: 0.3460676074028015, acc: 87.5, f1: 80.30240291109855, r: 0.5734099314650796
06/02/2019 10:27:39 step: 2962, epoch: 89, batch: 24, loss: 0.2530786991119385, acc: 93.75, f1: 96.40194321045385, r: 0.7090626601321701
06/02/2019 10:27:42 step: 2967, epoch: 89, batch: 29, loss: 0.15454909205436707, acc: 96.875, f1: 96.55045351473923, r: 0.7091920465446229
06/02/2019 10:27:43 *** evaluating ***
06/02/2019 10:27:44 step: 90, epoch: 89, acc: 45.2991452991453, f1: 24.539521623796944, r: 0.2752618108446954
06/02/2019 10:27:44 *** epoch: 91 ***
06/02/2019 10:27:44 *** training ***
06/02/2019 10:27:47 step: 2975, epoch: 90, batch: 4, loss: 0.238795667886734, acc: 92.1875, f1: 89.46550048590865, r: 0.6183721445961035
06/02/2019 10:27:49 step: 2980, epoch: 90, batch: 9, loss: 0.0765734389424324, acc: 98.4375, f1: 93.19727891156464, r: 0.6746712045615898
06/02/2019 10:27:51 step: 2985, epoch: 90, batch: 14, loss: 0.1990538239479065, acc: 90.625, f1: 91.9223244853497, r: 0.68284049318682
06/02/2019 10:27:54 step: 2990, epoch: 90, batch: 19, loss: 0.17220813035964966, acc: 93.75, f1: 91.75740113905007, r: 0.7931893428623069
06/02/2019 10:27:56 step: 2995, epoch: 90, batch: 24, loss: 0.19904233515262604, acc: 93.75, f1: 88.32670233367097, r: 0.6664103011596813
06/02/2019 10:27:58 step: 3000, epoch: 90, batch: 29, loss: 0.1726413071155548, acc: 93.75, f1: 96.87326359115075, r: 0.6793985926287586
06/02/2019 10:27:59 *** evaluating ***
06/02/2019 10:28:00 step: 91, epoch: 90, acc: 48.717948717948715, f1: 28.76267214187006, r: 0.2854275803579945
06/02/2019 10:28:00 *** epoch: 92 ***
06/02/2019 10:28:00 *** training ***
06/02/2019 10:28:02 step: 3008, epoch: 91, batch: 4, loss: 0.11606556922197342, acc: 96.875, f1: 96.32090132090131, r: 0.8113194333245253
06/02/2019 10:28:04 step: 3013, epoch: 91, batch: 9, loss: 0.07258094102144241, acc: 100.0, f1: 100.0, r: 0.8081725654002475
06/02/2019 10:28:06 step: 3018, epoch: 91, batch: 14, loss: 0.08751580864191055, acc: 96.875, f1: 83.27067669172934, r: 0.7824912852170218
06/02/2019 10:28:08 step: 3023, epoch: 91, batch: 19, loss: 0.09191375225782394, acc: 96.875, f1: 98.05360859776559, r: 0.7018247331765493
06/02/2019 10:28:10 step: 3028, epoch: 91, batch: 24, loss: 0.06968440860509872, acc: 98.4375, f1: 98.65047233468286, r: 0.6742897830421569
06/02/2019 10:28:13 step: 3033, epoch: 91, batch: 29, loss: 0.14239954948425293, acc: 98.4375, f1: 87.1951219512195, r: 0.6405020590910624
06/02/2019 10:28:14 *** evaluating ***
06/02/2019 10:28:15 step: 92, epoch: 91, acc: 45.72649572649573, f1: 25.981417733390476, r: 0.2874598130425382
06/02/2019 10:28:15 *** epoch: 93 ***
06/02/2019 10:28:15 *** training ***
06/02/2019 10:28:18 step: 3041, epoch: 92, batch: 4, loss: 0.10266581922769547, acc: 98.4375, f1: 95.55555555555556, r: 0.559268923153328
06/02/2019 10:28:20 step: 3046, epoch: 92, batch: 9, loss: 0.09140372276306152, acc: 98.4375, f1: 99.07773386034255, r: 0.7822020901240805
06/02/2019 10:28:22 step: 3051, epoch: 92, batch: 14, loss: 0.14969336986541748, acc: 96.875, f1: 97.72325951657258, r: 0.6525273805506833
06/02/2019 10:28:25 step: 3056, epoch: 92, batch: 19, loss: 0.09557247161865234, acc: 98.4375, f1: 94.04761904761905, r: 0.7893755508570347
06/02/2019 10:28:26 step: 3061, epoch: 92, batch: 24, loss: 0.14339786767959595, acc: 95.3125, f1: 94.73497732426304, r: 0.6533425323993458
06/02/2019 10:28:29 step: 3066, epoch: 92, batch: 29, loss: 0.04360993206501007, acc: 100.0, f1: 100.0, r: 0.7558888810028894
06/02/2019 10:28:30 *** evaluating ***
06/02/2019 10:28:31 step: 93, epoch: 92, acc: 48.29059829059829, f1: 27.087292521510737, r: 0.2895293748754584
06/02/2019 10:28:31 *** epoch: 94 ***
06/02/2019 10:28:31 *** training ***
06/02/2019 10:28:33 step: 3074, epoch: 93, batch: 4, loss: 0.052996955811977386, acc: 100.0, f1: 100.0, r: 0.7507462056751963
06/02/2019 10:28:35 step: 3079, epoch: 93, batch: 9, loss: 0.03818177431821823, acc: 100.0, f1: 100.0, r: 0.668623067271119
06/02/2019 10:28:37 step: 3084, epoch: 93, batch: 14, loss: 0.10620717704296112, acc: 96.875, f1: 97.38760150524857, r: 0.7933878673319771
06/02/2019 10:28:39 step: 3089, epoch: 93, batch: 19, loss: 0.044304728507995605, acc: 100.0, f1: 100.0, r: 0.6058312020015516
06/02/2019 10:28:43 step: 3094, epoch: 93, batch: 24, loss: 0.1842823326587677, acc: 95.3125, f1: 92.42424242424244, r: 0.7157885145826839
06/02/2019 10:28:45 step: 3099, epoch: 93, batch: 29, loss: 0.17760741710662842, acc: 95.3125, f1: 92.72953804868699, r: 0.7010721209160146
06/02/2019 10:28:46 *** evaluating ***
06/02/2019 10:28:46 step: 94, epoch: 93, acc: 51.70940170940172, f1: 28.306734353977724, r: 0.2904038453103532
06/02/2019 10:28:46 *** epoch: 95 ***
06/02/2019 10:28:46 *** training ***
06/02/2019 10:28:48 step: 3107, epoch: 94, batch: 4, loss: 0.051176924258470535, acc: 100.0, f1: 100.0, r: 0.5992686876878098
06/02/2019 10:28:51 step: 3112, epoch: 94, batch: 9, loss: 0.05580482631921768, acc: 100.0, f1: 100.0, r: 0.6699441336370091
06/02/2019 10:28:53 step: 3117, epoch: 94, batch: 14, loss: 0.09880062192678452, acc: 98.4375, f1: 98.78335949764521, r: 0.8174407282406939
06/02/2019 10:28:56 step: 3122, epoch: 94, batch: 19, loss: 0.33074861764907837, acc: 90.625, f1: 85.9159416971917, r: 0.702347763733794
06/02/2019 10:28:58 step: 3127, epoch: 94, batch: 24, loss: 0.090995654463768, acc: 98.4375, f1: 98.99874843554443, r: 0.7767122479201595
06/02/2019 10:29:00 step: 3132, epoch: 94, batch: 29, loss: 0.250851035118103, acc: 93.75, f1: 93.46692053478904, r: 0.5828152932798172
06/02/2019 10:29:01 *** evaluating ***
06/02/2019 10:29:02 step: 95, epoch: 94, acc: 44.01709401709402, f1: 25.91147828129628, r: 0.2581338661440629
06/02/2019 10:29:02 *** epoch: 96 ***
06/02/2019 10:29:02 *** training ***
06/02/2019 10:29:04 step: 3140, epoch: 95, batch: 4, loss: 0.07855616509914398, acc: 96.875, f1: 95.93088071348942, r: 0.7550308256349256
06/02/2019 10:29:07 step: 3145, epoch: 95, batch: 9, loss: 0.08504961431026459, acc: 96.875, f1: 95.9848484848485, r: 0.7767318110821047
06/02/2019 10:29:09 step: 3150, epoch: 95, batch: 14, loss: 0.04883284494280815, acc: 98.4375, f1: 98.939883645766, r: 0.7389470788035097
06/02/2019 10:29:11 step: 3155, epoch: 95, batch: 19, loss: 0.09110455960035324, acc: 96.875, f1: 94.94505494505495, r: 0.6959847650115487
06/02/2019 10:29:14 step: 3160, epoch: 95, batch: 24, loss: 0.06733091920614243, acc: 100.0, f1: 100.0, r: 0.7732615418294178
06/02/2019 10:29:16 step: 3165, epoch: 95, batch: 29, loss: 0.09104959666728973, acc: 96.875, f1: 97.1108052214043, r: 0.6410950237577367
06/02/2019 10:29:17 *** evaluating ***
06/02/2019 10:29:18 step: 96, epoch: 95, acc: 47.863247863247864, f1: 29.65347278367797, r: 0.27563649296027104
06/02/2019 10:29:18 *** epoch: 97 ***
06/02/2019 10:29:18 *** training ***
06/02/2019 10:29:20 step: 3173, epoch: 96, batch: 4, loss: 0.08640619367361069, acc: 98.4375, f1: 94.04761904761905, r: 0.7575307640932452
06/02/2019 10:29:22 step: 3178, epoch: 96, batch: 9, loss: 0.07758883386850357, acc: 98.4375, f1: 99.20694459329118, r: 0.7094077594217887
06/02/2019 10:29:24 step: 3183, epoch: 96, batch: 14, loss: 0.10878341645002365, acc: 96.875, f1: 96.99142367066895, r: 0.7669616471470303
06/02/2019 10:29:26 step: 3188, epoch: 96, batch: 19, loss: 0.05509857088327408, acc: 100.0, f1: 100.0, r: 0.6208495879659566
06/02/2019 10:29:29 step: 3193, epoch: 96, batch: 24, loss: 0.1295095980167389, acc: 98.4375, f1: 95.23809523809523, r: 0.7121234565606915
06/02/2019 10:29:31 step: 3198, epoch: 96, batch: 29, loss: 0.07726694643497467, acc: 96.875, f1: 96.49900539926115, r: 0.795601814445853
06/02/2019 10:29:32 *** evaluating ***
06/02/2019 10:29:33 step: 97, epoch: 96, acc: 51.28205128205128, f1: 26.380622772812924, r: 0.24570681277391176
06/02/2019 10:29:33 *** epoch: 98 ***
06/02/2019 10:29:33 *** training ***
06/02/2019 10:29:35 step: 3206, epoch: 97, batch: 4, loss: 0.12631946802139282, acc: 93.75, f1: 80.05639097744361, r: 0.6924067459469487
06/02/2019 10:29:37 step: 3211, epoch: 97, batch: 9, loss: 0.0911111831665039, acc: 96.875, f1: 83.06763285024155, r: 0.6843526207289278
06/02/2019 10:29:40 step: 3216, epoch: 97, batch: 14, loss: 0.0562032088637352, acc: 98.4375, f1: 97.07792207792207, r: 0.7976516813945053
06/02/2019 10:29:42 step: 3221, epoch: 97, batch: 19, loss: 0.03576025366783142, acc: 100.0, f1: 100.0, r: 0.7421279123171769
06/02/2019 10:29:45 step: 3226, epoch: 97, batch: 24, loss: 0.032439179718494415, acc: 100.0, f1: 100.0, r: 0.7332978597902686
06/02/2019 10:29:47 step: 3231, epoch: 97, batch: 29, loss: 0.03713466227054596, acc: 100.0, f1: 100.0, r: 0.7703687253892075
06/02/2019 10:29:48 *** evaluating ***
06/02/2019 10:29:49 step: 98, epoch: 97, acc: 49.14529914529914, f1: 28.28111204157045, r: 0.27522705484957943
06/02/2019 10:29:49 *** epoch: 99 ***
06/02/2019 10:29:49 *** training ***
06/02/2019 10:29:51 step: 3239, epoch: 98, batch: 4, loss: 0.04008191451430321, acc: 98.4375, f1: 99.20634920634922, r: 0.7547048679888456
06/02/2019 10:29:53 step: 3244, epoch: 98, batch: 9, loss: 0.06626949459314346, acc: 98.4375, f1: 97.81105990783409, r: 0.7872153593534964
06/02/2019 10:29:55 step: 3249, epoch: 98, batch: 14, loss: 0.03728397935628891, acc: 100.0, f1: 100.0, r: 0.6510304928369843
06/02/2019 10:29:58 step: 3254, epoch: 98, batch: 19, loss: 0.05646434426307678, acc: 100.0, f1: 100.0, r: 0.7190111061537334
06/02/2019 10:30:01 step: 3259, epoch: 98, batch: 24, loss: 0.04420275241136551, acc: 98.4375, f1: 96.36363636363636, r: 0.7993583292319569
06/02/2019 10:30:02 step: 3264, epoch: 98, batch: 29, loss: 0.046605706214904785, acc: 98.4375, f1: 97.6023976023976, r: 0.6745007337182163
06/02/2019 10:30:03 *** evaluating ***
06/02/2019 10:30:04 step: 99, epoch: 98, acc: 49.572649572649574, f1: 29.022462212117382, r: 0.27654515739410973
06/02/2019 10:30:04 *** epoch: 100 ***
06/02/2019 10:30:04 *** training ***
06/02/2019 10:30:07 step: 3272, epoch: 99, batch: 4, loss: 0.05836345627903938, acc: 96.875, f1: 94.57294028722599, r: 0.5955071883013445
06/02/2019 10:30:09 step: 3277, epoch: 99, batch: 9, loss: 0.022215217351913452, acc: 100.0, f1: 100.0, r: 0.7756851372280962
06/02/2019 10:30:11 step: 3282, epoch: 99, batch: 14, loss: 0.013345621526241302, acc: 100.0, f1: 100.0, r: 0.7072169072777896
06/02/2019 10:30:13 step: 3287, epoch: 99, batch: 19, loss: 0.04694432392716408, acc: 100.0, f1: 100.0, r: 0.7240630434004897
06/02/2019 10:30:15 step: 3292, epoch: 99, batch: 24, loss: 0.03000452369451523, acc: 100.0, f1: 100.0, r: 0.6776419220104412
06/02/2019 10:30:17 step: 3297, epoch: 99, batch: 29, loss: 0.029351133853197098, acc: 100.0, f1: 100.0, r: 0.7747649169127269
06/02/2019 10:30:19 *** evaluating ***
06/02/2019 10:30:19 step: 100, epoch: 99, acc: 49.14529914529914, f1: 29.975482684316525, r: 0.26313406889180235
06/02/2019 10:30:19 *** epoch: 101 ***
06/02/2019 10:30:19 *** training ***
06/02/2019 10:30:21 step: 3305, epoch: 100, batch: 4, loss: 0.01257498562335968, acc: 100.0, f1: 100.0, r: 0.7537885538793008
06/02/2019 10:30:24 step: 3310, epoch: 100, batch: 9, loss: 0.06548034399747849, acc: 96.875, f1: 89.41798941798942, r: 0.7667408114497538
06/02/2019 10:30:26 step: 3315, epoch: 100, batch: 14, loss: 0.029853373765945435, acc: 100.0, f1: 100.0, r: 0.7357204673341967
06/02/2019 10:30:28 step: 3320, epoch: 100, batch: 19, loss: 0.050209853798151016, acc: 96.875, f1: 95.11111111111113, r: 0.7944200026703545
06/02/2019 10:30:31 step: 3325, epoch: 100, batch: 24, loss: 0.03239027410745621, acc: 100.0, f1: 100.0, r: 0.8174758908518358
06/02/2019 10:30:33 step: 3330, epoch: 100, batch: 29, loss: 0.023808859288692474, acc: 100.0, f1: 100.0, r: 0.7083114370854073
06/02/2019 10:30:34 *** evaluating ***
06/02/2019 10:30:34 step: 101, epoch: 100, acc: 50.0, f1: 29.326572227629654, r: 0.2820007631222369
06/02/2019 10:30:34 *** epoch: 102 ***
06/02/2019 10:30:34 *** training ***
06/02/2019 10:30:37 step: 3338, epoch: 101, batch: 4, loss: 0.042530566453933716, acc: 98.4375, f1: 97.47474747474747, r: 0.7717963130258148
06/02/2019 10:30:39 step: 3343, epoch: 101, batch: 9, loss: 0.01239447295665741, acc: 100.0, f1: 100.0, r: 0.6991552212694351
06/02/2019 10:30:41 step: 3348, epoch: 101, batch: 14, loss: 0.030195608735084534, acc: 100.0, f1: 100.0, r: 0.7045569744943432
06/02/2019 10:30:44 step: 3353, epoch: 101, batch: 19, loss: 0.029380425810813904, acc: 100.0, f1: 100.0, r: 0.6853475611119584
06/02/2019 10:30:46 step: 3358, epoch: 101, batch: 24, loss: 0.015655145049095154, acc: 100.0, f1: 100.0, r: 0.7956434265822567
06/02/2019 10:30:48 step: 3363, epoch: 101, batch: 29, loss: 0.031671084463596344, acc: 98.4375, f1: 96.8602825745683, r: 0.7584351534803678
06/02/2019 10:30:49 *** evaluating ***
06/02/2019 10:30:50 step: 102, epoch: 101, acc: 49.572649572649574, f1: 29.09581192302364, r: 0.27176233324573956
06/02/2019 10:30:50 *** epoch: 103 ***
06/02/2019 10:30:50 *** training ***
06/02/2019 10:30:53 step: 3371, epoch: 102, batch: 4, loss: 0.015097379684448242, acc: 100.0, f1: 100.0, r: 0.7366038629410402
06/02/2019 10:30:55 step: 3376, epoch: 102, batch: 9, loss: 0.0242537260055542, acc: 100.0, f1: 100.0, r: 0.7708588622790523
06/02/2019 10:30:57 step: 3381, epoch: 102, batch: 14, loss: 0.0504121407866478, acc: 98.4375, f1: 96.11111111111111, r: 0.7903007115086808
06/02/2019 10:30:59 step: 3386, epoch: 102, batch: 19, loss: 0.03346308320760727, acc: 98.4375, f1: 98.65047233468286, r: 0.701967379887682
06/02/2019 10:31:01 step: 3391, epoch: 102, batch: 24, loss: 0.05518334358930588, acc: 98.4375, f1: 97.47899159663865, r: 0.7354302823229262
06/02/2019 10:31:04 step: 3396, epoch: 102, batch: 29, loss: 0.024220749735832214, acc: 100.0, f1: 100.0, r: 0.7447975171637919
06/02/2019 10:31:05 *** evaluating ***
06/02/2019 10:31:06 step: 103, epoch: 102, acc: 50.85470085470085, f1: 28.89911968620097, r: 0.27007456000930213
06/02/2019 10:31:06 *** epoch: 104 ***
06/02/2019 10:31:06 *** training ***
06/02/2019 10:31:08 step: 3404, epoch: 103, batch: 4, loss: 0.008625641465187073, acc: 100.0, f1: 100.0, r: 0.7620677689531407
06/02/2019 10:31:10 step: 3409, epoch: 103, batch: 9, loss: 0.00984683632850647, acc: 100.0, f1: 100.0, r: 0.7056104445938605
06/02/2019 10:31:12 step: 3414, epoch: 103, batch: 14, loss: 0.025856882333755493, acc: 100.0, f1: 100.0, r: 0.79969328358332
06/02/2019 10:31:15 step: 3419, epoch: 103, batch: 19, loss: 0.02140531688928604, acc: 100.0, f1: 100.0, r: 0.7413688644215578
06/02/2019 10:31:17 step: 3424, epoch: 103, batch: 24, loss: 0.013459503650665283, acc: 100.0, f1: 100.0, r: 0.7156480051466401
06/02/2019 10:31:20 step: 3429, epoch: 103, batch: 29, loss: 0.013266533613204956, acc: 100.0, f1: 100.0, r: 0.6789526898443884
06/02/2019 10:31:21 *** evaluating ***
06/02/2019 10:31:21 step: 104, epoch: 103, acc: 52.56410256410257, f1: 30.479647394347943, r: 0.2834413039131239
06/02/2019 10:31:21 *** epoch: 105 ***
06/02/2019 10:31:21 *** training ***
06/02/2019 10:31:24 step: 3437, epoch: 104, batch: 4, loss: 0.010617516934871674, acc: 100.0, f1: 100.0, r: 0.6125309418299533
06/02/2019 10:31:26 step: 3442, epoch: 104, batch: 9, loss: 0.021243035793304443, acc: 100.0, f1: 100.0, r: 0.770826678107532
06/02/2019 10:31:28 step: 3447, epoch: 104, batch: 14, loss: 0.013518884778022766, acc: 100.0, f1: 100.0, r: 0.6974623694073938
06/02/2019 10:31:30 step: 3452, epoch: 104, batch: 19, loss: 0.022183053195476532, acc: 100.0, f1: 100.0, r: 0.8379490698648804
06/02/2019 10:31:33 step: 3457, epoch: 104, batch: 24, loss: 0.0076183900237083435, acc: 100.0, f1: 100.0, r: 0.7113087292487713
06/02/2019 10:31:35 step: 3462, epoch: 104, batch: 29, loss: 0.011074088513851166, acc: 100.0, f1: 100.0, r: 0.7119294995981873
06/02/2019 10:31:36 *** evaluating ***
06/02/2019 10:31:37 step: 105, epoch: 104, acc: 50.85470085470085, f1: 29.078626444159177, r: 0.2649372488483341
06/02/2019 10:31:37 *** epoch: 106 ***
06/02/2019 10:31:37 *** training ***
06/02/2019 10:31:39 step: 3470, epoch: 105, batch: 4, loss: 0.00912618637084961, acc: 100.0, f1: 100.0, r: 0.60547076812077
06/02/2019 10:31:41 step: 3475, epoch: 105, batch: 9, loss: 0.021991446614265442, acc: 98.4375, f1: 99.24764890282131, r: 0.7204501497060624
06/02/2019 10:31:44 step: 3480, epoch: 105, batch: 14, loss: 0.016802392899990082, acc: 100.0, f1: 100.0, r: 0.7118481631200574
06/02/2019 10:31:46 step: 3485, epoch: 105, batch: 19, loss: 0.026036761701107025, acc: 100.0, f1: 100.0, r: 0.6776681282885273
06/02/2019 10:31:49 step: 3490, epoch: 105, batch: 24, loss: 0.03265884518623352, acc: 98.4375, f1: 96.4625850340136, r: 0.6926989412693075
06/02/2019 10:31:50 step: 3495, epoch: 105, batch: 29, loss: 0.009360365569591522, acc: 100.0, f1: 100.0, r: 0.6757568246591728
06/02/2019 10:31:52 *** evaluating ***
06/02/2019 10:31:52 step: 106, epoch: 105, acc: 50.427350427350426, f1: 28.978195601165414, r: 0.26307262374183227
06/02/2019 10:31:52 *** epoch: 107 ***
06/02/2019 10:31:52 *** training ***
06/02/2019 10:31:55 step: 3503, epoch: 106, batch: 4, loss: 0.017906829714775085, acc: 100.0, f1: 100.0, r: 0.7988178494024994
06/02/2019 10:31:57 step: 3508, epoch: 106, batch: 9, loss: 0.024435050785541534, acc: 100.0, f1: 100.0, r: 0.7490759593607224
06/02/2019 10:32:00 step: 3513, epoch: 106, batch: 14, loss: 0.011905081570148468, acc: 100.0, f1: 100.0, r: 0.8003185459257134
06/02/2019 10:32:01 step: 3518, epoch: 106, batch: 19, loss: 0.018797345459461212, acc: 100.0, f1: 100.0, r: 0.6835364350448278
06/02/2019 10:32:03 step: 3523, epoch: 106, batch: 24, loss: 0.021418213844299316, acc: 98.4375, f1: 94.13919413919413, r: 0.7413621532652148
06/02/2019 10:32:05 step: 3528, epoch: 106, batch: 29, loss: 0.016259461641311646, acc: 100.0, f1: 100.0, r: 0.774302918979137
06/02/2019 10:32:07 *** evaluating ***
06/02/2019 10:32:07 step: 107, epoch: 106, acc: 49.14529914529914, f1: 27.531518249295793, r: 0.2458707938214683
06/02/2019 10:32:07 *** epoch: 108 ***
06/02/2019 10:32:07 *** training ***
06/02/2019 10:32:10 step: 3536, epoch: 107, batch: 4, loss: 0.008476175367832184, acc: 100.0, f1: 100.0, r: 0.7007530749165815
06/02/2019 10:32:12 step: 3541, epoch: 107, batch: 9, loss: 0.027934066951274872, acc: 98.4375, f1: 97.78325123152709, r: 0.7763656294139121
06/02/2019 10:32:15 step: 3546, epoch: 107, batch: 14, loss: 0.008938141167163849, acc: 100.0, f1: 100.0, r: 0.8141461689130205
06/02/2019 10:32:17 step: 3551, epoch: 107, batch: 19, loss: 0.015206143260002136, acc: 100.0, f1: 100.0, r: 0.7237282176936491
06/02/2019 10:32:19 step: 3556, epoch: 107, batch: 24, loss: 0.007998757064342499, acc: 100.0, f1: 100.0, r: 0.7583189001663033
06/02/2019 10:32:21 step: 3561, epoch: 107, batch: 29, loss: 0.006894931197166443, acc: 100.0, f1: 100.0, r: 0.8091000636947656
06/02/2019 10:32:22 *** evaluating ***
06/02/2019 10:32:23 step: 108, epoch: 107, acc: 47.863247863247864, f1: 25.366426157420012, r: 0.22732399574366616
06/02/2019 10:32:23 *** epoch: 109 ***
06/02/2019 10:32:23 *** training ***
06/02/2019 10:32:25 step: 3569, epoch: 108, batch: 4, loss: 0.008395366370677948, acc: 100.0, f1: 100.0, r: 0.7563921859954894
06/02/2019 10:32:27 step: 3574, epoch: 108, batch: 9, loss: 0.015873156487941742, acc: 100.0, f1: 100.0, r: 0.7893684040758903
06/02/2019 10:32:29 step: 3579, epoch: 108, batch: 14, loss: 0.01564713567495346, acc: 100.0, f1: 100.0, r: 0.7431712874199012
06/02/2019 10:32:32 step: 3584, epoch: 108, batch: 19, loss: 0.023398809134960175, acc: 100.0, f1: 100.0, r: 0.8584579407397228
06/02/2019 10:32:34 step: 3589, epoch: 108, batch: 24, loss: 0.009820207953453064, acc: 100.0, f1: 100.0, r: 0.6547361697367656
06/02/2019 10:32:36 step: 3594, epoch: 108, batch: 29, loss: 0.008030273020267487, acc: 100.0, f1: 100.0, r: 0.6943205391842313
06/02/2019 10:32:38 *** evaluating ***
06/02/2019 10:32:38 step: 109, epoch: 108, acc: 47.43589743589743, f1: 26.283683063868825, r: 0.2341555062300897
06/02/2019 10:32:38 *** epoch: 110 ***
06/02/2019 10:32:38 *** training ***
06/02/2019 10:32:41 step: 3602, epoch: 109, batch: 4, loss: 0.013673581182956696, acc: 100.0, f1: 100.0, r: 0.714087689989764
06/02/2019 10:32:43 step: 3607, epoch: 109, batch: 9, loss: 0.0051959604024887085, acc: 100.0, f1: 100.0, r: 0.7864838691642717
06/02/2019 10:32:45 step: 3612, epoch: 109, batch: 14, loss: 0.004361428320407867, acc: 100.0, f1: 100.0, r: 0.702100880012315
06/02/2019 10:32:48 step: 3617, epoch: 109, batch: 19, loss: 0.009357355535030365, acc: 100.0, f1: 100.0, r: 0.8122956459396364
06/02/2019 10:32:50 step: 3622, epoch: 109, batch: 24, loss: 0.0098872110247612, acc: 100.0, f1: 100.0, r: 0.6498628541312176
06/02/2019 10:32:52 step: 3627, epoch: 109, batch: 29, loss: 0.02602342516183853, acc: 100.0, f1: 100.0, r: 0.6660318031629519
06/02/2019 10:32:53 *** evaluating ***
06/02/2019 10:32:54 step: 110, epoch: 109, acc: 49.14529914529914, f1: 27.64476118541652, r: 0.25285804161632774
06/02/2019 10:32:54 *** epoch: 111 ***
06/02/2019 10:32:54 *** training ***
06/02/2019 10:32:56 step: 3635, epoch: 110, batch: 4, loss: 0.02605048567056656, acc: 100.0, f1: 100.0, r: 0.805627803975811
06/02/2019 10:32:58 step: 3640, epoch: 110, batch: 9, loss: 0.028310872614383698, acc: 100.0, f1: 100.0, r: 0.6922235960619125
06/02/2019 10:33:00 step: 3645, epoch: 110, batch: 14, loss: 0.011147722601890564, acc: 100.0, f1: 100.0, r: 0.6036669514807997
06/02/2019 10:33:03 step: 3650, epoch: 110, batch: 19, loss: 0.011014662683010101, acc: 100.0, f1: 100.0, r: 0.7952202014339127
06/02/2019 10:33:05 step: 3655, epoch: 110, batch: 24, loss: 0.00915893167257309, acc: 100.0, f1: 100.0, r: 0.7261950894222308
06/02/2019 10:33:07 step: 3660, epoch: 110, batch: 29, loss: 0.005696684122085571, acc: 100.0, f1: 100.0, r: 0.6443605413763299
06/02/2019 10:33:08 *** evaluating ***
06/02/2019 10:33:09 step: 111, epoch: 110, acc: 50.427350427350426, f1: 29.526797140609617, r: 0.24946795133128588
06/02/2019 10:33:09 *** epoch: 112 ***
06/02/2019 10:33:09 *** training ***
06/02/2019 10:33:11 step: 3668, epoch: 111, batch: 4, loss: 0.03888658806681633, acc: 98.4375, f1: 96.76470588235294, r: 0.8343570296219627
06/02/2019 10:33:13 step: 3673, epoch: 111, batch: 9, loss: 0.016194559633731842, acc: 100.0, f1: 100.0, r: 0.6111362157235275
06/02/2019 10:33:15 step: 3678, epoch: 111, batch: 14, loss: 0.024964340031147003, acc: 98.4375, f1: 95.84415584415584, r: 0.6569962483654417
06/02/2019 10:33:18 step: 3683, epoch: 111, batch: 19, loss: 0.014635451138019562, acc: 100.0, f1: 100.0, r: 0.7486516461221444
06/02/2019 10:33:21 step: 3688, epoch: 111, batch: 24, loss: 0.0155477374792099, acc: 100.0, f1: 100.0, r: 0.7669781233997957
06/02/2019 10:33:23 step: 3693, epoch: 111, batch: 29, loss: 0.03561811521649361, acc: 98.4375, f1: 97.92008757526, r: 0.7109913443250455
06/02/2019 10:33:24 *** evaluating ***
06/02/2019 10:33:25 step: 112, epoch: 111, acc: 50.0, f1: 29.036600598217237, r: 0.25142351677837566
06/02/2019 10:33:25 *** epoch: 113 ***
06/02/2019 10:33:25 *** training ***
06/02/2019 10:33:27 step: 3701, epoch: 112, batch: 4, loss: 0.013801421970129013, acc: 100.0, f1: 100.0, r: 0.6741471202999094
06/02/2019 10:33:29 step: 3706, epoch: 112, batch: 9, loss: 0.007213406264781952, acc: 100.0, f1: 100.0, r: 0.7638670745836197
06/02/2019 10:33:31 step: 3711, epoch: 112, batch: 14, loss: 0.010310731828212738, acc: 100.0, f1: 100.0, r: 0.805985983431778
06/02/2019 10:33:34 step: 3716, epoch: 112, batch: 19, loss: 0.01885240525007248, acc: 100.0, f1: 100.0, r: 0.8102219948292042
06/02/2019 10:33:36 step: 3721, epoch: 112, batch: 24, loss: 0.03277144581079483, acc: 100.0, f1: 100.0, r: 0.7819894494612608
06/02/2019 10:33:38 step: 3726, epoch: 112, batch: 29, loss: 0.008085541427135468, acc: 100.0, f1: 100.0, r: 0.7208769046004875
06/02/2019 10:33:39 *** evaluating ***
06/02/2019 10:33:40 step: 113, epoch: 112, acc: 50.427350427350426, f1: 28.63248027526976, r: 0.24739064626618032
06/02/2019 10:33:40 *** epoch: 114 ***
06/02/2019 10:33:40 *** training ***
06/02/2019 10:33:42 step: 3734, epoch: 113, batch: 4, loss: 0.007741101086139679, acc: 100.0, f1: 100.0, r: 0.6411157931462569
06/02/2019 10:33:44 step: 3739, epoch: 113, batch: 9, loss: 0.006790824234485626, acc: 100.0, f1: 100.0, r: 0.7707810724740709
06/02/2019 10:33:47 step: 3744, epoch: 113, batch: 14, loss: 0.013680152595043182, acc: 100.0, f1: 100.0, r: 0.749139546094324
06/02/2019 10:33:49 step: 3749, epoch: 113, batch: 19, loss: 0.007304064929485321, acc: 100.0, f1: 100.0, r: 0.6733906436782885
06/02/2019 10:33:51 step: 3754, epoch: 113, batch: 24, loss: 0.010161392390727997, acc: 100.0, f1: 100.0, r: 0.6209149157826964
06/02/2019 10:33:54 step: 3759, epoch: 113, batch: 29, loss: 0.015382058918476105, acc: 100.0, f1: 100.0, r: 0.81749953599679
06/02/2019 10:33:55 *** evaluating ***
06/02/2019 10:33:56 step: 114, epoch: 113, acc: 49.14529914529914, f1: 26.767439226280466, r: 0.25147265371061306
06/02/2019 10:33:56 *** epoch: 115 ***
06/02/2019 10:33:56 *** training ***
06/02/2019 10:33:58 step: 3767, epoch: 114, batch: 4, loss: 0.005076661705970764, acc: 100.0, f1: 100.0, r: 0.6758061657111831
06/02/2019 10:34:00 step: 3772, epoch: 114, batch: 9, loss: 0.004312470555305481, acc: 100.0, f1: 100.0, r: 0.6508924355579072
06/02/2019 10:34:02 step: 3777, epoch: 114, batch: 14, loss: 0.010030098259449005, acc: 100.0, f1: 100.0, r: 0.7781702888787612
06/02/2019 10:34:05 step: 3782, epoch: 114, batch: 19, loss: 0.021628089249134064, acc: 100.0, f1: 100.0, r: 0.6964767877286048
06/02/2019 10:34:07 step: 3787, epoch: 114, batch: 24, loss: 0.020672224462032318, acc: 100.0, f1: 100.0, r: 0.7876463252522499
06/02/2019 10:34:10 step: 3792, epoch: 114, batch: 29, loss: 0.010400660336017609, acc: 100.0, f1: 100.0, r: 0.7550881326877528
06/02/2019 10:34:11 *** evaluating ***
06/02/2019 10:34:11 step: 115, epoch: 114, acc: 48.29059829059829, f1: 27.650769964843192, r: 0.25135413765208925
06/02/2019 10:34:11 *** epoch: 116 ***
06/02/2019 10:34:11 *** training ***
06/02/2019 10:34:14 step: 3800, epoch: 115, batch: 4, loss: 0.010773271322250366, acc: 100.0, f1: 100.0, r: 0.6498079345198148
06/02/2019 10:34:16 step: 3805, epoch: 115, batch: 9, loss: 0.0030104368925094604, acc: 100.0, f1: 100.0, r: 0.7311473253326416
06/02/2019 10:34:18 step: 3810, epoch: 115, batch: 14, loss: 0.003741428256034851, acc: 100.0, f1: 100.0, r: 0.756156170838788
06/02/2019 10:34:20 step: 3815, epoch: 115, batch: 19, loss: 0.01691630482673645, acc: 100.0, f1: 100.0, r: 0.821089227620265
06/02/2019 10:34:22 step: 3820, epoch: 115, batch: 24, loss: 0.010300055146217346, acc: 100.0, f1: 100.0, r: 0.8071088845892807
06/02/2019 10:34:24 step: 3825, epoch: 115, batch: 29, loss: 0.009722284972667694, acc: 100.0, f1: 100.0, r: 0.6878587411703492
06/02/2019 10:34:26 *** evaluating ***
06/02/2019 10:34:27 step: 116, epoch: 115, acc: 50.0, f1: 27.349158358324217, r: 0.25387743135972923
06/02/2019 10:34:27 *** epoch: 117 ***
06/02/2019 10:34:27 *** training ***
06/02/2019 10:34:29 step: 3833, epoch: 116, batch: 4, loss: 0.00739741325378418, acc: 100.0, f1: 100.0, r: 0.7018397974909373
06/02/2019 10:34:31 step: 3838, epoch: 116, batch: 9, loss: 0.007336542010307312, acc: 100.0, f1: 100.0, r: 0.7465754119949007
06/02/2019 10:34:33 step: 3843, epoch: 116, batch: 14, loss: 0.006180666387081146, acc: 100.0, f1: 100.0, r: 0.7399264554339899
06/02/2019 10:34:35 step: 3848, epoch: 116, batch: 19, loss: 0.024532072246074677, acc: 100.0, f1: 100.0, r: 0.7340374238215047
06/02/2019 10:34:38 step: 3853, epoch: 116, batch: 24, loss: 0.008439704775810242, acc: 100.0, f1: 100.0, r: 0.7052317226359445
06/02/2019 10:34:41 step: 3858, epoch: 116, batch: 29, loss: 0.008810199797153473, acc: 100.0, f1: 100.0, r: 0.8397220897141711
06/02/2019 10:34:42 *** evaluating ***
06/02/2019 10:34:42 step: 117, epoch: 116, acc: 50.85470085470085, f1: 29.972333630421865, r: 0.2600396956121523
06/02/2019 10:34:42 *** epoch: 118 ***
06/02/2019 10:34:42 *** training ***
06/02/2019 10:34:44 step: 3866, epoch: 117, batch: 4, loss: 0.011518143117427826, acc: 100.0, f1: 100.0, r: 0.7872926068215346
06/02/2019 10:34:47 step: 3871, epoch: 117, batch: 9, loss: 0.01803722232580185, acc: 100.0, f1: 100.0, r: 0.7754419602139152
06/02/2019 10:34:49 step: 3876, epoch: 117, batch: 14, loss: 0.026877224445343018, acc: 100.0, f1: 100.0, r: 0.8295142999902866
06/02/2019 10:34:51 step: 3881, epoch: 117, batch: 19, loss: 0.015361718833446503, acc: 100.0, f1: 100.0, r: 0.6924533802709916
06/02/2019 10:34:54 step: 3886, epoch: 117, batch: 24, loss: 0.01075395941734314, acc: 100.0, f1: 100.0, r: 0.7338465495474492
06/02/2019 10:34:56 step: 3891, epoch: 117, batch: 29, loss: 0.00943060964345932, acc: 100.0, f1: 100.0, r: 0.7094875047750612
06/02/2019 10:34:57 *** evaluating ***
06/02/2019 10:34:58 step: 118, epoch: 117, acc: 50.427350427350426, f1: 28.945148876837855, r: 0.2515111188624925
06/02/2019 10:34:58 *** epoch: 119 ***
06/02/2019 10:34:58 *** training ***
06/02/2019 10:35:00 step: 3899, epoch: 118, batch: 4, loss: 0.007428981363773346, acc: 100.0, f1: 100.0, r: 0.8191716348476752
06/02/2019 10:35:03 step: 3904, epoch: 118, batch: 9, loss: 0.01237187534570694, acc: 100.0, f1: 100.0, r: 0.756659719499813
06/02/2019 10:35:04 step: 3909, epoch: 118, batch: 14, loss: 0.02541903406381607, acc: 98.4375, f1: 99.31172468987596, r: 0.6592712443466417
06/02/2019 10:35:07 step: 3914, epoch: 118, batch: 19, loss: 0.012110739946365356, acc: 100.0, f1: 100.0, r: 0.6902645191385258
06/02/2019 10:35:09 step: 3919, epoch: 118, batch: 24, loss: 0.005607537925243378, acc: 100.0, f1: 100.0, r: 0.6913153061711533
06/02/2019 10:35:11 step: 3924, epoch: 118, batch: 29, loss: 0.026736080646514893, acc: 100.0, f1: 100.0, r: 0.8113433186486012
06/02/2019 10:35:13 *** evaluating ***
06/02/2019 10:35:13 step: 119, epoch: 118, acc: 49.572649572649574, f1: 27.609085915121966, r: 0.2452363644455578
06/02/2019 10:35:13 *** epoch: 120 ***
06/02/2019 10:35:13 *** training ***
06/02/2019 10:35:15 step: 3932, epoch: 119, batch: 4, loss: 0.011176235973834991, acc: 100.0, f1: 100.0, r: 0.6612356611544836
06/02/2019 10:35:18 step: 3937, epoch: 119, batch: 9, loss: 0.016257695853710175, acc: 100.0, f1: 100.0, r: 0.7975580423352959
06/02/2019 10:35:20 step: 3942, epoch: 119, batch: 14, loss: 0.010641470551490784, acc: 100.0, f1: 100.0, r: 0.7480246655654056
06/02/2019 10:35:23 step: 3947, epoch: 119, batch: 19, loss: 0.03841593116521835, acc: 98.4375, f1: 97.94941900205058, r: 0.7464579651733205
06/02/2019 10:35:25 step: 3952, epoch: 119, batch: 24, loss: 0.013913922011852264, acc: 100.0, f1: 100.0, r: 0.8306801772988649
06/02/2019 10:35:27 step: 3957, epoch: 119, batch: 29, loss: 0.022051289677619934, acc: 98.4375, f1: 97.00680272108845, r: 0.6740602459888324
06/02/2019 10:35:28 *** evaluating ***
06/02/2019 10:35:29 step: 120, epoch: 119, acc: 49.572649572649574, f1: 27.361690093480597, r: 0.2529514809395013
06/02/2019 10:35:29 *** epoch: 121 ***
06/02/2019 10:35:29 *** training ***
06/02/2019 10:35:31 step: 3965, epoch: 120, batch: 4, loss: 0.008851222693920135, acc: 100.0, f1: 100.0, r: 0.807309934671106
06/02/2019 10:35:34 step: 3970, epoch: 120, batch: 9, loss: 0.007380478084087372, acc: 100.0, f1: 100.0, r: 0.5804489053375582
06/02/2019 10:35:37 step: 3975, epoch: 120, batch: 14, loss: 0.013808824121952057, acc: 100.0, f1: 100.0, r: 0.7999285800898654
06/02/2019 10:35:39 step: 3980, epoch: 120, batch: 19, loss: 0.006890147924423218, acc: 100.0, f1: 100.0, r: 0.7140552531336656
06/02/2019 10:35:41 step: 3985, epoch: 120, batch: 24, loss: 0.013155698776245117, acc: 100.0, f1: 100.0, r: 0.6594038289915672
06/02/2019 10:35:43 step: 3990, epoch: 120, batch: 29, loss: 0.007760375738143921, acc: 100.0, f1: 100.0, r: 0.6976198157638083
06/02/2019 10:35:44 *** evaluating ***
06/02/2019 10:35:45 step: 121, epoch: 120, acc: 51.28205128205128, f1: 29.80619967222175, r: 0.26490872661442155
06/02/2019 10:35:45 *** epoch: 122 ***
06/02/2019 10:35:45 *** training ***
06/02/2019 10:35:47 step: 3998, epoch: 121, batch: 4, loss: 0.012281209230422974, acc: 100.0, f1: 100.0, r: 0.6370584136565149
06/02/2019 10:35:49 step: 4003, epoch: 121, batch: 9, loss: 0.011716008186340332, acc: 100.0, f1: 100.0, r: 0.6745179924188581
06/02/2019 10:35:51 step: 4008, epoch: 121, batch: 14, loss: 0.004467874765396118, acc: 100.0, f1: 100.0, r: 0.7674588742990853
06/02/2019 10:35:53 step: 4013, epoch: 121, batch: 19, loss: 0.014578230679035187, acc: 100.0, f1: 100.0, r: 0.7874029276666044
06/02/2019 10:35:56 step: 4018, epoch: 121, batch: 24, loss: 0.0065430402755737305, acc: 100.0, f1: 100.0, r: 0.7067240630844623
06/02/2019 10:35:58 step: 4023, epoch: 121, batch: 29, loss: 0.0064467936754226685, acc: 100.0, f1: 100.0, r: 0.7224088186546146
06/02/2019 10:35:59 *** evaluating ***
06/02/2019 10:36:00 step: 122, epoch: 121, acc: 50.0, f1: 28.028292872388405, r: 0.24860716693622367
06/02/2019 10:36:00 *** epoch: 123 ***
06/02/2019 10:36:00 *** training ***
06/02/2019 10:36:02 step: 4031, epoch: 122, batch: 4, loss: 0.005335398018360138, acc: 100.0, f1: 100.0, r: 0.8212279240277931
06/02/2019 10:36:04 step: 4036, epoch: 122, batch: 9, loss: 0.009027674794197083, acc: 100.0, f1: 100.0, r: 0.8267497752410815
06/02/2019 10:36:06 step: 4041, epoch: 122, batch: 14, loss: 0.018088489770889282, acc: 100.0, f1: 100.0, r: 0.7394255202464382
06/02/2019 10:36:08 step: 4046, epoch: 122, batch: 19, loss: 0.010418012738227844, acc: 100.0, f1: 100.0, r: 0.6614968396155858
06/02/2019 10:36:11 step: 4051, epoch: 122, batch: 24, loss: 0.01933005452156067, acc: 100.0, f1: 100.0, r: 0.7870904830865884
06/02/2019 10:36:13 step: 4056, epoch: 122, batch: 29, loss: 0.012015171349048615, acc: 100.0, f1: 100.0, r: 0.7769185793529617
06/02/2019 10:36:14 *** evaluating ***
06/02/2019 10:36:15 step: 123, epoch: 122, acc: 50.0, f1: 29.41424351495245, r: 0.25361528710045417
06/02/2019 10:36:15 *** epoch: 124 ***
06/02/2019 10:36:15 *** training ***
06/02/2019 10:36:17 step: 4064, epoch: 123, batch: 4, loss: 0.007841795682907104, acc: 100.0, f1: 100.0, r: 0.6561010597499035
06/02/2019 10:36:19 step: 4069, epoch: 123, batch: 9, loss: 0.00373106449842453, acc: 100.0, f1: 100.0, r: 0.7373337113965943
06/02/2019 10:36:21 step: 4074, epoch: 123, batch: 14, loss: 0.012255817651748657, acc: 100.0, f1: 100.0, r: 0.7629885554170885
06/02/2019 10:36:24 step: 4079, epoch: 123, batch: 19, loss: 0.012277364730834961, acc: 100.0, f1: 100.0, r: 0.7285704936588493
06/02/2019 10:36:27 step: 4084, epoch: 123, batch: 24, loss: 0.0060743242502212524, acc: 100.0, f1: 100.0, r: 0.702000090908785
06/02/2019 10:36:29 step: 4089, epoch: 123, batch: 29, loss: 0.007196374237537384, acc: 100.0, f1: 100.0, r: 0.8042272612602213
06/02/2019 10:36:30 *** evaluating ***
06/02/2019 10:36:31 step: 124, epoch: 123, acc: 50.85470085470085, f1: 30.799442155613626, r: 0.25986562521399464
06/02/2019 10:36:31 *** epoch: 125 ***
06/02/2019 10:36:31 *** training ***
06/02/2019 10:36:33 step: 4097, epoch: 124, batch: 4, loss: 0.006067067384719849, acc: 100.0, f1: 100.0, r: 0.6998075563564947
06/02/2019 10:36:36 step: 4102, epoch: 124, batch: 9, loss: 0.0046350061893463135, acc: 100.0, f1: 100.0, r: 0.7782584589161617
06/02/2019 10:36:38 step: 4107, epoch: 124, batch: 14, loss: 0.02537243813276291, acc: 100.0, f1: 100.0, r: 0.7887479342824701
06/02/2019 10:36:40 step: 4112, epoch: 124, batch: 19, loss: 0.003473326563835144, acc: 100.0, f1: 100.0, r: 0.5973019120032955
06/02/2019 10:36:42 step: 4117, epoch: 124, batch: 24, loss: 0.024026192724704742, acc: 100.0, f1: 100.0, r: 0.8095843996745288
06/02/2019 10:36:44 step: 4122, epoch: 124, batch: 29, loss: 0.0052957311272621155, acc: 100.0, f1: 100.0, r: 0.7675308920291901
06/02/2019 10:36:45 *** evaluating ***
06/02/2019 10:36:46 step: 125, epoch: 124, acc: 47.863247863247864, f1: 27.48835414676999, r: 0.25952564177464466
06/02/2019 10:36:46 *** epoch: 126 ***
06/02/2019 10:36:46 *** training ***
06/02/2019 10:36:49 step: 4130, epoch: 125, batch: 4, loss: 0.008756615221500397, acc: 100.0, f1: 100.0, r: 0.8391823788301838
06/02/2019 10:36:51 step: 4135, epoch: 125, batch: 9, loss: 0.0043882280588150024, acc: 100.0, f1: 100.0, r: 0.671391083998811
06/02/2019 10:36:53 step: 4140, epoch: 125, batch: 14, loss: 0.017800413072109222, acc: 100.0, f1: 100.0, r: 0.6745853365382884
06/02/2019 10:36:55 step: 4145, epoch: 125, batch: 19, loss: 0.018145836889743805, acc: 100.0, f1: 100.0, r: 0.7157307094145638
06/02/2019 10:36:57 step: 4150, epoch: 125, batch: 24, loss: 0.003826439380645752, acc: 100.0, f1: 100.0, r: 0.6807394839358147
06/02/2019 10:37:00 step: 4155, epoch: 125, batch: 29, loss: 0.0056180208921432495, acc: 100.0, f1: 100.0, r: 0.6240937463780295
06/02/2019 10:37:01 *** evaluating ***
06/02/2019 10:37:02 step: 126, epoch: 125, acc: 48.717948717948715, f1: 27.67622277937812, r: 0.2513587994786015
06/02/2019 10:37:02 *** epoch: 127 ***
06/02/2019 10:37:02 *** training ***
06/02/2019 10:37:04 step: 4163, epoch: 126, batch: 4, loss: 0.004765510559082031, acc: 100.0, f1: 100.0, r: 0.6030009490535304
06/02/2019 10:37:07 step: 4168, epoch: 126, batch: 9, loss: 0.01709076017141342, acc: 100.0, f1: 100.0, r: 0.8353244777839254
06/02/2019 10:37:09 step: 4173, epoch: 126, batch: 14, loss: 0.009737282991409302, acc: 100.0, f1: 100.0, r: 0.820112368578902
06/02/2019 10:37:11 step: 4178, epoch: 126, batch: 19, loss: 0.004716195166110992, acc: 100.0, f1: 100.0, r: 0.5786678518250027
06/02/2019 10:37:14 step: 4183, epoch: 126, batch: 24, loss: 0.004681393504142761, acc: 100.0, f1: 100.0, r: 0.691220236572648
06/02/2019 10:37:16 step: 4188, epoch: 126, batch: 29, loss: 0.010645240545272827, acc: 100.0, f1: 100.0, r: 0.7144662840524243
06/02/2019 10:37:17 *** evaluating ***
06/02/2019 10:37:18 step: 127, epoch: 126, acc: 47.863247863247864, f1: 27.66793424163245, r: 0.2556504653226088
06/02/2019 10:37:18 *** epoch: 128 ***
06/02/2019 10:37:18 *** training ***
06/02/2019 10:37:20 step: 4196, epoch: 127, batch: 4, loss: 0.002949640154838562, acc: 100.0, f1: 100.0, r: 0.6874410467580038
06/02/2019 10:37:23 step: 4201, epoch: 127, batch: 9, loss: 0.006705261766910553, acc: 100.0, f1: 100.0, r: 0.7119145933786984
06/02/2019 10:37:25 step: 4206, epoch: 127, batch: 14, loss: 0.0019666701555252075, acc: 100.0, f1: 100.0, r: 0.6487093459834923
06/02/2019 10:37:27 step: 4211, epoch: 127, batch: 19, loss: 0.003871604800224304, acc: 100.0, f1: 100.0, r: 0.6393827686856028
06/02/2019 10:37:29 step: 4216, epoch: 127, batch: 24, loss: 0.019190870225429535, acc: 100.0, f1: 100.0, r: 0.7111566748369823
06/02/2019 10:37:32 step: 4221, epoch: 127, batch: 29, loss: 0.012079894542694092, acc: 100.0, f1: 100.0, r: 0.6514306069058495
06/02/2019 10:37:33 *** evaluating ***
06/02/2019 10:37:34 step: 128, epoch: 127, acc: 49.14529914529914, f1: 29.053059231662363, r: 0.25201353263674026
06/02/2019 10:37:34 *** epoch: 129 ***
06/02/2019 10:37:34 *** training ***
06/02/2019 10:37:36 step: 4229, epoch: 128, batch: 4, loss: 0.005775682628154755, acc: 100.0, f1: 100.0, r: 0.7887327148117347
06/02/2019 10:37:38 step: 4234, epoch: 128, batch: 9, loss: 0.004882656037807465, acc: 100.0, f1: 100.0, r: 0.6760215317122009
06/02/2019 10:37:40 step: 4239, epoch: 128, batch: 14, loss: 0.009037941694259644, acc: 100.0, f1: 100.0, r: 0.8034686533664362
06/02/2019 10:37:42 step: 4244, epoch: 128, batch: 19, loss: 0.004175126552581787, acc: 100.0, f1: 100.0, r: 0.806751747021512
06/02/2019 10:37:45 step: 4249, epoch: 128, batch: 24, loss: 0.007022768259048462, acc: 100.0, f1: 100.0, r: 0.7767418367750618
06/02/2019 10:37:47 step: 4254, epoch: 128, batch: 29, loss: 0.008933395147323608, acc: 100.0, f1: 100.0, r: 0.6516514962941928
06/02/2019 10:37:48 *** evaluating ***
06/02/2019 10:37:49 step: 129, epoch: 128, acc: 48.29059829059829, f1: 26.67822037362911, r: 0.24593912075283259
06/02/2019 10:37:49 *** epoch: 130 ***
06/02/2019 10:37:49 *** training ***
06/02/2019 10:37:52 step: 4262, epoch: 129, batch: 4, loss: 0.01693648099899292, acc: 98.4375, f1: 97.81105990783409, r: 0.7028012099254691
06/02/2019 10:37:54 step: 4267, epoch: 129, batch: 9, loss: 0.005482852458953857, acc: 100.0, f1: 100.0, r: 0.6468503927072218
06/02/2019 10:37:56 step: 4272, epoch: 129, batch: 14, loss: 0.0030674710869789124, acc: 100.0, f1: 100.0, r: 0.7671528715380885
06/02/2019 10:37:58 step: 4277, epoch: 129, batch: 19, loss: 0.00396323949098587, acc: 100.0, f1: 100.0, r: 0.7111008857426458
06/02/2019 10:38:01 step: 4282, epoch: 129, batch: 24, loss: 0.012009680271148682, acc: 100.0, f1: 100.0, r: 0.7807251849910116
06/02/2019 10:38:03 step: 4287, epoch: 129, batch: 29, loss: 0.008363030850887299, acc: 100.0, f1: 100.0, r: 0.7423907581927022
06/02/2019 10:38:05 *** evaluating ***
06/02/2019 10:38:05 step: 130, epoch: 129, acc: 50.0, f1: 28.343049519531704, r: 0.2580374278460066
06/02/2019 10:38:05 *** epoch: 131 ***
06/02/2019 10:38:05 *** training ***
06/02/2019 10:38:07 step: 4295, epoch: 130, batch: 4, loss: 0.0022306740283966064, acc: 100.0, f1: 100.0, r: 0.6075275541608053
06/02/2019 10:38:09 step: 4300, epoch: 130, batch: 9, loss: 0.013700976967811584, acc: 100.0, f1: 100.0, r: 0.7949222699414903
06/02/2019 10:38:11 step: 4305, epoch: 130, batch: 14, loss: 0.0017344504594802856, acc: 100.0, f1: 100.0, r: 0.6935279987519264
06/02/2019 10:38:13 step: 4310, epoch: 130, batch: 19, loss: 0.01163562387228012, acc: 100.0, f1: 100.0, r: 0.696457601613444
06/02/2019 10:38:16 step: 4315, epoch: 130, batch: 24, loss: 0.0038262605667114258, acc: 100.0, f1: 100.0, r: 0.7300487938503775
06/02/2019 10:38:18 step: 4320, epoch: 130, batch: 29, loss: 0.004805274307727814, acc: 100.0, f1: 100.0, r: 0.8177500837844509
06/02/2019 10:38:20 *** evaluating ***
06/02/2019 10:38:20 step: 131, epoch: 130, acc: 49.14529914529914, f1: 28.06431126868062, r: 0.2553678318125136
06/02/2019 10:38:20 *** epoch: 132 ***
06/02/2019 10:38:20 *** training ***
06/02/2019 10:38:23 step: 4328, epoch: 131, batch: 4, loss: 0.0032778680324554443, acc: 100.0, f1: 100.0, r: 0.7197999321492243
06/02/2019 10:38:26 step: 4333, epoch: 131, batch: 9, loss: 0.007608436048030853, acc: 100.0, f1: 100.0, r: 0.7086939579661893
06/02/2019 10:38:28 step: 4338, epoch: 131, batch: 14, loss: 0.0021039023995399475, acc: 100.0, f1: 100.0, r: 0.5893882578751556
06/02/2019 10:38:31 step: 4343, epoch: 131, batch: 19, loss: 0.005184143781661987, acc: 100.0, f1: 100.0, r: 0.782432416269375
06/02/2019 10:38:32 step: 4348, epoch: 131, batch: 24, loss: 0.007685497403144836, acc: 100.0, f1: 100.0, r: 0.6694874466412275
06/02/2019 10:38:35 step: 4353, epoch: 131, batch: 29, loss: 0.007263019680976868, acc: 100.0, f1: 100.0, r: 0.7887615083916812
06/02/2019 10:38:36 *** evaluating ***
06/02/2019 10:38:37 step: 132, epoch: 131, acc: 50.0, f1: 28.514430908662465, r: 0.26191543942837314
06/02/2019 10:38:37 *** epoch: 133 ***
06/02/2019 10:38:37 *** training ***
06/02/2019 10:38:39 step: 4361, epoch: 132, batch: 4, loss: 0.008502788841724396, acc: 100.0, f1: 100.0, r: 0.6690338025886347
06/02/2019 10:38:42 step: 4366, epoch: 132, batch: 9, loss: 0.019362546503543854, acc: 100.0, f1: 100.0, r: 0.7331430097465549
06/02/2019 10:38:44 step: 4371, epoch: 132, batch: 14, loss: 0.005847960710525513, acc: 100.0, f1: 100.0, r: 0.7788886818097117
06/02/2019 10:38:47 step: 4376, epoch: 132, batch: 19, loss: 0.011792056262493134, acc: 100.0, f1: 100.0, r: 0.691082351023226
06/02/2019 10:38:49 step: 4381, epoch: 132, batch: 24, loss: 0.00528872013092041, acc: 100.0, f1: 100.0, r: 0.7211951183618251
06/02/2019 10:38:51 step: 4386, epoch: 132, batch: 29, loss: 0.0036827251315116882, acc: 100.0, f1: 100.0, r: 0.8342597939180221
06/02/2019 10:38:52 *** evaluating ***
06/02/2019 10:38:53 step: 133, epoch: 132, acc: 50.427350427350426, f1: 29.63424609736693, r: 0.2643559357482552
06/02/2019 10:38:53 *** epoch: 134 ***
06/02/2019 10:38:53 *** training ***
06/02/2019 10:38:55 step: 4394, epoch: 133, batch: 4, loss: 0.005263783037662506, acc: 100.0, f1: 100.0, r: 0.8387789042095294
06/02/2019 10:38:57 step: 4399, epoch: 133, batch: 9, loss: 0.005423605442047119, acc: 100.0, f1: 100.0, r: 0.7596921280944046
06/02/2019 10:39:00 step: 4404, epoch: 133, batch: 14, loss: 0.013217009603977203, acc: 100.0, f1: 100.0, r: 0.7240033170950712
06/02/2019 10:39:02 step: 4409, epoch: 133, batch: 19, loss: 0.004323408007621765, acc: 100.0, f1: 100.0, r: 0.7328343267561638
06/02/2019 10:39:04 step: 4414, epoch: 133, batch: 24, loss: 0.006945475935935974, acc: 100.0, f1: 100.0, r: 0.634841527598604
06/02/2019 10:39:06 step: 4419, epoch: 133, batch: 29, loss: 0.013379760086536407, acc: 100.0, f1: 100.0, r: 0.6814426514630244
06/02/2019 10:39:07 *** evaluating ***
06/02/2019 10:39:08 step: 134, epoch: 133, acc: 49.572649572649574, f1: 28.115547902203776, r: 0.25655953390871145
06/02/2019 10:39:08 *** epoch: 135 ***
06/02/2019 10:39:08 *** training ***
06/02/2019 10:39:10 step: 4427, epoch: 134, batch: 4, loss: 0.012234233319759369, acc: 100.0, f1: 100.0, r: 0.7103747344198172
06/02/2019 10:39:13 step: 4432, epoch: 134, batch: 9, loss: 0.009257584810256958, acc: 100.0, f1: 100.0, r: 0.8703659580929848
06/02/2019 10:39:15 step: 4437, epoch: 134, batch: 14, loss: 0.008091993629932404, acc: 100.0, f1: 100.0, r: 0.7699867832025551
06/02/2019 10:39:17 step: 4442, epoch: 134, batch: 19, loss: 0.013406023383140564, acc: 100.0, f1: 100.0, r: 0.7551141095690792
06/02/2019 10:39:19 step: 4447, epoch: 134, batch: 24, loss: 0.004952356219291687, acc: 100.0, f1: 100.0, r: 0.6735239520865085
06/02/2019 10:39:22 step: 4452, epoch: 134, batch: 29, loss: 0.007881782948970795, acc: 100.0, f1: 100.0, r: 0.7479822392930839
06/02/2019 10:39:23 *** evaluating ***
06/02/2019 10:39:23 step: 135, epoch: 134, acc: 50.0, f1: 28.677342697385356, r: 0.2495405183873395
06/02/2019 10:39:23 *** epoch: 136 ***
06/02/2019 10:39:23 *** training ***
06/02/2019 10:39:26 step: 4460, epoch: 135, batch: 4, loss: 0.014430291950702667, acc: 100.0, f1: 100.0, r: 0.6932029130093699
06/02/2019 10:39:28 step: 4465, epoch: 135, batch: 9, loss: 0.006099626421928406, acc: 100.0, f1: 100.0, r: 0.822458385247394
06/02/2019 10:39:30 step: 4470, epoch: 135, batch: 14, loss: 0.01063033938407898, acc: 100.0, f1: 100.0, r: 0.677745028966636
06/02/2019 10:39:33 step: 4475, epoch: 135, batch: 19, loss: 0.008231960237026215, acc: 100.0, f1: 100.0, r: 0.751322886204921
06/02/2019 10:39:35 step: 4480, epoch: 135, batch: 24, loss: 0.006764084100723267, acc: 100.0, f1: 100.0, r: 0.8027764611890857
06/02/2019 10:39:37 step: 4485, epoch: 135, batch: 29, loss: 0.01141510158777237, acc: 100.0, f1: 100.0, r: 0.8182830476294202
06/02/2019 10:39:38 *** evaluating ***
06/02/2019 10:39:39 step: 136, epoch: 135, acc: 48.717948717948715, f1: 27.07276670105617, r: 0.24547734832299206
06/02/2019 10:39:39 *** epoch: 137 ***
06/02/2019 10:39:39 *** training ***
06/02/2019 10:39:41 step: 4493, epoch: 136, batch: 4, loss: 0.012621045112609863, acc: 100.0, f1: 100.0, r: 0.8399797355572731
06/02/2019 10:39:43 step: 4498, epoch: 136, batch: 9, loss: 0.0038182884454727173, acc: 100.0, f1: 100.0, r: 0.6542588668880177
06/02/2019 10:39:46 step: 4503, epoch: 136, batch: 14, loss: 0.0027017742395401, acc: 100.0, f1: 100.0, r: 0.6751171518725932
06/02/2019 10:39:48 step: 4508, epoch: 136, batch: 19, loss: 0.01901431381702423, acc: 100.0, f1: 100.0, r: 0.755450495095437
06/02/2019 10:39:51 step: 4513, epoch: 136, batch: 24, loss: 0.0035796016454696655, acc: 100.0, f1: 100.0, r: 0.8130680865907548
06/02/2019 10:39:53 step: 4518, epoch: 136, batch: 29, loss: 0.007720351219177246, acc: 100.0, f1: 100.0, r: 0.7041379903708331
06/02/2019 10:39:54 *** evaluating ***
06/02/2019 10:39:55 step: 137, epoch: 136, acc: 48.717948717948715, f1: 28.732628303073348, r: 0.24966064482216221
06/02/2019 10:39:55 *** epoch: 138 ***
06/02/2019 10:39:55 *** training ***
06/02/2019 10:39:57 step: 4526, epoch: 137, batch: 4, loss: 0.006167836487293243, acc: 100.0, f1: 100.0, r: 0.7057780398400051
06/02/2019 10:39:59 step: 4531, epoch: 137, batch: 9, loss: 0.014395952224731445, acc: 100.0, f1: 100.0, r: 0.8168865601983955
06/02/2019 10:40:01 step: 4536, epoch: 137, batch: 14, loss: 0.0040789395570755005, acc: 100.0, f1: 100.0, r: 0.6857037465935472
06/02/2019 10:40:03 step: 4541, epoch: 137, batch: 19, loss: 0.003656633198261261, acc: 100.0, f1: 100.0, r: 0.6998258044045454
06/02/2019 10:40:05 step: 4546, epoch: 137, batch: 24, loss: 0.001723550260066986, acc: 100.0, f1: 100.0, r: 0.7583051699365589
06/02/2019 10:40:08 step: 4551, epoch: 137, batch: 29, loss: 0.0038024336099624634, acc: 100.0, f1: 100.0, r: 0.738008360856006
06/02/2019 10:40:09 *** evaluating ***
06/02/2019 10:40:10 step: 138, epoch: 137, acc: 49.14529914529914, f1: 27.927891157030682, r: 0.24208655001672202
06/02/2019 10:40:10 *** epoch: 139 ***
06/02/2019 10:40:10 *** training ***
06/02/2019 10:40:12 step: 4559, epoch: 138, batch: 4, loss: 0.002007529139518738, acc: 100.0, f1: 100.0, r: 0.6845362296790828
06/02/2019 10:40:14 step: 4564, epoch: 138, batch: 9, loss: 0.01268080621957779, acc: 100.0, f1: 100.0, r: 0.7697725200747723
06/02/2019 10:40:17 step: 4569, epoch: 138, batch: 14, loss: 0.014988407492637634, acc: 100.0, f1: 100.0, r: 0.6595785881538982
06/02/2019 10:40:20 step: 4574, epoch: 138, batch: 19, loss: 0.005719952285289764, acc: 100.0, f1: 100.0, r: 0.7930491658455122
06/02/2019 10:40:22 step: 4579, epoch: 138, batch: 24, loss: 0.008671633899211884, acc: 100.0, f1: 100.0, r: 0.6949577736425413
06/02/2019 10:40:25 step: 4584, epoch: 138, batch: 29, loss: 0.003720715641975403, acc: 100.0, f1: 100.0, r: 0.6742408431054352
06/02/2019 10:40:26 *** evaluating ***
06/02/2019 10:40:26 step: 139, epoch: 138, acc: 50.0, f1: 28.752955915525945, r: 0.2681570998791343
06/02/2019 10:40:26 *** epoch: 140 ***
06/02/2019 10:40:26 *** training ***
06/02/2019 10:40:29 step: 4592, epoch: 139, batch: 4, loss: 0.0075196921825408936, acc: 100.0, f1: 100.0, r: 0.7362494783350862
06/02/2019 10:40:31 step: 4597, epoch: 139, batch: 9, loss: 0.0045052021741867065, acc: 100.0, f1: 100.0, r: 0.6754563514926408
06/02/2019 10:40:33 step: 4602, epoch: 139, batch: 14, loss: 0.007325679063796997, acc: 100.0, f1: 100.0, r: 0.8283121813593877
06/02/2019 10:40:36 step: 4607, epoch: 139, batch: 19, loss: 0.0037988796830177307, acc: 100.0, f1: 100.0, r: 0.8278612842853478
06/02/2019 10:40:38 step: 4612, epoch: 139, batch: 24, loss: 0.005459226667881012, acc: 100.0, f1: 100.0, r: 0.6633634388284044
06/02/2019 10:40:40 step: 4617, epoch: 139, batch: 29, loss: 0.007884711027145386, acc: 100.0, f1: 100.0, r: 0.7414076649968334
06/02/2019 10:40:42 *** evaluating ***
06/02/2019 10:40:42 step: 140, epoch: 139, acc: 50.427350427350426, f1: 28.24417073489184, r: 0.26268743046973325
06/02/2019 10:40:42 *** epoch: 141 ***
06/02/2019 10:40:42 *** training ***
06/02/2019 10:40:44 step: 4625, epoch: 140, batch: 4, loss: 0.0030798912048339844, acc: 100.0, f1: 100.0, r: 0.7702847668713767
06/02/2019 10:40:47 step: 4630, epoch: 140, batch: 9, loss: 0.02357298880815506, acc: 98.4375, f1: 99.04247104247104, r: 0.6623217820882707
06/02/2019 10:40:49 step: 4635, epoch: 140, batch: 14, loss: 0.008722394704818726, acc: 100.0, f1: 100.0, r: 0.6995194380285434
06/02/2019 10:40:52 step: 4640, epoch: 140, batch: 19, loss: 0.00920356810092926, acc: 100.0, f1: 100.0, r: 0.7074120554426079
06/02/2019 10:40:54 step: 4645, epoch: 140, batch: 24, loss: 0.0049882978200912476, acc: 100.0, f1: 100.0, r: 0.6789857039980006
06/02/2019 10:40:57 step: 4650, epoch: 140, batch: 29, loss: 0.0032517611980438232, acc: 100.0, f1: 100.0, r: 0.7786869689589585
06/02/2019 10:40:58 *** evaluating ***
06/02/2019 10:40:59 step: 141, epoch: 140, acc: 49.572649572649574, f1: 27.974722244772355, r: 0.2578790455926988
06/02/2019 10:40:59 *** epoch: 142 ***
06/02/2019 10:40:59 *** training ***
06/02/2019 10:41:01 step: 4658, epoch: 141, batch: 4, loss: 0.022562313824892044, acc: 100.0, f1: 100.0, r: 0.7075130673247001
06/02/2019 10:41:03 step: 4663, epoch: 141, batch: 9, loss: 0.005651779472827911, acc: 100.0, f1: 100.0, r: 0.7655598320937542
06/02/2019 10:41:05 step: 4668, epoch: 141, batch: 14, loss: 0.002399742603302002, acc: 100.0, f1: 100.0, r: 0.6485914562482941
06/02/2019 10:41:07 step: 4673, epoch: 141, batch: 19, loss: 0.006070032715797424, acc: 100.0, f1: 100.0, r: 0.7976386750860238
06/02/2019 10:41:09 step: 4678, epoch: 141, batch: 24, loss: 0.0023433715105056763, acc: 100.0, f1: 100.0, r: 0.8375299114360335
06/02/2019 10:41:12 step: 4683, epoch: 141, batch: 29, loss: 0.002362385392189026, acc: 100.0, f1: 100.0, r: 0.6946706191953316
06/02/2019 10:41:13 *** evaluating ***
06/02/2019 10:41:14 step: 142, epoch: 141, acc: 50.85470085470085, f1: 28.12714130274535, r: 0.25136162831904796
06/02/2019 10:41:14 *** epoch: 143 ***
06/02/2019 10:41:14 *** training ***
06/02/2019 10:41:16 step: 4691, epoch: 142, batch: 4, loss: 0.0024618208408355713, acc: 100.0, f1: 100.0, r: 0.5989461730284046
06/02/2019 10:41:18 step: 4696, epoch: 142, batch: 9, loss: 0.006350472569465637, acc: 100.0, f1: 100.0, r: 0.7843885320418661
06/02/2019 10:41:21 step: 4701, epoch: 142, batch: 14, loss: 0.0033745020627975464, acc: 100.0, f1: 100.0, r: 0.829614796969605
06/02/2019 10:41:23 step: 4706, epoch: 142, batch: 19, loss: 0.016080543398857117, acc: 100.0, f1: 100.0, r: 0.7307721986262946
06/02/2019 10:41:25 step: 4711, epoch: 142, batch: 24, loss: 0.0020017996430397034, acc: 100.0, f1: 100.0, r: 0.7547384140710307
06/02/2019 10:41:28 step: 4716, epoch: 142, batch: 29, loss: 0.0045738667249679565, acc: 100.0, f1: 100.0, r: 0.7236786045646725
06/02/2019 10:41:29 *** evaluating ***
06/02/2019 10:41:29 step: 143, epoch: 142, acc: 48.717948717948715, f1: 26.77485752001496, r: 0.24772660427590512
06/02/2019 10:41:29 *** epoch: 144 ***
06/02/2019 10:41:29 *** training ***
06/02/2019 10:41:32 step: 4724, epoch: 143, batch: 4, loss: 0.021227769553661346, acc: 98.4375, f1: 98.60853432282003, r: 0.7803947258118844
06/02/2019 10:41:34 step: 4729, epoch: 143, batch: 9, loss: 0.0056957826018333435, acc: 100.0, f1: 100.0, r: 0.6892163693339494
06/02/2019 10:41:36 step: 4734, epoch: 143, batch: 14, loss: 0.00990390032529831, acc: 100.0, f1: 100.0, r: 0.6875604192637411
06/02/2019 10:41:38 step: 4739, epoch: 143, batch: 19, loss: 0.011760585010051727, acc: 100.0, f1: 100.0, r: 0.8219343481966439
06/02/2019 10:41:40 step: 4744, epoch: 143, batch: 24, loss: 0.008541211485862732, acc: 100.0, f1: 100.0, r: 0.8465577580529471
06/02/2019 10:41:43 step: 4749, epoch: 143, batch: 29, loss: 0.007368400692939758, acc: 100.0, f1: 100.0, r: 0.7126254211761934
06/02/2019 10:41:44 *** evaluating ***
06/02/2019 10:41:45 step: 144, epoch: 143, acc: 50.0, f1: 28.3626232115186, r: 0.24868851556822125
06/02/2019 10:41:45 *** epoch: 145 ***
06/02/2019 10:41:45 *** training ***
06/02/2019 10:41:47 step: 4757, epoch: 144, batch: 4, loss: 0.012284673750400543, acc: 100.0, f1: 100.0, r: 0.6727384317952709
06/02/2019 10:41:49 step: 4762, epoch: 144, batch: 9, loss: 0.004755347967147827, acc: 100.0, f1: 100.0, r: 0.6997119742057866
06/02/2019 10:41:51 step: 4767, epoch: 144, batch: 14, loss: 0.024605490267276764, acc: 100.0, f1: 100.0, r: 0.6543511400403165
06/02/2019 10:41:54 step: 4772, epoch: 144, batch: 19, loss: 0.011324122548103333, acc: 100.0, f1: 100.0, r: 0.6616562103252044
06/02/2019 10:41:56 step: 4777, epoch: 144, batch: 24, loss: 0.008428215980529785, acc: 100.0, f1: 100.0, r: 0.6965028798900474
06/02/2019 10:41:59 step: 4782, epoch: 144, batch: 29, loss: 0.004781484603881836, acc: 100.0, f1: 100.0, r: 0.8124074164837395
06/02/2019 10:42:00 *** evaluating ***
06/02/2019 10:42:00 step: 145, epoch: 144, acc: 50.85470085470085, f1: 28.81287378851882, r: 0.2489040698167052
06/02/2019 10:42:00 *** epoch: 146 ***
06/02/2019 10:42:00 *** training ***
06/02/2019 10:42:03 step: 4790, epoch: 145, batch: 4, loss: 0.00442890077829361, acc: 100.0, f1: 100.0, r: 0.8518506404992799
06/02/2019 10:42:05 step: 4795, epoch: 145, batch: 9, loss: 0.0038233324885368347, acc: 100.0, f1: 100.0, r: 0.594180703567486
06/02/2019 10:42:07 step: 4800, epoch: 145, batch: 14, loss: 0.01376836746931076, acc: 100.0, f1: 100.0, r: 0.7152433331823451
06/02/2019 10:42:10 step: 4805, epoch: 145, batch: 19, loss: 0.002239145338535309, acc: 100.0, f1: 100.0, r: 0.6606154828276841
06/02/2019 10:42:12 step: 4810, epoch: 145, batch: 24, loss: 0.003429330885410309, acc: 100.0, f1: 100.0, r: 0.7261820205788967
06/02/2019 10:42:14 step: 4815, epoch: 145, batch: 29, loss: 0.0022820234298706055, acc: 100.0, f1: 100.0, r: 0.7560054110898844
06/02/2019 10:42:15 *** evaluating ***
06/02/2019 10:42:16 step: 146, epoch: 145, acc: 51.70940170940172, f1: 29.051136395245177, r: 0.25359781438524476
06/02/2019 10:42:16 *** epoch: 147 ***
06/02/2019 10:42:16 *** training ***
06/02/2019 10:42:18 step: 4823, epoch: 146, batch: 4, loss: 0.016555599868297577, acc: 100.0, f1: 100.0, r: 0.7087931621575695
06/02/2019 10:42:20 step: 4828, epoch: 146, batch: 9, loss: 0.009086541831493378, acc: 100.0, f1: 100.0, r: 0.7095834044838163
06/02/2019 10:42:23 step: 4833, epoch: 146, batch: 14, loss: 0.008383788168430328, acc: 100.0, f1: 100.0, r: 0.7273905832613016
06/02/2019 10:42:25 step: 4838, epoch: 146, batch: 19, loss: 0.004596084356307983, acc: 100.0, f1: 100.0, r: 0.8527880909246215
06/02/2019 10:42:28 step: 4843, epoch: 146, batch: 24, loss: 0.0020921677350997925, acc: 100.0, f1: 100.0, r: 0.686756716331728
06/02/2019 10:42:30 step: 4848, epoch: 146, batch: 29, loss: 0.00824977457523346, acc: 100.0, f1: 100.0, r: 0.8000515004578002
06/02/2019 10:42:31 *** evaluating ***
06/02/2019 10:42:31 step: 147, epoch: 146, acc: 50.0, f1: 27.65776203153651, r: 0.24497787558534329
06/02/2019 10:42:31 *** epoch: 148 ***
06/02/2019 10:42:31 *** training ***
06/02/2019 10:42:34 step: 4856, epoch: 147, batch: 4, loss: 0.02855490893125534, acc: 98.4375, f1: 95.33333333333334, r: 0.7439240750310105
06/02/2019 10:42:36 step: 4861, epoch: 147, batch: 9, loss: 0.005113348364830017, acc: 100.0, f1: 100.0, r: 0.7274061719926898
06/02/2019 10:42:38 step: 4866, epoch: 147, batch: 14, loss: 0.008136942982673645, acc: 100.0, f1: 100.0, r: 0.706645709058093
06/02/2019 10:42:41 step: 4871, epoch: 147, batch: 19, loss: 0.007422588765621185, acc: 100.0, f1: 100.0, r: 0.8492451356915738
06/02/2019 10:42:43 step: 4876, epoch: 147, batch: 24, loss: 0.004037611186504364, acc: 100.0, f1: 100.0, r: 0.6628213911683466
06/02/2019 10:42:45 step: 4881, epoch: 147, batch: 29, loss: 0.004105657339096069, acc: 100.0, f1: 100.0, r: 0.6654095687341134
06/02/2019 10:42:47 *** evaluating ***
06/02/2019 10:42:47 step: 148, epoch: 147, acc: 48.717948717948715, f1: 26.84588374356835, r: 0.2453659608623076
06/02/2019 10:42:47 *** epoch: 149 ***
06/02/2019 10:42:47 *** training ***
06/02/2019 10:42:49 step: 4889, epoch: 148, batch: 4, loss: 0.003248371183872223, acc: 100.0, f1: 100.0, r: 0.720509504163985
06/02/2019 10:42:51 step: 4894, epoch: 148, batch: 9, loss: 0.0030724704265594482, acc: 100.0, f1: 100.0, r: 0.7686357352890801
06/02/2019 10:42:54 step: 4899, epoch: 148, batch: 14, loss: 0.010345458984375, acc: 100.0, f1: 100.0, r: 0.6171130949137636
06/02/2019 10:42:56 step: 4904, epoch: 148, batch: 19, loss: 0.010528221726417542, acc: 100.0, f1: 100.0, r: 0.723227825193167
06/02/2019 10:42:59 step: 4909, epoch: 148, batch: 24, loss: 0.0038965046405792236, acc: 100.0, f1: 100.0, r: 0.7173664693954112
06/02/2019 10:43:01 step: 4914, epoch: 148, batch: 29, loss: 0.0044279322028160095, acc: 100.0, f1: 100.0, r: 0.6842972675854372
06/02/2019 10:43:02 *** evaluating ***
06/02/2019 10:43:03 step: 149, epoch: 148, acc: 49.14529914529914, f1: 27.1021030574502, r: 0.24322305151663598
06/02/2019 10:43:03 *** epoch: 150 ***
06/02/2019 10:43:03 *** training ***
06/02/2019 10:43:05 step: 4922, epoch: 149, batch: 4, loss: 0.002270318567752838, acc: 100.0, f1: 100.0, r: 0.6701920671636525
06/02/2019 10:43:07 step: 4927, epoch: 149, batch: 9, loss: 0.0016017928719520569, acc: 100.0, f1: 100.0, r: 0.73660812869204
06/02/2019 10:43:09 step: 4932, epoch: 149, batch: 14, loss: 0.005948327481746674, acc: 100.0, f1: 100.0, r: 0.7764807369361929
06/02/2019 10:43:11 step: 4937, epoch: 149, batch: 19, loss: 0.01182498037815094, acc: 100.0, f1: 100.0, r: 0.8041656158637225
06/02/2019 10:43:14 step: 4942, epoch: 149, batch: 24, loss: 0.0033935531973838806, acc: 100.0, f1: 100.0, r: 0.7799933529640153
06/02/2019 10:43:16 step: 4947, epoch: 149, batch: 29, loss: 0.0020086467266082764, acc: 100.0, f1: 100.0, r: 0.6814246263197028
06/02/2019 10:43:17 *** evaluating ***
06/02/2019 10:43:18 step: 150, epoch: 149, acc: 48.717948717948715, f1: 26.922397245915384, r: 0.24272870087726692
06/02/2019 10:43:18 *** epoch: 151 ***
06/02/2019 10:43:18 *** training ***
06/02/2019 10:43:20 step: 4955, epoch: 150, batch: 4, loss: 0.04215438291430473, acc: 98.4375, f1: 98.36363636363636, r: 0.7548909612783681
06/02/2019 10:43:22 step: 4960, epoch: 150, batch: 9, loss: 0.005255423486232758, acc: 100.0, f1: 100.0, r: 0.6995097386878442
06/02/2019 10:43:24 step: 4965, epoch: 150, batch: 14, loss: 0.01172906905412674, acc: 100.0, f1: 100.0, r: 0.6456106019849481
06/02/2019 10:43:27 step: 4970, epoch: 150, batch: 19, loss: 0.0010256916284561157, acc: 100.0, f1: 100.0, r: 0.7270340328963715
06/02/2019 10:43:30 step: 4975, epoch: 150, batch: 24, loss: 0.012272901833057404, acc: 100.0, f1: 100.0, r: 0.7323072265957237
06/02/2019 10:43:32 step: 4980, epoch: 150, batch: 29, loss: 0.0030255764722824097, acc: 100.0, f1: 100.0, r: 0.6930963559820577
06/02/2019 10:43:33 *** evaluating ***
06/02/2019 10:43:33 step: 151, epoch: 150, acc: 47.863247863247864, f1: 26.982648049699243, r: 0.24687208027847474
06/02/2019 10:43:33 *** epoch: 152 ***
06/02/2019 10:43:33 *** training ***
06/02/2019 10:43:36 step: 4988, epoch: 151, batch: 4, loss: 0.004614129662513733, acc: 100.0, f1: 100.0, r: 0.7839236299855944
06/02/2019 10:43:38 step: 4993, epoch: 151, batch: 9, loss: 0.004187434911727905, acc: 100.0, f1: 100.0, r: 0.7395258954400536
06/02/2019 10:43:40 step: 4998, epoch: 151, batch: 14, loss: 0.0027057304978370667, acc: 100.0, f1: 100.0, r: 0.7213874910811169
06/02/2019 10:43:43 step: 5003, epoch: 151, batch: 19, loss: 0.0011674761772155762, acc: 100.0, f1: 100.0, r: 0.7782711436900818
06/02/2019 10:43:45 step: 5008, epoch: 151, batch: 24, loss: 0.008967533707618713, acc: 100.0, f1: 100.0, r: 0.6872282868177234
06/02/2019 10:43:47 step: 5013, epoch: 151, batch: 29, loss: 0.008212342858314514, acc: 100.0, f1: 100.0, r: 0.7741037434706883
06/02/2019 10:43:48 *** evaluating ***
06/02/2019 10:43:49 step: 152, epoch: 151, acc: 49.14529914529914, f1: 27.626675262493222, r: 0.24921216047857303
06/02/2019 10:43:49 *** epoch: 153 ***
06/02/2019 10:43:49 *** training ***
06/02/2019 10:43:51 step: 5021, epoch: 152, batch: 4, loss: 0.005481652915477753, acc: 100.0, f1: 100.0, r: 0.8029410546671802
06/02/2019 10:43:54 step: 5026, epoch: 152, batch: 9, loss: 0.014429733157157898, acc: 100.0, f1: 100.0, r: 0.720149226190364
06/02/2019 10:43:56 step: 5031, epoch: 152, batch: 14, loss: 0.002567797899246216, acc: 100.0, f1: 100.0, r: 0.7713976775461714
06/02/2019 10:43:58 step: 5036, epoch: 152, batch: 19, loss: 0.005568958818912506, acc: 100.0, f1: 100.0, r: 0.798248360190288
06/02/2019 10:44:00 step: 5041, epoch: 152, batch: 24, loss: 0.007817842066287994, acc: 100.0, f1: 100.0, r: 0.7290286524390133
06/02/2019 10:44:03 step: 5046, epoch: 152, batch: 29, loss: 0.00892263650894165, acc: 100.0, f1: 100.0, r: 0.7624725374995922
06/02/2019 10:44:04 *** evaluating ***
06/02/2019 10:44:04 step: 153, epoch: 152, acc: 49.572649572649574, f1: 28.16075898684516, r: 0.25213047031482383
06/02/2019 10:44:04 *** epoch: 154 ***
06/02/2019 10:44:04 *** training ***
06/02/2019 10:44:06 step: 5054, epoch: 153, batch: 4, loss: 0.004788145422935486, acc: 100.0, f1: 100.0, r: 0.7962867969477416
06/02/2019 10:44:08 step: 5059, epoch: 153, batch: 9, loss: 0.005785644054412842, acc: 100.0, f1: 100.0, r: 0.6968270159479907
06/02/2019 10:44:11 step: 5064, epoch: 153, batch: 14, loss: 0.003275856375694275, acc: 100.0, f1: 100.0, r: 0.765107616247876
06/02/2019 10:44:13 step: 5069, epoch: 153, batch: 19, loss: 0.007425270974636078, acc: 100.0, f1: 100.0, r: 0.8067192615024812
06/02/2019 10:44:15 step: 5074, epoch: 153, batch: 24, loss: 0.00533013790845871, acc: 100.0, f1: 100.0, r: 0.7689368338472906
06/02/2019 10:44:18 step: 5079, epoch: 153, batch: 29, loss: 0.006524279713630676, acc: 100.0, f1: 100.0, r: 0.7138331304613644
06/02/2019 10:44:19 *** evaluating ***
06/02/2019 10:44:20 step: 154, epoch: 153, acc: 48.717948717948715, f1: 27.29121097410084, r: 0.25055137661728616
06/02/2019 10:44:20 *** epoch: 155 ***
06/02/2019 10:44:20 *** training ***
06/02/2019 10:44:22 step: 5087, epoch: 154, batch: 4, loss: 0.005051523447036743, acc: 100.0, f1: 100.0, r: 0.679190486129365
06/02/2019 10:44:24 step: 5092, epoch: 154, batch: 9, loss: 0.0046583786606788635, acc: 100.0, f1: 100.0, r: 0.707254048406658
06/02/2019 10:44:26 step: 5097, epoch: 154, batch: 14, loss: 0.003984816372394562, acc: 100.0, f1: 100.0, r: 0.8284021059360936
06/02/2019 10:44:28 step: 5102, epoch: 154, batch: 19, loss: 0.010003231465816498, acc: 100.0, f1: 100.0, r: 0.7126387363894802
06/02/2019 10:44:31 step: 5107, epoch: 154, batch: 24, loss: 0.003457017242908478, acc: 100.0, f1: 100.0, r: 0.8010395325455857
06/02/2019 10:44:33 step: 5112, epoch: 154, batch: 29, loss: 0.0014474689960479736, acc: 100.0, f1: 100.0, r: 0.6854934076839915
06/02/2019 10:44:34 *** evaluating ***
06/02/2019 10:44:35 step: 155, epoch: 154, acc: 48.717948717948715, f1: 27.43562276662397, r: 0.2513866412153351
06/02/2019 10:44:35 *** epoch: 156 ***
06/02/2019 10:44:35 *** training ***
06/02/2019 10:44:37 step: 5120, epoch: 155, batch: 4, loss: 0.004694715142250061, acc: 100.0, f1: 100.0, r: 0.8154104999624703
06/02/2019 10:44:40 step: 5125, epoch: 155, batch: 9, loss: 0.008915625512599945, acc: 100.0, f1: 100.0, r: 0.7165220836979592
06/02/2019 10:44:42 step: 5130, epoch: 155, batch: 14, loss: 0.0020137429237365723, acc: 100.0, f1: 100.0, r: 0.578325615307437
06/02/2019 10:44:44 step: 5135, epoch: 155, batch: 19, loss: 0.005111202597618103, acc: 100.0, f1: 100.0, r: 0.7198020155701366
06/02/2019 10:44:47 step: 5140, epoch: 155, batch: 24, loss: 0.0020525753498077393, acc: 100.0, f1: 100.0, r: 0.7114544774385004
06/02/2019 10:44:49 step: 5145, epoch: 155, batch: 29, loss: 0.00629638135433197, acc: 100.0, f1: 100.0, r: 0.6977697292092152
06/02/2019 10:44:50 *** evaluating ***
06/02/2019 10:44:50 step: 156, epoch: 155, acc: 50.0, f1: 28.34666033020786, r: 0.25029411202198215
06/02/2019 10:44:50 *** epoch: 157 ***
06/02/2019 10:44:50 *** training ***
06/02/2019 10:44:52 step: 5153, epoch: 156, batch: 4, loss: 0.005280263721942902, acc: 100.0, f1: 100.0, r: 0.7031583050184582
06/02/2019 10:44:55 step: 5158, epoch: 156, batch: 9, loss: 0.004230566322803497, acc: 100.0, f1: 100.0, r: 0.745274168952822
06/02/2019 10:44:57 step: 5163, epoch: 156, batch: 14, loss: 0.005604289472103119, acc: 100.0, f1: 100.0, r: 0.7849823258015026
06/02/2019 10:45:00 step: 5168, epoch: 156, batch: 19, loss: 0.002132415771484375, acc: 100.0, f1: 100.0, r: 0.8594135542379299
06/02/2019 10:45:02 step: 5173, epoch: 156, batch: 24, loss: 0.007493965327739716, acc: 100.0, f1: 100.0, r: 0.7002851392422226
06/02/2019 10:45:05 step: 5178, epoch: 156, batch: 29, loss: 0.004915572702884674, acc: 100.0, f1: 100.0, r: 0.7869495654728313
06/02/2019 10:45:06 *** evaluating ***
06/02/2019 10:45:06 step: 157, epoch: 156, acc: 50.0, f1: 28.282576584585605, r: 0.2579584073720434
06/02/2019 10:45:06 *** epoch: 158 ***
06/02/2019 10:45:06 *** training ***
06/02/2019 10:45:09 step: 5186, epoch: 157, batch: 4, loss: 0.005242481827735901, acc: 100.0, f1: 100.0, r: 0.7599525424226952
06/02/2019 10:45:11 step: 5191, epoch: 157, batch: 9, loss: 0.004067555069923401, acc: 100.0, f1: 100.0, r: 0.77015593033975
06/02/2019 10:45:14 step: 5196, epoch: 157, batch: 14, loss: 0.0017597377300262451, acc: 100.0, f1: 100.0, r: 0.7349070748299921
06/02/2019 10:45:16 step: 5201, epoch: 157, batch: 19, loss: 0.004015013575553894, acc: 100.0, f1: 100.0, r: 0.7437778677556282
06/02/2019 10:45:19 step: 5206, epoch: 157, batch: 24, loss: 0.003035716712474823, acc: 100.0, f1: 100.0, r: 0.6790409015743756
06/02/2019 10:45:21 step: 5211, epoch: 157, batch: 29, loss: 0.012833140790462494, acc: 100.0, f1: 100.0, r: 0.8194219837065254
06/02/2019 10:45:22 *** evaluating ***
06/02/2019 10:45:22 step: 158, epoch: 157, acc: 48.29059829059829, f1: 27.310197051937124, r: 0.24643781730068076
06/02/2019 10:45:22 *** epoch: 159 ***
06/02/2019 10:45:22 *** training ***
06/02/2019 10:45:25 step: 5219, epoch: 158, batch: 4, loss: 0.0028223469853401184, acc: 100.0, f1: 100.0, r: 0.7791271063427865
06/02/2019 10:45:27 step: 5224, epoch: 158, batch: 9, loss: 0.008798711001873016, acc: 100.0, f1: 100.0, r: 0.6975791778298657
06/02/2019 10:45:30 step: 5229, epoch: 158, batch: 14, loss: 0.0027066320180892944, acc: 100.0, f1: 100.0, r: 0.7908282526969295
06/02/2019 10:45:32 step: 5234, epoch: 158, batch: 19, loss: 0.003532804548740387, acc: 100.0, f1: 100.0, r: 0.7287541269122774
06/02/2019 10:45:34 step: 5239, epoch: 158, batch: 24, loss: 0.008012808859348297, acc: 100.0, f1: 100.0, r: 0.7986993416582626
06/02/2019 10:45:36 step: 5244, epoch: 158, batch: 29, loss: 0.004792883992195129, acc: 100.0, f1: 100.0, r: 0.7635195999158944
06/02/2019 10:45:37 *** evaluating ***
06/02/2019 10:45:38 step: 159, epoch: 158, acc: 50.0, f1: 27.86245987545433, r: 0.24562737535664028
06/02/2019 10:45:38 *** epoch: 160 ***
06/02/2019 10:45:38 *** training ***
06/02/2019 10:45:40 step: 5252, epoch: 159, batch: 4, loss: 0.006866335868835449, acc: 100.0, f1: 100.0, r: 0.8234862033314586
06/02/2019 10:45:42 step: 5257, epoch: 159, batch: 9, loss: 0.0034397095441818237, acc: 100.0, f1: 100.0, r: 0.6701416877489655
06/02/2019 10:45:45 step: 5262, epoch: 159, batch: 14, loss: 0.002191796898841858, acc: 100.0, f1: 100.0, r: 0.7978275222149784
06/02/2019 10:45:47 step: 5267, epoch: 159, batch: 19, loss: 0.007321864366531372, acc: 100.0, f1: 100.0, r: 0.7003172173123884
06/02/2019 10:45:49 step: 5272, epoch: 159, batch: 24, loss: 0.003613784909248352, acc: 100.0, f1: 100.0, r: 0.7364158833380833
06/02/2019 10:45:51 step: 5277, epoch: 159, batch: 29, loss: 0.009029179811477661, acc: 100.0, f1: 100.0, r: 0.8397323286161194
06/02/2019 10:45:53 *** evaluating ***
06/02/2019 10:45:53 step: 160, epoch: 159, acc: 48.29059829059829, f1: 27.28516521179809, r: 0.24554034604015545
06/02/2019 10:45:53 *** epoch: 161 ***
06/02/2019 10:45:53 *** training ***
06/02/2019 10:45:56 step: 5285, epoch: 160, batch: 4, loss: 0.014403238892555237, acc: 98.4375, f1: 85.39682539682539, r: 0.609785745846051
06/02/2019 10:45:58 step: 5290, epoch: 160, batch: 9, loss: 0.007042251527309418, acc: 100.0, f1: 100.0, r: 0.7184038565222574
06/02/2019 10:46:01 step: 5295, epoch: 160, batch: 14, loss: 0.0034799575805664062, acc: 100.0, f1: 100.0, r: 0.6833992697465079
06/02/2019 10:46:03 step: 5300, epoch: 160, batch: 19, loss: 0.00944172590970993, acc: 100.0, f1: 100.0, r: 0.750952963743731
06/02/2019 10:46:05 step: 5305, epoch: 160, batch: 24, loss: 0.01029513031244278, acc: 100.0, f1: 100.0, r: 0.814655549689238
06/02/2019 10:46:07 step: 5310, epoch: 160, batch: 29, loss: 0.0035997331142425537, acc: 100.0, f1: 100.0, r: 0.5828832555636361
06/02/2019 10:46:08 *** evaluating ***
06/02/2019 10:46:09 step: 161, epoch: 160, acc: 50.0, f1: 29.26426820728291, r: 0.25070542887954256
06/02/2019 10:46:09 *** epoch: 162 ***
06/02/2019 10:46:09 *** training ***
06/02/2019 10:46:12 step: 5318, epoch: 161, batch: 4, loss: 0.018923811614513397, acc: 98.4375, f1: 98.8994708994709, r: 0.6409931472654745
06/02/2019 10:46:14 step: 5323, epoch: 161, batch: 9, loss: 0.002712063491344452, acc: 100.0, f1: 100.0, r: 0.6807960171639114
06/02/2019 10:46:16 step: 5328, epoch: 161, batch: 14, loss: 0.0020262449979782104, acc: 100.0, f1: 100.0, r: 0.6718053922566927
06/02/2019 10:46:18 step: 5333, epoch: 161, batch: 19, loss: 0.006263844668865204, acc: 100.0, f1: 100.0, r: 0.7270421764967241
06/02/2019 10:46:20 step: 5338, epoch: 161, batch: 24, loss: 0.003678463399410248, acc: 100.0, f1: 100.0, r: 0.8013493097491493
06/02/2019 10:46:23 step: 5343, epoch: 161, batch: 29, loss: 0.004246324300765991, acc: 100.0, f1: 100.0, r: 0.8592624881184088
06/02/2019 10:46:24 *** evaluating ***
06/02/2019 10:46:25 step: 162, epoch: 161, acc: 49.14529914529914, f1: 28.199223286875725, r: 0.24518610810798677
06/02/2019 10:46:25 *** epoch: 163 ***
06/02/2019 10:46:25 *** training ***
06/02/2019 10:46:27 step: 5351, epoch: 162, batch: 4, loss: 0.02233187109231949, acc: 98.4375, f1: 95.76719576719577, r: 0.6458156411952564
06/02/2019 10:46:29 step: 5356, epoch: 162, batch: 9, loss: 0.004363551735877991, acc: 100.0, f1: 100.0, r: 0.70946281955759
06/02/2019 10:46:31 step: 5361, epoch: 162, batch: 14, loss: 0.005377918481826782, acc: 100.0, f1: 100.0, r: 0.6598901929708263
06/02/2019 10:46:33 step: 5366, epoch: 162, batch: 19, loss: 0.0030217915773391724, acc: 100.0, f1: 100.0, r: 0.7556144162851663
06/02/2019 10:46:35 step: 5371, epoch: 162, batch: 24, loss: 0.0076858848333358765, acc: 100.0, f1: 100.0, r: 0.8195765439228
06/02/2019 10:46:38 step: 5376, epoch: 162, batch: 29, loss: 0.00654660165309906, acc: 100.0, f1: 100.0, r: 0.8291091719193555
06/02/2019 10:46:39 *** evaluating ***
06/02/2019 10:46:40 step: 163, epoch: 162, acc: 50.427350427350426, f1: 28.742678378715453, r: 0.2359456991811096
06/02/2019 10:46:40 *** epoch: 164 ***
06/02/2019 10:46:40 *** training ***
06/02/2019 10:46:42 step: 5384, epoch: 163, batch: 4, loss: 0.005170352756977081, acc: 100.0, f1: 100.0, r: 0.7548391947324771
06/02/2019 10:46:44 step: 5389, epoch: 163, batch: 9, loss: 0.0035480260848999023, acc: 100.0, f1: 100.0, r: 0.6907641336416006
06/02/2019 10:46:47 step: 5394, epoch: 163, batch: 14, loss: 0.006983555853366852, acc: 100.0, f1: 100.0, r: 0.793870769244434
06/02/2019 10:46:49 step: 5399, epoch: 163, batch: 19, loss: 0.0026152580976486206, acc: 100.0, f1: 100.0, r: 0.718363391631586
06/02/2019 10:46:52 step: 5404, epoch: 163, batch: 24, loss: 0.004499055445194244, acc: 100.0, f1: 100.0, r: 0.735169536095897
06/02/2019 10:46:54 step: 5409, epoch: 163, batch: 29, loss: 0.0037096217274665833, acc: 100.0, f1: 100.0, r: 0.8162351818445014
06/02/2019 10:46:55 *** evaluating ***
06/02/2019 10:46:55 step: 164, epoch: 163, acc: 49.14529914529914, f1: 27.888028633575807, r: 0.23463771586179413
06/02/2019 10:46:55 *** epoch: 165 ***
06/02/2019 10:46:55 *** training ***
06/02/2019 10:46:58 step: 5417, epoch: 164, batch: 4, loss: 0.0031109079718589783, acc: 100.0, f1: 100.0, r: 0.6893711162660371
06/02/2019 10:47:00 step: 5422, epoch: 164, batch: 9, loss: 0.002683490514755249, acc: 100.0, f1: 100.0, r: 0.6787446775169178
06/02/2019 10:47:02 step: 5427, epoch: 164, batch: 14, loss: 0.031975992023944855, acc: 98.4375, f1: 97.1188475390156, r: 0.7536860419574671
06/02/2019 10:47:04 step: 5432, epoch: 164, batch: 19, loss: 0.006834931671619415, acc: 100.0, f1: 100.0, r: 0.6728825966028166
06/02/2019 10:47:07 step: 5437, epoch: 164, batch: 24, loss: 0.008114539086818695, acc: 100.0, f1: 100.0, r: 0.8557980809266716
06/02/2019 10:47:09 step: 5442, epoch: 164, batch: 29, loss: 0.0092252716422081, acc: 100.0, f1: 100.0, r: 0.714294966390993
06/02/2019 10:47:10 *** evaluating ***
06/02/2019 10:47:11 step: 165, epoch: 164, acc: 49.572649572649574, f1: 28.236510175223767, r: 0.23518756920181322
06/02/2019 10:47:11 *** epoch: 166 ***
06/02/2019 10:47:11 *** training ***
06/02/2019 10:47:14 step: 5450, epoch: 165, batch: 4, loss: 0.005043603479862213, acc: 100.0, f1: 100.0, r: 0.6931485509306573
06/02/2019 10:47:16 step: 5455, epoch: 165, batch: 9, loss: 0.003116898238658905, acc: 100.0, f1: 100.0, r: 0.8172076269001967
06/02/2019 10:47:18 step: 5460, epoch: 165, batch: 14, loss: 0.019692562520503998, acc: 98.4375, f1: 96.6137566137566, r: 0.7089939152055897
06/02/2019 10:47:20 step: 5465, epoch: 165, batch: 19, loss: 0.002747848629951477, acc: 100.0, f1: 100.0, r: 0.6607380797247506
06/02/2019 10:47:22 step: 5470, epoch: 165, batch: 24, loss: 0.12926122546195984, acc: 98.4375, f1: 99.15933528836754, r: 0.5739454759341414
06/02/2019 10:47:24 step: 5475, epoch: 165, batch: 29, loss: 0.010076045989990234, acc: 100.0, f1: 100.0, r: 0.6907200998089827
06/02/2019 10:47:26 *** evaluating ***
06/02/2019 10:47:27 step: 166, epoch: 165, acc: 51.28205128205128, f1: 28.725786822826294, r: 0.2418742432488806
06/02/2019 10:47:27 *** epoch: 167 ***
06/02/2019 10:47:27 *** training ***
06/02/2019 10:47:29 step: 5483, epoch: 166, batch: 4, loss: 0.0033723413944244385, acc: 100.0, f1: 100.0, r: 0.6929444312800406
06/02/2019 10:47:31 step: 5488, epoch: 166, batch: 9, loss: 0.005038328468799591, acc: 100.0, f1: 100.0, r: 0.7534012442507136
06/02/2019 10:47:33 step: 5493, epoch: 166, batch: 14, loss: 0.0019334256649017334, acc: 100.0, f1: 100.0, r: 0.6506823839574413
06/02/2019 10:47:36 step: 5498, epoch: 166, batch: 19, loss: 0.003925882279872894, acc: 100.0, f1: 100.0, r: 0.7455084309123036
06/02/2019 10:47:38 step: 5503, epoch: 166, batch: 24, loss: 0.002932615578174591, acc: 100.0, f1: 100.0, r: 0.7080848134126111
06/02/2019 10:47:41 step: 5508, epoch: 166, batch: 29, loss: 0.003158874809741974, acc: 100.0, f1: 100.0, r: 0.7958445443364914
06/02/2019 10:47:42 *** evaluating ***
06/02/2019 10:47:42 step: 167, epoch: 166, acc: 49.572649572649574, f1: 28.082639770997897, r: 0.24433535054539873
06/02/2019 10:47:42 *** epoch: 168 ***
06/02/2019 10:47:42 *** training ***
06/02/2019 10:47:45 step: 5516, epoch: 167, batch: 4, loss: 0.0026293322443962097, acc: 100.0, f1: 100.0, r: 0.6783788423145548
06/02/2019 10:47:47 step: 5521, epoch: 167, batch: 9, loss: 0.0015254393219947815, acc: 100.0, f1: 100.0, r: 0.7646086541316136
06/02/2019 10:47:50 step: 5526, epoch: 167, batch: 14, loss: 0.0026696547865867615, acc: 100.0, f1: 100.0, r: 0.8229242117739035
06/02/2019 10:47:52 step: 5531, epoch: 167, batch: 19, loss: 0.004079848527908325, acc: 100.0, f1: 100.0, r: 0.8340301428792687
06/02/2019 10:47:54 step: 5536, epoch: 167, batch: 24, loss: 0.0035597383975982666, acc: 100.0, f1: 100.0, r: 0.7837475694205226
06/02/2019 10:47:56 step: 5541, epoch: 167, batch: 29, loss: 0.00977400690317154, acc: 100.0, f1: 100.0, r: 0.7807553064918145
06/02/2019 10:47:58 *** evaluating ***
06/02/2019 10:47:58 step: 168, epoch: 167, acc: 49.14529914529914, f1: 27.28819980850149, r: 0.2421884984897822
06/02/2019 10:47:58 *** epoch: 169 ***
06/02/2019 10:47:58 *** training ***
06/02/2019 10:48:01 step: 5549, epoch: 168, batch: 4, loss: 0.010690383613109589, acc: 100.0, f1: 100.0, r: 0.8197771012143132
06/02/2019 10:48:03 step: 5554, epoch: 168, batch: 9, loss: 0.002632439136505127, acc: 100.0, f1: 100.0, r: 0.6988020503861558
06/02/2019 10:48:06 step: 5559, epoch: 168, batch: 14, loss: 0.0017764940857887268, acc: 100.0, f1: 100.0, r: 0.8134535275638499
06/02/2019 10:48:08 step: 5564, epoch: 168, batch: 19, loss: 0.002000093460083008, acc: 100.0, f1: 100.0, r: 0.76320719116386
06/02/2019 10:48:10 step: 5569, epoch: 168, batch: 24, loss: 0.005761474370956421, acc: 100.0, f1: 100.0, r: 0.8459714868301738
06/02/2019 10:48:13 step: 5574, epoch: 168, batch: 29, loss: 0.0029703378677368164, acc: 100.0, f1: 100.0, r: 0.8062868670551916
06/02/2019 10:48:14 *** evaluating ***
06/02/2019 10:48:15 step: 169, epoch: 168, acc: 48.717948717948715, f1: 27.72179574811154, r: 0.24115062801651557
06/02/2019 10:48:15 *** epoch: 170 ***
06/02/2019 10:48:15 *** training ***
06/02/2019 10:48:17 step: 5582, epoch: 169, batch: 4, loss: 0.0037572234869003296, acc: 100.0, f1: 100.0, r: 0.8044035122627837
06/02/2019 10:48:20 step: 5587, epoch: 169, batch: 9, loss: 0.006595544517040253, acc: 100.0, f1: 100.0, r: 0.7773849532162345
06/02/2019 10:48:22 step: 5592, epoch: 169, batch: 14, loss: 0.002575606107711792, acc: 100.0, f1: 100.0, r: 0.6954060296361936
06/02/2019 10:48:24 step: 5597, epoch: 169, batch: 19, loss: 0.003428719937801361, acc: 100.0, f1: 100.0, r: 0.7854583577687
06/02/2019 10:48:26 step: 5602, epoch: 169, batch: 24, loss: 0.005977705121040344, acc: 100.0, f1: 100.0, r: 0.7774048999015961
06/02/2019 10:48:29 step: 5607, epoch: 169, batch: 29, loss: 0.0016444027423858643, acc: 100.0, f1: 100.0, r: 0.6912893052419874
06/02/2019 10:48:29 *** evaluating ***
06/02/2019 10:48:30 step: 170, epoch: 169, acc: 48.717948717948715, f1: 27.047353231563754, r: 0.238021408923152
06/02/2019 10:48:30 *** epoch: 171 ***
06/02/2019 10:48:30 *** training ***
06/02/2019 10:48:32 step: 5615, epoch: 170, batch: 4, loss: 0.0018056780099868774, acc: 100.0, f1: 100.0, r: 0.771974127683729
06/02/2019 10:48:35 step: 5620, epoch: 170, batch: 9, loss: 0.008456707000732422, acc: 100.0, f1: 100.0, r: 0.704060616357147
06/02/2019 10:48:37 step: 5625, epoch: 170, batch: 14, loss: 0.009346649050712585, acc: 100.0, f1: 100.0, r: 0.7369132368107196
06/02/2019 10:48:39 step: 5630, epoch: 170, batch: 19, loss: 0.00420515239238739, acc: 100.0, f1: 100.0, r: 0.6835831534928775
06/02/2019 10:48:41 step: 5635, epoch: 170, batch: 24, loss: 0.003160417079925537, acc: 100.0, f1: 100.0, r: 0.5820839179361397
06/02/2019 10:48:44 step: 5640, epoch: 170, batch: 29, loss: 0.02623392641544342, acc: 98.4375, f1: 98.98009950248756, r: 0.7313993518264384
06/02/2019 10:48:45 *** evaluating ***
06/02/2019 10:48:46 step: 171, epoch: 170, acc: 49.572649572649574, f1: 27.941631939085248, r: 0.24459413179196912
06/02/2019 10:48:46 *** epoch: 172 ***
06/02/2019 10:48:46 *** training ***
06/02/2019 10:48:48 step: 5648, epoch: 171, batch: 4, loss: 0.006160803139209747, acc: 100.0, f1: 100.0, r: 0.7540269651506358
06/02/2019 10:48:50 step: 5653, epoch: 171, batch: 9, loss: 0.0023183152079582214, acc: 100.0, f1: 100.0, r: 0.6091657901656973
06/02/2019 10:48:52 step: 5658, epoch: 171, batch: 14, loss: 0.003342077136039734, acc: 100.0, f1: 100.0, r: 0.7942732142173737
06/02/2019 10:48:54 step: 5663, epoch: 171, batch: 19, loss: 0.011506684124469757, acc: 100.0, f1: 100.0, r: 0.6965250613651206
06/02/2019 10:48:57 step: 5668, epoch: 171, batch: 24, loss: 0.0031175538897514343, acc: 100.0, f1: 100.0, r: 0.7996637142132176
06/02/2019 10:48:59 step: 5673, epoch: 171, batch: 29, loss: 0.003656744956970215, acc: 100.0, f1: 100.0, r: 0.771724386716628
06/02/2019 10:49:01 *** evaluating ***
06/02/2019 10:49:01 step: 172, epoch: 171, acc: 50.0, f1: 28.61281390798318, r: 0.24286507194554927
06/02/2019 10:49:01 *** epoch: 173 ***
06/02/2019 10:49:01 *** training ***
06/02/2019 10:49:03 step: 5681, epoch: 172, batch: 4, loss: 0.01616780459880829, acc: 100.0, f1: 100.0, r: 0.8260580948577319
06/02/2019 10:49:05 step: 5686, epoch: 172, batch: 9, loss: 0.00417722761631012, acc: 100.0, f1: 100.0, r: 0.7552658742574578
06/02/2019 10:49:08 step: 5691, epoch: 172, batch: 14, loss: 0.0027922242879867554, acc: 100.0, f1: 100.0, r: 0.684108696136153
06/02/2019 10:49:11 step: 5696, epoch: 172, batch: 19, loss: 0.0035997331142425537, acc: 100.0, f1: 100.0, r: 0.7283136207215906
06/02/2019 10:49:13 step: 5701, epoch: 172, batch: 24, loss: 0.008494682610034943, acc: 100.0, f1: 100.0, r: 0.7140749662079398
06/02/2019 10:49:15 step: 5706, epoch: 172, batch: 29, loss: 0.0022566840052604675, acc: 100.0, f1: 100.0, r: 0.7790666288189845
06/02/2019 10:49:16 *** evaluating ***
06/02/2019 10:49:17 step: 173, epoch: 172, acc: 48.717948717948715, f1: 27.129235213474345, r: 0.2367545764779751
06/02/2019 10:49:17 *** epoch: 174 ***
06/02/2019 10:49:17 *** training ***
06/02/2019 10:49:19 step: 5714, epoch: 173, batch: 4, loss: 0.006792731583118439, acc: 100.0, f1: 100.0, r: 0.8280311099899975
06/02/2019 10:49:21 step: 5719, epoch: 173, batch: 9, loss: 0.0031603723764419556, acc: 100.0, f1: 100.0, r: 0.642111840152851
06/02/2019 10:49:23 step: 5724, epoch: 173, batch: 14, loss: 0.010938286781311035, acc: 100.0, f1: 100.0, r: 0.8528199562488148
06/02/2019 10:49:26 step: 5729, epoch: 173, batch: 19, loss: 0.0024846866726875305, acc: 100.0, f1: 100.0, r: 0.7225082559311696
06/02/2019 10:49:28 step: 5734, epoch: 173, batch: 24, loss: 0.0012423843145370483, acc: 100.0, f1: 100.0, r: 0.678827320419791
06/02/2019 10:49:30 step: 5739, epoch: 173, batch: 29, loss: 0.0035000592470169067, acc: 100.0, f1: 100.0, r: 0.7296424179925539
06/02/2019 10:49:31 *** evaluating ***
06/02/2019 10:49:32 step: 174, epoch: 173, acc: 49.572649572649574, f1: 27.98241177236204, r: 0.23172106475300783
06/02/2019 10:49:32 *** epoch: 175 ***
06/02/2019 10:49:32 *** training ***
06/02/2019 10:49:34 step: 5747, epoch: 174, batch: 4, loss: 0.0024042055010795593, acc: 100.0, f1: 100.0, r: 0.7714726974300858
06/02/2019 10:49:37 step: 5752, epoch: 174, batch: 9, loss: 0.0070621296763420105, acc: 100.0, f1: 100.0, r: 0.6354263251551412
06/02/2019 10:49:39 step: 5757, epoch: 174, batch: 14, loss: 0.002461172640323639, acc: 100.0, f1: 100.0, r: 0.6435850137172014
06/02/2019 10:49:41 step: 5762, epoch: 174, batch: 19, loss: 0.009621106088161469, acc: 100.0, f1: 100.0, r: 0.7970736591928984
06/02/2019 10:49:44 step: 5767, epoch: 174, batch: 24, loss: 0.0037193968892097473, acc: 100.0, f1: 100.0, r: 0.7954178510152112
06/02/2019 10:49:46 step: 5772, epoch: 174, batch: 29, loss: 0.016204290091991425, acc: 98.4375, f1: 98.56784819190835, r: 0.711609686184863
06/02/2019 10:49:47 *** evaluating ***
06/02/2019 10:49:47 step: 175, epoch: 174, acc: 50.0, f1: 28.37531980331997, r: 0.23102464901997088
06/02/2019 10:49:47 *** epoch: 176 ***
06/02/2019 10:49:47 *** training ***
06/02/2019 10:49:49 step: 5780, epoch: 175, batch: 4, loss: 0.006645329296588898, acc: 100.0, f1: 100.0, r: 0.7718817086572513
06/02/2019 10:49:52 step: 5785, epoch: 175, batch: 9, loss: 0.003722161054611206, acc: 100.0, f1: 100.0, r: 0.8027204308734897
06/02/2019 10:49:54 step: 5790, epoch: 175, batch: 14, loss: 0.005268387496471405, acc: 100.0, f1: 100.0, r: 0.7491213210896774
06/02/2019 10:49:56 step: 5795, epoch: 175, batch: 19, loss: 0.0031637772917747498, acc: 100.0, f1: 100.0, r: 0.6712269220525717
06/02/2019 10:49:58 step: 5800, epoch: 175, batch: 24, loss: 0.008340567350387573, acc: 100.0, f1: 100.0, r: 0.7559500528507842
06/02/2019 10:50:01 step: 5805, epoch: 175, batch: 29, loss: 0.001314893364906311, acc: 100.0, f1: 100.0, r: 0.7022690407626687
06/02/2019 10:50:02 *** evaluating ***
06/02/2019 10:50:03 step: 176, epoch: 175, acc: 50.0, f1: 28.054116009213093, r: 0.2307629122430663
06/02/2019 10:50:03 *** epoch: 177 ***
06/02/2019 10:50:03 *** training ***
06/02/2019 10:50:05 step: 5813, epoch: 176, batch: 4, loss: 0.0021260827779769897, acc: 100.0, f1: 100.0, r: 0.8425630106239481
06/02/2019 10:50:07 step: 5818, epoch: 176, batch: 9, loss: 0.002060048282146454, acc: 100.0, f1: 100.0, r: 0.7223388115540174
06/02/2019 10:50:10 step: 5823, epoch: 176, batch: 14, loss: 0.0043448880314826965, acc: 100.0, f1: 100.0, r: 0.7703360660301602
06/02/2019 10:50:12 step: 5828, epoch: 176, batch: 19, loss: 0.009043924510478973, acc: 100.0, f1: 100.0, r: 0.8037943259886956
06/02/2019 10:50:14 step: 5833, epoch: 176, batch: 24, loss: 0.01137622445821762, acc: 100.0, f1: 100.0, r: 0.7146835456841668
06/02/2019 10:50:16 step: 5838, epoch: 176, batch: 29, loss: 0.0024074241518974304, acc: 100.0, f1: 100.0, r: 0.7141736124858407
06/02/2019 10:50:17 *** evaluating ***
06/02/2019 10:50:18 step: 177, epoch: 176, acc: 50.0, f1: 28.054116009213093, r: 0.23014402220160282
06/02/2019 10:50:18 *** epoch: 178 ***
06/02/2019 10:50:18 *** training ***
06/02/2019 10:50:20 step: 5846, epoch: 177, batch: 4, loss: 0.0012824684381484985, acc: 100.0, f1: 100.0, r: 0.7349991495925949
06/02/2019 10:50:22 step: 5851, epoch: 177, batch: 9, loss: 0.006423637270927429, acc: 100.0, f1: 100.0, r: 0.7003352964318202
06/02/2019 10:50:24 step: 5856, epoch: 177, batch: 14, loss: 0.0037241876125335693, acc: 100.0, f1: 100.0, r: 0.7850714991368966
06/02/2019 10:50:26 step: 5861, epoch: 177, batch: 19, loss: 0.005075380206108093, acc: 100.0, f1: 100.0, r: 0.7734771774401593
06/02/2019 10:50:28 step: 5866, epoch: 177, batch: 24, loss: 0.009767867624759674, acc: 100.0, f1: 100.0, r: 0.8116926170616533
06/02/2019 10:50:31 step: 5871, epoch: 177, batch: 29, loss: 0.0036262720823287964, acc: 100.0, f1: 100.0, r: 0.8005366652414404
06/02/2019 10:50:33 *** evaluating ***
06/02/2019 10:50:33 step: 178, epoch: 177, acc: 50.427350427350426, f1: 28.656232184508795, r: 0.23450709093801808
06/02/2019 10:50:33 *** epoch: 179 ***
06/02/2019 10:50:33 *** training ***
06/02/2019 10:50:35 step: 5879, epoch: 178, batch: 4, loss: 0.012355700135231018, acc: 100.0, f1: 100.0, r: 0.8233632616431988
06/02/2019 10:50:38 step: 5884, epoch: 178, batch: 9, loss: 0.01387043297290802, acc: 100.0, f1: 100.0, r: 0.6853604712678998
06/02/2019 10:50:40 step: 5889, epoch: 178, batch: 14, loss: 0.0015460699796676636, acc: 100.0, f1: 100.0, r: 0.7345426712230588
06/02/2019 10:50:43 step: 5894, epoch: 178, batch: 19, loss: 0.0021955594420433044, acc: 100.0, f1: 100.0, r: 0.805695216463172
06/02/2019 10:50:45 step: 5899, epoch: 178, batch: 24, loss: 0.0038004666566848755, acc: 100.0, f1: 100.0, r: 0.8067500531887409
06/02/2019 10:50:47 step: 5904, epoch: 178, batch: 29, loss: 0.001247115433216095, acc: 100.0, f1: 100.0, r: 0.8099926617640049
06/02/2019 10:50:48 *** evaluating ***
06/02/2019 10:50:49 step: 179, epoch: 178, acc: 50.0, f1: 28.93849631088756, r: 0.23491713256472746
06/02/2019 10:50:49 *** epoch: 180 ***
06/02/2019 10:50:49 *** training ***
06/02/2019 10:50:51 step: 5912, epoch: 179, batch: 4, loss: 0.004122935235500336, acc: 100.0, f1: 100.0, r: 0.8534438067891208
06/02/2019 10:50:54 step: 5917, epoch: 179, batch: 9, loss: 0.001867145299911499, acc: 100.0, f1: 100.0, r: 0.7325230663080315
06/02/2019 10:50:56 step: 5922, epoch: 179, batch: 14, loss: 0.00852416455745697, acc: 100.0, f1: 100.0, r: 0.753537415307598
06/02/2019 10:50:58 step: 5927, epoch: 179, batch: 19, loss: 0.0028761476278305054, acc: 100.0, f1: 100.0, r: 0.8176678641633583
06/02/2019 10:51:01 step: 5932, epoch: 179, batch: 24, loss: 0.0016436129808425903, acc: 100.0, f1: 100.0, r: 0.6761012367843444
06/02/2019 10:51:03 step: 5937, epoch: 179, batch: 29, loss: 0.0018236711621284485, acc: 100.0, f1: 100.0, r: 0.8304655096114273
06/02/2019 10:51:04 *** evaluating ***
06/02/2019 10:51:05 step: 180, epoch: 179, acc: 50.0, f1: 28.798107430674534, r: 0.2389893597498726
06/02/2019 10:51:05 *** epoch: 181 ***
06/02/2019 10:51:05 *** training ***
06/02/2019 10:51:07 step: 5945, epoch: 180, batch: 4, loss: 0.005733683705329895, acc: 100.0, f1: 100.0, r: 0.7475118281983423
06/02/2019 10:51:10 step: 5950, epoch: 180, batch: 9, loss: 0.0056476593017578125, acc: 100.0, f1: 100.0, r: 0.7068563406512235
06/02/2019 10:51:12 step: 5955, epoch: 180, batch: 14, loss: 0.006181307137012482, acc: 100.0, f1: 100.0, r: 0.7925957106799406
06/02/2019 10:51:15 step: 5960, epoch: 180, batch: 19, loss: 0.002698563039302826, acc: 100.0, f1: 100.0, r: 0.7629325874441598
06/02/2019 10:51:17 step: 5965, epoch: 180, batch: 24, loss: 0.003136768937110901, acc: 100.0, f1: 100.0, r: 0.7062489557890455
06/02/2019 10:51:19 step: 5970, epoch: 180, batch: 29, loss: 0.0024214833974838257, acc: 100.0, f1: 100.0, r: 0.7140993215701962
06/02/2019 10:51:20 *** evaluating ***
06/02/2019 10:51:21 step: 181, epoch: 180, acc: 50.0, f1: 28.645917905001077, r: 0.23671406897705835
06/02/2019 10:51:21 *** epoch: 182 ***
06/02/2019 10:51:21 *** training ***
06/02/2019 10:51:23 step: 5978, epoch: 181, batch: 4, loss: 0.004063338041305542, acc: 100.0, f1: 100.0, r: 0.73119696397915
06/02/2019 10:51:26 step: 5983, epoch: 181, batch: 9, loss: 0.0017848014831542969, acc: 100.0, f1: 100.0, r: 0.8053401140933293
06/02/2019 10:51:28 step: 5988, epoch: 181, batch: 14, loss: 0.00388132780790329, acc: 100.0, f1: 100.0, r: 0.7633051635286027
06/02/2019 10:51:31 step: 5993, epoch: 181, batch: 19, loss: 0.003290913999080658, acc: 100.0, f1: 100.0, r: 0.6690145127793298
06/02/2019 10:51:33 step: 5998, epoch: 181, batch: 24, loss: 0.0032775476574897766, acc: 100.0, f1: 100.0, r: 0.696881712284147
06/02/2019 10:51:36 step: 6003, epoch: 181, batch: 29, loss: 0.009792327880859375, acc: 100.0, f1: 100.0, r: 0.7170610190975055
06/02/2019 10:51:37 *** evaluating ***
06/02/2019 10:51:38 step: 182, epoch: 181, acc: 48.29059829059829, f1: 27.692933012238818, r: 0.2385299944546648
06/02/2019 10:51:38 *** epoch: 183 ***
06/02/2019 10:51:38 *** training ***
06/02/2019 10:51:40 step: 6011, epoch: 182, batch: 4, loss: 0.0029728487133979797, acc: 100.0, f1: 100.0, r: 0.7023621765589497
06/02/2019 10:51:43 step: 6016, epoch: 182, batch: 9, loss: 0.005417138338088989, acc: 100.0, f1: 100.0, r: 0.8333447550089863
06/02/2019 10:51:45 step: 6021, epoch: 182, batch: 14, loss: 0.001433156430721283, acc: 100.0, f1: 100.0, r: 0.6491695253354655
06/02/2019 10:51:47 step: 6026, epoch: 182, batch: 19, loss: 0.004667192697525024, acc: 100.0, f1: 100.0, r: 0.7584325239788048
06/02/2019 10:51:50 step: 6031, epoch: 182, batch: 24, loss: 0.00977303832769394, acc: 100.0, f1: 100.0, r: 0.7047956140616177
06/02/2019 10:51:52 step: 6036, epoch: 182, batch: 29, loss: 0.003697790205478668, acc: 100.0, f1: 100.0, r: 0.8013017563090635
06/02/2019 10:51:53 *** evaluating ***
06/02/2019 10:51:54 step: 183, epoch: 182, acc: 48.717948717948715, f1: 27.569355848665428, r: 0.24182234272554795
06/02/2019 10:51:54 *** epoch: 184 ***
06/02/2019 10:51:54 *** training ***
06/02/2019 10:51:56 step: 6044, epoch: 183, batch: 4, loss: 0.005579829216003418, acc: 100.0, f1: 100.0, r: 0.744462051807687
06/02/2019 10:51:58 step: 6049, epoch: 183, batch: 9, loss: 0.006368212401866913, acc: 100.0, f1: 100.0, r: 0.722212749246412
06/02/2019 10:52:00 step: 6054, epoch: 183, batch: 14, loss: 0.002232566475868225, acc: 100.0, f1: 100.0, r: 0.7115933532440849
06/02/2019 10:52:03 step: 6059, epoch: 183, batch: 19, loss: 0.0019389912486076355, acc: 100.0, f1: 100.0, r: 0.8229489721125308
06/02/2019 10:52:05 step: 6064, epoch: 183, batch: 24, loss: 0.003692641854286194, acc: 100.0, f1: 100.0, r: 0.7104179714977227
06/02/2019 10:52:08 step: 6069, epoch: 183, batch: 29, loss: 0.0024376362562179565, acc: 100.0, f1: 100.0, r: 0.6646618667501658
06/02/2019 10:52:09 *** evaluating ***
06/02/2019 10:52:10 step: 184, epoch: 183, acc: 49.572649572649574, f1: 28.031972763255197, r: 0.23666020230172882
06/02/2019 10:52:10 *** epoch: 185 ***
06/02/2019 10:52:10 *** training ***
06/02/2019 10:52:12 step: 6077, epoch: 184, batch: 4, loss: 0.0015087798237800598, acc: 100.0, f1: 100.0, r: 0.7150337946263123
06/02/2019 10:52:14 step: 6082, epoch: 184, batch: 9, loss: 0.0060704052448272705, acc: 100.0, f1: 100.0, r: 0.7518261073458449
06/02/2019 10:52:17 step: 6087, epoch: 184, batch: 14, loss: 0.005449816584587097, acc: 100.0, f1: 100.0, r: 0.7637641179464099
06/02/2019 10:52:19 step: 6092, epoch: 184, batch: 19, loss: 0.002385549247264862, acc: 100.0, f1: 100.0, r: 0.7344691795970889
06/02/2019 10:52:21 step: 6097, epoch: 184, batch: 24, loss: 0.002406701445579529, acc: 100.0, f1: 100.0, r: 0.7619473707499118
06/02/2019 10:52:24 step: 6102, epoch: 184, batch: 29, loss: 0.002293303608894348, acc: 100.0, f1: 100.0, r: 0.7179958878466052
06/02/2019 10:52:25 *** evaluating ***
06/02/2019 10:52:26 step: 185, epoch: 184, acc: 50.85470085470085, f1: 28.961020863194776, r: 0.23896297291189758
06/02/2019 10:52:26 *** epoch: 186 ***
06/02/2019 10:52:26 *** training ***
06/02/2019 10:52:29 step: 6110, epoch: 185, batch: 4, loss: 0.005773723125457764, acc: 100.0, f1: 100.0, r: 0.811645766533814
06/02/2019 10:52:31 step: 6115, epoch: 185, batch: 9, loss: 0.008337914943695068, acc: 100.0, f1: 100.0, r: 0.7433517255371738
06/02/2019 10:52:33 step: 6120, epoch: 185, batch: 14, loss: 0.0038042813539505005, acc: 100.0, f1: 100.0, r: 0.7112173474944615
06/02/2019 10:52:36 step: 6125, epoch: 185, batch: 19, loss: 0.0027672946453094482, acc: 100.0, f1: 100.0, r: 0.630946419235874
06/02/2019 10:52:38 step: 6130, epoch: 185, batch: 24, loss: 0.006059460341930389, acc: 100.0, f1: 100.0, r: 0.8039411701616236
06/02/2019 10:52:40 step: 6135, epoch: 185, batch: 29, loss: 0.004851415753364563, acc: 100.0, f1: 100.0, r: 0.6647764942879932
06/02/2019 10:52:41 *** evaluating ***
06/02/2019 10:52:42 step: 186, epoch: 185, acc: 50.427350427350426, f1: 28.851402604140397, r: 0.24023017956167902
06/02/2019 10:52:42 *** epoch: 187 ***
06/02/2019 10:52:42 *** training ***
06/02/2019 10:52:44 step: 6143, epoch: 186, batch: 4, loss: 0.002664521336555481, acc: 100.0, f1: 100.0, r: 0.6686891908398176
06/02/2019 10:52:46 step: 6148, epoch: 186, batch: 9, loss: 0.003063499927520752, acc: 100.0, f1: 100.0, r: 0.8546492149270554
06/02/2019 10:52:49 step: 6153, epoch: 186, batch: 14, loss: 0.003846868872642517, acc: 100.0, f1: 100.0, r: 0.7670733072706314
06/02/2019 10:52:51 step: 6158, epoch: 186, batch: 19, loss: 0.002284795045852661, acc: 100.0, f1: 100.0, r: 0.8190616965788444
06/02/2019 10:52:53 step: 6163, epoch: 186, batch: 24, loss: 0.0036056190729141235, acc: 100.0, f1: 100.0, r: 0.7913511075990912
06/02/2019 10:52:56 step: 6168, epoch: 186, batch: 29, loss: 0.0021117106080055237, acc: 100.0, f1: 100.0, r: 0.6633391772187651
06/02/2019 10:52:57 *** evaluating ***
06/02/2019 10:52:58 step: 187, epoch: 186, acc: 50.427350427350426, f1: 28.410634665472124, r: 0.23856603655491934
06/02/2019 10:52:58 *** epoch: 188 ***
06/02/2019 10:52:58 *** training ***
06/02/2019 10:53:00 step: 6176, epoch: 187, batch: 4, loss: 0.0023713111877441406, acc: 100.0, f1: 100.0, r: 0.5747576756870894
06/02/2019 10:53:03 step: 6181, epoch: 187, batch: 9, loss: 0.003963135182857513, acc: 100.0, f1: 100.0, r: 0.7052436900551522
06/02/2019 10:53:06 step: 6186, epoch: 187, batch: 14, loss: 0.002606704831123352, acc: 100.0, f1: 100.0, r: 0.6944213486554182
06/02/2019 10:53:08 step: 6191, epoch: 187, batch: 19, loss: 0.0025688931345939636, acc: 100.0, f1: 100.0, r: 0.7216423585533632
06/02/2019 10:53:10 step: 6196, epoch: 187, batch: 24, loss: 0.003789156675338745, acc: 100.0, f1: 100.0, r: 0.7213040231750032
06/02/2019 10:53:13 step: 6201, epoch: 187, batch: 29, loss: 0.00318339467048645, acc: 100.0, f1: 100.0, r: 0.8409889661737575
06/02/2019 10:53:14 *** evaluating ***
06/02/2019 10:53:14 step: 188, epoch: 187, acc: 50.85470085470085, f1: 29.015813477887782, r: 0.24018726920003214
06/02/2019 10:53:14 *** epoch: 189 ***
06/02/2019 10:53:14 *** training ***
06/02/2019 10:53:17 step: 6209, epoch: 188, batch: 4, loss: 0.004792392253875732, acc: 100.0, f1: 100.0, r: 0.783065832070458
06/02/2019 10:53:19 step: 6214, epoch: 188, batch: 9, loss: 0.004729464650154114, acc: 100.0, f1: 100.0, r: 0.7004920868348931
06/02/2019 10:53:22 step: 6219, epoch: 188, batch: 14, loss: 0.005559638142585754, acc: 100.0, f1: 100.0, r: 0.8188795917838653
06/02/2019 10:53:25 step: 6224, epoch: 188, batch: 19, loss: 0.0023521631956100464, acc: 100.0, f1: 100.0, r: 0.6601808231394887
06/02/2019 10:53:27 step: 6229, epoch: 188, batch: 24, loss: 0.0037432461977005005, acc: 100.0, f1: 100.0, r: 0.7732688452897196
06/02/2019 10:53:29 step: 6234, epoch: 188, batch: 29, loss: 0.0032842159271240234, acc: 100.0, f1: 100.0, r: 0.7860771964171529
06/02/2019 10:53:30 *** evaluating ***
06/02/2019 10:53:30 step: 189, epoch: 188, acc: 50.427350427350426, f1: 28.324486025996954, r: 0.24071612920800672
06/02/2019 10:53:30 *** epoch: 190 ***
06/02/2019 10:53:30 *** training ***
06/02/2019 10:53:33 step: 6242, epoch: 189, batch: 4, loss: 0.003955841064453125, acc: 100.0, f1: 100.0, r: 0.7487865969852159
06/02/2019 10:53:35 step: 6247, epoch: 189, batch: 9, loss: 0.0030884668231010437, acc: 100.0, f1: 100.0, r: 0.6960029360895502
06/02/2019 10:53:38 step: 6252, epoch: 189, batch: 14, loss: 0.0016684085130691528, acc: 100.0, f1: 100.0, r: 0.751336435524922
06/02/2019 10:53:40 step: 6257, epoch: 189, batch: 19, loss: 0.002496100962162018, acc: 100.0, f1: 100.0, r: 0.7582211970619154
06/02/2019 10:53:42 step: 6262, epoch: 189, batch: 24, loss: 0.002105027437210083, acc: 100.0, f1: 100.0, r: 0.629385585080911
06/02/2019 10:53:45 step: 6267, epoch: 189, batch: 29, loss: 0.004188947379589081, acc: 100.0, f1: 100.0, r: 0.7519114505655485
06/02/2019 10:53:46 *** evaluating ***
06/02/2019 10:53:47 step: 190, epoch: 189, acc: 50.0, f1: 27.284346170900793, r: 0.2403389735332218
06/02/2019 10:53:47 *** epoch: 191 ***
06/02/2019 10:53:47 *** training ***
06/02/2019 10:53:49 step: 6275, epoch: 190, batch: 4, loss: 0.004749014973640442, acc: 100.0, f1: 100.0, r: 0.8246400250632058
06/02/2019 10:53:51 step: 6280, epoch: 190, batch: 9, loss: 0.0017476752400398254, acc: 100.0, f1: 100.0, r: 0.6611737734053723
06/02/2019 10:53:53 step: 6285, epoch: 190, batch: 14, loss: 0.004129044711589813, acc: 100.0, f1: 100.0, r: 0.7691421814360848
06/02/2019 10:53:56 step: 6290, epoch: 190, batch: 19, loss: 0.002097010612487793, acc: 100.0, f1: 100.0, r: 0.8386018256194783
06/02/2019 10:53:58 step: 6295, epoch: 190, batch: 24, loss: 0.002069249749183655, acc: 100.0, f1: 100.0, r: 0.7522682640332644
06/02/2019 10:54:00 step: 6300, epoch: 190, batch: 29, loss: 0.00908457487821579, acc: 100.0, f1: 100.0, r: 0.7641547558952799
06/02/2019 10:54:02 *** evaluating ***
06/02/2019 10:54:02 step: 191, epoch: 190, acc: 50.85470085470085, f1: 28.355200775962558, r: 0.2421892705575451
06/02/2019 10:54:02 *** epoch: 192 ***
06/02/2019 10:54:02 *** training ***
06/02/2019 10:54:04 step: 6308, epoch: 191, batch: 4, loss: 0.012487143278121948, acc: 100.0, f1: 100.0, r: 0.7113441628355638
06/02/2019 10:54:07 step: 6313, epoch: 191, batch: 9, loss: 0.004326097667217255, acc: 100.0, f1: 100.0, r: 0.73456911180166
06/02/2019 10:54:09 step: 6318, epoch: 191, batch: 14, loss: 0.0021088644862174988, acc: 100.0, f1: 100.0, r: 0.7357816926636972
06/02/2019 10:54:12 step: 6323, epoch: 191, batch: 19, loss: 0.004037000238895416, acc: 100.0, f1: 100.0, r: 0.7716189481319364
06/02/2019 10:54:14 step: 6328, epoch: 191, batch: 24, loss: 0.00862535834312439, acc: 100.0, f1: 100.0, r: 0.8594502189796296
06/02/2019 10:54:16 step: 6333, epoch: 191, batch: 29, loss: 0.002535410225391388, acc: 100.0, f1: 100.0, r: 0.7888351852170393
06/02/2019 10:54:18 *** evaluating ***
06/02/2019 10:54:18 step: 192, epoch: 191, acc: 51.28205128205128, f1: 28.89690170940171, r: 0.24579287976193784
06/02/2019 10:54:18 *** epoch: 193 ***
06/02/2019 10:54:18 *** training ***
06/02/2019 10:54:21 step: 6341, epoch: 192, batch: 4, loss: 0.002128593623638153, acc: 100.0, f1: 100.0, r: 0.6408340951504722
06/02/2019 10:54:24 step: 6346, epoch: 192, batch: 9, loss: 0.002889975905418396, acc: 100.0, f1: 100.0, r: 0.8223847312398318
06/02/2019 10:54:26 step: 6351, epoch: 192, batch: 14, loss: 0.002317674458026886, acc: 100.0, f1: 100.0, r: 0.7891078478173387
06/02/2019 10:54:28 step: 6356, epoch: 192, batch: 19, loss: 0.0011638477444648743, acc: 100.0, f1: 100.0, r: 0.747821207270915
06/02/2019 10:54:31 step: 6361, epoch: 192, batch: 24, loss: 0.0036132261157035828, acc: 100.0, f1: 100.0, r: 0.7360976981192601
06/02/2019 10:54:32 step: 6366, epoch: 192, batch: 29, loss: 0.0017172619700431824, acc: 100.0, f1: 100.0, r: 0.7403820100991558
06/02/2019 10:54:34 *** evaluating ***
06/02/2019 10:54:34 step: 193, epoch: 192, acc: 49.572649572649574, f1: 28.026588805585966, r: 0.2474060173421183
06/02/2019 10:54:34 *** epoch: 194 ***
06/02/2019 10:54:34 *** training ***
06/02/2019 10:54:36 step: 6374, epoch: 193, batch: 4, loss: 0.016944319009780884, acc: 98.4375, f1: 96.73469387755101, r: 0.7803274048193963
06/02/2019 10:54:38 step: 6379, epoch: 193, batch: 9, loss: 0.005438320338726044, acc: 100.0, f1: 100.0, r: 0.6870554178848424
06/02/2019 10:54:41 step: 6384, epoch: 193, batch: 14, loss: 0.008359581232070923, acc: 100.0, f1: 100.0, r: 0.738970208965035
06/02/2019 10:54:43 step: 6389, epoch: 193, batch: 19, loss: 0.0017709285020828247, acc: 100.0, f1: 100.0, r: 0.761057042129676
06/02/2019 10:54:46 step: 6394, epoch: 193, batch: 24, loss: 0.0015060901641845703, acc: 100.0, f1: 100.0, r: 0.8357745029360043
06/02/2019 10:54:48 step: 6399, epoch: 193, batch: 29, loss: 0.001811303198337555, acc: 100.0, f1: 100.0, r: 0.8244677628410201
06/02/2019 10:54:50 *** evaluating ***
06/02/2019 10:54:50 step: 194, epoch: 193, acc: 50.85470085470085, f1: 34.07687663547171, r: 0.2476514768610024
06/02/2019 10:54:50 *** epoch: 195 ***
06/02/2019 10:54:50 *** training ***
06/02/2019 10:54:53 step: 6407, epoch: 194, batch: 4, loss: 0.0014957860112190247, acc: 100.0, f1: 100.0, r: 0.8173414326137064
06/02/2019 10:54:55 step: 6412, epoch: 194, batch: 9, loss: 0.003950759768486023, acc: 100.0, f1: 100.0, r: 0.7557225078239896
06/02/2019 10:54:58 step: 6417, epoch: 194, batch: 14, loss: 0.013079516589641571, acc: 100.0, f1: 100.0, r: 0.7594614550840818
06/02/2019 10:55:00 step: 6422, epoch: 194, batch: 19, loss: 0.0014488697052001953, acc: 100.0, f1: 100.0, r: 0.7143712818340116
06/02/2019 10:55:02 step: 6427, epoch: 194, batch: 24, loss: 0.0017227455973625183, acc: 100.0, f1: 100.0, r: 0.7288890023949413
06/02/2019 10:55:05 step: 6432, epoch: 194, batch: 29, loss: 0.0017960518598556519, acc: 100.0, f1: 100.0, r: 0.7268439116386157
06/02/2019 10:55:06 *** evaluating ***
06/02/2019 10:55:07 step: 195, epoch: 194, acc: 50.0, f1: 27.557563590299267, r: 0.2409281536849615
06/02/2019 10:55:07 *** epoch: 196 ***
06/02/2019 10:55:07 *** training ***
06/02/2019 10:55:09 step: 6440, epoch: 195, batch: 4, loss: 0.005554527044296265, acc: 100.0, f1: 100.0, r: 0.7785952239661416
06/02/2019 10:55:12 step: 6445, epoch: 195, batch: 9, loss: 0.005727112293243408, acc: 100.0, f1: 100.0, r: 0.8325965381339182
06/02/2019 10:55:14 step: 6450, epoch: 195, batch: 14, loss: 0.002366676926612854, acc: 100.0, f1: 100.0, r: 0.6835560480509048
06/02/2019 10:55:16 step: 6455, epoch: 195, batch: 19, loss: 0.005361519753932953, acc: 100.0, f1: 100.0, r: 0.8079568872079376
06/02/2019 10:55:19 step: 6460, epoch: 195, batch: 24, loss: 0.0033053308725357056, acc: 100.0, f1: 100.0, r: 0.7651469014577388
06/02/2019 10:55:21 step: 6465, epoch: 195, batch: 29, loss: 0.0036933422088623047, acc: 100.0, f1: 100.0, r: 0.7816772160129171
06/02/2019 10:55:22 *** evaluating ***
06/02/2019 10:55:23 step: 196, epoch: 195, acc: 50.0, f1: 28.287007384254515, r: 0.24218929731959468
06/02/2019 10:55:23 *** epoch: 197 ***
06/02/2019 10:55:23 *** training ***
06/02/2019 10:55:26 step: 6473, epoch: 196, batch: 4, loss: 0.0038096457719802856, acc: 100.0, f1: 100.0, r: 0.700413062727535
06/02/2019 10:55:28 step: 6478, epoch: 196, batch: 9, loss: 0.002917088568210602, acc: 100.0, f1: 100.0, r: 0.6901279548681811
06/02/2019 10:55:30 step: 6483, epoch: 196, batch: 14, loss: 0.0027405545115470886, acc: 100.0, f1: 100.0, r: 0.8139384827716845
06/02/2019 10:55:33 step: 6488, epoch: 196, batch: 19, loss: 0.005706764757633209, acc: 100.0, f1: 100.0, r: 0.6870685364817242
06/02/2019 10:55:35 step: 6493, epoch: 196, batch: 24, loss: 0.011035814881324768, acc: 100.0, f1: 100.0, r: 0.8332278925625562
06/02/2019 10:55:37 step: 6498, epoch: 196, batch: 29, loss: 0.003412604331970215, acc: 100.0, f1: 100.0, r: 0.7911016405843999
06/02/2019 10:55:38 *** evaluating ***
06/02/2019 10:55:39 step: 197, epoch: 196, acc: 49.572649572649574, f1: 27.29233357862783, r: 0.23966455579540466
06/02/2019 10:55:39 *** epoch: 198 ***
06/02/2019 10:55:39 *** training ***
06/02/2019 10:55:42 step: 6506, epoch: 197, batch: 4, loss: 0.0033816471695899963, acc: 100.0, f1: 100.0, r: 0.7232383135230838
06/02/2019 10:55:44 step: 6511, epoch: 197, batch: 9, loss: 0.00515410304069519, acc: 100.0, f1: 100.0, r: 0.749884182772338
06/02/2019 10:55:47 step: 6516, epoch: 197, batch: 14, loss: 0.005623310804367065, acc: 100.0, f1: 100.0, r: 0.6965142381539157
06/02/2019 10:55:49 step: 6521, epoch: 197, batch: 19, loss: 0.001975513994693756, acc: 100.0, f1: 100.0, r: 0.7618545841630553
06/02/2019 10:55:51 step: 6526, epoch: 197, batch: 24, loss: 0.004511646926403046, acc: 100.0, f1: 100.0, r: 0.7270738311947229
06/02/2019 10:55:54 step: 6531, epoch: 197, batch: 29, loss: 0.001729324460029602, acc: 100.0, f1: 100.0, r: 0.7344632495243386
06/02/2019 10:55:55 *** evaluating ***
06/02/2019 10:55:56 step: 198, epoch: 197, acc: 49.14529914529914, f1: 27.965261778279814, r: 0.2435137261800554
06/02/2019 10:55:56 *** epoch: 199 ***
06/02/2019 10:55:56 *** training ***
06/02/2019 10:55:58 step: 6539, epoch: 198, batch: 4, loss: 0.008760340511798859, acc: 100.0, f1: 100.0, r: 0.731936051421265
06/02/2019 10:56:01 step: 6544, epoch: 198, batch: 9, loss: 0.0014635175466537476, acc: 100.0, f1: 100.0, r: 0.7367773776692249
06/02/2019 10:56:03 step: 6549, epoch: 198, batch: 14, loss: 0.003348410129547119, acc: 100.0, f1: 100.0, r: 0.7617300131614552
06/02/2019 10:56:05 step: 6554, epoch: 198, batch: 19, loss: 0.002960875630378723, acc: 100.0, f1: 100.0, r: 0.8166429522145143
06/02/2019 10:56:08 step: 6559, epoch: 198, batch: 24, loss: 0.006143338978290558, acc: 100.0, f1: 100.0, r: 0.7904966983770206
06/02/2019 10:56:10 step: 6564, epoch: 198, batch: 29, loss: 0.00314880907535553, acc: 100.0, f1: 100.0, r: 0.7303807567992089
06/02/2019 10:56:11 *** evaluating ***
06/02/2019 10:56:12 step: 199, epoch: 198, acc: 49.14529914529914, f1: 27.942781611640616, r: 0.24386012682801952
06/02/2019 10:56:12 *** epoch: 200 ***
06/02/2019 10:56:12 *** training ***
06/02/2019 10:56:15 step: 6572, epoch: 199, batch: 4, loss: 0.0031690075993537903, acc: 100.0, f1: 100.0, r: 0.7415902979771444
06/02/2019 10:56:17 step: 6577, epoch: 199, batch: 9, loss: 0.0031159818172454834, acc: 100.0, f1: 100.0, r: 0.671081031327945
06/02/2019 10:56:19 step: 6582, epoch: 199, batch: 14, loss: 0.002708226442337036, acc: 100.0, f1: 100.0, r: 0.6850139210681992
06/02/2019 10:56:22 step: 6587, epoch: 199, batch: 19, loss: 0.0050188228487968445, acc: 100.0, f1: 100.0, r: 0.7950591786301879
06/02/2019 10:56:24 step: 6592, epoch: 199, batch: 24, loss: 0.0027771294116973877, acc: 100.0, f1: 100.0, r: 0.7053760216788327
06/02/2019 10:56:26 step: 6597, epoch: 199, batch: 29, loss: 0.0047736093401908875, acc: 100.0, f1: 100.0, r: 0.8047740729875992
06/02/2019 10:56:28 *** evaluating ***
06/02/2019 10:56:28 step: 200, epoch: 199, acc: 49.572649572649574, f1: 27.938794128012535, r: 0.24147345691131217
06/02/2019 10:56:28 *** epoch: 201 ***
06/02/2019 10:56:28 *** training ***
06/02/2019 10:56:31 step: 6605, epoch: 200, batch: 4, loss: 0.004271790385246277, acc: 100.0, f1: 100.0, r: 0.652057126446302
06/02/2019 10:56:33 step: 6610, epoch: 200, batch: 9, loss: 0.0024480000138282776, acc: 100.0, f1: 100.0, r: 0.8341822453822254
06/02/2019 10:56:36 step: 6615, epoch: 200, batch: 14, loss: 0.0025060251355171204, acc: 100.0, f1: 100.0, r: 0.7303753443825983
06/02/2019 10:56:38 step: 6620, epoch: 200, batch: 19, loss: 0.0026898980140686035, acc: 100.0, f1: 100.0, r: 0.6830111300958152
06/02/2019 10:56:41 step: 6625, epoch: 200, batch: 24, loss: 0.004838898777961731, acc: 100.0, f1: 100.0, r: 0.729866275865081
06/02/2019 10:56:43 step: 6630, epoch: 200, batch: 29, loss: 0.0023485571146011353, acc: 100.0, f1: 100.0, r: 0.7034685842480445
06/02/2019 10:56:44 *** evaluating ***
06/02/2019 10:56:45 step: 201, epoch: 200, acc: 49.572649572649574, f1: 28.1538538361063, r: 0.24151445918434797
06/02/2019 10:56:45 *** epoch: 202 ***
06/02/2019 10:56:45 *** training ***
06/02/2019 10:56:47 step: 6638, epoch: 201, batch: 4, loss: 0.0013485178351402283, acc: 100.0, f1: 100.0, r: 0.7515635779095499
06/02/2019 10:56:49 step: 6643, epoch: 201, batch: 9, loss: 0.0013689324259757996, acc: 100.0, f1: 100.0, r: 0.7052600942520063
06/02/2019 10:56:51 step: 6648, epoch: 201, batch: 14, loss: 0.004629150032997131, acc: 100.0, f1: 100.0, r: 0.679362095427093
06/02/2019 10:56:54 step: 6653, epoch: 201, batch: 19, loss: 0.005267582833766937, acc: 100.0, f1: 100.0, r: 0.7903123318628659
06/02/2019 10:56:57 step: 6658, epoch: 201, batch: 24, loss: 0.004294142127037048, acc: 100.0, f1: 100.0, r: 0.7266676294202415
06/02/2019 10:56:59 step: 6663, epoch: 201, batch: 29, loss: 0.0028655976057052612, acc: 100.0, f1: 100.0, r: 0.7157433044371717
06/02/2019 10:57:00 *** evaluating ***
06/02/2019 10:57:01 step: 202, epoch: 201, acc: 49.14529914529914, f1: 28.151711358111747, r: 0.24059501225766577
06/02/2019 10:57:01 *** epoch: 203 ***
06/02/2019 10:57:01 *** training ***
06/02/2019 10:57:03 step: 6671, epoch: 202, batch: 4, loss: 0.004723794758319855, acc: 100.0, f1: 100.0, r: 0.6796953738580183
06/02/2019 10:57:05 step: 6676, epoch: 202, batch: 9, loss: 0.0030193477869033813, acc: 100.0, f1: 100.0, r: 0.8170533821924115
06/02/2019 10:57:07 step: 6681, epoch: 202, batch: 14, loss: 0.0054258182644844055, acc: 100.0, f1: 100.0, r: 0.8252117138827274
06/02/2019 10:57:09 step: 6686, epoch: 202, batch: 19, loss: 0.003892168402671814, acc: 100.0, f1: 100.0, r: 0.6567695304451403
06/02/2019 10:57:11 step: 6691, epoch: 202, batch: 24, loss: 0.003093145787715912, acc: 100.0, f1: 100.0, r: 0.7321101545605369
06/02/2019 10:57:14 step: 6696, epoch: 202, batch: 29, loss: 0.0012055933475494385, acc: 100.0, f1: 100.0, r: 0.7187611220796963
06/02/2019 10:57:15 *** evaluating ***
06/02/2019 10:57:16 step: 203, epoch: 202, acc: 49.14529914529914, f1: 27.841196433873783, r: 0.2415353314820911
06/02/2019 10:57:16 *** epoch: 204 ***
06/02/2019 10:57:16 *** training ***
06/02/2019 10:57:18 step: 6704, epoch: 203, batch: 4, loss: 0.004868343472480774, acc: 100.0, f1: 100.0, r: 0.7955131602618439
06/02/2019 10:57:21 step: 6709, epoch: 203, batch: 9, loss: 0.0015162825584411621, acc: 100.0, f1: 100.0, r: 0.8050180562597522
06/02/2019 10:57:23 step: 6714, epoch: 203, batch: 14, loss: 0.0025920569896698, acc: 100.0, f1: 100.0, r: 0.7339190313150117
06/02/2019 10:57:25 step: 6719, epoch: 203, batch: 19, loss: 0.004104152321815491, acc: 100.0, f1: 100.0, r: 0.794514255054126
06/02/2019 10:57:28 step: 6724, epoch: 203, batch: 24, loss: 0.0030312463641166687, acc: 100.0, f1: 100.0, r: 0.802796062136437
06/02/2019 10:57:30 step: 6729, epoch: 203, batch: 29, loss: 0.003948777914047241, acc: 100.0, f1: 100.0, r: 0.7761304972535665
06/02/2019 10:57:31 *** evaluating ***
06/02/2019 10:57:32 step: 204, epoch: 203, acc: 48.29059829059829, f1: 27.425510374364357, r: 0.23775599371862763
06/02/2019 10:57:32 *** epoch: 205 ***
06/02/2019 10:57:32 *** training ***
06/02/2019 10:57:35 step: 6737, epoch: 204, batch: 4, loss: 0.0018797516822814941, acc: 100.0, f1: 100.0, r: 0.8245437247826728
06/02/2019 10:57:37 step: 6742, epoch: 204, batch: 9, loss: 0.00445953756570816, acc: 100.0, f1: 100.0, r: 0.7009397223389794
06/02/2019 10:57:39 step: 6747, epoch: 204, batch: 14, loss: 0.0005183666944503784, acc: 100.0, f1: 100.0, r: 0.6839447766197121
06/02/2019 10:57:42 step: 6752, epoch: 204, batch: 19, loss: 0.0015749111771583557, acc: 100.0, f1: 100.0, r: 0.6119836250140637
06/02/2019 10:57:44 step: 6757, epoch: 204, batch: 24, loss: 0.003611914813518524, acc: 100.0, f1: 100.0, r: 0.8128619687793559
06/02/2019 10:57:47 step: 6762, epoch: 204, batch: 29, loss: 0.004105523228645325, acc: 100.0, f1: 100.0, r: 0.7524238855968777
06/02/2019 10:57:48 *** evaluating ***
06/02/2019 10:57:49 step: 205, epoch: 204, acc: 48.29059829059829, f1: 27.282270558725163, r: 0.23999573416526573
06/02/2019 10:57:49 *** epoch: 206 ***
06/02/2019 10:57:49 *** training ***
06/02/2019 10:57:51 step: 6770, epoch: 205, batch: 4, loss: 0.0018334835767745972, acc: 100.0, f1: 100.0, r: 0.6971659881185481
06/02/2019 10:57:54 step: 6775, epoch: 205, batch: 9, loss: 0.003367088735103607, acc: 100.0, f1: 100.0, r: 0.6558456528337756
06/02/2019 10:57:57 step: 6780, epoch: 205, batch: 14, loss: 0.0029293522238731384, acc: 100.0, f1: 100.0, r: 0.6909931609701855
06/02/2019 10:57:59 step: 6785, epoch: 205, batch: 19, loss: 0.0024482086300849915, acc: 100.0, f1: 100.0, r: 0.6510055375049036
06/02/2019 10:58:01 step: 6790, epoch: 205, batch: 24, loss: 0.002246275544166565, acc: 100.0, f1: 100.0, r: 0.7865233755517677
06/02/2019 10:58:03 step: 6795, epoch: 205, batch: 29, loss: 0.0017456486821174622, acc: 100.0, f1: 100.0, r: 0.7078108469276987
06/02/2019 10:58:04 *** evaluating ***
06/02/2019 10:58:05 step: 206, epoch: 205, acc: 50.0, f1: 28.309388976670448, r: 0.24193390566537548
06/02/2019 10:58:05 *** epoch: 207 ***
06/02/2019 10:58:05 *** training ***
06/02/2019 10:58:07 step: 6803, epoch: 206, batch: 4, loss: 0.0024591386318206787, acc: 100.0, f1: 100.0, r: 0.7394764662627521
06/02/2019 10:58:10 step: 6808, epoch: 206, batch: 9, loss: 0.00210726261138916, acc: 100.0, f1: 100.0, r: 0.6957894763114034
06/02/2019 10:58:12 step: 6813, epoch: 206, batch: 14, loss: 0.0027113929390907288, acc: 100.0, f1: 100.0, r: 0.795817128554228
06/02/2019 10:58:15 step: 6818, epoch: 206, batch: 19, loss: 0.0014153718948364258, acc: 100.0, f1: 100.0, r: 0.6722759950827135
06/02/2019 10:58:17 step: 6823, epoch: 206, batch: 24, loss: 0.0020101219415664673, acc: 100.0, f1: 100.0, r: 0.7508600779409832
06/02/2019 10:58:19 step: 6828, epoch: 206, batch: 29, loss: 0.0022644996643066406, acc: 100.0, f1: 100.0, r: 0.7079106694875943
06/02/2019 10:58:20 *** evaluating ***
06/02/2019 10:58:21 step: 207, epoch: 206, acc: 49.572649572649574, f1: 28.655357699514816, r: 0.2393587923787301
06/02/2019 10:58:21 *** epoch: 208 ***
06/02/2019 10:58:21 *** training ***
06/02/2019 10:58:23 step: 6836, epoch: 207, batch: 4, loss: 0.0007193386554718018, acc: 100.0, f1: 100.0, r: 0.7540159031975511
06/02/2019 10:58:25 step: 6841, epoch: 207, batch: 9, loss: 0.0034223049879074097, acc: 100.0, f1: 100.0, r: 0.731081824701108
06/02/2019 10:58:28 step: 6846, epoch: 207, batch: 14, loss: 0.0014233589172363281, acc: 100.0, f1: 100.0, r: 0.7196327138882331
06/02/2019 10:58:31 step: 6851, epoch: 207, batch: 19, loss: 0.003056354820728302, acc: 100.0, f1: 100.0, r: 0.7954275107504749
06/02/2019 10:58:33 step: 6856, epoch: 207, batch: 24, loss: 0.005157530307769775, acc: 100.0, f1: 100.0, r: 0.739031891588176
06/02/2019 10:58:36 step: 6861, epoch: 207, batch: 29, loss: 0.0025562867522239685, acc: 100.0, f1: 100.0, r: 0.7201286263493123
06/02/2019 10:58:37 *** evaluating ***
06/02/2019 10:58:38 step: 208, epoch: 207, acc: 50.427350427350426, f1: 28.823031977115743, r: 0.2402879496460628
06/02/2019 10:58:38 *** epoch: 209 ***
06/02/2019 10:58:38 *** training ***
06/02/2019 10:58:40 step: 6869, epoch: 208, batch: 4, loss: 0.005217120051383972, acc: 100.0, f1: 100.0, r: 0.6080653380442989
06/02/2019 10:58:42 step: 6874, epoch: 208, batch: 9, loss: 0.005706295371055603, acc: 100.0, f1: 100.0, r: 0.6327947413115013
06/02/2019 10:58:45 step: 6879, epoch: 208, batch: 14, loss: 0.006943531334400177, acc: 100.0, f1: 100.0, r: 0.6362199067787723
06/02/2019 10:58:48 step: 6884, epoch: 208, batch: 19, loss: 0.003263048827648163, acc: 100.0, f1: 100.0, r: 0.7176129980124375
06/02/2019 10:58:49 step: 6889, epoch: 208, batch: 24, loss: 0.01053408533334732, acc: 100.0, f1: 100.0, r: 0.8030149203840354
06/02/2019 10:58:52 step: 6894, epoch: 208, batch: 29, loss: 0.0017013698816299438, acc: 100.0, f1: 100.0, r: 0.8030875411569881
06/02/2019 10:58:53 *** evaluating ***
06/02/2019 10:58:54 step: 209, epoch: 208, acc: 50.427350427350426, f1: 28.99145091273832, r: 0.24473385820006058
06/02/2019 10:58:54 *** epoch: 210 ***
06/02/2019 10:58:54 *** training ***
06/02/2019 10:58:56 step: 6902, epoch: 209, batch: 4, loss: 0.012612365186214447, acc: 100.0, f1: 100.0, r: 0.8004457731508056
06/02/2019 10:58:59 step: 6907, epoch: 209, batch: 9, loss: 0.0009266436100006104, acc: 100.0, f1: 100.0, r: 0.6765440148782631
06/02/2019 10:59:01 step: 6912, epoch: 209, batch: 14, loss: 0.0027274638414382935, acc: 100.0, f1: 100.0, r: 0.7157220543870885
06/02/2019 10:59:04 step: 6917, epoch: 209, batch: 19, loss: 0.0012921541929244995, acc: 100.0, f1: 100.0, r: 0.7266428468620458
06/02/2019 10:59:06 step: 6922, epoch: 209, batch: 24, loss: 0.005558274686336517, acc: 100.0, f1: 100.0, r: 0.7159942946482875
06/02/2019 10:59:09 step: 6927, epoch: 209, batch: 29, loss: 0.0014988631010055542, acc: 100.0, f1: 100.0, r: 0.7862654596351922
06/02/2019 10:59:10 *** evaluating ***
06/02/2019 10:59:10 step: 210, epoch: 209, acc: 50.0, f1: 28.96748268688416, r: 0.24261108325040484
06/02/2019 10:59:10 *** epoch: 211 ***
06/02/2019 10:59:10 *** training ***
06/02/2019 10:59:12 step: 6935, epoch: 210, batch: 4, loss: 0.003064684569835663, acc: 100.0, f1: 100.0, r: 0.6933598805526751
06/02/2019 10:59:15 step: 6940, epoch: 210, batch: 9, loss: 0.0013001710176467896, acc: 100.0, f1: 100.0, r: 0.8268456628153105
06/02/2019 10:59:17 step: 6945, epoch: 210, batch: 14, loss: 0.006706364452838898, acc: 100.0, f1: 100.0, r: 0.721196442300497
06/02/2019 10:59:19 step: 6950, epoch: 210, batch: 19, loss: 0.0012349337339401245, acc: 100.0, f1: 100.0, r: 0.7753171062622947
06/02/2019 10:59:21 step: 6955, epoch: 210, batch: 24, loss: 0.004106573760509491, acc: 100.0, f1: 100.0, r: 0.7702223608779446
06/02/2019 10:59:24 step: 6960, epoch: 210, batch: 29, loss: 0.0045742467045784, acc: 100.0, f1: 100.0, r: 0.8429596321296168
06/02/2019 10:59:25 *** evaluating ***
06/02/2019 10:59:26 step: 211, epoch: 210, acc: 48.717948717948715, f1: 28.07594029324394, r: 0.23954286486446125
06/02/2019 10:59:26 *** epoch: 212 ***
06/02/2019 10:59:26 *** training ***
06/02/2019 10:59:29 step: 6968, epoch: 211, batch: 4, loss: 0.0018905624747276306, acc: 100.0, f1: 100.0, r: 0.8123194798926758
06/02/2019 10:59:30 step: 6973, epoch: 211, batch: 9, loss: 0.001096673309803009, acc: 100.0, f1: 100.0, r: 0.699209182589705
06/02/2019 10:59:33 step: 6978, epoch: 211, batch: 14, loss: 0.00206177681684494, acc: 100.0, f1: 100.0, r: 0.6664517942452578
06/02/2019 10:59:35 step: 6983, epoch: 211, batch: 19, loss: 0.003271922469139099, acc: 100.0, f1: 100.0, r: 0.7191730546295174
06/02/2019 10:59:38 step: 6988, epoch: 211, batch: 24, loss: 0.0027369633316993713, acc: 100.0, f1: 100.0, r: 0.7238357087834573
06/02/2019 10:59:40 step: 6993, epoch: 211, batch: 29, loss: 0.0030412524938583374, acc: 100.0, f1: 100.0, r: 0.831066688528373
06/02/2019 10:59:41 *** evaluating ***
06/02/2019 10:59:42 step: 212, epoch: 211, acc: 49.14529914529914, f1: 27.990548622194872, r: 0.23983836264692687
06/02/2019 10:59:42 *** epoch: 213 ***
06/02/2019 10:59:42 *** training ***
06/02/2019 10:59:44 step: 7001, epoch: 212, batch: 4, loss: 0.006613843142986298, acc: 100.0, f1: 100.0, r: 0.7084221340300532
06/02/2019 10:59:47 step: 7006, epoch: 212, batch: 9, loss: 0.002809382975101471, acc: 100.0, f1: 100.0, r: 0.7127793628072184
06/02/2019 10:59:49 step: 7011, epoch: 212, batch: 14, loss: 0.0021099895238876343, acc: 100.0, f1: 100.0, r: 0.7799699913695954
06/02/2019 10:59:51 step: 7016, epoch: 212, batch: 19, loss: 0.004271097481250763, acc: 100.0, f1: 100.0, r: 0.6918701727664229
06/02/2019 10:59:54 step: 7021, epoch: 212, batch: 24, loss: 0.006683528423309326, acc: 100.0, f1: 100.0, r: 0.685225013131679
06/02/2019 10:59:56 step: 7026, epoch: 212, batch: 29, loss: 0.0021417662501335144, acc: 100.0, f1: 100.0, r: 0.6951032746573428
06/02/2019 10:59:57 *** evaluating ***
06/02/2019 10:59:58 step: 213, epoch: 212, acc: 48.29059829059829, f1: 27.46904144045627, r: 0.24132705416187317
06/02/2019 10:59:58 *** epoch: 214 ***
06/02/2019 10:59:58 *** training ***
06/02/2019 11:00:00 step: 7034, epoch: 213, batch: 4, loss: 0.0015646517276763916, acc: 100.0, f1: 100.0, r: 0.7790949904388303
06/02/2019 11:00:03 step: 7039, epoch: 213, batch: 9, loss: 0.004753917455673218, acc: 100.0, f1: 100.0, r: 0.6972710925713635
06/02/2019 11:00:05 step: 7044, epoch: 213, batch: 14, loss: 0.004359059035778046, acc: 100.0, f1: 100.0, r: 0.7095911834904848
06/02/2019 11:00:08 step: 7049, epoch: 213, batch: 19, loss: 0.0022666901350021362, acc: 100.0, f1: 100.0, r: 0.822644906900855
06/02/2019 11:00:10 step: 7054, epoch: 213, batch: 24, loss: 0.0007127374410629272, acc: 100.0, f1: 100.0, r: 0.6882735808214396
06/02/2019 11:00:13 step: 7059, epoch: 213, batch: 29, loss: 0.003918461501598358, acc: 100.0, f1: 100.0, r: 0.7663708088299742
06/02/2019 11:00:14 *** evaluating ***
06/02/2019 11:00:14 step: 214, epoch: 213, acc: 48.717948717948715, f1: 29.540346518583473, r: 0.2476290078251872
06/02/2019 11:00:14 *** epoch: 215 ***
06/02/2019 11:00:14 *** training ***
06/02/2019 11:00:16 step: 7067, epoch: 214, batch: 4, loss: 0.002488143742084503, acc: 100.0, f1: 100.0, r: 0.7821571360394042
06/02/2019 11:00:19 step: 7072, epoch: 214, batch: 9, loss: 0.0029741525650024414, acc: 100.0, f1: 100.0, r: 0.7285591799600478
06/02/2019 11:00:21 step: 7077, epoch: 214, batch: 14, loss: 0.006194159388542175, acc: 100.0, f1: 100.0, r: 0.6743721571462405
06/02/2019 11:00:24 step: 7082, epoch: 214, batch: 19, loss: 0.001752316951751709, acc: 100.0, f1: 100.0, r: 0.7594636180591058
06/02/2019 11:00:26 step: 7087, epoch: 214, batch: 24, loss: 0.004588283598423004, acc: 100.0, f1: 100.0, r: 0.7699150368950809
06/02/2019 11:00:29 step: 7092, epoch: 214, batch: 29, loss: 0.0041458457708358765, acc: 100.0, f1: 100.0, r: 0.5844062594158216
06/02/2019 11:00:30 *** evaluating ***
06/02/2019 11:00:31 step: 215, epoch: 214, acc: 50.0, f1: 29.42925635166347, r: 0.25749174638539407
06/02/2019 11:00:31 *** epoch: 216 ***
06/02/2019 11:00:31 *** training ***
06/02/2019 11:00:33 step: 7100, epoch: 215, batch: 4, loss: 0.003281310200691223, acc: 100.0, f1: 100.0, r: 0.8641959349625085
06/02/2019 11:00:35 step: 7105, epoch: 215, batch: 9, loss: 0.0019957199692726135, acc: 100.0, f1: 100.0, r: 0.694228892026114
06/02/2019 11:00:38 step: 7110, epoch: 215, batch: 14, loss: 0.008042536675930023, acc: 100.0, f1: 100.0, r: 0.8056683875762901
06/02/2019 11:00:41 step: 7115, epoch: 215, batch: 19, loss: 0.003221854567527771, acc: 100.0, f1: 100.0, r: 0.616268952396358
06/02/2019 11:00:43 step: 7120, epoch: 215, batch: 24, loss: 0.003868997097015381, acc: 100.0, f1: 100.0, r: 0.655628640111254
06/02/2019 11:00:45 step: 7125, epoch: 215, batch: 29, loss: 0.004693314433097839, acc: 100.0, f1: 100.0, r: 0.8376400525179601
06/02/2019 11:00:46 *** evaluating ***
06/02/2019 11:00:47 step: 216, epoch: 215, acc: 48.29059829059829, f1: 27.39937172315514, r: 0.2396566962315446
06/02/2019 11:00:47 *** epoch: 217 ***
06/02/2019 11:00:47 *** training ***
06/02/2019 11:00:49 step: 7133, epoch: 216, batch: 4, loss: 0.003538481891155243, acc: 100.0, f1: 100.0, r: 0.7229687977597614
06/02/2019 11:00:51 step: 7138, epoch: 216, batch: 9, loss: 0.0026512816548347473, acc: 100.0, f1: 100.0, r: 0.7947455894892085
06/02/2019 11:00:54 step: 7143, epoch: 216, batch: 14, loss: 0.0017310231924057007, acc: 100.0, f1: 100.0, r: 0.6802374091252905
06/02/2019 11:00:56 step: 7148, epoch: 216, batch: 19, loss: 0.00432518869638443, acc: 100.0, f1: 100.0, r: 0.7036022992022616
06/02/2019 11:00:59 step: 7153, epoch: 216, batch: 24, loss: 0.0016644969582557678, acc: 100.0, f1: 100.0, r: 0.6632332356368607
06/02/2019 11:01:01 step: 7158, epoch: 216, batch: 29, loss: 0.0022006183862686157, acc: 100.0, f1: 100.0, r: 0.7523541471665707
06/02/2019 11:01:02 *** evaluating ***
06/02/2019 11:01:03 step: 217, epoch: 216, acc: 49.572649572649574, f1: 28.321491066437027, r: 0.23932907073371157
06/02/2019 11:01:03 *** epoch: 218 ***
06/02/2019 11:01:03 *** training ***
06/02/2019 11:01:05 step: 7166, epoch: 217, batch: 4, loss: 0.0020741447806358337, acc: 100.0, f1: 100.0, r: 0.7172063826184271
06/02/2019 11:01:07 step: 7171, epoch: 217, batch: 9, loss: 0.004058189690113068, acc: 100.0, f1: 100.0, r: 0.7602938989087324
06/02/2019 11:01:10 step: 7176, epoch: 217, batch: 14, loss: 0.005901999771595001, acc: 100.0, f1: 100.0, r: 0.6718531666547679
06/02/2019 11:01:13 step: 7181, epoch: 217, batch: 19, loss: 0.005067773163318634, acc: 100.0, f1: 100.0, r: 0.764298674809787
06/02/2019 11:01:15 step: 7186, epoch: 217, batch: 24, loss: 0.008359760046005249, acc: 100.0, f1: 100.0, r: 0.7710846799561718
06/02/2019 11:01:18 step: 7191, epoch: 217, batch: 29, loss: 0.0028683096170425415, acc: 100.0, f1: 100.0, r: 0.7857621669146113
06/02/2019 11:01:19 *** evaluating ***
06/02/2019 11:01:19 step: 218, epoch: 217, acc: 50.427350427350426, f1: 29.696468213509476, r: 0.24624713153628183
06/02/2019 11:01:19 *** epoch: 219 ***
06/02/2019 11:01:19 *** training ***
06/02/2019 11:01:21 step: 7199, epoch: 218, batch: 4, loss: 0.0037917718291282654, acc: 100.0, f1: 100.0, r: 0.8026959983103737
06/02/2019 11:01:23 step: 7204, epoch: 218, batch: 9, loss: 0.005756169557571411, acc: 100.0, f1: 100.0, r: 0.7877804739083574
06/02/2019 11:01:26 step: 7209, epoch: 218, batch: 14, loss: 0.0038766488432884216, acc: 100.0, f1: 100.0, r: 0.6971047830302208
06/02/2019 11:01:28 step: 7214, epoch: 218, batch: 19, loss: 0.003695279359817505, acc: 100.0, f1: 100.0, r: 0.7189091100002388
06/02/2019 11:01:31 step: 7219, epoch: 218, batch: 24, loss: 0.002272181212902069, acc: 100.0, f1: 100.0, r: 0.8181716128643615
06/02/2019 11:01:33 step: 7224, epoch: 218, batch: 29, loss: 0.001433052122592926, acc: 100.0, f1: 100.0, r: 0.7282248495490695
06/02/2019 11:01:35 *** evaluating ***
06/02/2019 11:01:36 step: 219, epoch: 218, acc: 50.0, f1: 29.693054045921528, r: 0.2478755549221046
06/02/2019 11:01:36 *** epoch: 220 ***
06/02/2019 11:01:36 *** training ***
06/02/2019 11:01:38 step: 7232, epoch: 219, batch: 4, loss: 0.06550735235214233, acc: 98.4375, f1: 96.86028257456829, r: 0.7030862845573007
06/02/2019 11:01:40 step: 7237, epoch: 219, batch: 9, loss: 0.0057494789361953735, acc: 100.0, f1: 100.0, r: 0.8324041142753451
06/02/2019 11:01:42 step: 7242, epoch: 219, batch: 14, loss: 0.03805837035179138, acc: 96.875, f1: 96.84748427672957, r: 0.7407459766630679
06/02/2019 11:01:44 step: 7247, epoch: 219, batch: 19, loss: 0.0020171403884887695, acc: 100.0, f1: 100.0, r: 0.6819891268066713
06/02/2019 11:01:47 step: 7252, epoch: 219, batch: 24, loss: 0.014604784548282623, acc: 100.0, f1: 100.0, r: 0.7663996719912469
06/02/2019 11:01:49 step: 7257, epoch: 219, batch: 29, loss: 0.0010340064764022827, acc: 100.0, f1: 100.0, r: 0.7087404543955574
06/02/2019 11:01:50 *** evaluating ***
06/02/2019 11:01:51 step: 220, epoch: 219, acc: 50.427350427350426, f1: 28.472120506894942, r: 0.2449413380143017
06/02/2019 11:01:51 *** epoch: 221 ***
06/02/2019 11:01:51 *** training ***
06/02/2019 11:01:53 step: 7265, epoch: 220, batch: 4, loss: 0.0014971494674682617, acc: 100.0, f1: 100.0, r: 0.7112046505069853
06/02/2019 11:01:56 step: 7270, epoch: 220, batch: 9, loss: 0.001964554190635681, acc: 100.0, f1: 100.0, r: 0.7052198546009094
06/02/2019 11:01:58 step: 7275, epoch: 220, batch: 14, loss: 0.007625378668308258, acc: 100.0, f1: 100.0, r: 0.829414582303978
06/02/2019 11:02:01 step: 7280, epoch: 220, batch: 19, loss: 0.0021553784608840942, acc: 100.0, f1: 100.0, r: 0.7687620378006471
06/02/2019 11:02:03 step: 7285, epoch: 220, batch: 24, loss: 0.0033451691269874573, acc: 100.0, f1: 100.0, r: 0.7128759650052152
06/02/2019 11:02:05 step: 7290, epoch: 220, batch: 29, loss: 0.004024907946586609, acc: 100.0, f1: 100.0, r: 0.8090153716562997
06/02/2019 11:02:06 *** evaluating ***
06/02/2019 11:02:07 step: 221, epoch: 220, acc: 49.572649572649574, f1: 28.055450294101192, r: 0.24594377787260388
06/02/2019 11:02:07 *** epoch: 222 ***
06/02/2019 11:02:07 *** training ***
06/02/2019 11:02:09 step: 7298, epoch: 221, batch: 4, loss: 0.0017510876059532166, acc: 100.0, f1: 100.0, r: 0.7415941175679163
06/02/2019 11:02:11 step: 7303, epoch: 221, batch: 9, loss: 0.001352459192276001, acc: 100.0, f1: 100.0, r: 0.698206185191732
06/02/2019 11:02:13 step: 7308, epoch: 221, batch: 14, loss: 0.0030186325311660767, acc: 100.0, f1: 100.0, r: 0.6877648703570567
06/02/2019 11:02:16 step: 7313, epoch: 221, batch: 19, loss: 0.002620324492454529, acc: 100.0, f1: 100.0, r: 0.8094156276911685
06/02/2019 11:02:19 step: 7318, epoch: 221, batch: 24, loss: 0.004351362586021423, acc: 100.0, f1: 100.0, r: 0.8162684102790907
06/02/2019 11:02:21 step: 7323, epoch: 221, batch: 29, loss: 0.0018400251865386963, acc: 100.0, f1: 100.0, r: 0.7382738372122464
06/02/2019 11:02:22 *** evaluating ***
06/02/2019 11:02:22 step: 222, epoch: 221, acc: 50.0, f1: 29.947526791968883, r: 0.251397243089462
06/02/2019 11:02:22 *** epoch: 223 ***
06/02/2019 11:02:22 *** training ***
06/02/2019 11:02:25 step: 7331, epoch: 222, batch: 4, loss: 0.0029321983456611633, acc: 100.0, f1: 100.0, r: 0.6711317138354838
06/02/2019 11:02:27 step: 7336, epoch: 222, batch: 9, loss: 0.005735315382480621, acc: 100.0, f1: 100.0, r: 0.7271130940695328
06/02/2019 11:02:30 step: 7341, epoch: 222, batch: 14, loss: 0.0016394704580307007, acc: 100.0, f1: 100.0, r: 0.7079893330077047
06/02/2019 11:02:32 step: 7346, epoch: 222, batch: 19, loss: 0.001080140471458435, acc: 100.0, f1: 100.0, r: 0.7907590341470707
06/02/2019 11:02:34 step: 7351, epoch: 222, batch: 24, loss: 0.005898520350456238, acc: 100.0, f1: 100.0, r: 0.6610028243068293
06/02/2019 11:02:36 step: 7356, epoch: 222, batch: 29, loss: 0.001965142786502838, acc: 100.0, f1: 100.0, r: 0.6599832550808615
06/02/2019 11:02:38 *** evaluating ***
06/02/2019 11:02:38 step: 223, epoch: 222, acc: 49.572649572649574, f1: 29.40871121669516, r: 0.2513200464012716
06/02/2019 11:02:38 *** epoch: 224 ***
06/02/2019 11:02:38 *** training ***
06/02/2019 11:02:40 step: 7364, epoch: 223, batch: 4, loss: 0.0013585463166236877, acc: 100.0, f1: 100.0, r: 0.6386378946063616
06/02/2019 11:02:43 step: 7369, epoch: 223, batch: 9, loss: 0.0031699612736701965, acc: 100.0, f1: 100.0, r: 0.7007102601758537
06/02/2019 11:02:45 step: 7374, epoch: 223, batch: 14, loss: 0.004895120859146118, acc: 100.0, f1: 100.0, r: 0.7213008610697346
06/02/2019 11:02:48 step: 7379, epoch: 223, batch: 19, loss: 0.012507513165473938, acc: 100.0, f1: 100.0, r: 0.7127176928942686
06/02/2019 11:02:50 step: 7384, epoch: 223, batch: 24, loss: 0.003144770860671997, acc: 100.0, f1: 100.0, r: 0.8242016805021493
06/02/2019 11:02:53 step: 7389, epoch: 223, batch: 29, loss: 0.002852499485015869, acc: 100.0, f1: 100.0, r: 0.6922705829583882
06/02/2019 11:02:54 *** evaluating ***
06/02/2019 11:02:55 step: 224, epoch: 223, acc: 49.572649572649574, f1: 28.18844249738613, r: 0.243693612088261
06/02/2019 11:02:55 *** epoch: 225 ***
06/02/2019 11:02:55 *** training ***
06/02/2019 11:02:57 step: 7397, epoch: 224, batch: 4, loss: 0.0017230063676834106, acc: 100.0, f1: 100.0, r: 0.7228787506467612
06/02/2019 11:03:00 step: 7402, epoch: 224, batch: 9, loss: 0.0021071135997772217, acc: 100.0, f1: 100.0, r: 0.5998491772594882
06/02/2019 11:03:02 step: 7407, epoch: 224, batch: 14, loss: 0.0032740160822868347, acc: 100.0, f1: 100.0, r: 0.7019529177852778
06/02/2019 11:03:04 step: 7412, epoch: 224, batch: 19, loss: 0.003954507410526276, acc: 100.0, f1: 100.0, r: 0.6316846552459223
06/02/2019 11:03:06 step: 7417, epoch: 224, batch: 24, loss: 0.0046505406498909, acc: 100.0, f1: 100.0, r: 0.7339639572067097
06/02/2019 11:03:08 step: 7422, epoch: 224, batch: 29, loss: 0.0005912631750106812, acc: 100.0, f1: 100.0, r: 0.627143854409524
06/02/2019 11:03:10 *** evaluating ***
06/02/2019 11:03:11 step: 225, epoch: 224, acc: 50.0, f1: 28.09166130567059, r: 0.24089533118388137
06/02/2019 11:03:11 *** epoch: 226 ***
06/02/2019 11:03:11 *** training ***
06/02/2019 11:03:13 step: 7430, epoch: 225, batch: 4, loss: 0.0032317712903022766, acc: 100.0, f1: 100.0, r: 0.7501067805521218
06/02/2019 11:03:16 step: 7435, epoch: 225, batch: 9, loss: 0.003639012575149536, acc: 100.0, f1: 100.0, r: 0.7113076560020465
06/02/2019 11:03:18 step: 7440, epoch: 225, batch: 14, loss: 0.0023277029395103455, acc: 100.0, f1: 100.0, r: 0.8001436890310989
06/02/2019 11:03:20 step: 7445, epoch: 225, batch: 19, loss: 0.001214437186717987, acc: 100.0, f1: 100.0, r: 0.7580764155010044
06/02/2019 11:03:23 step: 7450, epoch: 225, batch: 24, loss: 0.002348795533180237, acc: 100.0, f1: 100.0, r: 0.7368191194562704
06/02/2019 11:03:25 step: 7455, epoch: 225, batch: 29, loss: 0.0015107765793800354, acc: 100.0, f1: 100.0, r: 0.816168387388629
06/02/2019 11:03:26 *** evaluating ***
06/02/2019 11:03:27 step: 226, epoch: 225, acc: 50.427350427350426, f1: 28.444556730454902, r: 0.24172703629980194
06/02/2019 11:03:27 *** epoch: 227 ***
06/02/2019 11:03:27 *** training ***
06/02/2019 11:03:29 step: 7463, epoch: 226, batch: 4, loss: 0.0023472607135772705, acc: 100.0, f1: 100.0, r: 0.7921359175093402
06/02/2019 11:03:32 step: 7468, epoch: 226, batch: 9, loss: 0.0014430731534957886, acc: 100.0, f1: 100.0, r: 0.7592947160789685
06/02/2019 11:03:34 step: 7473, epoch: 226, batch: 14, loss: 0.0021234527230262756, acc: 100.0, f1: 100.0, r: 0.8348378683498433
06/02/2019 11:03:37 step: 7478, epoch: 226, batch: 19, loss: 0.0010905638337135315, acc: 100.0, f1: 100.0, r: 0.682547876925571
06/02/2019 11:03:39 step: 7483, epoch: 226, batch: 24, loss: 0.0029834434390068054, acc: 100.0, f1: 100.0, r: 0.8524669658427966
06/02/2019 11:03:41 step: 7488, epoch: 226, batch: 29, loss: 0.005901522934436798, acc: 100.0, f1: 100.0, r: 0.7275975703911263
06/02/2019 11:03:43 *** evaluating ***
06/02/2019 11:03:43 step: 227, epoch: 226, acc: 50.427350427350426, f1: 28.444556730454902, r: 0.23979041392910944
06/02/2019 11:03:43 *** epoch: 228 ***
06/02/2019 11:03:43 *** training ***
06/02/2019 11:03:46 step: 7496, epoch: 227, batch: 4, loss: 0.0033222511410713196, acc: 100.0, f1: 100.0, r: 0.7681580568769752
06/02/2019 11:03:48 step: 7501, epoch: 227, batch: 9, loss: 0.004091642796993256, acc: 100.0, f1: 100.0, r: 0.7656816253915653
06/02/2019 11:03:50 step: 7506, epoch: 227, batch: 14, loss: 0.0014072433114051819, acc: 100.0, f1: 100.0, r: 0.6760656098752141
06/02/2019 11:03:53 step: 7511, epoch: 227, batch: 19, loss: 0.0009210556745529175, acc: 100.0, f1: 100.0, r: 0.8003046355774046
06/02/2019 11:03:55 step: 7516, epoch: 227, batch: 24, loss: 0.003479704260826111, acc: 100.0, f1: 100.0, r: 0.7173057858502627
06/02/2019 11:03:57 step: 7521, epoch: 227, batch: 29, loss: 0.0022889375686645508, acc: 100.0, f1: 100.0, r: 0.6522412955268765
06/02/2019 11:03:58 *** evaluating ***
06/02/2019 11:03:59 step: 228, epoch: 227, acc: 50.85470085470085, f1: 28.849474852693202, r: 0.24568321421462302
06/02/2019 11:03:59 *** epoch: 229 ***
06/02/2019 11:03:59 *** training ***
06/02/2019 11:04:01 step: 7529, epoch: 228, batch: 4, loss: 0.0012987852096557617, acc: 100.0, f1: 100.0, r: 0.8078837342234878
06/02/2019 11:04:04 step: 7534, epoch: 228, batch: 9, loss: 0.0030966922640800476, acc: 100.0, f1: 100.0, r: 0.7286266536462795
06/02/2019 11:04:06 step: 7539, epoch: 228, batch: 14, loss: 0.0024474486708641052, acc: 100.0, f1: 100.0, r: 0.8385350539474857
06/02/2019 11:04:09 step: 7544, epoch: 228, batch: 19, loss: 0.001973286271095276, acc: 100.0, f1: 100.0, r: 0.7305648147020981
06/02/2019 11:04:11 step: 7549, epoch: 228, batch: 24, loss: 0.0026434436440467834, acc: 100.0, f1: 100.0, r: 0.7849728972729859
06/02/2019 11:04:13 step: 7554, epoch: 228, batch: 29, loss: 0.002400629222393036, acc: 100.0, f1: 100.0, r: 0.690384945651805
06/02/2019 11:04:14 *** evaluating ***
06/02/2019 11:04:15 step: 229, epoch: 228, acc: 51.28205128205128, f1: 29.50312225805091, r: 0.24894585976218814
06/02/2019 11:04:15 *** epoch: 230 ***
06/02/2019 11:04:15 *** training ***
06/02/2019 11:04:18 step: 7562, epoch: 229, batch: 4, loss: 0.0022850781679153442, acc: 100.0, f1: 100.0, r: 0.6935420280124573
06/02/2019 11:04:21 step: 7567, epoch: 229, batch: 9, loss: 0.0018153339624404907, acc: 100.0, f1: 100.0, r: 0.7349761420841041
06/02/2019 11:04:23 step: 7572, epoch: 229, batch: 14, loss: 0.0012918561697006226, acc: 100.0, f1: 100.0, r: 0.7011502833494117
06/02/2019 11:04:25 step: 7577, epoch: 229, batch: 19, loss: 0.00208456814289093, acc: 100.0, f1: 100.0, r: 0.7564987057998112
06/02/2019 11:04:27 step: 7582, epoch: 229, batch: 24, loss: 0.0021233484148979187, acc: 100.0, f1: 100.0, r: 0.8104004981111846
06/02/2019 11:04:29 step: 7587, epoch: 229, batch: 29, loss: 0.0017923638224601746, acc: 100.0, f1: 100.0, r: 0.7377024864203939
06/02/2019 11:04:31 *** evaluating ***
06/02/2019 11:04:31 step: 230, epoch: 229, acc: 49.572649572649574, f1: 28.245998825771967, r: 0.248895702763321
06/02/2019 11:04:31 *** epoch: 231 ***
06/02/2019 11:04:31 *** training ***
06/02/2019 11:04:34 step: 7595, epoch: 230, batch: 4, loss: 0.0027713775634765625, acc: 100.0, f1: 100.0, r: 0.8165910824074148
06/02/2019 11:04:36 step: 7600, epoch: 230, batch: 9, loss: 0.0018408223986625671, acc: 100.0, f1: 100.0, r: 0.6778393242916033
06/02/2019 11:04:38 step: 7605, epoch: 230, batch: 14, loss: 0.010686837136745453, acc: 100.0, f1: 100.0, r: 0.7506908016963344
06/02/2019 11:04:40 step: 7610, epoch: 230, batch: 19, loss: 0.0020233839750289917, acc: 100.0, f1: 100.0, r: 0.6381749004493852
06/02/2019 11:04:42 step: 7615, epoch: 230, batch: 24, loss: 0.001552581787109375, acc: 100.0, f1: 100.0, r: 0.8498702916807658
06/02/2019 11:04:45 step: 7620, epoch: 230, batch: 29, loss: 0.004578456282615662, acc: 100.0, f1: 100.0, r: 0.8474320869937665
06/02/2019 11:04:46 *** evaluating ***
06/02/2019 11:04:47 step: 231, epoch: 230, acc: 50.427350427350426, f1: 28.895878315761014, r: 0.24894578302395615
06/02/2019 11:04:47 *** epoch: 232 ***
06/02/2019 11:04:47 *** training ***
06/02/2019 11:04:49 step: 7628, epoch: 231, batch: 4, loss: 0.003357112407684326, acc: 100.0, f1: 100.0, r: 0.6734743561685861
06/02/2019 11:04:52 step: 7633, epoch: 231, batch: 9, loss: 0.0012335702776908875, acc: 100.0, f1: 100.0, r: 0.8220660621678119
06/02/2019 11:04:54 step: 7638, epoch: 231, batch: 14, loss: 0.003410235047340393, acc: 100.0, f1: 100.0, r: 0.8228031072070131
06/02/2019 11:04:56 step: 7643, epoch: 231, batch: 19, loss: 0.002589389681816101, acc: 100.0, f1: 100.0, r: 0.7399889997852992
06/02/2019 11:04:59 step: 7648, epoch: 231, batch: 24, loss: 0.0014438331127166748, acc: 100.0, f1: 100.0, r: 0.7518766858562222
06/02/2019 11:05:02 step: 7653, epoch: 231, batch: 29, loss: 0.0026224032044410706, acc: 100.0, f1: 100.0, r: 0.7276478509349944
06/02/2019 11:05:03 *** evaluating ***
06/02/2019 11:05:03 step: 232, epoch: 231, acc: 49.572649572649574, f1: 28.293142220609845, r: 0.2491075499029592
06/02/2019 11:05:03 *** epoch: 233 ***
06/02/2019 11:05:03 *** training ***
06/02/2019 11:05:06 step: 7661, epoch: 232, batch: 4, loss: 0.0018545836210250854, acc: 100.0, f1: 100.0, r: 0.8341138049152267
06/02/2019 11:05:08 step: 7666, epoch: 232, batch: 9, loss: 0.004067502915859222, acc: 100.0, f1: 100.0, r: 0.8055994327434951
06/02/2019 11:05:11 step: 7671, epoch: 232, batch: 14, loss: 0.0007316321134567261, acc: 100.0, f1: 100.0, r: 0.680150180215526
06/02/2019 11:05:13 step: 7676, epoch: 232, batch: 19, loss: 0.0015827789902687073, acc: 100.0, f1: 100.0, r: 0.5996094261304652
06/02/2019 11:05:15 step: 7681, epoch: 232, batch: 24, loss: 0.0037948042154312134, acc: 100.0, f1: 100.0, r: 0.735240509695824
06/02/2019 11:05:18 step: 7686, epoch: 232, batch: 29, loss: 0.003222845494747162, acc: 100.0, f1: 100.0, r: 0.69241674894186
06/02/2019 11:05:19 *** evaluating ***
06/02/2019 11:05:20 step: 233, epoch: 232, acc: 50.0, f1: 28.423916881581995, r: 0.24228753152572757
06/02/2019 11:05:20 *** epoch: 234 ***
06/02/2019 11:05:20 *** training ***
06/02/2019 11:05:22 step: 7694, epoch: 233, batch: 4, loss: 0.0011981800198554993, acc: 100.0, f1: 100.0, r: 0.7014297311612785
06/02/2019 11:05:25 step: 7699, epoch: 233, batch: 9, loss: 0.001545444130897522, acc: 100.0, f1: 100.0, r: 0.5362114608276439
06/02/2019 11:05:27 step: 7704, epoch: 233, batch: 14, loss: 0.0008986145257949829, acc: 100.0, f1: 100.0, r: 0.661385719218023
06/02/2019 11:05:30 step: 7709, epoch: 233, batch: 19, loss: 0.0031021088361740112, acc: 100.0, f1: 100.0, r: 0.7069048055422862
06/02/2019 11:05:32 step: 7714, epoch: 233, batch: 24, loss: 0.005434371531009674, acc: 100.0, f1: 100.0, r: 0.8418295613720018
06/02/2019 11:05:35 step: 7719, epoch: 233, batch: 29, loss: 0.0024740248918533325, acc: 100.0, f1: 100.0, r: 0.823284858695998
06/02/2019 11:05:36 *** evaluating ***
06/02/2019 11:05:36 step: 234, epoch: 233, acc: 50.0, f1: 28.46657334326142, r: 0.24322828046223777
06/02/2019 11:05:36 *** epoch: 235 ***
06/02/2019 11:05:36 *** training ***
06/02/2019 11:05:38 step: 7727, epoch: 234, batch: 4, loss: 0.0005653947591781616, acc: 100.0, f1: 100.0, r: 0.7945724248045912
06/02/2019 11:05:41 step: 7732, epoch: 234, batch: 9, loss: 0.001513853669166565, acc: 100.0, f1: 100.0, r: 0.7881856657750974
06/02/2019 11:05:43 step: 7737, epoch: 234, batch: 14, loss: 0.002547748386859894, acc: 100.0, f1: 100.0, r: 0.7253328233791296
06/02/2019 11:05:45 step: 7742, epoch: 234, batch: 19, loss: 0.0008804798126220703, acc: 100.0, f1: 100.0, r: 0.7109957118557986
06/02/2019 11:05:48 step: 7747, epoch: 234, batch: 24, loss: 0.00479578971862793, acc: 100.0, f1: 100.0, r: 0.684774419698411
06/02/2019 11:05:50 step: 7752, epoch: 234, batch: 29, loss: 0.003027990460395813, acc: 100.0, f1: 100.0, r: 0.7390232961164638
06/02/2019 11:05:51 *** evaluating ***
06/02/2019 11:05:52 step: 235, epoch: 234, acc: 49.14529914529914, f1: 27.985481244010334, r: 0.23959097295555742
06/02/2019 11:05:52 *** epoch: 236 ***
06/02/2019 11:05:52 *** training ***
06/02/2019 11:05:54 step: 7760, epoch: 235, batch: 4, loss: 0.0014035552740097046, acc: 100.0, f1: 100.0, r: 0.6927945694202982
06/02/2019 11:05:57 step: 7765, epoch: 235, batch: 9, loss: 0.0015928521752357483, acc: 100.0, f1: 100.0, r: 0.6965016036457483
06/02/2019 11:05:59 step: 7770, epoch: 235, batch: 14, loss: 0.0009894222021102905, acc: 100.0, f1: 100.0, r: 0.6588935526046303
06/02/2019 11:06:01 step: 7775, epoch: 235, batch: 19, loss: 0.002244710922241211, acc: 100.0, f1: 100.0, r: 0.7573217540484762
06/02/2019 11:06:03 step: 7780, epoch: 235, batch: 24, loss: 0.005174584686756134, acc: 100.0, f1: 100.0, r: 0.807876816190759
06/02/2019 11:06:06 step: 7785, epoch: 235, batch: 29, loss: 0.004245884716510773, acc: 100.0, f1: 100.0, r: 0.7175366615029762
06/02/2019 11:06:07 *** evaluating ***
06/02/2019 11:06:08 step: 236, epoch: 235, acc: 49.572649572649574, f1: 28.209698506716265, r: 0.2442934665851248
06/02/2019 11:06:08 *** epoch: 237 ***
06/02/2019 11:06:08 *** training ***
06/02/2019 11:06:10 step: 7793, epoch: 236, batch: 4, loss: 0.00098525732755661, acc: 100.0, f1: 100.0, r: 0.7447441462521112
06/02/2019 11:06:12 step: 7798, epoch: 236, batch: 9, loss: 0.004131235182285309, acc: 100.0, f1: 100.0, r: 0.7746731327573371
06/02/2019 11:06:15 step: 7803, epoch: 236, batch: 14, loss: 0.003446519374847412, acc: 100.0, f1: 100.0, r: 0.7535505497524229
06/02/2019 11:06:17 step: 7808, epoch: 236, batch: 19, loss: 0.0026693344116210938, acc: 100.0, f1: 100.0, r: 0.8047429204708076
06/02/2019 11:06:20 step: 7813, epoch: 236, batch: 24, loss: 0.0017568469047546387, acc: 100.0, f1: 100.0, r: 0.7856360733565316
06/02/2019 11:06:22 step: 7818, epoch: 236, batch: 29, loss: 0.003429114818572998, acc: 100.0, f1: 100.0, r: 0.7344151271495479
06/02/2019 11:06:23 *** evaluating ***
06/02/2019 11:06:24 step: 237, epoch: 236, acc: 50.427350427350426, f1: 28.207920274887243, r: 0.24431007674416458
06/02/2019 11:06:24 *** epoch: 238 ***
06/02/2019 11:06:24 *** training ***
06/02/2019 11:06:27 step: 7826, epoch: 237, batch: 4, loss: 0.0010561347007751465, acc: 100.0, f1: 100.0, r: 0.7696104936601783
06/02/2019 11:06:29 step: 7831, epoch: 237, batch: 9, loss: 0.002700917422771454, acc: 100.0, f1: 100.0, r: 0.6746651670828234
06/02/2019 11:06:31 step: 7836, epoch: 237, batch: 14, loss: 0.002595752477645874, acc: 100.0, f1: 100.0, r: 0.754303016804851
06/02/2019 11:06:34 step: 7841, epoch: 237, batch: 19, loss: 0.0030853748321533203, acc: 100.0, f1: 100.0, r: 0.6662675958435929
06/02/2019 11:06:36 step: 7846, epoch: 237, batch: 24, loss: 0.0010129213333129883, acc: 100.0, f1: 100.0, r: 0.6888829677276385
06/02/2019 11:06:38 step: 7851, epoch: 237, batch: 29, loss: 0.0022734999656677246, acc: 100.0, f1: 100.0, r: 0.6891138828363423
06/02/2019 11:06:40 *** evaluating ***
06/02/2019 11:06:40 step: 238, epoch: 237, acc: 49.14529914529914, f1: 27.923971053520326, r: 0.243065910507368
06/02/2019 11:06:40 *** epoch: 239 ***
06/02/2019 11:06:40 *** training ***
06/02/2019 11:06:43 step: 7859, epoch: 238, batch: 4, loss: 0.003958806395530701, acc: 100.0, f1: 100.0, r: 0.706670717740578
06/02/2019 11:06:46 step: 7864, epoch: 238, batch: 9, loss: 0.002802431583404541, acc: 100.0, f1: 100.0, r: 0.8133071862829011
06/02/2019 11:06:48 step: 7869, epoch: 238, batch: 14, loss: 0.003591589629650116, acc: 100.0, f1: 100.0, r: 0.7080673352398685
06/02/2019 11:06:50 step: 7874, epoch: 238, batch: 19, loss: 0.0013254880905151367, acc: 100.0, f1: 100.0, r: 0.835067755891642
06/02/2019 11:06:52 step: 7879, epoch: 238, batch: 24, loss: 0.0027114227414131165, acc: 100.0, f1: 100.0, r: 0.7615710823114439
06/02/2019 11:06:54 step: 7884, epoch: 238, batch: 29, loss: 0.0036101043224334717, acc: 100.0, f1: 100.0, r: 0.7903000207070998
06/02/2019 11:06:55 *** evaluating ***
06/02/2019 11:06:56 step: 239, epoch: 238, acc: 49.572649572649574, f1: 28.143897403765823, r: 0.2446491601013902
06/02/2019 11:06:56 *** epoch: 240 ***
06/02/2019 11:06:56 *** training ***
06/02/2019 11:06:59 step: 7892, epoch: 239, batch: 4, loss: 0.0017948746681213379, acc: 100.0, f1: 100.0, r: 0.8482073245541673
06/02/2019 11:07:01 step: 7897, epoch: 239, batch: 9, loss: 0.002659432590007782, acc: 100.0, f1: 100.0, r: 0.769563100003853
06/02/2019 11:07:04 step: 7902, epoch: 239, batch: 14, loss: 0.002059362828731537, acc: 100.0, f1: 100.0, r: 0.6901272976386079
06/02/2019 11:07:06 step: 7907, epoch: 239, batch: 19, loss: 0.003193296492099762, acc: 100.0, f1: 100.0, r: 0.8352708539056793
06/02/2019 11:07:08 step: 7912, epoch: 239, batch: 24, loss: 0.001854047179222107, acc: 100.0, f1: 100.0, r: 0.696450790720054
06/02/2019 11:07:10 step: 7917, epoch: 239, batch: 29, loss: 0.0034546256065368652, acc: 100.0, f1: 100.0, r: 0.76980508417954
06/02/2019 11:07:12 *** evaluating ***
06/02/2019 11:07:12 step: 240, epoch: 239, acc: 49.572649572649574, f1: 28.109121055836795, r: 0.24184653770876036
06/02/2019 11:07:12 *** epoch: 241 ***
06/02/2019 11:07:12 *** training ***
06/02/2019 11:07:15 step: 7925, epoch: 240, batch: 4, loss: 0.005397617816925049, acc: 100.0, f1: 100.0, r: 0.791557267221711
06/02/2019 11:07:17 step: 7930, epoch: 240, batch: 9, loss: 0.001706242561340332, acc: 100.0, f1: 100.0, r: 0.7595887878365999
06/02/2019 11:07:19 step: 7935, epoch: 240, batch: 14, loss: 0.002104625105857849, acc: 100.0, f1: 100.0, r: 0.8253444059279865
06/02/2019 11:07:21 step: 7940, epoch: 240, batch: 19, loss: 0.002608850598335266, acc: 100.0, f1: 100.0, r: 0.7845000012272375
06/02/2019 11:07:24 step: 7945, epoch: 240, batch: 24, loss: 0.0011401921510696411, acc: 100.0, f1: 100.0, r: 0.7948199285706705
06/02/2019 11:07:26 step: 7950, epoch: 240, batch: 29, loss: 0.0014393925666809082, acc: 100.0, f1: 100.0, r: 0.7141823490887177
06/02/2019 11:07:27 *** evaluating ***
06/02/2019 11:07:28 step: 241, epoch: 240, acc: 50.0, f1: 28.027375754354956, r: 0.24280145329095168
06/02/2019 11:07:28 *** epoch: 242 ***
06/02/2019 11:07:28 *** training ***
06/02/2019 11:07:30 step: 7958, epoch: 241, batch: 4, loss: 0.004690259695053101, acc: 100.0, f1: 100.0, r: 0.7182870052253248
06/02/2019 11:07:33 step: 7963, epoch: 241, batch: 9, loss: 0.0014502853155136108, acc: 100.0, f1: 100.0, r: 0.8152362650559265
06/02/2019 11:07:35 step: 7968, epoch: 241, batch: 14, loss: 0.0013657286763191223, acc: 100.0, f1: 100.0, r: 0.8244899762182003
06/02/2019 11:07:37 step: 7973, epoch: 241, batch: 19, loss: 0.08065209537744522, acc: 98.4375, f1: 97.6608187134503, r: 0.6869792554486566
06/02/2019 11:07:39 step: 7978, epoch: 241, batch: 24, loss: 0.002148464322090149, acc: 100.0, f1: 100.0, r: 0.7640199846965176
06/02/2019 11:07:41 step: 7983, epoch: 241, batch: 29, loss: 0.0023722872138023376, acc: 100.0, f1: 100.0, r: 0.7490204348133678
06/02/2019 11:07:43 *** evaluating ***
06/02/2019 11:07:44 step: 242, epoch: 241, acc: 51.70940170940172, f1: 28.955835639535877, r: 0.24419821380241785
06/02/2019 11:07:44 *** epoch: 243 ***
06/02/2019 11:07:44 *** training ***
06/02/2019 11:07:46 step: 7991, epoch: 242, batch: 4, loss: 0.06085330247879028, acc: 98.4375, f1: 95.33333333333334, r: 0.7635267883897491
06/02/2019 11:07:48 step: 7996, epoch: 242, batch: 9, loss: 0.0032036229968070984, acc: 100.0, f1: 100.0, r: 0.7204708140766215
06/02/2019 11:07:50 step: 8001, epoch: 242, batch: 14, loss: 0.0024395063519477844, acc: 100.0, f1: 100.0, r: 0.8049666484301197
06/02/2019 11:07:53 step: 8006, epoch: 242, batch: 19, loss: 0.0025261342525482178, acc: 100.0, f1: 100.0, r: 0.7911880911664301
06/02/2019 11:07:55 step: 8011, epoch: 242, batch: 24, loss: 0.002431921660900116, acc: 100.0, f1: 100.0, r: 0.7878378036879887
06/02/2019 11:07:57 step: 8016, epoch: 242, batch: 29, loss: 0.0011294931173324585, acc: 100.0, f1: 100.0, r: 0.7489276896126392
06/02/2019 11:07:59 *** evaluating ***
06/02/2019 11:07:59 step: 243, epoch: 242, acc: 50.85470085470085, f1: 28.620188273499835, r: 0.24469938866494337
06/02/2019 11:07:59 *** epoch: 244 ***
06/02/2019 11:07:59 *** training ***
06/02/2019 11:08:01 step: 8024, epoch: 243, batch: 4, loss: 0.0014466866850852966, acc: 100.0, f1: 100.0, r: 0.7209904725111307
06/02/2019 11:08:04 step: 8029, epoch: 243, batch: 9, loss: 0.0016743838787078857, acc: 100.0, f1: 100.0, r: 0.6781097741973418
06/02/2019 11:08:06 step: 8034, epoch: 243, batch: 14, loss: 0.0031153038144111633, acc: 100.0, f1: 100.0, r: 0.8615472680695908
06/02/2019 11:08:08 step: 8039, epoch: 243, batch: 19, loss: 0.001886412501335144, acc: 100.0, f1: 100.0, r: 0.6543589042846731
06/02/2019 11:08:10 step: 8044, epoch: 243, batch: 24, loss: 0.002484351396560669, acc: 100.0, f1: 100.0, r: 0.720000643003608
06/02/2019 11:08:13 step: 8049, epoch: 243, batch: 29, loss: 0.0006082803010940552, acc: 100.0, f1: 100.0, r: 0.7149364207208795
06/02/2019 11:08:14 *** evaluating ***
06/02/2019 11:08:15 step: 244, epoch: 243, acc: 50.427350427350426, f1: 28.75080627159523, r: 0.2465015887297051
06/02/2019 11:08:15 *** epoch: 245 ***
06/02/2019 11:08:15 *** training ***
06/02/2019 11:08:17 step: 8057, epoch: 244, batch: 4, loss: 0.004727557301521301, acc: 100.0, f1: 100.0, r: 0.7521511246301067
06/02/2019 11:08:20 step: 8062, epoch: 244, batch: 9, loss: 0.0022661685943603516, acc: 100.0, f1: 100.0, r: 0.6203947301174749
06/02/2019 11:08:22 step: 8067, epoch: 244, batch: 14, loss: 0.0014202892780303955, acc: 100.0, f1: 100.0, r: 0.7415533981021338
06/02/2019 11:08:24 step: 8072, epoch: 244, batch: 19, loss: 0.001048155128955841, acc: 100.0, f1: 100.0, r: 0.7973634419230884
06/02/2019 11:08:26 step: 8077, epoch: 244, batch: 24, loss: 0.0027967169880867004, acc: 100.0, f1: 100.0, r: 0.7068817583370088
06/02/2019 11:08:28 step: 8082, epoch: 244, batch: 29, loss: 0.0028475746512413025, acc: 100.0, f1: 100.0, r: 0.7007399169610167
06/02/2019 11:08:30 *** evaluating ***
06/02/2019 11:08:31 step: 245, epoch: 244, acc: 50.85470085470085, f1: 28.24242619161269, r: 0.23704086494104606
06/02/2019 11:08:31 *** epoch: 246 ***
06/02/2019 11:08:31 *** training ***
06/02/2019 11:08:33 step: 8090, epoch: 245, batch: 4, loss: 0.004041619598865509, acc: 100.0, f1: 100.0, r: 0.684749750648052
06/02/2019 11:08:35 step: 8095, epoch: 245, batch: 9, loss: 0.1737724244594574, acc: 98.4375, f1: 96.66666666666667, r: 0.7965177974801365
06/02/2019 11:08:37 step: 8100, epoch: 245, batch: 14, loss: 0.0014402121305465698, acc: 100.0, f1: 100.0, r: 0.6220822795169727
06/02/2019 11:08:39 step: 8105, epoch: 245, batch: 19, loss: 0.000951230525970459, acc: 100.0, f1: 100.0, r: 0.7931577402737958
06/02/2019 11:08:42 step: 8110, epoch: 245, batch: 24, loss: 0.0030595138669013977, acc: 100.0, f1: 100.0, r: 0.7083550064537838
06/02/2019 11:08:44 step: 8115, epoch: 245, batch: 29, loss: 0.0020241141319274902, acc: 100.0, f1: 100.0, r: 0.6929595762122804
06/02/2019 11:08:45 *** evaluating ***
06/02/2019 11:08:45 step: 246, epoch: 245, acc: 50.427350427350426, f1: 28.47527095195285, r: 0.23942515731489755
06/02/2019 11:08:45 *** epoch: 247 ***
06/02/2019 11:08:45 *** training ***
06/02/2019 11:08:48 step: 8123, epoch: 246, batch: 4, loss: 0.0018909722566604614, acc: 100.0, f1: 100.0, r: 0.795432243167141
06/02/2019 11:08:50 step: 8128, epoch: 246, batch: 9, loss: 0.001900218427181244, acc: 100.0, f1: 100.0, r: 0.7676503129071331
06/02/2019 11:08:53 step: 8133, epoch: 246, batch: 14, loss: 0.0012226253747940063, acc: 100.0, f1: 100.0, r: 0.7917219551863467
06/02/2019 11:08:54 step: 8138, epoch: 246, batch: 19, loss: 0.006428927183151245, acc: 100.0, f1: 100.0, r: 0.7028702395297338
06/02/2019 11:08:56 step: 8143, epoch: 246, batch: 24, loss: 0.029366977512836456, acc: 98.4375, f1: 99.1140642303433, r: 0.7949209118565557
06/02/2019 11:08:59 step: 8148, epoch: 246, batch: 29, loss: 0.002529740333557129, acc: 100.0, f1: 100.0, r: 0.7142164125181572
06/02/2019 11:09:00 *** evaluating ***
06/02/2019 11:09:01 step: 247, epoch: 246, acc: 50.427350427350426, f1: 28.35071451562254, r: 0.25851542213734413
06/02/2019 11:09:01 *** epoch: 248 ***
06/02/2019 11:09:01 *** training ***
06/02/2019 11:09:03 step: 8156, epoch: 247, batch: 4, loss: 0.0023827552795410156, acc: 100.0, f1: 100.0, r: 0.8166606390390719
06/02/2019 11:09:06 step: 8161, epoch: 247, batch: 9, loss: 0.002974003553390503, acc: 100.0, f1: 100.0, r: 0.6674924675011552
06/02/2019 11:09:08 step: 8166, epoch: 247, batch: 14, loss: 0.004350967705249786, acc: 100.0, f1: 100.0, r: 0.7660552392433982
06/02/2019 11:09:10 step: 8171, epoch: 247, batch: 19, loss: 0.0011036843061447144, acc: 100.0, f1: 100.0, r: 0.6974403757544205
06/02/2019 11:09:12 step: 8176, epoch: 247, batch: 24, loss: 0.0019186362624168396, acc: 100.0, f1: 100.0, r: 0.7194047213173386
06/02/2019 11:09:14 step: 8181, epoch: 247, batch: 29, loss: 0.003300219774246216, acc: 100.0, f1: 100.0, r: 0.7934540308042345
06/02/2019 11:09:15 *** evaluating ***
06/02/2019 11:09:16 step: 248, epoch: 247, acc: 51.70940170940172, f1: 29.67520919749429, r: 0.26733453639093924
06/02/2019 11:09:16 *** epoch: 249 ***
06/02/2019 11:09:16 *** training ***
06/02/2019 11:09:18 step: 8189, epoch: 248, batch: 4, loss: 0.0030890554189682007, acc: 100.0, f1: 100.0, r: 0.8336949566769686
06/02/2019 11:09:20 step: 8194, epoch: 248, batch: 9, loss: 0.0011401772499084473, acc: 100.0, f1: 100.0, r: 0.5762456612189413
06/02/2019 11:09:22 step: 8199, epoch: 248, batch: 14, loss: 0.0021028220653533936, acc: 100.0, f1: 100.0, r: 0.7315098783294407
06/02/2019 11:09:25 step: 8204, epoch: 248, batch: 19, loss: 0.00299641489982605, acc: 100.0, f1: 100.0, r: 0.8064595481921861
06/02/2019 11:09:27 step: 8209, epoch: 248, batch: 24, loss: 0.003607608377933502, acc: 100.0, f1: 100.0, r: 0.6789634062481786
06/02/2019 11:09:29 step: 8214, epoch: 248, batch: 29, loss: 0.0020350664854049683, acc: 100.0, f1: 100.0, r: 0.693847083423401
06/02/2019 11:09:31 *** evaluating ***
06/02/2019 11:09:31 step: 249, epoch: 248, acc: 50.0, f1: 29.28239291607112, r: 0.2638810796458835
06/02/2019 11:09:31 *** epoch: 250 ***
06/02/2019 11:09:31 *** training ***
06/02/2019 11:09:34 step: 8222, epoch: 249, batch: 4, loss: 0.002415306866168976, acc: 100.0, f1: 100.0, r: 0.7487657833009662
06/02/2019 11:09:36 step: 8227, epoch: 249, batch: 9, loss: 0.0016123950481414795, acc: 100.0, f1: 100.0, r: 0.7039666350378401
06/02/2019 11:09:38 step: 8232, epoch: 249, batch: 14, loss: 0.0017545223236083984, acc: 100.0, f1: 100.0, r: 0.6903987999351987
06/02/2019 11:09:41 step: 8237, epoch: 249, batch: 19, loss: 0.0021185949444770813, acc: 100.0, f1: 100.0, r: 0.6967568254007129
06/02/2019 11:09:43 step: 8242, epoch: 249, batch: 24, loss: 0.0011862069368362427, acc: 100.0, f1: 100.0, r: 0.7509743251287245
06/02/2019 11:09:45 step: 8247, epoch: 249, batch: 29, loss: 0.0008917748928070068, acc: 100.0, f1: 100.0, r: 0.6173237668404495
06/02/2019 11:09:46 *** evaluating ***
06/02/2019 11:09:47 step: 250, epoch: 249, acc: 52.13675213675214, f1: 29.691378367856924, r: 0.2612950951202206
06/02/2019 11:09:47 *** epoch: 251 ***
06/02/2019 11:09:47 *** training ***
06/02/2019 11:09:49 step: 8255, epoch: 250, batch: 4, loss: 0.001216992735862732, acc: 100.0, f1: 100.0, r: 0.6820921686041265
06/02/2019 11:09:51 step: 8260, epoch: 250, batch: 9, loss: 0.0011112913489341736, acc: 100.0, f1: 100.0, r: 0.8388379740733451
06/02/2019 11:09:53 step: 8265, epoch: 250, batch: 14, loss: 0.001443326473236084, acc: 100.0, f1: 100.0, r: 0.8099841964512096
06/02/2019 11:09:55 step: 8270, epoch: 250, batch: 19, loss: 0.005741797387599945, acc: 100.0, f1: 100.0, r: 0.8078628900372271
06/02/2019 11:09:58 step: 8275, epoch: 250, batch: 24, loss: 0.0016337484121322632, acc: 100.0, f1: 100.0, r: 0.8038912985429119
06/02/2019 11:10:00 step: 8280, epoch: 250, batch: 29, loss: 0.0012183934450149536, acc: 100.0, f1: 100.0, r: 0.6690674042797294
06/02/2019 11:10:01 *** evaluating ***
06/02/2019 11:10:02 step: 251, epoch: 250, acc: 50.85470085470085, f1: 29.767807448006288, r: 0.2552984269893382
06/02/2019 11:10:02 *** epoch: 252 ***
06/02/2019 11:10:02 *** training ***
06/02/2019 11:10:04 step: 8288, epoch: 251, batch: 4, loss: 0.005189612507820129, acc: 100.0, f1: 100.0, r: 0.7336366863991164
06/02/2019 11:10:07 step: 8293, epoch: 251, batch: 9, loss: 0.0013804659247398376, acc: 100.0, f1: 100.0, r: 0.6823815814715166
06/02/2019 11:10:09 step: 8298, epoch: 251, batch: 14, loss: 0.0013982728123664856, acc: 100.0, f1: 100.0, r: 0.7568141003514153
06/02/2019 11:10:11 step: 8303, epoch: 251, batch: 19, loss: 0.0009382814168930054, acc: 100.0, f1: 100.0, r: 0.7542892870126497
06/02/2019 11:10:13 step: 8308, epoch: 251, batch: 24, loss: 0.001090504229068756, acc: 100.0, f1: 100.0, r: 0.7018805959670879
06/02/2019 11:10:16 step: 8313, epoch: 251, batch: 29, loss: 0.0014614015817642212, acc: 100.0, f1: 100.0, r: 0.69859136926517
06/02/2019 11:10:17 *** evaluating ***
06/02/2019 11:10:18 step: 252, epoch: 251, acc: 50.0, f1: 28.758278821845117, r: 0.2508014208678826
06/02/2019 11:10:18 *** epoch: 253 ***
06/02/2019 11:10:18 *** training ***
06/02/2019 11:10:20 step: 8321, epoch: 252, batch: 4, loss: 0.0036896243691444397, acc: 100.0, f1: 100.0, r: 0.699987107086704
06/02/2019 11:10:23 step: 8326, epoch: 252, batch: 9, loss: 0.0024928227066993713, acc: 100.0, f1: 100.0, r: 0.6828534339774459
06/02/2019 11:10:25 step: 8331, epoch: 252, batch: 14, loss: 0.002290666103363037, acc: 100.0, f1: 100.0, r: 0.693611113523943
06/02/2019 11:10:27 step: 8336, epoch: 252, batch: 19, loss: 0.004064843058586121, acc: 100.0, f1: 100.0, r: 0.7316246902743654
06/02/2019 11:10:29 step: 8341, epoch: 252, batch: 24, loss: 0.0030833110213279724, acc: 100.0, f1: 100.0, r: 0.7808838674343032
06/02/2019 11:10:32 step: 8346, epoch: 252, batch: 29, loss: 0.003944404423236847, acc: 100.0, f1: 100.0, r: 0.7753356404436373
06/02/2019 11:10:33 *** evaluating ***
06/02/2019 11:10:34 step: 253, epoch: 252, acc: 50.0, f1: 28.660797389907426, r: 0.24435143819574726
06/02/2019 11:10:34 *** epoch: 254 ***
06/02/2019 11:10:34 *** training ***
06/02/2019 11:10:36 step: 8354, epoch: 253, batch: 4, loss: 0.0010466501116752625, acc: 100.0, f1: 100.0, r: 0.6873062961300201
06/02/2019 11:10:38 step: 8359, epoch: 253, batch: 9, loss: 0.0010120794177055359, acc: 100.0, f1: 100.0, r: 0.7156216897804802
06/02/2019 11:10:40 step: 8364, epoch: 253, batch: 14, loss: 0.003151997923851013, acc: 100.0, f1: 100.0, r: 0.737924913087742
06/02/2019 11:10:43 step: 8369, epoch: 253, batch: 19, loss: 0.0009805560111999512, acc: 100.0, f1: 100.0, r: 0.6899269302262573
06/02/2019 11:10:45 step: 8374, epoch: 253, batch: 24, loss: 0.004726380109786987, acc: 100.0, f1: 100.0, r: 0.7005057197385942
06/02/2019 11:10:47 step: 8379, epoch: 253, batch: 29, loss: 0.0019269585609436035, acc: 100.0, f1: 100.0, r: 0.7179133157727078
06/02/2019 11:10:49 *** evaluating ***
06/02/2019 11:10:49 step: 254, epoch: 253, acc: 50.0, f1: 28.801891590386532, r: 0.24990000468746743
06/02/2019 11:10:49 *** epoch: 255 ***
06/02/2019 11:10:49 *** training ***
06/02/2019 11:10:51 step: 8387, epoch: 254, batch: 4, loss: 0.004391804337501526, acc: 100.0, f1: 100.0, r: 0.8450917761580028
06/02/2019 11:10:54 step: 8392, epoch: 254, batch: 9, loss: 0.004472449421882629, acc: 100.0, f1: 100.0, r: 0.7089057750608213
06/02/2019 11:10:56 step: 8397, epoch: 254, batch: 14, loss: 0.0013640299439430237, acc: 100.0, f1: 100.0, r: 0.8034200960209306
06/02/2019 11:10:58 step: 8402, epoch: 254, batch: 19, loss: 0.0008682012557983398, acc: 100.0, f1: 100.0, r: 0.7977165945400594
06/02/2019 11:11:00 step: 8407, epoch: 254, batch: 24, loss: 0.002735927700996399, acc: 100.0, f1: 100.0, r: 0.7103739526218278
06/02/2019 11:11:03 step: 8412, epoch: 254, batch: 29, loss: 0.004110462963581085, acc: 100.0, f1: 100.0, r: 0.7563495124302259
06/02/2019 11:11:04 *** evaluating ***
06/02/2019 11:11:05 step: 255, epoch: 254, acc: 49.572649572649574, f1: 28.173403850934243, r: 0.24383137832879112
06/02/2019 11:11:05 *** epoch: 256 ***
06/02/2019 11:11:05 *** training ***
06/02/2019 11:11:07 step: 8420, epoch: 255, batch: 4, loss: 0.0022029131650924683, acc: 100.0, f1: 100.0, r: 0.8322073171105003
06/02/2019 11:11:09 step: 8425, epoch: 255, batch: 9, loss: 0.0013281479477882385, acc: 100.0, f1: 100.0, r: 0.7541342741810417
06/02/2019 11:11:11 step: 8430, epoch: 255, batch: 14, loss: 0.002410873770713806, acc: 100.0, f1: 100.0, r: 0.7195828479300218
06/02/2019 11:11:14 step: 8435, epoch: 255, batch: 19, loss: 0.0029658228158950806, acc: 100.0, f1: 100.0, r: 0.7417038175090473
06/02/2019 11:11:17 step: 8440, epoch: 255, batch: 24, loss: 0.00043232738971710205, acc: 100.0, f1: 100.0, r: 0.7930245121041759
06/02/2019 11:11:19 step: 8445, epoch: 255, batch: 29, loss: 0.0014001652598381042, acc: 100.0, f1: 100.0, r: 0.7334751246032681
06/02/2019 11:11:20 *** evaluating ***
06/02/2019 11:11:21 step: 256, epoch: 255, acc: 50.427350427350426, f1: 29.564210647636912, r: 0.25443819110883426
06/02/2019 11:11:21 *** epoch: 257 ***
06/02/2019 11:11:21 *** training ***
06/02/2019 11:11:23 step: 8453, epoch: 256, batch: 4, loss: 0.004954293370246887, acc: 100.0, f1: 100.0, r: 0.7955784659254924
06/02/2019 11:11:25 step: 8458, epoch: 256, batch: 9, loss: 0.0022167563438415527, acc: 100.0, f1: 100.0, r: 0.7863798715563085
06/02/2019 11:11:27 step: 8463, epoch: 256, batch: 14, loss: 0.0013628453016281128, acc: 100.0, f1: 100.0, r: 0.78697035454882
06/02/2019 11:11:30 step: 8468, epoch: 256, batch: 19, loss: 0.0007267296314239502, acc: 100.0, f1: 100.0, r: 0.8195532658406945
06/02/2019 11:11:32 step: 8473, epoch: 256, batch: 24, loss: 0.0012927502393722534, acc: 100.0, f1: 100.0, r: 0.715468378273507
06/02/2019 11:11:35 step: 8478, epoch: 256, batch: 29, loss: 0.0012781620025634766, acc: 100.0, f1: 100.0, r: 0.7980559850038413
06/02/2019 11:11:36 *** evaluating ***
06/02/2019 11:11:37 step: 257, epoch: 256, acc: 50.85470085470085, f1: 28.57070545609548, r: 0.2523574374084045
06/02/2019 11:11:37 *** epoch: 258 ***
06/02/2019 11:11:37 *** training ***
06/02/2019 11:11:39 step: 8486, epoch: 257, batch: 4, loss: 0.0009834319353103638, acc: 100.0, f1: 100.0, r: 0.6726690890163678
06/02/2019 11:11:41 step: 8491, epoch: 257, batch: 9, loss: 0.002108238637447357, acc: 100.0, f1: 100.0, r: 0.7943928298795477
06/02/2019 11:11:44 step: 8496, epoch: 257, batch: 14, loss: 0.0011633485555648804, acc: 100.0, f1: 100.0, r: 0.7056938162951687
06/02/2019 11:11:46 step: 8501, epoch: 257, batch: 19, loss: 0.03357608988881111, acc: 98.4375, f1: 99.15212461796312, r: 0.692188645046468
06/02/2019 11:11:48 step: 8506, epoch: 257, batch: 24, loss: 0.0015625730156898499, acc: 100.0, f1: 100.0, r: 0.7131168349146751
06/02/2019 11:11:50 step: 8511, epoch: 257, batch: 29, loss: 0.005946196615695953, acc: 100.0, f1: 100.0, r: 0.6650274685821281
06/02/2019 11:11:51 *** evaluating ***
06/02/2019 11:11:52 step: 258, epoch: 257, acc: 50.85470085470085, f1: 28.951133796335032, r: 0.2517707891156413
06/02/2019 11:11:52 *** epoch: 259 ***
06/02/2019 11:11:52 *** training ***
06/02/2019 11:11:54 step: 8519, epoch: 258, batch: 4, loss: 0.0012430399656295776, acc: 100.0, f1: 100.0, r: 0.8073473079262797
06/02/2019 11:11:56 step: 8524, epoch: 258, batch: 9, loss: 0.0047630369663238525, acc: 100.0, f1: 100.0, r: 0.7101837668824229
06/02/2019 11:11:59 step: 8529, epoch: 258, batch: 14, loss: 0.001163974404335022, acc: 100.0, f1: 100.0, r: 0.6451860770252159
06/02/2019 11:12:02 step: 8534, epoch: 258, batch: 19, loss: 0.0010519027709960938, acc: 100.0, f1: 100.0, r: 0.6640020431746467
06/02/2019 11:12:04 step: 8539, epoch: 258, batch: 24, loss: 0.0019590556621551514, acc: 100.0, f1: 100.0, r: 0.7310285510068051
06/02/2019 11:12:06 step: 8544, epoch: 258, batch: 29, loss: 0.003916621208190918, acc: 100.0, f1: 100.0, r: 0.8076913776875729
06/02/2019 11:12:07 *** evaluating ***
06/02/2019 11:12:07 step: 259, epoch: 258, acc: 51.28205128205128, f1: 29.17833400483481, r: 0.24999792270048946
06/02/2019 11:12:07 *** epoch: 260 ***
06/02/2019 11:12:07 *** training ***
06/02/2019 11:12:10 step: 8552, epoch: 259, batch: 4, loss: 0.00323331356048584, acc: 100.0, f1: 100.0, r: 0.7033431794268812
06/02/2019 11:12:12 step: 8557, epoch: 259, batch: 9, loss: 0.0017146170139312744, acc: 100.0, f1: 100.0, r: 0.6604090057655212
06/02/2019 11:12:14 step: 8562, epoch: 259, batch: 14, loss: 0.003329448401927948, acc: 100.0, f1: 100.0, r: 0.72123941303641
06/02/2019 11:12:17 step: 8567, epoch: 259, batch: 19, loss: 0.0016972273588180542, acc: 100.0, f1: 100.0, r: 0.7711727374510068
06/02/2019 11:12:19 step: 8572, epoch: 259, batch: 24, loss: 0.006159387528896332, acc: 100.0, f1: 100.0, r: 0.7611263831985201
06/02/2019 11:12:21 step: 8577, epoch: 259, batch: 29, loss: 0.0010553449392318726, acc: 100.0, f1: 100.0, r: 0.8249202264552635
06/02/2019 11:12:22 *** evaluating ***
06/02/2019 11:12:23 step: 260, epoch: 259, acc: 50.427350427350426, f1: 28.58478062697308, r: 0.25164016431358494
06/02/2019 11:12:23 *** epoch: 261 ***
06/02/2019 11:12:23 *** training ***
06/02/2019 11:12:25 step: 8585, epoch: 260, batch: 4, loss: 0.0028063803911209106, acc: 100.0, f1: 100.0, r: 0.7559472866213679
06/02/2019 11:12:28 step: 8590, epoch: 260, batch: 9, loss: 0.0015159845352172852, acc: 100.0, f1: 100.0, r: 0.8252381056286616
06/02/2019 11:12:30 step: 8595, epoch: 260, batch: 14, loss: 0.0038658753037452698, acc: 100.0, f1: 100.0, r: 0.7798054389567843
06/02/2019 11:12:32 step: 8600, epoch: 260, batch: 19, loss: 0.004596881568431854, acc: 100.0, f1: 100.0, r: 0.6908018997326842
06/02/2019 11:12:36 step: 8605, epoch: 260, batch: 24, loss: 0.0020955950021743774, acc: 100.0, f1: 100.0, r: 0.793722779661892
06/02/2019 11:12:37 step: 8610, epoch: 260, batch: 29, loss: 0.003599025309085846, acc: 100.0, f1: 100.0, r: 0.8017991994550235
06/02/2019 11:12:38 *** evaluating ***
06/02/2019 11:12:39 step: 261, epoch: 260, acc: 49.572649572649574, f1: 28.58332191572281, r: 0.24251299373728105
06/02/2019 11:12:39 *** epoch: 262 ***
06/02/2019 11:12:39 *** training ***
06/02/2019 11:12:42 step: 8618, epoch: 261, batch: 4, loss: 0.0008569657802581787, acc: 100.0, f1: 100.0, r: 0.6618246277903695
06/02/2019 11:12:44 step: 8623, epoch: 261, batch: 9, loss: 0.0023775696754455566, acc: 100.0, f1: 100.0, r: 0.8341485455716058
06/02/2019 11:12:46 step: 8628, epoch: 261, batch: 14, loss: 0.0010596811771392822, acc: 100.0, f1: 100.0, r: 0.7713594844480482
06/02/2019 11:12:49 step: 8633, epoch: 261, batch: 19, loss: 0.0021766051650047302, acc: 100.0, f1: 100.0, r: 0.7344356464310725
06/02/2019 11:12:51 step: 8638, epoch: 261, batch: 24, loss: 0.0029041990637779236, acc: 100.0, f1: 100.0, r: 0.6817359120497561
06/02/2019 11:12:53 step: 8643, epoch: 261, batch: 29, loss: 0.0018303319811820984, acc: 100.0, f1: 100.0, r: 0.7392790380333635
06/02/2019 11:12:54 *** evaluating ***
06/02/2019 11:12:55 step: 262, epoch: 261, acc: 50.427350427350426, f1: 28.684735641610853, r: 0.24553243789364254
06/02/2019 11:12:55 *** epoch: 263 ***
06/02/2019 11:12:55 *** training ***
06/02/2019 11:12:57 step: 8651, epoch: 262, batch: 4, loss: 0.002045169472694397, acc: 100.0, f1: 100.0, r: 0.8069613318277825
06/02/2019 11:13:00 step: 8656, epoch: 262, batch: 9, loss: 0.0012596100568771362, acc: 100.0, f1: 100.0, r: 0.8049581078857483
06/02/2019 11:13:02 step: 8661, epoch: 262, batch: 14, loss: 0.0008569806814193726, acc: 100.0, f1: 100.0, r: 0.7910467336689249
06/02/2019 11:13:05 step: 8666, epoch: 262, batch: 19, loss: 0.0030914396047592163, acc: 100.0, f1: 100.0, r: 0.7076272531418258
06/02/2019 11:13:07 step: 8671, epoch: 262, batch: 24, loss: 0.0018531084060668945, acc: 100.0, f1: 100.0, r: 0.737148292429106
06/02/2019 11:13:09 step: 8676, epoch: 262, batch: 29, loss: 0.0015750676393508911, acc: 100.0, f1: 100.0, r: 0.8430722650720776
06/02/2019 11:13:10 *** evaluating ***
06/02/2019 11:13:11 step: 263, epoch: 262, acc: 50.0, f1: 28.098387680431024, r: 0.23812242990813196
06/02/2019 11:13:11 *** epoch: 264 ***
06/02/2019 11:13:11 *** training ***
06/02/2019 11:13:13 step: 8684, epoch: 263, batch: 4, loss: 0.002354264259338379, acc: 100.0, f1: 100.0, r: 0.7595949664710341
06/02/2019 11:13:16 step: 8689, epoch: 263, batch: 9, loss: 0.002415582537651062, acc: 100.0, f1: 100.0, r: 0.7124871553614804
06/02/2019 11:13:18 step: 8694, epoch: 263, batch: 14, loss: 0.0008200556039810181, acc: 100.0, f1: 100.0, r: 0.7545318563711977
06/02/2019 11:13:20 step: 8699, epoch: 263, batch: 19, loss: 0.0030274391174316406, acc: 100.0, f1: 100.0, r: 0.7397523787793973
06/02/2019 11:13:23 step: 8704, epoch: 263, batch: 24, loss: 0.0033155158162117004, acc: 100.0, f1: 100.0, r: 0.8203066448622702
06/02/2019 11:13:25 step: 8709, epoch: 263, batch: 29, loss: 0.00180034339427948, acc: 100.0, f1: 100.0, r: 0.8065379920970099
06/02/2019 11:13:26 *** evaluating ***
06/02/2019 11:13:27 step: 264, epoch: 263, acc: 49.14529914529914, f1: 28.017811029558725, r: 0.23878219947396792
06/02/2019 11:13:27 *** epoch: 265 ***
06/02/2019 11:13:27 *** training ***
06/02/2019 11:13:29 step: 8717, epoch: 264, batch: 4, loss: 0.0053319185972213745, acc: 100.0, f1: 100.0, r: 0.6770382035106066
06/02/2019 11:13:31 step: 8722, epoch: 264, batch: 9, loss: 0.0021898671984672546, acc: 100.0, f1: 100.0, r: 0.7161187627857086
06/02/2019 11:13:34 step: 8727, epoch: 264, batch: 14, loss: 0.0018470510840415955, acc: 100.0, f1: 100.0, r: 0.7337353801093339
06/02/2019 11:13:36 step: 8732, epoch: 264, batch: 19, loss: 0.001519232988357544, acc: 100.0, f1: 100.0, r: 0.716628474846074
06/02/2019 11:13:39 step: 8737, epoch: 264, batch: 24, loss: 0.004713766276836395, acc: 100.0, f1: 100.0, r: 0.78476010745908
06/02/2019 11:13:41 step: 8742, epoch: 264, batch: 29, loss: 0.001964777708053589, acc: 100.0, f1: 100.0, r: 0.7726746007460392
06/02/2019 11:13:42 *** evaluating ***
06/02/2019 11:13:43 step: 265, epoch: 264, acc: 50.427350427350426, f1: 28.497964403152565, r: 0.2418867127407618
06/02/2019 11:13:43 *** epoch: 266 ***
06/02/2019 11:13:43 *** training ***
06/02/2019 11:13:45 step: 8750, epoch: 265, batch: 4, loss: 0.00407882034778595, acc: 100.0, f1: 100.0, r: 0.6796589433671363
06/02/2019 11:13:48 step: 8755, epoch: 265, batch: 9, loss: 0.0017209872603416443, acc: 100.0, f1: 100.0, r: 0.7827946522213365
06/02/2019 11:13:50 step: 8760, epoch: 265, batch: 14, loss: 0.0016083866357803345, acc: 100.0, f1: 100.0, r: 0.7771722500819015
06/02/2019 11:13:52 step: 8765, epoch: 265, batch: 19, loss: 0.0025360286235809326, acc: 100.0, f1: 100.0, r: 0.7018682438605212
06/02/2019 11:13:55 step: 8770, epoch: 265, batch: 24, loss: 0.00249452143907547, acc: 100.0, f1: 100.0, r: 0.7995223826322039
06/02/2019 11:13:56 step: 8775, epoch: 265, batch: 29, loss: 0.0016278252005577087, acc: 100.0, f1: 100.0, r: 0.8075106996437392
06/02/2019 11:13:58 *** evaluating ***
06/02/2019 11:13:58 step: 266, epoch: 265, acc: 50.427350427350426, f1: 28.604054079886637, r: 0.24152071271456962
06/02/2019 11:13:58 *** epoch: 267 ***
06/02/2019 11:13:58 *** training ***
06/02/2019 11:14:00 step: 8783, epoch: 266, batch: 4, loss: 0.0012311190366744995, acc: 100.0, f1: 100.0, r: 0.681707341873891
06/02/2019 11:14:03 step: 8788, epoch: 266, batch: 9, loss: 0.0013270676136016846, acc: 100.0, f1: 100.0, r: 0.7366559017123986
06/02/2019 11:14:05 step: 8793, epoch: 266, batch: 14, loss: 0.0006914958357810974, acc: 100.0, f1: 100.0, r: 0.6694123675838166
06/02/2019 11:14:07 step: 8798, epoch: 266, batch: 19, loss: 0.0011533349752426147, acc: 100.0, f1: 100.0, r: 0.6885236794874566
06/02/2019 11:14:09 step: 8803, epoch: 266, batch: 24, loss: 0.003921039402484894, acc: 100.0, f1: 100.0, r: 0.7584074172917616
06/02/2019 11:14:11 step: 8808, epoch: 266, batch: 29, loss: 0.0025764629244804382, acc: 100.0, f1: 100.0, r: 0.6827623274562136
06/02/2019 11:14:13 *** evaluating ***
06/02/2019 11:14:14 step: 267, epoch: 266, acc: 50.0, f1: 28.651375179839317, r: 0.24110579371437224
06/02/2019 11:14:14 *** epoch: 268 ***
06/02/2019 11:14:14 *** training ***
06/02/2019 11:14:16 step: 8816, epoch: 267, batch: 4, loss: 0.001028403639793396, acc: 100.0, f1: 100.0, r: 0.7242086868657166
06/02/2019 11:14:19 step: 8821, epoch: 267, batch: 9, loss: 0.001426801085472107, acc: 100.0, f1: 100.0, r: 0.7380852360317655
06/02/2019 11:14:21 step: 8826, epoch: 267, batch: 14, loss: 0.001981392502784729, acc: 100.0, f1: 100.0, r: 0.7177142544662544
06/02/2019 11:14:23 step: 8831, epoch: 267, batch: 19, loss: 0.0024436861276626587, acc: 100.0, f1: 100.0, r: 0.7429882241316789
06/02/2019 11:14:25 step: 8836, epoch: 267, batch: 24, loss: 0.0022901594638824463, acc: 100.0, f1: 100.0, r: 0.7760536992660729
06/02/2019 11:14:28 step: 8841, epoch: 267, batch: 29, loss: 0.0011462420225143433, acc: 100.0, f1: 100.0, r: 0.7284782026208086
06/02/2019 11:14:29 *** evaluating ***
06/02/2019 11:14:29 step: 268, epoch: 267, acc: 50.427350427350426, f1: 28.64932570966531, r: 0.24234390458795196
06/02/2019 11:14:29 *** epoch: 269 ***
06/02/2019 11:14:29 *** training ***
06/02/2019 11:14:32 step: 8849, epoch: 268, batch: 4, loss: 0.0016112998127937317, acc: 100.0, f1: 100.0, r: 0.8337354046933028
06/02/2019 11:14:34 step: 8854, epoch: 268, batch: 9, loss: 0.0029452666640281677, acc: 100.0, f1: 100.0, r: 0.6430497151381462
06/02/2019 11:14:36 step: 8859, epoch: 268, batch: 14, loss: 0.0018213242292404175, acc: 100.0, f1: 100.0, r: 0.7794437116803785
06/02/2019 11:14:38 step: 8864, epoch: 268, batch: 19, loss: 0.0014925673604011536, acc: 100.0, f1: 100.0, r: 0.660956781562296
06/02/2019 11:14:41 step: 8869, epoch: 268, batch: 24, loss: 0.003579452633857727, acc: 100.0, f1: 100.0, r: 0.7634022433070944
06/02/2019 11:14:43 step: 8874, epoch: 268, batch: 29, loss: 0.0030751079320907593, acc: 100.0, f1: 100.0, r: 0.7520669537788754
06/02/2019 11:14:44 *** evaluating ***
06/02/2019 11:14:45 step: 269, epoch: 268, acc: 50.85470085470085, f1: 29.0020600314718, r: 0.2385042974107909
06/02/2019 11:14:45 *** epoch: 270 ***
06/02/2019 11:14:45 *** training ***
06/02/2019 11:14:47 step: 8882, epoch: 269, batch: 4, loss: 0.0006539076566696167, acc: 100.0, f1: 100.0, r: 0.7754552894601205
06/02/2019 11:14:49 step: 8887, epoch: 269, batch: 9, loss: 0.0011221915483474731, acc: 100.0, f1: 100.0, r: 0.7184852752376228
06/02/2019 11:14:51 step: 8892, epoch: 269, batch: 14, loss: 0.002584882080554962, acc: 100.0, f1: 100.0, r: 0.8053548036380722
06/02/2019 11:14:53 step: 8897, epoch: 269, batch: 19, loss: 0.0019746124744415283, acc: 100.0, f1: 100.0, r: 0.7103381927267532
06/02/2019 11:14:56 step: 8902, epoch: 269, batch: 24, loss: 0.0034821555018424988, acc: 100.0, f1: 100.0, r: 0.7031406090055329
06/02/2019 11:14:58 step: 8907, epoch: 269, batch: 29, loss: 0.0008389800786972046, acc: 100.0, f1: 100.0, r: 0.701771016610639
06/02/2019 11:14:59 *** evaluating ***
06/02/2019 11:15:00 step: 270, epoch: 269, acc: 50.427350427350426, f1: 28.664276241008167, r: 0.24118020424457137
06/02/2019 11:15:00 *** epoch: 271 ***
06/02/2019 11:15:00 *** training ***
06/02/2019 11:15:02 step: 8915, epoch: 270, batch: 4, loss: 0.0006523877382278442, acc: 100.0, f1: 100.0, r: 0.6996164739215447
06/02/2019 11:15:05 step: 8920, epoch: 270, batch: 9, loss: 0.0015479177236557007, acc: 100.0, f1: 100.0, r: 0.7399258662231648
06/02/2019 11:15:07 step: 8925, epoch: 270, batch: 14, loss: 0.002819366753101349, acc: 100.0, f1: 100.0, r: 0.6727154014219167
06/02/2019 11:15:09 step: 8930, epoch: 270, batch: 19, loss: 0.012258820235729218, acc: 98.4375, f1: 97.23404255319149, r: 0.7849239325910741
06/02/2019 11:15:12 step: 8935, epoch: 270, batch: 24, loss: 0.0010053813457489014, acc: 100.0, f1: 100.0, r: 0.7622416935555398
06/02/2019 11:15:14 step: 8940, epoch: 270, batch: 29, loss: 0.0010384321212768555, acc: 100.0, f1: 100.0, r: 0.6683869280746936
06/02/2019 11:15:15 *** evaluating ***
06/02/2019 11:15:16 step: 271, epoch: 270, acc: 51.28205128205128, f1: 29.189935047691883, r: 0.24811171117843286
06/02/2019 11:15:16 *** epoch: 272 ***
06/02/2019 11:15:16 *** training ***
06/02/2019 11:15:18 step: 8948, epoch: 271, batch: 4, loss: 0.0030891448259353638, acc: 100.0, f1: 100.0, r: 0.78707908378411
06/02/2019 11:15:20 step: 8953, epoch: 271, batch: 9, loss: 0.003933735191822052, acc: 100.0, f1: 100.0, r: 0.8089318575007214
06/02/2019 11:15:23 step: 8958, epoch: 271, batch: 14, loss: 0.0011948496103286743, acc: 100.0, f1: 100.0, r: 0.7795645472796564
06/02/2019 11:15:25 step: 8963, epoch: 271, batch: 19, loss: 0.0011036992073059082, acc: 100.0, f1: 100.0, r: 0.7012152532994964
06/02/2019 11:15:27 step: 8968, epoch: 271, batch: 24, loss: 0.004157602787017822, acc: 100.0, f1: 100.0, r: 0.7712337667287934
06/02/2019 11:15:30 step: 8973, epoch: 271, batch: 29, loss: 0.002639569342136383, acc: 100.0, f1: 100.0, r: 0.7214123446008899
06/02/2019 11:15:31 *** evaluating ***
06/02/2019 11:15:32 step: 272, epoch: 271, acc: 50.85470085470085, f1: 28.856511537236695, r: 0.2408426659622905
06/02/2019 11:15:32 *** epoch: 273 ***
06/02/2019 11:15:32 *** training ***
06/02/2019 11:15:34 step: 8981, epoch: 272, batch: 4, loss: 0.008449576795101166, acc: 100.0, f1: 100.0, r: 0.7722360670080517
06/02/2019 11:15:36 step: 8986, epoch: 272, batch: 9, loss: 0.0008507072925567627, acc: 100.0, f1: 100.0, r: 0.6785539944807563
06/02/2019 11:15:38 step: 8991, epoch: 272, batch: 14, loss: 0.0016668811440467834, acc: 100.0, f1: 100.0, r: 0.7082513816623693
06/02/2019 11:15:40 step: 8996, epoch: 272, batch: 19, loss: 0.0056168511509895325, acc: 100.0, f1: 100.0, r: 0.8528015774165303
06/02/2019 11:15:43 step: 9001, epoch: 272, batch: 24, loss: 0.001299254596233368, acc: 100.0, f1: 100.0, r: 0.8222030850336426
06/02/2019 11:15:45 step: 9006, epoch: 272, batch: 29, loss: 0.003402397036552429, acc: 100.0, f1: 100.0, r: 0.7923750363172999
06/02/2019 11:15:47 *** evaluating ***
06/02/2019 11:15:48 step: 273, epoch: 272, acc: 50.427350427350426, f1: 28.730007271389525, r: 0.2420974806574771
06/02/2019 11:15:48 *** epoch: 274 ***
06/02/2019 11:15:48 *** training ***
06/02/2019 11:15:50 step: 9014, epoch: 273, batch: 4, loss: 0.0049951523542404175, acc: 100.0, f1: 100.0, r: 0.8283783921831661
06/02/2019 11:15:53 step: 9019, epoch: 273, batch: 9, loss: 0.0016973242163658142, acc: 100.0, f1: 100.0, r: 0.758301051289409
06/02/2019 11:15:55 step: 9024, epoch: 273, batch: 14, loss: 0.0009916797280311584, acc: 100.0, f1: 100.0, r: 0.751736323956189
06/02/2019 11:15:57 step: 9029, epoch: 273, batch: 19, loss: 0.0195092111825943, acc: 98.4375, f1: 98.49624060150377, r: 0.7122167679937038
06/02/2019 11:15:59 step: 9034, epoch: 273, batch: 24, loss: 0.0025919675827026367, acc: 100.0, f1: 100.0, r: 0.8031865463998153
06/02/2019 11:16:01 step: 9039, epoch: 273, batch: 29, loss: 0.0015502870082855225, acc: 100.0, f1: 100.0, r: 0.6749779787359975
06/02/2019 11:16:02 *** evaluating ***
06/02/2019 11:16:03 step: 274, epoch: 273, acc: 49.14529914529914, f1: 28.060548508502926, r: 0.24237889227879122
06/02/2019 11:16:03 *** epoch: 275 ***
06/02/2019 11:16:03 *** training ***
06/02/2019 11:16:06 step: 9047, epoch: 274, batch: 4, loss: 0.0011730790138244629, acc: 100.0, f1: 100.0, r: 0.774967348224107
06/02/2019 11:16:07 step: 9052, epoch: 274, batch: 9, loss: 0.004944629967212677, acc: 100.0, f1: 100.0, r: 0.6968470130027054
06/02/2019 11:16:10 step: 9057, epoch: 274, batch: 14, loss: 0.0025289952754974365, acc: 100.0, f1: 100.0, r: 0.7111374728572916
06/02/2019 11:16:12 step: 9062, epoch: 274, batch: 19, loss: 0.0012511387467384338, acc: 100.0, f1: 100.0, r: 0.7393874179651484
06/02/2019 11:16:14 step: 9067, epoch: 274, batch: 24, loss: 0.0024582818150520325, acc: 100.0, f1: 100.0, r: 0.735280522249228
06/02/2019 11:16:17 step: 9072, epoch: 274, batch: 29, loss: 0.0029334574937820435, acc: 100.0, f1: 100.0, r: 0.7384965638889064
06/02/2019 11:16:18 *** evaluating ***
06/02/2019 11:16:19 step: 275, epoch: 274, acc: 50.85470085470085, f1: 28.872114063016507, r: 0.23976253762996186
06/02/2019 11:16:19 *** epoch: 276 ***
06/02/2019 11:16:19 *** training ***
06/02/2019 11:16:21 step: 9080, epoch: 275, batch: 4, loss: 0.001894444227218628, acc: 100.0, f1: 100.0, r: 0.7719389666420082
06/02/2019 11:16:24 step: 9085, epoch: 275, batch: 9, loss: 0.006218567490577698, acc: 100.0, f1: 100.0, r: 0.7124356132090458
06/02/2019 11:16:26 step: 9090, epoch: 275, batch: 14, loss: 0.002657845616340637, acc: 100.0, f1: 100.0, r: 0.7854421249788754
06/02/2019 11:16:28 step: 9095, epoch: 275, batch: 19, loss: 0.00123625248670578, acc: 100.0, f1: 100.0, r: 0.8316454715794265
06/02/2019 11:16:30 step: 9100, epoch: 275, batch: 24, loss: 0.0006176978349685669, acc: 100.0, f1: 100.0, r: 0.6698826176927457
06/02/2019 11:16:32 step: 9105, epoch: 275, batch: 29, loss: 0.001940317451953888, acc: 100.0, f1: 100.0, r: 0.7236039811104688
06/02/2019 11:16:34 *** evaluating ***
06/02/2019 11:16:34 step: 276, epoch: 275, acc: 50.427350427350426, f1: 28.761551271226192, r: 0.23650270046683639
06/02/2019 11:16:34 *** epoch: 277 ***
06/02/2019 11:16:34 *** training ***
06/02/2019 11:16:37 step: 9113, epoch: 276, batch: 4, loss: 0.002984270453453064, acc: 100.0, f1: 100.0, r: 0.7780808009297538
06/02/2019 11:16:39 step: 9118, epoch: 276, batch: 9, loss: 0.0025675520300865173, acc: 100.0, f1: 100.0, r: 0.6193879553268127
06/02/2019 11:16:41 step: 9123, epoch: 276, batch: 14, loss: 0.0019411146640777588, acc: 100.0, f1: 100.0, r: 0.6945184580757814
06/02/2019 11:16:43 step: 9128, epoch: 276, batch: 19, loss: 0.001843571662902832, acc: 100.0, f1: 100.0, r: 0.6752141441425712
06/02/2019 11:16:45 step: 9133, epoch: 276, batch: 24, loss: 0.0024529844522476196, acc: 100.0, f1: 100.0, r: 0.5601932917254452
06/02/2019 11:16:47 step: 9138, epoch: 276, batch: 29, loss: 0.003036990761756897, acc: 100.0, f1: 100.0, r: 0.8404002165807192
06/02/2019 11:16:49 *** evaluating ***
06/02/2019 11:16:49 step: 277, epoch: 276, acc: 51.28205128205128, f1: 29.7098659712365, r: 0.23987358369008385
06/02/2019 11:16:49 *** epoch: 278 ***
06/02/2019 11:16:49 *** training ***
06/02/2019 11:16:51 step: 9146, epoch: 277, batch: 4, loss: 0.0010359138250350952, acc: 100.0, f1: 100.0, r: 0.8251313520659854
06/02/2019 11:16:54 step: 9151, epoch: 277, batch: 9, loss: 0.001563727855682373, acc: 100.0, f1: 100.0, r: 0.6607084613642562
06/02/2019 11:16:56 step: 9156, epoch: 277, batch: 14, loss: 0.027515824884176254, acc: 98.4375, f1: 93.19727891156462, r: 0.6731670383563967
06/02/2019 11:16:58 step: 9161, epoch: 277, batch: 19, loss: 0.0018754079937934875, acc: 100.0, f1: 100.0, r: 0.8262527114863053
06/02/2019 11:17:00 step: 9166, epoch: 277, batch: 24, loss: 0.0016933232545852661, acc: 100.0, f1: 100.0, r: 0.8447742641230594
06/02/2019 11:17:03 step: 9171, epoch: 277, batch: 29, loss: 0.001963675022125244, acc: 100.0, f1: 100.0, r: 0.8234946002087791
06/02/2019 11:17:04 *** evaluating ***
06/02/2019 11:17:05 step: 278, epoch: 277, acc: 51.28205128205128, f1: 30.490824336522625, r: 0.2574620495750603
06/02/2019 11:17:05 *** epoch: 279 ***
06/02/2019 11:17:05 *** training ***
06/02/2019 11:17:07 step: 9179, epoch: 278, batch: 4, loss: 0.0015917718410491943, acc: 100.0, f1: 100.0, r: 0.7907321663819278
06/02/2019 11:17:09 step: 9184, epoch: 278, batch: 9, loss: 0.03805532306432724, acc: 98.4375, f1: 98.3201581027668, r: 0.8170938815578491
06/02/2019 11:17:11 step: 9189, epoch: 278, batch: 14, loss: 0.0018957480788230896, acc: 100.0, f1: 100.0, r: 0.7885168623357379
06/02/2019 11:17:13 step: 9194, epoch: 278, batch: 19, loss: 0.002196952700614929, acc: 100.0, f1: 100.0, r: 0.6850589138329326
06/02/2019 11:17:16 step: 9199, epoch: 278, batch: 24, loss: 0.0012427940964698792, acc: 100.0, f1: 100.0, r: 0.7253238203357782
06/02/2019 11:17:18 step: 9204, epoch: 278, batch: 29, loss: 0.0030904188752174377, acc: 100.0, f1: 100.0, r: 0.7765356556592359
06/02/2019 11:17:19 *** evaluating ***
06/02/2019 11:17:20 step: 279, epoch: 278, acc: 51.28205128205128, f1: 29.6751674661046, r: 0.25200337904497194
06/02/2019 11:17:20 *** epoch: 280 ***
06/02/2019 11:17:20 *** training ***
06/02/2019 11:17:22 step: 9212, epoch: 279, batch: 4, loss: 0.007542848587036133, acc: 100.0, f1: 100.0, r: 0.7961466836616666
06/02/2019 11:17:24 step: 9217, epoch: 279, batch: 9, loss: 0.002420201897621155, acc: 100.0, f1: 100.0, r: 0.7725738974494578
06/02/2019 11:17:27 step: 9222, epoch: 279, batch: 14, loss: 0.0018438175320625305, acc: 100.0, f1: 100.0, r: 0.6888724167707795
06/02/2019 11:17:29 step: 9227, epoch: 279, batch: 19, loss: 0.004233121871948242, acc: 100.0, f1: 100.0, r: 0.7304567916791348
06/02/2019 11:17:32 step: 9232, epoch: 279, batch: 24, loss: 0.002328738570213318, acc: 100.0, f1: 100.0, r: 0.7895392191136734
06/02/2019 11:17:34 step: 9237, epoch: 279, batch: 29, loss: 0.004506133496761322, acc: 100.0, f1: 100.0, r: 0.7225379876783642
06/02/2019 11:17:35 *** evaluating ***
06/02/2019 11:17:36 step: 280, epoch: 279, acc: 50.427350427350426, f1: 28.90168210112998, r: 0.2419749379851042
06/02/2019 11:17:36 *** epoch: 281 ***
06/02/2019 11:17:36 *** training ***
06/02/2019 11:17:38 step: 9245, epoch: 280, batch: 4, loss: 0.004465267062187195, acc: 100.0, f1: 100.0, r: 0.7107609233968007
06/02/2019 11:17:40 step: 9250, epoch: 280, batch: 9, loss: 0.0032396242022514343, acc: 100.0, f1: 100.0, r: 0.7152513076707575
06/02/2019 11:17:42 step: 9255, epoch: 280, batch: 14, loss: 0.001644417643547058, acc: 100.0, f1: 100.0, r: 0.6266339380071912
06/02/2019 11:17:45 step: 9260, epoch: 280, batch: 19, loss: 0.0030119046568870544, acc: 100.0, f1: 100.0, r: 0.7822275636833871
06/02/2019 11:17:47 step: 9265, epoch: 280, batch: 24, loss: 0.0014253556728363037, acc: 100.0, f1: 100.0, r: 0.7486252699612365
06/02/2019 11:17:50 step: 9270, epoch: 280, batch: 29, loss: 0.0010138526558876038, acc: 100.0, f1: 100.0, r: 0.7346110601512099
06/02/2019 11:17:51 *** evaluating ***
06/02/2019 11:17:51 step: 281, epoch: 280, acc: 50.85470085470085, f1: 29.338565308960046, r: 0.24081784888189692
06/02/2019 11:17:51 *** epoch: 282 ***
06/02/2019 11:17:51 *** training ***
06/02/2019 11:17:54 step: 9278, epoch: 281, batch: 4, loss: 0.0034602656960487366, acc: 100.0, f1: 100.0, r: 0.6144528500752258
06/02/2019 11:17:56 step: 9283, epoch: 281, batch: 9, loss: 0.0009029358625411987, acc: 100.0, f1: 100.0, r: 0.7288343174604626
06/02/2019 11:17:58 step: 9288, epoch: 281, batch: 14, loss: 0.0037815943360328674, acc: 100.0, f1: 100.0, r: 0.7000163076849136
06/02/2019 11:18:00 step: 9293, epoch: 281, batch: 19, loss: 0.0025411471724510193, acc: 100.0, f1: 100.0, r: 0.8347749835695699
06/02/2019 11:18:02 step: 9298, epoch: 281, batch: 24, loss: 0.0037839114665985107, acc: 100.0, f1: 100.0, r: 0.7810250647124368
06/02/2019 11:18:05 step: 9303, epoch: 281, batch: 29, loss: 0.0008505433797836304, acc: 100.0, f1: 100.0, r: 0.779972383231412
06/02/2019 11:18:06 *** evaluating ***
06/02/2019 11:18:06 step: 282, epoch: 281, acc: 50.0, f1: 28.7518720884647, r: 0.24109436257064723
06/02/2019 11:18:06 *** epoch: 283 ***
06/02/2019 11:18:06 *** training ***
06/02/2019 11:18:08 step: 9311, epoch: 282, batch: 4, loss: 0.003557048738002777, acc: 100.0, f1: 100.0, r: 0.7201993672434719
06/02/2019 11:18:11 step: 9316, epoch: 282, batch: 9, loss: 0.0011944472789764404, acc: 100.0, f1: 100.0, r: 0.658800433976327
06/02/2019 11:18:13 step: 9321, epoch: 282, batch: 14, loss: 0.001173168420791626, acc: 100.0, f1: 100.0, r: 0.7365718920282146
06/02/2019 11:18:15 step: 9326, epoch: 282, batch: 19, loss: 0.0012115538120269775, acc: 100.0, f1: 100.0, r: 0.7767981259493331
06/02/2019 11:18:17 step: 9331, epoch: 282, batch: 24, loss: 0.001331038773059845, acc: 100.0, f1: 100.0, r: 0.661067148280852
06/02/2019 11:18:20 step: 9336, epoch: 282, batch: 29, loss: 0.0025061964988708496, acc: 100.0, f1: 100.0, r: 0.7040227058469409
06/02/2019 11:18:21 *** evaluating ***
06/02/2019 11:18:22 step: 283, epoch: 282, acc: 50.85470085470085, f1: 28.797441101233655, r: 0.23947480621280973
06/02/2019 11:18:22 *** epoch: 284 ***
06/02/2019 11:18:22 *** training ***
06/02/2019 11:18:24 step: 9344, epoch: 283, batch: 4, loss: 0.0011804699897766113, acc: 100.0, f1: 100.0, r: 0.7216125058890535
06/02/2019 11:18:27 step: 9349, epoch: 283, batch: 9, loss: 0.004654295742511749, acc: 100.0, f1: 100.0, r: 0.7376576760524222
06/02/2019 11:18:29 step: 9354, epoch: 283, batch: 14, loss: 0.0038477927446365356, acc: 100.0, f1: 100.0, r: 0.7828333467852543
06/02/2019 11:18:32 step: 9359, epoch: 283, batch: 19, loss: 0.0031976625323295593, acc: 100.0, f1: 100.0, r: 0.6619587966362316
06/02/2019 11:18:34 step: 9364, epoch: 283, batch: 24, loss: 0.003779768943786621, acc: 100.0, f1: 100.0, r: 0.8277747255224249
06/02/2019 11:18:36 step: 9369, epoch: 283, batch: 29, loss: 0.005592547357082367, acc: 100.0, f1: 100.0, r: 0.7844709274802169
06/02/2019 11:18:37 *** evaluating ***
06/02/2019 11:18:38 step: 284, epoch: 283, acc: 50.85470085470085, f1: 29.23697862165354, r: 0.2434659203384734
06/02/2019 11:18:38 *** epoch: 285 ***
06/02/2019 11:18:38 *** training ***
06/02/2019 11:18:40 step: 9377, epoch: 284, batch: 4, loss: 0.0022502467036247253, acc: 100.0, f1: 100.0, r: 0.7691583046485483
06/02/2019 11:18:42 step: 9382, epoch: 284, batch: 9, loss: 0.0011417865753173828, acc: 100.0, f1: 100.0, r: 0.7113585187251171
06/02/2019 11:18:44 step: 9387, epoch: 284, batch: 14, loss: 0.0022667348384857178, acc: 100.0, f1: 100.0, r: 0.7351767661288777
06/02/2019 11:18:46 step: 9392, epoch: 284, batch: 19, loss: 0.0014082789421081543, acc: 100.0, f1: 100.0, r: 0.6705324015229943
06/02/2019 11:18:48 step: 9397, epoch: 284, batch: 24, loss: 0.004510119557380676, acc: 100.0, f1: 100.0, r: 0.6867510944918119
06/02/2019 11:18:51 step: 9402, epoch: 284, batch: 29, loss: 0.0030679851770401, acc: 100.0, f1: 100.0, r: 0.687961792531451
06/02/2019 11:18:52 *** evaluating ***
06/02/2019 11:18:53 step: 285, epoch: 284, acc: 49.14529914529914, f1: 28.55411649611956, r: 0.24543320371727373
06/02/2019 11:18:53 *** epoch: 286 ***
06/02/2019 11:18:53 *** training ***
06/02/2019 11:18:55 step: 9410, epoch: 285, batch: 4, loss: 0.0017185360193252563, acc: 100.0, f1: 100.0, r: 0.7190349261039505
06/02/2019 11:18:57 step: 9415, epoch: 285, batch: 9, loss: 0.001836240291595459, acc: 100.0, f1: 100.0, r: 0.695406249428698
06/02/2019 11:18:59 step: 9420, epoch: 285, batch: 14, loss: 0.003519691526889801, acc: 100.0, f1: 100.0, r: 0.7846462210973404
06/02/2019 11:19:02 step: 9425, epoch: 285, batch: 19, loss: 0.0031221508979797363, acc: 100.0, f1: 100.0, r: 0.7791788466773575
06/02/2019 11:19:04 step: 9430, epoch: 285, batch: 24, loss: 0.0024041682481765747, acc: 100.0, f1: 100.0, r: 0.8126539322154303
06/02/2019 11:19:06 step: 9435, epoch: 285, batch: 29, loss: 0.004725605249404907, acc: 100.0, f1: 100.0, r: 0.6997812474462617
06/02/2019 11:19:07 *** evaluating ***
06/02/2019 11:19:08 step: 286, epoch: 285, acc: 51.28205128205128, f1: 29.262518573571526, r: 0.24111568499513175
06/02/2019 11:19:08 *** epoch: 287 ***
06/02/2019 11:19:08 *** training ***
06/02/2019 11:19:11 step: 9443, epoch: 286, batch: 4, loss: 0.00170975923538208, acc: 100.0, f1: 100.0, r: 0.7679430463629129
06/02/2019 11:19:13 step: 9448, epoch: 286, batch: 9, loss: 0.0017091408371925354, acc: 100.0, f1: 100.0, r: 0.7519869912234276
06/02/2019 11:19:15 step: 9453, epoch: 286, batch: 14, loss: 0.001019790768623352, acc: 100.0, f1: 100.0, r: 0.8326030731643055
06/02/2019 11:19:17 step: 9458, epoch: 286, batch: 19, loss: 0.005096316337585449, acc: 100.0, f1: 100.0, r: 0.7834635047339722
06/02/2019 11:19:20 step: 9463, epoch: 286, batch: 24, loss: 0.0011062473058700562, acc: 100.0, f1: 100.0, r: 0.7267985388067026
06/02/2019 11:19:22 step: 9468, epoch: 286, batch: 29, loss: 0.002218388020992279, acc: 100.0, f1: 100.0, r: 0.7411847458127253
06/02/2019 11:19:24 *** evaluating ***
06/02/2019 11:19:24 step: 287, epoch: 286, acc: 50.85470085470085, f1: 28.9160147211452, r: 0.23774160754021734
06/02/2019 11:19:24 *** epoch: 288 ***
06/02/2019 11:19:24 *** training ***
06/02/2019 11:19:26 step: 9476, epoch: 287, batch: 4, loss: 0.0026999786496162415, acc: 100.0, f1: 100.0, r: 0.735297278298711
06/02/2019 11:19:28 step: 9481, epoch: 287, batch: 9, loss: 0.0007570832967758179, acc: 100.0, f1: 100.0, r: 0.6599707470242423
06/02/2019 11:19:31 step: 9486, epoch: 287, batch: 14, loss: 0.0010306835174560547, acc: 100.0, f1: 100.0, r: 0.6359687734360817
06/02/2019 11:19:34 step: 9491, epoch: 287, batch: 19, loss: 0.0022922083735466003, acc: 100.0, f1: 100.0, r: 0.6373548209482072
06/02/2019 11:19:36 step: 9496, epoch: 287, batch: 24, loss: 0.0007551312446594238, acc: 100.0, f1: 100.0, r: 0.7941170493365814
06/02/2019 11:19:38 step: 9501, epoch: 287, batch: 29, loss: 0.001568421721458435, acc: 100.0, f1: 100.0, r: 0.7779459494491161
06/02/2019 11:19:39 *** evaluating ***
06/02/2019 11:19:40 step: 288, epoch: 287, acc: 50.0, f1: 28.546495217123447, r: 0.23999716137619714
06/02/2019 11:19:40 *** epoch: 289 ***
06/02/2019 11:19:40 *** training ***
06/02/2019 11:19:42 step: 9509, epoch: 288, batch: 4, loss: 0.002721741795539856, acc: 100.0, f1: 100.0, r: 0.8247101136846327
06/02/2019 11:19:45 step: 9514, epoch: 288, batch: 9, loss: 0.0014666691422462463, acc: 100.0, f1: 100.0, r: 0.6753958828627402
06/02/2019 11:19:47 step: 9519, epoch: 288, batch: 14, loss: 0.002066686749458313, acc: 100.0, f1: 100.0, r: 0.7797624604693136
06/02/2019 11:19:49 step: 9524, epoch: 288, batch: 19, loss: 0.0004712790250778198, acc: 100.0, f1: 100.0, r: 0.7907278719888988
06/02/2019 11:19:52 step: 9529, epoch: 288, batch: 24, loss: 0.002849765121936798, acc: 100.0, f1: 100.0, r: 0.7843846456584829
06/02/2019 11:19:54 step: 9534, epoch: 288, batch: 29, loss: 0.002683281898498535, acc: 100.0, f1: 100.0, r: 0.8347602063671198
06/02/2019 11:19:55 *** evaluating ***
06/02/2019 11:19:55 step: 289, epoch: 288, acc: 50.85470085470085, f1: 28.920479706459588, r: 0.24001942017799527
06/02/2019 11:19:55 *** epoch: 290 ***
06/02/2019 11:19:55 *** training ***
06/02/2019 11:19:57 step: 9542, epoch: 289, batch: 4, loss: 0.0021808817982673645, acc: 100.0, f1: 100.0, r: 0.8105287939582773
06/02/2019 11:19:59 step: 9547, epoch: 289, batch: 9, loss: 0.004296734929084778, acc: 100.0, f1: 100.0, r: 0.7154020557328165
06/02/2019 11:20:02 step: 9552, epoch: 289, batch: 14, loss: 0.0034953951835632324, acc: 100.0, f1: 100.0, r: 0.7411585322612813
06/02/2019 11:20:04 step: 9557, epoch: 289, batch: 19, loss: 0.001857846975326538, acc: 100.0, f1: 100.0, r: 0.8148589426101612
06/02/2019 11:20:06 step: 9562, epoch: 289, batch: 24, loss: 0.001274704933166504, acc: 100.0, f1: 100.0, r: 0.716224021867655
06/02/2019 11:20:09 step: 9567, epoch: 289, batch: 29, loss: 0.0016290992498397827, acc: 100.0, f1: 100.0, r: 0.7090311719316861
06/02/2019 11:20:10 *** evaluating ***
06/02/2019 11:20:11 step: 290, epoch: 289, acc: 50.427350427350426, f1: 28.906764749925124, r: 0.23570092676418633
06/02/2019 11:20:11 *** epoch: 291 ***
06/02/2019 11:20:11 *** training ***
06/02/2019 11:20:14 step: 9575, epoch: 290, batch: 4, loss: 0.00268515944480896, acc: 100.0, f1: 100.0, r: 0.7694762998337976
06/02/2019 11:20:16 step: 9580, epoch: 290, batch: 9, loss: 0.00038483738899230957, acc: 100.0, f1: 100.0, r: 0.6268620388192536
06/02/2019 11:20:18 step: 9585, epoch: 290, batch: 14, loss: 0.0017478540539741516, acc: 100.0, f1: 100.0, r: 0.7525301888012271
06/02/2019 11:20:20 step: 9590, epoch: 290, batch: 19, loss: 0.004293628036975861, acc: 100.0, f1: 100.0, r: 0.8014809192881169
06/02/2019 11:20:22 step: 9595, epoch: 290, batch: 24, loss: 0.0011019110679626465, acc: 100.0, f1: 100.0, r: 0.6197553535341904
06/02/2019 11:20:24 step: 9600, epoch: 290, batch: 29, loss: 0.0018819868564605713, acc: 100.0, f1: 100.0, r: 0.6904897638175593
06/02/2019 11:20:26 *** evaluating ***
06/02/2019 11:20:27 step: 291, epoch: 290, acc: 50.0, f1: 28.474424032885093, r: 0.23823416558614116
06/02/2019 11:20:27 *** epoch: 292 ***
06/02/2019 11:20:27 *** training ***
06/02/2019 11:20:29 step: 9608, epoch: 291, batch: 4, loss: 0.0017116814851760864, acc: 100.0, f1: 100.0, r: 0.7305098872042388
06/02/2019 11:20:31 step: 9613, epoch: 291, batch: 9, loss: 0.0015027672052383423, acc: 100.0, f1: 100.0, r: 0.6213631484627342
06/02/2019 11:20:34 step: 9618, epoch: 291, batch: 14, loss: 0.004886053502559662, acc: 100.0, f1: 100.0, r: 0.8308841443850609
06/02/2019 11:20:36 step: 9623, epoch: 291, batch: 19, loss: 0.003427743911743164, acc: 100.0, f1: 100.0, r: 0.8125346419053227
06/02/2019 11:20:38 step: 9628, epoch: 291, batch: 24, loss: 0.001391291618347168, acc: 100.0, f1: 100.0, r: 0.813076901699647
06/02/2019 11:20:40 step: 9633, epoch: 291, batch: 29, loss: 0.001744009554386139, acc: 100.0, f1: 100.0, r: 0.7588346982627798
06/02/2019 11:20:41 *** evaluating ***
06/02/2019 11:20:42 step: 292, epoch: 291, acc: 50.427350427350426, f1: 28.677130657246654, r: 0.23509331552621107
06/02/2019 11:20:42 *** epoch: 293 ***
06/02/2019 11:20:42 *** training ***
06/02/2019 11:20:44 step: 9641, epoch: 292, batch: 4, loss: 0.0020192116498947144, acc: 100.0, f1: 100.0, r: 0.8021117178482222
06/02/2019 11:20:46 step: 9646, epoch: 292, batch: 9, loss: 0.005486056208610535, acc: 100.0, f1: 100.0, r: 0.7438396571572458
06/02/2019 11:20:49 step: 9651, epoch: 292, batch: 14, loss: 0.002957843244075775, acc: 100.0, f1: 100.0, r: 0.7031880035890357
06/02/2019 11:20:50 step: 9656, epoch: 292, batch: 19, loss: 0.0035991594195365906, acc: 100.0, f1: 100.0, r: 0.7994682793685063
06/02/2019 11:20:53 step: 9661, epoch: 292, batch: 24, loss: 0.001638658344745636, acc: 100.0, f1: 100.0, r: 0.6241165911139411
06/02/2019 11:20:55 step: 9666, epoch: 292, batch: 29, loss: 0.004287339746952057, acc: 100.0, f1: 100.0, r: 0.7744825473526789
06/02/2019 11:20:57 *** evaluating ***
06/02/2019 11:20:57 step: 293, epoch: 292, acc: 50.427350427350426, f1: 28.638435204149015, r: 0.23387277705853635
06/02/2019 11:20:57 *** epoch: 294 ***
06/02/2019 11:20:57 *** training ***
06/02/2019 11:20:59 step: 9674, epoch: 293, batch: 4, loss: 0.003336884081363678, acc: 100.0, f1: 100.0, r: 0.6618446344621798
06/02/2019 11:21:01 step: 9679, epoch: 293, batch: 9, loss: 0.0019427984952926636, acc: 100.0, f1: 100.0, r: 0.8139378634350614
06/02/2019 11:21:04 step: 9684, epoch: 293, batch: 14, loss: 0.0010471940040588379, acc: 100.0, f1: 100.0, r: 0.754010482276356
06/02/2019 11:21:06 step: 9689, epoch: 293, batch: 19, loss: 0.0006557554006576538, acc: 100.0, f1: 100.0, r: 0.7821350379610172
06/02/2019 11:21:09 step: 9694, epoch: 293, batch: 24, loss: 0.004164882004261017, acc: 100.0, f1: 100.0, r: 0.6750183870510066
06/02/2019 11:21:11 step: 9699, epoch: 293, batch: 29, loss: 0.0018633008003234863, acc: 100.0, f1: 100.0, r: 0.8115223667344977
06/02/2019 11:21:12 *** evaluating ***
06/02/2019 11:21:13 step: 294, epoch: 293, acc: 50.85470085470085, f1: 28.845618876516323, r: 0.23468558655231198
06/02/2019 11:21:13 *** epoch: 295 ***
06/02/2019 11:21:13 *** training ***
06/02/2019 11:21:16 step: 9707, epoch: 294, batch: 4, loss: 0.0016513913869857788, acc: 100.0, f1: 100.0, r: 0.7971147795103908
06/02/2019 11:21:18 step: 9712, epoch: 294, batch: 9, loss: 0.0015850663185119629, acc: 100.0, f1: 100.0, r: 0.7010744386753663
06/02/2019 11:21:20 step: 9717, epoch: 294, batch: 14, loss: 0.006233386695384979, acc: 100.0, f1: 100.0, r: 0.8517365434506272
06/02/2019 11:21:22 step: 9722, epoch: 294, batch: 19, loss: 0.003226235508918762, acc: 100.0, f1: 100.0, r: 0.7525479440219539
06/02/2019 11:21:25 step: 9727, epoch: 294, batch: 24, loss: 0.005339689552783966, acc: 100.0, f1: 100.0, r: 0.7365866451290067
06/02/2019 11:21:27 step: 9732, epoch: 294, batch: 29, loss: 0.004186704754829407, acc: 100.0, f1: 100.0, r: 0.7035813018330759
06/02/2019 11:21:28 *** evaluating ***
06/02/2019 11:21:29 step: 295, epoch: 294, acc: 48.717948717948715, f1: 27.526832302658388, r: 0.2355408980177178
06/02/2019 11:21:29 *** epoch: 296 ***
06/02/2019 11:21:29 *** training ***
06/02/2019 11:21:31 step: 9740, epoch: 295, batch: 4, loss: 0.0028102993965148926, acc: 100.0, f1: 100.0, r: 0.6596399733049443
06/02/2019 11:21:33 step: 9745, epoch: 295, batch: 9, loss: 0.0015888884663581848, acc: 100.0, f1: 100.0, r: 0.7126294784065794
06/02/2019 11:21:35 step: 9750, epoch: 295, batch: 14, loss: 0.003548562526702881, acc: 100.0, f1: 100.0, r: 0.7011124463713823
06/02/2019 11:21:38 step: 9755, epoch: 295, batch: 19, loss: 0.00276782363653183, acc: 100.0, f1: 100.0, r: 0.8207136714562325
06/02/2019 11:21:40 step: 9760, epoch: 295, batch: 24, loss: 0.004309594631195068, acc: 100.0, f1: 100.0, r: 0.8259373455065722
06/02/2019 11:21:43 step: 9765, epoch: 295, batch: 29, loss: 0.002025604248046875, acc: 100.0, f1: 100.0, r: 0.8297964867593958
06/02/2019 11:21:44 *** evaluating ***
06/02/2019 11:21:44 step: 296, epoch: 295, acc: 50.0, f1: 28.507948274441976, r: 0.23986585149177944
06/02/2019 11:21:44 *** epoch: 297 ***
06/02/2019 11:21:44 *** training ***
06/02/2019 11:21:47 step: 9773, epoch: 296, batch: 4, loss: 0.00100785493850708, acc: 100.0, f1: 100.0, r: 0.8057913862952645
06/02/2019 11:21:49 step: 9778, epoch: 296, batch: 9, loss: 0.0015984177589416504, acc: 100.0, f1: 100.0, r: 0.833578318342804
06/02/2019 11:21:52 step: 9783, epoch: 296, batch: 14, loss: 0.0019192993640899658, acc: 100.0, f1: 100.0, r: 0.7230805291915005
06/02/2019 11:21:54 step: 9788, epoch: 296, batch: 19, loss: 0.00187663733959198, acc: 100.0, f1: 100.0, r: 0.6794950640341545
06/02/2019 11:21:56 step: 9793, epoch: 296, batch: 24, loss: 0.0017597079277038574, acc: 100.0, f1: 100.0, r: 0.7363573699572972
06/02/2019 11:21:58 step: 9798, epoch: 296, batch: 29, loss: 0.0010488927364349365, acc: 100.0, f1: 100.0, r: 0.7706731864471507
06/02/2019 11:21:59 *** evaluating ***
06/02/2019 11:22:00 step: 297, epoch: 296, acc: 50.427350427350426, f1: 28.909907032479175, r: 0.2339641746904016
06/02/2019 11:22:00 *** epoch: 298 ***
06/02/2019 11:22:00 *** training ***
06/02/2019 11:22:03 step: 9806, epoch: 297, batch: 4, loss: 0.0016811341047286987, acc: 100.0, f1: 100.0, r: 0.6355118969919015
06/02/2019 11:22:05 step: 9811, epoch: 297, batch: 9, loss: 0.004360586404800415, acc: 100.0, f1: 100.0, r: 0.7967155512985035
06/02/2019 11:22:07 step: 9816, epoch: 297, batch: 14, loss: 0.00447101891040802, acc: 100.0, f1: 100.0, r: 0.6909677437679387
06/02/2019 11:22:09 step: 9821, epoch: 297, batch: 19, loss: 0.0010345280170440674, acc: 100.0, f1: 100.0, r: 0.7476524749127265
06/02/2019 11:22:11 step: 9826, epoch: 297, batch: 24, loss: 0.0023951902985572815, acc: 100.0, f1: 100.0, r: 0.7293138682290116
06/02/2019 11:22:14 step: 9831, epoch: 297, batch: 29, loss: 0.0012287348508834839, acc: 100.0, f1: 100.0, r: 0.7724594171604843
06/02/2019 11:22:15 *** evaluating ***
06/02/2019 11:22:16 step: 298, epoch: 297, acc: 48.717948717948715, f1: 27.992443431273216, r: 0.2321470138311223
06/02/2019 11:22:16 *** epoch: 299 ***
06/02/2019 11:22:16 *** training ***
06/02/2019 11:22:18 step: 9839, epoch: 298, batch: 4, loss: 0.0008657574653625488, acc: 100.0, f1: 100.0, r: 0.6233262601944111
06/02/2019 11:22:21 step: 9844, epoch: 298, batch: 9, loss: 0.0009926483035087585, acc: 100.0, f1: 100.0, r: 0.6740575503189333
06/02/2019 11:22:23 step: 9849, epoch: 298, batch: 14, loss: 0.0013269037008285522, acc: 100.0, f1: 100.0, r: 0.6667024415469968
06/02/2019 11:22:25 step: 9854, epoch: 298, batch: 19, loss: 0.0013340860605239868, acc: 100.0, f1: 100.0, r: 0.8239901696277023
06/02/2019 11:22:28 step: 9859, epoch: 298, batch: 24, loss: 0.0035327672958374023, acc: 100.0, f1: 100.0, r: 0.8492508597486483
06/02/2019 11:22:30 step: 9864, epoch: 298, batch: 29, loss: 0.0004920065402984619, acc: 100.0, f1: 100.0, r: 0.8332977255076961
06/02/2019 11:22:31 *** evaluating ***
06/02/2019 11:22:32 step: 299, epoch: 298, acc: 49.572649572649574, f1: 28.510166506109076, r: 0.2360928114841037
06/02/2019 11:22:32 *** epoch: 300 ***
06/02/2019 11:22:32 *** training ***
06/02/2019 11:22:34 step: 9872, epoch: 299, batch: 4, loss: 0.001126304268836975, acc: 100.0, f1: 100.0, r: 0.7558222232607483
06/02/2019 11:22:36 step: 9877, epoch: 299, batch: 9, loss: 0.0010900795459747314, acc: 100.0, f1: 100.0, r: 0.7131018231676688
06/02/2019 11:22:39 step: 9882, epoch: 299, batch: 14, loss: 0.0009965822100639343, acc: 100.0, f1: 100.0, r: 0.7113681040086022
06/02/2019 11:22:41 step: 9887, epoch: 299, batch: 19, loss: 0.0032758712768554688, acc: 100.0, f1: 100.0, r: 0.7506602698382651
06/02/2019 11:22:43 step: 9892, epoch: 299, batch: 24, loss: 0.0031262487173080444, acc: 100.0, f1: 100.0, r: 0.7269133620350697
06/02/2019 11:22:46 step: 9897, epoch: 299, batch: 29, loss: 0.002291247248649597, acc: 100.0, f1: 100.0, r: 0.7561107564203141
06/02/2019 11:22:47 *** evaluating ***
06/02/2019 11:22:48 step: 300, epoch: 299, acc: 50.85470085470085, f1: 28.933681594250295, r: 0.23523120326756822
06/02/2019 11:22:48 
*** Best acc model ***
epoch: 37
acc: 58.119658119658126
f1: 23.15816397994166
corr: 0.3295768368380554
06/02/2019 11:22:48 Loading Test Data
06/02/2019 11:22:48 load data from data/word2vec_temp/test_text.npy, data/word2vec_temp/test_label.npy, training: False
06/02/2019 11:23:08 loaded. total len: 2228
06/02/2019 11:23:08 Test: length: 2228, total batch: 35, batch size: 64
06/02/2019 11:23:08 
*** Test Result ***
acc: 50.85470085470085
f1: 28.933681594250295
corr: 0.23523120326756822
