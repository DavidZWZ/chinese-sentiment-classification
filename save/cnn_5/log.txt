06/02/2019 09:13:00 {'input_path': 'data/word2vec_temp', 'output_path': 'save/cnn_5', 'gpu': True, 'seed': 20000125, 'display_per_batch': 5, 'optimizer': 'adagrad', 'lr': 0.01, 'lr_decay': 0, 'weight_decay': 0.0001, 'momentum': 0.985, 'num_epochs': 300, 'batch_size': 64, 'num_labels': 8, 'embedding_size': 300, 'type': 'cnn', 'cnn': {'max_length': 512, 'conv_1': {'size': 512, 'kernel_size': 3, 'dropout': 0.9}, 'max_pool_1': {'kernel_size': 2, 'stride': 2}, 'fc': {'hidden_size': 512, 'dropout': 0.9}, 'loss': 'cross_entropy'}}
06/02/2019 09:13:00 Loading Train Data
06/02/2019 09:13:00 load data from data/word2vec_temp/train_text.npy, data/word2vec_temp/train_label.npy, training: True
06/02/2019 09:13:25 loaded. total len: 2342
06/02/2019 09:13:25 Train: length: 2108, total batch: 33, batch size: 64
06/02/2019 09:13:25 Dev: length: 234, total batch: 4, batch size: 64
06/02/2019 09:13:25 Loading model cnn
06/02/2019 09:13:34 *** epoch: 1 ***
06/02/2019 09:13:34 *** training ***
06/02/2019 09:13:34 step: 5, epoch: 0, batch: 4, loss: 2.40442156791687, acc: 29.6875, f1: 7.406655844155844, r: -0.06375403178297467
06/02/2019 09:13:35 step: 10, epoch: 0, batch: 9, loss: 1.9233695268630981, acc: 28.125, f1: 6.743243243243244, r: 0.02697821188181055
06/02/2019 09:13:35 step: 15, epoch: 0, batch: 14, loss: 1.9063915014266968, acc: 26.5625, f1: 8.455882352941178, r: 0.030156396876557277
06/02/2019 09:13:35 step: 20, epoch: 0, batch: 19, loss: 1.8543461561203003, acc: 32.8125, f1: 12.208293153326904, r: 0.07199304672045331
06/02/2019 09:13:35 step: 25, epoch: 0, batch: 24, loss: 1.8168048858642578, acc: 28.125, f1: 10.476190476190476, r: 0.10264288121319588
06/02/2019 09:13:36 step: 30, epoch: 0, batch: 29, loss: 1.8290183544158936, acc: 28.125, f1: 8.71740200098409, r: 0.08373395673583398
06/02/2019 09:13:36 *** evaluating ***
06/02/2019 09:13:36 step: 1, epoch: 0, acc: 44.871794871794876, f1: 7.766272189349112, r: 0.2148984676222541
06/02/2019 09:13:36 *** epoch: 2 ***
06/02/2019 09:13:36 *** training ***
06/02/2019 09:13:36 step: 38, epoch: 1, batch: 4, loss: 1.8360562324523926, acc: 21.875, f1: 5.966353677621282, r: 0.168002787350362
06/02/2019 09:13:36 step: 43, epoch: 1, batch: 9, loss: 1.738628625869751, acc: 35.9375, f1: 6.686046511627907, r: 0.20504151270025422
06/02/2019 09:13:36 step: 48, epoch: 1, batch: 14, loss: 1.6604851484298706, acc: 35.9375, f1: 11.568711568711567, r: 0.19072262363743653
06/02/2019 09:13:37 step: 53, epoch: 1, batch: 19, loss: 1.7333552837371826, acc: 43.75, f1: 10.874861572535991, r: 0.05072609742048419
06/02/2019 09:13:37 step: 58, epoch: 1, batch: 24, loss: 1.6154417991638184, acc: 40.625, f1: 11.886860007160758, r: 0.12299796067983401
06/02/2019 09:13:37 step: 63, epoch: 1, batch: 29, loss: 1.7419557571411133, acc: 31.25, f1: 9.801684801684802, r: 0.22587026371469882
06/02/2019 09:13:37 *** evaluating ***
06/02/2019 09:13:37 step: 2, epoch: 1, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.20821240740547933
06/02/2019 09:13:37 *** epoch: 3 ***
06/02/2019 09:13:37 *** training ***
06/02/2019 09:13:38 step: 71, epoch: 2, batch: 4, loss: 1.8327043056488037, acc: 32.8125, f1: 10.028449502133713, r: 0.16057700716439033
06/02/2019 09:13:38 step: 76, epoch: 2, batch: 9, loss: 1.5963901281356812, acc: 53.125, f1: 18.360339351051422, r: 0.10141359104538826
06/02/2019 09:13:38 step: 81, epoch: 2, batch: 14, loss: 1.7558890581130981, acc: 40.625, f1: 13.21021021021021, r: 0.19816670001152434
06/02/2019 09:13:38 step: 86, epoch: 2, batch: 19, loss: 1.5468844175338745, acc: 50.0, f1: 13.636363636363638, r: 0.10923521762795199
06/02/2019 09:13:39 step: 91, epoch: 2, batch: 24, loss: 1.580138087272644, acc: 42.1875, f1: 13.736263736263737, r: 0.29132203094702547
06/02/2019 09:13:39 step: 96, epoch: 2, batch: 29, loss: 1.7155288457870483, acc: 39.0625, f1: 12.551510989010989, r: 0.14226419202903004
06/02/2019 09:13:39 *** evaluating ***
06/02/2019 09:13:39 step: 3, epoch: 2, acc: 44.871794871794876, f1: 7.859281437125748, r: 0.198226757208096
06/02/2019 09:13:39 *** epoch: 4 ***
06/02/2019 09:13:39 *** training ***
06/02/2019 09:13:39 step: 104, epoch: 3, batch: 4, loss: 1.6330368518829346, acc: 35.9375, f1: 9.975889089813139, r: 0.06851066211838015
06/02/2019 09:13:40 step: 109, epoch: 3, batch: 9, loss: 1.5142204761505127, acc: 46.875, f1: 17.166439497198244, r: 0.21160325514682007
06/02/2019 09:13:40 step: 114, epoch: 3, batch: 14, loss: 1.7187126874923706, acc: 35.9375, f1: 9.134615384615385, r: 0.13337963401728753
06/02/2019 09:13:40 step: 119, epoch: 3, batch: 19, loss: 1.415265440940857, acc: 48.4375, f1: 12.579533404029695, r: 0.20793531757095138
06/02/2019 09:13:40 step: 124, epoch: 3, batch: 24, loss: 1.5718157291412354, acc: 37.5, f1: 8.856421356421357, r: 0.1766758845775092
06/02/2019 09:13:41 step: 129, epoch: 3, batch: 29, loss: 1.7485026121139526, acc: 39.0625, f1: 13.849206349206348, r: 0.12232382700266425
06/02/2019 09:13:41 *** evaluating ***
06/02/2019 09:13:41 step: 4, epoch: 3, acc: 44.871794871794876, f1: 7.859281437125748, r: 0.19935118312042477
06/02/2019 09:13:41 *** epoch: 5 ***
06/02/2019 09:13:41 *** training ***
06/02/2019 09:13:41 step: 137, epoch: 4, batch: 4, loss: 1.3904565572738647, acc: 48.4375, f1: 17.02047952047952, r: 0.21638029537326964
06/02/2019 09:13:41 step: 142, epoch: 4, batch: 9, loss: 1.6807305812835693, acc: 32.8125, f1: 11.020531400966185, r: 0.14592946170414173
06/02/2019 09:13:42 step: 147, epoch: 4, batch: 14, loss: 1.4822574853897095, acc: 39.0625, f1: 15.897435897435896, r: 0.18364863078081417
06/02/2019 09:13:42 step: 152, epoch: 4, batch: 19, loss: 1.5603487491607666, acc: 43.75, f1: 12.48945147679325, r: 0.14526770629983954
06/02/2019 09:13:42 step: 157, epoch: 4, batch: 24, loss: 1.5432695150375366, acc: 45.3125, f1: 17.354497354497354, r: 0.19198299051835158
06/02/2019 09:13:42 step: 162, epoch: 4, batch: 29, loss: 1.8106892108917236, acc: 31.25, f1: 9.122670807453417, r: 0.18207990798316406
06/02/2019 09:13:42 *** evaluating ***
06/02/2019 09:13:43 step: 5, epoch: 4, acc: 45.2991452991453, f1: 8.406626506024097, r: 0.21139045591262173
06/02/2019 09:13:43 *** epoch: 6 ***
06/02/2019 09:13:43 *** training ***
06/02/2019 09:13:43 step: 170, epoch: 5, batch: 4, loss: 1.779141902923584, acc: 39.0625, f1: 7.8125, r: 0.14858598231158218
06/02/2019 09:13:43 step: 175, epoch: 5, batch: 9, loss: 1.5121210813522339, acc: 40.625, f1: 10.51801363193768, r: 0.21556967871628846
06/02/2019 09:13:43 step: 180, epoch: 5, batch: 14, loss: 1.596372127532959, acc: 42.1875, f1: 14.121212121212121, r: 0.23521904226609014
06/02/2019 09:13:43 step: 185, epoch: 5, batch: 19, loss: 1.6090223789215088, acc: 35.9375, f1: 8.73076923076923, r: 0.18060531486268733
06/02/2019 09:13:44 step: 190, epoch: 5, batch: 24, loss: 1.4032589197158813, acc: 45.3125, f1: 17.598007364089238, r: 0.22479503676663712
06/02/2019 09:13:44 step: 195, epoch: 5, batch: 29, loss: 1.6094261407852173, acc: 50.0, f1: 18.136249715197085, r: 0.12321541649787873
06/02/2019 09:13:44 *** evaluating ***
06/02/2019 09:13:44 step: 6, epoch: 5, acc: 45.72649572649573, f1: 8.863275039745627, r: 0.21644534052025607
06/02/2019 09:13:44 *** epoch: 7 ***
06/02/2019 09:13:44 *** training ***
06/02/2019 09:13:44 step: 203, epoch: 6, batch: 4, loss: 1.474685549736023, acc: 40.625, f1: 14.049886621315192, r: 0.22203161379074612
06/02/2019 09:13:45 step: 208, epoch: 6, batch: 9, loss: 1.5092253684997559, acc: 39.0625, f1: 11.879485645933014, r: 0.20803165115522335
06/02/2019 09:13:45 step: 213, epoch: 6, batch: 14, loss: 1.6373403072357178, acc: 35.9375, f1: 9.88471177944862, r: 0.18004244169514755
06/02/2019 09:13:45 step: 218, epoch: 6, batch: 19, loss: 1.6299962997436523, acc: 39.0625, f1: 11.25572082379863, r: 0.22166975983651885
06/02/2019 09:13:45 step: 223, epoch: 6, batch: 24, loss: 1.4568841457366943, acc: 42.1875, f1: 16.794432044269808, r: 0.28495896009309674
06/02/2019 09:13:46 step: 228, epoch: 6, batch: 29, loss: 1.6452070474624634, acc: 45.3125, f1: 17.72331356560415, r: 0.20301278731295808
06/02/2019 09:13:46 *** evaluating ***
06/02/2019 09:13:46 step: 7, epoch: 6, acc: 46.58119658119658, f1: 9.817306047996352, r: 0.22676244142655744
06/02/2019 09:13:46 *** epoch: 8 ***
06/02/2019 09:13:46 *** training ***
06/02/2019 09:13:46 step: 236, epoch: 7, batch: 4, loss: 1.4744839668273926, acc: 42.1875, f1: 12.656633906633905, r: 0.227337854095043
06/02/2019 09:13:46 step: 241, epoch: 7, batch: 9, loss: 1.493167519569397, acc: 45.3125, f1: 12.599573257467997, r: 0.2217707425733858
06/02/2019 09:13:46 step: 246, epoch: 7, batch: 14, loss: 1.5089149475097656, acc: 39.0625, f1: 13.069143446852424, r: 0.3254237837986527
06/02/2019 09:13:47 step: 251, epoch: 7, batch: 19, loss: 1.4927148818969727, acc: 39.0625, f1: 15.436609426379249, r: 0.23298710815640755
06/02/2019 09:13:47 step: 256, epoch: 7, batch: 24, loss: 1.5632891654968262, acc: 43.75, f1: 13.749999999999998, r: 0.23565681341742017
06/02/2019 09:13:47 step: 261, epoch: 7, batch: 29, loss: 1.4527294635772705, acc: 43.75, f1: 15.00375939849624, r: 0.17672941527893538
06/02/2019 09:13:47 *** evaluating ***
06/02/2019 09:13:47 step: 8, epoch: 7, acc: 50.0, f1: 14.222096791375854, r: 0.236392496679546
06/02/2019 09:13:47 *** epoch: 9 ***
06/02/2019 09:13:47 *** training ***
06/02/2019 09:13:48 step: 269, epoch: 8, batch: 4, loss: 1.5997518301010132, acc: 37.5, f1: 14.78937728937729, r: 0.21376385084645788
06/02/2019 09:13:48 step: 274, epoch: 8, batch: 9, loss: 1.5210041999816895, acc: 37.5, f1: 14.925595238095235, r: 0.2383757468736237
06/02/2019 09:13:48 step: 279, epoch: 8, batch: 14, loss: 1.49201500415802, acc: 48.4375, f1: 20.351473922902493, r: 0.2131446786186179
06/02/2019 09:13:48 step: 284, epoch: 8, batch: 19, loss: 1.4739525318145752, acc: 42.1875, f1: 13.645021645021643, r: 0.18482347910753796
06/02/2019 09:13:48 step: 289, epoch: 8, batch: 24, loss: 1.3698477745056152, acc: 54.6875, f1: 14.80978260869565, r: 0.30408605614728523
06/02/2019 09:13:49 step: 294, epoch: 8, batch: 29, loss: 1.4653195142745972, acc: 45.3125, f1: 18.312925170068027, r: 0.24109024545557753
06/02/2019 09:13:49 *** evaluating ***
06/02/2019 09:13:49 step: 9, epoch: 8, acc: 52.13675213675214, f1: 15.793630714180134, r: 0.2526000496920603
06/02/2019 09:13:49 *** epoch: 10 ***
06/02/2019 09:13:49 *** training ***
06/02/2019 09:13:49 step: 302, epoch: 9, batch: 4, loss: 1.4076406955718994, acc: 48.4375, f1: 15.033783783783782, r: 0.35662626966389066
06/02/2019 09:13:49 step: 307, epoch: 9, batch: 9, loss: 1.4319199323654175, acc: 48.4375, f1: 24.733969043670538, r: 0.29553977766379386
06/02/2019 09:13:50 step: 312, epoch: 9, batch: 14, loss: 1.248543381690979, acc: 64.0625, f1: 29.1578947368421, r: 0.20829726210681618
06/02/2019 09:13:50 step: 317, epoch: 9, batch: 19, loss: 1.5921045541763306, acc: 40.625, f1: 13.995215311004783, r: 0.26646646089525483
06/02/2019 09:13:50 step: 322, epoch: 9, batch: 24, loss: 1.5411866903305054, acc: 37.5, f1: 17.279272451686243, r: 0.21251586691211288
06/02/2019 09:13:50 step: 327, epoch: 9, batch: 29, loss: 1.6600830554962158, acc: 39.0625, f1: 18.218150896722324, r: 0.22167701228723646
06/02/2019 09:13:50 *** evaluating ***
06/02/2019 09:13:51 step: 10, epoch: 9, acc: 53.41880341880342, f1: 16.945421120161804, r: 0.270016691844083
06/02/2019 09:13:51 *** epoch: 11 ***
06/02/2019 09:13:51 *** training ***
06/02/2019 09:13:51 step: 335, epoch: 10, batch: 4, loss: 1.484262466430664, acc: 48.4375, f1: 23.12221440300258, r: 0.2831903456993254
06/02/2019 09:13:51 step: 340, epoch: 10, batch: 9, loss: 1.3839441537857056, acc: 50.0, f1: 20.940550133096718, r: 0.2886334392109453
06/02/2019 09:13:51 step: 345, epoch: 10, batch: 14, loss: 1.453942894935608, acc: 53.125, f1: 19.00928932178932, r: 0.27660030225412624
06/02/2019 09:13:51 step: 350, epoch: 10, batch: 19, loss: 1.36464262008667, acc: 51.5625, f1: 21.321733821733822, r: 0.2908993710373608
06/02/2019 09:13:52 step: 355, epoch: 10, batch: 24, loss: 1.3794529438018799, acc: 43.75, f1: 11.708683473389355, r: 0.3031835245400653
06/02/2019 09:13:52 step: 360, epoch: 10, batch: 29, loss: 1.2573024034500122, acc: 56.25, f1: 24.01360544217687, r: 0.26508674117926123
06/02/2019 09:13:52 *** evaluating ***
06/02/2019 09:13:52 step: 11, epoch: 10, acc: 54.700854700854705, f1: 17.715942170434996, r: 0.2805759849838654
06/02/2019 09:13:52 *** epoch: 12 ***
06/02/2019 09:13:52 *** training ***
06/02/2019 09:13:52 step: 368, epoch: 11, batch: 4, loss: 1.5095559358596802, acc: 45.3125, f1: 17.75744416873449, r: 0.29004742170896536
06/02/2019 09:13:53 step: 373, epoch: 11, batch: 9, loss: 1.616199254989624, acc: 35.9375, f1: 14.51881451881452, r: 0.26327939204674905
06/02/2019 09:13:53 step: 378, epoch: 11, batch: 14, loss: 1.4882144927978516, acc: 46.875, f1: 25.10893246187364, r: 0.3116678354568955
06/02/2019 09:13:53 step: 383, epoch: 11, batch: 19, loss: 1.5603655576705933, acc: 45.3125, f1: 21.37285051277349, r: 0.281312337453967
06/02/2019 09:13:53 step: 388, epoch: 11, batch: 24, loss: 1.6252939701080322, acc: 42.1875, f1: 16.015151515151516, r: 0.20819170988779429
06/02/2019 09:13:53 step: 393, epoch: 11, batch: 29, loss: 1.2298669815063477, acc: 53.125, f1: 21.313283208020053, r: 0.3119784692401215
06/02/2019 09:13:54 *** evaluating ***
06/02/2019 09:13:54 step: 12, epoch: 11, acc: 56.837606837606835, f1: 17.68798595513515, r: 0.2988601749788297
06/02/2019 09:13:54 *** epoch: 13 ***
06/02/2019 09:13:54 *** training ***
06/02/2019 09:13:54 step: 401, epoch: 12, batch: 4, loss: 1.2697325944900513, acc: 57.8125, f1: 25.698289651778023, r: 0.27017215844972425
06/02/2019 09:13:54 step: 406, epoch: 12, batch: 9, loss: 1.4873515367507935, acc: 48.4375, f1: 23.809523809523807, r: 0.3308870297592832
06/02/2019 09:13:54 step: 411, epoch: 12, batch: 14, loss: 1.2807745933532715, acc: 59.375, f1: 24.43963443963444, r: 0.25990597168610435
06/02/2019 09:13:55 step: 416, epoch: 12, batch: 19, loss: 1.4440958499908447, acc: 46.875, f1: 21.986210510800674, r: 0.2954292642331254
06/02/2019 09:13:55 step: 421, epoch: 12, batch: 24, loss: 1.2951993942260742, acc: 57.8125, f1: 25.566502463054185, r: 0.3401574983685483
06/02/2019 09:13:55 step: 426, epoch: 12, batch: 29, loss: 1.4091969728469849, acc: 43.75, f1: 19.716433941997852, r: 0.30641723352830064
06/02/2019 09:13:55 *** evaluating ***
06/02/2019 09:13:55 step: 13, epoch: 12, acc: 56.837606837606835, f1: 18.193660442900565, r: 0.3102260790909571
06/02/2019 09:13:55 *** epoch: 14 ***
06/02/2019 09:13:55 *** training ***
06/02/2019 09:13:56 step: 434, epoch: 13, batch: 4, loss: 1.3349348306655884, acc: 53.125, f1: 22.781385281385283, r: 0.3587664879548769
06/02/2019 09:13:56 step: 439, epoch: 13, batch: 9, loss: 1.4713600873947144, acc: 46.875, f1: 18.314516129032256, r: 0.29184664568497753
06/02/2019 09:13:56 step: 444, epoch: 13, batch: 14, loss: 1.2701361179351807, acc: 48.4375, f1: 22.71428571428571, r: 0.23650458409179673
06/02/2019 09:13:56 step: 449, epoch: 13, batch: 19, loss: 1.4114784002304077, acc: 45.3125, f1: 20.035448422545198, r: 0.23795519226630577
06/02/2019 09:13:56 step: 454, epoch: 13, batch: 24, loss: 1.547532558441162, acc: 43.75, f1: 19.55493115360288, r: 0.32364065163935624
06/02/2019 09:13:57 step: 459, epoch: 13, batch: 29, loss: 1.3837336301803589, acc: 56.25, f1: 19.994007789873162, r: 0.20596888432233385
06/02/2019 09:13:57 *** evaluating ***
06/02/2019 09:13:57 step: 14, epoch: 13, acc: 56.837606837606835, f1: 19.009048746951972, r: 0.2971723621534068
06/02/2019 09:13:57 *** epoch: 15 ***
06/02/2019 09:13:57 *** training ***
06/02/2019 09:13:57 step: 467, epoch: 14, batch: 4, loss: 1.3402142524719238, acc: 60.9375, f1: 32.85183066361556, r: 0.32289571392478594
06/02/2019 09:13:57 step: 472, epoch: 14, batch: 9, loss: 1.3067327737808228, acc: 51.5625, f1: 19.571847877907146, r: 0.2761794442024885
06/02/2019 09:13:58 step: 477, epoch: 14, batch: 14, loss: 1.4535374641418457, acc: 48.4375, f1: 20.24177432846919, r: 0.37837869850222194
06/02/2019 09:13:58 step: 482, epoch: 14, batch: 19, loss: 1.3245941400527954, acc: 53.125, f1: 24.304267161410024, r: 0.28804079954340106
06/02/2019 09:13:58 step: 487, epoch: 14, batch: 24, loss: 1.2106361389160156, acc: 59.375, f1: 22.428571428571427, r: 0.33464017061425066
06/02/2019 09:13:58 step: 492, epoch: 14, batch: 29, loss: 1.5130515098571777, acc: 56.25, f1: 26.97111631537861, r: 0.31318311612815264
06/02/2019 09:13:58 *** evaluating ***
06/02/2019 09:13:58 step: 15, epoch: 14, acc: 57.692307692307686, f1: 19.06299724809551, r: 0.31039382034647545
06/02/2019 09:13:58 *** epoch: 16 ***
06/02/2019 09:13:58 *** training ***
06/02/2019 09:13:59 step: 500, epoch: 15, batch: 4, loss: 1.4664497375488281, acc: 43.75, f1: 19.23195084485407, r: 0.282508075319739
06/02/2019 09:13:59 step: 505, epoch: 15, batch: 9, loss: 1.2905066013336182, acc: 56.25, f1: 25.80365143369176, r: 0.38660753961488253
06/02/2019 09:13:59 step: 510, epoch: 15, batch: 14, loss: 1.5639495849609375, acc: 42.1875, f1: 18.697405329593266, r: 0.3085803360283518
06/02/2019 09:13:59 step: 515, epoch: 15, batch: 19, loss: 1.3723855018615723, acc: 60.9375, f1: 24.64042599912165, r: 0.30187952156771525
06/02/2019 09:14:00 step: 520, epoch: 15, batch: 24, loss: 1.3995901346206665, acc: 54.6875, f1: 22.666666666666668, r: 0.33677902126720377
06/02/2019 09:14:00 step: 525, epoch: 15, batch: 29, loss: 1.416473627090454, acc: 53.125, f1: 20.261150234741784, r: 0.3154421685992934
06/02/2019 09:14:00 *** evaluating ***
06/02/2019 09:14:00 step: 16, epoch: 15, acc: 56.837606837606835, f1: 18.140400484322637, r: 0.3296757027789572
06/02/2019 09:14:00 *** epoch: 17 ***
06/02/2019 09:14:00 *** training ***
06/02/2019 09:14:00 step: 533, epoch: 16, batch: 4, loss: 1.274041771888733, acc: 53.125, f1: 21.7741935483871, r: 0.3666536939192894
06/02/2019 09:14:00 step: 538, epoch: 16, batch: 9, loss: 1.3016397953033447, acc: 51.5625, f1: 20.04385964912281, r: 0.3793852327425333
06/02/2019 09:14:01 step: 543, epoch: 16, batch: 14, loss: 1.4572967290878296, acc: 48.4375, f1: 19.92424242424243, r: 0.23778027882788944
06/02/2019 09:14:01 step: 548, epoch: 16, batch: 19, loss: 1.2297571897506714, acc: 57.8125, f1: 26.393542491623514, r: 0.37733404085391736
06/02/2019 09:14:01 step: 553, epoch: 16, batch: 24, loss: 1.2907826900482178, acc: 51.5625, f1: 19.985082485082486, r: 0.29024802989658904
06/02/2019 09:14:01 step: 558, epoch: 16, batch: 29, loss: 1.4625405073165894, acc: 51.5625, f1: 23.246158875449492, r: 0.20573538973802358
06/02/2019 09:14:02 *** evaluating ***
06/02/2019 09:14:02 step: 17, epoch: 16, acc: 57.692307692307686, f1: 19.886836864023174, r: 0.318080307213223
06/02/2019 09:14:02 *** epoch: 18 ***
06/02/2019 09:14:02 *** training ***
06/02/2019 09:14:02 step: 566, epoch: 17, batch: 4, loss: 1.2388529777526855, acc: 50.0, f1: 27.367757447806706, r: 0.34885330468024695
06/02/2019 09:14:02 step: 571, epoch: 17, batch: 9, loss: 1.2437487840652466, acc: 46.875, f1: 17.799145299145298, r: 0.32964209120399873
06/02/2019 09:14:02 step: 576, epoch: 17, batch: 14, loss: 1.277512788772583, acc: 59.375, f1: 28.26510721247563, r: 0.35460111775941994
06/02/2019 09:14:02 step: 581, epoch: 17, batch: 19, loss: 1.6305475234985352, acc: 45.3125, f1: 22.688825952372753, r: 0.31003506045124735
06/02/2019 09:14:03 step: 586, epoch: 17, batch: 24, loss: 1.3461819887161255, acc: 50.0, f1: 22.847985347985347, r: 0.34943466251631483
06/02/2019 09:14:03 step: 591, epoch: 17, batch: 29, loss: 1.0824185609817505, acc: 67.1875, f1: 53.550768228187586, r: 0.32744846450542037
06/02/2019 09:14:03 *** evaluating ***
06/02/2019 09:14:03 step: 18, epoch: 17, acc: 58.119658119658126, f1: 19.638551712670438, r: 0.3286747600536615
06/02/2019 09:14:03 *** epoch: 19 ***
06/02/2019 09:14:03 *** training ***
06/02/2019 09:14:03 step: 599, epoch: 18, batch: 4, loss: 1.4937835931777954, acc: 51.5625, f1: 26.118324222035326, r: 0.3332947292603893
06/02/2019 09:14:04 step: 604, epoch: 18, batch: 9, loss: 1.3753610849380493, acc: 56.25, f1: 31.752495462794915, r: 0.39167699574422105
06/02/2019 09:14:04 step: 609, epoch: 18, batch: 14, loss: 1.1153185367584229, acc: 62.5, f1: 36.27705627705628, r: 0.31340213797018757
06/02/2019 09:14:04 step: 614, epoch: 18, batch: 19, loss: 1.117915391921997, acc: 57.8125, f1: 24.416433239962654, r: 0.3586698350822765
06/02/2019 09:14:04 step: 619, epoch: 18, batch: 24, loss: 1.4091280698776245, acc: 56.25, f1: 22.225490196078436, r: 0.3859632724228411
06/02/2019 09:14:05 step: 624, epoch: 18, batch: 29, loss: 1.2771800756454468, acc: 50.0, f1: 22.698070607553365, r: 0.34261352399369466
06/02/2019 09:14:05 *** evaluating ***
06/02/2019 09:14:05 step: 19, epoch: 18, acc: 57.26495726495726, f1: 19.638832753821312, r: 0.3243678826145515
06/02/2019 09:14:05 *** epoch: 20 ***
06/02/2019 09:14:05 *** training ***
06/02/2019 09:14:05 step: 632, epoch: 19, batch: 4, loss: 1.2448862791061401, acc: 57.8125, f1: 34.42995108491157, r: 0.37624364638407465
06/02/2019 09:14:05 step: 637, epoch: 19, batch: 9, loss: 1.1287444829940796, acc: 62.5, f1: 29.07337526205451, r: 0.4338832261571105
06/02/2019 09:14:05 step: 642, epoch: 19, batch: 14, loss: 1.108418583869934, acc: 56.25, f1: 28.23071601521964, r: 0.38877767459773804
06/02/2019 09:14:06 step: 647, epoch: 19, batch: 19, loss: 1.150107502937317, acc: 59.375, f1: 22.385830132309003, r: 0.33908336897424224
06/02/2019 09:14:06 step: 652, epoch: 19, batch: 24, loss: 1.2651864290237427, acc: 51.5625, f1: 30.759144237405106, r: 0.3821865420054834
06/02/2019 09:14:06 step: 657, epoch: 19, batch: 29, loss: 1.1566886901855469, acc: 56.25, f1: 21.967005888376857, r: 0.3785020849315091
06/02/2019 09:14:06 *** evaluating ***
06/02/2019 09:14:06 step: 20, epoch: 19, acc: 56.41025641025641, f1: 18.890703875590294, r: 0.3282324116503062
06/02/2019 09:14:06 *** epoch: 21 ***
06/02/2019 09:14:06 *** training ***
06/02/2019 09:14:07 step: 665, epoch: 20, batch: 4, loss: 1.0734155178070068, acc: 59.375, f1: 27.786691542288555, r: 0.3988318407885436
06/02/2019 09:14:07 step: 670, epoch: 20, batch: 9, loss: 1.1618038415908813, acc: 53.125, f1: 25.778522552716097, r: 0.3455554433470086
06/02/2019 09:14:07 step: 675, epoch: 20, batch: 14, loss: 0.9021139144897461, acc: 64.0625, f1: 33.11542202574025, r: 0.49407525045676
06/02/2019 09:14:07 step: 680, epoch: 20, batch: 19, loss: 1.1612976789474487, acc: 59.375, f1: 31.330309901738467, r: 0.48852878356286394
06/02/2019 09:14:08 step: 685, epoch: 20, batch: 24, loss: 1.2389839887619019, acc: 56.25, f1: 30.473585322723252, r: 0.40317521697228775
06/02/2019 09:14:08 step: 690, epoch: 20, batch: 29, loss: 1.1944164037704468, acc: 62.5, f1: 35.42124542124542, r: 0.3913051746131962
06/02/2019 09:14:08 *** evaluating ***
06/02/2019 09:14:08 step: 21, epoch: 20, acc: 57.26495726495726, f1: 19.150313621326127, r: 0.3376122877997432
06/02/2019 09:14:08 *** epoch: 22 ***
06/02/2019 09:14:08 *** training ***
06/02/2019 09:14:08 step: 698, epoch: 21, batch: 4, loss: 1.2012578248977661, acc: 51.5625, f1: 19.202670503079624, r: 0.2151280991763694
06/02/2019 09:14:08 step: 703, epoch: 21, batch: 9, loss: 1.380756139755249, acc: 46.875, f1: 27.06669226830517, r: 0.32692047325487467
06/02/2019 09:14:09 step: 708, epoch: 21, batch: 14, loss: 1.1105362176895142, acc: 70.3125, f1: 40.63118531203637, r: 0.3322951354402716
06/02/2019 09:14:09 step: 713, epoch: 21, batch: 19, loss: 1.4436254501342773, acc: 51.5625, f1: 31.940412528647823, r: 0.41754738987604545
06/02/2019 09:14:09 step: 718, epoch: 21, batch: 24, loss: 1.0497061014175415, acc: 71.875, f1: 39.53803427487638, r: 0.37032656540843767
06/02/2019 09:14:09 step: 723, epoch: 21, batch: 29, loss: 1.393788456916809, acc: 48.4375, f1: 24.33359213250518, r: 0.3556237105063629
06/02/2019 09:14:09 *** evaluating ***
06/02/2019 09:14:10 step: 22, epoch: 21, acc: 58.97435897435898, f1: 19.652882205513784, r: 0.34096078889728537
06/02/2019 09:14:10 *** epoch: 23 ***
06/02/2019 09:14:10 *** training ***
06/02/2019 09:14:10 step: 731, epoch: 22, batch: 4, loss: 0.837480902671814, acc: 65.625, f1: 44.03093434343434, r: 0.378299193036723
06/02/2019 09:14:10 step: 736, epoch: 22, batch: 9, loss: 0.94624924659729, acc: 65.625, f1: 29.339349376114082, r: 0.4484601169017703
06/02/2019 09:14:10 step: 741, epoch: 22, batch: 14, loss: 1.0291001796722412, acc: 64.0625, f1: 40.28364033785557, r: 0.4199597971611455
06/02/2019 09:14:10 step: 746, epoch: 22, batch: 19, loss: 1.3173433542251587, acc: 57.8125, f1: 27.4885357368754, r: 0.4006137074669184
06/02/2019 09:14:11 step: 751, epoch: 22, batch: 24, loss: 1.3333708047866821, acc: 53.125, f1: 23.038194444444443, r: 0.33990289030569776
06/02/2019 09:14:11 step: 756, epoch: 22, batch: 29, loss: 1.4326889514923096, acc: 53.125, f1: 26.652805949966197, r: 0.3022428613308977
06/02/2019 09:14:11 *** evaluating ***
06/02/2019 09:14:11 step: 23, epoch: 22, acc: 58.97435897435898, f1: 19.655859916782248, r: 0.3404673274600616
06/02/2019 09:14:11 *** epoch: 24 ***
06/02/2019 09:14:11 *** training ***
06/02/2019 09:14:11 step: 764, epoch: 23, batch: 4, loss: 1.276492714881897, acc: 62.5, f1: 49.617816091954026, r: 0.46434269528542366
06/02/2019 09:14:12 step: 769, epoch: 23, batch: 9, loss: 1.1225173473358154, acc: 64.0625, f1: 44.184265010351965, r: 0.4372767478964419
06/02/2019 09:14:12 step: 774, epoch: 23, batch: 14, loss: 1.0150431394577026, acc: 65.625, f1: 40.2242418706613, r: 0.386352533444516
06/02/2019 09:14:12 step: 779, epoch: 23, batch: 19, loss: 1.1040767431259155, acc: 62.5, f1: 28.19548872180451, r: 0.3635899749529752
06/02/2019 09:14:12 step: 784, epoch: 23, batch: 24, loss: 0.9298452734947205, acc: 71.875, f1: 27.370600414078673, r: 0.44650762901728236
06/02/2019 09:14:12 step: 789, epoch: 23, batch: 29, loss: 1.1968202590942383, acc: 56.25, f1: 29.97831016464557, r: 0.41282759697063826
06/02/2019 09:14:12 *** evaluating ***
06/02/2019 09:14:13 step: 24, epoch: 23, acc: 59.82905982905983, f1: 20.174307696194855, r: 0.3415962016596773
06/02/2019 09:14:13 *** epoch: 25 ***
06/02/2019 09:14:13 *** training ***
06/02/2019 09:14:13 step: 797, epoch: 24, batch: 4, loss: 1.172308087348938, acc: 62.5, f1: 38.48051948051948, r: 0.4702671551654861
06/02/2019 09:14:13 step: 802, epoch: 24, batch: 9, loss: 0.9136120080947876, acc: 65.625, f1: 36.75000000000001, r: 0.4274946011803622
06/02/2019 09:14:13 step: 807, epoch: 24, batch: 14, loss: 0.9822583198547363, acc: 62.5, f1: 40.26523417827766, r: 0.540498341239826
06/02/2019 09:14:13 step: 812, epoch: 24, batch: 19, loss: 1.1077666282653809, acc: 60.9375, f1: 31.419457735247214, r: 0.3575981466512963
06/02/2019 09:14:14 step: 817, epoch: 24, batch: 24, loss: 1.2420144081115723, acc: 53.125, f1: 33.813364055299544, r: 0.36682989114741316
06/02/2019 09:14:14 step: 822, epoch: 24, batch: 29, loss: 1.0315985679626465, acc: 70.3125, f1: 49.98106060606061, r: 0.4872928698364525
06/02/2019 09:14:14 *** evaluating ***
06/02/2019 09:14:14 step: 25, epoch: 24, acc: 56.837606837606835, f1: 19.48384351390131, r: 0.34259578598490775
06/02/2019 09:14:14 *** epoch: 26 ***
06/02/2019 09:14:14 *** training ***
06/02/2019 09:14:14 step: 830, epoch: 25, batch: 4, loss: 1.1558046340942383, acc: 57.8125, f1: 35.15064102564102, r: 0.4659276950129311
06/02/2019 09:14:14 step: 835, epoch: 25, batch: 9, loss: 0.838830292224884, acc: 67.1875, f1: 37.05627705627706, r: 0.5166124668256051
06/02/2019 09:14:15 step: 840, epoch: 25, batch: 14, loss: 0.8252643942832947, acc: 78.125, f1: 60.043468664158326, r: 0.5158060066220813
06/02/2019 09:14:15 step: 845, epoch: 25, batch: 19, loss: 1.0107395648956299, acc: 65.625, f1: 44.97190180419994, r: 0.47790228086270997
06/02/2019 09:14:15 step: 850, epoch: 25, batch: 24, loss: 1.1440647840499878, acc: 54.6875, f1: 42.718091998704246, r: 0.3118762300982648
06/02/2019 09:14:15 step: 855, epoch: 25, batch: 29, loss: 0.9846314191818237, acc: 62.5, f1: 41.97278911564625, r: 0.47322063494683164
06/02/2019 09:14:15 *** evaluating ***
06/02/2019 09:14:15 step: 26, epoch: 25, acc: 57.692307692307686, f1: 19.82264059469942, r: 0.3454309220027701
06/02/2019 09:14:15 *** epoch: 27 ***
06/02/2019 09:14:15 *** training ***
06/02/2019 09:14:16 step: 863, epoch: 26, batch: 4, loss: 1.0309045314788818, acc: 62.5, f1: 46.18337789661319, r: 0.531730817066731
06/02/2019 09:14:16 step: 868, epoch: 26, batch: 9, loss: 0.929986834526062, acc: 65.625, f1: 32.88684108645598, r: 0.3856184174899723
06/02/2019 09:14:16 step: 873, epoch: 26, batch: 14, loss: 0.7763359546661377, acc: 71.875, f1: 44.914021164021165, r: 0.48950689580386325
06/02/2019 09:14:16 step: 878, epoch: 26, batch: 19, loss: 0.8479583263397217, acc: 67.1875, f1: 47.960774953256156, r: 0.4046893695187711
06/02/2019 09:14:16 step: 883, epoch: 26, batch: 24, loss: 0.9648886322975159, acc: 59.375, f1: 35.30114484818318, r: 0.36581698913851945
06/02/2019 09:14:17 step: 888, epoch: 26, batch: 29, loss: 1.0739439725875854, acc: 59.375, f1: 37.02944642900537, r: 0.5051069831644128
06/02/2019 09:14:17 *** evaluating ***
06/02/2019 09:14:17 step: 27, epoch: 26, acc: 59.401709401709404, f1: 22.22101182375489, r: 0.35516222762192784
06/02/2019 09:14:17 *** epoch: 28 ***
06/02/2019 09:14:17 *** training ***
06/02/2019 09:14:17 step: 896, epoch: 27, batch: 4, loss: 1.0289405584335327, acc: 64.0625, f1: 43.170634920634924, r: 0.3918333630587657
06/02/2019 09:14:17 step: 901, epoch: 27, batch: 9, loss: 0.8354735970497131, acc: 75.0, f1: 54.35543929110106, r: 0.5005914154871975
06/02/2019 09:14:17 step: 906, epoch: 27, batch: 14, loss: 0.7351912260055542, acc: 73.4375, f1: 45.29938661681681, r: 0.4259072980858759
06/02/2019 09:14:18 step: 911, epoch: 27, batch: 19, loss: 0.9811489582061768, acc: 65.625, f1: 57.041969330104926, r: 0.45629821458525976
06/02/2019 09:14:18 step: 916, epoch: 27, batch: 24, loss: 1.3794044256210327, acc: 54.6875, f1: 32.04638752052546, r: 0.34835005129125984
06/02/2019 09:14:18 step: 921, epoch: 27, batch: 29, loss: 1.2161645889282227, acc: 59.375, f1: 43.75493458154749, r: 0.3392409474885455
06/02/2019 09:14:18 *** evaluating ***
06/02/2019 09:14:18 step: 28, epoch: 27, acc: 59.401709401709404, f1: 21.895112924410302, r: 0.36116562349867837
06/02/2019 09:14:18 *** epoch: 29 ***
06/02/2019 09:14:18 *** training ***
06/02/2019 09:14:19 step: 929, epoch: 28, batch: 4, loss: 0.8563094735145569, acc: 70.3125, f1: 39.06830104923324, r: 0.4468318096811751
06/02/2019 09:14:19 step: 934, epoch: 28, batch: 9, loss: 1.0625337362289429, acc: 62.5, f1: 43.18681318681318, r: 0.46259847164552065
06/02/2019 09:14:19 step: 939, epoch: 28, batch: 14, loss: 0.6245360970497131, acc: 81.25, f1: 57.24242424242424, r: 0.35723571907946766
06/02/2019 09:14:19 step: 944, epoch: 28, batch: 19, loss: 0.7703445553779602, acc: 71.875, f1: 56.04791198776161, r: 0.543330549142425
06/02/2019 09:14:19 step: 949, epoch: 28, batch: 24, loss: 0.9654873609542847, acc: 57.8125, f1: 32.67832053076415, r: 0.49127880169687016
06/02/2019 09:14:20 step: 954, epoch: 28, batch: 29, loss: 0.8992481231689453, acc: 70.3125, f1: 48.427197802197796, r: 0.46929252148247164
06/02/2019 09:14:20 *** evaluating ***
06/02/2019 09:14:20 step: 29, epoch: 28, acc: 58.54700854700855, f1: 21.966952834583534, r: 0.3558124132116197
06/02/2019 09:14:20 *** epoch: 30 ***
06/02/2019 09:14:20 *** training ***
06/02/2019 09:14:20 step: 962, epoch: 29, batch: 4, loss: 1.0332541465759277, acc: 62.5, f1: 41.18822265748405, r: 0.41312404687978077
06/02/2019 09:14:20 step: 967, epoch: 29, batch: 9, loss: 0.7717177867889404, acc: 67.1875, f1: 56.738437001594896, r: 0.4794436991451824
06/02/2019 09:14:21 step: 972, epoch: 29, batch: 14, loss: 0.8555722236633301, acc: 76.5625, f1: 71.34083978418954, r: 0.48105836047810274
06/02/2019 09:14:21 step: 977, epoch: 29, batch: 19, loss: 0.7985200881958008, acc: 78.125, f1: 62.06671982987773, r: 0.576081241299153
06/02/2019 09:14:21 step: 982, epoch: 29, batch: 24, loss: 0.8174208998680115, acc: 70.3125, f1: 48.82246376811594, r: 0.5234651296153107
06/02/2019 09:14:21 step: 987, epoch: 29, batch: 29, loss: 1.0257912874221802, acc: 67.1875, f1: 44.29931972789117, r: 0.40393695278385483
06/02/2019 09:14:21 *** evaluating ***
06/02/2019 09:14:22 step: 30, epoch: 29, acc: 58.97435897435898, f1: 22.171059902740936, r: 0.3679246470250878
06/02/2019 09:14:22 *** epoch: 31 ***
06/02/2019 09:14:22 *** training ***
06/02/2019 09:14:22 step: 995, epoch: 30, batch: 4, loss: 0.8386499881744385, acc: 71.875, f1: 48.360131931560495, r: 0.5527630235404443
06/02/2019 09:14:22 step: 1000, epoch: 30, batch: 9, loss: 1.119950771331787, acc: 53.125, f1: 37.54545454545455, r: 0.4795952161761615
06/02/2019 09:14:22 step: 1005, epoch: 30, batch: 14, loss: 1.0104055404663086, acc: 59.375, f1: 48.06972789115646, r: 0.42451372830843787
06/02/2019 09:14:22 step: 1010, epoch: 30, batch: 19, loss: 0.8699595928192139, acc: 73.4375, f1: 53.5947385947386, r: 0.4440509802012233
06/02/2019 09:14:23 step: 1015, epoch: 30, batch: 24, loss: 0.7254642248153687, acc: 78.125, f1: 67.22581932476986, r: 0.551048563571978
06/02/2019 09:14:23 step: 1020, epoch: 30, batch: 29, loss: 0.8265165090560913, acc: 75.0, f1: 58.57770800627943, r: 0.42952172658569493
06/02/2019 09:14:23 *** evaluating ***
06/02/2019 09:14:23 step: 31, epoch: 30, acc: 58.97435897435898, f1: 21.402189978489513, r: 0.36541198921125184
06/02/2019 09:14:23 *** epoch: 32 ***
06/02/2019 09:14:23 *** training ***
06/02/2019 09:14:23 step: 1028, epoch: 31, batch: 4, loss: 0.8726481199264526, acc: 68.75, f1: 57.76625031283417, r: 0.5132568047857206
06/02/2019 09:14:24 step: 1033, epoch: 31, batch: 9, loss: 1.0362262725830078, acc: 60.9375, f1: 43.07692307692307, r: 0.4147817718411032
06/02/2019 09:14:24 step: 1038, epoch: 31, batch: 14, loss: 0.831874430179596, acc: 68.75, f1: 37.87174303683737, r: 0.47004962934011807
06/02/2019 09:14:24 step: 1043, epoch: 31, batch: 19, loss: 0.753934383392334, acc: 71.875, f1: 49.60408684546616, r: 0.5090167132876744
06/02/2019 09:14:24 step: 1048, epoch: 31, batch: 24, loss: 0.7354665994644165, acc: 75.0, f1: 68.68659744599593, r: 0.5139990999425025
06/02/2019 09:14:24 step: 1053, epoch: 31, batch: 29, loss: 0.8228590488433838, acc: 71.875, f1: 61.52740174479305, r: 0.49472820239474796
06/02/2019 09:14:25 *** evaluating ***
06/02/2019 09:14:25 step: 32, epoch: 31, acc: 58.97435897435898, f1: 21.522214854111407, r: 0.371504505174684
06/02/2019 09:14:25 *** epoch: 33 ***
06/02/2019 09:14:25 *** training ***
06/02/2019 09:14:25 step: 1061, epoch: 32, batch: 4, loss: 0.8793115615844727, acc: 70.3125, f1: 54.0418118466899, r: 0.6158662954929566
06/02/2019 09:14:25 step: 1066, epoch: 32, batch: 9, loss: 0.860584020614624, acc: 70.3125, f1: 60.23632264233767, r: 0.5677366711702736
06/02/2019 09:14:25 step: 1071, epoch: 32, batch: 14, loss: 0.7457382082939148, acc: 76.5625, f1: 45.98928907721281, r: 0.4905976603138358
06/02/2019 09:14:26 step: 1076, epoch: 32, batch: 19, loss: 0.9238907098770142, acc: 59.375, f1: 31.0218253968254, r: 0.5796846765485437
06/02/2019 09:14:26 step: 1081, epoch: 32, batch: 24, loss: 0.5585328936576843, acc: 81.25, f1: 67.31413514022209, r: 0.5330275115613885
06/02/2019 09:14:26 step: 1086, epoch: 32, batch: 29, loss: 0.8881033658981323, acc: 62.5, f1: 41.28053737218157, r: 0.4656033049625601
06/02/2019 09:14:26 *** evaluating ***
06/02/2019 09:14:26 step: 33, epoch: 32, acc: 60.256410256410255, f1: 25.47431229773463, r: 0.3764006234949956
06/02/2019 09:14:26 *** epoch: 34 ***
06/02/2019 09:14:26 *** training ***
06/02/2019 09:14:27 step: 1094, epoch: 33, batch: 4, loss: 0.8114095330238342, acc: 70.3125, f1: 50.55817198674342, r: 0.48589797433974546
06/02/2019 09:14:27 step: 1099, epoch: 33, batch: 9, loss: 0.6992722749710083, acc: 82.8125, f1: 66.00198412698413, r: 0.5195283946431083
06/02/2019 09:14:27 step: 1104, epoch: 33, batch: 14, loss: 0.6539318561553955, acc: 71.875, f1: 39.66562896344979, r: 0.4982425783705469
06/02/2019 09:14:27 step: 1109, epoch: 33, batch: 19, loss: 0.7688353657722473, acc: 71.875, f1: 56.241308429246715, r: 0.48491902007653587
06/02/2019 09:14:27 step: 1114, epoch: 33, batch: 24, loss: 0.9288167357444763, acc: 65.625, f1: 45.101708074534166, r: 0.48538709369938826
06/02/2019 09:14:28 step: 1119, epoch: 33, batch: 29, loss: 0.720046877861023, acc: 76.5625, f1: 46.71961550993809, r: 0.5167002091728573
06/02/2019 09:14:28 *** evaluating ***
06/02/2019 09:14:28 step: 34, epoch: 33, acc: 60.256410256410255, f1: 23.72795197033563, r: 0.37248510843784627
06/02/2019 09:14:28 *** epoch: 35 ***
06/02/2019 09:14:28 *** training ***
06/02/2019 09:14:28 step: 1127, epoch: 34, batch: 4, loss: 0.60085129737854, acc: 78.125, f1: 53.330627705627705, r: 0.5679385976085025
06/02/2019 09:14:28 step: 1132, epoch: 34, batch: 9, loss: 0.6804457306861877, acc: 78.125, f1: 62.73985456251465, r: 0.5432965060149475
06/02/2019 09:14:28 step: 1137, epoch: 34, batch: 14, loss: 1.0135537385940552, acc: 64.0625, f1: 33.11268472906404, r: 0.43020407526966753
06/02/2019 09:14:29 step: 1142, epoch: 34, batch: 19, loss: 0.9344861507415771, acc: 73.4375, f1: 67.23248512285058, r: 0.5078659087865351
06/02/2019 09:14:29 step: 1147, epoch: 34, batch: 24, loss: 0.7755049467086792, acc: 70.3125, f1: 53.14162028447742, r: 0.5065258623273522
06/02/2019 09:14:29 step: 1152, epoch: 34, batch: 29, loss: 0.6715111136436462, acc: 78.125, f1: 57.4277449408936, r: 0.534566538343775
06/02/2019 09:14:29 *** evaluating ***
06/02/2019 09:14:29 step: 35, epoch: 34, acc: 58.97435897435898, f1: 21.93162101056838, r: 0.37768120817912687
06/02/2019 09:14:29 *** epoch: 36 ***
06/02/2019 09:14:29 *** training ***
06/02/2019 09:14:29 step: 1160, epoch: 35, batch: 4, loss: 0.637125551700592, acc: 76.5625, f1: 60.82290974278406, r: 0.5454028182705541
06/02/2019 09:14:30 step: 1165, epoch: 35, batch: 9, loss: 0.4926791787147522, acc: 87.5, f1: 74.18598084395049, r: 0.6660857512586758
06/02/2019 09:14:30 step: 1170, epoch: 35, batch: 14, loss: 0.5659388303756714, acc: 82.8125, f1: 68.00788305396601, r: 0.511097798301329
06/02/2019 09:14:30 step: 1175, epoch: 35, batch: 19, loss: 0.8933079242706299, acc: 67.1875, f1: 46.353607632677395, r: 0.4996709959782182
06/02/2019 09:14:30 step: 1180, epoch: 35, batch: 24, loss: 0.6565229296684265, acc: 76.5625, f1: 62.850537942449705, r: 0.550127024017203
06/02/2019 09:14:30 step: 1185, epoch: 35, batch: 29, loss: 0.8498499393463135, acc: 73.4375, f1: 49.577348180651576, r: 0.4257862198281257
06/02/2019 09:14:31 *** evaluating ***
06/02/2019 09:14:31 step: 36, epoch: 35, acc: 58.54700854700855, f1: 22.289973681780406, r: 0.3792390917465049
06/02/2019 09:14:31 *** epoch: 37 ***
06/02/2019 09:14:31 *** training ***
06/02/2019 09:14:31 step: 1193, epoch: 36, batch: 4, loss: 0.9248533248901367, acc: 71.875, f1: 63.197278911564624, r: 0.48401067897235206
06/02/2019 09:14:31 step: 1198, epoch: 36, batch: 9, loss: 0.7836693525314331, acc: 73.4375, f1: 58.1756072444927, r: 0.5402008841352756
06/02/2019 09:14:31 step: 1203, epoch: 36, batch: 14, loss: 0.7509384155273438, acc: 75.0, f1: 56.9252432155658, r: 0.5236030326469323
06/02/2019 09:14:31 step: 1208, epoch: 36, batch: 19, loss: 0.7049427628517151, acc: 78.125, f1: 59.040657198551926, r: 0.43574208574786555
06/02/2019 09:14:32 step: 1213, epoch: 36, batch: 24, loss: 0.7763892412185669, acc: 79.6875, f1: 55.58189655172413, r: 0.5207522342088889
06/02/2019 09:14:32 step: 1218, epoch: 36, batch: 29, loss: 0.5930733680725098, acc: 73.4375, f1: 44.388888888888886, r: 0.555738353728457
06/02/2019 09:14:32 *** evaluating ***
06/02/2019 09:14:32 step: 37, epoch: 36, acc: 59.82905982905983, f1: 22.57842197978799, r: 0.3791531465670315
06/02/2019 09:14:32 *** epoch: 38 ***
06/02/2019 09:14:32 *** training ***
06/02/2019 09:14:32 step: 1226, epoch: 37, batch: 4, loss: 0.6159242987632751, acc: 78.125, f1: 61.393574748837906, r: 0.6630957830184636
06/02/2019 09:14:33 step: 1231, epoch: 37, batch: 9, loss: 0.5188835859298706, acc: 81.25, f1: 53.01244588744588, r: 0.5932210696242477
06/02/2019 09:14:33 step: 1236, epoch: 37, batch: 14, loss: 0.8761758804321289, acc: 67.1875, f1: 44.617551566080984, r: 0.48838500825716286
06/02/2019 09:14:33 step: 1241, epoch: 37, batch: 19, loss: 0.5720884203910828, acc: 78.125, f1: 72.14785214785215, r: 0.5970424451425561
06/02/2019 09:14:33 step: 1246, epoch: 37, batch: 24, loss: 0.7167630791664124, acc: 71.875, f1: 50.50150081400082, r: 0.5233155384270586
06/02/2019 09:14:34 step: 1251, epoch: 37, batch: 29, loss: 0.5521359443664551, acc: 82.8125, f1: 73.23299525088628, r: 0.5839437396757556
06/02/2019 09:14:34 *** evaluating ***
06/02/2019 09:14:34 step: 38, epoch: 37, acc: 60.256410256410255, f1: 24.147011418583006, r: 0.382822094609349
06/02/2019 09:14:34 *** epoch: 39 ***
06/02/2019 09:14:34 *** training ***
06/02/2019 09:14:34 step: 1259, epoch: 38, batch: 4, loss: 0.6868472695350647, acc: 73.4375, f1: 43.185766139486645, r: 0.5792989588579512
06/02/2019 09:14:34 step: 1264, epoch: 38, batch: 9, loss: 0.6614628434181213, acc: 73.4375, f1: 60.159443454564524, r: 0.5314620486069831
06/02/2019 09:14:34 step: 1269, epoch: 38, batch: 14, loss: 0.7907806634902954, acc: 65.625, f1: 41.39406487232574, r: 0.5014698434182556
06/02/2019 09:14:35 step: 1274, epoch: 38, batch: 19, loss: 0.4830635190010071, acc: 81.25, f1: 65.24009884458692, r: 0.610045984012047
06/02/2019 09:14:35 step: 1279, epoch: 38, batch: 24, loss: 0.8678325414657593, acc: 68.75, f1: 54.26745189676224, r: 0.6343820319447881
06/02/2019 09:14:35 step: 1284, epoch: 38, batch: 29, loss: 0.5409132838249207, acc: 78.125, f1: 59.1742758047106, r: 0.6011019790512646
06/02/2019 09:14:35 *** evaluating ***
06/02/2019 09:14:35 step: 39, epoch: 38, acc: 60.68376068376068, f1: 25.177615862398472, r: 0.3891255761630511
06/02/2019 09:14:35 *** epoch: 40 ***
06/02/2019 09:14:35 *** training ***
06/02/2019 09:14:36 step: 1292, epoch: 39, batch: 4, loss: 0.6750781536102295, acc: 84.375, f1: 73.14443535188217, r: 0.6146358299628033
06/02/2019 09:14:36 step: 1297, epoch: 39, batch: 9, loss: 0.579056441783905, acc: 78.125, f1: 63.65967365967366, r: 0.6008808469900889
06/02/2019 09:14:36 step: 1302, epoch: 39, batch: 14, loss: 0.5085668563842773, acc: 85.9375, f1: 66.88562715726685, r: 0.5796639100154444
06/02/2019 09:14:36 step: 1307, epoch: 39, batch: 19, loss: 0.48665475845336914, acc: 87.5, f1: 80.85571070533477, r: 0.6500209525738497
06/02/2019 09:14:37 step: 1312, epoch: 39, batch: 24, loss: 0.6663626432418823, acc: 70.3125, f1: 60.87400177462289, r: 0.5312948899113816
06/02/2019 09:14:37 step: 1317, epoch: 39, batch: 29, loss: 0.7345461249351501, acc: 79.6875, f1: 67.44113029827315, r: 0.5002147049284895
06/02/2019 09:14:37 *** evaluating ***
06/02/2019 09:14:37 step: 40, epoch: 39, acc: 58.97435897435898, f1: 21.94656488549618, r: 0.38258309954566333
06/02/2019 09:14:37 *** epoch: 41 ***
06/02/2019 09:14:37 *** training ***
06/02/2019 09:14:37 step: 1325, epoch: 40, batch: 4, loss: 0.5008257627487183, acc: 84.375, f1: 75.6226781492739, r: 0.5265025865148413
06/02/2019 09:14:37 step: 1330, epoch: 40, batch: 9, loss: 0.5816054344177246, acc: 82.8125, f1: 65.53630988934243, r: 0.687456681847267
06/02/2019 09:14:38 step: 1335, epoch: 40, batch: 14, loss: 0.5229020714759827, acc: 78.125, f1: 59.62555962555962, r: 0.4669960617073471
06/02/2019 09:14:38 step: 1340, epoch: 40, batch: 19, loss: 0.7517043352127075, acc: 75.0, f1: 61.03683897801544, r: 0.5361543188279151
06/02/2019 09:14:38 step: 1345, epoch: 40, batch: 24, loss: 0.4697292745113373, acc: 87.5, f1: 75.25173085971635, r: 0.6044955941415373
06/02/2019 09:14:38 step: 1350, epoch: 40, batch: 29, loss: 0.569882333278656, acc: 84.375, f1: 71.76595334490072, r: 0.48371570162759536
06/02/2019 09:14:38 *** evaluating ***
06/02/2019 09:14:39 step: 41, epoch: 40, acc: 60.68376068376068, f1: 24.833226331940235, r: 0.3930028453506347
06/02/2019 09:14:39 *** epoch: 42 ***
06/02/2019 09:14:39 *** training ***
06/02/2019 09:14:39 step: 1358, epoch: 41, batch: 4, loss: 0.594841480255127, acc: 78.125, f1: 52.38095238095237, r: 0.5163403566353074
06/02/2019 09:14:39 step: 1363, epoch: 41, batch: 9, loss: 0.40684160590171814, acc: 90.625, f1: 85.61904761904762, r: 0.6415755363811443
06/02/2019 09:14:39 step: 1368, epoch: 41, batch: 14, loss: 0.5281868577003479, acc: 73.4375, f1: 63.70615778779044, r: 0.5621461925976894
06/02/2019 09:14:39 step: 1373, epoch: 41, batch: 19, loss: 0.6564331650733948, acc: 75.0, f1: 47.81858766233766, r: 0.47561927937280213
06/02/2019 09:14:40 step: 1378, epoch: 41, batch: 24, loss: 0.6888826489448547, acc: 78.125, f1: 57.02380952380952, r: 0.4171784796207798
06/02/2019 09:14:40 step: 1383, epoch: 41, batch: 29, loss: 0.7079370617866516, acc: 79.6875, f1: 68.96250642013354, r: 0.48898095070504416
06/02/2019 09:14:40 *** evaluating ***
06/02/2019 09:14:40 step: 42, epoch: 41, acc: 59.82905982905983, f1: 23.171197650950866, r: 0.39130111218038865
06/02/2019 09:14:40 *** epoch: 43 ***
06/02/2019 09:14:40 *** training ***
06/02/2019 09:14:40 step: 1391, epoch: 42, batch: 4, loss: 0.5841825008392334, acc: 81.25, f1: 80.23554085665887, r: 0.6058431256731149
06/02/2019 09:14:41 step: 1396, epoch: 42, batch: 9, loss: 0.4700014591217041, acc: 82.8125, f1: 50.373402621172005, r: 0.6081197540461207
06/02/2019 09:14:41 step: 1401, epoch: 42, batch: 14, loss: 0.6133849024772644, acc: 76.5625, f1: 63.59438650322109, r: 0.5633639232697675
06/02/2019 09:14:41 step: 1406, epoch: 42, batch: 19, loss: 0.5565670728683472, acc: 84.375, f1: 57.78500870713985, r: 0.6051132589845305
06/02/2019 09:14:41 step: 1411, epoch: 42, batch: 24, loss: 0.4240154027938843, acc: 87.5, f1: 73.66952578132702, r: 0.5401726215069977
06/02/2019 09:14:42 step: 1416, epoch: 42, batch: 29, loss: 0.6464807391166687, acc: 75.0, f1: 69.24422516259251, r: 0.569538255093297
06/02/2019 09:14:42 *** evaluating ***
06/02/2019 09:14:42 step: 43, epoch: 42, acc: 58.97435897435898, f1: 23.247049167119865, r: 0.3890379300216594
06/02/2019 09:14:42 *** epoch: 44 ***
06/02/2019 09:14:42 *** training ***
06/02/2019 09:14:42 step: 1424, epoch: 43, batch: 4, loss: 0.4038064181804657, acc: 89.0625, f1: 74.65260017050299, r: 0.6711146933186679
06/02/2019 09:14:42 step: 1429, epoch: 43, batch: 9, loss: 0.47976312041282654, acc: 81.25, f1: 59.24826496255068, r: 0.5087213313726333
06/02/2019 09:14:42 step: 1434, epoch: 43, batch: 14, loss: 0.6155576705932617, acc: 76.5625, f1: 59.78231292517007, r: 0.5640688159808271
06/02/2019 09:14:43 step: 1439, epoch: 43, batch: 19, loss: 0.5228655934333801, acc: 78.125, f1: 46.98796970588323, r: 0.4650816268601671
06/02/2019 09:14:43 step: 1444, epoch: 43, batch: 24, loss: 0.49648338556289673, acc: 84.375, f1: 74.93940104777543, r: 0.5605040028516596
06/02/2019 09:14:43 step: 1449, epoch: 43, batch: 29, loss: 0.5674206018447876, acc: 84.375, f1: 65.9166797981257, r: 0.6070790095800013
06/02/2019 09:14:43 *** evaluating ***
06/02/2019 09:14:43 step: 44, epoch: 43, acc: 60.68376068376068, f1: 26.229745766714686, r: 0.39625909761206396
06/02/2019 09:14:43 *** epoch: 45 ***
06/02/2019 09:14:43 *** training ***
06/02/2019 09:14:44 step: 1457, epoch: 44, batch: 4, loss: 0.6674452424049377, acc: 75.0, f1: 56.07632933104632, r: 0.63863500163057
06/02/2019 09:14:44 step: 1462, epoch: 44, batch: 9, loss: 0.6298094987869263, acc: 85.9375, f1: 71.99537037037037, r: 0.5916124585908837
06/02/2019 09:14:44 step: 1467, epoch: 44, batch: 14, loss: 0.6485188007354736, acc: 78.125, f1: 64.41668766896731, r: 0.46824839550094316
06/02/2019 09:14:44 step: 1472, epoch: 44, batch: 19, loss: 0.3758834898471832, acc: 89.0625, f1: 68.8127805389179, r: 0.6507806496993693
06/02/2019 09:14:44 step: 1477, epoch: 44, batch: 24, loss: 0.5022470951080322, acc: 82.8125, f1: 77.41946778711484, r: 0.6230538537277063
06/02/2019 09:14:45 step: 1482, epoch: 44, batch: 29, loss: 0.4735947847366333, acc: 84.375, f1: 79.9136729618287, r: 0.638146517015692
06/02/2019 09:14:45 *** evaluating ***
06/02/2019 09:14:45 step: 45, epoch: 44, acc: 59.82905982905983, f1: 25.79851991297584, r: 0.39470147695606217
06/02/2019 09:14:45 *** epoch: 46 ***
06/02/2019 09:14:45 *** training ***
06/02/2019 09:14:45 step: 1490, epoch: 45, batch: 4, loss: 0.6036684513092041, acc: 76.5625, f1: 67.29635443921158, r: 0.5783513242958668
06/02/2019 09:14:45 step: 1495, epoch: 45, batch: 9, loss: 0.5654110312461853, acc: 79.6875, f1: 67.9784468213991, r: 0.5392356207639232
06/02/2019 09:14:46 step: 1500, epoch: 45, batch: 14, loss: 0.4265204668045044, acc: 85.9375, f1: 82.48132235350282, r: 0.5650055498424388
06/02/2019 09:14:46 step: 1505, epoch: 45, batch: 19, loss: 0.3867674767971039, acc: 90.625, f1: 71.24295518384189, r: 0.5601204766950201
06/02/2019 09:14:46 step: 1510, epoch: 45, batch: 24, loss: 0.42532044649124146, acc: 81.25, f1: 68.79477004477003, r: 0.6880907927979729
06/02/2019 09:14:46 step: 1515, epoch: 45, batch: 29, loss: 0.6107727885246277, acc: 81.25, f1: 66.16414959813326, r: 0.6192845729880706
06/02/2019 09:14:46 *** evaluating ***
06/02/2019 09:14:47 step: 46, epoch: 45, acc: 60.256410256410255, f1: 26.34336677814939, r: 0.39387212126444227
06/02/2019 09:14:47 *** epoch: 47 ***
06/02/2019 09:14:47 *** training ***
06/02/2019 09:14:47 step: 1523, epoch: 46, batch: 4, loss: 0.3800533413887024, acc: 85.9375, f1: 61.127142179773756, r: 0.5441344475629555
06/02/2019 09:14:47 step: 1528, epoch: 46, batch: 9, loss: 0.4910847842693329, acc: 82.8125, f1: 61.67929292929293, r: 0.6013251108206406
06/02/2019 09:14:47 step: 1533, epoch: 46, batch: 14, loss: 0.5501988530158997, acc: 84.375, f1: 72.59841057635174, r: 0.7090729077239821
06/02/2019 09:14:47 step: 1538, epoch: 46, batch: 19, loss: 0.6437339186668396, acc: 82.8125, f1: 79.1141456582633, r: 0.5248973266872657
06/02/2019 09:14:48 step: 1543, epoch: 46, batch: 24, loss: 0.8469654321670532, acc: 73.4375, f1: 47.520438407921304, r: 0.4885545241637284
06/02/2019 09:14:48 step: 1548, epoch: 46, batch: 29, loss: 0.44189828634262085, acc: 78.125, f1: 67.29516250944823, r: 0.5949766733526296
06/02/2019 09:14:48 *** evaluating ***
06/02/2019 09:14:48 step: 47, epoch: 46, acc: 58.97435897435898, f1: 24.676135810845306, r: 0.3909495489306811
06/02/2019 09:14:48 *** epoch: 48 ***
06/02/2019 09:14:48 *** training ***
06/02/2019 09:14:48 step: 1556, epoch: 47, batch: 4, loss: 0.34690091013908386, acc: 90.625, f1: 72.51957601222307, r: 0.5889688116081051
06/02/2019 09:14:49 step: 1561, epoch: 47, batch: 9, loss: 0.33250531554222107, acc: 87.5, f1: 73.68433761889509, r: 0.6555442614849761
06/02/2019 09:14:49 step: 1566, epoch: 47, batch: 14, loss: 0.6640504002571106, acc: 73.4375, f1: 44.76575374141297, r: 0.550724647841964
06/02/2019 09:14:49 step: 1571, epoch: 47, batch: 19, loss: 0.3139079809188843, acc: 87.5, f1: 75.28684410827267, r: 0.7365594630244247
06/02/2019 09:14:49 step: 1576, epoch: 47, batch: 24, loss: 0.39242902398109436, acc: 89.0625, f1: 74.65864527629233, r: 0.6126083468031376
06/02/2019 09:14:49 step: 1581, epoch: 47, batch: 29, loss: 0.5316696166992188, acc: 79.6875, f1: 67.80693444486548, r: 0.636943259524345
06/02/2019 09:14:50 *** evaluating ***
06/02/2019 09:14:50 step: 48, epoch: 47, acc: 60.68376068376068, f1: 23.619111309951002, r: 0.3938513140593606
06/02/2019 09:14:50 *** epoch: 49 ***
06/02/2019 09:14:50 *** training ***
06/02/2019 09:14:50 step: 1589, epoch: 48, batch: 4, loss: 0.4825863242149353, acc: 79.6875, f1: 69.97084548104957, r: 0.5705935973566818
06/02/2019 09:14:50 step: 1594, epoch: 48, batch: 9, loss: 0.6353499293327332, acc: 78.125, f1: 60.45289855072463, r: 0.5642925729710586
06/02/2019 09:14:50 step: 1599, epoch: 48, batch: 14, loss: 0.7001703977584839, acc: 73.4375, f1: 51.07175925925927, r: 0.5833180368825226
06/02/2019 09:14:51 step: 1604, epoch: 48, batch: 19, loss: 0.7029418349266052, acc: 75.0, f1: 60.5602810229783, r: 0.4750260585682113
06/02/2019 09:14:51 step: 1609, epoch: 48, batch: 24, loss: 0.398065984249115, acc: 87.5, f1: 86.7792207792208, r: 0.6376995081885763
06/02/2019 09:14:51 step: 1614, epoch: 48, batch: 29, loss: 0.5449809432029724, acc: 79.6875, f1: 78.01328502415458, r: 0.7151359869808517
06/02/2019 09:14:51 *** evaluating ***
06/02/2019 09:14:51 step: 49, epoch: 48, acc: 58.97435897435898, f1: 23.98015127103473, r: 0.39346928498498657
06/02/2019 09:14:51 *** epoch: 50 ***
06/02/2019 09:14:51 *** training ***
06/02/2019 09:14:52 step: 1622, epoch: 49, batch: 4, loss: 0.5726378560066223, acc: 81.25, f1: 78.40757808499744, r: 0.6446920996831381
06/02/2019 09:14:52 step: 1627, epoch: 49, batch: 9, loss: 0.5351116061210632, acc: 81.25, f1: 77.28894548937653, r: 0.6974368252511622
06/02/2019 09:14:52 step: 1632, epoch: 49, batch: 14, loss: 0.3979234993457794, acc: 90.625, f1: 89.00384077075806, r: 0.6047002556994036
06/02/2019 09:14:52 step: 1637, epoch: 49, batch: 19, loss: 0.2514784634113312, acc: 90.625, f1: 77.24476911976912, r: 0.7072379504035416
06/02/2019 09:14:52 step: 1642, epoch: 49, batch: 24, loss: 0.5168955326080322, acc: 81.25, f1: 68.05261643166622, r: 0.6937468324839949
06/02/2019 09:14:53 step: 1647, epoch: 49, batch: 29, loss: 0.7285714745521545, acc: 70.3125, f1: 59.18319775462633, r: 0.5420034183584238
06/02/2019 09:14:53 *** evaluating ***
06/02/2019 09:14:53 step: 50, epoch: 49, acc: 60.68376068376068, f1: 26.483915981198592, r: 0.40120496393152766
06/02/2019 09:14:53 *** epoch: 51 ***
06/02/2019 09:14:53 *** training ***
06/02/2019 09:14:53 step: 1655, epoch: 50, batch: 4, loss: 0.31049713492393494, acc: 89.0625, f1: 85.15994012888423, r: 0.5587823938565732
06/02/2019 09:14:53 step: 1660, epoch: 50, batch: 9, loss: 0.36167827248573303, acc: 84.375, f1: 57.44489807456225, r: 0.5190663809686022
06/02/2019 09:14:54 step: 1665, epoch: 50, batch: 14, loss: 0.398278146982193, acc: 84.375, f1: 67.27937830363706, r: 0.5791152470584573
06/02/2019 09:14:54 step: 1670, epoch: 50, batch: 19, loss: 0.32604995369911194, acc: 90.625, f1: 82.79365079365078, r: 0.5442830068679115
06/02/2019 09:14:54 step: 1675, epoch: 50, batch: 24, loss: 0.5179166793823242, acc: 85.9375, f1: 75.36590393733252, r: 0.7671201963190607
06/02/2019 09:14:54 step: 1680, epoch: 50, batch: 29, loss: 0.6664409637451172, acc: 75.0, f1: 67.44773866484392, r: 0.5800205170931596
06/02/2019 09:14:54 *** evaluating ***
06/02/2019 09:14:55 step: 51, epoch: 50, acc: 59.401709401709404, f1: 25.10509534578202, r: 0.39317343455800974
06/02/2019 09:14:55 *** epoch: 52 ***
06/02/2019 09:14:55 *** training ***
06/02/2019 09:14:55 step: 1688, epoch: 51, batch: 4, loss: 0.5888162851333618, acc: 76.5625, f1: 66.98001623467462, r: 0.5836348385652
06/02/2019 09:14:55 step: 1693, epoch: 51, batch: 9, loss: 0.4486270248889923, acc: 85.9375, f1: 63.068611282897, r: 0.7091445130424499
06/02/2019 09:14:55 step: 1698, epoch: 51, batch: 14, loss: 0.47648918628692627, acc: 82.8125, f1: 61.609531772575245, r: 0.6208371587699475
06/02/2019 09:14:55 step: 1703, epoch: 51, batch: 19, loss: 0.29999107122421265, acc: 89.0625, f1: 85.56398684058259, r: 0.6282841585144809
06/02/2019 09:14:56 step: 1708, epoch: 51, batch: 24, loss: 0.5098869800567627, acc: 87.5, f1: 66.63725766986637, r: 0.5665129508483864
06/02/2019 09:14:56 step: 1713, epoch: 51, batch: 29, loss: 0.4700530767440796, acc: 87.5, f1: 91.5802094179071, r: 0.5846459734190009
06/02/2019 09:14:56 *** evaluating ***
06/02/2019 09:14:56 step: 52, epoch: 51, acc: 60.256410256410255, f1: 24.12123007294188, r: 0.40171182258712734
06/02/2019 09:14:56 *** epoch: 53 ***
06/02/2019 09:14:56 *** training ***
06/02/2019 09:14:56 step: 1721, epoch: 52, batch: 4, loss: 0.38401007652282715, acc: 85.9375, f1: 77.79120879120879, r: 0.7175651304323145
06/02/2019 09:14:57 step: 1726, epoch: 52, batch: 9, loss: 0.24216997623443604, acc: 92.1875, f1: 75.34391534391534, r: 0.5648076951928626
06/02/2019 09:14:57 step: 1731, epoch: 52, batch: 14, loss: 0.40005379915237427, acc: 90.625, f1: 73.27322039927083, r: 0.5183449300605476
06/02/2019 09:14:57 step: 1736, epoch: 52, batch: 19, loss: 0.31706440448760986, acc: 85.9375, f1: 81.35140222096744, r: 0.6785228733444644
06/02/2019 09:14:57 step: 1741, epoch: 52, batch: 24, loss: 0.4826987385749817, acc: 87.5, f1: 84.563690277976, r: 0.6436011909782726
06/02/2019 09:14:58 step: 1746, epoch: 52, batch: 29, loss: 0.6327022314071655, acc: 79.6875, f1: 59.929944607363964, r: 0.6504187939420947
06/02/2019 09:14:58 *** evaluating ***
06/02/2019 09:14:58 step: 53, epoch: 52, acc: 59.401709401709404, f1: 27.22660672660673, r: 0.3971935126007432
06/02/2019 09:14:58 *** epoch: 54 ***
06/02/2019 09:14:58 *** training ***
06/02/2019 09:14:58 step: 1754, epoch: 53, batch: 4, loss: 0.34917834401130676, acc: 90.625, f1: 77.62896825396825, r: 0.6582275938573687
06/02/2019 09:14:58 step: 1759, epoch: 53, batch: 9, loss: 0.4271675944328308, acc: 85.9375, f1: 79.8357764158295, r: 0.5985262118514412
06/02/2019 09:14:58 step: 1764, epoch: 53, batch: 14, loss: 0.2875997722148895, acc: 92.1875, f1: 88.57782487493795, r: 0.6874922444321443
06/02/2019 09:14:59 step: 1769, epoch: 53, batch: 19, loss: 0.3794706165790558, acc: 84.375, f1: 77.26035868893011, r: 0.6758615324397712
06/02/2019 09:14:59 step: 1774, epoch: 53, batch: 24, loss: 0.45290783047676086, acc: 75.0, f1: 60.51443885443197, r: 0.5426626988174332
06/02/2019 09:14:59 step: 1779, epoch: 53, batch: 29, loss: 0.48752400279045105, acc: 84.375, f1: 63.486842105263165, r: 0.6390794964644129
06/02/2019 09:14:59 *** evaluating ***
06/02/2019 09:14:59 step: 54, epoch: 53, acc: 60.256410256410255, f1: 24.84022382537921, r: 0.40242927679956003
06/02/2019 09:14:59 *** epoch: 55 ***
06/02/2019 09:14:59 *** training ***
06/02/2019 09:15:00 step: 1787, epoch: 54, batch: 4, loss: 0.5068838596343994, acc: 81.25, f1: 75.69057926200784, r: 0.5576420735638474
06/02/2019 09:15:00 step: 1792, epoch: 54, batch: 9, loss: 0.2657057046890259, acc: 93.75, f1: 80.33731025323338, r: 0.7023374254880768
06/02/2019 09:15:00 step: 1797, epoch: 54, batch: 14, loss: 0.3547380864620209, acc: 90.625, f1: 82.00770321318653, r: 0.7206300847217738
06/02/2019 09:15:00 step: 1802, epoch: 54, batch: 19, loss: 0.35591018199920654, acc: 85.9375, f1: 81.4467418546366, r: 0.6076270640608747
06/02/2019 09:15:00 step: 1807, epoch: 54, batch: 24, loss: 0.485454797744751, acc: 82.8125, f1: 77.47319347319348, r: 0.6387278378169965
06/02/2019 09:15:01 step: 1812, epoch: 54, batch: 29, loss: 0.49037089943885803, acc: 82.8125, f1: 75.47146807440926, r: 0.7101041314884292
06/02/2019 09:15:01 *** evaluating ***
06/02/2019 09:15:01 step: 55, epoch: 54, acc: 58.97435897435898, f1: 22.982142857142858, r: 0.39956182615348634
06/02/2019 09:15:01 *** epoch: 56 ***
06/02/2019 09:15:01 *** training ***
06/02/2019 09:15:01 step: 1820, epoch: 55, batch: 4, loss: 0.3426172137260437, acc: 89.0625, f1: 76.80236430236431, r: 0.682502741294508
06/02/2019 09:15:01 step: 1825, epoch: 55, batch: 9, loss: 0.4302569627761841, acc: 84.375, f1: 74.11525974025975, r: 0.6891566316608556
06/02/2019 09:15:01 step: 1830, epoch: 55, batch: 14, loss: 0.46049219369888306, acc: 84.375, f1: 67.56764928193498, r: 0.5531542915253317
06/02/2019 09:15:02 step: 1835, epoch: 55, batch: 19, loss: 0.41540125012397766, acc: 84.375, f1: 83.09317666460524, r: 0.7152702411192394
06/02/2019 09:15:02 step: 1840, epoch: 55, batch: 24, loss: 0.43832820653915405, acc: 84.375, f1: 71.7072940287226, r: 0.6180341419131947
06/02/2019 09:15:02 step: 1845, epoch: 55, batch: 29, loss: 0.24155987799167633, acc: 90.625, f1: 78.082045949693, r: 0.7392718512846511
06/02/2019 09:15:02 *** evaluating ***
06/02/2019 09:15:02 step: 56, epoch: 55, acc: 60.256410256410255, f1: 25.425125403787064, r: 0.4056089745797741
06/02/2019 09:15:02 *** epoch: 57 ***
06/02/2019 09:15:02 *** training ***
06/02/2019 09:15:02 step: 1853, epoch: 56, batch: 4, loss: 0.34198620915412903, acc: 89.0625, f1: 73.32792207792207, r: 0.7709865064952917
06/02/2019 09:15:03 step: 1858, epoch: 56, batch: 9, loss: 0.5520415902137756, acc: 84.375, f1: 82.31722354013377, r: 0.6564871582768127
06/02/2019 09:15:03 step: 1863, epoch: 56, batch: 14, loss: 0.25640103220939636, acc: 87.5, f1: 80.57412762675919, r: 0.6014844260700049
06/02/2019 09:15:03 step: 1868, epoch: 56, batch: 19, loss: 0.3143400251865387, acc: 85.9375, f1: 67.0794994479205, r: 0.5924427708490295
06/02/2019 09:15:03 step: 1873, epoch: 56, batch: 24, loss: 0.26587599515914917, acc: 92.1875, f1: 91.49960587521882, r: 0.6989233240860325
06/02/2019 09:15:04 step: 1878, epoch: 56, batch: 29, loss: 0.4516829252243042, acc: 84.375, f1: 78.82760917243677, r: 0.6845270463740333
06/02/2019 09:15:04 *** evaluating ***
06/02/2019 09:15:04 step: 57, epoch: 56, acc: 58.97435897435898, f1: 23.125478927203062, r: 0.40603005224609295
06/02/2019 09:15:04 *** epoch: 58 ***
06/02/2019 09:15:04 *** training ***
06/02/2019 09:15:04 step: 1886, epoch: 57, batch: 4, loss: 0.2627074122428894, acc: 90.625, f1: 88.21360603969299, r: 0.6777895019800494
06/02/2019 09:15:04 step: 1891, epoch: 57, batch: 9, loss: 0.3766622841358185, acc: 84.375, f1: 80.51844420245042, r: 0.5656801112016678
06/02/2019 09:15:04 step: 1896, epoch: 57, batch: 14, loss: 0.3876737654209137, acc: 84.375, f1: 61.318057787378265, r: 0.6711865504311391
06/02/2019 09:15:05 step: 1901, epoch: 57, batch: 19, loss: 0.5374460816383362, acc: 79.6875, f1: 67.33516483516483, r: 0.5740852193634544
06/02/2019 09:15:05 step: 1906, epoch: 57, batch: 24, loss: 0.4552411735057831, acc: 87.5, f1: 84.93733961714257, r: 0.6543202677895477
06/02/2019 09:15:05 step: 1911, epoch: 57, batch: 29, loss: 0.38787218928337097, acc: 82.8125, f1: 67.14234807004843, r: 0.5377555302395868
06/02/2019 09:15:05 *** evaluating ***
06/02/2019 09:15:05 step: 58, epoch: 57, acc: 59.82905982905983, f1: 24.42620351600934, r: 0.403342664334016
06/02/2019 09:15:05 *** epoch: 59 ***
06/02/2019 09:15:05 *** training ***
06/02/2019 09:15:05 step: 1919, epoch: 58, batch: 4, loss: 0.3037451207637787, acc: 90.625, f1: 79.66022073164932, r: 0.5998265284268038
06/02/2019 09:15:06 step: 1924, epoch: 58, batch: 9, loss: 0.22562728822231293, acc: 90.625, f1: 84.33862433862433, r: 0.7104274614174516
06/02/2019 09:15:06 step: 1929, epoch: 58, batch: 14, loss: 0.31151819229125977, acc: 84.375, f1: 68.2443746729461, r: 0.5855197610297527
06/02/2019 09:15:06 step: 1934, epoch: 58, batch: 19, loss: 0.27011603116989136, acc: 92.1875, f1: 81.28656169111304, r: 0.7444560654140827
06/02/2019 09:15:06 step: 1939, epoch: 58, batch: 24, loss: 0.48448923230171204, acc: 84.375, f1: 79.95967574092575, r: 0.6882643620273596
06/02/2019 09:15:06 step: 1944, epoch: 58, batch: 29, loss: 0.36253097653388977, acc: 87.5, f1: 68.43435390605201, r: 0.5375171982675491
06/02/2019 09:15:06 *** evaluating ***
06/02/2019 09:15:07 step: 59, epoch: 58, acc: 60.256410256410255, f1: 23.21474488672612, r: 0.41188100693862056
06/02/2019 09:15:07 *** epoch: 60 ***
06/02/2019 09:15:07 *** training ***
06/02/2019 09:15:07 step: 1952, epoch: 59, batch: 4, loss: 0.38290390372276306, acc: 87.5, f1: 73.23323188924692, r: 0.6912626778564969
06/02/2019 09:15:07 step: 1957, epoch: 59, batch: 9, loss: 0.2685064673423767, acc: 87.5, f1: 59.02319902319901, r: 0.6724097268652802
06/02/2019 09:15:07 step: 1962, epoch: 59, batch: 14, loss: 0.4232543110847473, acc: 82.8125, f1: 67.4997483137018, r: 0.6425427578903098
06/02/2019 09:15:07 step: 1967, epoch: 59, batch: 19, loss: 0.5075480937957764, acc: 85.9375, f1: 69.22608521746453, r: 0.5026482315965459
06/02/2019 09:15:08 step: 1972, epoch: 59, batch: 24, loss: 0.19005757570266724, acc: 92.1875, f1: 85.17434988179669, r: 0.7428580181022236
06/02/2019 09:15:08 step: 1977, epoch: 59, batch: 29, loss: 0.4801444411277771, acc: 84.375, f1: 67.29351788562315, r: 0.6553500438024912
06/02/2019 09:15:08 *** evaluating ***
06/02/2019 09:15:08 step: 60, epoch: 59, acc: 61.111111111111114, f1: 25.458901605380074, r: 0.4093886126273496
06/02/2019 09:15:08 *** epoch: 61 ***
06/02/2019 09:15:08 *** training ***
06/02/2019 09:15:08 step: 1985, epoch: 60, batch: 4, loss: 0.392081081867218, acc: 79.6875, f1: 65.80847723704866, r: 0.5601105665293465
06/02/2019 09:15:08 step: 1990, epoch: 60, batch: 9, loss: 0.3746413290500641, acc: 85.9375, f1: 75.7961489680618, r: 0.6095706892857486
06/02/2019 09:15:08 step: 1995, epoch: 60, batch: 14, loss: 0.4251580238342285, acc: 82.8125, f1: 53.31174136321195, r: 0.6389050872784031
06/02/2019 09:15:09 step: 2000, epoch: 60, batch: 19, loss: 0.29313868284225464, acc: 87.5, f1: 66.1856661856662, r: 0.5941357487798977
06/02/2019 09:15:09 step: 2005, epoch: 60, batch: 24, loss: 0.5855557918548584, acc: 84.375, f1: 61.39491295741296, r: 0.5891719777158525
06/02/2019 09:15:09 step: 2010, epoch: 60, batch: 29, loss: 0.5230259895324707, acc: 82.8125, f1: 79.868672993673, r: 0.715255970383207
06/02/2019 09:15:09 *** evaluating ***
06/02/2019 09:15:09 step: 61, epoch: 60, acc: 60.68376068376068, f1: 25.75249857224443, r: 0.40863494676551837
06/02/2019 09:15:09 *** epoch: 62 ***
06/02/2019 09:15:09 *** training ***
06/02/2019 09:15:09 step: 2018, epoch: 61, batch: 4, loss: 0.29847192764282227, acc: 90.625, f1: 91.1579760109172, r: 0.7267702618991292
06/02/2019 09:15:10 step: 2023, epoch: 61, batch: 9, loss: 0.2689957320690155, acc: 90.625, f1: 85.63242784380304, r: 0.7184519462870054
06/02/2019 09:15:10 step: 2028, epoch: 61, batch: 14, loss: 0.17072805762290955, acc: 93.75, f1: 91.21978715728716, r: 0.8279405589565746
06/02/2019 09:15:10 step: 2033, epoch: 61, batch: 19, loss: 0.2678622305393219, acc: 92.1875, f1: 83.85580319084156, r: 0.5829502270192909
06/02/2019 09:15:10 step: 2038, epoch: 61, batch: 24, loss: 0.2843852639198303, acc: 90.625, f1: 78.10236561468088, r: 0.5812246747546248
06/02/2019 09:15:11 step: 2043, epoch: 61, batch: 29, loss: 0.31725192070007324, acc: 87.5, f1: 82.13685474189676, r: 0.649369394912286
06/02/2019 09:15:11 *** evaluating ***
06/02/2019 09:15:11 step: 62, epoch: 61, acc: 59.401709401709404, f1: 26.559808318828935, r: 0.4100939333487122
06/02/2019 09:15:11 *** epoch: 63 ***
06/02/2019 09:15:11 *** training ***
06/02/2019 09:15:11 step: 2051, epoch: 62, batch: 4, loss: 0.22477522492408752, acc: 93.75, f1: 94.29563492063492, r: 0.7471584914080625
06/02/2019 09:15:11 step: 2056, epoch: 62, batch: 9, loss: 0.34165671467781067, acc: 89.0625, f1: 78.03581426971684, r: 0.6821263895832933
06/02/2019 09:15:11 step: 2061, epoch: 62, batch: 14, loss: 0.4297248423099518, acc: 89.0625, f1: 85.80359323216466, r: 0.6614928550100995
06/02/2019 09:15:12 step: 2066, epoch: 62, batch: 19, loss: 0.2536638379096985, acc: 92.1875, f1: 88.79802162410859, r: 0.6716858528854588
06/02/2019 09:15:12 step: 2071, epoch: 62, batch: 24, loss: 0.2834947109222412, acc: 90.625, f1: 62.31217761830007, r: 0.5395134755121989
06/02/2019 09:15:12 step: 2076, epoch: 62, batch: 29, loss: 0.29633280634880066, acc: 90.625, f1: 82.87078362018096, r: 0.6817971303481947
06/02/2019 09:15:12 *** evaluating ***
06/02/2019 09:15:12 step: 63, epoch: 62, acc: 61.111111111111114, f1: 26.405916371716643, r: 0.4107331632855319
06/02/2019 09:15:12 *** epoch: 64 ***
06/02/2019 09:15:12 *** training ***
06/02/2019 09:15:13 step: 2084, epoch: 63, batch: 4, loss: 0.25109678506851196, acc: 89.0625, f1: 80.4908054908055, r: 0.6785293755080782
06/02/2019 09:15:13 step: 2089, epoch: 63, batch: 9, loss: 0.3657079339027405, acc: 87.5, f1: 83.31014223871365, r: 0.6174746849523965
06/02/2019 09:15:13 step: 2094, epoch: 63, batch: 14, loss: 0.2718946933746338, acc: 90.625, f1: 81.72789115646259, r: 0.6705861291496354
06/02/2019 09:15:13 step: 2099, epoch: 63, batch: 19, loss: 0.5926268696784973, acc: 75.0, f1: 57.373429416112344, r: 0.6169301739250646
06/02/2019 09:15:14 step: 2104, epoch: 63, batch: 24, loss: 0.3698025941848755, acc: 84.375, f1: 82.03962703962704, r: 0.611601736066439
06/02/2019 09:15:14 step: 2109, epoch: 63, batch: 29, loss: 0.2595919966697693, acc: 93.75, f1: 91.80533751962322, r: 0.64236060692883
06/02/2019 09:15:14 *** evaluating ***
06/02/2019 09:15:14 step: 64, epoch: 63, acc: 59.401709401709404, f1: 23.489125452806608, r: 0.4077798896067579
06/02/2019 09:15:14 *** epoch: 65 ***
06/02/2019 09:15:14 *** training ***
06/02/2019 09:15:14 step: 2117, epoch: 64, batch: 4, loss: 0.4039202630519867, acc: 84.375, f1: 63.35275244849713, r: 0.5854011868137092
06/02/2019 09:15:14 step: 2122, epoch: 64, batch: 9, loss: 0.35136961936950684, acc: 84.375, f1: 67.36446886446886, r: 0.7113970537435097
06/02/2019 09:15:15 step: 2127, epoch: 64, batch: 14, loss: 0.406296044588089, acc: 89.0625, f1: 83.8186494109839, r: 0.6463220553485399
06/02/2019 09:15:15 step: 2132, epoch: 64, batch: 19, loss: 0.4141068756580353, acc: 87.5, f1: 82.92421759527024, r: 0.7647418348568191
06/02/2019 09:15:15 step: 2137, epoch: 64, batch: 24, loss: 0.2680571675300598, acc: 90.625, f1: 66.2940459594764, r: 0.6598510098958889
06/02/2019 09:15:15 step: 2142, epoch: 64, batch: 29, loss: 0.3500502407550812, acc: 89.0625, f1: 77.54449254449254, r: 0.65789923987758
06/02/2019 09:15:15 *** evaluating ***
06/02/2019 09:15:16 step: 65, epoch: 64, acc: 59.82905982905983, f1: 24.598861283643892, r: 0.4086349821347161
06/02/2019 09:15:16 *** epoch: 66 ***
06/02/2019 09:15:16 *** training ***
06/02/2019 09:15:16 step: 2150, epoch: 65, batch: 4, loss: 0.20993413031101227, acc: 92.1875, f1: 75.79492705570291, r: 0.6938993461750051
06/02/2019 09:15:16 step: 2155, epoch: 65, batch: 9, loss: 0.2992510199546814, acc: 90.625, f1: 81.05392973000806, r: 0.6018712521199527
06/02/2019 09:15:16 step: 2160, epoch: 65, batch: 14, loss: 0.40732795000076294, acc: 84.375, f1: 63.296401706111325, r: 0.6108316443701273
06/02/2019 09:15:16 step: 2165, epoch: 65, batch: 19, loss: 0.24082142114639282, acc: 90.625, f1: 88.00524749199194, r: 0.66579983636902
06/02/2019 09:15:17 step: 2170, epoch: 65, batch: 24, loss: 0.2860206961631775, acc: 89.0625, f1: 83.43767264751864, r: 0.7034701314034242
06/02/2019 09:15:17 step: 2175, epoch: 65, batch: 29, loss: 0.3626628518104553, acc: 85.9375, f1: 78.08561808561808, r: 0.6863282555859014
06/02/2019 09:15:17 *** evaluating ***
06/02/2019 09:15:17 step: 66, epoch: 65, acc: 61.53846153846154, f1: 28.415959852982752, r: 0.41161641167880425
06/02/2019 09:15:17 *** epoch: 67 ***
06/02/2019 09:15:17 *** training ***
06/02/2019 09:15:17 step: 2183, epoch: 66, batch: 4, loss: 0.4329686164855957, acc: 85.9375, f1: 71.70312385354758, r: 0.6201516562374749
06/02/2019 09:15:17 step: 2188, epoch: 66, batch: 9, loss: 0.20313692092895508, acc: 93.75, f1: 82.91931572393, r: 0.7131780853880212
06/02/2019 09:15:18 step: 2193, epoch: 66, batch: 14, loss: 0.49363893270492554, acc: 84.375, f1: 80.91995306281021, r: 0.5772084976615989
06/02/2019 09:15:18 step: 2198, epoch: 66, batch: 19, loss: 0.3481980562210083, acc: 84.375, f1: 77.03677136816671, r: 0.6805300336048057
06/02/2019 09:15:18 step: 2203, epoch: 66, batch: 24, loss: 0.40050017833709717, acc: 89.0625, f1: 74.32235142118863, r: 0.6053249321698697
06/02/2019 09:15:18 step: 2208, epoch: 66, batch: 29, loss: 0.2265004962682724, acc: 95.3125, f1: 91.16974827004618, r: 0.7537550264189992
06/02/2019 09:15:18 *** evaluating ***
06/02/2019 09:15:18 step: 67, epoch: 66, acc: 61.111111111111114, f1: 27.955154998258447, r: 0.40983175822525375
06/02/2019 09:15:18 *** epoch: 68 ***
06/02/2019 09:15:18 *** training ***
06/02/2019 09:15:19 step: 2216, epoch: 67, batch: 4, loss: 0.3546777069568634, acc: 87.5, f1: 82.32795881512926, r: 0.6103976423133619
06/02/2019 09:15:19 step: 2221, epoch: 67, batch: 9, loss: 0.25113213062286377, acc: 90.625, f1: 78.21103733847362, r: 0.6318802633504695
06/02/2019 09:15:19 step: 2226, epoch: 67, batch: 14, loss: 0.35746389627456665, acc: 89.0625, f1: 73.91300366300366, r: 0.5927817663126446
06/02/2019 09:15:19 step: 2231, epoch: 67, batch: 19, loss: 0.3241555690765381, acc: 89.0625, f1: 89.35719392862251, r: 0.6387228057884535
06/02/2019 09:15:19 step: 2236, epoch: 67, batch: 24, loss: 0.26007890701293945, acc: 92.1875, f1: 90.08320959137656, r: 0.5555288567048036
06/02/2019 09:15:20 step: 2241, epoch: 67, batch: 29, loss: 0.31079646944999695, acc: 85.9375, f1: 73.52633477633476, r: 0.6735361956940734
06/02/2019 09:15:20 *** evaluating ***
06/02/2019 09:15:20 step: 68, epoch: 67, acc: 59.82905982905983, f1: 26.18645794124307, r: 0.41124912997634805
06/02/2019 09:15:20 *** epoch: 69 ***
06/02/2019 09:15:20 *** training ***
06/02/2019 09:15:20 step: 2249, epoch: 68, batch: 4, loss: 0.3543330430984497, acc: 92.1875, f1: 85.92521700848893, r: 0.6540139654653647
06/02/2019 09:15:20 step: 2254, epoch: 68, batch: 9, loss: 0.23650775849819183, acc: 92.1875, f1: 90.25247525910949, r: 0.664883724208003
06/02/2019 09:15:20 step: 2259, epoch: 68, batch: 14, loss: 0.2546272873878479, acc: 89.0625, f1: 60.73290598290597, r: 0.649608549174978
06/02/2019 09:15:21 step: 2264, epoch: 68, batch: 19, loss: 0.3024887442588806, acc: 85.9375, f1: 68.23843700159489, r: 0.6295663187441399
06/02/2019 09:15:21 step: 2269, epoch: 68, batch: 24, loss: 0.38345232605934143, acc: 84.375, f1: 81.03809306816825, r: 0.6033569051265265
06/02/2019 09:15:21 step: 2274, epoch: 68, batch: 29, loss: 0.1131211519241333, acc: 96.875, f1: 93.71748699479792, r: 0.7647837517813895
06/02/2019 09:15:21 *** evaluating ***
06/02/2019 09:15:21 step: 69, epoch: 68, acc: 60.256410256410255, f1: 24.216346153846157, r: 0.40657487703454315
06/02/2019 09:15:21 *** epoch: 70 ***
06/02/2019 09:15:21 *** training ***
06/02/2019 09:15:21 step: 2282, epoch: 69, batch: 4, loss: 0.19376440346240997, acc: 98.4375, f1: 98.58823529411765, r: 0.6937783229359406
06/02/2019 09:15:22 step: 2287, epoch: 69, batch: 9, loss: 0.25453388690948486, acc: 90.625, f1: 78.28282828282829, r: 0.7111896181482922
06/02/2019 09:15:22 step: 2292, epoch: 69, batch: 14, loss: 0.19186672568321228, acc: 96.875, f1: 96.72619047619048, r: 0.5697238808137681
06/02/2019 09:15:22 step: 2297, epoch: 69, batch: 19, loss: 0.32089921832084656, acc: 87.5, f1: 87.5503432819101, r: 0.663048784940851
06/02/2019 09:15:22 step: 2302, epoch: 69, batch: 24, loss: 0.22567111253738403, acc: 93.75, f1: 92.10982372747078, r: 0.741493961516191
06/02/2019 09:15:22 step: 2307, epoch: 69, batch: 29, loss: 0.1300572007894516, acc: 96.875, f1: 80.26479408967889, r: 0.5586469994603113
06/02/2019 09:15:22 *** evaluating ***
06/02/2019 09:15:23 step: 70, epoch: 69, acc: 60.68376068376068, f1: 28.043243395522122, r: 0.409633419874642
06/02/2019 09:15:23 *** epoch: 71 ***
06/02/2019 09:15:23 *** training ***
06/02/2019 09:15:23 step: 2315, epoch: 70, batch: 4, loss: 0.2693231403827667, acc: 90.625, f1: 79.96397243107769, r: 0.6912387441803199
06/02/2019 09:15:23 step: 2320, epoch: 70, batch: 9, loss: 0.25762391090393066, acc: 92.1875, f1: 90.27555777555777, r: 0.6055909980214041
06/02/2019 09:15:23 step: 2325, epoch: 70, batch: 14, loss: 0.3261871337890625, acc: 92.1875, f1: 78.89610389610388, r: 0.6166817581285862
06/02/2019 09:15:23 step: 2330, epoch: 70, batch: 19, loss: 0.14055022597312927, acc: 96.875, f1: 93.7525131510094, r: 0.6770395451367346
06/02/2019 09:15:24 step: 2335, epoch: 70, batch: 24, loss: 0.34417295455932617, acc: 87.5, f1: 87.83286294315707, r: 0.766073677945676
06/02/2019 09:15:24 step: 2340, epoch: 70, batch: 29, loss: 0.28450942039489746, acc: 92.1875, f1: 92.3469387755102, r: 0.6591446619306541
06/02/2019 09:15:24 *** evaluating ***
06/02/2019 09:15:24 step: 71, epoch: 70, acc: 59.82905982905983, f1: 27.429748110328877, r: 0.4062896366995181
06/02/2019 09:15:24 *** epoch: 72 ***
06/02/2019 09:15:24 *** training ***
06/02/2019 09:15:24 step: 2348, epoch: 71, batch: 4, loss: 0.32474973797798157, acc: 92.1875, f1: 91.46083888100695, r: 0.6443891108717718
06/02/2019 09:15:24 step: 2353, epoch: 71, batch: 9, loss: 0.27140846848487854, acc: 93.75, f1: 92.936019702937, r: 0.7656761294485253
06/02/2019 09:15:25 step: 2358, epoch: 71, batch: 14, loss: 0.18594320118427277, acc: 93.75, f1: 86.10119047619047, r: 0.636629223221884
06/02/2019 09:15:25 step: 2363, epoch: 71, batch: 19, loss: 0.2814568877220154, acc: 92.1875, f1: 76.87927941797912, r: 0.6085592812572089
06/02/2019 09:15:25 step: 2368, epoch: 71, batch: 24, loss: 0.26751378178596497, acc: 89.0625, f1: 76.49780719147199, r: 0.6302599886122762
06/02/2019 09:15:25 step: 2373, epoch: 71, batch: 29, loss: 0.3041563034057617, acc: 87.5, f1: 64.0265984015984, r: 0.5875045459567708
06/02/2019 09:15:25 *** evaluating ***
06/02/2019 09:15:25 step: 72, epoch: 71, acc: 59.401709401709404, f1: 26.17961227467805, r: 0.408576034587251
06/02/2019 09:15:25 *** epoch: 73 ***
06/02/2019 09:15:25 *** training ***
06/02/2019 09:15:25 step: 2381, epoch: 72, batch: 4, loss: 0.2864745855331421, acc: 92.1875, f1: 86.08350586611456, r: 0.6762120135436098
06/02/2019 09:15:26 step: 2386, epoch: 72, batch: 9, loss: 0.18795032799243927, acc: 93.75, f1: 91.38736263736264, r: 0.7643738747180314
06/02/2019 09:15:26 step: 2391, epoch: 72, batch: 14, loss: 0.2618199586868286, acc: 93.75, f1: 94.30518584245291, r: 0.6484068641678719
06/02/2019 09:15:26 step: 2396, epoch: 72, batch: 19, loss: 0.2636425495147705, acc: 90.625, f1: 83.49034141958671, r: 0.6947655337255453
06/02/2019 09:15:26 step: 2401, epoch: 72, batch: 24, loss: 0.2817212641239166, acc: 87.5, f1: 83.38528138528139, r: 0.7330528198076007
06/02/2019 09:15:26 step: 2406, epoch: 72, batch: 29, loss: 0.33881139755249023, acc: 87.5, f1: 87.44471783985462, r: 0.5855562651613386
06/02/2019 09:15:27 *** evaluating ***
06/02/2019 09:15:27 step: 73, epoch: 72, acc: 61.53846153846154, f1: 27.848348021615347, r: 0.4115626913921036
06/02/2019 09:15:27 *** epoch: 74 ***
06/02/2019 09:15:27 *** training ***
06/02/2019 09:15:27 step: 2414, epoch: 73, batch: 4, loss: 0.227286234498024, acc: 90.625, f1: 87.56547619047619, r: 0.7288845628577528
06/02/2019 09:15:27 step: 2419, epoch: 73, batch: 9, loss: 0.3070436418056488, acc: 90.625, f1: 85.26244588744589, r: 0.6424600500071616
06/02/2019 09:15:27 step: 2424, epoch: 73, batch: 14, loss: 0.2581908702850342, acc: 85.9375, f1: 76.41329241329241, r: 0.6815730251504118
06/02/2019 09:15:27 step: 2429, epoch: 73, batch: 19, loss: 0.2103102207183838, acc: 95.3125, f1: 95.81124403784503, r: 0.6791576777233872
06/02/2019 09:15:28 step: 2434, epoch: 73, batch: 24, loss: 0.19665317237377167, acc: 95.3125, f1: 95.33806566104703, r: 0.6375710646249546
06/02/2019 09:15:28 step: 2439, epoch: 73, batch: 29, loss: 0.20159006118774414, acc: 90.625, f1: 88.1288609349834, r: 0.6463918834551622
06/02/2019 09:15:28 *** evaluating ***
06/02/2019 09:15:28 step: 74, epoch: 73, acc: 59.82905982905983, f1: 24.562161539347848, r: 0.4111104297231984
06/02/2019 09:15:28 *** epoch: 75 ***
06/02/2019 09:15:28 *** training ***
06/02/2019 09:15:28 step: 2447, epoch: 74, batch: 4, loss: 0.25201189517974854, acc: 90.625, f1: 75.50509310377731, r: 0.6935249710381324
06/02/2019 09:15:28 step: 2452, epoch: 74, batch: 9, loss: 0.1487129032611847, acc: 93.75, f1: 91.63112020254877, r: 0.7089939795705325
06/02/2019 09:15:29 step: 2457, epoch: 74, batch: 14, loss: 0.30879202485084534, acc: 90.625, f1: 90.40834980727145, r: 0.6517521092251861
06/02/2019 09:15:29 step: 2462, epoch: 74, batch: 19, loss: 0.3652932345867157, acc: 87.5, f1: 87.35569985569987, r: 0.696059406413439
06/02/2019 09:15:29 step: 2467, epoch: 74, batch: 24, loss: 0.2921525835990906, acc: 90.625, f1: 90.29552760165005, r: 0.6267312264623637
06/02/2019 09:15:29 step: 2472, epoch: 74, batch: 29, loss: 0.2055635303258896, acc: 93.75, f1: 91.11111111111111, r: 0.7688259425331884
06/02/2019 09:15:29 *** evaluating ***
06/02/2019 09:15:29 step: 75, epoch: 74, acc: 59.82905982905983, f1: 26.56328320802005, r: 0.40878888401877694
06/02/2019 09:15:29 *** epoch: 76 ***
06/02/2019 09:15:29 *** training ***
06/02/2019 09:15:30 step: 2480, epoch: 75, batch: 4, loss: 0.3737211227416992, acc: 87.5, f1: 74.28925736961452, r: 0.599071113950197
06/02/2019 09:15:30 step: 2485, epoch: 75, batch: 9, loss: 0.3282589614391327, acc: 89.0625, f1: 84.27518134039873, r: 0.6584943850609905
06/02/2019 09:15:30 step: 2490, epoch: 75, batch: 14, loss: 0.19120250642299652, acc: 96.875, f1: 95.41950113378685, r: 0.6726080557799978
06/02/2019 09:15:30 step: 2495, epoch: 75, batch: 19, loss: 0.3051370680332184, acc: 89.0625, f1: 68.16123642439432, r: 0.5609562142313609
06/02/2019 09:15:31 step: 2500, epoch: 75, batch: 24, loss: 0.21029388904571533, acc: 89.0625, f1: 90.11636008918617, r: 0.7357038075561438
06/02/2019 09:15:31 step: 2505, epoch: 75, batch: 29, loss: 0.1904333531856537, acc: 93.75, f1: 79.89005879031454, r: 0.6923385627746411
06/02/2019 09:15:31 *** evaluating ***
06/02/2019 09:15:31 step: 76, epoch: 75, acc: 59.82905982905983, f1: 25.10963403290136, r: 0.4074157772611606
06/02/2019 09:15:31 *** epoch: 77 ***
06/02/2019 09:15:31 *** training ***
06/02/2019 09:15:31 step: 2513, epoch: 76, batch: 4, loss: 0.19860884547233582, acc: 92.1875, f1: 88.01466854173991, r: 0.7614207634651832
06/02/2019 09:15:31 step: 2518, epoch: 76, batch: 9, loss: 0.22805023193359375, acc: 92.1875, f1: 89.66986289607007, r: 0.6241771868557249
06/02/2019 09:15:32 step: 2523, epoch: 76, batch: 14, loss: 0.2597634196281433, acc: 89.0625, f1: 81.84890798424634, r: 0.5794554750328821
06/02/2019 09:15:32 step: 2528, epoch: 76, batch: 19, loss: 0.14386269450187683, acc: 96.875, f1: 92.34654234654234, r: 0.7532445953113162
06/02/2019 09:15:32 step: 2533, epoch: 76, batch: 24, loss: 0.18731358647346497, acc: 92.1875, f1: 82.46565934065934, r: 0.7369937349352569
06/02/2019 09:15:32 step: 2538, epoch: 76, batch: 29, loss: 0.19157209992408752, acc: 90.625, f1: 79.65730812132531, r: 0.6247090339941714
06/02/2019 09:15:32 *** evaluating ***
06/02/2019 09:15:32 step: 77, epoch: 76, acc: 59.401709401709404, f1: 25.75583907573482, r: 0.4056457960972001
06/02/2019 09:15:32 *** epoch: 78 ***
06/02/2019 09:15:32 *** training ***
06/02/2019 09:15:33 step: 2546, epoch: 77, batch: 4, loss: 0.3398401737213135, acc: 89.0625, f1: 74.90409697713672, r: 0.5648337414791635
06/02/2019 09:15:33 step: 2551, epoch: 77, batch: 9, loss: 0.2405223548412323, acc: 87.5, f1: 70.63402283990519, r: 0.5652048245112685
06/02/2019 09:15:33 step: 2556, epoch: 77, batch: 14, loss: 0.3193240165710449, acc: 90.625, f1: 88.68347338935575, r: 0.6483592711064043
06/02/2019 09:15:33 step: 2561, epoch: 77, batch: 19, loss: 0.2729220688343048, acc: 92.1875, f1: 78.40611492499366, r: 0.5889139530272653
06/02/2019 09:15:34 step: 2566, epoch: 77, batch: 24, loss: 0.1768442690372467, acc: 96.875, f1: 96.82048257899652, r: 0.6707807076841619
06/02/2019 09:15:34 step: 2571, epoch: 77, batch: 29, loss: 0.1663162112236023, acc: 96.875, f1: 96.16693899782135, r: 0.7447847753988027
06/02/2019 09:15:34 *** evaluating ***
06/02/2019 09:15:34 step: 78, epoch: 77, acc: 60.68376068376068, f1: 24.078713365835767, r: 0.4054977592090105
06/02/2019 09:15:34 *** epoch: 79 ***
06/02/2019 09:15:34 *** training ***
06/02/2019 09:15:34 step: 2579, epoch: 78, batch: 4, loss: 0.3901876211166382, acc: 89.0625, f1: 72.45391705069125, r: 0.5868908179793144
06/02/2019 09:15:35 step: 2584, epoch: 78, batch: 9, loss: 0.29630160331726074, acc: 87.5, f1: 70.77340551605258, r: 0.5294994327157574
06/02/2019 09:15:35 step: 2589, epoch: 78, batch: 14, loss: 0.288463830947876, acc: 92.1875, f1: 89.96933621933621, r: 0.7615850819066495
06/02/2019 09:15:35 step: 2594, epoch: 78, batch: 19, loss: 0.4230528771877289, acc: 85.9375, f1: 69.48863636363636, r: 0.5656924661847547
06/02/2019 09:15:35 step: 2599, epoch: 78, batch: 24, loss: 0.3395727872848511, acc: 87.5, f1: 73.41739438722198, r: 0.6498023916636161
06/02/2019 09:15:35 step: 2604, epoch: 78, batch: 29, loss: 0.24670326709747314, acc: 90.625, f1: 87.33912907268171, r: 0.7268853625327005
06/02/2019 09:15:36 *** evaluating ***
06/02/2019 09:15:36 step: 79, epoch: 78, acc: 60.256410256410255, f1: 25.197547409768507, r: 0.4039932072291853
06/02/2019 09:15:36 *** epoch: 80 ***
06/02/2019 09:15:36 *** training ***
06/02/2019 09:15:36 step: 2612, epoch: 79, batch: 4, loss: 0.23349805176258087, acc: 92.1875, f1: 77.68187475491449, r: 0.6339869950688101
06/02/2019 09:15:36 step: 2617, epoch: 79, batch: 9, loss: 0.16596193611621857, acc: 93.75, f1: 87.44384937933326, r: 0.660545356149922
06/02/2019 09:15:36 step: 2622, epoch: 79, batch: 14, loss: 0.2047514021396637, acc: 90.625, f1: 90.04903688043223, r: 0.7274049552876086
06/02/2019 09:15:36 step: 2627, epoch: 79, batch: 19, loss: 0.2868100106716156, acc: 87.5, f1: 78.64982970246128, r: 0.6181938630022147
06/02/2019 09:15:37 step: 2632, epoch: 79, batch: 24, loss: 0.2768586277961731, acc: 90.625, f1: 86.39886180647049, r: 0.7714985920389478
06/02/2019 09:15:37 step: 2637, epoch: 79, batch: 29, loss: 0.2283082902431488, acc: 87.5, f1: 83.90639913072846, r: 0.6331101910178838
06/02/2019 09:15:37 *** evaluating ***
06/02/2019 09:15:37 step: 80, epoch: 79, acc: 60.256410256410255, f1: 28.15008337965536, r: 0.4111171147909412
06/02/2019 09:15:37 *** epoch: 81 ***
06/02/2019 09:15:37 *** training ***
06/02/2019 09:15:37 step: 2645, epoch: 80, batch: 4, loss: 0.231597438454628, acc: 92.1875, f1: 78.22314427736528, r: 0.6744429319112585
06/02/2019 09:15:38 step: 2650, epoch: 80, batch: 9, loss: 0.20441210269927979, acc: 93.75, f1: 78.77941874424188, r: 0.6301350555667914
06/02/2019 09:15:38 step: 2655, epoch: 80, batch: 14, loss: 0.2493332326412201, acc: 90.625, f1: 85.82071143814485, r: 0.6556620797802276
06/02/2019 09:15:38 step: 2660, epoch: 80, batch: 19, loss: 0.1801231950521469, acc: 92.1875, f1: 88.86867790594496, r: 0.6040358513077716
06/02/2019 09:15:38 step: 2665, epoch: 80, batch: 24, loss: 0.3670831620693207, acc: 89.0625, f1: 88.71908836129734, r: 0.6909739005680684
06/02/2019 09:15:38 step: 2670, epoch: 80, batch: 29, loss: 0.20271192491054535, acc: 89.0625, f1: 86.36616453328044, r: 0.5983109904101738
06/02/2019 09:15:38 *** evaluating ***
06/02/2019 09:15:39 step: 81, epoch: 80, acc: 60.256410256410255, f1: 25.886340335492875, r: 0.4095846391639506
06/02/2019 09:15:39 *** epoch: 82 ***
06/02/2019 09:15:39 *** training ***
06/02/2019 09:15:39 step: 2678, epoch: 81, batch: 4, loss: 0.17821161448955536, acc: 90.625, f1: 76.13095238095238, r: 0.6870676693560523
06/02/2019 09:15:39 step: 2683, epoch: 81, batch: 9, loss: 0.1266038864850998, acc: 93.75, f1: 77.9354494072236, r: 0.7304976349209301
06/02/2019 09:15:39 step: 2688, epoch: 81, batch: 14, loss: 0.26021087169647217, acc: 90.625, f1: 74.93094294876977, r: 0.693553258720089
06/02/2019 09:15:39 step: 2693, epoch: 81, batch: 19, loss: 0.18635524809360504, acc: 95.3125, f1: 90.58282403870639, r: 0.7229924195680255
06/02/2019 09:15:40 step: 2698, epoch: 81, batch: 24, loss: 0.23522844910621643, acc: 92.1875, f1: 91.10693400167085, r: 0.611361424568333
06/02/2019 09:15:40 step: 2703, epoch: 81, batch: 29, loss: 0.1704060286283493, acc: 93.75, f1: 93.57009681694633, r: 0.5352271164114993
06/02/2019 09:15:40 *** evaluating ***
06/02/2019 09:15:40 step: 82, epoch: 81, acc: 58.97435897435898, f1: 23.846237673997244, r: 0.4052421481655866
06/02/2019 09:15:40 *** epoch: 83 ***
06/02/2019 09:15:40 *** training ***
06/02/2019 09:15:40 step: 2711, epoch: 82, batch: 4, loss: 0.13748615980148315, acc: 93.75, f1: 96.52173913043478, r: 0.7150784844848529
06/02/2019 09:15:41 step: 2716, epoch: 82, batch: 9, loss: 0.3163892924785614, acc: 92.1875, f1: 90.5250192138779, r: 0.6974631087127513
06/02/2019 09:15:41 step: 2721, epoch: 82, batch: 14, loss: 0.30230075120925903, acc: 90.625, f1: 92.14577950728554, r: 0.6252319051940709
06/02/2019 09:15:41 step: 2726, epoch: 82, batch: 19, loss: 0.3010423481464386, acc: 90.625, f1: 77.48148601922186, r: 0.6166187320003956
06/02/2019 09:15:41 step: 2731, epoch: 82, batch: 24, loss: 0.30265378952026367, acc: 92.1875, f1: 79.34027777777779, r: 0.6524635212648789
06/02/2019 09:15:41 step: 2736, epoch: 82, batch: 29, loss: 0.30091163516044617, acc: 84.375, f1: 82.66045548654245, r: 0.6501927331055963
06/02/2019 09:15:42 *** evaluating ***
06/02/2019 09:15:42 step: 83, epoch: 82, acc: 60.68376068376068, f1: 26.241264429922754, r: 0.40745779049818154
06/02/2019 09:15:42 *** epoch: 84 ***
06/02/2019 09:15:42 *** training ***
06/02/2019 09:15:42 step: 2744, epoch: 83, batch: 4, loss: 0.1770029366016388, acc: 95.3125, f1: 94.05209495347353, r: 0.7173675029976742
06/02/2019 09:15:42 step: 2749, epoch: 83, batch: 9, loss: 0.21884746849536896, acc: 92.1875, f1: 81.72601540616246, r: 0.7445986161957088
06/02/2019 09:15:42 step: 2754, epoch: 83, batch: 14, loss: 0.1652928590774536, acc: 93.75, f1: 92.35741714002583, r: 0.7771263481872154
06/02/2019 09:15:42 step: 2759, epoch: 83, batch: 19, loss: 0.18586131930351257, acc: 93.75, f1: 93.79667951096522, r: 0.6738736666746014
06/02/2019 09:15:43 step: 2764, epoch: 83, batch: 24, loss: 0.18859827518463135, acc: 93.75, f1: 76.57738095238096, r: 0.6205539340379639
06/02/2019 09:15:43 step: 2769, epoch: 83, batch: 29, loss: 0.11508169025182724, acc: 96.875, f1: 96.99879951980792, r: 0.7526307506107166
06/02/2019 09:15:43 *** evaluating ***
06/02/2019 09:15:43 step: 84, epoch: 83, acc: 61.111111111111114, f1: 27.90069142073859, r: 0.4049064279903291
06/02/2019 09:15:43 *** epoch: 85 ***
06/02/2019 09:15:43 *** training ***
06/02/2019 09:15:43 step: 2777, epoch: 84, batch: 4, loss: 0.2064247578382492, acc: 92.1875, f1: 89.43742443742444, r: 0.756723277110364
06/02/2019 09:15:43 step: 2782, epoch: 84, batch: 9, loss: 0.28705912828445435, acc: 92.1875, f1: 92.97157622739017, r: 0.6433120495590688
06/02/2019 09:15:44 step: 2787, epoch: 84, batch: 14, loss: 0.18548308312892914, acc: 95.3125, f1: 90.38025466596896, r: 0.6061915827860527
06/02/2019 09:15:44 step: 2792, epoch: 84, batch: 19, loss: 0.15421432256698608, acc: 95.3125, f1: 93.80261248185778, r: 0.7037380422071656
06/02/2019 09:15:44 step: 2797, epoch: 84, batch: 24, loss: 0.20701637864112854, acc: 90.625, f1: 85.62833714721586, r: 0.7407460974531543
06/02/2019 09:15:44 step: 2802, epoch: 84, batch: 29, loss: 0.19296903908252716, acc: 95.3125, f1: 92.88690476190476, r: 0.7774898731900912
06/02/2019 09:15:45 *** evaluating ***
06/02/2019 09:15:45 step: 85, epoch: 84, acc: 61.111111111111114, f1: 26.553785415181753, r: 0.40731761925162036
06/02/2019 09:15:45 *** epoch: 86 ***
06/02/2019 09:15:45 *** training ***
06/02/2019 09:15:45 step: 2810, epoch: 85, batch: 4, loss: 0.3248974084854126, acc: 92.1875, f1: 77.69946558056314, r: 0.5830166499250898
06/02/2019 09:15:45 step: 2815, epoch: 85, batch: 9, loss: 0.2384151816368103, acc: 89.0625, f1: 81.43627262225645, r: 0.6772124365022646
06/02/2019 09:15:45 step: 2820, epoch: 85, batch: 14, loss: 0.12830454111099243, acc: 93.75, f1: 90.60329607499419, r: 0.6558381039471923
06/02/2019 09:15:46 step: 2825, epoch: 85, batch: 19, loss: 0.24756057560443878, acc: 90.625, f1: 77.85346673254281, r: 0.6473051168972482
06/02/2019 09:15:46 step: 2830, epoch: 85, batch: 24, loss: 0.14735454320907593, acc: 93.75, f1: 87.32560699773813, r: 0.7805723514278079
06/02/2019 09:15:46 step: 2835, epoch: 85, batch: 29, loss: 0.4340691864490509, acc: 82.8125, f1: 64.65902828551786, r: 0.5219477545916926
06/02/2019 09:15:46 *** evaluating ***
06/02/2019 09:15:46 step: 86, epoch: 85, acc: 61.965811965811966, f1: 31.28282024616127, r: 0.4081827118544628
06/02/2019 09:15:46 *** epoch: 87 ***
06/02/2019 09:15:46 *** training ***
06/02/2019 09:15:47 step: 2843, epoch: 86, batch: 4, loss: 0.296604722738266, acc: 87.5, f1: 84.46086349813058, r: 0.5988413589734696
06/02/2019 09:15:47 step: 2848, epoch: 86, batch: 9, loss: 0.13219338655471802, acc: 95.3125, f1: 94.05067758328627, r: 0.7619412089271407
06/02/2019 09:15:47 step: 2853, epoch: 86, batch: 14, loss: 0.08835490792989731, acc: 100.0, f1: 100.0, r: 0.6341231795572481
06/02/2019 09:15:47 step: 2858, epoch: 86, batch: 19, loss: 0.19358369708061218, acc: 93.75, f1: 95.04608294930875, r: 0.6516865830095578
06/02/2019 09:15:48 step: 2863, epoch: 86, batch: 24, loss: 0.18768340349197388, acc: 93.75, f1: 78.79655067155068, r: 0.7411966371358567
06/02/2019 09:15:48 step: 2868, epoch: 86, batch: 29, loss: 0.3037276566028595, acc: 89.0625, f1: 79.30918144166783, r: 0.7555772558688261
06/02/2019 09:15:48 *** evaluating ***
06/02/2019 09:15:48 step: 87, epoch: 86, acc: 61.53846153846154, f1: 28.184405533276173, r: 0.40910913531671816
06/02/2019 09:15:48 *** epoch: 88 ***
06/02/2019 09:15:48 *** training ***
06/02/2019 09:15:48 step: 2876, epoch: 87, batch: 4, loss: 0.16422298550605774, acc: 95.3125, f1: 93.94097222222221, r: 0.7371915806215824
06/02/2019 09:15:48 step: 2881, epoch: 87, batch: 9, loss: 0.32870662212371826, acc: 87.5, f1: 88.31935975609755, r: 0.7435705753196687
06/02/2019 09:15:49 step: 2886, epoch: 87, batch: 14, loss: 0.23299363255500793, acc: 90.625, f1: 76.6196297481292, r: 0.7474175989986112
06/02/2019 09:15:49 step: 2891, epoch: 87, batch: 19, loss: 0.20595595240592957, acc: 95.3125, f1: 85.28703703703704, r: 0.6566908728415639
06/02/2019 09:15:49 step: 2896, epoch: 87, batch: 24, loss: 0.2812432050704956, acc: 92.1875, f1: 77.9905468680979, r: 0.5196191806905335
06/02/2019 09:15:49 step: 2901, epoch: 87, batch: 29, loss: 0.1394660323858261, acc: 95.3125, f1: 90.11733880154932, r: 0.6829849931449126
06/02/2019 09:15:50 *** evaluating ***
06/02/2019 09:15:50 step: 88, epoch: 87, acc: 58.97435897435898, f1: 25.284527300656336, r: 0.40166077926974036
06/02/2019 09:15:50 *** epoch: 89 ***
06/02/2019 09:15:50 *** training ***
06/02/2019 09:15:50 step: 2909, epoch: 88, batch: 4, loss: 0.19638937711715698, acc: 92.1875, f1: 92.05604977033549, r: 0.6747174874900501
06/02/2019 09:15:50 step: 2914, epoch: 88, batch: 9, loss: 0.21780136227607727, acc: 93.75, f1: 91.12690201245188, r: 0.7036515357259867
06/02/2019 09:15:50 step: 2919, epoch: 88, batch: 14, loss: 0.2246265411376953, acc: 93.75, f1: 90.04475821336287, r: 0.5928227192814742
06/02/2019 09:15:51 step: 2924, epoch: 88, batch: 19, loss: 0.093062624335289, acc: 98.4375, f1: 99.1186839012926, r: 0.7776099134143638
06/02/2019 09:15:51 step: 2929, epoch: 88, batch: 24, loss: 0.24108685553073883, acc: 93.75, f1: 93.26965305226175, r: 0.6968595343716926
06/02/2019 09:15:51 step: 2934, epoch: 88, batch: 29, loss: 0.10415197163820267, acc: 96.875, f1: 92.39277271192165, r: 0.7869916368504868
06/02/2019 09:15:51 *** evaluating ***
06/02/2019 09:15:51 step: 89, epoch: 88, acc: 61.53846153846154, f1: 28.902590492396318, r: 0.4100568937754487
06/02/2019 09:15:51 *** epoch: 90 ***
06/02/2019 09:15:51 *** training ***
06/02/2019 09:15:52 step: 2942, epoch: 89, batch: 4, loss: 0.18075574934482574, acc: 90.625, f1: 88.64692088956795, r: 0.7646932529940144
06/02/2019 09:15:52 step: 2947, epoch: 89, batch: 9, loss: 0.30636096000671387, acc: 90.625, f1: 88.93757726055863, r: 0.6192937101472858
06/02/2019 09:15:52 step: 2952, epoch: 89, batch: 14, loss: 0.3920074999332428, acc: 87.5, f1: 75.3475813402284, r: 0.713883687819657
06/02/2019 09:15:52 step: 2957, epoch: 89, batch: 19, loss: 0.1382961869239807, acc: 93.75, f1: 94.08015158015158, r: 0.8235730136856981
06/02/2019 09:15:53 step: 2962, epoch: 89, batch: 24, loss: 0.22656984627246857, acc: 90.625, f1: 75.93898633966836, r: 0.5229444387232255
06/02/2019 09:15:53 step: 2967, epoch: 89, batch: 29, loss: 0.2983306050300598, acc: 89.0625, f1: 78.88550043160238, r: 0.6776049483329465
06/02/2019 09:15:53 *** evaluating ***
06/02/2019 09:15:53 step: 90, epoch: 89, acc: 60.68376068376068, f1: 28.105764118922018, r: 0.40238768393189217
06/02/2019 09:15:53 *** epoch: 91 ***
06/02/2019 09:15:53 *** training ***
06/02/2019 09:15:53 step: 2975, epoch: 90, batch: 4, loss: 0.1884484589099884, acc: 93.75, f1: 85.09833024118738, r: 0.6684427772965398
06/02/2019 09:15:54 step: 2980, epoch: 90, batch: 9, loss: 0.2742471694946289, acc: 96.875, f1: 97.781438074121, r: 0.6832679267676903
06/02/2019 09:15:54 step: 2985, epoch: 90, batch: 14, loss: 0.18534661829471588, acc: 95.3125, f1: 97.5067385444744, r: 0.7607044188183365
06/02/2019 09:15:54 step: 2990, epoch: 90, batch: 19, loss: 0.19851601123809814, acc: 95.3125, f1: 93.57431779372249, r: 0.8377053283686489
06/02/2019 09:15:54 step: 2995, epoch: 90, batch: 24, loss: 0.18487542867660522, acc: 93.75, f1: 78.49128073184677, r: 0.6412693598904167
06/02/2019 09:15:55 step: 3000, epoch: 90, batch: 29, loss: 0.22093269228935242, acc: 92.1875, f1: 91.79292929292929, r: 0.7638886563624743
06/02/2019 09:15:55 *** evaluating ***
06/02/2019 09:15:55 step: 91, epoch: 90, acc: 61.53846153846154, f1: 30.47323571254078, r: 0.4050053264684143
06/02/2019 09:15:55 *** epoch: 92 ***
06/02/2019 09:15:55 *** training ***
06/02/2019 09:15:55 step: 3008, epoch: 91, batch: 4, loss: 0.09252581745386124, acc: 96.875, f1: 98.20831342570473, r: 0.6552230020162929
06/02/2019 09:15:55 step: 3013, epoch: 91, batch: 9, loss: 0.21705196797847748, acc: 95.3125, f1: 92.80310001984878, r: 0.544804166843997
06/02/2019 09:15:55 step: 3018, epoch: 91, batch: 14, loss: 0.270597368478775, acc: 93.75, f1: 91.25818625818626, r: 0.6689630544146966
06/02/2019 09:15:56 step: 3023, epoch: 91, batch: 19, loss: 0.17423713207244873, acc: 92.1875, f1: 80.92432086176694, r: 0.6886866109971316
06/02/2019 09:15:56 step: 3028, epoch: 91, batch: 24, loss: 0.22711633145809174, acc: 92.1875, f1: 90.28493728493729, r: 0.7547801600669476
06/02/2019 09:15:56 step: 3033, epoch: 91, batch: 29, loss: 0.11250472813844681, acc: 98.4375, f1: 98.1111111111111, r: 0.7205756117719441
06/02/2019 09:15:56 *** evaluating ***
06/02/2019 09:15:56 step: 92, epoch: 91, acc: 61.111111111111114, f1: 27.363552700773795, r: 0.3992017372913807
06/02/2019 09:15:56 *** epoch: 93 ***
06/02/2019 09:15:56 *** training ***
06/02/2019 09:15:57 step: 3041, epoch: 92, batch: 4, loss: 0.09099497646093369, acc: 98.4375, f1: 99.27107959022851, r: 0.7776841463253051
06/02/2019 09:15:57 step: 3046, epoch: 92, batch: 9, loss: 0.2673060894012451, acc: 92.1875, f1: 87.97268907563026, r: 0.7198702710404004
06/02/2019 09:15:57 step: 3051, epoch: 92, batch: 14, loss: 0.25718992948532104, acc: 93.75, f1: 77.67983144674874, r: 0.7535319734812642
06/02/2019 09:15:57 step: 3056, epoch: 92, batch: 19, loss: 0.3019043505191803, acc: 90.625, f1: 83.18181818181819, r: 0.6047938072835755
06/02/2019 09:15:58 step: 3061, epoch: 92, batch: 24, loss: 0.1406945437192917, acc: 95.3125, f1: 86.80896513194651, r: 0.6333353600900621
06/02/2019 09:15:58 step: 3066, epoch: 92, batch: 29, loss: 0.09078823029994965, acc: 98.4375, f1: 99.14468995010691, r: 0.637708462489439
06/02/2019 09:15:58 *** evaluating ***
06/02/2019 09:15:58 step: 93, epoch: 92, acc: 61.111111111111114, f1: 27.044776593043924, r: 0.40136830599044604
06/02/2019 09:15:58 *** epoch: 94 ***
06/02/2019 09:15:58 *** training ***
06/02/2019 09:15:58 step: 3074, epoch: 93, batch: 4, loss: 0.2189233899116516, acc: 93.75, f1: 88.50877192982456, r: 0.7296677414417291
06/02/2019 09:15:59 step: 3079, epoch: 93, batch: 9, loss: 0.25922778248786926, acc: 89.0625, f1: 86.40623868884738, r: 0.7120691165376827
06/02/2019 09:15:59 step: 3084, epoch: 93, batch: 14, loss: 0.21320125460624695, acc: 95.3125, f1: 95.27007600096056, r: 0.7451057406320041
06/02/2019 09:15:59 step: 3089, epoch: 93, batch: 19, loss: 0.2645663619041443, acc: 89.0625, f1: 86.10804682233253, r: 0.6385542802850566
06/02/2019 09:15:59 step: 3094, epoch: 93, batch: 24, loss: 0.2995269000530243, acc: 87.5, f1: 82.60945403802546, r: 0.567075994065536
06/02/2019 09:15:59 step: 3099, epoch: 93, batch: 29, loss: 0.2591412365436554, acc: 92.1875, f1: 92.29998072103336, r: 0.7778466931424982
06/02/2019 09:16:00 *** evaluating ***
06/02/2019 09:16:00 step: 94, epoch: 93, acc: 61.111111111111114, f1: 27.04413851062989, r: 0.4023946920061681
06/02/2019 09:16:00 *** epoch: 95 ***
06/02/2019 09:16:00 *** training ***
06/02/2019 09:16:00 step: 3107, epoch: 94, batch: 4, loss: 0.20541870594024658, acc: 93.75, f1: 87.73242630385487, r: 0.6290386314961083
06/02/2019 09:16:00 step: 3112, epoch: 94, batch: 9, loss: 0.33275502920150757, acc: 89.0625, f1: 72.2841115821798, r: 0.44307406522327947
06/02/2019 09:16:00 step: 3117, epoch: 94, batch: 14, loss: 0.21986737847328186, acc: 92.1875, f1: 92.33458066342165, r: 0.6288800392137174
06/02/2019 09:16:01 step: 3122, epoch: 94, batch: 19, loss: 0.11957091093063354, acc: 93.75, f1: 95.08054650431602, r: 0.7245368546444528
06/02/2019 09:16:01 step: 3127, epoch: 94, batch: 24, loss: 0.11224659532308578, acc: 95.3125, f1: 93.57954545454545, r: 0.732437944070453
06/02/2019 09:16:01 step: 3132, epoch: 94, batch: 29, loss: 0.2018408179283142, acc: 92.1875, f1: 94.47239422084624, r: 0.7793731872268153
06/02/2019 09:16:01 *** evaluating ***
06/02/2019 09:16:01 step: 95, epoch: 94, acc: 61.965811965811966, f1: 28.568450884345705, r: 0.403061314482562
06/02/2019 09:16:01 *** epoch: 96 ***
06/02/2019 09:16:01 *** training ***
06/02/2019 09:16:02 step: 3140, epoch: 95, batch: 4, loss: 0.10583376884460449, acc: 96.875, f1: 93.42136854741896, r: 0.6655920504266387
06/02/2019 09:16:02 step: 3145, epoch: 95, batch: 9, loss: 0.20654639601707458, acc: 92.1875, f1: 89.85854886934244, r: 0.6836401358802152
06/02/2019 09:16:02 step: 3150, epoch: 95, batch: 14, loss: 0.14807534217834473, acc: 93.75, f1: 89.41517046780206, r: 0.6427278157959547
06/02/2019 09:16:02 step: 3155, epoch: 95, batch: 19, loss: 0.11841034889221191, acc: 95.3125, f1: 95.28623622240644, r: 0.6822052615624916
06/02/2019 09:16:03 step: 3160, epoch: 95, batch: 24, loss: 0.15470242500305176, acc: 93.75, f1: 82.03030303030303, r: 0.822992599678199
06/02/2019 09:16:03 step: 3165, epoch: 95, batch: 29, loss: 0.20661595463752747, acc: 96.875, f1: 97.63755980861244, r: 0.7658216208514275
06/02/2019 09:16:03 *** evaluating ***
06/02/2019 09:16:03 step: 96, epoch: 95, acc: 59.82905982905983, f1: 26.45457643124642, r: 0.3917444203331206
06/02/2019 09:16:03 *** epoch: 97 ***
06/02/2019 09:16:03 *** training ***
06/02/2019 09:16:03 step: 3173, epoch: 96, batch: 4, loss: 0.2624395787715912, acc: 95.3125, f1: 95.18849206349206, r: 0.6879165109621374
06/02/2019 09:16:03 step: 3178, epoch: 96, batch: 9, loss: 0.11501773446798325, acc: 98.4375, f1: 91.66666666666666, r: 0.7755150661485724
06/02/2019 09:16:04 step: 3183, epoch: 96, batch: 14, loss: 0.12775902450084686, acc: 96.875, f1: 92.88864103437949, r: 0.6729725941810418
06/02/2019 09:16:04 step: 3188, epoch: 96, batch: 19, loss: 0.20771348476409912, acc: 95.3125, f1: 94.81189777963971, r: 0.6600234200458008
06/02/2019 09:16:04 step: 3193, epoch: 96, batch: 24, loss: 0.1895104944705963, acc: 96.875, f1: 97.6951581027668, r: 0.702615356629065
06/02/2019 09:16:04 step: 3198, epoch: 96, batch: 29, loss: 0.16923820972442627, acc: 95.3125, f1: 92.5686136523575, r: 0.7007885322841338
06/02/2019 09:16:04 *** evaluating ***
06/02/2019 09:16:05 step: 97, epoch: 96, acc: 61.111111111111114, f1: 27.00919790965155, r: 0.4026561305982174
06/02/2019 09:16:05 *** epoch: 98 ***
06/02/2019 09:16:05 *** training ***
06/02/2019 09:16:05 step: 3206, epoch: 97, batch: 4, loss: 0.1442302167415619, acc: 95.3125, f1: 88.70247506117073, r: 0.6020321986910953
06/02/2019 09:16:05 step: 3211, epoch: 97, batch: 9, loss: 0.09939965605735779, acc: 96.875, f1: 97.54870129870129, r: 0.6757221036470558
06/02/2019 09:16:05 step: 3216, epoch: 97, batch: 14, loss: 0.0774780809879303, acc: 100.0, f1: 100.0, r: 0.5097408781684887
06/02/2019 09:16:06 step: 3221, epoch: 97, batch: 19, loss: 0.24345840513706207, acc: 92.1875, f1: 79.13149350649351, r: 0.7174405088611712
06/02/2019 09:16:06 step: 3226, epoch: 97, batch: 24, loss: 0.10755621641874313, acc: 98.4375, f1: 98.91304347826086, r: 0.7685813000872799
06/02/2019 09:16:06 step: 3231, epoch: 97, batch: 29, loss: 0.3370453119277954, acc: 89.0625, f1: 89.72902097902097, r: 0.7690239392506693
06/02/2019 09:16:06 *** evaluating ***
06/02/2019 09:16:06 step: 98, epoch: 97, acc: 60.68376068376068, f1: 26.88689271255061, r: 0.4031371675680189
06/02/2019 09:16:06 *** epoch: 99 ***
06/02/2019 09:16:06 *** training ***
06/02/2019 09:16:06 step: 3239, epoch: 98, batch: 4, loss: 0.12817953526973724, acc: 95.3125, f1: 83.04788110212638, r: 0.755923639165864
06/02/2019 09:16:07 step: 3244, epoch: 98, batch: 9, loss: 0.2507694661617279, acc: 95.3125, f1: 93.38494018296973, r: 0.6865155624995615
06/02/2019 09:16:07 step: 3249, epoch: 98, batch: 14, loss: 0.20720453560352325, acc: 93.75, f1: 78.34943639291465, r: 0.538291783675726
06/02/2019 09:16:07 step: 3254, epoch: 98, batch: 19, loss: 0.1572200506925583, acc: 93.75, f1: 90.68114425257284, r: 0.6688125453653239
06/02/2019 09:16:07 step: 3259, epoch: 98, batch: 24, loss: 0.16372957825660706, acc: 92.1875, f1: 66.58747563352827, r: 0.7124748876344007
06/02/2019 09:16:08 step: 3264, epoch: 98, batch: 29, loss: 0.20803716778755188, acc: 92.1875, f1: 85.68126921387791, r: 0.7173758784009354
06/02/2019 09:16:08 *** evaluating ***
06/02/2019 09:16:08 step: 99, epoch: 98, acc: 59.82905982905983, f1: 22.767677315568566, r: 0.4070674277214744
06/02/2019 09:16:08 *** epoch: 100 ***
06/02/2019 09:16:08 *** training ***
06/02/2019 09:16:08 step: 3272, epoch: 99, batch: 4, loss: 0.12621626257896423, acc: 95.3125, f1: 94.79249063966671, r: 0.706819502782275
06/02/2019 09:16:08 step: 3277, epoch: 99, batch: 9, loss: 0.14572960138320923, acc: 98.4375, f1: 97.23404255319149, r: 0.7727263979376948
06/02/2019 09:16:09 step: 3282, epoch: 99, batch: 14, loss: 0.19520911574363708, acc: 90.625, f1: 90.20251807016513, r: 0.8109328770313449
06/02/2019 09:16:09 step: 3287, epoch: 99, batch: 19, loss: 0.10749465972185135, acc: 95.3125, f1: 94.29815952476052, r: 0.6205408435073536
06/02/2019 09:16:09 step: 3292, epoch: 99, batch: 24, loss: 0.12461452931165695, acc: 96.875, f1: 84.42433577204467, r: 0.5390632141251374
06/02/2019 09:16:09 step: 3297, epoch: 99, batch: 29, loss: 0.14703026413917542, acc: 93.75, f1: 95.12753912753912, r: 0.6647228650376098
06/02/2019 09:16:09 *** evaluating ***
06/02/2019 09:16:09 step: 100, epoch: 99, acc: 60.68376068376068, f1: 29.881020799124247, r: 0.4082396073375561
06/02/2019 09:16:09 *** epoch: 101 ***
06/02/2019 09:16:09 *** training ***
06/02/2019 09:16:10 step: 3305, epoch: 100, batch: 4, loss: 0.193370521068573, acc: 95.3125, f1: 95.02701080432173, r: 0.6786488895371808
06/02/2019 09:16:10 step: 3310, epoch: 100, batch: 9, loss: 0.1855490356683731, acc: 93.75, f1: 91.46123396123396, r: 0.7003004387534569
06/02/2019 09:16:10 step: 3315, epoch: 100, batch: 14, loss: 0.1364297717809677, acc: 93.75, f1: 91.31350036720882, r: 0.7254649479982693
06/02/2019 09:16:10 step: 3320, epoch: 100, batch: 19, loss: 0.221915602684021, acc: 90.625, f1: 77.40053597196453, r: 0.6087192132889085
06/02/2019 09:16:10 step: 3325, epoch: 100, batch: 24, loss: 0.17836111783981323, acc: 92.1875, f1: 93.67309617309618, r: 0.6977782180097939
06/02/2019 09:16:11 step: 3330, epoch: 100, batch: 29, loss: 0.18824008107185364, acc: 93.75, f1: 96.48207475209765, r: 0.6054071125927819
06/02/2019 09:16:11 *** evaluating ***
06/02/2019 09:16:11 step: 101, epoch: 100, acc: 63.24786324786324, f1: 32.40562208383991, r: 0.40905042976019557
06/02/2019 09:16:11 *** epoch: 102 ***
06/02/2019 09:16:11 *** training ***
06/02/2019 09:16:11 step: 3338, epoch: 101, batch: 4, loss: 0.1483679711818695, acc: 95.3125, f1: 95.08658008658008, r: 0.7330035285515145
06/02/2019 09:16:11 step: 3343, epoch: 101, batch: 9, loss: 0.09464667737483978, acc: 96.875, f1: 98.18007662835248, r: 0.7824525368853597
06/02/2019 09:16:12 step: 3348, epoch: 101, batch: 14, loss: 0.10020377486944199, acc: 96.875, f1: 96.7464744811337, r: 0.6561628288748793
06/02/2019 09:16:12 step: 3353, epoch: 101, batch: 19, loss: 0.147678405046463, acc: 95.3125, f1: 93.79514847495143, r: 0.6914470371022793
06/02/2019 09:16:12 step: 3358, epoch: 101, batch: 24, loss: 0.18713685870170593, acc: 95.3125, f1: 93.13851917578626, r: 0.6650609246491455
06/02/2019 09:16:12 step: 3363, epoch: 101, batch: 29, loss: 0.16758853197097778, acc: 90.625, f1: 89.1106719367589, r: 0.6352951631717414
06/02/2019 09:16:13 *** evaluating ***
06/02/2019 09:16:13 step: 102, epoch: 101, acc: 61.111111111111114, f1: 28.828449829270177, r: 0.40686568971368053
06/02/2019 09:16:13 *** epoch: 103 ***
06/02/2019 09:16:13 *** training ***
06/02/2019 09:16:13 step: 3371, epoch: 102, batch: 4, loss: 0.13644567131996155, acc: 96.875, f1: 94.60955972583879, r: 0.7436378165912388
06/02/2019 09:16:13 step: 3376, epoch: 102, batch: 9, loss: 0.10861703753471375, acc: 96.875, f1: 95.78090397762529, r: 0.6926023567071067
06/02/2019 09:16:13 step: 3381, epoch: 102, batch: 14, loss: 0.18094471096992493, acc: 93.75, f1: 86.11393875914439, r: 0.7238543909421113
06/02/2019 09:16:14 step: 3386, epoch: 102, batch: 19, loss: 0.12154298275709152, acc: 95.3125, f1: 96.70362158167036, r: 0.6657281655370978
06/02/2019 09:16:14 step: 3391, epoch: 102, batch: 24, loss: 0.15318652987480164, acc: 93.75, f1: 90.76543084077132, r: 0.7136690567577608
06/02/2019 09:16:14 step: 3396, epoch: 102, batch: 29, loss: 0.1150437518954277, acc: 98.4375, f1: 85.71428571428572, r: 0.7573289601812296
06/02/2019 09:16:14 *** evaluating ***
06/02/2019 09:16:14 step: 103, epoch: 102, acc: 60.256410256410255, f1: 27.329796640141467, r: 0.4050647131146223
06/02/2019 09:16:14 *** epoch: 104 ***
06/02/2019 09:16:14 *** training ***
06/02/2019 09:16:15 step: 3404, epoch: 103, batch: 4, loss: 0.20007257163524628, acc: 93.75, f1: 92.06325886998155, r: 0.6989605223673795
06/02/2019 09:16:15 step: 3409, epoch: 103, batch: 9, loss: 0.2076379358768463, acc: 92.1875, f1: 89.51597744360902, r: 0.6161212907556494
06/02/2019 09:16:15 step: 3414, epoch: 103, batch: 14, loss: 0.19117097556591034, acc: 93.75, f1: 89.68253968253968, r: 0.6431548369141207
06/02/2019 09:16:15 step: 3419, epoch: 103, batch: 19, loss: 0.20460471510887146, acc: 90.625, f1: 91.14199525964233, r: 0.6458525928993307
06/02/2019 09:16:16 step: 3424, epoch: 103, batch: 24, loss: 0.11889338493347168, acc: 96.875, f1: 95.83983665616319, r: 0.6585187955243216
06/02/2019 09:16:16 step: 3429, epoch: 103, batch: 29, loss: 0.10194138437509537, acc: 96.875, f1: 97.46392496392497, r: 0.6775138117007306
06/02/2019 09:16:16 *** evaluating ***
06/02/2019 09:16:16 step: 104, epoch: 103, acc: 60.256410256410255, f1: 25.83807536007906, r: 0.40329405450885497
06/02/2019 09:16:16 *** epoch: 105 ***
06/02/2019 09:16:16 *** training ***
06/02/2019 09:16:16 step: 3437, epoch: 104, batch: 4, loss: 0.3679405748844147, acc: 87.5, f1: 84.8344413522985, r: 0.7389778927427656
06/02/2019 09:16:17 step: 3442, epoch: 104, batch: 9, loss: 0.11514817923307419, acc: 96.875, f1: 96.2012987012987, r: 0.8185674071887719
06/02/2019 09:16:17 step: 3447, epoch: 104, batch: 14, loss: 0.21908116340637207, acc: 92.1875, f1: 75.05803571428571, r: 0.642367277553059
06/02/2019 09:16:17 step: 3452, epoch: 104, batch: 19, loss: 0.17930740118026733, acc: 95.3125, f1: 79.44444444444446, r: 0.7042980435457126
06/02/2019 09:16:17 step: 3457, epoch: 104, batch: 24, loss: 0.13652902841567993, acc: 95.3125, f1: 93.96023366611601, r: 0.6243357439135315
06/02/2019 09:16:17 step: 3462, epoch: 104, batch: 29, loss: 0.2273881435394287, acc: 90.625, f1: 76.5580808080808, r: 0.6935262946326963
06/02/2019 09:16:18 *** evaluating ***
06/02/2019 09:16:18 step: 105, epoch: 104, acc: 59.401709401709404, f1: 24.018176418212157, r: 0.39516655382857513
06/02/2019 09:16:18 *** epoch: 106 ***
06/02/2019 09:16:18 *** training ***
06/02/2019 09:16:18 step: 3470, epoch: 105, batch: 4, loss: 0.16239871084690094, acc: 92.1875, f1: 90.50874908168036, r: 0.8188164989269282
06/02/2019 09:16:18 step: 3475, epoch: 105, batch: 9, loss: 0.15250837802886963, acc: 95.3125, f1: 96.04423292273236, r: 0.7799349642541056
06/02/2019 09:16:18 step: 3480, epoch: 105, batch: 14, loss: 0.1724797934293747, acc: 95.3125, f1: 91.12024756852344, r: 0.760702765549828
06/02/2019 09:16:19 step: 3485, epoch: 105, batch: 19, loss: 0.11270706355571747, acc: 96.875, f1: 96.81004574621596, r: 0.696753195745876
06/02/2019 09:16:19 step: 3490, epoch: 105, batch: 24, loss: 0.15052732825279236, acc: 95.3125, f1: 96.47451963241437, r: 0.6315110784327023
06/02/2019 09:16:19 step: 3495, epoch: 105, batch: 29, loss: 0.10697704553604126, acc: 96.875, f1: 98.1358225108225, r: 0.7703633959537624
06/02/2019 09:16:19 *** evaluating ***
06/02/2019 09:16:19 step: 106, epoch: 105, acc: 58.54700854700855, f1: 25.670289855072465, r: 0.4046757914657777
06/02/2019 09:16:19 *** epoch: 107 ***
06/02/2019 09:16:19 *** training ***
06/02/2019 09:16:20 step: 3503, epoch: 106, batch: 4, loss: 0.19930286705493927, acc: 90.625, f1: 91.87969924812029, r: 0.7183168464258385
06/02/2019 09:16:20 step: 3508, epoch: 106, batch: 9, loss: 0.11619070917367935, acc: 96.875, f1: 94.69423784839606, r: 0.719430315557049
06/02/2019 09:16:20 step: 3513, epoch: 106, batch: 14, loss: 0.16273485124111176, acc: 92.1875, f1: 90.78231292517007, r: 0.7424292720241765
06/02/2019 09:16:20 step: 3518, epoch: 106, batch: 19, loss: 0.13841572403907776, acc: 93.75, f1: 90.7218044010497, r: 0.7452661026340064
06/02/2019 09:16:20 step: 3523, epoch: 106, batch: 24, loss: 0.13222232460975647, acc: 93.75, f1: 92.11048068341196, r: 0.7609082146130377
06/02/2019 09:16:21 step: 3528, epoch: 106, batch: 29, loss: 0.16603747010231018, acc: 92.1875, f1: 92.34575722380599, r: 0.6537939885150919
06/02/2019 09:16:21 *** evaluating ***
06/02/2019 09:16:21 step: 107, epoch: 106, acc: 61.111111111111114, f1: 26.476538597166734, r: 0.4070026117884867
06/02/2019 09:16:21 *** epoch: 108 ***
06/02/2019 09:16:21 *** training ***
06/02/2019 09:16:21 step: 3536, epoch: 107, batch: 4, loss: 0.17838320136070251, acc: 95.3125, f1: 96.08526105536976, r: 0.7088991029482757
06/02/2019 09:16:21 step: 3541, epoch: 107, batch: 9, loss: 0.17489227652549744, acc: 92.1875, f1: 87.37253694581281, r: 0.7851050207073728
06/02/2019 09:16:22 step: 3546, epoch: 107, batch: 14, loss: 0.14259660243988037, acc: 95.3125, f1: 92.09201651062116, r: 0.6900020129371996
06/02/2019 09:16:22 step: 3551, epoch: 107, batch: 19, loss: 0.110541433095932, acc: 96.875, f1: 97.26731601731602, r: 0.7885809548133047
06/02/2019 09:16:22 step: 3556, epoch: 107, batch: 24, loss: 0.17171622812747955, acc: 96.875, f1: 83.38894809483045, r: 0.6327827818826802
06/02/2019 09:16:22 step: 3561, epoch: 107, batch: 29, loss: 0.1933709681034088, acc: 92.1875, f1: 90.35594261628096, r: 0.806037850138043
06/02/2019 09:16:22 *** evaluating ***
06/02/2019 09:16:22 step: 108, epoch: 107, acc: 60.68376068376068, f1: 27.121751378515413, r: 0.405877107021124
06/02/2019 09:16:22 *** epoch: 109 ***
06/02/2019 09:16:22 *** training ***
06/02/2019 09:16:23 step: 3569, epoch: 108, batch: 4, loss: 0.09686176478862762, acc: 96.875, f1: 95.61268763306117, r: 0.7497435086414259
06/02/2019 09:16:23 step: 3574, epoch: 108, batch: 9, loss: 0.10905536264181137, acc: 96.875, f1: 84.4625850340136, r: 0.6438843311747042
06/02/2019 09:16:23 step: 3579, epoch: 108, batch: 14, loss: 0.1841559112071991, acc: 96.875, f1: 94.73922902494331, r: 0.6902886188597457
06/02/2019 09:16:23 step: 3584, epoch: 108, batch: 19, loss: 0.21187974512577057, acc: 92.1875, f1: 90.54288948176894, r: 0.7418909680792202
06/02/2019 09:16:24 step: 3589, epoch: 108, batch: 24, loss: 0.09610114246606827, acc: 98.4375, f1: 97.57236227824464, r: 0.6582095436217249
06/02/2019 09:16:24 step: 3594, epoch: 108, batch: 29, loss: 0.07865297049283981, acc: 98.4375, f1: 99.13702623906705, r: 0.6621238181440244
06/02/2019 09:16:24 *** evaluating ***
06/02/2019 09:16:24 step: 109, epoch: 108, acc: 60.256410256410255, f1: 24.74012549155679, r: 0.40582677877469386
06/02/2019 09:16:24 *** epoch: 110 ***
06/02/2019 09:16:24 *** training ***
06/02/2019 09:16:24 step: 3602, epoch: 109, batch: 4, loss: 0.17637476325035095, acc: 93.75, f1: 90.0415739418297, r: 0.8087653969954977
06/02/2019 09:16:25 step: 3607, epoch: 109, batch: 9, loss: 0.14647048711776733, acc: 96.875, f1: 98.61021331609567, r: 0.7277137070151922
06/02/2019 09:16:25 step: 3612, epoch: 109, batch: 14, loss: 0.10869333893060684, acc: 96.875, f1: 98.30357142857142, r: 0.8025058946624044
06/02/2019 09:16:25 step: 3617, epoch: 109, batch: 19, loss: 0.2209104597568512, acc: 93.75, f1: 88.7699691955011, r: 0.5043401481742751
06/02/2019 09:16:25 step: 3622, epoch: 109, batch: 24, loss: 0.057034630328416824, acc: 98.4375, f1: 93.33333333333333, r: 0.773103882822297
06/02/2019 09:16:26 step: 3627, epoch: 109, batch: 29, loss: 0.2516617774963379, acc: 93.75, f1: 95.09341251943468, r: 0.6256272240691724
06/02/2019 09:16:26 *** evaluating ***
06/02/2019 09:16:26 step: 110, epoch: 109, acc: 60.256410256410255, f1: 26.744851102555934, r: 0.40515646380995585
06/02/2019 09:16:26 *** epoch: 111 ***
06/02/2019 09:16:26 *** training ***
06/02/2019 09:16:26 step: 3635, epoch: 110, batch: 4, loss: 0.13665816187858582, acc: 98.4375, f1: 97.97843665768194, r: 0.791260138306451
06/02/2019 09:16:26 step: 3640, epoch: 110, batch: 9, loss: 0.2049199938774109, acc: 89.0625, f1: 84.48218448218448, r: 0.7131589675641957
06/02/2019 09:16:26 step: 3645, epoch: 110, batch: 14, loss: 0.21909259259700775, acc: 93.75, f1: 95.9142776727916, r: 0.7204502307426205
06/02/2019 09:16:27 step: 3650, epoch: 110, batch: 19, loss: 0.1692650318145752, acc: 93.75, f1: 68.0981240981241, r: 0.6670767083801303
06/02/2019 09:16:27 step: 3655, epoch: 110, batch: 24, loss: 0.2130710780620575, acc: 93.75, f1: 91.93740380636191, r: 0.6890880520945593
06/02/2019 09:16:27 step: 3660, epoch: 110, batch: 29, loss: 0.2154916524887085, acc: 92.1875, f1: 87.8283166109253, r: 0.7181154125123393
06/02/2019 09:16:27 *** evaluating ***
06/02/2019 09:16:27 step: 111, epoch: 110, acc: 59.82905982905983, f1: 24.700300629326406, r: 0.4041878616022783
06/02/2019 09:16:27 *** epoch: 112 ***
06/02/2019 09:16:27 *** training ***
06/02/2019 09:16:28 step: 3668, epoch: 111, batch: 4, loss: 0.38479822874069214, acc: 85.9375, f1: 72.41071428571428, r: 0.6133443989704105
06/02/2019 09:16:28 step: 3673, epoch: 111, batch: 9, loss: 0.19102907180786133, acc: 92.1875, f1: 89.39935064935065, r: 0.7352206261939037
06/02/2019 09:16:28 step: 3678, epoch: 111, batch: 14, loss: 0.18993772566318512, acc: 90.625, f1: 83.03199703896567, r: 0.559573235059539
06/02/2019 09:16:28 step: 3683, epoch: 111, batch: 19, loss: 0.11235663294792175, acc: 96.875, f1: 96.4265010351967, r: 0.6643385498496325
06/02/2019 09:16:28 step: 3688, epoch: 111, batch: 24, loss: 0.07536168396472931, acc: 100.0, f1: 100.0, r: 0.7308654416170549
06/02/2019 09:16:29 step: 3693, epoch: 111, batch: 29, loss: 0.21564434468746185, acc: 95.3125, f1: 90.36075036075036, r: 0.6898091414044908
06/02/2019 09:16:29 *** evaluating ***
06/02/2019 09:16:29 step: 112, epoch: 111, acc: 60.68376068376068, f1: 25.16232585656556, r: 0.4094465415290621
06/02/2019 09:16:29 *** epoch: 113 ***
06/02/2019 09:16:29 *** training ***
06/02/2019 09:16:29 step: 3701, epoch: 112, batch: 4, loss: 0.10867173969745636, acc: 95.3125, f1: 96.08385093167702, r: 0.6700529802401023
06/02/2019 09:16:29 step: 3706, epoch: 112, batch: 9, loss: 0.15080338716506958, acc: 95.3125, f1: 92.72113943028486, r: 0.6984629847320267
06/02/2019 09:16:29 step: 3711, epoch: 112, batch: 14, loss: 0.1534457504749298, acc: 93.75, f1: 91.71382100486579, r: 0.6848141616886295
06/02/2019 09:16:30 step: 3716, epoch: 112, batch: 19, loss: 0.19629597663879395, acc: 95.3125, f1: 93.01738313366221, r: 0.8348376221729551
06/02/2019 09:16:30 step: 3721, epoch: 112, batch: 24, loss: 0.05862855538725853, acc: 96.875, f1: 93.62193362193364, r: 0.7030288726596379
06/02/2019 09:16:30 step: 3726, epoch: 112, batch: 29, loss: 0.23705148696899414, acc: 93.75, f1: 79.39279674573793, r: 0.630828980808385
06/02/2019 09:16:30 *** evaluating ***
06/02/2019 09:16:30 step: 113, epoch: 112, acc: 59.82905982905983, f1: 23.685690239716017, r: 0.4063708819191859
06/02/2019 09:16:30 *** epoch: 114 ***
06/02/2019 09:16:30 *** training ***
06/02/2019 09:16:30 step: 3734, epoch: 113, batch: 4, loss: 0.13326707482337952, acc: 96.875, f1: 93.83979885057472, r: 0.724908266353981
06/02/2019 09:16:31 step: 3739, epoch: 113, batch: 9, loss: 0.09983374178409576, acc: 96.875, f1: 97.41865444595474, r: 0.7538920035223782
06/02/2019 09:16:31 step: 3744, epoch: 113, batch: 14, loss: 0.11905668675899506, acc: 98.4375, f1: 99.39620471535365, r: 0.8015480572996974
06/02/2019 09:16:31 step: 3749, epoch: 113, batch: 19, loss: 0.06747191399335861, acc: 100.0, f1: 100.0, r: 0.6718952820482036
06/02/2019 09:16:31 step: 3754, epoch: 113, batch: 24, loss: 0.09979823231697083, acc: 96.875, f1: 94.99299719887955, r: 0.7576240193445406
06/02/2019 09:16:31 step: 3759, epoch: 113, batch: 29, loss: 0.17966875433921814, acc: 90.625, f1: 89.31723650416934, r: 0.7897863937842508
06/02/2019 09:16:32 *** evaluating ***
06/02/2019 09:16:32 step: 114, epoch: 113, acc: 60.68376068376068, f1: 26.32402379930805, r: 0.4110580409035284
06/02/2019 09:16:32 *** epoch: 115 ***
06/02/2019 09:16:32 *** training ***
06/02/2019 09:16:32 step: 3767, epoch: 114, batch: 4, loss: 0.10625741630792618, acc: 95.3125, f1: 95.0429292929293, r: 0.7585066287751189
06/02/2019 09:16:32 step: 3772, epoch: 114, batch: 9, loss: 0.17924155294895172, acc: 93.75, f1: 88.53993959257117, r: 0.6052945844899114
06/02/2019 09:16:32 step: 3777, epoch: 114, batch: 14, loss: 0.1939333826303482, acc: 90.625, f1: 67.57575757575758, r: 0.640901005666787
06/02/2019 09:16:33 step: 3782, epoch: 114, batch: 19, loss: 0.16346406936645508, acc: 95.3125, f1: 95.54652961980548, r: 0.7294320207323526
06/02/2019 09:16:33 step: 3787, epoch: 114, batch: 24, loss: 0.20634326338768005, acc: 92.1875, f1: 87.96333874458875, r: 0.7165783475659239
06/02/2019 09:16:33 step: 3792, epoch: 114, batch: 29, loss: 0.08731439709663391, acc: 96.875, f1: 92.21864184270201, r: 0.6949872129815202
06/02/2019 09:16:33 *** evaluating ***
06/02/2019 09:16:33 step: 115, epoch: 114, acc: 61.53846153846154, f1: 28.35247282378177, r: 0.4080599599320991
06/02/2019 09:16:33 *** epoch: 116 ***
06/02/2019 09:16:33 *** training ***
06/02/2019 09:16:34 step: 3800, epoch: 115, batch: 4, loss: 0.13107945024967194, acc: 95.3125, f1: 95.34793878982168, r: 0.7944461678952519
06/02/2019 09:16:34 step: 3805, epoch: 115, batch: 9, loss: 0.1408369243144989, acc: 95.3125, f1: 94.0937019969278, r: 0.6355028306276116
06/02/2019 09:16:34 step: 3810, epoch: 115, batch: 14, loss: 0.14101852476596832, acc: 93.75, f1: 89.43945293001897, r: 0.7260583267848054
06/02/2019 09:16:34 step: 3815, epoch: 115, batch: 19, loss: 0.05460888147354126, acc: 100.0, f1: 100.0, r: 0.7181229828544583
06/02/2019 09:16:34 step: 3820, epoch: 115, batch: 24, loss: 0.1767212450504303, acc: 93.75, f1: 90.31723484848484, r: 0.7186283325806715
06/02/2019 09:16:35 step: 3825, epoch: 115, batch: 29, loss: 0.10784634947776794, acc: 95.3125, f1: 95.18098228302311, r: 0.6756136343534745
06/02/2019 09:16:35 *** evaluating ***
06/02/2019 09:16:35 step: 116, epoch: 115, acc: 63.24786324786324, f1: 31.056528729796057, r: 0.41035069456009915
06/02/2019 09:16:35 *** epoch: 117 ***
06/02/2019 09:16:35 *** training ***
06/02/2019 09:16:35 step: 3833, epoch: 116, batch: 4, loss: 0.1741686910390854, acc: 92.1875, f1: 88.72293239640179, r: 0.638929396483108
06/02/2019 09:16:36 step: 3838, epoch: 116, batch: 9, loss: 0.19218388199806213, acc: 92.1875, f1: 87.60272657450076, r: 0.7807267660853849
06/02/2019 09:16:36 step: 3843, epoch: 116, batch: 14, loss: 0.0693029910326004, acc: 98.4375, f1: 97.67195767195767, r: 0.6516005872720179
06/02/2019 09:16:36 step: 3848, epoch: 116, batch: 19, loss: 0.29801517724990845, acc: 90.625, f1: 89.55046856140831, r: 0.8095135387903529
06/02/2019 09:16:37 step: 3853, epoch: 116, batch: 24, loss: 0.1974553018808365, acc: 95.3125, f1: 79.74923302654395, r: 0.5381278710279839
06/02/2019 09:16:37 step: 3858, epoch: 116, batch: 29, loss: 0.21974973380565643, acc: 92.1875, f1: 78.4006734006734, r: 0.6944026544180192
06/02/2019 09:16:37 *** evaluating ***
06/02/2019 09:16:37 step: 117, epoch: 116, acc: 60.256410256410255, f1: 25.96626578482082, r: 0.40887962248618603
06/02/2019 09:16:37 *** epoch: 118 ***
06/02/2019 09:16:37 *** training ***
06/02/2019 09:16:37 step: 3866, epoch: 117, batch: 4, loss: 0.16720888018608093, acc: 95.3125, f1: 94.14578477078477, r: 0.7854492780277847
06/02/2019 09:16:38 step: 3871, epoch: 117, batch: 9, loss: 0.12043577432632446, acc: 96.875, f1: 95.65797610910394, r: 0.6604549522976895
06/02/2019 09:16:38 step: 3876, epoch: 117, batch: 14, loss: 0.2639025151729584, acc: 92.1875, f1: 81.20915032679738, r: 0.7891996370996923
06/02/2019 09:16:38 step: 3881, epoch: 117, batch: 19, loss: 0.08606326580047607, acc: 98.4375, f1: 97.64172335600907, r: 0.8008606337042695
06/02/2019 09:16:39 step: 3886, epoch: 117, batch: 24, loss: 0.06000018119812012, acc: 100.0, f1: 100.0, r: 0.8103683761049991
06/02/2019 09:16:39 step: 3891, epoch: 117, batch: 29, loss: 0.13158385455608368, acc: 96.875, f1: 95.22144522144522, r: 0.7565909766476078
06/02/2019 09:16:39 *** evaluating ***
06/02/2019 09:16:39 step: 118, epoch: 117, acc: 59.82905982905983, f1: 24.828099197308042, r: 0.4056966515972242
06/02/2019 09:16:39 *** epoch: 119 ***
06/02/2019 09:16:39 *** training ***
06/02/2019 09:16:40 step: 3899, epoch: 118, batch: 4, loss: 0.27236199378967285, acc: 92.1875, f1: 89.14588057445201, r: 0.6514716911652068
06/02/2019 09:16:40 step: 3904, epoch: 118, batch: 9, loss: 0.13239550590515137, acc: 96.875, f1: 97.99744441604908, r: 0.732393223652798
06/02/2019 09:16:40 step: 3909, epoch: 118, batch: 14, loss: 0.2452201247215271, acc: 89.0625, f1: 88.42731829573935, r: 0.6827557485183854
06/02/2019 09:16:41 step: 3914, epoch: 118, batch: 19, loss: 0.14662891626358032, acc: 96.875, f1: 93.70431402374615, r: 0.6388540756697769
06/02/2019 09:16:41 step: 3919, epoch: 118, batch: 24, loss: 0.19664087891578674, acc: 92.1875, f1: 87.48682476943347, r: 0.6959676175541163
06/02/2019 09:16:41 step: 3924, epoch: 118, batch: 29, loss: 0.11135799437761307, acc: 95.3125, f1: 97.04732957012592, r: 0.7118523694847908
06/02/2019 09:16:42 *** evaluating ***
06/02/2019 09:16:42 step: 119, epoch: 118, acc: 61.111111111111114, f1: 26.350829099543006, r: 0.4058805585591325
06/02/2019 09:16:42 *** epoch: 120 ***
06/02/2019 09:16:42 *** training ***
06/02/2019 09:16:42 step: 3932, epoch: 119, batch: 4, loss: 0.14152994751930237, acc: 96.875, f1: 96.15966386554622, r: 0.7563291872265231
06/02/2019 09:16:42 step: 3937, epoch: 119, batch: 9, loss: 0.12732768058776855, acc: 95.3125, f1: 92.50871080139372, r: 0.6897477670862411
06/02/2019 09:16:43 step: 3942, epoch: 119, batch: 14, loss: 0.1046556904911995, acc: 96.875, f1: 96.01018176869572, r: 0.6877820497983409
06/02/2019 09:16:43 step: 3947, epoch: 119, batch: 19, loss: 0.05160490795969963, acc: 96.875, f1: 97.98643945363801, r: 0.6948949131436951
06/02/2019 09:16:43 step: 3952, epoch: 119, batch: 24, loss: 0.05724899843335152, acc: 100.0, f1: 100.0, r: 0.7350177145217186
06/02/2019 09:16:44 step: 3957, epoch: 119, batch: 29, loss: 0.1201637014746666, acc: 95.3125, f1: 93.28730418868277, r: 0.6436879511702596
06/02/2019 09:16:44 *** evaluating ***
06/02/2019 09:16:44 step: 120, epoch: 119, acc: 61.965811965811966, f1: 29.61309589054821, r: 0.4116826815203422
06/02/2019 09:16:44 *** epoch: 121 ***
06/02/2019 09:16:44 *** training ***
06/02/2019 09:16:44 step: 3965, epoch: 120, batch: 4, loss: 0.11240781843662262, acc: 95.3125, f1: 95.44650663202857, r: 0.669586286621848
06/02/2019 09:16:45 step: 3970, epoch: 120, batch: 9, loss: 0.14590832591056824, acc: 95.3125, f1: 94.97354497354497, r: 0.7861460391600071
06/02/2019 09:16:45 step: 3975, epoch: 120, batch: 14, loss: 0.0752490758895874, acc: 98.4375, f1: 99.06227106227107, r: 0.7276509866745569
06/02/2019 09:16:45 step: 3980, epoch: 120, batch: 19, loss: 0.1220891922712326, acc: 95.3125, f1: 94.82142857142857, r: 0.7596574540389899
06/02/2019 09:16:45 step: 3985, epoch: 120, batch: 24, loss: 0.1851755678653717, acc: 96.875, f1: 96.7000261643119, r: 0.6925374666881787
06/02/2019 09:16:46 step: 3990, epoch: 120, batch: 29, loss: 0.04614053666591644, acc: 98.4375, f1: 85.2216748768473, r: 0.6449380330756687
06/02/2019 09:16:46 *** evaluating ***
06/02/2019 09:16:46 step: 121, epoch: 120, acc: 61.965811965811966, f1: 30.299371740161213, r: 0.40873658755183956
06/02/2019 09:16:46 *** epoch: 122 ***
06/02/2019 09:16:46 *** training ***
06/02/2019 09:16:46 step: 3998, epoch: 121, batch: 4, loss: 0.09567193686962128, acc: 98.4375, f1: 94.70899470899471, r: 0.705478226764395
06/02/2019 09:16:47 step: 4003, epoch: 121, batch: 9, loss: 0.17969226837158203, acc: 92.1875, f1: 88.96461729345828, r: 0.6603670036346869
06/02/2019 09:16:47 step: 4008, epoch: 121, batch: 14, loss: 0.15340451896190643, acc: 95.3125, f1: 95.51190476190476, r: 0.8311617246783315
06/02/2019 09:16:47 step: 4013, epoch: 121, batch: 19, loss: 0.07831912487745285, acc: 98.4375, f1: 97.67080745341615, r: 0.7389279314772371
06/02/2019 09:16:48 step: 4018, epoch: 121, batch: 24, loss: 0.11522281169891357, acc: 98.4375, f1: 98.88888888888889, r: 0.7907552970928132
06/02/2019 09:16:48 step: 4023, epoch: 121, batch: 29, loss: 0.07769546657800674, acc: 98.4375, f1: 97.73940345368918, r: 0.6627053376880201
06/02/2019 09:16:48 *** evaluating ***
06/02/2019 09:16:48 step: 122, epoch: 121, acc: 62.82051282051282, f1: 32.22132071219293, r: 0.40739296394418756
06/02/2019 09:16:48 *** epoch: 123 ***
06/02/2019 09:16:48 *** training ***
06/02/2019 09:16:49 step: 4031, epoch: 122, batch: 4, loss: 0.11107446253299713, acc: 98.4375, f1: 99.04665607395637, r: 0.6509796002993451
06/02/2019 09:16:49 step: 4036, epoch: 122, batch: 9, loss: 0.07096332311630249, acc: 96.875, f1: 92.7046680741262, r: 0.6586189512918086
06/02/2019 09:16:49 step: 4041, epoch: 122, batch: 14, loss: 0.1158180981874466, acc: 96.875, f1: 97.61883181623959, r: 0.7156737513530153
06/02/2019 09:16:50 step: 4046, epoch: 122, batch: 19, loss: 0.15728086233139038, acc: 95.3125, f1: 92.42604763185876, r: 0.6238746775872314
06/02/2019 09:16:50 step: 4051, epoch: 122, batch: 24, loss: 0.13279564678668976, acc: 92.1875, f1: 78.914819071423, r: 0.5970545635439121
06/02/2019 09:16:50 step: 4056, epoch: 122, batch: 29, loss: 0.16112899780273438, acc: 96.875, f1: 97.03172533681008, r: 0.8101685452050376
06/02/2019 09:16:50 *** evaluating ***
06/02/2019 09:16:51 step: 123, epoch: 122, acc: 60.256410256410255, f1: 28.89285222632446, r: 0.40798410384360057
06/02/2019 09:16:51 *** epoch: 124 ***
06/02/2019 09:16:51 *** training ***
06/02/2019 09:16:51 step: 4064, epoch: 123, batch: 4, loss: 0.19825626909732819, acc: 95.3125, f1: 87.28515560813698, r: 0.6061098128979542
06/02/2019 09:16:51 step: 4069, epoch: 123, batch: 9, loss: 0.2952975034713745, acc: 93.75, f1: 85.86669283097854, r: 0.6768196360035856
06/02/2019 09:16:52 step: 4074, epoch: 123, batch: 14, loss: 0.0774499922990799, acc: 95.3125, f1: 80.2554588268874, r: 0.6662001052871378
06/02/2019 09:16:52 step: 4079, epoch: 123, batch: 19, loss: 0.047511253505945206, acc: 100.0, f1: 100.0, r: 0.6518646272257885
06/02/2019 09:16:52 step: 4084, epoch: 123, batch: 24, loss: 0.05955381318926811, acc: 98.4375, f1: 99.0204081632653, r: 0.6551003171982717
06/02/2019 09:16:53 step: 4089, epoch: 123, batch: 29, loss: 0.19380509853363037, acc: 95.3125, f1: 94.95538087643351, r: 0.7279731612402168
06/02/2019 09:16:53 *** evaluating ***
06/02/2019 09:16:53 step: 124, epoch: 123, acc: 60.256410256410255, f1: 24.20245439241782, r: 0.41331765157603384
06/02/2019 09:16:53 *** epoch: 125 ***
06/02/2019 09:16:53 *** training ***
06/02/2019 09:16:53 step: 4097, epoch: 124, batch: 4, loss: 0.08282013982534409, acc: 96.875, f1: 98.4265010351967, r: 0.6698261813933947
06/02/2019 09:16:53 step: 4102, epoch: 124, batch: 9, loss: 0.16345052421092987, acc: 95.3125, f1: 96.13121801060089, r: 0.7798865920443848
06/02/2019 09:16:54 step: 4107, epoch: 124, batch: 14, loss: 0.10312437266111374, acc: 96.875, f1: 92.95031055900621, r: 0.6530449384108947
06/02/2019 09:16:54 step: 4112, epoch: 124, batch: 19, loss: 0.18960008025169373, acc: 93.75, f1: 93.0605301131617, r: 0.6338137445392924
06/02/2019 09:16:55 step: 4117, epoch: 124, batch: 24, loss: 0.06189814954996109, acc: 98.4375, f1: 98.63636363636363, r: 0.7928324184881009
06/02/2019 09:16:55 step: 4122, epoch: 124, batch: 29, loss: 0.09537168592214584, acc: 95.3125, f1: 80.90097402597402, r: 0.7088933015475115
06/02/2019 09:16:55 *** evaluating ***
06/02/2019 09:16:55 step: 125, epoch: 124, acc: 61.965811965811966, f1: 31.450685136173483, r: 0.4137447627715311
06/02/2019 09:16:55 *** epoch: 126 ***
06/02/2019 09:16:55 *** training ***
06/02/2019 09:16:55 step: 4130, epoch: 125, batch: 4, loss: 0.11611627042293549, acc: 96.875, f1: 97.51587301587301, r: 0.780200898496994
06/02/2019 09:16:56 step: 4135, epoch: 125, batch: 9, loss: 0.1083240732550621, acc: 96.875, f1: 96.8609963494874, r: 0.7502268990456606
06/02/2019 09:16:56 step: 4140, epoch: 125, batch: 14, loss: 0.16757863759994507, acc: 95.3125, f1: 95.39599686028257, r: 0.6955712576064585
06/02/2019 09:16:56 step: 4145, epoch: 125, batch: 19, loss: 0.21280962228775024, acc: 95.3125, f1: 95.498289372034, r: 0.8229430806324282
06/02/2019 09:16:57 step: 4150, epoch: 125, batch: 24, loss: 0.10953336954116821, acc: 96.875, f1: 84.33311646063761, r: 0.707236044319537
06/02/2019 09:16:57 step: 4155, epoch: 125, batch: 29, loss: 0.11521424353122711, acc: 96.875, f1: 95.6890331890332, r: 0.7403451179225112
06/02/2019 09:16:57 *** evaluating ***
06/02/2019 09:16:57 step: 126, epoch: 125, acc: 61.111111111111114, f1: 29.710447253405476, r: 0.40773982458077773
06/02/2019 09:16:57 *** epoch: 127 ***
06/02/2019 09:16:57 *** training ***
06/02/2019 09:16:58 step: 4163, epoch: 126, batch: 4, loss: 0.10812567919492722, acc: 98.4375, f1: 99.05491698595146, r: 0.6375553190620887
06/02/2019 09:16:58 step: 4168, epoch: 126, batch: 9, loss: 0.14545631408691406, acc: 93.75, f1: 94.3452380952381, r: 0.6308048618304594
06/02/2019 09:16:58 step: 4173, epoch: 126, batch: 14, loss: 0.06052236631512642, acc: 98.4375, f1: 96.4625850340136, r: 0.665208067928478
06/02/2019 09:16:59 step: 4178, epoch: 126, batch: 19, loss: 0.08893945813179016, acc: 96.875, f1: 93.65928189457603, r: 0.6940700182517051
06/02/2019 09:16:59 step: 4183, epoch: 126, batch: 24, loss: 0.07962854951620102, acc: 100.0, f1: 100.0, r: 0.7911760236028569
06/02/2019 09:17:00 step: 4188, epoch: 126, batch: 29, loss: 0.12862448394298553, acc: 95.3125, f1: 93.52681652371093, r: 0.8021839770507256
06/02/2019 09:17:00 *** evaluating ***
06/02/2019 09:17:00 step: 127, epoch: 126, acc: 59.82905982905983, f1: 27.919609630135945, r: 0.4097452620237225
06/02/2019 09:17:00 *** epoch: 128 ***
06/02/2019 09:17:00 *** training ***
06/02/2019 09:17:00 step: 4196, epoch: 127, batch: 4, loss: 0.04153246805071831, acc: 100.0, f1: 100.0, r: 0.7941305413194665
06/02/2019 09:17:00 step: 4201, epoch: 127, batch: 9, loss: 0.1647224724292755, acc: 93.75, f1: 94.33106575963718, r: 0.6924483536503697
06/02/2019 09:17:01 step: 4206, epoch: 127, batch: 14, loss: 0.11961309611797333, acc: 95.3125, f1: 94.66986794717887, r: 0.6819991282648246
06/02/2019 09:17:01 step: 4211, epoch: 127, batch: 19, loss: 0.1893632709980011, acc: 92.1875, f1: 82.5925925925926, r: 0.6910508651652999
06/02/2019 09:17:01 step: 4216, epoch: 127, batch: 24, loss: 0.1334773600101471, acc: 95.3125, f1: 93.86422891726653, r: 0.5570774399611136
06/02/2019 09:17:02 step: 4221, epoch: 127, batch: 29, loss: 0.08493922650814056, acc: 96.875, f1: 95.95942724916054, r: 0.6329295995056137
06/02/2019 09:17:02 *** evaluating ***
06/02/2019 09:17:02 step: 128, epoch: 127, acc: 61.965811965811966, f1: 29.410951630293113, r: 0.41358037169899425
06/02/2019 09:17:02 *** epoch: 129 ***
06/02/2019 09:17:02 *** training ***
06/02/2019 09:17:02 step: 4229, epoch: 128, batch: 4, loss: 0.12969441711902618, acc: 95.3125, f1: 97.26150075414782, r: 0.7717731678900952
06/02/2019 09:17:03 step: 4234, epoch: 128, batch: 9, loss: 0.15442994236946106, acc: 93.75, f1: 90.54071325499898, r: 0.6433800892560881
06/02/2019 09:17:03 step: 4239, epoch: 128, batch: 14, loss: 0.04433264210820198, acc: 100.0, f1: 100.0, r: 0.7111383371170664
06/02/2019 09:17:04 step: 4244, epoch: 128, batch: 19, loss: 0.11711668223142624, acc: 95.3125, f1: 97.08139083139082, r: 0.7920432052617948
06/02/2019 09:17:04 step: 4249, epoch: 128, batch: 24, loss: 0.1069733127951622, acc: 95.3125, f1: 82.71586223623576, r: 0.6714840090814492
06/02/2019 09:17:04 step: 4254, epoch: 128, batch: 29, loss: 0.21922701597213745, acc: 90.625, f1: 92.51048362907068, r: 0.6800755431112125
06/02/2019 09:17:04 *** evaluating ***
06/02/2019 09:17:05 step: 129, epoch: 128, acc: 60.68376068376068, f1: 29.036238066298814, r: 0.4108070757083643
06/02/2019 09:17:05 *** epoch: 130 ***
06/02/2019 09:17:05 *** training ***
06/02/2019 09:17:05 step: 4262, epoch: 129, batch: 4, loss: 0.10429185628890991, acc: 96.875, f1: 97.68279965648388, r: 0.7955402107690699
06/02/2019 09:17:05 step: 4267, epoch: 129, batch: 9, loss: 0.09356161952018738, acc: 96.875, f1: 90.95860566448802, r: 0.7794448179221083
06/02/2019 09:17:05 step: 4272, epoch: 129, batch: 14, loss: 0.07794228196144104, acc: 98.4375, f1: 99.2158439730572, r: 0.7106200081059452
06/02/2019 09:17:06 step: 4277, epoch: 129, batch: 19, loss: 0.053760092705488205, acc: 100.0, f1: 100.0, r: 0.7353341080845517
06/02/2019 09:17:06 step: 4282, epoch: 129, batch: 24, loss: 0.21276900172233582, acc: 89.0625, f1: 87.90494148633682, r: 0.6926318502149947
06/02/2019 09:17:06 step: 4287, epoch: 129, batch: 29, loss: 0.08007949590682983, acc: 98.4375, f1: 99.30118798043326, r: 0.7469761042006132
06/02/2019 09:17:07 *** evaluating ***
06/02/2019 09:17:07 step: 130, epoch: 129, acc: 60.256410256410255, f1: 25.499182227068285, r: 0.4095962985026512
06/02/2019 09:17:07 *** epoch: 131 ***
06/02/2019 09:17:07 *** training ***
06/02/2019 09:17:07 step: 4295, epoch: 130, batch: 4, loss: 0.15275046229362488, acc: 95.3125, f1: 96.08178935910028, r: 0.7391505016307497
06/02/2019 09:17:07 step: 4300, epoch: 130, batch: 9, loss: 0.10744066536426544, acc: 96.875, f1: 97.82118252706489, r: 0.6933010097304444
06/02/2019 09:17:08 step: 4305, epoch: 130, batch: 14, loss: 0.1570301055908203, acc: 93.75, f1: 94.20995670995671, r: 0.674668002688396
06/02/2019 09:17:08 step: 4310, epoch: 130, batch: 19, loss: 0.11626601964235306, acc: 96.875, f1: 95.94155844155846, r: 0.8032457778101628
06/02/2019 09:17:08 step: 4315, epoch: 130, batch: 24, loss: 0.11224480718374252, acc: 95.3125, f1: 89.09634551495016, r: 0.6619457272530034
06/02/2019 09:17:09 step: 4320, epoch: 130, batch: 29, loss: 0.12969505786895752, acc: 96.875, f1: 94.57792207792207, r: 0.7746311924671412
06/02/2019 09:17:09 *** evaluating ***
06/02/2019 09:17:09 step: 131, epoch: 130, acc: 61.111111111111114, f1: 28.2945582261234, r: 0.41622300716929456
06/02/2019 09:17:09 *** epoch: 132 ***
06/02/2019 09:17:09 *** training ***
06/02/2019 09:17:09 step: 4328, epoch: 131, batch: 4, loss: 0.10612423717975616, acc: 95.3125, f1: 93.66582491582491, r: 0.7659752794056731
06/02/2019 09:17:10 step: 4333, epoch: 131, batch: 9, loss: 0.17323219776153564, acc: 93.75, f1: 91.70843776106935, r: 0.6792222022477531
06/02/2019 09:17:10 step: 4338, epoch: 131, batch: 14, loss: 0.06032436341047287, acc: 100.0, f1: 100.0, r: 0.6783749617463523
06/02/2019 09:17:10 step: 4343, epoch: 131, batch: 19, loss: 0.07608900219202042, acc: 98.4375, f1: 98.9875019775352, r: 0.6926487138046001
06/02/2019 09:17:11 step: 4348, epoch: 131, batch: 24, loss: 0.11098662763834, acc: 93.75, f1: 94.0141369047619, r: 0.7621679314792381
06/02/2019 09:17:11 step: 4353, epoch: 131, batch: 29, loss: 0.10540825873613358, acc: 95.3125, f1: 92.2608543417367, r: 0.7308047621657896
06/02/2019 09:17:11 *** evaluating ***
06/02/2019 09:17:12 step: 132, epoch: 131, acc: 61.111111111111114, f1: 31.236995371172615, r: 0.41723897382139086
06/02/2019 09:17:12 *** epoch: 133 ***
06/02/2019 09:17:12 *** training ***
06/02/2019 09:17:12 step: 4361, epoch: 132, batch: 4, loss: 0.10986921936273575, acc: 96.875, f1: 95.77327327327328, r: 0.7358570587106639
06/02/2019 09:17:12 step: 4366, epoch: 132, batch: 9, loss: 0.07649057358503342, acc: 98.4375, f1: 97.64957264957265, r: 0.8060626003648048
06/02/2019 09:17:13 step: 4371, epoch: 132, batch: 14, loss: 0.07485179603099823, acc: 96.875, f1: 96.51178451178451, r: 0.6269567254909614
06/02/2019 09:17:13 step: 4376, epoch: 132, batch: 19, loss: 0.19531840085983276, acc: 95.3125, f1: 81.52145473574045, r: 0.6948592956762503
06/02/2019 09:17:13 step: 4381, epoch: 132, batch: 24, loss: 0.1325182318687439, acc: 95.3125, f1: 78.2905982905983, r: 0.7311589020285676
06/02/2019 09:17:14 step: 4386, epoch: 132, batch: 29, loss: 0.10871016979217529, acc: 96.875, f1: 73.44322344322343, r: 0.6754970860394821
06/02/2019 09:17:14 *** evaluating ***
06/02/2019 09:17:14 step: 133, epoch: 132, acc: 61.965811965811966, f1: 30.237681332074818, r: 0.41454459044834285
06/02/2019 09:17:14 *** epoch: 134 ***
06/02/2019 09:17:14 *** training ***
06/02/2019 09:17:14 step: 4394, epoch: 133, batch: 4, loss: 0.15154950320720673, acc: 95.3125, f1: 93.2945008332071, r: 0.7506448440907919
06/02/2019 09:17:15 step: 4399, epoch: 133, batch: 9, loss: 0.05880985036492348, acc: 98.4375, f1: 99.15902964959568, r: 0.721133505043762
06/02/2019 09:17:15 step: 4404, epoch: 133, batch: 14, loss: 0.11336352676153183, acc: 95.3125, f1: 79.58825635517364, r: 0.6297285087149479
06/02/2019 09:17:15 step: 4409, epoch: 133, batch: 19, loss: 0.03513854742050171, acc: 98.4375, f1: 97.38775510204081, r: 0.7719959554676745
06/02/2019 09:17:16 step: 4414, epoch: 133, batch: 24, loss: 0.12758997082710266, acc: 95.3125, f1: 95.1461038961039, r: 0.7392895885994247
06/02/2019 09:17:16 step: 4419, epoch: 133, batch: 29, loss: 0.09138378500938416, acc: 95.3125, f1: 95.19736842105262, r: 0.7300845355178623
06/02/2019 09:17:16 *** evaluating ***
06/02/2019 09:17:16 step: 134, epoch: 133, acc: 59.401709401709404, f1: 26.533642468580997, r: 0.411473924132936
06/02/2019 09:17:16 *** epoch: 135 ***
06/02/2019 09:17:16 *** training ***
06/02/2019 09:17:17 step: 4427, epoch: 134, batch: 4, loss: 0.15899346768856049, acc: 95.3125, f1: 96.26573426573427, r: 0.7757590975992089
06/02/2019 09:17:17 step: 4432, epoch: 134, batch: 9, loss: 0.12962856888771057, acc: 95.3125, f1: 94.44616467276566, r: 0.6887189957475023
06/02/2019 09:17:17 step: 4437, epoch: 134, batch: 14, loss: 0.09826231002807617, acc: 95.3125, f1: 96.8505179031495, r: 0.6917760871235654
06/02/2019 09:17:18 step: 4442, epoch: 134, batch: 19, loss: 0.09194552898406982, acc: 95.3125, f1: 96.0984393757503, r: 0.6932126836071603
06/02/2019 09:17:18 step: 4447, epoch: 134, batch: 24, loss: 0.14340555667877197, acc: 95.3125, f1: 92.21981721981722, r: 0.6347247092066356
06/02/2019 09:17:18 step: 4452, epoch: 134, batch: 29, loss: 0.11306148767471313, acc: 96.875, f1: 96.67045520778692, r: 0.8071348688851274
06/02/2019 09:17:18 *** evaluating ***
06/02/2019 09:17:19 step: 135, epoch: 134, acc: 59.401709401709404, f1: 26.564094726803873, r: 0.4109217791785536
06/02/2019 09:17:19 *** epoch: 136 ***
06/02/2019 09:17:19 *** training ***
06/02/2019 09:17:19 step: 4460, epoch: 135, batch: 4, loss: 0.17349594831466675, acc: 96.875, f1: 97.64306731519846, r: 0.694056762529858
06/02/2019 09:17:19 step: 4465, epoch: 135, batch: 9, loss: 0.04625878110527992, acc: 98.4375, f1: 99.09848997070092, r: 0.632070239314133
06/02/2019 09:17:19 step: 4470, epoch: 135, batch: 14, loss: 0.15177622437477112, acc: 92.1875, f1: 80.51787101787102, r: 0.6755968377814844
06/02/2019 09:17:20 step: 4475, epoch: 135, batch: 19, loss: 0.07877790927886963, acc: 96.875, f1: 94.20995670995671, r: 0.7052145674843018
06/02/2019 09:17:20 step: 4480, epoch: 135, batch: 24, loss: 0.1540893018245697, acc: 92.1875, f1: 82.01762820512822, r: 0.6952928286739755
06/02/2019 09:17:20 step: 4485, epoch: 135, batch: 29, loss: 0.05792884901165962, acc: 100.0, f1: 100.0, r: 0.5710420064677839
06/02/2019 09:17:21 *** evaluating ***
06/02/2019 09:17:21 step: 136, epoch: 135, acc: 60.256410256410255, f1: 27.520556126014345, r: 0.4105283276747274
06/02/2019 09:17:21 *** epoch: 137 ***
06/02/2019 09:17:21 *** training ***
06/02/2019 09:17:21 step: 4493, epoch: 136, batch: 4, loss: 0.16950714588165283, acc: 95.3125, f1: 97.69749802994482, r: 0.7197141348437495
06/02/2019 09:17:21 step: 4498, epoch: 136, batch: 9, loss: 0.13830718398094177, acc: 93.75, f1: 81.92750872553827, r: 0.60953683725015
06/02/2019 09:17:22 step: 4503, epoch: 136, batch: 14, loss: 0.05336719751358032, acc: 96.875, f1: 94.0400684586731, r: 0.6864461978475619
06/02/2019 09:17:22 step: 4508, epoch: 136, batch: 19, loss: 0.09169480949640274, acc: 95.3125, f1: 97.95698924731182, r: 0.7990921690837494
06/02/2019 09:17:22 step: 4513, epoch: 136, batch: 24, loss: 0.021556975319981575, acc: 100.0, f1: 100.0, r: 0.8213735462434355
06/02/2019 09:17:23 step: 4518, epoch: 136, batch: 29, loss: 0.1393013596534729, acc: 96.875, f1: 96.53198653198655, r: 0.5879353220578319
06/02/2019 09:17:23 *** evaluating ***
06/02/2019 09:17:23 step: 137, epoch: 136, acc: 61.965811965811966, f1: 29.715036147084827, r: 0.40913432187281396
06/02/2019 09:17:23 *** epoch: 138 ***
06/02/2019 09:17:23 *** training ***
06/02/2019 09:17:23 step: 4526, epoch: 137, batch: 4, loss: 0.08959534764289856, acc: 96.875, f1: 92.50221043324491, r: 0.6199213273690475
06/02/2019 09:17:24 step: 4531, epoch: 137, batch: 9, loss: 0.08467395603656769, acc: 96.875, f1: 91.93007662835248, r: 0.7603665253298403
06/02/2019 09:17:24 step: 4536, epoch: 137, batch: 14, loss: 0.07901205122470856, acc: 96.875, f1: 95.22167487684729, r: 0.6789032273736988
06/02/2019 09:17:24 step: 4541, epoch: 137, batch: 19, loss: 0.08135253190994263, acc: 96.875, f1: 97.10317460317461, r: 0.7012856881016399
06/02/2019 09:17:25 step: 4546, epoch: 137, batch: 24, loss: 0.07916469871997833, acc: 98.4375, f1: 95.71428571428572, r: 0.7601106160516574
06/02/2019 09:17:25 step: 4551, epoch: 137, batch: 29, loss: 0.05581340193748474, acc: 100.0, f1: 100.0, r: 0.8380024397244827
06/02/2019 09:17:25 *** evaluating ***
06/02/2019 09:17:25 step: 138, epoch: 137, acc: 60.256410256410255, f1: 27.662859311365306, r: 0.41097053651772175
06/02/2019 09:17:25 *** epoch: 139 ***
06/02/2019 09:17:25 *** training ***
06/02/2019 09:17:26 step: 4559, epoch: 138, batch: 4, loss: 0.09957146644592285, acc: 98.4375, f1: 99.1386735572782, r: 0.7096190007506944
06/02/2019 09:17:26 step: 4564, epoch: 138, batch: 9, loss: 0.1980229616165161, acc: 92.1875, f1: 92.3695652173913, r: 0.7287482866834012
06/02/2019 09:17:26 step: 4569, epoch: 138, batch: 14, loss: 0.07655186206102371, acc: 96.875, f1: 82.40948114968272, r: 0.7709148151414846
06/02/2019 09:17:27 step: 4574, epoch: 138, batch: 19, loss: 0.19959402084350586, acc: 93.75, f1: 87.73811442014207, r: 0.6356922351057921
06/02/2019 09:17:27 step: 4579, epoch: 138, batch: 24, loss: 0.0772218331694603, acc: 98.4375, f1: 97.73242630385487, r: 0.7160211468915999
06/02/2019 09:17:27 step: 4584, epoch: 138, batch: 29, loss: 0.10824091732501984, acc: 96.875, f1: 95.1486748545572, r: 0.7187935818069855
06/02/2019 09:17:27 *** evaluating ***
06/02/2019 09:17:28 step: 139, epoch: 138, acc: 60.256410256410255, f1: 28.925801290951192, r: 0.4161216584030035
06/02/2019 09:17:28 *** epoch: 140 ***
06/02/2019 09:17:28 *** training ***
06/02/2019 09:17:28 step: 4592, epoch: 139, batch: 4, loss: 0.17236174643039703, acc: 93.75, f1: 78.09829663090532, r: 0.7462797936874519
06/02/2019 09:17:28 step: 4597, epoch: 139, batch: 9, loss: 0.07478803396224976, acc: 96.875, f1: 81.61265232045632, r: 0.6804973732592228
06/02/2019 09:17:28 step: 4602, epoch: 139, batch: 14, loss: 0.054840087890625, acc: 100.0, f1: 100.0, r: 0.6928520419962382
06/02/2019 09:17:29 step: 4607, epoch: 139, batch: 19, loss: 0.10774241387844086, acc: 95.3125, f1: 90.06591337099812, r: 0.7381798486887605
06/02/2019 09:17:29 step: 4612, epoch: 139, batch: 24, loss: 0.1110457256436348, acc: 95.3125, f1: 96.47257504400362, r: 0.7159691563915808
06/02/2019 09:17:29 step: 4617, epoch: 139, batch: 29, loss: 0.13593348860740662, acc: 93.75, f1: 89.74358974358974, r: 0.8027650920734574
06/02/2019 09:17:30 *** evaluating ***
06/02/2019 09:17:30 step: 140, epoch: 139, acc: 61.965811965811966, f1: 29.32168018471375, r: 0.41623899424846544
06/02/2019 09:17:30 *** epoch: 141 ***
06/02/2019 09:17:30 *** training ***
06/02/2019 09:17:30 step: 4625, epoch: 140, batch: 4, loss: 0.1393962949514389, acc: 95.3125, f1: 92.82566508953316, r: 0.7287106255936953
06/02/2019 09:17:30 step: 4630, epoch: 140, batch: 9, loss: 0.12094525247812271, acc: 96.875, f1: 96.30811662726556, r: 0.7382418089824005
06/02/2019 09:17:31 step: 4635, epoch: 140, batch: 14, loss: 0.13356508314609528, acc: 95.3125, f1: 80.02136752136751, r: 0.7363019816601665
06/02/2019 09:17:31 step: 4640, epoch: 140, batch: 19, loss: 0.13392198085784912, acc: 95.3125, f1: 87.38500873676637, r: 0.7252253147039265
06/02/2019 09:17:31 step: 4645, epoch: 140, batch: 24, loss: 0.20434454083442688, acc: 95.3125, f1: 96.07293868921775, r: 0.7471140491821666
06/02/2019 09:17:32 step: 4650, epoch: 140, batch: 29, loss: 0.06717170029878616, acc: 96.875, f1: 95.01729378471859, r: 0.8261240664017413
06/02/2019 09:17:32 *** evaluating ***
06/02/2019 09:17:32 step: 141, epoch: 140, acc: 61.111111111111114, f1: 27.22573389771513, r: 0.4115209901993009
06/02/2019 09:17:32 *** epoch: 142 ***
06/02/2019 09:17:32 *** training ***
06/02/2019 09:17:32 step: 4658, epoch: 141, batch: 4, loss: 0.059481196105480194, acc: 98.4375, f1: 99.19437939110071, r: 0.6976848102624903
06/02/2019 09:17:33 step: 4663, epoch: 141, batch: 9, loss: 0.15181389451026917, acc: 95.3125, f1: 90.7905982905983, r: 0.7459602477117219
06/02/2019 09:17:33 step: 4668, epoch: 141, batch: 14, loss: 0.03680155426263809, acc: 98.4375, f1: 97.49835418038182, r: 0.66813905347334
06/02/2019 09:17:33 step: 4673, epoch: 141, batch: 19, loss: 0.2008332461118698, acc: 92.1875, f1: 93.80411255411255, r: 0.7114675366985345
06/02/2019 09:17:34 step: 4678, epoch: 141, batch: 24, loss: 0.1745074838399887, acc: 96.875, f1: 96.51953366239081, r: 0.6519589697118574
06/02/2019 09:17:34 step: 4683, epoch: 141, batch: 29, loss: 0.0972849652171135, acc: 96.875, f1: 96.54813218390805, r: 0.8054696073248081
06/02/2019 09:17:34 *** evaluating ***
06/02/2019 09:17:34 step: 142, epoch: 141, acc: 60.68376068376068, f1: 28.47161911555479, r: 0.41476172980262344
06/02/2019 09:17:34 *** epoch: 143 ***
06/02/2019 09:17:34 *** training ***
06/02/2019 09:17:35 step: 4691, epoch: 142, batch: 4, loss: 0.0769181177020073, acc: 98.4375, f1: 97.97843665768194, r: 0.8088239711019387
06/02/2019 09:17:35 step: 4696, epoch: 142, batch: 9, loss: 0.08431100845336914, acc: 98.4375, f1: 97.67080745341615, r: 0.7246953717241928
06/02/2019 09:17:35 step: 4701, epoch: 142, batch: 14, loss: 0.0805913656949997, acc: 98.4375, f1: 98.53846153846153, r: 0.7008047910593133
06/02/2019 09:17:36 step: 4706, epoch: 142, batch: 19, loss: 0.034123241901397705, acc: 98.4375, f1: 97.87644787644787, r: 0.7580812390116481
06/02/2019 09:17:36 step: 4711, epoch: 142, batch: 24, loss: 0.04692245274782181, acc: 100.0, f1: 100.0, r: 0.7926060524663056
06/02/2019 09:17:36 step: 4716, epoch: 142, batch: 29, loss: 0.12926726043224335, acc: 93.75, f1: 90.15522875816993, r: 0.7538204289231938
06/02/2019 09:17:36 *** evaluating ***
06/02/2019 09:17:37 step: 143, epoch: 142, acc: 59.401709401709404, f1: 25.81334966203387, r: 0.405237419097374
06/02/2019 09:17:37 *** epoch: 144 ***
06/02/2019 09:17:37 *** training ***
06/02/2019 09:17:37 step: 4724, epoch: 143, batch: 4, loss: 0.09590523689985275, acc: 96.875, f1: 97.27564102564102, r: 0.6470368490631239
06/02/2019 09:17:37 step: 4729, epoch: 143, batch: 9, loss: 0.1586403250694275, acc: 93.75, f1: 94.88448558814692, r: 0.7405891087511263
06/02/2019 09:17:37 step: 4734, epoch: 143, batch: 14, loss: 0.13455113768577576, acc: 96.875, f1: 98.33566403932537, r: 0.7680381730253443
06/02/2019 09:17:38 step: 4739, epoch: 143, batch: 19, loss: 0.06884532421827316, acc: 98.4375, f1: 98.9254718280755, r: 0.6616538662166106
06/02/2019 09:17:38 step: 4744, epoch: 143, batch: 24, loss: 0.07288792729377747, acc: 98.4375, f1: 97.73242630385488, r: 0.7082767583077499
06/02/2019 09:17:38 step: 4749, epoch: 143, batch: 29, loss: 0.059216465801000595, acc: 98.4375, f1: 97.55102040816325, r: 0.7156060667413467
06/02/2019 09:17:39 *** evaluating ***
06/02/2019 09:17:39 step: 144, epoch: 143, acc: 60.68376068376068, f1: 26.372828587634412, r: 0.4039114820790979
06/02/2019 09:17:39 *** epoch: 145 ***
06/02/2019 09:17:39 *** training ***
06/02/2019 09:17:39 step: 4757, epoch: 144, batch: 4, loss: 0.24383991956710815, acc: 93.75, f1: 80.91122540250447, r: 0.6879465430028096
06/02/2019 09:17:39 step: 4762, epoch: 144, batch: 9, loss: 0.043172355741262436, acc: 100.0, f1: 100.0, r: 0.7974975522563345
06/02/2019 09:17:40 step: 4767, epoch: 144, batch: 14, loss: 0.1298181563615799, acc: 98.4375, f1: 97.72727272727273, r: 0.7938632250021258
06/02/2019 09:17:40 step: 4772, epoch: 144, batch: 19, loss: 0.12264590710401535, acc: 96.875, f1: 93.125, r: 0.6617489346531753
06/02/2019 09:17:41 step: 4777, epoch: 144, batch: 24, loss: 0.08203688263893127, acc: 96.875, f1: 82.73809523809523, r: 0.7329838241366861
06/02/2019 09:17:41 step: 4782, epoch: 144, batch: 29, loss: 0.07963363081216812, acc: 96.875, f1: 96.02483470407999, r: 0.7397291977275369
06/02/2019 09:17:41 *** evaluating ***
06/02/2019 09:17:41 step: 145, epoch: 144, acc: 60.68376068376068, f1: 27.52299661012297, r: 0.40778136270028786
06/02/2019 09:17:41 *** epoch: 146 ***
06/02/2019 09:17:41 *** training ***
06/02/2019 09:17:41 step: 4790, epoch: 145, batch: 4, loss: 0.12506705522537231, acc: 95.3125, f1: 72.52631578947368, r: 0.6548832305858195
06/02/2019 09:17:42 step: 4795, epoch: 145, batch: 9, loss: 0.044509924948215485, acc: 96.875, f1: 95.53154595171402, r: 0.6821994573349609
06/02/2019 09:17:42 step: 4800, epoch: 145, batch: 14, loss: 0.07796857506036758, acc: 98.4375, f1: 96.42857142857143, r: 0.8206875116776194
06/02/2019 09:17:43 step: 4805, epoch: 145, batch: 19, loss: 0.15160483121871948, acc: 96.875, f1: 97.08333333333333, r: 0.7833855273474688
06/02/2019 09:17:43 step: 4810, epoch: 145, batch: 24, loss: 0.04767623916268349, acc: 98.4375, f1: 91.66666666666666, r: 0.7517885175235726
06/02/2019 09:17:43 step: 4815, epoch: 145, batch: 29, loss: 0.06302550435066223, acc: 98.4375, f1: 94.6969696969697, r: 0.7693755615163526
06/02/2019 09:17:43 *** evaluating ***
06/02/2019 09:17:44 step: 146, epoch: 145, acc: 61.53846153846154, f1: 29.436965811965816, r: 0.4073188803540836
06/02/2019 09:17:44 *** epoch: 147 ***
06/02/2019 09:17:44 *** training ***
06/02/2019 09:17:44 step: 4823, epoch: 146, batch: 4, loss: 0.11770675331354141, acc: 96.875, f1: 98.22640893421293, r: 0.6866161950496124
06/02/2019 09:17:44 step: 4828, epoch: 146, batch: 9, loss: 0.1404939442873001, acc: 95.3125, f1: 93.67845117845118, r: 0.7472655545630169
06/02/2019 09:17:45 step: 4833, epoch: 146, batch: 14, loss: 0.054844655096530914, acc: 96.875, f1: 96.75274725274726, r: 0.7879317666116376
06/02/2019 09:17:45 step: 4838, epoch: 146, batch: 19, loss: 0.07209093123674393, acc: 98.4375, f1: 95.37037037037037, r: 0.7204491201617346
06/02/2019 09:17:45 step: 4843, epoch: 146, batch: 24, loss: 0.12280771136283875, acc: 93.75, f1: 80.72735507246375, r: 0.7140854251043987
06/02/2019 09:17:46 step: 4848, epoch: 146, batch: 29, loss: 0.18488556146621704, acc: 93.75, f1: 90.3685684800236, r: 0.6895170647180358
06/02/2019 09:17:46 *** evaluating ***
06/02/2019 09:17:46 step: 147, epoch: 146, acc: 60.68376068376068, f1: 28.1299396606974, r: 0.4061756867659594
06/02/2019 09:17:46 *** epoch: 148 ***
06/02/2019 09:17:46 *** training ***
06/02/2019 09:17:46 step: 4856, epoch: 147, batch: 4, loss: 0.05812590569257736, acc: 98.4375, f1: 96.77655677655677, r: 0.730412254629559
06/02/2019 09:17:47 step: 4861, epoch: 147, batch: 9, loss: 0.03305381163954735, acc: 98.4375, f1: 96.66666666666667, r: 0.7612890287230731
06/02/2019 09:17:47 step: 4866, epoch: 147, batch: 14, loss: 0.11338666826486588, acc: 96.875, f1: 97.5044901906604, r: 0.7065139327261211
06/02/2019 09:17:47 step: 4871, epoch: 147, batch: 19, loss: 0.08900250494480133, acc: 96.875, f1: 96.75241686869593, r: 0.771088408677149
06/02/2019 09:17:48 step: 4876, epoch: 147, batch: 24, loss: 0.07728995382785797, acc: 98.4375, f1: 99.02818270165209, r: 0.7221763133558675
06/02/2019 09:17:48 step: 4881, epoch: 147, batch: 29, loss: 0.09939301013946533, acc: 98.4375, f1: 97.59288330716903, r: 0.6680182764393006
06/02/2019 09:17:48 *** evaluating ***
06/02/2019 09:17:48 step: 148, epoch: 147, acc: 61.53846153846154, f1: 29.133524662936427, r: 0.41072614527655366
06/02/2019 09:17:48 *** epoch: 149 ***
06/02/2019 09:17:48 *** training ***
06/02/2019 09:17:49 step: 4889, epoch: 148, batch: 4, loss: 0.1006508320569992, acc: 95.3125, f1: 95.10327635327636, r: 0.7243563013737361
06/02/2019 09:17:49 step: 4894, epoch: 148, batch: 9, loss: 0.11161449551582336, acc: 96.875, f1: 97.68279965648387, r: 0.7697433222687449
06/02/2019 09:17:49 step: 4899, epoch: 148, batch: 14, loss: 0.03302443027496338, acc: 100.0, f1: 100.0, r: 0.6824677862213112
06/02/2019 09:17:50 step: 4904, epoch: 148, batch: 19, loss: 0.024870704859495163, acc: 100.0, f1: 100.0, r: 0.727487223554719
06/02/2019 09:17:50 step: 4909, epoch: 148, batch: 24, loss: 0.15275204181671143, acc: 95.3125, f1: 90.37301587301587, r: 0.7241995006813101
06/02/2019 09:17:50 step: 4914, epoch: 148, batch: 29, loss: 0.05262468010187149, acc: 100.0, f1: 100.0, r: 0.7148277210701223
06/02/2019 09:17:50 *** evaluating ***
06/02/2019 09:17:51 step: 149, epoch: 148, acc: 60.68376068376068, f1: 28.795363386322993, r: 0.4104129593774709
06/02/2019 09:17:51 *** epoch: 150 ***
06/02/2019 09:17:51 *** training ***
06/02/2019 09:17:51 step: 4922, epoch: 149, batch: 4, loss: 0.05404404550790787, acc: 100.0, f1: 100.0, r: 0.6180725301186116
06/02/2019 09:17:51 step: 4927, epoch: 149, batch: 9, loss: 0.046009037643671036, acc: 100.0, f1: 100.0, r: 0.6793031975469448
06/02/2019 09:17:52 step: 4932, epoch: 149, batch: 14, loss: 0.13682493567466736, acc: 95.3125, f1: 91.35466988727859, r: 0.7311972124736646
06/02/2019 09:17:52 step: 4937, epoch: 149, batch: 19, loss: 0.15537001192569733, acc: 93.75, f1: 93.53861192570871, r: 0.670066682837936
06/02/2019 09:17:52 step: 4942, epoch: 149, batch: 24, loss: 0.08528456836938858, acc: 96.875, f1: 97.83390628978864, r: 0.6877833062198178
06/02/2019 09:17:53 step: 4947, epoch: 149, batch: 29, loss: 0.07811002433300018, acc: 96.875, f1: 93.71428571428571, r: 0.7292696321424909
06/02/2019 09:17:53 *** evaluating ***
06/02/2019 09:17:53 step: 150, epoch: 149, acc: 61.53846153846154, f1: 29.18438167807916, r: 0.4041713672891405
06/02/2019 09:17:53 *** epoch: 151 ***
06/02/2019 09:17:53 *** training ***
06/02/2019 09:17:53 step: 4955, epoch: 150, batch: 4, loss: 0.1217392161488533, acc: 96.875, f1: 95.96140574401444, r: 0.7572506105799082
06/02/2019 09:17:53 step: 4960, epoch: 150, batch: 9, loss: 0.13334304094314575, acc: 95.3125, f1: 92.1885521885522, r: 0.8139633077014244
06/02/2019 09:17:54 step: 4965, epoch: 150, batch: 14, loss: 0.03195486590266228, acc: 100.0, f1: 100.0, r: 0.7244115614042596
06/02/2019 09:17:54 step: 4970, epoch: 150, batch: 19, loss: 0.10882052779197693, acc: 96.875, f1: 97.67803517803519, r: 0.7773764329564994
06/02/2019 09:17:54 step: 4975, epoch: 150, batch: 24, loss: 0.05489377677440643, acc: 96.875, f1: 94.5797673908273, r: 0.7970116305036774
06/02/2019 09:17:55 step: 4980, epoch: 150, batch: 29, loss: 0.1611742526292801, acc: 96.875, f1: 96.8996794119947, r: 0.6887982770689292
06/02/2019 09:17:55 *** evaluating ***
06/02/2019 09:17:55 step: 151, epoch: 150, acc: 60.68376068376068, f1: 27.137507049301234, r: 0.40134377262455484
06/02/2019 09:17:55 *** epoch: 152 ***
06/02/2019 09:17:55 *** training ***
06/02/2019 09:17:55 step: 4988, epoch: 151, batch: 4, loss: 0.10707270354032516, acc: 93.75, f1: 84.01233185715944, r: 0.7655658488159571
06/02/2019 09:17:56 step: 4993, epoch: 151, batch: 9, loss: 0.09758920222520828, acc: 98.4375, f1: 98.6842105263158, r: 0.7296517821920532
06/02/2019 09:17:56 step: 4998, epoch: 151, batch: 14, loss: 0.07133995741605759, acc: 98.4375, f1: 98.06076276664513, r: 0.6970651496059004
06/02/2019 09:17:56 step: 5003, epoch: 151, batch: 19, loss: 0.10178577154874802, acc: 93.75, f1: 79.96705744826046, r: 0.6194910319048196
06/02/2019 09:17:57 step: 5008, epoch: 151, batch: 24, loss: 0.15346762537956238, acc: 96.875, f1: 97.59891271519179, r: 0.7218011321690542
06/02/2019 09:17:57 step: 5013, epoch: 151, batch: 29, loss: 0.08619437366724014, acc: 98.4375, f1: 99.12128877646118, r: 0.7074050837708424
06/02/2019 09:17:57 *** evaluating ***
06/02/2019 09:17:57 step: 152, epoch: 151, acc: 59.82905982905983, f1: 25.993230268370837, r: 0.4019683304103347
06/02/2019 09:17:57 *** epoch: 153 ***
06/02/2019 09:17:57 *** training ***
06/02/2019 09:17:58 step: 5021, epoch: 152, batch: 4, loss: 0.07341205328702927, acc: 98.4375, f1: 98.35286664554957, r: 0.6109999285383214
06/02/2019 09:17:58 step: 5026, epoch: 152, batch: 9, loss: 0.07068020105361938, acc: 98.4375, f1: 98.44322344322345, r: 0.7945816310095387
06/02/2019 09:17:58 step: 5031, epoch: 152, batch: 14, loss: 0.15424208343029022, acc: 93.75, f1: 91.53138528138528, r: 0.7252194886596492
06/02/2019 09:17:59 step: 5036, epoch: 152, batch: 19, loss: 0.13478735089302063, acc: 90.625, f1: 86.94408369408369, r: 0.698958747252196
06/02/2019 09:17:59 step: 5041, epoch: 152, batch: 24, loss: 0.0607869029045105, acc: 96.875, f1: 97.35309017223909, r: 0.7477944232967133
06/02/2019 09:17:59 step: 5046, epoch: 152, batch: 29, loss: 0.05915052071213722, acc: 98.4375, f1: 98.46819846819848, r: 0.6814948829687097
06/02/2019 09:18:00 *** evaluating ***
06/02/2019 09:18:00 step: 153, epoch: 152, acc: 61.111111111111114, f1: 27.888083761428106, r: 0.407090117842302
06/02/2019 09:18:00 *** epoch: 154 ***
06/02/2019 09:18:00 *** training ***
06/02/2019 09:18:00 step: 5054, epoch: 153, batch: 4, loss: 0.09306979179382324, acc: 96.875, f1: 97.63725929243171, r: 0.659422758888565
06/02/2019 09:18:00 step: 5059, epoch: 153, batch: 9, loss: 0.16950780153274536, acc: 95.3125, f1: 96.65334665334666, r: 0.5639756768997175
06/02/2019 09:18:01 step: 5064, epoch: 153, batch: 14, loss: 0.06656558811664581, acc: 98.4375, f1: 98.77250409165302, r: 0.7654983792231045
06/02/2019 09:18:01 step: 5069, epoch: 153, batch: 19, loss: 0.07142017781734467, acc: 98.4375, f1: 97.71428571428571, r: 0.762922317558695
06/02/2019 09:18:01 step: 5074, epoch: 153, batch: 24, loss: 0.04617951065301895, acc: 98.4375, f1: 99.13702623906705, r: 0.6227410411401795
06/02/2019 09:18:02 step: 5079, epoch: 153, batch: 29, loss: 0.0383414626121521, acc: 100.0, f1: 100.0, r: 0.785200987893627
06/02/2019 09:18:02 *** evaluating ***
06/02/2019 09:18:02 step: 154, epoch: 153, acc: 61.53846153846154, f1: 29.694005134381342, r: 0.405230438569451
06/02/2019 09:18:02 *** epoch: 155 ***
06/02/2019 09:18:02 *** training ***
06/02/2019 09:18:02 step: 5087, epoch: 154, batch: 4, loss: 0.06935667991638184, acc: 96.875, f1: 97.89721279003962, r: 0.7974892588370426
06/02/2019 09:18:03 step: 5092, epoch: 154, batch: 9, loss: 0.2157350331544876, acc: 92.1875, f1: 92.98266045548654, r: 0.792936349873992
06/02/2019 09:18:03 step: 5097, epoch: 154, batch: 14, loss: 0.0858616754412651, acc: 96.875, f1: 94.99329796673524, r: 0.6962281952130902
06/02/2019 09:18:03 step: 5102, epoch: 154, batch: 19, loss: 0.1779859960079193, acc: 95.3125, f1: 88.08964024481266, r: 0.6564823684745481
06/02/2019 09:18:03 step: 5107, epoch: 154, batch: 24, loss: 0.1600622981786728, acc: 93.75, f1: 71.26984126984127, r: 0.6184004132377164
06/02/2019 09:18:04 step: 5112, epoch: 154, batch: 29, loss: 0.1458708941936493, acc: 95.3125, f1: 90.62142158916353, r: 0.659722468698889
06/02/2019 09:18:04 *** evaluating ***
06/02/2019 09:18:04 step: 155, epoch: 154, acc: 61.53846153846154, f1: 28.90732602672032, r: 0.4079357979474368
06/02/2019 09:18:04 *** epoch: 156 ***
06/02/2019 09:18:04 *** training ***
06/02/2019 09:18:05 step: 5120, epoch: 155, batch: 4, loss: 0.13356970250606537, acc: 93.75, f1: 93.23246539995309, r: 0.650270669366584
06/02/2019 09:18:05 step: 5125, epoch: 155, batch: 9, loss: 0.13161712884902954, acc: 92.1875, f1: 71.68582203465925, r: 0.6810316346264114
06/02/2019 09:18:05 step: 5130, epoch: 155, batch: 14, loss: 0.07730910927057266, acc: 96.875, f1: 97.78617117053193, r: 0.6581354633541875
06/02/2019 09:18:06 step: 5135, epoch: 155, batch: 19, loss: 0.0901229977607727, acc: 96.875, f1: 96.4401145073414, r: 0.6857989368725714
06/02/2019 09:18:06 step: 5140, epoch: 155, batch: 24, loss: 0.15312480926513672, acc: 93.75, f1: 92.29126133211905, r: 0.6956546132077907
06/02/2019 09:18:06 step: 5145, epoch: 155, batch: 29, loss: 0.06623321026563644, acc: 98.4375, f1: 99.21115921115921, r: 0.7255660528170151
06/02/2019 09:18:06 *** evaluating ***
06/02/2019 09:18:07 step: 156, epoch: 155, acc: 59.82905982905983, f1: 25.732990796587135, r: 0.40470909945859895
06/02/2019 09:18:07 *** epoch: 157 ***
06/02/2019 09:18:07 *** training ***
06/02/2019 09:18:07 step: 5153, epoch: 156, batch: 4, loss: 0.05663447082042694, acc: 96.875, f1: 92.83097854526424, r: 0.7200372456233387
06/02/2019 09:18:07 step: 5158, epoch: 156, batch: 9, loss: 0.11701443791389465, acc: 95.3125, f1: 97.46031746031746, r: 0.7310858236421142
06/02/2019 09:18:08 step: 5163, epoch: 156, batch: 14, loss: 0.07317552715539932, acc: 95.3125, f1: 96.35846560846561, r: 0.7833137245873388
06/02/2019 09:18:08 step: 5168, epoch: 156, batch: 19, loss: 0.09892866015434265, acc: 95.3125, f1: 96.09427609427608, r: 0.7254282578833717
06/02/2019 09:18:08 step: 5173, epoch: 156, batch: 24, loss: 0.11005862057209015, acc: 95.3125, f1: 87.67323479564914, r: 0.6536588799430058
06/02/2019 09:18:08 step: 5178, epoch: 156, batch: 29, loss: 0.053624991327524185, acc: 98.4375, f1: 99.24465733235077, r: 0.7052133252469757
06/02/2019 09:18:09 *** evaluating ***
06/02/2019 09:18:09 step: 157, epoch: 156, acc: 60.68376068376068, f1: 26.415413676690587, r: 0.40230893104675913
06/02/2019 09:18:09 *** epoch: 158 ***
06/02/2019 09:18:09 *** training ***
06/02/2019 09:18:09 step: 5186, epoch: 157, batch: 4, loss: 0.08513586968183517, acc: 96.875, f1: 95.75416209960318, r: 0.5778254568344322
06/02/2019 09:18:10 step: 5191, epoch: 157, batch: 9, loss: 0.0849587693810463, acc: 96.875, f1: 98.35164835164835, r: 0.646466772687747
06/02/2019 09:18:10 step: 5196, epoch: 157, batch: 14, loss: 0.03755194693803787, acc: 98.4375, f1: 96.84210526315789, r: 0.7364327938857818
06/02/2019 09:18:10 step: 5201, epoch: 157, batch: 19, loss: 0.06178326532244682, acc: 96.875, f1: 95.03750907477615, r: 0.6812435348400949
06/02/2019 09:18:11 step: 5206, epoch: 157, batch: 24, loss: 0.06687816232442856, acc: 98.4375, f1: 94.87179487179486, r: 0.7911320220977602
06/02/2019 09:18:11 step: 5211, epoch: 157, batch: 29, loss: 0.04301530122756958, acc: 98.4375, f1: 97.81105990783409, r: 0.7984076508573201
06/02/2019 09:18:11 *** evaluating ***
06/02/2019 09:18:11 step: 158, epoch: 157, acc: 61.965811965811966, f1: 28.680439893675192, r: 0.4028091937595152
06/02/2019 09:18:11 *** epoch: 159 ***
06/02/2019 09:18:11 *** training ***
06/02/2019 09:18:12 step: 5219, epoch: 158, batch: 4, loss: 0.08406662195920944, acc: 98.4375, f1: 94.70899470899471, r: 0.6447187617308431
06/02/2019 09:18:12 step: 5224, epoch: 158, batch: 9, loss: 0.07669097185134888, acc: 98.4375, f1: 99.35728411338167, r: 0.7114383537285007
06/02/2019 09:18:12 step: 5229, epoch: 158, batch: 14, loss: 0.10170052945613861, acc: 95.3125, f1: 82.58476307189542, r: 0.7494776054902278
06/02/2019 09:18:13 step: 5234, epoch: 158, batch: 19, loss: 0.1334204524755478, acc: 93.75, f1: 92.87512487512488, r: 0.7248564404990459
06/02/2019 09:18:13 step: 5239, epoch: 158, batch: 24, loss: 0.08385487645864487, acc: 96.875, f1: 95.96242306446389, r: 0.6559482862967381
06/02/2019 09:18:13 step: 5244, epoch: 158, batch: 29, loss: 0.11656863987445831, acc: 95.3125, f1: 81.62280701754386, r: 0.652280589102403
06/02/2019 09:18:13 *** evaluating ***
06/02/2019 09:18:13 step: 159, epoch: 158, acc: 62.39316239316239, f1: 29.99365481071854, r: 0.40415633924400296
06/02/2019 09:18:13 *** epoch: 160 ***
06/02/2019 09:18:13 *** training ***
06/02/2019 09:18:14 step: 5252, epoch: 159, batch: 4, loss: 0.11842657625675201, acc: 93.75, f1: 93.32631556896263, r: 0.7343795249253219
06/02/2019 09:18:14 step: 5257, epoch: 159, batch: 9, loss: 0.04723995551466942, acc: 100.0, f1: 100.0, r: 0.7781393122514554
06/02/2019 09:18:14 step: 5262, epoch: 159, batch: 14, loss: 0.0464339517056942, acc: 100.0, f1: 100.0, r: 0.7955511141779482
06/02/2019 09:18:15 step: 5267, epoch: 159, batch: 19, loss: 0.06890921294689178, acc: 98.4375, f1: 95.10204081632654, r: 0.729931780333152
06/02/2019 09:18:15 step: 5272, epoch: 159, batch: 24, loss: 0.08170776069164276, acc: 98.4375, f1: 98.36601307189542, r: 0.776410593809447
06/02/2019 09:18:15 step: 5277, epoch: 159, batch: 29, loss: 0.06851301342248917, acc: 98.4375, f1: 98.26839826839827, r: 0.6542020481530911
06/02/2019 09:18:16 *** evaluating ***
06/02/2019 09:18:16 step: 160, epoch: 159, acc: 61.53846153846154, f1: 29.297188795275215, r: 0.4018482585258882
06/02/2019 09:18:16 *** epoch: 161 ***
06/02/2019 09:18:16 *** training ***
06/02/2019 09:18:16 step: 5285, epoch: 160, batch: 4, loss: 0.13493585586547852, acc: 95.3125, f1: 96.1030817009078, r: 0.755729079670908
06/02/2019 09:18:16 step: 5290, epoch: 160, batch: 9, loss: 0.048298075795173645, acc: 98.4375, f1: 99.3006993006993, r: 0.7354355090687442
06/02/2019 09:18:17 step: 5295, epoch: 160, batch: 14, loss: 0.06539901345968246, acc: 96.875, f1: 96.13636363636364, r: 0.8215870551559785
06/02/2019 09:18:17 step: 5300, epoch: 160, batch: 19, loss: 0.051864635199308395, acc: 100.0, f1: 100.0, r: 0.7649923061576391
06/02/2019 09:18:17 step: 5305, epoch: 160, batch: 24, loss: 0.14687411487102509, acc: 93.75, f1: 92.56734006734007, r: 0.7680562806793801
06/02/2019 09:18:18 step: 5310, epoch: 160, batch: 29, loss: 0.11482478678226471, acc: 95.3125, f1: 92.05235940530059, r: 0.6824066672732156
06/02/2019 09:18:18 *** evaluating ***
06/02/2019 09:18:18 step: 161, epoch: 160, acc: 61.53846153846154, f1: 29.195227101070664, r: 0.40355594663221095
06/02/2019 09:18:18 *** epoch: 162 ***
06/02/2019 09:18:18 *** training ***
06/02/2019 09:18:18 step: 5318, epoch: 161, batch: 4, loss: 0.1541408896446228, acc: 93.75, f1: 89.04887095459361, r: 0.7775689555074702
06/02/2019 09:18:19 step: 5323, epoch: 161, batch: 9, loss: 0.15191641449928284, acc: 95.3125, f1: 91.92012288786484, r: 0.6426338716691513
06/02/2019 09:18:19 step: 5328, epoch: 161, batch: 14, loss: 0.06936001777648926, acc: 96.875, f1: 98.08111278699513, r: 0.7232933781759536
06/02/2019 09:18:19 step: 5333, epoch: 161, batch: 19, loss: 0.1253761649131775, acc: 96.875, f1: 93.00976800976802, r: 0.5695007998457225
06/02/2019 09:18:20 step: 5338, epoch: 161, batch: 24, loss: 0.05252014100551605, acc: 100.0, f1: 100.0, r: 0.6685736061547062
06/02/2019 09:18:20 step: 5343, epoch: 161, batch: 29, loss: 0.08075207471847534, acc: 96.875, f1: 97.36463133640552, r: 0.7386395752280333
06/02/2019 09:18:20 *** evaluating ***
06/02/2019 09:18:20 step: 162, epoch: 161, acc: 61.111111111111114, f1: 27.91596488727383, r: 0.4067900160784546
06/02/2019 09:18:20 *** epoch: 163 ***
06/02/2019 09:18:20 *** training ***
06/02/2019 09:18:21 step: 5351, epoch: 162, batch: 4, loss: 0.17535509169101715, acc: 93.75, f1: 92.76201868307132, r: 0.7666713558155578
06/02/2019 09:18:21 step: 5356, epoch: 162, batch: 9, loss: 0.1123773381114006, acc: 96.875, f1: 94.50280112044818, r: 0.749243312722927
06/02/2019 09:18:21 step: 5361, epoch: 162, batch: 14, loss: 0.0972585380077362, acc: 95.3125, f1: 88.43286099063707, r: 0.677152672517912
06/02/2019 09:18:22 step: 5366, epoch: 162, batch: 19, loss: 0.0771869644522667, acc: 96.875, f1: 96.45247087107552, r: 0.6797041180990251
06/02/2019 09:18:22 step: 5371, epoch: 162, batch: 24, loss: 0.1310863494873047, acc: 95.3125, f1: 83.74604990737716, r: 0.6209081055016851
06/02/2019 09:18:22 step: 5376, epoch: 162, batch: 29, loss: 0.06911028176546097, acc: 98.4375, f1: 96.82539682539682, r: 0.6202134220408645
06/02/2019 09:18:23 *** evaluating ***
06/02/2019 09:18:23 step: 163, epoch: 162, acc: 61.965811965811966, f1: 28.26743060723643, r: 0.403231507801483
06/02/2019 09:18:23 *** epoch: 164 ***
06/02/2019 09:18:23 *** training ***
06/02/2019 09:18:23 step: 5384, epoch: 163, batch: 4, loss: 0.09475772827863693, acc: 96.875, f1: 95.06586603360797, r: 0.671527960404867
06/02/2019 09:18:23 step: 5389, epoch: 163, batch: 9, loss: 0.09340150654315948, acc: 98.4375, f1: 95.52845528455285, r: 0.8602040638747738
06/02/2019 09:18:24 step: 5394, epoch: 163, batch: 14, loss: 0.06993398815393448, acc: 96.875, f1: 98.53703703703704, r: 0.819630088553597
06/02/2019 09:18:24 step: 5399, epoch: 163, batch: 19, loss: 0.09219826757907867, acc: 98.4375, f1: 98.22082679225537, r: 0.6707845256772077
06/02/2019 09:18:25 step: 5404, epoch: 163, batch: 24, loss: 0.042883191257715225, acc: 98.4375, f1: 99.28046540110228, r: 0.7078955576025447
06/02/2019 09:18:25 step: 5409, epoch: 163, batch: 29, loss: 0.06824159622192383, acc: 98.4375, f1: 98.99874843554443, r: 0.7163822616849642
06/02/2019 09:18:25 *** evaluating ***
06/02/2019 09:18:25 step: 164, epoch: 163, acc: 61.965811965811966, f1: 29.168614881850175, r: 0.4036036659475885
06/02/2019 09:18:25 *** epoch: 165 ***
06/02/2019 09:18:25 *** training ***
06/02/2019 09:18:26 step: 5417, epoch: 164, batch: 4, loss: 0.09331630170345306, acc: 96.875, f1: 98.07881773399015, r: 0.6894424280912531
06/02/2019 09:18:26 step: 5422, epoch: 164, batch: 9, loss: 0.227649986743927, acc: 92.1875, f1: 91.7152895413765, r: 0.713660626568111
06/02/2019 09:18:26 step: 5427, epoch: 164, batch: 14, loss: 0.06170349568128586, acc: 98.4375, f1: 97.25274725274726, r: 0.761518931175041
06/02/2019 09:18:26 step: 5432, epoch: 164, batch: 19, loss: 0.07653117179870605, acc: 95.3125, f1: 95.58204594969301, r: 0.758659505698031
06/02/2019 09:18:27 step: 5437, epoch: 164, batch: 24, loss: 0.12253622710704803, acc: 93.75, f1: 90.36507936507938, r: 0.642544017574284
06/02/2019 09:18:27 step: 5442, epoch: 164, batch: 29, loss: 0.060339875519275665, acc: 98.4375, f1: 85.71428571428572, r: 0.7921739595352238
06/02/2019 09:18:27 *** evaluating ***
06/02/2019 09:18:27 step: 165, epoch: 164, acc: 61.53846153846154, f1: 28.659229444113166, r: 0.40113365134159473
06/02/2019 09:18:27 *** epoch: 166 ***
06/02/2019 09:18:27 *** training ***
06/02/2019 09:18:28 step: 5450, epoch: 165, batch: 4, loss: 0.06596578657627106, acc: 98.4375, f1: 99.28193499622071, r: 0.7797069782519483
06/02/2019 09:18:28 step: 5455, epoch: 165, batch: 9, loss: 0.09169764071702957, acc: 98.4375, f1: 96.39097744360903, r: 0.6781085638295495
06/02/2019 09:18:28 step: 5460, epoch: 165, batch: 14, loss: 0.09530476480722427, acc: 96.875, f1: 97.40447461035696, r: 0.7322898371413193
06/02/2019 09:18:29 step: 5465, epoch: 165, batch: 19, loss: 0.06149425730109215, acc: 96.875, f1: 98.27835288704854, r: 0.725142938848718
06/02/2019 09:18:29 step: 5470, epoch: 165, batch: 24, loss: 0.07273033261299133, acc: 96.875, f1: 95.41666666666666, r: 0.7855296293167942
06/02/2019 09:18:29 step: 5475, epoch: 165, batch: 29, loss: 0.13856881856918335, acc: 95.3125, f1: 95.10521885521887, r: 0.5851089191134452
06/02/2019 09:18:30 *** evaluating ***
06/02/2019 09:18:30 step: 166, epoch: 165, acc: 60.68376068376068, f1: 28.397441160372196, r: 0.39926482178255807
06/02/2019 09:18:30 *** epoch: 167 ***
06/02/2019 09:18:30 *** training ***
06/02/2019 09:18:30 step: 5483, epoch: 166, batch: 4, loss: 0.07534648478031158, acc: 96.875, f1: 84.0702947845805, r: 0.7328577030396968
06/02/2019 09:18:30 step: 5488, epoch: 166, batch: 9, loss: 0.16670019924640656, acc: 95.3125, f1: 94.51827107654657, r: 0.6974027376099042
06/02/2019 09:18:31 step: 5493, epoch: 166, batch: 14, loss: 0.07668668031692505, acc: 98.4375, f1: 97.70855710705335, r: 0.6609121206315912
06/02/2019 09:18:31 step: 5498, epoch: 166, batch: 19, loss: 0.058613572269678116, acc: 98.4375, f1: 98.83040935672514, r: 0.6125117127309747
06/02/2019 09:18:31 step: 5503, epoch: 166, batch: 24, loss: 0.2459324151277542, acc: 92.1875, f1: 79.41116231438812, r: 0.6201830237974729
06/02/2019 09:18:32 step: 5508, epoch: 166, batch: 29, loss: 0.06677107512950897, acc: 100.0, f1: 100.0, r: 0.682408799202591
06/02/2019 09:18:32 *** evaluating ***
06/02/2019 09:18:32 step: 167, epoch: 166, acc: 60.68376068376068, f1: 28.444580758512057, r: 0.3990399140761496
06/02/2019 09:18:32 *** epoch: 168 ***
06/02/2019 09:18:32 *** training ***
06/02/2019 09:18:32 step: 5516, epoch: 167, batch: 4, loss: 0.05609247833490372, acc: 98.4375, f1: 99.3097643097643, r: 0.6939228940183895
06/02/2019 09:18:33 step: 5521, epoch: 167, batch: 9, loss: 0.07293323427438736, acc: 96.875, f1: 83.49844002641706, r: 0.658530920107223
06/02/2019 09:18:33 step: 5526, epoch: 167, batch: 14, loss: 0.052009060978889465, acc: 98.4375, f1: 99.27943024717217, r: 0.6598116699579831
06/02/2019 09:18:33 step: 5531, epoch: 167, batch: 19, loss: 0.05845562741160393, acc: 98.4375, f1: 99.30118798043326, r: 0.7701356988168636
06/02/2019 09:18:33 step: 5536, epoch: 167, batch: 24, loss: 0.03924058377742767, acc: 98.4375, f1: 95.71428571428572, r: 0.7690597524135335
06/02/2019 09:18:34 step: 5541, epoch: 167, batch: 29, loss: 0.07912539690732956, acc: 98.4375, f1: 99.28070175438597, r: 0.7512748456002545
06/02/2019 09:18:34 *** evaluating ***
06/02/2019 09:18:34 step: 168, epoch: 167, acc: 61.965811965811966, f1: 29.006843521655213, r: 0.4048241023756712
06/02/2019 09:18:34 *** epoch: 169 ***
06/02/2019 09:18:34 *** training ***
06/02/2019 09:18:34 step: 5549, epoch: 168, batch: 4, loss: 0.0926564410328865, acc: 96.875, f1: 98.875, r: 0.8065012518548016
06/02/2019 09:18:35 step: 5554, epoch: 168, batch: 9, loss: 0.08102744072675705, acc: 96.875, f1: 97.58424557122768, r: 0.7731347298291903
06/02/2019 09:18:35 step: 5559, epoch: 168, batch: 14, loss: 0.12444263696670532, acc: 95.3125, f1: 95.28286528286529, r: 0.6363148951650687
06/02/2019 09:18:35 step: 5564, epoch: 168, batch: 19, loss: 0.04283546656370163, acc: 100.0, f1: 100.0, r: 0.6802693025887904
06/02/2019 09:18:36 step: 5569, epoch: 168, batch: 24, loss: 0.04936983808875084, acc: 98.4375, f1: 95.23809523809524, r: 0.6193274271046971
06/02/2019 09:18:36 step: 5574, epoch: 168, batch: 29, loss: 0.07569967955350876, acc: 96.875, f1: 98.50414078674949, r: 0.7522876315124791
06/02/2019 09:18:36 *** evaluating ***
06/02/2019 09:18:36 step: 169, epoch: 168, acc: 60.68376068376068, f1: 27.20894909688013, r: 0.4002884286579599
06/02/2019 09:18:36 *** epoch: 170 ***
06/02/2019 09:18:36 *** training ***
06/02/2019 09:18:37 step: 5582, epoch: 169, batch: 4, loss: 0.23854929208755493, acc: 93.75, f1: 93.19943850556095, r: 0.6161091234808234
06/02/2019 09:18:37 step: 5587, epoch: 169, batch: 9, loss: 0.09327761828899384, acc: 95.3125, f1: 95.9000323389118, r: 0.8095074565913232
06/02/2019 09:18:37 step: 5592, epoch: 169, batch: 14, loss: 0.06642919778823853, acc: 96.875, f1: 98.15184815184816, r: 0.6727967851216373
06/02/2019 09:18:38 step: 5597, epoch: 169, batch: 19, loss: 0.09188701957464218, acc: 98.4375, f1: 98.96774193548387, r: 0.7408820639840491
06/02/2019 09:18:38 step: 5602, epoch: 169, batch: 24, loss: 0.06443091481924057, acc: 98.4375, f1: 97.27891156462584, r: 0.6721707667160145
06/02/2019 09:18:38 step: 5607, epoch: 169, batch: 29, loss: 0.06709567457437515, acc: 96.875, f1: 96.12975133556247, r: 0.6895558640936867
06/02/2019 09:18:39 *** evaluating ***
06/02/2019 09:18:39 step: 170, epoch: 169, acc: 61.53846153846154, f1: 28.944849869772472, r: 0.4093072332440835
06/02/2019 09:18:39 *** epoch: 171 ***
06/02/2019 09:18:39 *** training ***
06/02/2019 09:18:39 step: 5615, epoch: 170, batch: 4, loss: 0.14621543884277344, acc: 96.875, f1: 92.93650793650794, r: 0.6906418499119864
06/02/2019 09:18:39 step: 5620, epoch: 170, batch: 9, loss: 0.16758993268013, acc: 95.3125, f1: 91.04657936956076, r: 0.6469922110619506
06/02/2019 09:18:40 step: 5625, epoch: 170, batch: 14, loss: 0.03310856968164444, acc: 98.4375, f1: 95.71428571428572, r: 0.7458483593528608
06/02/2019 09:18:40 step: 5630, epoch: 170, batch: 19, loss: 0.0789589211344719, acc: 98.4375, f1: 97.73242630385488, r: 0.6920611063837567
06/02/2019 09:18:40 step: 5635, epoch: 170, batch: 24, loss: 0.14167918264865875, acc: 93.75, f1: 95.97518128130373, r: 0.710377210715604
06/02/2019 09:18:41 step: 5640, epoch: 170, batch: 29, loss: 0.02294190600514412, acc: 100.0, f1: 100.0, r: 0.7223371035358146
06/02/2019 09:18:41 *** evaluating ***
06/02/2019 09:18:41 step: 171, epoch: 170, acc: 61.53846153846154, f1: 30.646598048941797, r: 0.41035887340423643
06/02/2019 09:18:41 *** epoch: 172 ***
06/02/2019 09:18:41 *** training ***
06/02/2019 09:18:41 step: 5648, epoch: 171, batch: 4, loss: 0.21052557229995728, acc: 95.3125, f1: 90.05092946269419, r: 0.5930617550908169
06/02/2019 09:18:42 step: 5653, epoch: 171, batch: 9, loss: 0.09687034785747528, acc: 98.4375, f1: 98.77250409165302, r: 0.7170885486353928
06/02/2019 09:18:42 step: 5658, epoch: 171, batch: 14, loss: 0.05825038254261017, acc: 98.4375, f1: 98.17219817219816, r: 0.6656413180162799
06/02/2019 09:18:42 step: 5663, epoch: 171, batch: 19, loss: 0.05777214840054512, acc: 98.4375, f1: 95.28985507246377, r: 0.7429476738008522
06/02/2019 09:18:43 step: 5668, epoch: 171, batch: 24, loss: 0.10706689953804016, acc: 95.3125, f1: 91.37311762311762, r: 0.8025792617561462
06/02/2019 09:18:43 step: 5673, epoch: 171, batch: 29, loss: 0.0664413571357727, acc: 96.875, f1: 97.9860365198711, r: 0.6675210517013566
06/02/2019 09:18:43 *** evaluating ***
06/02/2019 09:18:43 step: 172, epoch: 171, acc: 61.965811965811966, f1: 29.946223496146096, r: 0.4113456071563293
06/02/2019 09:18:43 *** epoch: 173 ***
06/02/2019 09:18:43 *** training ***
06/02/2019 09:18:44 step: 5681, epoch: 172, batch: 4, loss: 0.0168234184384346, acc: 100.0, f1: 100.0, r: 0.7892844360117288
06/02/2019 09:18:44 step: 5686, epoch: 172, batch: 9, loss: 0.13158103823661804, acc: 95.3125, f1: 92.13667285095856, r: 0.667756659063921
06/02/2019 09:18:44 step: 5691, epoch: 172, batch: 14, loss: 0.034449778497219086, acc: 98.4375, f1: 94.70899470899471, r: 0.712562143141442
06/02/2019 09:18:45 step: 5696, epoch: 172, batch: 19, loss: 0.10238981246948242, acc: 96.875, f1: 98.02197802197803, r: 0.663775917080524
06/02/2019 09:18:45 step: 5701, epoch: 172, batch: 24, loss: 0.16271936893463135, acc: 98.4375, f1: 99.28193499622071, r: 0.7508706732583772
06/02/2019 09:18:45 step: 5706, epoch: 172, batch: 29, loss: 0.2310810685157776, acc: 95.3125, f1: 95.59974747474747, r: 0.7781345706556329
06/02/2019 09:18:46 *** evaluating ***
06/02/2019 09:18:46 step: 173, epoch: 172, acc: 62.39316239316239, f1: 30.957073614618924, r: 0.414568268714247
06/02/2019 09:18:46 *** epoch: 174 ***
06/02/2019 09:18:46 *** training ***
06/02/2019 09:18:46 step: 5714, epoch: 173, batch: 4, loss: 0.061272259801626205, acc: 98.4375, f1: 99.11483253588517, r: 0.8152428016128563
06/02/2019 09:18:46 step: 5719, epoch: 173, batch: 9, loss: 0.043856363743543625, acc: 100.0, f1: 100.0, r: 0.7249788311454903
06/02/2019 09:18:47 step: 5724, epoch: 173, batch: 14, loss: 0.10860834270715714, acc: 95.3125, f1: 91.89547906807275, r: 0.6080165682279121
06/02/2019 09:18:47 step: 5729, epoch: 173, batch: 19, loss: 0.11432197690010071, acc: 95.3125, f1: 90.35790598290599, r: 0.7159789434006142
06/02/2019 09:18:47 step: 5734, epoch: 173, batch: 24, loss: 0.12886814773082733, acc: 93.75, f1: 94.87190024751318, r: 0.7066712321315088
06/02/2019 09:18:48 step: 5739, epoch: 173, batch: 29, loss: 0.11778547614812851, acc: 95.3125, f1: 97.01680672268908, r: 0.7462219650301285
06/02/2019 09:18:48 *** evaluating ***
06/02/2019 09:18:48 step: 174, epoch: 173, acc: 62.82051282051282, f1: 31.57118874475891, r: 0.40979172226065225
06/02/2019 09:18:48 *** epoch: 175 ***
06/02/2019 09:18:48 *** training ***
06/02/2019 09:18:48 step: 5747, epoch: 174, batch: 4, loss: 0.06290582567453384, acc: 98.4375, f1: 99.11111111111111, r: 0.7079222599539688
06/02/2019 09:18:48 step: 5752, epoch: 174, batch: 9, loss: 0.08442410826683044, acc: 95.3125, f1: 93.58106805475227, r: 0.7751943230617476
06/02/2019 09:18:49 step: 5757, epoch: 174, batch: 14, loss: 0.05828990787267685, acc: 98.4375, f1: 99.20343779478041, r: 0.6907148515345807
06/02/2019 09:18:49 step: 5762, epoch: 174, batch: 19, loss: 0.025042371824383736, acc: 100.0, f1: 100.0, r: 0.7171808892031686
06/02/2019 09:18:50 step: 5767, epoch: 174, batch: 24, loss: 0.08730380237102509, acc: 95.3125, f1: 95.77380952380953, r: 0.7940813421833519
06/02/2019 09:18:50 step: 5772, epoch: 174, batch: 29, loss: 0.11491338908672333, acc: 95.3125, f1: 93.02771855010661, r: 0.7753137128568024
06/02/2019 09:18:50 *** evaluating ***
06/02/2019 09:18:50 step: 175, epoch: 174, acc: 62.39316239316239, f1: 31.941791047394496, r: 0.41070495056208345
06/02/2019 09:18:50 *** epoch: 176 ***
06/02/2019 09:18:50 *** training ***
06/02/2019 09:18:50 step: 5780, epoch: 175, batch: 4, loss: 0.10230611264705658, acc: 96.875, f1: 95.25870954442384, r: 0.721526848805066
06/02/2019 09:18:51 step: 5785, epoch: 175, batch: 9, loss: 0.06054411083459854, acc: 100.0, f1: 100.0, r: 0.7769991806786782
06/02/2019 09:18:51 step: 5790, epoch: 175, batch: 14, loss: 0.03407881408929825, acc: 100.0, f1: 100.0, r: 0.799629432665295
06/02/2019 09:18:51 step: 5795, epoch: 175, batch: 19, loss: 0.05918336659669876, acc: 96.875, f1: 97.47835738068812, r: 0.6979378822937745
06/02/2019 09:18:52 step: 5800, epoch: 175, batch: 24, loss: 0.05302863195538521, acc: 100.0, f1: 100.0, r: 0.7292779613664413
06/02/2019 09:18:52 step: 5805, epoch: 175, batch: 29, loss: 0.10345630347728729, acc: 96.875, f1: 95.14555492816363, r: 0.7612022820962159
06/02/2019 09:18:52 *** evaluating ***
06/02/2019 09:18:52 step: 176, epoch: 175, acc: 62.82051282051282, f1: 30.481357473544968, r: 0.4097069137235431
06/02/2019 09:18:52 *** epoch: 177 ***
06/02/2019 09:18:52 *** training ***
06/02/2019 09:18:53 step: 5813, epoch: 176, batch: 4, loss: 0.12075771391391754, acc: 96.875, f1: 95.53913519430762, r: 0.6572803412806474
06/02/2019 09:18:53 step: 5818, epoch: 176, batch: 9, loss: 0.09733732044696808, acc: 98.4375, f1: 98.57549857549857, r: 0.7113291419274326
06/02/2019 09:18:53 step: 5823, epoch: 176, batch: 14, loss: 0.12142565846443176, acc: 96.875, f1: 91.5843901757328, r: 0.6584464936680581
06/02/2019 09:18:54 step: 5828, epoch: 176, batch: 19, loss: 0.014933224767446518, acc: 100.0, f1: 100.0, r: 0.6801531252037513
06/02/2019 09:18:54 step: 5833, epoch: 176, batch: 24, loss: 0.21505597233772278, acc: 95.3125, f1: 94.91742979242979, r: 0.8056236766835989
06/02/2019 09:18:54 step: 5838, epoch: 176, batch: 29, loss: 0.048458367586135864, acc: 98.4375, f1: 99.19078742608154, r: 0.658149039243962
06/02/2019 09:18:55 *** evaluating ***
06/02/2019 09:18:55 step: 177, epoch: 176, acc: 61.111111111111114, f1: 29.281723754933196, r: 0.40988964646461373
06/02/2019 09:18:55 *** epoch: 178 ***
06/02/2019 09:18:55 *** training ***
06/02/2019 09:18:55 step: 5846, epoch: 177, batch: 4, loss: 0.05069125071167946, acc: 96.875, f1: 96.05352896307264, r: 0.7933924232729147
06/02/2019 09:18:55 step: 5851, epoch: 177, batch: 9, loss: 0.05543515831232071, acc: 98.4375, f1: 97.86666666666667, r: 0.5416217689035363
06/02/2019 09:18:56 step: 5856, epoch: 177, batch: 14, loss: 0.11030087620019913, acc: 98.4375, f1: 97.94941900205059, r: 0.6595592940226449
06/02/2019 09:18:56 step: 5861, epoch: 177, batch: 19, loss: 0.02259945683181286, acc: 100.0, f1: 100.0, r: 0.6558970549400063
06/02/2019 09:18:56 step: 5866, epoch: 177, batch: 24, loss: 0.10885133594274521, acc: 96.875, f1: 83.62545459319654, r: 0.6611885185775472
06/02/2019 09:18:57 step: 5871, epoch: 177, batch: 29, loss: 0.03705241531133652, acc: 100.0, f1: 100.0, r: 0.6238196546928574
06/02/2019 09:18:57 *** evaluating ***
06/02/2019 09:18:57 step: 178, epoch: 177, acc: 62.39316239316239, f1: 29.569552002153443, r: 0.41100184351995406
06/02/2019 09:18:57 *** epoch: 179 ***
06/02/2019 09:18:57 *** training ***
06/02/2019 09:18:57 step: 5879, epoch: 178, batch: 4, loss: 0.0673171654343605, acc: 96.875, f1: 94.75274725274726, r: 0.747046027503678
06/02/2019 09:18:58 step: 5884, epoch: 178, batch: 9, loss: 0.0637277290225029, acc: 96.875, f1: 84.16891284815813, r: 0.7821104816662485
06/02/2019 09:18:58 step: 5889, epoch: 178, batch: 14, loss: 0.052885234355926514, acc: 96.875, f1: 94.53061224489795, r: 0.7139451302032497
06/02/2019 09:18:58 step: 5894, epoch: 178, batch: 19, loss: 0.11269974708557129, acc: 95.3125, f1: 96.96775446775447, r: 0.807009178813117
06/02/2019 09:18:58 step: 5899, epoch: 178, batch: 24, loss: 0.049483831971883774, acc: 98.4375, f1: 99.21866751135043, r: 0.709700239005791
06/02/2019 09:18:59 step: 5904, epoch: 178, batch: 29, loss: 0.07441110163927078, acc: 96.875, f1: 98.19552349241583, r: 0.7524672928311833
06/02/2019 09:18:59 *** evaluating ***
06/02/2019 09:18:59 step: 179, epoch: 178, acc: 60.68376068376068, f1: 28.219456991344156, r: 0.4084476443644765
06/02/2019 09:18:59 *** epoch: 180 ***
06/02/2019 09:18:59 *** training ***
06/02/2019 09:18:59 step: 5912, epoch: 179, batch: 4, loss: 0.07000207901000977, acc: 98.4375, f1: 98.12115322319403, r: 0.7120875766041981
06/02/2019 09:19:00 step: 5917, epoch: 179, batch: 9, loss: 0.17984800040721893, acc: 93.75, f1: 94.94245524296676, r: 0.7787979958482895
06/02/2019 09:19:00 step: 5922, epoch: 179, batch: 14, loss: 0.08607270568609238, acc: 96.875, f1: 92.82501124606387, r: 0.8014481627266861
06/02/2019 09:19:00 step: 5927, epoch: 179, batch: 19, loss: 0.0433240607380867, acc: 98.4375, f1: 99.28063179294708, r: 0.6612665277844473
06/02/2019 09:19:01 step: 5932, epoch: 179, batch: 24, loss: 0.05509834364056587, acc: 100.0, f1: 100.0, r: 0.7065246116546409
06/02/2019 09:19:01 step: 5937, epoch: 179, batch: 29, loss: 0.1614014059305191, acc: 93.75, f1: 89.8584533282809, r: 0.7486401676525687
06/02/2019 09:19:01 *** evaluating ***
06/02/2019 09:19:01 step: 180, epoch: 179, acc: 62.82051282051282, f1: 28.799956028923194, r: 0.40725398763981313
06/02/2019 09:19:01 *** epoch: 181 ***
06/02/2019 09:19:01 *** training ***
06/02/2019 09:19:01 step: 5945, epoch: 180, batch: 4, loss: 0.03516995906829834, acc: 100.0, f1: 100.0, r: 0.7172829313561447
06/02/2019 09:19:02 step: 5950, epoch: 180, batch: 9, loss: 0.1310281753540039, acc: 93.75, f1: 96.00373482726424, r: 0.5819564257365185
06/02/2019 09:19:02 step: 5955, epoch: 180, batch: 14, loss: 0.17236235737800598, acc: 93.75, f1: 80.90902933008196, r: 0.6336255561276647
06/02/2019 09:19:02 step: 5960, epoch: 180, batch: 19, loss: 0.04706454277038574, acc: 98.4375, f1: 97.03703703703704, r: 0.8114501563244517
06/02/2019 09:19:03 step: 5965, epoch: 180, batch: 24, loss: 0.04030021280050278, acc: 100.0, f1: 100.0, r: 0.7812881569816146
06/02/2019 09:19:03 step: 5970, epoch: 180, batch: 29, loss: 0.10854720324277878, acc: 96.875, f1: 97.26212045856884, r: 0.710323271468612
06/02/2019 09:19:03 *** evaluating ***
06/02/2019 09:19:03 step: 181, epoch: 180, acc: 63.24786324786324, f1: 30.98067664565826, r: 0.4091458050879875
06/02/2019 09:19:03 *** epoch: 182 ***
06/02/2019 09:19:03 *** training ***
06/02/2019 09:19:04 step: 5978, epoch: 181, batch: 4, loss: 0.027926746755838394, acc: 100.0, f1: 100.0, r: 0.684207540928525
06/02/2019 09:19:04 step: 5983, epoch: 181, batch: 9, loss: 0.042983949184417725, acc: 98.4375, f1: 93.33333333333333, r: 0.7694020834159951
06/02/2019 09:19:04 step: 5988, epoch: 181, batch: 14, loss: 0.07646497339010239, acc: 95.3125, f1: 97.05614507443777, r: 0.7476769677620922
06/02/2019 09:19:04 step: 5993, epoch: 181, batch: 19, loss: 0.06298863887786865, acc: 98.4375, f1: 97.84126984126985, r: 0.71218198612411
06/02/2019 09:19:05 step: 5998, epoch: 181, batch: 24, loss: 0.05561760812997818, acc: 96.875, f1: 96.18055555555556, r: 0.7936895806985117
06/02/2019 09:19:05 step: 6003, epoch: 181, batch: 29, loss: 0.0768742486834526, acc: 96.875, f1: 96.13525870118163, r: 0.7439563540183332
06/02/2019 09:19:05 *** evaluating ***
06/02/2019 09:19:05 step: 182, epoch: 181, acc: 62.82051282051282, f1: 30.83481317055124, r: 0.4087587388238213
06/02/2019 09:19:05 *** epoch: 183 ***
06/02/2019 09:19:05 *** training ***
06/02/2019 09:19:05 step: 6011, epoch: 182, batch: 4, loss: 0.05645693093538284, acc: 98.4375, f1: 94.44444444444444, r: 0.7603346958022759
06/02/2019 09:19:06 step: 6016, epoch: 182, batch: 9, loss: 0.15275681018829346, acc: 93.75, f1: 94.05312166222984, r: 0.6704837327548077
06/02/2019 09:19:06 step: 6021, epoch: 182, batch: 14, loss: 0.1518498659133911, acc: 95.3125, f1: 93.6753219680049, r: 0.6905352024421968
06/02/2019 09:19:06 step: 6026, epoch: 182, batch: 19, loss: 0.07647158205509186, acc: 98.4375, f1: 99.23521913913127, r: 0.7198063445687279
06/02/2019 09:19:07 step: 6031, epoch: 182, batch: 24, loss: 0.110364630818367, acc: 95.3125, f1: 92.2337590006763, r: 0.6811674644347699
06/02/2019 09:19:07 step: 6036, epoch: 182, batch: 29, loss: 0.07716277241706848, acc: 96.875, f1: 95.88056680161944, r: 0.7830538348387703
06/02/2019 09:19:07 *** evaluating ***
06/02/2019 09:19:07 step: 183, epoch: 182, acc: 62.82051282051282, f1: 31.001468556388712, r: 0.4053540002229418
06/02/2019 09:19:07 *** epoch: 184 ***
06/02/2019 09:19:07 *** training ***
06/02/2019 09:19:07 step: 6044, epoch: 183, batch: 4, loss: 0.07843603193759918, acc: 96.875, f1: 98.42537695798566, r: 0.7454794366075237
06/02/2019 09:19:08 step: 6049, epoch: 183, batch: 9, loss: 0.15765152871608734, acc: 96.875, f1: 96.84445533502138, r: 0.6185828407388028
06/02/2019 09:19:08 step: 6054, epoch: 183, batch: 14, loss: 0.1400613784790039, acc: 96.875, f1: 96.36141636141636, r: 0.6817482928386627
06/02/2019 09:19:08 step: 6059, epoch: 183, batch: 19, loss: 0.04084482789039612, acc: 98.4375, f1: 98.78729080766433, r: 0.6768988042258691
06/02/2019 09:19:09 step: 6064, epoch: 183, batch: 24, loss: 0.05593617260456085, acc: 98.4375, f1: 98.67434153148439, r: 0.6792382096555089
06/02/2019 09:19:09 step: 6069, epoch: 183, batch: 29, loss: 0.05769208446145058, acc: 96.875, f1: 95.49486461251166, r: 0.7455580181351198
06/02/2019 09:19:09 *** evaluating ***
06/02/2019 09:19:09 step: 184, epoch: 183, acc: 60.256410256410255, f1: 26.237297496318114, r: 0.400077300799178
06/02/2019 09:19:09 *** epoch: 185 ***
06/02/2019 09:19:09 *** training ***
06/02/2019 09:19:10 step: 6077, epoch: 184, batch: 4, loss: 0.05803489312529564, acc: 98.4375, f1: 98.99355877616746, r: 0.7594043133553368
06/02/2019 09:19:10 step: 6082, epoch: 184, batch: 9, loss: 0.1111777052283287, acc: 98.4375, f1: 97.92008757525998, r: 0.7154386842708301
06/02/2019 09:19:10 step: 6087, epoch: 184, batch: 14, loss: 0.08249718695878983, acc: 95.3125, f1: 93.982683982684, r: 0.7119950033563284
06/02/2019 09:19:11 step: 6092, epoch: 184, batch: 19, loss: 0.014224188402295113, acc: 100.0, f1: 100.0, r: 0.7655723291252983
06/02/2019 09:19:11 step: 6097, epoch: 184, batch: 24, loss: 0.10742921382188797, acc: 95.3125, f1: 96.03278130671507, r: 0.7262223736384942
06/02/2019 09:19:11 step: 6102, epoch: 184, batch: 29, loss: 0.02952926978468895, acc: 98.4375, f1: 98.68131868131869, r: 0.8598161028551159
06/02/2019 09:19:11 *** evaluating ***
06/02/2019 09:19:12 step: 185, epoch: 184, acc: 62.39316239316239, f1: 30.48138399930679, r: 0.4026971231156274
06/02/2019 09:19:12 *** epoch: 186 ***
06/02/2019 09:19:12 *** training ***
06/02/2019 09:19:12 step: 6110, epoch: 185, batch: 4, loss: 0.0344853550195694, acc: 100.0, f1: 100.0, r: 0.6971877500321855
06/02/2019 09:19:12 step: 6115, epoch: 185, batch: 9, loss: 0.10517176985740662, acc: 95.3125, f1: 79.24414210128496, r: 0.6001708948009217
06/02/2019 09:19:13 step: 6120, epoch: 185, batch: 14, loss: 0.16134262084960938, acc: 92.1875, f1: 81.72816079836953, r: 0.5842712327744193
06/02/2019 09:19:13 step: 6125, epoch: 185, batch: 19, loss: 0.06256063282489777, acc: 96.875, f1: 95.49812030075188, r: 0.6992846102572461
06/02/2019 09:19:13 step: 6130, epoch: 185, batch: 24, loss: 0.12464988976716995, acc: 95.3125, f1: 81.8752209261223, r: 0.6987672350020753
06/02/2019 09:19:14 step: 6135, epoch: 185, batch: 29, loss: 0.09235391020774841, acc: 95.3125, f1: 92.80190239867659, r: 0.78009619763547
06/02/2019 09:19:14 *** evaluating ***
06/02/2019 09:19:14 step: 186, epoch: 185, acc: 62.82051282051282, f1: 31.472428902116402, r: 0.4008366348681053
06/02/2019 09:19:14 *** epoch: 187 ***
06/02/2019 09:19:14 *** training ***
06/02/2019 09:19:14 step: 6143, epoch: 186, batch: 4, loss: 0.08578839898109436, acc: 96.875, f1: 97.1825396825397, r: 0.7753650549505111
06/02/2019 09:19:15 step: 6148, epoch: 186, batch: 9, loss: 0.0425078459084034, acc: 100.0, f1: 100.0, r: 0.7233819599785968
06/02/2019 09:19:15 step: 6153, epoch: 186, batch: 14, loss: 0.03394584730267525, acc: 100.0, f1: 100.0, r: 0.7596387003944914
06/02/2019 09:19:15 step: 6158, epoch: 186, batch: 19, loss: 0.13443586230278015, acc: 92.1875, f1: 80.11128364389234, r: 0.6859784715134752
06/02/2019 09:19:16 step: 6163, epoch: 186, batch: 24, loss: 0.08374710381031036, acc: 98.4375, f1: 99.19078742608154, r: 0.7014055776809529
06/02/2019 09:19:16 step: 6168, epoch: 186, batch: 29, loss: 0.1250188648700714, acc: 96.875, f1: 95.73497110442925, r: 0.6433660182753433
06/02/2019 09:19:16 *** evaluating ***
06/02/2019 09:19:16 step: 187, epoch: 186, acc: 62.82051282051282, f1: 30.391318920730683, r: 0.3996028978339533
06/02/2019 09:19:16 *** epoch: 188 ***
06/02/2019 09:19:16 *** training ***
06/02/2019 09:19:17 step: 6176, epoch: 187, batch: 4, loss: 0.06988510489463806, acc: 96.875, f1: 98.02266081871345, r: 0.8039748095267694
06/02/2019 09:19:17 step: 6181, epoch: 187, batch: 9, loss: 0.0689612552523613, acc: 98.4375, f1: 92.38095238095238, r: 0.6435668526306277
06/02/2019 09:19:17 step: 6186, epoch: 187, batch: 14, loss: 0.02438890002667904, acc: 100.0, f1: 100.0, r: 0.7307217781081196
06/02/2019 09:19:18 step: 6191, epoch: 187, batch: 19, loss: 0.03593815863132477, acc: 100.0, f1: 100.0, r: 0.6674969702610638
06/02/2019 09:19:18 step: 6196, epoch: 187, batch: 24, loss: 0.0813288614153862, acc: 96.875, f1: 95.01133786848072, r: 0.7156499791528632
06/02/2019 09:19:18 step: 6201, epoch: 187, batch: 29, loss: 0.187437042593956, acc: 96.875, f1: 98.65288220551378, r: 0.7092122827577829
06/02/2019 09:19:18 *** evaluating ***
06/02/2019 09:19:19 step: 188, epoch: 187, acc: 61.965811965811966, f1: 27.29505369461631, r: 0.3978943035035428
06/02/2019 09:19:19 *** epoch: 189 ***
06/02/2019 09:19:19 *** training ***
06/02/2019 09:19:19 step: 6209, epoch: 188, batch: 4, loss: 0.036292701959609985, acc: 100.0, f1: 100.0, r: 0.6265121635474258
06/02/2019 09:19:19 step: 6214, epoch: 188, batch: 9, loss: 0.07223121076822281, acc: 96.875, f1: 97.77470714403565, r: 0.7350215001338455
06/02/2019 09:19:19 step: 6219, epoch: 188, batch: 14, loss: 0.08954747766256332, acc: 96.875, f1: 95.64814814814815, r: 0.7561361110931847
06/02/2019 09:19:20 step: 6224, epoch: 188, batch: 19, loss: 0.09326720237731934, acc: 95.3125, f1: 95.99012099012099, r: 0.7358367302109057
06/02/2019 09:19:20 step: 6229, epoch: 188, batch: 24, loss: 0.07112234085798264, acc: 96.875, f1: 96.7202502966879, r: 0.6903544038297151
06/02/2019 09:19:21 step: 6234, epoch: 188, batch: 29, loss: 0.10161271691322327, acc: 95.3125, f1: 93.15883190883191, r: 0.7025280448352729
06/02/2019 09:19:21 *** evaluating ***
06/02/2019 09:19:21 step: 189, epoch: 188, acc: 62.82051282051282, f1: 30.832003593569652, r: 0.4027995138751893
06/02/2019 09:19:21 *** epoch: 190 ***
06/02/2019 09:19:21 *** training ***
06/02/2019 09:19:21 step: 6242, epoch: 189, batch: 4, loss: 0.060898762196302414, acc: 98.4375, f1: 99.22876872029414, r: 0.6691053016529698
06/02/2019 09:19:21 step: 6247, epoch: 189, batch: 9, loss: 0.06321633607149124, acc: 100.0, f1: 100.0, r: 0.7024611308358738
06/02/2019 09:19:22 step: 6252, epoch: 189, batch: 14, loss: 0.1328345686197281, acc: 96.875, f1: 92.35209235209237, r: 0.6999682489330249
06/02/2019 09:19:22 step: 6257, epoch: 189, batch: 19, loss: 0.034836024045944214, acc: 100.0, f1: 100.0, r: 0.6699206662425012
06/02/2019 09:19:22 step: 6262, epoch: 189, batch: 24, loss: 0.10224650800228119, acc: 98.4375, f1: 99.00598955014655, r: 0.6926872896136952
06/02/2019 09:19:23 step: 6267, epoch: 189, batch: 29, loss: 0.15692204236984253, acc: 93.75, f1: 92.22105508870216, r: 0.7445953192385942
06/02/2019 09:19:23 *** evaluating ***
06/02/2019 09:19:23 step: 190, epoch: 189, acc: 61.965811965811966, f1: 29.112894940255167, r: 0.40389104082532556
06/02/2019 09:19:23 *** epoch: 191 ***
06/02/2019 09:19:23 *** training ***
06/02/2019 09:19:23 step: 6275, epoch: 190, batch: 4, loss: 0.12170377373695374, acc: 95.3125, f1: 79.32870370370371, r: 0.6587714029078716
06/02/2019 09:19:24 step: 6280, epoch: 190, batch: 9, loss: 0.07829774916172028, acc: 95.3125, f1: 83.61924479936903, r: 0.6808251948504026
06/02/2019 09:19:24 step: 6285, epoch: 190, batch: 14, loss: 0.11588505655527115, acc: 96.875, f1: 95.14059514059514, r: 0.6682116960798702
06/02/2019 09:19:24 step: 6290, epoch: 190, batch: 19, loss: 0.07829825580120087, acc: 96.875, f1: 97.8836884287385, r: 0.7517303510947149
06/02/2019 09:19:25 step: 6295, epoch: 190, batch: 24, loss: 0.04645787924528122, acc: 98.4375, f1: 97.79158040027606, r: 0.7291729894609104
06/02/2019 09:19:25 step: 6300, epoch: 190, batch: 29, loss: 0.1139758974313736, acc: 95.3125, f1: 93.82034632034632, r: 0.6874490564329311
06/02/2019 09:19:25 *** evaluating ***
06/02/2019 09:19:25 step: 191, epoch: 190, acc: 62.39316239316239, f1: 27.53346333292227, r: 0.40615557779291733
06/02/2019 09:19:25 *** epoch: 192 ***
06/02/2019 09:19:25 *** training ***
06/02/2019 09:19:26 step: 6308, epoch: 191, batch: 4, loss: 0.1103537380695343, acc: 95.3125, f1: 90.83675330597356, r: 0.7710920769236682
06/02/2019 09:19:26 step: 6313, epoch: 191, batch: 9, loss: 0.0993337407708168, acc: 95.3125, f1: 88.03357883899581, r: 0.7413073924004927
06/02/2019 09:19:26 step: 6318, epoch: 191, batch: 14, loss: 0.060195837169885635, acc: 98.4375, f1: 98.20728291316527, r: 0.637864825192747
06/02/2019 09:19:27 step: 6323, epoch: 191, batch: 19, loss: 0.07531921565532684, acc: 96.875, f1: 93.96331738437003, r: 0.714244643284903
06/02/2019 09:19:27 step: 6328, epoch: 191, batch: 24, loss: 0.07682126760482788, acc: 98.4375, f1: 95.40229885057472, r: 0.6903935257681214
06/02/2019 09:19:27 step: 6333, epoch: 191, batch: 29, loss: 0.054403744637966156, acc: 96.875, f1: 96.45833333333333, r: 0.7546134185411645
06/02/2019 09:19:28 *** evaluating ***
06/02/2019 09:19:28 step: 192, epoch: 191, acc: 63.67521367521367, f1: 32.25029327970504, r: 0.41505851714723413
06/02/2019 09:19:28 *** epoch: 193 ***
06/02/2019 09:19:28 *** training ***
06/02/2019 09:19:28 step: 6341, epoch: 192, batch: 4, loss: 0.14320668578147888, acc: 96.875, f1: 94.57164331184488, r: 0.7563440704961083
06/02/2019 09:19:28 step: 6346, epoch: 192, batch: 9, loss: 0.040374625474214554, acc: 100.0, f1: 100.0, r: 0.7994953632359818
06/02/2019 09:19:29 step: 6351, epoch: 192, batch: 14, loss: 0.07667914032936096, acc: 96.875, f1: 97.82986111111111, r: 0.7309156628352267
06/02/2019 09:19:29 step: 6356, epoch: 192, batch: 19, loss: 0.16103322803974152, acc: 95.3125, f1: 91.60583742498636, r: 0.7493083806163312
06/02/2019 09:19:29 step: 6361, epoch: 192, batch: 24, loss: 0.0685265064239502, acc: 98.4375, f1: 96.1111111111111, r: 0.795458455748373
06/02/2019 09:19:30 step: 6366, epoch: 192, batch: 29, loss: 0.061195746064186096, acc: 98.4375, f1: 95.10204081632652, r: 0.6795463139564137
06/02/2019 09:19:30 *** evaluating ***
06/02/2019 09:19:30 step: 193, epoch: 192, acc: 61.965811965811966, f1: 27.766865280220156, r: 0.4071073273920199
06/02/2019 09:19:30 *** epoch: 194 ***
06/02/2019 09:19:30 *** training ***
06/02/2019 09:19:30 step: 6374, epoch: 193, batch: 4, loss: 0.034705713391304016, acc: 100.0, f1: 100.0, r: 0.6694407064816512
06/02/2019 09:19:31 step: 6379, epoch: 193, batch: 9, loss: 0.10801057517528534, acc: 93.75, f1: 84.99807987711213, r: 0.5653664422434528
06/02/2019 09:19:31 step: 6384, epoch: 193, batch: 14, loss: 0.07398437708616257, acc: 98.4375, f1: 98.4265010351967, r: 0.7033791840629764
06/02/2019 09:19:32 step: 6389, epoch: 193, batch: 19, loss: 0.18811029195785522, acc: 93.75, f1: 90.33705730354157, r: 0.7452086325559248
06/02/2019 09:19:32 step: 6394, epoch: 193, batch: 24, loss: 0.14697732031345367, acc: 96.875, f1: 96.0138044579534, r: 0.7612142763943245
06/02/2019 09:19:32 step: 6399, epoch: 193, batch: 29, loss: 0.17620839178562164, acc: 95.3125, f1: 92.32763532763532, r: 0.7152424262831385
06/02/2019 09:19:32 *** evaluating ***
06/02/2019 09:19:33 step: 194, epoch: 193, acc: 61.53846153846154, f1: 30.212690090618544, r: 0.40902880999539853
06/02/2019 09:19:33 *** epoch: 195 ***
06/02/2019 09:19:33 *** training ***
06/02/2019 09:19:33 step: 6407, epoch: 194, batch: 4, loss: 0.08563929051160812, acc: 98.4375, f1: 97.77777777777779, r: 0.7962705380358721
06/02/2019 09:19:33 step: 6412, epoch: 194, batch: 9, loss: 0.04755314439535141, acc: 100.0, f1: 100.0, r: 0.7357360667842928
06/02/2019 09:19:33 step: 6417, epoch: 194, batch: 14, loss: 0.20949293673038483, acc: 95.3125, f1: 96.73611111111111, r: 0.7321468934552064
06/02/2019 09:19:34 step: 6422, epoch: 194, batch: 19, loss: 0.02034015767276287, acc: 100.0, f1: 100.0, r: 0.7887934398980851
06/02/2019 09:19:34 step: 6427, epoch: 194, batch: 24, loss: 0.08184070885181427, acc: 98.4375, f1: 96.11111111111111, r: 0.7882006615939618
06/02/2019 09:19:34 step: 6432, epoch: 194, batch: 29, loss: 0.02743130922317505, acc: 100.0, f1: 100.0, r: 0.6682448194107041
06/02/2019 09:19:35 *** evaluating ***
06/02/2019 09:19:35 step: 195, epoch: 194, acc: 61.965811965811966, f1: 28.054317686670625, r: 0.4074760206844506
06/02/2019 09:19:35 *** epoch: 196 ***
06/02/2019 09:19:35 *** training ***
06/02/2019 09:19:35 step: 6440, epoch: 195, batch: 4, loss: 0.06441082060337067, acc: 96.875, f1: 97.89141414141415, r: 0.7408837079140295
06/02/2019 09:19:35 step: 6445, epoch: 195, batch: 9, loss: 0.10362009704113007, acc: 96.875, f1: 96.67903525046383, r: 0.6568141308639066
06/02/2019 09:19:36 step: 6450, epoch: 195, batch: 14, loss: 0.028806563466787338, acc: 98.4375, f1: 99.31773879142301, r: 0.8010445030196747
06/02/2019 09:19:36 step: 6455, epoch: 195, batch: 19, loss: 0.09605289250612259, acc: 96.875, f1: 92.75793650793652, r: 0.6758116159517475
06/02/2019 09:19:36 step: 6460, epoch: 195, batch: 24, loss: 0.07955480366945267, acc: 98.4375, f1: 99.20694459329118, r: 0.5973485544752813
06/02/2019 09:19:37 step: 6465, epoch: 195, batch: 29, loss: 0.06657004356384277, acc: 98.4375, f1: 98.9648033126294, r: 0.5904919730538248
06/02/2019 09:19:37 *** evaluating ***
06/02/2019 09:19:37 step: 196, epoch: 195, acc: 61.53846153846154, f1: 28.62749995102936, r: 0.4066070119941467
06/02/2019 09:19:37 *** epoch: 197 ***
06/02/2019 09:19:37 *** training ***
06/02/2019 09:19:37 step: 6473, epoch: 196, batch: 4, loss: 0.0869034007191658, acc: 96.875, f1: 92.5, r: 0.7154408939341242
06/02/2019 09:19:38 step: 6478, epoch: 196, batch: 9, loss: 0.11272148787975311, acc: 96.875, f1: 96.41959423065414, r: 0.7736885568985136
06/02/2019 09:19:38 step: 6483, epoch: 196, batch: 14, loss: 0.06124712526798248, acc: 98.4375, f1: 97.667638483965, r: 0.6812820151190946
06/02/2019 09:19:39 step: 6488, epoch: 196, batch: 19, loss: 0.04840654134750366, acc: 98.4375, f1: 96.84210526315789, r: 0.8062742128946094
06/02/2019 09:19:39 step: 6493, epoch: 196, batch: 24, loss: 0.04845857992768288, acc: 98.4375, f1: 96.57142857142857, r: 0.7374341553101469
06/02/2019 09:19:39 step: 6498, epoch: 196, batch: 29, loss: 0.10646548122167587, acc: 98.4375, f1: 97.46657283603096, r: 0.6533863077583143
06/02/2019 09:19:39 *** evaluating ***
06/02/2019 09:19:40 step: 197, epoch: 196, acc: 60.68376068376068, f1: 27.22671739896149, r: 0.4075146663782187
06/02/2019 09:19:40 *** epoch: 198 ***
06/02/2019 09:19:40 *** training ***
06/02/2019 09:19:40 step: 6506, epoch: 197, batch: 4, loss: 0.10439146310091019, acc: 96.875, f1: 97.01912744465936, r: 0.5903948794129419
06/02/2019 09:19:40 step: 6511, epoch: 197, batch: 9, loss: 0.09226187318563461, acc: 95.3125, f1: 93.79417683765509, r: 0.6454340371116206
06/02/2019 09:19:41 step: 6516, epoch: 197, batch: 14, loss: 0.09597557038068771, acc: 98.4375, f1: 99.37689969604864, r: 0.8225349146088512
06/02/2019 09:19:41 step: 6521, epoch: 197, batch: 19, loss: 0.07230406254529953, acc: 96.875, f1: 94.27087198515771, r: 0.7693226421454804
06/02/2019 09:19:41 step: 6526, epoch: 197, batch: 24, loss: 0.11449220776557922, acc: 98.4375, f1: 99.19289749798224, r: 0.8426762801937592
06/02/2019 09:19:42 step: 6531, epoch: 197, batch: 29, loss: 0.08115588128566742, acc: 96.875, f1: 97.541928721174, r: 0.7112084015859401
06/02/2019 09:19:42 *** evaluating ***
06/02/2019 09:19:42 step: 198, epoch: 197, acc: 60.68376068376068, f1: 28.395500753810367, r: 0.40748434587678567
06/02/2019 09:19:42 *** epoch: 199 ***
06/02/2019 09:19:42 *** training ***
06/02/2019 09:19:42 step: 6539, epoch: 198, batch: 4, loss: 0.07074244320392609, acc: 96.875, f1: 98.62202380952381, r: 0.8156300614498333
06/02/2019 09:19:43 step: 6544, epoch: 198, batch: 9, loss: 0.06365660578012466, acc: 96.875, f1: 96.35204081632654, r: 0.7606916272993085
06/02/2019 09:19:43 step: 6549, epoch: 198, batch: 14, loss: 0.08804307878017426, acc: 96.875, f1: 96.74362674362675, r: 0.7169567433976425
06/02/2019 09:19:43 step: 6554, epoch: 198, batch: 19, loss: 0.1373194307088852, acc: 95.3125, f1: 94.12332552358127, r: 0.7798363351226392
06/02/2019 09:19:44 step: 6559, epoch: 198, batch: 24, loss: 0.19812484085559845, acc: 92.1875, f1: 90.151171579743, r: 0.6287415611470224
06/02/2019 09:19:44 step: 6564, epoch: 198, batch: 29, loss: 0.05450264737010002, acc: 98.4375, f1: 99.25490196078431, r: 0.8142262579439357
06/02/2019 09:19:44 *** evaluating ***
06/02/2019 09:19:44 step: 199, epoch: 198, acc: 60.68376068376068, f1: 28.501807863352823, r: 0.4045622819151974
06/02/2019 09:19:44 *** epoch: 200 ***
06/02/2019 09:19:44 *** training ***
06/02/2019 09:19:45 step: 6572, epoch: 199, batch: 4, loss: 0.019513249397277832, acc: 100.0, f1: 100.0, r: 0.7162047304572812
06/02/2019 09:19:45 step: 6577, epoch: 199, batch: 9, loss: 0.03782925382256508, acc: 100.0, f1: 100.0, r: 0.8006004122215216
06/02/2019 09:19:45 step: 6582, epoch: 199, batch: 14, loss: 0.09668789803981781, acc: 95.3125, f1: 91.81818181818183, r: 0.7430001501881118
06/02/2019 09:19:46 step: 6587, epoch: 199, batch: 19, loss: 0.1004309430718422, acc: 95.3125, f1: 94.842555105713, r: 0.7419862375090943
06/02/2019 09:19:46 step: 6592, epoch: 199, batch: 24, loss: 0.044223301112651825, acc: 98.4375, f1: 94.9874686716792, r: 0.6684993950228927
06/02/2019 09:19:46 step: 6597, epoch: 199, batch: 29, loss: 0.021349195390939713, acc: 100.0, f1: 100.0, r: 0.7873328107641742
06/02/2019 09:19:46 *** evaluating ***
06/02/2019 09:19:47 step: 200, epoch: 199, acc: 61.53846153846154, f1: 29.704066438810532, r: 0.4057281050433187
06/02/2019 09:19:47 *** epoch: 201 ***
06/02/2019 09:19:47 *** training ***
06/02/2019 09:19:47 step: 6605, epoch: 200, batch: 4, loss: 0.012249715626239777, acc: 100.0, f1: 100.0, r: 0.5804981946845229
06/02/2019 09:19:47 step: 6610, epoch: 200, batch: 9, loss: 0.08471506088972092, acc: 96.875, f1: 97.75539275539276, r: 0.8040988494633117
06/02/2019 09:19:48 step: 6615, epoch: 200, batch: 14, loss: 0.13109439611434937, acc: 95.3125, f1: 92.2420634920635, r: 0.7984518759485318
06/02/2019 09:19:48 step: 6620, epoch: 200, batch: 19, loss: 0.20066334307193756, acc: 92.1875, f1: 90.09595959595961, r: 0.5876384905823483
06/02/2019 09:19:48 step: 6625, epoch: 200, batch: 24, loss: 0.1258748173713684, acc: 92.1875, f1: 90.82326566197534, r: 0.694251897517971
06/02/2019 09:19:49 step: 6630, epoch: 200, batch: 29, loss: 0.11448073387145996, acc: 95.3125, f1: 96.88426652712367, r: 0.7671975512917073
06/02/2019 09:19:49 *** evaluating ***
06/02/2019 09:19:49 step: 201, epoch: 200, acc: 61.53846153846154, f1: 31.576201329483332, r: 0.4046585612115976
06/02/2019 09:19:49 *** epoch: 202 ***
06/02/2019 09:19:49 *** training ***
06/02/2019 09:19:49 step: 6638, epoch: 201, batch: 4, loss: 0.06942332535982132, acc: 96.875, f1: 98.62318840579711, r: 0.7873123190003185
06/02/2019 09:19:50 step: 6643, epoch: 201, batch: 9, loss: 0.07410091161727905, acc: 98.4375, f1: 94.61697722567288, r: 0.7060316019650781
06/02/2019 09:19:50 step: 6648, epoch: 201, batch: 14, loss: 0.055328939110040665, acc: 98.4375, f1: 94.9874686716792, r: 0.6943836541158094
06/02/2019 09:19:50 step: 6653, epoch: 201, batch: 19, loss: 0.03195223957300186, acc: 100.0, f1: 100.0, r: 0.8002787228702423
06/02/2019 09:19:51 step: 6658, epoch: 201, batch: 24, loss: 0.056758251041173935, acc: 100.0, f1: 100.0, r: 0.6793984985564925
06/02/2019 09:19:51 step: 6663, epoch: 201, batch: 29, loss: 0.07866627722978592, acc: 95.3125, f1: 91.01384563913838, r: 0.6926763863150082
06/02/2019 09:19:51 *** evaluating ***
06/02/2019 09:19:51 step: 202, epoch: 201, acc: 63.24786324786324, f1: 31.98285170711641, r: 0.4065313253983319
06/02/2019 09:19:51 *** epoch: 203 ***
06/02/2019 09:19:51 *** training ***
06/02/2019 09:19:52 step: 6671, epoch: 202, batch: 4, loss: 0.12343766540288925, acc: 93.75, f1: 91.26007326007326, r: 0.6636913163963916
06/02/2019 09:19:52 step: 6676, epoch: 202, batch: 9, loss: 0.09399676322937012, acc: 96.875, f1: 96.82379024484287, r: 0.8052648779037519
06/02/2019 09:19:52 step: 6681, epoch: 202, batch: 14, loss: 0.07337066531181335, acc: 98.4375, f1: 99.00598955014655, r: 0.6579087895751842
06/02/2019 09:19:53 step: 6686, epoch: 202, batch: 19, loss: 0.013320263475179672, acc: 100.0, f1: 100.0, r: 0.7437989227403421
06/02/2019 09:19:53 step: 6691, epoch: 202, batch: 24, loss: 0.0885993018746376, acc: 95.3125, f1: 82.79677644566421, r: 0.6757089868760293
06/02/2019 09:19:53 step: 6696, epoch: 202, batch: 29, loss: 0.03162683546543121, acc: 100.0, f1: 100.0, r: 0.6983570012203237
06/02/2019 09:19:54 *** evaluating ***
06/02/2019 09:19:54 step: 203, epoch: 202, acc: 61.965811965811966, f1: 31.529313949277952, r: 0.4053740397571025
06/02/2019 09:19:54 *** epoch: 204 ***
06/02/2019 09:19:54 *** training ***
06/02/2019 09:19:54 step: 6704, epoch: 203, batch: 4, loss: 0.042757898569107056, acc: 98.4375, f1: 98.62098685628098, r: 0.6852613267013932
06/02/2019 09:19:54 step: 6709, epoch: 203, batch: 9, loss: 0.06799020618200302, acc: 98.4375, f1: 98.0045351473923, r: 0.7034902675337429
06/02/2019 09:19:55 step: 6714, epoch: 203, batch: 14, loss: 0.03693263977766037, acc: 98.4375, f1: 98.88888888888889, r: 0.8051596941630518
06/02/2019 09:19:55 step: 6719, epoch: 203, batch: 19, loss: 0.028221875429153442, acc: 98.4375, f1: 98.62700228832952, r: 0.7672593045799276
06/02/2019 09:19:55 step: 6724, epoch: 203, batch: 24, loss: 0.05225425213575363, acc: 98.4375, f1: 98.12987012987013, r: 0.7228999843981908
06/02/2019 09:19:56 step: 6729, epoch: 203, batch: 29, loss: 0.1091446578502655, acc: 96.875, f1: 96.32822477650065, r: 0.7583267068121848
06/02/2019 09:19:56 *** evaluating ***
06/02/2019 09:19:56 step: 204, epoch: 203, acc: 61.53846153846154, f1: 31.49644889283546, r: 0.40808606857921365
06/02/2019 09:19:56 *** epoch: 205 ***
06/02/2019 09:19:56 *** training ***
06/02/2019 09:19:56 step: 6737, epoch: 204, batch: 4, loss: 0.03381182625889778, acc: 100.0, f1: 100.0, r: 0.6083348175210436
06/02/2019 09:19:57 step: 6742, epoch: 204, batch: 9, loss: 0.07096970826387405, acc: 96.875, f1: 96.98988122380736, r: 0.8005994944852532
06/02/2019 09:19:57 step: 6747, epoch: 204, batch: 14, loss: 0.09108854830265045, acc: 96.875, f1: 98.26388888888889, r: 0.7661023507159974
06/02/2019 09:19:57 step: 6752, epoch: 204, batch: 19, loss: 0.12933236360549927, acc: 95.3125, f1: 94.9203365556749, r: 0.7300103697666775
06/02/2019 09:19:57 step: 6757, epoch: 204, batch: 24, loss: 0.0704098492860794, acc: 95.3125, f1: 95.49512987012987, r: 0.7631974725705121
06/02/2019 09:19:58 step: 6762, epoch: 204, batch: 29, loss: 0.04783041775226593, acc: 100.0, f1: 100.0, r: 0.8310988007321489
06/02/2019 09:19:58 *** evaluating ***
06/02/2019 09:19:58 step: 205, epoch: 204, acc: 62.82051282051282, f1: 30.39811251603445, r: 0.4053734964570158
06/02/2019 09:19:58 *** epoch: 206 ***
06/02/2019 09:19:58 *** training ***
06/02/2019 09:19:58 step: 6770, epoch: 205, batch: 4, loss: 0.0595308393239975, acc: 98.4375, f1: 93.96825396825398, r: 0.6068622824093848
06/02/2019 09:19:59 step: 6775, epoch: 205, batch: 9, loss: 0.03584664314985275, acc: 100.0, f1: 100.0, r: 0.6911253152676347
06/02/2019 09:19:59 step: 6780, epoch: 205, batch: 14, loss: 0.03133634850382805, acc: 100.0, f1: 100.0, r: 0.830743922803352
06/02/2019 09:19:59 step: 6785, epoch: 205, batch: 19, loss: 0.08771788328886032, acc: 96.875, f1: 95.8983666061706, r: 0.7093930138637938
06/02/2019 09:20:00 step: 6790, epoch: 205, batch: 24, loss: 0.045575156807899475, acc: 96.875, f1: 96.87146892655367, r: 0.8543505920030936
06/02/2019 09:20:00 step: 6795, epoch: 205, batch: 29, loss: 0.2196996808052063, acc: 95.3125, f1: 92.0405179615706, r: 0.7694502690256407
06/02/2019 09:20:00 *** evaluating ***
06/02/2019 09:20:00 step: 206, epoch: 205, acc: 62.39316239316239, f1: 31.497394356940454, r: 0.40636468308208934
06/02/2019 09:20:00 *** epoch: 207 ***
06/02/2019 09:20:00 *** training ***
06/02/2019 09:20:01 step: 6803, epoch: 206, batch: 4, loss: 0.04601288214325905, acc: 98.4375, f1: 98.68131868131869, r: 0.7115427999537292
06/02/2019 09:20:01 step: 6808, epoch: 206, batch: 9, loss: 0.05237168073654175, acc: 100.0, f1: 100.0, r: 0.8183601093556843
06/02/2019 09:20:01 step: 6813, epoch: 206, batch: 14, loss: 0.12308651208877563, acc: 93.75, f1: 75.83333333333333, r: 0.7422815309495113
06/02/2019 09:20:02 step: 6818, epoch: 206, batch: 19, loss: 0.05572028085589409, acc: 98.4375, f1: 99.32386747802569, r: 0.7363698156381007
06/02/2019 09:20:02 step: 6823, epoch: 206, batch: 24, loss: 0.19255517423152924, acc: 92.1875, f1: 91.70495459824728, r: 0.760010004409289
06/02/2019 09:20:02 step: 6828, epoch: 206, batch: 29, loss: 0.056796737015247345, acc: 98.4375, f1: 98.63155712212316, r: 0.7173775545593885
06/02/2019 09:20:02 *** evaluating ***
06/02/2019 09:20:03 step: 207, epoch: 206, acc: 62.39316239316239, f1: 31.016988829488827, r: 0.4035786752416047
06/02/2019 09:20:03 *** epoch: 208 ***
06/02/2019 09:20:03 *** training ***
06/02/2019 09:20:03 step: 6836, epoch: 207, batch: 4, loss: 0.20724192261695862, acc: 95.3125, f1: 95.37567750677508, r: 0.7234712931344275
06/02/2019 09:20:03 step: 6841, epoch: 207, batch: 9, loss: 0.030469253659248352, acc: 100.0, f1: 100.0, r: 0.701530595153465
06/02/2019 09:20:04 step: 6846, epoch: 207, batch: 14, loss: 0.06609007716178894, acc: 96.875, f1: 97.39086568354861, r: 0.6907863042317244
06/02/2019 09:20:04 step: 6851, epoch: 207, batch: 19, loss: 0.10024493932723999, acc: 96.875, f1: 97.25274725274727, r: 0.7209870827754997
06/02/2019 09:20:04 step: 6856, epoch: 207, batch: 24, loss: 0.010331906378269196, acc: 100.0, f1: 100.0, r: 0.7377181212979701
06/02/2019 09:20:04 step: 6861, epoch: 207, batch: 29, loss: 0.07369425147771835, acc: 96.875, f1: 84.3956043956044, r: 0.7270180406790705
06/02/2019 09:20:05 *** evaluating ***
06/02/2019 09:20:05 step: 208, epoch: 207, acc: 63.24786324786324, f1: 32.467751174092456, r: 0.40462863631885365
06/02/2019 09:20:05 *** epoch: 209 ***
06/02/2019 09:20:05 *** training ***
06/02/2019 09:20:05 step: 6869, epoch: 208, batch: 4, loss: 0.06106773391366005, acc: 98.4375, f1: 97.55639097744361, r: 0.8329612929047068
06/02/2019 09:20:06 step: 6874, epoch: 208, batch: 9, loss: 0.06331481784582138, acc: 98.4375, f1: 98.42118665648077, r: 0.6905665211426111
06/02/2019 09:20:06 step: 6879, epoch: 208, batch: 14, loss: 0.029838653281331062, acc: 100.0, f1: 100.0, r: 0.77757993090457
06/02/2019 09:20:06 step: 6884, epoch: 208, batch: 19, loss: 0.031077755615115166, acc: 100.0, f1: 100.0, r: 0.5626714342240066
06/02/2019 09:20:07 step: 6889, epoch: 208, batch: 24, loss: 0.03211294114589691, acc: 100.0, f1: 100.0, r: 0.7267884252270156
06/02/2019 09:20:07 step: 6894, epoch: 208, batch: 29, loss: 0.06255442649126053, acc: 98.4375, f1: 98.66690686362817, r: 0.7134203395172833
06/02/2019 09:20:07 *** evaluating ***
06/02/2019 09:20:07 step: 209, epoch: 208, acc: 60.68376068376068, f1: 30.963088326585208, r: 0.40198667393098186
06/02/2019 09:20:07 *** epoch: 210 ***
06/02/2019 09:20:07 *** training ***
06/02/2019 09:20:08 step: 6902, epoch: 209, batch: 4, loss: 0.09853512793779373, acc: 95.3125, f1: 95.75163398692811, r: 0.8028319136358628
06/02/2019 09:20:08 step: 6907, epoch: 209, batch: 9, loss: 0.08973293006420135, acc: 95.3125, f1: 81.27409432672592, r: 0.6147983189388149
06/02/2019 09:20:08 step: 6912, epoch: 209, batch: 14, loss: 0.042723771184682846, acc: 100.0, f1: 100.0, r: 0.7492607237764889
06/02/2019 09:20:09 step: 6917, epoch: 209, batch: 19, loss: 0.11300639808177948, acc: 96.875, f1: 94.19603385120627, r: 0.7041665969229085
06/02/2019 09:20:09 step: 6922, epoch: 209, batch: 24, loss: 0.10996405780315399, acc: 95.3125, f1: 80.73648007590133, r: 0.7240744064641333
06/02/2019 09:20:09 step: 6927, epoch: 209, batch: 29, loss: 0.07970572263002396, acc: 98.4375, f1: 98.23232323232322, r: 0.7967354902850982
06/02/2019 09:20:09 *** evaluating ***
06/02/2019 09:20:10 step: 210, epoch: 209, acc: 60.68376068376068, f1: 28.5185266004285, r: 0.40044597854770503
06/02/2019 09:20:10 *** epoch: 211 ***
06/02/2019 09:20:10 *** training ***
06/02/2019 09:20:10 step: 6935, epoch: 210, batch: 4, loss: 0.015314653515815735, acc: 100.0, f1: 100.0, r: 0.6852894978092873
06/02/2019 09:20:10 step: 6940, epoch: 210, batch: 9, loss: 0.04606662318110466, acc: 98.4375, f1: 99.16891284815813, r: 0.7308572205270242
06/02/2019 09:20:11 step: 6945, epoch: 210, batch: 14, loss: 0.00836152583360672, acc: 100.0, f1: 100.0, r: 0.7019738813200569
06/02/2019 09:20:11 step: 6950, epoch: 210, batch: 19, loss: 0.11124565452337265, acc: 96.875, f1: 95.28520499108734, r: 0.7839602220034549
06/02/2019 09:20:11 step: 6955, epoch: 210, batch: 24, loss: 0.06892676651477814, acc: 96.875, f1: 97.71825396825398, r: 0.7364201873802279
06/02/2019 09:20:12 step: 6960, epoch: 210, batch: 29, loss: 0.033435337245464325, acc: 98.4375, f1: 96.42857142857143, r: 0.7757688542618858
06/02/2019 09:20:12 *** evaluating ***
06/02/2019 09:20:12 step: 211, epoch: 210, acc: 61.53846153846154, f1: 29.104030599432217, r: 0.39875050492886605
06/02/2019 09:20:12 *** epoch: 212 ***
06/02/2019 09:20:12 *** training ***
06/02/2019 09:20:12 step: 6968, epoch: 211, batch: 4, loss: 0.2563229203224182, acc: 92.1875, f1: 88.14484126984125, r: 0.7014463358991407
06/02/2019 09:20:13 step: 6973, epoch: 211, batch: 9, loss: 0.03335950896143913, acc: 100.0, f1: 100.0, r: 0.7114210650801709
06/02/2019 09:20:13 step: 6978, epoch: 211, batch: 14, loss: 0.08495642989873886, acc: 96.875, f1: 96.4777021919879, r: 0.6715212127230826
06/02/2019 09:20:13 step: 6983, epoch: 211, batch: 19, loss: 0.07333197444677353, acc: 96.875, f1: 94.80002990207072, r: 0.6943373948622967
06/02/2019 09:20:13 step: 6988, epoch: 211, batch: 24, loss: 0.1674901247024536, acc: 95.3125, f1: 94.14458726391575, r: 0.7734827148982869
06/02/2019 09:20:14 step: 6993, epoch: 211, batch: 29, loss: 0.0657988116145134, acc: 95.3125, f1: 89.6931216931217, r: 0.7130203100256373
06/02/2019 09:20:14 *** evaluating ***
06/02/2019 09:20:14 step: 212, epoch: 211, acc: 61.111111111111114, f1: 30.13755377213354, r: 0.40316932794044846
06/02/2019 09:20:14 *** epoch: 213 ***
06/02/2019 09:20:14 *** training ***
06/02/2019 09:20:14 step: 7001, epoch: 212, batch: 4, loss: 0.09234729409217834, acc: 98.4375, f1: 96.73469387755101, r: 0.6842505961279197
06/02/2019 09:20:15 step: 7006, epoch: 212, batch: 9, loss: 0.1434219628572464, acc: 93.75, f1: 91.46403242147923, r: 0.7739261021278977
06/02/2019 09:20:15 step: 7011, epoch: 212, batch: 14, loss: 0.12868452072143555, acc: 93.75, f1: 94.63031853057429, r: 0.7987087721006216
06/02/2019 09:20:15 step: 7016, epoch: 212, batch: 19, loss: 0.05189017951488495, acc: 96.875, f1: 94.56247456247456, r: 0.7452085353293325
06/02/2019 09:20:16 step: 7021, epoch: 212, batch: 24, loss: 0.09868304431438446, acc: 96.875, f1: 98.38392857142857, r: 0.7744064539580295
06/02/2019 09:20:16 step: 7026, epoch: 212, batch: 29, loss: 0.1459946632385254, acc: 92.1875, f1: 78.88180272108843, r: 0.7996721261610045
06/02/2019 09:20:16 *** evaluating ***
06/02/2019 09:20:16 step: 213, epoch: 212, acc: 61.111111111111114, f1: 28.818860976027306, r: 0.4022159152629818
06/02/2019 09:20:16 *** epoch: 214 ***
06/02/2019 09:20:16 *** training ***
06/02/2019 09:20:17 step: 7034, epoch: 213, batch: 4, loss: 0.11475528031587601, acc: 93.75, f1: 92.60608296968141, r: 0.7637922732621004
06/02/2019 09:20:17 step: 7039, epoch: 213, batch: 9, loss: 0.0450713075697422, acc: 98.4375, f1: 99.10625620655412, r: 0.7740311902346524
06/02/2019 09:20:17 step: 7044, epoch: 213, batch: 14, loss: 0.11717646569013596, acc: 93.75, f1: 94.4857913642908, r: 0.736218756815325
06/02/2019 09:20:18 step: 7049, epoch: 213, batch: 19, loss: 0.02969992160797119, acc: 100.0, f1: 100.0, r: 0.7383155999926733
06/02/2019 09:20:18 step: 7054, epoch: 213, batch: 24, loss: 0.10421624779701233, acc: 95.3125, f1: 85.25067750677506, r: 0.7232364616888539
06/02/2019 09:20:18 step: 7059, epoch: 213, batch: 29, loss: 0.02723267301917076, acc: 100.0, f1: 100.0, r: 0.7253551076377841
06/02/2019 09:20:18 *** evaluating ***
06/02/2019 09:20:19 step: 214, epoch: 213, acc: 61.53846153846154, f1: 26.30667281876712, r: 0.40472268172700043
06/02/2019 09:20:19 *** epoch: 215 ***
06/02/2019 09:20:19 *** training ***
06/02/2019 09:20:19 step: 7067, epoch: 214, batch: 4, loss: 0.04272332787513733, acc: 98.4375, f1: 95.37037037037037, r: 0.7573539378961714
06/02/2019 09:20:19 step: 7072, epoch: 214, batch: 9, loss: 0.03189389407634735, acc: 98.4375, f1: 99.26525841195625, r: 0.730739708363558
06/02/2019 09:20:19 step: 7077, epoch: 214, batch: 14, loss: 0.04344923049211502, acc: 96.875, f1: 97.13712853562477, r: 0.738617900872562
06/02/2019 09:20:20 step: 7082, epoch: 214, batch: 19, loss: 0.06395195424556732, acc: 98.4375, f1: 98.89012208657047, r: 0.6598799103380795
06/02/2019 09:20:20 step: 7087, epoch: 214, batch: 24, loss: 0.018543347716331482, acc: 100.0, f1: 100.0, r: 0.6977407648243746
06/02/2019 09:20:20 step: 7092, epoch: 214, batch: 29, loss: 0.07885938882827759, acc: 96.875, f1: 98.4775641025641, r: 0.7621585037125418
06/02/2019 09:20:20 *** evaluating ***
06/02/2019 09:20:20 step: 215, epoch: 214, acc: 61.111111111111114, f1: 26.373486398112757, r: 0.40689650660848015
06/02/2019 09:20:20 *** epoch: 216 ***
06/02/2019 09:20:20 *** training ***
06/02/2019 09:20:21 step: 7100, epoch: 215, batch: 4, loss: 0.06569388508796692, acc: 95.3125, f1: 90.90057001821708, r: 0.7645441434184133
06/02/2019 09:20:21 step: 7105, epoch: 215, batch: 9, loss: 0.08287748694419861, acc: 96.875, f1: 95.51485551485551, r: 0.6793811499243895
06/02/2019 09:20:21 step: 7110, epoch: 215, batch: 14, loss: 0.12627148628234863, acc: 95.3125, f1: 82.3525641025641, r: 0.6449830809589351
06/02/2019 09:20:22 step: 7115, epoch: 215, batch: 19, loss: 0.04254733398556709, acc: 100.0, f1: 100.0, r: 0.6235482020814188
06/02/2019 09:20:22 step: 7120, epoch: 215, batch: 24, loss: 0.14156505465507507, acc: 93.75, f1: 81.88215725161537, r: 0.6510059326537654
06/02/2019 09:20:23 step: 7125, epoch: 215, batch: 29, loss: 0.05567917227745056, acc: 96.875, f1: 92.59803921568628, r: 0.7732930784334991
06/02/2019 09:20:23 *** evaluating ***
06/02/2019 09:20:23 step: 216, epoch: 215, acc: 61.111111111111114, f1: 26.337361677344205, r: 0.4039616278783217
06/02/2019 09:20:23 *** epoch: 217 ***
06/02/2019 09:20:23 *** training ***
06/02/2019 09:20:23 step: 7133, epoch: 216, batch: 4, loss: 0.19021213054656982, acc: 93.75, f1: 93.58187134502924, r: 0.8156267372847669
06/02/2019 09:20:23 step: 7138, epoch: 216, batch: 9, loss: 0.04362053796648979, acc: 98.4375, f1: 99.30118798043326, r: 0.791065037788059
06/02/2019 09:20:24 step: 7143, epoch: 216, batch: 14, loss: 0.0781557634472847, acc: 100.0, f1: 100.0, r: 0.6359060058807966
06/02/2019 09:20:24 step: 7148, epoch: 216, batch: 19, loss: 0.04169883579015732, acc: 100.0, f1: 100.0, r: 0.7717913011577586
06/02/2019 09:20:24 step: 7153, epoch: 216, batch: 24, loss: 0.18634293973445892, acc: 96.875, f1: 94.65249662618082, r: 0.7503038257902173
06/02/2019 09:20:25 step: 7158, epoch: 216, batch: 29, loss: 0.06498578935861588, acc: 98.4375, f1: 98.06763285024154, r: 0.7505819051648293
06/02/2019 09:20:25 *** evaluating ***
06/02/2019 09:20:25 step: 217, epoch: 216, acc: 61.965811965811966, f1: 27.217805561018938, r: 0.40510061982871326
06/02/2019 09:20:25 *** epoch: 218 ***
06/02/2019 09:20:25 *** training ***
06/02/2019 09:20:25 step: 7166, epoch: 217, batch: 4, loss: 0.024750396609306335, acc: 100.0, f1: 100.0, r: 0.7164041120865652
06/02/2019 09:20:26 step: 7171, epoch: 217, batch: 9, loss: 0.025661494582891464, acc: 98.4375, f1: 97.20730397422128, r: 0.6960328290168729
06/02/2019 09:20:26 step: 7176, epoch: 217, batch: 14, loss: 0.01285259798169136, acc: 100.0, f1: 100.0, r: 0.5768928145674546
06/02/2019 09:20:26 step: 7181, epoch: 217, batch: 19, loss: 0.013453010469675064, acc: 100.0, f1: 100.0, r: 0.6891851600899562
06/02/2019 09:20:27 step: 7186, epoch: 217, batch: 24, loss: 0.1226954534649849, acc: 95.3125, f1: 96.06430606430607, r: 0.7002756347644632
06/02/2019 09:20:27 step: 7191, epoch: 217, batch: 29, loss: 0.06014001742005348, acc: 98.4375, f1: 96.6137566137566, r: 0.7996809091991954
06/02/2019 09:20:27 *** evaluating ***
06/02/2019 09:20:27 step: 218, epoch: 217, acc: 60.256410256410255, f1: 25.721063886661323, r: 0.4016891420308903
06/02/2019 09:20:27 *** epoch: 219 ***
06/02/2019 09:20:27 *** training ***
06/02/2019 09:20:28 step: 7199, epoch: 218, batch: 4, loss: 0.07910817861557007, acc: 96.875, f1: 97.48706320134892, r: 0.8211249517093078
06/02/2019 09:20:28 step: 7204, epoch: 218, batch: 9, loss: 0.07997894287109375, acc: 96.875, f1: 95.32356532356533, r: 0.7642950876898068
06/02/2019 09:20:28 step: 7209, epoch: 218, batch: 14, loss: 0.1072450652718544, acc: 98.4375, f1: 97.57236227824464, r: 0.7090445963770063
06/02/2019 09:20:29 step: 7214, epoch: 218, batch: 19, loss: 0.043020639568567276, acc: 98.4375, f1: 95.71428571428571, r: 0.7568128554050253
06/02/2019 09:20:29 step: 7219, epoch: 218, batch: 24, loss: 0.06600583344697952, acc: 98.4375, f1: 96.76470588235294, r: 0.7877347753956156
06/02/2019 09:20:29 step: 7224, epoch: 218, batch: 29, loss: 0.07429637014865875, acc: 96.875, f1: 97.15463607258312, r: 0.7144424599245915
06/02/2019 09:20:29 *** evaluating ***
06/02/2019 09:20:29 step: 219, epoch: 218, acc: 60.68376068376068, f1: 26.59628176191885, r: 0.4007863881725372
06/02/2019 09:20:29 *** epoch: 220 ***
06/02/2019 09:20:29 *** training ***
06/02/2019 09:20:30 step: 7232, epoch: 219, batch: 4, loss: 0.0971539318561554, acc: 95.3125, f1: 93.67557212384799, r: 0.8254344155859994
06/02/2019 09:20:30 step: 7237, epoch: 219, batch: 9, loss: 0.06580424308776855, acc: 96.875, f1: 97.74789915966387, r: 0.7042057108935537
06/02/2019 09:20:30 step: 7242, epoch: 219, batch: 14, loss: 0.04986990988254547, acc: 100.0, f1: 100.0, r: 0.7697696887720977
06/02/2019 09:20:31 step: 7247, epoch: 219, batch: 19, loss: 0.02651156112551689, acc: 100.0, f1: 100.0, r: 0.8110938791922216
06/02/2019 09:20:31 step: 7252, epoch: 219, batch: 24, loss: 0.038724616169929504, acc: 100.0, f1: 100.0, r: 0.7904847844129439
06/02/2019 09:20:31 step: 7257, epoch: 219, batch: 29, loss: 0.08070316165685654, acc: 96.875, f1: 96.79914070891515, r: 0.7172853250592826
06/02/2019 09:20:31 *** evaluating ***
06/02/2019 09:20:32 step: 220, epoch: 219, acc: 62.39316239316239, f1: 27.36933904528764, r: 0.4029997735031655
06/02/2019 09:20:32 *** epoch: 221 ***
06/02/2019 09:20:32 *** training ***
06/02/2019 09:20:32 step: 7265, epoch: 220, batch: 4, loss: 0.038881927728652954, acc: 96.875, f1: 92.01680672268907, r: 0.6691009217968324
06/02/2019 09:20:32 step: 7270, epoch: 220, batch: 9, loss: 0.09838836640119553, acc: 98.4375, f1: 98.88888888888889, r: 0.7457942159689699
06/02/2019 09:20:33 step: 7275, epoch: 220, batch: 14, loss: 0.07983902841806412, acc: 96.875, f1: 95.57703081232492, r: 0.6882553294595319
06/02/2019 09:20:33 step: 7280, epoch: 220, batch: 19, loss: 0.023220520466566086, acc: 98.4375, f1: 98.89992360580597, r: 0.6867441907709615
06/02/2019 09:20:33 step: 7285, epoch: 220, batch: 24, loss: 0.10719038546085358, acc: 95.3125, f1: 92.38030713640471, r: 0.7951295239853269
06/02/2019 09:20:33 step: 7290, epoch: 220, batch: 29, loss: 0.03523142635822296, acc: 98.4375, f1: 99.27272727272727, r: 0.8087375471878286
06/02/2019 09:20:34 *** evaluating ***
06/02/2019 09:20:34 step: 221, epoch: 220, acc: 61.111111111111114, f1: 26.20184752911914, r: 0.4014905489660513
06/02/2019 09:20:34 *** epoch: 222 ***
06/02/2019 09:20:34 *** training ***
06/02/2019 09:20:34 step: 7298, epoch: 221, batch: 4, loss: 0.07820503413677216, acc: 98.4375, f1: 97.55639097744361, r: 0.7601889123973019
06/02/2019 09:20:34 step: 7303, epoch: 221, batch: 9, loss: 0.041321657598018646, acc: 98.4375, f1: 93.19727891156462, r: 0.6529921495479764
06/02/2019 09:20:35 step: 7308, epoch: 221, batch: 14, loss: 0.05672648176550865, acc: 98.4375, f1: 98.63523573200992, r: 0.7887788586639254
06/02/2019 09:20:35 step: 7313, epoch: 221, batch: 19, loss: 0.04010281711816788, acc: 98.4375, f1: 98.43175692232296, r: 0.6871921804695298
06/02/2019 09:20:35 step: 7318, epoch: 221, batch: 24, loss: 0.04855524003505707, acc: 100.0, f1: 100.0, r: 0.7777113213456757
06/02/2019 09:20:36 step: 7323, epoch: 221, batch: 29, loss: 0.03305218368768692, acc: 98.4375, f1: 98.1111111111111, r: 0.7712126561068455
06/02/2019 09:20:36 *** evaluating ***
06/02/2019 09:20:36 step: 222, epoch: 221, acc: 61.965811965811966, f1: 27.51980034545824, r: 0.40012723603367806
06/02/2019 09:20:36 *** epoch: 223 ***
06/02/2019 09:20:36 *** training ***
06/02/2019 09:20:36 step: 7331, epoch: 222, batch: 4, loss: 0.04758113622665405, acc: 98.4375, f1: 98.46819846819848, r: 0.7213264076520106
06/02/2019 09:20:37 step: 7336, epoch: 222, batch: 9, loss: 0.05620693042874336, acc: 96.875, f1: 98.47027972027972, r: 0.7632571986556314
06/02/2019 09:20:37 step: 7341, epoch: 222, batch: 14, loss: 0.03987129405140877, acc: 100.0, f1: 100.0, r: 0.6687073297491075
06/02/2019 09:20:37 step: 7346, epoch: 222, batch: 19, loss: 0.07627378404140472, acc: 96.875, f1: 95.53467365967366, r: 0.7326278646733946
06/02/2019 09:20:38 step: 7351, epoch: 222, batch: 24, loss: 0.0920960083603859, acc: 93.75, f1: 81.46621148459384, r: 0.7186953986097668
06/02/2019 09:20:38 step: 7356, epoch: 222, batch: 29, loss: 0.05126439034938812, acc: 100.0, f1: 100.0, r: 0.636536567689468
06/02/2019 09:20:38 *** evaluating ***
06/02/2019 09:20:38 step: 223, epoch: 222, acc: 60.68376068376068, f1: 28.417719863821368, r: 0.40545720303380467
06/02/2019 09:20:38 *** epoch: 224 ***
06/02/2019 09:20:38 *** training ***
06/02/2019 09:20:39 step: 7364, epoch: 223, batch: 4, loss: 0.0431041456758976, acc: 100.0, f1: 100.0, r: 0.7038550020666079
06/02/2019 09:20:39 step: 7369, epoch: 223, batch: 9, loss: 0.06332512199878693, acc: 98.4375, f1: 99.06432748538012, r: 0.8159198541028555
06/02/2019 09:20:39 step: 7374, epoch: 223, batch: 14, loss: 0.08168762922286987, acc: 96.875, f1: 98.2685284640172, r: 0.685349021185535
06/02/2019 09:20:40 step: 7379, epoch: 223, batch: 19, loss: 0.08146712929010391, acc: 95.3125, f1: 93.55983302411873, r: 0.6934072313292319
06/02/2019 09:20:40 step: 7384, epoch: 223, batch: 24, loss: 0.0754879042506218, acc: 98.4375, f1: 87.06896551724138, r: 0.7670102055218655
06/02/2019 09:20:40 step: 7389, epoch: 223, batch: 29, loss: 0.12750926613807678, acc: 96.875, f1: 97.79541446208113, r: 0.6387821627656234
06/02/2019 09:20:40 *** evaluating ***
06/02/2019 09:20:40 step: 224, epoch: 223, acc: 61.111111111111114, f1: 28.46663566612415, r: 0.40524833602694654
06/02/2019 09:20:40 *** epoch: 225 ***
06/02/2019 09:20:40 *** training ***
06/02/2019 09:20:41 step: 7397, epoch: 224, batch: 4, loss: 0.0728057473897934, acc: 96.875, f1: 85.1576305930586, r: 0.6666165286007106
06/02/2019 09:20:41 step: 7402, epoch: 224, batch: 9, loss: 0.008016929030418396, acc: 100.0, f1: 100.0, r: 0.746254449212469
06/02/2019 09:20:41 step: 7407, epoch: 224, batch: 14, loss: 0.1302911341190338, acc: 95.3125, f1: 92.41755181604806, r: 0.642459546747938
06/02/2019 09:20:42 step: 7412, epoch: 224, batch: 19, loss: 0.04839010164141655, acc: 98.4375, f1: 87.09677419354838, r: 0.6436921058389942
06/02/2019 09:20:42 step: 7417, epoch: 224, batch: 24, loss: 0.04957358539104462, acc: 98.4375, f1: 98.14814814814814, r: 0.758788852821592
06/02/2019 09:20:42 step: 7422, epoch: 224, batch: 29, loss: 0.03660716116428375, acc: 98.4375, f1: 97.12121212121212, r: 0.7456166264499109
06/02/2019 09:20:43 *** evaluating ***
06/02/2019 09:20:43 step: 225, epoch: 224, acc: 62.39316239316239, f1: 30.24986185814432, r: 0.40189258076850043
06/02/2019 09:20:43 *** epoch: 226 ***
06/02/2019 09:20:43 *** training ***
06/02/2019 09:20:43 step: 7430, epoch: 225, batch: 4, loss: 0.09041066467761993, acc: 96.875, f1: 97.24603174603175, r: 0.6590206164071877
06/02/2019 09:20:43 step: 7435, epoch: 225, batch: 9, loss: 0.07944031804800034, acc: 96.875, f1: 95.03113845219109, r: 0.7388266131274911
06/02/2019 09:20:44 step: 7440, epoch: 225, batch: 14, loss: 0.026156865060329437, acc: 100.0, f1: 100.0, r: 0.8083650818952961
06/02/2019 09:20:44 step: 7445, epoch: 225, batch: 19, loss: 0.056322067975997925, acc: 98.4375, f1: 99.06896551724138, r: 0.8050227724467742
06/02/2019 09:20:44 step: 7450, epoch: 225, batch: 24, loss: 0.11054659634828568, acc: 96.875, f1: 96.11638361638362, r: 0.7904732930417474
06/02/2019 09:20:45 step: 7455, epoch: 225, batch: 29, loss: 0.1496260166168213, acc: 93.75, f1: 95.03553050722863, r: 0.6556421556492998
06/02/2019 09:20:45 *** evaluating ***
06/02/2019 09:20:45 step: 226, epoch: 225, acc: 61.111111111111114, f1: 28.529460735343086, r: 0.40744809150489547
06/02/2019 09:20:45 *** epoch: 227 ***
06/02/2019 09:20:45 *** training ***
06/02/2019 09:20:45 step: 7463, epoch: 226, batch: 4, loss: 0.044385433197021484, acc: 98.4375, f1: 99.06910132474042, r: 0.651147996651165
06/02/2019 09:20:46 step: 7468, epoch: 226, batch: 9, loss: 0.0530642569065094, acc: 98.4375, f1: 86.11111111111111, r: 0.683732182292644
06/02/2019 09:20:46 step: 7473, epoch: 226, batch: 14, loss: 0.028737585991621017, acc: 100.0, f1: 100.0, r: 0.7057195412049795
06/02/2019 09:20:46 step: 7478, epoch: 226, batch: 19, loss: 0.05901793763041496, acc: 98.4375, f1: 99.33081674673987, r: 0.7341209650659632
06/02/2019 09:20:47 step: 7483, epoch: 226, batch: 24, loss: 0.05841118097305298, acc: 96.875, f1: 96.93747230693045, r: 0.7270653360367454
06/02/2019 09:20:47 step: 7488, epoch: 226, batch: 29, loss: 0.04118671640753746, acc: 98.4375, f1: 98.9175331149409, r: 0.7621477244916547
06/02/2019 09:20:47 *** evaluating ***
06/02/2019 09:20:47 step: 227, epoch: 226, acc: 62.39316239316239, f1: 31.546766157120743, r: 0.4055525454503267
06/02/2019 09:20:47 *** epoch: 228 ***
06/02/2019 09:20:47 *** training ***
06/02/2019 09:20:48 step: 7496, epoch: 227, batch: 4, loss: 0.06042522192001343, acc: 98.4375, f1: 98.17056766209309, r: 0.7342996121241172
06/02/2019 09:20:48 step: 7501, epoch: 227, batch: 9, loss: 0.04672582447528839, acc: 98.4375, f1: 98.38383838383838, r: 0.5901175394225422
06/02/2019 09:20:48 step: 7506, epoch: 227, batch: 14, loss: 0.029225066304206848, acc: 100.0, f1: 100.0, r: 0.6877253486500883
06/02/2019 09:20:49 step: 7511, epoch: 227, batch: 19, loss: 0.04008961841464043, acc: 98.4375, f1: 99.06692406692407, r: 0.8114635756815939
06/02/2019 09:20:49 step: 7516, epoch: 227, batch: 24, loss: 0.035811249166727066, acc: 98.4375, f1: 99.06142167011733, r: 0.6279570892879558
06/02/2019 09:20:49 step: 7521, epoch: 227, batch: 29, loss: 0.16150856018066406, acc: 95.3125, f1: 92.007062007062, r: 0.7444460395786731
06/02/2019 09:20:50 *** evaluating ***
06/02/2019 09:20:50 step: 228, epoch: 227, acc: 61.53846153846154, f1: 27.330040437925835, r: 0.40331522533217884
06/02/2019 09:20:50 *** epoch: 229 ***
06/02/2019 09:20:50 *** training ***
06/02/2019 09:20:50 step: 7529, epoch: 228, batch: 4, loss: 0.0965743288397789, acc: 96.875, f1: 92.31829573934837, r: 0.7389017751684617
06/02/2019 09:20:50 step: 7534, epoch: 228, batch: 9, loss: 0.02798859030008316, acc: 100.0, f1: 100.0, r: 0.7149377645518344
06/02/2019 09:20:51 step: 7539, epoch: 228, batch: 14, loss: 0.09174449741840363, acc: 98.4375, f1: 98.7780772686433, r: 0.6826232171970142
06/02/2019 09:20:51 step: 7544, epoch: 228, batch: 19, loss: 0.0649414211511612, acc: 96.875, f1: 94.64028035456607, r: 0.7612826224299872
06/02/2019 09:20:51 step: 7549, epoch: 228, batch: 24, loss: 0.10445620864629745, acc: 98.4375, f1: 99.32235442439524, r: 0.7003886196944739
06/02/2019 09:20:52 step: 7554, epoch: 228, batch: 29, loss: 0.011616189032793045, acc: 100.0, f1: 100.0, r: 0.5878502168969801
06/02/2019 09:20:52 *** evaluating ***
06/02/2019 09:20:52 step: 229, epoch: 228, acc: 61.111111111111114, f1: 26.161263113534726, r: 0.4036124254652711
06/02/2019 09:20:52 *** epoch: 230 ***
06/02/2019 09:20:52 *** training ***
06/02/2019 09:20:52 step: 7562, epoch: 229, batch: 4, loss: 0.16745775938034058, acc: 93.75, f1: 87.34122584732341, r: 0.7741014193170463
06/02/2019 09:20:53 step: 7567, epoch: 229, batch: 9, loss: 0.04340899735689163, acc: 100.0, f1: 100.0, r: 0.7031902212279969
06/02/2019 09:20:53 step: 7572, epoch: 229, batch: 14, loss: 0.04100780934095383, acc: 98.4375, f1: 96.04395604395604, r: 0.6768598711710182
06/02/2019 09:20:53 step: 7577, epoch: 229, batch: 19, loss: 0.07052885740995407, acc: 96.875, f1: 95.7869142351901, r: 0.7108808654090903
06/02/2019 09:20:54 step: 7582, epoch: 229, batch: 24, loss: 0.08110621571540833, acc: 98.4375, f1: 97.38095238095238, r: 0.7590033725364516
06/02/2019 09:20:54 step: 7587, epoch: 229, batch: 29, loss: 0.10779713839292526, acc: 96.875, f1: 74.375, r: 0.7543346825974848
06/02/2019 09:20:54 *** evaluating ***
06/02/2019 09:20:54 step: 230, epoch: 229, acc: 61.53846153846154, f1: 27.23919520638444, r: 0.4046022260705422
06/02/2019 09:20:54 *** epoch: 231 ***
06/02/2019 09:20:54 *** training ***
06/02/2019 09:20:55 step: 7595, epoch: 230, batch: 4, loss: 0.07349742949008942, acc: 98.4375, f1: 98.79862700228833, r: 0.7738953947496876
06/02/2019 09:20:55 step: 7600, epoch: 230, batch: 9, loss: 0.013004977256059647, acc: 100.0, f1: 100.0, r: 0.7807500202742808
06/02/2019 09:20:55 step: 7605, epoch: 230, batch: 14, loss: 0.05819133296608925, acc: 95.3125, f1: 93.01242236024845, r: 0.7213587945568402
06/02/2019 09:20:56 step: 7610, epoch: 230, batch: 19, loss: 0.08736460655927658, acc: 95.3125, f1: 77.20409088333618, r: 0.740324729941075
06/02/2019 09:20:56 step: 7615, epoch: 230, batch: 24, loss: 0.04676872864365578, acc: 98.4375, f1: 97.75132275132275, r: 0.8320699096191634
06/02/2019 09:20:56 step: 7620, epoch: 230, batch: 29, loss: 0.08520126342773438, acc: 96.875, f1: 85.28325123152709, r: 0.721484524719206
06/02/2019 09:20:57 *** evaluating ***
06/02/2019 09:20:57 step: 231, epoch: 230, acc: 62.82051282051282, f1: 31.847839332583426, r: 0.4034869965967914
06/02/2019 09:20:57 *** epoch: 232 ***
06/02/2019 09:20:57 *** training ***
06/02/2019 09:20:57 step: 7628, epoch: 231, batch: 4, loss: 0.0155889131128788, acc: 100.0, f1: 100.0, r: 0.8261624813964792
06/02/2019 09:20:57 step: 7633, epoch: 231, batch: 9, loss: 0.03975962847471237, acc: 98.4375, f1: 98.01587301587303, r: 0.7725761326083245
06/02/2019 09:20:58 step: 7638, epoch: 231, batch: 14, loss: 0.08283139020204544, acc: 96.875, f1: 97.67482517482517, r: 0.7125423662838857
06/02/2019 09:20:58 step: 7643, epoch: 231, batch: 19, loss: 0.0401630699634552, acc: 100.0, f1: 100.0, r: 0.7765706892566197
06/02/2019 09:20:58 step: 7648, epoch: 231, batch: 24, loss: 0.08647355437278748, acc: 95.3125, f1: 93.78828211126347, r: 0.6972080101850715
06/02/2019 09:20:59 step: 7653, epoch: 231, batch: 29, loss: 0.09845340251922607, acc: 96.875, f1: 96.25102335306417, r: 0.7144567612848878
06/02/2019 09:20:59 *** evaluating ***
06/02/2019 09:20:59 step: 232, epoch: 231, acc: 62.82051282051282, f1: 31.507151124338623, r: 0.4030297186949694
06/02/2019 09:20:59 *** epoch: 233 ***
06/02/2019 09:20:59 *** training ***
06/02/2019 09:20:59 step: 7661, epoch: 232, batch: 4, loss: 0.12549462914466858, acc: 96.875, f1: 84.59610564873724, r: 0.6509991740280141
06/02/2019 09:20:59 step: 7666, epoch: 232, batch: 9, loss: 0.03010948747396469, acc: 100.0, f1: 100.0, r: 0.65112552595551
06/02/2019 09:21:00 step: 7671, epoch: 232, batch: 14, loss: 0.20516107976436615, acc: 93.75, f1: 95.06944444444446, r: 0.734933302450506
06/02/2019 09:21:00 step: 7676, epoch: 232, batch: 19, loss: 0.06576155871152878, acc: 96.875, f1: 97.2442031652558, r: 0.8392798500684109
06/02/2019 09:21:00 step: 7681, epoch: 232, batch: 24, loss: 0.05173134803771973, acc: 95.3125, f1: 96.07486263736263, r: 0.7344393226297337
06/02/2019 09:21:01 step: 7686, epoch: 232, batch: 29, loss: 0.09791447222232819, acc: 96.875, f1: 83.54312354312354, r: 0.673837696709886
06/02/2019 09:21:01 *** evaluating ***
06/02/2019 09:21:01 step: 233, epoch: 232, acc: 62.82051282051282, f1: 31.058299370799375, r: 0.40134833418217164
06/02/2019 09:21:01 *** epoch: 234 ***
06/02/2019 09:21:01 *** training ***
06/02/2019 09:21:02 step: 7694, epoch: 233, batch: 4, loss: 0.06453409045934677, acc: 98.4375, f1: 99.32386747802569, r: 0.7583185424428427
06/02/2019 09:21:02 step: 7699, epoch: 233, batch: 9, loss: 0.05075771361589432, acc: 96.875, f1: 86.27976190476191, r: 0.7868369623613435
06/02/2019 09:21:02 step: 7704, epoch: 233, batch: 14, loss: 0.06303548812866211, acc: 96.875, f1: 93.50438179706472, r: 0.643236964416194
06/02/2019 09:21:03 step: 7709, epoch: 233, batch: 19, loss: 0.08625221997499466, acc: 96.875, f1: 97.22791828054987, r: 0.7494615092037546
06/02/2019 09:21:03 step: 7714, epoch: 233, batch: 24, loss: 0.118588387966156, acc: 93.75, f1: 90.69538926681784, r: 0.6309566683111449
06/02/2019 09:21:03 step: 7719, epoch: 233, batch: 29, loss: 0.06639770418405533, acc: 96.875, f1: 98.51851851851852, r: 0.6816068190234885
06/02/2019 09:21:03 *** evaluating ***
06/02/2019 09:21:03 step: 234, epoch: 233, acc: 62.82051282051282, f1: 31.507151124338623, r: 0.40010701105253477
06/02/2019 09:21:03 *** epoch: 235 ***
06/02/2019 09:21:03 *** training ***
06/02/2019 09:21:04 step: 7727, epoch: 234, batch: 4, loss: 0.044079121202230453, acc: 100.0, f1: 100.0, r: 0.7349935416508132
06/02/2019 09:21:04 step: 7732, epoch: 234, batch: 9, loss: 0.012728109955787659, acc: 100.0, f1: 100.0, r: 0.6699627766987263
06/02/2019 09:21:04 step: 7737, epoch: 234, batch: 14, loss: 0.017779942601919174, acc: 100.0, f1: 100.0, r: 0.7445210802109684
06/02/2019 09:21:05 step: 7742, epoch: 234, batch: 19, loss: 0.11153603345155716, acc: 95.3125, f1: 95.45673076923077, r: 0.7679946478009326
06/02/2019 09:21:05 step: 7747, epoch: 234, batch: 24, loss: 0.06508458405733109, acc: 96.875, f1: 93.05125558995529, r: 0.7370632770750004
06/02/2019 09:21:05 step: 7752, epoch: 234, batch: 29, loss: 0.033187225461006165, acc: 98.4375, f1: 94.7454844006568, r: 0.6812576191010957
06/02/2019 09:21:06 *** evaluating ***
06/02/2019 09:21:06 step: 235, epoch: 234, acc: 63.67521367521367, f1: 31.85500263240274, r: 0.40207581913462664
06/02/2019 09:21:06 *** epoch: 236 ***
06/02/2019 09:21:06 *** training ***
06/02/2019 09:21:06 step: 7760, epoch: 235, batch: 4, loss: 0.030404429882764816, acc: 100.0, f1: 100.0, r: 0.744899858441565
06/02/2019 09:21:06 step: 7765, epoch: 235, batch: 9, loss: 0.04470597952604294, acc: 96.875, f1: 95.82792207792208, r: 0.7503581426177077
06/02/2019 09:21:07 step: 7770, epoch: 235, batch: 14, loss: 0.02142912708222866, acc: 100.0, f1: 100.0, r: 0.7178179757835322
06/02/2019 09:21:07 step: 7775, epoch: 235, batch: 19, loss: 0.014056447893381119, acc: 100.0, f1: 100.0, r: 0.777677242629519
06/02/2019 09:21:07 step: 7780, epoch: 235, batch: 24, loss: 0.11107592284679413, acc: 95.3125, f1: 92.66026726896293, r: 0.7102662515460082
06/02/2019 09:21:08 step: 7785, epoch: 235, batch: 29, loss: 0.07948534935712814, acc: 96.875, f1: 97.61973180076629, r: 0.6173753629579046
06/02/2019 09:21:08 *** evaluating ***
06/02/2019 09:21:08 step: 236, epoch: 235, acc: 61.965811965811966, f1: 29.66833583255277, r: 0.39873814054894663
06/02/2019 09:21:08 *** epoch: 237 ***
06/02/2019 09:21:08 *** training ***
06/02/2019 09:21:08 step: 7793, epoch: 236, batch: 4, loss: 0.04025685042142868, acc: 98.4375, f1: 99.27626137303557, r: 0.7579855821196361
06/02/2019 09:21:09 step: 7798, epoch: 236, batch: 9, loss: 0.04271123558282852, acc: 98.4375, f1: 98.9352262644188, r: 0.6108421318636699
06/02/2019 09:21:09 step: 7803, epoch: 236, batch: 14, loss: 0.08677070587873459, acc: 95.3125, f1: 95.72871572871573, r: 0.7080244444966375
06/02/2019 09:21:09 step: 7808, epoch: 236, batch: 19, loss: 0.00810784101486206, acc: 100.0, f1: 100.0, r: 0.8284296167901357
06/02/2019 09:21:10 step: 7813, epoch: 236, batch: 24, loss: 0.07019814848899841, acc: 96.875, f1: 96.7298714357538, r: 0.7813442022401874
06/02/2019 09:21:10 step: 7818, epoch: 236, batch: 29, loss: 0.1098359078168869, acc: 93.75, f1: 87.65393336821907, r: 0.6967545714078522
06/02/2019 09:21:10 *** evaluating ***
06/02/2019 09:21:10 step: 237, epoch: 236, acc: 61.965811965811966, f1: 30.11750995989494, r: 0.4003219587744288
06/02/2019 09:21:10 *** epoch: 238 ***
06/02/2019 09:21:10 *** training ***
06/02/2019 09:21:11 step: 7826, epoch: 237, batch: 4, loss: 0.055921174585819244, acc: 98.4375, f1: 98.14921920185078, r: 0.6938938964550818
06/02/2019 09:21:11 step: 7831, epoch: 237, batch: 9, loss: 0.025668945163488388, acc: 98.4375, f1: 99.34169278996865, r: 0.7460481739676856
06/02/2019 09:21:11 step: 7836, epoch: 237, batch: 14, loss: 0.07844258844852448, acc: 96.875, f1: 97.26430976430976, r: 0.7571175008054963
06/02/2019 09:21:12 step: 7841, epoch: 237, batch: 19, loss: 0.04957716539502144, acc: 98.4375, f1: 98.88888888888889, r: 0.8235398695091484
06/02/2019 09:21:12 step: 7846, epoch: 237, batch: 24, loss: 0.048146773129701614, acc: 98.4375, f1: 98.59767891682785, r: 0.7883990440375964
06/02/2019 09:21:12 step: 7851, epoch: 237, batch: 29, loss: 0.09140343964099884, acc: 95.3125, f1: 96.30458635066931, r: 0.6849036516150999
06/02/2019 09:21:12 *** evaluating ***
06/02/2019 09:21:13 step: 238, epoch: 237, acc: 62.39316239316239, f1: 30.362217553222358, r: 0.40170494308820404
06/02/2019 09:21:13 *** epoch: 239 ***
06/02/2019 09:21:13 *** training ***
06/02/2019 09:21:13 step: 7859, epoch: 238, batch: 4, loss: 0.14047133922576904, acc: 92.1875, f1: 76.92649976657329, r: 0.7330897558328581
06/02/2019 09:21:13 step: 7864, epoch: 238, batch: 9, loss: 0.03460637852549553, acc: 100.0, f1: 100.0, r: 0.646514318080659
06/02/2019 09:21:13 step: 7869, epoch: 238, batch: 14, loss: 0.050857238471508026, acc: 98.4375, f1: 96.65024630541872, r: 0.7080558488729182
06/02/2019 09:21:14 step: 7874, epoch: 238, batch: 19, loss: 0.12040931731462479, acc: 95.3125, f1: 94.27433601143001, r: 0.652944407314254
06/02/2019 09:21:14 step: 7879, epoch: 238, batch: 24, loss: 0.08457837253808975, acc: 96.875, f1: 97.01178451178451, r: 0.80887913130795
06/02/2019 09:21:14 step: 7884, epoch: 238, batch: 29, loss: 0.062491223216056824, acc: 98.4375, f1: 99.16891284815813, r: 0.7622098330481677
06/02/2019 09:21:15 *** evaluating ***
06/02/2019 09:21:15 step: 239, epoch: 238, acc: 62.39316239316239, f1: 30.05223390103905, r: 0.4061661933471018
06/02/2019 09:21:15 *** epoch: 240 ***
06/02/2019 09:21:15 *** training ***
06/02/2019 09:21:15 step: 7892, epoch: 239, batch: 4, loss: 0.021658506244421005, acc: 100.0, f1: 100.0, r: 0.7674006801661348
06/02/2019 09:21:15 step: 7897, epoch: 239, batch: 9, loss: 0.020374499261379242, acc: 100.0, f1: 100.0, r: 0.786299869357118
06/02/2019 09:21:16 step: 7902, epoch: 239, batch: 14, loss: 0.04994204640388489, acc: 98.4375, f1: 97.55639097744361, r: 0.7456402354506675
06/02/2019 09:21:16 step: 7907, epoch: 239, batch: 19, loss: 0.02896023541688919, acc: 98.4375, f1: 99.02818270165209, r: 0.684139142903475
06/02/2019 09:21:16 step: 7912, epoch: 239, batch: 24, loss: 0.03359418362379074, acc: 98.4375, f1: 98.36734693877551, r: 0.7313244861074794
06/02/2019 09:21:17 step: 7917, epoch: 239, batch: 29, loss: 0.029459185898303986, acc: 98.4375, f1: 99.06910132474042, r: 0.7844173548580228
06/02/2019 09:21:17 *** evaluating ***
06/02/2019 09:21:17 step: 240, epoch: 239, acc: 61.965811965811966, f1: 30.49646226415095, r: 0.4027223084912809
06/02/2019 09:21:17 *** epoch: 241 ***
06/02/2019 09:21:17 *** training ***
06/02/2019 09:21:17 step: 7925, epoch: 240, batch: 4, loss: 0.08538326621055603, acc: 96.875, f1: 96.17845117845118, r: 0.5586149919717421
06/02/2019 09:21:18 step: 7930, epoch: 240, batch: 9, loss: 0.01786229759454727, acc: 100.0, f1: 100.0, r: 0.692327843638474
06/02/2019 09:21:18 step: 7935, epoch: 240, batch: 14, loss: 0.03304223716259003, acc: 98.4375, f1: 98.32967032967034, r: 0.6890892470012441
06/02/2019 09:21:18 step: 7940, epoch: 240, batch: 19, loss: 0.06881459057331085, acc: 96.875, f1: 95.87878787878789, r: 0.7696188039890618
06/02/2019 09:21:19 step: 7945, epoch: 240, batch: 24, loss: 0.014724325388669968, acc: 100.0, f1: 100.0, r: 0.8213693126004399
06/02/2019 09:21:19 step: 7950, epoch: 240, batch: 29, loss: 0.1324118673801422, acc: 96.875, f1: 97.67412635260476, r: 0.6827223055999491
06/02/2019 09:21:19 *** evaluating ***
06/02/2019 09:21:19 step: 241, epoch: 240, acc: 61.965811965811966, f1: 30.49646226415095, r: 0.4036343091392901
06/02/2019 09:21:19 *** epoch: 242 ***
06/02/2019 09:21:19 *** training ***
06/02/2019 09:21:20 step: 7958, epoch: 241, batch: 4, loss: 0.09345076233148575, acc: 96.875, f1: 97.49192606335464, r: 0.6614762006624625
06/02/2019 09:21:20 step: 7963, epoch: 241, batch: 9, loss: 0.07024753838777542, acc: 100.0, f1: 100.0, r: 0.7532369932796263
06/02/2019 09:21:20 step: 7968, epoch: 241, batch: 14, loss: 0.0955575481057167, acc: 96.875, f1: 94.46112064036592, r: 0.741687600251744
06/02/2019 09:21:21 step: 7973, epoch: 241, batch: 19, loss: 0.03509734943509102, acc: 98.4375, f1: 98.9459815546772, r: 0.6883374807813332
06/02/2019 09:21:21 step: 7978, epoch: 241, batch: 24, loss: 0.1076548621058464, acc: 98.4375, f1: 98.01587301587303, r: 0.8233492631360011
06/02/2019 09:21:21 step: 7983, epoch: 241, batch: 29, loss: 0.06080176681280136, acc: 96.875, f1: 95.45049857549857, r: 0.779565172977567
06/02/2019 09:21:21 *** evaluating ***
06/02/2019 09:21:22 step: 242, epoch: 241, acc: 61.965811965811966, f1: 29.193679377502907, r: 0.4052572733099381
06/02/2019 09:21:22 *** epoch: 243 ***
06/02/2019 09:21:22 *** training ***
06/02/2019 09:21:22 step: 7991, epoch: 242, batch: 4, loss: 0.04555392265319824, acc: 98.4375, f1: 99.35897435897436, r: 0.8131902387024934
06/02/2019 09:21:22 step: 7996, epoch: 242, batch: 9, loss: 0.07532190531492233, acc: 98.4375, f1: 96.66666666666667, r: 0.7924251709311531
06/02/2019 09:21:23 step: 8001, epoch: 242, batch: 14, loss: 0.028314214199781418, acc: 100.0, f1: 100.0, r: 0.7572426960027754
06/02/2019 09:21:23 step: 8006, epoch: 242, batch: 19, loss: 0.08131638914346695, acc: 96.875, f1: 98.2615268329554, r: 0.6495434117978706
06/02/2019 09:21:23 step: 8011, epoch: 242, batch: 24, loss: 0.05011597275733948, acc: 98.4375, f1: 86.53846153846155, r: 0.7789003399152566
06/02/2019 09:21:24 step: 8016, epoch: 242, batch: 29, loss: 0.19298826158046722, acc: 96.875, f1: 92.18933950391751, r: 0.725059275948411
06/02/2019 09:21:24 *** evaluating ***
06/02/2019 09:21:24 step: 243, epoch: 242, acc: 63.67521367521367, f1: 31.068380821885466, r: 0.40431121997944097
06/02/2019 09:21:24 *** epoch: 244 ***
06/02/2019 09:21:24 *** training ***
06/02/2019 09:21:24 step: 8024, epoch: 243, batch: 4, loss: 0.127858966588974, acc: 96.875, f1: 82.59748427672955, r: 0.6810660996131306
06/02/2019 09:21:25 step: 8029, epoch: 243, batch: 9, loss: 0.025462783873081207, acc: 100.0, f1: 100.0, r: 0.6932952794494339
06/02/2019 09:21:25 step: 8034, epoch: 243, batch: 14, loss: 0.016395732760429382, acc: 100.0, f1: 100.0, r: 0.7266799250213505
06/02/2019 09:21:25 step: 8039, epoch: 243, batch: 19, loss: 0.04252943396568298, acc: 98.4375, f1: 98.76750700280112, r: 0.7239187118490503
06/02/2019 09:21:26 step: 8044, epoch: 243, batch: 24, loss: 0.06562166661024094, acc: 98.4375, f1: 99.37689969604864, r: 0.8001288909939583
06/02/2019 09:21:26 step: 8049, epoch: 243, batch: 29, loss: 0.02642260678112507, acc: 100.0, f1: 100.0, r: 0.7017998469636457
06/02/2019 09:21:26 *** evaluating ***
06/02/2019 09:21:26 step: 244, epoch: 243, acc: 62.39316239316239, f1: 30.330786143286147, r: 0.40174756935015044
06/02/2019 09:21:26 *** epoch: 245 ***
06/02/2019 09:21:26 *** training ***
06/02/2019 09:21:27 step: 8057, epoch: 244, batch: 4, loss: 0.01709921471774578, acc: 100.0, f1: 100.0, r: 0.8104610009964905
06/02/2019 09:21:27 step: 8062, epoch: 244, batch: 9, loss: 0.1586414873600006, acc: 93.75, f1: 82.3489010989011, r: 0.6693383157869832
06/02/2019 09:21:27 step: 8067, epoch: 244, batch: 14, loss: 0.14063456654548645, acc: 95.3125, f1: 95.40660225442834, r: 0.7505142511686057
06/02/2019 09:21:27 step: 8072, epoch: 244, batch: 19, loss: 0.07318055629730225, acc: 96.875, f1: 96.74981103552531, r: 0.6845755974499659
06/02/2019 09:21:28 step: 8077, epoch: 244, batch: 24, loss: 0.07348735630512238, acc: 96.875, f1: 97.51011515717398, r: 0.6891239595637724
06/02/2019 09:21:28 step: 8082, epoch: 244, batch: 29, loss: 0.0829569548368454, acc: 96.875, f1: 96.35555497244435, r: 0.6936622431450158
06/02/2019 09:21:28 *** evaluating ***
06/02/2019 09:21:28 step: 245, epoch: 244, acc: 63.24786324786324, f1: 27.943507869978458, r: 0.402239746392787
06/02/2019 09:21:28 *** epoch: 246 ***
06/02/2019 09:21:28 *** training ***
06/02/2019 09:21:29 step: 8090, epoch: 245, batch: 4, loss: 0.05221487581729889, acc: 98.4375, f1: 99.03722721437741, r: 0.7283140478466218
06/02/2019 09:21:29 step: 8095, epoch: 245, batch: 9, loss: 0.028093179687857628, acc: 100.0, f1: 100.0, r: 0.6834374764841256
06/02/2019 09:21:29 step: 8100, epoch: 245, batch: 14, loss: 0.011294461786746979, acc: 100.0, f1: 100.0, r: 0.8116709877796772
06/02/2019 09:21:30 step: 8105, epoch: 245, batch: 19, loss: 0.08985859155654907, acc: 96.875, f1: 94.51689819614349, r: 0.7488158052280132
06/02/2019 09:21:30 step: 8110, epoch: 245, batch: 24, loss: 0.13297034800052643, acc: 96.875, f1: 94.20918367346938, r: 0.6545726929007147
06/02/2019 09:21:30 step: 8115, epoch: 245, batch: 29, loss: 0.03148696571588516, acc: 98.4375, f1: 98.08018068887634, r: 0.7057018277113303
06/02/2019 09:21:30 *** evaluating ***
06/02/2019 09:21:31 step: 246, epoch: 245, acc: 62.82051282051282, f1: 28.796717587797758, r: 0.4019167624316873
06/02/2019 09:21:31 *** epoch: 247 ***
06/02/2019 09:21:31 *** training ***
06/02/2019 09:21:31 step: 8123, epoch: 246, batch: 4, loss: 0.03153180703520775, acc: 98.4375, f1: 96.82539682539684, r: 0.6611691317930032
06/02/2019 09:21:31 step: 8128, epoch: 246, batch: 9, loss: 0.09618648886680603, acc: 96.875, f1: 93.90756302521008, r: 0.7964687241061789
06/02/2019 09:21:31 step: 8133, epoch: 246, batch: 14, loss: 0.07667795568704605, acc: 96.875, f1: 95.82414526094125, r: 0.7288513202486577
06/02/2019 09:21:32 step: 8138, epoch: 246, batch: 19, loss: 0.04068769887089729, acc: 96.875, f1: 97.86931818181819, r: 0.7943566546357218
06/02/2019 09:21:32 step: 8143, epoch: 246, batch: 24, loss: 0.09769156575202942, acc: 98.4375, f1: 97.38775510204081, r: 0.6543924335697618
06/02/2019 09:21:32 step: 8148, epoch: 246, batch: 29, loss: 0.05064750462770462, acc: 98.4375, f1: 97.79158040027606, r: 0.6972327922460405
06/02/2019 09:21:32 *** evaluating ***
06/02/2019 09:21:33 step: 247, epoch: 246, acc: 61.965811965811966, f1: 27.298880364918098, r: 0.39894832965163546
06/02/2019 09:21:33 *** epoch: 248 ***
06/02/2019 09:21:33 *** training ***
06/02/2019 09:21:33 step: 8156, epoch: 247, batch: 4, loss: 0.041920341551303864, acc: 100.0, f1: 100.0, r: 0.852684932238468
06/02/2019 09:21:33 step: 8161, epoch: 247, batch: 9, loss: 0.0856289267539978, acc: 95.3125, f1: 96.90934065934067, r: 0.7662328608479756
06/02/2019 09:21:33 step: 8166, epoch: 247, batch: 14, loss: 0.03737596422433853, acc: 100.0, f1: 100.0, r: 0.7857717911571466
06/02/2019 09:21:34 step: 8171, epoch: 247, batch: 19, loss: 0.021076511591672897, acc: 100.0, f1: 100.0, r: 0.7223024298315752
06/02/2019 09:21:34 step: 8176, epoch: 247, batch: 24, loss: 0.025075700134038925, acc: 100.0, f1: 100.0, r: 0.7633730910649346
06/02/2019 09:21:34 step: 8181, epoch: 247, batch: 29, loss: 0.037595201283693314, acc: 98.4375, f1: 99.31633407243163, r: 0.7273850308094894
06/02/2019 09:21:35 *** evaluating ***
06/02/2019 09:21:35 step: 248, epoch: 247, acc: 62.82051282051282, f1: 32.08849174318098, r: 0.4070411492975706
06/02/2019 09:21:35 *** epoch: 249 ***
06/02/2019 09:21:35 *** training ***
06/02/2019 09:21:35 step: 8189, epoch: 248, batch: 4, loss: 0.008273914456367493, acc: 100.0, f1: 100.0, r: 0.7644409371536591
06/02/2019 09:21:35 step: 8194, epoch: 248, batch: 9, loss: 0.020972982048988342, acc: 100.0, f1: 100.0, r: 0.7950375031458035
06/02/2019 09:21:35 step: 8199, epoch: 248, batch: 14, loss: 0.0511552169919014, acc: 96.875, f1: 97.19423433709147, r: 0.7132558819272147
06/02/2019 09:21:36 step: 8204, epoch: 248, batch: 19, loss: 0.15782304108142853, acc: 95.3125, f1: 95.32693803474203, r: 0.6769796623600063
06/02/2019 09:21:36 step: 8209, epoch: 248, batch: 24, loss: 0.022841714322566986, acc: 100.0, f1: 100.0, r: 0.6701035753205599
06/02/2019 09:21:36 step: 8214, epoch: 248, batch: 29, loss: 0.010794758796691895, acc: 100.0, f1: 100.0, r: 0.7141165538760099
06/02/2019 09:21:36 *** evaluating ***
06/02/2019 09:21:37 step: 249, epoch: 248, acc: 62.82051282051282, f1: 31.618812607919633, r: 0.40532480567779133
06/02/2019 09:21:37 *** epoch: 250 ***
06/02/2019 09:21:37 *** training ***
06/02/2019 09:21:37 step: 8222, epoch: 249, batch: 4, loss: 0.05946904420852661, acc: 98.4375, f1: 99.20135769192372, r: 0.6435297180828874
06/02/2019 09:21:37 step: 8227, epoch: 249, batch: 9, loss: 0.018338583409786224, acc: 100.0, f1: 100.0, r: 0.7505658430247655
06/02/2019 09:21:37 step: 8232, epoch: 249, batch: 14, loss: 0.11702603101730347, acc: 98.4375, f1: 95.33333333333334, r: 0.7684945197829176
06/02/2019 09:21:38 step: 8237, epoch: 249, batch: 19, loss: 0.06374529004096985, acc: 96.875, f1: 86.8671679197995, r: 0.6541219247165311
06/02/2019 09:21:38 step: 8242, epoch: 249, batch: 24, loss: 0.14953404664993286, acc: 96.875, f1: 97.23655599992405, r: 0.7257692534346083
06/02/2019 09:21:38 step: 8247, epoch: 249, batch: 29, loss: 0.09301875531673431, acc: 98.4375, f1: 93.96825396825396, r: 0.5364673819464792
06/02/2019 09:21:38 *** evaluating ***
06/02/2019 09:21:39 step: 250, epoch: 249, acc: 62.39316239316239, f1: 31.67353011103011, r: 0.4037036092162038
06/02/2019 09:21:39 *** epoch: 251 ***
06/02/2019 09:21:39 *** training ***
06/02/2019 09:21:39 step: 8255, epoch: 250, batch: 4, loss: 0.06238047778606415, acc: 98.4375, f1: 99.3331164606376, r: 0.8064130688097223
06/02/2019 09:21:39 step: 8260, epoch: 250, batch: 9, loss: 0.14692746102809906, acc: 93.75, f1: 92.81105990783409, r: 0.7838654621903653
06/02/2019 09:21:39 step: 8265, epoch: 250, batch: 14, loss: 0.13935065269470215, acc: 93.75, f1: 91.8374741200828, r: 0.7181135110649062
06/02/2019 09:21:40 step: 8270, epoch: 250, batch: 19, loss: 0.05733620375394821, acc: 96.875, f1: 95.66544566544566, r: 0.7762714176781462
06/02/2019 09:21:40 step: 8275, epoch: 250, batch: 24, loss: 0.03682902455329895, acc: 100.0, f1: 100.0, r: 0.6900523405074082
06/02/2019 09:21:40 step: 8280, epoch: 250, batch: 29, loss: 0.038389768451452255, acc: 100.0, f1: 100.0, r: 0.7003564520739902
06/02/2019 09:21:40 *** evaluating ***
06/02/2019 09:21:41 step: 251, epoch: 250, acc: 61.53846153846154, f1: 30.359112035618846, r: 0.4004454498653305
06/02/2019 09:21:41 *** epoch: 252 ***
06/02/2019 09:21:41 *** training ***
06/02/2019 09:21:41 step: 8288, epoch: 251, batch: 4, loss: 0.03278845176100731, acc: 100.0, f1: 100.0, r: 0.7112127638517578
06/02/2019 09:21:41 step: 8293, epoch: 251, batch: 9, loss: 0.08423567563295364, acc: 96.875, f1: 96.66666666666667, r: 0.7183069108827524
06/02/2019 09:21:41 step: 8298, epoch: 251, batch: 14, loss: 0.0345420241355896, acc: 98.4375, f1: 85.28138528138528, r: 0.6016042202623398
06/02/2019 09:21:42 step: 8303, epoch: 251, batch: 19, loss: 0.09140196442604065, acc: 95.3125, f1: 95.24990751377558, r: 0.6056150245389702
06/02/2019 09:21:42 step: 8308, epoch: 251, batch: 24, loss: 0.014611948281526566, acc: 100.0, f1: 100.0, r: 0.7124126960187199
06/02/2019 09:21:42 step: 8313, epoch: 251, batch: 29, loss: 0.0446205697953701, acc: 98.4375, f1: 99.13600891861762, r: 0.7353467764756112
06/02/2019 09:21:43 *** evaluating ***
06/02/2019 09:21:43 step: 252, epoch: 251, acc: 62.82051282051282, f1: 30.968220147907644, r: 0.40781833871583795
06/02/2019 09:21:43 *** epoch: 253 ***
06/02/2019 09:21:43 *** training ***
06/02/2019 09:21:43 step: 8321, epoch: 252, batch: 4, loss: 0.08047564327716827, acc: 98.4375, f1: 98.59225741578683, r: 0.621543442191627
06/02/2019 09:21:43 step: 8326, epoch: 252, batch: 9, loss: 0.03368857502937317, acc: 98.4375, f1: 98.96774193548387, r: 0.678332267458883
06/02/2019 09:21:44 step: 8331, epoch: 252, batch: 14, loss: 0.030631549656391144, acc: 100.0, f1: 100.0, r: 0.7109734952346267
06/02/2019 09:21:44 step: 8336, epoch: 252, batch: 19, loss: 0.05248282104730606, acc: 96.875, f1: 91.83087027914614, r: 0.7976497591147574
06/02/2019 09:21:44 step: 8341, epoch: 252, batch: 24, loss: 0.020654503256082535, acc: 100.0, f1: 100.0, r: 0.8283910175979747
06/02/2019 09:21:45 step: 8346, epoch: 252, batch: 29, loss: 0.1646503359079361, acc: 95.3125, f1: 96.18224153107874, r: 0.7483326541442459
06/02/2019 09:21:45 *** evaluating ***
06/02/2019 09:21:45 step: 253, epoch: 252, acc: 63.24786324786324, f1: 31.14768702438936, r: 0.4076276181439249
06/02/2019 09:21:45 *** epoch: 254 ***
06/02/2019 09:21:45 *** training ***
06/02/2019 09:21:45 step: 8354, epoch: 253, batch: 4, loss: 0.10681352019309998, acc: 95.3125, f1: 94.17123824451411, r: 0.8092286653750385
06/02/2019 09:21:46 step: 8359, epoch: 253, batch: 9, loss: 0.026560835540294647, acc: 98.4375, f1: 97.20730397422128, r: 0.7184455176780623
06/02/2019 09:21:46 step: 8364, epoch: 253, batch: 14, loss: 0.014514371752738953, acc: 100.0, f1: 100.0, r: 0.8021608744245178
06/02/2019 09:21:46 step: 8369, epoch: 253, batch: 19, loss: 0.0630844235420227, acc: 96.875, f1: 95.72974644403216, r: 0.7538480807426803
06/02/2019 09:21:47 step: 8374, epoch: 253, batch: 24, loss: 0.05430961027741432, acc: 98.4375, f1: 98.66690686362818, r: 0.7193084002927201
06/02/2019 09:21:47 step: 8379, epoch: 253, batch: 29, loss: 0.05710527300834656, acc: 96.875, f1: 98.66666666666667, r: 0.7580459224099267
06/02/2019 09:21:47 *** evaluating ***
06/02/2019 09:21:47 step: 254, epoch: 253, acc: 62.82051282051282, f1: 30.70453922458639, r: 0.40808595424481564
06/02/2019 09:21:47 *** epoch: 255 ***
06/02/2019 09:21:47 *** training ***
06/02/2019 09:21:48 step: 8387, epoch: 254, batch: 4, loss: 0.06225258857011795, acc: 96.875, f1: 97.40667134284155, r: 0.7485658199934563
06/02/2019 09:21:48 step: 8392, epoch: 254, batch: 9, loss: 0.07411404699087143, acc: 96.875, f1: 97.06154274575329, r: 0.6515570992624579
06/02/2019 09:21:48 step: 8397, epoch: 254, batch: 14, loss: 0.03682355210185051, acc: 98.4375, f1: 98.03030303030303, r: 0.7663680258500565
06/02/2019 09:21:49 step: 8402, epoch: 254, batch: 19, loss: 0.11655960232019424, acc: 95.3125, f1: 94.79875283446712, r: 0.6546140572947731
06/02/2019 09:21:49 step: 8407, epoch: 254, batch: 24, loss: 0.06379136443138123, acc: 98.4375, f1: 98.8588983980689, r: 0.7249683681724917
06/02/2019 09:21:49 step: 8412, epoch: 254, batch: 29, loss: 0.1048269122838974, acc: 95.3125, f1: 85.13659836748293, r: 0.7189194218164426
06/02/2019 09:21:49 *** evaluating ***
06/02/2019 09:21:50 step: 255, epoch: 254, acc: 62.82051282051282, f1: 30.849564492058956, r: 0.40986131281983923
06/02/2019 09:21:50 *** epoch: 256 ***
06/02/2019 09:21:50 *** training ***
06/02/2019 09:21:50 step: 8420, epoch: 255, batch: 4, loss: 0.037762753665447235, acc: 98.4375, f1: 99.06910132474042, r: 0.65933641260337
06/02/2019 09:21:50 step: 8425, epoch: 255, batch: 9, loss: 0.06399703025817871, acc: 98.4375, f1: 94.9874686716792, r: 0.6987506736807162
06/02/2019 09:21:51 step: 8430, epoch: 255, batch: 14, loss: 0.045223359018564224, acc: 100.0, f1: 100.0, r: 0.8407063401833124
06/02/2019 09:21:51 step: 8435, epoch: 255, batch: 19, loss: 0.07865838706493378, acc: 98.4375, f1: 97.93650793650794, r: 0.7368337283684139
06/02/2019 09:21:51 step: 8440, epoch: 255, batch: 24, loss: 0.029767433181405067, acc: 100.0, f1: 100.0, r: 0.7207278160618807
06/02/2019 09:21:52 step: 8445, epoch: 255, batch: 29, loss: 0.02425948902964592, acc: 100.0, f1: 100.0, r: 0.6586490562999946
06/02/2019 09:21:52 *** evaluating ***
06/02/2019 09:21:52 step: 256, epoch: 255, acc: 62.82051282051282, f1: 30.06685981803906, r: 0.41022263063154535
06/02/2019 09:21:52 *** epoch: 257 ***
06/02/2019 09:21:52 *** training ***
06/02/2019 09:21:52 step: 8453, epoch: 256, batch: 4, loss: 0.1315915882587433, acc: 95.3125, f1: 95.73584582574448, r: 0.6924208986003197
06/02/2019 09:21:52 step: 8458, epoch: 256, batch: 9, loss: 0.056137073785066605, acc: 96.875, f1: 95.3078426773008, r: 0.6695278677323161
06/02/2019 09:21:53 step: 8463, epoch: 256, batch: 14, loss: 0.029122773557901382, acc: 98.4375, f1: 98.3201581027668, r: 0.8012294343557986
06/02/2019 09:21:53 step: 8468, epoch: 256, batch: 19, loss: 0.04131760448217392, acc: 98.4375, f1: 95.43010752688173, r: 0.7545580719354736
06/02/2019 09:21:53 step: 8473, epoch: 256, batch: 24, loss: 0.1059504896402359, acc: 95.3125, f1: 89.20859538784067, r: 0.7873767718631731
06/02/2019 09:21:54 step: 8478, epoch: 256, batch: 29, loss: 0.028887853026390076, acc: 100.0, f1: 100.0, r: 0.6792786010593767
06/02/2019 09:21:54 *** evaluating ***
06/02/2019 09:21:54 step: 257, epoch: 256, acc: 61.965811965811966, f1: 27.53369210670077, r: 0.40797831321429173
06/02/2019 09:21:54 *** epoch: 258 ***
06/02/2019 09:21:54 *** training ***
06/02/2019 09:21:55 step: 8486, epoch: 257, batch: 4, loss: 0.0556076355278492, acc: 100.0, f1: 100.0, r: 0.809136740435658
06/02/2019 09:21:55 step: 8491, epoch: 257, batch: 9, loss: 0.05233186110854149, acc: 98.4375, f1: 97.86096256684492, r: 0.6715546658901055
06/02/2019 09:21:55 step: 8496, epoch: 257, batch: 14, loss: 0.06256036460399628, acc: 96.875, f1: 91.08225108225109, r: 0.6313410131609108
06/02/2019 09:21:55 step: 8501, epoch: 257, batch: 19, loss: 0.02033114619553089, acc: 100.0, f1: 100.0, r: 0.7096564295384601
06/02/2019 09:21:56 step: 8506, epoch: 257, batch: 24, loss: 0.09416602551937103, acc: 95.3125, f1: 93.3451536643026, r: 0.766095735861666
06/02/2019 09:21:56 step: 8511, epoch: 257, batch: 29, loss: 0.02584242820739746, acc: 100.0, f1: 100.0, r: 0.7398443134545726
06/02/2019 09:21:56 *** evaluating ***
06/02/2019 09:21:56 step: 258, epoch: 257, acc: 62.82051282051282, f1: 30.65824459108246, r: 0.40683122436832597
06/02/2019 09:21:56 *** epoch: 259 ***
06/02/2019 09:21:56 *** training ***
06/02/2019 09:21:57 step: 8519, epoch: 258, batch: 4, loss: 0.10804358124732971, acc: 96.875, f1: 91.76470588235294, r: 0.7260652205886251
06/02/2019 09:21:57 step: 8524, epoch: 258, batch: 9, loss: 0.05738132447004318, acc: 98.4375, f1: 95.37037037037037, r: 0.8225737539708962
06/02/2019 09:21:57 step: 8529, epoch: 258, batch: 14, loss: 0.027235429733991623, acc: 98.4375, f1: 99.40700808625337, r: 0.8020460873095329
06/02/2019 09:21:58 step: 8534, epoch: 258, batch: 19, loss: 0.08570452034473419, acc: 96.875, f1: 98.39015151515152, r: 0.8067175531699953
06/02/2019 09:21:58 step: 8539, epoch: 258, batch: 24, loss: 0.05690035969018936, acc: 96.875, f1: 91.26050420168067, r: 0.5630793751466153
06/02/2019 09:21:58 step: 8544, epoch: 258, batch: 29, loss: 0.024610716849565506, acc: 100.0, f1: 100.0, r: 0.8301538041528617
06/02/2019 09:21:59 *** evaluating ***
06/02/2019 09:21:59 step: 259, epoch: 258, acc: 62.82051282051282, f1: 30.09004706533776, r: 0.40668141860793533
06/02/2019 09:21:59 *** epoch: 260 ***
06/02/2019 09:21:59 *** training ***
06/02/2019 09:21:59 step: 8552, epoch: 259, batch: 4, loss: 0.08894108980894089, acc: 98.4375, f1: 94.04761904761905, r: 0.7275408996266518
06/02/2019 09:21:59 step: 8557, epoch: 259, batch: 9, loss: 0.08324044197797775, acc: 98.4375, f1: 98.92156862745098, r: 0.7430837026412458
06/02/2019 09:22:00 step: 8562, epoch: 259, batch: 14, loss: 0.03975217416882515, acc: 98.4375, f1: 98.65047233468286, r: 0.7506034115151429
06/02/2019 09:22:00 step: 8567, epoch: 259, batch: 19, loss: 0.03699688985943794, acc: 100.0, f1: 100.0, r: 0.620995821954289
06/02/2019 09:22:00 step: 8572, epoch: 259, batch: 24, loss: 0.04001547768712044, acc: 100.0, f1: 100.0, r: 0.7843790539547055
06/02/2019 09:22:01 step: 8577, epoch: 259, batch: 29, loss: 0.05854091793298721, acc: 98.4375, f1: 98.15295815295816, r: 0.6491385926059978
06/02/2019 09:22:01 *** evaluating ***
06/02/2019 09:22:01 step: 260, epoch: 259, acc: 61.965811965811966, f1: 27.063253660637375, r: 0.40392391157297497
06/02/2019 09:22:01 *** epoch: 261 ***
06/02/2019 09:22:01 *** training ***
06/02/2019 09:22:01 step: 8585, epoch: 260, batch: 4, loss: 0.042538948357105255, acc: 98.4375, f1: 98.9175331149409, r: 0.6648216876729941
06/02/2019 09:22:02 step: 8590, epoch: 260, batch: 9, loss: 0.02722574584186077, acc: 100.0, f1: 100.0, r: 0.8078233306480986
06/02/2019 09:22:02 step: 8595, epoch: 260, batch: 14, loss: 0.0739540159702301, acc: 96.875, f1: 95.4949874686717, r: 0.7835347054890626
06/02/2019 09:22:02 step: 8600, epoch: 260, batch: 19, loss: 0.07748499512672424, acc: 98.4375, f1: 97.64172335600908, r: 0.7256508906506537
06/02/2019 09:22:03 step: 8605, epoch: 260, batch: 24, loss: 0.10209622979164124, acc: 98.4375, f1: 85.46365914786966, r: 0.5991421967448236
06/02/2019 09:22:03 step: 8610, epoch: 260, batch: 29, loss: 0.029769491404294968, acc: 98.4375, f1: 98.86128364389234, r: 0.7585139765907533
06/02/2019 09:22:03 *** evaluating ***
06/02/2019 09:22:03 step: 261, epoch: 260, acc: 62.39316239316239, f1: 27.534465321045342, r: 0.40260891540874155
06/02/2019 09:22:03 *** epoch: 262 ***
06/02/2019 09:22:03 *** training ***
06/02/2019 09:22:04 step: 8618, epoch: 261, batch: 4, loss: 0.027196727693080902, acc: 100.0, f1: 100.0, r: 0.671720462309599
06/02/2019 09:22:04 step: 8623, epoch: 261, batch: 9, loss: 0.07213001698255539, acc: 96.875, f1: 93.48404255319149, r: 0.7217286627582334
06/02/2019 09:22:04 step: 8628, epoch: 261, batch: 14, loss: 0.02449856884777546, acc: 100.0, f1: 100.0, r: 0.7319663764833934
06/02/2019 09:22:05 step: 8633, epoch: 261, batch: 19, loss: 0.007647320628166199, acc: 100.0, f1: 100.0, r: 0.7647096232859248
06/02/2019 09:22:05 step: 8638, epoch: 261, batch: 24, loss: 0.061315495520830154, acc: 100.0, f1: 100.0, r: 0.6227293445004148
06/02/2019 09:22:05 step: 8643, epoch: 261, batch: 29, loss: 0.06583726406097412, acc: 95.3125, f1: 93.66738783157817, r: 0.6745877652460148
06/02/2019 09:22:06 *** evaluating ***
06/02/2019 09:22:06 step: 262, epoch: 261, acc: 62.82051282051282, f1: 28.862132558078468, r: 0.40890365635129655
06/02/2019 09:22:06 *** epoch: 263 ***
06/02/2019 09:22:06 *** training ***
06/02/2019 09:22:06 step: 8651, epoch: 262, batch: 4, loss: 0.1038510799407959, acc: 98.4375, f1: 96.38418079096044, r: 0.5865999382920519
06/02/2019 09:22:06 step: 8656, epoch: 262, batch: 9, loss: 0.08780937641859055, acc: 96.875, f1: 86.31118881118881, r: 0.7396423969842529
06/02/2019 09:22:07 step: 8661, epoch: 262, batch: 14, loss: 0.10078997164964676, acc: 98.4375, f1: 97.61904761904762, r: 0.7690585626300049
06/02/2019 09:22:07 step: 8666, epoch: 262, batch: 19, loss: 0.08187653124332428, acc: 95.3125, f1: 89.63762929280172, r: 0.7129124131379028
06/02/2019 09:22:07 step: 8671, epoch: 262, batch: 24, loss: 0.0652538388967514, acc: 98.4375, f1: 99.30976430976432, r: 0.7469800690706561
06/02/2019 09:22:08 step: 8676, epoch: 262, batch: 29, loss: 0.03948058560490608, acc: 98.4375, f1: 98.30316742081449, r: 0.7697364447292201
06/02/2019 09:22:08 *** evaluating ***
06/02/2019 09:22:08 step: 263, epoch: 262, acc: 61.965811965811966, f1: 27.568151850303067, r: 0.40946370178579095
06/02/2019 09:22:08 *** epoch: 264 ***
06/02/2019 09:22:08 *** training ***
06/02/2019 09:22:08 step: 8684, epoch: 263, batch: 4, loss: 0.05951292812824249, acc: 98.4375, f1: 99.07759714055115, r: 0.682823929538779
06/02/2019 09:22:09 step: 8689, epoch: 263, batch: 9, loss: 0.06550633162260056, acc: 98.4375, f1: 97.20730397422128, r: 0.6555997885090795
06/02/2019 09:22:09 step: 8694, epoch: 263, batch: 14, loss: 0.022258786484599113, acc: 100.0, f1: 100.0, r: 0.8329730374138489
06/02/2019 09:22:09 step: 8699, epoch: 263, batch: 19, loss: 0.03973013535141945, acc: 98.4375, f1: 96.52173913043478, r: 0.73645125484957
06/02/2019 09:22:10 step: 8704, epoch: 263, batch: 24, loss: 0.046357862651348114, acc: 98.4375, f1: 97.90209790209789, r: 0.75473228037913
06/02/2019 09:22:10 step: 8709, epoch: 263, batch: 29, loss: 0.07123001664876938, acc: 96.875, f1: 96.84966727162735, r: 0.7611180593078879
06/02/2019 09:22:10 *** evaluating ***
06/02/2019 09:22:10 step: 264, epoch: 263, acc: 61.111111111111114, f1: 26.261325995979156, r: 0.4060809741697442
06/02/2019 09:22:10 *** epoch: 265 ***
06/02/2019 09:22:10 *** training ***
06/02/2019 09:22:11 step: 8717, epoch: 264, batch: 4, loss: 0.0585935041308403, acc: 98.4375, f1: 99.13702623906705, r: 0.6628631729902069
06/02/2019 09:22:11 step: 8722, epoch: 264, batch: 9, loss: 0.015176519751548767, acc: 100.0, f1: 100.0, r: 0.6902874333015723
06/02/2019 09:22:11 step: 8727, epoch: 264, batch: 14, loss: 0.06857815384864807, acc: 95.3125, f1: 93.39346188402793, r: 0.6772626398155478
06/02/2019 09:22:12 step: 8732, epoch: 264, batch: 19, loss: 0.23963919281959534, acc: 95.3125, f1: 93.57304829673252, r: 0.4985150671019782
06/02/2019 09:22:12 step: 8737, epoch: 264, batch: 24, loss: 0.027415292337536812, acc: 100.0, f1: 100.0, r: 0.6933943416988427
06/02/2019 09:22:12 step: 8742, epoch: 264, batch: 29, loss: 0.03313568979501724, acc: 98.4375, f1: 98.95652173913044, r: 0.7902473508638141
06/02/2019 09:22:12 *** evaluating ***
06/02/2019 09:22:13 step: 265, epoch: 264, acc: 61.53846153846154, f1: 30.633058498827857, r: 0.40860567421259864
06/02/2019 09:22:13 *** epoch: 266 ***
06/02/2019 09:22:13 *** training ***
06/02/2019 09:22:13 step: 8750, epoch: 265, batch: 4, loss: 0.03336233273148537, acc: 100.0, f1: 100.0, r: 0.8160895241209727
06/02/2019 09:22:13 step: 8755, epoch: 265, batch: 9, loss: 0.06459297239780426, acc: 96.875, f1: 91.94444444444444, r: 0.8157438339559298
06/02/2019 09:22:14 step: 8760, epoch: 265, batch: 14, loss: 0.049586329609155655, acc: 98.4375, f1: 97.49835418038182, r: 0.6690948629147149
06/02/2019 09:22:14 step: 8765, epoch: 265, batch: 19, loss: 0.032610490918159485, acc: 100.0, f1: 100.0, r: 0.6776707363348726
06/02/2019 09:22:14 step: 8770, epoch: 265, batch: 24, loss: 0.016016945242881775, acc: 100.0, f1: 100.0, r: 0.7545681211312448
06/02/2019 09:22:15 step: 8775, epoch: 265, batch: 29, loss: 0.06501024961471558, acc: 100.0, f1: 100.0, r: 0.7373177126614088
06/02/2019 09:22:15 *** evaluating ***
06/02/2019 09:22:15 step: 266, epoch: 265, acc: 61.965811965811966, f1: 26.69989735997067, r: 0.40618936494199864
06/02/2019 09:22:15 *** epoch: 267 ***
06/02/2019 09:22:15 *** training ***
06/02/2019 09:22:15 step: 8783, epoch: 266, batch: 4, loss: 0.09676822274923325, acc: 95.3125, f1: 79.51659451659452, r: 0.5084075404943424
06/02/2019 09:22:16 step: 8788, epoch: 266, batch: 9, loss: 0.111007459461689, acc: 96.875, f1: 95.77922077922078, r: 0.7239680338055305
06/02/2019 09:22:16 step: 8793, epoch: 266, batch: 14, loss: 0.017497580498456955, acc: 100.0, f1: 100.0, r: 0.7408458965992066
06/02/2019 09:22:16 step: 8798, epoch: 266, batch: 19, loss: 0.06569727510213852, acc: 96.875, f1: 96.07142857142857, r: 0.7904124835374751
06/02/2019 09:22:17 step: 8803, epoch: 266, batch: 24, loss: 0.009914794936776161, acc: 100.0, f1: 100.0, r: 0.6601895149023261
06/02/2019 09:22:17 step: 8808, epoch: 266, batch: 29, loss: 0.1252763271331787, acc: 95.3125, f1: 90.5458089668616, r: 0.7612883216243772
06/02/2019 09:22:17 *** evaluating ***
06/02/2019 09:22:17 step: 267, epoch: 266, acc: 61.965811965811966, f1: 26.777634551360762, r: 0.4034679400743935
06/02/2019 09:22:17 *** epoch: 268 ***
06/02/2019 09:22:17 *** training ***
06/02/2019 09:22:18 step: 8816, epoch: 267, batch: 4, loss: 0.03437439724802971, acc: 98.4375, f1: 96.63299663299664, r: 0.6097782722161791
06/02/2019 09:22:18 step: 8821, epoch: 267, batch: 9, loss: 0.015280846506357193, acc: 100.0, f1: 100.0, r: 0.6976607299653975
06/02/2019 09:22:18 step: 8826, epoch: 267, batch: 14, loss: 0.02954564243555069, acc: 98.4375, f1: 97.14285714285714, r: 0.7862490416959065
06/02/2019 09:22:19 step: 8831, epoch: 267, batch: 19, loss: 0.04571671038866043, acc: 96.875, f1: 96.5986394557823, r: 0.7019177767107135
06/02/2019 09:22:19 step: 8836, epoch: 267, batch: 24, loss: 0.01181928999722004, acc: 100.0, f1: 100.0, r: 0.7123927228848822
06/02/2019 09:22:19 step: 8841, epoch: 267, batch: 29, loss: 0.009876884520053864, acc: 100.0, f1: 100.0, r: 0.7665374204075046
06/02/2019 09:22:20 *** evaluating ***
06/02/2019 09:22:20 step: 268, epoch: 267, acc: 61.53846153846154, f1: 26.576347985395298, r: 0.408885442553309
06/02/2019 09:22:20 *** epoch: 269 ***
06/02/2019 09:22:20 *** training ***
06/02/2019 09:22:20 step: 8849, epoch: 268, batch: 4, loss: 0.05269372835755348, acc: 96.875, f1: 93.31176999101527, r: 0.7726229668850191
06/02/2019 09:22:20 step: 8854, epoch: 268, batch: 9, loss: 0.024640968069434166, acc: 100.0, f1: 100.0, r: 0.7158212917901919
06/02/2019 09:22:21 step: 8859, epoch: 268, batch: 14, loss: 0.034263044595718384, acc: 98.4375, f1: 95.0, r: 0.7650435666483243
06/02/2019 09:22:21 step: 8864, epoch: 268, batch: 19, loss: 0.04165319353342056, acc: 98.4375, f1: 98.94419306184012, r: 0.7818477432276248
06/02/2019 09:22:22 step: 8869, epoch: 268, batch: 24, loss: 0.01518966630101204, acc: 100.0, f1: 100.0, r: 0.8282464067616961
06/02/2019 09:22:22 step: 8874, epoch: 268, batch: 29, loss: 0.010834861546754837, acc: 100.0, f1: 100.0, r: 0.7420850036171741
06/02/2019 09:22:22 *** evaluating ***
06/02/2019 09:22:22 step: 269, epoch: 268, acc: 61.965811965811966, f1: 27.764055350001716, r: 0.4083681926538008
06/02/2019 09:22:22 *** epoch: 270 ***
06/02/2019 09:22:22 *** training ***
06/02/2019 09:22:23 step: 8882, epoch: 269, batch: 4, loss: 0.025423504412174225, acc: 100.0, f1: 100.0, r: 0.8409177233190795
06/02/2019 09:22:23 step: 8887, epoch: 269, batch: 9, loss: 0.04706485569477081, acc: 96.875, f1: 97.29896956283764, r: 0.6900153546408677
06/02/2019 09:22:23 step: 8892, epoch: 269, batch: 14, loss: 0.06804616749286652, acc: 98.4375, f1: 97.93650793650794, r: 0.7624954528935699
06/02/2019 09:22:24 step: 8897, epoch: 269, batch: 19, loss: 0.04169401526451111, acc: 98.4375, f1: 99.21866751135043, r: 0.7024982072426245
06/02/2019 09:22:24 step: 8902, epoch: 269, batch: 24, loss: 0.05789896100759506, acc: 96.875, f1: 97.01814058956916, r: 0.6719718172776716
06/02/2019 09:22:24 step: 8907, epoch: 269, batch: 29, loss: 0.010477878153324127, acc: 100.0, f1: 100.0, r: 0.6791424016075379
06/02/2019 09:22:24 *** evaluating ***
06/02/2019 09:22:25 step: 270, epoch: 269, acc: 61.53846153846154, f1: 26.304278641367073, r: 0.4038708952032388
06/02/2019 09:22:25 *** epoch: 271 ***
06/02/2019 09:22:25 *** training ***
06/02/2019 09:22:25 step: 8915, epoch: 270, batch: 4, loss: 0.03696808964014053, acc: 100.0, f1: 100.0, r: 0.8361324866965397
06/02/2019 09:22:25 step: 8920, epoch: 270, batch: 9, loss: 0.044617749750614166, acc: 98.4375, f1: 97.667638483965, r: 0.68923059573894
06/02/2019 09:22:26 step: 8925, epoch: 270, batch: 14, loss: 0.11480926722288132, acc: 95.3125, f1: 84.24008424008424, r: 0.7304984171536651
06/02/2019 09:22:26 step: 8930, epoch: 270, batch: 19, loss: 0.03593912348151207, acc: 98.4375, f1: 99.01006997781191, r: 0.6704962780266535
06/02/2019 09:22:26 step: 8935, epoch: 270, batch: 24, loss: 0.006372289732098579, acc: 100.0, f1: 100.0, r: 0.6604333196393689
06/02/2019 09:22:27 step: 8940, epoch: 270, batch: 29, loss: 0.022154036909341812, acc: 100.0, f1: 100.0, r: 0.6824187833406276
06/02/2019 09:22:27 *** evaluating ***
06/02/2019 09:22:27 step: 271, epoch: 270, acc: 61.111111111111114, f1: 27.462834998241526, r: 0.4039027243396868
06/02/2019 09:22:27 *** epoch: 272 ***
06/02/2019 09:22:27 *** training ***
06/02/2019 09:22:27 step: 8948, epoch: 271, batch: 4, loss: 0.041133224964141846, acc: 98.4375, f1: 97.24867724867724, r: 0.6081794983585591
06/02/2019 09:22:28 step: 8953, epoch: 271, batch: 9, loss: 0.020634319633245468, acc: 100.0, f1: 100.0, r: 0.6705153413877847
06/02/2019 09:22:28 step: 8958, epoch: 271, batch: 14, loss: 0.1905089169740677, acc: 93.75, f1: 91.84601113172542, r: 0.5717052360901054
06/02/2019 09:22:28 step: 8963, epoch: 271, batch: 19, loss: 0.04105779156088829, acc: 96.875, f1: 97.5685234305924, r: 0.601239402365174
06/02/2019 09:22:29 step: 8968, epoch: 271, batch: 24, loss: 0.08944690972566605, acc: 98.4375, f1: 94.77726574500768, r: 0.6717243372011726
06/02/2019 09:22:29 step: 8973, epoch: 271, batch: 29, loss: 0.016069643199443817, acc: 100.0, f1: 100.0, r: 0.607188732397818
06/02/2019 09:22:29 *** evaluating ***
06/02/2019 09:22:29 step: 272, epoch: 271, acc: 61.965811965811966, f1: 26.46231731403425, r: 0.4011677030769497
06/02/2019 09:22:29 *** epoch: 273 ***
06/02/2019 09:22:29 *** training ***
06/02/2019 09:22:30 step: 8981, epoch: 272, batch: 4, loss: 0.11306286603212357, acc: 95.3125, f1: 95.82948986722572, r: 0.7208333499349932
06/02/2019 09:22:30 step: 8986, epoch: 272, batch: 9, loss: 0.0717654824256897, acc: 98.4375, f1: 98.60955371159453, r: 0.6693960688398796
06/02/2019 09:22:30 step: 8991, epoch: 272, batch: 14, loss: 0.09356940537691116, acc: 95.3125, f1: 95.1461038961039, r: 0.7829711581239861
06/02/2019 09:22:31 step: 8996, epoch: 272, batch: 19, loss: 0.03675927221775055, acc: 100.0, f1: 100.0, r: 0.6599702135371374
06/02/2019 09:22:31 step: 9001, epoch: 272, batch: 24, loss: 0.05898909643292427, acc: 98.4375, f1: 98.97872340425532, r: 0.5942339478384574
06/02/2019 09:22:31 step: 9006, epoch: 272, batch: 29, loss: 0.048358332365751266, acc: 98.4375, f1: 97.86096256684492, r: 0.7520864842782172
06/02/2019 09:22:31 *** evaluating ***
06/02/2019 09:22:32 step: 273, epoch: 272, acc: 61.965811965811966, f1: 26.46231731403425, r: 0.39930284901361107
06/02/2019 09:22:32 *** epoch: 274 ***
06/02/2019 09:22:32 *** training ***
06/02/2019 09:22:32 step: 9014, epoch: 273, batch: 4, loss: 0.12436646223068237, acc: 93.75, f1: 95.49959157252285, r: 0.7694586581660023
06/02/2019 09:22:32 step: 9019, epoch: 273, batch: 9, loss: 0.08202717453241348, acc: 98.4375, f1: 95.55555555555556, r: 0.7310256196410473
06/02/2019 09:22:33 step: 9024, epoch: 273, batch: 14, loss: 0.06342048197984695, acc: 96.875, f1: 95.92630095516468, r: 0.7111519204374536
06/02/2019 09:22:33 step: 9029, epoch: 273, batch: 19, loss: 0.11577339470386505, acc: 93.75, f1: 77.76593521421107, r: 0.7204393132579922
06/02/2019 09:22:33 step: 9034, epoch: 273, batch: 24, loss: 0.03002801164984703, acc: 100.0, f1: 100.0, r: 0.6541440874229968
06/02/2019 09:22:34 step: 9039, epoch: 273, batch: 29, loss: 0.05657310038805008, acc: 96.875, f1: 92.09837781266351, r: 0.7040275637286922
06/02/2019 09:22:34 *** evaluating ***
06/02/2019 09:22:34 step: 274, epoch: 273, acc: 61.965811965811966, f1: 26.777634551360762, r: 0.4018870045122608
06/02/2019 09:22:34 *** epoch: 275 ***
06/02/2019 09:22:34 *** training ***
06/02/2019 09:22:34 step: 9047, epoch: 274, batch: 4, loss: 0.008500508964061737, acc: 100.0, f1: 100.0, r: 0.8120756642728861
06/02/2019 09:22:35 step: 9052, epoch: 274, batch: 9, loss: 0.02301041968166828, acc: 98.4375, f1: 98.8795518207283, r: 0.713076149692741
06/02/2019 09:22:35 step: 9057, epoch: 274, batch: 14, loss: 0.06078130006790161, acc: 98.4375, f1: 99.06259154379455, r: 0.5880354137211303
06/02/2019 09:22:35 step: 9062, epoch: 274, batch: 19, loss: 0.18799373507499695, acc: 95.3125, f1: 91.72222222222223, r: 0.7148043050242527
06/02/2019 09:22:36 step: 9067, epoch: 274, batch: 24, loss: 0.04597873613238335, acc: 98.4375, f1: 98.0045351473923, r: 0.6843365011940157
06/02/2019 09:22:36 step: 9072, epoch: 274, batch: 29, loss: 0.08493241667747498, acc: 98.4375, f1: 98.8994708994709, r: 0.7038458651226076
06/02/2019 09:22:36 *** evaluating ***
06/02/2019 09:22:36 step: 275, epoch: 274, acc: 62.39316239316239, f1: 30.50485321969697, r: 0.40167727491147714
06/02/2019 09:22:36 *** epoch: 276 ***
06/02/2019 09:22:36 *** training ***
06/02/2019 09:22:37 step: 9080, epoch: 275, batch: 4, loss: 0.030223652720451355, acc: 100.0, f1: 100.0, r: 0.6585850894529479
06/02/2019 09:22:37 step: 9085, epoch: 275, batch: 9, loss: 0.09497059881687164, acc: 96.875, f1: 92.4829931972789, r: 0.7522372463283424
06/02/2019 09:22:37 step: 9090, epoch: 275, batch: 14, loss: 0.10248169302940369, acc: 95.3125, f1: 95.43137254901961, r: 0.7791449519013166
06/02/2019 09:22:38 step: 9095, epoch: 275, batch: 19, loss: 0.07083012163639069, acc: 96.875, f1: 93.06764936055555, r: 0.6753283984595803
06/02/2019 09:22:38 step: 9100, epoch: 275, batch: 24, loss: 0.02608218416571617, acc: 98.4375, f1: 97.33806566104703, r: 0.7253243911950726
06/02/2019 09:22:38 step: 9105, epoch: 275, batch: 29, loss: 0.013612117618322372, acc: 100.0, f1: 100.0, r: 0.7226256065593599
06/02/2019 09:22:38 *** evaluating ***
06/02/2019 09:22:39 step: 276, epoch: 275, acc: 61.53846153846154, f1: 27.334166338042053, r: 0.4043063487221566
06/02/2019 09:22:39 *** epoch: 277 ***
06/02/2019 09:22:39 *** training ***
06/02/2019 09:22:39 step: 9113, epoch: 276, batch: 4, loss: 0.02236902341246605, acc: 100.0, f1: 100.0, r: 0.6579125936603968
06/02/2019 09:22:39 step: 9118, epoch: 276, batch: 9, loss: 0.07359009236097336, acc: 96.875, f1: 96.96329852579852, r: 0.7791370321761079
06/02/2019 09:22:40 step: 9123, epoch: 276, batch: 14, loss: 0.01439029723405838, acc: 100.0, f1: 100.0, r: 0.6538790180804892
06/02/2019 09:22:40 step: 9128, epoch: 276, batch: 19, loss: 0.03967183455824852, acc: 98.4375, f1: 99.35728411338167, r: 0.8040082117397093
06/02/2019 09:22:40 step: 9133, epoch: 276, batch: 24, loss: 0.08316221088171005, acc: 96.875, f1: 98.47619047619048, r: 0.7189379216761641
06/02/2019 09:22:41 step: 9138, epoch: 276, batch: 29, loss: 0.016312945634126663, acc: 100.0, f1: 100.0, r: 0.7709282533717913
06/02/2019 09:22:41 *** evaluating ***
06/02/2019 09:22:41 step: 277, epoch: 276, acc: 60.68376068376068, f1: 27.14567245817246, r: 0.40120776833494065
06/02/2019 09:22:41 *** epoch: 278 ***
06/02/2019 09:22:41 *** training ***
06/02/2019 09:22:41 step: 9146, epoch: 277, batch: 4, loss: 0.12359077483415604, acc: 98.4375, f1: 94.87179487179486, r: 0.7728875620249642
06/02/2019 09:22:41 step: 9151, epoch: 277, batch: 9, loss: 0.06401129066944122, acc: 96.875, f1: 91.41025641025641, r: 0.7509989299519249
06/02/2019 09:22:42 step: 9156, epoch: 277, batch: 14, loss: 0.01105569675564766, acc: 100.0, f1: 100.0, r: 0.6711886408104049
06/02/2019 09:22:42 step: 9161, epoch: 277, batch: 19, loss: 0.052403245121240616, acc: 98.4375, f1: 96.65024630541872, r: 0.707107524576686
06/02/2019 09:22:42 step: 9166, epoch: 277, batch: 24, loss: 0.04762847721576691, acc: 98.4375, f1: 98.93081761006289, r: 0.7802635329143488
06/02/2019 09:22:43 step: 9171, epoch: 277, batch: 29, loss: 0.03874404728412628, acc: 98.4375, f1: 97.43008314436887, r: 0.6450566363285462
06/02/2019 09:22:43 *** evaluating ***
06/02/2019 09:22:43 step: 278, epoch: 277, acc: 60.256410256410255, f1: 26.281866135620056, r: 0.4058209128819151
06/02/2019 09:22:43 *** epoch: 279 ***
06/02/2019 09:22:43 *** training ***
06/02/2019 09:22:43 step: 9179, epoch: 278, batch: 4, loss: 0.07273125648498535, acc: 96.875, f1: 97.48045065760084, r: 0.8313887719432499
06/02/2019 09:22:44 step: 9184, epoch: 278, batch: 9, loss: 0.10372669994831085, acc: 95.3125, f1: 90.99734755562305, r: 0.6859877457586825
06/02/2019 09:22:44 step: 9189, epoch: 278, batch: 14, loss: 0.07348841428756714, acc: 96.875, f1: 96.14607959022851, r: 0.8245294539787231
06/02/2019 09:22:44 step: 9194, epoch: 278, batch: 19, loss: 0.02484806254506111, acc: 100.0, f1: 100.0, r: 0.7223610066868731
06/02/2019 09:22:44 step: 9199, epoch: 278, batch: 24, loss: 0.04305234178900719, acc: 98.4375, f1: 99.20079920079921, r: 0.6980175701018894
06/02/2019 09:22:45 step: 9204, epoch: 278, batch: 29, loss: 0.01644851639866829, acc: 100.0, f1: 100.0, r: 0.8002314055978818
06/02/2019 09:22:45 *** evaluating ***
06/02/2019 09:22:45 step: 279, epoch: 278, acc: 61.111111111111114, f1: 27.185104304879847, r: 0.40686685407026546
06/02/2019 09:22:45 *** epoch: 280 ***
06/02/2019 09:22:45 *** training ***
06/02/2019 09:22:45 step: 9212, epoch: 279, batch: 4, loss: 0.14086943864822388, acc: 95.3125, f1: 91.07004643962848, r: 0.7751291544391093
06/02/2019 09:22:46 step: 9217, epoch: 279, batch: 9, loss: 0.0807279646396637, acc: 98.4375, f1: 99.23404255319149, r: 0.7953018585332923
06/02/2019 09:22:46 step: 9222, epoch: 279, batch: 14, loss: 0.05125360190868378, acc: 96.875, f1: 98.48003629764067, r: 0.755888807912595
06/02/2019 09:22:46 step: 9227, epoch: 279, batch: 19, loss: 0.06555692106485367, acc: 96.875, f1: 95.96825396825398, r: 0.7270233070399917
06/02/2019 09:22:47 step: 9232, epoch: 279, batch: 24, loss: 0.031339649111032486, acc: 98.4375, f1: 98.91589438713062, r: 0.6760486913263272
06/02/2019 09:22:47 step: 9237, epoch: 279, batch: 29, loss: 0.022982649505138397, acc: 100.0, f1: 100.0, r: 0.7166645582110903
06/02/2019 09:22:47 *** evaluating ***
06/02/2019 09:22:47 step: 280, epoch: 279, acc: 61.53846153846154, f1: 26.66614447692488, r: 0.4092117517744055
06/02/2019 09:22:47 *** epoch: 281 ***
06/02/2019 09:22:47 *** training ***
06/02/2019 09:22:48 step: 9245, epoch: 280, batch: 4, loss: 0.18141499161720276, acc: 93.75, f1: 92.63800381812804, r: 0.6473442388669142
06/02/2019 09:22:48 step: 9250, epoch: 280, batch: 9, loss: 0.02682887203991413, acc: 100.0, f1: 100.0, r: 0.831279022564062
06/02/2019 09:22:48 step: 9255, epoch: 280, batch: 14, loss: 0.02956923097372055, acc: 100.0, f1: 100.0, r: 0.7122270433326493
06/02/2019 09:22:49 step: 9260, epoch: 280, batch: 19, loss: 0.04522467404603958, acc: 100.0, f1: 100.0, r: 0.7724462549497765
06/02/2019 09:22:49 step: 9265, epoch: 280, batch: 24, loss: 0.17332018911838531, acc: 95.3125, f1: 95.85685337964973, r: 0.6897577280347492
06/02/2019 09:22:49 step: 9270, epoch: 280, batch: 29, loss: 0.10328949242830276, acc: 96.875, f1: 98.05718475073314, r: 0.7070135306619679
06/02/2019 09:22:50 *** evaluating ***
06/02/2019 09:22:50 step: 281, epoch: 280, acc: 62.39316239316239, f1: 27.841646451390545, r: 0.41133347244707863
06/02/2019 09:22:50 *** epoch: 282 ***
06/02/2019 09:22:50 *** training ***
06/02/2019 09:22:50 step: 9278, epoch: 281, batch: 4, loss: 0.05626968294382095, acc: 96.875, f1: 93.12629399585921, r: 0.7135860619541597
06/02/2019 09:22:50 step: 9283, epoch: 281, batch: 9, loss: 0.09510570019483566, acc: 96.875, f1: 95.71050642479213, r: 0.8182158312534962
06/02/2019 09:22:51 step: 9288, epoch: 281, batch: 14, loss: 0.01915191486477852, acc: 100.0, f1: 100.0, r: 0.7719735559457843
06/02/2019 09:22:51 step: 9293, epoch: 281, batch: 19, loss: 0.017941098660230637, acc: 100.0, f1: 100.0, r: 0.7204546738432421
06/02/2019 09:22:51 step: 9298, epoch: 281, batch: 24, loss: 0.031421974301338196, acc: 98.4375, f1: 95.0, r: 0.7667054909953755
06/02/2019 09:22:52 step: 9303, epoch: 281, batch: 29, loss: 0.06478452682495117, acc: 98.4375, f1: 95.33333333333334, r: 0.7201028226162686
06/02/2019 09:22:52 *** evaluating ***
06/02/2019 09:22:52 step: 282, epoch: 281, acc: 61.53846153846154, f1: 27.651230110469243, r: 0.4038791081044612
06/02/2019 09:22:52 *** epoch: 283 ***
06/02/2019 09:22:52 *** training ***
06/02/2019 09:22:52 step: 9311, epoch: 282, batch: 4, loss: 0.04330294579267502, acc: 100.0, f1: 100.0, r: 0.6894077617425451
06/02/2019 09:22:53 step: 9316, epoch: 282, batch: 9, loss: 0.05434873700141907, acc: 100.0, f1: 100.0, r: 0.6857837873768072
06/02/2019 09:22:53 step: 9321, epoch: 282, batch: 14, loss: 0.03789138048887253, acc: 100.0, f1: 100.0, r: 0.8440554494735641
06/02/2019 09:22:53 step: 9326, epoch: 282, batch: 19, loss: 0.08258611708879471, acc: 98.4375, f1: 96.9047619047619, r: 0.7190743367634365
06/02/2019 09:22:54 step: 9331, epoch: 282, batch: 24, loss: 0.024004992097616196, acc: 100.0, f1: 100.0, r: 0.6746943403686427
06/02/2019 09:22:54 step: 9336, epoch: 282, batch: 29, loss: 0.038967207074165344, acc: 98.4375, f1: 97.33806566104704, r: 0.6686102423089954
06/02/2019 09:22:54 *** evaluating ***
06/02/2019 09:22:54 step: 283, epoch: 282, acc: 61.965811965811966, f1: 27.345137172448865, r: 0.3996197948883383
06/02/2019 09:22:54 *** epoch: 284 ***
06/02/2019 09:22:54 *** training ***
06/02/2019 09:22:55 step: 9344, epoch: 283, batch: 4, loss: 0.06688378006219864, acc: 96.875, f1: 94.01340996168582, r: 0.752914889866402
06/02/2019 09:22:55 step: 9349, epoch: 283, batch: 9, loss: 0.030957911163568497, acc: 100.0, f1: 100.0, r: 0.8295277542732171
06/02/2019 09:22:55 step: 9354, epoch: 283, batch: 14, loss: 0.05190369859337807, acc: 98.4375, f1: 98.82743795192496, r: 0.6778808749584284
06/02/2019 09:22:56 step: 9359, epoch: 283, batch: 19, loss: 0.13062743842601776, acc: 95.3125, f1: 94.64127490443279, r: 0.8011188637654214
06/02/2019 09:22:56 step: 9364, epoch: 283, batch: 24, loss: 0.03129442036151886, acc: 100.0, f1: 100.0, r: 0.7349435884902351
06/02/2019 09:22:56 step: 9369, epoch: 283, batch: 29, loss: 0.020019065588712692, acc: 98.4375, f1: 99.0096038415366, r: 0.8351506478322526
06/02/2019 09:22:56 *** evaluating ***
06/02/2019 09:22:57 step: 284, epoch: 283, acc: 61.111111111111114, f1: 27.46422448363206, r: 0.40616490974344216
06/02/2019 09:22:57 *** epoch: 285 ***
06/02/2019 09:22:57 *** training ***
06/02/2019 09:22:57 step: 9377, epoch: 284, batch: 4, loss: 0.032987192273139954, acc: 98.4375, f1: 98.30316742081449, r: 0.8338274667988601
06/02/2019 09:22:57 step: 9382, epoch: 284, batch: 9, loss: 0.05510495603084564, acc: 96.875, f1: 92.38095238095238, r: 0.6891506223778964
06/02/2019 09:22:58 step: 9387, epoch: 284, batch: 14, loss: 0.011329594999551773, acc: 100.0, f1: 100.0, r: 0.7033465122356302
06/02/2019 09:22:58 step: 9392, epoch: 284, batch: 19, loss: 0.028233231976628304, acc: 98.4375, f1: 99.0599876314162, r: 0.7324503794034849
06/02/2019 09:22:58 step: 9397, epoch: 284, batch: 24, loss: 0.03668554872274399, acc: 96.875, f1: 97.84425451092117, r: 0.6471173900731674
06/02/2019 09:22:59 step: 9402, epoch: 284, batch: 29, loss: 0.05251707136631012, acc: 96.875, f1: 97.0, r: 0.8093266559422084
06/02/2019 09:22:59 *** evaluating ***
06/02/2019 09:22:59 step: 285, epoch: 284, acc: 61.965811965811966, f1: 27.849264705882355, r: 0.4057431536712247
06/02/2019 09:22:59 *** epoch: 286 ***
06/02/2019 09:22:59 *** training ***
06/02/2019 09:22:59 step: 9410, epoch: 285, batch: 4, loss: 0.06039711460471153, acc: 96.875, f1: 95.63546876240375, r: 0.7780615050729649
06/02/2019 09:23:00 step: 9415, epoch: 285, batch: 9, loss: 0.09055904299020767, acc: 98.4375, f1: 98.74776386404294, r: 0.815725143090475
06/02/2019 09:23:00 step: 9420, epoch: 285, batch: 14, loss: 0.02268519625067711, acc: 100.0, f1: 100.0, r: 0.7956102158393732
06/02/2019 09:23:00 step: 9425, epoch: 285, batch: 19, loss: 0.0362931564450264, acc: 98.4375, f1: 96.83890577507597, r: 0.7163625276104263
06/02/2019 09:23:01 step: 9430, epoch: 285, batch: 24, loss: 0.03672017157077789, acc: 98.4375, f1: 94.04761904761905, r: 0.7304993368002771
06/02/2019 09:23:01 step: 9435, epoch: 285, batch: 29, loss: 0.07276298105716705, acc: 95.3125, f1: 89.84883902134953, r: 0.6641284763948982
06/02/2019 09:23:01 *** evaluating ***
06/02/2019 09:23:01 step: 286, epoch: 285, acc: 61.965811965811966, f1: 28.53702766531714, r: 0.4103935577774887
06/02/2019 09:23:01 *** epoch: 287 ***
06/02/2019 09:23:01 *** training ***
06/02/2019 09:23:02 step: 9443, epoch: 286, batch: 4, loss: 0.0676712691783905, acc: 98.4375, f1: 95.71428571428571, r: 0.7036366814751465
06/02/2019 09:23:02 step: 9448, epoch: 286, batch: 9, loss: 0.03745280206203461, acc: 100.0, f1: 100.0, r: 0.8201024342285634
06/02/2019 09:23:02 step: 9453, epoch: 286, batch: 14, loss: 0.05154407396912575, acc: 98.4375, f1: 99.26314819931841, r: 0.6716278447324235
06/02/2019 09:23:03 step: 9458, epoch: 286, batch: 19, loss: 0.02343148924410343, acc: 100.0, f1: 100.0, r: 0.7175718129817193
06/02/2019 09:23:03 step: 9463, epoch: 286, batch: 24, loss: 0.01793193444609642, acc: 100.0, f1: 100.0, r: 0.7605639914813522
06/02/2019 09:23:03 step: 9468, epoch: 286, batch: 29, loss: 0.0686873346567154, acc: 96.875, f1: 97.51182033096927, r: 0.7716813172804272
06/02/2019 09:23:04 *** evaluating ***
06/02/2019 09:23:04 step: 287, epoch: 286, acc: 63.24786324786324, f1: 31.97011408730159, r: 0.4116456407062786
06/02/2019 09:23:04 *** epoch: 288 ***
06/02/2019 09:23:04 *** training ***
06/02/2019 09:23:04 step: 9476, epoch: 287, batch: 4, loss: 0.13434138894081116, acc: 92.1875, f1: 89.68533506477203, r: 0.7694042606373708
06/02/2019 09:23:04 step: 9481, epoch: 287, batch: 9, loss: 0.053178925067186356, acc: 96.875, f1: 94.86592940463566, r: 0.76561048322843
06/02/2019 09:23:05 step: 9486, epoch: 287, batch: 14, loss: 0.03310135006904602, acc: 100.0, f1: 100.0, r: 0.7623331340645039
06/02/2019 09:23:05 step: 9491, epoch: 287, batch: 19, loss: 0.031108781695365906, acc: 98.4375, f1: 98.12987012987013, r: 0.6899607588726898
06/02/2019 09:23:05 step: 9496, epoch: 287, batch: 24, loss: 0.03194570168852806, acc: 98.4375, f1: 96.8922305764411, r: 0.6521857480618077
06/02/2019 09:23:06 step: 9501, epoch: 287, batch: 29, loss: 0.07894310355186462, acc: 98.4375, f1: 97.61904761904762, r: 0.7383536307129736
06/02/2019 09:23:06 *** evaluating ***
06/02/2019 09:23:06 step: 288, epoch: 287, acc: 62.82051282051282, f1: 28.794609036796537, r: 0.41283198117369785
06/02/2019 09:23:06 *** epoch: 289 ***
06/02/2019 09:23:06 *** training ***
06/02/2019 09:23:06 step: 9509, epoch: 288, batch: 4, loss: 0.0446050725877285, acc: 98.4375, f1: 98.78729080766433, r: 0.607020183982262
06/02/2019 09:23:07 step: 9514, epoch: 288, batch: 9, loss: 0.12372396141290665, acc: 95.3125, f1: 96.0239651416122, r: 0.7600553349473201
06/02/2019 09:23:07 step: 9519, epoch: 288, batch: 14, loss: 0.05533016473054886, acc: 96.875, f1: 96.14512471655328, r: 0.7549454337797782
06/02/2019 09:23:07 step: 9524, epoch: 288, batch: 19, loss: 0.06232374534010887, acc: 98.4375, f1: 99.33797909407666, r: 0.8451260936823868
06/02/2019 09:23:08 step: 9529, epoch: 288, batch: 24, loss: 0.07708131521940231, acc: 98.4375, f1: 98.40975351179434, r: 0.7330054724432884
06/02/2019 09:23:08 step: 9534, epoch: 288, batch: 29, loss: 0.02534901164472103, acc: 100.0, f1: 100.0, r: 0.6893134054543008
06/02/2019 09:23:08 *** evaluating ***
06/02/2019 09:23:08 step: 289, epoch: 288, acc: 62.39316239316239, f1: 28.387531971919135, r: 0.40941748695923114
06/02/2019 09:23:08 *** epoch: 290 ***
06/02/2019 09:23:08 *** training ***
06/02/2019 09:23:09 step: 9542, epoch: 289, batch: 4, loss: 0.033628396689891815, acc: 98.4375, f1: 96.1111111111111, r: 0.7716495130966565
06/02/2019 09:23:09 step: 9547, epoch: 289, batch: 9, loss: 0.04357270896434784, acc: 98.4375, f1: 98.70370370370371, r: 0.7525429626147477
06/02/2019 09:23:09 step: 9552, epoch: 289, batch: 14, loss: 0.06356093287467957, acc: 96.875, f1: 98.08673469387756, r: 0.6831244769541537
06/02/2019 09:23:09 step: 9557, epoch: 289, batch: 19, loss: 0.11817494034767151, acc: 98.4375, f1: 99.12462006079026, r: 0.7064052407300581
06/02/2019 09:23:10 step: 9562, epoch: 289, batch: 24, loss: 0.11103454977273941, acc: 96.875, f1: 93.16168327796235, r: 0.7667373995205687
06/02/2019 09:23:10 step: 9567, epoch: 289, batch: 29, loss: 0.025023922324180603, acc: 98.4375, f1: 99.29118773946361, r: 0.8198411575535984
06/02/2019 09:23:10 *** evaluating ***
06/02/2019 09:23:10 step: 290, epoch: 289, acc: 61.965811965811966, f1: 29.394860671343725, r: 0.4150671447536893
06/02/2019 09:23:10 *** epoch: 291 ***
06/02/2019 09:23:10 *** training ***
06/02/2019 09:23:11 step: 9575, epoch: 290, batch: 4, loss: 0.04938742518424988, acc: 96.875, f1: 86.31530487985981, r: 0.7775664429372058
06/02/2019 09:23:11 step: 9580, epoch: 290, batch: 9, loss: 0.06339632719755173, acc: 98.4375, f1: 96.33699633699634, r: 0.56199124667795
06/02/2019 09:23:11 step: 9585, epoch: 290, batch: 14, loss: 0.034743864089250565, acc: 98.4375, f1: 98.2051282051282, r: 0.7472516095374694
06/02/2019 09:23:12 step: 9590, epoch: 290, batch: 19, loss: 0.0702347531914711, acc: 96.875, f1: 93.71657754010695, r: 0.7344934309075914
06/02/2019 09:23:12 step: 9595, epoch: 290, batch: 24, loss: 0.11748582124710083, acc: 93.75, f1: 93.9393215480172, r: 0.6704931143635838
06/02/2019 09:23:12 step: 9600, epoch: 290, batch: 29, loss: 0.013765271753072739, acc: 100.0, f1: 100.0, r: 0.7467876755861184
06/02/2019 09:23:12 *** evaluating ***
06/02/2019 09:23:12 step: 291, epoch: 290, acc: 62.82051282051282, f1: 31.51358830552263, r: 0.4163989979392759
06/02/2019 09:23:12 *** epoch: 292 ***
06/02/2019 09:23:12 *** training ***
06/02/2019 09:23:13 step: 9608, epoch: 291, batch: 4, loss: 0.04329511150717735, acc: 100.0, f1: 100.0, r: 0.7497270712703127
06/02/2019 09:23:13 step: 9613, epoch: 291, batch: 9, loss: 0.06765271723270416, acc: 96.875, f1: 95.56505606195047, r: 0.7074362944200963
06/02/2019 09:23:13 step: 9618, epoch: 291, batch: 14, loss: 0.03880658000707626, acc: 100.0, f1: 100.0, r: 0.750167824581808
06/02/2019 09:23:14 step: 9623, epoch: 291, batch: 19, loss: 0.11814893037080765, acc: 96.875, f1: 97.57496041895018, r: 0.7247560679626529
06/02/2019 09:23:14 step: 9628, epoch: 291, batch: 24, loss: 0.050401754677295685, acc: 98.4375, f1: 99.03961584633853, r: 0.7094309303859461
06/02/2019 09:23:14 step: 9633, epoch: 291, batch: 29, loss: 0.06200851500034332, acc: 98.4375, f1: 94.55782312925169, r: 0.6488096868713383
06/02/2019 09:23:15 *** evaluating ***
06/02/2019 09:23:15 step: 292, epoch: 291, acc: 62.39316239316239, f1: 28.523546765734263, r: 0.4179264706186755
06/02/2019 09:23:15 *** epoch: 293 ***
06/02/2019 09:23:15 *** training ***
06/02/2019 09:23:15 step: 9641, epoch: 292, batch: 4, loss: 0.10324655473232269, acc: 95.3125, f1: 94.28454715219421, r: 0.7762659196992377
06/02/2019 09:23:15 step: 9646, epoch: 292, batch: 9, loss: 0.06023690477013588, acc: 98.4375, f1: 99.35358758888171, r: 0.6907775251608713
06/02/2019 09:23:16 step: 9651, epoch: 292, batch: 14, loss: 0.027701569721102715, acc: 100.0, f1: 100.0, r: 0.6917263786534935
06/02/2019 09:23:16 step: 9656, epoch: 292, batch: 19, loss: 0.07748305052518845, acc: 95.3125, f1: 95.62645687645687, r: 0.796103576993238
06/02/2019 09:23:16 step: 9661, epoch: 292, batch: 24, loss: 0.031720083206892014, acc: 100.0, f1: 100.0, r: 0.7862353421876781
06/02/2019 09:23:17 step: 9666, epoch: 292, batch: 29, loss: 0.023949656635522842, acc: 100.0, f1: 100.0, r: 0.6796850836543732
06/02/2019 09:23:17 *** evaluating ***
06/02/2019 09:23:17 step: 293, epoch: 292, acc: 62.39316239316239, f1: 31.26906318082788, r: 0.4168957907088257
06/02/2019 09:23:17 *** epoch: 294 ***
06/02/2019 09:23:17 *** training ***
06/02/2019 09:23:17 step: 9674, epoch: 293, batch: 4, loss: 0.02666410431265831, acc: 100.0, f1: 100.0, r: 0.7037858443744247
06/02/2019 09:23:18 step: 9679, epoch: 293, batch: 9, loss: 0.07817120850086212, acc: 96.875, f1: 92.98595360886796, r: 0.644012113953215
06/02/2019 09:23:18 step: 9684, epoch: 293, batch: 14, loss: 0.02893558144569397, acc: 98.4375, f1: 99.0599876314162, r: 0.6884041529380291
06/02/2019 09:23:18 step: 9689, epoch: 293, batch: 19, loss: 0.01797039434313774, acc: 100.0, f1: 100.0, r: 0.7095965618617079
06/02/2019 09:23:19 step: 9694, epoch: 293, batch: 24, loss: 0.03880155831575394, acc: 100.0, f1: 100.0, r: 0.7958824935105644
06/02/2019 09:23:19 step: 9699, epoch: 293, batch: 29, loss: 0.044779904186725616, acc: 98.4375, f1: 98.16207184628237, r: 0.7593885707217526
06/02/2019 09:23:19 *** evaluating ***
06/02/2019 09:23:19 step: 294, epoch: 293, acc: 61.965811965811966, f1: 30.027076001340706, r: 0.41087547499850063
06/02/2019 09:23:19 *** epoch: 295 ***
06/02/2019 09:23:19 *** training ***
06/02/2019 09:23:20 step: 9707, epoch: 294, batch: 4, loss: 0.05696532875299454, acc: 96.875, f1: 94.50875211744777, r: 0.692747396229465
06/02/2019 09:23:20 step: 9712, epoch: 294, batch: 9, loss: 0.04612076282501221, acc: 98.4375, f1: 99.01577652337531, r: 0.70795266143501
06/02/2019 09:23:20 step: 9717, epoch: 294, batch: 14, loss: 0.04593247175216675, acc: 96.875, f1: 98.01587301587303, r: 0.5421043884523638
06/02/2019 09:23:21 step: 9722, epoch: 294, batch: 19, loss: 0.021488826721906662, acc: 100.0, f1: 100.0, r: 0.6305177221183546
06/02/2019 09:23:21 step: 9727, epoch: 294, batch: 24, loss: 0.020765284076333046, acc: 100.0, f1: 100.0, r: 0.7807407677064829
06/02/2019 09:23:21 step: 9732, epoch: 294, batch: 29, loss: 0.06567857414484024, acc: 96.875, f1: 98.67724867724867, r: 0.6630556967937816
06/02/2019 09:23:21 *** evaluating ***
06/02/2019 09:23:22 step: 295, epoch: 294, acc: 61.53846153846154, f1: 27.203942635754508, r: 0.4083534671484393
06/02/2019 09:23:22 *** epoch: 296 ***
06/02/2019 09:23:22 *** training ***
06/02/2019 09:23:22 step: 9740, epoch: 295, batch: 4, loss: 0.05006103962659836, acc: 96.875, f1: 95.26200417273827, r: 0.7717988957225641
06/02/2019 09:23:22 step: 9745, epoch: 295, batch: 9, loss: 0.029565371572971344, acc: 98.4375, f1: 95.78947368421052, r: 0.7464935538819533
06/02/2019 09:23:22 step: 9750, epoch: 295, batch: 14, loss: 0.023607546463608742, acc: 100.0, f1: 100.0, r: 0.7823393464027719
06/02/2019 09:23:23 step: 9755, epoch: 295, batch: 19, loss: 0.02027890831232071, acc: 100.0, f1: 100.0, r: 0.6103048779325306
06/02/2019 09:23:23 step: 9760, epoch: 295, batch: 24, loss: 0.044622186571359634, acc: 98.4375, f1: 99.22171018945212, r: 0.6983124168938892
06/02/2019 09:23:23 step: 9765, epoch: 295, batch: 29, loss: 0.11199716478586197, acc: 98.4375, f1: 97.03703703703704, r: 0.7489910494001306
06/02/2019 09:23:23 *** evaluating ***
06/02/2019 09:23:24 step: 296, epoch: 295, acc: 62.39316239316239, f1: 30.363787031824728, r: 0.4098368852122949
06/02/2019 09:23:24 *** epoch: 297 ***
06/02/2019 09:23:24 *** training ***
06/02/2019 09:23:24 step: 9773, epoch: 296, batch: 4, loss: 0.04189828410744667, acc: 98.4375, f1: 98.20868786386028, r: 0.7061199769762152
06/02/2019 09:23:24 step: 9778, epoch: 296, batch: 9, loss: 0.02868572622537613, acc: 100.0, f1: 100.0, r: 0.6618037085216587
06/02/2019 09:23:24 step: 9783, epoch: 296, batch: 14, loss: 0.08244334906339645, acc: 96.875, f1: 97.86321671525754, r: 0.685489975187098
06/02/2019 09:23:25 step: 9788, epoch: 296, batch: 19, loss: 0.037819813936948776, acc: 98.4375, f1: 99.04108096327776, r: 0.6760381174171914
06/02/2019 09:23:25 step: 9793, epoch: 296, batch: 24, loss: 0.12201683968305588, acc: 96.875, f1: 98.29166666666667, r: 0.6555245318319411
06/02/2019 09:23:25 step: 9798, epoch: 296, batch: 29, loss: 0.06123913824558258, acc: 98.4375, f1: 87.12121212121212, r: 0.761841665789631
06/02/2019 09:23:26 *** evaluating ***
06/02/2019 09:23:26 step: 297, epoch: 296, acc: 61.965811965811966, f1: 30.193454360526346, r: 0.4083505251589844
06/02/2019 09:23:26 *** epoch: 298 ***
06/02/2019 09:23:26 *** training ***
06/02/2019 09:23:26 step: 9806, epoch: 297, batch: 4, loss: 0.02584594488143921, acc: 100.0, f1: 100.0, r: 0.6700131411205562
06/02/2019 09:23:26 step: 9811, epoch: 297, batch: 9, loss: 0.06715156883001328, acc: 95.3125, f1: 95.85414585414586, r: 0.6772982302408433
06/02/2019 09:23:27 step: 9816, epoch: 297, batch: 14, loss: 0.06446633487939835, acc: 96.875, f1: 95.40006478781989, r: 0.6783313994308678
06/02/2019 09:23:27 step: 9821, epoch: 297, batch: 19, loss: 0.030610382556915283, acc: 98.4375, f1: 99.26415094339622, r: 0.7804086206165616
06/02/2019 09:23:27 step: 9826, epoch: 297, batch: 24, loss: 0.04198579117655754, acc: 98.4375, f1: 98.57142857142858, r: 0.7928965770602587
06/02/2019 09:23:28 step: 9831, epoch: 297, batch: 29, loss: 0.0329442098736763, acc: 98.4375, f1: 96.76470588235294, r: 0.7231065501474513
06/02/2019 09:23:28 *** evaluating ***
06/02/2019 09:23:28 step: 298, epoch: 297, acc: 62.39316239316239, f1: 31.455577821295645, r: 0.41132721546218026
06/02/2019 09:23:28 *** epoch: 299 ***
06/02/2019 09:23:28 *** training ***
06/02/2019 09:23:28 step: 9839, epoch: 298, batch: 4, loss: 0.03707429766654968, acc: 100.0, f1: 100.0, r: 0.7896720885170881
06/02/2019 09:23:29 step: 9844, epoch: 298, batch: 9, loss: 0.07783354818820953, acc: 95.3125, f1: 89.41307734411183, r: 0.598060385440611
06/02/2019 09:23:29 step: 9849, epoch: 298, batch: 14, loss: 0.10730103403329849, acc: 95.3125, f1: 95.3395162756865, r: 0.6898673375145427
06/02/2019 09:23:29 step: 9854, epoch: 298, batch: 19, loss: 0.0895930603146553, acc: 96.875, f1: 95.88056680161944, r: 0.7190157254428596
06/02/2019 09:23:30 step: 9859, epoch: 298, batch: 24, loss: 0.008716009557247162, acc: 100.0, f1: 100.0, r: 0.7333555032126736
06/02/2019 09:23:30 step: 9864, epoch: 298, batch: 29, loss: 0.09531254321336746, acc: 98.4375, f1: 98.86811867604185, r: 0.6799673867870485
06/02/2019 09:23:30 *** evaluating ***
06/02/2019 09:23:30 step: 299, epoch: 298, acc: 61.965811965811966, f1: 27.023143090228196, r: 0.4064835360031714
06/02/2019 09:23:30 *** epoch: 300 ***
06/02/2019 09:23:30 *** training ***
06/02/2019 09:23:30 step: 9872, epoch: 299, batch: 4, loss: 0.01912793144583702, acc: 100.0, f1: 100.0, r: 0.6959127960469661
06/02/2019 09:23:31 step: 9877, epoch: 299, batch: 9, loss: 0.04819686710834503, acc: 98.4375, f1: 87.27272727272728, r: 0.7563195150476365
06/02/2019 09:23:31 step: 9882, epoch: 299, batch: 14, loss: 0.05154544115066528, acc: 98.4375, f1: 99.25893635571055, r: 0.8161717961435672
06/02/2019 09:23:31 step: 9887, epoch: 299, batch: 19, loss: 0.03426671400666237, acc: 98.4375, f1: 99.06432748538012, r: 0.8162855480860993
06/02/2019 09:23:32 step: 9892, epoch: 299, batch: 24, loss: 0.05250213295221329, acc: 98.4375, f1: 98.38383838383838, r: 0.6936655292241175
06/02/2019 09:23:32 step: 9897, epoch: 299, batch: 29, loss: 0.0078815259039402, acc: 100.0, f1: 100.0, r: 0.7363487144505443
06/02/2019 09:23:32 *** evaluating ***
06/02/2019 09:23:32 step: 300, epoch: 299, acc: 62.39316239316239, f1: 30.24986185814432, r: 0.41161515233751966
06/02/2019 09:23:32 
*** Best acc model ***
epoch: 192
acc: 63.67521367521367
f1: 32.25029327970504
corr: 0.41505851714723413
06/02/2019 09:23:32 Loading Test Data
06/02/2019 09:23:32 load data from data/word2vec_temp/test_text.npy, data/word2vec_temp/test_label.npy, training: False
06/02/2019 09:23:59 loaded. total len: 2228
06/02/2019 09:23:59 Test: length: 2228, total batch: 35, batch size: 64
06/02/2019 09:23:59 
*** Test Result ***
acc: 62.39316239316239
f1: 30.24986185814432
corr: 0.41161515233751966
