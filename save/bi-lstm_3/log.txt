06/02/2019 09:51:05 {'input_path': 'data/word2vec_temp', 'output_path': 'save/bi-lstm_3', 'gpu': True, 'seed': 20000125, 'display_per_batch': 5, 'optimizer': 'adagrad', 'lr': 0.01, 'lr_decay': 0, 'weight_decay': 0.0001, 'momentum': 0.985, 'num_epochs': 300, 'batch_size': 64, 'num_labels': 8, 'embedding_size': 300, 'type': 'rnn', 'rnn': {'type': 'lstm', 'bidirectional': True, 'rnn_hidden_size': 256, 'mlp_hidden_size': 1024, 'dropout': 0.9, 'p_coefficient': 0.3, 'num_layers': 1, 'param_da': 350, 'param_r': 30, 'loss': 'cross_entropy'}}
06/02/2019 09:51:05 Loading Train Data
06/02/2019 09:51:05 load data from data/word2vec_temp/train_text.npy, data/word2vec_temp/train_label.npy, training: True
06/02/2019 09:51:27 loaded. total len: 2342
06/02/2019 09:51:27 Train: length: 2108, total batch: 33, batch size: 64
06/02/2019 09:51:27 Dev: length: 234, total batch: 4, batch size: 64
06/02/2019 09:51:27 Loading model rnn
06/02/2019 09:51:36 *** epoch: 1 ***
06/02/2019 09:51:36 *** training ***
06/02/2019 09:51:37 step: 5, epoch: 0, batch: 4, loss: 15.101044654846191, acc: 15.625, f1: 6.562137049941928, r: -0.0485510256313319
06/02/2019 09:51:38 step: 10, epoch: 0, batch: 9, loss: 6.967242240905762, acc: 14.0625, f1: 8.548280423280422, r: -0.023298355217346065
06/02/2019 09:51:39 step: 15, epoch: 0, batch: 14, loss: 4.248515605926514, acc: 21.875, f1: 8.786848072562357, r: 0.06408544759200567
06/02/2019 09:51:40 step: 20, epoch: 0, batch: 19, loss: 3.0638747215270996, acc: 35.9375, f1: 8.424908424908425, r: -0.021089916862568676
06/02/2019 09:51:42 step: 25, epoch: 0, batch: 24, loss: 3.1000816822052, acc: 28.125, f1: 9.25848345203184, r: -0.11640353143684624
06/02/2019 09:51:43 step: 30, epoch: 0, batch: 29, loss: 2.719721555709839, acc: 32.8125, f1: 12.140171858216972, r: 0.042645172735700745
06/02/2019 09:51:43 *** evaluating ***
06/02/2019 09:51:44 step: 1, epoch: 0, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.23694046050761708
06/02/2019 09:51:44 *** epoch: 2 ***
06/02/2019 09:51:44 *** training ***
06/02/2019 09:51:45 step: 38, epoch: 1, batch: 4, loss: 2.6498501300811768, acc: 26.5625, f1: 10.553018772196856, r: 0.04840432853478564
06/02/2019 09:51:46 step: 43, epoch: 1, batch: 9, loss: 2.3999218940734863, acc: 31.25, f1: 7.187500000000001, r: 0.06932095354507281
06/02/2019 09:51:47 step: 48, epoch: 1, batch: 14, loss: 2.1796035766601562, acc: 48.4375, f1: 16.61434236615995, r: 0.08072641775940506
06/02/2019 09:51:48 step: 53, epoch: 1, batch: 19, loss: 2.257993221282959, acc: 46.875, f1: 11.111111111111112, r: 0.038082377324788134
06/02/2019 09:51:49 step: 58, epoch: 1, batch: 24, loss: 2.4842448234558105, acc: 25.0, f1: 6.190476190476191, r: 0.01648812147423641
06/02/2019 09:51:50 step: 63, epoch: 1, batch: 29, loss: 2.341831684112549, acc: 31.25, f1: 6.968641114982578, r: -0.02399206536655837
06/02/2019 09:51:51 *** evaluating ***
06/02/2019 09:51:51 step: 2, epoch: 1, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.21597081725401218
06/02/2019 09:51:51 *** epoch: 3 ***
06/02/2019 09:51:51 *** training ***
06/02/2019 09:51:52 step: 71, epoch: 2, batch: 4, loss: 2.220278263092041, acc: 31.25, f1: 6.905864197530863, r: 0.06561244742203612
06/02/2019 09:51:53 step: 76, epoch: 2, batch: 9, loss: 2.1189351081848145, acc: 48.4375, f1: 9.32330827067669, r: 0.010225193172621196
06/02/2019 09:51:54 step: 81, epoch: 2, batch: 14, loss: 2.040372133255005, acc: 43.75, f1: 8.79120879120879, r: 0.07697694770335842
06/02/2019 09:51:55 step: 86, epoch: 2, batch: 19, loss: 2.4133858680725098, acc: 32.8125, f1: 6.25, r: 0.028356703617843676
06/02/2019 09:51:57 step: 91, epoch: 2, batch: 24, loss: 2.243866443634033, acc: 45.3125, f1: 7.795698924731183, r: 0.012212803056989336
06/02/2019 09:51:58 step: 96, epoch: 2, batch: 29, loss: 2.262629747390747, acc: 43.75, f1: 7.777777777777778, r: 0.02808338728285457
06/02/2019 09:51:58 *** evaluating ***
06/02/2019 09:51:59 step: 3, epoch: 2, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.2359494307480166
06/02/2019 09:51:59 *** epoch: 4 ***
06/02/2019 09:51:59 *** training ***
06/02/2019 09:52:00 step: 104, epoch: 3, batch: 4, loss: 2.1672167778015137, acc: 40.625, f1: 7.222222222222221, r: 0.0936978103065407
06/02/2019 09:52:01 step: 109, epoch: 3, batch: 9, loss: 2.00662899017334, acc: 57.8125, f1: 12.755102040816327, r: 0.10014277587560846
06/02/2019 09:52:02 step: 114, epoch: 3, batch: 14, loss: 2.2193572521209717, acc: 28.125, f1: 5.487804878048781, r: 0.05387583425731686
06/02/2019 09:52:03 step: 119, epoch: 3, batch: 19, loss: 2.364588737487793, acc: 35.9375, f1: 8.015394912985274, r: 0.019143309865915954
06/02/2019 09:52:05 step: 124, epoch: 3, batch: 24, loss: 2.3054161071777344, acc: 34.375, f1: 7.308970099667775, r: 0.09984314299673847
06/02/2019 09:52:06 step: 129, epoch: 3, batch: 29, loss: 2.0974502563476562, acc: 39.0625, f1: 8.563218390804597, r: 0.03484839657208923
06/02/2019 09:52:06 *** evaluating ***
06/02/2019 09:52:07 step: 4, epoch: 3, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.2598981400442055
06/02/2019 09:52:07 *** epoch: 5 ***
06/02/2019 09:52:07 *** training ***
06/02/2019 09:52:08 step: 137, epoch: 4, batch: 4, loss: 2.0026283264160156, acc: 45.3125, f1: 12.868797868797868, r: 0.089576519916548
06/02/2019 09:52:09 step: 142, epoch: 4, batch: 9, loss: 2.1804542541503906, acc: 31.25, f1: 7.7564102564102555, r: 0.1458153861105184
06/02/2019 09:52:10 step: 147, epoch: 4, batch: 14, loss: 1.983214020729065, acc: 48.4375, f1: 8.157894736842106, r: 0.06633531623444408
06/02/2019 09:52:11 step: 152, epoch: 4, batch: 19, loss: 2.141695976257324, acc: 42.1875, f1: 8.47723704866562, r: 0.10796353588398463
06/02/2019 09:52:12 step: 157, epoch: 4, batch: 24, loss: 2.1788949966430664, acc: 32.8125, f1: 7.0588235294117645, r: 0.023862748640434146
06/02/2019 09:52:13 step: 162, epoch: 4, batch: 29, loss: 2.2251999378204346, acc: 34.375, f1: 6.470588235294117, r: 0.06921296523407659
06/02/2019 09:52:14 *** evaluating ***
06/02/2019 09:52:14 step: 5, epoch: 4, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.2633361067432444
06/02/2019 09:52:14 *** epoch: 6 ***
06/02/2019 09:52:14 *** training ***
06/02/2019 09:52:15 step: 170, epoch: 5, batch: 4, loss: 2.140984058380127, acc: 31.25, f1: 6.493506493506494, r: 0.037140578032116824
06/02/2019 09:52:17 step: 175, epoch: 5, batch: 9, loss: 2.108700752258301, acc: 34.375, f1: 7.916666666666666, r: 0.17657531957833644
06/02/2019 09:52:18 step: 180, epoch: 5, batch: 14, loss: 2.144486427307129, acc: 40.625, f1: 8.346709470304976, r: 0.04211109584556245
06/02/2019 09:52:19 step: 185, epoch: 5, batch: 19, loss: 1.9972689151763916, acc: 45.3125, f1: 7.795698924731183, r: 0.08843420654802903
06/02/2019 09:52:20 step: 190, epoch: 5, batch: 24, loss: 2.252455234527588, acc: 35.9375, f1: 8.02310654685494, r: -0.05361021123462578
06/02/2019 09:52:21 step: 195, epoch: 5, batch: 29, loss: 2.042048454284668, acc: 26.5625, f1: 6.227106227106226, r: 0.1190577131817279
06/02/2019 09:52:22 *** evaluating ***
06/02/2019 09:52:22 step: 6, epoch: 5, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.2417282605690142
06/02/2019 09:52:22 *** epoch: 7 ***
06/02/2019 09:52:22 *** training ***
06/02/2019 09:52:23 step: 203, epoch: 6, batch: 4, loss: 2.046658515930176, acc: 35.9375, f1: 6.845238095238095, r: 0.0460038277984697
06/02/2019 09:52:24 step: 208, epoch: 6, batch: 9, loss: 2.1420438289642334, acc: 34.375, f1: 6.395348837209303, r: 0.07008588337301211
06/02/2019 09:52:25 step: 213, epoch: 6, batch: 14, loss: 2.0852675437927246, acc: 35.9375, f1: 6.609195402298851, r: 0.09906719938936424
06/02/2019 09:52:26 step: 218, epoch: 6, batch: 19, loss: 1.9325220584869385, acc: 43.75, f1: 8.695652173913045, r: 0.11891634846618183
06/02/2019 09:52:28 step: 223, epoch: 6, batch: 24, loss: 1.9631788730621338, acc: 39.0625, f1: 7.02247191011236, r: 0.1319185665336721
06/02/2019 09:52:29 step: 228, epoch: 6, batch: 29, loss: 1.9978880882263184, acc: 39.0625, f1: 8.025682182985554, r: 0.18468691777021973
06/02/2019 09:52:30 *** evaluating ***
06/02/2019 09:52:30 step: 7, epoch: 6, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.2547091839356288
06/02/2019 09:52:30 *** epoch: 8 ***
06/02/2019 09:52:30 *** training ***
06/02/2019 09:52:31 step: 236, epoch: 7, batch: 4, loss: 1.961294412612915, acc: 39.0625, f1: 10.178571428571429, r: 0.11131062499847744
06/02/2019 09:52:32 step: 241, epoch: 7, batch: 9, loss: 1.9988250732421875, acc: 39.0625, f1: 8.210180623973727, r: 0.0469951349572148
06/02/2019 09:52:34 step: 246, epoch: 7, batch: 14, loss: 1.881812334060669, acc: 45.3125, f1: 11.17216117216117, r: 0.15136232178463044
06/02/2019 09:52:35 step: 251, epoch: 7, batch: 19, loss: 2.139998197555542, acc: 28.125, f1: 8.512540290088639, r: 0.07485830146219696
06/02/2019 09:52:36 step: 256, epoch: 7, batch: 24, loss: 1.9722274541854858, acc: 42.1875, f1: 8.47723704866562, r: 0.053268181603897444
06/02/2019 09:52:37 step: 261, epoch: 7, batch: 29, loss: 2.021610736846924, acc: 40.625, f1: 7.222222222222221, r: 0.11897890282255706
06/02/2019 09:52:37 *** evaluating ***
06/02/2019 09:52:38 step: 8, epoch: 7, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.2556166470397741
06/02/2019 09:52:38 *** epoch: 9 ***
06/02/2019 09:52:38 *** training ***
06/02/2019 09:52:39 step: 269, epoch: 8, batch: 4, loss: 1.9007151126861572, acc: 45.3125, f1: 12.545018007202883, r: 0.1454960599778972
06/02/2019 09:52:40 step: 274, epoch: 8, batch: 9, loss: 1.8986892700195312, acc: 40.625, f1: 10.045219638242894, r: 0.06211531903357693
06/02/2019 09:52:41 step: 279, epoch: 8, batch: 14, loss: 1.9967374801635742, acc: 32.8125, f1: 7.809810671256455, r: 0.09360750571434187
06/02/2019 09:52:42 step: 284, epoch: 8, batch: 19, loss: 1.9661011695861816, acc: 32.8125, f1: 7.0588235294117645, r: 0.17254306571438838
06/02/2019 09:52:43 step: 289, epoch: 8, batch: 24, loss: 1.9504772424697876, acc: 43.75, f1: 7.692307692307692, r: 0.13541144998561264
06/02/2019 09:52:44 step: 294, epoch: 8, batch: 29, loss: 1.7504689693450928, acc: 39.0625, f1: 10.401855870687719, r: 0.14639035753193763
06/02/2019 09:52:45 *** evaluating ***
06/02/2019 09:52:46 step: 9, epoch: 8, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.2777145887484264
06/02/2019 09:52:46 *** epoch: 10 ***
06/02/2019 09:52:46 *** training ***
06/02/2019 09:52:47 step: 302, epoch: 9, batch: 4, loss: 1.892009973526001, acc: 42.1875, f1: 11.93376575436376, r: 0.11317283061393052
06/02/2019 09:52:48 step: 307, epoch: 9, batch: 9, loss: 1.8697130680084229, acc: 32.8125, f1: 8.433734939759036, r: 0.146526912609769
06/02/2019 09:52:49 step: 312, epoch: 9, batch: 14, loss: 1.960162878036499, acc: 34.375, f1: 7.720588235294118, r: 0.13072732492447975
06/02/2019 09:52:50 step: 317, epoch: 9, batch: 19, loss: 1.8843154907226562, acc: 37.5, f1: 7.881773399014777, r: 0.14000879785568848
06/02/2019 09:52:51 step: 322, epoch: 9, batch: 24, loss: 1.9634127616882324, acc: 31.25, f1: 6.884681583476763, r: 0.09745881141439997
06/02/2019 09:52:52 step: 327, epoch: 9, batch: 29, loss: 1.810034990310669, acc: 39.0625, f1: 9.249471458773783, r: 0.13787841163339262
06/02/2019 09:52:53 *** evaluating ***
06/02/2019 09:52:53 step: 10, epoch: 9, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.27393386956729493
06/02/2019 09:52:53 *** epoch: 11 ***
06/02/2019 09:52:53 *** training ***
06/02/2019 09:52:54 step: 335, epoch: 10, batch: 4, loss: 1.905744194984436, acc: 42.1875, f1: 10.346320346320347, r: 0.10518882798639254
06/02/2019 09:52:55 step: 340, epoch: 10, batch: 9, loss: 1.5772227048873901, acc: 40.625, f1: 9.986320109439125, r: 0.2141013295493326
06/02/2019 09:52:56 step: 345, epoch: 10, batch: 14, loss: 1.9081037044525146, acc: 35.9375, f1: 9.957943291276626, r: 0.08299966669369237
06/02/2019 09:52:58 step: 350, epoch: 10, batch: 19, loss: 1.8955729007720947, acc: 43.75, f1: 10.612244897959185, r: 0.16157572213743973
06/02/2019 09:52:59 step: 355, epoch: 10, batch: 24, loss: 1.7679811716079712, acc: 37.5, f1: 8.687782805429864, r: 0.20414083153510115
06/02/2019 09:53:00 step: 360, epoch: 10, batch: 29, loss: 1.9397646188735962, acc: 35.9375, f1: 9.900542495479204, r: 0.20309957262170253
06/02/2019 09:53:01 *** evaluating ***
06/02/2019 09:53:01 step: 11, epoch: 10, acc: 46.58119658119658, f1: 9.692340791738383, r: 0.2767482801125835
06/02/2019 09:53:01 *** epoch: 12 ***
06/02/2019 09:53:01 *** training ***
06/02/2019 09:53:02 step: 368, epoch: 11, batch: 4, loss: 1.7201956510543823, acc: 39.0625, f1: 8.025682182985552, r: 0.09013798197755012
06/02/2019 09:53:03 step: 373, epoch: 11, batch: 9, loss: 1.9396381378173828, acc: 31.25, f1: 7.530864197530865, r: 0.13429774898387276
06/02/2019 09:53:04 step: 378, epoch: 11, batch: 14, loss: 1.884174108505249, acc: 29.6875, f1: 5.72289156626506, r: 0.2717177639635514
06/02/2019 09:53:05 step: 383, epoch: 11, batch: 19, loss: 1.8702709674835205, acc: 42.1875, f1: 9.886363636363637, r: 0.12723453133442642
06/02/2019 09:53:07 step: 388, epoch: 11, batch: 24, loss: 1.7372230291366577, acc: 39.0625, f1: 10.079575596816976, r: 0.20843325597299428
06/02/2019 09:53:08 step: 393, epoch: 11, batch: 29, loss: 2.0026206970214844, acc: 28.125, f1: 7.302823758519961, r: 0.1542003947041598
06/02/2019 09:53:08 *** evaluating ***
06/02/2019 09:53:09 step: 12, epoch: 11, acc: 45.72649572649573, f1: 8.769709664281143, r: 0.2905672971726935
06/02/2019 09:53:09 *** epoch: 13 ***
06/02/2019 09:53:09 *** training ***
06/02/2019 09:53:10 step: 401, epoch: 12, batch: 4, loss: 1.7417190074920654, acc: 40.625, f1: 8.253968253968253, r: 0.122139069376733
06/02/2019 09:53:11 step: 406, epoch: 12, batch: 9, loss: 1.740115761756897, acc: 39.0625, f1: 9.852941176470589, r: 0.12665571181857907
06/02/2019 09:53:12 step: 411, epoch: 12, batch: 14, loss: 1.6909379959106445, acc: 46.875, f1: 10.467032967032965, r: 0.14502979425878557
06/02/2019 09:53:13 step: 416, epoch: 12, batch: 19, loss: 1.7709773778915405, acc: 51.5625, f1: 11.546052631578947, r: 0.17852988911584086
06/02/2019 09:53:14 step: 421, epoch: 12, batch: 24, loss: 1.8110692501068115, acc: 37.5, f1: 11.473214285714286, r: 0.15605220651485066
06/02/2019 09:53:16 step: 426, epoch: 12, batch: 29, loss: 1.730555772781372, acc: 39.0625, f1: 10.830564784053157, r: 0.17080705973074622
06/02/2019 09:53:16 *** evaluating ***
06/02/2019 09:53:17 step: 13, epoch: 12, acc: 49.14529914529914, f1: 12.084405303779933, r: 0.2908601808335857
06/02/2019 09:53:17 *** epoch: 14 ***
06/02/2019 09:53:17 *** training ***
06/02/2019 09:53:18 step: 434, epoch: 13, batch: 4, loss: 1.7731106281280518, acc: 32.8125, f1: 7.883275261324041, r: 0.20055886236777842
06/02/2019 09:53:19 step: 439, epoch: 13, batch: 9, loss: 2.0258541107177734, acc: 45.3125, f1: 13.628899835796387, r: 0.09185709654945858
06/02/2019 09:53:20 step: 444, epoch: 13, batch: 14, loss: 1.8803948163986206, acc: 29.6875, f1: 6.7857142857142865, r: 0.14776063260585548
06/02/2019 09:53:21 step: 449, epoch: 13, batch: 19, loss: 1.7634005546569824, acc: 42.1875, f1: 9.803370786516853, r: 0.2883844325736906
06/02/2019 09:53:23 step: 454, epoch: 13, batch: 24, loss: 1.7232178449630737, acc: 37.5, f1: 7.792207792207792, r: 0.21776985001779736
06/02/2019 09:53:24 step: 459, epoch: 13, batch: 29, loss: 1.7314635515213013, acc: 43.75, f1: 8.79120879120879, r: 0.08599959032448751
06/02/2019 09:53:24 *** evaluating ***
06/02/2019 09:53:25 step: 14, epoch: 13, acc: 45.72649572649573, f1: 8.769709664281143, r: 0.2949984477250999
06/02/2019 09:53:25 *** epoch: 15 ***
06/02/2019 09:53:25 *** training ***
06/02/2019 09:53:26 step: 467, epoch: 14, batch: 4, loss: 1.6524524688720703, acc: 39.0625, f1: 8.116883116883116, r: 0.159169707611465
06/02/2019 09:53:27 step: 472, epoch: 14, batch: 9, loss: 1.8434466123580933, acc: 34.375, f1: 6.470588235294119, r: 0.10106518813051968
06/02/2019 09:53:28 step: 477, epoch: 14, batch: 14, loss: 1.766782283782959, acc: 35.9375, f1: 8.033088235294116, r: 0.2571869762180605
06/02/2019 09:53:29 step: 482, epoch: 14, batch: 19, loss: 1.6897135972976685, acc: 45.3125, f1: 9.427668539325843, r: 0.19310216543687703
06/02/2019 09:53:30 step: 487, epoch: 14, batch: 24, loss: 1.6990073919296265, acc: 43.75, f1: 7.608695652173914, r: 0.12563240956002597
06/02/2019 09:53:32 step: 492, epoch: 14, batch: 29, loss: 1.6069823503494263, acc: 45.3125, f1: 8.90937019969278, r: 0.25609225901214006
06/02/2019 09:53:32 *** evaluating ***
06/02/2019 09:53:32 step: 15, epoch: 14, acc: 45.72649572649573, f1: 8.769709664281143, r: 0.28942195858102016
06/02/2019 09:53:32 *** epoch: 16 ***
06/02/2019 09:53:32 *** training ***
06/02/2019 09:53:33 step: 500, epoch: 15, batch: 4, loss: 1.6969176530838013, acc: 35.9375, f1: 7.6411960132890355, r: 0.17630211231897622
06/02/2019 09:53:35 step: 505, epoch: 15, batch: 9, loss: 1.6210331916809082, acc: 39.0625, f1: 8.210180623973729, r: 0.13022823514454324
06/02/2019 09:53:36 step: 510, epoch: 15, batch: 14, loss: 1.7718210220336914, acc: 28.125, f1: 8.745684695051782, r: 0.2075495065365226
06/02/2019 09:53:37 step: 515, epoch: 15, batch: 19, loss: 1.6769264936447144, acc: 39.0625, f1: 9.060077519379844, r: 0.18825616662840183
06/02/2019 09:53:38 step: 520, epoch: 15, batch: 24, loss: 1.5669182538986206, acc: 37.5, f1: 12.896825396825399, r: 0.250465419800067
06/02/2019 09:53:39 step: 525, epoch: 15, batch: 29, loss: 1.9046372175216675, acc: 42.1875, f1: 9.886363636363637, r: 0.1683650212947782
06/02/2019 09:53:40 *** evaluating ***
06/02/2019 09:53:40 step: 16, epoch: 15, acc: 48.29059829059829, f1: 11.40282131661442, r: 0.2891075668985452
06/02/2019 09:53:40 *** epoch: 17 ***
06/02/2019 09:53:40 *** training ***
06/02/2019 09:53:41 step: 533, epoch: 16, batch: 4, loss: 1.607877492904663, acc: 39.0625, f1: 8.447712418300656, r: 0.1773747850842174
06/02/2019 09:53:42 step: 538, epoch: 16, batch: 9, loss: 1.5732372999191284, acc: 45.3125, f1: 9.105180533751962, r: 0.15583986082816179
06/02/2019 09:53:43 step: 543, epoch: 16, batch: 14, loss: 1.7791441679000854, acc: 32.8125, f1: 7.8395061728395055, r: 0.2004430166749773
06/02/2019 09:53:45 step: 548, epoch: 16, batch: 19, loss: 1.7282177209854126, acc: 34.375, f1: 8.188153310104529, r: 0.21240493959699902
06/02/2019 09:53:46 step: 553, epoch: 16, batch: 24, loss: 1.7363371849060059, acc: 35.9375, f1: 6.609195402298851, r: 0.11917545328048265
06/02/2019 09:53:47 step: 558, epoch: 16, batch: 29, loss: 1.865330696105957, acc: 35.9375, f1: 8.105912930474336, r: 0.21728462626399514
06/02/2019 09:53:48 *** evaluating ***
06/02/2019 09:53:48 step: 17, epoch: 16, acc: 54.27350427350427, f1: 15.247069156451676, r: 0.27930408121831046
06/02/2019 09:53:48 *** epoch: 18 ***
06/02/2019 09:53:48 *** training ***
06/02/2019 09:53:49 step: 566, epoch: 17, batch: 4, loss: 1.6911654472351074, acc: 34.375, f1: 7.573149741824441, r: 0.256133898525864
06/02/2019 09:53:50 step: 571, epoch: 17, batch: 9, loss: 1.6095621585845947, acc: 42.1875, f1: 10.208618331053351, r: 0.2101413523872791
06/02/2019 09:53:51 step: 576, epoch: 17, batch: 14, loss: 1.7736072540283203, acc: 37.5, f1: 7.973421926910299, r: 0.16957811092997638
06/02/2019 09:53:52 step: 581, epoch: 17, batch: 19, loss: 1.603740930557251, acc: 40.625, f1: 10.173913043478262, r: 0.21053755597289228
06/02/2019 09:53:54 step: 586, epoch: 17, batch: 24, loss: 1.6784393787384033, acc: 39.0625, f1: 14.412698412698413, r: 0.1988477153432058
06/02/2019 09:53:55 step: 591, epoch: 17, batch: 29, loss: 1.5019114017486572, acc: 40.625, f1: 10.545350718568573, r: 0.209039538850051
06/02/2019 09:53:55 *** evaluating ***
06/02/2019 09:53:56 step: 18, epoch: 17, acc: 51.70940170940172, f1: 14.187808237437244, r: 0.2866131274110666
06/02/2019 09:53:56 *** epoch: 19 ***
06/02/2019 09:53:56 *** training ***
06/02/2019 09:53:57 step: 599, epoch: 18, batch: 4, loss: 1.6105650663375854, acc: 43.75, f1: 16.849603367330417, r: 0.17008868691296494
06/02/2019 09:53:58 step: 604, epoch: 18, batch: 9, loss: 1.7508231401443481, acc: 34.375, f1: 9.767718880285884, r: 0.17393703993105922
06/02/2019 09:53:59 step: 609, epoch: 18, batch: 14, loss: 1.7750352621078491, acc: 46.875, f1: 12.795608108108109, r: 0.17390236226230357
06/02/2019 09:54:00 step: 614, epoch: 18, batch: 19, loss: 1.6046487092971802, acc: 45.3125, f1: 18.11904761904762, r: 0.21181173937283404
06/02/2019 09:54:01 step: 619, epoch: 18, batch: 24, loss: 1.5432730913162231, acc: 40.625, f1: 11.512297226582943, r: 0.2510760653748094
06/02/2019 09:54:02 step: 624, epoch: 18, batch: 29, loss: 1.6738463640213013, acc: 42.1875, f1: 12.546345811051692, r: 0.19817544518609273
06/02/2019 09:54:03 *** evaluating ***
06/02/2019 09:54:03 step: 19, epoch: 18, acc: 55.98290598290598, f1: 16.12074030552291, r: 0.2913196190267181
06/02/2019 09:54:03 *** epoch: 20 ***
06/02/2019 09:54:03 *** training ***
06/02/2019 09:54:04 step: 632, epoch: 19, batch: 4, loss: 1.5418221950531006, acc: 50.0, f1: 18.605967078189302, r: 0.18996980853092743
06/02/2019 09:54:06 step: 637, epoch: 19, batch: 9, loss: 1.7289990186691284, acc: 39.0625, f1: 15.8553076402975, r: 0.19465830685978744
06/02/2019 09:54:07 step: 642, epoch: 19, batch: 14, loss: 1.5597046613693237, acc: 43.75, f1: 16.337444068536506, r: 0.20762453999312236
06/02/2019 09:54:08 step: 647, epoch: 19, batch: 19, loss: 1.5864511728286743, acc: 48.4375, f1: 18.984824699110415, r: 0.23646438362959152
06/02/2019 09:54:09 step: 652, epoch: 19, batch: 24, loss: 1.6696776151657104, acc: 39.0625, f1: 13.589872568745806, r: 0.18601447206358432
06/02/2019 09:54:10 step: 657, epoch: 19, batch: 29, loss: 1.5659599304199219, acc: 40.625, f1: 15.114910829196543, r: 0.21545025070884127
06/02/2019 09:54:10 *** evaluating ***
06/02/2019 09:54:11 step: 20, epoch: 19, acc: 53.41880341880342, f1: 15.17831043554516, r: 0.2900273894657958
06/02/2019 09:54:11 *** epoch: 21 ***
06/02/2019 09:54:11 *** training ***
06/02/2019 09:54:12 step: 665, epoch: 20, batch: 4, loss: 1.4495460987091064, acc: 53.125, f1: 14.305555555555555, r: 0.240911976946176
06/02/2019 09:54:13 step: 670, epoch: 20, batch: 9, loss: 1.7820782661437988, acc: 34.375, f1: 12.163299663299664, r: 0.25072026793984675
06/02/2019 09:54:14 step: 675, epoch: 20, batch: 14, loss: 1.585900902748108, acc: 42.1875, f1: 11.962962962962964, r: 0.14250734186331143
06/02/2019 09:54:15 step: 680, epoch: 20, batch: 19, loss: 1.5287327766418457, acc: 39.0625, f1: 8.969907407407408, r: 0.1895033932434218
06/02/2019 09:54:16 step: 685, epoch: 20, batch: 24, loss: 1.5076698064804077, acc: 45.3125, f1: 15.686406008986653, r: 0.1996039830220604
06/02/2019 09:54:17 step: 690, epoch: 20, batch: 29, loss: 1.3655338287353516, acc: 57.8125, f1: 17.311752287121745, r: 0.18131742792929667
06/02/2019 09:54:18 *** evaluating ***
06/02/2019 09:54:18 step: 21, epoch: 20, acc: 51.28205128205128, f1: 13.835695561380906, r: 0.3083523704808415
06/02/2019 09:54:18 *** epoch: 22 ***
06/02/2019 09:54:18 *** training ***
06/02/2019 09:54:19 step: 698, epoch: 21, batch: 4, loss: 1.6874476671218872, acc: 46.875, f1: 14.5010395010395, r: 0.2027264033246205
06/02/2019 09:54:20 step: 703, epoch: 21, batch: 9, loss: 1.5969502925872803, acc: 48.4375, f1: 16.74194519722975, r: 0.14629199434371937
06/02/2019 09:54:21 step: 708, epoch: 21, batch: 14, loss: 1.7570922374725342, acc: 39.0625, f1: 14.924517008612376, r: 0.22628807778383697
06/02/2019 09:54:23 step: 713, epoch: 21, batch: 19, loss: 1.5168942213058472, acc: 46.875, f1: 20.260879471405786, r: 0.15497392648383707
06/02/2019 09:54:24 step: 718, epoch: 21, batch: 24, loss: 1.5905089378356934, acc: 45.3125, f1: 14.30995475113122, r: 0.2117248218621733
06/02/2019 09:54:25 step: 723, epoch: 21, batch: 29, loss: 1.5714961290359497, acc: 48.4375, f1: 14.333333333333334, r: 0.23356437926816737
06/02/2019 09:54:26 *** evaluating ***
06/02/2019 09:54:26 step: 22, epoch: 21, acc: 54.27350427350427, f1: 15.345563968161521, r: 0.3020060629498978
06/02/2019 09:54:26 *** epoch: 23 ***
06/02/2019 09:54:26 *** training ***
06/02/2019 09:54:27 step: 731, epoch: 22, batch: 4, loss: 1.547072172164917, acc: 48.4375, f1: 15.33228676085819, r: 0.12489663444701209
06/02/2019 09:54:28 step: 736, epoch: 22, batch: 9, loss: 1.5177708864212036, acc: 45.3125, f1: 12.38095238095238, r: 0.20195473820044785
06/02/2019 09:54:29 step: 741, epoch: 22, batch: 14, loss: 1.514008641242981, acc: 48.4375, f1: 14.0148830616584, r: 0.1659460491825872
06/02/2019 09:54:31 step: 746, epoch: 22, batch: 19, loss: 1.5612949132919312, acc: 39.0625, f1: 12.68939393939394, r: 0.21456433436958555
06/02/2019 09:54:32 step: 751, epoch: 22, batch: 24, loss: 1.5484076738357544, acc: 42.1875, f1: 13.80392156862745, r: 0.22946683699159054
06/02/2019 09:54:33 step: 756, epoch: 22, batch: 29, loss: 1.519748330116272, acc: 45.3125, f1: 13.541666666666668, r: 0.24528921401500114
06/02/2019 09:54:33 *** evaluating ***
06/02/2019 09:54:34 step: 23, epoch: 22, acc: 53.84615384615385, f1: 15.204049099314524, r: 0.3018837574777333
06/02/2019 09:54:34 *** epoch: 24 ***
06/02/2019 09:54:34 *** training ***
06/02/2019 09:54:35 step: 764, epoch: 23, batch: 4, loss: 1.5782084465026855, acc: 45.3125, f1: 14.642857142857144, r: 0.27055307791972033
06/02/2019 09:54:36 step: 769, epoch: 23, batch: 9, loss: 1.6061022281646729, acc: 37.5, f1: 10.76320939334638, r: 0.1907890741020048
06/02/2019 09:54:37 step: 774, epoch: 23, batch: 14, loss: 1.5867925882339478, acc: 34.375, f1: 10.416666666666666, r: 0.1829942843332626
06/02/2019 09:54:39 step: 779, epoch: 23, batch: 19, loss: 1.608660101890564, acc: 45.3125, f1: 12.673297166968053, r: 0.22163937049678697
06/02/2019 09:54:40 step: 784, epoch: 23, batch: 24, loss: 1.6869620084762573, acc: 34.375, f1: 11.062615410441497, r: 0.1275597132124426
06/02/2019 09:54:41 step: 789, epoch: 23, batch: 29, loss: 1.6707684993743896, acc: 32.8125, f1: 9.57142857142857, r: 0.22273836222571206
06/02/2019 09:54:41 *** evaluating ***
06/02/2019 09:54:42 step: 24, epoch: 23, acc: 54.27350427350427, f1: 15.397869674185467, r: 0.3072010240187063
06/02/2019 09:54:42 *** epoch: 25 ***
06/02/2019 09:54:42 *** training ***
06/02/2019 09:54:43 step: 797, epoch: 24, batch: 4, loss: 1.6350353956222534, acc: 35.9375, f1: 12.828947368421053, r: 0.23672963109799783
06/02/2019 09:54:44 step: 802, epoch: 24, batch: 9, loss: 1.7108752727508545, acc: 35.9375, f1: 13.322640884310713, r: 0.20437099678589737
06/02/2019 09:54:45 step: 807, epoch: 24, batch: 14, loss: 1.7711753845214844, acc: 26.5625, f1: 13.306982872200265, r: 0.2677881400809745
06/02/2019 09:54:46 step: 812, epoch: 24, batch: 19, loss: 1.5304988622665405, acc: 50.0, f1: 14.119318181818183, r: 0.2908749045931341
06/02/2019 09:54:47 step: 817, epoch: 24, batch: 24, loss: 1.692589282989502, acc: 28.125, f1: 9.203980099502486, r: 0.104715518351001
06/02/2019 09:54:48 step: 822, epoch: 24, batch: 29, loss: 1.6339867115020752, acc: 35.9375, f1: 9.729064039408867, r: 0.24396563790825543
06/02/2019 09:54:49 *** evaluating ***
06/02/2019 09:54:49 step: 25, epoch: 24, acc: 51.28205128205128, f1: 13.783403656821381, r: 0.30881448407275597
06/02/2019 09:54:49 *** epoch: 26 ***
06/02/2019 09:54:49 *** training ***
06/02/2019 09:54:51 step: 830, epoch: 25, batch: 4, loss: 1.5468723773956299, acc: 43.75, f1: 14.229376257545272, r: 0.22883041546967187
06/02/2019 09:54:52 step: 835, epoch: 25, batch: 9, loss: 1.5966572761535645, acc: 35.9375, f1: 9.590126811594203, r: 0.24960130684282558
06/02/2019 09:54:53 step: 840, epoch: 25, batch: 14, loss: 1.4783236980438232, acc: 43.75, f1: 13.009868421052632, r: 0.2218190962800823
06/02/2019 09:54:54 step: 845, epoch: 25, batch: 19, loss: 1.477612018585205, acc: 43.75, f1: 17.171312969632297, r: 0.23544989521707946
06/02/2019 09:54:55 step: 850, epoch: 25, batch: 24, loss: 1.51935613155365, acc: 42.1875, f1: 14.394525752884887, r: 0.2548650092344344
06/02/2019 09:54:57 step: 855, epoch: 25, batch: 29, loss: 1.7450295686721802, acc: 42.1875, f1: 14.83305966064587, r: 0.2916792605450394
06/02/2019 09:54:57 *** evaluating ***
06/02/2019 09:54:58 step: 26, epoch: 25, acc: 50.85470085470085, f1: 13.533243486073676, r: 0.31179131533170845
06/02/2019 09:54:58 *** epoch: 27 ***
06/02/2019 09:54:58 *** training ***
06/02/2019 09:54:59 step: 863, epoch: 26, batch: 4, loss: 1.4966974258422852, acc: 45.3125, f1: 18.51966873706004, r: 0.26855066111796266
06/02/2019 09:55:00 step: 868, epoch: 26, batch: 9, loss: 1.6399585008621216, acc: 37.5, f1: 12.092391304347828, r: 0.26717090931840276
06/02/2019 09:55:01 step: 873, epoch: 26, batch: 14, loss: 1.5830851793289185, acc: 40.625, f1: 12.5, r: 0.22022331440687157
06/02/2019 09:55:02 step: 878, epoch: 26, batch: 19, loss: 1.3867205381393433, acc: 51.5625, f1: 10.757902298850574, r: 0.2229242452288865
06/02/2019 09:55:03 step: 883, epoch: 26, batch: 24, loss: 1.5898592472076416, acc: 34.375, f1: 11.881720430107526, r: 0.2619134518973693
06/02/2019 09:55:04 step: 888, epoch: 26, batch: 29, loss: 1.6421399116516113, acc: 43.75, f1: 14.686032863849768, r: 0.17216564702426518
06/02/2019 09:55:05 *** evaluating ***
06/02/2019 09:55:05 step: 27, epoch: 26, acc: 52.991452991452995, f1: 14.912280701754387, r: 0.31733677310708475
06/02/2019 09:55:05 *** epoch: 28 ***
06/02/2019 09:55:05 *** training ***
06/02/2019 09:55:06 step: 896, epoch: 27, batch: 4, loss: 1.763781189918518, acc: 39.0625, f1: 10.236842105263158, r: 0.14035298889018386
06/02/2019 09:55:07 step: 901, epoch: 27, batch: 9, loss: 1.5283243656158447, acc: 34.375, f1: 12.337662337662339, r: 0.23757067972618798
06/02/2019 09:55:08 step: 906, epoch: 27, batch: 14, loss: 1.7585138082504272, acc: 35.9375, f1: 11.596491228070176, r: 0.19477321117665172
06/02/2019 09:55:10 step: 911, epoch: 27, batch: 19, loss: 1.5436546802520752, acc: 46.875, f1: 16.840417000801924, r: 0.28160142089374834
06/02/2019 09:55:11 step: 916, epoch: 27, batch: 24, loss: 1.6056028604507446, acc: 40.625, f1: 12.925170068027208, r: 0.0680770011143457
06/02/2019 09:55:12 step: 921, epoch: 27, batch: 29, loss: 1.637794852256775, acc: 31.25, f1: 11.35476545312611, r: 0.28034490611357926
06/02/2019 09:55:13 *** evaluating ***
06/02/2019 09:55:13 step: 28, epoch: 27, acc: 55.12820512820513, f1: 15.669276659209547, r: 0.32228384982318337
06/02/2019 09:55:13 *** epoch: 29 ***
06/02/2019 09:55:13 *** training ***
06/02/2019 09:55:14 step: 929, epoch: 28, batch: 4, loss: 1.6365244388580322, acc: 46.875, f1: 16.287094547964113, r: 0.2340434154806061
06/02/2019 09:55:15 step: 934, epoch: 28, batch: 9, loss: 1.6565226316452026, acc: 39.0625, f1: 14.385964912280702, r: 0.1953700872787032
06/02/2019 09:55:16 step: 939, epoch: 28, batch: 14, loss: 1.501991868019104, acc: 35.9375, f1: 13.897180762852404, r: 0.2320257542722836
06/02/2019 09:55:17 step: 944, epoch: 28, batch: 19, loss: 1.4368728399276733, acc: 43.75, f1: 13.712686567164178, r: 0.2828897343178562
06/02/2019 09:55:18 step: 949, epoch: 28, batch: 24, loss: 1.4855971336364746, acc: 51.5625, f1: 18.279942279942283, r: 0.248789251142928
06/02/2019 09:55:19 step: 954, epoch: 28, batch: 29, loss: 1.7137097120285034, acc: 40.625, f1: 12.31390163720991, r: 0.18450942188869127
06/02/2019 09:55:20 *** evaluating ***
06/02/2019 09:55:20 step: 29, epoch: 28, acc: 56.41025641025641, f1: 16.310759995860213, r: 0.3134985435355794
06/02/2019 09:55:20 *** epoch: 30 ***
06/02/2019 09:55:20 *** training ***
06/02/2019 09:55:21 step: 962, epoch: 29, batch: 4, loss: 1.666638970375061, acc: 31.25, f1: 9.316239316239315, r: 0.19424659516447995
06/02/2019 09:55:23 step: 967, epoch: 29, batch: 9, loss: 1.7567962408065796, acc: 37.5, f1: 14.782276546982429, r: 0.17771612718803745
06/02/2019 09:55:24 step: 972, epoch: 29, batch: 14, loss: 1.5853471755981445, acc: 46.875, f1: 13.636363636363635, r: 0.2550837593068129
06/02/2019 09:55:25 step: 977, epoch: 29, batch: 19, loss: 1.5915452241897583, acc: 37.5, f1: 11.415343915343914, r: 0.29426635226374553
06/02/2019 09:55:26 step: 982, epoch: 29, batch: 24, loss: 1.6568100452423096, acc: 40.625, f1: 14.027614571092831, r: 0.16963517446213733
06/02/2019 09:55:27 step: 987, epoch: 29, batch: 29, loss: 1.6659531593322754, acc: 37.5, f1: 12.323588709677418, r: 0.22694031299743403
06/02/2019 09:55:28 *** evaluating ***
06/02/2019 09:55:28 step: 30, epoch: 29, acc: 56.41025641025641, f1: 16.2690250925545, r: 0.31324737665282765
06/02/2019 09:55:28 *** epoch: 31 ***
06/02/2019 09:55:28 *** training ***
06/02/2019 09:55:29 step: 995, epoch: 30, batch: 4, loss: 1.283617377281189, acc: 57.8125, f1: 21.914700544464612, r: 0.24605267017655652
06/02/2019 09:55:30 step: 1000, epoch: 30, batch: 9, loss: 1.5593677759170532, acc: 45.3125, f1: 13.94993894993895, r: 0.28597997871460645
06/02/2019 09:55:31 step: 1005, epoch: 30, batch: 14, loss: 1.391347050666809, acc: 50.0, f1: 15.883514623366038, r: 0.237653632978182
06/02/2019 09:55:33 step: 1010, epoch: 30, batch: 19, loss: 1.3785136938095093, acc: 50.0, f1: 18.16355234076753, r: 0.24111639098450216
06/02/2019 09:55:34 step: 1015, epoch: 30, batch: 24, loss: 1.6198103427886963, acc: 45.3125, f1: 17.81744956046935, r: 0.20272550859254196
06/02/2019 09:55:35 step: 1020, epoch: 30, batch: 29, loss: 1.6856962442398071, acc: 31.25, f1: 12.145691609977325, r: 0.2412341451114191
06/02/2019 09:55:36 *** evaluating ***
06/02/2019 09:55:36 step: 31, epoch: 30, acc: 56.837606837606835, f1: 16.47164026571283, r: 0.3222518371127558
06/02/2019 09:55:36 *** epoch: 32 ***
06/02/2019 09:55:36 *** training ***
06/02/2019 09:55:37 step: 1028, epoch: 31, batch: 4, loss: 1.5519472360610962, acc: 34.375, f1: 13.401149933657678, r: 0.22235393510646986
06/02/2019 09:55:38 step: 1033, epoch: 31, batch: 9, loss: 1.6277284622192383, acc: 37.5, f1: 13.649508386350492, r: 0.2068816566311868
06/02/2019 09:55:39 step: 1038, epoch: 31, batch: 14, loss: 1.4824074506759644, acc: 37.5, f1: 10.277777777777779, r: 0.2516927092711533
06/02/2019 09:55:40 step: 1043, epoch: 31, batch: 19, loss: 1.4230141639709473, acc: 51.5625, f1: 18.662913441238558, r: 0.2912876190042275
06/02/2019 09:55:41 step: 1048, epoch: 31, batch: 24, loss: 1.5593405961990356, acc: 45.3125, f1: 16.714620274339246, r: 0.22622698790647
06/02/2019 09:55:42 step: 1053, epoch: 31, batch: 29, loss: 1.6147671937942505, acc: 34.375, f1: 10.173160173160174, r: 0.22310360662279213
06/02/2019 09:55:43 *** evaluating ***
06/02/2019 09:55:43 step: 32, epoch: 31, acc: 56.837606837606835, f1: 16.51412313177019, r: 0.3248811152212206
06/02/2019 09:55:43 *** epoch: 33 ***
06/02/2019 09:55:43 *** training ***
06/02/2019 09:55:45 step: 1061, epoch: 32, batch: 4, loss: 1.4238730669021606, acc: 40.625, f1: 12.072981366459626, r: 0.2594999306537623
06/02/2019 09:55:46 step: 1066, epoch: 32, batch: 9, loss: 1.6115895509719849, acc: 39.0625, f1: 14.24666563275434, r: 0.2871032795705716
06/02/2019 09:55:47 step: 1071, epoch: 32, batch: 14, loss: 1.6207072734832764, acc: 45.3125, f1: 15.786314525810324, r: 0.2758362417458711
06/02/2019 09:55:48 step: 1076, epoch: 32, batch: 19, loss: 1.4422415494918823, acc: 45.3125, f1: 14.339826839826841, r: 0.24090039656626977
06/02/2019 09:55:49 step: 1081, epoch: 32, batch: 24, loss: 1.4103789329528809, acc: 53.125, f1: 19.47317739881341, r: 0.23223786065775726
06/02/2019 09:55:50 step: 1086, epoch: 32, batch: 29, loss: 1.6137475967407227, acc: 35.9375, f1: 13.929381276320054, r: 0.29759557247068624
06/02/2019 09:55:50 *** evaluating ***
06/02/2019 09:55:51 step: 33, epoch: 32, acc: 55.98290598290598, f1: 16.071623838162928, r: 0.32630736830237383
06/02/2019 09:55:51 *** epoch: 34 ***
06/02/2019 09:55:51 *** training ***
06/02/2019 09:55:52 step: 1094, epoch: 33, batch: 4, loss: 1.5698376893997192, acc: 46.875, f1: 16.10663082437276, r: 0.21027556649054918
06/02/2019 09:55:53 step: 1099, epoch: 33, batch: 9, loss: 1.6549739837646484, acc: 34.375, f1: 9.864253393665159, r: 0.21553655538685357
06/02/2019 09:55:54 step: 1104, epoch: 33, batch: 14, loss: 1.2909612655639648, acc: 54.6875, f1: 22.829982858033347, r: 0.28631463251959494
06/02/2019 09:55:55 step: 1109, epoch: 33, batch: 19, loss: 1.423579454421997, acc: 43.75, f1: 14.594594594594595, r: 0.20349044423082888
06/02/2019 09:55:56 step: 1114, epoch: 33, batch: 24, loss: 1.4367610216140747, acc: 53.125, f1: 18.199279711884753, r: 0.2537880869218395
06/02/2019 09:55:58 step: 1119, epoch: 33, batch: 29, loss: 1.3574504852294922, acc: 56.25, f1: 16.380718954248362, r: 0.3059894736407404
06/02/2019 09:55:58 *** evaluating ***
06/02/2019 09:55:59 step: 34, epoch: 33, acc: 57.26495726495726, f1: 16.873692679002414, r: 0.3238659740133778
06/02/2019 09:55:59 *** epoch: 35 ***
06/02/2019 09:55:59 *** training ***
06/02/2019 09:56:00 step: 1127, epoch: 34, batch: 4, loss: 1.6966757774353027, acc: 42.1875, f1: 13.387423935091277, r: 0.22177524216441707
06/02/2019 09:56:01 step: 1132, epoch: 34, batch: 9, loss: 1.4675846099853516, acc: 54.6875, f1: 19.103073342962606, r: 0.2980396331262024
06/02/2019 09:56:02 step: 1137, epoch: 34, batch: 14, loss: 1.3443522453308105, acc: 56.25, f1: 20.21978021978022, r: 0.3688017370463666
06/02/2019 09:56:03 step: 1142, epoch: 34, batch: 19, loss: 1.5110154151916504, acc: 45.3125, f1: 12.914989939637827, r: 0.2891056632187143
06/02/2019 09:56:04 step: 1147, epoch: 34, batch: 24, loss: 1.6487115621566772, acc: 40.625, f1: 11.41929035482259, r: 0.21151093825853973
06/02/2019 09:56:05 step: 1152, epoch: 34, batch: 29, loss: 1.6060748100280762, acc: 46.875, f1: 14.820954907161804, r: 0.2505462003379851
06/02/2019 09:56:06 *** evaluating ***
06/02/2019 09:56:06 step: 35, epoch: 34, acc: 54.27350427350427, f1: 15.787226775956285, r: 0.28773091046699917
06/02/2019 09:56:06 *** epoch: 36 ***
06/02/2019 09:56:06 *** training ***
06/02/2019 09:56:07 step: 1160, epoch: 35, batch: 4, loss: 1.6935985088348389, acc: 50.0, f1: 17.755883962780512, r: 0.2055889383475628
06/02/2019 09:56:08 step: 1165, epoch: 35, batch: 9, loss: 1.3364877700805664, acc: 51.5625, f1: 20.567042606516296, r: 0.30975710829650166
06/02/2019 09:56:09 step: 1170, epoch: 35, batch: 14, loss: 1.5889872312545776, acc: 40.625, f1: 14.638113309840065, r: 0.2261567198220807
06/02/2019 09:56:11 step: 1175, epoch: 35, batch: 19, loss: 1.4549767971038818, acc: 46.875, f1: 16.377484522645812, r: 0.2657123536733507
06/02/2019 09:56:12 step: 1180, epoch: 35, batch: 24, loss: 1.609548568725586, acc: 43.75, f1: 16.95447409733124, r: 0.2540005110993308
06/02/2019 09:56:13 step: 1185, epoch: 35, batch: 29, loss: 1.5634766817092896, acc: 46.875, f1: 14.873544761140106, r: 0.3205864698981076
06/02/2019 09:56:13 *** evaluating ***
06/02/2019 09:56:14 step: 36, epoch: 35, acc: 58.119658119658126, f1: 17.005649717514125, r: 0.32385573898118575
06/02/2019 09:56:14 *** epoch: 37 ***
06/02/2019 09:56:14 *** training ***
06/02/2019 09:56:15 step: 1193, epoch: 36, batch: 4, loss: 1.4237475395202637, acc: 46.875, f1: 14.020593869731801, r: 0.2567849482460268
06/02/2019 09:56:16 step: 1198, epoch: 36, batch: 9, loss: 1.6548608541488647, acc: 35.9375, f1: 12.111378205128204, r: 0.20151892971910446
06/02/2019 09:56:17 step: 1203, epoch: 36, batch: 14, loss: 1.4664849042892456, acc: 50.0, f1: 14.884135472370765, r: 0.25214426332168094
06/02/2019 09:56:18 step: 1208, epoch: 36, batch: 19, loss: 1.5410375595092773, acc: 50.0, f1: 15.791649441918148, r: 0.26318165619476136
06/02/2019 09:56:20 step: 1213, epoch: 36, batch: 24, loss: 1.5344825983047485, acc: 43.75, f1: 13.406593406593403, r: 0.23330163991070801
06/02/2019 09:56:21 step: 1218, epoch: 36, batch: 29, loss: 1.8112162351608276, acc: 29.6875, f1: 11.083743842364532, r: 0.2115616996130465
06/02/2019 09:56:21 *** evaluating ***
06/02/2019 09:56:21 step: 37, epoch: 36, acc: 55.98290598290598, f1: 16.156816957068436, r: 0.3329604294798393
06/02/2019 09:56:21 *** epoch: 38 ***
06/02/2019 09:56:21 *** training ***
06/02/2019 09:56:22 step: 1226, epoch: 37, batch: 4, loss: 1.381613850593567, acc: 48.4375, f1: 15.219178082191782, r: 0.30083098453667223
06/02/2019 09:56:24 step: 1231, epoch: 37, batch: 9, loss: 1.5643813610076904, acc: 48.4375, f1: 16.84027777777778, r: 0.1836710642492375
06/02/2019 09:56:25 step: 1236, epoch: 37, batch: 14, loss: 1.4974621534347534, acc: 43.75, f1: 15.018315018315015, r: 0.33952411483276623
06/02/2019 09:56:26 step: 1241, epoch: 37, batch: 19, loss: 1.591433048248291, acc: 46.875, f1: 11.766194331983806, r: 0.1788442079938686
06/02/2019 09:56:27 step: 1246, epoch: 37, batch: 24, loss: 1.6779255867004395, acc: 40.625, f1: 12.05128205128205, r: 0.2138371146621153
06/02/2019 09:56:28 step: 1251, epoch: 37, batch: 29, loss: 1.5402880907058716, acc: 46.875, f1: 19.68896777916566, r: 0.24068734303369707
06/02/2019 09:56:29 *** evaluating ***
06/02/2019 09:56:29 step: 38, epoch: 37, acc: 56.41025641025641, f1: 16.323024054982817, r: 0.33387736882617247
06/02/2019 09:56:29 *** epoch: 39 ***
06/02/2019 09:56:29 *** training ***
06/02/2019 09:56:30 step: 1259, epoch: 38, batch: 4, loss: 1.6181837320327759, acc: 40.625, f1: 13.640973630831644, r: 0.29632573335393103
06/02/2019 09:56:31 step: 1264, epoch: 38, batch: 9, loss: 1.422206997871399, acc: 46.875, f1: 18.532412965186072, r: 0.3026961736521108
06/02/2019 09:56:33 step: 1269, epoch: 38, batch: 14, loss: 1.5144497156143188, acc: 45.3125, f1: 17.344877344877347, r: 0.29945398267246637
06/02/2019 09:56:34 step: 1274, epoch: 38, batch: 19, loss: 1.5333112478256226, acc: 37.5, f1: 11.383928571428573, r: 0.21869852286855623
06/02/2019 09:56:35 step: 1279, epoch: 38, batch: 24, loss: 1.3865153789520264, acc: 51.5625, f1: 17.827130852340936, r: 0.2765823200601533
06/02/2019 09:56:36 step: 1284, epoch: 38, batch: 29, loss: 1.2914457321166992, acc: 51.5625, f1: 15.154109589041099, r: 0.32951438721718457
06/02/2019 09:56:37 *** evaluating ***
06/02/2019 09:56:37 step: 39, epoch: 38, acc: 57.26495726495726, f1: 16.69998788319399, r: 0.3194144262757719
06/02/2019 09:56:37 *** epoch: 40 ***
06/02/2019 09:56:37 *** training ***
06/02/2019 09:56:38 step: 1292, epoch: 39, batch: 4, loss: 1.5462418794631958, acc: 46.875, f1: 16.55907445381129, r: 0.23021222026589166
06/02/2019 09:56:39 step: 1297, epoch: 39, batch: 9, loss: 1.4999300241470337, acc: 43.75, f1: 14.279622296679866, r: 0.24946784831193655
06/02/2019 09:56:40 step: 1302, epoch: 39, batch: 14, loss: 1.4452935457229614, acc: 53.125, f1: 17.574786324786324, r: 0.2942196483622681
06/02/2019 09:56:41 step: 1307, epoch: 39, batch: 19, loss: 1.482485294342041, acc: 43.75, f1: 15.238095238095237, r: 0.3409430326148429
06/02/2019 09:56:42 step: 1312, epoch: 39, batch: 24, loss: 1.3400959968566895, acc: 51.5625, f1: 16.643323996265174, r: 0.2856687440642514
06/02/2019 09:56:44 step: 1317, epoch: 39, batch: 29, loss: 1.390060305595398, acc: 50.0, f1: 16.95691389717616, r: 0.25154115935493176
06/02/2019 09:56:44 *** evaluating ***
06/02/2019 09:56:44 step: 40, epoch: 39, acc: 57.26495726495726, f1: 16.698508566498212, r: 0.31926908102298107
06/02/2019 09:56:44 *** epoch: 41 ***
06/02/2019 09:56:44 *** training ***
06/02/2019 09:56:46 step: 1325, epoch: 40, batch: 4, loss: 1.4344220161437988, acc: 46.875, f1: 16.06900452488688, r: 0.32172098233296714
06/02/2019 09:56:47 step: 1330, epoch: 40, batch: 9, loss: 1.431086540222168, acc: 42.1875, f1: 13.19109461966605, r: 0.227954961825573
06/02/2019 09:56:48 step: 1335, epoch: 40, batch: 14, loss: 1.4327565431594849, acc: 46.875, f1: 15.83886780518659, r: 0.25587646318669477
06/02/2019 09:56:49 step: 1340, epoch: 40, batch: 19, loss: 1.4005110263824463, acc: 51.5625, f1: 18.33333333333333, r: 0.33315093401197415
06/02/2019 09:56:50 step: 1345, epoch: 40, batch: 24, loss: 1.4732016324996948, acc: 53.125, f1: 16.89280868385346, r: 0.2541939538402379
06/02/2019 09:56:51 step: 1350, epoch: 40, batch: 29, loss: 1.3762850761413574, acc: 48.4375, f1: 16.77123552123552, r: 0.3656190687469685
06/02/2019 09:56:52 *** evaluating ***
06/02/2019 09:56:52 step: 41, epoch: 40, acc: 57.692307692307686, f1: 16.89291751474799, r: 0.34157554064436463
06/02/2019 09:56:52 *** epoch: 42 ***
06/02/2019 09:56:52 *** training ***
06/02/2019 09:56:53 step: 1358, epoch: 41, batch: 4, loss: 1.5411633253097534, acc: 39.0625, f1: 14.285714285714285, r: 0.23422566252151383
06/02/2019 09:56:54 step: 1363, epoch: 41, batch: 9, loss: 1.450289011001587, acc: 39.0625, f1: 15.291815604585235, r: 0.32252012206112923
06/02/2019 09:56:55 step: 1368, epoch: 41, batch: 14, loss: 1.4496641159057617, acc: 48.4375, f1: 14.423973880597016, r: 0.2938956481516649
06/02/2019 09:56:57 step: 1373, epoch: 41, batch: 19, loss: 1.593513011932373, acc: 48.4375, f1: 16.94157974920895, r: 0.22181728434200643
06/02/2019 09:56:58 step: 1378, epoch: 41, batch: 24, loss: 1.2236571311950684, acc: 54.6875, f1: 19.06097642423028, r: 0.27955226526849697
06/02/2019 09:56:59 step: 1383, epoch: 41, batch: 29, loss: 1.3879215717315674, acc: 56.25, f1: 15.965989607935757, r: 0.330673515050002
06/02/2019 09:57:00 *** evaluating ***
06/02/2019 09:57:00 step: 42, epoch: 41, acc: 56.41025641025641, f1: 16.3958772685419, r: 0.3201103460784476
06/02/2019 09:57:00 *** epoch: 43 ***
06/02/2019 09:57:00 *** training ***
06/02/2019 09:57:01 step: 1391, epoch: 42, batch: 4, loss: 1.4199824333190918, acc: 45.3125, f1: 16.244897959183675, r: 0.2936669038065644
06/02/2019 09:57:02 step: 1396, epoch: 42, batch: 9, loss: 1.5946176052093506, acc: 39.0625, f1: 14.44567491079119, r: 0.20151331997259977
06/02/2019 09:57:03 step: 1401, epoch: 42, batch: 14, loss: 1.4824414253234863, acc: 51.5625, f1: 14.86013986013986, r: 0.2440812967821095
06/02/2019 09:57:05 step: 1406, epoch: 42, batch: 19, loss: 1.5548828840255737, acc: 42.1875, f1: 16.33059922533607, r: 0.25494037507075634
06/02/2019 09:57:06 step: 1411, epoch: 42, batch: 24, loss: 1.4235599040985107, acc: 54.6875, f1: 18.43112244897959, r: 0.236175259607056
06/02/2019 09:57:07 step: 1416, epoch: 42, batch: 29, loss: 1.5244554281234741, acc: 42.1875, f1: 16.12579458012713, r: 0.22802123507800676
06/02/2019 09:57:07 *** evaluating ***
06/02/2019 09:57:08 step: 43, epoch: 42, acc: 57.692307692307686, f1: 16.94097222222222, r: 0.33648688390981896
06/02/2019 09:57:08 *** epoch: 44 ***
06/02/2019 09:57:08 *** training ***
06/02/2019 09:57:09 step: 1424, epoch: 43, batch: 4, loss: 1.5918971300125122, acc: 37.5, f1: 13.460719608260591, r: 0.23774212357210284
06/02/2019 09:57:10 step: 1429, epoch: 43, batch: 9, loss: 1.4656617641448975, acc: 43.75, f1: 16.090370574459293, r: 0.26815833179954957
06/02/2019 09:57:11 step: 1434, epoch: 43, batch: 14, loss: 1.5493988990783691, acc: 40.625, f1: 10.876712328767123, r: 0.20595709970187426
06/02/2019 09:57:12 step: 1439, epoch: 43, batch: 19, loss: 1.4987996816635132, acc: 42.1875, f1: 17.14047022317699, r: 0.3198622041075124
06/02/2019 09:57:14 step: 1444, epoch: 43, batch: 24, loss: 1.1755714416503906, acc: 60.9375, f1: 23.892773892773892, r: 0.3214785956806359
06/02/2019 09:57:15 step: 1449, epoch: 43, batch: 29, loss: 1.4233125448226929, acc: 50.0, f1: 15.176715176715177, r: 0.3435488798485238
06/02/2019 09:57:15 *** evaluating ***
06/02/2019 09:57:16 step: 44, epoch: 43, acc: 55.98290598290598, f1: 16.308019976498237, r: 0.333962508901887
06/02/2019 09:57:16 *** epoch: 45 ***
06/02/2019 09:57:16 *** training ***
06/02/2019 09:57:17 step: 1457, epoch: 44, batch: 4, loss: 1.3518834114074707, acc: 46.875, f1: 16.117216117216117, r: 0.23066048428342956
06/02/2019 09:57:18 step: 1462, epoch: 44, batch: 9, loss: 1.5470969676971436, acc: 40.625, f1: 15.463108320251177, r: 0.25733009235152715
06/02/2019 09:57:19 step: 1467, epoch: 44, batch: 14, loss: 1.3277454376220703, acc: 51.5625, f1: 14.418989135569202, r: 0.3051564460209216
06/02/2019 09:57:20 step: 1472, epoch: 44, batch: 19, loss: 1.4006810188293457, acc: 51.5625, f1: 17.711598746081506, r: 0.2828420699671999
06/02/2019 09:57:21 step: 1477, epoch: 44, batch: 24, loss: 1.2999011278152466, acc: 53.125, f1: 17.727848101265824, r: 0.2635796248431481
06/02/2019 09:57:22 step: 1482, epoch: 44, batch: 29, loss: 1.4089311361312866, acc: 53.125, f1: 23.039029935581663, r: 0.2481091441783092
06/02/2019 09:57:23 *** evaluating ***
06/02/2019 09:57:23 step: 45, epoch: 44, acc: 58.54700854700855, f1: 17.131075156905414, r: 0.33315631657432176
06/02/2019 09:57:23 *** epoch: 46 ***
06/02/2019 09:57:23 *** training ***
06/02/2019 09:57:25 step: 1490, epoch: 45, batch: 4, loss: 1.305700421333313, acc: 46.875, f1: 12.009291521486642, r: 0.32543886629347146
06/02/2019 09:57:25 step: 1495, epoch: 45, batch: 9, loss: 1.5093505382537842, acc: 39.0625, f1: 15.241080758322138, r: 0.24260334631283537
06/02/2019 09:57:26 step: 1500, epoch: 45, batch: 14, loss: 1.4971979856491089, acc: 42.1875, f1: 15.689865689865693, r: 0.23458053795769374
06/02/2019 09:57:27 step: 1505, epoch: 45, batch: 19, loss: 1.3733538389205933, acc: 53.125, f1: 15.90126291618829, r: 0.2743221044698313
06/02/2019 09:57:29 step: 1510, epoch: 45, batch: 24, loss: 1.2325297594070435, acc: 56.25, f1: 22.56902761104442, r: 0.31873427006335975
06/02/2019 09:57:30 step: 1515, epoch: 45, batch: 29, loss: 1.4875191450119019, acc: 53.125, f1: 23.003663003663004, r: 0.27019486116362673
06/02/2019 09:57:31 *** evaluating ***
06/02/2019 09:57:31 step: 46, epoch: 45, acc: 57.692307692307686, f1: 16.820532319391635, r: 0.33241283144881434
06/02/2019 09:57:31 *** epoch: 47 ***
06/02/2019 09:57:31 *** training ***
06/02/2019 09:57:32 step: 1523, epoch: 46, batch: 4, loss: 1.6214994192123413, acc: 46.875, f1: 14.693264693264691, r: 0.21052243344006838
06/02/2019 09:57:33 step: 1528, epoch: 46, batch: 9, loss: 1.548625111579895, acc: 39.0625, f1: 13.193779904306218, r: 0.3640588235107609
06/02/2019 09:57:34 step: 1533, epoch: 46, batch: 14, loss: 1.3658849000930786, acc: 54.6875, f1: 18.11298432416445, r: 0.25112466439303305
06/02/2019 09:57:35 step: 1538, epoch: 46, batch: 19, loss: 1.5609632730484009, acc: 42.1875, f1: 13.942307692307693, r: 0.3437380302601367
06/02/2019 09:57:36 step: 1543, epoch: 46, batch: 24, loss: 1.3425474166870117, acc: 51.5625, f1: 15.062260536398467, r: 0.3036565693045836
06/02/2019 09:57:37 step: 1548, epoch: 46, batch: 29, loss: 1.6604766845703125, acc: 37.5, f1: 12.655141843971633, r: 0.22116617315848655
06/02/2019 09:57:38 *** evaluating ***
06/02/2019 09:57:38 step: 47, epoch: 46, acc: 55.98290598290598, f1: 16.296101774042953, r: 0.31487769777837693
06/02/2019 09:57:38 *** epoch: 48 ***
06/02/2019 09:57:38 *** training ***
06/02/2019 09:57:39 step: 1556, epoch: 47, batch: 4, loss: 1.5143256187438965, acc: 35.9375, f1: 12.31934731934732, r: 0.2563729976596769
06/02/2019 09:57:40 step: 1561, epoch: 47, batch: 9, loss: 1.4685314893722534, acc: 45.3125, f1: 13.472222222222221, r: 0.3029749207949415
06/02/2019 09:57:41 step: 1566, epoch: 47, batch: 14, loss: 1.341900110244751, acc: 56.25, f1: 16.792929292929294, r: 0.2910387017520209
06/02/2019 09:57:43 step: 1571, epoch: 47, batch: 19, loss: 1.538763403892517, acc: 45.3125, f1: 13.608597285067875, r: 0.3150923890215148
06/02/2019 09:57:44 step: 1576, epoch: 47, batch: 24, loss: 1.5745505094528198, acc: 51.5625, f1: 14.555555555555555, r: 0.19986182480489081
06/02/2019 09:57:45 step: 1581, epoch: 47, batch: 29, loss: 1.4148441553115845, acc: 51.5625, f1: 18.033395176252316, r: 0.20448804048725258
06/02/2019 09:57:46 *** evaluating ***
06/02/2019 09:57:46 step: 48, epoch: 47, acc: 58.119658119658126, f1: 17.19097222222222, r: 0.34053660897305543
06/02/2019 09:57:46 *** epoch: 49 ***
06/02/2019 09:57:46 *** training ***
06/02/2019 09:57:47 step: 1589, epoch: 48, batch: 4, loss: 1.3124444484710693, acc: 53.125, f1: 19.051799824407375, r: 0.21333540003561888
06/02/2019 09:57:48 step: 1594, epoch: 48, batch: 9, loss: 1.6379462480545044, acc: 48.4375, f1: 17.72061272061272, r: 0.3108849090383752
06/02/2019 09:57:49 step: 1599, epoch: 48, batch: 14, loss: 1.329628586769104, acc: 51.5625, f1: 20.165104863900044, r: 0.18424798471110132
06/02/2019 09:57:50 step: 1604, epoch: 48, batch: 19, loss: 1.438655138015747, acc: 46.875, f1: 14.81818181818182, r: 0.2946277285867232
06/02/2019 09:57:52 step: 1609, epoch: 48, batch: 24, loss: 1.392011046409607, acc: 42.1875, f1: 13.832199546485263, r: 0.24059649325851495
06/02/2019 09:57:53 step: 1614, epoch: 48, batch: 29, loss: 1.243821144104004, acc: 57.8125, f1: 21.604696673189824, r: 0.31281730489894044
06/02/2019 09:57:53 *** evaluating ***
06/02/2019 09:57:54 step: 49, epoch: 48, acc: 58.119658119658126, f1: 16.949797160243406, r: 0.3377742941237767
06/02/2019 09:57:54 *** epoch: 50 ***
06/02/2019 09:57:54 *** training ***
06/02/2019 09:57:55 step: 1622, epoch: 49, batch: 4, loss: 1.4730217456817627, acc: 50.0, f1: 14.39732142857143, r: 0.244714916741467
06/02/2019 09:57:56 step: 1627, epoch: 49, batch: 9, loss: 1.3226553201675415, acc: 51.5625, f1: 18.985797827903088, r: 0.24896467712220655
06/02/2019 09:57:57 step: 1632, epoch: 49, batch: 14, loss: 1.5337090492248535, acc: 48.4375, f1: 17.716346153846153, r: 0.28505145712739394
06/02/2019 09:57:58 step: 1637, epoch: 49, batch: 19, loss: 1.5371696949005127, acc: 46.875, f1: 17.88617886178862, r: 0.31232604220578647
06/02/2019 09:57:59 step: 1642, epoch: 49, batch: 24, loss: 1.5311650037765503, acc: 48.4375, f1: 15.64102564102564, r: 0.2644214598266136
06/02/2019 09:58:00 step: 1647, epoch: 49, batch: 29, loss: 1.4468724727630615, acc: 48.4375, f1: 16.324404761904766, r: 0.2911585838028101
06/02/2019 09:58:01 *** evaluating ***
06/02/2019 09:58:01 step: 50, epoch: 49, acc: 57.26495726495726, f1: 16.728023704767892, r: 0.3221048581576263
06/02/2019 09:58:01 *** epoch: 51 ***
06/02/2019 09:58:01 *** training ***
06/02/2019 09:58:02 step: 1655, epoch: 50, batch: 4, loss: 1.2440838813781738, acc: 51.5625, f1: 18.090849242922975, r: 0.3120395882725763
06/02/2019 09:58:04 step: 1660, epoch: 50, batch: 9, loss: 1.3338338136672974, acc: 48.4375, f1: 15.9079476861167, r: 0.2848877729397212
06/02/2019 09:58:05 step: 1665, epoch: 50, batch: 14, loss: 1.2821028232574463, acc: 51.5625, f1: 20.304414003044137, r: 0.23357450266102894
06/02/2019 09:58:06 step: 1670, epoch: 50, batch: 19, loss: 1.3642574548721313, acc: 45.3125, f1: 13.108108108108107, r: 0.17540767808680224
06/02/2019 09:58:07 step: 1675, epoch: 50, batch: 24, loss: 1.4385215044021606, acc: 50.0, f1: 19.582255979314805, r: 0.29892182590030314
06/02/2019 09:58:08 step: 1680, epoch: 50, batch: 29, loss: 1.5571998357772827, acc: 46.875, f1: 12.960829493087559, r: 0.23053585999072876
06/02/2019 09:58:09 *** evaluating ***
06/02/2019 09:58:09 step: 51, epoch: 50, acc: 57.692307692307686, f1: 16.850124378109456, r: 0.3385014629939591
06/02/2019 09:58:09 *** epoch: 52 ***
06/02/2019 09:58:09 *** training ***
06/02/2019 09:58:10 step: 1688, epoch: 51, batch: 4, loss: 1.4133799076080322, acc: 48.4375, f1: 15.18876207199298, r: 0.24822483835617093
06/02/2019 09:58:11 step: 1693, epoch: 51, batch: 9, loss: 1.4630110263824463, acc: 40.625, f1: 17.977150537634408, r: 0.2603017995527052
06/02/2019 09:58:13 step: 1698, epoch: 51, batch: 14, loss: 1.4760178327560425, acc: 43.75, f1: 16.00985221674877, r: 0.3029447131701335
06/02/2019 09:58:14 step: 1703, epoch: 51, batch: 19, loss: 1.6341952085494995, acc: 42.1875, f1: 16.468897747288402, r: 0.25216447293504063
06/02/2019 09:58:15 step: 1708, epoch: 51, batch: 24, loss: 1.2724615335464478, acc: 56.25, f1: 21.68743418743419, r: 0.30617781445200276
06/02/2019 09:58:16 step: 1713, epoch: 51, batch: 29, loss: 1.303598165512085, acc: 53.125, f1: 16.885964912280706, r: 0.30199948786912595
06/02/2019 09:58:17 *** evaluating ***
06/02/2019 09:58:17 step: 52, epoch: 51, acc: 58.119658119658126, f1: 16.92518101367659, r: 0.3402496514643374
06/02/2019 09:58:17 *** epoch: 53 ***
06/02/2019 09:58:17 *** training ***
06/02/2019 09:58:18 step: 1721, epoch: 52, batch: 4, loss: 1.5124948024749756, acc: 46.875, f1: 17.735352205398286, r: 0.24222486390206702
06/02/2019 09:58:19 step: 1726, epoch: 52, batch: 9, loss: 1.5790441036224365, acc: 51.5625, f1: 18.065934065934066, r: 0.23915927997015818
06/02/2019 09:58:20 step: 1731, epoch: 52, batch: 14, loss: 1.3014756441116333, acc: 53.125, f1: 18.485958485958484, r: 0.30451805099785745
06/02/2019 09:58:22 step: 1736, epoch: 52, batch: 19, loss: 1.4871790409088135, acc: 45.3125, f1: 15.192479242434642, r: 0.3006419998333163
06/02/2019 09:58:23 step: 1741, epoch: 52, batch: 24, loss: 1.3361222743988037, acc: 51.5625, f1: 21.762295081967213, r: 0.2718223324290707
06/02/2019 09:58:24 step: 1746, epoch: 52, batch: 29, loss: 1.4608068466186523, acc: 45.3125, f1: 17.117794486215537, r: 0.29623034422852934
06/02/2019 09:58:24 *** evaluating ***
06/02/2019 09:58:25 step: 53, epoch: 52, acc: 57.692307692307686, f1: 16.874520631998774, r: 0.3345866186712829
06/02/2019 09:58:25 *** epoch: 54 ***
06/02/2019 09:58:25 *** training ***
06/02/2019 09:58:26 step: 1754, epoch: 53, batch: 4, loss: 1.1937446594238281, acc: 57.8125, f1: 19.314019314019312, r: 0.3136724773039098
06/02/2019 09:58:27 step: 1759, epoch: 53, batch: 9, loss: 1.411778450012207, acc: 51.5625, f1: 15.674328174328172, r: 0.26783673269475744
06/02/2019 09:58:28 step: 1764, epoch: 53, batch: 14, loss: 1.3435487747192383, acc: 46.875, f1: 15.837742504409169, r: 0.2713635240669941
06/02/2019 09:58:29 step: 1769, epoch: 53, batch: 19, loss: 1.3758602142333984, acc: 39.0625, f1: 15.927897239372646, r: 0.33320223071714633
06/02/2019 09:58:30 step: 1774, epoch: 53, batch: 24, loss: 1.342134714126587, acc: 48.4375, f1: 16.15032776563831, r: 0.3348457906724182
06/02/2019 09:58:31 step: 1779, epoch: 53, batch: 29, loss: 1.4994453191757202, acc: 48.4375, f1: 17.103808353808354, r: 0.36132069423690055
06/02/2019 09:58:32 *** evaluating ***
06/02/2019 09:58:32 step: 54, epoch: 53, acc: 57.26495726495726, f1: 16.64179104477612, r: 0.3330855667750984
06/02/2019 09:58:32 *** epoch: 55 ***
06/02/2019 09:58:32 *** training ***
06/02/2019 09:58:34 step: 1787, epoch: 54, batch: 4, loss: 1.2837735414505005, acc: 43.75, f1: 14.969818913480884, r: 0.3888855609073758
06/02/2019 09:58:35 step: 1792, epoch: 54, batch: 9, loss: 1.3436763286590576, acc: 50.0, f1: 14.884135472370769, r: 0.3147137018369813
06/02/2019 09:58:36 step: 1797, epoch: 54, batch: 14, loss: 1.3173739910125732, acc: 51.5625, f1: 17.87624140565317, r: 0.3224344771344325
06/02/2019 09:58:37 step: 1802, epoch: 54, batch: 19, loss: 1.2544595003128052, acc: 53.125, f1: 19.466248037676607, r: 0.2785391758054967
06/02/2019 09:58:38 step: 1807, epoch: 54, batch: 24, loss: 1.5707733631134033, acc: 40.625, f1: 14.72972972972973, r: 0.3458838396089866
06/02/2019 09:58:39 step: 1812, epoch: 54, batch: 29, loss: 1.4692003726959229, acc: 46.875, f1: 15.199637023593466, r: 0.29937364650896736
06/02/2019 09:58:40 *** evaluating ***
06/02/2019 09:58:40 step: 55, epoch: 54, acc: 57.26495726495726, f1: 16.72009184160305, r: 0.32582239505110555
06/02/2019 09:58:40 *** epoch: 56 ***
06/02/2019 09:58:40 *** training ***
06/02/2019 09:58:42 step: 1820, epoch: 55, batch: 4, loss: 1.6718670129776, acc: 32.8125, f1: 15.512265512265513, r: 0.17909558360363667
06/02/2019 09:58:43 step: 1825, epoch: 55, batch: 9, loss: 1.5144981145858765, acc: 50.0, f1: 19.428066037735846, r: 0.32258591852417384
06/02/2019 09:58:44 step: 1830, epoch: 55, batch: 14, loss: 1.3839876651763916, acc: 53.125, f1: 15.696465696465694, r: 0.29215580162333477
06/02/2019 09:58:45 step: 1835, epoch: 55, batch: 19, loss: 1.468937635421753, acc: 48.4375, f1: 15.033702455464612, r: 0.24949980025242574
06/02/2019 09:58:46 step: 1840, epoch: 55, batch: 24, loss: 1.524558186531067, acc: 46.875, f1: 15.534246575342465, r: 0.2797077475781577
06/02/2019 09:58:47 step: 1845, epoch: 55, batch: 29, loss: 1.3009876012802124, acc: 54.6875, f1: 19.5042184789958, r: 0.29301752304123974
06/02/2019 09:58:48 *** evaluating ***
06/02/2019 09:58:48 step: 56, epoch: 55, acc: 57.26495726495726, f1: 16.614046491472436, r: 0.32433691633696005
06/02/2019 09:58:48 *** epoch: 57 ***
06/02/2019 09:58:48 *** training ***
06/02/2019 09:58:49 step: 1853, epoch: 56, batch: 4, loss: 1.4350665807724, acc: 42.1875, f1: 15.719696969696967, r: 0.30261308082206173
06/02/2019 09:58:50 step: 1858, epoch: 56, batch: 9, loss: 1.1280471086502075, acc: 60.9375, f1: 22.903565722832184, r: 0.2519683487151061
06/02/2019 09:58:51 step: 1863, epoch: 56, batch: 14, loss: 1.4133307933807373, acc: 48.4375, f1: 17.080745341614907, r: 0.3228879377078965
06/02/2019 09:58:53 step: 1868, epoch: 56, batch: 19, loss: 1.250573754310608, acc: 50.0, f1: 15.362318840579709, r: 0.3946825786870901
06/02/2019 09:58:54 step: 1873, epoch: 56, batch: 24, loss: 1.5657721757888794, acc: 50.0, f1: 18.40349666436623, r: 0.2703646319280358
06/02/2019 09:58:55 step: 1878, epoch: 56, batch: 29, loss: 1.513174295425415, acc: 37.5, f1: 12.061403508771928, r: 0.3043286597216424
06/02/2019 09:58:56 *** evaluating ***
06/02/2019 09:58:56 step: 57, epoch: 56, acc: 56.837606837606835, f1: 16.683132746542125, r: 0.34286347923243543
06/02/2019 09:58:56 *** epoch: 58 ***
06/02/2019 09:58:56 *** training ***
06/02/2019 09:58:57 step: 1886, epoch: 57, batch: 4, loss: 1.3920565843582153, acc: 51.5625, f1: 18.59593837535014, r: 0.2517855417199402
06/02/2019 09:58:58 step: 1891, epoch: 57, batch: 9, loss: 1.2459453344345093, acc: 56.25, f1: 17.39656690140845, r: 0.2738696789940755
06/02/2019 09:58:59 step: 1896, epoch: 57, batch: 14, loss: 1.6800546646118164, acc: 42.1875, f1: 13.291925465838508, r: 0.2612701280237941
06/02/2019 09:59:01 step: 1901, epoch: 57, batch: 19, loss: 1.2379627227783203, acc: 56.25, f1: 21.424611424611424, r: 0.3533299057316426
06/02/2019 09:59:02 step: 1906, epoch: 57, batch: 24, loss: 1.327261209487915, acc: 51.5625, f1: 15.338846308995564, r: 0.30579525956435416
06/02/2019 09:59:03 step: 1911, epoch: 57, batch: 29, loss: 1.3081804513931274, acc: 48.4375, f1: 17.13109330196309, r: 0.33787832497073833
06/02/2019 09:59:04 *** evaluating ***
06/02/2019 09:59:04 step: 58, epoch: 57, acc: 57.26495726495726, f1: 16.61147380704576, r: 0.337022349116877
06/02/2019 09:59:04 *** epoch: 59 ***
06/02/2019 09:59:04 *** training ***
06/02/2019 09:59:05 step: 1919, epoch: 58, batch: 4, loss: 1.1506075859069824, acc: 62.5, f1: 20.465838509316768, r: 0.2780675654036245
06/02/2019 09:59:06 step: 1924, epoch: 58, batch: 9, loss: 1.3812812566757202, acc: 45.3125, f1: 14.625850340136054, r: 0.3215063457108343
06/02/2019 09:59:07 step: 1929, epoch: 58, batch: 14, loss: 1.5612422227859497, acc: 45.3125, f1: 17.057416267942582, r: 0.2558408923629943
06/02/2019 09:59:09 step: 1934, epoch: 58, batch: 19, loss: 1.3674275875091553, acc: 42.1875, f1: 12.643678160919542, r: 0.32580883071728334
06/02/2019 09:59:10 step: 1939, epoch: 58, batch: 24, loss: 1.1416170597076416, acc: 57.8125, f1: 24.564007421150276, r: 0.39809989565406023
06/02/2019 09:59:11 step: 1944, epoch: 58, batch: 29, loss: 1.394854187965393, acc: 51.5625, f1: 19.142117589054603, r: 0.29361471212185436
06/02/2019 09:59:12 *** evaluating ***
06/02/2019 09:59:12 step: 59, epoch: 58, acc: 57.692307692307686, f1: 16.792493528904227, r: 0.34227826478367057
06/02/2019 09:59:12 *** epoch: 60 ***
06/02/2019 09:59:12 *** training ***
06/02/2019 09:59:13 step: 1952, epoch: 59, batch: 4, loss: 1.454782247543335, acc: 50.0, f1: 18.253968253968257, r: 0.2889665447593981
06/02/2019 09:59:14 step: 1957, epoch: 59, batch: 9, loss: 1.2203904390335083, acc: 53.125, f1: 18.928674144377336, r: 0.3514905248499241
06/02/2019 09:59:15 step: 1962, epoch: 59, batch: 14, loss: 1.3463878631591797, acc: 48.4375, f1: 18.05860805860806, r: 0.3287999028766206
06/02/2019 09:59:16 step: 1967, epoch: 59, batch: 19, loss: 1.338397741317749, acc: 46.875, f1: 15.576923076923077, r: 0.24534818056754268
06/02/2019 09:59:18 step: 1972, epoch: 59, batch: 24, loss: 1.2221574783325195, acc: 51.5625, f1: 17.142857142857146, r: 0.38830658782292715
06/02/2019 09:59:19 step: 1977, epoch: 59, batch: 29, loss: 1.1637969017028809, acc: 53.125, f1: 16.98717948717949, r: 0.36893843262132026
06/02/2019 09:59:19 *** evaluating ***
06/02/2019 09:59:19 step: 60, epoch: 59, acc: 56.837606837606835, f1: 16.489328311362208, r: 0.34779839188726064
06/02/2019 09:59:19 *** epoch: 61 ***
06/02/2019 09:59:19 *** training ***
06/02/2019 09:59:21 step: 1985, epoch: 60, batch: 4, loss: 1.3465497493743896, acc: 50.0, f1: 16.271929824561404, r: 0.36938827542560404
06/02/2019 09:59:22 step: 1990, epoch: 60, batch: 9, loss: 1.3107905387878418, acc: 45.3125, f1: 14.238911290322578, r: 0.3441873021361124
06/02/2019 09:59:23 step: 1995, epoch: 60, batch: 14, loss: 1.2741470336914062, acc: 59.375, f1: 21.306471306471305, r: 0.3510692361324002
06/02/2019 09:59:24 step: 2000, epoch: 60, batch: 19, loss: 1.2562366724014282, acc: 54.6875, f1: 22.430142006413195, r: 0.286084136649856
06/02/2019 09:59:25 step: 2005, epoch: 60, batch: 24, loss: 1.1902474164962769, acc: 54.6875, f1: 18.88888888888889, r: 0.37258599338244186
06/02/2019 09:59:26 step: 2010, epoch: 60, batch: 29, loss: 1.5439599752426147, acc: 40.625, f1: 14.824396589102474, r: 0.30094541173146033
06/02/2019 09:59:27 *** evaluating ***
06/02/2019 09:59:27 step: 61, epoch: 60, acc: 57.26495726495726, f1: 16.746794871794872, r: 0.3480911393724178
06/02/2019 09:59:27 *** epoch: 62 ***
06/02/2019 09:59:27 *** training ***
06/02/2019 09:59:29 step: 2018, epoch: 61, batch: 4, loss: 1.4139127731323242, acc: 45.3125, f1: 17.61488379135438, r: 0.3027370207043368
06/02/2019 09:59:30 step: 2023, epoch: 61, batch: 9, loss: 1.2508138418197632, acc: 46.875, f1: 18.442028985507246, r: 0.3457433482207422
06/02/2019 09:59:31 step: 2028, epoch: 61, batch: 14, loss: 1.431021809577942, acc: 43.75, f1: 17.69875885486798, r: 0.2974476033479864
06/02/2019 09:59:32 step: 2033, epoch: 61, batch: 19, loss: 1.3049081563949585, acc: 48.4375, f1: 15.727891156462587, r: 0.2574106826857458
06/02/2019 09:59:33 step: 2038, epoch: 61, batch: 24, loss: 1.1315969228744507, acc: 64.0625, f1: 21.883273857488202, r: 0.3181564066368694
06/02/2019 09:59:34 step: 2043, epoch: 61, batch: 29, loss: 1.4115190505981445, acc: 50.0, f1: 15.591397849462362, r: 0.24602293306868891
06/02/2019 09:59:35 *** evaluating ***
06/02/2019 09:59:35 step: 62, epoch: 61, acc: 56.41025641025641, f1: 17.678627801746813, r: 0.33779996752728636
06/02/2019 09:59:35 *** epoch: 63 ***
06/02/2019 09:59:35 *** training ***
06/02/2019 09:59:37 step: 2051, epoch: 62, batch: 4, loss: 1.207415223121643, acc: 59.375, f1: 19.04761904761905, r: 0.3240090201845679
06/02/2019 09:59:38 step: 2056, epoch: 62, batch: 9, loss: 1.3346561193466187, acc: 46.875, f1: 17.776907001044933, r: 0.343484404274333
06/02/2019 09:59:39 step: 2061, epoch: 62, batch: 14, loss: 1.358813762664795, acc: 50.0, f1: 15.972222222222221, r: 0.3310625902004579
06/02/2019 09:59:40 step: 2066, epoch: 62, batch: 19, loss: 1.195852518081665, acc: 57.8125, f1: 18.986737102394063, r: 0.321496643434288
06/02/2019 09:59:41 step: 2071, epoch: 62, batch: 24, loss: 1.3722403049468994, acc: 51.5625, f1: 20.054945054945055, r: 0.24794765035404542
06/02/2019 09:59:42 step: 2076, epoch: 62, batch: 29, loss: 1.3560583591461182, acc: 46.875, f1: 19.10391742904058, r: 0.3505395754845501
06/02/2019 09:59:43 *** evaluating ***
06/02/2019 09:59:43 step: 63, epoch: 62, acc: 57.692307692307686, f1: 17.414730848306526, r: 0.347814892734952
06/02/2019 09:59:43 *** epoch: 64 ***
06/02/2019 09:59:43 *** training ***
06/02/2019 09:59:45 step: 2084, epoch: 63, batch: 4, loss: 1.573257565498352, acc: 40.625, f1: 21.222060252672495, r: 0.2863022744308315
06/02/2019 09:59:46 step: 2089, epoch: 63, batch: 9, loss: 1.3292315006256104, acc: 46.875, f1: 18.95991332611051, r: 0.33232812091305214
06/02/2019 09:59:47 step: 2094, epoch: 63, batch: 14, loss: 1.2998876571655273, acc: 54.6875, f1: 19.24242424242424, r: 0.2647263308854999
06/02/2019 09:59:48 step: 2099, epoch: 63, batch: 19, loss: 1.345314383506775, acc: 53.125, f1: 17.488734624284493, r: 0.20310947215129244
06/02/2019 09:59:49 step: 2104, epoch: 63, batch: 24, loss: 1.2752786874771118, acc: 46.875, f1: 17.303532804614274, r: 0.29739141274676073
06/02/2019 09:59:50 step: 2109, epoch: 63, batch: 29, loss: 1.2792686223983765, acc: 51.5625, f1: 28.31402831402831, r: 0.3221122658608475
06/02/2019 09:59:51 *** evaluating ***
06/02/2019 09:59:51 step: 64, epoch: 63, acc: 57.692307692307686, f1: 17.95803359590016, r: 0.3354486528478283
06/02/2019 09:59:51 *** epoch: 65 ***
06/02/2019 09:59:51 *** training ***
06/02/2019 09:59:52 step: 2117, epoch: 64, batch: 4, loss: 1.385459065437317, acc: 48.4375, f1: 18.805704099821746, r: 0.3704180755079486
06/02/2019 09:59:54 step: 2122, epoch: 64, batch: 9, loss: 1.4103212356567383, acc: 42.1875, f1: 15.73158076790038, r: 0.36310126741966486
06/02/2019 09:59:55 step: 2127, epoch: 64, batch: 14, loss: 1.3765822649002075, acc: 48.4375, f1: 20.807823129251698, r: 0.33044911720162434
06/02/2019 09:59:56 step: 2132, epoch: 64, batch: 19, loss: 1.4817020893096924, acc: 39.0625, f1: 15.495337995337996, r: 0.3962491705556229
06/02/2019 09:59:57 step: 2137, epoch: 64, batch: 24, loss: 1.3180339336395264, acc: 46.875, f1: 17.203389830508474, r: 0.3667997435163946
06/02/2019 09:59:58 step: 2142, epoch: 64, batch: 29, loss: 1.1066538095474243, acc: 51.5625, f1: 21.292068324031796, r: 0.27055862236941386
06/02/2019 09:59:59 *** evaluating ***
06/02/2019 09:59:59 step: 65, epoch: 64, acc: 57.26495726495726, f1: 17.31379056047198, r: 0.3486823739550723
06/02/2019 09:59:59 *** epoch: 66 ***
06/02/2019 09:59:59 *** training ***
06/02/2019 10:00:00 step: 2150, epoch: 65, batch: 4, loss: 1.272422194480896, acc: 51.5625, f1: 19.275252525252526, r: 0.39522903050065855
06/02/2019 10:00:01 step: 2155, epoch: 65, batch: 9, loss: 1.291751503944397, acc: 57.8125, f1: 19.574797937569677, r: 0.36637283635571993
06/02/2019 10:00:02 step: 2160, epoch: 65, batch: 14, loss: 1.467529058456421, acc: 42.1875, f1: 18.3039858039858, r: 0.37382395008011016
06/02/2019 10:00:03 step: 2165, epoch: 65, batch: 19, loss: 1.3086328506469727, acc: 56.25, f1: 21.772486772486772, r: 0.3361891890177934
06/02/2019 10:00:04 step: 2170, epoch: 65, batch: 24, loss: 1.4032505750656128, acc: 46.875, f1: 14.753320683111953, r: 0.35031694975471184
06/02/2019 10:00:06 step: 2175, epoch: 65, batch: 29, loss: 1.1519432067871094, acc: 50.0, f1: 17.30392156862745, r: 0.40236688282142985
06/02/2019 10:00:06 *** evaluating ***
06/02/2019 10:00:06 step: 66, epoch: 65, acc: 57.692307692307686, f1: 17.50618605959083, r: 0.35383033742118086
06/02/2019 10:00:06 *** epoch: 67 ***
06/02/2019 10:00:06 *** training ***
06/02/2019 10:00:07 step: 2183, epoch: 66, batch: 4, loss: 1.5641989707946777, acc: 37.5, f1: 15.783236371471668, r: 0.27836072751058544
06/02/2019 10:00:09 step: 2188, epoch: 66, batch: 9, loss: 1.270595908164978, acc: 48.4375, f1: 16.557539682539684, r: 0.38728327483698205
06/02/2019 10:00:10 step: 2193, epoch: 66, batch: 14, loss: 1.3067067861557007, acc: 54.6875, f1: 18.56475716064757, r: 0.2829747371068978
06/02/2019 10:00:11 step: 2198, epoch: 66, batch: 19, loss: 1.4004439115524292, acc: 43.75, f1: 17.794486215538846, r: 0.22791115357580488
06/02/2019 10:00:12 step: 2203, epoch: 66, batch: 24, loss: 1.32809579372406, acc: 48.4375, f1: 19.286124323437758, r: 0.31864573167862226
06/02/2019 10:00:13 step: 2208, epoch: 66, batch: 29, loss: 1.3818520307540894, acc: 51.5625, f1: 17.45004995004995, r: 0.31289191783769915
06/02/2019 10:00:14 *** evaluating ***
06/02/2019 10:00:14 step: 67, epoch: 66, acc: 58.119658119658126, f1: 18.01999367788841, r: 0.3480586136756639
06/02/2019 10:00:14 *** epoch: 68 ***
06/02/2019 10:00:14 *** training ***
06/02/2019 10:00:16 step: 2216, epoch: 67, batch: 4, loss: 1.4434031248092651, acc: 48.4375, f1: 16.96748591909882, r: 0.3404918858383582
06/02/2019 10:00:17 step: 2221, epoch: 67, batch: 9, loss: 1.3219637870788574, acc: 54.6875, f1: 15.666666666666668, r: 0.2732232398061083
06/02/2019 10:00:18 step: 2226, epoch: 67, batch: 14, loss: 1.3894391059875488, acc: 54.6875, f1: 16.539408866995075, r: 0.3720779832154051
06/02/2019 10:00:19 step: 2231, epoch: 67, batch: 19, loss: 1.0692517757415771, acc: 60.9375, f1: 33.095238095238095, r: 0.3875358364197564
06/02/2019 10:00:20 step: 2236, epoch: 67, batch: 24, loss: 1.3470011949539185, acc: 43.75, f1: 16.82908545727137, r: 0.26619052337682747
06/02/2019 10:00:21 step: 2241, epoch: 67, batch: 29, loss: 1.1769614219665527, acc: 54.6875, f1: 19.580223880597014, r: 0.4135579188044805
06/02/2019 10:00:22 *** evaluating ***
06/02/2019 10:00:22 step: 68, epoch: 67, acc: 57.26495726495726, f1: 17.64031183966191, r: 0.3517833270895821
06/02/2019 10:00:22 *** epoch: 69 ***
06/02/2019 10:00:22 *** training ***
06/02/2019 10:00:23 step: 2249, epoch: 68, batch: 4, loss: 1.3375024795532227, acc: 51.5625, f1: 18.29455266955267, r: 0.3672530004345265
06/02/2019 10:00:24 step: 2254, epoch: 68, batch: 9, loss: 1.2083278894424438, acc: 46.875, f1: 17.601840421950403, r: 0.3156673605811924
06/02/2019 10:00:25 step: 2259, epoch: 68, batch: 14, loss: 1.2698299884796143, acc: 46.875, f1: 14.050839552238806, r: 0.3363156474643766
06/02/2019 10:00:27 step: 2264, epoch: 68, batch: 19, loss: 1.1474510431289673, acc: 60.9375, f1: 23.980813131359806, r: 0.40715268657422415
06/02/2019 10:00:28 step: 2269, epoch: 68, batch: 24, loss: 1.4132959842681885, acc: 53.125, f1: 19.367458521870287, r: 0.24192746562898407
06/02/2019 10:00:29 step: 2274, epoch: 68, batch: 29, loss: 1.3901318311691284, acc: 42.1875, f1: 15.759637188208616, r: 0.349889334752004
06/02/2019 10:00:29 *** evaluating ***
06/02/2019 10:00:30 step: 69, epoch: 68, acc: 58.97435897435898, f1: 18.37699142156863, r: 0.34128006337506
06/02/2019 10:00:30 *** epoch: 70 ***
06/02/2019 10:00:30 *** training ***
06/02/2019 10:00:31 step: 2282, epoch: 69, batch: 4, loss: 1.1554927825927734, acc: 54.6875, f1: 18.64795918367347, r: 0.32227140213237293
06/02/2019 10:00:32 step: 2287, epoch: 69, batch: 9, loss: 1.2854195833206177, acc: 54.6875, f1: 21.779274281469192, r: 0.24629445502619252
06/02/2019 10:00:33 step: 2292, epoch: 69, batch: 14, loss: 1.4145699739456177, acc: 42.1875, f1: 15.38548752834467, r: 0.34539741351639897
06/02/2019 10:00:35 step: 2297, epoch: 69, batch: 19, loss: 1.4731378555297852, acc: 45.3125, f1: 15.41866028708134, r: 0.3109525117365267
06/02/2019 10:00:36 step: 2302, epoch: 69, batch: 24, loss: 1.400753378868103, acc: 43.75, f1: 16.255615453728662, r: 0.2995776024055183
06/02/2019 10:00:37 step: 2307, epoch: 69, batch: 29, loss: 1.2733451128005981, acc: 46.875, f1: 14.953478068232167, r: 0.34549233072321117
06/02/2019 10:00:37 *** evaluating ***
06/02/2019 10:00:38 step: 70, epoch: 69, acc: 56.41025641025641, f1: 16.463761699974704, r: 0.3562161772067563
06/02/2019 10:00:38 *** epoch: 71 ***
06/02/2019 10:00:38 *** training ***
06/02/2019 10:00:39 step: 2315, epoch: 70, batch: 4, loss: 1.3916534185409546, acc: 51.5625, f1: 17.660256410256412, r: 0.29951332601660374
06/02/2019 10:00:40 step: 2320, epoch: 70, batch: 9, loss: 1.2390061616897583, acc: 50.0, f1: 22.26070226070226, r: 0.41626537344820647
06/02/2019 10:00:41 step: 2325, epoch: 70, batch: 14, loss: 1.0619280338287354, acc: 57.8125, f1: 25.263769132163944, r: 0.3026921457477849
06/02/2019 10:00:42 step: 2330, epoch: 70, batch: 19, loss: 1.1965317726135254, acc: 53.125, f1: 21.757518796992482, r: 0.3791079744280877
06/02/2019 10:00:43 step: 2335, epoch: 70, batch: 24, loss: 1.3539010286331177, acc: 53.125, f1: 18.055555555555554, r: 0.2749909226140752
06/02/2019 10:00:45 step: 2340, epoch: 70, batch: 29, loss: 1.3791643381118774, acc: 48.4375, f1: 21.90380184331797, r: 0.3203398366462772
06/02/2019 10:00:45 *** evaluating ***
06/02/2019 10:00:45 step: 71, epoch: 70, acc: 57.692307692307686, f1: 17.478718044980095, r: 0.3551887140492376
06/02/2019 10:00:45 *** epoch: 72 ***
06/02/2019 10:00:45 *** training ***
06/02/2019 10:00:46 step: 2348, epoch: 71, batch: 4, loss: 1.1426527500152588, acc: 51.5625, f1: 23.014732713008577, r: 0.37076959379245117
06/02/2019 10:00:48 step: 2353, epoch: 71, batch: 9, loss: 1.6637486219406128, acc: 40.625, f1: 16.78118393234672, r: 0.32753062520976167
06/02/2019 10:00:49 step: 2358, epoch: 71, batch: 14, loss: 1.3978016376495361, acc: 45.3125, f1: 18.790064102564102, r: 0.3487137740314239
06/02/2019 10:00:50 step: 2363, epoch: 71, batch: 19, loss: 1.3164362907409668, acc: 48.4375, f1: 21.284722222222218, r: 0.33150483061500113
06/02/2019 10:00:51 step: 2368, epoch: 71, batch: 24, loss: 1.1912641525268555, acc: 53.125, f1: 16.117216117216117, r: 0.2919482705677378
06/02/2019 10:00:52 step: 2373, epoch: 71, batch: 29, loss: 1.1329436302185059, acc: 60.9375, f1: 21.98396926338103, r: 0.4306452153187186
06/02/2019 10:00:53 *** evaluating ***
06/02/2019 10:00:53 step: 72, epoch: 71, acc: 57.26495726495726, f1: 17.47385530110972, r: 0.3614713285200193
06/02/2019 10:00:53 *** epoch: 73 ***
06/02/2019 10:00:53 *** training ***
06/02/2019 10:00:54 step: 2381, epoch: 72, batch: 4, loss: 1.2955325841903687, acc: 50.0, f1: 17.445549242424242, r: 0.34141581246395725
06/02/2019 10:00:55 step: 2386, epoch: 72, batch: 9, loss: 1.100521445274353, acc: 60.9375, f1: 19.28070175438597, r: 0.4232179256693817
06/02/2019 10:00:57 step: 2391, epoch: 72, batch: 14, loss: 1.3325899839401245, acc: 50.0, f1: 17.424485125858126, r: 0.24672572932512787
06/02/2019 10:00:58 step: 2396, epoch: 72, batch: 19, loss: 1.2983272075653076, acc: 51.5625, f1: 20.496329679412746, r: 0.3422766428427189
06/02/2019 10:00:59 step: 2401, epoch: 72, batch: 24, loss: 1.197263479232788, acc: 54.6875, f1: 24.865938430983118, r: 0.38408614205555386
06/02/2019 10:01:00 step: 2406, epoch: 72, batch: 29, loss: 1.2249066829681396, acc: 56.25, f1: 22.765435495334387, r: 0.3702975019046776
06/02/2019 10:01:01 *** evaluating ***
06/02/2019 10:01:01 step: 73, epoch: 72, acc: 57.692307692307686, f1: 18.04563453258191, r: 0.36820774701505077
06/02/2019 10:01:01 *** epoch: 74 ***
06/02/2019 10:01:01 *** training ***
06/02/2019 10:01:02 step: 2414, epoch: 73, batch: 4, loss: 1.2125135660171509, acc: 56.25, f1: 20.113378684807255, r: 0.39996580091019596
06/02/2019 10:01:03 step: 2419, epoch: 73, batch: 9, loss: 1.0867669582366943, acc: 57.8125, f1: 18.671023965141615, r: 0.30394163386356393
06/02/2019 10:01:05 step: 2424, epoch: 73, batch: 14, loss: 1.3123112916946411, acc: 51.5625, f1: 21.880615688665223, r: 0.32480630978941794
06/02/2019 10:01:06 step: 2429, epoch: 73, batch: 19, loss: 1.1933484077453613, acc: 54.6875, f1: 26.177512747385705, r: 0.2809324401519435
06/02/2019 10:01:07 step: 2434, epoch: 73, batch: 24, loss: 1.1570698022842407, acc: 50.0, f1: 17.018928783634664, r: 0.40861460382258125
06/02/2019 10:01:08 step: 2439, epoch: 73, batch: 29, loss: 1.2310289144515991, acc: 62.5, f1: 24.016290726817047, r: 0.4200379714957004
06/02/2019 10:01:08 *** evaluating ***
06/02/2019 10:01:09 step: 74, epoch: 73, acc: 58.54700854700855, f1: 18.854801413592007, r: 0.3650374282957533
06/02/2019 10:01:09 *** epoch: 75 ***
06/02/2019 10:01:09 *** training ***
06/02/2019 10:01:10 step: 2447, epoch: 74, batch: 4, loss: 0.981054961681366, acc: 57.8125, f1: 18.865546218487395, r: 0.3113963281877638
06/02/2019 10:01:11 step: 2452, epoch: 74, batch: 9, loss: 1.3200979232788086, acc: 48.4375, f1: 17.784926470588236, r: 0.3876248009470727
06/02/2019 10:01:12 step: 2457, epoch: 74, batch: 14, loss: 1.2723631858825684, acc: 53.125, f1: 20.49291120990028, r: 0.25727146014675073
06/02/2019 10:01:13 step: 2462, epoch: 74, batch: 19, loss: 1.2724032402038574, acc: 45.3125, f1: 17.137476459510356, r: 0.3230194524861819
06/02/2019 10:01:14 step: 2467, epoch: 74, batch: 24, loss: 1.1609821319580078, acc: 50.0, f1: 15.225659229208924, r: 0.29810123936405447
06/02/2019 10:01:15 step: 2472, epoch: 74, batch: 29, loss: 1.3185195922851562, acc: 45.3125, f1: 14.427083333333336, r: 0.3398978501702453
06/02/2019 10:01:16 *** evaluating ***
06/02/2019 10:01:16 step: 75, epoch: 74, acc: 56.41025641025641, f1: 17.40760594419131, r: 0.35958771197953243
06/02/2019 10:01:16 *** epoch: 76 ***
06/02/2019 10:01:16 *** training ***
06/02/2019 10:01:17 step: 2480, epoch: 75, batch: 4, loss: 1.2070318460464478, acc: 59.375, f1: 32.31531908951264, r: 0.4223735228973939
06/02/2019 10:01:18 step: 2485, epoch: 75, batch: 9, loss: 1.0470682382583618, acc: 59.375, f1: 22.544122544122544, r: 0.40797902384578044
06/02/2019 10:01:19 step: 2490, epoch: 75, batch: 14, loss: 1.3511348962783813, acc: 42.1875, f1: 16.707089552238806, r: 0.37044376766416454
06/02/2019 10:01:20 step: 2495, epoch: 75, batch: 19, loss: 1.1976982355117798, acc: 50.0, f1: 16.76814988290398, r: 0.40983042334558306
06/02/2019 10:01:22 step: 2500, epoch: 75, batch: 24, loss: 1.1199308633804321, acc: 59.375, f1: 19.222689075630253, r: 0.3058121403354311
06/02/2019 10:01:23 step: 2505, epoch: 75, batch: 29, loss: 1.214998722076416, acc: 53.125, f1: 26.660574043941182, r: 0.37019564466819943
06/02/2019 10:01:23 *** evaluating ***
06/02/2019 10:01:24 step: 76, epoch: 75, acc: 57.692307692307686, f1: 17.752364455904498, r: 0.3682947592217042
06/02/2019 10:01:24 *** epoch: 77 ***
06/02/2019 10:01:24 *** training ***
06/02/2019 10:01:24 step: 2513, epoch: 76, batch: 4, loss: 1.0854206085205078, acc: 59.375, f1: 20.245098039215687, r: 0.43409204870327145
06/02/2019 10:01:26 step: 2518, epoch: 76, batch: 9, loss: 1.259230136871338, acc: 50.0, f1: 29.052358648377385, r: 0.404185753648941
06/02/2019 10:01:27 step: 2523, epoch: 76, batch: 14, loss: 1.3013581037521362, acc: 39.0625, f1: 24.04536368179546, r: 0.40807290868372575
06/02/2019 10:01:28 step: 2528, epoch: 76, batch: 19, loss: 1.2202116250991821, acc: 51.5625, f1: 21.6894461202873, r: 0.3373184697142523
06/02/2019 10:01:29 step: 2533, epoch: 76, batch: 24, loss: 1.0602190494537354, acc: 56.25, f1: 23.02867383512545, r: 0.3140330468272613
06/02/2019 10:01:30 step: 2538, epoch: 76, batch: 29, loss: 1.330651044845581, acc: 42.1875, f1: 18.21193299454169, r: 0.38237572647701346
06/02/2019 10:01:31 *** evaluating ***
06/02/2019 10:01:31 step: 77, epoch: 76, acc: 58.119658119658126, f1: 18.642340303512505, r: 0.3388486556423234
06/02/2019 10:01:31 *** epoch: 78 ***
06/02/2019 10:01:31 *** training ***
06/02/2019 10:01:32 step: 2546, epoch: 77, batch: 4, loss: 0.9792928099632263, acc: 59.375, f1: 25.197905249056145, r: 0.32889454565029735
06/02/2019 10:01:33 step: 2551, epoch: 77, batch: 9, loss: 1.1909358501434326, acc: 54.6875, f1: 16.76693404634581, r: 0.37575327102306577
06/02/2019 10:01:35 step: 2556, epoch: 77, batch: 14, loss: 1.2813085317611694, acc: 46.875, f1: 20.113960113960115, r: 0.3678783901487381
06/02/2019 10:01:36 step: 2561, epoch: 77, batch: 19, loss: 1.0210953950881958, acc: 64.0625, f1: 27.992424242424242, r: 0.4493423334982354
06/02/2019 10:01:37 step: 2566, epoch: 77, batch: 24, loss: 1.1949573755264282, acc: 54.6875, f1: 20.6356048166393, r: 0.3779532740838465
06/02/2019 10:01:38 step: 2571, epoch: 77, batch: 29, loss: 1.0968705415725708, acc: 54.6875, f1: 20.07471209669744, r: 0.3674298446627657
06/02/2019 10:01:39 *** evaluating ***
06/02/2019 10:01:39 step: 78, epoch: 77, acc: 57.692307692307686, f1: 18.334071089590708, r: 0.3709707442113929
06/02/2019 10:01:39 *** epoch: 79 ***
06/02/2019 10:01:39 *** training ***
06/02/2019 10:01:40 step: 2579, epoch: 78, batch: 4, loss: 1.1401475667953491, acc: 51.5625, f1: 17.579908675799086, r: 0.4256841671647056
06/02/2019 10:01:41 step: 2584, epoch: 78, batch: 9, loss: 1.294543981552124, acc: 37.5, f1: 15.616883116883118, r: 0.37198320382841515
06/02/2019 10:01:42 step: 2589, epoch: 78, batch: 14, loss: 1.226464033126831, acc: 51.5625, f1: 22.69547495111405, r: 0.35702870050829133
06/02/2019 10:01:44 step: 2594, epoch: 78, batch: 19, loss: 1.2114086151123047, acc: 56.25, f1: 21.844055347891665, r: 0.3696316040841619
06/02/2019 10:01:45 step: 2599, epoch: 78, batch: 24, loss: 1.0929828882217407, acc: 60.9375, f1: 30.368814192343613, r: 0.3792127266728958
06/02/2019 10:01:46 step: 2604, epoch: 78, batch: 29, loss: 1.1742682456970215, acc: 53.125, f1: 20.76338386950984, r: 0.3957375477919663
06/02/2019 10:01:46 *** evaluating ***
06/02/2019 10:01:47 step: 79, epoch: 78, acc: 58.119658119658126, f1: 18.622851838076855, r: 0.3731344365210914
06/02/2019 10:01:47 *** epoch: 80 ***
06/02/2019 10:01:47 *** training ***
06/02/2019 10:01:48 step: 2612, epoch: 79, batch: 4, loss: 1.4584630727767944, acc: 40.625, f1: 15.368131868131869, r: 0.3309177363366885
06/02/2019 10:01:49 step: 2617, epoch: 79, batch: 9, loss: 1.1697155237197876, acc: 62.5, f1: 28.149350649350644, r: 0.44408431381813845
06/02/2019 10:01:50 step: 2622, epoch: 79, batch: 14, loss: 1.243891716003418, acc: 50.0, f1: 18.814484126984127, r: 0.3200700731106172
06/02/2019 10:01:51 step: 2627, epoch: 79, batch: 19, loss: 1.2423068284988403, acc: 46.875, f1: 21.781786302534403, r: 0.39573524728904935
06/02/2019 10:01:53 step: 2632, epoch: 79, batch: 24, loss: 1.1039085388183594, acc: 62.5, f1: 38.974747793854384, r: 0.43192440055941433
06/02/2019 10:01:54 step: 2637, epoch: 79, batch: 29, loss: 1.266943097114563, acc: 46.875, f1: 16.020225294418843, r: 0.30730268341317485
06/02/2019 10:01:54 *** evaluating ***
06/02/2019 10:01:55 step: 80, epoch: 79, acc: 57.692307692307686, f1: 18.62957095996036, r: 0.3738576208813126
06/02/2019 10:01:55 *** epoch: 81 ***
06/02/2019 10:01:55 *** training ***
06/02/2019 10:01:56 step: 2645, epoch: 80, batch: 4, loss: 1.0480674505233765, acc: 64.0625, f1: 30.347985347985347, r: 0.40155412858563494
06/02/2019 10:01:57 step: 2650, epoch: 80, batch: 9, loss: 1.1047992706298828, acc: 54.6875, f1: 18.82847666429756, r: 0.3624486254602803
06/02/2019 10:01:58 step: 2655, epoch: 80, batch: 14, loss: 1.3372704982757568, acc: 50.0, f1: 16.65554871373837, r: 0.3646761840131124
06/02/2019 10:01:59 step: 2660, epoch: 80, batch: 19, loss: 1.2887279987335205, acc: 48.4375, f1: 19.337354374948358, r: 0.40163419350664925
06/02/2019 10:02:00 step: 2665, epoch: 80, batch: 24, loss: 1.145188808441162, acc: 50.0, f1: 19.928415810768747, r: 0.33649047290468515
06/02/2019 10:02:01 step: 2670, epoch: 80, batch: 29, loss: 1.2671425342559814, acc: 53.125, f1: 18.515356187077863, r: 0.3953567245346726
06/02/2019 10:02:02 *** evaluating ***
06/02/2019 10:02:02 step: 81, epoch: 80, acc: 58.119658119658126, f1: 19.468437220744256, r: 0.3788045679113266
06/02/2019 10:02:02 *** epoch: 82 ***
06/02/2019 10:02:02 *** training ***
06/02/2019 10:02:03 step: 2678, epoch: 81, batch: 4, loss: 1.1055753231048584, acc: 50.0, f1: 18.090386624869385, r: 0.4142877523552264
06/02/2019 10:02:04 step: 2683, epoch: 81, batch: 9, loss: 1.199917197227478, acc: 54.6875, f1: 24.76445610773969, r: 0.3651246174230444
06/02/2019 10:02:05 step: 2688, epoch: 81, batch: 14, loss: 1.1580036878585815, acc: 50.0, f1: 17.851153039832283, r: 0.3658741711167595
06/02/2019 10:02:06 step: 2693, epoch: 81, batch: 19, loss: 1.2044349908828735, acc: 62.5, f1: 27.274774774774773, r: 0.3831702114822558
06/02/2019 10:02:08 step: 2698, epoch: 81, batch: 24, loss: 1.108533501625061, acc: 54.6875, f1: 16.517857142857142, r: 0.3726351717468151
06/02/2019 10:02:09 step: 2703, epoch: 81, batch: 29, loss: 1.1733732223510742, acc: 53.125, f1: 21.778207045530078, r: 0.3782597168623929
06/02/2019 10:02:09 *** evaluating ***
06/02/2019 10:02:10 step: 82, epoch: 81, acc: 58.54700854700855, f1: 19.321382584902626, r: 0.3815010826414716
06/02/2019 10:02:10 *** epoch: 83 ***
06/02/2019 10:02:10 *** training ***
06/02/2019 10:02:11 step: 2711, epoch: 82, batch: 4, loss: 1.2833954095840454, acc: 51.5625, f1: 22.041292041292042, r: 0.37091756938822
06/02/2019 10:02:12 step: 2716, epoch: 82, batch: 9, loss: 1.02556312084198, acc: 59.375, f1: 25.267454954954953, r: 0.3068230377387848
06/02/2019 10:02:13 step: 2721, epoch: 82, batch: 14, loss: 1.1009392738342285, acc: 54.6875, f1: 25.153061224489797, r: 0.4516154978971799
06/02/2019 10:02:15 step: 2726, epoch: 82, batch: 19, loss: 1.2013123035430908, acc: 56.25, f1: 28.95927601809955, r: 0.47296894914663
06/02/2019 10:02:16 step: 2731, epoch: 82, batch: 24, loss: 1.1456224918365479, acc: 59.375, f1: 23.960684300803464, r: 0.3997891829551181
06/02/2019 10:02:17 step: 2736, epoch: 82, batch: 29, loss: 1.300246238708496, acc: 42.1875, f1: 14.346042471042471, r: 0.3929238551821565
06/02/2019 10:02:17 *** evaluating ***
06/02/2019 10:02:18 step: 83, epoch: 82, acc: 58.119658119658126, f1: 19.81479664729046, r: 0.38799298594184567
06/02/2019 10:02:18 *** epoch: 84 ***
06/02/2019 10:02:18 *** training ***
06/02/2019 10:02:18 step: 2744, epoch: 83, batch: 4, loss: 1.0584083795547485, acc: 60.9375, f1: 23.42105263157895, r: 0.37963641479632443
06/02/2019 10:02:20 step: 2749, epoch: 83, batch: 9, loss: 1.2991278171539307, acc: 43.75, f1: 19.16714864083285, r: 0.33261466902184844
06/02/2019 10:02:21 step: 2754, epoch: 83, batch: 14, loss: 1.3062537908554077, acc: 39.0625, f1: 16.558704453441297, r: 0.3739145771209362
06/02/2019 10:02:22 step: 2759, epoch: 83, batch: 19, loss: 1.2640615701675415, acc: 50.0, f1: 17.967186874749903, r: 0.3240509291409316
06/02/2019 10:02:23 step: 2764, epoch: 83, batch: 24, loss: 1.1655205488204956, acc: 50.0, f1: 16.940789473684212, r: 0.3364383112227249
06/02/2019 10:02:24 step: 2769, epoch: 83, batch: 29, loss: 1.0645649433135986, acc: 67.1875, f1: 26.53061224489796, r: 0.260672240820064
06/02/2019 10:02:25 *** evaluating ***
06/02/2019 10:02:25 step: 84, epoch: 83, acc: 58.119658119658126, f1: 19.47500756200847, r: 0.38771711286123006
06/02/2019 10:02:25 *** epoch: 85 ***
06/02/2019 10:02:25 *** training ***
06/02/2019 10:02:26 step: 2777, epoch: 84, batch: 4, loss: 1.0484213829040527, acc: 60.9375, f1: 26.643862673274445, r: 0.37848770903765805
06/02/2019 10:02:27 step: 2782, epoch: 84, batch: 9, loss: 1.221130609512329, acc: 48.4375, f1: 18.682539682539684, r: 0.40684126647418434
06/02/2019 10:02:28 step: 2787, epoch: 84, batch: 14, loss: 1.099220633506775, acc: 57.8125, f1: 25.58116134697711, r: 0.4704424837372305
06/02/2019 10:02:30 step: 2792, epoch: 84, batch: 19, loss: 1.2840189933776855, acc: 45.3125, f1: 20.601851851851848, r: 0.3099650062337753
06/02/2019 10:02:31 step: 2797, epoch: 84, batch: 24, loss: 1.1101131439208984, acc: 59.375, f1: 23.538995726495727, r: 0.3687828979995914
06/02/2019 10:02:32 step: 2802, epoch: 84, batch: 29, loss: 1.1078486442565918, acc: 59.375, f1: 29.311667146007093, r: 0.35824757133177115
06/02/2019 10:02:33 *** evaluating ***
06/02/2019 10:02:33 step: 85, epoch: 84, acc: 56.837606837606835, f1: 18.447562196168143, r: 0.3763206450110261
06/02/2019 10:02:33 *** epoch: 86 ***
06/02/2019 10:02:33 *** training ***
06/02/2019 10:02:34 step: 2810, epoch: 85, batch: 4, loss: 1.133484125137329, acc: 53.125, f1: 21.58653846153846, r: 0.4650073222581191
06/02/2019 10:02:35 step: 2815, epoch: 85, batch: 9, loss: 1.0911999940872192, acc: 54.6875, f1: 26.706349206349206, r: 0.41421957527804154
06/02/2019 10:02:36 step: 2820, epoch: 85, batch: 14, loss: 1.0667247772216797, acc: 59.375, f1: 30.76401040965585, r: 0.4447151347307305
06/02/2019 10:02:37 step: 2825, epoch: 85, batch: 19, loss: 1.070003628730774, acc: 54.6875, f1: 28.244504667268895, r: 0.3308107970971269
06/02/2019 10:02:38 step: 2830, epoch: 85, batch: 24, loss: 1.1200706958770752, acc: 51.5625, f1: 24.959114959114963, r: 0.42535807384217544
06/02/2019 10:02:39 step: 2835, epoch: 85, batch: 29, loss: 1.2258102893829346, acc: 50.0, f1: 19.268534238822866, r: 0.37138123991343
06/02/2019 10:02:40 *** evaluating ***
06/02/2019 10:02:40 step: 86, epoch: 85, acc: 58.119658119658126, f1: 19.577430145611967, r: 0.3823847846666151
06/02/2019 10:02:40 *** epoch: 87 ***
06/02/2019 10:02:40 *** training ***
06/02/2019 10:02:42 step: 2843, epoch: 86, batch: 4, loss: 1.0234935283660889, acc: 56.25, f1: 17.79039856195376, r: 0.3372815216939774
06/02/2019 10:02:43 step: 2848, epoch: 86, batch: 9, loss: 1.061100959777832, acc: 62.5, f1: 35.063131313131315, r: 0.5161292862627291
06/02/2019 10:02:44 step: 2853, epoch: 86, batch: 14, loss: 1.1390715837478638, acc: 57.8125, f1: 19.67050481394744, r: 0.38133666678572437
06/02/2019 10:02:44 step: 2858, epoch: 86, batch: 19, loss: 1.0134778022766113, acc: 60.9375, f1: 46.0762097037243, r: 0.36048768434663425
06/02/2019 10:02:46 step: 2863, epoch: 86, batch: 24, loss: 1.1111217737197876, acc: 54.6875, f1: 22.574283778552072, r: 0.383252651382679
06/02/2019 10:02:47 step: 2868, epoch: 86, batch: 29, loss: 1.1204897165298462, acc: 54.6875, f1: 27.661993179234557, r: 0.36834224578125385
06/02/2019 10:02:47 *** evaluating ***
06/02/2019 10:02:48 step: 87, epoch: 86, acc: 57.26495726495726, f1: 19.398450727302517, r: 0.3736595999257537
06/02/2019 10:02:48 *** epoch: 88 ***
06/02/2019 10:02:48 *** training ***
06/02/2019 10:02:49 step: 2876, epoch: 87, batch: 4, loss: 0.862772524356842, acc: 62.5, f1: 31.716391716391712, r: 0.336729441302533
06/02/2019 10:02:50 step: 2881, epoch: 87, batch: 9, loss: 1.4571176767349243, acc: 45.3125, f1: 23.652889727319497, r: 0.3032796062207642
06/02/2019 10:02:51 step: 2886, epoch: 87, batch: 14, loss: 1.217786192893982, acc: 50.0, f1: 18.16041773668892, r: 0.3774071328467142
06/02/2019 10:02:52 step: 2891, epoch: 87, batch: 19, loss: 1.0996304750442505, acc: 51.5625, f1: 23.57817620975516, r: 0.4509476559855972
06/02/2019 10:02:53 step: 2896, epoch: 87, batch: 24, loss: 1.0626800060272217, acc: 65.625, f1: 38.5455324703155, r: 0.4107692085038848
06/02/2019 10:02:54 step: 2901, epoch: 87, batch: 29, loss: 1.1328809261322021, acc: 60.9375, f1: 23.190324858757062, r: 0.3725981658262465
06/02/2019 10:02:55 *** evaluating ***
06/02/2019 10:02:55 step: 88, epoch: 87, acc: 57.692307692307686, f1: 19.80788776017113, r: 0.375410321537152
06/02/2019 10:02:55 *** epoch: 89 ***
06/02/2019 10:02:55 *** training ***
06/02/2019 10:02:56 step: 2909, epoch: 88, batch: 4, loss: 0.9790531396865845, acc: 59.375, f1: 35.64921620822243, r: 0.4350872908632745
06/02/2019 10:02:57 step: 2914, epoch: 88, batch: 9, loss: 1.1034247875213623, acc: 51.5625, f1: 23.576097105508868, r: 0.45656613194462536
06/02/2019 10:02:59 step: 2919, epoch: 88, batch: 14, loss: 1.10482656955719, acc: 59.375, f1: 22.394553644553643, r: 0.41449634702266347
06/02/2019 10:03:00 step: 2924, epoch: 88, batch: 19, loss: 0.997093915939331, acc: 59.375, f1: 36.88545688545688, r: 0.44542181723993785
06/02/2019 10:03:01 step: 2929, epoch: 88, batch: 24, loss: 1.0858122110366821, acc: 51.5625, f1: 25.616598313966733, r: 0.4710558374738749
06/02/2019 10:03:02 step: 2934, epoch: 88, batch: 29, loss: 1.19344961643219, acc: 53.125, f1: 21.057919621749406, r: 0.38208935439322017
06/02/2019 10:03:03 *** evaluating ***
06/02/2019 10:03:03 step: 89, epoch: 88, acc: 57.26495726495726, f1: 19.734345351043643, r: 0.3820603255996735
06/02/2019 10:03:03 *** epoch: 90 ***
06/02/2019 10:03:03 *** training ***
06/02/2019 10:03:04 step: 2942, epoch: 89, batch: 4, loss: 0.990410566329956, acc: 60.9375, f1: 25.6422569027611, r: 0.41354471859700764
06/02/2019 10:03:05 step: 2947, epoch: 89, batch: 9, loss: 1.3004062175750732, acc: 51.5625, f1: 18.705685618729095, r: 0.4176799971557703
06/02/2019 10:03:06 step: 2952, epoch: 89, batch: 14, loss: 1.2634141445159912, acc: 42.1875, f1: 17.994949494949495, r: 0.4612387841441136
06/02/2019 10:03:07 step: 2957, epoch: 89, batch: 19, loss: 1.1529079675674438, acc: 56.25, f1: 20.442073170731707, r: 0.341142959224921
06/02/2019 10:03:08 step: 2962, epoch: 89, batch: 24, loss: 1.0479919910430908, acc: 54.6875, f1: 20.783730158730158, r: 0.3683399219490873
06/02/2019 10:03:09 step: 2967, epoch: 89, batch: 29, loss: 1.0788822174072266, acc: 65.625, f1: 29.2220936957779, r: 0.36864059370962643
06/02/2019 10:03:10 *** evaluating ***
06/02/2019 10:03:11 step: 90, epoch: 89, acc: 56.837606837606835, f1: 19.440882924641148, r: 0.3903251840216737
06/02/2019 10:03:11 *** epoch: 91 ***
06/02/2019 10:03:11 *** training ***
06/02/2019 10:03:12 step: 2975, epoch: 90, batch: 4, loss: 1.1368036270141602, acc: 53.125, f1: 20.946345484837238, r: 0.4050188006970775
06/02/2019 10:03:13 step: 2980, epoch: 90, batch: 9, loss: 0.9412848353385925, acc: 60.9375, f1: 43.265306122448976, r: 0.40533097905863824
06/02/2019 10:03:14 step: 2985, epoch: 90, batch: 14, loss: 1.1771591901779175, acc: 54.6875, f1: 22.580645161290324, r: 0.3330897627713015
06/02/2019 10:03:15 step: 2990, epoch: 90, batch: 19, loss: 1.0043725967407227, acc: 59.375, f1: 24.889955982392955, r: 0.31709588166082503
06/02/2019 10:03:17 step: 2995, epoch: 90, batch: 24, loss: 1.2689642906188965, acc: 46.875, f1: 22.08592132505176, r: 0.40415836653343273
06/02/2019 10:03:18 step: 3000, epoch: 90, batch: 29, loss: 1.019514560699463, acc: 54.6875, f1: 30.461275419258616, r: 0.40862225526212076
06/02/2019 10:03:18 *** evaluating ***
06/02/2019 10:03:19 step: 91, epoch: 90, acc: 56.837606837606835, f1: 19.016678533800487, r: 0.3800802481029007
06/02/2019 10:03:19 *** epoch: 92 ***
06/02/2019 10:03:19 *** training ***
06/02/2019 10:03:20 step: 3008, epoch: 91, batch: 4, loss: 1.380530834197998, acc: 35.9375, f1: 17.56143162393162, r: 0.4380778537408971
06/02/2019 10:03:21 step: 3013, epoch: 91, batch: 9, loss: 1.2310407161712646, acc: 53.125, f1: 28.606321839080458, r: 0.37259266032465743
06/02/2019 10:03:22 step: 3018, epoch: 91, batch: 14, loss: 1.051749587059021, acc: 57.8125, f1: 32.54452187379017, r: 0.3486114288948095
06/02/2019 10:03:23 step: 3023, epoch: 91, batch: 19, loss: 1.130067229270935, acc: 53.125, f1: 21.29668843664787, r: 0.35104614456142735
06/02/2019 10:03:24 step: 3028, epoch: 91, batch: 24, loss: 1.2094968557357788, acc: 53.125, f1: 21.266968325791858, r: 0.4004682628809503
06/02/2019 10:03:26 step: 3033, epoch: 91, batch: 29, loss: 1.0688767433166504, acc: 54.6875, f1: 20.809625414888572, r: 0.38038849144596676
06/02/2019 10:03:26 *** evaluating ***
06/02/2019 10:03:27 step: 92, epoch: 91, acc: 59.401709401709404, f1: 24.58134926810038, r: 0.40168301840750603
06/02/2019 10:03:27 *** epoch: 93 ***
06/02/2019 10:03:27 *** training ***
06/02/2019 10:03:28 step: 3041, epoch: 92, batch: 4, loss: 1.2043191194534302, acc: 51.5625, f1: 17.505411255411254, r: 0.36603197825202544
06/02/2019 10:03:29 step: 3046, epoch: 92, batch: 9, loss: 1.1067169904708862, acc: 60.9375, f1: 32.68702602992416, r: 0.40187667190377174
06/02/2019 10:03:30 step: 3051, epoch: 92, batch: 14, loss: 0.9291954636573792, acc: 67.1875, f1: 41.01197386133866, r: 0.40795802912785434
06/02/2019 10:03:31 step: 3056, epoch: 92, batch: 19, loss: 1.1750651597976685, acc: 46.875, f1: 19.8748970717825, r: 0.37003353967747304
06/02/2019 10:03:32 step: 3061, epoch: 92, batch: 24, loss: 1.228354573249817, acc: 51.5625, f1: 20.834984224814733, r: 0.41764447761469714
06/02/2019 10:03:33 step: 3066, epoch: 92, batch: 29, loss: 1.1568965911865234, acc: 54.6875, f1: 31.976911976911982, r: 0.3992057920209344
06/02/2019 10:03:34 *** evaluating ***
06/02/2019 10:03:34 step: 93, epoch: 92, acc: 58.97435897435898, f1: 21.494463927303205, r: 0.3721255058892878
06/02/2019 10:03:34 *** epoch: 94 ***
06/02/2019 10:03:34 *** training ***
06/02/2019 10:03:35 step: 3074, epoch: 93, batch: 4, loss: 1.097005009651184, acc: 50.0, f1: 21.521829070611293, r: 0.37876855068743165
06/02/2019 10:03:36 step: 3079, epoch: 93, batch: 9, loss: 0.9919309020042419, acc: 57.8125, f1: 41.496598639455776, r: 0.4944056955058709
06/02/2019 10:03:38 step: 3084, epoch: 93, batch: 14, loss: 0.910402238368988, acc: 59.375, f1: 36.678428578849335, r: 0.34999145250239244
06/02/2019 10:03:39 step: 3089, epoch: 93, batch: 19, loss: 1.3398079872131348, acc: 50.0, f1: 21.03174603174603, r: 0.31557183326254046
06/02/2019 10:03:40 step: 3094, epoch: 93, batch: 24, loss: 1.008676528930664, acc: 57.8125, f1: 22.97112462006079, r: 0.4835198199411427
06/02/2019 10:03:41 step: 3099, epoch: 93, batch: 29, loss: 1.1599359512329102, acc: 46.875, f1: 24.14622414622415, r: 0.4166851821893459
06/02/2019 10:03:42 *** evaluating ***
06/02/2019 10:03:42 step: 94, epoch: 93, acc: 57.692307692307686, f1: 20.089433892679327, r: 0.3932654907821974
06/02/2019 10:03:42 *** epoch: 95 ***
06/02/2019 10:03:42 *** training ***
06/02/2019 10:03:43 step: 3107, epoch: 94, batch: 4, loss: 1.065278172492981, acc: 57.8125, f1: 29.27666949632145, r: 0.3749120130489272
06/02/2019 10:03:44 step: 3112, epoch: 94, batch: 9, loss: 1.224467158317566, acc: 51.5625, f1: 20.92459409393048, r: 0.4276666416272858
06/02/2019 10:03:45 step: 3117, epoch: 94, batch: 14, loss: 0.9992339611053467, acc: 65.625, f1: 35.9577922077922, r: 0.4176651213832138
06/02/2019 10:03:47 step: 3122, epoch: 94, batch: 19, loss: 0.8930496573448181, acc: 64.0625, f1: 34.92694805194806, r: 0.43297723344818
06/02/2019 10:03:48 step: 3127, epoch: 94, batch: 24, loss: 1.203951120376587, acc: 54.6875, f1: 24.84051036682616, r: 0.34165354138424703
06/02/2019 10:03:49 step: 3132, epoch: 94, batch: 29, loss: 1.2008267641067505, acc: 48.4375, f1: 17.385163524869405, r: 0.35376573943932443
06/02/2019 10:03:50 *** evaluating ***
06/02/2019 10:03:50 step: 95, epoch: 94, acc: 56.837606837606835, f1: 19.988679902473006, r: 0.38133201791845817
06/02/2019 10:03:50 *** epoch: 96 ***
06/02/2019 10:03:50 *** training ***
06/02/2019 10:03:51 step: 3140, epoch: 95, batch: 4, loss: 0.9985818266868591, acc: 60.9375, f1: 32.12525879917184, r: 0.44196748761346555
06/02/2019 10:03:52 step: 3145, epoch: 95, batch: 9, loss: 1.1145832538604736, acc: 53.125, f1: 21.548107065348447, r: 0.4399103859510134
06/02/2019 10:03:53 step: 3150, epoch: 95, batch: 14, loss: 0.9501459002494812, acc: 62.5, f1: 30.303718275891978, r: 0.335153701914664
06/02/2019 10:03:55 step: 3155, epoch: 95, batch: 19, loss: 0.994520902633667, acc: 67.1875, f1: 39.513977806231324, r: 0.5007938961543773
06/02/2019 10:03:56 step: 3160, epoch: 95, batch: 24, loss: 1.008400321006775, acc: 53.125, f1: 23.541666666666668, r: 0.5100331602005512
06/02/2019 10:03:57 step: 3165, epoch: 95, batch: 29, loss: 1.0875576734542847, acc: 54.6875, f1: 23.093434343434343, r: 0.40146389931405657
06/02/2019 10:03:58 *** evaluating ***
06/02/2019 10:03:58 step: 96, epoch: 95, acc: 57.692307692307686, f1: 21.637166976150027, r: 0.387048591239822
06/02/2019 10:03:58 *** epoch: 97 ***
06/02/2019 10:03:58 *** training ***
06/02/2019 10:03:59 step: 3173, epoch: 96, batch: 4, loss: 1.1269240379333496, acc: 57.8125, f1: 24.007936507936506, r: 0.4208174159995134
06/02/2019 10:04:00 step: 3178, epoch: 96, batch: 9, loss: 1.0443766117095947, acc: 54.6875, f1: 25.633971291866033, r: 0.43304106504685935
06/02/2019 10:04:02 step: 3183, epoch: 96, batch: 14, loss: 1.0074156522750854, acc: 59.375, f1: 34.11160268303126, r: 0.4537820068179393
06/02/2019 10:04:03 step: 3188, epoch: 96, batch: 19, loss: 1.1188195943832397, acc: 48.4375, f1: 23.645487264673314, r: 0.4553060660637833
06/02/2019 10:04:04 step: 3193, epoch: 96, batch: 24, loss: 1.1865819692611694, acc: 48.4375, f1: 22.09757834757835, r: 0.3897863885166036
06/02/2019 10:04:05 step: 3198, epoch: 96, batch: 29, loss: 1.0452263355255127, acc: 56.25, f1: 25.620013061873525, r: 0.4141366000247181
06/02/2019 10:04:05 *** evaluating ***
06/02/2019 10:04:06 step: 97, epoch: 96, acc: 57.692307692307686, f1: 22.688307053109945, r: 0.39300563517466636
06/02/2019 10:04:06 *** epoch: 98 ***
06/02/2019 10:04:06 *** training ***
06/02/2019 10:04:07 step: 3206, epoch: 97, batch: 4, loss: 1.0773370265960693, acc: 57.8125, f1: 26.911027568922307, r: 0.47255835318871825
06/02/2019 10:04:08 step: 3211, epoch: 97, batch: 9, loss: 1.0189933776855469, acc: 54.6875, f1: 31.145833333333332, r: 0.4982102644429436
06/02/2019 10:04:09 step: 3216, epoch: 97, batch: 14, loss: 1.0924320220947266, acc: 53.125, f1: 30.041666666666668, r: 0.4820353672250052
06/02/2019 10:04:11 step: 3221, epoch: 97, batch: 19, loss: 1.0965632200241089, acc: 56.25, f1: 24.71230158730159, r: 0.47862697869451953
06/02/2019 10:04:12 step: 3226, epoch: 97, batch: 24, loss: 1.1158499717712402, acc: 56.25, f1: 22.33282889020594, r: 0.4089057731770398
06/02/2019 10:04:13 step: 3231, epoch: 97, batch: 29, loss: 0.8961398005485535, acc: 70.3125, f1: 39.58256029684601, r: 0.40951324318272064
06/02/2019 10:04:13 *** evaluating ***
06/02/2019 10:04:14 step: 98, epoch: 97, acc: 58.119658119658126, f1: 20.979001656200186, r: 0.3886928926694933
06/02/2019 10:04:14 *** epoch: 99 ***
06/02/2019 10:04:14 *** training ***
06/02/2019 10:04:15 step: 3239, epoch: 98, batch: 4, loss: 1.0660285949707031, acc: 51.5625, f1: 26.765615337043908, r: 0.43698481994070837
06/02/2019 10:04:16 step: 3244, epoch: 98, batch: 9, loss: 1.1124730110168457, acc: 50.0, f1: 28.233311915470832, r: 0.471807878768393
06/02/2019 10:04:17 step: 3249, epoch: 98, batch: 14, loss: 0.772951602935791, acc: 78.125, f1: 38.38968423190172, r: 0.42248101605620797
06/02/2019 10:04:18 step: 3254, epoch: 98, batch: 19, loss: 1.0496326684951782, acc: 57.8125, f1: 31.864674441205054, r: 0.42877785529638546
06/02/2019 10:04:19 step: 3259, epoch: 98, batch: 24, loss: 1.1490241289138794, acc: 53.125, f1: 31.37457881703406, r: 0.3994995591758956
06/02/2019 10:04:21 step: 3264, epoch: 98, batch: 29, loss: 1.0368139743804932, acc: 57.8125, f1: 31.00956284153005, r: 0.44150588103156635
06/02/2019 10:04:21 *** evaluating ***
06/02/2019 10:04:22 step: 99, epoch: 98, acc: 57.692307692307686, f1: 22.82318722943723, r: 0.3758275624751516
06/02/2019 10:04:22 *** epoch: 100 ***
06/02/2019 10:04:22 *** training ***
06/02/2019 10:04:23 step: 3272, epoch: 99, batch: 4, loss: 0.882858395576477, acc: 60.9375, f1: 32.15074855699856, r: 0.43123591813188716
06/02/2019 10:04:24 step: 3277, epoch: 99, batch: 9, loss: 0.8077336549758911, acc: 62.5, f1: 35.86231120477696, r: 0.5158035097966923
06/02/2019 10:04:25 step: 3282, epoch: 99, batch: 14, loss: 1.004927396774292, acc: 56.25, f1: 27.991071428571423, r: 0.5072768108396883
06/02/2019 10:04:26 step: 3287, epoch: 99, batch: 19, loss: 0.9457658529281616, acc: 54.6875, f1: 24.931479838312136, r: 0.4688439555277066
06/02/2019 10:04:27 step: 3292, epoch: 99, batch: 24, loss: 1.187656044960022, acc: 50.0, f1: 24.68005952380953, r: 0.41458497176943165
06/02/2019 10:04:28 step: 3297, epoch: 99, batch: 29, loss: 0.8298525214195251, acc: 67.1875, f1: 38.46153846153847, r: 0.5292026053359273
06/02/2019 10:04:29 *** evaluating ***
06/02/2019 10:04:29 step: 100, epoch: 99, acc: 56.41025641025641, f1: 19.847134960689395, r: 0.3657728417727819
06/02/2019 10:04:29 *** epoch: 101 ***
06/02/2019 10:04:29 *** training ***
06/02/2019 10:04:31 step: 3305, epoch: 100, batch: 4, loss: 0.9206726551055908, acc: 65.625, f1: 32.63888888888889, r: 0.47105049155015805
06/02/2019 10:04:32 step: 3310, epoch: 100, batch: 9, loss: 0.8127369284629822, acc: 60.9375, f1: 32.91244239631336, r: 0.4698134390416858
06/02/2019 10:04:33 step: 3315, epoch: 100, batch: 14, loss: 1.023269534111023, acc: 57.8125, f1: 35.34793429566949, r: 0.4337368830108659
06/02/2019 10:04:34 step: 3320, epoch: 100, batch: 19, loss: 0.842373251914978, acc: 62.5, f1: 27.168161950770646, r: 0.452831489993074
06/02/2019 10:04:36 step: 3325, epoch: 100, batch: 24, loss: 0.9077520966529846, acc: 64.0625, f1: 42.63231027308005, r: 0.3928001960300174
06/02/2019 10:04:37 step: 3330, epoch: 100, batch: 29, loss: 1.1851789951324463, acc: 53.125, f1: 27.661095963776226, r: 0.42665253529923536
06/02/2019 10:04:37 *** evaluating ***
06/02/2019 10:04:38 step: 101, epoch: 100, acc: 58.97435897435898, f1: 23.83557749123787, r: 0.3952293006883014
06/02/2019 10:04:38 *** epoch: 102 ***
06/02/2019 10:04:38 *** training ***
06/02/2019 10:04:39 step: 3338, epoch: 101, batch: 4, loss: 0.8595446944236755, acc: 64.0625, f1: 34.17949635994749, r: 0.42994970900282364
06/02/2019 10:04:40 step: 3343, epoch: 101, batch: 9, loss: 1.098676323890686, acc: 56.25, f1: 20.892394921730556, r: 0.3981128221615803
06/02/2019 10:04:41 step: 3348, epoch: 101, batch: 14, loss: 0.9017099738121033, acc: 65.625, f1: 27.26061252180655, r: 0.3864321733390495
06/02/2019 10:04:42 step: 3353, epoch: 101, batch: 19, loss: 0.8692640662193298, acc: 64.0625, f1: 27.27161727161727, r: 0.295776188724355
06/02/2019 10:04:44 step: 3358, epoch: 101, batch: 24, loss: 1.0911586284637451, acc: 59.375, f1: 33.19257262653489, r: 0.4255477305378868
06/02/2019 10:04:45 step: 3363, epoch: 101, batch: 29, loss: 0.9685604572296143, acc: 62.5, f1: 26.947697111631534, r: 0.3271438719381467
06/02/2019 10:04:45 *** evaluating ***
06/02/2019 10:04:46 step: 102, epoch: 101, acc: 60.256410256410255, f1: 25.3916076393486, r: 0.3987937539620879
06/02/2019 10:04:46 *** epoch: 103 ***
06/02/2019 10:04:46 *** training ***
06/02/2019 10:04:47 step: 3371, epoch: 102, batch: 4, loss: 0.9722316265106201, acc: 57.8125, f1: 31.357291357291356, r: 0.37261805150617394
06/02/2019 10:04:48 step: 3376, epoch: 102, batch: 9, loss: 1.0305346250534058, acc: 56.25, f1: 33.678650705565246, r: 0.4989855342085002
06/02/2019 10:04:49 step: 3381, epoch: 102, batch: 14, loss: 1.1425191164016724, acc: 54.6875, f1: 28.51043119950683, r: 0.3729552716161037
06/02/2019 10:04:51 step: 3386, epoch: 102, batch: 19, loss: 1.1030921936035156, acc: 50.0, f1: 24.93161094224924, r: 0.490603319791446
06/02/2019 10:04:52 step: 3391, epoch: 102, batch: 24, loss: 0.912760317325592, acc: 59.375, f1: 29.336846728151073, r: 0.4233183963698139
06/02/2019 10:04:53 step: 3396, epoch: 102, batch: 29, loss: 0.9381582736968994, acc: 64.0625, f1: 44.727155434122636, r: 0.5187814598366061
06/02/2019 10:04:53 *** evaluating ***
06/02/2019 10:04:53 step: 103, epoch: 102, acc: 57.692307692307686, f1: 23.069389771762285, r: 0.38591058489738983
06/02/2019 10:04:53 *** epoch: 104 ***
06/02/2019 10:04:53 *** training ***
06/02/2019 10:04:55 step: 3404, epoch: 103, batch: 4, loss: 0.9178441762924194, acc: 60.9375, f1: 32.057823129251695, r: 0.446189351824566
06/02/2019 10:04:56 step: 3409, epoch: 103, batch: 9, loss: 0.8701202273368835, acc: 65.625, f1: 32.37571951857666, r: 0.4771976359495588
06/02/2019 10:04:57 step: 3414, epoch: 103, batch: 14, loss: 1.1504350900650024, acc: 46.875, f1: 35.795219273480136, r: 0.4004870107949478
06/02/2019 10:04:58 step: 3419, epoch: 103, batch: 19, loss: 1.2756413221359253, acc: 43.75, f1: 20.319028797289665, r: 0.41116414533741263
06/02/2019 10:04:59 step: 3424, epoch: 103, batch: 24, loss: 1.267663836479187, acc: 54.6875, f1: 25.58608058608059, r: 0.39164678160283906
06/02/2019 10:05:01 step: 3429, epoch: 103, batch: 29, loss: 1.0211609601974487, acc: 54.6875, f1: 30.09688723974438, r: 0.4779813336204634
06/02/2019 10:05:01 *** evaluating ***
06/02/2019 10:05:01 step: 104, epoch: 103, acc: 59.82905982905983, f1: 26.445061364430465, r: 0.4044888165238602
06/02/2019 10:05:01 *** epoch: 105 ***
06/02/2019 10:05:01 *** training ***
06/02/2019 10:05:03 step: 3437, epoch: 104, batch: 4, loss: 0.8548678159713745, acc: 65.625, f1: 39.78354978354978, r: 0.4735595629591194
06/02/2019 10:05:04 step: 3442, epoch: 104, batch: 9, loss: 0.9433531761169434, acc: 59.375, f1: 27.12920293565455, r: 0.50699794385235
06/02/2019 10:05:05 step: 3447, epoch: 104, batch: 14, loss: 1.0917150974273682, acc: 46.875, f1: 24.228110599078345, r: 0.4342434679928595
06/02/2019 10:05:06 step: 3452, epoch: 104, batch: 19, loss: 0.893704354763031, acc: 57.8125, f1: 25.17811377208795, r: 0.4981577796900409
06/02/2019 10:05:08 step: 3457, epoch: 104, batch: 24, loss: 0.9038181900978088, acc: 62.5, f1: 24.995114324799687, r: 0.3765493629416663
06/02/2019 10:05:09 step: 3462, epoch: 104, batch: 29, loss: 1.027575969696045, acc: 57.8125, f1: 27.300023457658924, r: 0.4034410532347475
06/02/2019 10:05:09 *** evaluating ***
06/02/2019 10:05:10 step: 105, epoch: 104, acc: 59.82905982905983, f1: 24.547619047619047, r: 0.4085776447448426
06/02/2019 10:05:10 *** epoch: 106 ***
06/02/2019 10:05:10 *** training ***
06/02/2019 10:05:11 step: 3470, epoch: 105, batch: 4, loss: 1.2439346313476562, acc: 45.3125, f1: 17.958333333333336, r: 0.44285584148033846
06/02/2019 10:05:12 step: 3475, epoch: 105, batch: 9, loss: 0.9259564280509949, acc: 62.5, f1: 35.84351503759399, r: 0.5213500135205388
06/02/2019 10:05:13 step: 3480, epoch: 105, batch: 14, loss: 1.2940926551818848, acc: 51.5625, f1: 29.750797448165866, r: 0.39773004868314626
06/02/2019 10:05:14 step: 3485, epoch: 105, batch: 19, loss: 1.1190516948699951, acc: 56.25, f1: 29.01669758812616, r: 0.4081365444491263
06/02/2019 10:05:15 step: 3490, epoch: 105, batch: 24, loss: 0.9987851977348328, acc: 56.25, f1: 27.26382488479263, r: 0.43895688123785037
06/02/2019 10:05:17 step: 3495, epoch: 105, batch: 29, loss: 1.0277472734451294, acc: 57.8125, f1: 29.82967133213439, r: 0.45900767881525434
06/02/2019 10:05:17 *** evaluating ***
06/02/2019 10:05:18 step: 106, epoch: 105, acc: 59.82905982905983, f1: 28.490652580011822, r: 0.4017433446530573
06/02/2019 10:05:18 *** epoch: 107 ***
06/02/2019 10:05:18 *** training ***
06/02/2019 10:05:19 step: 3503, epoch: 106, batch: 4, loss: 0.8541479110717773, acc: 62.5, f1: 31.821219715956563, r: 0.43340652561883186
06/02/2019 10:05:20 step: 3508, epoch: 106, batch: 9, loss: 1.0089712142944336, acc: 53.125, f1: 28.89906327194463, r: 0.4485740320977858
06/02/2019 10:05:21 step: 3513, epoch: 106, batch: 14, loss: 0.9332867860794067, acc: 57.8125, f1: 24.468624406284253, r: 0.40059004865298375
06/02/2019 10:05:23 step: 3518, epoch: 106, batch: 19, loss: 1.0454840660095215, acc: 60.9375, f1: 27.335792310966095, r: 0.37085691141737465
06/02/2019 10:05:24 step: 3523, epoch: 106, batch: 24, loss: 0.8910033702850342, acc: 57.8125, f1: 35.88950731807875, r: 0.38702348163973915
06/02/2019 10:05:25 step: 3528, epoch: 106, batch: 29, loss: 0.8839884400367737, acc: 64.0625, f1: 45.976004671656845, r: 0.5331717504427408
06/02/2019 10:05:25 *** evaluating ***
06/02/2019 10:05:26 step: 107, epoch: 106, acc: 58.54700854700855, f1: 23.03818518435942, r: 0.4103028062094445
06/02/2019 10:05:26 *** epoch: 108 ***
06/02/2019 10:05:26 *** training ***
06/02/2019 10:05:27 step: 3536, epoch: 107, batch: 4, loss: 0.98648601770401, acc: 54.6875, f1: 29.186746670322368, r: 0.4477383884995334
06/02/2019 10:05:28 step: 3541, epoch: 107, batch: 9, loss: 1.0948714017868042, acc: 54.6875, f1: 30.52297410192147, r: 0.3919766949418998
06/02/2019 10:05:29 step: 3546, epoch: 107, batch: 14, loss: 1.0227012634277344, acc: 59.375, f1: 36.71798543978995, r: 0.4283387236459785
06/02/2019 10:05:30 step: 3551, epoch: 107, batch: 19, loss: 0.8980472683906555, acc: 62.5, f1: 32.6656978580906, r: 0.3888828019481517
06/02/2019 10:05:32 step: 3556, epoch: 107, batch: 24, loss: 1.5401270389556885, acc: 46.875, f1: 28.13366838086333, r: 0.38405968722306305
06/02/2019 10:05:33 step: 3561, epoch: 107, batch: 29, loss: 0.9899936318397522, acc: 59.375, f1: 40.532346689982155, r: 0.4345287133288611
06/02/2019 10:05:33 *** evaluating ***
06/02/2019 10:05:34 step: 108, epoch: 107, acc: 59.401709401709404, f1: 25.409343622392534, r: 0.3908951209679357
06/02/2019 10:05:34 *** epoch: 109 ***
06/02/2019 10:05:34 *** training ***
06/02/2019 10:05:35 step: 3569, epoch: 108, batch: 4, loss: 0.6912350058555603, acc: 67.1875, f1: 45.3533321138955, r: 0.47863403153087597
06/02/2019 10:05:36 step: 3574, epoch: 108, batch: 9, loss: 1.0169645547866821, acc: 57.8125, f1: 29.49731182795699, r: 0.4736388836619802
06/02/2019 10:05:37 step: 3579, epoch: 108, batch: 14, loss: 1.0364049673080444, acc: 51.5625, f1: 22.40401259467546, r: 0.439541996318208
06/02/2019 10:05:38 step: 3584, epoch: 108, batch: 19, loss: 0.8792738914489746, acc: 68.75, f1: 47.91461412151068, r: 0.4548723910403532
06/02/2019 10:05:40 step: 3589, epoch: 108, batch: 24, loss: 0.8483082056045532, acc: 62.5, f1: 42.590424147801194, r: 0.4425897686378798
06/02/2019 10:05:41 step: 3594, epoch: 108, batch: 29, loss: 1.0697585344314575, acc: 48.4375, f1: 20.091957731709282, r: 0.3109679292621355
06/02/2019 10:05:41 *** evaluating ***
06/02/2019 10:05:42 step: 109, epoch: 108, acc: 61.111111111111114, f1: 28.74253524679612, r: 0.4061202024905911
06/02/2019 10:05:42 *** epoch: 110 ***
06/02/2019 10:05:42 *** training ***
06/02/2019 10:05:43 step: 3602, epoch: 109, batch: 4, loss: 1.008029818534851, acc: 56.25, f1: 25.32882718304713, r: 0.30873077134251
06/02/2019 10:05:44 step: 3607, epoch: 109, batch: 9, loss: 0.9494566321372986, acc: 62.5, f1: 27.787223899570957, r: 0.3780959452344065
06/02/2019 10:05:45 step: 3612, epoch: 109, batch: 14, loss: 1.3063063621520996, acc: 54.6875, f1: 42.744523646779285, r: 0.39732394708035823
06/02/2019 10:05:46 step: 3617, epoch: 109, batch: 19, loss: 1.1523857116699219, acc: 53.125, f1: 24.410670746877646, r: 0.4915644495931605
06/02/2019 10:05:48 step: 3622, epoch: 109, batch: 24, loss: 1.0680315494537354, acc: 48.4375, f1: 24.861329147043435, r: 0.39383063319245115
06/02/2019 10:05:49 step: 3627, epoch: 109, batch: 29, loss: 0.9167487621307373, acc: 65.625, f1: 39.36422082875172, r: 0.3922937506185957
06/02/2019 10:05:49 *** evaluating ***
06/02/2019 10:05:50 step: 110, epoch: 109, acc: 60.256410256410255, f1: 28.41122512721008, r: 0.4083743855385859
06/02/2019 10:05:50 *** epoch: 111 ***
06/02/2019 10:05:50 *** training ***
06/02/2019 10:05:51 step: 3635, epoch: 110, batch: 4, loss: 0.9117698669433594, acc: 65.625, f1: 34.19080919080919, r: 0.4616282715939621
06/02/2019 10:05:52 step: 3640, epoch: 110, batch: 9, loss: 1.1197872161865234, acc: 59.375, f1: 30.355718085106385, r: 0.5129407479739921
06/02/2019 10:05:53 step: 3645, epoch: 110, batch: 14, loss: 0.7846050262451172, acc: 71.875, f1: 62.923318147585704, r: 0.5879586985756466
06/02/2019 10:05:55 step: 3650, epoch: 110, batch: 19, loss: 0.8263699412345886, acc: 57.8125, f1: 26.396708881180924, r: 0.4623393662408943
06/02/2019 10:05:56 step: 3655, epoch: 110, batch: 24, loss: 0.9866213202476501, acc: 59.375, f1: 37.429237429237425, r: 0.4834267793440159
06/02/2019 10:05:57 step: 3660, epoch: 110, batch: 29, loss: 0.9490668773651123, acc: 59.375, f1: 27.977319960242163, r: 0.47071995137534983
06/02/2019 10:05:57 *** evaluating ***
06/02/2019 10:05:58 step: 111, epoch: 110, acc: 59.82905982905983, f1: 25.41791425017609, r: 0.4013401004829901
06/02/2019 10:05:58 *** epoch: 112 ***
06/02/2019 10:05:58 *** training ***
06/02/2019 10:05:59 step: 3668, epoch: 111, batch: 4, loss: 0.9995524883270264, acc: 59.375, f1: 28.099206349206344, r: 0.5081260142239246
06/02/2019 10:06:00 step: 3673, epoch: 111, batch: 9, loss: 0.8585155010223389, acc: 68.75, f1: 41.72712563286184, r: 0.4768633748569696
06/02/2019 10:06:01 step: 3678, epoch: 111, batch: 14, loss: 0.8208476901054382, acc: 62.5, f1: 29.62864250177683, r: 0.4939763601916397
06/02/2019 10:06:02 step: 3683, epoch: 111, batch: 19, loss: 0.9546164870262146, acc: 62.5, f1: 34.276118385160935, r: 0.538538697780678
06/02/2019 10:06:03 step: 3688, epoch: 111, batch: 24, loss: 0.8333296775817871, acc: 67.1875, f1: 51.64426217057796, r: 0.4256737422076027
06/02/2019 10:06:05 step: 3693, epoch: 111, batch: 29, loss: 1.010926365852356, acc: 51.5625, f1: 26.461943267873185, r: 0.43547278663908384
06/02/2019 10:06:05 *** evaluating ***
06/02/2019 10:06:06 step: 112, epoch: 111, acc: 59.401709401709404, f1: 26.642948066405303, r: 0.4058596689984899
06/02/2019 10:06:06 *** epoch: 113 ***
06/02/2019 10:06:06 *** training ***
06/02/2019 10:06:07 step: 3701, epoch: 112, batch: 4, loss: 0.9102046489715576, acc: 60.9375, f1: 27.4969474969475, r: 0.37773548518289946
06/02/2019 10:06:08 step: 3706, epoch: 112, batch: 9, loss: 0.916037917137146, acc: 62.5, f1: 40.24612835564847, r: 0.5528180305473277
06/02/2019 10:06:09 step: 3711, epoch: 112, batch: 14, loss: 0.9486398696899414, acc: 60.9375, f1: 39.99206048467133, r: 0.4978869981958724
06/02/2019 10:06:10 step: 3716, epoch: 112, batch: 19, loss: 1.0212410688400269, acc: 51.5625, f1: 22.85353535353535, r: 0.3949134561513869
06/02/2019 10:06:12 step: 3721, epoch: 112, batch: 24, loss: 1.1084939241409302, acc: 60.9375, f1: 37.97810618500274, r: 0.42768608064714275
06/02/2019 10:06:13 step: 3726, epoch: 112, batch: 29, loss: 0.9882301092147827, acc: 67.1875, f1: 34.749051612259166, r: 0.5065123307990447
06/02/2019 10:06:13 *** evaluating ***
06/02/2019 10:06:14 step: 113, epoch: 112, acc: 59.401709401709404, f1: 26.00744102099205, r: 0.40191200611963135
06/02/2019 10:06:14 *** epoch: 114 ***
06/02/2019 10:06:14 *** training ***
06/02/2019 10:06:15 step: 3734, epoch: 113, batch: 4, loss: 0.847563624382019, acc: 65.625, f1: 44.32350718065004, r: 0.4806685619591778
06/02/2019 10:06:16 step: 3739, epoch: 113, batch: 9, loss: 0.8490681648254395, acc: 60.9375, f1: 32.005019351717465, r: 0.5636978492545858
06/02/2019 10:06:17 step: 3744, epoch: 113, batch: 14, loss: 1.0240827798843384, acc: 60.9375, f1: 25.64311594202898, r: 0.5154919074439284
06/02/2019 10:06:19 step: 3749, epoch: 113, batch: 19, loss: 0.7769361138343811, acc: 70.3125, f1: 41.35745207173779, r: 0.5145523661786752
06/02/2019 10:06:20 step: 3754, epoch: 113, batch: 24, loss: 1.0679442882537842, acc: 53.125, f1: 26.16128389154705, r: 0.4586522088403972
06/02/2019 10:06:21 step: 3759, epoch: 113, batch: 29, loss: 1.0580987930297852, acc: 53.125, f1: 20.677451971688573, r: 0.3370438106078267
06/02/2019 10:06:21 *** evaluating ***
06/02/2019 10:06:22 step: 114, epoch: 113, acc: 61.53846153846154, f1: 28.66052260412062, r: 0.40129788993083026
06/02/2019 10:06:22 *** epoch: 115 ***
06/02/2019 10:06:22 *** training ***
06/02/2019 10:06:23 step: 3767, epoch: 114, batch: 4, loss: 0.8527316451072693, acc: 67.1875, f1: 36.38785155178598, r: 0.42359456914900984
06/02/2019 10:06:24 step: 3772, epoch: 114, batch: 9, loss: 0.8894338011741638, acc: 64.0625, f1: 33.3016533016533, r: 0.49781375279840173
06/02/2019 10:06:25 step: 3777, epoch: 114, batch: 14, loss: 0.8211572170257568, acc: 64.0625, f1: 36.778096818664764, r: 0.4674695545695472
06/02/2019 10:06:26 step: 3782, epoch: 114, batch: 19, loss: 0.9538503289222717, acc: 54.6875, f1: 32.83781918564527, r: 0.5396694814046994
06/02/2019 10:06:27 step: 3787, epoch: 114, batch: 24, loss: 0.6976625323295593, acc: 68.75, f1: 43.13885554221688, r: 0.5127843364073742
06/02/2019 10:06:29 step: 3792, epoch: 114, batch: 29, loss: 0.8839701414108276, acc: 65.625, f1: 42.70570699142127, r: 0.47927222281378756
06/02/2019 10:06:29 *** evaluating ***
06/02/2019 10:06:30 step: 115, epoch: 114, acc: 59.401709401709404, f1: 24.720365157557275, r: 0.3876173994591198
06/02/2019 10:06:30 *** epoch: 116 ***
06/02/2019 10:06:30 *** training ***
06/02/2019 10:06:31 step: 3800, epoch: 115, batch: 4, loss: 0.8566005825996399, acc: 68.75, f1: 49.34510481308511, r: 0.4358161343515826
06/02/2019 10:06:32 step: 3805, epoch: 115, batch: 9, loss: 0.940971851348877, acc: 60.9375, f1: 34.00390459213988, r: 0.48290490350721027
06/02/2019 10:06:33 step: 3810, epoch: 115, batch: 14, loss: 0.7211824655532837, acc: 65.625, f1: 51.558845812498646, r: 0.44598109248569084
06/02/2019 10:06:34 step: 3815, epoch: 115, batch: 19, loss: 0.9214943051338196, acc: 64.0625, f1: 36.065791096312736, r: 0.4964871647969877
06/02/2019 10:06:36 step: 3820, epoch: 115, batch: 24, loss: 0.8558838963508606, acc: 59.375, f1: 36.10698510394559, r: 0.5017405718007452
06/02/2019 10:06:37 step: 3825, epoch: 115, batch: 29, loss: 1.1916050910949707, acc: 56.25, f1: 28.63247863247863, r: 0.39471146746965485
06/02/2019 10:06:37 *** evaluating ***
06/02/2019 10:06:38 step: 116, epoch: 115, acc: 56.41025641025641, f1: 28.661162387577484, r: 0.3990674565667485
06/02/2019 10:06:38 *** epoch: 117 ***
06/02/2019 10:06:38 *** training ***
06/02/2019 10:06:39 step: 3833, epoch: 116, batch: 4, loss: 1.059309482574463, acc: 56.25, f1: 36.817509612540675, r: 0.40354852423830734
06/02/2019 10:06:40 step: 3838, epoch: 116, batch: 9, loss: 0.7227954864501953, acc: 75.0, f1: 46.424134871339845, r: 0.4169476322231326
06/02/2019 10:06:41 step: 3843, epoch: 116, batch: 14, loss: 0.7721439003944397, acc: 67.1875, f1: 32.196564470537076, r: 0.4111983136284751
06/02/2019 10:06:42 step: 3848, epoch: 116, batch: 19, loss: 0.8919528722763062, acc: 64.0625, f1: 27.417004048582992, r: 0.5072153386723456
06/02/2019 10:06:43 step: 3853, epoch: 116, batch: 24, loss: 0.9785007834434509, acc: 54.6875, f1: 23.02721088435374, r: 0.5124129463188802
06/02/2019 10:06:44 step: 3858, epoch: 116, batch: 29, loss: 1.1161890029907227, acc: 54.6875, f1: 34.282676387939546, r: 0.5478937119202052
06/02/2019 10:06:45 *** evaluating ***
06/02/2019 10:06:46 step: 117, epoch: 116, acc: 59.401709401709404, f1: 27.24014336917563, r: 0.3976528632463671
06/02/2019 10:06:46 *** epoch: 118 ***
06/02/2019 10:06:46 *** training ***
06/02/2019 10:06:47 step: 3866, epoch: 117, batch: 4, loss: 0.8747690320014954, acc: 59.375, f1: 29.78014960773582, r: 0.4561219200868657
06/02/2019 10:06:48 step: 3871, epoch: 117, batch: 9, loss: 0.7395337820053101, acc: 67.1875, f1: 45.87597357783694, r: 0.5104328429472343
06/02/2019 10:06:49 step: 3876, epoch: 117, batch: 14, loss: 0.9682642817497253, acc: 56.25, f1: 36.6724537037037, r: 0.4768335843092582
06/02/2019 10:06:50 step: 3881, epoch: 117, batch: 19, loss: 0.7735714316368103, acc: 70.3125, f1: 41.21947018498742, r: 0.5238486314553327
06/02/2019 10:06:51 step: 3886, epoch: 117, batch: 24, loss: 0.6894136071205139, acc: 71.875, f1: 60.5689699936946, r: 0.5187056812373051
06/02/2019 10:06:53 step: 3891, epoch: 117, batch: 29, loss: 0.87972491979599, acc: 64.0625, f1: 44.460976508830136, r: 0.46100547906798844
06/02/2019 10:06:53 *** evaluating ***
06/02/2019 10:06:53 step: 118, epoch: 117, acc: 59.401709401709404, f1: 27.366338112305854, r: 0.40694780651953905
06/02/2019 10:06:53 *** epoch: 119 ***
06/02/2019 10:06:53 *** training ***
06/02/2019 10:06:55 step: 3899, epoch: 118, batch: 4, loss: 0.8737613558769226, acc: 64.0625, f1: 36.83639705882353, r: 0.5043370759831779
06/02/2019 10:06:56 step: 3904, epoch: 118, batch: 9, loss: 0.9582481980323792, acc: 60.9375, f1: 35.39455782312926, r: 0.4953133247420082
06/02/2019 10:06:57 step: 3909, epoch: 118, batch: 14, loss: 0.9144896864891052, acc: 71.875, f1: 43.132836990595614, r: 0.3940217475568793
06/02/2019 10:06:58 step: 3914, epoch: 118, batch: 19, loss: 0.9137922525405884, acc: 62.5, f1: 49.96671132221004, r: 0.5211835424223109
06/02/2019 10:06:59 step: 3919, epoch: 118, batch: 24, loss: 0.691896915435791, acc: 73.4375, f1: 39.33209647495362, r: 0.5330273696361335
06/02/2019 10:07:01 step: 3924, epoch: 118, batch: 29, loss: 0.9018305540084839, acc: 62.5, f1: 47.60302775441548, r: 0.48386017454465613
06/02/2019 10:07:01 *** evaluating ***
06/02/2019 10:07:02 step: 119, epoch: 118, acc: 58.54700854700855, f1: 22.621425488224734, r: 0.39579083996627373
06/02/2019 10:07:02 *** epoch: 120 ***
06/02/2019 10:07:02 *** training ***
06/02/2019 10:07:03 step: 3932, epoch: 119, batch: 4, loss: 0.805018424987793, acc: 65.625, f1: 40.879508553993674, r: 0.543230680588657
06/02/2019 10:07:04 step: 3937, epoch: 119, batch: 9, loss: 1.080939769744873, acc: 54.6875, f1: 28.782975192173303, r: 0.5225400016020469
06/02/2019 10:07:05 step: 3942, epoch: 119, batch: 14, loss: 0.8297075033187866, acc: 65.625, f1: 31.115763546798032, r: 0.5042199029931154
06/02/2019 10:07:06 step: 3947, epoch: 119, batch: 19, loss: 0.8745359778404236, acc: 64.0625, f1: 41.77927335822073, r: 0.43225608018812905
06/02/2019 10:07:07 step: 3952, epoch: 119, batch: 24, loss: 0.811896562576294, acc: 67.1875, f1: 44.26406926406926, r: 0.4622532807577859
06/02/2019 10:07:08 step: 3957, epoch: 119, batch: 29, loss: 1.2490817308425903, acc: 50.0, f1: 26.37819069069069, r: 0.3709230116560711
06/02/2019 10:07:09 *** evaluating ***
06/02/2019 10:07:09 step: 120, epoch: 119, acc: 58.54700854700855, f1: 24.889314378098373, r: 0.3935414905966083
06/02/2019 10:07:09 *** epoch: 121 ***
06/02/2019 10:07:09 *** training ***
06/02/2019 10:07:10 step: 3965, epoch: 120, batch: 4, loss: 0.5985511541366577, acc: 78.125, f1: 54.37184069029911, r: 0.5416068616880048
06/02/2019 10:07:11 step: 3970, epoch: 120, batch: 9, loss: 0.7968851327896118, acc: 60.9375, f1: 32.41634491634491, r: 0.43553902540826744
06/02/2019 10:07:13 step: 3975, epoch: 120, batch: 14, loss: 0.8674030900001526, acc: 62.5, f1: 46.55676664605235, r: 0.45076999151959424
06/02/2019 10:07:14 step: 3980, epoch: 120, batch: 19, loss: 0.7478646636009216, acc: 68.75, f1: 40.21566710126032, r: 0.5177734341386707
06/02/2019 10:07:15 step: 3985, epoch: 120, batch: 24, loss: 0.9070432186126709, acc: 54.6875, f1: 39.6800821667681, r: 0.541091782628153
06/02/2019 10:07:16 step: 3990, epoch: 120, batch: 29, loss: 0.6778174042701721, acc: 73.4375, f1: 50.06901311249137, r: 0.5985154492597613
06/02/2019 10:07:17 *** evaluating ***
06/02/2019 10:07:17 step: 121, epoch: 120, acc: 60.256410256410255, f1: 28.367113981110283, r: 0.40512602084535215
06/02/2019 10:07:17 *** epoch: 122 ***
06/02/2019 10:07:17 *** training ***
06/02/2019 10:07:18 step: 3998, epoch: 121, batch: 4, loss: 0.6638187766075134, acc: 70.3125, f1: 50.555241859589685, r: 0.5181246399599756
06/02/2019 10:07:20 step: 4003, epoch: 121, batch: 9, loss: 0.81196528673172, acc: 70.3125, f1: 50.17707601040935, r: 0.5574600996215247
06/02/2019 10:07:21 step: 4008, epoch: 121, batch: 14, loss: 1.027048945426941, acc: 62.5, f1: 44.66600529100529, r: 0.5186892473172103
06/02/2019 10:07:22 step: 4013, epoch: 121, batch: 19, loss: 0.8376719951629639, acc: 68.75, f1: 45.20031055900621, r: 0.480735069682161
06/02/2019 10:07:23 step: 4018, epoch: 121, batch: 24, loss: 1.0594183206558228, acc: 56.25, f1: 33.10788113695091, r: 0.428083839295891
06/02/2019 10:07:24 step: 4023, epoch: 121, batch: 29, loss: 1.0102031230926514, acc: 53.125, f1: 32.06349206349206, r: 0.45193488763912515
06/02/2019 10:07:25 *** evaluating ***
06/02/2019 10:07:25 step: 122, epoch: 121, acc: 58.97435897435898, f1: 27.381835833599315, r: 0.3954975282492869
06/02/2019 10:07:25 *** epoch: 123 ***
06/02/2019 10:07:25 *** training ***
06/02/2019 10:07:26 step: 4031, epoch: 122, batch: 4, loss: 0.6174384951591492, acc: 75.0, f1: 58.91605891605892, r: 0.5788342593856696
06/02/2019 10:07:28 step: 4036, epoch: 122, batch: 9, loss: 0.7427988648414612, acc: 67.1875, f1: 45.02039627039627, r: 0.4715640936355463
06/02/2019 10:07:29 step: 4041, epoch: 122, batch: 14, loss: 0.8797353506088257, acc: 59.375, f1: 41.175810904071774, r: 0.4777273063701763
06/02/2019 10:07:30 step: 4046, epoch: 122, batch: 19, loss: 0.7680520415306091, acc: 67.1875, f1: 29.76608187134503, r: 0.49184185757375287
06/02/2019 10:07:31 step: 4051, epoch: 122, batch: 24, loss: 0.9719125628471375, acc: 57.8125, f1: 45.99285450614354, r: 0.5268771841733993
06/02/2019 10:07:32 step: 4056, epoch: 122, batch: 29, loss: 1.023743987083435, acc: 53.125, f1: 29.59183673469387, r: 0.47890901697882643
06/02/2019 10:07:33 *** evaluating ***
06/02/2019 10:07:33 step: 123, epoch: 122, acc: 57.26495726495726, f1: 24.68832610087856, r: 0.3973791867508232
06/02/2019 10:07:33 *** epoch: 124 ***
06/02/2019 10:07:33 *** training ***
06/02/2019 10:07:35 step: 4064, epoch: 123, batch: 4, loss: 0.9699662327766418, acc: 59.375, f1: 26.49961300309598, r: 0.40829572262881614
06/02/2019 10:07:36 step: 4069, epoch: 123, batch: 9, loss: 0.9196171164512634, acc: 64.0625, f1: 40.782828282828284, r: 0.47371604958799746
06/02/2019 10:07:37 step: 4074, epoch: 123, batch: 14, loss: 0.7321798801422119, acc: 64.0625, f1: 34.426406926406926, r: 0.6169230341496347
06/02/2019 10:07:38 step: 4079, epoch: 123, batch: 19, loss: 0.8052199482917786, acc: 65.625, f1: 40.62453290394467, r: 0.5372678381409416
06/02/2019 10:07:39 step: 4084, epoch: 123, batch: 24, loss: 0.9826099276542664, acc: 62.5, f1: 27.828239954725525, r: 0.36632961271063963
06/02/2019 10:07:40 step: 4089, epoch: 123, batch: 29, loss: 0.9084264636039734, acc: 67.1875, f1: 39.700049374588545, r: 0.4865655775831805
06/02/2019 10:07:41 *** evaluating ***
06/02/2019 10:07:41 step: 124, epoch: 123, acc: 56.837606837606835, f1: 22.503703450581565, r: 0.38028902890679217
06/02/2019 10:07:41 *** epoch: 125 ***
06/02/2019 10:07:41 *** training ***
06/02/2019 10:07:42 step: 4097, epoch: 124, batch: 4, loss: 0.808440089225769, acc: 62.5, f1: 26.19833700171191, r: 0.5382497575168976
06/02/2019 10:07:44 step: 4102, epoch: 124, batch: 9, loss: 0.8046466112136841, acc: 73.4375, f1: 46.31494998342824, r: 0.4242102285890095
06/02/2019 10:07:45 step: 4107, epoch: 124, batch: 14, loss: 0.8607703447341919, acc: 59.375, f1: 33.016650152513066, r: 0.464004390910044
06/02/2019 10:07:46 step: 4112, epoch: 124, batch: 19, loss: 0.8572399616241455, acc: 65.625, f1: 39.91396558623449, r: 0.4549640679314762
06/02/2019 10:07:47 step: 4117, epoch: 124, batch: 24, loss: 0.7583596110343933, acc: 62.5, f1: 43.20300880208715, r: 0.5206637711611437
06/02/2019 10:07:48 step: 4122, epoch: 124, batch: 29, loss: 0.912738025188446, acc: 64.0625, f1: 45.147201737837996, r: 0.43404130121583195
06/02/2019 10:07:49 *** evaluating ***
06/02/2019 10:07:49 step: 125, epoch: 124, acc: 59.401709401709404, f1: 27.304196933010495, r: 0.39215135556532665
06/02/2019 10:07:49 *** epoch: 126 ***
06/02/2019 10:07:49 *** training ***
06/02/2019 10:07:50 step: 4130, epoch: 125, batch: 4, loss: 0.7229984998703003, acc: 68.75, f1: 47.27633477633478, r: 0.557753848087891
06/02/2019 10:07:52 step: 4135, epoch: 125, batch: 9, loss: 0.9167027473449707, acc: 56.25, f1: 30.70673843700159, r: 0.36802753220057843
06/02/2019 10:07:53 step: 4140, epoch: 125, batch: 14, loss: 1.0071392059326172, acc: 56.25, f1: 35.069736227824464, r: 0.40740000279258537
06/02/2019 10:07:54 step: 4145, epoch: 125, batch: 19, loss: 1.1692836284637451, acc: 57.8125, f1: 31.878597592883306, r: 0.42631182679360347
06/02/2019 10:07:55 step: 4150, epoch: 125, batch: 24, loss: 0.9662840962409973, acc: 65.625, f1: 37.451620706337685, r: 0.5240318005435888
06/02/2019 10:07:56 step: 4155, epoch: 125, batch: 29, loss: 0.8060145378112793, acc: 70.3125, f1: 56.155352583924014, r: 0.464341789445992
06/02/2019 10:07:57 *** evaluating ***
06/02/2019 10:07:57 step: 126, epoch: 125, acc: 57.26495726495726, f1: 27.833584709413827, r: 0.3871701850534201
06/02/2019 10:07:57 *** epoch: 127 ***
06/02/2019 10:07:57 *** training ***
06/02/2019 10:07:59 step: 4163, epoch: 126, batch: 4, loss: 0.757863461971283, acc: 67.1875, f1: 43.66197353441075, r: 0.5513524922430769
06/02/2019 10:08:00 step: 4168, epoch: 126, batch: 9, loss: 0.8623298406600952, acc: 62.5, f1: 34.37244545054393, r: 0.4868603351926252
06/02/2019 10:08:01 step: 4173, epoch: 126, batch: 14, loss: 0.8459020853042603, acc: 71.875, f1: 36.143058918482645, r: 0.44695007311344714
06/02/2019 10:08:02 step: 4178, epoch: 126, batch: 19, loss: 0.7928239703178406, acc: 64.0625, f1: 43.93483709273183, r: 0.492136951132392
06/02/2019 10:08:03 step: 4183, epoch: 126, batch: 24, loss: 0.8015905618667603, acc: 70.3125, f1: 43.30775045060759, r: 0.4310076829143612
06/02/2019 10:08:04 step: 4188, epoch: 126, batch: 29, loss: 0.7446533441543579, acc: 71.875, f1: 48.31348454501163, r: 0.5142262848167976
06/02/2019 10:08:05 *** evaluating ***
06/02/2019 10:08:06 step: 127, epoch: 126, acc: 58.119658119658126, f1: 27.75432721784653, r: 0.3851417374827653
06/02/2019 10:08:06 *** epoch: 128 ***
06/02/2019 10:08:06 *** training ***
06/02/2019 10:08:07 step: 4196, epoch: 127, batch: 4, loss: 0.8575562238693237, acc: 60.9375, f1: 31.003224206349206, r: 0.50101305892008
06/02/2019 10:08:08 step: 4201, epoch: 127, batch: 9, loss: 0.7046568989753723, acc: 68.75, f1: 45.7689537353403, r: 0.5248085698558869
06/02/2019 10:08:09 step: 4206, epoch: 127, batch: 14, loss: 0.8362647891044617, acc: 65.625, f1: 27.939876215738284, r: 0.5059672236068987
06/02/2019 10:08:11 step: 4211, epoch: 127, batch: 19, loss: 0.7926115989685059, acc: 65.625, f1: 39.09797033567526, r: 0.5883005026394235
06/02/2019 10:08:12 step: 4216, epoch: 127, batch: 24, loss: 0.7099695801734924, acc: 73.4375, f1: 62.210884353741505, r: 0.5439474822999926
06/02/2019 10:08:13 step: 4221, epoch: 127, batch: 29, loss: 1.022484540939331, acc: 59.375, f1: 32.99915531622849, r: 0.5842877848175758
06/02/2019 10:08:13 *** evaluating ***
06/02/2019 10:08:14 step: 128, epoch: 127, acc: 57.26495726495726, f1: 26.90640830800405, r: 0.39686401150761874
06/02/2019 10:08:14 *** epoch: 129 ***
06/02/2019 10:08:14 *** training ***
06/02/2019 10:08:15 step: 4229, epoch: 128, batch: 4, loss: 0.6712111234664917, acc: 68.75, f1: 45.73660997991815, r: 0.6248778713596506
06/02/2019 10:08:16 step: 4234, epoch: 128, batch: 9, loss: 0.8280789852142334, acc: 60.9375, f1: 42.5973198644006, r: 0.41448472104665735
06/02/2019 10:08:17 step: 4239, epoch: 128, batch: 14, loss: 0.8394197821617126, acc: 70.3125, f1: 55.66035885871163, r: 0.4571670124541346
06/02/2019 10:08:18 step: 4244, epoch: 128, batch: 19, loss: 0.9439299702644348, acc: 73.4375, f1: 49.70158967541819, r: 0.5143184534942232
06/02/2019 10:08:19 step: 4249, epoch: 128, batch: 24, loss: 0.740059494972229, acc: 68.75, f1: 49.914175710021254, r: 0.4457960223889004
06/02/2019 10:08:21 step: 4254, epoch: 128, batch: 29, loss: 0.9251972436904907, acc: 65.625, f1: 38.53167401261775, r: 0.4200348683886218
06/02/2019 10:08:22 *** evaluating ***
06/02/2019 10:08:22 step: 129, epoch: 128, acc: 60.68376068376068, f1: 30.237447061018685, r: 0.3936916131577606
06/02/2019 10:08:22 *** epoch: 130 ***
06/02/2019 10:08:22 *** training ***
06/02/2019 10:08:23 step: 4262, epoch: 129, batch: 4, loss: 0.9298025369644165, acc: 65.625, f1: 48.91891640377081, r: 0.4833952731914783
06/02/2019 10:08:24 step: 4267, epoch: 129, batch: 9, loss: 0.7346217036247253, acc: 64.0625, f1: 38.85794233300322, r: 0.5712434662930773
06/02/2019 10:08:25 step: 4272, epoch: 129, batch: 14, loss: 0.7613276243209839, acc: 65.625, f1: 43.85998964803313, r: 0.5780502175008609
06/02/2019 10:08:27 step: 4277, epoch: 129, batch: 19, loss: 0.721905529499054, acc: 75.0, f1: 48.342911877394634, r: 0.48932728752886323
06/02/2019 10:08:28 step: 4282, epoch: 129, batch: 24, loss: 0.7430201172828674, acc: 65.625, f1: 32.774436090225564, r: 0.46660495042850086
06/02/2019 10:08:29 step: 4287, epoch: 129, batch: 29, loss: 0.8098927140235901, acc: 67.1875, f1: 32.376126126126124, r: 0.506416298438142
06/02/2019 10:08:29 *** evaluating ***
06/02/2019 10:08:30 step: 130, epoch: 129, acc: 57.692307692307686, f1: 27.931268251981646, r: 0.3956501507796569
06/02/2019 10:08:30 *** epoch: 131 ***
06/02/2019 10:08:30 *** training ***
06/02/2019 10:08:31 step: 4295, epoch: 130, batch: 4, loss: 0.8450844287872314, acc: 60.9375, f1: 31.386450204384985, r: 0.4956598320310525
06/02/2019 10:08:32 step: 4300, epoch: 130, batch: 9, loss: 0.9209814667701721, acc: 65.625, f1: 37.123655913978496, r: 0.4964675719719914
06/02/2019 10:08:33 step: 4305, epoch: 130, batch: 14, loss: 0.5923656225204468, acc: 76.5625, f1: 42.81483929604231, r: 0.48574567186010786
06/02/2019 10:08:34 step: 4310, epoch: 130, batch: 19, loss: 0.7854523658752441, acc: 70.3125, f1: 41.30096992289042, r: 0.5861217533678659
06/02/2019 10:08:36 step: 4315, epoch: 130, batch: 24, loss: 0.9087557792663574, acc: 68.75, f1: 44.99999999999999, r: 0.4589874591704938
06/02/2019 10:08:37 step: 4320, epoch: 130, batch: 29, loss: 0.8408315181732178, acc: 62.5, f1: 36.68154761904762, r: 0.5077829090981365
06/02/2019 10:08:37 *** evaluating ***
06/02/2019 10:08:38 step: 131, epoch: 130, acc: 57.692307692307686, f1: 27.532128138699342, r: 0.39679322931718114
06/02/2019 10:08:38 *** epoch: 132 ***
06/02/2019 10:08:38 *** training ***
06/02/2019 10:08:39 step: 4328, epoch: 131, batch: 4, loss: 0.8996097445487976, acc: 67.1875, f1: 38.807504607194055, r: 0.4945074017339359
06/02/2019 10:08:40 step: 4333, epoch: 131, batch: 9, loss: 0.7029920220375061, acc: 68.75, f1: 55.263547066423826, r: 0.4678415185740347
06/02/2019 10:08:41 step: 4338, epoch: 131, batch: 14, loss: 0.8651397824287415, acc: 62.5, f1: 37.9885668763323, r: 0.5916573052474888
06/02/2019 10:08:42 step: 4343, epoch: 131, batch: 19, loss: 0.6067893505096436, acc: 76.5625, f1: 41.72077922077923, r: 0.511183653843148
06/02/2019 10:08:43 step: 4348, epoch: 131, batch: 24, loss: 0.7974145412445068, acc: 64.0625, f1: 49.4878762541806, r: 0.4729625999559531
06/02/2019 10:08:45 step: 4353, epoch: 131, batch: 29, loss: 0.7050337791442871, acc: 78.125, f1: 47.327127659574465, r: 0.49216596929419476
06/02/2019 10:08:45 *** evaluating ***
06/02/2019 10:08:46 step: 132, epoch: 131, acc: 54.27350427350427, f1: 25.075824353065606, r: 0.3903701827864309
06/02/2019 10:08:46 *** epoch: 133 ***
06/02/2019 10:08:46 *** training ***
06/02/2019 10:08:47 step: 4361, epoch: 132, batch: 4, loss: 0.7435041069984436, acc: 62.5, f1: 43.1542693211015, r: 0.5040981244038021
06/02/2019 10:08:48 step: 4366, epoch: 132, batch: 9, loss: 0.8909681439399719, acc: 59.375, f1: 42.883279914529915, r: 0.4497493516190565
06/02/2019 10:08:49 step: 4371, epoch: 132, batch: 14, loss: 0.8624649047851562, acc: 71.875, f1: 49.36734412540864, r: 0.5588502413635563
06/02/2019 10:08:50 step: 4376, epoch: 132, batch: 19, loss: 0.7144812345504761, acc: 78.125, f1: 47.10461107519931, r: 0.46320532631792055
06/02/2019 10:08:52 step: 4381, epoch: 132, batch: 24, loss: 0.8360897898674011, acc: 64.0625, f1: 40.54150970733121, r: 0.5152553572338255
06/02/2019 10:08:53 step: 4386, epoch: 132, batch: 29, loss: 0.836768627166748, acc: 64.0625, f1: 38.06563706563706, r: 0.42613262487326614
06/02/2019 10:08:53 *** evaluating ***
06/02/2019 10:08:54 step: 133, epoch: 132, acc: 58.54700854700855, f1: 26.414609053497944, r: 0.38445795458066356
06/02/2019 10:08:54 *** epoch: 134 ***
06/02/2019 10:08:54 *** training ***
06/02/2019 10:08:55 step: 4394, epoch: 133, batch: 4, loss: 0.5409730672836304, acc: 78.125, f1: 61.59494232475598, r: 0.5952707460122987
06/02/2019 10:08:56 step: 4399, epoch: 133, batch: 9, loss: 0.7842820882797241, acc: 64.0625, f1: 58.788076807077516, r: 0.5251804277315048
06/02/2019 10:08:57 step: 4404, epoch: 133, batch: 14, loss: 0.8150251507759094, acc: 65.625, f1: 51.30385487528345, r: 0.5085772578988459
06/02/2019 10:08:58 step: 4409, epoch: 133, batch: 19, loss: 0.5997955799102783, acc: 73.4375, f1: 61.34910330855602, r: 0.5974370224777621
06/02/2019 10:09:00 step: 4414, epoch: 133, batch: 24, loss: 0.6516606211662292, acc: 75.0, f1: 46.96132092683817, r: 0.49222241570203795
06/02/2019 10:09:01 step: 4419, epoch: 133, batch: 29, loss: 0.5241421461105347, acc: 79.6875, f1: 50.504535147392296, r: 0.4787584205399082
06/02/2019 10:09:01 *** evaluating ***
06/02/2019 10:09:02 step: 134, epoch: 133, acc: 58.54700854700855, f1: 28.606943133264593, r: 0.3915420344534359
06/02/2019 10:09:02 *** epoch: 135 ***
06/02/2019 10:09:02 *** training ***
06/02/2019 10:09:03 step: 4427, epoch: 134, batch: 4, loss: 0.6113337278366089, acc: 70.3125, f1: 51.25296394625377, r: 0.48339512383978134
06/02/2019 10:09:04 step: 4432, epoch: 134, batch: 9, loss: 0.7260792255401611, acc: 73.4375, f1: 43.84818408237612, r: 0.3844466350802586
06/02/2019 10:09:05 step: 4437, epoch: 134, batch: 14, loss: 0.7399497032165527, acc: 70.3125, f1: 43.77406219511482, r: 0.4778947483075485
06/02/2019 10:09:06 step: 4442, epoch: 134, batch: 19, loss: 0.8354453444480896, acc: 59.375, f1: 33.492063492063494, r: 0.44893947300564335
06/02/2019 10:09:07 step: 4447, epoch: 134, batch: 24, loss: 0.744436502456665, acc: 70.3125, f1: 44.37229437229438, r: 0.5369474153239002
06/02/2019 10:09:09 step: 4452, epoch: 134, batch: 29, loss: 0.6910789608955383, acc: 76.5625, f1: 44.44006568144499, r: 0.4508527759900321
06/02/2019 10:09:09 *** evaluating ***
06/02/2019 10:09:10 step: 135, epoch: 134, acc: 58.119658119658126, f1: 29.897671158093253, r: 0.40327868973887
06/02/2019 10:09:10 *** epoch: 136 ***
06/02/2019 10:09:10 *** training ***
06/02/2019 10:09:11 step: 4460, epoch: 135, batch: 4, loss: 0.7605534195899963, acc: 62.5, f1: 41.52020202020202, r: 0.5461311127592677
06/02/2019 10:09:12 step: 4465, epoch: 135, batch: 9, loss: 0.5840045213699341, acc: 84.375, f1: 56.48036050578425, r: 0.5376170641383735
06/02/2019 10:09:13 step: 4470, epoch: 135, batch: 14, loss: 0.8015456199645996, acc: 70.3125, f1: 38.81313131313131, r: 0.410187306502482
06/02/2019 10:09:14 step: 4475, epoch: 135, batch: 19, loss: 0.8832759261131287, acc: 64.0625, f1: 35.15422077922078, r: 0.4426900566002912
06/02/2019 10:09:16 step: 4480, epoch: 135, batch: 24, loss: 0.4684559106826782, acc: 79.6875, f1: 54.72762227420612, r: 0.5165748871356725
06/02/2019 10:09:17 step: 4485, epoch: 135, batch: 29, loss: 0.7564343810081482, acc: 68.75, f1: 57.933931148216864, r: 0.5020881037233583
06/02/2019 10:09:17 *** evaluating ***
06/02/2019 10:09:18 step: 136, epoch: 135, acc: 58.54700854700855, f1: 28.659515083934938, r: 0.37975216227977576
06/02/2019 10:09:18 *** epoch: 137 ***
06/02/2019 10:09:18 *** training ***
06/02/2019 10:09:19 step: 4493, epoch: 136, batch: 4, loss: 0.4961504638195038, acc: 81.25, f1: 77.89318303811058, r: 0.5058419740880019
06/02/2019 10:09:20 step: 4498, epoch: 136, batch: 9, loss: 0.6578229665756226, acc: 75.0, f1: 57.58333333333333, r: 0.6657724136927345
06/02/2019 10:09:21 step: 4503, epoch: 136, batch: 14, loss: 0.6476532816886902, acc: 78.125, f1: 46.51131427770772, r: 0.6055335356094145
06/02/2019 10:09:22 step: 4508, epoch: 136, batch: 19, loss: 0.77076655626297, acc: 64.0625, f1: 40.23796192609183, r: 0.4923900899618383
06/02/2019 10:09:23 step: 4513, epoch: 136, batch: 24, loss: 0.6309902667999268, acc: 78.125, f1: 58.89804639804641, r: 0.47189326569153106
06/02/2019 10:09:25 step: 4518, epoch: 136, batch: 29, loss: 0.7047227621078491, acc: 71.875, f1: 54.99476713762428, r: 0.49436011928787915
06/02/2019 10:09:25 *** evaluating ***
06/02/2019 10:09:26 step: 137, epoch: 136, acc: 58.119658119658126, f1: 29.520791200118378, r: 0.4002687309384255
06/02/2019 10:09:26 *** epoch: 138 ***
06/02/2019 10:09:26 *** training ***
06/02/2019 10:09:27 step: 4526, epoch: 137, batch: 4, loss: 0.6302474737167358, acc: 70.3125, f1: 40.23546260609507, r: 0.5425728146490264
06/02/2019 10:09:28 step: 4531, epoch: 137, batch: 9, loss: 0.788115918636322, acc: 71.875, f1: 55.607385811467445, r: 0.5113333857893386
06/02/2019 10:09:29 step: 4536, epoch: 137, batch: 14, loss: 0.5764314532279968, acc: 76.5625, f1: 47.539682539682545, r: 0.5072078257352854
06/02/2019 10:09:30 step: 4541, epoch: 137, batch: 19, loss: 0.7607528567314148, acc: 73.4375, f1: 62.906474544405576, r: 0.577267271156172
06/02/2019 10:09:31 step: 4546, epoch: 137, batch: 24, loss: 0.6426105499267578, acc: 65.625, f1: 46.76788124156545, r: 0.5230418055957776
06/02/2019 10:09:32 step: 4551, epoch: 137, batch: 29, loss: 0.7414287328720093, acc: 70.3125, f1: 43.30818965517241, r: 0.5648018431616435
06/02/2019 10:09:33 *** evaluating ***
06/02/2019 10:09:34 step: 138, epoch: 137, acc: 60.68376068376068, f1: 31.424393234787296, r: 0.39920264230418867
06/02/2019 10:09:34 *** epoch: 139 ***
06/02/2019 10:09:34 *** training ***
06/02/2019 10:09:35 step: 4559, epoch: 138, batch: 4, loss: 0.718923032283783, acc: 70.3125, f1: 40.021344396344396, r: 0.5363964977727557
06/02/2019 10:09:36 step: 4564, epoch: 138, batch: 9, loss: 0.6743925213813782, acc: 67.1875, f1: 39.95650183150183, r: 0.5175294276509141
06/02/2019 10:09:37 step: 4569, epoch: 138, batch: 14, loss: 0.501021146774292, acc: 78.125, f1: 65.46142030012999, r: 0.584124822118934
06/02/2019 10:09:38 step: 4574, epoch: 138, batch: 19, loss: 0.8392078876495361, acc: 65.625, f1: 40.583432770932774, r: 0.526580264300561
06/02/2019 10:09:39 step: 4579, epoch: 138, batch: 24, loss: 0.934227466583252, acc: 64.0625, f1: 46.695971022152776, r: 0.5400834809297249
06/02/2019 10:09:40 step: 4584, epoch: 138, batch: 29, loss: 0.5943604111671448, acc: 79.6875, f1: 44.12337662337662, r: 0.5351244161251263
06/02/2019 10:09:41 *** evaluating ***
06/02/2019 10:09:42 step: 139, epoch: 138, acc: 58.119658119658126, f1: 30.133508648214537, r: 0.4065171551926244
06/02/2019 10:09:42 *** epoch: 140 ***
06/02/2019 10:09:42 *** training ***
06/02/2019 10:09:42 step: 4592, epoch: 139, batch: 4, loss: 0.7158973217010498, acc: 73.4375, f1: 40.575980392156865, r: 0.4917200607995662
06/02/2019 10:09:44 step: 4597, epoch: 139, batch: 9, loss: 0.4921760559082031, acc: 78.125, f1: 58.994708994709, r: 0.5057159558887219
06/02/2019 10:09:45 step: 4602, epoch: 139, batch: 14, loss: 0.6673004627227783, acc: 73.4375, f1: 48.84639574262216, r: 0.4969590479408642
06/02/2019 10:09:46 step: 4607, epoch: 139, batch: 19, loss: 0.7750473022460938, acc: 64.0625, f1: 39.06385281385281, r: 0.5651299325090009
06/02/2019 10:09:47 step: 4612, epoch: 139, batch: 24, loss: 0.6908369660377502, acc: 68.75, f1: 43.023571547559555, r: 0.6144567137060621
06/02/2019 10:09:48 step: 4617, epoch: 139, batch: 29, loss: 0.6691680550575256, acc: 67.1875, f1: 44.852716727716725, r: 0.6029173732524874
06/02/2019 10:09:49 *** evaluating ***
06/02/2019 10:09:49 step: 140, epoch: 139, acc: 60.256410256410255, f1: 30.388345133241423, r: 0.38876993302983837
06/02/2019 10:09:49 *** epoch: 141 ***
06/02/2019 10:09:49 *** training ***
06/02/2019 10:09:50 step: 4625, epoch: 140, batch: 4, loss: 0.6347450613975525, acc: 71.875, f1: 61.57388663967611, r: 0.5985467684319175
06/02/2019 10:09:51 step: 4630, epoch: 140, batch: 9, loss: 0.6517378091812134, acc: 73.4375, f1: 54.07893374741201, r: 0.5821813650844541
06/02/2019 10:09:53 step: 4635, epoch: 140, batch: 14, loss: 0.6231108903884888, acc: 78.125, f1: 60.556426332288396, r: 0.5894231507339326
06/02/2019 10:09:54 step: 4640, epoch: 140, batch: 19, loss: 0.5383944511413574, acc: 79.6875, f1: 62.21861471861472, r: 0.5292236942865646
06/02/2019 10:09:55 step: 4645, epoch: 140, batch: 24, loss: 0.5467785596847534, acc: 81.25, f1: 76.4625850340136, r: 0.5311528230549878
06/02/2019 10:09:56 step: 4650, epoch: 140, batch: 29, loss: 0.6024009585380554, acc: 75.0, f1: 43.257110536522305, r: 0.5525600315684546
06/02/2019 10:09:57 *** evaluating ***
06/02/2019 10:09:57 step: 141, epoch: 140, acc: 56.837606837606835, f1: 25.882335257335253, r: 0.3802405975251457
06/02/2019 10:09:57 *** epoch: 142 ***
06/02/2019 10:09:57 *** training ***
06/02/2019 10:09:59 step: 4658, epoch: 141, batch: 4, loss: 0.5857697129249573, acc: 78.125, f1: 56.699353096868634, r: 0.561064494419982
06/02/2019 10:10:00 step: 4663, epoch: 141, batch: 9, loss: 0.6227947473526001, acc: 76.5625, f1: 46.49296051187877, r: 0.5157860427393588
06/02/2019 10:10:01 step: 4668, epoch: 141, batch: 14, loss: 0.7242944240570068, acc: 70.3125, f1: 55.04960317460317, r: 0.6145507198158736
06/02/2019 10:10:02 step: 4673, epoch: 141, batch: 19, loss: 0.72715163230896, acc: 75.0, f1: 64.28571428571429, r: 0.5203162052400409
06/02/2019 10:10:03 step: 4678, epoch: 141, batch: 24, loss: 0.5411463379859924, acc: 84.375, f1: 75.95616024187451, r: 0.503276444096798
06/02/2019 10:10:04 step: 4683, epoch: 141, batch: 29, loss: 0.7477259039878845, acc: 65.625, f1: 42.10997335997336, r: 0.5872604530345413
06/02/2019 10:10:05 *** evaluating ***
06/02/2019 10:10:05 step: 142, epoch: 141, acc: 59.401709401709404, f1: 30.389597617318532, r: 0.38404850434438376
06/02/2019 10:10:05 *** epoch: 143 ***
06/02/2019 10:10:05 *** training ***
06/02/2019 10:10:07 step: 4691, epoch: 142, batch: 4, loss: 0.7190015912055969, acc: 73.4375, f1: 61.78072401981319, r: 0.5232494771887186
06/02/2019 10:10:08 step: 4696, epoch: 142, batch: 9, loss: 0.4306804835796356, acc: 81.25, f1: 41.81216849409069, r: 0.5293343551066522
06/02/2019 10:10:09 step: 4701, epoch: 142, batch: 14, loss: 0.5828158855438232, acc: 70.3125, f1: 42.64208173690932, r: 0.612535994008383
06/02/2019 10:10:10 step: 4706, epoch: 142, batch: 19, loss: 0.5320395827293396, acc: 78.125, f1: 53.108680201477085, r: 0.5565826023150707
06/02/2019 10:10:11 step: 4711, epoch: 142, batch: 24, loss: 0.6629237532615662, acc: 70.3125, f1: 46.60044542762005, r: 0.4895489862702187
06/02/2019 10:10:12 step: 4716, epoch: 142, batch: 29, loss: 0.5929749011993408, acc: 73.4375, f1: 51.595959595959606, r: 0.5436689065121767
06/02/2019 10:10:13 *** evaluating ***
06/02/2019 10:10:13 step: 143, epoch: 142, acc: 55.55555555555556, f1: 24.86298922573432, r: 0.36139947028492353
06/02/2019 10:10:13 *** epoch: 144 ***
06/02/2019 10:10:13 *** training ***
06/02/2019 10:10:15 step: 4724, epoch: 143, batch: 4, loss: 0.6140570640563965, acc: 78.125, f1: 63.20716320716322, r: 0.5422107610306885
06/02/2019 10:10:16 step: 4729, epoch: 143, batch: 9, loss: 0.7544857263565063, acc: 70.3125, f1: 37.44609557109557, r: 0.4945063225467708
06/02/2019 10:10:17 step: 4734, epoch: 143, batch: 14, loss: 0.6055387854576111, acc: 82.8125, f1: 66.96393557422968, r: 0.6214323708683623
06/02/2019 10:10:18 step: 4739, epoch: 143, batch: 19, loss: 0.6202462911605835, acc: 76.5625, f1: 59.133458646616546, r: 0.5599420158427689
06/02/2019 10:10:19 step: 4744, epoch: 143, batch: 24, loss: 0.7485186457633972, acc: 73.4375, f1: 57.497317767686795, r: 0.5804259888538368
06/02/2019 10:10:20 step: 4749, epoch: 143, batch: 29, loss: 0.6388381719589233, acc: 70.3125, f1: 47.07938063733212, r: 0.559624041446306
06/02/2019 10:10:21 *** evaluating ***
06/02/2019 10:10:21 step: 144, epoch: 143, acc: 57.692307692307686, f1: 25.932760141093475, r: 0.36156544723797857
06/02/2019 10:10:21 *** epoch: 145 ***
06/02/2019 10:10:21 *** training ***
06/02/2019 10:10:23 step: 4757, epoch: 144, batch: 4, loss: 1.1847891807556152, acc: 53.125, f1: 28.447802197802197, r: 0.35954428299504126
06/02/2019 10:10:24 step: 4762, epoch: 144, batch: 9, loss: 0.7037712931632996, acc: 76.5625, f1: 61.208062770562776, r: 0.5624358333847514
06/02/2019 10:10:25 step: 4767, epoch: 144, batch: 14, loss: 0.7603530287742615, acc: 67.1875, f1: 41.66616716616717, r: 0.5752530877881746
06/02/2019 10:10:26 step: 4772, epoch: 144, batch: 19, loss: 0.6867204308509827, acc: 73.4375, f1: 60.21428571428571, r: 0.5890043459522485
06/02/2019 10:10:27 step: 4777, epoch: 144, batch: 24, loss: 0.6713927388191223, acc: 78.125, f1: 55.08928571428571, r: 0.6349293158599255
06/02/2019 10:10:29 step: 4782, epoch: 144, batch: 29, loss: 0.6447206735610962, acc: 71.875, f1: 38.47273922831732, r: 0.5407640549728796
06/02/2019 10:10:29 *** evaluating ***
06/02/2019 10:10:29 step: 145, epoch: 144, acc: 56.41025641025641, f1: 26.95283699421116, r: 0.386968634525705
06/02/2019 10:10:29 *** epoch: 146 ***
06/02/2019 10:10:29 *** training ***
06/02/2019 10:10:31 step: 4790, epoch: 145, batch: 4, loss: 0.5653342008590698, acc: 76.5625, f1: 52.65625000000001, r: 0.5936311564270371
06/02/2019 10:10:32 step: 4795, epoch: 145, batch: 9, loss: 0.7766886949539185, acc: 65.625, f1: 45.875, r: 0.590432444027519
06/02/2019 10:10:33 step: 4800, epoch: 145, batch: 14, loss: 0.5167830586433411, acc: 85.9375, f1: 57.792832167832174, r: 0.6180145702375162
06/02/2019 10:10:34 step: 4805, epoch: 145, batch: 19, loss: 0.5274862051010132, acc: 79.6875, f1: 57.56829716013389, r: 0.5125541594251188
06/02/2019 10:10:35 step: 4810, epoch: 145, batch: 24, loss: 0.7226443290710449, acc: 67.1875, f1: 43.65053927553928, r: 0.5684463578382568
06/02/2019 10:10:36 step: 4815, epoch: 145, batch: 29, loss: 0.46705588698387146, acc: 81.25, f1: 59.31994114260124, r: 0.5417607829978472
06/02/2019 10:10:37 *** evaluating ***
06/02/2019 10:10:37 step: 146, epoch: 145, acc: 59.401709401709404, f1: 29.69084979318365, r: 0.3905714024511626
06/02/2019 10:10:37 *** epoch: 147 ***
06/02/2019 10:10:37 *** training ***
06/02/2019 10:10:39 step: 4823, epoch: 146, batch: 4, loss: 0.8310127854347229, acc: 67.1875, f1: 44.559420939648376, r: 0.5383542868195041
06/02/2019 10:10:40 step: 4828, epoch: 146, batch: 9, loss: 0.5203954577445984, acc: 78.125, f1: 63.674832644922375, r: 0.4857614602165164
06/02/2019 10:10:41 step: 4833, epoch: 146, batch: 14, loss: 0.8360183238983154, acc: 62.5, f1: 44.75894744790093, r: 0.5709892153832926
06/02/2019 10:10:42 step: 4838, epoch: 146, batch: 19, loss: 0.7459836602210999, acc: 70.3125, f1: 58.40388007054674, r: 0.4361490821823151
06/02/2019 10:10:43 step: 4843, epoch: 146, batch: 24, loss: 0.6617593169212341, acc: 71.875, f1: 47.631710813046965, r: 0.5880388173467421
06/02/2019 10:10:44 step: 4848, epoch: 146, batch: 29, loss: 0.5660960078239441, acc: 78.125, f1: 56.38592389799748, r: 0.5862097465046708
06/02/2019 10:10:45 *** evaluating ***
06/02/2019 10:10:45 step: 147, epoch: 146, acc: 58.119658119658126, f1: 28.184698255397393, r: 0.3744857371479197
06/02/2019 10:10:45 *** epoch: 148 ***
06/02/2019 10:10:45 *** training ***
06/02/2019 10:10:47 step: 4856, epoch: 147, batch: 4, loss: 0.5805631875991821, acc: 75.0, f1: 37.29059892852996, r: 0.5236646648783401
06/02/2019 10:10:48 step: 4861, epoch: 147, batch: 9, loss: 0.6046143770217896, acc: 75.0, f1: 59.505907626208376, r: 0.4694934456419338
06/02/2019 10:10:49 step: 4866, epoch: 147, batch: 14, loss: 0.7555993795394897, acc: 70.3125, f1: 41.69612620084318, r: 0.4967833466847594
06/02/2019 10:10:50 step: 4871, epoch: 147, batch: 19, loss: 0.5357479453086853, acc: 81.25, f1: 66.47495361781077, r: 0.5620935222656461
06/02/2019 10:10:51 step: 4876, epoch: 147, batch: 24, loss: 0.5747862458229065, acc: 73.4375, f1: 51.952165481577254, r: 0.47452753260213604
06/02/2019 10:10:52 step: 4881, epoch: 147, batch: 29, loss: 0.5437717437744141, acc: 81.25, f1: 57.50654107796965, r: 0.5129997609408967
06/02/2019 10:10:53 *** evaluating ***
06/02/2019 10:10:53 step: 148, epoch: 147, acc: 57.692307692307686, f1: 27.601085258759994, r: 0.37744378818128865
06/02/2019 10:10:53 *** epoch: 149 ***
06/02/2019 10:10:53 *** training ***
06/02/2019 10:10:55 step: 4889, epoch: 148, batch: 4, loss: 0.6806533336639404, acc: 71.875, f1: 54.71093322386425, r: 0.5271335162425854
06/02/2019 10:10:56 step: 4894, epoch: 148, batch: 9, loss: 0.7319156527519226, acc: 68.75, f1: 39.78431372549019, r: 0.5671390174845219
06/02/2019 10:10:57 step: 4899, epoch: 148, batch: 14, loss: 0.5628146529197693, acc: 76.5625, f1: 59.55441110613524, r: 0.5835824101218712
06/02/2019 10:10:58 step: 4904, epoch: 148, batch: 19, loss: 0.6315785050392151, acc: 73.4375, f1: 44.270348295772024, r: 0.586670822588117
06/02/2019 10:10:59 step: 4909, epoch: 148, batch: 24, loss: 0.5243175625801086, acc: 78.125, f1: 50.51713938948893, r: 0.42888303587097926
06/02/2019 10:11:00 step: 4914, epoch: 148, batch: 29, loss: 0.5962722897529602, acc: 73.4375, f1: 47.88545960256024, r: 0.5111059644078669
06/02/2019 10:11:01 *** evaluating ***
06/02/2019 10:11:01 step: 149, epoch: 148, acc: 56.837606837606835, f1: 28.849123797636384, r: 0.3715672163026693
06/02/2019 10:11:01 *** epoch: 150 ***
06/02/2019 10:11:01 *** training ***
06/02/2019 10:11:03 step: 4922, epoch: 149, batch: 4, loss: 0.6055968403816223, acc: 75.0, f1: 50.8420554425397, r: 0.48884789902351866
06/02/2019 10:11:04 step: 4927, epoch: 149, batch: 9, loss: 0.7063174247741699, acc: 67.1875, f1: 52.97275641025641, r: 0.5966378781972529
06/02/2019 10:11:05 step: 4932, epoch: 149, batch: 14, loss: 0.6465606689453125, acc: 70.3125, f1: 44.566493263972255, r: 0.5009318034854421
06/02/2019 10:11:06 step: 4937, epoch: 149, batch: 19, loss: 0.6512916684150696, acc: 68.75, f1: 48.11265082956259, r: 0.6015180873356833
06/02/2019 10:11:07 step: 4942, epoch: 149, batch: 24, loss: 0.5118086338043213, acc: 79.6875, f1: 68.12324929971989, r: 0.6396865952170003
06/02/2019 10:11:09 step: 4947, epoch: 149, batch: 29, loss: 0.5379673838615417, acc: 76.5625, f1: 45.28107344632769, r: 0.5380548692073213
06/02/2019 10:11:09 *** evaluating ***
06/02/2019 10:11:10 step: 150, epoch: 149, acc: 55.98290598290598, f1: 27.127204002508908, r: 0.35780573653271086
06/02/2019 10:11:10 *** epoch: 151 ***
06/02/2019 10:11:10 *** training ***
06/02/2019 10:11:11 step: 4955, epoch: 150, batch: 4, loss: 0.6179492473602295, acc: 70.3125, f1: 50.731899007761086, r: 0.5131936747528386
06/02/2019 10:11:12 step: 4960, epoch: 150, batch: 9, loss: 0.7753849029541016, acc: 65.625, f1: 38.77576228119706, r: 0.5296154113853365
06/02/2019 10:11:13 step: 4965, epoch: 150, batch: 14, loss: 0.7566490769386292, acc: 70.3125, f1: 57.06177420463134, r: 0.5441535914022683
06/02/2019 10:11:14 step: 4970, epoch: 150, batch: 19, loss: 0.641288161277771, acc: 68.75, f1: 51.469057258530945, r: 0.5442077163314798
06/02/2019 10:11:15 step: 4975, epoch: 150, batch: 24, loss: 0.4936312139034271, acc: 79.6875, f1: 61.07793769614652, r: 0.5914844610674599
06/02/2019 10:11:16 step: 4980, epoch: 150, batch: 29, loss: 0.6198879480361938, acc: 71.875, f1: 67.79111057912901, r: 0.5840202040775047
06/02/2019 10:11:17 *** evaluating ***
06/02/2019 10:11:17 step: 151, epoch: 150, acc: 60.256410256410255, f1: 30.385035325286058, r: 0.36633621912969433
06/02/2019 10:11:17 *** epoch: 152 ***
06/02/2019 10:11:17 *** training ***
06/02/2019 10:11:19 step: 4988, epoch: 151, batch: 4, loss: 0.5015145540237427, acc: 82.8125, f1: 64.49810606060605, r: 0.6092567056660234
06/02/2019 10:11:20 step: 4993, epoch: 151, batch: 9, loss: 0.5975426435470581, acc: 78.125, f1: 58.67317014782453, r: 0.5398687294493023
06/02/2019 10:11:21 step: 4998, epoch: 151, batch: 14, loss: 0.5599966049194336, acc: 78.125, f1: 62.04804242977554, r: 0.5805435316994958
06/02/2019 10:11:22 step: 5003, epoch: 151, batch: 19, loss: 0.3850468397140503, acc: 85.9375, f1: 73.98071724054843, r: 0.5617798051928606
06/02/2019 10:11:23 step: 5008, epoch: 151, batch: 24, loss: 0.523720383644104, acc: 79.6875, f1: 46.49446958270487, r: 0.5237004872727842
06/02/2019 10:11:24 step: 5013, epoch: 151, batch: 29, loss: 0.5815885663032532, acc: 75.0, f1: 42.44300855138294, r: 0.5006100574143594
06/02/2019 10:11:25 *** evaluating ***
06/02/2019 10:11:25 step: 152, epoch: 151, acc: 57.26495726495726, f1: 29.263951538725237, r: 0.36446027026287436
06/02/2019 10:11:25 *** epoch: 153 ***
06/02/2019 10:11:25 *** training ***
06/02/2019 10:11:26 step: 5021, epoch: 152, batch: 4, loss: 0.5313898324966431, acc: 78.125, f1: 66.17978620019437, r: 0.5473105775912933
06/02/2019 10:11:27 step: 5026, epoch: 152, batch: 9, loss: 0.5461518168449402, acc: 79.6875, f1: 77.80726436427176, r: 0.5544363054996133
06/02/2019 10:11:29 step: 5031, epoch: 152, batch: 14, loss: 0.4771571159362793, acc: 84.375, f1: 67.0577321837826, r: 0.560350803509341
06/02/2019 10:11:30 step: 5036, epoch: 152, batch: 19, loss: 0.3957252502441406, acc: 87.5, f1: 87.38710750166277, r: 0.583658689541058
06/02/2019 10:11:31 step: 5041, epoch: 152, batch: 24, loss: 0.5239207744598389, acc: 73.4375, f1: 53.80802038329456, r: 0.5621798028468344
06/02/2019 10:11:32 step: 5046, epoch: 152, batch: 29, loss: 0.7008233070373535, acc: 73.4375, f1: 58.1537265443812, r: 0.5660921868445791
06/02/2019 10:11:33 *** evaluating ***
06/02/2019 10:11:33 step: 153, epoch: 152, acc: 57.26495726495726, f1: 28.596483369948523, r: 0.35972642962398776
06/02/2019 10:11:33 *** epoch: 154 ***
06/02/2019 10:11:33 *** training ***
06/02/2019 10:11:34 step: 5054, epoch: 153, batch: 4, loss: 0.5231149792671204, acc: 84.375, f1: 75.31788551278807, r: 0.545119120711691
06/02/2019 10:11:36 step: 5059, epoch: 153, batch: 9, loss: 0.5715057253837585, acc: 78.125, f1: 54.92048162124888, r: 0.5721311632983735
06/02/2019 10:11:37 step: 5064, epoch: 153, batch: 14, loss: 0.6450769901275635, acc: 70.3125, f1: 39.552970177970174, r: 0.44100368541495266
06/02/2019 10:11:38 step: 5069, epoch: 153, batch: 19, loss: 0.5144367218017578, acc: 84.375, f1: 69.07513279795889, r: 0.6598270560860574
06/02/2019 10:11:39 step: 5074, epoch: 153, batch: 24, loss: 0.579013466835022, acc: 75.0, f1: 74.75284850782418, r: 0.5955510003432134
06/02/2019 10:11:40 step: 5079, epoch: 153, batch: 29, loss: 0.43955954909324646, acc: 79.6875, f1: 72.72418058132344, r: 0.6232426440527533
06/02/2019 10:11:41 *** evaluating ***
06/02/2019 10:11:41 step: 154, epoch: 153, acc: 54.27350427350427, f1: 29.274644734227724, r: 0.3658465947258809
06/02/2019 10:11:41 *** epoch: 155 ***
06/02/2019 10:11:41 *** training ***
06/02/2019 10:11:42 step: 5087, epoch: 154, batch: 4, loss: 0.5975238084793091, acc: 73.4375, f1: 47.03054806828392, r: 0.6977076935331845
06/02/2019 10:11:43 step: 5092, epoch: 154, batch: 9, loss: 0.5956639647483826, acc: 71.875, f1: 60.08716950588872, r: 0.6297494718375147
06/02/2019 10:11:45 step: 5097, epoch: 154, batch: 14, loss: 0.6882067918777466, acc: 81.25, f1: 66.15196078431373, r: 0.5698137279241681
06/02/2019 10:11:46 step: 5102, epoch: 154, batch: 19, loss: 0.5081580877304077, acc: 79.6875, f1: 62.55952380952381, r: 0.5459202313824172
06/02/2019 10:11:47 step: 5107, epoch: 154, batch: 24, loss: 0.7282159328460693, acc: 68.75, f1: 48.57703866960529, r: 0.5241077754916385
06/02/2019 10:11:48 step: 5112, epoch: 154, batch: 29, loss: 0.6390093564987183, acc: 73.4375, f1: 54.160256410256416, r: 0.5854435690465755
06/02/2019 10:11:49 *** evaluating ***
06/02/2019 10:11:50 step: 155, epoch: 154, acc: 55.98290598290598, f1: 30.129071141046783, r: 0.3737139678012901
06/02/2019 10:11:50 *** epoch: 156 ***
06/02/2019 10:11:50 *** training ***
06/02/2019 10:11:51 step: 5120, epoch: 155, batch: 4, loss: 0.4503200948238373, acc: 84.375, f1: 62.14218455743878, r: 0.6616978618798978
06/02/2019 10:11:52 step: 5125, epoch: 155, batch: 9, loss: 0.4517761170864105, acc: 84.375, f1: 57.209967320261434, r: 0.635235411722446
06/02/2019 10:11:53 step: 5130, epoch: 155, batch: 14, loss: 0.4815257787704468, acc: 82.8125, f1: 55.34071906354515, r: 0.6435501174745435
06/02/2019 10:11:54 step: 5135, epoch: 155, batch: 19, loss: 0.5258757472038269, acc: 78.125, f1: 67.00012631047113, r: 0.5115889545203396
06/02/2019 10:11:56 step: 5140, epoch: 155, batch: 24, loss: 0.46949371695518494, acc: 84.375, f1: 59.161258169637165, r: 0.5595114513765079
06/02/2019 10:11:57 step: 5145, epoch: 155, batch: 29, loss: 0.6842512488365173, acc: 70.3125, f1: 46.760710553814, r: 0.5547299847792941
06/02/2019 10:11:57 *** evaluating ***
06/02/2019 10:11:58 step: 156, epoch: 155, acc: 52.13675213675214, f1: 28.297497384237293, r: 0.36772031372331143
06/02/2019 10:11:58 *** epoch: 157 ***
06/02/2019 10:11:58 *** training ***
06/02/2019 10:11:59 step: 5153, epoch: 156, batch: 4, loss: 0.4815201163291931, acc: 78.125, f1: 54.922438672438666, r: 0.6303558030144556
06/02/2019 10:12:00 step: 5158, epoch: 156, batch: 9, loss: 0.49145710468292236, acc: 79.6875, f1: 63.916666666666664, r: 0.6127924728621552
06/02/2019 10:12:01 step: 5163, epoch: 156, batch: 14, loss: 0.5210319757461548, acc: 76.5625, f1: 59.791666666666664, r: 0.6156644823706972
06/02/2019 10:12:02 step: 5168, epoch: 156, batch: 19, loss: 0.5668177008628845, acc: 75.0, f1: 50.91666666666666, r: 0.5868957556693792
06/02/2019 10:12:03 step: 5173, epoch: 156, batch: 24, loss: 0.5300611257553101, acc: 73.4375, f1: 51.456480027908604, r: 0.607627901875682
06/02/2019 10:12:04 step: 5178, epoch: 156, batch: 29, loss: 0.6714291572570801, acc: 73.4375, f1: 53.2672556201968, r: 0.6186895842984068
06/02/2019 10:12:05 *** evaluating ***
06/02/2019 10:12:06 step: 157, epoch: 156, acc: 57.26495726495726, f1: 28.931999802742837, r: 0.3523930189532405
06/02/2019 10:12:06 *** epoch: 158 ***
06/02/2019 10:12:06 *** training ***
06/02/2019 10:12:07 step: 5186, epoch: 157, batch: 4, loss: 0.5538045763969421, acc: 78.125, f1: 50.471230158730165, r: 0.5648061866584676
06/02/2019 10:12:08 step: 5191, epoch: 157, batch: 9, loss: 0.3900306820869446, acc: 81.25, f1: 61.59307912181477, r: 0.550293453895546
06/02/2019 10:12:09 step: 5196, epoch: 157, batch: 14, loss: 0.41115158796310425, acc: 85.9375, f1: 58.92067831917457, r: 0.5423243624979676
06/02/2019 10:12:10 step: 5201, epoch: 157, batch: 19, loss: 0.526481032371521, acc: 79.6875, f1: 70.76066790352505, r: 0.6104273287658092
06/02/2019 10:12:11 step: 5206, epoch: 157, batch: 24, loss: 0.5425776243209839, acc: 79.6875, f1: 65.30982905982907, r: 0.616450551948672
06/02/2019 10:12:13 step: 5211, epoch: 157, batch: 29, loss: 0.6992318630218506, acc: 71.875, f1: 55.43393930749514, r: 0.5603410381766666
06/02/2019 10:12:13 *** evaluating ***
06/02/2019 10:12:14 step: 158, epoch: 157, acc: 56.837606837606835, f1: 27.706668780384696, r: 0.34208321803384095
06/02/2019 10:12:14 *** epoch: 159 ***
06/02/2019 10:12:14 *** training ***
06/02/2019 10:12:14 step: 5219, epoch: 158, batch: 4, loss: 0.5257413387298584, acc: 82.8125, f1: 59.325583420411, r: 0.615166406038848
06/02/2019 10:12:16 step: 5224, epoch: 158, batch: 9, loss: 0.5446798801422119, acc: 76.5625, f1: 71.49479980143596, r: 0.5697303070750113
06/02/2019 10:12:17 step: 5229, epoch: 158, batch: 14, loss: 0.32884880900382996, acc: 90.625, f1: 65.21590715139102, r: 0.5613984200702306
06/02/2019 10:12:18 step: 5234, epoch: 158, batch: 19, loss: 0.5096461772918701, acc: 76.5625, f1: 64.75625055216892, r: 0.560653925308306
06/02/2019 10:12:20 step: 5239, epoch: 158, batch: 24, loss: 0.752071738243103, acc: 71.875, f1: 53.241629985327464, r: 0.47743399069344966
06/02/2019 10:12:21 step: 5244, epoch: 158, batch: 29, loss: 0.5150661468505859, acc: 78.125, f1: 48.26640271493212, r: 0.5436134712238698
06/02/2019 10:12:21 *** evaluating ***
06/02/2019 10:12:22 step: 159, epoch: 158, acc: 57.692307692307686, f1: 27.22786527031834, r: 0.3321564248183233
06/02/2019 10:12:22 *** epoch: 160 ***
06/02/2019 10:12:22 *** training ***
06/02/2019 10:12:23 step: 5252, epoch: 159, batch: 4, loss: 0.48235809803009033, acc: 79.6875, f1: 61.908944658944655, r: 0.580819823235019
06/02/2019 10:12:24 step: 5257, epoch: 159, batch: 9, loss: 0.4673577845096588, acc: 79.6875, f1: 55.91765127103473, r: 0.6054075697756667
06/02/2019 10:12:25 step: 5262, epoch: 159, batch: 14, loss: 0.6236635446548462, acc: 76.5625, f1: 43.956196735519285, r: 0.5173998586079929
06/02/2019 10:12:26 step: 5267, epoch: 159, batch: 19, loss: 0.5147080421447754, acc: 81.25, f1: 73.07169021454735, r: 0.5517525325936954
06/02/2019 10:12:28 step: 5272, epoch: 159, batch: 24, loss: 0.4867907762527466, acc: 82.8125, f1: 62.6239994282447, r: 0.5986709340486246
06/02/2019 10:12:29 step: 5277, epoch: 159, batch: 29, loss: 0.623573899269104, acc: 70.3125, f1: 65.05668934240363, r: 0.5741113373202588
06/02/2019 10:12:29 *** evaluating ***
06/02/2019 10:12:30 step: 160, epoch: 159, acc: 58.54700854700855, f1: 28.17565719926235, r: 0.35156570570708734
06/02/2019 10:12:30 *** epoch: 161 ***
06/02/2019 10:12:30 *** training ***
06/02/2019 10:12:31 step: 5285, epoch: 160, batch: 4, loss: 0.424484521150589, acc: 85.9375, f1: 76.10014766140375, r: 0.5451940395191448
06/02/2019 10:12:32 step: 5290, epoch: 160, batch: 9, loss: 0.7361996173858643, acc: 71.875, f1: 46.72202797202797, r: 0.5366720067087962
06/02/2019 10:12:34 step: 5295, epoch: 160, batch: 14, loss: 0.6728094220161438, acc: 71.875, f1: 44.21152518978606, r: 0.46650278690949776
06/02/2019 10:12:35 step: 5300, epoch: 160, batch: 19, loss: 0.5083540678024292, acc: 81.25, f1: 59.056396752249285, r: 0.5010408386472418
06/02/2019 10:12:36 step: 5305, epoch: 160, batch: 24, loss: 0.4078204035758972, acc: 87.5, f1: 72.98167293233084, r: 0.656176703184887
06/02/2019 10:12:37 step: 5310, epoch: 160, batch: 29, loss: 0.5642068386077881, acc: 75.0, f1: 47.46025832091405, r: 0.5768642360413287
06/02/2019 10:12:38 *** evaluating ***
06/02/2019 10:12:38 step: 161, epoch: 160, acc: 57.26495726495726, f1: 29.820132427250535, r: 0.3554682319297606
06/02/2019 10:12:38 *** epoch: 162 ***
06/02/2019 10:12:38 *** training ***
06/02/2019 10:12:39 step: 5318, epoch: 161, batch: 4, loss: 0.5383650660514832, acc: 76.5625, f1: 56.01909050585875, r: 0.5628390008563043
06/02/2019 10:12:40 step: 5323, epoch: 161, batch: 9, loss: 0.5250245928764343, acc: 71.875, f1: 61.600274725274716, r: 0.5837688016698244
06/02/2019 10:12:42 step: 5328, epoch: 161, batch: 14, loss: 0.593766450881958, acc: 75.0, f1: 48.647933588150984, r: 0.5878035466275218
06/02/2019 10:12:43 step: 5333, epoch: 161, batch: 19, loss: 0.505713164806366, acc: 84.375, f1: 78.91697588126159, r: 0.5529541297106204
06/02/2019 10:12:44 step: 5338, epoch: 161, batch: 24, loss: 0.30162015557289124, acc: 89.0625, f1: 58.55555555555556, r: 0.445070056068832
06/02/2019 10:12:45 step: 5343, epoch: 161, batch: 29, loss: 0.46307769417762756, acc: 84.375, f1: 79.32227793274359, r: 0.6651258298546958
06/02/2019 10:12:46 *** evaluating ***
06/02/2019 10:12:46 step: 162, epoch: 161, acc: 58.119658119658126, f1: 30.8281859752448, r: 0.3611837948857108
06/02/2019 10:12:46 *** epoch: 163 ***
06/02/2019 10:12:46 *** training ***
06/02/2019 10:12:47 step: 5351, epoch: 162, batch: 4, loss: 0.5729607939720154, acc: 79.6875, f1: 58.40778630292696, r: 0.48161333360907077
06/02/2019 10:12:48 step: 5356, epoch: 162, batch: 9, loss: 0.44539278745651245, acc: 81.25, f1: 54.86556743909685, r: 0.5595424995241397
06/02/2019 10:12:50 step: 5361, epoch: 162, batch: 14, loss: 0.48281267285346985, acc: 84.375, f1: 71.79894179894181, r: 0.6082299600252323
06/02/2019 10:12:51 step: 5366, epoch: 162, batch: 19, loss: 0.38916781544685364, acc: 84.375, f1: 58.806240536448186, r: 0.567622246588453
06/02/2019 10:12:52 step: 5371, epoch: 162, batch: 24, loss: 0.4867582321166992, acc: 84.375, f1: 75.72871572871573, r: 0.5593240461698331
06/02/2019 10:12:53 step: 5376, epoch: 162, batch: 29, loss: 0.47374626994132996, acc: 82.8125, f1: 54.33079481792718, r: 0.5433680863616545
06/02/2019 10:12:53 *** evaluating ***
06/02/2019 10:12:54 step: 163, epoch: 162, acc: 55.98290598290598, f1: 26.667711224237205, r: 0.3431248532876426
06/02/2019 10:12:54 *** epoch: 164 ***
06/02/2019 10:12:54 *** training ***
06/02/2019 10:12:55 step: 5384, epoch: 163, batch: 4, loss: 0.4062148928642273, acc: 89.0625, f1: 77.82312925170068, r: 0.585905104375805
06/02/2019 10:12:56 step: 5389, epoch: 163, batch: 9, loss: 0.41703733801841736, acc: 84.375, f1: 77.46114417989418, r: 0.6675531673998922
06/02/2019 10:12:57 step: 5394, epoch: 163, batch: 14, loss: 0.49606677889823914, acc: 81.25, f1: 61.47115272115272, r: 0.5467839430656373
06/02/2019 10:12:58 step: 5399, epoch: 163, batch: 19, loss: 0.4493322968482971, acc: 84.375, f1: 77.88855330715796, r: 0.7073027671260618
06/02/2019 10:13:00 step: 5404, epoch: 163, batch: 24, loss: 0.5709050893783569, acc: 70.3125, f1: 51.6438526012994, r: 0.6260301120654149
06/02/2019 10:13:01 step: 5409, epoch: 163, batch: 29, loss: 0.657940685749054, acc: 73.4375, f1: 49.6698343079922, r: 0.5646560185887985
06/02/2019 10:13:01 *** evaluating ***
06/02/2019 10:13:02 step: 164, epoch: 163, acc: 57.692307692307686, f1: 28.041711900071032, r: 0.3452185809298341
06/02/2019 10:13:02 *** epoch: 165 ***
06/02/2019 10:13:02 *** training ***
06/02/2019 10:13:03 step: 5417, epoch: 164, batch: 4, loss: 0.5974825024604797, acc: 79.6875, f1: 62.45565007020532, r: 0.550435034921877
06/02/2019 10:13:04 step: 5422, epoch: 164, batch: 9, loss: 0.6533036231994629, acc: 75.0, f1: 57.585067262486625, r: 0.5937209989342498
06/02/2019 10:13:05 step: 5427, epoch: 164, batch: 14, loss: 0.4171064794063568, acc: 82.8125, f1: 58.026556776556774, r: 0.6587104231041938
06/02/2019 10:13:06 step: 5432, epoch: 164, batch: 19, loss: 0.4776638150215149, acc: 85.9375, f1: 80.34227470252314, r: 0.6166892057628146
06/02/2019 10:13:07 step: 5437, epoch: 164, batch: 24, loss: 0.29319441318511963, acc: 92.1875, f1: 73.358206121364, r: 0.6298835424327511
06/02/2019 10:13:09 step: 5442, epoch: 164, batch: 29, loss: 0.3650384843349457, acc: 85.9375, f1: 73.54874377476523, r: 0.5824844915305115
06/02/2019 10:13:09 *** evaluating ***
06/02/2019 10:13:10 step: 165, epoch: 164, acc: 55.98290598290598, f1: 26.117223063413984, r: 0.33599028376171935
06/02/2019 10:13:10 *** epoch: 166 ***
06/02/2019 10:13:10 *** training ***
06/02/2019 10:13:11 step: 5450, epoch: 165, batch: 4, loss: 0.48428890109062195, acc: 76.5625, f1: 63.22687615264577, r: 0.6687913524366592
06/02/2019 10:13:12 step: 5455, epoch: 165, batch: 9, loss: 0.4075303077697754, acc: 82.8125, f1: 72.01490906754063, r: 0.6178356028175281
06/02/2019 10:13:13 step: 5460, epoch: 165, batch: 14, loss: 0.4783353805541992, acc: 79.6875, f1: 74.7470950102529, r: 0.5779461944807001
06/02/2019 10:13:14 step: 5465, epoch: 165, batch: 19, loss: 0.5848963856697083, acc: 73.4375, f1: 63.87018453391544, r: 0.6291524996380726
06/02/2019 10:13:16 step: 5470, epoch: 165, batch: 24, loss: 0.521487832069397, acc: 75.0, f1: 64.48696231304926, r: 0.511674369153021
06/02/2019 10:13:17 step: 5475, epoch: 165, batch: 29, loss: 0.3920183479785919, acc: 85.9375, f1: 61.15584006558951, r: 0.6052634297147147
06/02/2019 10:13:17 *** evaluating ***
06/02/2019 10:13:18 step: 166, epoch: 165, acc: 57.26495726495726, f1: 26.80971146476363, r: 0.33333637107904357
06/02/2019 10:13:18 *** epoch: 167 ***
06/02/2019 10:13:18 *** training ***
06/02/2019 10:13:19 step: 5483, epoch: 166, batch: 4, loss: 0.35970306396484375, acc: 87.5, f1: 73.88847940677208, r: 0.6518443144856587
06/02/2019 10:13:20 step: 5488, epoch: 166, batch: 9, loss: 0.5719529390335083, acc: 76.5625, f1: 60.41561802586648, r: 0.4992552670907244
06/02/2019 10:13:21 step: 5493, epoch: 166, batch: 14, loss: 0.7393421530723572, acc: 67.1875, f1: 49.235440649614084, r: 0.5411931476199979
06/02/2019 10:13:22 step: 5498, epoch: 166, batch: 19, loss: 0.5007312893867493, acc: 82.8125, f1: 63.116002214839426, r: 0.6077148866919305
06/02/2019 10:13:24 step: 5503, epoch: 166, batch: 24, loss: 0.52301025390625, acc: 78.125, f1: 59.38578329882678, r: 0.6058396018297082
06/02/2019 10:13:25 step: 5508, epoch: 166, batch: 29, loss: 0.3809817135334015, acc: 90.625, f1: 72.04738392238393, r: 0.6232762208977927
06/02/2019 10:13:25 *** evaluating ***
06/02/2019 10:13:26 step: 167, epoch: 166, acc: 52.13675213675214, f1: 25.47616648938746, r: 0.3269910191311061
06/02/2019 10:13:26 *** epoch: 168 ***
06/02/2019 10:13:26 *** training ***
06/02/2019 10:13:27 step: 5516, epoch: 167, batch: 4, loss: 0.4469131529331207, acc: 81.25, f1: 75.91830095733803, r: 0.6061296043898459
06/02/2019 10:13:28 step: 5521, epoch: 167, batch: 9, loss: 0.4500308036804199, acc: 84.375, f1: 63.27334570191713, r: 0.6155867471099664
06/02/2019 10:13:29 step: 5526, epoch: 167, batch: 14, loss: 0.44353413581848145, acc: 78.125, f1: 51.13154960981048, r: 0.6183662451075438
06/02/2019 10:13:30 step: 5531, epoch: 167, batch: 19, loss: 0.4163760244846344, acc: 87.5, f1: 73.71407317238113, r: 0.6318597256221558
06/02/2019 10:13:32 step: 5536, epoch: 167, batch: 24, loss: 0.4216446876525879, acc: 84.375, f1: 70.5232746181022, r: 0.5548310169840366
06/02/2019 10:13:33 step: 5541, epoch: 167, batch: 29, loss: 0.5210143327713013, acc: 78.125, f1: 52.92610837438424, r: 0.6163987516226959
06/02/2019 10:13:33 *** evaluating ***
06/02/2019 10:13:34 step: 168, epoch: 167, acc: 57.26495726495726, f1: 26.86607932297587, r: 0.3239198610792917
06/02/2019 10:13:34 *** epoch: 169 ***
06/02/2019 10:13:34 *** training ***
06/02/2019 10:13:35 step: 5549, epoch: 168, batch: 4, loss: 0.587427020072937, acc: 75.0, f1: 65.24585921325051, r: 0.6638154102096449
06/02/2019 10:13:36 step: 5554, epoch: 168, batch: 9, loss: 0.4768686592578888, acc: 82.8125, f1: 58.93453107738822, r: 0.648383411198544
06/02/2019 10:13:37 step: 5559, epoch: 168, batch: 14, loss: 0.36474764347076416, acc: 87.5, f1: 82.87157287157288, r: 0.6012839573331625
06/02/2019 10:13:38 step: 5564, epoch: 168, batch: 19, loss: 0.3310094475746155, acc: 85.9375, f1: 68.70701384001876, r: 0.5922742063421278
06/02/2019 10:13:39 step: 5569, epoch: 168, batch: 24, loss: 0.5632249116897583, acc: 78.125, f1: 44.957279562542716, r: 0.580239929581171
06/02/2019 10:13:40 step: 5574, epoch: 168, batch: 29, loss: 0.33048713207244873, acc: 85.9375, f1: 58.896538780001606, r: 0.5871461060400343
06/02/2019 10:13:41 *** evaluating ***
06/02/2019 10:13:41 step: 169, epoch: 168, acc: 55.12820512820513, f1: 33.66360242570746, r: 0.339509177476652
06/02/2019 10:13:41 *** epoch: 170 ***
06/02/2019 10:13:41 *** training ***
06/02/2019 10:13:43 step: 5582, epoch: 169, batch: 4, loss: 0.5861462354660034, acc: 76.5625, f1: 50.498746536482386, r: 0.6069168856248002
06/02/2019 10:13:44 step: 5587, epoch: 169, batch: 9, loss: 0.3975360095500946, acc: 79.6875, f1: 61.7311260168403, r: 0.5949269636051241
06/02/2019 10:13:45 step: 5592, epoch: 169, batch: 14, loss: 0.6670470833778381, acc: 76.5625, f1: 66.086877214926, r: 0.5382257640871977
06/02/2019 10:13:46 step: 5597, epoch: 169, batch: 19, loss: 0.38785427808761597, acc: 87.5, f1: 75.0984604368063, r: 0.5722714997602198
06/02/2019 10:13:47 step: 5602, epoch: 169, batch: 24, loss: 0.42408397793769836, acc: 81.25, f1: 50.60526315789473, r: 0.5674926514356713
06/02/2019 10:13:48 step: 5607, epoch: 169, batch: 29, loss: 0.4468689560890198, acc: 79.6875, f1: 53.19444444444446, r: 0.6771354136657014
06/02/2019 10:13:49 *** evaluating ***
06/02/2019 10:13:49 step: 170, epoch: 169, acc: 57.692307692307686, f1: 29.112111522542094, r: 0.3514623770329817
06/02/2019 10:13:49 *** epoch: 171 ***
06/02/2019 10:13:49 *** training ***
06/02/2019 10:13:51 step: 5615, epoch: 170, batch: 4, loss: 0.5624969005584717, acc: 81.25, f1: 50.72389078600259, r: 0.5213479274166533
06/02/2019 10:13:52 step: 5620, epoch: 170, batch: 9, loss: 0.4332655370235443, acc: 81.25, f1: 62.89728405297459, r: 0.6416290215199013
06/02/2019 10:13:53 step: 5625, epoch: 170, batch: 14, loss: 0.35399338603019714, acc: 85.9375, f1: 63.33333333333333, r: 0.6049119466795572
06/02/2019 10:13:54 step: 5630, epoch: 170, batch: 19, loss: 0.5950614809989929, acc: 67.1875, f1: 43.403294023983676, r: 0.5026029408084476
06/02/2019 10:13:55 step: 5635, epoch: 170, batch: 24, loss: 0.3256395757198334, acc: 85.9375, f1: 58.42518227386648, r: 0.5475161002360716
06/02/2019 10:13:57 step: 5640, epoch: 170, batch: 29, loss: 0.3121357858181, acc: 85.9375, f1: 65.91036414565826, r: 0.6662774437108502
06/02/2019 10:13:57 *** evaluating ***
06/02/2019 10:13:57 step: 171, epoch: 170, acc: 55.98290598290598, f1: 27.423083646267287, r: 0.32919411563645223
06/02/2019 10:13:57 *** epoch: 172 ***
06/02/2019 10:13:57 *** training ***
06/02/2019 10:13:58 step: 5648, epoch: 171, batch: 4, loss: 0.42695289850234985, acc: 82.8125, f1: 59.27684117125109, r: 0.47223029097840485
06/02/2019 10:13:59 step: 5653, epoch: 171, batch: 9, loss: 0.4511919915676117, acc: 85.9375, f1: 67.21681096681095, r: 0.6527534631013384
06/02/2019 10:14:00 step: 5658, epoch: 171, batch: 14, loss: 0.5696873068809509, acc: 71.875, f1: 47.63203690288846, r: 0.5220008799181364
06/02/2019 10:14:02 step: 5663, epoch: 171, batch: 19, loss: 0.5244793891906738, acc: 79.6875, f1: 56.438318989855716, r: 0.5727025622179532
06/02/2019 10:14:03 step: 5668, epoch: 171, batch: 24, loss: 0.35564935207366943, acc: 89.0625, f1: 68.61998746867167, r: 0.6014340385450038
06/02/2019 10:14:04 step: 5673, epoch: 171, batch: 29, loss: 0.31060683727264404, acc: 87.5, f1: 81.77343981070689, r: 0.6445595677202252
06/02/2019 10:14:05 *** evaluating ***
06/02/2019 10:14:05 step: 172, epoch: 171, acc: 54.700854700854705, f1: 29.703896561538112, r: 0.34346208646365856
06/02/2019 10:14:05 *** epoch: 173 ***
06/02/2019 10:14:05 *** training ***
06/02/2019 10:14:06 step: 5681, epoch: 172, batch: 4, loss: 0.4262821674346924, acc: 87.5, f1: 70.33365301778598, r: 0.6430375885315308
06/02/2019 10:14:08 step: 5686, epoch: 172, batch: 9, loss: 0.5247682929039001, acc: 75.0, f1: 73.94943793120085, r: 0.6236847014324209
06/02/2019 10:14:09 step: 5691, epoch: 172, batch: 14, loss: 0.4414267838001251, acc: 82.8125, f1: 67.0172121785025, r: 0.5162461477272604
06/02/2019 10:14:10 step: 5696, epoch: 172, batch: 19, loss: 0.4542198181152344, acc: 79.6875, f1: 70.89771622934889, r: 0.5511410958913825
06/02/2019 10:14:11 step: 5701, epoch: 172, batch: 24, loss: 0.4414275288581848, acc: 84.375, f1: 68.92914653784219, r: 0.5783544316578312
06/02/2019 10:14:12 step: 5706, epoch: 172, batch: 29, loss: 0.39705872535705566, acc: 81.25, f1: 52.83837359395168, r: 0.6930272492589693
06/02/2019 10:14:13 *** evaluating ***
06/02/2019 10:14:13 step: 173, epoch: 172, acc: 55.98290598290598, f1: 28.17444953645175, r: 0.3263356477884661
06/02/2019 10:14:13 *** epoch: 174 ***
06/02/2019 10:14:13 *** training ***
06/02/2019 10:14:14 step: 5714, epoch: 173, batch: 4, loss: 0.47239989042282104, acc: 75.0, f1: 46.860457414702694, r: 0.6137775918276847
06/02/2019 10:14:15 step: 5719, epoch: 173, batch: 9, loss: 0.2985909581184387, acc: 87.5, f1: 82.06671799154446, r: 0.6228416539033719
06/02/2019 10:14:16 step: 5724, epoch: 173, batch: 14, loss: 0.5288087725639343, acc: 81.25, f1: 59.9404761904762, r: 0.621024590082562
06/02/2019 10:14:18 step: 5729, epoch: 173, batch: 19, loss: 0.43351420760154724, acc: 82.8125, f1: 60.49978004022122, r: 0.6448808912159198
06/02/2019 10:14:19 step: 5734, epoch: 173, batch: 24, loss: 0.3746170401573181, acc: 85.9375, f1: 58.35228189001774, r: 0.6200597739110871
06/02/2019 10:14:20 step: 5739, epoch: 173, batch: 29, loss: 0.33493590354919434, acc: 90.625, f1: 85.19223173140223, r: 0.6430383552749533
06/02/2019 10:14:20 *** evaluating ***
06/02/2019 10:14:21 step: 174, epoch: 173, acc: 50.85470085470085, f1: 24.869055668244307, r: 0.3203082411159347
06/02/2019 10:14:21 *** epoch: 175 ***
06/02/2019 10:14:21 *** training ***
06/02/2019 10:14:22 step: 5747, epoch: 174, batch: 4, loss: 0.32664984464645386, acc: 90.625, f1: 83.03801194901898, r: 0.6140480619406196
06/02/2019 10:14:23 step: 5752, epoch: 174, batch: 9, loss: 0.6574265956878662, acc: 68.75, f1: 57.1582203941008, r: 0.5827059684599317
06/02/2019 10:14:24 step: 5757, epoch: 174, batch: 14, loss: 0.4554387032985687, acc: 84.375, f1: 64.46056547619048, r: 0.5993006679520287
06/02/2019 10:14:25 step: 5762, epoch: 174, batch: 19, loss: 0.4321516454219818, acc: 78.125, f1: 53.59245013794991, r: 0.5355234361866092
06/02/2019 10:14:27 step: 5767, epoch: 174, batch: 24, loss: 0.4511360228061676, acc: 81.25, f1: 69.9611617696724, r: 0.6274047244210045
06/02/2019 10:14:28 step: 5772, epoch: 174, batch: 29, loss: 0.49858367443084717, acc: 78.125, f1: 46.94933659099392, r: 0.5952101881310925
06/02/2019 10:14:28 *** evaluating ***
06/02/2019 10:14:29 step: 175, epoch: 174, acc: 55.55555555555556, f1: 27.07196991774715, r: 0.33047020643281716
06/02/2019 10:14:29 *** epoch: 176 ***
06/02/2019 10:14:29 *** training ***
06/02/2019 10:14:30 step: 5780, epoch: 175, batch: 4, loss: 0.43526288866996765, acc: 81.25, f1: 67.10268445322794, r: 0.6113953381423531
06/02/2019 10:14:31 step: 5785, epoch: 175, batch: 9, loss: 0.3427155315876007, acc: 87.5, f1: 69.93081220053988, r: 0.5688154274904943
06/02/2019 10:14:32 step: 5790, epoch: 175, batch: 14, loss: 0.34859487414360046, acc: 81.25, f1: 53.87061917674163, r: 0.5022734742261431
06/02/2019 10:14:34 step: 5795, epoch: 175, batch: 19, loss: 0.2997177243232727, acc: 87.5, f1: 84.10923789805778, r: 0.6405411445667594
06/02/2019 10:14:35 step: 5800, epoch: 175, batch: 24, loss: 0.2419021725654602, acc: 90.625, f1: 70.74866310160428, r: 0.5974594469578002
06/02/2019 10:14:36 step: 5805, epoch: 175, batch: 29, loss: 0.40148207545280457, acc: 87.5, f1: 62.94818512689356, r: 0.6571502600848691
06/02/2019 10:14:37 *** evaluating ***
06/02/2019 10:14:37 step: 176, epoch: 175, acc: 53.41880341880342, f1: 27.490734421062502, r: 0.3343809839288094
06/02/2019 10:14:37 *** epoch: 177 ***
06/02/2019 10:14:37 *** training ***
06/02/2019 10:14:38 step: 5813, epoch: 176, batch: 4, loss: 0.41487324237823486, acc: 82.8125, f1: 65.77873254564983, r: 0.5914284629717508
06/02/2019 10:14:39 step: 5818, epoch: 176, batch: 9, loss: 0.38353201746940613, acc: 85.9375, f1: 75.43576917257424, r: 0.6162653880161365
06/02/2019 10:14:40 step: 5823, epoch: 176, batch: 14, loss: 0.5115236043930054, acc: 81.25, f1: 57.24296536796537, r: 0.5946765483094224
06/02/2019 10:14:42 step: 5828, epoch: 176, batch: 19, loss: 0.4150644540786743, acc: 79.6875, f1: 69.17316017316017, r: 0.7121191844020006
06/02/2019 10:14:43 step: 5833, epoch: 176, batch: 24, loss: 0.3629354238510132, acc: 85.9375, f1: 80.80901177675372, r: 0.5644322688487802
06/02/2019 10:14:44 step: 5838, epoch: 176, batch: 29, loss: 0.5866420269012451, acc: 76.5625, f1: 61.61840075633178, r: 0.6647436868798045
06/02/2019 10:14:45 *** evaluating ***
06/02/2019 10:14:45 step: 177, epoch: 176, acc: 55.55555555555556, f1: 29.243662666703617, r: 0.3226886053217696
06/02/2019 10:14:45 *** epoch: 178 ***
06/02/2019 10:14:45 *** training ***
06/02/2019 10:14:46 step: 5846, epoch: 177, batch: 4, loss: 0.3462848365306854, acc: 85.9375, f1: 65.6618978000557, r: 0.6717300313423178
06/02/2019 10:14:47 step: 5851, epoch: 177, batch: 9, loss: 0.4149828851222992, acc: 84.375, f1: 62.2940947940948, r: 0.7012702775098769
06/02/2019 10:14:48 step: 5856, epoch: 177, batch: 14, loss: 0.44070959091186523, acc: 79.6875, f1: 56.062271062271066, r: 0.5631269481535852
06/02/2019 10:14:50 step: 5861, epoch: 177, batch: 19, loss: 0.3111425042152405, acc: 87.5, f1: 71.0268817204301, r: 0.6638417147478265
06/02/2019 10:14:51 step: 5866, epoch: 177, batch: 24, loss: 0.4244420528411865, acc: 84.375, f1: 72.71604938271604, r: 0.5774804518789878
06/02/2019 10:14:52 step: 5871, epoch: 177, batch: 29, loss: 0.42562469840049744, acc: 87.5, f1: 81.24090195518767, r: 0.5679564218094545
06/02/2019 10:14:53 *** evaluating ***
06/02/2019 10:14:53 step: 178, epoch: 177, acc: 55.55555555555556, f1: 27.323351428674115, r: 0.32066658471944987
06/02/2019 10:14:53 *** epoch: 179 ***
06/02/2019 10:14:53 *** training ***
06/02/2019 10:14:54 step: 5879, epoch: 178, batch: 4, loss: 0.40193048119544983, acc: 81.25, f1: 69.88082821781195, r: 0.6853353932894567
06/02/2019 10:14:55 step: 5884, epoch: 178, batch: 9, loss: 0.3217509984970093, acc: 85.9375, f1: 83.81287171609753, r: 0.6571573713286861
06/02/2019 10:14:56 step: 5889, epoch: 178, batch: 14, loss: 0.5240621566772461, acc: 76.5625, f1: 62.70243685057171, r: 0.6773294368456253
06/02/2019 10:14:57 step: 5894, epoch: 178, batch: 19, loss: 0.42989084124565125, acc: 78.125, f1: 51.8796992481203, r: 0.5859428578255931
06/02/2019 10:14:59 step: 5899, epoch: 178, batch: 24, loss: 0.4124315679073334, acc: 81.25, f1: 67.52660675363049, r: 0.603102877738599
06/02/2019 10:15:00 step: 5904, epoch: 178, batch: 29, loss: 0.4430031478404999, acc: 85.9375, f1: 68.3030523255814, r: 0.6233542851710284
06/02/2019 10:15:00 *** evaluating ***
06/02/2019 10:15:01 step: 179, epoch: 178, acc: 55.55555555555556, f1: 26.907137577869282, r: 0.3146670354084531
06/02/2019 10:15:01 *** epoch: 180 ***
06/02/2019 10:15:01 *** training ***
06/02/2019 10:15:02 step: 5912, epoch: 179, batch: 4, loss: 0.45831120014190674, acc: 79.6875, f1: 71.25050875050876, r: 0.5358797276591519
06/02/2019 10:15:03 step: 5917, epoch: 179, batch: 9, loss: 0.3606063425540924, acc: 85.9375, f1: 70.2139540026537, r: 0.6910316141753743
06/02/2019 10:15:04 step: 5922, epoch: 179, batch: 14, loss: 0.4786202013492584, acc: 81.25, f1: 55.945352202145685, r: 0.6610834029871001
06/02/2019 10:15:05 step: 5927, epoch: 179, batch: 19, loss: 0.3607966899871826, acc: 85.9375, f1: 66.93126921387791, r: 0.6645542852639409
06/02/2019 10:15:07 step: 5932, epoch: 179, batch: 24, loss: 0.41859614849090576, acc: 87.5, f1: 71.07939679368252, r: 0.6039590844275509
06/02/2019 10:15:08 step: 5937, epoch: 179, batch: 29, loss: 0.49464109539985657, acc: 82.8125, f1: 63.15635806668804, r: 0.6151072942201701
06/02/2019 10:15:08 *** evaluating ***
06/02/2019 10:15:09 step: 180, epoch: 179, acc: 52.991452991452995, f1: 26.520138059798636, r: 0.3270997975503533
06/02/2019 10:15:09 *** epoch: 181 ***
06/02/2019 10:15:09 *** training ***
06/02/2019 10:15:10 step: 5945, epoch: 180, batch: 4, loss: 0.344083696603775, acc: 82.8125, f1: 72.23530437816152, r: 0.595951691677085
06/02/2019 10:15:11 step: 5950, epoch: 180, batch: 9, loss: 0.42108777165412903, acc: 82.8125, f1: 56.04391284815813, r: 0.5081004329877029
06/02/2019 10:15:12 step: 5955, epoch: 180, batch: 14, loss: 0.48060306906700134, acc: 73.4375, f1: 53.74415470836011, r: 0.5959780278524782
06/02/2019 10:15:14 step: 5960, epoch: 180, batch: 19, loss: 0.4209541082382202, acc: 76.5625, f1: 60.088435374149654, r: 0.5940441034384425
06/02/2019 10:15:15 step: 5965, epoch: 180, batch: 24, loss: 0.3693602383136749, acc: 89.0625, f1: 81.4021164021164, r: 0.641543372721986
06/02/2019 10:15:16 step: 5970, epoch: 180, batch: 29, loss: 0.539107620716095, acc: 75.0, f1: 52.49587789661319, r: 0.6444696063760992
06/02/2019 10:15:16 *** evaluating ***
06/02/2019 10:15:17 step: 181, epoch: 180, acc: 55.98290598290598, f1: 27.176424019426005, r: 0.31198045564561117
06/02/2019 10:15:17 *** epoch: 182 ***
06/02/2019 10:15:17 *** training ***
06/02/2019 10:15:18 step: 5978, epoch: 181, batch: 4, loss: 0.3719632029533386, acc: 84.375, f1: 66.87718590260964, r: 0.6710043293104916
06/02/2019 10:15:19 step: 5983, epoch: 181, batch: 9, loss: 0.38192006945610046, acc: 78.125, f1: 60.106207351970056, r: 0.6216317500674498
06/02/2019 10:15:20 step: 5988, epoch: 181, batch: 14, loss: 0.3069249093532562, acc: 90.625, f1: 73.6273620559335, r: 0.5760554371274671
06/02/2019 10:15:22 step: 5993, epoch: 181, batch: 19, loss: 0.24495945870876312, acc: 89.0625, f1: 60.40349143610013, r: 0.6394521762545311
06/02/2019 10:15:23 step: 5998, epoch: 181, batch: 24, loss: 0.3044166564941406, acc: 85.9375, f1: 76.2776407266203, r: 0.6176503418952717
06/02/2019 10:15:24 step: 6003, epoch: 181, batch: 29, loss: 0.32363343238830566, acc: 90.625, f1: 68.24675324675323, r: 0.5723332817816065
06/02/2019 10:15:24 *** evaluating ***
06/02/2019 10:15:25 step: 182, epoch: 181, acc: 50.427350427350426, f1: 26.115707209457206, r: 0.3117720233894075
06/02/2019 10:15:25 *** epoch: 183 ***
06/02/2019 10:15:25 *** training ***
06/02/2019 10:15:26 step: 6011, epoch: 182, batch: 4, loss: 0.43135637044906616, acc: 84.375, f1: 73.40142496392497, r: 0.699846919893739
06/02/2019 10:15:27 step: 6016, epoch: 182, batch: 9, loss: 0.4208448827266693, acc: 78.125, f1: 64.00074246197266, r: 0.5920418155808451
06/02/2019 10:15:28 step: 6021, epoch: 182, batch: 14, loss: 0.2741047143936157, acc: 89.0625, f1: 81.88308290349107, r: 0.6718581037398019
06/02/2019 10:15:30 step: 6026, epoch: 182, batch: 19, loss: 0.405769020318985, acc: 84.375, f1: 74.70619658119656, r: 0.6685393425433799
06/02/2019 10:15:31 step: 6031, epoch: 182, batch: 24, loss: 0.18680451810359955, acc: 96.875, f1: 92.76013385939555, r: 0.6622627575777356
06/02/2019 10:15:32 step: 6036, epoch: 182, batch: 29, loss: 0.5581279993057251, acc: 78.125, f1: 45.616883116883116, r: 0.4358672179790459
06/02/2019 10:15:32 *** evaluating ***
06/02/2019 10:15:33 step: 183, epoch: 182, acc: 55.55555555555556, f1: 25.43719026071967, r: 0.3143163639115298
06/02/2019 10:15:33 *** epoch: 184 ***
06/02/2019 10:15:33 *** training ***
06/02/2019 10:15:34 step: 6044, epoch: 183, batch: 4, loss: 0.40231895446777344, acc: 84.375, f1: 70.21490060555114, r: 0.5546856710621945
06/02/2019 10:15:35 step: 6049, epoch: 183, batch: 9, loss: 0.3583395183086395, acc: 89.0625, f1: 68.99591149591149, r: 0.6289307945831804
06/02/2019 10:15:36 step: 6054, epoch: 183, batch: 14, loss: 0.5430706143379211, acc: 76.5625, f1: 41.02742165242165, r: 0.48517513582822686
06/02/2019 10:15:38 step: 6059, epoch: 183, batch: 19, loss: 0.3725748062133789, acc: 84.375, f1: 68.28601953601954, r: 0.6696068876005574
06/02/2019 10:15:39 step: 6064, epoch: 183, batch: 24, loss: 0.3680734932422638, acc: 92.1875, f1: 89.28011643528885, r: 0.6644675152577236
06/02/2019 10:15:40 step: 6069, epoch: 183, batch: 29, loss: 0.30219489336013794, acc: 87.5, f1: 56.292684837092736, r: 0.6327879183588306
06/02/2019 10:15:40 *** evaluating ***
06/02/2019 10:15:41 step: 184, epoch: 183, acc: 55.55555555555556, f1: 25.98485499117485, r: 0.31747260677603384
06/02/2019 10:15:41 *** epoch: 185 ***
06/02/2019 10:15:41 *** training ***
06/02/2019 10:15:42 step: 6077, epoch: 184, batch: 4, loss: 0.3739129304885864, acc: 81.25, f1: 51.30615213485069, r: 0.5336268652787649
06/02/2019 10:15:43 step: 6082, epoch: 184, batch: 9, loss: 0.6346375346183777, acc: 71.875, f1: 53.62086584912671, r: 0.6314604504157216
06/02/2019 10:15:44 step: 6087, epoch: 184, batch: 14, loss: 0.21409091353416443, acc: 95.3125, f1: 94.12077201550886, r: 0.6014169238887183
06/02/2019 10:15:45 step: 6092, epoch: 184, batch: 19, loss: 0.25184518098831177, acc: 90.625, f1: 78.55389386430166, r: 0.6763049149244534
06/02/2019 10:15:47 step: 6097, epoch: 184, batch: 24, loss: 0.4082724153995514, acc: 84.375, f1: 73.86312100597816, r: 0.5800181029193303
06/02/2019 10:15:48 step: 6102, epoch: 184, batch: 29, loss: 0.49449723958969116, acc: 82.8125, f1: 58.12881562881563, r: 0.49839269670542835
06/02/2019 10:15:48 *** evaluating ***
06/02/2019 10:15:49 step: 185, epoch: 184, acc: 56.41025641025641, f1: 27.35500138159712, r: 0.32203167081788875
06/02/2019 10:15:49 *** epoch: 186 ***
06/02/2019 10:15:49 *** training ***
06/02/2019 10:15:50 step: 6110, epoch: 185, batch: 4, loss: 0.40372294187545776, acc: 82.8125, f1: 74.30586432859558, r: 0.5838231251670654
06/02/2019 10:15:51 step: 6115, epoch: 185, batch: 9, loss: 0.3880772590637207, acc: 81.25, f1: 61.561776782381486, r: 0.5592744569196889
06/02/2019 10:15:52 step: 6120, epoch: 185, batch: 14, loss: 0.36166295409202576, acc: 87.5, f1: 71.45176426426426, r: 0.6299358061617395
06/02/2019 10:15:54 step: 6125, epoch: 185, batch: 19, loss: 0.373404324054718, acc: 84.375, f1: 76.40591966173362, r: 0.5747107573054444
06/02/2019 10:15:55 step: 6130, epoch: 185, batch: 24, loss: 0.5531936883926392, acc: 75.0, f1: 53.89880952380952, r: 0.5808973853389657
06/02/2019 10:15:56 step: 6135, epoch: 185, batch: 29, loss: 0.27933773398399353, acc: 87.5, f1: 76.74709848096944, r: 0.690737181180596
06/02/2019 10:15:56 *** evaluating ***
06/02/2019 10:15:57 step: 186, epoch: 185, acc: 53.41880341880342, f1: 29.667446324230294, r: 0.32275770365787676
06/02/2019 10:15:57 *** epoch: 187 ***
06/02/2019 10:15:57 *** training ***
06/02/2019 10:15:58 step: 6143, epoch: 186, batch: 4, loss: 0.3779517710208893, acc: 84.375, f1: 66.03422619047619, r: 0.6219391461477654
06/02/2019 10:15:59 step: 6148, epoch: 186, batch: 9, loss: 0.3434775471687317, acc: 85.9375, f1: 71.56240818005524, r: 0.7485914666356399
06/02/2019 10:16:00 step: 6153, epoch: 186, batch: 14, loss: 0.3503033220767975, acc: 85.9375, f1: 60.96454079863691, r: 0.644057473867228
06/02/2019 10:16:01 step: 6158, epoch: 186, batch: 19, loss: 0.41558513045310974, acc: 84.375, f1: 76.0515873015873, r: 0.6039522352700631
06/02/2019 10:16:02 step: 6163, epoch: 186, batch: 24, loss: 0.38504403829574585, acc: 84.375, f1: 71.44601430315716, r: 0.5626852891534736
06/02/2019 10:16:04 step: 6168, epoch: 186, batch: 29, loss: 0.2780117690563202, acc: 85.9375, f1: 66.98099002446828, r: 0.6252161643455807
06/02/2019 10:16:04 *** evaluating ***
06/02/2019 10:16:05 step: 187, epoch: 186, acc: 48.717948717948715, f1: 32.31973607565581, r: 0.33897001650073516
06/02/2019 10:16:05 *** epoch: 188 ***
06/02/2019 10:16:05 *** training ***
06/02/2019 10:16:06 step: 6176, epoch: 187, batch: 4, loss: 0.47096991539001465, acc: 82.8125, f1: 62.34733390260679, r: 0.6192650768689479
06/02/2019 10:16:07 step: 6181, epoch: 187, batch: 9, loss: 0.3259904086589813, acc: 87.5, f1: 80.14670514670514, r: 0.6301237691537264
06/02/2019 10:16:08 step: 6186, epoch: 187, batch: 14, loss: 0.38614746928215027, acc: 85.9375, f1: 66.60714285714286, r: 0.57539257535933
06/02/2019 10:16:09 step: 6191, epoch: 187, batch: 19, loss: 0.22381575405597687, acc: 93.75, f1: 88.95333572025301, r: 0.595820038321943
06/02/2019 10:16:11 step: 6196, epoch: 187, batch: 24, loss: 0.36671873927116394, acc: 89.0625, f1: 67.07442067736186, r: 0.6020946438948744
06/02/2019 10:16:12 step: 6201, epoch: 187, batch: 29, loss: 0.4868450164794922, acc: 82.8125, f1: 61.737283880141014, r: 0.6244697950543352
06/02/2019 10:16:13 *** evaluating ***
06/02/2019 10:16:13 step: 188, epoch: 187, acc: 57.692307692307686, f1: 27.724475920269665, r: 0.30951693921219386
06/02/2019 10:16:13 *** epoch: 189 ***
06/02/2019 10:16:13 *** training ***
06/02/2019 10:16:14 step: 6209, epoch: 188, batch: 4, loss: 0.46566516160964966, acc: 81.25, f1: 61.09532828282829, r: 0.5534980391436528
06/02/2019 10:16:15 step: 6214, epoch: 188, batch: 9, loss: 0.34613531827926636, acc: 89.0625, f1: 83.52193149987268, r: 0.7107597612959833
06/02/2019 10:16:17 step: 6219, epoch: 188, batch: 14, loss: 0.3346564471721649, acc: 85.9375, f1: 50.83877995642702, r: 0.5689810564208759
06/02/2019 10:16:18 step: 6224, epoch: 188, batch: 19, loss: 0.4179207682609558, acc: 81.25, f1: 62.78114478114478, r: 0.6041217411278526
06/02/2019 10:16:19 step: 6229, epoch: 188, batch: 24, loss: 0.2831629514694214, acc: 89.0625, f1: 67.3439326630816, r: 0.6681703796062147
06/02/2019 10:16:20 step: 6234, epoch: 188, batch: 29, loss: 0.32239747047424316, acc: 92.1875, f1: 81.80518584245293, r: 0.6671693745133046
06/02/2019 10:16:21 *** evaluating ***
06/02/2019 10:16:21 step: 189, epoch: 188, acc: 55.55555555555556, f1: 28.017930137495355, r: 0.30240747384345334
06/02/2019 10:16:21 *** epoch: 190 ***
06/02/2019 10:16:21 *** training ***
06/02/2019 10:16:22 step: 6242, epoch: 189, batch: 4, loss: 0.4017282724380493, acc: 81.25, f1: 56.61949188277462, r: 0.6499167437307249
06/02/2019 10:16:24 step: 6247, epoch: 189, batch: 9, loss: 0.3976114094257355, acc: 82.8125, f1: 79.70467032967032, r: 0.6757222576586086
06/02/2019 10:16:25 step: 6252, epoch: 189, batch: 14, loss: 0.4494701623916626, acc: 79.6875, f1: 73.63095238095238, r: 0.6568677938375016
06/02/2019 10:16:26 step: 6257, epoch: 189, batch: 19, loss: 0.34240660071372986, acc: 85.9375, f1: 77.15248599439776, r: 0.7351677303414931
06/02/2019 10:16:27 step: 6262, epoch: 189, batch: 24, loss: 0.3380539119243622, acc: 85.9375, f1: 79.92703533026113, r: 0.6650191999228608
06/02/2019 10:16:28 step: 6267, epoch: 189, batch: 29, loss: 0.3706320524215698, acc: 85.9375, f1: 76.09293717486995, r: 0.7393709272798832
06/02/2019 10:16:29 *** evaluating ***
06/02/2019 10:16:29 step: 190, epoch: 189, acc: 55.55555555555556, f1: 24.63119535317811, r: 0.29732347405990667
06/02/2019 10:16:29 *** epoch: 191 ***
06/02/2019 10:16:29 *** training ***
06/02/2019 10:16:31 step: 6275, epoch: 190, batch: 4, loss: 0.3560856580734253, acc: 89.0625, f1: 85.17006802721089, r: 0.6048738403547004
06/02/2019 10:16:32 step: 6280, epoch: 190, batch: 9, loss: 0.4358014166355133, acc: 78.125, f1: 48.524011299435024, r: 0.6429716252732665
06/02/2019 10:16:33 step: 6285, epoch: 190, batch: 14, loss: 0.3440798819065094, acc: 85.9375, f1: 65.30448328062066, r: 0.6426153620135423
06/02/2019 10:16:34 step: 6290, epoch: 190, batch: 19, loss: 0.39892005920410156, acc: 85.9375, f1: 70.07287611061196, r: 0.6058865083934362
06/02/2019 10:16:35 step: 6295, epoch: 190, batch: 24, loss: 0.4175351858139038, acc: 81.25, f1: 63.974220224220225, r: 0.6662598410997184
06/02/2019 10:16:36 step: 6300, epoch: 190, batch: 29, loss: 0.3472863733768463, acc: 85.9375, f1: 77.64525229329482, r: 0.6469191864202992
06/02/2019 10:16:37 *** evaluating ***
06/02/2019 10:16:37 step: 191, epoch: 190, acc: 55.55555555555556, f1: 26.38812038900067, r: 0.29058423860513616
06/02/2019 10:16:37 *** epoch: 192 ***
06/02/2019 10:16:37 *** training ***
06/02/2019 10:16:39 step: 6308, epoch: 191, batch: 4, loss: 0.3538929224014282, acc: 84.375, f1: 70.20975056689342, r: 0.5782665643065914
06/02/2019 10:16:40 step: 6313, epoch: 191, batch: 9, loss: 0.43984612822532654, acc: 78.125, f1: 61.637761824097225, r: 0.5427433237638928
06/02/2019 10:16:41 step: 6318, epoch: 191, batch: 14, loss: 0.28343620896339417, acc: 95.3125, f1: 82.04371299198885, r: 0.6617250495638937
06/02/2019 10:16:42 step: 6323, epoch: 191, batch: 19, loss: 0.2895239293575287, acc: 84.375, f1: 71.58789697424356, r: 0.6594433909212125
06/02/2019 10:16:43 step: 6328, epoch: 191, batch: 24, loss: 0.4326482117176056, acc: 81.25, f1: 71.85243328100472, r: 0.6044029933158633
06/02/2019 10:16:44 step: 6333, epoch: 191, batch: 29, loss: 0.2699088752269745, acc: 85.9375, f1: 67.60714285714285, r: 0.6737794173590136
06/02/2019 10:16:45 *** evaluating ***
06/02/2019 10:16:45 step: 192, epoch: 191, acc: 57.26495726495726, f1: 27.625608062194402, r: 0.3058898142161065
06/02/2019 10:16:45 *** epoch: 193 ***
06/02/2019 10:16:45 *** training ***
06/02/2019 10:16:47 step: 6341, epoch: 192, batch: 4, loss: 0.43576180934906006, acc: 82.8125, f1: 80.56687047288553, r: 0.6563201181229884
06/02/2019 10:16:48 step: 6346, epoch: 192, batch: 9, loss: 0.3930240869522095, acc: 82.8125, f1: 62.97348484848485, r: 0.6405423335028811
06/02/2019 10:16:49 step: 6351, epoch: 192, batch: 14, loss: 0.3136346936225891, acc: 87.5, f1: 66.86343354546119, r: 0.5426749599336188
06/02/2019 10:16:50 step: 6356, epoch: 192, batch: 19, loss: 0.3025340139865875, acc: 85.9375, f1: 62.65873015873016, r: 0.5837953903169872
06/02/2019 10:16:51 step: 6361, epoch: 192, batch: 24, loss: 0.375696063041687, acc: 85.9375, f1: 66.93121693121694, r: 0.7298629485534756
06/02/2019 10:16:52 step: 6366, epoch: 192, batch: 29, loss: 0.4930751919746399, acc: 85.9375, f1: 74.67261904761907, r: 0.6697793238631822
06/02/2019 10:16:53 *** evaluating ***
06/02/2019 10:16:53 step: 193, epoch: 192, acc: 54.700854700854705, f1: 28.226985862434773, r: 0.30648668147614416
06/02/2019 10:16:53 *** epoch: 194 ***
06/02/2019 10:16:53 *** training ***
06/02/2019 10:16:54 step: 6374, epoch: 193, batch: 4, loss: 0.4382511377334595, acc: 82.8125, f1: 71.1058782487354, r: 0.5589195216186424
06/02/2019 10:16:56 step: 6379, epoch: 193, batch: 9, loss: 0.3196447193622589, acc: 87.5, f1: 64.25843253968254, r: 0.5996676643497846
06/02/2019 10:16:57 step: 6384, epoch: 193, batch: 14, loss: 0.34499919414520264, acc: 87.5, f1: 65.0455997869791, r: 0.6344169821286145
06/02/2019 10:16:58 step: 6389, epoch: 193, batch: 19, loss: 0.5732247233390808, acc: 75.0, f1: 61.71130952380952, r: 0.7086125250454834
06/02/2019 10:16:59 step: 6394, epoch: 193, batch: 24, loss: 0.2558998763561249, acc: 90.625, f1: 88.64669705963345, r: 0.683580470116145
06/02/2019 10:17:00 step: 6399, epoch: 193, batch: 29, loss: 0.25597599148750305, acc: 85.9375, f1: 71.57633545221367, r: 0.6093454108279103
06/02/2019 10:17:01 *** evaluating ***
06/02/2019 10:17:01 step: 194, epoch: 193, acc: 52.56410256410257, f1: 25.772449799030262, r: 0.3068151061505892
06/02/2019 10:17:01 *** epoch: 195 ***
06/02/2019 10:17:01 *** training ***
06/02/2019 10:17:02 step: 6407, epoch: 194, batch: 4, loss: 0.3054485619068146, acc: 89.0625, f1: 78.56524427953, r: 0.6129247715167124
06/02/2019 10:17:03 step: 6412, epoch: 194, batch: 9, loss: 0.3675430715084076, acc: 84.375, f1: 71.43055555555556, r: 0.7021027273690178
06/02/2019 10:17:05 step: 6417, epoch: 194, batch: 14, loss: 0.2742718458175659, acc: 89.0625, f1: 75.3469387755102, r: 0.6360199349795486
06/02/2019 10:17:06 step: 6422, epoch: 194, batch: 19, loss: 0.4103039503097534, acc: 82.8125, f1: 63.392648409451446, r: 0.6211005732489353
06/02/2019 10:17:07 step: 6427, epoch: 194, batch: 24, loss: 0.318050742149353, acc: 87.5, f1: 70.98214285714285, r: 0.6021707785179863
06/02/2019 10:17:08 step: 6432, epoch: 194, batch: 29, loss: 0.40750861167907715, acc: 81.25, f1: 69.28841991341992, r: 0.6550887510947613
06/02/2019 10:17:09 *** evaluating ***
06/02/2019 10:17:09 step: 195, epoch: 194, acc: 55.98290598290598, f1: 27.022307125996864, r: 0.30535922000421856
06/02/2019 10:17:09 *** epoch: 196 ***
06/02/2019 10:17:09 *** training ***
06/02/2019 10:17:10 step: 6440, epoch: 195, batch: 4, loss: 0.27473342418670654, acc: 89.0625, f1: 62.92435634540898, r: 0.6496342474959209
06/02/2019 10:17:11 step: 6445, epoch: 195, batch: 9, loss: 0.38899970054626465, acc: 85.9375, f1: 66.83490836958677, r: 0.6142036497356897
06/02/2019 10:17:12 step: 6450, epoch: 195, batch: 14, loss: 0.3504292070865631, acc: 82.8125, f1: 53.80475259507518, r: 0.6223511920886721
06/02/2019 10:17:13 step: 6455, epoch: 195, batch: 19, loss: 0.21806256473064423, acc: 90.625, f1: 72.56363506363506, r: 0.6258935476010958
06/02/2019 10:17:15 step: 6460, epoch: 195, batch: 24, loss: 0.22376962006092072, acc: 92.1875, f1: 75.07296466973887, r: 0.5928661415415182
06/02/2019 10:17:16 step: 6465, epoch: 195, batch: 29, loss: 0.3590882122516632, acc: 85.9375, f1: 67.24639490943838, r: 0.5962718434459646
06/02/2019 10:17:16 *** evaluating ***
06/02/2019 10:17:17 step: 196, epoch: 195, acc: 55.12820512820513, f1: 28.385647269660552, r: 0.32638827913762575
06/02/2019 10:17:17 *** epoch: 197 ***
06/02/2019 10:17:17 *** training ***
06/02/2019 10:17:18 step: 6473, epoch: 196, batch: 4, loss: 0.2954345643520355, acc: 84.375, f1: 66.2132758802509, r: 0.7472533196634594
06/02/2019 10:17:19 step: 6478, epoch: 196, batch: 9, loss: 0.3066883385181427, acc: 92.1875, f1: 86.50445393211048, r: 0.7553900529087315
06/02/2019 10:17:20 step: 6483, epoch: 196, batch: 14, loss: 0.28689926862716675, acc: 89.0625, f1: 71.0044341623289, r: 0.6632196813021912
06/02/2019 10:17:21 step: 6488, epoch: 196, batch: 19, loss: 0.35639023780822754, acc: 87.5, f1: 61.06646013560907, r: 0.587206097423893
06/02/2019 10:17:23 step: 6493, epoch: 196, batch: 24, loss: 0.1681787222623825, acc: 95.3125, f1: 95.6076087655035, r: 0.7077377607171623
06/02/2019 10:17:24 step: 6498, epoch: 196, batch: 29, loss: 0.4343365430831909, acc: 84.375, f1: 69.87585211723143, r: 0.6333293627735455
06/02/2019 10:17:24 *** evaluating ***
06/02/2019 10:17:25 step: 197, epoch: 196, acc: 54.700854700854705, f1: 30.660946196660475, r: 0.3275912566254526
06/02/2019 10:17:25 *** epoch: 198 ***
06/02/2019 10:17:25 *** training ***
06/02/2019 10:17:26 step: 6506, epoch: 197, batch: 4, loss: 0.22970600426197052, acc: 89.0625, f1: 82.23633509347795, r: 0.6061770617657332
06/02/2019 10:17:27 step: 6511, epoch: 197, batch: 9, loss: 0.24350818991661072, acc: 89.0625, f1: 62.64204545454546, r: 0.6208145073742285
06/02/2019 10:17:28 step: 6516, epoch: 197, batch: 14, loss: 0.36145415902137756, acc: 82.8125, f1: 68.92827306361141, r: 0.604877999502049
06/02/2019 10:17:29 step: 6521, epoch: 197, batch: 19, loss: 0.26696181297302246, acc: 89.0625, f1: 72.80874079837213, r: 0.7337020029601123
06/02/2019 10:17:31 step: 6526, epoch: 197, batch: 24, loss: 0.38605016469955444, acc: 87.5, f1: 66.59035409035408, r: 0.6118149993863469
06/02/2019 10:17:32 step: 6531, epoch: 197, batch: 29, loss: 0.21309277415275574, acc: 93.75, f1: 82.10811754168752, r: 0.6607119594190501
06/02/2019 10:17:33 *** evaluating ***
06/02/2019 10:17:33 step: 198, epoch: 197, acc: 55.98290598290598, f1: 29.507089763073807, r: 0.32121362420082605
06/02/2019 10:17:33 *** epoch: 199 ***
06/02/2019 10:17:33 *** training ***
06/02/2019 10:17:34 step: 6539, epoch: 198, batch: 4, loss: 0.4469394385814667, acc: 79.6875, f1: 67.91132478632478, r: 0.6961343373674159
06/02/2019 10:17:35 step: 6544, epoch: 198, batch: 9, loss: 0.390916645526886, acc: 82.8125, f1: 64.69820696640238, r: 0.698512565823777
06/02/2019 10:17:36 step: 6549, epoch: 198, batch: 14, loss: 0.42342740297317505, acc: 82.8125, f1: 71.85797046091164, r: 0.6704489043318516
06/02/2019 10:17:37 step: 6554, epoch: 198, batch: 19, loss: 0.2981342375278473, acc: 87.5, f1: 69.29160021265284, r: 0.7723425528748491
06/02/2019 10:17:39 step: 6559, epoch: 198, batch: 24, loss: 0.30709823966026306, acc: 84.375, f1: 68.58008658008657, r: 0.6046764974161862
06/02/2019 10:17:40 step: 6564, epoch: 198, batch: 29, loss: 0.262546181678772, acc: 90.625, f1: 62.918657013484605, r: 0.678945629444005
06/02/2019 10:17:41 *** evaluating ***
06/02/2019 10:17:41 step: 199, epoch: 198, acc: 55.12820512820513, f1: 29.286920674810425, r: 0.3174681300354706
06/02/2019 10:17:41 *** epoch: 200 ***
06/02/2019 10:17:41 *** training ***
06/02/2019 10:17:42 step: 6572, epoch: 199, batch: 4, loss: 0.4802986979484558, acc: 79.6875, f1: 70.38990705931496, r: 0.6329113851332587
06/02/2019 10:17:43 step: 6577, epoch: 199, batch: 9, loss: 0.3472711145877838, acc: 89.0625, f1: 80.31212484993998, r: 0.5897481926172138
06/02/2019 10:17:44 step: 6582, epoch: 199, batch: 14, loss: 0.30092546343803406, acc: 84.375, f1: 72.28455754771545, r: 0.5929837766526139
06/02/2019 10:17:46 step: 6587, epoch: 199, batch: 19, loss: 0.24536846578121185, acc: 89.0625, f1: 73.78954167874045, r: 0.6064998939430376
06/02/2019 10:17:47 step: 6592, epoch: 199, batch: 24, loss: 0.3160424530506134, acc: 87.5, f1: 68.23555867907048, r: 0.5997085861811131
06/02/2019 10:17:48 step: 6597, epoch: 199, batch: 29, loss: 0.3329116404056549, acc: 81.25, f1: 54.89417989417989, r: 0.5774366879093488
06/02/2019 10:17:49 *** evaluating ***
06/02/2019 10:17:49 step: 200, epoch: 199, acc: 51.70940170940172, f1: 26.797243475526678, r: 0.30361466390502834
06/02/2019 10:17:49 *** epoch: 201 ***
06/02/2019 10:17:49 *** training ***
06/02/2019 10:17:50 step: 6605, epoch: 200, batch: 4, loss: 0.2854241132736206, acc: 89.0625, f1: 72.19418676561533, r: 0.5769779744604929
06/02/2019 10:17:52 step: 6610, epoch: 200, batch: 9, loss: 0.39736461639404297, acc: 85.9375, f1: 70.01262043278851, r: 0.5961122176505004
06/02/2019 10:17:53 step: 6615, epoch: 200, batch: 14, loss: 0.2773367464542389, acc: 92.1875, f1: 83.18384933291766, r: 0.6645332167059765
06/02/2019 10:17:54 step: 6620, epoch: 200, batch: 19, loss: 0.24960669875144958, acc: 95.3125, f1: 92.47933019361591, r: 0.6199535972063136
06/02/2019 10:17:55 step: 6625, epoch: 200, batch: 24, loss: 0.22592908143997192, acc: 92.1875, f1: 93.95271395271395, r: 0.6462116674839197
06/02/2019 10:17:56 step: 6630, epoch: 200, batch: 29, loss: 0.2765181362628937, acc: 92.1875, f1: 85.76613923161239, r: 0.6978885213587949
06/02/2019 10:17:57 *** evaluating ***
06/02/2019 10:17:57 step: 201, epoch: 200, acc: 56.41025641025641, f1: 29.423839771843408, r: 0.3133537474412615
06/02/2019 10:17:57 *** epoch: 202 ***
06/02/2019 10:17:57 *** training ***
06/02/2019 10:17:59 step: 6638, epoch: 201, batch: 4, loss: 0.3179723918437958, acc: 87.5, f1: 85.78332586953277, r: 0.6086979900659252
06/02/2019 10:18:00 step: 6643, epoch: 201, batch: 9, loss: 0.4864572286605835, acc: 81.25, f1: 60.54576785886435, r: 0.5401107038404561
06/02/2019 10:18:01 step: 6648, epoch: 201, batch: 14, loss: 0.353423535823822, acc: 82.8125, f1: 64.80134932533733, r: 0.7298697867005298
06/02/2019 10:18:02 step: 6653, epoch: 201, batch: 19, loss: 0.2434418946504593, acc: 90.625, f1: 63.25900007254749, r: 0.5523310104369339
06/02/2019 10:18:03 step: 6658, epoch: 201, batch: 24, loss: 0.2620457410812378, acc: 89.0625, f1: 66.07434640522875, r: 0.7186002784404332
06/02/2019 10:18:05 step: 6663, epoch: 201, batch: 29, loss: 0.3230540156364441, acc: 85.9375, f1: 76.72088532382651, r: 0.6972148316443179
06/02/2019 10:18:05 *** evaluating ***
06/02/2019 10:18:06 step: 202, epoch: 201, acc: 55.55555555555556, f1: 30.034491808870932, r: 0.3103243473196005
06/02/2019 10:18:06 *** epoch: 203 ***
06/02/2019 10:18:06 *** training ***
06/02/2019 10:18:07 step: 6671, epoch: 202, batch: 4, loss: 0.2605779767036438, acc: 90.625, f1: 75.64680933101985, r: 0.6074151446442085
06/02/2019 10:18:08 step: 6676, epoch: 202, batch: 9, loss: 0.27596646547317505, acc: 89.0625, f1: 71.48150030621568, r: 0.7294644888449375
06/02/2019 10:18:09 step: 6681, epoch: 202, batch: 14, loss: 0.2806662321090698, acc: 85.9375, f1: 81.7687074829932, r: 0.6766122199999863
06/02/2019 10:18:10 step: 6686, epoch: 202, batch: 19, loss: 0.2327415943145752, acc: 92.1875, f1: 91.41226301841492, r: 0.6972554757536324
06/02/2019 10:18:12 step: 6691, epoch: 202, batch: 24, loss: 0.3951922357082367, acc: 82.8125, f1: 68.08988145300172, r: 0.7046019582328211
06/02/2019 10:18:13 step: 6696, epoch: 202, batch: 29, loss: 0.4149750769138336, acc: 84.375, f1: 77.53781544542414, r: 0.7202335885935919
06/02/2019 10:18:13 *** evaluating ***
06/02/2019 10:18:14 step: 203, epoch: 202, acc: 57.26495726495726, f1: 28.506842539334286, r: 0.3010496427938885
06/02/2019 10:18:14 *** epoch: 204 ***
06/02/2019 10:18:14 *** training ***
06/02/2019 10:18:15 step: 6704, epoch: 203, batch: 4, loss: 0.3448486924171448, acc: 89.0625, f1: 71.49398220826792, r: 0.6381681809303864
06/02/2019 10:18:16 step: 6709, epoch: 203, batch: 9, loss: 0.342468798160553, acc: 87.5, f1: 72.01254414809401, r: 0.711618717671031
06/02/2019 10:18:17 step: 6714, epoch: 203, batch: 14, loss: 0.33152762055397034, acc: 87.5, f1: 72.25885225885226, r: 0.7212919839255949
06/02/2019 10:18:18 step: 6719, epoch: 203, batch: 19, loss: 0.39667484164237976, acc: 85.9375, f1: 74.3344004268374, r: 0.6342143270617672
06/02/2019 10:18:20 step: 6724, epoch: 203, batch: 24, loss: 0.2450747787952423, acc: 93.75, f1: 55.01470588235294, r: 0.49362198681086594
06/02/2019 10:18:21 step: 6729, epoch: 203, batch: 29, loss: 0.21140076220035553, acc: 90.625, f1: 79.19270833333333, r: 0.7434758888679092
06/02/2019 10:18:21 *** evaluating ***
06/02/2019 10:18:22 step: 204, epoch: 203, acc: 56.41025641025641, f1: 28.302164554905783, r: 0.2987424612360651
06/02/2019 10:18:22 *** epoch: 205 ***
06/02/2019 10:18:22 *** training ***
06/02/2019 10:18:23 step: 6737, epoch: 204, batch: 4, loss: 0.3060837388038635, acc: 85.9375, f1: 62.07411067193676, r: 0.6967342904843293
06/02/2019 10:18:24 step: 6742, epoch: 204, batch: 9, loss: 0.3764190673828125, acc: 85.9375, f1: 66.70279205561464, r: 0.5960950203846168
06/02/2019 10:18:25 step: 6747, epoch: 204, batch: 14, loss: 0.3117619752883911, acc: 85.9375, f1: 71.875, r: 0.7043473979183353
06/02/2019 10:18:26 step: 6752, epoch: 204, batch: 19, loss: 0.21001586318016052, acc: 89.0625, f1: 77.91348203772706, r: 0.5895915549425055
06/02/2019 10:18:27 step: 6757, epoch: 204, batch: 24, loss: 0.4324442148208618, acc: 90.625, f1: 84.03101509557003, r: 0.6794000084408948
06/02/2019 10:18:28 step: 6762, epoch: 204, batch: 29, loss: 0.25304296612739563, acc: 89.0625, f1: 80.6686765615337, r: 0.6471831831022398
06/02/2019 10:18:29 *** evaluating ***
06/02/2019 10:18:29 step: 205, epoch: 204, acc: 54.27350427350427, f1: 28.685798262967154, r: 0.3133627682337512
06/02/2019 10:18:29 *** epoch: 206 ***
06/02/2019 10:18:29 *** training ***
06/02/2019 10:18:31 step: 6770, epoch: 205, batch: 4, loss: 0.26750415563583374, acc: 95.3125, f1: 90.43073833306909, r: 0.6261918512671851
06/02/2019 10:18:32 step: 6775, epoch: 205, batch: 9, loss: 0.4851962625980377, acc: 81.25, f1: 74.90033141348931, r: 0.6673555625915921
06/02/2019 10:18:33 step: 6780, epoch: 205, batch: 14, loss: 0.31926342844963074, acc: 87.5, f1: 72.20177337282601, r: 0.6858026339809661
06/02/2019 10:18:34 step: 6785, epoch: 205, batch: 19, loss: 0.20997321605682373, acc: 93.75, f1: 92.79870568917222, r: 0.6885762414297344
06/02/2019 10:18:35 step: 6790, epoch: 205, batch: 24, loss: 0.2320617437362671, acc: 90.625, f1: 90.04140786749483, r: 0.6268731574520873
06/02/2019 10:18:36 step: 6795, epoch: 205, batch: 29, loss: 0.30896154046058655, acc: 90.625, f1: 73.9294733044733, r: 0.6575009725811451
06/02/2019 10:18:37 *** evaluating ***
06/02/2019 10:18:37 step: 206, epoch: 205, acc: 57.26495726495726, f1: 29.75502128252382, r: 0.3061742316839971
06/02/2019 10:18:37 *** epoch: 207 ***
06/02/2019 10:18:37 *** training ***
06/02/2019 10:18:38 step: 6803, epoch: 206, batch: 4, loss: 0.1919972449541092, acc: 92.1875, f1: 84.677841359869, r: 0.6609271046160088
06/02/2019 10:18:40 step: 6808, epoch: 206, batch: 9, loss: 0.426385760307312, acc: 82.8125, f1: 63.17496229260935, r: 0.6665620521470662
06/02/2019 10:18:41 step: 6813, epoch: 206, batch: 14, loss: 0.44972047209739685, acc: 79.6875, f1: 67.05278949524474, r: 0.6223699747896466
06/02/2019 10:18:42 step: 6818, epoch: 206, batch: 19, loss: 0.18884290754795074, acc: 95.3125, f1: 90.87719298245614, r: 0.5722494740913024
06/02/2019 10:18:43 step: 6823, epoch: 206, batch: 24, loss: 0.35778018832206726, acc: 89.0625, f1: 71.03625541125541, r: 0.6405188318368994
06/02/2019 10:18:44 step: 6828, epoch: 206, batch: 29, loss: 0.4600726366043091, acc: 82.8125, f1: 70.24962356484096, r: 0.6346369401904932
06/02/2019 10:18:45 *** evaluating ***
06/02/2019 10:18:46 step: 207, epoch: 206, acc: 55.98290598290598, f1: 27.96063790939912, r: 0.3050090365770065
06/02/2019 10:18:46 *** epoch: 208 ***
06/02/2019 10:18:46 *** training ***
06/02/2019 10:18:47 step: 6836, epoch: 207, batch: 4, loss: 0.3269040286540985, acc: 87.5, f1: 80.2314553793728, r: 0.6429403430490366
06/02/2019 10:18:48 step: 6841, epoch: 207, batch: 9, loss: 0.2721087336540222, acc: 87.5, f1: 72.26190476190474, r: 0.7527396037714478
06/02/2019 10:18:49 step: 6846, epoch: 207, batch: 14, loss: 0.20991484820842743, acc: 90.625, f1: 65.4491341991342, r: 0.635994432627986
06/02/2019 10:18:50 step: 6851, epoch: 207, batch: 19, loss: 0.4204965829849243, acc: 81.25, f1: 61.621549344375424, r: 0.5450091671034444
06/02/2019 10:18:52 step: 6856, epoch: 207, batch: 24, loss: 0.28973832726478577, acc: 95.3125, f1: 93.41240004773839, r: 0.7736436427981447
06/02/2019 10:18:53 step: 6861, epoch: 207, batch: 29, loss: 0.4093124270439148, acc: 87.5, f1: 62.92087542087542, r: 0.5531582894341888
06/02/2019 10:18:53 *** evaluating ***
06/02/2019 10:18:53 step: 208, epoch: 207, acc: 56.837606837606835, f1: 29.043501010736694, r: 0.30222344198494844
06/02/2019 10:18:53 *** epoch: 209 ***
06/02/2019 10:18:53 *** training ***
06/02/2019 10:18:55 step: 6869, epoch: 208, batch: 4, loss: 0.3099140226840973, acc: 89.0625, f1: 65.4592670401494, r: 0.6386865968059313
06/02/2019 10:18:56 step: 6874, epoch: 208, batch: 9, loss: 0.3093315362930298, acc: 85.9375, f1: 66.1137727448703, r: 0.6413163696221678
06/02/2019 10:18:57 step: 6879, epoch: 208, batch: 14, loss: 0.3102705478668213, acc: 89.0625, f1: 69.78181239050805, r: 0.607192765491581
06/02/2019 10:18:58 step: 6884, epoch: 208, batch: 19, loss: 0.2791902720928192, acc: 89.0625, f1: 77.5174221656098, r: 0.5736497152560281
06/02/2019 10:18:59 step: 6889, epoch: 208, batch: 24, loss: 0.43618422746658325, acc: 84.375, f1: 74.66203502677668, r: 0.654846688747567
06/02/2019 10:19:00 step: 6894, epoch: 208, batch: 29, loss: 0.2743862271308899, acc: 92.1875, f1: 88.88652482269504, r: 0.6197747021355998
06/02/2019 10:19:01 *** evaluating ***
06/02/2019 10:19:02 step: 209, epoch: 208, acc: 57.26495726495726, f1: 29.2093515037594, r: 0.30618931321449616
06/02/2019 10:19:02 *** epoch: 210 ***
06/02/2019 10:19:02 *** training ***
06/02/2019 10:19:02 step: 6902, epoch: 209, batch: 4, loss: 0.25741589069366455, acc: 90.625, f1: 75.0784632034632, r: 0.6736643479100378
06/02/2019 10:19:04 step: 6907, epoch: 209, batch: 9, loss: 0.22512109577655792, acc: 92.1875, f1: 76.74653194611177, r: 0.7213445019875865
06/02/2019 10:19:05 step: 6912, epoch: 209, batch: 14, loss: 0.2853474020957947, acc: 82.8125, f1: 64.85225648335404, r: 0.7103396123603632
06/02/2019 10:19:06 step: 6917, epoch: 209, batch: 19, loss: 0.2511894404888153, acc: 90.625, f1: 69.70238095238095, r: 0.5998859370000533
06/02/2019 10:19:07 step: 6922, epoch: 209, batch: 24, loss: 0.3911118805408478, acc: 84.375, f1: 83.16502463054188, r: 0.6592258834586943
06/02/2019 10:19:09 step: 6927, epoch: 209, batch: 29, loss: 0.4525740444660187, acc: 81.25, f1: 67.14866660687093, r: 0.6041736200139789
06/02/2019 10:19:09 *** evaluating ***
06/02/2019 10:19:10 step: 210, epoch: 209, acc: 54.27350427350427, f1: 27.335094732137264, r: 0.30000877709447565
06/02/2019 10:19:10 *** epoch: 211 ***
06/02/2019 10:19:10 *** training ***
06/02/2019 10:19:11 step: 6935, epoch: 210, batch: 4, loss: 0.15231190621852875, acc: 92.1875, f1: 80.22222222222221, r: 0.6102324692168151
06/02/2019 10:19:12 step: 6940, epoch: 210, batch: 9, loss: 0.27575188875198364, acc: 87.5, f1: 85.12987012987014, r: 0.6171997652566013
06/02/2019 10:19:13 step: 6945, epoch: 210, batch: 14, loss: 0.3369464874267578, acc: 84.375, f1: 67.13258685241443, r: 0.6382988720354793
06/02/2019 10:19:14 step: 6950, epoch: 210, batch: 19, loss: 0.29977288842201233, acc: 87.5, f1: 72.84798534798536, r: 0.6823206963991485
06/02/2019 10:19:15 step: 6955, epoch: 210, batch: 24, loss: 0.20919311046600342, acc: 93.75, f1: 89.43644347371055, r: 0.7009552355041894
06/02/2019 10:19:17 step: 6960, epoch: 210, batch: 29, loss: 0.3184206187725067, acc: 84.375, f1: 72.8661948650428, r: 0.6459394486742203
06/02/2019 10:19:17 *** evaluating ***
06/02/2019 10:19:18 step: 211, epoch: 210, acc: 56.41025641025641, f1: 28.10411922024195, r: 0.2973763729218654
06/02/2019 10:19:18 *** epoch: 212 ***
06/02/2019 10:19:18 *** training ***
06/02/2019 10:19:19 step: 6968, epoch: 211, batch: 4, loss: 0.3506724536418915, acc: 82.8125, f1: 62.278159166031024, r: 0.6706386495225674
06/02/2019 10:19:20 step: 6973, epoch: 211, batch: 9, loss: 0.3420754075050354, acc: 84.375, f1: 57.34629235706823, r: 0.5158255751630227
06/02/2019 10:19:21 step: 6978, epoch: 211, batch: 14, loss: 0.1735987663269043, acc: 93.75, f1: 89.62061747776033, r: 0.5967617816821327
06/02/2019 10:19:22 step: 6983, epoch: 211, batch: 19, loss: 0.2417382299900055, acc: 87.5, f1: 84.55508640237703, r: 0.6602310940393101
06/02/2019 10:19:24 step: 6988, epoch: 211, batch: 24, loss: 0.16092684864997864, acc: 96.875, f1: 94.11764705882354, r: 0.6653032039714025
06/02/2019 10:19:25 step: 6993, epoch: 211, batch: 29, loss: 0.30686312913894653, acc: 90.625, f1: 73.48389355742297, r: 0.6771291235042907
06/02/2019 10:19:25 *** evaluating ***
06/02/2019 10:19:26 step: 212, epoch: 211, acc: 52.991452991452995, f1: 26.737229162720606, r: 0.2969339327344107
06/02/2019 10:19:26 *** epoch: 213 ***
06/02/2019 10:19:26 *** training ***
06/02/2019 10:19:27 step: 7001, epoch: 212, batch: 4, loss: 0.20693504810333252, acc: 92.1875, f1: 80.56684981684981, r: 0.7791560091433843
06/02/2019 10:19:28 step: 7006, epoch: 212, batch: 9, loss: 0.28289079666137695, acc: 92.1875, f1: 81.51414476995872, r: 0.6261630354085705
06/02/2019 10:19:29 step: 7011, epoch: 212, batch: 14, loss: 0.26814988255500793, acc: 87.5, f1: 67.6830808080808, r: 0.6957257745195683
06/02/2019 10:19:30 step: 7016, epoch: 212, batch: 19, loss: 0.17552244663238525, acc: 93.75, f1: 94.45373907138614, r: 0.7859123229077509
06/02/2019 10:19:32 step: 7021, epoch: 212, batch: 24, loss: 0.24568435549736023, acc: 92.1875, f1: 84.78678038379532, r: 0.6256894487126781
06/02/2019 10:19:33 step: 7026, epoch: 212, batch: 29, loss: 0.30258047580718994, acc: 89.0625, f1: 72.7470035633301, r: 0.6230845661674812
06/02/2019 10:19:33 *** evaluating ***
06/02/2019 10:19:34 step: 213, epoch: 212, acc: 56.41025641025641, f1: 27.93398006706417, r: 0.29639297840216094
06/02/2019 10:19:34 *** epoch: 214 ***
06/02/2019 10:19:34 *** training ***
06/02/2019 10:19:35 step: 7034, epoch: 213, batch: 4, loss: 0.26911962032318115, acc: 92.1875, f1: 88.22549458654935, r: 0.6471824034876672
06/02/2019 10:19:36 step: 7039, epoch: 213, batch: 9, loss: 0.4327365756034851, acc: 82.8125, f1: 57.99603174603174, r: 0.6312260563953912
06/02/2019 10:19:37 step: 7044, epoch: 213, batch: 14, loss: 0.2417270988225937, acc: 90.625, f1: 71.27502429543246, r: 0.5635505280286066
06/02/2019 10:19:39 step: 7049, epoch: 213, batch: 19, loss: 0.272309809923172, acc: 90.625, f1: 65.55258467023172, r: 0.6253110287213852
06/02/2019 10:19:40 step: 7054, epoch: 213, batch: 24, loss: 0.4801686406135559, acc: 84.375, f1: 76.88707729468601, r: 0.6416053146655584
06/02/2019 10:19:41 step: 7059, epoch: 213, batch: 29, loss: 0.19314169883728027, acc: 90.625, f1: 75.8189351498175, r: 0.7509626112873683
06/02/2019 10:19:42 *** evaluating ***
06/02/2019 10:19:42 step: 214, epoch: 213, acc: 56.837606837606835, f1: 29.025568181818183, r: 0.29699864380529073
06/02/2019 10:19:42 *** epoch: 215 ***
06/02/2019 10:19:42 *** training ***
06/02/2019 10:19:43 step: 7067, epoch: 214, batch: 4, loss: 0.32513219118118286, acc: 87.5, f1: 75.45808004052685, r: 0.6785819710321238
06/02/2019 10:19:44 step: 7072, epoch: 214, batch: 9, loss: 0.17550529539585114, acc: 93.75, f1: 77.51344086021506, r: 0.7226442470293266
06/02/2019 10:19:45 step: 7077, epoch: 214, batch: 14, loss: 0.2826690077781677, acc: 92.1875, f1: 75.50649350649351, r: 0.705590395601974
06/02/2019 10:19:47 step: 7082, epoch: 214, batch: 19, loss: 0.2647099196910858, acc: 85.9375, f1: 64.95790499822758, r: 0.7242634204139774
06/02/2019 10:19:48 step: 7087, epoch: 214, batch: 24, loss: 0.28825849294662476, acc: 85.9375, f1: 67.55210489993098, r: 0.7025734606176224
06/02/2019 10:19:49 step: 7092, epoch: 214, batch: 29, loss: 0.34978023171424866, acc: 85.9375, f1: 64.9234693877551, r: 0.7195015307983382
06/02/2019 10:19:50 *** evaluating ***
06/02/2019 10:19:50 step: 215, epoch: 214, acc: 55.98290598290598, f1: 27.455037617763573, r: 0.2925234597283944
06/02/2019 10:19:50 *** epoch: 216 ***
06/02/2019 10:19:50 *** training ***
06/02/2019 10:19:51 step: 7100, epoch: 215, batch: 4, loss: 0.20954790711402893, acc: 92.1875, f1: 91.64944374621795, r: 0.6663251390970527
06/02/2019 10:19:52 step: 7105, epoch: 215, batch: 9, loss: 0.26389482617378235, acc: 85.9375, f1: 60.82627517410126, r: 0.6553072329548271
06/02/2019 10:19:54 step: 7110, epoch: 215, batch: 14, loss: 0.27994707226753235, acc: 89.0625, f1: 87.02086746150968, r: 0.5866391108237842
06/02/2019 10:19:55 step: 7115, epoch: 215, batch: 19, loss: 0.303104043006897, acc: 82.8125, f1: 78.15155274832695, r: 0.756450888303243
06/02/2019 10:19:56 step: 7120, epoch: 215, batch: 24, loss: 0.36539632081985474, acc: 84.375, f1: 62.67247245508116, r: 0.6507090712654054
06/02/2019 10:19:57 step: 7125, epoch: 215, batch: 29, loss: 0.166981041431427, acc: 92.1875, f1: 88.39499688764394, r: 0.7484332914478428
06/02/2019 10:19:58 *** evaluating ***
06/02/2019 10:19:58 step: 216, epoch: 215, acc: 52.13675213675214, f1: 27.97372742200328, r: 0.328438752079841
06/02/2019 10:19:58 *** epoch: 217 ***
06/02/2019 10:19:58 *** training ***
06/02/2019 10:19:59 step: 7133, epoch: 216, batch: 4, loss: 0.18014374375343323, acc: 92.1875, f1: 80.83168751371517, r: 0.7120524365287825
06/02/2019 10:20:00 step: 7138, epoch: 216, batch: 9, loss: 0.2865491509437561, acc: 89.0625, f1: 70.74091478696742, r: 0.6589096508352645
06/02/2019 10:20:02 step: 7143, epoch: 216, batch: 14, loss: 0.30194541811943054, acc: 87.5, f1: 71.19630922372409, r: 0.6133369423765516
06/02/2019 10:20:03 step: 7148, epoch: 216, batch: 19, loss: 0.3183142840862274, acc: 93.75, f1: 93.13851917578626, r: 0.61058766248331
06/02/2019 10:20:04 step: 7153, epoch: 216, batch: 24, loss: 0.3184928894042969, acc: 85.9375, f1: 75.18490754622688, r: 0.6380168845922743
06/02/2019 10:20:05 step: 7158, epoch: 216, batch: 29, loss: 0.19229993224143982, acc: 90.625, f1: 80.86116264687693, r: 0.5713088386127148
06/02/2019 10:20:06 *** evaluating ***
06/02/2019 10:20:06 step: 217, epoch: 216, acc: 57.26495726495726, f1: 29.534012897857288, r: 0.305551534718729
06/02/2019 10:20:06 *** epoch: 218 ***
06/02/2019 10:20:06 *** training ***
06/02/2019 10:20:07 step: 7166, epoch: 217, batch: 4, loss: 0.3028208017349243, acc: 87.5, f1: 72.63850915166705, r: 0.6836397158249681
06/02/2019 10:20:09 step: 7171, epoch: 217, batch: 9, loss: 0.3417392075061798, acc: 87.5, f1: 65.99287808965228, r: 0.726394985217929
06/02/2019 10:20:10 step: 7176, epoch: 217, batch: 14, loss: 0.3929550349712372, acc: 81.25, f1: 68.75662714478503, r: 0.6637868748509019
06/02/2019 10:20:11 step: 7181, epoch: 217, batch: 19, loss: 0.3118656873703003, acc: 87.5, f1: 68.53470684509485, r: 0.6722594487466041
06/02/2019 10:20:12 step: 7186, epoch: 217, batch: 24, loss: 0.2348962128162384, acc: 89.0625, f1: 78.63378684807255, r: 0.5739201189066263
06/02/2019 10:20:13 step: 7191, epoch: 217, batch: 29, loss: 0.31467169523239136, acc: 92.1875, f1: 81.14348114348114, r: 0.6326406119912549
06/02/2019 10:20:14 *** evaluating ***
06/02/2019 10:20:14 step: 218, epoch: 217, acc: 55.98290598290598, f1: 28.62719298245614, r: 0.3037212310028781
06/02/2019 10:20:14 *** epoch: 219 ***
06/02/2019 10:20:14 *** training ***
06/02/2019 10:20:15 step: 7199, epoch: 218, batch: 4, loss: 0.24845513701438904, acc: 85.9375, f1: 69.15888686925064, r: 0.5113164809359634
06/02/2019 10:20:16 step: 7204, epoch: 218, batch: 9, loss: 0.3460230231285095, acc: 87.5, f1: 69.86998746867168, r: 0.7239001126520158
06/02/2019 10:20:17 step: 7209, epoch: 218, batch: 14, loss: 0.35067448019981384, acc: 85.9375, f1: 67.53590914538066, r: 0.7148022654025682
06/02/2019 10:20:19 step: 7214, epoch: 218, batch: 19, loss: 0.34525007009506226, acc: 85.9375, f1: 81.53712548849327, r: 0.6433963356335142
06/02/2019 10:20:20 step: 7219, epoch: 218, batch: 24, loss: 0.31773120164871216, acc: 81.25, f1: 65.49096880131361, r: 0.56019241026715
06/02/2019 10:20:21 step: 7224, epoch: 218, batch: 29, loss: 0.33082127571105957, acc: 82.8125, f1: 72.46598639455783, r: 0.6444266110300545
06/02/2019 10:20:22 *** evaluating ***
06/02/2019 10:20:22 step: 219, epoch: 218, acc: 57.26495726495726, f1: 29.41321290824171, r: 0.3034721459655888
06/02/2019 10:20:22 *** epoch: 220 ***
06/02/2019 10:20:22 *** training ***
06/02/2019 10:20:23 step: 7232, epoch: 219, batch: 4, loss: 0.31280413269996643, acc: 85.9375, f1: 66.48801080715975, r: 0.6882434830956556
06/02/2019 10:20:24 step: 7237, epoch: 219, batch: 9, loss: 0.40828219056129456, acc: 82.8125, f1: 76.13090690821782, r: 0.6302177284349627
06/02/2019 10:20:25 step: 7242, epoch: 219, batch: 14, loss: 0.23520155251026154, acc: 90.625, f1: 73.21428571428572, r: 0.7098214520501782
06/02/2019 10:20:26 step: 7247, epoch: 219, batch: 19, loss: 0.16880565881729126, acc: 93.75, f1: 88.6729544552744, r: 0.6887498512009332
06/02/2019 10:20:28 step: 7252, epoch: 219, batch: 24, loss: 0.2878338396549225, acc: 90.625, f1: 76.53554175293307, r: 0.6052372904523226
06/02/2019 10:20:29 step: 7257, epoch: 219, batch: 29, loss: 0.3018900156021118, acc: 84.375, f1: 57.97705314009662, r: 0.5999497762639723
06/02/2019 10:20:30 *** evaluating ***
06/02/2019 10:20:30 step: 220, epoch: 219, acc: 56.837606837606835, f1: 27.530689925292705, r: 0.2938036404010721
06/02/2019 10:20:30 *** epoch: 221 ***
06/02/2019 10:20:30 *** training ***
06/02/2019 10:20:31 step: 7265, epoch: 220, batch: 4, loss: 0.29409435391426086, acc: 85.9375, f1: 69.6576227390181, r: 0.6260353396918277
06/02/2019 10:20:32 step: 7270, epoch: 220, batch: 9, loss: 0.3355211019515991, acc: 89.0625, f1: 76.30179344465058, r: 0.6102980657399226
06/02/2019 10:20:33 step: 7275, epoch: 220, batch: 14, loss: 0.24030661582946777, acc: 87.5, f1: 78.8151134579706, r: 0.6459964890038712
06/02/2019 10:20:34 step: 7280, epoch: 220, batch: 19, loss: 0.19849896430969238, acc: 92.1875, f1: 83.56902356902357, r: 0.6227175575387565
06/02/2019 10:20:36 step: 7285, epoch: 220, batch: 24, loss: 0.1860629916191101, acc: 92.1875, f1: 78.69458156222862, r: 0.8013303564777968
06/02/2019 10:20:37 step: 7290, epoch: 220, batch: 29, loss: 0.20368391275405884, acc: 92.1875, f1: 77.38921957671958, r: 0.5530350459098652
06/02/2019 10:20:37 *** evaluating ***
06/02/2019 10:20:38 step: 221, epoch: 220, acc: 56.837606837606835, f1: 28.699614200367556, r: 0.3042470314356337
06/02/2019 10:20:38 *** epoch: 222 ***
06/02/2019 10:20:38 *** training ***
06/02/2019 10:20:39 step: 7298, epoch: 221, batch: 4, loss: 0.3273943066596985, acc: 82.8125, f1: 67.96448706332427, r: 0.6463777409982787
06/02/2019 10:20:40 step: 7303, epoch: 221, batch: 9, loss: 0.30436763167381287, acc: 89.0625, f1: 63.41796587055876, r: 0.6343755483815742
06/02/2019 10:20:41 step: 7308, epoch: 221, batch: 14, loss: 0.19180576503276825, acc: 92.1875, f1: 88.61091103107911, r: 0.7239076097299022
06/02/2019 10:20:43 step: 7313, epoch: 221, batch: 19, loss: 0.2279575616121292, acc: 87.5, f1: 63.389355742296914, r: 0.6841043908849245
06/02/2019 10:20:44 step: 7318, epoch: 221, batch: 24, loss: 0.19525228440761566, acc: 93.75, f1: 89.64349977507872, r: 0.5649378780411065
06/02/2019 10:20:45 step: 7323, epoch: 221, batch: 29, loss: 0.1877060979604721, acc: 92.1875, f1: 88.55652511196706, r: 0.6224392621618066
06/02/2019 10:20:46 *** evaluating ***
06/02/2019 10:20:46 step: 222, epoch: 221, acc: 58.97435897435898, f1: 29.83582345134638, r: 0.30670190674160447
06/02/2019 10:20:46 *** epoch: 223 ***
06/02/2019 10:20:46 *** training ***
06/02/2019 10:20:47 step: 7331, epoch: 222, batch: 4, loss: 0.4278830885887146, acc: 81.25, f1: 75.44581726952548, r: 0.6887604943161382
06/02/2019 10:20:48 step: 7336, epoch: 222, batch: 9, loss: 0.24003368616104126, acc: 87.5, f1: 75.36107326276073, r: 0.6989396145819798
06/02/2019 10:20:49 step: 7341, epoch: 222, batch: 14, loss: 0.20692043006420135, acc: 90.625, f1: 81.53370439084725, r: 0.6266155613317975
06/02/2019 10:20:50 step: 7346, epoch: 222, batch: 19, loss: 0.3525884747505188, acc: 87.5, f1: 77.78955314009663, r: 0.7089184535634938
06/02/2019 10:20:51 step: 7351, epoch: 222, batch: 24, loss: 0.18386560678482056, acc: 93.75, f1: 90.02634099616857, r: 0.7913932635265684
06/02/2019 10:20:53 step: 7356, epoch: 222, batch: 29, loss: 0.2557193636894226, acc: 87.5, f1: 80.51005109690054, r: 0.6180157079787371
06/02/2019 10:20:53 *** evaluating ***
06/02/2019 10:20:54 step: 223, epoch: 222, acc: 54.700854700854705, f1: 26.751008559123367, r: 0.2919671326672919
06/02/2019 10:20:54 *** epoch: 224 ***
06/02/2019 10:20:54 *** training ***
06/02/2019 10:20:55 step: 7364, epoch: 223, batch: 4, loss: 0.18975691497325897, acc: 92.1875, f1: 69.79461516407328, r: 0.5851614430033374
06/02/2019 10:20:56 step: 7369, epoch: 223, batch: 9, loss: 0.18729346990585327, acc: 93.75, f1: 76.28861003861005, r: 0.6147380857887333
06/02/2019 10:20:57 step: 7374, epoch: 223, batch: 14, loss: 0.14753645658493042, acc: 95.3125, f1: 92.04143475572046, r: 0.6224414502354751
06/02/2019 10:20:58 step: 7379, epoch: 223, batch: 19, loss: 0.29958000779151917, acc: 89.0625, f1: 83.76143560872625, r: 0.6050649238602198
06/02/2019 10:20:59 step: 7384, epoch: 223, batch: 24, loss: 0.29328811168670654, acc: 87.5, f1: 69.3503875157101, r: 0.661159500508586
06/02/2019 10:21:01 step: 7389, epoch: 223, batch: 29, loss: 0.29601365327835083, acc: 89.0625, f1: 72.29040404040404, r: 0.6363901134569029
06/02/2019 10:21:01 *** evaluating ***
06/02/2019 10:21:02 step: 224, epoch: 223, acc: 59.401709401709404, f1: 28.903786542675437, r: 0.29862591272410505
06/02/2019 10:21:02 *** epoch: 225 ***
06/02/2019 10:21:02 *** training ***
06/02/2019 10:21:03 step: 7397, epoch: 224, batch: 4, loss: 0.39271795749664307, acc: 84.375, f1: 69.57860376748322, r: 0.6827394201903553
06/02/2019 10:21:04 step: 7402, epoch: 224, batch: 9, loss: 0.34178197383880615, acc: 89.0625, f1: 72.2624269005848, r: 0.6284328945101786
06/02/2019 10:21:05 step: 7407, epoch: 224, batch: 14, loss: 0.29669317603111267, acc: 87.5, f1: 77.55345844631559, r: 0.6752817722677397
06/02/2019 10:21:06 step: 7412, epoch: 224, batch: 19, loss: 0.19498740136623383, acc: 92.1875, f1: 84.19095524358681, r: 0.5893493719788296
06/02/2019 10:21:07 step: 7417, epoch: 224, batch: 24, loss: 0.27688324451446533, acc: 90.625, f1: 79.03920153920154, r: 0.7136641134114764
06/02/2019 10:21:09 step: 7422, epoch: 224, batch: 29, loss: 0.38160067796707153, acc: 90.625, f1: 79.87768665158372, r: 0.6985970796526747
06/02/2019 10:21:10 *** evaluating ***
06/02/2019 10:21:10 step: 225, epoch: 224, acc: 58.119658119658126, f1: 29.684983534446292, r: 0.3102699451060965
06/02/2019 10:21:10 *** epoch: 226 ***
06/02/2019 10:21:10 *** training ***
06/02/2019 10:21:11 step: 7430, epoch: 225, batch: 4, loss: 0.2475339025259018, acc: 87.5, f1: 84.491341991342, r: 0.7419652259550584
06/02/2019 10:21:12 step: 7435, epoch: 225, batch: 9, loss: 0.2199169546365738, acc: 89.0625, f1: 84.10275977834036, r: 0.6345044348099633
06/02/2019 10:21:13 step: 7440, epoch: 225, batch: 14, loss: 0.25222232937812805, acc: 89.0625, f1: 76.15028434160212, r: 0.6949798484113671
06/02/2019 10:21:14 step: 7445, epoch: 225, batch: 19, loss: 0.24966856837272644, acc: 90.625, f1: 67.86063218390804, r: 0.6143822633143889
06/02/2019 10:21:15 step: 7450, epoch: 225, batch: 24, loss: 0.1810605674982071, acc: 93.75, f1: 78.11698717948718, r: 0.6913331711418342
06/02/2019 10:21:17 step: 7455, epoch: 225, batch: 29, loss: 0.16141867637634277, acc: 95.3125, f1: 96.38043541333671, r: 0.6800692790911962
06/02/2019 10:21:17 *** evaluating ***
06/02/2019 10:21:18 step: 226, epoch: 225, acc: 57.692307692307686, f1: 30.384364111498257, r: 0.3177419547500226
06/02/2019 10:21:18 *** epoch: 227 ***
06/02/2019 10:21:18 *** training ***
06/02/2019 10:21:19 step: 7463, epoch: 226, batch: 4, loss: 0.2257349044084549, acc: 92.1875, f1: 76.43074250976986, r: 0.6125466100966918
06/02/2019 10:21:20 step: 7468, epoch: 226, batch: 9, loss: 0.22525760531425476, acc: 89.0625, f1: 81.76809854376533, r: 0.6172592517319824
06/02/2019 10:21:21 step: 7473, epoch: 226, batch: 14, loss: 0.2945765554904938, acc: 84.375, f1: 57.95787545787545, r: 0.6152263778657724
06/02/2019 10:21:23 step: 7478, epoch: 226, batch: 19, loss: 0.26007214188575745, acc: 90.625, f1: 81.32506591153206, r: 0.6191229878034397
06/02/2019 10:21:24 step: 7483, epoch: 226, batch: 24, loss: 0.17440496385097504, acc: 92.1875, f1: 86.4387910939635, r: 0.6159228576575765
06/02/2019 10:21:25 step: 7488, epoch: 226, batch: 29, loss: 0.2428663820028305, acc: 90.625, f1: 77.65364417003762, r: 0.6509953418485059
06/02/2019 10:21:25 *** evaluating ***
06/02/2019 10:21:26 step: 227, epoch: 226, acc: 55.98290598290598, f1: 29.566138935663467, r: 0.32923336131464365
06/02/2019 10:21:26 *** epoch: 228 ***
06/02/2019 10:21:26 *** training ***
06/02/2019 10:21:27 step: 7496, epoch: 227, batch: 4, loss: 0.25741174817085266, acc: 89.0625, f1: 72.36658456486043, r: 0.6278489856785465
06/02/2019 10:21:28 step: 7501, epoch: 227, batch: 9, loss: 0.2811296284198761, acc: 90.625, f1: 70.92911877394637, r: 0.7199299533256633
06/02/2019 10:21:29 step: 7506, epoch: 227, batch: 14, loss: 0.17868241667747498, acc: 93.75, f1: 86.53451544552247, r: 0.6555148725582991
06/02/2019 10:21:31 step: 7511, epoch: 227, batch: 19, loss: 0.1922498643398285, acc: 92.1875, f1: 77.12828837828837, r: 0.6534251385656521
06/02/2019 10:21:32 step: 7516, epoch: 227, batch: 24, loss: 0.21853914856910706, acc: 89.0625, f1: 78.59323044105653, r: 0.7021155272320523
06/02/2019 10:21:33 step: 7521, epoch: 227, batch: 29, loss: 0.23182545602321625, acc: 89.0625, f1: 83.70733217134936, r: 0.6692328362287595
06/02/2019 10:21:33 *** evaluating ***
06/02/2019 10:21:34 step: 228, epoch: 227, acc: 55.98290598290598, f1: 27.246058189077534, r: 0.2941322987255218
06/02/2019 10:21:34 *** epoch: 229 ***
06/02/2019 10:21:34 *** training ***
06/02/2019 10:21:35 step: 7529, epoch: 228, batch: 4, loss: 0.15196722745895386, acc: 95.3125, f1: 94.56115779645192, r: 0.6325213872952
06/02/2019 10:21:36 step: 7534, epoch: 228, batch: 9, loss: 0.15978607535362244, acc: 92.1875, f1: 87.28269085411942, r: 0.7235072669815593
06/02/2019 10:21:37 step: 7539, epoch: 228, batch: 14, loss: 0.24817529320716858, acc: 90.625, f1: 67.0093537414966, r: 0.5838334662406565
06/02/2019 10:21:38 step: 7544, epoch: 228, batch: 19, loss: 0.2995310127735138, acc: 89.0625, f1: 84.90767735665695, r: 0.6476391467068439
06/02/2019 10:21:40 step: 7549, epoch: 228, batch: 24, loss: 0.24098087847232819, acc: 90.625, f1: 77.94696969696969, r: 0.6084796795828027
06/02/2019 10:21:41 step: 7554, epoch: 228, batch: 29, loss: 0.20191949605941772, acc: 96.875, f1: 96.35720601237843, r: 0.6322535626752392
06/02/2019 10:21:41 *** evaluating ***
06/02/2019 10:21:42 step: 229, epoch: 228, acc: 57.26495726495726, f1: 28.806939216853216, r: 0.30003759529926366
06/02/2019 10:21:42 *** epoch: 230 ***
06/02/2019 10:21:42 *** training ***
06/02/2019 10:21:43 step: 7562, epoch: 229, batch: 4, loss: 0.2485058307647705, acc: 89.0625, f1: 80.94499178981937, r: 0.7358852379187573
06/02/2019 10:21:44 step: 7567, epoch: 229, batch: 9, loss: 0.17607107758522034, acc: 93.75, f1: 88.32430739758325, r: 0.725484346653263
06/02/2019 10:21:45 step: 7572, epoch: 229, batch: 14, loss: 0.2271072417497635, acc: 92.1875, f1: 82.78846153846153, r: 0.6800743334850278
06/02/2019 10:21:46 step: 7577, epoch: 229, batch: 19, loss: 0.2503695487976074, acc: 90.625, f1: 76.59107402031931, r: 0.7317316089649709
06/02/2019 10:21:47 step: 7582, epoch: 229, batch: 24, loss: 0.24626785516738892, acc: 87.5, f1: 83.07107858128265, r: 0.650588295999768
06/02/2019 10:21:49 step: 7587, epoch: 229, batch: 29, loss: 0.3504859507083893, acc: 84.375, f1: 76.73076923076924, r: 0.5554737990783344
06/02/2019 10:21:49 *** evaluating ***
06/02/2019 10:21:50 step: 230, epoch: 229, acc: 57.26495726495726, f1: 27.602387978200394, r: 0.301877279376818
06/02/2019 10:21:50 *** epoch: 231 ***
06/02/2019 10:21:50 *** training ***
06/02/2019 10:21:51 step: 7595, epoch: 230, batch: 4, loss: 0.10034356266260147, acc: 96.875, f1: 81.64528189855919, r: 0.5383961920879586
06/02/2019 10:21:52 step: 7600, epoch: 230, batch: 9, loss: 0.18784978985786438, acc: 95.3125, f1: 94.51507556770714, r: 0.6800826015025802
06/02/2019 10:21:53 step: 7605, epoch: 230, batch: 14, loss: 0.14887794852256775, acc: 92.1875, f1: 73.78943725129123, r: 0.6383473866948136
06/02/2019 10:21:54 step: 7610, epoch: 230, batch: 19, loss: 0.2229272723197937, acc: 93.75, f1: 82.60317460317461, r: 0.5812764363197693
06/02/2019 10:21:55 step: 7615, epoch: 230, batch: 24, loss: 0.2681839168071747, acc: 89.0625, f1: 61.59859876965139, r: 0.6744851583916034
06/02/2019 10:21:57 step: 7620, epoch: 230, batch: 29, loss: 0.30069535970687866, acc: 89.0625, f1: 73.18326859045506, r: 0.7004398376864422
06/02/2019 10:21:57 *** evaluating ***
06/02/2019 10:21:58 step: 231, epoch: 230, acc: 56.837606837606835, f1: 29.648110835050296, r: 0.3067166069426857
06/02/2019 10:21:58 *** epoch: 232 ***
06/02/2019 10:21:58 *** training ***
06/02/2019 10:21:59 step: 7628, epoch: 231, batch: 4, loss: 0.31334298849105835, acc: 87.5, f1: 72.05990829346092, r: 0.7096824754117261
06/02/2019 10:22:00 step: 7633, epoch: 231, batch: 9, loss: 0.3041083812713623, acc: 85.9375, f1: 72.3266806722689, r: 0.6381072363984535
06/02/2019 10:22:01 step: 7638, epoch: 231, batch: 14, loss: 0.26042234897613525, acc: 87.5, f1: 74.31277056277057, r: 0.6484271447448416
06/02/2019 10:22:02 step: 7643, epoch: 231, batch: 19, loss: 0.1948665827512741, acc: 92.1875, f1: 81.60584886128363, r: 0.7397677341536788
06/02/2019 10:22:04 step: 7648, epoch: 231, batch: 24, loss: 0.15872788429260254, acc: 96.875, f1: 88.04927376355948, r: 0.6532386910456169
06/02/2019 10:22:05 step: 7653, epoch: 231, batch: 29, loss: 0.2078276127576828, acc: 89.0625, f1: 80.43043860498129, r: 0.6388496436074089
06/02/2019 10:22:05 *** evaluating ***
06/02/2019 10:22:06 step: 232, epoch: 231, acc: 57.26495726495726, f1: 29.23723518122695, r: 0.29769826099162866
06/02/2019 10:22:06 *** epoch: 233 ***
06/02/2019 10:22:06 *** training ***
06/02/2019 10:22:07 step: 7661, epoch: 232, batch: 4, loss: 0.35304972529411316, acc: 85.9375, f1: 65.10118234256166, r: 0.6370421362666364
06/02/2019 10:22:08 step: 7666, epoch: 232, batch: 9, loss: 0.16157886385917664, acc: 95.3125, f1: 92.82200103628675, r: 0.6308514573771095
06/02/2019 10:22:09 step: 7671, epoch: 232, batch: 14, loss: 0.13514818251132965, acc: 95.3125, f1: 78.1557067271353, r: 0.5540496368142465
06/02/2019 10:22:10 step: 7676, epoch: 232, batch: 19, loss: 0.38292133808135986, acc: 84.375, f1: 62.93290043290043, r: 0.6157833635552807
06/02/2019 10:22:11 step: 7681, epoch: 232, batch: 24, loss: 0.20407207310199738, acc: 92.1875, f1: 82.92544699872286, r: 0.6921252900612082
06/02/2019 10:22:13 step: 7686, epoch: 232, batch: 29, loss: 0.2520884871482849, acc: 90.625, f1: 92.12113673220485, r: 0.748981869700807
06/02/2019 10:22:13 *** evaluating ***
06/02/2019 10:22:14 step: 233, epoch: 232, acc: 58.54700854700855, f1: 29.28652217770975, r: 0.30422895867163163
06/02/2019 10:22:14 *** epoch: 234 ***
06/02/2019 10:22:14 *** training ***
06/02/2019 10:22:15 step: 7694, epoch: 233, batch: 4, loss: 0.2196885645389557, acc: 90.625, f1: 84.75198412698413, r: 0.6616180362421105
06/02/2019 10:22:16 step: 7699, epoch: 233, batch: 9, loss: 0.24801653623580933, acc: 89.0625, f1: 71.92284688995217, r: 0.6675505486669022
06/02/2019 10:22:17 step: 7704, epoch: 233, batch: 14, loss: 0.22745488584041595, acc: 95.3125, f1: 78.75850340136054, r: 0.581765131270563
06/02/2019 10:22:18 step: 7709, epoch: 233, batch: 19, loss: 0.18750427663326263, acc: 90.625, f1: 64.02699501756105, r: 0.510258518521527
06/02/2019 10:22:19 step: 7714, epoch: 233, batch: 24, loss: 0.26459556818008423, acc: 90.625, f1: 88.70639241782618, r: 0.7766367889384739
06/02/2019 10:22:21 step: 7719, epoch: 233, batch: 29, loss: 0.1312805414199829, acc: 96.875, f1: 86.28004437322817, r: 0.6238072444165241
06/02/2019 10:22:21 *** evaluating ***
06/02/2019 10:22:22 step: 234, epoch: 233, acc: 56.837606837606835, f1: 29.31961043605329, r: 0.3108733807652634
06/02/2019 10:22:22 *** epoch: 235 ***
06/02/2019 10:22:22 *** training ***
06/02/2019 10:22:23 step: 7727, epoch: 234, batch: 4, loss: 0.2625577449798584, acc: 85.9375, f1: 77.09025668209343, r: 0.6687358713317476
06/02/2019 10:22:24 step: 7732, epoch: 234, batch: 9, loss: 0.24312348663806915, acc: 89.0625, f1: 79.43731338689321, r: 0.6350106799827673
06/02/2019 10:22:25 step: 7737, epoch: 234, batch: 14, loss: 0.2360355108976364, acc: 89.0625, f1: 75.00813627556107, r: 0.6833482916574138
06/02/2019 10:22:26 step: 7742, epoch: 234, batch: 19, loss: 0.29250630736351013, acc: 90.625, f1: 83.64650039321424, r: 0.6774903916601135
06/02/2019 10:22:27 step: 7747, epoch: 234, batch: 24, loss: 0.2185753881931305, acc: 90.625, f1: 74.18183545178226, r: 0.5884969806686707
06/02/2019 10:22:29 step: 7752, epoch: 234, batch: 29, loss: 0.2579227387905121, acc: 89.0625, f1: 86.59348170375928, r: 0.5575957153893552
06/02/2019 10:22:29 *** evaluating ***
06/02/2019 10:22:30 step: 235, epoch: 234, acc: 58.119658119658126, f1: 29.14263609551017, r: 0.3060983007078076
06/02/2019 10:22:30 *** epoch: 236 ***
06/02/2019 10:22:30 *** training ***
06/02/2019 10:22:31 step: 7760, epoch: 235, batch: 4, loss: 0.18221335113048553, acc: 93.75, f1: 86.49863593962974, r: 0.5993843576976324
06/02/2019 10:22:32 step: 7765, epoch: 235, batch: 9, loss: 0.17238576710224152, acc: 92.1875, f1: 90.11278195488721, r: 0.7670740588660949
06/02/2019 10:22:33 step: 7770, epoch: 235, batch: 14, loss: 0.1423359215259552, acc: 92.1875, f1: 80.70984997772695, r: 0.6686113623785941
06/02/2019 10:22:34 step: 7775, epoch: 235, batch: 19, loss: 0.22414924204349518, acc: 90.625, f1: 86.11008039579468, r: 0.6524317223151596
06/02/2019 10:22:35 step: 7780, epoch: 235, batch: 24, loss: 0.203314408659935, acc: 90.625, f1: 83.217009747622, r: 0.647721151306988
06/02/2019 10:22:36 step: 7785, epoch: 235, batch: 29, loss: 0.16309784352779388, acc: 93.75, f1: 87.84730108259521, r: 0.637189267223107
06/02/2019 10:22:37 *** evaluating ***
06/02/2019 10:22:37 step: 236, epoch: 235, acc: 58.54700854700855, f1: 28.646583213352528, r: 0.2926004253300133
06/02/2019 10:22:37 *** epoch: 237 ***
06/02/2019 10:22:37 *** training ***
06/02/2019 10:22:39 step: 7793, epoch: 236, batch: 4, loss: 0.26323938369750977, acc: 89.0625, f1: 85.98091251152475, r: 0.6201650906732511
06/02/2019 10:22:40 step: 7798, epoch: 236, batch: 9, loss: 0.1746891438961029, acc: 93.75, f1: 93.55500821018062, r: 0.6453570239114313
06/02/2019 10:22:41 step: 7803, epoch: 236, batch: 14, loss: 0.15858644247055054, acc: 95.3125, f1: 90.51624964049468, r: 0.5861457708770774
06/02/2019 10:22:42 step: 7808, epoch: 236, batch: 19, loss: 0.16146282851696014, acc: 92.1875, f1: 77.06134238432375, r: 0.6698211034218652
06/02/2019 10:22:43 step: 7813, epoch: 236, batch: 24, loss: 0.4133601784706116, acc: 81.25, f1: 57.25464396284829, r: 0.5203374531024959
06/02/2019 10:22:45 step: 7818, epoch: 236, batch: 29, loss: 0.16962426900863647, acc: 92.1875, f1: 86.56067336918402, r: 0.6267801503724515
06/02/2019 10:22:45 *** evaluating ***
06/02/2019 10:22:46 step: 237, epoch: 236, acc: 58.119658119658126, f1: 30.029942645074225, r: 0.3015182116886347
06/02/2019 10:22:46 *** epoch: 238 ***
06/02/2019 10:22:46 *** training ***
06/02/2019 10:22:46 step: 7826, epoch: 237, batch: 4, loss: 0.2502896785736084, acc: 92.1875, f1: 93.32091097308489, r: 0.6989426090106683
06/02/2019 10:22:48 step: 7831, epoch: 237, batch: 9, loss: 0.18701967597007751, acc: 93.75, f1: 88.28427685570543, r: 0.73707001380192
06/02/2019 10:22:49 step: 7836, epoch: 237, batch: 14, loss: 0.28197410702705383, acc: 87.5, f1: 75.08867943650553, r: 0.7548211734536873
06/02/2019 10:22:50 step: 7841, epoch: 237, batch: 19, loss: 0.25370845198631287, acc: 89.0625, f1: 77.46386054421768, r: 0.6365126544497856
06/02/2019 10:22:51 step: 7846, epoch: 237, batch: 24, loss: 0.23111224174499512, acc: 89.0625, f1: 73.20874493414387, r: 0.6863814966094236
06/02/2019 10:22:53 step: 7851, epoch: 237, batch: 29, loss: 0.23163695633411407, acc: 95.3125, f1: 93.20728291316527, r: 0.624439324325117
06/02/2019 10:22:53 *** evaluating ***
06/02/2019 10:22:54 step: 238, epoch: 237, acc: 57.692307692307686, f1: 30.399151287309177, r: 0.3080383110754066
06/02/2019 10:22:54 *** epoch: 239 ***
06/02/2019 10:22:54 *** training ***
06/02/2019 10:22:55 step: 7859, epoch: 238, batch: 4, loss: 0.21593378484249115, acc: 92.1875, f1: 89.81671350092404, r: 0.7948657890082257
06/02/2019 10:22:56 step: 7864, epoch: 238, batch: 9, loss: 0.29422786831855774, acc: 82.8125, f1: 69.50964690853468, r: 0.6179189063989348
06/02/2019 10:22:57 step: 7869, epoch: 238, batch: 14, loss: 0.2530246078968048, acc: 90.625, f1: 76.91400301766178, r: 0.6762253392926526
06/02/2019 10:22:58 step: 7874, epoch: 238, batch: 19, loss: 0.27552375197410583, acc: 84.375, f1: 69.46791695146959, r: 0.7196014575304783
06/02/2019 10:22:59 step: 7879, epoch: 238, batch: 24, loss: 0.22112895548343658, acc: 92.1875, f1: 91.20948602406403, r: 0.6527367476663466
06/02/2019 10:23:01 step: 7884, epoch: 238, batch: 29, loss: 0.27679896354675293, acc: 89.0625, f1: 80.71295289275584, r: 0.6689643590140797
06/02/2019 10:23:01 *** evaluating ***
06/02/2019 10:23:02 step: 239, epoch: 238, acc: 55.98290598290598, f1: 29.350302267885997, r: 0.3182909875488511
06/02/2019 10:23:02 *** epoch: 240 ***
06/02/2019 10:23:02 *** training ***
06/02/2019 10:23:03 step: 7892, epoch: 239, batch: 4, loss: 0.31691572070121765, acc: 85.9375, f1: 62.05955334987593, r: 0.7429060723995813
06/02/2019 10:23:04 step: 7897, epoch: 239, batch: 9, loss: 0.21288295090198517, acc: 89.0625, f1: 75.47712632195392, r: 0.6899617480596254
06/02/2019 10:23:05 step: 7902, epoch: 239, batch: 14, loss: 0.2801850438117981, acc: 85.9375, f1: 70.65236350950637, r: 0.6680807223580707
06/02/2019 10:23:06 step: 7907, epoch: 239, batch: 19, loss: 0.2793125510215759, acc: 87.5, f1: 76.47002353524093, r: 0.6745041491285108
06/02/2019 10:23:07 step: 7912, epoch: 239, batch: 24, loss: 0.21792107820510864, acc: 90.625, f1: 79.11445279866331, r: 0.601664894774446
06/02/2019 10:23:08 step: 7917, epoch: 239, batch: 29, loss: 0.31504932045936584, acc: 87.5, f1: 80.07461482616762, r: 0.5761502162512514
06/02/2019 10:23:09 *** evaluating ***
06/02/2019 10:23:09 step: 240, epoch: 239, acc: 56.837606837606835, f1: 29.66327055801462, r: 0.31261808279534925
06/02/2019 10:23:09 *** epoch: 241 ***
06/02/2019 10:23:09 *** training ***
06/02/2019 10:23:10 step: 7925, epoch: 240, batch: 4, loss: 0.14698268473148346, acc: 92.1875, f1: 75.4920634920635, r: 0.6376685890528492
06/02/2019 10:23:12 step: 7930, epoch: 240, batch: 9, loss: 0.23018358647823334, acc: 90.625, f1: 77.59356906534325, r: 0.7470766834327982
06/02/2019 10:23:13 step: 7935, epoch: 240, batch: 14, loss: 0.23669376969337463, acc: 93.75, f1: 82.50098952624528, r: 0.6973397095553526
06/02/2019 10:23:14 step: 7940, epoch: 240, batch: 19, loss: 0.12307275086641312, acc: 96.875, f1: 94.29401390185703, r: 0.6242462834129702
06/02/2019 10:23:15 step: 7945, epoch: 240, batch: 24, loss: 0.14598184823989868, acc: 98.4375, f1: 96.04938271604938, r: 0.554437804758613
06/02/2019 10:23:16 step: 7950, epoch: 240, batch: 29, loss: 0.299701452255249, acc: 90.625, f1: 86.5952380952381, r: 0.6394392969415359
06/02/2019 10:23:17 *** evaluating ***
06/02/2019 10:23:17 step: 241, epoch: 240, acc: 56.41025641025641, f1: 36.976290516206475, r: 0.3131481456400187
06/02/2019 10:23:17 *** epoch: 242 ***
06/02/2019 10:23:17 *** training ***
06/02/2019 10:23:19 step: 7958, epoch: 241, batch: 4, loss: 0.16670513153076172, acc: 95.3125, f1: 85.50929462694168, r: 0.7447813138015753
06/02/2019 10:23:20 step: 7963, epoch: 241, batch: 9, loss: 0.14500993490219116, acc: 93.75, f1: 88.53862493614047, r: 0.7040007611673189
06/02/2019 10:23:21 step: 7968, epoch: 241, batch: 14, loss: 0.2785836160182953, acc: 92.1875, f1: 89.27033750563162, r: 0.7000011719383371
06/02/2019 10:23:22 step: 7973, epoch: 241, batch: 19, loss: 0.28593283891677856, acc: 89.0625, f1: 84.86790986790986, r: 0.7405546518670423
06/02/2019 10:23:23 step: 7978, epoch: 241, batch: 24, loss: 0.27226632833480835, acc: 85.9375, f1: 67.84340797629834, r: 0.5938909744030143
06/02/2019 10:23:25 step: 7983, epoch: 241, batch: 29, loss: 0.23201444745063782, acc: 90.625, f1: 90.78461768116941, r: 0.6966269431601766
06/02/2019 10:23:25 *** evaluating ***
06/02/2019 10:23:26 step: 242, epoch: 241, acc: 55.98290598290598, f1: 30.013188494014454, r: 0.31377329371957313
06/02/2019 10:23:26 *** epoch: 243 ***
06/02/2019 10:23:26 *** training ***
06/02/2019 10:23:27 step: 7991, epoch: 242, batch: 4, loss: 0.12789921462535858, acc: 93.75, f1: 88.85288730876965, r: 0.7462207998831729
06/02/2019 10:23:28 step: 7996, epoch: 242, batch: 9, loss: 0.16331550478935242, acc: 92.1875, f1: 81.8018116337444, r: 0.8119864227142923
06/02/2019 10:23:29 step: 8001, epoch: 242, batch: 14, loss: 0.1912129521369934, acc: 90.625, f1: 85.76095779220779, r: 0.710152668808248
06/02/2019 10:23:30 step: 8006, epoch: 242, batch: 19, loss: 0.12199043482542038, acc: 93.75, f1: 80.04830917874396, r: 0.6040030460663065
06/02/2019 10:23:31 step: 8011, epoch: 242, batch: 24, loss: 0.3313996493816376, acc: 89.0625, f1: 76.24053030303031, r: 0.6658473048313184
06/02/2019 10:23:32 step: 8016, epoch: 242, batch: 29, loss: 0.12177673727273941, acc: 95.3125, f1: 83.32229495571814, r: 0.6347328828142556
06/02/2019 10:23:33 *** evaluating ***
06/02/2019 10:23:34 step: 243, epoch: 242, acc: 57.692307692307686, f1: 38.86724072020321, r: 0.32318175636138274
06/02/2019 10:23:34 *** epoch: 244 ***
06/02/2019 10:23:34 *** training ***
06/02/2019 10:23:35 step: 8024, epoch: 243, batch: 4, loss: 0.14190702140331268, acc: 93.75, f1: 82.83938887199757, r: 0.7174939137368147
06/02/2019 10:23:36 step: 8029, epoch: 243, batch: 9, loss: 0.2507691979408264, acc: 89.0625, f1: 72.54419191919192, r: 0.6816722647158117
06/02/2019 10:23:37 step: 8034, epoch: 243, batch: 14, loss: 0.11417441070079803, acc: 95.3125, f1: 80.6626582877555, r: 0.6151957269518423
06/02/2019 10:23:38 step: 8039, epoch: 243, batch: 19, loss: 0.1318410485982895, acc: 93.75, f1: 90.31436817151103, r: 0.7277152054399748
06/02/2019 10:23:39 step: 8044, epoch: 243, batch: 24, loss: 0.2267630398273468, acc: 89.0625, f1: 79.41478185638364, r: 0.7625762311976536
06/02/2019 10:23:40 step: 8049, epoch: 243, batch: 29, loss: 0.18494994938373566, acc: 92.1875, f1: 76.9652305366591, r: 0.6371732381966938
06/02/2019 10:23:41 *** evaluating ***
06/02/2019 10:23:41 step: 244, epoch: 243, acc: 57.26495726495726, f1: 29.278042985258523, r: 0.30412582020300555
06/02/2019 10:23:41 *** epoch: 245 ***
06/02/2019 10:23:41 *** training ***
06/02/2019 10:23:43 step: 8057, epoch: 244, batch: 4, loss: 0.24985238909721375, acc: 92.1875, f1: 80.52311066126856, r: 0.7197725005809632
06/02/2019 10:23:44 step: 8062, epoch: 244, batch: 9, loss: 0.18875902891159058, acc: 95.3125, f1: 82.21320346320347, r: 0.7333319867101854
06/02/2019 10:23:45 step: 8067, epoch: 244, batch: 14, loss: 0.1464540958404541, acc: 92.1875, f1: 67.44505494505493, r: 0.5884238443660255
06/02/2019 10:23:46 step: 8072, epoch: 244, batch: 19, loss: 0.2990412712097168, acc: 89.0625, f1: 74.51566911349519, r: 0.6534526727833364
06/02/2019 10:23:47 step: 8077, epoch: 244, batch: 24, loss: 0.29714488983154297, acc: 87.5, f1: 79.14459444064707, r: 0.7075635784650034
06/02/2019 10:23:48 step: 8082, epoch: 244, batch: 29, loss: 0.15885576605796814, acc: 93.75, f1: 85.89591567852437, r: 0.6923052895425129
06/02/2019 10:23:49 *** evaluating ***
06/02/2019 10:23:49 step: 245, epoch: 244, acc: 56.837606837606835, f1: 29.451721731457415, r: 0.30202503607820774
06/02/2019 10:23:49 *** epoch: 246 ***
06/02/2019 10:23:49 *** training ***
06/02/2019 10:23:51 step: 8090, epoch: 245, batch: 4, loss: 0.27276328206062317, acc: 85.9375, f1: 71.29419191919192, r: 0.6189492139474805
06/02/2019 10:23:52 step: 8095, epoch: 245, batch: 9, loss: 0.3607540428638458, acc: 84.375, f1: 60.19836826107823, r: 0.5979424772563249
06/02/2019 10:23:53 step: 8100, epoch: 245, batch: 14, loss: 0.2670256495475769, acc: 92.1875, f1: 86.56309274233803, r: 0.7045938932665589
06/02/2019 10:23:54 step: 8105, epoch: 245, batch: 19, loss: 0.19764059782028198, acc: 90.625, f1: 74.41919191919192, r: 0.725881972359046
06/02/2019 10:23:55 step: 8110, epoch: 245, batch: 24, loss: 0.14497333765029907, acc: 98.4375, f1: 97.95186891961086, r: 0.6541385507083631
06/02/2019 10:23:56 step: 8115, epoch: 245, batch: 29, loss: 0.2076978236436844, acc: 92.1875, f1: 91.02577634827516, r: 0.6502984007872068
06/02/2019 10:23:57 *** evaluating ***
06/02/2019 10:23:57 step: 246, epoch: 245, acc: 55.12820512820513, f1: 29.36479548380545, r: 0.3041514060324854
06/02/2019 10:23:57 *** epoch: 247 ***
06/02/2019 10:23:57 *** training ***
06/02/2019 10:23:59 step: 8123, epoch: 246, batch: 4, loss: 0.21766211092472076, acc: 93.75, f1: 91.15100585688822, r: 0.5629781130630926
06/02/2019 10:24:00 step: 8128, epoch: 246, batch: 9, loss: 0.29558274149894714, acc: 85.9375, f1: 77.18744450505687, r: 0.5818345693259019
06/02/2019 10:24:01 step: 8133, epoch: 246, batch: 14, loss: 0.26236069202423096, acc: 92.1875, f1: 84.75555057187711, r: 0.6645317591054252
06/02/2019 10:24:02 step: 8138, epoch: 246, batch: 19, loss: 0.16370953619480133, acc: 93.75, f1: 84.90571667263396, r: 0.617170918813214
06/02/2019 10:24:03 step: 8143, epoch: 246, batch: 24, loss: 0.23360054194927216, acc: 87.5, f1: 72.38005050505049, r: 0.7561832601217953
06/02/2019 10:24:04 step: 8148, epoch: 246, batch: 29, loss: 0.14832563698291779, acc: 92.1875, f1: 84.12870321591814, r: 0.7166723163812445
06/02/2019 10:24:05 *** evaluating ***
06/02/2019 10:24:05 step: 247, epoch: 246, acc: 58.54700854700855, f1: 29.999217422623836, r: 0.29747912084479944
06/02/2019 10:24:05 *** epoch: 248 ***
06/02/2019 10:24:05 *** training ***
06/02/2019 10:24:07 step: 8156, epoch: 247, batch: 4, loss: 0.20613758265972137, acc: 92.1875, f1: 68.75951025571518, r: 0.6481657548653671
06/02/2019 10:24:08 step: 8161, epoch: 247, batch: 9, loss: 0.1482708603143692, acc: 95.3125, f1: 80.85232501488659, r: 0.7675255659602943
06/02/2019 10:24:09 step: 8166, epoch: 247, batch: 14, loss: 0.16138646006584167, acc: 93.75, f1: 81.02678571428572, r: 0.7040970463147932
06/02/2019 10:24:10 step: 8171, epoch: 247, batch: 19, loss: 0.11862228810787201, acc: 93.75, f1: 88.56386999244143, r: 0.6616947949538959
06/02/2019 10:24:11 step: 8176, epoch: 247, batch: 24, loss: 0.24462880194187164, acc: 92.1875, f1: 86.5390749601276, r: 0.7040090723141654
06/02/2019 10:24:12 step: 8181, epoch: 247, batch: 29, loss: 0.20909488201141357, acc: 90.625, f1: 92.39713774597496, r: 0.7375278534169784
06/02/2019 10:24:13 *** evaluating ***
06/02/2019 10:24:13 step: 248, epoch: 247, acc: 57.692307692307686, f1: 30.23128541540873, r: 0.29216695456456143
06/02/2019 10:24:13 *** epoch: 249 ***
06/02/2019 10:24:13 *** training ***
06/02/2019 10:24:14 step: 8189, epoch: 248, batch: 4, loss: 0.30511409044265747, acc: 90.625, f1: 77.5532581453634, r: 0.6590840181838067
06/02/2019 10:24:16 step: 8194, epoch: 248, batch: 9, loss: 0.294555127620697, acc: 89.0625, f1: 78.03179753054215, r: 0.7315260555663587
06/02/2019 10:24:17 step: 8199, epoch: 248, batch: 14, loss: 0.16258081793785095, acc: 90.625, f1: 84.53572250014375, r: 0.6756934160210458
06/02/2019 10:24:18 step: 8204, epoch: 248, batch: 19, loss: 0.11580498516559601, acc: 98.4375, f1: 97.78325123152709, r: 0.7602360267823499
06/02/2019 10:24:19 step: 8209, epoch: 248, batch: 24, loss: 0.1429896354675293, acc: 93.75, f1: 93.1945143804982, r: 0.6933908534360057
06/02/2019 10:24:20 step: 8214, epoch: 248, batch: 29, loss: 0.2414005547761917, acc: 90.625, f1: 74.11356209150327, r: 0.6441442158340651
06/02/2019 10:24:21 *** evaluating ***
06/02/2019 10:24:21 step: 249, epoch: 248, acc: 55.98290598290598, f1: 28.3933469001845, r: 0.29669133368329265
06/02/2019 10:24:21 *** epoch: 250 ***
06/02/2019 10:24:21 *** training ***
06/02/2019 10:24:22 step: 8222, epoch: 249, batch: 4, loss: 0.17175379395484924, acc: 93.75, f1: 88.68583797155225, r: 0.6316799658211009
06/02/2019 10:24:23 step: 8227, epoch: 249, batch: 9, loss: 0.18206797540187836, acc: 95.3125, f1: 71.0909090909091, r: 0.6317470411563504
06/02/2019 10:24:25 step: 8232, epoch: 249, batch: 14, loss: 0.21383321285247803, acc: 89.0625, f1: 79.86755308183879, r: 0.6081121906753821
06/02/2019 10:24:26 step: 8237, epoch: 249, batch: 19, loss: 0.35871198773384094, acc: 82.8125, f1: 53.33681425786689, r: 0.5678329924729448
06/02/2019 10:24:27 step: 8242, epoch: 249, batch: 24, loss: 0.1560162603855133, acc: 93.75, f1: 90.92712842712842, r: 0.7941396397916975
06/02/2019 10:24:28 step: 8247, epoch: 249, batch: 29, loss: 0.2401442527770996, acc: 95.3125, f1: 77.24812030075189, r: 0.6213869727636793
06/02/2019 10:24:29 *** evaluating ***
06/02/2019 10:24:29 step: 250, epoch: 249, acc: 57.692307692307686, f1: 29.594088081250746, r: 0.29202492312263395
06/02/2019 10:24:29 *** epoch: 251 ***
06/02/2019 10:24:29 *** training ***
06/02/2019 10:24:30 step: 8255, epoch: 250, batch: 4, loss: 0.20207534730434418, acc: 93.75, f1: 90.6140350877193, r: 0.694053311321377
06/02/2019 10:24:31 step: 8260, epoch: 250, batch: 9, loss: 0.14000022411346436, acc: 96.875, f1: 94.36328377504849, r: 0.6861279924966466
06/02/2019 10:24:33 step: 8265, epoch: 250, batch: 14, loss: 0.22514690458774567, acc: 92.1875, f1: 94.84654234654235, r: 0.819477043273517
06/02/2019 10:24:34 step: 8270, epoch: 250, batch: 19, loss: 0.1275782287120819, acc: 96.875, f1: 94.82103952692188, r: 0.6417950186974368
06/02/2019 10:24:35 step: 8275, epoch: 250, batch: 24, loss: 0.2621634006500244, acc: 93.75, f1: 79.92424242424244, r: 0.7151506220783084
06/02/2019 10:24:36 step: 8280, epoch: 250, batch: 29, loss: 0.17664171755313873, acc: 92.1875, f1: 79.93941284738248, r: 0.6669006723335129
06/02/2019 10:24:37 *** evaluating ***
06/02/2019 10:24:37 step: 251, epoch: 250, acc: 56.837606837606835, f1: 28.878641965939956, r: 0.28965872438124024
06/02/2019 10:24:37 *** epoch: 252 ***
06/02/2019 10:24:37 *** training ***
06/02/2019 10:24:39 step: 8288, epoch: 251, batch: 4, loss: 0.23033127188682556, acc: 90.625, f1: 80.16570881226053, r: 0.7016846504634368
06/02/2019 10:24:40 step: 8293, epoch: 251, batch: 9, loss: 0.13420706987380981, acc: 93.75, f1: 75.30369290573373, r: 0.6615039671820596
06/02/2019 10:24:41 step: 8298, epoch: 251, batch: 14, loss: 0.18185676634311676, acc: 92.1875, f1: 88.42174278279755, r: 0.7010903781881452
06/02/2019 10:24:42 step: 8303, epoch: 251, batch: 19, loss: 0.14634953439235687, acc: 96.875, f1: 97.55157123578176, r: 0.7186690511396049
06/02/2019 10:24:43 step: 8308, epoch: 251, batch: 24, loss: 0.18942756950855255, acc: 92.1875, f1: 84.24369747899159, r: 0.7446331962664794
06/02/2019 10:24:45 step: 8313, epoch: 251, batch: 29, loss: 0.15890470147132874, acc: 96.875, f1: 94.15024630541872, r: 0.6367519391051678
06/02/2019 10:24:45 *** evaluating ***
06/02/2019 10:24:46 step: 252, epoch: 251, acc: 56.837606837606835, f1: 29.268526477230587, r: 0.2949321257529256
06/02/2019 10:24:46 *** epoch: 253 ***
06/02/2019 10:24:46 *** training ***
06/02/2019 10:24:47 step: 8321, epoch: 252, batch: 4, loss: 0.1864616572856903, acc: 93.75, f1: 87.5612087670199, r: 0.6135399095482401
06/02/2019 10:24:48 step: 8326, epoch: 252, batch: 9, loss: 0.19720317423343658, acc: 90.625, f1: 57.13888170882211, r: 0.5540886521982904
06/02/2019 10:24:49 step: 8331, epoch: 252, batch: 14, loss: 0.14560377597808838, acc: 93.75, f1: 84.3269230769231, r: 0.6783567819159918
06/02/2019 10:24:50 step: 8336, epoch: 252, batch: 19, loss: 0.3497580885887146, acc: 84.375, f1: 66.51306035234606, r: 0.7905743093053434
06/02/2019 10:24:52 step: 8341, epoch: 252, batch: 24, loss: 0.3041240870952606, acc: 85.9375, f1: 69.89885114885115, r: 0.6524305624914655
06/02/2019 10:24:53 step: 8346, epoch: 252, batch: 29, loss: 0.31123241782188416, acc: 93.75, f1: 84.78835978835978, r: 0.6256754902455699
06/02/2019 10:24:53 *** evaluating ***
06/02/2019 10:24:54 step: 253, epoch: 252, acc: 55.12820512820513, f1: 28.223983331940815, r: 0.29352653660324807
06/02/2019 10:24:54 *** epoch: 254 ***
06/02/2019 10:24:54 *** training ***
06/02/2019 10:24:55 step: 8354, epoch: 253, batch: 4, loss: 0.15426112711429596, acc: 93.75, f1: 78.70039682539682, r: 0.6293124458836731
06/02/2019 10:24:56 step: 8359, epoch: 253, batch: 9, loss: 0.1662152111530304, acc: 95.3125, f1: 93.82332643202209, r: 0.6690391639101059
06/02/2019 10:24:57 step: 8364, epoch: 253, batch: 14, loss: 0.15497273206710815, acc: 93.75, f1: 89.11499493414387, r: 0.6957013269632366
06/02/2019 10:24:59 step: 8369, epoch: 253, batch: 19, loss: 0.16849590837955475, acc: 93.75, f1: 83.65079365079366, r: 0.6444607840312213
06/02/2019 10:25:00 step: 8374, epoch: 253, batch: 24, loss: 0.1598534733057022, acc: 95.3125, f1: 88.59292501624732, r: 0.6862967272749342
06/02/2019 10:25:01 step: 8379, epoch: 253, batch: 29, loss: 0.1095210537314415, acc: 96.875, f1: 94.2063492063492, r: 0.7382538829821635
06/02/2019 10:25:02 *** evaluating ***
06/02/2019 10:25:02 step: 254, epoch: 253, acc: 54.700854700854705, f1: 29.14351375454483, r: 0.2956060988869793
06/02/2019 10:25:02 *** epoch: 255 ***
06/02/2019 10:25:02 *** training ***
06/02/2019 10:25:03 step: 8387, epoch: 254, batch: 4, loss: 0.14526139199733734, acc: 93.75, f1: 93.68318756073857, r: 0.6564871076357645
06/02/2019 10:25:04 step: 8392, epoch: 254, batch: 9, loss: 0.21821671724319458, acc: 90.625, f1: 72.72041062801932, r: 0.702427513227969
06/02/2019 10:25:05 step: 8397, epoch: 254, batch: 14, loss: 0.18569883704185486, acc: 89.0625, f1: 78.07181305536568, r: 0.7614349946086919
06/02/2019 10:25:07 step: 8402, epoch: 254, batch: 19, loss: 0.11150477081537247, acc: 95.3125, f1: 90.94276094276094, r: 0.6777012484188002
06/02/2019 10:25:08 step: 8407, epoch: 254, batch: 24, loss: 0.10828103125095367, acc: 98.4375, f1: 94.5578231292517, r: 0.6580908982453622
06/02/2019 10:25:09 step: 8412, epoch: 254, batch: 29, loss: 0.1582011878490448, acc: 95.3125, f1: 87.77777777777777, r: 0.7667634382755657
06/02/2019 10:25:10 *** evaluating ***
06/02/2019 10:25:10 step: 255, epoch: 254, acc: 55.12820512820513, f1: 29.193334193158112, r: 0.29767346677413253
06/02/2019 10:25:10 *** epoch: 256 ***
06/02/2019 10:25:10 *** training ***
06/02/2019 10:25:11 step: 8420, epoch: 255, batch: 4, loss: 0.14101769030094147, acc: 95.3125, f1: 82.07671957671958, r: 0.8001316317178333
06/02/2019 10:25:12 step: 8425, epoch: 255, batch: 9, loss: 0.11162863671779633, acc: 96.875, f1: 91.92743764172336, r: 0.6656132504053787
06/02/2019 10:25:13 step: 8430, epoch: 255, batch: 14, loss: 0.19461041688919067, acc: 93.75, f1: 92.62585034013605, r: 0.656995134112346
06/02/2019 10:25:14 step: 8435, epoch: 255, batch: 19, loss: 0.35155797004699707, acc: 87.5, f1: 81.76114890400605, r: 0.6942669748149753
06/02/2019 10:25:16 step: 8440, epoch: 255, batch: 24, loss: 0.09641769528388977, acc: 96.875, f1: 92.6984126984127, r: 0.6520067299712282
06/02/2019 10:25:17 step: 8445, epoch: 255, batch: 29, loss: 0.11584401875734329, acc: 93.75, f1: 90.70810142238714, r: 0.6667013110120391
06/02/2019 10:25:18 *** evaluating ***
06/02/2019 10:25:18 step: 256, epoch: 255, acc: 55.55555555555556, f1: 28.497550140114047, r: 0.2875421217413826
06/02/2019 10:25:18 *** epoch: 257 ***
06/02/2019 10:25:18 *** training ***
06/02/2019 10:25:19 step: 8453, epoch: 256, batch: 4, loss: 0.17861582338809967, acc: 93.75, f1: 91.92359476044874, r: 0.6895477360131085
06/02/2019 10:25:20 step: 8458, epoch: 256, batch: 9, loss: 0.18239854276180267, acc: 90.625, f1: 65.41716988727859, r: 0.6399860176416029
06/02/2019 10:25:21 step: 8463, epoch: 256, batch: 14, loss: 0.2516770660877228, acc: 92.1875, f1: 80.71868106350865, r: 0.6796206619647839
06/02/2019 10:25:23 step: 8468, epoch: 256, batch: 19, loss: 0.18830052018165588, acc: 90.625, f1: 69.37689969604864, r: 0.6435320769451778
06/02/2019 10:25:24 step: 8473, epoch: 256, batch: 24, loss: 0.1611073762178421, acc: 92.1875, f1: 70.40301422452451, r: 0.67139038620333
06/02/2019 10:25:25 step: 8478, epoch: 256, batch: 29, loss: 0.18837970495224, acc: 90.625, f1: 82.52526697177727, r: 0.7210859184406191
06/02/2019 10:25:26 *** evaluating ***
06/02/2019 10:25:26 step: 257, epoch: 256, acc: 57.692307692307686, f1: 29.867586119668232, r: 0.28877464363082833
06/02/2019 10:25:26 *** epoch: 258 ***
06/02/2019 10:25:26 *** training ***
06/02/2019 10:25:27 step: 8486, epoch: 257, batch: 4, loss: 0.18379220366477966, acc: 95.3125, f1: 69.65249662618083, r: 0.6164075695234942
06/02/2019 10:25:28 step: 8491, epoch: 257, batch: 9, loss: 0.14843875169754028, acc: 93.75, f1: 74.08008658008657, r: 0.7099994577584213
06/02/2019 10:25:29 step: 8496, epoch: 257, batch: 14, loss: 0.15307463705539703, acc: 93.75, f1: 83.18673285778549, r: 0.6218889872632489
06/02/2019 10:25:30 step: 8501, epoch: 257, batch: 19, loss: 0.16825252771377563, acc: 90.625, f1: 79.32239756367663, r: 0.68694106945345
06/02/2019 10:25:31 step: 8506, epoch: 257, batch: 24, loss: 0.19843405485153198, acc: 92.1875, f1: 86.56530327015501, r: 0.6160059521258887
06/02/2019 10:25:33 step: 8511, epoch: 257, batch: 29, loss: 0.19825251400470734, acc: 93.75, f1: 78.67431588019824, r: 0.7460437674052391
06/02/2019 10:25:34 *** evaluating ***
06/02/2019 10:25:34 step: 258, epoch: 257, acc: 57.692307692307686, f1: 29.02602143152111, r: 0.2951795226817371
06/02/2019 10:25:34 *** epoch: 259 ***
06/02/2019 10:25:34 *** training ***
06/02/2019 10:25:35 step: 8519, epoch: 258, batch: 4, loss: 0.2835424542427063, acc: 87.5, f1: 67.83969728242174, r: 0.6676188748188874
06/02/2019 10:25:36 step: 8524, epoch: 258, batch: 9, loss: 0.19222389161586761, acc: 90.625, f1: 80.47843665768195, r: 0.7208404209850555
06/02/2019 10:25:38 step: 8529, epoch: 258, batch: 14, loss: 0.10865238308906555, acc: 96.875, f1: 95.58312655086849, r: 0.7292202306806965
06/02/2019 10:25:39 step: 8534, epoch: 258, batch: 19, loss: 0.16674064099788666, acc: 93.75, f1: 90.33068585775723, r: 0.6741057708463155
06/02/2019 10:25:40 step: 8539, epoch: 258, batch: 24, loss: 0.1780773252248764, acc: 95.3125, f1: 93.83000512032771, r: 0.743885750895923
06/02/2019 10:25:41 step: 8544, epoch: 258, batch: 29, loss: 0.12369948625564575, acc: 96.875, f1: 91.54761904761905, r: 0.8160415013290526
06/02/2019 10:25:41 *** evaluating ***
06/02/2019 10:25:42 step: 259, epoch: 258, acc: 55.98290598290598, f1: 28.37708073334642, r: 0.3036684932405078
06/02/2019 10:25:42 *** epoch: 260 ***
06/02/2019 10:25:42 *** training ***
06/02/2019 10:25:43 step: 8552, epoch: 259, batch: 4, loss: 0.2117805778980255, acc: 95.3125, f1: 93.00453514739229, r: 0.6575622836959507
06/02/2019 10:25:44 step: 8557, epoch: 259, batch: 9, loss: 0.15256714820861816, acc: 93.75, f1: 88.60897798459094, r: 0.7133171097214455
06/02/2019 10:25:46 step: 8562, epoch: 259, batch: 14, loss: 0.1769622266292572, acc: 93.75, f1: 91.73641972645295, r: 0.737769257798244
06/02/2019 10:25:47 step: 8567, epoch: 259, batch: 19, loss: 0.25291943550109863, acc: 89.0625, f1: 80.39381914381914, r: 0.7443275038429372
06/02/2019 10:25:48 step: 8572, epoch: 259, batch: 24, loss: 0.3260333240032196, acc: 85.9375, f1: 68.97596153846153, r: 0.6269886172162987
06/02/2019 10:25:49 step: 8577, epoch: 259, batch: 29, loss: 0.18247778713703156, acc: 93.75, f1: 92.84526326386792, r: 0.6875765708303525
06/02/2019 10:25:50 *** evaluating ***
06/02/2019 10:25:50 step: 260, epoch: 259, acc: 53.84615384615385, f1: 28.389328497270395, r: 0.2923217736036257
06/02/2019 10:25:50 *** epoch: 261 ***
06/02/2019 10:25:50 *** training ***
06/02/2019 10:25:51 step: 8585, epoch: 260, batch: 4, loss: 0.20351716876029968, acc: 89.0625, f1: 87.66237912789636, r: 0.6557799509364504
06/02/2019 10:25:52 step: 8590, epoch: 260, batch: 9, loss: 0.2536925971508026, acc: 89.0625, f1: 87.92874396135267, r: 0.7704079815155712
06/02/2019 10:25:53 step: 8595, epoch: 260, batch: 14, loss: 0.14297936856746674, acc: 92.1875, f1: 74.4056390608115, r: 0.6686726933220012
06/02/2019 10:25:54 step: 8600, epoch: 260, batch: 19, loss: 0.1632474958896637, acc: 92.1875, f1: 83.05387669455573, r: 0.6705208410910618
06/02/2019 10:25:56 step: 8605, epoch: 260, batch: 24, loss: 0.1552196890115738, acc: 95.3125, f1: 89.42307692307692, r: 0.8094491649042774
06/02/2019 10:25:57 step: 8610, epoch: 260, batch: 29, loss: 0.19472086429595947, acc: 93.75, f1: 85.90287372202266, r: 0.6701898617546116
06/02/2019 10:25:58 *** evaluating ***
06/02/2019 10:25:58 step: 261, epoch: 260, acc: 57.26495726495726, f1: 29.72980605102044, r: 0.28799220369637174
06/02/2019 10:25:58 *** epoch: 262 ***
06/02/2019 10:25:58 *** training ***
06/02/2019 10:25:59 step: 8618, epoch: 261, batch: 4, loss: 0.18980740010738373, acc: 92.1875, f1: 73.06234335839599, r: 0.6833766721519372
06/02/2019 10:26:00 step: 8623, epoch: 261, batch: 9, loss: 0.07457190752029419, acc: 100.0, f1: 100.0, r: 0.7032047863144246
06/02/2019 10:26:01 step: 8628, epoch: 261, batch: 14, loss: 0.15363061428070068, acc: 92.1875, f1: 74.08472479901052, r: 0.6578975057149096
06/02/2019 10:26:02 step: 8633, epoch: 261, batch: 19, loss: 0.08755102008581161, acc: 96.875, f1: 94.11681914144968, r: 0.7190374838784571
06/02/2019 10:26:04 step: 8638, epoch: 261, batch: 24, loss: 0.19742588698863983, acc: 93.75, f1: 90.07747404924824, r: 0.7500539630653947
06/02/2019 10:26:05 step: 8643, epoch: 261, batch: 29, loss: 0.10751674324274063, acc: 98.4375, f1: 97.83549783549783, r: 0.8022538916911122
06/02/2019 10:26:06 *** evaluating ***
06/02/2019 10:26:06 step: 262, epoch: 261, acc: 58.119658119658126, f1: 28.513052085045754, r: 0.2912071488022355
06/02/2019 10:26:06 *** epoch: 263 ***
06/02/2019 10:26:06 *** training ***
06/02/2019 10:26:07 step: 8651, epoch: 262, batch: 4, loss: 0.21762724220752716, acc: 93.75, f1: 82.19142891682785, r: 0.7725019910940994
06/02/2019 10:26:08 step: 8656, epoch: 262, batch: 9, loss: 0.21380795538425446, acc: 90.625, f1: 79.24085079455922, r: 0.8271690834342124
06/02/2019 10:26:09 step: 8661, epoch: 262, batch: 14, loss: 0.1759534478187561, acc: 90.625, f1: 85.50809212573918, r: 0.7904676857671813
06/02/2019 10:26:11 step: 8666, epoch: 262, batch: 19, loss: 0.26299208402633667, acc: 89.0625, f1: 64.29563492063492, r: 0.6922623294446179
06/02/2019 10:26:12 step: 8671, epoch: 262, batch: 24, loss: 0.19517284631729126, acc: 92.1875, f1: 87.78921568627452, r: 0.646326855961454
06/02/2019 10:26:13 step: 8676, epoch: 262, batch: 29, loss: 0.15904474258422852, acc: 93.75, f1: 87.62926986202567, r: 0.6797121200277619
06/02/2019 10:26:14 *** evaluating ***
06/02/2019 10:26:14 step: 263, epoch: 262, acc: 57.692307692307686, f1: 30.858531285695673, r: 0.29869136344647745
06/02/2019 10:26:14 *** epoch: 264 ***
06/02/2019 10:26:14 *** training ***
06/02/2019 10:26:15 step: 8684, epoch: 263, batch: 4, loss: 0.3157787024974823, acc: 89.0625, f1: 72.01680672268907, r: 0.6805811919794054
06/02/2019 10:26:16 step: 8689, epoch: 263, batch: 9, loss: 0.2916778326034546, acc: 90.625, f1: 86.2350320245057, r: 0.5830780784940631
06/02/2019 10:26:17 step: 8694, epoch: 263, batch: 14, loss: 0.24910447001457214, acc: 90.625, f1: 89.86445335710042, r: 0.728830532305764
06/02/2019 10:26:18 step: 8699, epoch: 263, batch: 19, loss: 0.2107216864824295, acc: 93.75, f1: 90.4940437713547, r: 0.7072881408909956
06/02/2019 10:26:20 step: 8704, epoch: 263, batch: 24, loss: 0.23769864439964294, acc: 87.5, f1: 80.1126303364155, r: 0.8138926869259355
06/02/2019 10:26:21 step: 8709, epoch: 263, batch: 29, loss: 0.15871593356132507, acc: 93.75, f1: 77.64746827246827, r: 0.703942120987538
06/02/2019 10:26:21 *** evaluating ***
06/02/2019 10:26:22 step: 264, epoch: 263, acc: 57.692307692307686, f1: 31.48949601123514, r: 0.30661919193287085
06/02/2019 10:26:22 *** epoch: 265 ***
06/02/2019 10:26:22 *** training ***
06/02/2019 10:26:23 step: 8717, epoch: 264, batch: 4, loss: 0.14251703023910522, acc: 95.3125, f1: 85.94866141449992, r: 0.6152022515679151
06/02/2019 10:26:24 step: 8722, epoch: 264, batch: 9, loss: 0.20036566257476807, acc: 92.1875, f1: 78.28732775161346, r: 0.6384408972591726
06/02/2019 10:26:25 step: 8727, epoch: 264, batch: 14, loss: 0.2565757632255554, acc: 92.1875, f1: 83.45796107424015, r: 0.7262235674294598
06/02/2019 10:26:26 step: 8732, epoch: 264, batch: 19, loss: 0.12187006324529648, acc: 95.3125, f1: 95.40146694063746, r: 0.632464892682871
06/02/2019 10:26:27 step: 8737, epoch: 264, batch: 24, loss: 0.15719105303287506, acc: 95.3125, f1: 90.15873015873015, r: 0.7547226472748829
06/02/2019 10:26:29 step: 8742, epoch: 264, batch: 29, loss: 0.15620063245296478, acc: 95.3125, f1: 88.2815734989648, r: 0.6395327856463572
06/02/2019 10:26:29 *** evaluating ***
06/02/2019 10:26:30 step: 265, epoch: 264, acc: 57.692307692307686, f1: 29.77464309576216, r: 0.28795522982088845
06/02/2019 10:26:30 *** epoch: 266 ***
06/02/2019 10:26:30 *** training ***
06/02/2019 10:26:31 step: 8750, epoch: 265, batch: 4, loss: 0.17782102525234222, acc: 92.1875, f1: 78.55763517528224, r: 0.6634372946656797
06/02/2019 10:26:32 step: 8755, epoch: 265, batch: 9, loss: 0.18147622048854828, acc: 92.1875, f1: 87.99536836010999, r: 0.6170634789773437
06/02/2019 10:26:33 step: 8760, epoch: 265, batch: 14, loss: 0.22100724279880524, acc: 87.5, f1: 71.77386105957534, r: 0.5924048564943504
06/02/2019 10:26:34 step: 8765, epoch: 265, batch: 19, loss: 0.1744242012500763, acc: 93.75, f1: 79.24008424008424, r: 0.7593048402922309
06/02/2019 10:26:35 step: 8770, epoch: 265, batch: 24, loss: 0.10701712220907211, acc: 95.3125, f1: 82.50281930954199, r: 0.7015006995740576
06/02/2019 10:26:37 step: 8775, epoch: 265, batch: 29, loss: 0.2536689043045044, acc: 90.625, f1: 88.10570381998953, r: 0.6134646122285387
06/02/2019 10:26:37 *** evaluating ***
06/02/2019 10:26:38 step: 266, epoch: 265, acc: 53.41880341880342, f1: 27.831139583435103, r: 0.293608663300659
06/02/2019 10:26:38 *** epoch: 267 ***
06/02/2019 10:26:38 *** training ***
06/02/2019 10:26:39 step: 8783, epoch: 266, batch: 4, loss: 0.1684943288564682, acc: 92.1875, f1: 85.39604350567465, r: 0.7109292179713654
06/02/2019 10:26:40 step: 8788, epoch: 266, batch: 9, loss: 0.1260882019996643, acc: 98.4375, f1: 87.03703703703704, r: 0.7312672907482872
06/02/2019 10:26:41 step: 8793, epoch: 266, batch: 14, loss: 0.06374970823526382, acc: 98.4375, f1: 98.96800825593395, r: 0.670005036576622
06/02/2019 10:26:43 step: 8798, epoch: 266, batch: 19, loss: 0.21027973294258118, acc: 92.1875, f1: 86.94213349675535, r: 0.67304592513877
06/02/2019 10:26:44 step: 8803, epoch: 266, batch: 24, loss: 0.2303197681903839, acc: 93.75, f1: 78.78654970760235, r: 0.6751774477400908
06/02/2019 10:26:45 step: 8808, epoch: 266, batch: 29, loss: 0.2144414335489273, acc: 85.9375, f1: 80.67139899438035, r: 0.679723831406911
06/02/2019 10:26:45 *** evaluating ***
06/02/2019 10:26:46 step: 267, epoch: 266, acc: 54.700854700854705, f1: 29.646183267666643, r: 0.2840605976747589
06/02/2019 10:26:46 *** epoch: 268 ***
06/02/2019 10:26:46 *** training ***
06/02/2019 10:26:47 step: 8816, epoch: 267, batch: 4, loss: 0.1515226513147354, acc: 93.75, f1: 94.01090868196131, r: 0.7496933540523973
06/02/2019 10:26:48 step: 8821, epoch: 267, batch: 9, loss: 0.16078756749629974, acc: 93.75, f1: 81.5482054890922, r: 0.6352796109429363
06/02/2019 10:26:49 step: 8826, epoch: 267, batch: 14, loss: 0.19961921870708466, acc: 90.625, f1: 84.19654975363686, r: 0.732909455459653
06/02/2019 10:26:50 step: 8831, epoch: 267, batch: 19, loss: 0.19172148406505585, acc: 93.75, f1: 91.70189210950079, r: 0.7116966450102065
06/02/2019 10:26:51 step: 8836, epoch: 267, batch: 24, loss: 0.13346949219703674, acc: 96.875, f1: 93.20652173913044, r: 0.7370363713000648
06/02/2019 10:26:52 step: 8841, epoch: 267, batch: 29, loss: 0.3493442237377167, acc: 92.1875, f1: 83.91203703703704, r: 0.6769852708605913
06/02/2019 10:26:53 *** evaluating ***
06/02/2019 10:26:54 step: 268, epoch: 267, acc: 58.54700854700855, f1: 31.309320243431905, r: 0.3051792039606311
06/02/2019 10:26:54 *** epoch: 269 ***
06/02/2019 10:26:54 *** training ***
06/02/2019 10:26:55 step: 8849, epoch: 268, batch: 4, loss: 0.1568773090839386, acc: 95.3125, f1: 89.44852249200076, r: 0.5564325960944854
06/02/2019 10:26:56 step: 8854, epoch: 268, batch: 9, loss: 0.10148531198501587, acc: 98.4375, f1: 97.49835418038182, r: 0.6444982069560117
06/02/2019 10:26:57 step: 8859, epoch: 268, batch: 14, loss: 0.16212892532348633, acc: 95.3125, f1: 91.13616355633162, r: 0.6445160447253718
06/02/2019 10:26:58 step: 8864, epoch: 268, batch: 19, loss: 0.1392643004655838, acc: 95.3125, f1: 90.95238095238095, r: 0.5979313389887115
06/02/2019 10:27:00 step: 8869, epoch: 268, batch: 24, loss: 0.18891146779060364, acc: 90.625, f1: 68.17794565920504, r: 0.6192275465254315
06/02/2019 10:27:01 step: 8874, epoch: 268, batch: 29, loss: 0.3210400342941284, acc: 85.9375, f1: 72.14221014492753, r: 0.6017993455095663
06/02/2019 10:27:01 *** evaluating ***
06/02/2019 10:27:02 step: 269, epoch: 268, acc: 57.692307692307686, f1: 29.049441291724754, r: 0.29349477425674936
06/02/2019 10:27:02 *** epoch: 270 ***
06/02/2019 10:27:02 *** training ***
06/02/2019 10:27:03 step: 8882, epoch: 269, batch: 4, loss: 0.11440593004226685, acc: 95.3125, f1: 92.8926282051282, r: 0.7339211539089874
06/02/2019 10:27:04 step: 8887, epoch: 269, batch: 9, loss: 0.2117956578731537, acc: 87.5, f1: 86.96068772336521, r: 0.6061943325360712
06/02/2019 10:27:05 step: 8892, epoch: 269, batch: 14, loss: 0.25904542207717896, acc: 89.0625, f1: 79.95148521787866, r: 0.6350851825381697
06/02/2019 10:27:06 step: 8897, epoch: 269, batch: 19, loss: 0.16002511978149414, acc: 93.75, f1: 88.98467432950191, r: 0.7674541437739051
06/02/2019 10:27:07 step: 8902, epoch: 269, batch: 24, loss: 0.1903286874294281, acc: 89.0625, f1: 79.10780385935665, r: 0.6814827342022911
06/02/2019 10:27:08 step: 8907, epoch: 269, batch: 29, loss: 0.10956455022096634, acc: 96.875, f1: 82.4481658692185, r: 0.6986008104480401
06/02/2019 10:27:09 *** evaluating ***
06/02/2019 10:27:10 step: 270, epoch: 269, acc: 57.26495726495726, f1: 29.146986483943003, r: 0.2904925512548216
06/02/2019 10:27:10 *** epoch: 271 ***
06/02/2019 10:27:10 *** training ***
06/02/2019 10:27:11 step: 8915, epoch: 270, batch: 4, loss: 0.2528912127017975, acc: 92.1875, f1: 84.48092280390416, r: 0.642682232142487
06/02/2019 10:27:12 step: 8920, epoch: 270, batch: 9, loss: 0.07017482817173004, acc: 98.4375, f1: 96.65024630541872, r: 0.7366810885794662
06/02/2019 10:27:13 step: 8925, epoch: 270, batch: 14, loss: 0.13517150282859802, acc: 93.75, f1: 79.67171717171718, r: 0.702837110514459
06/02/2019 10:27:14 step: 8930, epoch: 270, batch: 19, loss: 0.1325729638338089, acc: 90.625, f1: 84.00226757369616, r: 0.6642344600382483
06/02/2019 10:27:16 step: 8935, epoch: 270, batch: 24, loss: 0.16463948786258698, acc: 93.75, f1: 84.00426742532005, r: 0.7399287735989061
06/02/2019 10:27:17 step: 8940, epoch: 270, batch: 29, loss: 0.16925659775733948, acc: 93.75, f1: 92.9357969391564, r: 0.7343940918003216
06/02/2019 10:27:18 *** evaluating ***
06/02/2019 10:27:18 step: 271, epoch: 270, acc: 56.837606837606835, f1: 27.18240637053627, r: 0.27585042331780696
06/02/2019 10:27:18 *** epoch: 272 ***
06/02/2019 10:27:18 *** training ***
06/02/2019 10:27:19 step: 8948, epoch: 271, batch: 4, loss: 0.2016158103942871, acc: 92.1875, f1: 84.07407407407408, r: 0.7106849998801316
06/02/2019 10:27:20 step: 8953, epoch: 271, batch: 9, loss: 0.07164403051137924, acc: 95.3125, f1: 96.53695324283561, r: 0.7273337435872778
06/02/2019 10:27:22 step: 8958, epoch: 271, batch: 14, loss: 0.12322288751602173, acc: 96.875, f1: 94.80970157661888, r: 0.7157875794678203
06/02/2019 10:27:23 step: 8963, epoch: 271, batch: 19, loss: 0.29557961225509644, acc: 92.1875, f1: 88.44020354196117, r: 0.6439075536121941
06/02/2019 10:27:24 step: 8968, epoch: 271, batch: 24, loss: 0.07489577680826187, acc: 96.875, f1: 91.33357690902449, r: 0.6482501237083953
06/02/2019 10:27:25 step: 8973, epoch: 271, batch: 29, loss: 0.10562489926815033, acc: 95.3125, f1: 74.78070175438596, r: 0.7559728729235009
06/02/2019 10:27:26 *** evaluating ***
06/02/2019 10:27:26 step: 272, epoch: 271, acc: 55.98290598290598, f1: 34.24940041828639, r: 0.3035075700358439
06/02/2019 10:27:26 *** epoch: 273 ***
06/02/2019 10:27:26 *** training ***
06/02/2019 10:27:27 step: 8981, epoch: 272, batch: 4, loss: 0.08010159432888031, acc: 96.875, f1: 94.85221674876847, r: 0.6924993361961466
06/02/2019 10:27:28 step: 8986, epoch: 272, batch: 9, loss: 0.12721304595470428, acc: 96.875, f1: 95.4299295475766, r: 0.7468988319856892
06/02/2019 10:27:29 step: 8991, epoch: 272, batch: 14, loss: 0.09680160880088806, acc: 95.3125, f1: 70.43650793650794, r: 0.6961016851536722
06/02/2019 10:27:30 step: 8996, epoch: 272, batch: 19, loss: 0.0883684754371643, acc: 95.3125, f1: 95.56993529118964, r: 0.6428845332012353
06/02/2019 10:27:31 step: 9001, epoch: 272, batch: 24, loss: 0.1870606243610382, acc: 93.75, f1: 81.6577540106952, r: 0.7687188827828646
06/02/2019 10:27:33 step: 9006, epoch: 272, batch: 29, loss: 0.07750490307807922, acc: 96.875, f1: 92.53475303164744, r: 0.7581621596345641
06/02/2019 10:27:33 *** evaluating ***
06/02/2019 10:27:34 step: 273, epoch: 272, acc: 55.98290598290598, f1: 28.47043357174236, r: 0.2909409592283979
06/02/2019 10:27:34 *** epoch: 274 ***
06/02/2019 10:27:34 *** training ***
06/02/2019 10:27:35 step: 9014, epoch: 273, batch: 4, loss: 0.1635620892047882, acc: 95.3125, f1: 93.25644785322204, r: 0.6927846343691838
06/02/2019 10:27:36 step: 9019, epoch: 273, batch: 9, loss: 0.12033810466527939, acc: 95.3125, f1: 92.92153119755066, r: 0.6896986582408048
06/02/2019 10:27:37 step: 9024, epoch: 273, batch: 14, loss: 0.4206774830818176, acc: 82.8125, f1: 57.82374661951321, r: 0.6784227294042617
06/02/2019 10:27:39 step: 9029, epoch: 273, batch: 19, loss: 0.17577265202999115, acc: 93.75, f1: 91.0821504130526, r: 0.6456915885192805
06/02/2019 10:27:40 step: 9034, epoch: 273, batch: 24, loss: 0.15920300781726837, acc: 93.75, f1: 83.1868131868132, r: 0.6775721433069996
06/02/2019 10:27:41 step: 9039, epoch: 273, batch: 29, loss: 0.25487715005874634, acc: 90.625, f1: 75.19583591012162, r: 0.7162575126892081
06/02/2019 10:27:41 *** evaluating ***
06/02/2019 10:27:42 step: 274, epoch: 273, acc: 58.54700854700855, f1: 27.794779943508352, r: 0.27532667460032234
06/02/2019 10:27:42 *** epoch: 275 ***
06/02/2019 10:27:42 *** training ***
06/02/2019 10:27:43 step: 9047, epoch: 274, batch: 4, loss: 0.11589286476373672, acc: 95.3125, f1: 93.89943588823769, r: 0.698096290390462
06/02/2019 10:27:44 step: 9052, epoch: 274, batch: 9, loss: 0.11310012638568878, acc: 96.875, f1: 93.05125558995528, r: 0.759743949031118
06/02/2019 10:27:45 step: 9057, epoch: 274, batch: 14, loss: 0.10194466263055801, acc: 96.875, f1: 94.18757667001226, r: 0.7051842648537272
06/02/2019 10:27:46 step: 9062, epoch: 274, batch: 19, loss: 0.1394675076007843, acc: 95.3125, f1: 89.36507936507937, r: 0.6211995212802396
06/02/2019 10:27:47 step: 9067, epoch: 274, batch: 24, loss: 0.17061571776866913, acc: 85.9375, f1: 79.536399420472, r: 0.7080124078535825
06/02/2019 10:27:48 step: 9072, epoch: 274, batch: 29, loss: 0.2201131284236908, acc: 92.1875, f1: 86.28856768882345, r: 0.7969125573158601
06/02/2019 10:27:49 *** evaluating ***
06/02/2019 10:27:49 step: 275, epoch: 274, acc: 57.692307692307686, f1: 29.44479561736199, r: 0.2781190847666707
06/02/2019 10:27:49 *** epoch: 276 ***
06/02/2019 10:27:49 *** training ***
06/02/2019 10:27:51 step: 9080, epoch: 275, batch: 4, loss: 0.11464984714984894, acc: 93.75, f1: 82.36670736670737, r: 0.805156480089802
06/02/2019 10:27:52 step: 9085, epoch: 275, batch: 9, loss: 0.10192044824361801, acc: 96.875, f1: 85.31095498308613, r: 0.7693011328190352
06/02/2019 10:27:53 step: 9090, epoch: 275, batch: 14, loss: 0.13685472309589386, acc: 93.75, f1: 80.79251700680273, r: 0.7107989065441214
06/02/2019 10:27:54 step: 9095, epoch: 275, batch: 19, loss: 0.13895392417907715, acc: 93.75, f1: 91.85943379491766, r: 0.6169083074035809
06/02/2019 10:27:55 step: 9100, epoch: 275, batch: 24, loss: 0.16811813414096832, acc: 92.1875, f1: 74.01085434173669, r: 0.675846288148415
06/02/2019 10:27:56 step: 9105, epoch: 275, batch: 29, loss: 0.07838393747806549, acc: 96.875, f1: 93.5814325730292, r: 0.6630291071549389
06/02/2019 10:27:57 *** evaluating ***
06/02/2019 10:27:57 step: 276, epoch: 275, acc: 58.119658119658126, f1: 29.11406721933038, r: 0.30280517660042194
06/02/2019 10:27:57 *** epoch: 277 ***
06/02/2019 10:27:57 *** training ***
06/02/2019 10:27:59 step: 9113, epoch: 276, batch: 4, loss: 0.16916586458683014, acc: 92.1875, f1: 79.99661628693887, r: 0.7201480211033469
06/02/2019 10:28:00 step: 9118, epoch: 276, batch: 9, loss: 0.21772268414497375, acc: 90.625, f1: 75.8506258234519, r: 0.7056257474886205
06/02/2019 10:28:01 step: 9123, epoch: 276, batch: 14, loss: 0.17907343804836273, acc: 95.3125, f1: 81.53864184970176, r: 0.7104876993539369
06/02/2019 10:28:02 step: 9128, epoch: 276, batch: 19, loss: 0.10137873142957687, acc: 96.875, f1: 96.504884004884, r: 0.77181171087406
06/02/2019 10:28:04 step: 9133, epoch: 276, batch: 24, loss: 0.17827381193637848, acc: 89.0625, f1: 70.09803921568627, r: 0.6392035628449065
06/02/2019 10:28:05 step: 9138, epoch: 276, batch: 29, loss: 0.06399030983448029, acc: 98.4375, f1: 95.38461538461539, r: 0.6098751263988521
06/02/2019 10:28:05 *** evaluating ***
06/02/2019 10:28:06 step: 277, epoch: 276, acc: 55.98290598290598, f1: 28.292602926696595, r: 0.2811937778755258
06/02/2019 10:28:06 *** epoch: 278 ***
06/02/2019 10:28:06 *** training ***
06/02/2019 10:28:07 step: 9146, epoch: 277, batch: 4, loss: 0.11944636702537537, acc: 96.875, f1: 95.0731807874665, r: 0.698045627043766
06/02/2019 10:28:08 step: 9151, epoch: 277, batch: 9, loss: 0.10193217545747757, acc: 95.3125, f1: 87.47252747252749, r: 0.610018231593869
06/02/2019 10:28:09 step: 9156, epoch: 277, batch: 14, loss: 0.09254495054483414, acc: 96.875, f1: 96.84930576003985, r: 0.6881274256268701
06/02/2019 10:28:10 step: 9161, epoch: 277, batch: 19, loss: 0.28629592061042786, acc: 87.5, f1: 66.87246588132972, r: 0.6340434414692571
06/02/2019 10:28:11 step: 9166, epoch: 277, batch: 24, loss: 0.12477201223373413, acc: 96.875, f1: 83.86067470466448, r: 0.6376212865330018
06/02/2019 10:28:12 step: 9171, epoch: 277, batch: 29, loss: 0.28048866987228394, acc: 85.9375, f1: 83.50509995920032, r: 0.6802358093380232
06/02/2019 10:28:13 *** evaluating ***
06/02/2019 10:28:13 step: 278, epoch: 277, acc: 57.692307692307686, f1: 28.630068408436383, r: 0.29038907550609805
06/02/2019 10:28:13 *** epoch: 279 ***
06/02/2019 10:28:13 *** training ***
06/02/2019 10:28:15 step: 9179, epoch: 278, batch: 4, loss: 0.1468111276626587, acc: 95.3125, f1: 91.93883526181664, r: 0.6736489362787615
06/02/2019 10:28:16 step: 9184, epoch: 278, batch: 9, loss: 0.18347565829753876, acc: 90.625, f1: 72.62583542188806, r: 0.7094966575781069
06/02/2019 10:28:17 step: 9189, epoch: 278, batch: 14, loss: 0.16687552630901337, acc: 93.75, f1: 80.69712324288983, r: 0.6959468510269635
06/02/2019 10:28:18 step: 9194, epoch: 278, batch: 19, loss: 0.2539157271385193, acc: 92.1875, f1: 76.11111111111111, r: 0.639700448029039
06/02/2019 10:28:19 step: 9199, epoch: 278, batch: 24, loss: 0.1944325566291809, acc: 93.75, f1: 86.91134952004518, r: 0.6650528351086926
06/02/2019 10:28:21 step: 9204, epoch: 278, batch: 29, loss: 0.2864842712879181, acc: 92.1875, f1: 92.36734693877551, r: 0.6527701222505887
06/02/2019 10:28:21 *** evaluating ***
06/02/2019 10:28:21 step: 279, epoch: 278, acc: 53.84615384615385, f1: 29.031453124269333, r: 0.3036372886262787
06/02/2019 10:28:21 *** epoch: 280 ***
06/02/2019 10:28:21 *** training ***
06/02/2019 10:28:23 step: 9212, epoch: 279, batch: 4, loss: 0.12199718505144119, acc: 96.875, f1: 83.74686716791979, r: 0.6796246564099138
06/02/2019 10:28:24 step: 9217, epoch: 279, batch: 9, loss: 0.07792390137910843, acc: 93.75, f1: 77.8501400560224, r: 0.6633773218869642
06/02/2019 10:28:25 step: 9222, epoch: 279, batch: 14, loss: 0.27815189957618713, acc: 85.9375, f1: 77.03872516372516, r: 0.6405661319502438
06/02/2019 10:28:26 step: 9227, epoch: 279, batch: 19, loss: 0.09136991202831268, acc: 98.4375, f1: 94.70899470899471, r: 0.7166472586422985
06/02/2019 10:28:28 step: 9232, epoch: 279, batch: 24, loss: 0.17515751719474792, acc: 95.3125, f1: 80.04387662924248, r: 0.6002764170211445
06/02/2019 10:28:29 step: 9237, epoch: 279, batch: 29, loss: 0.14128445088863373, acc: 93.75, f1: 91.01834282099937, r: 0.7191530824154118
06/02/2019 10:28:29 *** evaluating ***
06/02/2019 10:28:30 step: 280, epoch: 279, acc: 59.82905982905983, f1: 31.913378740824395, r: 0.2973446467410288
06/02/2019 10:28:30 *** epoch: 281 ***
06/02/2019 10:28:30 *** training ***
06/02/2019 10:28:31 step: 9245, epoch: 280, batch: 4, loss: 0.15785962343215942, acc: 90.625, f1: 84.58089322685596, r: 0.6353089285876766
06/02/2019 10:28:32 step: 9250, epoch: 280, batch: 9, loss: 0.19937460124492645, acc: 92.1875, f1: 78.80891622996886, r: 0.7657054858972976
06/02/2019 10:28:33 step: 9255, epoch: 280, batch: 14, loss: 0.28255361318588257, acc: 85.9375, f1: 80.2358989694516, r: 0.7332386028853846
06/02/2019 10:28:35 step: 9260, epoch: 280, batch: 19, loss: 0.16605818271636963, acc: 95.3125, f1: 92.0, r: 0.7041491024338218
06/02/2019 10:28:36 step: 9265, epoch: 280, batch: 24, loss: 0.20183327794075012, acc: 93.75, f1: 89.48003124694853, r: 0.6746269410207956
06/02/2019 10:28:37 step: 9270, epoch: 280, batch: 29, loss: 0.16435931622982025, acc: 95.3125, f1: 91.0515873015873, r: 0.7550366710410564
06/02/2019 10:28:38 *** evaluating ***
06/02/2019 10:28:38 step: 281, epoch: 280, acc: 56.837606837606835, f1: 28.604274362719416, r: 0.292065437833135
06/02/2019 10:28:38 *** epoch: 282 ***
06/02/2019 10:28:38 *** training ***
06/02/2019 10:28:39 step: 9278, epoch: 281, batch: 4, loss: 0.06760255247354507, acc: 96.875, f1: 95.19899913988583, r: 0.650291186073802
06/02/2019 10:28:40 step: 9283, epoch: 281, batch: 9, loss: 0.15219858288764954, acc: 90.625, f1: 88.80440038684719, r: 0.7326513852433721
06/02/2019 10:28:42 step: 9288, epoch: 281, batch: 14, loss: 0.10442708432674408, acc: 95.3125, f1: 91.65824915824916, r: 0.6733269219505439
06/02/2019 10:28:43 step: 9293, epoch: 281, batch: 19, loss: 0.1629020869731903, acc: 89.0625, f1: 84.3421926910299, r: 0.7580091353054569
06/02/2019 10:28:44 step: 9298, epoch: 281, batch: 24, loss: 0.07815223932266235, acc: 96.875, f1: 95.01713728969848, r: 0.7702951228508711
06/02/2019 10:28:45 step: 9303, epoch: 281, batch: 29, loss: 0.14096277952194214, acc: 93.75, f1: 86.96074703173196, r: 0.6676342454677299
06/02/2019 10:28:45 *** evaluating ***
06/02/2019 10:28:46 step: 282, epoch: 281, acc: 55.12820512820513, f1: 28.508059922533608, r: 0.2957435116487042
06/02/2019 10:28:46 *** epoch: 283 ***
06/02/2019 10:28:46 *** training ***
06/02/2019 10:28:47 step: 9311, epoch: 282, batch: 4, loss: 0.16926461458206177, acc: 95.3125, f1: 89.99999999999999, r: 0.7432578110097775
06/02/2019 10:28:48 step: 9316, epoch: 282, batch: 9, loss: 0.10557758808135986, acc: 95.3125, f1: 94.32539682539682, r: 0.8521262717142226
06/02/2019 10:28:49 step: 9321, epoch: 282, batch: 14, loss: 0.22577106952667236, acc: 90.625, f1: 75.34888955582232, r: 0.6533008208788622
06/02/2019 10:28:50 step: 9326, epoch: 282, batch: 19, loss: 0.10156164318323135, acc: 95.3125, f1: 95.86907226658779, r: 0.6239470978263045
06/02/2019 10:28:51 step: 9331, epoch: 282, batch: 24, loss: 0.21114790439605713, acc: 93.75, f1: 86.03741496598639, r: 0.6624644852397968
06/02/2019 10:28:53 step: 9336, epoch: 282, batch: 29, loss: 0.22877903282642365, acc: 89.0625, f1: 71.4301948051948, r: 0.745896868071858
06/02/2019 10:28:53 *** evaluating ***
06/02/2019 10:28:54 step: 283, epoch: 282, acc: 58.119658119658126, f1: 29.68700457872742, r: 0.290714403985892
06/02/2019 10:28:54 *** epoch: 284 ***
06/02/2019 10:28:54 *** training ***
06/02/2019 10:28:55 step: 9344, epoch: 283, batch: 4, loss: 0.0759211927652359, acc: 96.875, f1: 93.13462226996062, r: 0.8058144474301038
06/02/2019 10:28:56 step: 9349, epoch: 283, batch: 9, loss: 0.04881444200873375, acc: 98.4375, f1: 97.27891156462584, r: 0.6511703826445885
06/02/2019 10:28:57 step: 9354, epoch: 283, batch: 14, loss: 0.12504833936691284, acc: 95.3125, f1: 88.54382332643202, r: 0.6697083936383635
06/02/2019 10:28:59 step: 9359, epoch: 283, batch: 19, loss: 0.16096895933151245, acc: 93.75, f1: 87.78044871794872, r: 0.7125951008168648
06/02/2019 10:29:00 step: 9364, epoch: 283, batch: 24, loss: 0.1642853170633316, acc: 93.75, f1: 93.9244193961175, r: 0.6804127798095384
06/02/2019 10:29:01 step: 9369, epoch: 283, batch: 29, loss: 0.1279917061328888, acc: 93.75, f1: 78.03812514016596, r: 0.6037096646986913
06/02/2019 10:29:02 *** evaluating ***
06/02/2019 10:29:02 step: 284, epoch: 283, acc: 58.54700854700855, f1: 29.59407742016438, r: 0.28449325630912076
06/02/2019 10:29:02 *** epoch: 285 ***
06/02/2019 10:29:02 *** training ***
06/02/2019 10:29:03 step: 9377, epoch: 284, batch: 4, loss: 0.15305541455745697, acc: 95.3125, f1: 86.69624252201386, r: 0.5377164508854315
06/02/2019 10:29:04 step: 9382, epoch: 284, batch: 9, loss: 0.18726198375225067, acc: 93.75, f1: 86.5405982905983, r: 0.7378292296883618
06/02/2019 10:29:06 step: 9387, epoch: 284, batch: 14, loss: 0.12252123653888702, acc: 98.4375, f1: 99.35897435897436, r: 0.7086465519708681
06/02/2019 10:29:07 step: 9392, epoch: 284, batch: 19, loss: 0.20304103195667267, acc: 90.625, f1: 76.777950310559, r: 0.6869530169356657
06/02/2019 10:29:08 step: 9397, epoch: 284, batch: 24, loss: 0.08831088244915009, acc: 95.3125, f1: 83.14439324116742, r: 0.7425833779725057
06/02/2019 10:29:09 step: 9402, epoch: 284, batch: 29, loss: 0.20874640345573425, acc: 89.0625, f1: 78.29262955182072, r: 0.7441591284783112
06/02/2019 10:29:09 *** evaluating ***
06/02/2019 10:29:10 step: 285, epoch: 284, acc: 58.54700854700855, f1: 30.265204119065086, r: 0.2921404630429792
06/02/2019 10:29:10 *** epoch: 286 ***
06/02/2019 10:29:10 *** training ***
06/02/2019 10:29:11 step: 9410, epoch: 285, batch: 4, loss: 0.13557346165180206, acc: 95.3125, f1: 93.7081629343003, r: 0.7289949957128979
06/02/2019 10:29:12 step: 9415, epoch: 285, batch: 9, loss: 0.09963510930538177, acc: 98.4375, f1: 96.52173913043478, r: 0.6527080011389614
06/02/2019 10:29:13 step: 9420, epoch: 285, batch: 14, loss: 0.13026627898216248, acc: 95.3125, f1: 95.46922288584172, r: 0.6985029293987969
06/02/2019 10:29:14 step: 9425, epoch: 285, batch: 19, loss: 0.10914283245801926, acc: 95.3125, f1: 83.96885521885523, r: 0.8069738829081142
06/02/2019 10:29:16 step: 9430, epoch: 285, batch: 24, loss: 0.10165293514728546, acc: 96.875, f1: 79.65367965367966, r: 0.6357310647656274
06/02/2019 10:29:17 step: 9435, epoch: 285, batch: 29, loss: 0.13934597373008728, acc: 95.3125, f1: 94.2687074829932, r: 0.641559999350662
06/02/2019 10:29:17 *** evaluating ***
06/02/2019 10:29:18 step: 286, epoch: 285, acc: 59.401709401709404, f1: 30.192217631113405, r: 0.281433170774004
06/02/2019 10:29:18 *** epoch: 287 ***
06/02/2019 10:29:18 *** training ***
06/02/2019 10:29:19 step: 9443, epoch: 286, batch: 4, loss: 0.14438322186470032, acc: 93.75, f1: 86.70859538784069, r: 0.6882036543684198
06/02/2019 10:29:20 step: 9448, epoch: 286, batch: 9, loss: 0.11219795793294907, acc: 93.75, f1: 79.92127496159755, r: 0.7446483852604068
06/02/2019 10:29:21 step: 9453, epoch: 286, batch: 14, loss: 0.20554527640342712, acc: 90.625, f1: 86.2379162191192, r: 0.6802678658676284
06/02/2019 10:29:22 step: 9458, epoch: 286, batch: 19, loss: 0.1352524608373642, acc: 93.75, f1: 89.8234445293269, r: 0.6910501831381177
06/02/2019 10:29:23 step: 9463, epoch: 286, batch: 24, loss: 0.07089991122484207, acc: 100.0, f1: 100.0, r: 0.6895440735955731
06/02/2019 10:29:24 step: 9468, epoch: 286, batch: 29, loss: 0.15447330474853516, acc: 96.875, f1: 92.72893772893774, r: 0.7773977471205
06/02/2019 10:29:25 *** evaluating ***
06/02/2019 10:29:25 step: 287, epoch: 286, acc: 57.692307692307686, f1: 29.01856401856402, r: 0.27643875298021403
06/02/2019 10:29:25 *** epoch: 288 ***
06/02/2019 10:29:25 *** training ***
06/02/2019 10:29:27 step: 9476, epoch: 287, batch: 4, loss: 0.14616478979587555, acc: 95.3125, f1: 91.94644424934152, r: 0.6814777451435502
06/02/2019 10:29:28 step: 9481, epoch: 287, batch: 9, loss: 0.10850737243890762, acc: 96.875, f1: 93.19327731092437, r: 0.7956632498942502
06/02/2019 10:29:29 step: 9486, epoch: 287, batch: 14, loss: 0.14204485714435577, acc: 95.3125, f1: 93.29578426352619, r: 0.625804776541295
06/02/2019 10:29:30 step: 9491, epoch: 287, batch: 19, loss: 0.21749931573867798, acc: 93.75, f1: 91.11540549234047, r: 0.727492539266406
06/02/2019 10:29:31 step: 9496, epoch: 287, batch: 24, loss: 0.15285655856132507, acc: 93.75, f1: 77.82154249259511, r: 0.7098243327905885
06/02/2019 10:29:32 step: 9501, epoch: 287, batch: 29, loss: 0.18819445371627808, acc: 92.1875, f1: 79.85165819843239, r: 0.6730624815650488
06/02/2019 10:29:33 *** evaluating ***
06/02/2019 10:29:34 step: 288, epoch: 287, acc: 58.119658119658126, f1: 28.797032645716858, r: 0.27552318630241046
06/02/2019 10:29:34 *** epoch: 289 ***
06/02/2019 10:29:34 *** training ***
06/02/2019 10:29:35 step: 9509, epoch: 288, batch: 4, loss: 0.19468747079372406, acc: 92.1875, f1: 74.278887182113, r: 0.6115671662983806
06/02/2019 10:29:36 step: 9514, epoch: 288, batch: 9, loss: 0.12260797619819641, acc: 98.4375, f1: 97.88359788359789, r: 0.71505086082363
06/02/2019 10:29:37 step: 9519, epoch: 288, batch: 14, loss: 0.11478186398744583, acc: 96.875, f1: 92.81117852546424, r: 0.6677576915445712
06/02/2019 10:29:38 step: 9524, epoch: 288, batch: 19, loss: 0.1493377536535263, acc: 95.3125, f1: 94.47433867919042, r: 0.6995107957090542
06/02/2019 10:29:39 step: 9529, epoch: 288, batch: 24, loss: 0.2296256273984909, acc: 89.0625, f1: 88.11628186628187, r: 0.7492294694851571
06/02/2019 10:29:40 step: 9534, epoch: 288, batch: 29, loss: 0.0533997118473053, acc: 98.4375, f1: 99.16883116883118, r: 0.7211919442380742
06/02/2019 10:29:41 *** evaluating ***
06/02/2019 10:29:41 step: 289, epoch: 288, acc: 56.837606837606835, f1: 29.583287262273327, r: 0.28725270249966495
06/02/2019 10:29:41 *** epoch: 290 ***
06/02/2019 10:29:41 *** training ***
06/02/2019 10:29:43 step: 9542, epoch: 289, batch: 4, loss: 0.12151715904474258, acc: 95.3125, f1: 92.66941391941393, r: 0.7805553263308487
06/02/2019 10:29:44 step: 9547, epoch: 289, batch: 9, loss: 0.07454310357570648, acc: 98.4375, f1: 95.84415584415584, r: 0.7445080738600669
06/02/2019 10:29:45 step: 9552, epoch: 289, batch: 14, loss: 0.18586871027946472, acc: 92.1875, f1: 85.26077097505667, r: 0.7394641130850248
06/02/2019 10:29:46 step: 9557, epoch: 289, batch: 19, loss: 0.12413445115089417, acc: 93.75, f1: 74.9899479311244, r: 0.5178918294628636
06/02/2019 10:29:47 step: 9562, epoch: 289, batch: 24, loss: 0.1196116954088211, acc: 90.625, f1: 76.8953634085213, r: 0.7142401250535085
06/02/2019 10:29:48 step: 9567, epoch: 289, batch: 29, loss: 0.1167127788066864, acc: 100.0, f1: 100.0, r: 0.7638915662029562
06/02/2019 10:29:49 *** evaluating ***
06/02/2019 10:29:49 step: 290, epoch: 289, acc: 55.55555555555556, f1: 28.079795853016098, r: 0.27563764998099644
06/02/2019 10:29:49 *** epoch: 291 ***
06/02/2019 10:29:49 *** training ***
06/02/2019 10:29:51 step: 9575, epoch: 290, batch: 4, loss: 0.13100355863571167, acc: 95.3125, f1: 83.80952380952381, r: 0.7093991579283591
06/02/2019 10:29:52 step: 9580, epoch: 290, batch: 9, loss: 0.1437639594078064, acc: 92.1875, f1: 76.73645320197043, r: 0.7107903254170299
06/02/2019 10:29:53 step: 9585, epoch: 290, batch: 14, loss: 0.2465481162071228, acc: 92.1875, f1: 83.85488323426613, r: 0.6686223289135553
06/02/2019 10:29:54 step: 9590, epoch: 290, batch: 19, loss: 0.09815413504838943, acc: 95.3125, f1: 90.16530644747148, r: 0.6547513343565102
06/02/2019 10:29:55 step: 9595, epoch: 290, batch: 24, loss: 0.19249877333641052, acc: 92.1875, f1: 80.00693933259724, r: 0.7446792680975922
06/02/2019 10:29:56 step: 9600, epoch: 290, batch: 29, loss: 0.12941338121891022, acc: 93.75, f1: 79.47067527576003, r: 0.6886749311317012
06/02/2019 10:29:57 *** evaluating ***
06/02/2019 10:29:58 step: 291, epoch: 290, acc: 57.692307692307686, f1: 28.497265395294136, r: 0.28342013921947234
06/02/2019 10:29:58 *** epoch: 292 ***
06/02/2019 10:29:58 *** training ***
06/02/2019 10:29:59 step: 9608, epoch: 291, batch: 4, loss: 0.10338275879621506, acc: 96.875, f1: 84.12121212121212, r: 0.8207106093363717
06/02/2019 10:30:00 step: 9613, epoch: 291, batch: 9, loss: 0.17972443997859955, acc: 92.1875, f1: 90.45386904761905, r: 0.7519520972065096
06/02/2019 10:30:01 step: 9618, epoch: 291, batch: 14, loss: 0.05776643753051758, acc: 98.4375, f1: 97.64309764309763, r: 0.5765155531197231
06/02/2019 10:30:02 step: 9623, epoch: 291, batch: 19, loss: 0.08678047358989716, acc: 98.4375, f1: 99.33051444679352, r: 0.8195205480014984
06/02/2019 10:30:03 step: 9628, epoch: 291, batch: 24, loss: 0.191534161567688, acc: 92.1875, f1: 86.14230225988702, r: 0.7362803297681032
06/02/2019 10:30:05 step: 9633, epoch: 291, batch: 29, loss: 0.12774142622947693, acc: 96.875, f1: 85.90250902291604, r: 0.7250618097205618
06/02/2019 10:30:05 *** evaluating ***
06/02/2019 10:30:06 step: 292, epoch: 291, acc: 55.98290598290598, f1: 28.90482167063747, r: 0.2907011235918536
06/02/2019 10:30:06 *** epoch: 293 ***
06/02/2019 10:30:06 *** training ***
06/02/2019 10:30:07 step: 9641, epoch: 292, batch: 4, loss: 0.9176220893859863, acc: 76.5625, f1: 58.080514446793515, r: 0.5503751209522694
06/02/2019 10:30:08 step: 9646, epoch: 292, batch: 9, loss: 0.9781419038772583, acc: 79.6875, f1: 61.81177678238148, r: 0.5778851807989548
06/02/2019 10:30:09 step: 9651, epoch: 292, batch: 14, loss: 1.0181411504745483, acc: 68.75, f1: 55.531039043970075, r: 0.5390806778706487
06/02/2019 10:30:10 step: 9656, epoch: 292, batch: 19, loss: 0.4709237217903137, acc: 89.0625, f1: 90.52018633540374, r: 0.6777849902008443
06/02/2019 10:30:11 step: 9661, epoch: 292, batch: 24, loss: 0.4364113211631775, acc: 93.75, f1: 73.39591567852437, r: 0.6535526248919064
06/02/2019 10:30:13 step: 9666, epoch: 292, batch: 29, loss: 0.5026371479034424, acc: 89.0625, f1: 85.19156674527518, r: 0.7799772668003425
06/02/2019 10:30:13 *** evaluating ***
06/02/2019 10:30:14 step: 293, epoch: 292, acc: 54.700854700854705, f1: 32.07579704619903, r: 0.3069695495920649
06/02/2019 10:30:14 *** epoch: 294 ***
06/02/2019 10:30:14 *** training ***
06/02/2019 10:30:15 step: 9674, epoch: 293, batch: 4, loss: 0.47253483533859253, acc: 87.5, f1: 66.23774509803921, r: 0.6836459482444224
06/02/2019 10:30:16 step: 9679, epoch: 293, batch: 9, loss: 0.6112636923789978, acc: 87.5, f1: 84.6031746031746, r: 0.7256476811034158
06/02/2019 10:30:17 step: 9684, epoch: 293, batch: 14, loss: 0.3432558476924896, acc: 93.75, f1: 87.25198412698413, r: 0.626252408305912
06/02/2019 10:30:19 step: 9689, epoch: 293, batch: 19, loss: 0.4012511372566223, acc: 90.625, f1: 80.64935064935065, r: 0.7343059383547709
06/02/2019 10:30:20 step: 9694, epoch: 293, batch: 24, loss: 0.541989266872406, acc: 87.5, f1: 72.43131868131869, r: 0.6433171853220452
06/02/2019 10:30:21 step: 9699, epoch: 293, batch: 29, loss: 0.4517562985420227, acc: 92.1875, f1: 78.78623188405797, r: 0.7288707041291007
06/02/2019 10:30:21 *** evaluating ***
06/02/2019 10:30:22 step: 294, epoch: 293, acc: 55.55555555555556, f1: 27.11554009189956, r: 0.28118080046201116
06/02/2019 10:30:22 *** epoch: 295 ***
06/02/2019 10:30:22 *** training ***
06/02/2019 10:30:23 step: 9707, epoch: 294, batch: 4, loss: 0.3318485617637634, acc: 93.75, f1: 86.93310657596372, r: 0.6094380907788723
06/02/2019 10:30:24 step: 9712, epoch: 294, batch: 9, loss: 0.36990076303482056, acc: 95.3125, f1: 81.17874396135265, r: 0.730406420427101
06/02/2019 10:30:25 step: 9717, epoch: 294, batch: 14, loss: 0.3143477737903595, acc: 93.75, f1: 93.93568840579711, r: 0.7504129167298106
06/02/2019 10:30:26 step: 9722, epoch: 294, batch: 19, loss: 0.31787246465682983, acc: 90.625, f1: 86.96825396825398, r: 0.6756429124917415
06/02/2019 10:30:28 step: 9727, epoch: 294, batch: 24, loss: 0.35374799370765686, acc: 90.625, f1: 90.38843101343102, r: 0.7532331254153738
06/02/2019 10:30:29 step: 9732, epoch: 294, batch: 29, loss: 0.2418423593044281, acc: 96.875, f1: 97.74025974025975, r: 0.647482701395018
06/02/2019 10:30:29 *** evaluating ***
06/02/2019 10:30:30 step: 295, epoch: 294, acc: 54.700854700854705, f1: 28.06552895981087, r: 0.3217858542140573
06/02/2019 10:30:30 *** epoch: 296 ***
06/02/2019 10:30:30 *** training ***
06/02/2019 10:30:31 step: 9740, epoch: 295, batch: 4, loss: 0.2041894495487213, acc: 93.75, f1: 79.9724569533415, r: 0.7482596959052783
06/02/2019 10:30:32 step: 9745, epoch: 295, batch: 9, loss: 0.31126803159713745, acc: 95.3125, f1: 93.17515130208629, r: 0.7268741430260023
06/02/2019 10:30:33 step: 9750, epoch: 295, batch: 14, loss: 0.19488085806369781, acc: 96.875, f1: 95.85126980084964, r: 0.7046446659110963
06/02/2019 10:30:34 step: 9755, epoch: 295, batch: 19, loss: 0.38481152057647705, acc: 90.625, f1: 76.80463052418285, r: 0.5791574567281786
06/02/2019 10:30:35 step: 9760, epoch: 295, batch: 24, loss: 0.16227750480175018, acc: 98.4375, f1: 99.27437641723357, r: 0.6178205667339804
06/02/2019 10:30:37 step: 9765, epoch: 295, batch: 29, loss: 0.30305761098861694, acc: 87.5, f1: 79.01363559608241, r: 0.7297385539929392
06/02/2019 10:30:38 *** evaluating ***
06/02/2019 10:30:38 step: 296, epoch: 295, acc: 53.41880341880342, f1: 28.629966594362877, r: 0.2887409297006101
06/02/2019 10:30:38 *** epoch: 297 ***
06/02/2019 10:30:38 *** training ***
06/02/2019 10:30:39 step: 9773, epoch: 296, batch: 4, loss: 0.24930360913276672, acc: 90.625, f1: 84.2793909430972, r: 0.7535246389305694
06/02/2019 10:30:40 step: 9778, epoch: 296, batch: 9, loss: 0.1923855096101761, acc: 95.3125, f1: 84.43921978404737, r: 0.7982457373843064
06/02/2019 10:30:41 step: 9783, epoch: 296, batch: 14, loss: 0.18876275420188904, acc: 95.3125, f1: 95.85308871023157, r: 0.6234847988054574
06/02/2019 10:30:43 step: 9788, epoch: 296, batch: 19, loss: 0.2590281665325165, acc: 90.625, f1: 72.92008757525998, r: 0.6798508365282003
06/02/2019 10:30:44 step: 9793, epoch: 296, batch: 24, loss: 0.17533940076828003, acc: 95.3125, f1: 84.51344086021506, r: 0.7072958013259933
06/02/2019 10:30:45 step: 9798, epoch: 296, batch: 29, loss: 0.25205472111701965, acc: 95.3125, f1: 91.52965261339645, r: 0.7119558696469124
06/02/2019 10:30:46 *** evaluating ***
06/02/2019 10:30:46 step: 297, epoch: 296, acc: 52.991452991452995, f1: 28.378573106278765, r: 0.29961380161403683
06/02/2019 10:30:46 *** epoch: 298 ***
06/02/2019 10:30:46 *** training ***
06/02/2019 10:30:47 step: 9806, epoch: 297, batch: 4, loss: 0.15426850318908691, acc: 95.3125, f1: 82.37204352186932, r: 0.5644503342951961
06/02/2019 10:30:48 step: 9811, epoch: 297, batch: 9, loss: 0.1728835105895996, acc: 95.3125, f1: 93.54136806237648, r: 0.6086569512458254
06/02/2019 10:30:50 step: 9816, epoch: 297, batch: 14, loss: 0.22356534004211426, acc: 92.1875, f1: 69.55724850184879, r: 0.7407603091492535
06/02/2019 10:30:51 step: 9821, epoch: 297, batch: 19, loss: 0.336137592792511, acc: 87.5, f1: 83.84808174281856, r: 0.619022428730974
06/02/2019 10:30:52 step: 9826, epoch: 297, batch: 24, loss: 0.2230624258518219, acc: 95.3125, f1: 88.04915514592933, r: 0.7501288458482281
06/02/2019 10:30:53 step: 9831, epoch: 297, batch: 29, loss: 0.26459628343582153, acc: 92.1875, f1: 81.0435663627153, r: 0.7643913743382609
06/02/2019 10:30:53 *** evaluating ***
06/02/2019 10:30:54 step: 298, epoch: 297, acc: 57.26495726495726, f1: 29.168545468334823, r: 0.3048015096867006
06/02/2019 10:30:54 *** epoch: 299 ***
06/02/2019 10:30:54 *** training ***
06/02/2019 10:30:55 step: 9839, epoch: 298, batch: 4, loss: 0.2911907732486725, acc: 93.75, f1: 89.78468788887768, r: 0.5942439159032639
06/02/2019 10:30:56 step: 9844, epoch: 298, batch: 9, loss: 0.21907202899456024, acc: 90.625, f1: 69.18981481481481, r: 0.6901138059182533
06/02/2019 10:30:57 step: 9849, epoch: 298, batch: 14, loss: 0.1656656116247177, acc: 93.75, f1: 78.05607651912977, r: 0.7066259236518837
06/02/2019 10:30:58 step: 9854, epoch: 298, batch: 19, loss: 0.18659037351608276, acc: 95.3125, f1: 94.33009045912272, r: 0.6887452243635214
06/02/2019 10:31:00 step: 9859, epoch: 298, batch: 24, loss: 0.2009185403585434, acc: 90.625, f1: 75.78906725458451, r: 0.6150973297435858
06/02/2019 10:31:01 step: 9864, epoch: 298, batch: 29, loss: 0.1869959682226181, acc: 95.3125, f1: 92.93396045138525, r: 0.7429784192378871
06/02/2019 10:31:01 *** evaluating ***
06/02/2019 10:31:02 step: 299, epoch: 298, acc: 57.26495726495726, f1: 29.48574301515478, r: 0.2966108513582233
06/02/2019 10:31:02 *** epoch: 300 ***
06/02/2019 10:31:02 *** training ***
06/02/2019 10:31:03 step: 9872, epoch: 299, batch: 4, loss: 0.2510353624820709, acc: 90.625, f1: 66.84033997611584, r: 0.5780516702734598
06/02/2019 10:31:04 step: 9877, epoch: 299, batch: 9, loss: 0.22346660494804382, acc: 93.75, f1: 87.56004594820385, r: 0.7095807263641123
06/02/2019 10:31:05 step: 9882, epoch: 299, batch: 14, loss: 0.15163995325565338, acc: 96.875, f1: 93.7144098434421, r: 0.6366350927134081
06/02/2019 10:31:07 step: 9887, epoch: 299, batch: 19, loss: 0.1721048355102539, acc: 90.625, f1: 81.14089184060722, r: 0.7562981915354472
06/02/2019 10:31:08 step: 9892, epoch: 299, batch: 24, loss: 0.26999688148498535, acc: 85.9375, f1: 76.77585490231861, r: 0.6333638414657032
06/02/2019 10:31:09 step: 9897, epoch: 299, batch: 29, loss: 0.19338110089302063, acc: 93.75, f1: 78.27998088867655, r: 0.5730900011674996
06/02/2019 10:31:10 *** evaluating ***
06/02/2019 10:31:10 step: 300, epoch: 299, acc: 57.26495726495726, f1: 29.77324332743131, r: 0.29488748176765145
06/02/2019 10:31:10 
*** Best acc model ***
epoch: 114
acc: 61.53846153846154
f1: 28.66052260412062
corr: 0.40129788993083026
06/02/2019 10:31:10 Loading Test Data
06/02/2019 10:31:10 load data from data/word2vec_temp/test_text.npy, data/word2vec_temp/test_label.npy, training: False
06/02/2019 10:31:32 loaded. total len: 2228
06/02/2019 10:31:32 Test: length: 2228, total batch: 35, batch size: 64
06/02/2019 10:31:33 
*** Test Result ***
acc: 57.26495726495726
f1: 29.77324332743131
corr: 0.29488748176765145
