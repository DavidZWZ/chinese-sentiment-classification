06/02/2019 03:44:11 {'input_path': 'data/word2vec_temp', 'output_path': 'save/cnn_7', 'gpu': True, 'seed': 20000125, 'display_per_batch': 5, 'optimizer': 'adagrad', 'lr': 0.01, 'lr_decay': 0, 'weight_decay': 0.0001, 'momentum': 0.985, 'num_epochs': 300, 'batch_size': 64, 'num_labels': 8, 'embedding_size': 300, 'type': 'cnn', 'cnn': {'max_length': 512, 'conv_1': {'size': 512, 'kernel_size': 3}, 'max_pool_1': {'kernel_size': 2, 'stride': 2}, 'fc': {'hidden_size': 512, 'dropout': 0.9}, 'loss': 'cross_entropy'}}
06/02/2019 03:44:11 Loading Train Data
06/02/2019 03:44:11 load data from data/word2vec_temp/train_text.npy, data/word2vec_temp/train_label.npy, training: True
06/02/2019 03:44:37 loaded. total len: 2342
06/02/2019 03:44:37 Train: length: 2108, total batch: 33, batch size: 64
06/02/2019 03:44:37 Dev: length: 234, total batch: 4, batch size: 64
06/02/2019 03:44:37 Loading model cnn
06/02/2019 03:44:47 *** epoch: 1 ***
06/02/2019 03:44:47 *** training ***
06/02/2019 03:44:48 step: 5, epoch: 0, batch: 4, loss: 190.257080078125, acc: 7.8125, f1: 7.046400762752264, r: -0.017113367320131256
06/02/2019 03:44:48 step: 10, epoch: 0, batch: 9, loss: 42.11716079711914, acc: 18.75, f1: 8.509118541033436, r: -0.04359890574216938
06/02/2019 03:44:48 step: 15, epoch: 0, batch: 14, loss: 4.392084121704102, acc: 15.625, f1: 8.170995670995671, r: -0.009064896454608482
06/02/2019 03:44:48 step: 20, epoch: 0, batch: 19, loss: 2.3408875465393066, acc: 37.5, f1: 10.714285714285714, r: 0.024017886051605063
06/02/2019 03:44:49 step: 25, epoch: 0, batch: 24, loss: 2.0140531063079834, acc: 43.75, f1: 7.608695652173914, r: 0.012074261196426479
06/02/2019 03:44:49 step: 30, epoch: 0, batch: 29, loss: 2.0864193439483643, acc: 40.625, f1: 13.159577083845589, r: 0.036429555143264195
06/02/2019 03:44:49 *** evaluating ***
06/02/2019 03:44:49 step: 1, epoch: 0, acc: 45.72649572649573, f1: 11.5066761731916, r: 0.08038760793617786
06/02/2019 03:44:49 *** epoch: 2 ***
06/02/2019 03:44:49 *** training ***
06/02/2019 03:44:49 step: 38, epoch: 1, batch: 4, loss: 2.013500690460205, acc: 43.75, f1: 7.608695652173914, r: -0.07152924986838666
06/02/2019 03:44:50 step: 43, epoch: 1, batch: 9, loss: 1.9841547012329102, acc: 51.5625, f1: 11.821862348178138, r: 0.04653110149962407
06/02/2019 03:44:50 step: 48, epoch: 1, batch: 14, loss: 1.979712724685669, acc: 35.9375, f1: 7.6411960132890355, r: 0.08562601451827187
06/02/2019 03:44:50 step: 53, epoch: 1, batch: 19, loss: 1.9860975742340088, acc: 45.3125, f1: 7.795698924731183, r: 0.04242548675425646
06/02/2019 03:44:50 step: 58, epoch: 1, batch: 24, loss: 1.986388087272644, acc: 37.5, f1: 10.472659870250233, r: 0.06989120535995598
06/02/2019 03:44:50 step: 63, epoch: 1, batch: 29, loss: 2.149606466293335, acc: 43.75, f1: 11.9140989729225, r: -0.006039262936305689
06/02/2019 03:44:51 *** evaluating ***
06/02/2019 03:44:51 step: 2, epoch: 1, acc: 44.44444444444444, f1: 7.854984894259818, r: -0.023481414550010335
06/02/2019 03:44:51 *** epoch: 3 ***
06/02/2019 03:44:51 *** training ***
06/02/2019 03:44:51 step: 71, epoch: 2, batch: 4, loss: 2.00820255279541, acc: 35.9375, f1: 8.812260536398469, r: 0.06474246024705262
06/02/2019 03:44:51 step: 76, epoch: 2, batch: 9, loss: 1.9472887516021729, acc: 37.5, f1: 6.8181818181818175, r: -0.047654781132452154
06/02/2019 03:44:51 step: 81, epoch: 2, batch: 14, loss: 2.042163848876953, acc: 35.9375, f1: 6.686046511627907, r: -0.08597775105277211
06/02/2019 03:44:52 step: 86, epoch: 2, batch: 19, loss: 1.9718194007873535, acc: 43.75, f1: 8.695652173913045, r: 0.061131026268813185
06/02/2019 03:44:52 step: 91, epoch: 2, batch: 24, loss: 1.9578609466552734, acc: 40.625, f1: 7.303370786516854, r: 0.03873645369910124
06/02/2019 03:44:52 step: 96, epoch: 2, batch: 29, loss: 2.1041226387023926, acc: 37.5, f1: 6.896551724137931, r: 0.041868539221552176
06/02/2019 03:44:52 *** evaluating ***
06/02/2019 03:44:53 step: 3, epoch: 2, acc: 44.871794871794876, f1: 7.766272189349112, r: 0.012408659830932822
06/02/2019 03:44:53 *** epoch: 4 ***
06/02/2019 03:44:53 *** training ***
06/02/2019 03:44:53 step: 104, epoch: 3, batch: 4, loss: 1.989715337753296, acc: 31.25, f1: 6.802721088435374, r: nan
06/02/2019 03:44:53 step: 109, epoch: 3, batch: 9, loss: 1.9391764402389526, acc: 57.8125, f1: 12.21122112211221, r: 0.04103588288831591
06/02/2019 03:44:53 step: 114, epoch: 3, batch: 14, loss: 2.0376715660095215, acc: 45.3125, f1: 8.90937019969278, r: -0.05231260589402967
06/02/2019 03:44:53 step: 119, epoch: 3, batch: 19, loss: 1.9646145105361938, acc: 34.375, f1: 8.75, r: 0.0675905353487814
06/02/2019 03:44:54 step: 124, epoch: 3, batch: 24, loss: 1.9230389595031738, acc: 45.3125, f1: 8.90937019969278, r: -0.017012595241245064
06/02/2019 03:44:54 step: 129, epoch: 3, batch: 29, loss: 1.9781681299209595, acc: 32.8125, f1: 6.176470588235294, r: nan
06/02/2019 03:44:54 *** evaluating ***
06/02/2019 03:44:54 step: 4, epoch: 3, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.04145513564304974
06/02/2019 03:44:54 *** epoch: 5 ***
06/02/2019 03:44:54 *** training ***
06/02/2019 03:44:54 step: 137, epoch: 4, batch: 4, loss: 1.9707121849060059, acc: 35.9375, f1: 6.609195402298851, r: 0.018298301948566496
06/02/2019 03:44:55 step: 142, epoch: 4, batch: 9, loss: 2.0510778427124023, acc: 31.25, f1: 6.024096385542169, r: -0.008167168155059027
06/02/2019 03:44:55 step: 147, epoch: 4, batch: 14, loss: 2.07292103767395, acc: 40.625, f1: 7.386363636363637, r: -0.008194965491242934
06/02/2019 03:44:55 step: 152, epoch: 4, batch: 19, loss: 1.9111567735671997, acc: 50.0, f1: 10.606060606060606, r: 0.04302278081668964
06/02/2019 03:44:55 step: 157, epoch: 4, batch: 24, loss: 1.9127678871154785, acc: 42.1875, f1: 8.47723704866562, r: 0.04638204543733534
06/02/2019 03:44:55 step: 162, epoch: 4, batch: 29, loss: 1.9374899864196777, acc: 39.0625, f1: 8.682266009852217, r: 0.03673348353587661
06/02/2019 03:44:55 *** evaluating ***
06/02/2019 03:44:55 step: 5, epoch: 4, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.0832075572707635
06/02/2019 03:44:55 *** epoch: 6 ***
06/02/2019 03:44:55 *** training ***
06/02/2019 03:44:56 step: 170, epoch: 5, batch: 4, loss: 1.9354077577590942, acc: 40.625, f1: 8.346709470304976, r: 0.026515109753053274
06/02/2019 03:44:56 step: 175, epoch: 5, batch: 9, loss: 1.9894452095031738, acc: 26.5625, f1: 6.071428571428571, r: 0.11023852095481185
06/02/2019 03:44:56 step: 180, epoch: 5, batch: 14, loss: 1.94381844997406, acc: 35.9375, f1: 8.033088235294116, r: 0.011510646508291467
06/02/2019 03:44:56 step: 185, epoch: 5, batch: 19, loss: 1.9214447736740112, acc: 43.75, f1: 8.695652173913045, r: nan
06/02/2019 03:44:56 step: 190, epoch: 5, batch: 24, loss: 1.949774146080017, acc: 35.9375, f1: 8.743315508021391, r: -0.003934033902079181
06/02/2019 03:44:57 step: 195, epoch: 5, batch: 29, loss: 1.9455541372299194, acc: 42.1875, f1: 7.5, r: 0.0065502504787810995
06/02/2019 03:44:57 *** evaluating ***
06/02/2019 03:44:57 step: 6, epoch: 5, acc: 50.0, f1: 13.431480048159614, r: 0.10524272763650078
06/02/2019 03:44:57 *** epoch: 7 ***
06/02/2019 03:44:57 *** training ***
06/02/2019 03:44:57 step: 203, epoch: 6, batch: 4, loss: 2.0059988498687744, acc: 39.0625, f1: 8.210180623973727, r: -0.020492384430564926
06/02/2019 03:44:57 step: 208, epoch: 6, batch: 9, loss: 1.9134597778320312, acc: 42.1875, f1: 8.47723704866562, r: 0.050713507225572774
06/02/2019 03:44:58 step: 213, epoch: 6, batch: 14, loss: 1.9424519538879395, acc: 32.8125, f1: 7.0588235294117645, r: 0.020194082882612853
06/02/2019 03:44:58 step: 218, epoch: 6, batch: 19, loss: 1.9095159769058228, acc: 37.5, f1: 7.792207792207792, r: nan
06/02/2019 03:44:58 step: 223, epoch: 6, batch: 24, loss: 1.9258779287338257, acc: 28.125, f1: 6.2717770034843205, r: nan
06/02/2019 03:44:58 step: 228, epoch: 6, batch: 29, loss: 1.9025143384933472, acc: 35.9375, f1: 7.55336617405583, r: 0.08645024101436255
06/02/2019 03:44:59 *** evaluating ***
06/02/2019 03:44:59 step: 7, epoch: 6, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.04586274310263393
06/02/2019 03:44:59 *** epoch: 8 ***
06/02/2019 03:44:59 *** training ***
06/02/2019 03:44:59 step: 236, epoch: 7, batch: 4, loss: 1.93577241897583, acc: 37.5, f1: 6.8181818181818175, r: -0.08006516155723634
06/02/2019 03:44:59 step: 241, epoch: 7, batch: 9, loss: 1.930543303489685, acc: 37.5, f1: 6.8181818181818175, r: nan
06/02/2019 03:44:59 step: 246, epoch: 7, batch: 14, loss: 1.9195970296859741, acc: 45.3125, f1: 11.263736263736263, r: 0.06433779894943585
06/02/2019 03:44:59 step: 251, epoch: 7, batch: 19, loss: 1.888347864151001, acc: 40.625, f1: 9.629629629629628, r: 0.03830058289409216
06/02/2019 03:45:00 step: 256, epoch: 7, batch: 24, loss: 1.8801500797271729, acc: 37.5, f1: 11.686046511627907, r: 0.07729863121197421
06/02/2019 03:45:00 step: 261, epoch: 7, batch: 29, loss: 1.896289587020874, acc: 35.9375, f1: 6.609195402298851, r: -0.013760127515303758
06/02/2019 03:45:00 *** evaluating ***
06/02/2019 03:45:00 step: 8, epoch: 7, acc: 45.2991452991453, f1: 8.266272189349113, r: 0.061588818455099316
06/02/2019 03:45:00 *** epoch: 9 ***
06/02/2019 03:45:00 *** training ***
06/02/2019 03:45:00 step: 269, epoch: 8, batch: 4, loss: 1.8882979154586792, acc: 32.8125, f1: 6.176470588235294, r: 0.01713491979945717
06/02/2019 03:45:00 step: 274, epoch: 8, batch: 9, loss: 1.9493144750595093, acc: 34.375, f1: 6.395348837209303, r: nan
06/02/2019 03:45:01 step: 279, epoch: 8, batch: 14, loss: 1.9033548831939697, acc: 39.0625, f1: 8.025682182985554, r: nan
06/02/2019 03:45:01 step: 284, epoch: 8, batch: 19, loss: 1.8965380191802979, acc: 37.5, f1: 7.792207792207792, r: nan
06/02/2019 03:45:01 step: 289, epoch: 8, batch: 24, loss: 1.8812922239303589, acc: 48.4375, f1: 9.323308270676693, r: 0.031111548872512638
06/02/2019 03:45:01 step: 294, epoch: 8, batch: 29, loss: 1.977563500404358, acc: 26.5625, f1: 5.246913580246913, r: nan
06/02/2019 03:45:01 *** evaluating ***
06/02/2019 03:45:01 step: 9, epoch: 8, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.026260726943900258
06/02/2019 03:45:01 *** epoch: 10 ***
06/02/2019 03:45:01 *** training ***
06/02/2019 03:45:02 step: 302, epoch: 9, batch: 4, loss: 1.9370448589324951, acc: 25.0, f1: 5.0, r: nan
06/02/2019 03:45:02 step: 307, epoch: 9, batch: 9, loss: 1.8363150358200073, acc: 42.1875, f1: 8.47723704866562, r: 0.015320337497109414
06/02/2019 03:45:02 step: 312, epoch: 9, batch: 14, loss: 1.8874250650405884, acc: 43.75, f1: 8.695652173913045, r: nan
06/02/2019 03:45:02 step: 317, epoch: 9, batch: 19, loss: 1.8988542556762695, acc: 37.5, f1: 7.792207792207792, r: nan
06/02/2019 03:45:03 step: 322, epoch: 9, batch: 24, loss: 1.8185111284255981, acc: 40.625, f1: 8.253968253968253, r: 0.06388291451619081
06/02/2019 03:45:03 step: 327, epoch: 9, batch: 29, loss: 1.8802436590194702, acc: 43.75, f1: 8.695652173913045, r: -0.07536242221176465
06/02/2019 03:45:03 *** evaluating ***
06/02/2019 03:45:03 step: 10, epoch: 9, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.04041897604274997
06/02/2019 03:45:03 *** epoch: 11 ***
06/02/2019 03:45:03 *** training ***
06/02/2019 03:45:03 step: 335, epoch: 10, batch: 4, loss: 1.8508543968200684, acc: 40.625, f1: 13.35227272727273, r: 0.08406256451098187
06/02/2019 03:45:04 step: 340, epoch: 10, batch: 9, loss: 1.9393579959869385, acc: 35.9375, f1: 6.686046511627906, r: 0.027909416181359253
06/02/2019 03:45:04 step: 345, epoch: 10, batch: 14, loss: 1.8086172342300415, acc: 46.875, f1: 7.978723404255319, r: 0.05479055820416095
06/02/2019 03:45:04 step: 350, epoch: 10, batch: 19, loss: 1.9577020406723022, acc: 32.8125, f1: 7.0588235294117645, r: -0.0019168538006551208
06/02/2019 03:45:04 step: 355, epoch: 10, batch: 24, loss: 1.9135574102401733, acc: 34.375, f1: 6.395348837209303, r: nan
06/02/2019 03:45:05 step: 360, epoch: 10, batch: 29, loss: 1.8447304964065552, acc: 46.875, f1: 9.118541033434651, r: 0.0894022355100842
06/02/2019 03:45:05 *** evaluating ***
06/02/2019 03:45:05 step: 11, epoch: 10, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.01736311050175087
06/02/2019 03:45:05 *** epoch: 12 ***
06/02/2019 03:45:05 *** training ***
06/02/2019 03:45:05 step: 368, epoch: 11, batch: 4, loss: 1.8817275762557983, acc: 37.5, f1: 7.792207792207792, r: 0.0200008267983861
06/02/2019 03:45:05 step: 373, epoch: 11, batch: 9, loss: 1.9001765251159668, acc: 23.4375, f1: 5.4249547920434, r: 0.08097801691944066
06/02/2019 03:45:05 step: 378, epoch: 11, batch: 14, loss: 1.9212976694107056, acc: 34.375, f1: 6.470588235294117, r: -0.031007437737001375
06/02/2019 03:45:06 step: 383, epoch: 11, batch: 19, loss: 2.5325989723205566, acc: 43.75, f1: 8.88888888888889, r: 0.005300979559786624
06/02/2019 03:45:06 step: 388, epoch: 11, batch: 24, loss: 1.8614685535430908, acc: 37.5, f1: 7.792207792207792, r: 0.039160240229619385
06/02/2019 03:45:06 step: 393, epoch: 11, batch: 29, loss: 1.9075530767440796, acc: 37.5, f1: 7.792207792207792, r: 0.03949039242132352
06/02/2019 03:45:06 *** evaluating ***
06/02/2019 03:45:06 step: 12, epoch: 11, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.0292067776920182
06/02/2019 03:45:06 *** epoch: 13 ***
06/02/2019 03:45:06 *** training ***
06/02/2019 03:45:06 step: 401, epoch: 12, batch: 4, loss: 1.9384111166000366, acc: 39.0625, f1: 7.02247191011236, r: 0.04600194746222743
06/02/2019 03:45:06 step: 406, epoch: 12, batch: 9, loss: 1.9167721271514893, acc: 37.5, f1: 6.8181818181818175, r: nan
06/02/2019 03:45:07 step: 411, epoch: 12, batch: 14, loss: 1.9089529514312744, acc: 42.1875, f1: 7.417582417582418, r: nan
06/02/2019 03:45:07 step: 416, epoch: 12, batch: 19, loss: 1.8376359939575195, acc: 45.3125, f1: 7.795698924731183, r: -0.02629006403145277
06/02/2019 03:45:07 step: 421, epoch: 12, batch: 24, loss: 1.8599270582199097, acc: 42.1875, f1: 8.47723704866562, r: -0.04723608305884605
06/02/2019 03:45:07 step: 426, epoch: 12, batch: 29, loss: 1.873236894607544, acc: 34.375, f1: 6.395348837209303, r: 0.0658376085168908
06/02/2019 03:45:07 *** evaluating ***
06/02/2019 03:45:08 step: 13, epoch: 12, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.05579921575240158
06/02/2019 03:45:08 *** epoch: 14 ***
06/02/2019 03:45:08 *** training ***
06/02/2019 03:45:08 step: 434, epoch: 13, batch: 4, loss: 1.8452003002166748, acc: 35.9375, f1: 16.918767507002798, r: 0.08858551907557997
06/02/2019 03:45:08 step: 439, epoch: 13, batch: 9, loss: 1.7741166353225708, acc: 51.5625, f1: 14.603174603174601, r: 0.11345414862221773
06/02/2019 03:45:08 step: 444, epoch: 13, batch: 14, loss: 2.0334877967834473, acc: 42.1875, f1: 8.47723704866562, r: -0.04807724854314126
06/02/2019 03:45:09 step: 449, epoch: 13, batch: 19, loss: 1.8465642929077148, acc: 39.0625, f1: 7.02247191011236, r: 0.08849956843662438
06/02/2019 03:45:09 step: 454, epoch: 13, batch: 24, loss: 1.8394320011138916, acc: 40.625, f1: 10.6737012987013, r: 0.0549332744454244
06/02/2019 03:45:09 step: 459, epoch: 13, batch: 29, loss: 1.785481333732605, acc: 45.3125, f1: 8.90937019969278, r: 0.05480070164126948
06/02/2019 03:45:09 *** evaluating ***
06/02/2019 03:45:09 step: 14, epoch: 13, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.05297688419256981
06/02/2019 03:45:09 *** epoch: 15 ***
06/02/2019 03:45:09 *** training ***
06/02/2019 03:45:10 step: 467, epoch: 14, batch: 4, loss: 2.1371753215789795, acc: 31.25, f1: 5.952380952380952, r: -0.04636768305157841
06/02/2019 03:45:10 step: 472, epoch: 14, batch: 9, loss: 1.919950008392334, acc: 31.25, f1: 6.802721088435374, r: nan
06/02/2019 03:45:10 step: 477, epoch: 14, batch: 14, loss: 1.8700608015060425, acc: 42.1875, f1: 7.5, r: 0.012903543721827792
06/02/2019 03:45:10 step: 482, epoch: 14, batch: 19, loss: 1.8036305904388428, acc: 43.75, f1: 8.695652173913045, r: 0.05297234941106252
06/02/2019 03:45:10 step: 487, epoch: 14, batch: 24, loss: 1.890796422958374, acc: 37.5, f1: 7.8817733990147785, r: 0.058391605753429096
06/02/2019 03:45:11 step: 492, epoch: 14, batch: 29, loss: 1.8802207708358765, acc: 37.5, f1: 10.257475083056477, r: 0.07639948791701606
06/02/2019 03:45:11 *** evaluating ***
06/02/2019 03:45:11 step: 15, epoch: 14, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.09580898781679198
06/02/2019 03:45:11 *** epoch: 16 ***
06/02/2019 03:45:11 *** training ***
06/02/2019 03:45:11 step: 500, epoch: 15, batch: 4, loss: 1.875542402267456, acc: 37.5, f1: 7.792207792207792, r: -0.004744243804232633
06/02/2019 03:45:11 step: 505, epoch: 15, batch: 9, loss: 1.8643157482147217, acc: 35.9375, f1: 7.55336617405583, r: -0.0533864893252154
06/02/2019 03:45:11 step: 510, epoch: 15, batch: 14, loss: 1.8698859214782715, acc: 31.25, f1: 6.802721088435374, r: 0.06150003586394754
06/02/2019 03:45:12 step: 515, epoch: 15, batch: 19, loss: 1.9180036783218384, acc: 29.6875, f1: 6.540447504302925, r: -0.01883397881964105
06/02/2019 03:45:12 step: 520, epoch: 15, batch: 24, loss: 1.7494785785675049, acc: 45.3125, f1: 7.795698924731183, r: 0.09152617397952796
06/02/2019 03:45:12 step: 525, epoch: 15, batch: 29, loss: 1.900778889656067, acc: 39.0625, f1: 7.1022727272727275, r: 0.09541207493926918
06/02/2019 03:45:12 *** evaluating ***
06/02/2019 03:45:12 step: 16, epoch: 15, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.05366160457097061
06/02/2019 03:45:12 *** epoch: 17 ***
06/02/2019 03:45:12 *** training ***
06/02/2019 03:45:12 step: 533, epoch: 16, batch: 4, loss: 1.7974560260772705, acc: 40.625, f1: 15.43560606060606, r: 0.107022777620861
06/02/2019 03:45:13 step: 538, epoch: 16, batch: 9, loss: 1.8475574254989624, acc: 39.0625, f1: 8.025682182985554, r: 0.06768752854934784
06/02/2019 03:45:13 step: 543, epoch: 16, batch: 14, loss: 1.850975751876831, acc: 39.0625, f1: 8.116883116883118, r: 0.08497221553645871
06/02/2019 03:45:13 step: 548, epoch: 16, batch: 19, loss: 1.8685142993927002, acc: 37.5, f1: 6.8181818181818175, r: nan
06/02/2019 03:45:13 step: 553, epoch: 16, batch: 24, loss: 1.8786234855651855, acc: 34.375, f1: 6.395348837209303, r: 0.05992518845389383
06/02/2019 03:45:14 step: 558, epoch: 16, batch: 29, loss: 1.820538878440857, acc: 43.75, f1: 8.695652173913045, r: 0.040854723987643685
06/02/2019 03:45:14 *** evaluating ***
06/02/2019 03:45:14 step: 17, epoch: 16, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.06948742414748539
06/02/2019 03:45:14 *** epoch: 18 ***
06/02/2019 03:45:14 *** training ***
06/02/2019 03:45:14 step: 566, epoch: 17, batch: 4, loss: 1.9064594507217407, acc: 35.9375, f1: 7.55336617405583, r: -0.003252751140223241
06/02/2019 03:45:14 step: 571, epoch: 17, batch: 9, loss: 1.9354485273361206, acc: 35.9375, f1: 7.55336617405583, r: -0.010362261947211397
06/02/2019 03:45:15 step: 576, epoch: 17, batch: 14, loss: 1.8155717849731445, acc: 37.5, f1: 7.792207792207792, r: nan
06/02/2019 03:45:15 step: 581, epoch: 17, batch: 19, loss: 1.826650857925415, acc: 43.75, f1: 8.695652173913045, r: 0.0034124880506842025
06/02/2019 03:45:15 step: 586, epoch: 17, batch: 24, loss: 1.8993217945098877, acc: 29.6875, f1: 5.72289156626506, r: -0.005072655607849404
06/02/2019 03:45:15 step: 591, epoch: 17, batch: 29, loss: 1.8071963787078857, acc: 37.5, f1: 9.186046511627907, r: 0.045316010679018595
06/02/2019 03:45:16 *** evaluating ***
06/02/2019 03:45:16 step: 18, epoch: 17, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.05521631040794854
06/02/2019 03:45:16 *** epoch: 19 ***
06/02/2019 03:45:16 *** training ***
06/02/2019 03:45:16 step: 599, epoch: 18, batch: 4, loss: 1.8398034572601318, acc: 40.625, f1: 7.222222222222221, r: 0.1266317986488672
06/02/2019 03:45:16 step: 604, epoch: 18, batch: 9, loss: 1.8309743404388428, acc: 39.0625, f1: 8.025682182985554, r: 0.024297720328710283
06/02/2019 03:45:16 step: 609, epoch: 18, batch: 14, loss: 1.7996923923492432, acc: 48.4375, f1: 8.157894736842106, r: 0.04648516532131138
06/02/2019 03:45:16 step: 614, epoch: 18, batch: 19, loss: 1.8971210718154907, acc: 35.9375, f1: 7.55336617405583, r: nan
06/02/2019 03:45:17 step: 619, epoch: 18, batch: 24, loss: 1.7589811086654663, acc: 45.3125, f1: 8.90937019969278, r: 0.09879593091110032
06/02/2019 03:45:17 step: 624, epoch: 18, batch: 29, loss: 1.840296745300293, acc: 31.25, f1: 8.300881328141012, r: 0.04391895824679394
06/02/2019 03:45:17 *** evaluating ***
06/02/2019 03:45:17 step: 19, epoch: 18, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.06657557872811778
06/02/2019 03:45:17 *** epoch: 20 ***
06/02/2019 03:45:17 *** training ***
06/02/2019 03:45:17 step: 632, epoch: 19, batch: 4, loss: 1.8716984987258911, acc: 37.5, f1: 6.8181818181818175, r: 0.0765276869550953
06/02/2019 03:45:17 step: 637, epoch: 19, batch: 9, loss: 1.8809635639190674, acc: 35.9375, f1: 6.609195402298851, r: 0.028748245368342733
06/02/2019 03:45:18 step: 642, epoch: 19, batch: 14, loss: 1.830639123916626, acc: 39.0625, f1: 8.025682182985554, r: 0.0720919778909169
06/02/2019 03:45:18 step: 647, epoch: 19, batch: 19, loss: 1.9079862833023071, acc: 37.5, f1: 9.54595791805094, r: -0.008815876713135287
06/02/2019 03:45:18 step: 652, epoch: 19, batch: 24, loss: 1.8317832946777344, acc: 39.0625, f1: 8.025682182985554, r: nan
06/02/2019 03:45:18 step: 657, epoch: 19, batch: 29, loss: 1.9141945838928223, acc: 37.5, f1: 9.186046511627907, r: 0.03274699053131191
06/02/2019 03:45:18 *** evaluating ***
06/02/2019 03:45:18 step: 20, epoch: 19, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.04785460036911663
06/02/2019 03:45:18 *** epoch: 21 ***
06/02/2019 03:45:18 *** training ***
06/02/2019 03:45:19 step: 665, epoch: 20, batch: 4, loss: 1.7953108549118042, acc: 37.5, f1: 7.792207792207792, r: 0.03253281991114457
06/02/2019 03:45:19 step: 670, epoch: 20, batch: 9, loss: 1.9036483764648438, acc: 29.6875, f1: 5.72289156626506, r: -0.05908919907438809
06/02/2019 03:45:19 step: 675, epoch: 20, batch: 14, loss: 1.7679908275604248, acc: 42.1875, f1: 11.2987012987013, r: 0.024861246319671598
06/02/2019 03:45:20 step: 680, epoch: 20, batch: 19, loss: 1.9826580286026, acc: 26.5625, f1: 5.448717948717948, r: -0.020441351895837175
06/02/2019 03:45:20 step: 685, epoch: 20, batch: 24, loss: 1.8875020742416382, acc: 35.9375, f1: 6.609195402298851, r: nan
06/02/2019 03:45:20 step: 690, epoch: 20, batch: 29, loss: 1.7945821285247803, acc: 42.1875, f1: 7.417582417582418, r: 0.018501534512537582
06/02/2019 03:45:20 *** evaluating ***
06/02/2019 03:45:20 step: 21, epoch: 20, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.02611660430870185
06/02/2019 03:45:20 *** epoch: 22 ***
06/02/2019 03:45:20 *** training ***
06/02/2019 03:45:21 step: 698, epoch: 21, batch: 4, loss: 1.7880762815475464, acc: 34.375, f1: 9.047619047619047, r: -0.004407222484645229
06/02/2019 03:45:21 step: 703, epoch: 21, batch: 9, loss: 1.8336161375045776, acc: 40.625, f1: 8.253968253968253, r: nan
06/02/2019 03:45:21 step: 708, epoch: 21, batch: 14, loss: 1.8013207912445068, acc: 45.3125, f1: 7.795698924731183, r: nan
06/02/2019 03:45:21 step: 713, epoch: 21, batch: 19, loss: 1.8761118650436401, acc: 26.5625, f1: 5.246913580246913, r: 0.06149169013972524
06/02/2019 03:45:21 step: 718, epoch: 21, batch: 24, loss: 1.8051397800445557, acc: 37.5, f1: 21.926910299003325, r: 0.10694323708237292
06/02/2019 03:45:22 step: 723, epoch: 21, batch: 29, loss: 1.7523260116577148, acc: 45.3125, f1: 10.05050505050505, r: 0.10792644439481064
06/02/2019 03:45:22 *** evaluating ***
06/02/2019 03:45:22 step: 22, epoch: 21, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.049564889823382176
06/02/2019 03:45:22 *** epoch: 23 ***
06/02/2019 03:45:22 *** training ***
06/02/2019 03:45:22 step: 731, epoch: 22, batch: 4, loss: 1.9100286960601807, acc: 34.375, f1: 6.626506024096386, r: 0.0375047198884276
06/02/2019 03:45:22 step: 736, epoch: 22, batch: 9, loss: 1.7807525396347046, acc: 42.1875, f1: 8.47723704866562, r: 0.033840731152569234
06/02/2019 03:45:22 step: 741, epoch: 22, batch: 14, loss: 1.8716766834259033, acc: 42.1875, f1: 7.417582417582418, r: 0.053824828974806525
06/02/2019 03:45:23 step: 746, epoch: 22, batch: 19, loss: 1.8531973361968994, acc: 43.75, f1: 7.608695652173914, r: nan
06/02/2019 03:45:23 step: 751, epoch: 22, batch: 24, loss: 1.8431819677352905, acc: 35.9375, f1: 9.325396825396826, r: 0.0921955479591973
06/02/2019 03:45:23 step: 756, epoch: 22, batch: 29, loss: 1.835290551185608, acc: 39.0625, f1: 7.02247191011236, r: nan
06/02/2019 03:45:23 *** evaluating ***
06/02/2019 03:45:23 step: 23, epoch: 22, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.05019871359201826
06/02/2019 03:45:23 *** epoch: 24 ***
06/02/2019 03:45:23 *** training ***
06/02/2019 03:45:23 step: 764, epoch: 23, batch: 4, loss: 1.7943551540374756, acc: 34.375, f1: 7.308970099667775, r: 0.05898798426585397
06/02/2019 03:45:24 step: 769, epoch: 23, batch: 9, loss: 1.7615591287612915, acc: 46.875, f1: 7.9787234042553195, r: 0.005560915822229977
06/02/2019 03:45:24 step: 774, epoch: 23, batch: 14, loss: 1.8702715635299683, acc: 35.9375, f1: 9.299719887955185, r: 0.08791014029408394
06/02/2019 03:45:24 step: 779, epoch: 23, batch: 19, loss: 1.7822357416152954, acc: 45.3125, f1: 10.39426523297491, r: -0.01967610785253946
06/02/2019 03:45:24 step: 784, epoch: 23, batch: 24, loss: 1.8094136714935303, acc: 32.8125, f1: 7.660060975609756, r: 0.09481757500565034
06/02/2019 03:45:25 step: 789, epoch: 23, batch: 29, loss: 1.8519439697265625, acc: 35.9375, f1: 11.038439472174412, r: 0.12088133490344677
06/02/2019 03:45:25 *** evaluating ***
06/02/2019 03:45:25 step: 24, epoch: 23, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.030420472481757888
06/02/2019 03:45:25 *** epoch: 25 ***
06/02/2019 03:45:25 *** training ***
06/02/2019 03:45:25 step: 797, epoch: 24, batch: 4, loss: 1.9030383825302124, acc: 39.0625, f1: 7.02247191011236, r: 0.03380916450134837
06/02/2019 03:45:25 step: 802, epoch: 24, batch: 9, loss: 1.844167947769165, acc: 40.625, f1: 7.222222222222221, r: nan
06/02/2019 03:45:26 step: 807, epoch: 24, batch: 14, loss: 1.8046218156814575, acc: 45.3125, f1: 8.90937019969278, r: nan
06/02/2019 03:45:26 step: 812, epoch: 24, batch: 19, loss: 1.8353970050811768, acc: 37.5, f1: 9.838998211091234, r: 0.00903839437384716
06/02/2019 03:45:26 step: 817, epoch: 24, batch: 24, loss: 1.8074073791503906, acc: 35.9375, f1: 7.55336617405583, r: -0.07247574734532508
06/02/2019 03:45:26 step: 822, epoch: 24, batch: 29, loss: 1.7345551252365112, acc: 42.1875, f1: 7.417582417582418, r: 0.06299175683133713
06/02/2019 03:45:26 *** evaluating ***
06/02/2019 03:45:27 step: 25, epoch: 24, acc: 47.008547008547005, f1: 10.155610155610157, r: 0.07427805911616454
06/02/2019 03:45:27 *** epoch: 26 ***
06/02/2019 03:45:27 *** training ***
06/02/2019 03:45:27 step: 830, epoch: 25, batch: 4, loss: 1.785327434539795, acc: 35.9375, f1: 8.982259570494865, r: 0.011772160449458911
06/02/2019 03:45:27 step: 835, epoch: 25, batch: 9, loss: 1.7436591386795044, acc: 45.3125, f1: 7.880434782608696, r: 0.02314119290141174
06/02/2019 03:45:27 step: 840, epoch: 25, batch: 14, loss: 1.8724614381790161, acc: 37.5, f1: 7.8817733990147785, r: -0.0019092617415940108
06/02/2019 03:45:27 step: 845, epoch: 25, batch: 19, loss: 1.7903494834899902, acc: 50.0, f1: 8.333333333333332, r: 0.018833684369868563
06/02/2019 03:45:28 step: 850, epoch: 25, batch: 24, loss: 1.7841854095458984, acc: 40.625, f1: 8.253968253968253, r: 0.03915239160427279
06/02/2019 03:45:28 step: 855, epoch: 25, batch: 29, loss: 1.8319300413131714, acc: 31.25, f1: 7.181571815718158, r: 0.039832653573723155
06/02/2019 03:45:28 *** evaluating ***
06/02/2019 03:45:28 step: 26, epoch: 25, acc: 46.58119658119658, f1: 9.7111332889776, r: 0.08875879951261059
06/02/2019 03:45:28 *** epoch: 27 ***
06/02/2019 03:45:28 *** training ***
06/02/2019 03:45:28 step: 863, epoch: 26, batch: 4, loss: 1.7719874382019043, acc: 37.5, f1: 9.811046511627906, r: 0.09742661589206764
06/02/2019 03:45:28 step: 868, epoch: 26, batch: 9, loss: 1.8904703855514526, acc: 40.625, f1: 7.303370786516854, r: 0.06939509887829794
06/02/2019 03:45:29 step: 873, epoch: 26, batch: 14, loss: 1.7771276235580444, acc: 37.5, f1: 7.8817733990147785, r: 0.008939220894330628
06/02/2019 03:45:29 step: 878, epoch: 26, batch: 19, loss: 1.7941049337387085, acc: 37.5, f1: 6.8181818181818175, r: 0.053881897894513836
06/02/2019 03:45:29 step: 883, epoch: 26, batch: 24, loss: 1.7581669092178345, acc: 42.1875, f1: 7.417582417582418, r: 0.026750955960616763
06/02/2019 03:45:29 step: 888, epoch: 26, batch: 29, loss: 1.8374860286712646, acc: 29.6875, f1: 5.72289156626506, r: 0.07188034598500734
06/02/2019 03:45:29 *** evaluating ***
06/02/2019 03:45:29 step: 27, epoch: 26, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.06264979577168549
06/02/2019 03:45:29 *** epoch: 28 ***
06/02/2019 03:45:29 *** training ***
06/02/2019 03:45:30 step: 896, epoch: 27, batch: 4, loss: 1.790603756904602, acc: 39.0625, f1: 8.025682182985554, r: 0.02183109161938713
06/02/2019 03:45:30 step: 901, epoch: 27, batch: 9, loss: 1.7753922939300537, acc: 43.75, f1: 8.695652173913045, r: nan
06/02/2019 03:45:30 step: 906, epoch: 27, batch: 14, loss: 1.8841633796691895, acc: 25.0, f1: 5.0, r: 0.07584692920764623
06/02/2019 03:45:30 step: 911, epoch: 27, batch: 19, loss: 1.7440500259399414, acc: 39.0625, f1: 8.025682182985554, r: 0.07965107233265621
06/02/2019 03:45:31 step: 916, epoch: 27, batch: 24, loss: 1.7482372522354126, acc: 45.3125, f1: 7.795698924731183, r: 0.03224444176605362
06/02/2019 03:45:31 step: 921, epoch: 27, batch: 29, loss: 1.8357057571411133, acc: 32.8125, f1: 11.69753086419753, r: 0.08004250697238374
06/02/2019 03:45:31 *** evaluating ***
06/02/2019 03:45:31 step: 28, epoch: 27, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.07160194523949488
06/02/2019 03:45:31 *** epoch: 29 ***
06/02/2019 03:45:31 *** training ***
06/02/2019 03:45:31 step: 929, epoch: 28, batch: 4, loss: 1.715612769126892, acc: 48.4375, f1: 8.157894736842106, r: 0.10269494543592846
06/02/2019 03:45:32 step: 934, epoch: 28, batch: 9, loss: 1.80986750125885, acc: 42.1875, f1: 8.47723704866562, r: 0.0679827074299181
06/02/2019 03:45:32 step: 939, epoch: 28, batch: 14, loss: 1.7371243238449097, acc: 45.3125, f1: 10.39426523297491, r: 0.07031218989969719
06/02/2019 03:45:32 step: 944, epoch: 28, batch: 19, loss: 1.5870718955993652, acc: 48.4375, f1: 14.55858205351106, r: 0.14603740586310682
06/02/2019 03:45:32 step: 949, epoch: 28, batch: 24, loss: 1.8973385095596313, acc: 39.0625, f1: 8.025682182985554, r: 0.050546218212088635
06/02/2019 03:45:32 step: 954, epoch: 28, batch: 29, loss: 1.839646577835083, acc: 37.5, f1: 6.8181818181818175, r: 0.07016021047194657
06/02/2019 03:45:33 *** evaluating ***
06/02/2019 03:45:33 step: 29, epoch: 28, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.06326755965126546
06/02/2019 03:45:33 *** epoch: 30 ***
06/02/2019 03:45:33 *** training ***
06/02/2019 03:45:33 step: 962, epoch: 29, batch: 4, loss: 1.7968764305114746, acc: 34.375, f1: 6.395348837209303, r: -0.050301242849237975
06/02/2019 03:45:33 step: 967, epoch: 29, batch: 9, loss: 1.8340169191360474, acc: 40.625, f1: 7.222222222222221, r: 0.03549114119177749
06/02/2019 03:45:33 step: 972, epoch: 29, batch: 14, loss: 1.7640892267227173, acc: 40.625, f1: 8.253968253968253, r: 0.0548516734974048
06/02/2019 03:45:33 step: 977, epoch: 29, batch: 19, loss: 1.722885251045227, acc: 45.3125, f1: 8.90937019969278, r: 0.06827101173606064
06/02/2019 03:45:34 step: 982, epoch: 29, batch: 24, loss: 1.887197494506836, acc: 26.5625, f1: 6.98636806231743, r: 0.06200847888890899
06/02/2019 03:45:34 step: 987, epoch: 29, batch: 29, loss: 1.7643293142318726, acc: 40.625, f1: 7.222222222222221, r: 0.08704709050593606
06/02/2019 03:45:34 *** evaluating ***
06/02/2019 03:45:34 step: 30, epoch: 29, acc: 46.15384615384615, f1: 9.254807692307693, r: 0.10771749763070422
06/02/2019 03:45:34 *** epoch: 31 ***
06/02/2019 03:45:34 *** training ***
06/02/2019 03:45:34 step: 995, epoch: 30, batch: 4, loss: 1.7920936346054077, acc: 34.375, f1: 6.395348837209303, r: -0.08526637803769456
06/02/2019 03:45:34 step: 1000, epoch: 30, batch: 9, loss: 1.7730042934417725, acc: 43.75, f1: 7.608695652173914, r: 0.009087144284583673
06/02/2019 03:45:35 step: 1005, epoch: 30, batch: 14, loss: 1.7963584661483765, acc: 42.1875, f1: 8.47723704866562, r: 0.04590630779540345
06/02/2019 03:45:35 step: 1010, epoch: 30, batch: 19, loss: 1.8481202125549316, acc: 39.0625, f1: 9.67432950191571, r: 0.04786549518362513
06/02/2019 03:45:35 step: 1015, epoch: 30, batch: 24, loss: 1.8304702043533325, acc: 34.375, f1: 10.394519253484527, r: 0.030861352846021775
06/02/2019 03:45:35 step: 1020, epoch: 30, batch: 29, loss: 1.8064128160476685, acc: 32.8125, f1: 7.764227642276422, r: 0.08109581091611541
06/02/2019 03:45:35 *** evaluating ***
06/02/2019 03:45:36 step: 31, epoch: 30, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.07393516209846233
06/02/2019 03:45:36 *** epoch: 32 ***
06/02/2019 03:45:36 *** training ***
06/02/2019 03:45:36 step: 1028, epoch: 31, batch: 4, loss: 1.6882102489471436, acc: 40.625, f1: 8.253968253968253, r: nan
06/02/2019 03:45:36 step: 1033, epoch: 31, batch: 9, loss: 1.844390869140625, acc: 32.8125, f1: 6.176470588235294, r: nan
06/02/2019 03:45:36 step: 1038, epoch: 31, batch: 14, loss: 1.7989798784255981, acc: 39.0625, f1: 7.02247191011236, r: nan
06/02/2019 03:45:37 step: 1043, epoch: 31, batch: 19, loss: 1.774518609046936, acc: 53.125, f1: 8.673469387755102, r: -0.01818295819490525
06/02/2019 03:45:37 step: 1048, epoch: 31, batch: 24, loss: 1.7430251836776733, acc: 37.5, f1: 10.844337735094037, r: 0.045107969616405966
06/02/2019 03:45:37 step: 1053, epoch: 31, batch: 29, loss: 1.8458291292190552, acc: 32.8125, f1: 7.494684620836287, r: 0.08188844065049047
06/02/2019 03:45:37 *** evaluating ***
06/02/2019 03:45:37 step: 32, epoch: 31, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.07295513161095708
06/02/2019 03:45:37 *** epoch: 33 ***
06/02/2019 03:45:37 *** training ***
06/02/2019 03:45:37 step: 1061, epoch: 32, batch: 4, loss: 1.8796582221984863, acc: 34.375, f1: 6.395348837209303, r: nan
06/02/2019 03:45:38 step: 1066, epoch: 32, batch: 9, loss: 1.8039416074752808, acc: 35.9375, f1: 7.6411960132890355, r: 0.09183621762295126
06/02/2019 03:45:38 step: 1071, epoch: 32, batch: 14, loss: 1.7458840608596802, acc: 45.3125, f1: 12.777777777777777, r: 0.03792565131894524
06/02/2019 03:45:38 step: 1076, epoch: 32, batch: 19, loss: 1.8057975769042969, acc: 37.5, f1: 6.8181818181818175, r: nan
06/02/2019 03:45:38 step: 1081, epoch: 32, batch: 24, loss: 1.776842474937439, acc: 42.1875, f1: 7.417582417582418, r: nan
06/02/2019 03:45:38 step: 1086, epoch: 32, batch: 29, loss: 1.7874207496643066, acc: 32.8125, f1: 7.0588235294117645, r: 0.010968568511889867
06/02/2019 03:45:39 *** evaluating ***
06/02/2019 03:45:39 step: 33, epoch: 32, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.056771542282564987
06/02/2019 03:45:39 *** epoch: 34 ***
06/02/2019 03:45:39 *** training ***
06/02/2019 03:45:39 step: 1094, epoch: 33, batch: 4, loss: 1.816352367401123, acc: 39.0625, f1: 7.02247191011236, r: -0.0028353919802478287
06/02/2019 03:45:39 step: 1099, epoch: 33, batch: 9, loss: 1.7942426204681396, acc: 37.5, f1: 6.8181818181818175, r: 0.06156502100441531
06/02/2019 03:45:39 step: 1104, epoch: 33, batch: 14, loss: 1.6693825721740723, acc: 53.125, f1: 9.912536443148689, r: 0.07457225962438198
06/02/2019 03:45:39 step: 1109, epoch: 33, batch: 19, loss: 1.7422606945037842, acc: 43.75, f1: 10.144927536231885, r: 0.05073802951147817
06/02/2019 03:45:40 step: 1114, epoch: 33, batch: 24, loss: 1.8208626508712769, acc: 39.0625, f1: 8.682266009852217, r: 0.03665714147564098
06/02/2019 03:45:40 step: 1119, epoch: 33, batch: 29, loss: 1.7713234424591064, acc: 40.625, f1: 7.222222222222221, r: nan
06/02/2019 03:45:40 *** evaluating ***
06/02/2019 03:45:40 step: 34, epoch: 33, acc: 45.2991452991453, f1: 8.266272189349113, r: 0.07066652280718448
06/02/2019 03:45:40 *** epoch: 35 ***
06/02/2019 03:45:40 *** training ***
06/02/2019 03:45:40 step: 1127, epoch: 34, batch: 4, loss: 1.7456227540969849, acc: 39.0625, f1: 7.02247191011236, r: 0.06176101745404368
06/02/2019 03:45:41 step: 1132, epoch: 34, batch: 9, loss: 1.7949256896972656, acc: 42.1875, f1: 8.47723704866562, r: 0.016189098696355556
06/02/2019 03:45:41 step: 1137, epoch: 34, batch: 14, loss: 1.968248963356018, acc: 23.4375, f1: 4.746835443037975, r: -0.017013859847321334
06/02/2019 03:45:41 step: 1142, epoch: 34, batch: 19, loss: 1.7130612134933472, acc: 40.625, f1: 8.346709470304976, r: 0.1403811579089695
06/02/2019 03:45:41 step: 1147, epoch: 34, batch: 24, loss: 1.7669655084609985, acc: 37.5, f1: 6.8181818181818175, r: 0.06790786696034025
06/02/2019 03:45:42 step: 1152, epoch: 34, batch: 29, loss: 1.781459093093872, acc: 35.9375, f1: 10.800344234079173, r: 0.05048662793880809
06/02/2019 03:45:42 *** evaluating ***
06/02/2019 03:45:42 step: 35, epoch: 34, acc: 46.58119658119658, f1: 9.7111332889776, r: 0.09479001863704192
06/02/2019 03:45:42 *** epoch: 36 ***
06/02/2019 03:45:42 *** training ***
06/02/2019 03:45:42 step: 1160, epoch: 35, batch: 4, loss: 1.749956488609314, acc: 43.75, f1: 7.608695652173914, r: 0.05897478403911261
06/02/2019 03:45:42 step: 1165, epoch: 35, batch: 9, loss: 1.905228614807129, acc: 32.8125, f1: 7.0588235294117645, r: nan
06/02/2019 03:45:43 step: 1170, epoch: 35, batch: 14, loss: 1.8037405014038086, acc: 43.75, f1: 8.695652173913045, r: 0.07064682299282622
06/02/2019 03:45:43 step: 1175, epoch: 35, batch: 19, loss: 1.757567048072815, acc: 42.1875, f1: 8.47723704866562, r: nan
06/02/2019 03:45:43 step: 1180, epoch: 35, batch: 24, loss: 1.8583790063858032, acc: 29.6875, f1: 5.72289156626506, r: nan
06/02/2019 03:45:43 step: 1185, epoch: 35, batch: 29, loss: 1.8021310567855835, acc: 35.9375, f1: 7.55336617405583, r: 0.0525936957857157
06/02/2019 03:45:43 *** evaluating ***
06/02/2019 03:45:43 step: 36, epoch: 35, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.047788470472277814
06/02/2019 03:45:43 *** epoch: 37 ***
06/02/2019 03:45:43 *** training ***
06/02/2019 03:45:44 step: 1193, epoch: 36, batch: 4, loss: 1.8665255308151245, acc: 45.3125, f1: 8.90937019969278, r: 0.04932641054638234
06/02/2019 03:45:44 step: 1198, epoch: 36, batch: 9, loss: 1.8191132545471191, acc: 26.5625, f1: 5.3125, r: 0.07473854852139414
06/02/2019 03:45:44 step: 1203, epoch: 36, batch: 14, loss: 1.8189278841018677, acc: 32.8125, f1: 6.176470588235294, r: nan
06/02/2019 03:45:44 step: 1208, epoch: 36, batch: 19, loss: 1.8625400066375732, acc: 32.8125, f1: 6.25, r: -0.003055724917322998
06/02/2019 03:45:44 step: 1213, epoch: 36, batch: 24, loss: 1.7738724946975708, acc: 40.625, f1: 7.222222222222221, r: nan
06/02/2019 03:45:45 step: 1218, epoch: 36, batch: 29, loss: 1.8409849405288696, acc: 43.75, f1: 7.608695652173914, r: -0.019266386693103925
06/02/2019 03:45:45 *** evaluating ***
06/02/2019 03:45:45 step: 37, epoch: 36, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.07862587920890038
06/02/2019 03:45:45 *** epoch: 38 ***
06/02/2019 03:45:45 *** training ***
06/02/2019 03:45:45 step: 1226, epoch: 37, batch: 4, loss: 1.796423077583313, acc: 37.5, f1: 7.792207792207792, r: 0.02824191935310996
06/02/2019 03:45:45 step: 1231, epoch: 37, batch: 9, loss: 1.70924973487854, acc: 42.1875, f1: 8.47723704866562, r: -0.026023345839103585
06/02/2019 03:45:45 step: 1236, epoch: 37, batch: 14, loss: 1.7813504934310913, acc: 34.375, f1: 7.394957983193276, r: 0.07060046128567639
06/02/2019 03:45:46 step: 1241, epoch: 37, batch: 19, loss: 1.8704140186309814, acc: 34.375, f1: 6.6265060240963845, r: -0.1053038332855353
06/02/2019 03:45:46 step: 1246, epoch: 37, batch: 24, loss: 1.7061659097671509, acc: 45.3125, f1: 8.90937019969278, r: nan
06/02/2019 03:45:46 step: 1251, epoch: 37, batch: 29, loss: 1.8555057048797607, acc: 29.6875, f1: 6.540447504302925, r: nan
06/02/2019 03:45:46 *** evaluating ***
06/02/2019 03:45:47 step: 38, epoch: 37, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.03577698037476245
06/02/2019 03:45:47 *** epoch: 39 ***
06/02/2019 03:45:47 *** training ***
06/02/2019 03:45:47 step: 1259, epoch: 38, batch: 4, loss: 1.7638126611709595, acc: 40.625, f1: 7.222222222222221, r: 0.024672286660212962
06/02/2019 03:45:47 step: 1264, epoch: 38, batch: 9, loss: 1.8496583700180054, acc: 26.5625, f1: 5.246913580246913, r: nan
06/02/2019 03:45:47 step: 1269, epoch: 38, batch: 14, loss: 1.7149577140808105, acc: 42.1875, f1: 8.47723704866562, r: nan
06/02/2019 03:45:48 step: 1274, epoch: 38, batch: 19, loss: 1.8741590976715088, acc: 37.5, f1: 6.8181818181818175, r: 0.060920801549964924
06/02/2019 03:45:48 step: 1279, epoch: 38, batch: 24, loss: 1.6193397045135498, acc: 48.4375, f1: 14.544279250161605, r: 0.05645404454435762
06/02/2019 03:45:48 step: 1284, epoch: 38, batch: 29, loss: 1.8836697340011597, acc: 34.375, f1: 7.308970099667775, r: -0.004561123040269176
06/02/2019 03:45:48 *** evaluating ***
06/02/2019 03:45:48 step: 39, epoch: 38, acc: 45.2991452991453, f1: 8.266272189349113, r: 0.08570545871548312
06/02/2019 03:45:48 *** epoch: 40 ***
06/02/2019 03:45:48 *** training ***
06/02/2019 03:45:49 step: 1292, epoch: 39, batch: 4, loss: 1.7294341325759888, acc: 45.3125, f1: 8.90937019969278, r: 0.03876250350316442
06/02/2019 03:45:49 step: 1297, epoch: 39, batch: 9, loss: 1.8402942419052124, acc: 34.375, f1: 7.308970099667775, r: 0.03453381919215372
06/02/2019 03:45:49 step: 1302, epoch: 39, batch: 14, loss: 1.7022937536239624, acc: 46.875, f1: 9.118541033434651, r: 0.014732691787975849
06/02/2019 03:45:49 step: 1307, epoch: 39, batch: 19, loss: 1.7440909147262573, acc: 43.75, f1: 9.852216748768472, r: 0.036223204714116546
06/02/2019 03:45:49 step: 1312, epoch: 39, batch: 24, loss: 1.7811188697814941, acc: 40.625, f1: 10.974025974025974, r: 0.01734708380958436
06/02/2019 03:45:50 step: 1317, epoch: 39, batch: 29, loss: 1.8140403032302856, acc: 34.375, f1: 7.308970099667775, r: 0.06270647805609798
06/02/2019 03:45:50 *** evaluating ***
06/02/2019 03:45:50 step: 40, epoch: 39, acc: 45.72649572649573, f1: 8.769709664281143, r: 0.09786252783669434
06/02/2019 03:45:50 *** epoch: 41 ***
06/02/2019 03:45:50 *** training ***
06/02/2019 03:45:50 step: 1325, epoch: 40, batch: 4, loss: 1.7895421981811523, acc: 40.625, f1: 7.303370786516854, r: 0.08547074367778335
06/02/2019 03:45:50 step: 1330, epoch: 40, batch: 9, loss: 1.6819928884506226, acc: 50.0, f1: 9.62406015037594, r: 0.11534420763805295
06/02/2019 03:45:50 step: 1335, epoch: 40, batch: 14, loss: 1.8044726848602295, acc: 42.1875, f1: 8.47723704866562, r: nan
06/02/2019 03:45:51 step: 1340, epoch: 40, batch: 19, loss: 1.7693696022033691, acc: 40.625, f1: 8.346709470304976, r: 0.0020203259988433586
06/02/2019 03:45:51 step: 1345, epoch: 40, batch: 24, loss: 1.7701328992843628, acc: 39.0625, f1: 11.209150326797385, r: 0.06500523569081514
06/02/2019 03:45:51 step: 1350, epoch: 40, batch: 29, loss: 1.8385941982269287, acc: 32.8125, f1: 6.325301204819277, r: 0.010033275673749823
06/02/2019 03:45:51 *** evaluating ***
06/02/2019 03:45:51 step: 41, epoch: 40, acc: 45.2991452991453, f1: 8.266272189349113, r: 0.08228481974481458
06/02/2019 03:45:51 *** epoch: 42 ***
06/02/2019 03:45:51 *** training ***
06/02/2019 03:45:51 step: 1358, epoch: 41, batch: 4, loss: 1.71787691116333, acc: 43.75, f1: 9.285714285714286, r: 0.070862160601685
06/02/2019 03:45:52 step: 1363, epoch: 41, batch: 9, loss: 1.718164324760437, acc: 46.875, f1: 9.118541033434651, r: 0.024008068505321643
06/02/2019 03:45:52 step: 1368, epoch: 41, batch: 14, loss: 1.6432465314865112, acc: 48.4375, f1: 9.987593052109181, r: 0.00039305855129408065
06/02/2019 03:45:52 step: 1373, epoch: 41, batch: 19, loss: 1.8442436456680298, acc: 34.375, f1: 10.08605851979346, r: 0.06911786797304278
06/02/2019 03:45:52 step: 1378, epoch: 41, batch: 24, loss: 1.8320329189300537, acc: 37.5, f1: 7.8817733990147785, r: 0.06832413576703912
06/02/2019 03:45:53 step: 1383, epoch: 41, batch: 29, loss: 1.6872555017471313, acc: 42.1875, f1: 8.47723704866562, r: -0.009210238512828806
06/02/2019 03:45:53 *** evaluating ***
06/02/2019 03:45:53 step: 42, epoch: 41, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.09351924290470316
06/02/2019 03:45:53 *** epoch: 43 ***
06/02/2019 03:45:53 *** training ***
06/02/2019 03:45:53 step: 1391, epoch: 42, batch: 4, loss: 1.7882989645004272, acc: 34.375, f1: 7.394957983193276, r: 0.058366201222046284
06/02/2019 03:45:53 step: 1396, epoch: 42, batch: 9, loss: 1.6432334184646606, acc: 42.1875, f1: 8.47723704866562, r: 0.14688050254255897
06/02/2019 03:45:54 step: 1401, epoch: 42, batch: 14, loss: 1.8899818658828735, acc: 32.8125, f1: 6.176470588235294, r: 0.03718827051707761
06/02/2019 03:45:54 step: 1406, epoch: 42, batch: 19, loss: 1.7496412992477417, acc: 34.375, f1: 8.527131782945736, r: 0.028714293075034227
06/02/2019 03:45:54 step: 1411, epoch: 42, batch: 24, loss: 1.7619363069534302, acc: 35.9375, f1: 18.254829357903503, r: 0.15347274185664836
06/02/2019 03:45:54 step: 1416, epoch: 42, batch: 29, loss: 1.7378106117248535, acc: 34.375, f1: 7.575301204819277, r: 0.07961153032175644
06/02/2019 03:45:54 *** evaluating ***
06/02/2019 03:45:55 step: 43, epoch: 42, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.10257927175886902
06/02/2019 03:45:55 *** epoch: 44 ***
06/02/2019 03:45:55 *** training ***
06/02/2019 03:45:55 step: 1424, epoch: 43, batch: 4, loss: 1.8304340839385986, acc: 34.375, f1: 6.395348837209303, r: -0.005700962685340795
06/02/2019 03:45:55 step: 1429, epoch: 43, batch: 9, loss: 1.6711912155151367, acc: 35.9375, f1: 8.823529411764705, r: 0.08434133334495536
06/02/2019 03:45:55 step: 1434, epoch: 43, batch: 14, loss: 1.7907193899154663, acc: 40.625, f1: 7.222222222222221, r: 0.1179565821531709
06/02/2019 03:45:55 step: 1439, epoch: 43, batch: 19, loss: 1.7612385749816895, acc: 39.0625, f1: 7.1022727272727275, r: 0.12228549719660892
06/02/2019 03:45:56 step: 1444, epoch: 43, batch: 24, loss: 1.7089626789093018, acc: 43.75, f1: 7.692307692307692, r: 0.08843065124084909
06/02/2019 03:45:56 step: 1449, epoch: 43, batch: 29, loss: 1.7284148931503296, acc: 42.1875, f1: 11.203852327447832, r: 0.03474631371790205
06/02/2019 03:45:56 *** evaluating ***
06/02/2019 03:45:56 step: 44, epoch: 43, acc: 47.008547008547005, f1: 10.174096251940563, r: 0.11637736160749289
06/02/2019 03:45:56 *** epoch: 45 ***
06/02/2019 03:45:56 *** training ***
06/02/2019 03:45:56 step: 1457, epoch: 44, batch: 4, loss: 1.7773042917251587, acc: 43.75, f1: 12.653061224489795, r: 0.033421683257105
06/02/2019 03:45:56 step: 1462, epoch: 44, batch: 9, loss: 1.753487229347229, acc: 45.3125, f1: 7.795698924731183, r: -0.036522673117789645
06/02/2019 03:45:57 step: 1467, epoch: 44, batch: 14, loss: 1.6467512845993042, acc: 40.625, f1: 8.253968253968253, r: 0.00761374067605939
06/02/2019 03:45:57 step: 1472, epoch: 44, batch: 19, loss: 1.736722707748413, acc: 42.1875, f1: 10.13242375601926, r: 0.07271423501711885
06/02/2019 03:45:57 step: 1477, epoch: 44, batch: 24, loss: 1.7268624305725098, acc: 42.1875, f1: 9.803370786516853, r: 0.1265076103240612
06/02/2019 03:45:57 step: 1482, epoch: 44, batch: 29, loss: 1.761232852935791, acc: 48.4375, f1: 12.391193036354327, r: 0.007823460758633119
06/02/2019 03:45:57 *** evaluating ***
06/02/2019 03:45:57 step: 45, epoch: 44, acc: 47.863247863247864, f1: 11.229758860981601, r: 0.10287240822574845
06/02/2019 03:45:57 *** epoch: 46 ***
06/02/2019 03:45:57 *** training ***
06/02/2019 03:45:58 step: 1490, epoch: 45, batch: 4, loss: 1.812602162361145, acc: 39.0625, f1: 7.02247191011236, r: 0.040428019115254676
06/02/2019 03:45:58 step: 1495, epoch: 45, batch: 9, loss: 1.654933214187622, acc: 43.75, f1: 7.692307692307692, r: 0.1299214804637569
06/02/2019 03:45:58 step: 1500, epoch: 45, batch: 14, loss: 1.8216619491577148, acc: 35.9375, f1: 9.253822010732003, r: 0.028975809777394064
06/02/2019 03:45:58 step: 1505, epoch: 45, batch: 19, loss: 1.8016784191131592, acc: 34.375, f1: 7.394957983193276, r: 0.007526628162586702
06/02/2019 03:45:59 step: 1510, epoch: 45, batch: 24, loss: 1.6471490859985352, acc: 42.1875, f1: 11.729362591431554, r: 0.16595063820467515
06/02/2019 03:45:59 step: 1515, epoch: 45, batch: 29, loss: 1.8112999200820923, acc: 35.9375, f1: 8.743315508021391, r: 0.03943652132943172
06/02/2019 03:45:59 *** evaluating ***
06/02/2019 03:45:59 step: 46, epoch: 45, acc: 51.28205128205128, f1: 13.890386343216532, r: 0.10155609116875497
06/02/2019 03:45:59 *** epoch: 47 ***
06/02/2019 03:45:59 *** training ***
06/02/2019 03:45:59 step: 1523, epoch: 46, batch: 4, loss: 1.6923387050628662, acc: 37.5, f1: 7.8817733990147785, r: 0.14011545538764286
06/02/2019 03:46:00 step: 1528, epoch: 46, batch: 9, loss: 1.8985304832458496, acc: 32.8125, f1: 6.402439024390244, r: -0.051915129655057735
06/02/2019 03:46:00 step: 1533, epoch: 46, batch: 14, loss: 1.7550231218338013, acc: 32.8125, f1: 6.176470588235294, r: 0.12927946519047298
06/02/2019 03:46:00 step: 1538, epoch: 46, batch: 19, loss: 1.8130061626434326, acc: 31.25, f1: 7.936507936507936, r: -0.054998309456456014
06/02/2019 03:46:00 step: 1543, epoch: 46, batch: 24, loss: 1.7799055576324463, acc: 35.9375, f1: 8.39366515837104, r: 0.03378676657463774
06/02/2019 03:46:01 step: 1548, epoch: 46, batch: 29, loss: 1.7628531455993652, acc: 42.1875, f1: 7.417582417582418, r: 0.053217197793061465
06/02/2019 03:46:01 *** evaluating ***
06/02/2019 03:46:01 step: 47, epoch: 46, acc: 47.863247863247864, f1: 11.000689033762654, r: 0.11314417223904608
06/02/2019 03:46:01 *** epoch: 48 ***
06/02/2019 03:46:01 *** training ***
06/02/2019 03:46:01 step: 1556, epoch: 47, batch: 4, loss: 1.7076141834259033, acc: 43.75, f1: 7.608695652173914, r: 0.07021929212216431
06/02/2019 03:46:01 step: 1561, epoch: 47, batch: 9, loss: 1.6686396598815918, acc: 45.3125, f1: 9.965034965034963, r: 0.11775639920564386
06/02/2019 03:46:01 step: 1566, epoch: 47, batch: 14, loss: 1.808858871459961, acc: 32.8125, f1: 6.25, r: -0.0836140513657211
06/02/2019 03:46:02 step: 1571, epoch: 47, batch: 19, loss: 1.7182127237319946, acc: 39.0625, f1: 10.014238253440912, r: 0.10740787962493081
06/02/2019 03:46:02 step: 1576, epoch: 47, batch: 24, loss: 1.6457287073135376, acc: 50.0, f1: 9.523809523809524, r: 0.034978300915429
06/02/2019 03:46:02 step: 1581, epoch: 47, batch: 29, loss: 1.822712779045105, acc: 40.625, f1: 10.310077519379846, r: 0.0954413153223937
06/02/2019 03:46:02 *** evaluating ***
06/02/2019 03:46:02 step: 48, epoch: 47, acc: 46.58119658119658, f1: 9.722613348352576, r: 0.13000636549034392
06/02/2019 03:46:02 *** epoch: 49 ***
06/02/2019 03:46:02 *** training ***
06/02/2019 03:46:02 step: 1589, epoch: 48, batch: 4, loss: 1.8630881309509277, acc: 37.5, f1: 6.8181818181818175, r: 0.052957762286261194
06/02/2019 03:46:03 step: 1594, epoch: 48, batch: 9, loss: 1.701025366783142, acc: 40.625, f1: 10.021645021645021, r: 0.1033473445853266
06/02/2019 03:46:03 step: 1599, epoch: 48, batch: 14, loss: 1.634720802307129, acc: 45.3125, f1: 10.832025117739404, r: 0.04702418809716078
06/02/2019 03:46:03 step: 1604, epoch: 48, batch: 19, loss: 1.8735905885696411, acc: 31.25, f1: 6.884681583476763, r: 0.041634746582355484
06/02/2019 03:46:03 step: 1609, epoch: 48, batch: 24, loss: 1.8445922136306763, acc: 31.25, f1: 9.477351916376305, r: 0.10561403865276542
06/02/2019 03:46:04 step: 1614, epoch: 48, batch: 29, loss: 1.7385637760162354, acc: 42.1875, f1: 8.47723704866562, r: 0.045525820062711296
06/02/2019 03:46:04 *** evaluating ***
06/02/2019 03:46:04 step: 49, epoch: 48, acc: 45.2991452991453, f1: 8.266272189349113, r: 0.10048757342756731
06/02/2019 03:46:04 *** epoch: 50 ***
06/02/2019 03:46:04 *** training ***
06/02/2019 03:46:04 step: 1622, epoch: 49, batch: 4, loss: 1.7294400930404663, acc: 35.9375, f1: 7.641196013289038, r: 0.06871270643927972
06/02/2019 03:46:04 step: 1627, epoch: 49, batch: 9, loss: 1.6589126586914062, acc: 39.0625, f1: 9.385532797511019, r: 0.09230512003542415
06/02/2019 03:46:05 step: 1632, epoch: 49, batch: 14, loss: 1.678753137588501, acc: 40.625, f1: 12.98076923076923, r: 0.10095560529894482
06/02/2019 03:46:05 step: 1637, epoch: 49, batch: 19, loss: 1.647953748703003, acc: 45.3125, f1: 10.05050505050505, r: 0.11492842262953398
06/02/2019 03:46:05 step: 1642, epoch: 49, batch: 24, loss: 1.7652764320373535, acc: 39.0625, f1: 7.02247191011236, r: 0.09928123217782969
06/02/2019 03:46:05 step: 1647, epoch: 49, batch: 29, loss: 1.7999519109725952, acc: 37.5, f1: 10.498338870431892, r: 0.06835357608154324
06/02/2019 03:46:06 *** evaluating ***
06/02/2019 03:46:06 step: 50, epoch: 49, acc: 48.717948717948715, f1: 12.007718187215517, r: 0.11386558205524404
06/02/2019 03:46:06 *** epoch: 51 ***
06/02/2019 03:46:06 *** training ***
06/02/2019 03:46:06 step: 1655, epoch: 50, batch: 4, loss: 1.7195883989334106, acc: 34.375, f1: 8.423913043478262, r: 0.03951062812393312
06/02/2019 03:46:06 step: 1660, epoch: 50, batch: 9, loss: 1.8309768438339233, acc: 34.375, f1: 6.470588235294117, r: 0.03774878717763986
06/02/2019 03:46:06 step: 1665, epoch: 50, batch: 14, loss: 1.8305002450942993, acc: 26.5625, f1: 5.246913580246913, r: 0.07346279948370753
06/02/2019 03:46:06 step: 1670, epoch: 50, batch: 19, loss: 1.6505848169326782, acc: 43.75, f1: 11.168831168831169, r: 0.09144639316593614
06/02/2019 03:46:07 step: 1675, epoch: 50, batch: 24, loss: 1.6711821556091309, acc: 39.0625, f1: 9.705488621151272, r: 0.0998686487645449
06/02/2019 03:46:07 step: 1680, epoch: 50, batch: 29, loss: 1.7303639650344849, acc: 50.0, f1: 10.416666666666666, r: 0.0358414394385151
06/02/2019 03:46:07 *** evaluating ***
06/02/2019 03:46:07 step: 51, epoch: 50, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.10036628009383719
06/02/2019 03:46:07 *** epoch: 52 ***
06/02/2019 03:46:07 *** training ***
06/02/2019 03:46:07 step: 1688, epoch: 51, batch: 4, loss: 1.7255115509033203, acc: 37.5, f1: 7.792207792207792, r: 0.07357518213794507
06/02/2019 03:46:07 step: 1693, epoch: 51, batch: 9, loss: 1.6895990371704102, acc: 39.0625, f1: 9.363295880149813, r: 0.07174836198086562
06/02/2019 03:46:08 step: 1698, epoch: 51, batch: 14, loss: 1.6712967157363892, acc: 50.0, f1: 8.333333333333332, r: 0.054994358255033106
06/02/2019 03:46:08 step: 1703, epoch: 51, batch: 19, loss: 1.7175906896591187, acc: 39.0625, f1: 7.02247191011236, r: 0.0640500627007326
06/02/2019 03:46:08 step: 1708, epoch: 51, batch: 24, loss: 1.7231876850128174, acc: 34.375, f1: 7.308970099667775, r: 0.07315945462060752
06/02/2019 03:46:08 step: 1713, epoch: 51, batch: 29, loss: 1.8999555110931396, acc: 34.375, f1: 7.394957983193278, r: 0.11023013979849136
06/02/2019 03:46:08 *** evaluating ***
06/02/2019 03:46:09 step: 52, epoch: 51, acc: 46.15384615384615, f1: 9.254807692307693, r: 0.12191218305349444
06/02/2019 03:46:09 *** epoch: 53 ***
06/02/2019 03:46:09 *** training ***
06/02/2019 03:46:09 step: 1721, epoch: 52, batch: 4, loss: 1.6932846307754517, acc: 37.5, f1: 8.939897311990334, r: 0.038672516921683595
06/02/2019 03:46:09 step: 1726, epoch: 52, batch: 9, loss: 1.817299246788025, acc: 32.8125, f1: 6.176470588235294, r: 0.03251888845738865
06/02/2019 03:46:09 step: 1731, epoch: 52, batch: 14, loss: 1.6981819868087769, acc: 40.625, f1: 10.755336617405584, r: 0.1502080109373882
06/02/2019 03:46:10 step: 1736, epoch: 52, batch: 19, loss: 1.693972110748291, acc: 46.875, f1: 9.118541033434651, r: 0.03623216770676166
06/02/2019 03:46:10 step: 1741, epoch: 52, batch: 24, loss: 1.6542820930480957, acc: 50.0, f1: 9.62406015037594, r: 0.03978083950824564
06/02/2019 03:46:10 step: 1746, epoch: 52, batch: 29, loss: 1.7865642309188843, acc: 25.0, f1: 6.259018759018758, r: 0.04935913285151497
06/02/2019 03:46:10 *** evaluating ***
06/02/2019 03:46:11 step: 53, epoch: 52, acc: 49.14529914529914, f1: 12.084405303779933, r: 0.13391134324097234
06/02/2019 03:46:11 *** epoch: 54 ***
06/02/2019 03:46:11 *** training ***
06/02/2019 03:46:11 step: 1754, epoch: 53, batch: 4, loss: 1.6157349348068237, acc: 48.4375, f1: 11.597542242703533, r: 0.17479102292709855
06/02/2019 03:46:11 step: 1759, epoch: 53, batch: 9, loss: 1.7105430364608765, acc: 48.4375, f1: 9.987593052109181, r: 0.10870616389930937
06/02/2019 03:46:11 step: 1764, epoch: 53, batch: 14, loss: 1.7423700094223022, acc: 37.5, f1: 11.812725090036015, r: 0.08435903204805864
06/02/2019 03:46:11 step: 1769, epoch: 53, batch: 19, loss: 1.5759538412094116, acc: 59.375, f1: 10.749646393210748, r: 0.09224972925685797
06/02/2019 03:46:12 step: 1774, epoch: 53, batch: 24, loss: 1.7590079307556152, acc: 43.75, f1: 7.608695652173914, r: 0.041080560767490684
06/02/2019 03:46:12 step: 1779, epoch: 53, batch: 29, loss: 1.702897548675537, acc: 35.9375, f1: 7.55336617405583, r: 0.04634347479851369
06/02/2019 03:46:12 *** evaluating ***
06/02/2019 03:46:12 step: 54, epoch: 53, acc: 51.28205128205128, f1: 13.890386343216532, r: 0.1457239412308359
06/02/2019 03:46:12 *** epoch: 55 ***
06/02/2019 03:46:12 *** training ***
06/02/2019 03:46:12 step: 1787, epoch: 54, batch: 4, loss: 1.773024559020996, acc: 43.75, f1: 7.608695652173914, r: 0.04592631858557966
06/02/2019 03:46:12 step: 1792, epoch: 54, batch: 9, loss: 1.764516830444336, acc: 40.625, f1: 8.346709470304976, r: 0.012534500898480192
06/02/2019 03:46:13 step: 1797, epoch: 54, batch: 14, loss: 1.6820143461227417, acc: 45.3125, f1: 9.006211180124224, r: 0.09198615171736953
06/02/2019 03:46:13 step: 1802, epoch: 54, batch: 19, loss: 1.7318577766418457, acc: 37.5, f1: 10.881370091896407, r: 0.1279265383297324
06/02/2019 03:46:13 step: 1807, epoch: 54, batch: 24, loss: 1.7904587984085083, acc: 39.0625, f1: 7.1022727272727275, r: 0.06795830479489715
06/02/2019 03:46:13 step: 1812, epoch: 54, batch: 29, loss: 1.9742732048034668, acc: 31.25, f1: 6.024096385542169, r: 0.011317352525876746
06/02/2019 03:46:13 *** evaluating ***
06/02/2019 03:46:13 step: 55, epoch: 54, acc: 50.0, f1: 12.998776585733108, r: 0.12354887718349312
06/02/2019 03:46:13 *** epoch: 56 ***
06/02/2019 03:46:13 *** training ***
06/02/2019 03:46:14 step: 1820, epoch: 55, batch: 4, loss: 1.588119387626648, acc: 46.875, f1: 15.25044722719141, r: 0.18121042170973153
06/02/2019 03:46:14 step: 1825, epoch: 55, batch: 9, loss: 1.6486736536026, acc: 48.4375, f1: 13.015873015873018, r: 0.11927268564730208
06/02/2019 03:46:14 step: 1830, epoch: 55, batch: 14, loss: 1.7317609786987305, acc: 40.625, f1: 10.755336617405584, r: 0.10499444145861174
06/02/2019 03:46:14 step: 1835, epoch: 55, batch: 19, loss: 1.6650304794311523, acc: 43.75, f1: 12.777777777777777, r: 0.10782549040099088
06/02/2019 03:46:15 step: 1840, epoch: 55, batch: 24, loss: 1.783266544342041, acc: 40.625, f1: 8.664772727272728, r: 0.058634292737290795
06/02/2019 03:46:15 step: 1845, epoch: 55, batch: 29, loss: 1.6996147632598877, acc: 46.875, f1: 7.9787234042553195, r: -4.076135967759527e-05
06/02/2019 03:46:15 *** evaluating ***
06/02/2019 03:46:15 step: 56, epoch: 55, acc: 47.008547008547005, f1: 10.174096251940563, r: 0.11974577314324647
06/02/2019 03:46:15 *** epoch: 57 ***
06/02/2019 03:46:15 *** training ***
06/02/2019 03:46:16 step: 1853, epoch: 56, batch: 4, loss: 1.8810012340545654, acc: 28.125, f1: 7.2355769230769225, r: 0.13110456694162892
06/02/2019 03:46:16 step: 1858, epoch: 56, batch: 9, loss: 1.6751798391342163, acc: 39.0625, f1: 10.098039215686274, r: 0.12389810235052054
06/02/2019 03:46:16 step: 1863, epoch: 56, batch: 14, loss: 1.695651650428772, acc: 42.1875, f1: 7.417582417582418, r: nan
06/02/2019 03:46:16 step: 1868, epoch: 56, batch: 19, loss: 1.5909563302993774, acc: 48.4375, f1: 9.323308270676693, r: 0.04496289608825515
06/02/2019 03:46:17 step: 1873, epoch: 56, batch: 24, loss: 1.635483980178833, acc: 34.375, f1: 8.527131782945736, r: 0.0398139934209231
06/02/2019 03:46:17 step: 1878, epoch: 56, batch: 29, loss: 1.795549750328064, acc: 35.9375, f1: 6.609195402298851, r: 0.06705528973816767
06/02/2019 03:46:17 *** evaluating ***
06/02/2019 03:46:17 step: 57, epoch: 56, acc: 48.29059829059829, f1: 11.40282131661442, r: 0.12596194190697818
06/02/2019 03:46:17 *** epoch: 58 ***
06/02/2019 03:46:17 *** training ***
06/02/2019 03:46:17 step: 1886, epoch: 57, batch: 4, loss: 1.650347113609314, acc: 43.75, f1: 7.777777777777778, r: 0.0168659528946455
06/02/2019 03:46:17 step: 1891, epoch: 57, batch: 9, loss: 1.6662548780441284, acc: 35.9375, f1: 12.511878365536901, r: 0.18859269777022142
06/02/2019 03:46:18 step: 1896, epoch: 57, batch: 14, loss: 1.8257195949554443, acc: 32.8125, f1: 8.370288248337028, r: 0.1112164307108144
06/02/2019 03:46:18 step: 1901, epoch: 57, batch: 19, loss: 1.7165207862854004, acc: 32.8125, f1: 8.754355400696864, r: 0.08924817955100545
06/02/2019 03:46:18 step: 1906, epoch: 57, batch: 24, loss: 1.595266342163086, acc: 46.875, f1: 11.204013377926422, r: 0.09416572945368877
06/02/2019 03:46:18 step: 1911, epoch: 57, batch: 29, loss: 1.7087454795837402, acc: 37.5, f1: 9.228497600590622, r: 0.05799688078245751
06/02/2019 03:46:18 *** evaluating ***
06/02/2019 03:46:18 step: 58, epoch: 57, acc: 47.863247863247864, f1: 11.260162601626018, r: 0.12333905457044006
06/02/2019 03:46:18 *** epoch: 59 ***
06/02/2019 03:46:18 *** training ***
06/02/2019 03:46:19 step: 1919, epoch: 58, batch: 4, loss: 1.7392278909683228, acc: 45.3125, f1: 8.90937019969278, r: 0.05273159361019087
06/02/2019 03:46:19 step: 1924, epoch: 58, batch: 9, loss: 1.6236226558685303, acc: 50.0, f1: 9.62406015037594, r: 0.10069065446055617
06/02/2019 03:46:19 step: 1929, epoch: 58, batch: 14, loss: 1.8446578979492188, acc: 26.5625, f1: 8.059210526315788, r: 0.1195286743618357
06/02/2019 03:46:19 step: 1934, epoch: 58, batch: 19, loss: 1.7684504985809326, acc: 40.625, f1: 13.636363636363638, r: 0.07570205156555378
06/02/2019 03:46:20 step: 1939, epoch: 58, batch: 24, loss: 1.70924711227417, acc: 43.75, f1: 11.221264367816092, r: 0.0713671861209899
06/02/2019 03:46:20 step: 1944, epoch: 58, batch: 29, loss: 1.6462799310684204, acc: 42.1875, f1: 10.392441860465116, r: 0.12244369015625135
06/02/2019 03:46:20 *** evaluating ***
06/02/2019 03:46:20 step: 59, epoch: 58, acc: 50.427350427350426, f1: 13.272058823529411, r: 0.13121677750647795
06/02/2019 03:46:20 *** epoch: 60 ***
06/02/2019 03:46:20 *** training ***
06/02/2019 03:46:20 step: 1952, epoch: 59, batch: 4, loss: 1.6987210512161255, acc: 37.5, f1: 8.769379844961241, r: 0.06509097057853114
06/02/2019 03:46:21 step: 1957, epoch: 59, batch: 9, loss: 1.6960585117340088, acc: 43.75, f1: 9.250936329588015, r: 0.08246394415103969
06/02/2019 03:46:21 step: 1962, epoch: 59, batch: 14, loss: 1.7751622200012207, acc: 34.375, f1: 7.308970099667775, r: 0.07226494681235239
06/02/2019 03:46:21 step: 1967, epoch: 59, batch: 19, loss: 1.6943901777267456, acc: 45.3125, f1: 11.17216117216117, r: 0.0928459135238363
06/02/2019 03:46:22 step: 1972, epoch: 59, batch: 24, loss: 1.606246829032898, acc: 39.0625, f1: 10.26272577996716, r: 0.09522083857796386
06/02/2019 03:46:22 step: 1977, epoch: 59, batch: 29, loss: 1.6452382802963257, acc: 50.0, f1: 11.36968085106383, r: 0.07934654023219406
06/02/2019 03:46:22 *** evaluating ***
06/02/2019 03:46:22 step: 60, epoch: 59, acc: 50.427350427350426, f1: 13.272058823529411, r: 0.15190787502093217
06/02/2019 03:46:22 *** epoch: 61 ***
06/02/2019 03:46:22 *** training ***
06/02/2019 03:46:22 step: 1985, epoch: 60, batch: 4, loss: 1.559922218322754, acc: 48.4375, f1: 10.897262130658715, r: 0.07379270240587937
06/02/2019 03:46:22 step: 1990, epoch: 60, batch: 9, loss: 1.7556096315383911, acc: 29.6875, f1: 7.723577235772357, r: 0.052946133781047754
06/02/2019 03:46:23 step: 1995, epoch: 60, batch: 14, loss: 1.8285412788391113, acc: 32.8125, f1: 9.825783972125436, r: 0.09775904185559355
06/02/2019 03:46:23 step: 2000, epoch: 60, batch: 19, loss: 1.8255842924118042, acc: 32.8125, f1: 6.176470588235294, r: 0.060447299140312646
06/02/2019 03:46:23 step: 2005, epoch: 60, batch: 24, loss: 1.6770316362380981, acc: 45.3125, f1: 7.795698924731183, r: 0.08775013901446185
06/02/2019 03:46:23 step: 2010, epoch: 60, batch: 29, loss: 1.650261640548706, acc: 39.0625, f1: 8.025682182985554, r: 0.11896348168225686
06/02/2019 03:46:23 *** evaluating ***
06/02/2019 03:46:23 step: 61, epoch: 60, acc: 47.43589743589743, f1: 10.61015561015561, r: 0.11738069618579358
06/02/2019 03:46:23 *** epoch: 62 ***
06/02/2019 03:46:23 *** training ***
06/02/2019 03:46:24 step: 2018, epoch: 61, batch: 4, loss: 1.6879124641418457, acc: 46.875, f1: 7.9787234042553195, r: -0.012438357818809146
06/02/2019 03:46:24 step: 2023, epoch: 61, batch: 9, loss: 1.6800085306167603, acc: 42.1875, f1: 10.482374768089056, r: 0.10621915259244624
06/02/2019 03:46:24 step: 2028, epoch: 61, batch: 14, loss: 1.8933557271957397, acc: 25.0, f1: 8.16326530612245, r: 0.059906831877448885
06/02/2019 03:46:24 step: 2033, epoch: 61, batch: 19, loss: 1.6447868347167969, acc: 45.3125, f1: 10.709269662921347, r: 0.13093190144727773
06/02/2019 03:46:24 step: 2038, epoch: 61, batch: 24, loss: 1.7623090744018555, acc: 51.5625, f1: 13.19548872180451, r: 0.07730243808713745
06/02/2019 03:46:25 step: 2043, epoch: 61, batch: 29, loss: 1.7923165559768677, acc: 35.9375, f1: 10.042016806722689, r: 0.07742443991945674
06/02/2019 03:46:25 *** evaluating ***
06/02/2019 03:46:25 step: 62, epoch: 61, acc: 48.29059829059829, f1: 11.40282131661442, r: 0.12104266393649833
06/02/2019 03:46:25 *** epoch: 63 ***
06/02/2019 03:46:25 *** training ***
06/02/2019 03:46:25 step: 2051, epoch: 62, batch: 4, loss: 1.6665905714035034, acc: 37.5, f1: 9.228497600590622, r: 0.10182090699601232
06/02/2019 03:46:25 step: 2056, epoch: 62, batch: 9, loss: 1.7137614488601685, acc: 34.375, f1: 9.4267178604528, r: 0.08072573813705525
06/02/2019 03:46:26 step: 2061, epoch: 62, batch: 14, loss: 1.6611133813858032, acc: 39.0625, f1: 10.610859728506787, r: 0.1278093985515893
06/02/2019 03:46:26 step: 2066, epoch: 62, batch: 19, loss: 1.7193706035614014, acc: 42.1875, f1: 7.417582417582418, r: 0.015031141773199046
06/02/2019 03:46:26 step: 2071, epoch: 62, batch: 24, loss: 1.5722243785858154, acc: 45.3125, f1: 10.39426523297491, r: 0.11226282246279223
06/02/2019 03:46:27 step: 2076, epoch: 62, batch: 29, loss: 1.6511731147766113, acc: 45.3125, f1: 11.330049261083744, r: 0.11064168195772284
06/02/2019 03:46:27 *** evaluating ***
06/02/2019 03:46:27 step: 63, epoch: 62, acc: 50.0, f1: 12.950434742176967, r: 0.13357188512954388
06/02/2019 03:46:27 *** epoch: 64 ***
06/02/2019 03:46:27 *** training ***
06/02/2019 03:46:27 step: 2084, epoch: 63, batch: 4, loss: 1.5162763595581055, acc: 54.6875, f1: 10.846219931271477, r: 0.16827485384292648
06/02/2019 03:46:27 step: 2089, epoch: 63, batch: 9, loss: 1.5566476583480835, acc: 45.3125, f1: 13.4006734006734, r: 0.15415711664158843
06/02/2019 03:46:27 step: 2094, epoch: 63, batch: 14, loss: 1.5959445238113403, acc: 48.4375, f1: 14.078674948240163, r: 0.11651094508980742
06/02/2019 03:46:28 step: 2099, epoch: 63, batch: 19, loss: 1.8076550960540771, acc: 32.8125, f1: 7.0588235294117645, r: 0.01795033801895001
06/02/2019 03:46:28 step: 2104, epoch: 63, batch: 24, loss: 1.6605169773101807, acc: 43.75, f1: 8.695652173913045, r: 0.05706173961374092
06/02/2019 03:46:28 step: 2109, epoch: 63, batch: 29, loss: 1.85604989528656, acc: 29.6875, f1: 9.206349206349207, r: 0.06198745073689037
06/02/2019 03:46:28 *** evaluating ***
06/02/2019 03:46:28 step: 64, epoch: 63, acc: 49.572649572649574, f1: 12.712191358024691, r: 0.12245945960818196
06/02/2019 03:46:28 *** epoch: 65 ***
06/02/2019 03:46:28 *** training ***
06/02/2019 03:46:28 step: 2117, epoch: 64, batch: 4, loss: 1.7532392740249634, acc: 39.0625, f1: 7.102272727272727, r: 0.09457590863856885
06/02/2019 03:46:29 step: 2122, epoch: 64, batch: 9, loss: 1.7040235996246338, acc: 37.5, f1: 9.338896020539151, r: 0.16912879594001218
06/02/2019 03:46:29 step: 2127, epoch: 64, batch: 14, loss: 1.704343557357788, acc: 35.9375, f1: 7.6411960132890355, r: 0.0871686036897567
06/02/2019 03:46:29 step: 2132, epoch: 64, batch: 19, loss: 1.5776934623718262, acc: 46.875, f1: 14.147509578544062, r: 0.16723573296026192
06/02/2019 03:46:29 step: 2137, epoch: 64, batch: 24, loss: 1.6929714679718018, acc: 45.3125, f1: 9.006211180124224, r: 0.02739501046657665
06/02/2019 03:46:29 step: 2142, epoch: 64, batch: 29, loss: 1.5488042831420898, acc: 51.5625, f1: 15.541601255886969, r: 0.14082289920691793
06/02/2019 03:46:30 *** evaluating ***
06/02/2019 03:46:30 step: 65, epoch: 64, acc: 48.29059829059829, f1: 11.639594926555372, r: 0.13598565365750928
06/02/2019 03:46:30 *** epoch: 66 ***
06/02/2019 03:46:30 *** training ***
06/02/2019 03:46:30 step: 2150, epoch: 65, batch: 4, loss: 1.6252613067626953, acc: 40.625, f1: 11.845238095238095, r: 0.12364582476739838
06/02/2019 03:46:30 step: 2155, epoch: 65, batch: 9, loss: 1.5055160522460938, acc: 46.875, f1: 22.371675943104513, r: 0.15120947089710962
06/02/2019 03:46:30 step: 2160, epoch: 65, batch: 14, loss: 1.705102562904358, acc: 45.3125, f1: 13.580604655655366, r: 0.062248392024779886
06/02/2019 03:46:31 step: 2165, epoch: 65, batch: 19, loss: 1.53934645652771, acc: 51.5625, f1: 12.221462747778537, r: 0.13387981860212408
06/02/2019 03:46:31 step: 2170, epoch: 65, batch: 24, loss: 1.6662169694900513, acc: 48.4375, f1: 9.323308270676693, r: 0.054458025654808354
06/02/2019 03:46:31 step: 2175, epoch: 65, batch: 29, loss: 1.7080395221710205, acc: 37.5, f1: 9.503801520608244, r: 0.03224636631164303
06/02/2019 03:46:31 *** evaluating ***
06/02/2019 03:46:31 step: 66, epoch: 65, acc: 49.14529914529914, f1: 12.125883591517523, r: 0.15096285604049509
06/02/2019 03:46:31 *** epoch: 67 ***
06/02/2019 03:46:31 *** training ***
06/02/2019 03:46:32 step: 2183, epoch: 66, batch: 4, loss: 1.6164897680282593, acc: 50.0, f1: 16.277173913043477, r: 0.14904185467161427
06/02/2019 03:46:32 step: 2188, epoch: 66, batch: 9, loss: 1.7052502632141113, acc: 40.625, f1: 14.206349206349206, r: 0.1331478953266869
06/02/2019 03:46:32 step: 2193, epoch: 66, batch: 14, loss: 1.6723711490631104, acc: 42.1875, f1: 11.781609195402298, r: 0.08733007219726943
06/02/2019 03:46:32 step: 2198, epoch: 66, batch: 19, loss: 1.5784238576889038, acc: 46.875, f1: 12.250233426704016, r: 0.1735583158663711
06/02/2019 03:46:33 step: 2203, epoch: 66, batch: 24, loss: 1.810524344444275, acc: 35.9375, f1: 7.6411960132890355, r: 0.06008800574973722
06/02/2019 03:46:33 step: 2208, epoch: 66, batch: 29, loss: 1.8008582592010498, acc: 32.8125, f1: 7.643427741466957, r: 0.08226720019293546
06/02/2019 03:46:33 *** evaluating ***
06/02/2019 03:46:33 step: 67, epoch: 66, acc: 48.717948717948715, f1: 11.792282726289217, r: 0.14165227483435652
06/02/2019 03:46:33 *** epoch: 68 ***
06/02/2019 03:46:33 *** training ***
06/02/2019 03:46:33 step: 2216, epoch: 67, batch: 4, loss: 1.8292878866195679, acc: 37.5, f1: 9.88970588235294, r: 0.09623890522869485
06/02/2019 03:46:34 step: 2221, epoch: 67, batch: 9, loss: 1.7514833211898804, acc: 29.6875, f1: 6.0126582278481004, r: 0.011929871270675131
06/02/2019 03:46:34 step: 2226, epoch: 67, batch: 14, loss: 1.686257243156433, acc: 39.0625, f1: 10.02155172413793, r: 0.11796451856618702
06/02/2019 03:46:34 step: 2231, epoch: 67, batch: 19, loss: 1.7049977779388428, acc: 42.1875, f1: 8.47723704866562, r: 0.07430990307723559
06/02/2019 03:46:34 step: 2236, epoch: 67, batch: 24, loss: 1.6687017679214478, acc: 43.75, f1: 11.746031746031745, r: 0.14928057108043757
06/02/2019 03:46:34 step: 2241, epoch: 67, batch: 29, loss: 1.6990058422088623, acc: 35.9375, f1: 9.180216802168022, r: 0.03921219958619305
06/02/2019 03:46:35 *** evaluating ***
06/02/2019 03:46:35 step: 68, epoch: 67, acc: 50.427350427350426, f1: 13.272058823529411, r: 0.14927233235586432
06/02/2019 03:46:35 *** epoch: 69 ***
06/02/2019 03:46:35 *** training ***
06/02/2019 03:46:35 step: 2249, epoch: 68, batch: 4, loss: 1.6320244073867798, acc: 43.75, f1: 10.596264367816092, r: 0.1925579967361946
06/02/2019 03:46:35 step: 2254, epoch: 68, batch: 9, loss: 1.6651021242141724, acc: 37.5, f1: 8.248546511627906, r: 0.0645039775712258
06/02/2019 03:46:35 step: 2259, epoch: 68, batch: 14, loss: 1.79347562789917, acc: 31.25, f1: 6.024096385542169, r: 0.05466169846108496
06/02/2019 03:46:36 step: 2264, epoch: 68, batch: 19, loss: 1.597033977508545, acc: 39.0625, f1: 10.262725779967157, r: 0.13107179566626156
06/02/2019 03:46:36 step: 2269, epoch: 68, batch: 24, loss: 1.636012077331543, acc: 45.3125, f1: 10.507246376811596, r: 0.18041314468489492
06/02/2019 03:46:36 step: 2274, epoch: 68, batch: 29, loss: 1.7193111181259155, acc: 37.5, f1: 10.278745644599303, r: 0.12069733185318157
06/02/2019 03:46:36 *** evaluating ***
06/02/2019 03:46:36 step: 69, epoch: 68, acc: 49.14529914529914, f1: 12.125883591517523, r: 0.1396608563644719
06/02/2019 03:46:36 *** epoch: 70 ***
06/02/2019 03:46:36 *** training ***
06/02/2019 03:46:36 step: 2282, epoch: 69, batch: 4, loss: 1.7334479093551636, acc: 37.5, f1: 7.973421926910299, r: 0.12915367734270047
06/02/2019 03:46:37 step: 2287, epoch: 69, batch: 9, loss: 1.6831214427947998, acc: 40.625, f1: 11.291486291486292, r: 0.07619923809407012
06/02/2019 03:46:37 step: 2292, epoch: 69, batch: 14, loss: 1.754805564880371, acc: 40.625, f1: 8.253968253968253, r: 0.06651293566011743
06/02/2019 03:46:37 step: 2297, epoch: 69, batch: 19, loss: 1.7147307395935059, acc: 42.1875, f1: 8.571428571428571, r: 0.0028280799403461116
06/02/2019 03:46:38 step: 2302, epoch: 69, batch: 24, loss: 1.7574576139450073, acc: 32.8125, f1: 14.966847498493069, r: 0.14345500755277601
06/02/2019 03:46:38 step: 2307, epoch: 69, batch: 29, loss: 1.650986909866333, acc: 42.1875, f1: 8.667736757624397, r: 0.10463882906208155
06/02/2019 03:46:38 *** evaluating ***
06/02/2019 03:46:38 step: 70, epoch: 69, acc: 51.28205128205128, f1: 13.890386343216532, r: 0.1493780931386822
06/02/2019 03:46:38 *** epoch: 71 ***
06/02/2019 03:46:38 *** training ***
06/02/2019 03:46:38 step: 2315, epoch: 70, batch: 4, loss: 1.625996708869934, acc: 46.875, f1: 24.586466165413533, r: 0.1275439727129151
06/02/2019 03:46:38 step: 2320, epoch: 70, batch: 9, loss: 1.9165326356887817, acc: 35.9375, f1: 8.412220309810671, r: 0.08405665420376215
06/02/2019 03:46:39 step: 2325, epoch: 70, batch: 14, loss: 1.7302628755569458, acc: 43.75, f1: 8.695652173913045, r: 0.10078940333304502
06/02/2019 03:46:39 step: 2330, epoch: 70, batch: 19, loss: 1.6807600259780884, acc: 32.8125, f1: 7.142857142857142, r: 0.09032309876088093
06/02/2019 03:46:39 step: 2335, epoch: 70, batch: 24, loss: 1.6303961277008057, acc: 42.1875, f1: 10.387525796835588, r: 0.12935713658857023
06/02/2019 03:46:39 step: 2340, epoch: 70, batch: 29, loss: 1.7903259992599487, acc: 31.25, f1: 6.097560975609756, r: 0.17029116958193563
06/02/2019 03:46:39 *** evaluating ***
06/02/2019 03:46:39 step: 71, epoch: 70, acc: 50.0, f1: 12.950434742176967, r: 0.15130472018027255
06/02/2019 03:46:39 *** epoch: 72 ***
06/02/2019 03:46:39 *** training ***
06/02/2019 03:46:40 step: 2348, epoch: 71, batch: 4, loss: 1.816144585609436, acc: 34.375, f1: 10.339506172839505, r: 0.1795378624256464
06/02/2019 03:46:40 step: 2353, epoch: 71, batch: 9, loss: 1.7047897577285767, acc: 34.375, f1: 7.991967871485945, r: 0.1659140624203864
06/02/2019 03:46:40 step: 2358, epoch: 71, batch: 14, loss: 1.8485649824142456, acc: 32.8125, f1: 7.0588235294117645, r: 0.030439946829718977
06/02/2019 03:46:40 step: 2363, epoch: 71, batch: 19, loss: 1.737975835800171, acc: 35.9375, f1: 7.55336617405583, r: 0.018259866622007265
06/02/2019 03:46:40 step: 2368, epoch: 71, batch: 24, loss: 1.5785624980926514, acc: 42.1875, f1: 7.5, r: 0.18089439256153853
06/02/2019 03:46:41 step: 2373, epoch: 71, batch: 29, loss: 1.6443960666656494, acc: 43.75, f1: 11.434108527131784, r: 0.10222204327488679
06/02/2019 03:46:41 *** evaluating ***
06/02/2019 03:46:41 step: 72, epoch: 71, acc: 49.572649572649574, f1: 12.442002442002442, r: 0.15209591538384296
06/02/2019 03:46:41 *** epoch: 73 ***
06/02/2019 03:46:41 *** training ***
06/02/2019 03:46:41 step: 2381, epoch: 72, batch: 4, loss: 1.6815825700759888, acc: 39.0625, f1: 10.45731707317073, r: 0.14233399714854547
06/02/2019 03:46:41 step: 2386, epoch: 72, batch: 9, loss: 1.4975697994232178, acc: 51.5625, f1: 12.323846366399557, r: 0.18531124436700303
06/02/2019 03:46:41 step: 2391, epoch: 72, batch: 14, loss: 1.6056230068206787, acc: 42.1875, f1: 8.775252525252524, r: 0.12457300776048875
06/02/2019 03:46:42 step: 2396, epoch: 72, batch: 19, loss: 1.6861307621002197, acc: 35.9375, f1: 8.110119047619047, r: 0.10813195611396653
06/02/2019 03:46:42 step: 2401, epoch: 72, batch: 24, loss: 1.6765965223312378, acc: 39.0625, f1: 7.022471910112358, r: 0.001458868981283557
06/02/2019 03:46:42 step: 2406, epoch: 72, batch: 29, loss: 1.5957796573638916, acc: 40.625, f1: 8.253968253968253, r: 0.018904868251751403
06/02/2019 03:46:42 *** evaluating ***
06/02/2019 03:46:43 step: 73, epoch: 72, acc: 49.572649572649574, f1: 12.664920219099784, r: 0.1512443671681368
06/02/2019 03:46:43 *** epoch: 74 ***
06/02/2019 03:46:43 *** training ***
06/02/2019 03:46:43 step: 2414, epoch: 73, batch: 4, loss: 1.6678409576416016, acc: 45.3125, f1: 8.90937019969278, r: 0.05785391697216799
06/02/2019 03:46:43 step: 2419, epoch: 73, batch: 9, loss: 1.7994943857192993, acc: 31.25, f1: 6.802721088435374, r: 0.06342369780064605
06/02/2019 03:46:43 step: 2424, epoch: 73, batch: 14, loss: 1.7266466617584229, acc: 40.625, f1: 8.253968253968253, r: 0.08854572014168137
06/02/2019 03:46:43 step: 2429, epoch: 73, batch: 19, loss: 1.6400582790374756, acc: 42.1875, f1: 10.164141414141415, r: 0.18280707396570148
06/02/2019 03:46:44 step: 2434, epoch: 73, batch: 24, loss: 1.6324617862701416, acc: 50.0, f1: 14.511575381140599, r: 0.09369114236669465
06/02/2019 03:46:44 step: 2439, epoch: 73, batch: 29, loss: 1.6493903398513794, acc: 40.625, f1: 9.02534965034965, r: 0.05310007655079385
06/02/2019 03:46:44 *** evaluating ***
06/02/2019 03:46:44 step: 74, epoch: 73, acc: 50.0, f1: 12.950434742176967, r: 0.15042761239166955
06/02/2019 03:46:44 *** epoch: 75 ***
06/02/2019 03:46:44 *** training ***
06/02/2019 03:46:44 step: 2447, epoch: 74, batch: 4, loss: 1.7978122234344482, acc: 29.6875, f1: 11.600429645542429, r: 0.1491566801061109
06/02/2019 03:46:45 step: 2452, epoch: 74, batch: 9, loss: 1.5858758687973022, acc: 50.0, f1: 8.333333333333332, r: 0.11282741240978007
06/02/2019 03:46:45 step: 2457, epoch: 74, batch: 14, loss: 1.7455838918685913, acc: 35.9375, f1: 8.39366515837104, r: 0.02910042232831219
06/02/2019 03:46:45 step: 2462, epoch: 74, batch: 19, loss: 1.5928257703781128, acc: 43.75, f1: 8.791208791208792, r: 0.08919905302521014
06/02/2019 03:46:45 step: 2467, epoch: 74, batch: 24, loss: 1.6272655725479126, acc: 43.75, f1: 12.352941176470589, r: 0.15472337775157
06/02/2019 03:46:45 step: 2472, epoch: 74, batch: 29, loss: 1.5780012607574463, acc: 45.3125, f1: 14.94717160848782, r: 0.08936583248219121
06/02/2019 03:46:45 *** evaluating ***
06/02/2019 03:46:46 step: 75, epoch: 74, acc: 49.572649572649574, f1: 12.664920219099784, r: 0.1516893104506275
06/02/2019 03:46:46 *** epoch: 76 ***
06/02/2019 03:46:46 *** training ***
06/02/2019 03:46:46 step: 2480, epoch: 75, batch: 4, loss: 1.7876701354980469, acc: 37.5, f1: 7.792207792207792, r: -0.024170064691343467
06/02/2019 03:46:46 step: 2485, epoch: 75, batch: 9, loss: 1.7074223756790161, acc: 32.8125, f1: 8.641975308641975, r: 0.07483879985880323
06/02/2019 03:46:46 step: 2490, epoch: 75, batch: 14, loss: 1.5273574590682983, acc: 48.4375, f1: 11.357586512866016, r: 0.11409954495923882
06/02/2019 03:46:46 step: 2495, epoch: 75, batch: 19, loss: 1.7942503690719604, acc: 29.6875, f1: 5.864197530864198, r: 0.05083955396388424
06/02/2019 03:46:47 step: 2500, epoch: 75, batch: 24, loss: 1.6934868097305298, acc: 35.9375, f1: 8.256302521008402, r: 0.15456863462533488
06/02/2019 03:46:47 step: 2505, epoch: 75, batch: 29, loss: 1.6415271759033203, acc: 40.625, f1: 9.608323133414933, r: 0.1004648919457207
06/02/2019 03:46:47 *** evaluating ***
06/02/2019 03:46:47 step: 76, epoch: 75, acc: 51.70940170940172, f1: 14.187808237437244, r: 0.15791227390440363
06/02/2019 03:46:47 *** epoch: 77 ***
06/02/2019 03:46:47 *** training ***
06/02/2019 03:46:47 step: 2513, epoch: 76, batch: 4, loss: 1.8357104063034058, acc: 28.125, f1: 5.625000000000001, r: 0.1518181724016309
06/02/2019 03:46:48 step: 2518, epoch: 76, batch: 9, loss: 1.5751324892044067, acc: 45.3125, f1: 12.824302134646961, r: 0.1310244866381702
06/02/2019 03:46:48 step: 2523, epoch: 76, batch: 14, loss: 1.700401782989502, acc: 34.375, f1: 8.909587931558164, r: 0.12864675880287219
06/02/2019 03:46:48 step: 2528, epoch: 76, batch: 19, loss: 1.5664080381393433, acc: 46.875, f1: 10.153162055335969, r: 0.02787225101023553
06/02/2019 03:46:48 step: 2533, epoch: 76, batch: 24, loss: 1.7693508863449097, acc: 32.8125, f1: 6.176470588235294, r: 0.11900932751321391
06/02/2019 03:46:49 step: 2538, epoch: 76, batch: 29, loss: 1.600052833557129, acc: 54.6875, f1: 8.928571428571429, r: 0.10239208366694887
06/02/2019 03:46:49 *** evaluating ***
06/02/2019 03:46:49 step: 77, epoch: 76, acc: 50.427350427350426, f1: 13.272058823529411, r: 0.1502895767083943
06/02/2019 03:46:49 *** epoch: 78 ***
06/02/2019 03:46:49 *** training ***
06/02/2019 03:46:49 step: 2546, epoch: 77, batch: 4, loss: 1.8376682996749878, acc: 35.9375, f1: 6.686046511627906, r: -0.014600815143018695
06/02/2019 03:46:49 step: 2551, epoch: 77, batch: 9, loss: 1.6836254596710205, acc: 45.3125, f1: 9.340277777777779, r: 0.040419935672475246
06/02/2019 03:46:50 step: 2556, epoch: 77, batch: 14, loss: 1.5405269861221313, acc: 48.4375, f1: 14.809612983770288, r: 0.18184058731857544
06/02/2019 03:46:50 step: 2561, epoch: 77, batch: 19, loss: 1.6569228172302246, acc: 40.625, f1: 11.87675070028011, r: 0.14300291287943834
06/02/2019 03:46:50 step: 2566, epoch: 77, batch: 24, loss: 1.7103936672210693, acc: 28.125, f1: 7.828954366556749, r: 0.08197825271343147
06/02/2019 03:46:50 step: 2571, epoch: 77, batch: 29, loss: 1.7386332750320435, acc: 39.0625, f1: 8.025682182985554, r: 0.13029300184718906
06/02/2019 03:46:50 *** evaluating ***
06/02/2019 03:46:50 step: 78, epoch: 77, acc: 50.85470085470085, f1: 13.585252828131388, r: 0.15763835740278967
06/02/2019 03:46:50 *** epoch: 79 ***
06/02/2019 03:46:50 *** training ***
06/02/2019 03:46:51 step: 2579, epoch: 78, batch: 4, loss: 1.8187611103057861, acc: 31.25, f1: 8.917682926829269, r: 0.14265412523748527
06/02/2019 03:46:51 step: 2584, epoch: 78, batch: 9, loss: 1.8680505752563477, acc: 29.6875, f1: 9.361471861471863, r: 0.0501912363563427
06/02/2019 03:46:51 step: 2589, epoch: 78, batch: 14, loss: 1.6645023822784424, acc: 39.0625, f1: 12.126696832579185, r: 0.0037523204512402986
06/02/2019 03:46:51 step: 2594, epoch: 78, batch: 19, loss: 1.5812299251556396, acc: 51.5625, f1: 12.901046943600134, r: 0.09326633116063428
06/02/2019 03:46:51 step: 2599, epoch: 78, batch: 24, loss: 1.6808828115463257, acc: 40.625, f1: 13.468992248062014, r: 0.16051983804515382
06/02/2019 03:46:52 step: 2604, epoch: 78, batch: 29, loss: 1.61837637424469, acc: 45.3125, f1: 13.26945412311266, r: 0.05973091604751418
06/02/2019 03:46:52 *** evaluating ***
06/02/2019 03:46:52 step: 79, epoch: 78, acc: 49.572649572649574, f1: 12.664920219099784, r: 0.15829829599127135
06/02/2019 03:46:52 *** epoch: 80 ***
06/02/2019 03:46:52 *** training ***
06/02/2019 03:46:52 step: 2612, epoch: 79, batch: 4, loss: 1.5804991722106934, acc: 40.625, f1: 14.910390814005273, r: 0.20011474562971618
06/02/2019 03:46:52 step: 2617, epoch: 79, batch: 9, loss: 1.598679542541504, acc: 43.75, f1: 12.934192244537071, r: 0.14794596883015596
06/02/2019 03:46:52 step: 2622, epoch: 79, batch: 14, loss: 1.6923375129699707, acc: 40.625, f1: 7.222222222222221, r: 0.09703279132444083
06/02/2019 03:46:53 step: 2627, epoch: 79, batch: 19, loss: 1.5793168544769287, acc: 46.875, f1: 14.58128078817734, r: 0.1608229491773824
06/02/2019 03:46:53 step: 2632, epoch: 79, batch: 24, loss: 1.7743786573410034, acc: 32.8125, f1: 8.565353852384327, r: 0.13294768704703974
06/02/2019 03:46:53 step: 2637, epoch: 79, batch: 29, loss: 1.7373183965682983, acc: 40.625, f1: 8.346709470304976, r: 0.07437576913512159
06/02/2019 03:46:53 *** evaluating ***
06/02/2019 03:46:53 step: 80, epoch: 79, acc: 48.717948717948715, f1: 11.792282726289217, r: 0.14399523843034362
06/02/2019 03:46:53 *** epoch: 81 ***
06/02/2019 03:46:53 *** training ***
06/02/2019 03:46:54 step: 2645, epoch: 80, batch: 4, loss: 1.6445889472961426, acc: 43.75, f1: 11.931818181818182, r: 0.15394481948255578
06/02/2019 03:46:54 step: 2650, epoch: 80, batch: 9, loss: 1.547542929649353, acc: 45.3125, f1: 13.528138528138529, r: 0.1521739218045589
06/02/2019 03:46:54 step: 2655, epoch: 80, batch: 14, loss: 1.5443145036697388, acc: 51.5625, f1: 13.333333333333334, r: 0.13744687030459105
06/02/2019 03:46:54 step: 2660, epoch: 80, batch: 19, loss: 1.6495293378829956, acc: 34.375, f1: 9.183673469387756, r: 0.060731028252239194
06/02/2019 03:46:55 step: 2665, epoch: 80, batch: 24, loss: 1.7746903896331787, acc: 35.9375, f1: 10.637254901960784, r: 0.11406365570130941
06/02/2019 03:46:55 step: 2670, epoch: 80, batch: 29, loss: 1.555153250694275, acc: 43.75, f1: 11.428571428571429, r: 0.12194557680661502
06/02/2019 03:46:55 *** evaluating ***
06/02/2019 03:46:55 step: 81, epoch: 80, acc: 49.572649572649574, f1: 12.664920219099784, r: 0.15554738322850775
06/02/2019 03:46:55 *** epoch: 82 ***
06/02/2019 03:46:55 *** training ***
06/02/2019 03:46:55 step: 2678, epoch: 81, batch: 4, loss: 1.6459299325942993, acc: 40.625, f1: 10.591133004926109, r: 0.05524506729585904
06/02/2019 03:46:55 step: 2683, epoch: 81, batch: 9, loss: 1.6392709016799927, acc: 46.875, f1: 10.990168539325843, r: 0.16957877159552384
06/02/2019 03:46:56 step: 2688, epoch: 81, batch: 14, loss: 1.6702903509140015, acc: 39.0625, f1: 10.035596933187293, r: 0.1378224598647757
06/02/2019 03:46:56 step: 2693, epoch: 81, batch: 19, loss: 1.767665147781372, acc: 31.25, f1: 9.08289241622575, r: 0.08900997038570826
06/02/2019 03:46:56 step: 2698, epoch: 81, batch: 24, loss: 1.734557867050171, acc: 35.9375, f1: 9.775910364145659, r: 0.10753703242559416
06/02/2019 03:46:56 step: 2703, epoch: 81, batch: 29, loss: 1.6743272542953491, acc: 32.8125, f1: 7.809810671256455, r: 0.14571207056447433
06/02/2019 03:46:56 *** evaluating ***
06/02/2019 03:46:56 step: 82, epoch: 81, acc: 51.28205128205128, f1: 13.890386343216532, r: 0.1576838739791419
06/02/2019 03:46:56 *** epoch: 83 ***
06/02/2019 03:46:56 *** training ***
06/02/2019 03:46:57 step: 2711, epoch: 82, batch: 4, loss: 1.854936957359314, acc: 31.25, f1: 5.952380952380952, r: 0.010385777278735816
06/02/2019 03:46:57 step: 2716, epoch: 82, batch: 9, loss: 1.6149946451187134, acc: 43.75, f1: 9.285714285714286, r: 0.05408528369646787
06/02/2019 03:46:57 step: 2721, epoch: 82, batch: 14, loss: 1.825774908065796, acc: 28.125, f1: 6.2717770034843205, r: 0.08261732149473273
06/02/2019 03:46:57 step: 2726, epoch: 82, batch: 19, loss: 1.6838512420654297, acc: 43.75, f1: 14.053156146179402, r: 0.12068871152713224
06/02/2019 03:46:57 step: 2731, epoch: 82, batch: 24, loss: 1.6519273519515991, acc: 43.75, f1: 10.865538955426594, r: 0.11485194321147829
06/02/2019 03:46:58 step: 2736, epoch: 82, batch: 29, loss: 1.520163893699646, acc: 53.125, f1: 14.488348530901723, r: 0.13522904955325685
06/02/2019 03:46:58 *** evaluating ***
06/02/2019 03:46:58 step: 83, epoch: 82, acc: 51.28205128205128, f1: 13.890386343216532, r: 0.15811207702073915
06/02/2019 03:46:58 *** epoch: 84 ***
06/02/2019 03:46:58 *** training ***
06/02/2019 03:46:58 step: 2744, epoch: 83, batch: 4, loss: 1.6591460704803467, acc: 35.9375, f1: 8.110119047619047, r: 0.0989124558206248
06/02/2019 03:46:58 step: 2749, epoch: 83, batch: 9, loss: 1.6223599910736084, acc: 40.625, f1: 12.959400627720969, r: 0.15316117428121184
06/02/2019 03:46:58 step: 2754, epoch: 83, batch: 14, loss: 1.7042158842086792, acc: 37.5, f1: 13.445378151260504, r: 0.11569544640749906
06/02/2019 03:46:59 step: 2759, epoch: 83, batch: 19, loss: 1.792452096939087, acc: 28.125, f1: 6.850335070737155, r: 0.14488197647782325
06/02/2019 03:46:59 step: 2764, epoch: 83, batch: 24, loss: 1.6400243043899536, acc: 39.0625, f1: 8.979885057471265, r: 0.1653141381610798
06/02/2019 03:46:59 step: 2769, epoch: 83, batch: 29, loss: 1.6545289754867554, acc: 40.625, f1: 7.222222222222221, r: 0.13823199135278816
06/02/2019 03:46:59 *** evaluating ***
06/02/2019 03:46:59 step: 84, epoch: 83, acc: 49.572649572649574, f1: 12.664920219099784, r: 0.15021051955453144
06/02/2019 03:46:59 *** epoch: 85 ***
06/02/2019 03:46:59 *** training ***
06/02/2019 03:47:00 step: 2777, epoch: 84, batch: 4, loss: 1.591717004776001, acc: 46.875, f1: 9.118541033434651, r: 0.11522701592676998
06/02/2019 03:47:00 step: 2782, epoch: 84, batch: 9, loss: 1.5388175249099731, acc: 43.75, f1: 8.79120879120879, r: 0.1834053008622224
06/02/2019 03:47:00 step: 2787, epoch: 84, batch: 14, loss: 1.7016297578811646, acc: 40.625, f1: 10.392156862745097, r: 0.10189313106555675
06/02/2019 03:47:00 step: 2792, epoch: 84, batch: 19, loss: 1.5993678569793701, acc: 45.3125, f1: 18.506493506493506, r: 0.1409506213979565
06/02/2019 03:47:00 step: 2797, epoch: 84, batch: 24, loss: 1.6158915758132935, acc: 48.4375, f1: 8.157894736842106, r: 0.06516221199818331
06/02/2019 03:47:01 step: 2802, epoch: 84, batch: 29, loss: 1.7465256452560425, acc: 31.25, f1: 5.952380952380952, r: 0.040289723831009865
06/02/2019 03:47:01 *** evaluating ***
06/02/2019 03:47:01 step: 85, epoch: 84, acc: 50.85470085470085, f1: 13.585252828131388, r: 0.1672345204458858
06/02/2019 03:47:01 *** epoch: 86 ***
06/02/2019 03:47:01 *** training ***
06/02/2019 03:47:01 step: 2810, epoch: 85, batch: 4, loss: 1.652421474456787, acc: 39.0625, f1: 8.116883116883116, r: 0.050220683300056745
06/02/2019 03:47:01 step: 2815, epoch: 85, batch: 9, loss: 1.6679894924163818, acc: 37.5, f1: 10.714285714285715, r: 0.15251298673653152
06/02/2019 03:47:01 step: 2820, epoch: 85, batch: 14, loss: 1.6148921251296997, acc: 35.9375, f1: 9.523809523809526, r: 0.1413533250645979
06/02/2019 03:47:01 step: 2825, epoch: 85, batch: 19, loss: 1.636522650718689, acc: 40.625, f1: 10.15769944341373, r: 0.22034658765521115
06/02/2019 03:47:02 step: 2830, epoch: 85, batch: 24, loss: 1.7742948532104492, acc: 34.375, f1: 9.668989547038327, r: 0.11993008414230158
06/02/2019 03:47:02 step: 2835, epoch: 85, batch: 29, loss: 1.8218998908996582, acc: 34.375, f1: 6.707317073170731, r: 0.027463891490539717
06/02/2019 03:47:02 *** evaluating ***
06/02/2019 03:47:02 step: 86, epoch: 85, acc: 51.70940170940172, f1: 14.187808237437244, r: 0.16493040738405165
06/02/2019 03:47:02 *** epoch: 87 ***
06/02/2019 03:47:02 *** training ***
06/02/2019 03:47:02 step: 2843, epoch: 86, batch: 4, loss: 1.8431521654129028, acc: 39.0625, f1: 7.02247191011236, r: 0.06113498315524483
06/02/2019 03:47:02 step: 2848, epoch: 86, batch: 9, loss: 1.7924425601959229, acc: 35.9375, f1: 11.624520058254998, r: 0.08501373833511293
06/02/2019 03:47:03 step: 2853, epoch: 86, batch: 14, loss: 1.8193788528442383, acc: 35.9375, f1: 8.293172690763052, r: 0.06518286457162904
06/02/2019 03:47:03 step: 2858, epoch: 86, batch: 19, loss: 1.6318657398223877, acc: 32.8125, f1: 10.13755980861244, r: 0.2343769305587855
06/02/2019 03:47:03 step: 2863, epoch: 86, batch: 24, loss: 1.7999329566955566, acc: 35.9375, f1: 12.063492063492063, r: 0.1695007591553024
06/02/2019 03:47:03 step: 2868, epoch: 86, batch: 29, loss: 1.7171366214752197, acc: 42.1875, f1: 7.417582417582418, r: 0.06495651841265661
06/02/2019 03:47:03 *** evaluating ***
06/02/2019 03:47:03 step: 87, epoch: 86, acc: 50.427350427350426, f1: 13.272058823529411, r: 0.17005212131307668
06/02/2019 03:47:03 *** epoch: 88 ***
06/02/2019 03:47:03 *** training ***
06/02/2019 03:47:04 step: 2876, epoch: 87, batch: 4, loss: 1.759994626045227, acc: 34.375, f1: 9.943714821763603, r: 0.06546040761817014
06/02/2019 03:47:04 step: 2881, epoch: 87, batch: 9, loss: 1.6253037452697754, acc: 42.1875, f1: 10.482374768089056, r: 0.07305739091858293
06/02/2019 03:47:04 step: 2886, epoch: 87, batch: 14, loss: 1.7081937789916992, acc: 39.0625, f1: 9.249471458773783, r: 0.048442714590987784
06/02/2019 03:47:04 step: 2891, epoch: 87, batch: 19, loss: 1.51578688621521, acc: 48.4375, f1: 12.676609105180534, r: 0.16077560570220015
06/02/2019 03:47:05 step: 2896, epoch: 87, batch: 24, loss: 1.5931406021118164, acc: 42.1875, f1: 10.482374768089056, r: 0.13784905541369696
06/02/2019 03:47:05 step: 2901, epoch: 87, batch: 29, loss: 1.648486852645874, acc: 42.1875, f1: 10.978915662650602, r: 0.11736531657535296
06/02/2019 03:47:05 *** evaluating ***
06/02/2019 03:47:05 step: 88, epoch: 87, acc: 50.85470085470085, f1: 13.585252828131388, r: 0.17001634677868563
06/02/2019 03:47:05 *** epoch: 89 ***
06/02/2019 03:47:05 *** training ***
06/02/2019 03:47:05 step: 2909, epoch: 88, batch: 4, loss: 1.7972620725631714, acc: 35.9375, f1: 9.95410212277682, r: 0.10685305174227308
06/02/2019 03:47:06 step: 2914, epoch: 88, batch: 9, loss: 1.7804044485092163, acc: 35.9375, f1: 6.609195402298851, r: 0.0542745959468703
06/02/2019 03:47:06 step: 2919, epoch: 88, batch: 14, loss: 1.5553280115127563, acc: 48.4375, f1: 14.448051948051946, r: 0.13923020448251877
06/02/2019 03:47:06 step: 2924, epoch: 88, batch: 19, loss: 1.9293906688690186, acc: 28.125, f1: 8.4375, r: 0.05953282637191452
06/02/2019 03:47:06 step: 2929, epoch: 88, batch: 24, loss: 1.7078677415847778, acc: 39.0625, f1: 10.737491877842755, r: 0.13336345638958316
06/02/2019 03:47:06 step: 2934, epoch: 88, batch: 29, loss: 1.742845892906189, acc: 32.8125, f1: 9.270833333333334, r: 0.1964339105163904
06/02/2019 03:47:07 *** evaluating ***
06/02/2019 03:47:07 step: 89, epoch: 88, acc: 55.12820512820513, f1: 16.273948252024493, r: 0.18571103828771587
06/02/2019 03:47:07 *** epoch: 90 ***
06/02/2019 03:47:07 *** training ***
06/02/2019 03:47:07 step: 2942, epoch: 89, batch: 4, loss: 1.8283988237380981, acc: 32.8125, f1: 6.25, r: 0.0529834256730683
06/02/2019 03:47:07 step: 2947, epoch: 89, batch: 9, loss: 1.6581199169158936, acc: 40.625, f1: 8.253968253968253, r: 0.0785576908907651
06/02/2019 03:47:07 step: 2952, epoch: 89, batch: 14, loss: 1.7306610345840454, acc: 35.9375, f1: 7.55336617405583, r: 0.10162642297548873
06/02/2019 03:47:07 step: 2957, epoch: 89, batch: 19, loss: 1.8975991010665894, acc: 29.6875, f1: 8.295660036166366, r: 0.19191237491093294
06/02/2019 03:47:08 step: 2962, epoch: 89, batch: 24, loss: 1.5524600744247437, acc: 48.4375, f1: 11.514572384137601, r: 0.08790917681273207
06/02/2019 03:47:08 step: 2967, epoch: 89, batch: 29, loss: 1.6662583351135254, acc: 40.625, f1: 7.386363636363637, r: 0.044141172044062905
06/02/2019 03:47:08 *** evaluating ***
06/02/2019 03:47:08 step: 90, epoch: 89, acc: 50.85470085470085, f1: 13.585252828131388, r: 0.15711804447581218
06/02/2019 03:47:08 *** epoch: 91 ***
06/02/2019 03:47:08 *** training ***
06/02/2019 03:47:08 step: 2975, epoch: 90, batch: 4, loss: 1.6530085802078247, acc: 39.0625, f1: 10.171224124712497, r: 0.16900615930937438
06/02/2019 03:47:08 step: 2980, epoch: 90, batch: 9, loss: 1.554964542388916, acc: 42.1875, f1: 13.520408163265307, r: 0.1992858770686463
06/02/2019 03:47:09 step: 2985, epoch: 90, batch: 14, loss: 1.5682852268218994, acc: 46.875, f1: 18.84057971014493, r: 0.10877381183538784
06/02/2019 03:47:09 step: 2990, epoch: 90, batch: 19, loss: 1.6809245347976685, acc: 45.3125, f1: 13.96103896103896, r: 0.07186879307302192
06/02/2019 03:47:09 step: 2995, epoch: 90, batch: 24, loss: 2.12471866607666, acc: 42.1875, f1: 10.726164079822615, r: 0.1543644623780723
06/02/2019 03:47:09 step: 3000, epoch: 90, batch: 29, loss: 1.7689454555511475, acc: 39.0625, f1: 7.02247191011236, r: 0.12601054095696518
06/02/2019 03:47:09 *** evaluating ***
06/02/2019 03:47:09 step: 91, epoch: 90, acc: 52.13675213675214, f1: 14.477848101265822, r: 0.15762023512662546
06/02/2019 03:47:09 *** epoch: 92 ***
06/02/2019 03:47:09 *** training ***
06/02/2019 03:47:10 step: 3008, epoch: 91, batch: 4, loss: 1.8663098812103271, acc: 29.6875, f1: 13.01863572433193, r: 0.09772602458819259
06/02/2019 03:47:10 step: 3013, epoch: 91, batch: 9, loss: 1.6778637170791626, acc: 39.0625, f1: 10.931372549019608, r: 0.0853228901215722
06/02/2019 03:47:10 step: 3018, epoch: 91, batch: 14, loss: 1.621707797050476, acc: 46.875, f1: 13.384368440548217, r: 0.18025550469513574
06/02/2019 03:47:10 step: 3023, epoch: 91, batch: 19, loss: 1.7152533531188965, acc: 34.375, f1: 7.641090678503487, r: 0.10540918661476067
06/02/2019 03:47:11 step: 3028, epoch: 91, batch: 24, loss: 1.5543403625488281, acc: 40.625, f1: 10.630252100840337, r: 0.1077688284423769
06/02/2019 03:47:11 step: 3033, epoch: 91, batch: 29, loss: 1.7094342708587646, acc: 42.1875, f1: 12.4283421233662, r: 0.09871690832135901
06/02/2019 03:47:11 *** evaluating ***
06/02/2019 03:47:11 step: 92, epoch: 91, acc: 52.13675213675214, f1: 14.477848101265822, r: 0.15579013077995396
06/02/2019 03:47:11 *** epoch: 93 ***
06/02/2019 03:47:11 *** training ***
06/02/2019 03:47:11 step: 3041, epoch: 92, batch: 4, loss: 1.7503111362457275, acc: 34.375, f1: 10.264227642276422, r: 0.1159097729480473
06/02/2019 03:47:11 step: 3046, epoch: 92, batch: 9, loss: 1.7341959476470947, acc: 34.375, f1: 7.308970099667775, r: 0.05753829992331029
06/02/2019 03:47:12 step: 3051, epoch: 92, batch: 14, loss: 1.6629855632781982, acc: 40.625, f1: 9.89085289288129, r: 0.1504739681370377
06/02/2019 03:47:12 step: 3056, epoch: 92, batch: 19, loss: 1.5910708904266357, acc: 46.875, f1: 14.183569239749014, r: 0.18480275054974818
06/02/2019 03:47:12 step: 3061, epoch: 92, batch: 24, loss: 1.469282865524292, acc: 48.4375, f1: 11.538461538461538, r: 0.170700191635037
06/02/2019 03:47:12 step: 3066, epoch: 92, batch: 29, loss: 1.6085662841796875, acc: 43.75, f1: 8.695652173913045, r: 0.05924792055598485
06/02/2019 03:47:12 *** evaluating ***
06/02/2019 03:47:12 step: 93, epoch: 92, acc: 50.85470085470085, f1: 13.585252828131388, r: 0.16311716623466674
06/02/2019 03:47:12 *** epoch: 94 ***
06/02/2019 03:47:12 *** training ***
06/02/2019 03:47:13 step: 3074, epoch: 93, batch: 4, loss: 1.8262113332748413, acc: 31.25, f1: 7.459349593495935, r: 0.061279523257763
06/02/2019 03:47:13 step: 3079, epoch: 93, batch: 9, loss: 1.6112864017486572, acc: 39.0625, f1: 8.025682182985554, r: 0.12743541444440207
06/02/2019 03:47:13 step: 3084, epoch: 93, batch: 14, loss: 1.7015974521636963, acc: 40.625, f1: 8.491161616161616, r: 0.10902335991464066
06/02/2019 03:47:13 step: 3089, epoch: 93, batch: 19, loss: 1.5461914539337158, acc: 42.1875, f1: 10.227272727272728, r: 0.10632362989185928
06/02/2019 03:47:13 step: 3094, epoch: 93, batch: 24, loss: 1.705551028251648, acc: 43.75, f1: 10.44690603514133, r: 0.07172723387412157
06/02/2019 03:47:14 step: 3099, epoch: 93, batch: 29, loss: 1.6477293968200684, acc: 43.75, f1: 14.835164835164836, r: 0.13485482677903546
06/02/2019 03:47:14 *** evaluating ***
06/02/2019 03:47:14 step: 94, epoch: 93, acc: 49.572649572649574, f1: 12.53571965709129, r: 0.14703546515728963
06/02/2019 03:47:14 *** epoch: 95 ***
06/02/2019 03:47:14 *** training ***
06/02/2019 03:47:14 step: 3107, epoch: 94, batch: 4, loss: 1.6532546281814575, acc: 39.0625, f1: 13.214285714285715, r: 0.08088532802863543
06/02/2019 03:47:14 step: 3112, epoch: 94, batch: 9, loss: 1.7353556156158447, acc: 37.5, f1: 7.973421926910299, r: 0.09565265298488422
06/02/2019 03:47:14 step: 3117, epoch: 94, batch: 14, loss: 1.8661140203475952, acc: 32.8125, f1: 10.409652076318743, r: 0.043562896122766544
06/02/2019 03:47:14 step: 3122, epoch: 94, batch: 19, loss: 1.6043485403060913, acc: 37.5, f1: 11.564625850340136, r: 0.18270458250536514
06/02/2019 03:47:15 step: 3127, epoch: 94, batch: 24, loss: 1.7142434120178223, acc: 42.1875, f1: 9.172077922077923, r: 0.14447210135963656
06/02/2019 03:47:15 step: 3132, epoch: 94, batch: 29, loss: 1.6512322425842285, acc: 43.75, f1: 8.88888888888889, r: 0.028806454529691883
06/02/2019 03:47:15 *** evaluating ***
06/02/2019 03:47:15 step: 95, epoch: 94, acc: 52.13675213675214, f1: 14.477848101265822, r: 0.15208660213479594
06/02/2019 03:47:15 *** epoch: 96 ***
06/02/2019 03:47:15 *** training ***
06/02/2019 03:47:15 step: 3140, epoch: 95, batch: 4, loss: 1.5826170444488525, acc: 46.875, f1: 11.204013377926422, r: 0.1420677997046805
06/02/2019 03:47:16 step: 3145, epoch: 95, batch: 9, loss: 1.7534050941467285, acc: 39.0625, f1: 9.642857142857144, r: 0.1659955301386925
06/02/2019 03:47:16 step: 3150, epoch: 95, batch: 14, loss: 1.7136471271514893, acc: 34.375, f1: 8.75, r: 0.14045488628524397
06/02/2019 03:47:16 step: 3155, epoch: 95, batch: 19, loss: 1.569869041442871, acc: 42.1875, f1: 12.626230209670517, r: 0.14715561931259333
06/02/2019 03:47:16 step: 3160, epoch: 95, batch: 24, loss: 1.7457449436187744, acc: 28.125, f1: 8.75, r: 0.17772881852334574
06/02/2019 03:47:17 step: 3165, epoch: 95, batch: 29, loss: 1.8618730306625366, acc: 31.25, f1: 8.818011257035648, r: 0.12278283572825906
06/02/2019 03:47:17 *** evaluating ***
06/02/2019 03:47:17 step: 96, epoch: 95, acc: 52.13675213675214, f1: 14.477848101265822, r: 0.16094422909929976
06/02/2019 03:47:17 *** epoch: 97 ***
06/02/2019 03:47:17 *** training ***
06/02/2019 03:47:17 step: 3173, epoch: 96, batch: 4, loss: 1.5659170150756836, acc: 45.3125, f1: 11.430423509075194, r: 0.14764939784538242
06/02/2019 03:47:17 step: 3178, epoch: 96, batch: 9, loss: 1.708753228187561, acc: 28.125, f1: 8.34879406307978, r: 0.12914682362922145
06/02/2019 03:47:17 step: 3183, epoch: 96, batch: 14, loss: 1.6809732913970947, acc: 28.125, f1: 7.826384142173614, r: 0.16658561671551642
06/02/2019 03:47:18 step: 3188, epoch: 96, batch: 19, loss: 1.6421172618865967, acc: 39.0625, f1: 10.403185664509706, r: 0.14713722625317177
06/02/2019 03:47:18 step: 3193, epoch: 96, batch: 24, loss: 1.4615050554275513, acc: 50.0, f1: 13.036499406797402, r: 0.12898460065591427
06/02/2019 03:47:18 step: 3198, epoch: 96, batch: 29, loss: 1.4588009119033813, acc: 50.0, f1: 15.433455433455434, r: 0.15573475007390286
06/02/2019 03:47:18 *** evaluating ***
06/02/2019 03:47:18 step: 97, epoch: 96, acc: 53.84615384615385, f1: 15.57017543859649, r: 0.18053366011566463
06/02/2019 03:47:18 *** epoch: 98 ***
06/02/2019 03:47:18 *** training ***
06/02/2019 03:47:18 step: 3206, epoch: 97, batch: 4, loss: 1.719261646270752, acc: 34.375, f1: 11.296992481203008, r: 0.08299295164811579
06/02/2019 03:47:19 step: 3211, epoch: 97, batch: 9, loss: 1.6314396858215332, acc: 46.875, f1: 10.321969696969695, r: 0.1558520874197337
06/02/2019 03:47:19 step: 3216, epoch: 97, batch: 14, loss: 1.6189957857131958, acc: 45.3125, f1: 12.24563953488372, r: 0.08918680674299356
06/02/2019 03:47:19 step: 3221, epoch: 97, batch: 19, loss: 1.8013380765914917, acc: 32.8125, f1: 6.325301204819277, r: 0.052858950439353
06/02/2019 03:47:19 step: 3226, epoch: 97, batch: 24, loss: 1.6022473573684692, acc: 45.3125, f1: 7.880434782608696, r: 0.052576166700160354
06/02/2019 03:47:19 step: 3231, epoch: 97, batch: 29, loss: 1.7152633666992188, acc: 35.9375, f1: 10.08039579468151, r: 0.12701069699542622
06/02/2019 03:47:20 *** evaluating ***
06/02/2019 03:47:20 step: 98, epoch: 97, acc: 49.572649572649574, f1: 12.664920219099784, r: 0.15934929063105124
06/02/2019 03:47:20 *** epoch: 99 ***
06/02/2019 03:47:20 *** training ***
06/02/2019 03:47:20 step: 3239, epoch: 98, batch: 4, loss: 1.8830931186676025, acc: 31.25, f1: 8.917682926829269, r: 0.11373113067141652
06/02/2019 03:47:20 step: 3244, epoch: 98, batch: 9, loss: 1.6280269622802734, acc: 40.625, f1: 7.386363636363637, r: 0.06364617595824683
06/02/2019 03:47:20 step: 3249, epoch: 98, batch: 14, loss: 1.6758276224136353, acc: 43.75, f1: 7.692307692307692, r: 0.08567114033465277
06/02/2019 03:47:20 step: 3254, epoch: 98, batch: 19, loss: 1.8042337894439697, acc: 25.0, f1: 5.128205128205128, r: 0.07774860506786305
06/02/2019 03:47:21 step: 3259, epoch: 98, batch: 24, loss: 1.6731945276260376, acc: 43.75, f1: 11.04868913857678, r: 0.1185674205172795
06/02/2019 03:47:21 step: 3264, epoch: 98, batch: 29, loss: 1.4423091411590576, acc: 54.6875, f1: 18.559390048154093, r: 0.19391606179221657
06/02/2019 03:47:21 *** evaluating ***
06/02/2019 03:47:21 step: 99, epoch: 98, acc: 52.991452991452995, f1: 14.973375931842387, r: 0.17478368766438118
06/02/2019 03:47:21 *** epoch: 100 ***
06/02/2019 03:47:21 *** training ***
06/02/2019 03:47:21 step: 3272, epoch: 99, batch: 4, loss: 1.5061066150665283, acc: 50.0, f1: 20.744810744810742, r: 0.20157190713248274
06/02/2019 03:47:21 step: 3277, epoch: 99, batch: 9, loss: 1.478825330734253, acc: 51.5625, f1: 12.189126662810873, r: 0.16532165742552482
06/02/2019 03:47:22 step: 3282, epoch: 99, batch: 14, loss: 1.7357535362243652, acc: 28.125, f1: 7.652041496145426, r: 0.11218162203094512
06/02/2019 03:47:22 step: 3287, epoch: 99, batch: 19, loss: 1.8325458765029907, acc: 32.8125, f1: 6.325301204819277, r: 0.0791978891615567
06/02/2019 03:47:22 step: 3292, epoch: 99, batch: 24, loss: 1.5152918100357056, acc: 54.6875, f1: 13.776595744680852, r: 0.12044309342004723
06/02/2019 03:47:23 step: 3297, epoch: 99, batch: 29, loss: 1.7082796096801758, acc: 35.9375, f1: 13.930151185053145, r: 0.19058699838049376
06/02/2019 03:47:23 *** evaluating ***
06/02/2019 03:47:23 step: 100, epoch: 99, acc: 52.56410256410257, f1: 14.699173695988982, r: 0.16677658007259624
06/02/2019 03:47:23 *** epoch: 101 ***
06/02/2019 03:47:23 *** training ***
06/02/2019 03:47:23 step: 3305, epoch: 100, batch: 4, loss: 1.6906903982162476, acc: 37.5, f1: 7.792207792207792, r: 0.08845789816794132
06/02/2019 03:47:23 step: 3310, epoch: 100, batch: 9, loss: 1.7410552501678467, acc: 34.375, f1: 10.714285714285714, r: 0.20042722092843038
06/02/2019 03:47:23 step: 3315, epoch: 100, batch: 14, loss: 1.8720613718032837, acc: 34.375, f1: 10.718294051627385, r: 0.0589544484033303
06/02/2019 03:47:24 step: 3320, epoch: 100, batch: 19, loss: 1.6253423690795898, acc: 40.625, f1: 7.222222222222221, r: 0.04576673628661215
06/02/2019 03:47:24 step: 3325, epoch: 100, batch: 24, loss: 1.4965074062347412, acc: 51.5625, f1: 16.25216888374783, r: 0.18200489262161132
06/02/2019 03:47:24 step: 3330, epoch: 100, batch: 29, loss: 1.7800307273864746, acc: 31.25, f1: 9.542356377799416, r: 0.1935058244877535
06/02/2019 03:47:24 *** evaluating ***
06/02/2019 03:47:24 step: 101, epoch: 100, acc: 52.13675213675214, f1: 14.418351815612091, r: 0.16787292471633414
06/02/2019 03:47:24 *** epoch: 102 ***
06/02/2019 03:47:24 *** training ***
06/02/2019 03:47:24 step: 3338, epoch: 101, batch: 4, loss: 1.5994508266448975, acc: 37.5, f1: 10.676940011007154, r: 0.2227924983649155
06/02/2019 03:47:25 step: 3343, epoch: 101, batch: 9, loss: 1.5573487281799316, acc: 50.0, f1: 21.22153209109731, r: 0.10683492916516302
06/02/2019 03:47:25 step: 3348, epoch: 101, batch: 14, loss: 1.756348967552185, acc: 42.1875, f1: 7.670454545454546, r: 0.06561360751396257
06/02/2019 03:47:25 step: 3353, epoch: 101, batch: 19, loss: 1.5579088926315308, acc: 42.1875, f1: 18.02393684746626, r: 0.21013792844443419
06/02/2019 03:47:25 step: 3358, epoch: 101, batch: 24, loss: 1.6175075769424438, acc: 40.625, f1: 8.253968253968253, r: 0.13028198162627624
06/02/2019 03:47:25 step: 3363, epoch: 101, batch: 29, loss: 1.7826200723648071, acc: 35.9375, f1: 9.047619047619047, r: 0.15126889100604826
06/02/2019 03:47:26 *** evaluating ***
06/02/2019 03:47:26 step: 102, epoch: 101, acc: 49.572649572649574, f1: 12.712191358024691, r: 0.1638178891540929
06/02/2019 03:47:26 *** epoch: 103 ***
06/02/2019 03:47:26 *** training ***
06/02/2019 03:47:26 step: 3371, epoch: 102, batch: 4, loss: 1.6761400699615479, acc: 37.5, f1: 7.973421926910299, r: 0.17333673571931316
06/02/2019 03:47:26 step: 3376, epoch: 102, batch: 9, loss: 1.5572547912597656, acc: 45.3125, f1: 12.477260567148207, r: 0.13297209234298435
06/02/2019 03:47:26 step: 3381, epoch: 102, batch: 14, loss: 1.5285568237304688, acc: 46.875, f1: 18.49390919158361, r: 0.16159402948181628
06/02/2019 03:47:27 step: 3386, epoch: 102, batch: 19, loss: 1.5779491662979126, acc: 39.0625, f1: 10.416666666666666, r: 0.20971282605641242
06/02/2019 03:47:27 step: 3391, epoch: 102, batch: 24, loss: 1.6370582580566406, acc: 40.625, f1: 11.845238095238095, r: 0.07212133463835374
06/02/2019 03:47:27 step: 3396, epoch: 102, batch: 29, loss: 1.6866685152053833, acc: 37.5, f1: 9.345238095238095, r: 0.11070212773378606
06/02/2019 03:47:27 *** evaluating ***
06/02/2019 03:47:27 step: 103, epoch: 102, acc: 54.700854700854705, f1: 15.876462782000239, r: 0.18490455229680242
06/02/2019 03:47:27 *** epoch: 104 ***
06/02/2019 03:47:27 *** training ***
06/02/2019 03:47:27 step: 3404, epoch: 103, batch: 4, loss: 1.5434731245040894, acc: 50.0, f1: 15.799755799755802, r: 0.18598001995030658
06/02/2019 03:47:28 step: 3409, epoch: 103, batch: 9, loss: 1.8432706594467163, acc: 35.9375, f1: 7.6411960132890355, r: -0.023157353175825506
06/02/2019 03:47:28 step: 3414, epoch: 103, batch: 14, loss: 1.6689136028289795, acc: 40.625, f1: 9.19051878354204, r: 0.036944832114144247
06/02/2019 03:47:28 step: 3419, epoch: 103, batch: 19, loss: 1.6681959629058838, acc: 42.1875, f1: 7.417582417582418, r: 0.12206651428031408
06/02/2019 03:47:28 step: 3424, epoch: 103, batch: 24, loss: 1.7047953605651855, acc: 32.8125, f1: 7.142857142857142, r: 0.10456307952881373
06/02/2019 03:47:29 step: 3429, epoch: 103, batch: 29, loss: 1.5644489526748657, acc: 42.1875, f1: 13.456114152978262, r: 0.16183818960479351
06/02/2019 03:47:29 *** evaluating ***
06/02/2019 03:47:29 step: 104, epoch: 103, acc: 54.27350427350427, f1: 15.629058441558442, r: 0.1764823959643455
06/02/2019 03:47:29 *** epoch: 105 ***
06/02/2019 03:47:29 *** training ***
06/02/2019 03:47:29 step: 3437, epoch: 104, batch: 4, loss: 1.6090270280838013, acc: 42.1875, f1: 11.006191950464398, r: 0.18267801241540724
06/02/2019 03:47:29 step: 3442, epoch: 104, batch: 9, loss: 1.6964746713638306, acc: 39.0625, f1: 8.210180623973727, r: 0.1558410774671149
06/02/2019 03:47:30 step: 3447, epoch: 104, batch: 14, loss: 1.4283215999603271, acc: 51.5625, f1: 17.211360068502923, r: 0.22936462529133736
06/02/2019 03:47:30 step: 3452, epoch: 104, batch: 19, loss: 1.5930166244506836, acc: 48.4375, f1: 16.849529780564264, r: 0.14786993487732153
06/02/2019 03:47:30 step: 3457, epoch: 104, batch: 24, loss: 1.589432716369629, acc: 46.875, f1: 19.810924369747898, r: 0.14374230445244054
06/02/2019 03:47:30 step: 3462, epoch: 104, batch: 29, loss: 1.766582727432251, acc: 25.0, f1: 7.319770013268465, r: 0.11760542794469386
06/02/2019 03:47:30 *** evaluating ***
06/02/2019 03:47:30 step: 105, epoch: 104, acc: 51.28205128205128, f1: 13.835695561380906, r: 0.17828680298988672
06/02/2019 03:47:30 *** epoch: 106 ***
06/02/2019 03:47:30 *** training ***
06/02/2019 03:47:31 step: 3470, epoch: 105, batch: 4, loss: 1.5980416536331177, acc: 46.875, f1: 10.75268817204301, r: 0.1318421867688864
06/02/2019 03:47:31 step: 3475, epoch: 105, batch: 9, loss: 1.5863792896270752, acc: 42.1875, f1: 11.113595706618963, r: 0.17100206274828436
06/02/2019 03:47:31 step: 3480, epoch: 105, batch: 14, loss: 1.6151760816574097, acc: 42.1875, f1: 12.115171650055371, r: 0.11620146746666879
06/02/2019 03:47:31 step: 3485, epoch: 105, batch: 19, loss: 1.6037087440490723, acc: 48.4375, f1: 11.717495987158907, r: 0.16271829096893425
06/02/2019 03:47:31 step: 3490, epoch: 105, batch: 24, loss: 1.5692429542541504, acc: 45.3125, f1: 11.17216117216117, r: 0.15273616044127314
06/02/2019 03:47:32 step: 3495, epoch: 105, batch: 29, loss: 1.7092649936676025, acc: 39.0625, f1: 10.35437430786268, r: 0.10193345499072612
06/02/2019 03:47:32 *** evaluating ***
06/02/2019 03:47:32 step: 106, epoch: 105, acc: 50.427350427350426, f1: 13.323569070535173, r: 0.17112713281313038
06/02/2019 03:47:32 *** epoch: 107 ***
06/02/2019 03:47:32 *** training ***
06/02/2019 03:47:32 step: 3503, epoch: 106, batch: 4, loss: 1.6294784545898438, acc: 50.0, f1: 17.898671096345513, r: 0.24164722576742556
06/02/2019 03:47:33 step: 3508, epoch: 106, batch: 9, loss: 1.9002066850662231, acc: 34.375, f1: 9.943714821763603, r: 0.11112723869339143
06/02/2019 03:47:33 step: 3513, epoch: 106, batch: 14, loss: 1.6181188821792603, acc: 51.5625, f1: 12.087912087912088, r: 0.19667735307486722
06/02/2019 03:47:33 step: 3518, epoch: 106, batch: 19, loss: 1.6877423524856567, acc: 39.0625, f1: 9.249471458773783, r: 0.11236557149449447
06/02/2019 03:47:33 step: 3523, epoch: 106, batch: 24, loss: 1.6132780313491821, acc: 45.3125, f1: 10.192307692307692, r: 0.08098244706439547
06/02/2019 03:47:34 step: 3528, epoch: 106, batch: 29, loss: 1.882073998451233, acc: 31.25, f1: 6.024096385542169, r: 0.06461763115869233
06/02/2019 03:47:34 *** evaluating ***
06/02/2019 03:47:34 step: 107, epoch: 106, acc: 51.70940170940172, f1: 14.247529200359391, r: 0.16506982212849858
06/02/2019 03:47:34 *** epoch: 108 ***
06/02/2019 03:47:34 *** training ***
06/02/2019 03:47:34 step: 3536, epoch: 107, batch: 4, loss: 1.6083414554595947, acc: 40.625, f1: 13.541666666666666, r: 0.1584095288909985
06/02/2019 03:47:35 step: 3541, epoch: 107, batch: 9, loss: 1.5129834413528442, acc: 50.0, f1: 19.10570969961018, r: 0.17676297438232771
06/02/2019 03:47:35 step: 3546, epoch: 107, batch: 14, loss: 1.5680898427963257, acc: 42.1875, f1: 12.29181327703495, r: 0.1663072700680881
06/02/2019 03:47:35 step: 3551, epoch: 107, batch: 19, loss: 1.7232332229614258, acc: 40.625, f1: 13.180272108843536, r: 0.11236804194320094
06/02/2019 03:47:36 step: 3556, epoch: 107, batch: 24, loss: 1.5914320945739746, acc: 39.0625, f1: 13.84920634920635, r: 0.19584111718161618
06/02/2019 03:47:36 step: 3561, epoch: 107, batch: 29, loss: 1.6651402711868286, acc: 42.1875, f1: 8.667736757624397, r: 0.1502712789174747
06/02/2019 03:47:36 *** evaluating ***
06/02/2019 03:47:36 step: 108, epoch: 107, acc: 50.85470085470085, f1: 13.639705882352942, r: 0.1632160142386325
06/02/2019 03:47:36 *** epoch: 109 ***
06/02/2019 03:47:36 *** training ***
06/02/2019 03:47:36 step: 3569, epoch: 108, batch: 4, loss: 1.6233725547790527, acc: 42.1875, f1: 12.012987012987013, r: 0.16821484778552606
06/02/2019 03:47:37 step: 3574, epoch: 108, batch: 9, loss: 1.487877368927002, acc: 45.3125, f1: 10.170454545454547, r: 0.08492662266113846
06/02/2019 03:47:37 step: 3579, epoch: 108, batch: 14, loss: 1.5646541118621826, acc: 43.75, f1: 12.816926036020284, r: 0.09550028683408052
06/02/2019 03:47:37 step: 3584, epoch: 108, batch: 19, loss: 1.519202470779419, acc: 48.4375, f1: 17.289377289377285, r: 0.13064415672171026
06/02/2019 03:47:37 step: 3589, epoch: 108, batch: 24, loss: 1.7702958583831787, acc: 40.625, f1: 12.244897959183675, r: 0.07243584635145403
06/02/2019 03:47:38 step: 3594, epoch: 108, batch: 29, loss: 1.5097519159317017, acc: 46.875, f1: 11.30298273155416, r: 0.14044332867489728
06/02/2019 03:47:38 *** evaluating ***
06/02/2019 03:47:38 step: 109, epoch: 108, acc: 52.13675213675214, f1: 14.477848101265822, r: 0.17527413538037295
06/02/2019 03:47:38 *** epoch: 110 ***
06/02/2019 03:47:38 *** training ***
06/02/2019 03:47:38 step: 3602, epoch: 109, batch: 4, loss: 1.6044435501098633, acc: 40.625, f1: 13.590014064697609, r: 0.12301610394894434
06/02/2019 03:47:39 step: 3607, epoch: 109, batch: 9, loss: 1.4425543546676636, acc: 51.5625, f1: 18.21749678892536, r: 0.2396442028568459
06/02/2019 03:47:39 step: 3612, epoch: 109, batch: 14, loss: 1.5952996015548706, acc: 43.75, f1: 17.824074074074073, r: 0.16666932730949308
06/02/2019 03:47:39 step: 3617, epoch: 109, batch: 19, loss: 1.6534593105316162, acc: 37.5, f1: 10.231481481481481, r: 0.1749744640350982
06/02/2019 03:47:40 step: 3622, epoch: 109, batch: 24, loss: 1.627448320388794, acc: 37.5, f1: 13.852813852813853, r: 0.15858978557781656
06/02/2019 03:47:40 step: 3627, epoch: 109, batch: 29, loss: 1.5605367422103882, acc: 42.1875, f1: 12.415966386554622, r: 0.1328131942405151
06/02/2019 03:47:40 *** evaluating ***
06/02/2019 03:47:40 step: 110, epoch: 109, acc: 52.991452991452995, f1: 14.973375931842387, r: 0.15552614389725894
06/02/2019 03:47:40 *** epoch: 111 ***
06/02/2019 03:47:40 *** training ***
06/02/2019 03:47:40 step: 3635, epoch: 110, batch: 4, loss: 1.6775187253952026, acc: 34.375, f1: 9.35788949726232, r: 0.1473459759293363
06/02/2019 03:47:41 step: 3640, epoch: 110, batch: 9, loss: 1.6810259819030762, acc: 42.1875, f1: 8.47723704866562, r: 0.1293726937600456
06/02/2019 03:47:41 step: 3645, epoch: 110, batch: 14, loss: 1.6613473892211914, acc: 35.9375, f1: 8.39366515837104, r: 0.16029269199124418
06/02/2019 03:47:41 step: 3650, epoch: 110, batch: 19, loss: 1.5665972232818604, acc: 40.625, f1: 11.850649350649352, r: 0.09435553147425114
06/02/2019 03:47:42 step: 3655, epoch: 110, batch: 24, loss: 1.6732220649719238, acc: 40.625, f1: 12.28485370051635, r: 0.15529241425217807
06/02/2019 03:47:42 step: 3660, epoch: 110, batch: 29, loss: 1.5524250268936157, acc: 39.0625, f1: 21.27094740335158, r: 0.22742627041967392
06/02/2019 03:47:42 *** evaluating ***
06/02/2019 03:47:42 step: 111, epoch: 110, acc: 53.84615384615385, f1: 15.376264798656344, r: 0.19137286478227128
06/02/2019 03:47:42 *** epoch: 112 ***
06/02/2019 03:47:42 *** training ***
06/02/2019 03:47:42 step: 3668, epoch: 111, batch: 4, loss: 1.5566657781600952, acc: 43.75, f1: 13.165266106442578, r: 0.1588579713674049
06/02/2019 03:47:43 step: 3673, epoch: 111, batch: 9, loss: 1.8361879587173462, acc: 32.8125, f1: 10.180995475113122, r: 0.0969159799700725
06/02/2019 03:47:43 step: 3678, epoch: 111, batch: 14, loss: 1.581130027770996, acc: 37.5, f1: 11.292517006802722, r: 0.07969663910636547
06/02/2019 03:47:43 step: 3683, epoch: 111, batch: 19, loss: 1.6722326278686523, acc: 31.25, f1: 9.791666666666668, r: 0.11759070055182518
06/02/2019 03:47:44 step: 3688, epoch: 111, batch: 24, loss: 1.7223927974700928, acc: 39.0625, f1: 8.116883116883116, r: 0.08683338442471666
06/02/2019 03:47:44 step: 3693, epoch: 111, batch: 29, loss: 1.6833232641220093, acc: 32.8125, f1: 12.224231464737796, r: 0.2190178249424113
06/02/2019 03:47:44 *** evaluating ***
06/02/2019 03:47:44 step: 112, epoch: 111, acc: 50.0, f1: 12.998776585733108, r: 0.16675308246922876
06/02/2019 03:47:44 *** epoch: 113 ***
06/02/2019 03:47:44 *** training ***
06/02/2019 03:47:44 step: 3701, epoch: 112, batch: 4, loss: 1.621444821357727, acc: 48.4375, f1: 10.325091575091575, r: 0.07349097365519512
06/02/2019 03:47:45 step: 3706, epoch: 112, batch: 9, loss: 1.717941164970398, acc: 39.0625, f1: 11.148025101513474, r: 0.11402013074738399
06/02/2019 03:47:45 step: 3711, epoch: 112, batch: 14, loss: 1.6207174062728882, acc: 43.75, f1: 10.612244897959185, r: 0.029836130995202752
06/02/2019 03:47:45 step: 3716, epoch: 112, batch: 19, loss: 1.844218134880066, acc: 37.5, f1: 6.896551724137931, r: 0.10081016298100404
06/02/2019 03:47:46 step: 3721, epoch: 112, batch: 24, loss: 1.5370197296142578, acc: 45.3125, f1: 13.24049513704686, r: 0.15128660518938658
06/02/2019 03:47:46 step: 3726, epoch: 112, batch: 29, loss: 1.6728105545043945, acc: 34.375, f1: 11.07142857142857, r: 0.056280789140355066
06/02/2019 03:47:46 *** evaluating ***
06/02/2019 03:47:46 step: 113, epoch: 112, acc: 52.56410256410257, f1: 14.699173695988982, r: 0.18211229204230533
06/02/2019 03:47:46 *** epoch: 114 ***
06/02/2019 03:47:46 *** training ***
06/02/2019 03:47:46 step: 3734, epoch: 113, batch: 4, loss: 1.6663304567337036, acc: 39.0625, f1: 9.476817042606516, r: 0.06913498836945506
06/02/2019 03:47:47 step: 3739, epoch: 113, batch: 9, loss: 1.6466633081436157, acc: 39.0625, f1: 11.477623456790123, r: 0.19005668667564957
06/02/2019 03:47:47 step: 3744, epoch: 113, batch: 14, loss: 1.6422120332717896, acc: 40.625, f1: 13.899821109123433, r: 0.08213243153397368
06/02/2019 03:47:47 step: 3749, epoch: 113, batch: 19, loss: 1.6661956310272217, acc: 43.75, f1: 8.695652173913045, r: 0.15113361277725268
06/02/2019 03:47:48 step: 3754, epoch: 113, batch: 24, loss: 1.6245529651641846, acc: 34.375, f1: 7.308970099667775, r: 0.12749827703942745
06/02/2019 03:47:48 step: 3759, epoch: 113, batch: 29, loss: 1.5975916385650635, acc: 46.875, f1: 14.183569239749014, r: 0.2004088628191043
06/02/2019 03:47:48 *** evaluating ***
06/02/2019 03:47:48 step: 114, epoch: 113, acc: 52.56410256410257, f1: 14.76081756903675, r: 0.1815201272965096
06/02/2019 03:47:48 *** epoch: 115 ***
06/02/2019 03:47:48 *** training ***
06/02/2019 03:47:49 step: 3767, epoch: 114, batch: 4, loss: 1.745409369468689, acc: 32.8125, f1: 9.349593495934961, r: 0.12323919630142083
06/02/2019 03:47:49 step: 3772, epoch: 114, batch: 9, loss: 1.6422429084777832, acc: 32.8125, f1: 7.0588235294117645, r: 0.07504474729998195
06/02/2019 03:47:49 step: 3777, epoch: 114, batch: 14, loss: 1.5578781366348267, acc: 46.875, f1: 9.419152276295133, r: 0.10019558129729694
06/02/2019 03:47:50 step: 3782, epoch: 114, batch: 19, loss: 1.4702352285385132, acc: 45.3125, f1: 16.022408963585434, r: 0.12529958263148658
06/02/2019 03:47:50 step: 3787, epoch: 114, batch: 24, loss: 1.5361417531967163, acc: 39.0625, f1: 11.39455782312925, r: 0.20591484877020194
06/02/2019 03:47:50 step: 3792, epoch: 114, batch: 29, loss: 1.740410327911377, acc: 34.375, f1: 11.450617283950617, r: 0.14978281944680005
06/02/2019 03:47:51 *** evaluating ***
06/02/2019 03:47:51 step: 115, epoch: 114, acc: 56.41025641025641, f1: 16.892655367231637, r: 0.20341789482625514
06/02/2019 03:47:51 *** epoch: 116 ***
06/02/2019 03:47:51 *** training ***
06/02/2019 03:47:51 step: 3800, epoch: 115, batch: 4, loss: 1.7352163791656494, acc: 40.625, f1: 12.058823529411764, r: 0.05510993367944597
06/02/2019 03:47:51 step: 3805, epoch: 115, batch: 9, loss: 1.8027275800704956, acc: 32.8125, f1: 7.643427741466957, r: 0.04232426762856467
06/02/2019 03:47:52 step: 3810, epoch: 115, batch: 14, loss: 1.6295373439788818, acc: 37.5, f1: 8.32720588235294, r: 0.15790259327212217
06/02/2019 03:47:52 step: 3815, epoch: 115, batch: 19, loss: 1.524284839630127, acc: 50.0, f1: 12.467320261437909, r: 0.13761549612632662
06/02/2019 03:47:52 step: 3820, epoch: 115, batch: 24, loss: 1.5262655019760132, acc: 40.625, f1: 9.902597402597403, r: 0.05848535351501835
06/02/2019 03:47:52 step: 3825, epoch: 115, batch: 29, loss: 1.5687485933303833, acc: 42.1875, f1: 12.578505086245025, r: 0.06609726661241559
06/02/2019 03:47:53 *** evaluating ***
06/02/2019 03:47:53 step: 116, epoch: 115, acc: 52.56410256410257, f1: 14.76081756903675, r: 0.19623301966042575
06/02/2019 03:47:53 *** epoch: 117 ***
06/02/2019 03:47:53 *** training ***
06/02/2019 03:47:53 step: 3833, epoch: 116, batch: 4, loss: 1.598517656326294, acc: 46.875, f1: 11.111111111111112, r: 0.1434025648330858
06/02/2019 03:47:53 step: 3838, epoch: 116, batch: 9, loss: 1.574698805809021, acc: 39.0625, f1: 15.235712796688405, r: 0.23698292807323537
06/02/2019 03:47:54 step: 3843, epoch: 116, batch: 14, loss: 1.7487075328826904, acc: 37.5, f1: 6.976744186046512, r: 0.09794043529525584
06/02/2019 03:47:54 step: 3848, epoch: 116, batch: 19, loss: 1.7858539819717407, acc: 37.5, f1: 10.498338870431892, r: 0.11018013582508826
06/02/2019 03:47:54 step: 3853, epoch: 116, batch: 24, loss: 1.5220867395401, acc: 53.125, f1: 15.44740973312402, r: 0.07027971940434945
06/02/2019 03:47:55 step: 3858, epoch: 116, batch: 29, loss: 1.644713044166565, acc: 45.3125, f1: 12.292025243832471, r: 0.13998407040286065
06/02/2019 03:47:55 *** evaluating ***
06/02/2019 03:47:55 step: 117, epoch: 116, acc: 54.27350427350427, f1: 15.567820806691598, r: 0.19691766164361021
06/02/2019 03:47:55 *** epoch: 118 ***
06/02/2019 03:47:55 *** training ***
06/02/2019 03:47:55 step: 3866, epoch: 117, batch: 4, loss: 1.5244570970535278, acc: 53.125, f1: 15.440729483282675, r: 0.13240693003153292
06/02/2019 03:47:56 step: 3871, epoch: 117, batch: 9, loss: 1.544805884361267, acc: 45.3125, f1: 10.470085470085468, r: 0.17371121003470646
06/02/2019 03:47:56 step: 3876, epoch: 117, batch: 14, loss: 1.6940028667449951, acc: 40.625, f1: 10.392156862745097, r: 0.1285900173688935
06/02/2019 03:47:56 step: 3881, epoch: 117, batch: 19, loss: 1.5641405582427979, acc: 53.125, f1: 17.249866238630283, r: 0.18321911987492592
06/02/2019 03:47:57 step: 3886, epoch: 117, batch: 24, loss: 1.6597931385040283, acc: 40.625, f1: 10.562248995983936, r: 0.20903436271020606
06/02/2019 03:47:57 step: 3891, epoch: 117, batch: 29, loss: 1.6486740112304688, acc: 42.1875, f1: 10.808823529411764, r: 0.17114395944269123
06/02/2019 03:47:57 *** evaluating ***
06/02/2019 03:47:57 step: 118, epoch: 117, acc: 52.13675213675214, f1: 14.326145552560646, r: 0.17686081730685416
06/02/2019 03:47:57 *** epoch: 119 ***
06/02/2019 03:47:57 *** training ***
06/02/2019 03:47:57 step: 3899, epoch: 118, batch: 4, loss: 1.7013815641403198, acc: 40.625, f1: 17.189314750290357, r: 0.20006139573382054
06/02/2019 03:47:58 step: 3904, epoch: 118, batch: 9, loss: 1.5837386846542358, acc: 42.1875, f1: 21.02040816326531, r: 0.1729449737557946
06/02/2019 03:47:58 step: 3909, epoch: 118, batch: 14, loss: 1.5531717538833618, acc: 45.3125, f1: 13.181296615031554, r: 0.18101494148787517
06/02/2019 03:47:58 step: 3914, epoch: 118, batch: 19, loss: 1.5924599170684814, acc: 35.9375, f1: 10.800344234079173, r: 0.09113014594562649
06/02/2019 03:47:59 step: 3919, epoch: 118, batch: 24, loss: 1.5480302572250366, acc: 45.3125, f1: 13.42964151952916, r: 0.211836118662938
06/02/2019 03:47:59 step: 3924, epoch: 118, batch: 29, loss: 1.736690640449524, acc: 29.6875, f1: 7.410714285714287, r: 0.11044322295752611
06/02/2019 03:47:59 *** evaluating ***
06/02/2019 03:47:59 step: 119, epoch: 118, acc: 53.84615384615385, f1: 15.376264798656344, r: 0.19142842122207093
06/02/2019 03:47:59 *** epoch: 120 ***
06/02/2019 03:47:59 *** training ***
06/02/2019 03:47:59 step: 3932, epoch: 119, batch: 4, loss: 1.6280087232589722, acc: 42.1875, f1: 14.18967587034814, r: 0.1436457625414379
06/02/2019 03:48:00 step: 3937, epoch: 119, batch: 9, loss: 1.6086757183074951, acc: 43.75, f1: 11.904761904761905, r: 0.15892447174276728
06/02/2019 03:48:00 step: 3942, epoch: 119, batch: 14, loss: 1.6568058729171753, acc: 34.375, f1: 11.071428571428571, r: 0.12365590543998162
06/02/2019 03:48:01 step: 3947, epoch: 119, batch: 19, loss: 1.5992399454116821, acc: 39.0625, f1: 15.373244641537326, r: 0.20474057340363827
06/02/2019 03:48:01 step: 3952, epoch: 119, batch: 24, loss: 1.6277885437011719, acc: 34.375, f1: 8.928571428571429, r: 0.1342734704903401
06/02/2019 03:48:01 step: 3957, epoch: 119, batch: 29, loss: 1.6122608184814453, acc: 40.625, f1: 13.850174216027874, r: 0.16047970585817703
06/02/2019 03:48:01 *** evaluating ***
06/02/2019 03:48:02 step: 120, epoch: 119, acc: 53.84615384615385, f1: 15.427227801689783, r: 0.20443182372037672
06/02/2019 03:48:02 *** epoch: 121 ***
06/02/2019 03:48:02 *** training ***
06/02/2019 03:48:02 step: 3965, epoch: 120, batch: 4, loss: 1.5469855070114136, acc: 45.3125, f1: 20.525451559934318, r: 0.1273382500013685
06/02/2019 03:48:02 step: 3970, epoch: 120, batch: 9, loss: 1.5319583415985107, acc: 48.4375, f1: 11.27450980392157, r: 0.12153520308118311
06/02/2019 03:48:02 step: 3975, epoch: 120, batch: 14, loss: 1.410483717918396, acc: 45.3125, f1: 13.049866044421396, r: 0.17862303828897488
06/02/2019 03:48:03 step: 3980, epoch: 120, batch: 19, loss: 1.494634747505188, acc: 46.875, f1: 12.662337662337661, r: 0.14874626160511487
06/02/2019 03:48:03 step: 3985, epoch: 120, batch: 24, loss: 1.6595485210418701, acc: 34.375, f1: 11.72161172161172, r: 0.18097978013482643
06/02/2019 03:48:03 step: 3990, epoch: 120, batch: 29, loss: 1.569891333580017, acc: 48.4375, f1: 20.72345792249484, r: 0.14956255922715428
06/02/2019 03:48:03 *** evaluating ***
06/02/2019 03:48:03 step: 121, epoch: 120, acc: 53.84615384615385, f1: 15.502985760220486, r: 0.1857626720790327
06/02/2019 03:48:03 *** epoch: 122 ***
06/02/2019 03:48:03 *** training ***
06/02/2019 03:48:04 step: 3998, epoch: 121, batch: 4, loss: 1.53975510597229, acc: 48.4375, f1: 15.69278874340434, r: 0.12593241391061846
06/02/2019 03:48:04 step: 4003, epoch: 121, batch: 9, loss: 1.6359813213348389, acc: 40.625, f1: 10.1890756302521, r: 0.23413856813283043
06/02/2019 03:48:04 step: 4008, epoch: 121, batch: 14, loss: 1.6426581144332886, acc: 35.9375, f1: 6.609195402298851, r: 0.12557597974839504
06/02/2019 03:48:05 step: 4013, epoch: 121, batch: 19, loss: 1.6748273372650146, acc: 45.3125, f1: 11.505507955936354, r: 0.18048698335740732
06/02/2019 03:48:05 step: 4018, epoch: 121, batch: 24, loss: 1.5885709524154663, acc: 35.9375, f1: 16.680655811090595, r: 0.28254140001731315
06/02/2019 03:48:05 step: 4023, epoch: 121, batch: 29, loss: 1.4565517902374268, acc: 46.875, f1: 13.481377863262809, r: 0.19459232172659952
06/02/2019 03:48:05 *** evaluating ***
06/02/2019 03:48:05 step: 122, epoch: 121, acc: 52.56410256410257, f1: 14.750579255826631, r: 0.19180870962783877
06/02/2019 03:48:05 *** epoch: 123 ***
06/02/2019 03:48:05 *** training ***
06/02/2019 03:48:06 step: 4031, epoch: 122, batch: 4, loss: 1.4674752950668335, acc: 45.3125, f1: 18.866995073891626, r: 0.21064181010907776
06/02/2019 03:48:06 step: 4036, epoch: 122, batch: 9, loss: 1.561018705368042, acc: 46.875, f1: 13.00125313283208, r: 0.1220382129938101
06/02/2019 03:48:06 step: 4041, epoch: 122, batch: 14, loss: 1.5553479194641113, acc: 42.1875, f1: 14.145569620253164, r: 0.23513479069658602
06/02/2019 03:48:07 step: 4046, epoch: 122, batch: 19, loss: 1.7575528621673584, acc: 32.8125, f1: 14.111498257839722, r: 0.19379070443072482
06/02/2019 03:48:07 step: 4051, epoch: 122, batch: 24, loss: 1.6099624633789062, acc: 34.375, f1: 10.95238095238095, r: 0.10725028570575183
06/02/2019 03:48:07 step: 4056, epoch: 122, batch: 29, loss: 1.6858500242233276, acc: 32.8125, f1: 7.720588235294118, r: 0.1531217909825138
06/02/2019 03:48:08 *** evaluating ***
06/02/2019 03:48:08 step: 123, epoch: 122, acc: 55.12820512820513, f1: 16.14540865851291, r: 0.20685700446852312
06/02/2019 03:48:08 *** epoch: 124 ***
06/02/2019 03:48:08 *** training ***
06/02/2019 03:48:08 step: 4064, epoch: 123, batch: 4, loss: 1.57700777053833, acc: 35.9375, f1: 12.85714285714286, r: 0.21055416874542315
06/02/2019 03:48:08 step: 4069, epoch: 123, batch: 9, loss: 1.6770622730255127, acc: 40.625, f1: 12.925170068027212, r: 0.09698083127311442
06/02/2019 03:48:09 step: 4074, epoch: 123, batch: 14, loss: 1.5920166969299316, acc: 39.0625, f1: 11.790123456790125, r: 0.24751929314175655
06/02/2019 03:48:09 step: 4079, epoch: 123, batch: 19, loss: 1.594894289970398, acc: 46.875, f1: 19.717532467532468, r: 0.17396407231404082
06/02/2019 03:48:09 step: 4084, epoch: 123, batch: 24, loss: 1.707686185836792, acc: 42.1875, f1: 13.303634706894805, r: 0.17771475696100733
06/02/2019 03:48:09 step: 4089, epoch: 123, batch: 29, loss: 1.7497860193252563, acc: 34.375, f1: 8.99774543963927, r: 0.07937766161530653
06/02/2019 03:48:10 *** evaluating ***
06/02/2019 03:48:10 step: 124, epoch: 123, acc: 56.837606837606835, f1: 16.701266713581987, r: 0.23240291080549055
06/02/2019 03:48:10 *** epoch: 125 ***
06/02/2019 03:48:10 *** training ***
06/02/2019 03:48:10 step: 4097, epoch: 124, batch: 4, loss: 1.7873748540878296, acc: 31.25, f1: 10.095238095238097, r: 0.13645709173153228
06/02/2019 03:48:10 step: 4102, epoch: 124, batch: 9, loss: 1.5854017734527588, acc: 40.625, f1: 15.923406399596876, r: 0.13525459080084531
06/02/2019 03:48:10 step: 4107, epoch: 124, batch: 14, loss: 1.5535788536071777, acc: 42.1875, f1: 10.755336617405582, r: 0.1586498548225701
06/02/2019 03:48:11 step: 4112, epoch: 124, batch: 19, loss: 1.5886685848236084, acc: 37.5, f1: 11.459170013386881, r: 0.12966418957807338
06/02/2019 03:48:11 step: 4117, epoch: 124, batch: 24, loss: 1.5093016624450684, acc: 46.875, f1: 14.183569239749014, r: 0.19414660613466875
06/02/2019 03:48:11 step: 4122, epoch: 124, batch: 29, loss: 1.8516157865524292, acc: 39.0625, f1: 11.98364888123924, r: 0.05379775096315678
06/02/2019 03:48:12 *** evaluating ***
06/02/2019 03:48:12 step: 125, epoch: 124, acc: 55.55555555555556, f1: 16.290726817042607, r: 0.19953229521046664
06/02/2019 03:48:12 *** epoch: 126 ***
06/02/2019 03:48:12 *** training ***
06/02/2019 03:48:12 step: 4130, epoch: 125, batch: 4, loss: 1.6125797033309937, acc: 40.625, f1: 11.699695121951217, r: 0.15657229090472097
06/02/2019 03:48:12 step: 4135, epoch: 125, batch: 9, loss: 1.7179131507873535, acc: 32.8125, f1: 8.397212543554007, r: 0.14300257992036952
06/02/2019 03:48:13 step: 4140, epoch: 125, batch: 14, loss: 1.4796544313430786, acc: 43.75, f1: 11.812476928755999, r: 0.16660305580855322
06/02/2019 03:48:13 step: 4145, epoch: 125, batch: 19, loss: 1.6342933177947998, acc: 40.625, f1: 12.829131652661063, r: 0.11097361102912916
06/02/2019 03:48:13 step: 4150, epoch: 125, batch: 24, loss: 1.6207973957061768, acc: 35.9375, f1: 8.373983739837398, r: 0.2191357950115765
06/02/2019 03:48:14 step: 4155, epoch: 125, batch: 29, loss: 1.6176080703735352, acc: 34.375, f1: 11.363636363636365, r: 0.1363276716446987
06/02/2019 03:48:14 *** evaluating ***
06/02/2019 03:48:14 step: 126, epoch: 125, acc: 53.84615384615385, f1: 15.502985760220486, r: 0.19504209665667618
06/02/2019 03:48:14 *** epoch: 127 ***
06/02/2019 03:48:14 *** training ***
06/02/2019 03:48:14 step: 4163, epoch: 126, batch: 4, loss: 1.5683889389038086, acc: 45.3125, f1: 15.016943106830746, r: 0.18874549807044771
06/02/2019 03:48:15 step: 4168, epoch: 126, batch: 9, loss: 1.5437698364257812, acc: 43.75, f1: 14.365136298421808, r: 0.20239928765434254
06/02/2019 03:48:15 step: 4173, epoch: 126, batch: 14, loss: 1.4679502248764038, acc: 39.0625, f1: 13.164383561643836, r: 0.2531925012303196
06/02/2019 03:48:15 step: 4178, epoch: 126, batch: 19, loss: 1.7267909049987793, acc: 35.9375, f1: 10.408653846153847, r: 0.22602506961838448
06/02/2019 03:48:16 step: 4183, epoch: 126, batch: 24, loss: 1.848162055015564, acc: 32.8125, f1: 8.18089430894309, r: 0.12067606091289418
06/02/2019 03:48:16 step: 4188, epoch: 126, batch: 29, loss: 1.662768840789795, acc: 37.5, f1: 15.0, r: 0.15510838099900423
06/02/2019 03:48:16 *** evaluating ***
06/02/2019 03:48:16 step: 127, epoch: 126, acc: 55.55555555555556, f1: 16.393596632971327, r: 0.1932449513315757
06/02/2019 03:48:16 *** epoch: 128 ***
06/02/2019 03:48:16 *** training ***
06/02/2019 03:48:16 step: 4196, epoch: 127, batch: 4, loss: 1.7358736991882324, acc: 39.0625, f1: 10.068027210884354, r: 0.1168648374572259
06/02/2019 03:48:17 step: 4201, epoch: 127, batch: 9, loss: 1.6545461416244507, acc: 39.0625, f1: 8.210180623973727, r: 0.042994804612383586
06/02/2019 03:48:17 step: 4206, epoch: 127, batch: 14, loss: 1.4917582273483276, acc: 53.125, f1: 11.923963133640552, r: 0.12559543927062167
06/02/2019 03:48:17 step: 4211, epoch: 127, batch: 19, loss: 1.5650684833526611, acc: 34.375, f1: 11.73922460472168, r: 0.19900589111706357
06/02/2019 03:48:18 step: 4216, epoch: 127, batch: 24, loss: 1.7183713912963867, acc: 40.625, f1: 7.303370786516854, r: 0.14227401719203056
06/02/2019 03:48:18 step: 4221, epoch: 127, batch: 29, loss: 1.6017454862594604, acc: 39.0625, f1: 9.965277777777777, r: 0.1474215778392357
06/02/2019 03:48:18 *** evaluating ***
06/02/2019 03:48:19 step: 128, epoch: 127, acc: 55.12820512820513, f1: 16.208489974937347, r: 0.2062917326576572
06/02/2019 03:48:19 *** epoch: 129 ***
06/02/2019 03:48:19 *** training ***
06/02/2019 03:48:19 step: 4229, epoch: 128, batch: 4, loss: 1.461280107498169, acc: 50.0, f1: 11.666666666666666, r: 0.16189737785895172
06/02/2019 03:48:19 step: 4234, epoch: 128, batch: 9, loss: 1.6342484951019287, acc: 39.0625, f1: 11.286764705882351, r: 0.25143134117611865
06/02/2019 03:48:19 step: 4239, epoch: 128, batch: 14, loss: 1.615727424621582, acc: 37.5, f1: 10.119047619047619, r: 0.19453734564033165
06/02/2019 03:48:20 step: 4244, epoch: 128, batch: 19, loss: 1.4234782457351685, acc: 50.0, f1: 13.500784929356357, r: 0.12083142135018629
06/02/2019 03:48:20 step: 4249, epoch: 128, batch: 24, loss: 1.6650751829147339, acc: 37.5, f1: 13.214285714285715, r: 0.1364714522798357
06/02/2019 03:48:20 step: 4254, epoch: 128, batch: 29, loss: 1.7765952348709106, acc: 34.375, f1: 9.357889497262319, r: 0.10022421408196187
06/02/2019 03:48:21 *** evaluating ***
06/02/2019 03:48:21 step: 129, epoch: 128, acc: 55.12820512820513, f1: 16.208489974937347, r: 0.20509924546368344
06/02/2019 03:48:21 *** epoch: 130 ***
06/02/2019 03:48:21 *** training ***
06/02/2019 03:48:21 step: 4262, epoch: 129, batch: 4, loss: 1.5564513206481934, acc: 40.625, f1: 14.346661741619723, r: 0.22841871596828894
06/02/2019 03:48:21 step: 4267, epoch: 129, batch: 9, loss: 1.764729619026184, acc: 35.9375, f1: 12.098344693281401, r: 0.18410313254682775
06/02/2019 03:48:21 step: 4272, epoch: 129, batch: 14, loss: 1.5361639261245728, acc: 45.3125, f1: 13.32282913165266, r: 0.1792398908198296
06/02/2019 03:48:22 step: 4277, epoch: 129, batch: 19, loss: 1.5538592338562012, acc: 43.75, f1: 20.476190476190474, r: 0.21574704615443435
06/02/2019 03:48:22 step: 4282, epoch: 129, batch: 24, loss: 1.5839991569519043, acc: 42.1875, f1: 8.948863636363637, r: 0.16597543653300345
06/02/2019 03:48:22 step: 4287, epoch: 129, batch: 29, loss: 1.8459755182266235, acc: 31.25, f1: 9.791666666666668, r: 0.11072725400489614
06/02/2019 03:48:22 *** evaluating ***
06/02/2019 03:48:23 step: 130, epoch: 129, acc: 54.700854700854705, f1: 15.910870927318296, r: 0.20936034202564596
06/02/2019 03:48:23 *** epoch: 131 ***
06/02/2019 03:48:23 *** training ***
06/02/2019 03:48:23 step: 4295, epoch: 130, batch: 4, loss: 1.4745292663574219, acc: 54.6875, f1: 21.161695447409734, r: 0.16693355759124812
06/02/2019 03:48:23 step: 4300, epoch: 130, batch: 9, loss: 1.5487606525421143, acc: 48.4375, f1: 12.222222222222223, r: 0.1678595173986606
06/02/2019 03:48:24 step: 4305, epoch: 130, batch: 14, loss: 1.58042573928833, acc: 43.75, f1: 12.44047619047619, r: 0.1969601691559837
06/02/2019 03:48:24 step: 4310, epoch: 130, batch: 19, loss: 1.5497134923934937, acc: 45.3125, f1: 7.967032967032966, r: 0.14486499171872186
06/02/2019 03:48:24 step: 4315, epoch: 130, batch: 24, loss: 1.6112024784088135, acc: 45.3125, f1: 12.74936941068562, r: 0.14212105038678746
06/02/2019 03:48:25 step: 4320, epoch: 130, batch: 29, loss: 1.620964765548706, acc: 34.375, f1: 14.412698412698413, r: 0.25760916487632973
06/02/2019 03:48:25 *** evaluating ***
06/02/2019 03:48:25 step: 131, epoch: 130, acc: 56.41025641025641, f1: 16.890380313199106, r: 0.2205046106092131
06/02/2019 03:48:25 *** epoch: 132 ***
06/02/2019 03:48:25 *** training ***
06/02/2019 03:48:25 step: 4328, epoch: 131, batch: 4, loss: 1.6359957456588745, acc: 32.8125, f1: 8.035714285714285, r: 0.16878968787001672
06/02/2019 03:48:25 step: 4333, epoch: 131, batch: 9, loss: 1.5663782358169556, acc: 40.625, f1: 10.1890756302521, r: 0.1979677125275952
06/02/2019 03:48:26 step: 4338, epoch: 131, batch: 14, loss: 1.5103307962417603, acc: 50.0, f1: 16.403502207588115, r: 0.14210256950834374
06/02/2019 03:48:26 step: 4343, epoch: 131, batch: 19, loss: 1.598271369934082, acc: 37.5, f1: 13.492063492063492, r: 0.15415493594228336
06/02/2019 03:48:26 step: 4348, epoch: 131, batch: 24, loss: 1.4926996231079102, acc: 50.0, f1: 12.312734082397004, r: 0.20894399334318017
06/02/2019 03:48:27 step: 4353, epoch: 131, batch: 29, loss: 1.4756532907485962, acc: 40.625, f1: 12.977632805219013, r: 0.16669509840078695
06/02/2019 03:48:27 *** evaluating ***
06/02/2019 03:48:27 step: 132, epoch: 131, acc: 54.700854700854705, f1: 15.8138051968755, r: 0.2011233364796728
06/02/2019 03:48:27 *** epoch: 133 ***
06/02/2019 03:48:27 *** training ***
06/02/2019 03:48:27 step: 4361, epoch: 132, batch: 4, loss: 1.643890380859375, acc: 35.9375, f1: 10.800344234079173, r: 0.15017646726802766
06/02/2019 03:48:27 step: 4366, epoch: 132, batch: 9, loss: 1.495220422744751, acc: 45.3125, f1: 13.528138528138529, r: 0.16605730601058102
06/02/2019 03:48:28 step: 4371, epoch: 132, batch: 14, loss: 1.548941731452942, acc: 45.3125, f1: 11.330049261083744, r: 0.2330136619409525
06/02/2019 03:48:28 step: 4376, epoch: 132, batch: 19, loss: 1.5390150547027588, acc: 43.75, f1: 13.877551020408163, r: 0.11543391070855277
06/02/2019 03:48:28 step: 4381, epoch: 132, batch: 24, loss: 1.5069066286087036, acc: 45.3125, f1: 15.837262091845647, r: 0.15500714905490443
06/02/2019 03:48:29 step: 4386, epoch: 132, batch: 29, loss: 1.5439155101776123, acc: 42.1875, f1: 13.453947368421051, r: 0.1554901220853554
06/02/2019 03:48:29 *** evaluating ***
06/02/2019 03:48:29 step: 133, epoch: 132, acc: 54.700854700854705, f1: 15.910870927318296, r: 0.20709005974468866
06/02/2019 03:48:29 *** epoch: 134 ***
06/02/2019 03:48:29 *** training ***
06/02/2019 03:48:29 step: 4394, epoch: 133, batch: 4, loss: 1.5924835205078125, acc: 40.625, f1: 12.148859543817526, r: 0.1406403961426093
06/02/2019 03:48:29 step: 4399, epoch: 133, batch: 9, loss: 1.5149060487747192, acc: 40.625, f1: 12.829131652661063, r: 0.19320525117365014
06/02/2019 03:48:30 step: 4404, epoch: 133, batch: 14, loss: 1.5513098239898682, acc: 48.4375, f1: 13.522588522588523, r: 0.16902937876902607
06/02/2019 03:48:30 step: 4409, epoch: 133, batch: 19, loss: 1.4753193855285645, acc: 45.3125, f1: 11.51660839160839, r: 0.15698661168397532
06/02/2019 03:48:30 step: 4414, epoch: 133, batch: 24, loss: 1.6124440431594849, acc: 39.0625, f1: 12.445887445887447, r: 0.2636554969327888
06/02/2019 03:48:31 step: 4419, epoch: 133, batch: 29, loss: 1.931327223777771, acc: 28.125, f1: 9.04095904095904, r: 0.07128740902227335
06/02/2019 03:48:31 *** evaluating ***
06/02/2019 03:48:31 step: 134, epoch: 133, acc: 52.991452991452995, f1: 14.973375931842387, r: 0.19536758615424363
06/02/2019 03:48:31 *** epoch: 135 ***
06/02/2019 03:48:31 *** training ***
06/02/2019 03:48:31 step: 4427, epoch: 134, batch: 4, loss: 1.6616426706314087, acc: 37.5, f1: 6.8181818181818175, r: 0.12105575356960899
06/02/2019 03:48:31 step: 4432, epoch: 134, batch: 9, loss: 1.6360514163970947, acc: 39.0625, f1: 11.156186612576064, r: 0.1178576290301339
06/02/2019 03:48:32 step: 4437, epoch: 134, batch: 14, loss: 1.4740837812423706, acc: 43.75, f1: 12.348111658456485, r: 0.23851656530421367
06/02/2019 03:48:32 step: 4442, epoch: 134, batch: 19, loss: 1.598181962966919, acc: 35.9375, f1: 12.602212602212601, r: 0.25767283051597906
06/02/2019 03:48:32 step: 4447, epoch: 134, batch: 24, loss: 1.6053595542907715, acc: 43.75, f1: 14.885954381752702, r: 0.13983739040372983
06/02/2019 03:48:33 step: 4452, epoch: 134, batch: 29, loss: 1.7742037773132324, acc: 32.8125, f1: 9.222560975609756, r: 0.09412824002343573
06/02/2019 03:48:33 *** evaluating ***
06/02/2019 03:48:33 step: 135, epoch: 134, acc: 53.41880341880342, f1: 15.17831043554516, r: 0.19899217195847133
06/02/2019 03:48:33 *** epoch: 136 ***
06/02/2019 03:48:33 *** training ***
06/02/2019 03:48:33 step: 4460, epoch: 135, batch: 4, loss: 1.6931127309799194, acc: 34.375, f1: 8.675166297117515, r: 0.12749949207233102
06/02/2019 03:48:34 step: 4465, epoch: 135, batch: 9, loss: 1.5760291814804077, acc: 46.875, f1: 11.8006993006993, r: 0.20700542864361104
06/02/2019 03:48:34 step: 4470, epoch: 135, batch: 14, loss: 1.6653459072113037, acc: 40.625, f1: 11.384783798576901, r: 0.1336862379335663
06/02/2019 03:48:34 step: 4475, epoch: 135, batch: 19, loss: 1.6056971549987793, acc: 40.625, f1: 13.37108013937282, r: 0.18456463920779786
06/02/2019 03:48:34 step: 4480, epoch: 135, batch: 24, loss: 1.612458348274231, acc: 37.5, f1: 16.089743589743588, r: 0.18828847009148625
06/02/2019 03:48:35 step: 4485, epoch: 135, batch: 29, loss: 1.4669005870819092, acc: 46.875, f1: 13.303769401330376, r: 0.10987254745390233
06/02/2019 03:48:35 *** evaluating ***
06/02/2019 03:48:35 step: 136, epoch: 135, acc: 53.84615384615385, f1: 16.01057213930348, r: 0.21682336495171287
06/02/2019 03:48:35 *** epoch: 137 ***
06/02/2019 03:48:35 *** training ***
06/02/2019 03:48:35 step: 4493, epoch: 136, batch: 4, loss: 1.7975250482559204, acc: 34.375, f1: 10.856331168831169, r: 0.222734101291876
06/02/2019 03:48:35 step: 4498, epoch: 136, batch: 9, loss: 1.5588815212249756, acc: 40.625, f1: 12.252964426877469, r: 0.18526665141550888
06/02/2019 03:48:36 step: 4503, epoch: 136, batch: 14, loss: 1.5996026992797852, acc: 42.1875, f1: 11.176284083703234, r: 0.17427816099363933
06/02/2019 03:48:36 step: 4508, epoch: 136, batch: 19, loss: 1.5889809131622314, acc: 43.75, f1: 10.957792207792208, r: 0.16130769343461054
06/02/2019 03:48:36 step: 4513, epoch: 136, batch: 24, loss: 1.6128532886505127, acc: 42.1875, f1: 14.65986394557823, r: 0.16836195936052517
06/02/2019 03:48:37 step: 4518, epoch: 136, batch: 29, loss: 1.5541256666183472, acc: 46.875, f1: 11.486132914704344, r: 0.09855220553357373
06/02/2019 03:48:37 *** evaluating ***
06/02/2019 03:48:37 step: 137, epoch: 136, acc: 53.84615384615385, f1: 15.502985760220486, r: 0.20315841597878365
06/02/2019 03:48:37 *** epoch: 138 ***
06/02/2019 03:48:37 *** training ***
06/02/2019 03:48:37 step: 4526, epoch: 137, batch: 4, loss: 1.548535943031311, acc: 39.0625, f1: 12.363834422657952, r: 0.2007082070540536
06/02/2019 03:48:37 step: 4531, epoch: 137, batch: 9, loss: 1.6519135236740112, acc: 40.625, f1: 13.456790123456791, r: 0.1983258179224358
06/02/2019 03:48:38 step: 4536, epoch: 137, batch: 14, loss: 1.5966525077819824, acc: 50.0, f1: 11.11111111111111, r: 0.1478168456372886
06/02/2019 03:48:38 step: 4541, epoch: 137, batch: 19, loss: 1.516892671585083, acc: 42.1875, f1: 11.480251015134737, r: 0.17684046056811628
06/02/2019 03:48:38 step: 4546, epoch: 137, batch: 24, loss: 1.4873402118682861, acc: 48.4375, f1: 16.676003734827265, r: 0.15822660193636964
06/02/2019 03:48:39 step: 4551, epoch: 137, batch: 29, loss: 1.6850782632827759, acc: 37.5, f1: 9.345238095238095, r: 0.17190651618677086
06/02/2019 03:48:39 *** evaluating ***
06/02/2019 03:48:39 step: 138, epoch: 137, acc: 52.991452991452995, f1: 14.840182648401825, r: 0.2081536328695884
06/02/2019 03:48:39 *** epoch: 139 ***
06/02/2019 03:48:39 *** training ***
06/02/2019 03:48:39 step: 4559, epoch: 138, batch: 4, loss: 1.5546023845672607, acc: 43.75, f1: 21.400885935769658, r: 0.21535035544329462
06/02/2019 03:48:40 step: 4564, epoch: 138, batch: 9, loss: 1.6432181596755981, acc: 46.875, f1: 14.920634920634923, r: 0.24817141849685545
06/02/2019 03:48:40 step: 4569, epoch: 138, batch: 14, loss: 1.769895076751709, acc: 26.5625, f1: 6.757305194805194, r: 0.15801378909293787
06/02/2019 03:48:40 step: 4574, epoch: 138, batch: 19, loss: 1.5294395685195923, acc: 48.4375, f1: 13.988095238095239, r: 0.19670241614241424
06/02/2019 03:48:41 step: 4579, epoch: 138, batch: 24, loss: 1.6226590871810913, acc: 45.3125, f1: 11.92528735632184, r: 0.11233232217289238
06/02/2019 03:48:41 step: 4584, epoch: 138, batch: 29, loss: 1.5705232620239258, acc: 46.875, f1: 14.224806201550388, r: 0.18184219943836666
06/02/2019 03:48:41 *** evaluating ***
06/02/2019 03:48:41 step: 139, epoch: 138, acc: 54.700854700854705, f1: 15.94155844155844, r: 0.20333238990248023
06/02/2019 03:48:41 *** epoch: 140 ***
06/02/2019 03:48:41 *** training ***
06/02/2019 03:48:42 step: 4592, epoch: 139, batch: 4, loss: 1.6381064653396606, acc: 35.9375, f1: 12.585034013605442, r: 0.14937214074263236
06/02/2019 03:48:42 step: 4597, epoch: 139, batch: 9, loss: 1.831411600112915, acc: 29.6875, f1: 9.674922600619196, r: 0.1211570371397705
06/02/2019 03:48:42 step: 4602, epoch: 139, batch: 14, loss: 1.387140154838562, acc: 51.5625, f1: 18.041915561405286, r: 0.22835889549658417
06/02/2019 03:48:43 step: 4607, epoch: 139, batch: 19, loss: 1.6434818506240845, acc: 43.75, f1: 12.44047619047619, r: 0.19677408077813052
06/02/2019 03:48:43 step: 4612, epoch: 139, batch: 24, loss: 1.6214447021484375, acc: 42.1875, f1: 10.892857142857142, r: 0.13724689116635552
06/02/2019 03:48:43 step: 4617, epoch: 139, batch: 29, loss: 1.5080623626708984, acc: 45.3125, f1: 13.80813953488372, r: 0.17923604878109298
06/02/2019 03:48:43 *** evaluating ***
06/02/2019 03:48:44 step: 140, epoch: 139, acc: 56.41025641025641, f1: 16.890380313199106, r: 0.21749226609909997
06/02/2019 03:48:44 *** epoch: 141 ***
06/02/2019 03:48:44 *** training ***
06/02/2019 03:48:44 step: 4625, epoch: 140, batch: 4, loss: 1.3790351152420044, acc: 56.25, f1: 18.37535014005602, r: 0.1947938703004817
06/02/2019 03:48:44 step: 4630, epoch: 140, batch: 9, loss: 1.6338539123535156, acc: 42.1875, f1: 14.8355638551717, r: 0.15217050366036364
06/02/2019 03:48:45 step: 4635, epoch: 140, batch: 14, loss: 1.541721224784851, acc: 51.5625, f1: 12.795031055900624, r: 0.11156627588435339
06/02/2019 03:48:45 step: 4640, epoch: 140, batch: 19, loss: 1.5697523355484009, acc: 46.875, f1: 15.800865800865799, r: 0.2087030127746204
06/02/2019 03:48:45 step: 4645, epoch: 140, batch: 24, loss: 1.716060757637024, acc: 35.9375, f1: 17.261219792865365, r: 0.19238599975960508
06/02/2019 03:48:46 step: 4650, epoch: 140, batch: 29, loss: 1.511911153793335, acc: 43.75, f1: 11.04868913857678, r: 0.16568464543937814
06/02/2019 03:48:46 *** evaluating ***
06/02/2019 03:48:46 step: 141, epoch: 140, acc: 55.55555555555556, f1: 16.37532727552749, r: 0.20492565970042284
06/02/2019 03:48:46 *** epoch: 142 ***
06/02/2019 03:48:46 *** training ***
06/02/2019 03:48:46 step: 4658, epoch: 141, batch: 4, loss: 1.6032979488372803, acc: 40.625, f1: 14.579831932773107, r: 0.23510014157519205
06/02/2019 03:48:47 step: 4663, epoch: 141, batch: 9, loss: 1.7426809072494507, acc: 35.9375, f1: 6.686046511627906, r: 0.08204916039770863
06/02/2019 03:48:47 step: 4668, epoch: 141, batch: 14, loss: 1.4450863599777222, acc: 51.5625, f1: 15.64625850340136, r: 0.20365817269905465
06/02/2019 03:48:47 step: 4673, epoch: 141, batch: 19, loss: 1.37014901638031, acc: 53.125, f1: 16.462948815889995, r: 0.1679456756095207
06/02/2019 03:48:48 step: 4678, epoch: 141, batch: 24, loss: 1.6931077241897583, acc: 34.375, f1: 6.707317073170731, r: 0.12127274448946959
06/02/2019 03:48:48 step: 4683, epoch: 141, batch: 29, loss: 1.5132100582122803, acc: 40.625, f1: 14.206349206349206, r: 0.1672146755297599
06/02/2019 03:48:48 *** evaluating ***
06/02/2019 03:48:48 step: 142, epoch: 141, acc: 53.84615384615385, f1: 15.486387581935897, r: 0.19920944730536363
06/02/2019 03:48:48 *** epoch: 143 ***
06/02/2019 03:48:48 *** training ***
06/02/2019 03:48:49 step: 4691, epoch: 142, batch: 4, loss: 1.5688592195510864, acc: 42.1875, f1: 12.733100233100233, r: 0.1449934035824285
06/02/2019 03:48:49 step: 4696, epoch: 142, batch: 9, loss: 1.723554015159607, acc: 34.375, f1: 8.333333333333334, r: 0.11145759084110285
06/02/2019 03:48:49 step: 4701, epoch: 142, batch: 14, loss: 1.5720922946929932, acc: 42.1875, f1: 11.030061892130858, r: 0.18987051758563256
06/02/2019 03:48:49 step: 4706, epoch: 142, batch: 19, loss: 1.5252355337142944, acc: 40.625, f1: 12.5, r: 0.1761932053732814
06/02/2019 03:48:50 step: 4711, epoch: 142, batch: 24, loss: 1.6255240440368652, acc: 32.8125, f1: 9.082483781278961, r: 0.15629172949493764
06/02/2019 03:48:50 step: 4716, epoch: 142, batch: 29, loss: 1.6293623447418213, acc: 43.75, f1: 13.111268603827073, r: 0.16628639523687017
06/02/2019 03:48:50 *** evaluating ***
06/02/2019 03:48:50 step: 143, epoch: 142, acc: 54.700854700854705, f1: 15.8138051968755, r: 0.20326902690377927
06/02/2019 03:48:50 *** epoch: 144 ***
06/02/2019 03:48:50 *** training ***
06/02/2019 03:48:51 step: 4724, epoch: 143, batch: 4, loss: 1.5512170791625977, acc: 46.875, f1: 14.183569239749016, r: 0.16215367183298413
06/02/2019 03:48:51 step: 4729, epoch: 143, batch: 9, loss: 1.574587345123291, acc: 46.875, f1: 14.965123652504756, r: 0.24023166560182938
06/02/2019 03:48:51 step: 4734, epoch: 143, batch: 14, loss: 1.5337181091308594, acc: 40.625, f1: 9.19051878354204, r: 0.15111553177799014
06/02/2019 03:48:52 step: 4739, epoch: 143, batch: 19, loss: 1.6210981607437134, acc: 37.5, f1: 12.100737100737101, r: 0.1278760640609648
06/02/2019 03:48:52 step: 4744, epoch: 143, batch: 24, loss: 1.7542487382888794, acc: 31.25, f1: 7.355182926829268, r: 0.0853828300622502
06/02/2019 03:48:52 step: 4749, epoch: 143, batch: 29, loss: 1.5185199975967407, acc: 45.3125, f1: 13.567073170731708, r: 0.1298743162401873
06/02/2019 03:48:53 *** evaluating ***
06/02/2019 03:48:53 step: 144, epoch: 143, acc: 53.41880341880342, f1: 15.241228070175438, r: 0.2028369633431006
06/02/2019 03:48:53 *** epoch: 145 ***
06/02/2019 03:48:53 *** training ***
06/02/2019 03:48:53 step: 4757, epoch: 144, batch: 4, loss: 1.3799326419830322, acc: 54.6875, f1: 20.386904761904763, r: 0.24711051177486884
06/02/2019 03:48:53 step: 4762, epoch: 144, batch: 9, loss: 1.6493070125579834, acc: 34.375, f1: 9.269731989181215, r: 0.17313821565483475
06/02/2019 03:48:53 step: 4767, epoch: 144, batch: 14, loss: 1.595837116241455, acc: 43.75, f1: 12.016718913270637, r: 0.11495383496138958
06/02/2019 03:48:54 step: 4772, epoch: 144, batch: 19, loss: 1.5548630952835083, acc: 42.1875, f1: 12.899005756148613, r: 0.22631827699254425
06/02/2019 03:48:54 step: 4777, epoch: 144, batch: 24, loss: 1.6166582107543945, acc: 40.625, f1: 12.770562770562771, r: 0.18989406244613172
06/02/2019 03:48:54 step: 4782, epoch: 144, batch: 29, loss: 1.7110238075256348, acc: 31.25, f1: 13.807349423787782, r: 0.23781392579824462
06/02/2019 03:48:54 *** evaluating ***
06/02/2019 03:48:55 step: 145, epoch: 144, acc: 54.700854700854705, f1: 15.876462782000239, r: 0.21244463475705905
06/02/2019 03:48:55 *** epoch: 146 ***
06/02/2019 03:48:55 *** training ***
06/02/2019 03:48:55 step: 4790, epoch: 145, batch: 4, loss: 1.4804080724716187, acc: 46.875, f1: 14.48051948051948, r: 0.10187965704549069
06/02/2019 03:48:55 step: 4795, epoch: 145, batch: 9, loss: 1.6615253686904907, acc: 35.9375, f1: 11.325301204819278, r: 0.15997284653273672
06/02/2019 03:48:55 step: 4800, epoch: 145, batch: 14, loss: 1.580524206161499, acc: 34.375, f1: 14.212454212454212, r: 0.21308254056425924
06/02/2019 03:48:56 step: 4805, epoch: 145, batch: 19, loss: 1.5387707948684692, acc: 53.125, f1: 12.781954887218044, r: 0.16564062958612225
06/02/2019 03:48:56 step: 4810, epoch: 145, batch: 24, loss: 1.5591028928756714, acc: 51.5625, f1: 14.583333333333332, r: 0.17540381386765364
06/02/2019 03:48:56 step: 4815, epoch: 145, batch: 29, loss: 1.534977674484253, acc: 46.875, f1: 8.064516129032258, r: 0.150187094517219
06/02/2019 03:48:56 *** evaluating ***
06/02/2019 03:48:57 step: 146, epoch: 145, acc: 55.98290598290598, f1: 16.612602535421328, r: 0.21020210811907244
06/02/2019 03:48:57 *** epoch: 147 ***
06/02/2019 03:48:57 *** training ***
06/02/2019 03:48:57 step: 4823, epoch: 146, batch: 4, loss: 1.4579724073410034, acc: 45.3125, f1: 12.53397765025672, r: 0.16784705630044255
06/02/2019 03:48:57 step: 4828, epoch: 146, batch: 9, loss: 1.6493202447891235, acc: 37.5, f1: 10.893246187363834, r: 0.20917072370264508
06/02/2019 03:48:58 step: 4833, epoch: 146, batch: 14, loss: 1.8169194459915161, acc: 32.8125, f1: 12.71585557299843, r: 0.1619044467556538
06/02/2019 03:48:58 step: 4838, epoch: 146, batch: 19, loss: 1.6241580247879028, acc: 46.875, f1: 13.482501434308663, r: 0.15999764659800575
06/02/2019 03:48:58 step: 4843, epoch: 146, batch: 24, loss: 1.5206581354141235, acc: 46.875, f1: 14.653558052434457, r: 0.13282916622603078
06/02/2019 03:48:59 step: 4848, epoch: 146, batch: 29, loss: 1.42300283908844, acc: 56.25, f1: 14.544319600499374, r: 0.13470764974503605
06/02/2019 03:48:59 *** evaluating ***
06/02/2019 03:48:59 step: 147, epoch: 146, acc: 52.991452991452995, f1: 14.973375931842387, r: 0.20331887284749173
06/02/2019 03:48:59 *** epoch: 148 ***
06/02/2019 03:48:59 *** training ***
06/02/2019 03:48:59 step: 4856, epoch: 147, batch: 4, loss: 1.4855719804763794, acc: 50.0, f1: 14.920634920634921, r: 0.19205513077882003
06/02/2019 03:48:59 step: 4861, epoch: 147, batch: 9, loss: 1.359987497329712, acc: 48.4375, f1: 16.87406669985067, r: 0.23169398966410448
06/02/2019 03:49:00 step: 4866, epoch: 147, batch: 14, loss: 1.6562011241912842, acc: 43.75, f1: 11.78861788617886, r: 0.147991775396663
06/02/2019 03:49:00 step: 4871, epoch: 147, batch: 19, loss: 1.591700792312622, acc: 50.0, f1: 14.078674948240167, r: 0.12515168950200697
06/02/2019 03:49:00 step: 4876, epoch: 147, batch: 24, loss: 1.541499376296997, acc: 42.1875, f1: 12.654320987654321, r: 0.19737674090029678
06/02/2019 03:49:01 step: 4881, epoch: 147, batch: 29, loss: 1.5173659324645996, acc: 42.1875, f1: 12.97208538587849, r: 0.205382305547485
06/02/2019 03:49:01 *** evaluating ***
06/02/2019 03:49:01 step: 148, epoch: 147, acc: 56.41025641025641, f1: 16.83311622521349, r: 0.2117346654501576
06/02/2019 03:49:01 *** epoch: 149 ***
06/02/2019 03:49:01 *** training ***
06/02/2019 03:49:01 step: 4889, epoch: 148, batch: 4, loss: 1.6317306756973267, acc: 42.1875, f1: 12.142857142857144, r: 0.21617508367508068
06/02/2019 03:49:01 step: 4894, epoch: 148, batch: 9, loss: 1.632802963256836, acc: 45.3125, f1: 13.862541952429591, r: 0.14579754229913328
06/02/2019 03:49:02 step: 4899, epoch: 148, batch: 14, loss: 1.408523440361023, acc: 51.5625, f1: 17.59876364338839, r: 0.20966177115049248
06/02/2019 03:49:02 step: 4904, epoch: 148, batch: 19, loss: 1.573535442352295, acc: 46.875, f1: 15.882352941176473, r: 0.17286583834621796
06/02/2019 03:49:02 step: 4909, epoch: 148, batch: 24, loss: 1.4818936586380005, acc: 48.4375, f1: 14.874939874939875, r: 0.22086510888772976
06/02/2019 03:49:03 step: 4914, epoch: 148, batch: 29, loss: 1.6577816009521484, acc: 39.0625, f1: 7.267441860465117, r: 0.1540411054228878
06/02/2019 03:49:03 *** evaluating ***
06/02/2019 03:49:03 step: 149, epoch: 148, acc: 55.55555555555556, f1: 16.31343796540268, r: 0.21110998111003426
06/02/2019 03:49:03 *** epoch: 150 ***
06/02/2019 03:49:03 *** training ***
06/02/2019 03:49:03 step: 4922, epoch: 149, batch: 4, loss: 1.4802587032318115, acc: 42.1875, f1: 17.582417582417584, r: 0.19357889684859622
06/02/2019 03:49:03 step: 4927, epoch: 149, batch: 9, loss: 1.6536563634872437, acc: 40.625, f1: 10.101744186046512, r: 0.06730147035033836
06/02/2019 03:49:04 step: 4932, epoch: 149, batch: 14, loss: 1.4645127058029175, acc: 48.4375, f1: 14.086021505376342, r: 0.20219038827524083
06/02/2019 03:49:04 step: 4937, epoch: 149, batch: 19, loss: 1.4797310829162598, acc: 40.625, f1: 12.171893147502903, r: 0.23955554126982131
06/02/2019 03:49:05 step: 4942, epoch: 149, batch: 24, loss: 1.5273154973983765, acc: 43.75, f1: 15.017006802721088, r: 0.2507132223946845
06/02/2019 03:49:05 step: 4947, epoch: 149, batch: 29, loss: 1.7937135696411133, acc: 37.5, f1: 9.771908763505403, r: 0.12681915003616126
06/02/2019 03:49:05 *** evaluating ***
06/02/2019 03:49:05 step: 150, epoch: 149, acc: 52.991452991452995, f1: 15.103283322461406, r: 0.221305914672085
06/02/2019 03:49:05 *** epoch: 151 ***
06/02/2019 03:49:05 *** training ***
06/02/2019 03:49:06 step: 4955, epoch: 150, batch: 4, loss: 1.670602560043335, acc: 39.0625, f1: 9.623015873015872, r: 0.1732476266750658
06/02/2019 03:49:06 step: 4960, epoch: 150, batch: 9, loss: 1.6463631391525269, acc: 43.75, f1: 12.71326362135388, r: 0.18115782053176901
06/02/2019 03:49:06 step: 4965, epoch: 150, batch: 14, loss: 1.4560496807098389, acc: 48.4375, f1: 14.34590690208668, r: 0.17572374448323175
06/02/2019 03:49:07 step: 4970, epoch: 150, batch: 19, loss: 1.7107380628585815, acc: 34.375, f1: 11.097560975609756, r: 0.11453090517029973
06/02/2019 03:49:07 step: 4975, epoch: 150, batch: 24, loss: 1.5700989961624146, acc: 35.9375, f1: 9.43577430972389, r: 0.21638757756309646
06/02/2019 03:49:07 step: 4980, epoch: 150, batch: 29, loss: 1.7281997203826904, acc: 32.8125, f1: 12.764378478664193, r: 0.23663052427378123
06/02/2019 03:49:07 *** evaluating ***
06/02/2019 03:49:07 step: 151, epoch: 150, acc: 55.12820512820513, f1: 16.170454545454543, r: 0.2173125511457317
06/02/2019 03:49:07 *** epoch: 152 ***
06/02/2019 03:49:07 *** training ***
06/02/2019 03:49:08 step: 4988, epoch: 151, batch: 4, loss: 1.4788453578948975, acc: 43.75, f1: 13.301282051282051, r: 0.18348335991032508
06/02/2019 03:49:08 step: 4993, epoch: 151, batch: 9, loss: 1.4663163423538208, acc: 48.4375, f1: 15.692788743404337, r: 0.18742708739407526
06/02/2019 03:49:08 step: 4998, epoch: 151, batch: 14, loss: 1.575439453125, acc: 40.625, f1: 12.5896700143472, r: 0.16225313590583404
06/02/2019 03:49:09 step: 5003, epoch: 151, batch: 19, loss: 1.6003228425979614, acc: 43.75, f1: 11.232517482517483, r: 0.20064337974007843
06/02/2019 03:49:09 step: 5008, epoch: 151, batch: 24, loss: 1.6855461597442627, acc: 35.9375, f1: 10.49167634533488, r: 0.18179959440013962
06/02/2019 03:49:09 step: 5013, epoch: 151, batch: 29, loss: 1.6199136972427368, acc: 34.375, f1: 11.538461538461538, r: 0.1901818512524662
06/02/2019 03:49:10 *** evaluating ***
06/02/2019 03:49:10 step: 152, epoch: 151, acc: 55.55555555555556, f1: 16.54894406392694, r: 0.22461425997335308
06/02/2019 03:49:10 *** epoch: 153 ***
06/02/2019 03:49:10 *** training ***
06/02/2019 03:49:10 step: 5021, epoch: 152, batch: 4, loss: 1.4838805198669434, acc: 48.4375, f1: 13.500042240432542, r: 0.16805944582396612
06/02/2019 03:49:10 step: 5026, epoch: 152, batch: 9, loss: 1.6125502586364746, acc: 42.1875, f1: 13.453947368421051, r: 0.18837660617248975
06/02/2019 03:49:11 step: 5031, epoch: 152, batch: 14, loss: 1.8362430334091187, acc: 32.8125, f1: 7.809295967190703, r: 0.10103125833324245
06/02/2019 03:49:11 step: 5036, epoch: 152, batch: 19, loss: 1.3988984823226929, acc: 57.8125, f1: 17.36842105263158, r: 0.20514859349740688
06/02/2019 03:49:12 step: 5041, epoch: 152, batch: 24, loss: 1.574992060661316, acc: 32.8125, f1: 12.224231464737796, r: 0.187413263312729
06/02/2019 03:49:12 step: 5046, epoch: 152, batch: 29, loss: 1.3793070316314697, acc: 57.8125, f1: 19.682539682539684, r: 0.23675813639298166
06/02/2019 03:49:12 *** evaluating ***
06/02/2019 03:49:12 step: 153, epoch: 152, acc: 53.41880341880342, f1: 15.177745606627258, r: 0.20254811841774173
06/02/2019 03:49:12 *** epoch: 154 ***
06/02/2019 03:49:12 *** training ***
06/02/2019 03:49:12 step: 5054, epoch: 153, batch: 4, loss: 1.5717954635620117, acc: 40.625, f1: 13.537414965986393, r: 0.18925360118194537
06/02/2019 03:49:13 step: 5059, epoch: 153, batch: 9, loss: 1.553950548171997, acc: 43.75, f1: 11.637931034482756, r: 0.18001726731969964
06/02/2019 03:49:13 step: 5064, epoch: 153, batch: 14, loss: 1.4867920875549316, acc: 48.4375, f1: 15.067155067155067, r: 0.12332818458075784
06/02/2019 03:49:13 step: 5069, epoch: 153, batch: 19, loss: 1.7101956605911255, acc: 34.375, f1: 8.40863453815261, r: 0.1908775901843984
06/02/2019 03:49:14 step: 5074, epoch: 153, batch: 24, loss: 1.5936015844345093, acc: 45.3125, f1: 10.995670995670997, r: 0.10727395652535404
06/02/2019 03:49:14 step: 5079, epoch: 153, batch: 29, loss: 1.8509963750839233, acc: 40.625, f1: 14.766759345072597, r: 0.226712312197262
06/02/2019 03:49:14 *** evaluating ***
06/02/2019 03:49:14 step: 154, epoch: 153, acc: 55.55555555555556, f1: 16.393596632971327, r: 0.21944207592966697
06/02/2019 03:49:14 *** epoch: 155 ***
06/02/2019 03:49:14 *** training ***
06/02/2019 03:49:15 step: 5087, epoch: 154, batch: 4, loss: 1.505742073059082, acc: 45.3125, f1: 12.84786641929499, r: 0.17640034687816333
06/02/2019 03:49:15 step: 5092, epoch: 154, batch: 9, loss: 1.510317087173462, acc: 45.3125, f1: 16.224899598393574, r: 0.13803008853831303
06/02/2019 03:49:15 step: 5097, epoch: 154, batch: 14, loss: 1.5145128965377808, acc: 45.3125, f1: 14.117647058823529, r: 0.20864416948847384
06/02/2019 03:49:16 step: 5102, epoch: 154, batch: 19, loss: 1.5116276741027832, acc: 40.625, f1: 11.615210843373495, r: 0.27122760371811655
06/02/2019 03:49:16 step: 5107, epoch: 154, batch: 24, loss: 1.5563850402832031, acc: 40.625, f1: 12.369337979094075, r: 0.1607170582187381
06/02/2019 03:49:16 step: 5112, epoch: 154, batch: 29, loss: 1.680077314376831, acc: 29.6875, f1: 9.126984126984128, r: 0.2319994698675
06/02/2019 03:49:16 *** evaluating ***
06/02/2019 03:49:17 step: 155, epoch: 154, acc: 55.98290598290598, f1: 16.612602535421328, r: 0.2221545596936615
06/02/2019 03:49:17 *** epoch: 156 ***
06/02/2019 03:49:17 *** training ***
06/02/2019 03:49:17 step: 5120, epoch: 155, batch: 4, loss: 1.6143864393234253, acc: 34.375, f1: 7.394957983193276, r: 0.0933458136257012
06/02/2019 03:49:17 step: 5125, epoch: 155, batch: 9, loss: 1.4998857975006104, acc: 42.1875, f1: 13.877551020408166, r: 0.191501197890653
06/02/2019 03:49:18 step: 5130, epoch: 155, batch: 14, loss: 1.5498926639556885, acc: 43.75, f1: 13.733393043737872, r: 0.21111112990862574
06/02/2019 03:49:18 step: 5135, epoch: 155, batch: 19, loss: 1.6907217502593994, acc: 34.375, f1: 11.348484848484848, r: 0.17568218053434914
06/02/2019 03:49:18 step: 5140, epoch: 155, batch: 24, loss: 1.3396484851837158, acc: 54.6875, f1: 19.938080495356036, r: 0.2210588600183139
06/02/2019 03:49:18 step: 5145, epoch: 155, batch: 29, loss: 1.6602787971496582, acc: 45.3125, f1: 13.690476190476192, r: 0.21575345562443585
06/02/2019 03:49:19 *** evaluating ***
06/02/2019 03:49:19 step: 156, epoch: 155, acc: 55.98290598290598, f1: 16.612602535421328, r: 0.23481080246115846
06/02/2019 03:49:19 *** epoch: 157 ***
06/02/2019 03:49:19 *** training ***
06/02/2019 03:49:19 step: 5153, epoch: 156, batch: 4, loss: 1.4419937133789062, acc: 50.0, f1: 16.59090909090909, r: 0.2541480242853142
06/02/2019 03:49:19 step: 5158, epoch: 156, batch: 9, loss: 1.529017448425293, acc: 46.875, f1: 14.083694083694084, r: 0.12980777059777418
06/02/2019 03:49:20 step: 5163, epoch: 156, batch: 14, loss: 1.5526621341705322, acc: 42.1875, f1: 17.050344234079173, r: 0.22015961062047107
06/02/2019 03:49:20 step: 5168, epoch: 156, batch: 19, loss: 1.5464109182357788, acc: 43.75, f1: 14.525810324129651, r: 0.2446508618382271
06/02/2019 03:49:20 step: 5173, epoch: 156, batch: 24, loss: 1.4250671863555908, acc: 46.875, f1: 19.79094076655052, r: 0.17422185878716737
06/02/2019 03:49:21 step: 5178, epoch: 156, batch: 29, loss: 1.434393048286438, acc: 50.0, f1: 11.813186813186812, r: 0.17361672942919495
06/02/2019 03:49:21 *** evaluating ***
06/02/2019 03:49:21 step: 157, epoch: 156, acc: 56.41025641025641, f1: 16.827616827616826, r: 0.22442828094046252
06/02/2019 03:49:21 *** epoch: 158 ***
06/02/2019 03:49:21 *** training ***
06/02/2019 03:49:21 step: 5186, epoch: 157, batch: 4, loss: 1.5493806600570679, acc: 42.1875, f1: 13.26219512195122, r: 0.14246747657478404
06/02/2019 03:49:22 step: 5191, epoch: 157, batch: 9, loss: 1.6390414237976074, acc: 39.0625, f1: 12.445887445887447, r: 0.1938795293625218
06/02/2019 03:49:22 step: 5196, epoch: 157, batch: 14, loss: 1.6009814739227295, acc: 37.5, f1: 12.828947368421051, r: 0.19207590094965563
06/02/2019 03:49:22 step: 5201, epoch: 157, batch: 19, loss: 1.6054500341415405, acc: 46.875, f1: 10.22940074906367, r: 0.13840547119601368
06/02/2019 03:49:23 step: 5206, epoch: 157, batch: 24, loss: 1.6546046733856201, acc: 39.0625, f1: 11.492673992673993, r: 0.13156441522374493
06/02/2019 03:49:23 step: 5211, epoch: 157, batch: 29, loss: 1.6765412092208862, acc: 39.0625, f1: 14.87394957983193, r: 0.17237071282108848
06/02/2019 03:49:23 *** evaluating ***
06/02/2019 03:49:23 step: 158, epoch: 157, acc: 57.26495726495726, f1: 17.301957966588827, r: 0.2428951712814695
06/02/2019 03:49:23 *** epoch: 159 ***
06/02/2019 03:49:23 *** training ***
06/02/2019 03:49:23 step: 5219, epoch: 158, batch: 4, loss: 1.6409302949905396, acc: 40.625, f1: 13.177710843373495, r: 0.22887684635011418
06/02/2019 03:49:24 step: 5224, epoch: 158, batch: 9, loss: 1.5845541954040527, acc: 42.1875, f1: 11.76470588235294, r: 0.1471843999056726
06/02/2019 03:49:24 step: 5229, epoch: 158, batch: 14, loss: 1.5745254755020142, acc: 42.1875, f1: 20.495582848524027, r: 0.18179051640446334
06/02/2019 03:49:24 step: 5234, epoch: 158, batch: 19, loss: 1.408474087715149, acc: 50.0, f1: 14.696611505122142, r: 0.10183119666735693
06/02/2019 03:49:24 step: 5239, epoch: 158, batch: 24, loss: 1.6544300317764282, acc: 51.5625, f1: 18.123973727422005, r: 0.16612519777785503
06/02/2019 03:49:25 step: 5244, epoch: 158, batch: 29, loss: 1.5208241939544678, acc: 39.0625, f1: 11.286764705882351, r: 0.16866554380130142
06/02/2019 03:49:25 *** evaluating ***
06/02/2019 03:49:25 step: 159, epoch: 158, acc: 55.55555555555556, f1: 16.468716468716465, r: 0.2269876786584711
06/02/2019 03:49:25 *** epoch: 160 ***
06/02/2019 03:49:25 *** training ***
06/02/2019 03:49:25 step: 5252, epoch: 159, batch: 4, loss: 1.4609118700027466, acc: 46.875, f1: 14.974892395982781, r: 0.2571018199219359
06/02/2019 03:49:26 step: 5257, epoch: 159, batch: 9, loss: 1.51201593875885, acc: 42.1875, f1: 10.081148564294631, r: 0.19080555344735342
06/02/2019 03:49:26 step: 5262, epoch: 159, batch: 14, loss: 1.783227801322937, acc: 43.75, f1: 10.980392156862745, r: 0.11202647622795542
06/02/2019 03:49:26 step: 5267, epoch: 159, batch: 19, loss: 1.5155861377716064, acc: 39.0625, f1: 13.84920634920635, r: 0.1975870757206674
06/02/2019 03:49:27 step: 5272, epoch: 159, batch: 24, loss: 1.5143144130706787, acc: 45.3125, f1: 13.60294117647059, r: 0.192981507731168
06/02/2019 03:49:27 step: 5277, epoch: 159, batch: 29, loss: 1.646183967590332, acc: 40.625, f1: 12.098765432098766, r: 0.19286200144499707
06/02/2019 03:49:27 *** evaluating ***
06/02/2019 03:49:27 step: 160, epoch: 159, acc: 55.98290598290598, f1: 16.612602535421328, r: 0.23524952052277204
06/02/2019 03:49:27 *** epoch: 161 ***
06/02/2019 03:49:27 *** training ***
06/02/2019 03:49:27 step: 5285, epoch: 160, batch: 4, loss: 1.5555124282836914, acc: 43.75, f1: 12.209302325581394, r: 0.19447739019181082
06/02/2019 03:49:28 step: 5290, epoch: 160, batch: 9, loss: 1.4160387516021729, acc: 48.4375, f1: 14.143690349946974, r: 0.18338181912360843
06/02/2019 03:49:28 step: 5295, epoch: 160, batch: 14, loss: 1.577412486076355, acc: 40.625, f1: 15.335907335907336, r: 0.2803944878588598
06/02/2019 03:49:29 step: 5300, epoch: 160, batch: 19, loss: 1.6193910837173462, acc: 42.1875, f1: 12.212885154061626, r: 0.13845692215596755
06/02/2019 03:49:29 step: 5305, epoch: 160, batch: 24, loss: 1.7090727090835571, acc: 34.375, f1: 7.887801204819277, r: 0.14953797694134824
06/02/2019 03:49:29 step: 5310, epoch: 160, batch: 29, loss: 1.552014946937561, acc: 40.625, f1: 11.723930078360457, r: 0.21649813275775828
06/02/2019 03:49:29 *** evaluating ***
06/02/2019 03:49:29 step: 161, epoch: 160, acc: 56.837606837606835, f1: 17.03877790834313, r: 0.24784997899880337
06/02/2019 03:49:29 *** epoch: 162 ***
06/02/2019 03:49:29 *** training ***
06/02/2019 03:49:30 step: 5318, epoch: 161, batch: 4, loss: 1.5834052562713623, acc: 43.75, f1: 11.990820424555364, r: 0.17129123749706882
06/02/2019 03:49:30 step: 5323, epoch: 161, batch: 9, loss: 1.5456193685531616, acc: 45.3125, f1: 14.99350227420403, r: 0.22579715659938682
06/02/2019 03:49:30 step: 5328, epoch: 161, batch: 14, loss: 1.6645817756652832, acc: 37.5, f1: 10.133928571428571, r: 0.22221741787215907
06/02/2019 03:49:31 step: 5333, epoch: 161, batch: 19, loss: 1.4487425088882446, acc: 48.4375, f1: 11.366758241758241, r: 0.19658268818254956
06/02/2019 03:49:31 step: 5338, epoch: 161, batch: 24, loss: 1.652538537979126, acc: 39.0625, f1: 8.210180623973727, r: 0.1466225096290626
06/02/2019 03:49:31 step: 5343, epoch: 161, batch: 29, loss: 1.6771475076675415, acc: 26.5625, f1: 11.88141923436041, r: 0.19624523772747413
06/02/2019 03:49:31 *** evaluating ***
06/02/2019 03:49:31 step: 162, epoch: 161, acc: 55.98290598290598, f1: 16.552891552891552, r: 0.2484394424990113
06/02/2019 03:49:31 *** epoch: 163 ***
06/02/2019 03:49:31 *** training ***
06/02/2019 03:49:32 step: 5351, epoch: 162, batch: 4, loss: 1.7022498846054077, acc: 39.0625, f1: 13.523391812865498, r: 0.16431848966671275
06/02/2019 03:49:32 step: 5356, epoch: 162, batch: 9, loss: 1.3771390914916992, acc: 53.125, f1: 18.095238095238095, r: 0.2821547439373279
06/02/2019 03:49:32 step: 5361, epoch: 162, batch: 14, loss: 1.5732378959655762, acc: 42.1875, f1: 13.631669535283994, r: 0.11866870951537088
06/02/2019 03:49:33 step: 5366, epoch: 162, batch: 19, loss: 1.520331621170044, acc: 43.75, f1: 15.096807953950812, r: 0.2125193306902449
06/02/2019 03:49:33 step: 5371, epoch: 162, batch: 24, loss: 1.5477755069732666, acc: 39.0625, f1: 16.745805634694523, r: 0.18843071975766768
06/02/2019 03:49:33 step: 5376, epoch: 162, batch: 29, loss: 1.4997646808624268, acc: 43.75, f1: 13.392857142857146, r: 0.21344119254460048
06/02/2019 03:49:33 *** evaluating ***
06/02/2019 03:49:34 step: 163, epoch: 162, acc: 52.991452991452995, f1: 15.037220843672456, r: 0.22554376662009418
06/02/2019 03:49:34 *** epoch: 164 ***
06/02/2019 03:49:34 *** training ***
06/02/2019 03:49:34 step: 5384, epoch: 163, batch: 4, loss: 1.598028540611267, acc: 48.4375, f1: 13.398402839396631, r: 0.10663441088168157
06/02/2019 03:49:34 step: 5389, epoch: 163, batch: 9, loss: 1.5029430389404297, acc: 43.75, f1: 17.05282669138091, r: 0.16558728246529067
06/02/2019 03:49:35 step: 5394, epoch: 163, batch: 14, loss: 1.5953171253204346, acc: 48.4375, f1: 16.689872830223702, r: 0.17001287936825552
06/02/2019 03:49:35 step: 5399, epoch: 163, batch: 19, loss: 1.3806875944137573, acc: 50.0, f1: 16.70995670995671, r: 0.3180212904665896
06/02/2019 03:49:35 step: 5404, epoch: 163, batch: 24, loss: 1.619158148765564, acc: 40.625, f1: 10.65040650406504, r: 0.18165611814754654
06/02/2019 03:49:36 step: 5409, epoch: 163, batch: 29, loss: 1.7479902505874634, acc: 31.25, f1: 7.860576923076923, r: 0.18612043253853416
06/02/2019 03:49:36 *** evaluating ***
06/02/2019 03:49:36 step: 164, epoch: 163, acc: 56.837606837606835, f1: 17.14987242668904, r: 0.23083830995416196
06/02/2019 03:49:36 *** epoch: 165 ***
06/02/2019 03:49:36 *** training ***
06/02/2019 03:49:36 step: 5417, epoch: 164, batch: 4, loss: 1.4591177701950073, acc: 51.5625, f1: 12.907972177635097, r: 0.15209478202885876
06/02/2019 03:49:36 step: 5422, epoch: 164, batch: 9, loss: 1.536513090133667, acc: 48.4375, f1: 12.967032967032965, r: 0.1898434011245806
06/02/2019 03:49:37 step: 5427, epoch: 164, batch: 14, loss: 1.3288401365280151, acc: 48.4375, f1: 29.603174603174605, r: 0.2772632641346674
06/02/2019 03:49:37 step: 5432, epoch: 164, batch: 19, loss: 1.5669093132019043, acc: 42.1875, f1: 12.094907407407408, r: 0.25666528318164616
06/02/2019 03:49:37 step: 5437, epoch: 164, batch: 24, loss: 1.473667860031128, acc: 39.0625, f1: 13.24561403508772, r: 0.3199491311927034
06/02/2019 03:49:38 step: 5442, epoch: 164, batch: 29, loss: 1.8260648250579834, acc: 28.125, f1: 6.979166666666667, r: 0.10600074080228568
06/02/2019 03:49:38 *** evaluating ***
06/02/2019 03:49:38 step: 165, epoch: 164, acc: 55.55555555555556, f1: 16.393596632971327, r: 0.24090344671506786
06/02/2019 03:49:38 *** epoch: 166 ***
06/02/2019 03:49:38 *** training ***
06/02/2019 03:49:38 step: 5450, epoch: 165, batch: 4, loss: 1.5358220338821411, acc: 42.1875, f1: 11.360544217687076, r: 0.17835716822779196
06/02/2019 03:49:38 step: 5455, epoch: 165, batch: 9, loss: 1.5602498054504395, acc: 46.875, f1: 9.216589861751153, r: 0.02965880194168166
06/02/2019 03:49:39 step: 5460, epoch: 165, batch: 14, loss: 1.616378903388977, acc: 45.3125, f1: 15.804597701149426, r: 0.23848372997905168
06/02/2019 03:49:39 step: 5465, epoch: 165, batch: 19, loss: 1.5173344612121582, acc: 43.75, f1: 11.904761904761905, r: 0.19445123245047788
06/02/2019 03:49:39 step: 5470, epoch: 165, batch: 24, loss: 1.4946370124816895, acc: 43.75, f1: 13.69047619047619, r: 0.2700872127326224
06/02/2019 03:49:40 step: 5475, epoch: 165, batch: 29, loss: 1.415281057357788, acc: 46.875, f1: 13.78012048192771, r: 0.2716081263876988
06/02/2019 03:49:40 *** evaluating ***
06/02/2019 03:49:40 step: 166, epoch: 165, acc: 55.12820512820513, f1: 16.170454545454543, r: 0.22924166291907314
06/02/2019 03:49:40 *** epoch: 167 ***
06/02/2019 03:49:40 *** training ***
06/02/2019 03:49:40 step: 5483, epoch: 166, batch: 4, loss: 1.5150892734527588, acc: 40.625, f1: 12.672476397966594, r: 0.19624900578188476
06/02/2019 03:49:41 step: 5488, epoch: 166, batch: 9, loss: 1.5033133029937744, acc: 43.75, f1: 13.111268603827073, r: 0.18866969359122385
06/02/2019 03:49:41 step: 5493, epoch: 166, batch: 14, loss: 1.6524302959442139, acc: 37.5, f1: 14.45993031358885, r: 0.07007122729060278
06/02/2019 03:49:41 step: 5498, epoch: 166, batch: 19, loss: 1.4571360349655151, acc: 50.0, f1: 11.803444782168187, r: 0.21238736244500134
06/02/2019 03:49:42 step: 5503, epoch: 166, batch: 24, loss: 1.4789994955062866, acc: 43.75, f1: 16.30339429111276, r: 0.2034059373905199
06/02/2019 03:49:42 step: 5508, epoch: 166, batch: 29, loss: 1.4823561906814575, acc: 46.875, f1: 14.318181818181818, r: 0.22255124976190466
06/02/2019 03:49:42 *** evaluating ***
06/02/2019 03:49:42 step: 167, epoch: 166, acc: 55.98290598290598, f1: 16.682579318448884, r: 0.21946643994129603
06/02/2019 03:49:42 *** epoch: 168 ***
06/02/2019 03:49:42 *** training ***
06/02/2019 03:49:43 step: 5516, epoch: 167, batch: 4, loss: 1.5949788093566895, acc: 39.0625, f1: 9.705882352941176, r: 0.15881338993825128
06/02/2019 03:49:43 step: 5521, epoch: 167, batch: 9, loss: 1.5314548015594482, acc: 42.1875, f1: 14.756671899529044, r: 0.1357516997652894
06/02/2019 03:49:43 step: 5526, epoch: 167, batch: 14, loss: 1.5165354013442993, acc: 43.75, f1: 13.703794770920416, r: 0.12253460995355171
06/02/2019 03:49:43 step: 5531, epoch: 167, batch: 19, loss: 1.5680052042007446, acc: 39.0625, f1: 12.56988625409678, r: 0.1298394124922757
06/02/2019 03:49:44 step: 5536, epoch: 167, batch: 24, loss: 1.451812505722046, acc: 48.4375, f1: 13.522588522588523, r: 0.18464993005528935
06/02/2019 03:49:44 step: 5541, epoch: 167, batch: 29, loss: 1.5015006065368652, acc: 46.875, f1: 13.591006694454972, r: 0.20792776750664582
06/02/2019 03:49:44 *** evaluating ***
06/02/2019 03:49:44 step: 168, epoch: 167, acc: 56.41025641025641, f1: 16.821969696969695, r: 0.2231801935638692
06/02/2019 03:49:44 *** epoch: 169 ***
06/02/2019 03:49:44 *** training ***
06/02/2019 03:49:45 step: 5549, epoch: 168, batch: 4, loss: 1.4432785511016846, acc: 48.4375, f1: 13.723776223776223, r: 0.2629333052660708
06/02/2019 03:49:45 step: 5554, epoch: 168, batch: 9, loss: 1.3869267702102661, acc: 53.125, f1: 14.240362811791382, r: 0.16903940434667714
06/02/2019 03:49:45 step: 5559, epoch: 168, batch: 14, loss: 1.4845985174179077, acc: 42.1875, f1: 9.256978653530377, r: 0.18347600183631046
06/02/2019 03:49:46 step: 5564, epoch: 168, batch: 19, loss: 1.5973000526428223, acc: 45.3125, f1: 14.019607843137255, r: 0.18788880928116364
06/02/2019 03:49:46 step: 5569, epoch: 168, batch: 24, loss: 1.5643724203109741, acc: 37.5, f1: 11.475029036004646, r: 0.22674226008115855
06/02/2019 03:49:47 step: 5574, epoch: 168, batch: 29, loss: 1.549393653869629, acc: 42.1875, f1: 14.028974876214924, r: 0.22856685194607676
06/02/2019 03:49:47 *** evaluating ***
06/02/2019 03:49:47 step: 169, epoch: 168, acc: 55.98290598290598, f1: 16.657120284416354, r: 0.23359431247476997
06/02/2019 03:49:47 *** epoch: 170 ***
06/02/2019 03:49:47 *** training ***
06/02/2019 03:49:47 step: 5582, epoch: 169, batch: 4, loss: 1.4087601900100708, acc: 45.3125, f1: 16.330795641140465, r: 0.1853748885146873
06/02/2019 03:49:47 step: 5587, epoch: 169, batch: 9, loss: 1.528429627418518, acc: 43.75, f1: 12.621951219512194, r: 0.2442489975391197
06/02/2019 03:49:48 step: 5592, epoch: 169, batch: 14, loss: 1.5510936975479126, acc: 46.875, f1: 12.536337209302326, r: 0.21145206518145704
06/02/2019 03:49:48 step: 5597, epoch: 169, batch: 19, loss: 1.6329386234283447, acc: 35.9375, f1: 7.55336617405583, r: 0.15083311163490534
06/02/2019 03:49:48 step: 5602, epoch: 169, batch: 24, loss: 1.6412062644958496, acc: 35.9375, f1: 11.513157894736844, r: 0.18113741436864503
06/02/2019 03:49:49 step: 5607, epoch: 169, batch: 29, loss: 1.626889944076538, acc: 40.625, f1: 13.124274099883856, r: 0.19266675070960296
06/02/2019 03:49:49 *** evaluating ***
06/02/2019 03:49:49 step: 170, epoch: 169, acc: 57.26495726495726, f1: 17.40239767921429, r: 0.23644963404612387
06/02/2019 03:49:49 *** epoch: 171 ***
06/02/2019 03:49:49 *** training ***
06/02/2019 03:49:49 step: 5615, epoch: 170, batch: 4, loss: 1.5730316638946533, acc: 42.1875, f1: 11.812896405919663, r: 0.24442102124529053
06/02/2019 03:49:50 step: 5620, epoch: 170, batch: 9, loss: 1.3286545276641846, acc: 54.6875, f1: 16.060337178349602, r: 0.28780850881789777
06/02/2019 03:49:50 step: 5625, epoch: 170, batch: 14, loss: 1.6526451110839844, acc: 43.75, f1: 12.710084033613445, r: 0.1612546747460548
06/02/2019 03:49:50 step: 5630, epoch: 170, batch: 19, loss: 1.5041377544403076, acc: 37.5, f1: 13.277582159624412, r: 0.26160711429607086
06/02/2019 03:49:50 step: 5635, epoch: 170, batch: 24, loss: 1.608583688735962, acc: 42.1875, f1: 13.224275724275724, r: 0.20015003460968148
06/02/2019 03:49:51 step: 5640, epoch: 170, batch: 29, loss: 1.5644832849502563, acc: 42.1875, f1: 11.574074074074073, r: 0.24145486846250963
06/02/2019 03:49:51 *** evaluating ***
06/02/2019 03:49:51 step: 171, epoch: 170, acc: 57.692307692307686, f1: 17.450065132435952, r: 0.2504179580456627
06/02/2019 03:49:51 *** epoch: 172 ***
06/02/2019 03:49:51 *** training ***
06/02/2019 03:49:52 step: 5648, epoch: 171, batch: 4, loss: 1.3924974203109741, acc: 54.6875, f1: 16.19047619047619, r: 0.20422364039734775
06/02/2019 03:49:52 step: 5653, epoch: 171, batch: 9, loss: 1.3814862966537476, acc: 51.5625, f1: 17.941176470588236, r: 0.26999071054432006
06/02/2019 03:49:52 step: 5658, epoch: 171, batch: 14, loss: 1.5546684265136719, acc: 45.3125, f1: 10.137895812053117, r: 0.16392985616676015
06/02/2019 03:49:52 step: 5663, epoch: 171, batch: 19, loss: 1.456106185913086, acc: 45.3125, f1: 16.785714285714285, r: 0.25542941464108004
06/02/2019 03:49:53 step: 5668, epoch: 171, batch: 24, loss: 1.6005866527557373, acc: 45.3125, f1: 14.75512439367861, r: 0.13863126951450638
06/02/2019 03:49:53 step: 5673, epoch: 171, batch: 29, loss: 1.4070508480072021, acc: 46.875, f1: 12.370005473453748, r: 0.12791992681612666
06/02/2019 03:49:53 *** evaluating ***
06/02/2019 03:49:53 step: 172, epoch: 171, acc: 57.26495726495726, f1: 17.246218334244578, r: 0.23678007783776553
06/02/2019 03:49:53 *** epoch: 173 ***
06/02/2019 03:49:53 *** training ***
06/02/2019 03:49:54 step: 5681, epoch: 172, batch: 4, loss: 1.715155005455017, acc: 39.0625, f1: 11.078595317725751, r: 0.19040823938525692
06/02/2019 03:49:54 step: 5686, epoch: 172, batch: 9, loss: 1.5974174737930298, acc: 42.1875, f1: 11.519607843137255, r: 0.17386663199497585
06/02/2019 03:49:54 step: 5691, epoch: 172, batch: 14, loss: 1.6067436933517456, acc: 40.625, f1: 11.075069508804448, r: 0.19511024650392336
06/02/2019 03:49:55 step: 5696, epoch: 172, batch: 19, loss: 1.5123770236968994, acc: 42.1875, f1: 14.540306462358426, r: 0.2340512380502268
06/02/2019 03:49:55 step: 5701, epoch: 172, batch: 24, loss: 1.6238728761672974, acc: 39.0625, f1: 15.119047619047619, r: 0.19859201227732615
06/02/2019 03:49:55 step: 5706, epoch: 172, batch: 29, loss: 1.6575204133987427, acc: 34.375, f1: 10.38961038961039, r: 0.2505748562433986
06/02/2019 03:49:55 *** evaluating ***
06/02/2019 03:49:55 step: 173, epoch: 172, acc: 56.41025641025641, f1: 16.821969696969695, r: 0.23390790437568407
06/02/2019 03:49:55 *** epoch: 174 ***
06/02/2019 03:49:55 *** training ***
06/02/2019 03:49:56 step: 5714, epoch: 173, batch: 4, loss: 1.5072213411331177, acc: 46.875, f1: 13.909011880614313, r: 0.23796638578779217
06/02/2019 03:49:56 step: 5719, epoch: 173, batch: 9, loss: 1.553045392036438, acc: 46.875, f1: 12.865168539325843, r: 0.20104423491047008
06/02/2019 03:49:57 step: 5724, epoch: 173, batch: 14, loss: 1.7123523950576782, acc: 32.8125, f1: 12.473411043988767, r: 0.10540060061636908
06/02/2019 03:49:57 step: 5729, epoch: 173, batch: 19, loss: 1.5864566564559937, acc: 43.75, f1: 14.428096820123399, r: 0.18572979736013162
06/02/2019 03:49:57 step: 5734, epoch: 173, batch: 24, loss: 1.6166340112686157, acc: 48.4375, f1: 8.516483516483516, r: 0.18131295397926342
06/02/2019 03:49:58 step: 5739, epoch: 173, batch: 29, loss: 1.5328540802001953, acc: 53.125, f1: 17.794083925705113, r: 0.21608939681139752
06/02/2019 03:49:58 *** evaluating ***
06/02/2019 03:49:58 step: 174, epoch: 173, acc: 56.837606837606835, f1: 17.03877790834313, r: 0.22562594045982046
06/02/2019 03:49:58 *** epoch: 175 ***
06/02/2019 03:49:58 *** training ***
06/02/2019 03:49:58 step: 5747, epoch: 174, batch: 4, loss: 1.4950573444366455, acc: 34.375, f1: 12.337662337662337, r: 0.25945385411725486
06/02/2019 03:49:59 step: 5752, epoch: 174, batch: 9, loss: 1.6688240766525269, acc: 39.0625, f1: 13.537414965986393, r: 0.2073347074463416
06/02/2019 03:49:59 step: 5757, epoch: 174, batch: 14, loss: 1.6005969047546387, acc: 42.1875, f1: 11.018826135105204, r: 0.12842048715362397
06/02/2019 03:49:59 step: 5762, epoch: 174, batch: 19, loss: 1.6345120668411255, acc: 35.9375, f1: 13.108828006088281, r: 0.23118221212754292
06/02/2019 03:49:59 step: 5767, epoch: 174, batch: 24, loss: 1.4996018409729004, acc: 40.625, f1: 13.286713286713287, r: 0.17411997070474775
06/02/2019 03:50:00 step: 5772, epoch: 174, batch: 29, loss: 1.5507200956344604, acc: 40.625, f1: 10.091362126245848, r: 0.17624752785726885
06/02/2019 03:50:00 *** evaluating ***
06/02/2019 03:50:00 step: 175, epoch: 174, acc: 55.55555555555556, f1: 16.290726817042607, r: 0.23877205026348014
06/02/2019 03:50:00 *** epoch: 176 ***
06/02/2019 03:50:00 *** training ***
06/02/2019 03:50:00 step: 5780, epoch: 175, batch: 4, loss: 1.3901695013046265, acc: 50.0, f1: 16.742293850727584, r: 0.17566139705808512
06/02/2019 03:50:01 step: 5785, epoch: 175, batch: 9, loss: 1.3434391021728516, acc: 50.0, f1: 15.928270042194093, r: 0.23123998561381578
06/02/2019 03:50:01 step: 5790, epoch: 175, batch: 14, loss: 1.4103072881698608, acc: 51.5625, f1: 17.647058823529413, r: 0.20726855590984178
06/02/2019 03:50:01 step: 5795, epoch: 175, batch: 19, loss: 1.3567326068878174, acc: 46.875, f1: 16.542359915853893, r: 0.21715600711839303
06/02/2019 03:50:02 step: 5800, epoch: 175, batch: 24, loss: 1.739580512046814, acc: 32.8125, f1: 13.614035087719298, r: 0.21749565121560213
06/02/2019 03:50:02 step: 5805, epoch: 175, batch: 29, loss: 1.6920522451400757, acc: 26.5625, f1: 14.883540372670806, r: 0.24582869709941854
06/02/2019 03:50:02 *** evaluating ***
06/02/2019 03:50:02 step: 176, epoch: 175, acc: 58.119658119658126, f1: 17.65044009340758, r: 0.2582476454689866
06/02/2019 03:50:02 *** epoch: 177 ***
06/02/2019 03:50:02 *** training ***
06/02/2019 03:50:03 step: 5813, epoch: 176, batch: 4, loss: 1.3845655918121338, acc: 51.5625, f1: 26.97478991596639, r: 0.2531923887461729
06/02/2019 03:50:03 step: 5818, epoch: 176, batch: 9, loss: 1.436903476715088, acc: 45.3125, f1: 15.33290239172592, r: 0.26700254161315373
06/02/2019 03:50:03 step: 5823, epoch: 176, batch: 14, loss: 1.6255066394805908, acc: 37.5, f1: 9.95983935742972, r: 0.15594189428151203
06/02/2019 03:50:04 step: 5828, epoch: 176, batch: 19, loss: 1.4962759017944336, acc: 43.75, f1: 14.25287356321839, r: 0.22797972083907084
06/02/2019 03:50:04 step: 5833, epoch: 176, batch: 24, loss: 1.3172266483306885, acc: 60.9375, f1: 23.423677771503858, r: 0.24927259196443938
06/02/2019 03:50:04 step: 5838, epoch: 176, batch: 29, loss: 1.416109561920166, acc: 46.875, f1: 12.446120689655173, r: 0.1906123281833012
06/02/2019 03:50:04 *** evaluating ***
06/02/2019 03:50:04 step: 177, epoch: 176, acc: 57.26495726495726, f1: 17.246218334244578, r: 0.24481605900467157
06/02/2019 03:50:04 *** epoch: 178 ***
06/02/2019 03:50:04 *** training ***
06/02/2019 03:50:05 step: 5846, epoch: 177, batch: 4, loss: 1.555700421333313, acc: 48.4375, f1: 12.551299589603282, r: 0.17308433239968626
06/02/2019 03:50:05 step: 5851, epoch: 177, batch: 9, loss: 1.4592273235321045, acc: 51.5625, f1: 12.270066889632108, r: 0.14243263936250383
06/02/2019 03:50:05 step: 5856, epoch: 177, batch: 14, loss: 1.5578242540359497, acc: 51.5625, f1: 9.92481203007519, r: 0.15407737849738581
06/02/2019 03:50:06 step: 5861, epoch: 177, batch: 19, loss: 1.608819603919983, acc: 42.1875, f1: 13.165266106442578, r: 0.17590560814462342
06/02/2019 03:50:06 step: 5866, epoch: 177, batch: 24, loss: 1.4899475574493408, acc: 50.0, f1: 16.654854712969524, r: 0.1986894050791075
06/02/2019 03:50:06 step: 5871, epoch: 177, batch: 29, loss: 1.5220671892166138, acc: 43.75, f1: 14.04320987654321, r: 0.1622225265813707
06/02/2019 03:50:06 *** evaluating ***
06/02/2019 03:50:07 step: 178, epoch: 177, acc: 56.837606837606835, f1: 17.03877790834313, r: 0.24231933051085855
06/02/2019 03:50:07 *** epoch: 179 ***
06/02/2019 03:50:07 *** training ***
06/02/2019 03:50:07 step: 5879, epoch: 178, batch: 4, loss: 1.3195551633834839, acc: 50.0, f1: 14.181057038199896, r: 0.14142255900994338
06/02/2019 03:50:07 step: 5884, epoch: 178, batch: 9, loss: 1.6395533084869385, acc: 37.5, f1: 15.363068304244774, r: 0.23671591915572446
06/02/2019 03:50:08 step: 5889, epoch: 178, batch: 14, loss: 1.6423019170761108, acc: 40.625, f1: 11.615210843373495, r: 0.18932259885018762
06/02/2019 03:50:08 step: 5894, epoch: 178, batch: 19, loss: 1.730608344078064, acc: 23.4375, f1: 7.930735930735931, r: 0.1372630343071006
06/02/2019 03:50:08 step: 5899, epoch: 178, batch: 24, loss: 1.4410293102264404, acc: 50.0, f1: 13.916532905296949, r: 0.2086337885457449
06/02/2019 03:50:09 step: 5904, epoch: 178, batch: 29, loss: 1.6519521474838257, acc: 37.5, f1: 7.792207792207792, r: 0.1623657565542747
06/02/2019 03:50:09 *** evaluating ***
06/02/2019 03:50:09 step: 179, epoch: 178, acc: 57.692307692307686, f1: 17.394976498237366, r: 0.24340149160375812
06/02/2019 03:50:09 *** epoch: 180 ***
06/02/2019 03:50:09 *** training ***
06/02/2019 03:50:09 step: 5912, epoch: 179, batch: 4, loss: 1.4886503219604492, acc: 46.875, f1: 15.238095238095237, r: 0.14951857794341392
06/02/2019 03:50:10 step: 5917, epoch: 179, batch: 9, loss: 1.4237148761749268, acc: 43.75, f1: 12.719506407214048, r: 0.21981120090251594
06/02/2019 03:50:10 step: 5922, epoch: 179, batch: 14, loss: 1.4331084489822388, acc: 54.6875, f1: 19.683930527304025, r: 0.1897908641276127
06/02/2019 03:50:10 step: 5927, epoch: 179, batch: 19, loss: 1.3870093822479248, acc: 48.4375, f1: 17.6284083703234, r: 0.23670426089219723
06/02/2019 03:50:11 step: 5932, epoch: 179, batch: 24, loss: 1.409746766090393, acc: 45.3125, f1: 13.412473423104181, r: 0.21126237647993168
06/02/2019 03:50:11 step: 5937, epoch: 179, batch: 29, loss: 1.5803865194320679, acc: 40.625, f1: 16.258503401360542, r: 0.19527146501381962
06/02/2019 03:50:11 *** evaluating ***
06/02/2019 03:50:11 step: 180, epoch: 179, acc: 55.98290598290598, f1: 16.612602535421328, r: 0.24442349841857663
06/02/2019 03:50:11 *** epoch: 181 ***
06/02/2019 03:50:11 *** training ***
06/02/2019 03:50:11 step: 5945, epoch: 180, batch: 4, loss: 1.3664638996124268, acc: 48.4375, f1: 13.111995104039167, r: 0.17371581946418596
06/02/2019 03:50:12 step: 5950, epoch: 180, batch: 9, loss: 1.5197553634643555, acc: 43.75, f1: 17.537746806039486, r: 0.16848380885206568
06/02/2019 03:50:12 step: 5955, epoch: 180, batch: 14, loss: 1.4726784229278564, acc: 51.5625, f1: 16.44518272425249, r: 0.1733722620499999
06/02/2019 03:50:12 step: 5960, epoch: 180, batch: 19, loss: 1.5001176595687866, acc: 48.4375, f1: 14.019314019314018, r: 0.12972891896930522
06/02/2019 03:50:13 step: 5965, epoch: 180, batch: 24, loss: 1.5221418142318726, acc: 39.0625, f1: 11.286764705882351, r: 0.2232923854671167
06/02/2019 03:50:13 step: 5970, epoch: 180, batch: 29, loss: 1.5911104679107666, acc: 40.625, f1: 9.106984969053935, r: 0.13379518738201898
06/02/2019 03:50:13 *** evaluating ***
06/02/2019 03:50:14 step: 181, epoch: 180, acc: 55.98290598290598, f1: 16.612602535421328, r: 0.2340623481684658
06/02/2019 03:50:14 *** epoch: 182 ***
06/02/2019 03:50:14 *** training ***
06/02/2019 03:50:14 step: 5978, epoch: 181, batch: 4, loss: 1.4733097553253174, acc: 48.4375, f1: 15.36195286195286, r: 0.1903959768371369
06/02/2019 03:50:14 step: 5983, epoch: 181, batch: 9, loss: 1.558302879333496, acc: 45.3125, f1: 13.250773993808048, r: 0.128026804457208
06/02/2019 03:50:14 step: 5988, epoch: 181, batch: 14, loss: 1.5824776887893677, acc: 40.625, f1: 12.98076923076923, r: 0.21785886481587377
06/02/2019 03:50:15 step: 5993, epoch: 181, batch: 19, loss: 1.5270484685897827, acc: 42.1875, f1: 14.076655052264808, r: 0.18900142349807747
06/02/2019 03:50:15 step: 5998, epoch: 181, batch: 24, loss: 1.4564849138259888, acc: 48.4375, f1: 14.098837209302328, r: 0.20255367167073685
06/02/2019 03:50:15 step: 6003, epoch: 181, batch: 29, loss: 1.3594859838485718, acc: 57.8125, f1: 17.980115122972265, r: 0.18064629236749763
06/02/2019 03:50:15 *** evaluating ***
06/02/2019 03:50:15 step: 182, epoch: 181, acc: 55.98290598290598, f1: 16.612602535421328, r: 0.22696535385964506
06/02/2019 03:50:15 *** epoch: 183 ***
06/02/2019 03:50:15 *** training ***
06/02/2019 03:50:16 step: 6011, epoch: 182, batch: 4, loss: 1.4068690538406372, acc: 45.3125, f1: 13.99501661129568, r: 0.13675099754950798
06/02/2019 03:50:16 step: 6016, epoch: 182, batch: 9, loss: 1.499485731124878, acc: 48.4375, f1: 14.920634920634921, r: 0.18568007354054752
06/02/2019 03:50:16 step: 6021, epoch: 182, batch: 14, loss: 1.4263449907302856, acc: 46.875, f1: 14.294996751137102, r: 0.29375677703612446
06/02/2019 03:50:17 step: 6026, epoch: 182, batch: 19, loss: 1.5786705017089844, acc: 42.1875, f1: 9.481216457960643, r: 0.1619132367692444
06/02/2019 03:50:17 step: 6031, epoch: 182, batch: 24, loss: 1.3774371147155762, acc: 50.0, f1: 13.333333333333334, r: 0.23458347229363244
06/02/2019 03:50:17 step: 6036, epoch: 182, batch: 29, loss: 1.3122339248657227, acc: 54.6875, f1: 17.79781680113906, r: 0.2663085424575682
06/02/2019 03:50:17 *** evaluating ***
06/02/2019 03:50:18 step: 183, epoch: 182, acc: 57.692307692307686, f1: 17.38728219867074, r: 0.2596545580984984
06/02/2019 03:50:18 *** epoch: 184 ***
06/02/2019 03:50:18 *** training ***
06/02/2019 03:50:18 step: 6044, epoch: 183, batch: 4, loss: 1.314345359802246, acc: 50.0, f1: 15.016611295681065, r: 0.21215477141748762
06/02/2019 03:50:18 step: 6049, epoch: 183, batch: 9, loss: 1.4063243865966797, acc: 48.4375, f1: 18.362796623666192, r: 0.21386762607875348
06/02/2019 03:50:19 step: 6054, epoch: 183, batch: 14, loss: 1.6022670269012451, acc: 40.625, f1: 12.865146058423369, r: 0.16661376824367793
06/02/2019 03:50:19 step: 6059, epoch: 183, batch: 19, loss: 1.4728041887283325, acc: 43.75, f1: 16.783216783216783, r: 0.2338766797706706
06/02/2019 03:50:19 step: 6064, epoch: 183, batch: 24, loss: 1.69204843044281, acc: 35.9375, f1: 10.197368421052634, r: 0.210300059419432
06/02/2019 03:50:20 step: 6069, epoch: 183, batch: 29, loss: 1.4471971988677979, acc: 40.625, f1: 10.807583221376325, r: 0.1870203755649897
06/02/2019 03:50:20 *** evaluating ***
06/02/2019 03:50:20 step: 184, epoch: 183, acc: 55.98290598290598, f1: 16.612602535421328, r: 0.24210300450732616
06/02/2019 03:50:20 *** epoch: 185 ***
06/02/2019 03:50:20 *** training ***
06/02/2019 03:50:20 step: 6077, epoch: 184, batch: 4, loss: 1.5639185905456543, acc: 43.75, f1: 13.567073170731708, r: 0.23637584149548493
06/02/2019 03:50:20 step: 6082, epoch: 184, batch: 9, loss: 1.4854241609573364, acc: 45.3125, f1: 12.337662337662337, r: 0.20586817296875393
06/02/2019 03:50:21 step: 6087, epoch: 184, batch: 14, loss: 1.4353896379470825, acc: 50.0, f1: 16.218487394957982, r: 0.17404233368736582
06/02/2019 03:50:21 step: 6092, epoch: 184, batch: 19, loss: 1.4813657999038696, acc: 48.4375, f1: 13.204334365325076, r: 0.21759858099778376
06/02/2019 03:50:21 step: 6097, epoch: 184, batch: 24, loss: 1.6401748657226562, acc: 39.0625, f1: 11.488812392426851, r: 0.0792464261683657
06/02/2019 03:50:22 step: 6102, epoch: 184, batch: 29, loss: 1.4060473442077637, acc: 39.0625, f1: 16.018306636155607, r: 0.260241591345901
06/02/2019 03:50:22 *** evaluating ***
06/02/2019 03:50:22 step: 185, epoch: 184, acc: 56.41025641025641, f1: 17.010309278350512, r: 0.2482400149660997
06/02/2019 03:50:22 *** epoch: 186 ***
06/02/2019 03:50:22 *** training ***
06/02/2019 03:50:22 step: 6110, epoch: 185, batch: 4, loss: 1.6481584310531616, acc: 43.75, f1: 14.55128205128205, r: 0.21407679881754815
06/02/2019 03:50:22 step: 6115, epoch: 185, batch: 9, loss: 1.4753286838531494, acc: 46.875, f1: 14.48051948051948, r: 0.26564970050518905
06/02/2019 03:50:23 step: 6120, epoch: 185, batch: 14, loss: 1.6560323238372803, acc: 40.625, f1: 11.875, r: 0.21338045571106812
06/02/2019 03:50:23 step: 6125, epoch: 185, batch: 19, loss: 1.4820542335510254, acc: 51.5625, f1: 19.146825396825403, r: 0.24339430989960517
06/02/2019 03:50:23 step: 6130, epoch: 185, batch: 24, loss: 1.5972521305084229, acc: 35.9375, f1: 6.764705882352941, r: 0.17933477067282866
06/02/2019 03:50:24 step: 6135, epoch: 185, batch: 29, loss: 1.4624297618865967, acc: 45.3125, f1: 13.139825218476902, r: 0.23188557701704912
06/02/2019 03:50:24 *** evaluating ***
06/02/2019 03:50:24 step: 186, epoch: 185, acc: 57.692307692307686, f1: 17.313071193937624, r: 0.2544225988085521
06/02/2019 03:50:24 *** epoch: 187 ***
06/02/2019 03:50:24 *** training ***
06/02/2019 03:50:24 step: 6143, epoch: 186, batch: 4, loss: 1.5211329460144043, acc: 43.75, f1: 13.26530612244898, r: 0.2397113958399991
06/02/2019 03:50:25 step: 6148, epoch: 186, batch: 9, loss: 1.4361107349395752, acc: 45.3125, f1: 15.410602910602911, r: 0.23169775188346628
06/02/2019 03:50:25 step: 6153, epoch: 186, batch: 14, loss: 1.704898715019226, acc: 29.6875, f1: 9.025974025974026, r: 0.12224779072961256
06/02/2019 03:50:25 step: 6158, epoch: 186, batch: 19, loss: 1.5176193714141846, acc: 42.1875, f1: 13.666666666666666, r: 0.1451245105692985
06/02/2019 03:50:26 step: 6163, epoch: 186, batch: 24, loss: 1.4656890630722046, acc: 51.5625, f1: 13.752052545155994, r: 0.08447173706450443
06/02/2019 03:50:26 step: 6168, epoch: 186, batch: 29, loss: 1.4680564403533936, acc: 46.875, f1: 15.513522482163596, r: 0.14698444459798796
06/02/2019 03:50:26 *** evaluating ***
06/02/2019 03:50:26 step: 187, epoch: 186, acc: 56.837606837606835, f1: 17.099073672022, r: 0.25988798762024934
06/02/2019 03:50:26 *** epoch: 188 ***
06/02/2019 03:50:26 *** training ***
06/02/2019 03:50:27 step: 6176, epoch: 187, batch: 4, loss: 1.5334805250167847, acc: 40.625, f1: 14.305806710870003, r: 0.17862542160838013
06/02/2019 03:50:27 step: 6181, epoch: 187, batch: 9, loss: 1.6106959581375122, acc: 40.625, f1: 10.548172757475083, r: 0.13981097663993514
06/02/2019 03:50:27 step: 6186, epoch: 187, batch: 14, loss: 1.6925119161605835, acc: 34.375, f1: 10.38961038961039, r: 0.17451727091193706
06/02/2019 03:50:27 step: 6191, epoch: 187, batch: 19, loss: 1.4606519937515259, acc: 43.75, f1: 15.029134016475787, r: 0.26992859535849023
06/02/2019 03:50:28 step: 6196, epoch: 187, batch: 24, loss: 1.5057387351989746, acc: 46.875, f1: 14.550264550264549, r: 0.2542542723631001
06/02/2019 03:50:28 step: 6201, epoch: 187, batch: 29, loss: 1.6117990016937256, acc: 39.0625, f1: 12.660256410256409, r: 0.23233114312116224
06/02/2019 03:50:28 *** evaluating ***
06/02/2019 03:50:28 step: 188, epoch: 187, acc: 56.837606837606835, f1: 17.099073672022, r: 0.2673101712549348
06/02/2019 03:50:28 *** epoch: 189 ***
06/02/2019 03:50:28 *** training ***
06/02/2019 03:50:28 step: 6209, epoch: 188, batch: 4, loss: 1.3871091604232788, acc: 53.125, f1: 14.33349259436216, r: 0.2213424938107404
06/02/2019 03:50:29 step: 6214, epoch: 188, batch: 9, loss: 1.5756380558013916, acc: 46.875, f1: 17.63191763191763, r: 0.17118128295863194
06/02/2019 03:50:29 step: 6219, epoch: 188, batch: 14, loss: 1.4590145349502563, acc: 37.5, f1: 11.087768440709617, r: 0.22795958539989605
06/02/2019 03:50:30 step: 6224, epoch: 188, batch: 19, loss: 1.479457139968872, acc: 48.4375, f1: 13.095238095238097, r: 0.21319533209493802
06/02/2019 03:50:30 step: 6229, epoch: 188, batch: 24, loss: 1.5605897903442383, acc: 45.3125, f1: 11.694991055456171, r: 0.16559473470259659
06/02/2019 03:50:30 step: 6234, epoch: 188, batch: 29, loss: 1.4120221138000488, acc: 46.875, f1: 16.04503870513723, r: 0.2784580047830476
06/02/2019 03:50:31 *** evaluating ***
06/02/2019 03:50:31 step: 189, epoch: 188, acc: 56.837606837606835, f1: 17.1023421023421, r: 0.2622445375557794
06/02/2019 03:50:31 *** epoch: 190 ***
06/02/2019 03:50:31 *** training ***
06/02/2019 03:50:31 step: 6242, epoch: 189, batch: 4, loss: 1.4636321067810059, acc: 46.875, f1: 12.798287854467633, r: 0.1981947000181939
06/02/2019 03:50:31 step: 6247, epoch: 189, batch: 9, loss: 1.4010396003723145, acc: 50.0, f1: 15.238095238095237, r: 0.2552202968436022
06/02/2019 03:50:32 step: 6252, epoch: 189, batch: 14, loss: 1.4912978410720825, acc: 42.1875, f1: 14.791073124406456, r: 0.30669812869966234
06/02/2019 03:50:32 step: 6257, epoch: 189, batch: 19, loss: 1.7228881120681763, acc: 35.9375, f1: 7.55336617405583, r: 0.103797453271484
06/02/2019 03:50:32 step: 6262, epoch: 189, batch: 24, loss: 1.5356968641281128, acc: 39.0625, f1: 12.682926829268293, r: 0.21475367121375447
06/02/2019 03:50:33 step: 6267, epoch: 189, batch: 29, loss: 1.4577404260635376, acc: 46.875, f1: 16.843192973391886, r: 0.22813048594636462
06/02/2019 03:50:33 *** evaluating ***
06/02/2019 03:50:33 step: 190, epoch: 189, acc: 57.26495726495726, f1: 17.40239767921429, r: 0.2441397771766476
06/02/2019 03:50:33 *** epoch: 191 ***
06/02/2019 03:50:33 *** training ***
06/02/2019 03:50:33 step: 6275, epoch: 190, batch: 4, loss: 1.4929908514022827, acc: 48.4375, f1: 12.601010101010102, r: 0.1584846585843513
06/02/2019 03:50:33 step: 6280, epoch: 190, batch: 9, loss: 1.2593519687652588, acc: 56.25, f1: 19.856459330143544, r: 0.22556213591479363
06/02/2019 03:50:34 step: 6285, epoch: 190, batch: 14, loss: 1.531845211982727, acc: 40.625, f1: 14.039832800590116, r: 0.20666282703611877
06/02/2019 03:50:34 step: 6290, epoch: 190, batch: 19, loss: 1.364895224571228, acc: 48.4375, f1: 18.689885592791335, r: 0.22637602977362958
06/02/2019 03:50:34 step: 6295, epoch: 190, batch: 24, loss: 1.3290718793869019, acc: 50.0, f1: 15.799755799755802, r: 0.20157421801503464
06/02/2019 03:50:35 step: 6300, epoch: 190, batch: 29, loss: 1.5776801109313965, acc: 37.5, f1: 11.091018685955396, r: 0.20666857267473307
06/02/2019 03:50:35 *** evaluating ***
06/02/2019 03:50:35 step: 191, epoch: 190, acc: 56.41025641025641, f1: 16.748806406899742, r: 0.2286213134259135
06/02/2019 03:50:35 *** epoch: 192 ***
06/02/2019 03:50:35 *** training ***
06/02/2019 03:50:35 step: 6308, epoch: 191, batch: 4, loss: 1.460473895072937, acc: 53.125, f1: 11.461988304093566, r: 0.13307903186436426
06/02/2019 03:50:36 step: 6313, epoch: 191, batch: 9, loss: 1.667384386062622, acc: 31.25, f1: 12.817927170868346, r: 0.1636232386049221
06/02/2019 03:50:36 step: 6318, epoch: 191, batch: 14, loss: 1.4419989585876465, acc: 40.625, f1: 12.731481481481483, r: 0.26376468050030794
06/02/2019 03:50:36 step: 6323, epoch: 191, batch: 19, loss: 1.3609223365783691, acc: 54.6875, f1: 16.53129479216436, r: 0.224211421772641
06/02/2019 03:50:37 step: 6328, epoch: 191, batch: 24, loss: 1.4376106262207031, acc: 54.6875, f1: 15.573089700996679, r: 0.14599338637319303
06/02/2019 03:50:37 step: 6333, epoch: 191, batch: 29, loss: 1.4329584836959839, acc: 45.3125, f1: 16.587301587301585, r: 0.17267379914862876
06/02/2019 03:50:37 *** evaluating ***
06/02/2019 03:50:37 step: 192, epoch: 191, acc: 56.837606837606835, f1: 17.03877790834313, r: 0.256304301261159
06/02/2019 03:50:37 *** epoch: 193 ***
06/02/2019 03:50:37 *** training ***
06/02/2019 03:50:38 step: 6341, epoch: 192, batch: 4, loss: 1.5555901527404785, acc: 42.1875, f1: 13.873626373626374, r: 0.22113625121130226
06/02/2019 03:50:38 step: 6346, epoch: 192, batch: 9, loss: 1.4634712934494019, acc: 42.1875, f1: 10.919540229885056, r: 0.19641651269143304
06/02/2019 03:50:38 step: 6351, epoch: 192, batch: 14, loss: 1.5645751953125, acc: 39.0625, f1: 13.78795420607267, r: 0.21640482205494424
06/02/2019 03:50:39 step: 6356, epoch: 192, batch: 19, loss: 1.5073883533477783, acc: 43.75, f1: 14.10488245931284, r: 0.2628208664485082
06/02/2019 03:50:39 step: 6361, epoch: 192, batch: 24, loss: 1.597752332687378, acc: 39.0625, f1: 9.623015873015872, r: 0.1734473348735726
06/02/2019 03:50:39 step: 6366, epoch: 192, batch: 29, loss: 1.4300596714019775, acc: 53.125, f1: 14.772727272727273, r: 0.18763645411875382
06/02/2019 03:50:39 *** evaluating ***
06/02/2019 03:50:40 step: 193, epoch: 192, acc: 56.41025641025641, f1: 16.955394385780316, r: 0.25509192990710783
06/02/2019 03:50:40 *** epoch: 194 ***
06/02/2019 03:50:40 *** training ***
06/02/2019 03:50:40 step: 6374, epoch: 193, batch: 4, loss: 1.5171856880187988, acc: 43.75, f1: 14.076655052264808, r: 0.13887328807554256
06/02/2019 03:50:40 step: 6379, epoch: 193, batch: 9, loss: 1.4893934726715088, acc: 53.125, f1: 16.760299625468164, r: 0.30662429375991834
06/02/2019 03:50:41 step: 6384, epoch: 193, batch: 14, loss: 1.3047938346862793, acc: 54.6875, f1: 24.099514942888437, r: 0.19614932661386855
06/02/2019 03:50:41 step: 6389, epoch: 193, batch: 19, loss: 1.4683724641799927, acc: 48.4375, f1: 12.865168539325843, r: 0.20661014310793638
06/02/2019 03:50:41 step: 6394, epoch: 193, batch: 24, loss: 1.568712830543518, acc: 42.1875, f1: 11.519607843137255, r: 0.2584828243677435
06/02/2019 03:50:42 step: 6399, epoch: 193, batch: 29, loss: 1.7012401819229126, acc: 34.375, f1: 11.476608187134502, r: 0.17000499011343315
06/02/2019 03:50:42 *** evaluating ***
06/02/2019 03:50:42 step: 194, epoch: 193, acc: 57.692307692307686, f1: 17.466767625778225, r: 0.24211047589855497
06/02/2019 03:50:42 *** epoch: 195 ***
06/02/2019 03:50:42 *** training ***
06/02/2019 03:50:42 step: 6407, epoch: 194, batch: 4, loss: 1.411086916923523, acc: 50.0, f1: 19.21495505832855, r: 0.22861929504617495
06/02/2019 03:50:43 step: 6412, epoch: 194, batch: 9, loss: 1.4829094409942627, acc: 43.75, f1: 13.50140056022409, r: 0.2618751747356475
06/02/2019 03:50:43 step: 6417, epoch: 194, batch: 14, loss: 1.5343477725982666, acc: 45.3125, f1: 14.583333333333334, r: 0.22638750374782857
06/02/2019 03:50:43 step: 6422, epoch: 194, batch: 19, loss: 1.5758633613586426, acc: 43.75, f1: 14.540441176470587, r: 0.16600683359030208
06/02/2019 03:50:44 step: 6427, epoch: 194, batch: 24, loss: 1.6110395193099976, acc: 34.375, f1: 12.393320964749536, r: 0.2265566155974463
06/02/2019 03:50:44 step: 6432, epoch: 194, batch: 29, loss: 1.4255235195159912, acc: 50.0, f1: 16.558441558441558, r: 0.23717860787188774
06/02/2019 03:50:44 *** evaluating ***
06/02/2019 03:50:44 step: 195, epoch: 194, acc: 57.26495726495726, f1: 17.246218334244578, r: 0.2673680111840233
06/02/2019 03:50:44 *** epoch: 196 ***
06/02/2019 03:50:44 *** training ***
06/02/2019 03:50:45 step: 6440, epoch: 195, batch: 4, loss: 1.6086276769638062, acc: 50.0, f1: 14.488636363636365, r: 0.10971641501593668
06/02/2019 03:50:45 step: 6445, epoch: 195, batch: 9, loss: 1.4347355365753174, acc: 45.3125, f1: 19.215686274509807, r: 0.29960411094452766
06/02/2019 03:50:45 step: 6450, epoch: 195, batch: 14, loss: 1.391893982887268, acc: 46.875, f1: 13.482501434308663, r: 0.2586294554594224
06/02/2019 03:50:45 step: 6455, epoch: 195, batch: 19, loss: 1.3833448886871338, acc: 54.6875, f1: 18.559390048154093, r: 0.21556026695780217
06/02/2019 03:50:46 step: 6460, epoch: 195, batch: 24, loss: 1.6424940824508667, acc: 34.375, f1: 8.173076923076923, r: 0.12711885119401378
06/02/2019 03:50:46 step: 6465, epoch: 195, batch: 29, loss: 1.4493122100830078, acc: 46.875, f1: 11.092032967032967, r: 0.15013330755725673
06/02/2019 03:50:46 *** evaluating ***
06/02/2019 03:50:46 step: 196, epoch: 195, acc: 57.26495726495726, f1: 17.246218334244578, r: 0.25327374738015446
06/02/2019 03:50:46 *** epoch: 197 ***
06/02/2019 03:50:46 *** training ***
06/02/2019 03:50:47 step: 6473, epoch: 196, batch: 4, loss: 1.3050655126571655, acc: 45.3125, f1: 18.902131945610208, r: 0.26712906466116504
06/02/2019 03:50:47 step: 6478, epoch: 196, batch: 9, loss: 1.5251494646072388, acc: 48.4375, f1: 13.705395727867636, r: 0.21548950739531084
06/02/2019 03:50:47 step: 6483, epoch: 196, batch: 14, loss: 1.519446611404419, acc: 39.0625, f1: 12.802157900197116, r: 0.22815472487775124
06/02/2019 03:50:48 step: 6488, epoch: 196, batch: 19, loss: 1.5730429887771606, acc: 42.1875, f1: 11.286630036630036, r: 0.19536850987841306
06/02/2019 03:50:48 step: 6493, epoch: 196, batch: 24, loss: 1.57691490650177, acc: 48.4375, f1: 13.020833333333332, r: 0.16163885089992439
06/02/2019 03:50:49 step: 6498, epoch: 196, batch: 29, loss: 1.538643717765808, acc: 40.625, f1: 13.107682097229134, r: 0.1968514720496386
06/02/2019 03:50:49 *** evaluating ***
06/02/2019 03:50:49 step: 197, epoch: 196, acc: 56.41025641025641, f1: 16.95347583977007, r: 0.2695117317746274
06/02/2019 03:50:49 *** epoch: 198 ***
06/02/2019 03:50:49 *** training ***
06/02/2019 03:50:49 step: 6506, epoch: 197, batch: 4, loss: 1.3013854026794434, acc: 53.125, f1: 17.14975845410628, r: 0.1794286901085146
06/02/2019 03:50:50 step: 6511, epoch: 197, batch: 9, loss: 1.4854015111923218, acc: 43.75, f1: 17.045961624274874, r: 0.16652875307682063
06/02/2019 03:50:50 step: 6516, epoch: 197, batch: 14, loss: 1.4534651041030884, acc: 45.3125, f1: 15.374149659863946, r: 0.12591006182101985
06/02/2019 03:50:50 step: 6521, epoch: 197, batch: 19, loss: 1.446961522102356, acc: 46.875, f1: 14.754533392304289, r: 0.2017278847864532
06/02/2019 03:50:50 step: 6526, epoch: 197, batch: 24, loss: 1.4508485794067383, acc: 43.75, f1: 13.860544217687073, r: 0.13162566731924763
06/02/2019 03:50:51 step: 6531, epoch: 197, batch: 29, loss: 1.5965449810028076, acc: 37.5, f1: 13.815789473684209, r: 0.19182786398044793
06/02/2019 03:50:51 *** evaluating ***
06/02/2019 03:50:51 step: 198, epoch: 197, acc: 58.119658119658126, f1: 17.449313276651406, r: 0.27819421628248037
06/02/2019 03:50:51 *** epoch: 199 ***
06/02/2019 03:50:51 *** training ***
06/02/2019 03:50:51 step: 6539, epoch: 198, batch: 4, loss: 1.542354702949524, acc: 48.4375, f1: 14.819466248037674, r: 0.18150032013566408
06/02/2019 03:50:52 step: 6544, epoch: 198, batch: 9, loss: 1.358466625213623, acc: 50.0, f1: 17.93406593406593, r: 0.1845971348831676
06/02/2019 03:50:52 step: 6549, epoch: 198, batch: 14, loss: 1.523101806640625, acc: 45.3125, f1: 12.915282392026578, r: 0.18500739255875995
06/02/2019 03:50:52 step: 6554, epoch: 198, batch: 19, loss: 1.623979926109314, acc: 35.9375, f1: 12.171052631578947, r: 0.24715042426451225
06/02/2019 03:50:53 step: 6559, epoch: 198, batch: 24, loss: 1.3880751132965088, acc: 53.125, f1: 19.495798319327733, r: 0.23456670288591397
06/02/2019 03:50:53 step: 6564, epoch: 198, batch: 29, loss: 1.40921151638031, acc: 42.1875, f1: 16.26050420168067, r: 0.20143740322922268
06/02/2019 03:50:53 *** evaluating ***
06/02/2019 03:50:53 step: 199, epoch: 198, acc: 58.119658119658126, f1: 17.535099146041393, r: 0.293770494763039
06/02/2019 03:50:53 *** epoch: 200 ***
06/02/2019 03:50:53 *** training ***
06/02/2019 03:50:54 step: 6572, epoch: 199, batch: 4, loss: 1.4274932146072388, acc: 45.3125, f1: 15.997322623828644, r: 0.1938766279132926
06/02/2019 03:50:54 step: 6577, epoch: 199, batch: 9, loss: 1.4997678995132446, acc: 39.0625, f1: 13.84920634920635, r: 0.25835784088547786
06/02/2019 03:50:54 step: 6582, epoch: 199, batch: 14, loss: 1.4789775609970093, acc: 43.75, f1: 13.478915662650603, r: 0.3176152961771723
06/02/2019 03:50:55 step: 6587, epoch: 199, batch: 19, loss: 1.4408875703811646, acc: 43.75, f1: 13.34766464725425, r: 0.18338696395254184
06/02/2019 03:50:55 step: 6592, epoch: 199, batch: 24, loss: 1.3447818756103516, acc: 53.125, f1: 15.54531490015361, r: 0.22848020983328787
06/02/2019 03:50:55 step: 6597, epoch: 199, batch: 29, loss: 1.5028964281082153, acc: 43.75, f1: 14.451706608569353, r: 0.2374745770883436
06/02/2019 03:50:55 *** evaluating ***
06/02/2019 03:50:56 step: 200, epoch: 199, acc: 56.837606837606835, f1: 17.14987242668904, r: 0.2517255105796268
06/02/2019 03:50:56 *** epoch: 201 ***
06/02/2019 03:50:56 *** training ***
06/02/2019 03:50:56 step: 6605, epoch: 200, batch: 4, loss: 1.4965156316757202, acc: 43.75, f1: 11.623376623376625, r: 0.14708073746612035
06/02/2019 03:50:56 step: 6610, epoch: 200, batch: 9, loss: 1.571385383605957, acc: 40.625, f1: 12.225183211192538, r: 0.2181699762702374
06/02/2019 03:50:56 step: 6615, epoch: 200, batch: 14, loss: 1.5322718620300293, acc: 43.75, f1: 13.036672629695886, r: 0.21635112758639888
06/02/2019 03:50:57 step: 6620, epoch: 200, batch: 19, loss: 1.4377084970474243, acc: 45.3125, f1: 13.628899835796387, r: 0.24688716604785857
06/02/2019 03:50:57 step: 6625, epoch: 200, batch: 24, loss: 1.336749792098999, acc: 46.875, f1: 18.51851851851852, r: 0.3304190009665976
06/02/2019 03:50:57 step: 6630, epoch: 200, batch: 29, loss: 1.4103995561599731, acc: 48.4375, f1: 15.549482163406214, r: 0.2550835415145239
06/02/2019 03:50:57 *** evaluating ***
06/02/2019 03:50:58 step: 201, epoch: 200, acc: 57.692307692307686, f1: 17.53458446889985, r: 0.2538609801129979
06/02/2019 03:50:58 *** epoch: 202 ***
06/02/2019 03:50:58 *** training ***
06/02/2019 03:50:58 step: 6638, epoch: 201, batch: 4, loss: 1.4865669012069702, acc: 43.75, f1: 10.80705009276438, r: 0.14964388928697347
06/02/2019 03:50:58 step: 6643, epoch: 201, batch: 9, loss: 1.3942193984985352, acc: 48.4375, f1: 14.191176470588236, r: 0.24883660836551225
06/02/2019 03:50:58 step: 6648, epoch: 201, batch: 14, loss: 1.5753719806671143, acc: 45.3125, f1: 16.566626650660265, r: 0.16028203529281967
06/02/2019 03:50:59 step: 6653, epoch: 201, batch: 19, loss: 1.4344723224639893, acc: 46.875, f1: 15.891472868217054, r: 0.23882821790952105
06/02/2019 03:50:59 step: 6658, epoch: 201, batch: 24, loss: 1.3758336305618286, acc: 48.4375, f1: 15.151515151515152, r: 0.24492903398764648
06/02/2019 03:50:59 step: 6663, epoch: 201, batch: 29, loss: 1.571540355682373, acc: 42.1875, f1: 12.810063784549964, r: 0.1651699631050768
06/02/2019 03:50:59 *** evaluating ***
06/02/2019 03:51:00 step: 202, epoch: 201, acc: 57.26495726495726, f1: 17.246218334244578, r: 0.25579513589767555
06/02/2019 03:51:00 *** epoch: 203 ***
06/02/2019 03:51:00 *** training ***
06/02/2019 03:51:00 step: 6671, epoch: 202, batch: 4, loss: 1.4353415966033936, acc: 40.625, f1: 15.531444645368694, r: 0.2889185029240027
06/02/2019 03:51:00 step: 6676, epoch: 202, batch: 9, loss: 1.5286206007003784, acc: 45.3125, f1: 14.00568181818182, r: 0.21223981253035637
06/02/2019 03:51:01 step: 6681, epoch: 202, batch: 14, loss: 1.523546814918518, acc: 48.4375, f1: 15.17857142857143, r: 0.18722132669216934
06/02/2019 03:51:01 step: 6686, epoch: 202, batch: 19, loss: 1.5956693887710571, acc: 42.1875, f1: 14.390432098765432, r: 0.2312772117113904
06/02/2019 03:51:01 step: 6691, epoch: 202, batch: 24, loss: 1.498321294784546, acc: 50.0, f1: 9.72644376899696, r: 0.16101122986098804
06/02/2019 03:51:02 step: 6696, epoch: 202, batch: 29, loss: 1.443988561630249, acc: 40.625, f1: 14.8109243697479, r: 0.259263461194515
06/02/2019 03:51:02 *** evaluating ***
06/02/2019 03:51:02 step: 203, epoch: 202, acc: 57.692307692307686, f1: 17.394976498237366, r: 0.2626331224946833
06/02/2019 03:51:02 *** epoch: 204 ***
06/02/2019 03:51:02 *** training ***
06/02/2019 03:51:02 step: 6704, epoch: 203, batch: 4, loss: 1.5134986639022827, acc: 39.0625, f1: 12.176783421969557, r: 0.24811960552396606
06/02/2019 03:51:02 step: 6709, epoch: 203, batch: 9, loss: 1.4166754484176636, acc: 46.875, f1: 13.80813953488372, r: 0.24778625577468763
06/02/2019 03:51:03 step: 6714, epoch: 203, batch: 14, loss: 1.6131553649902344, acc: 35.9375, f1: 12.97474455369192, r: 0.2629739407212026
06/02/2019 03:51:03 step: 6719, epoch: 203, batch: 19, loss: 1.4095696210861206, acc: 51.5625, f1: 20.504201680672267, r: 0.24266784942006694
06/02/2019 03:51:03 step: 6724, epoch: 203, batch: 24, loss: 1.4067399501800537, acc: 46.875, f1: 16.954474097331236, r: 0.21699671082616168
06/02/2019 03:51:04 step: 6729, epoch: 203, batch: 29, loss: 1.409524917602539, acc: 43.75, f1: 12.016718913270639, r: 0.19663680055039856
06/02/2019 03:51:04 *** evaluating ***
06/02/2019 03:51:04 step: 204, epoch: 203, acc: 57.26495726495726, f1: 17.287059716424604, r: 0.24992681998556854
06/02/2019 03:51:04 *** epoch: 205 ***
06/02/2019 03:51:04 *** training ***
06/02/2019 03:51:04 step: 6737, epoch: 204, batch: 4, loss: 1.6212199926376343, acc: 45.3125, f1: 17.142857142857146, r: 0.25497586219968355
06/02/2019 03:51:04 step: 6742, epoch: 204, batch: 9, loss: 1.6213353872299194, acc: 34.375, f1: 12.316849816849814, r: 0.19372493179520908
06/02/2019 03:51:05 step: 6747, epoch: 204, batch: 14, loss: 1.5500468015670776, acc: 39.0625, f1: 11.927710843373495, r: 0.22932386241150315
06/02/2019 03:51:05 step: 6752, epoch: 204, batch: 19, loss: 1.3771079778671265, acc: 45.3125, f1: 11.72920892494929, r: 0.22312348641545235
06/02/2019 03:51:05 step: 6757, epoch: 204, batch: 24, loss: 1.3726528882980347, acc: 51.5625, f1: 13.574317492416583, r: 0.20853210596754315
06/02/2019 03:51:06 step: 6762, epoch: 204, batch: 29, loss: 1.3545891046524048, acc: 45.3125, f1: 12.575757575757576, r: 0.2984133877306932
06/02/2019 03:51:06 *** evaluating ***
06/02/2019 03:51:06 step: 205, epoch: 204, acc: 57.26495726495726, f1: 17.343750000000004, r: 0.2627370844498536
06/02/2019 03:51:06 *** epoch: 206 ***
06/02/2019 03:51:06 *** training ***
06/02/2019 03:51:06 step: 6770, epoch: 205, batch: 4, loss: 1.512830376625061, acc: 42.1875, f1: 14.266590170204626, r: 0.1443303617732907
06/02/2019 03:51:07 step: 6775, epoch: 205, batch: 9, loss: 1.5015196800231934, acc: 43.75, f1: 13.950742240215924, r: 0.2165286145998939
06/02/2019 03:51:07 step: 6780, epoch: 205, batch: 14, loss: 1.4028644561767578, acc: 48.4375, f1: 14.41027266962587, r: 0.20597836871527928
06/02/2019 03:51:07 step: 6785, epoch: 205, batch: 19, loss: 1.3371490240097046, acc: 48.4375, f1: 15.861954537912727, r: 0.2920994674972846
06/02/2019 03:51:08 step: 6790, epoch: 205, batch: 24, loss: 1.5119574069976807, acc: 40.625, f1: 11.532738095238095, r: 0.18405187393761932
06/02/2019 03:51:08 step: 6795, epoch: 205, batch: 29, loss: 1.2482603788375854, acc: 56.25, f1: 20.27421494913755, r: 0.25537293472987443
06/02/2019 03:51:08 *** evaluating ***
06/02/2019 03:51:08 step: 206, epoch: 205, acc: 56.837606837606835, f1: 17.03877790834313, r: 0.28132786261259696
06/02/2019 03:51:08 *** epoch: 207 ***
06/02/2019 03:51:08 *** training ***
06/02/2019 03:51:09 step: 6803, epoch: 206, batch: 4, loss: 1.4817110300064087, acc: 42.1875, f1: 13.636363636363635, r: 0.22534175852506613
06/02/2019 03:51:09 step: 6808, epoch: 206, batch: 9, loss: 1.4935392141342163, acc: 46.875, f1: 12.476928755998525, r: 0.18197855725809853
06/02/2019 03:51:09 step: 6813, epoch: 206, batch: 14, loss: 1.5021865367889404, acc: 43.75, f1: 13.300492610837436, r: 0.19707822833539806
06/02/2019 03:51:09 step: 6818, epoch: 206, batch: 19, loss: 1.4175338745117188, acc: 53.125, f1: 15.277777777777779, r: 0.19887765924478742
06/02/2019 03:51:10 step: 6823, epoch: 206, batch: 24, loss: 1.5897092819213867, acc: 32.8125, f1: 10.783572688334592, r: 0.2162062530720784
06/02/2019 03:51:10 step: 6828, epoch: 206, batch: 29, loss: 1.3520843982696533, acc: 46.875, f1: 17.10843373493976, r: 0.23528556440233583
06/02/2019 03:51:10 *** evaluating ***
06/02/2019 03:51:10 step: 207, epoch: 206, acc: 57.692307692307686, f1: 17.268374741200827, r: 0.26949686537658946
06/02/2019 03:51:10 *** epoch: 208 ***
06/02/2019 03:51:10 *** training ***
06/02/2019 03:51:11 step: 6836, epoch: 207, batch: 4, loss: 1.6224024295806885, acc: 39.0625, f1: 14.450549450549449, r: 0.18598358809440574
06/02/2019 03:51:11 step: 6841, epoch: 207, batch: 9, loss: 1.3269720077514648, acc: 53.125, f1: 16.07565011820331, r: 0.2841672885788013
06/02/2019 03:51:11 step: 6846, epoch: 207, batch: 14, loss: 1.4513992071151733, acc: 51.5625, f1: 14.120989304812834, r: 0.24590367119303025
06/02/2019 03:51:12 step: 6851, epoch: 207, batch: 19, loss: 1.5053400993347168, acc: 37.5, f1: 12.011278195488721, r: 0.2708902332341242
06/02/2019 03:51:12 step: 6856, epoch: 207, batch: 24, loss: 1.3009172677993774, acc: 51.5625, f1: 15.076728365764907, r: 0.2165569502091096
06/02/2019 03:51:13 step: 6861, epoch: 207, batch: 29, loss: 1.4655054807662964, acc: 51.5625, f1: 12.795031055900624, r: 0.18285008698626165
06/02/2019 03:51:13 *** evaluating ***
06/02/2019 03:51:13 step: 208, epoch: 207, acc: 57.26495726495726, f1: 17.178509300147198, r: 0.2772601235296612
06/02/2019 03:51:13 *** epoch: 209 ***
06/02/2019 03:51:13 *** training ***
06/02/2019 03:51:13 step: 6869, epoch: 208, batch: 4, loss: 1.425702691078186, acc: 51.5625, f1: 13.156565656565657, r: 0.20406744136893418
06/02/2019 03:51:14 step: 6874, epoch: 208, batch: 9, loss: 1.4840446710586548, acc: 37.5, f1: 13.90215264187867, r: 0.26760699823462997
06/02/2019 03:51:14 step: 6879, epoch: 208, batch: 14, loss: 1.5266427993774414, acc: 42.1875, f1: 14.028974876214924, r: 0.16977682578237474
06/02/2019 03:51:14 step: 6884, epoch: 208, batch: 19, loss: 1.4715827703475952, acc: 39.0625, f1: 15.570672713529857, r: 0.22317361342799863
06/02/2019 03:51:15 step: 6889, epoch: 208, batch: 24, loss: 1.475091814994812, acc: 45.3125, f1: 11.694991055456171, r: 0.24664787182606102
06/02/2019 03:51:15 step: 6894, epoch: 208, batch: 29, loss: 1.4063631296157837, acc: 45.3125, f1: 14.760348583877994, r: 0.2859877571122275
06/02/2019 03:51:15 *** evaluating ***
06/02/2019 03:51:15 step: 209, epoch: 208, acc: 56.41025641025641, f1: 16.827616827616826, r: 0.264249276108893
06/02/2019 03:51:15 *** epoch: 210 ***
06/02/2019 03:51:15 *** training ***
06/02/2019 03:51:15 step: 6902, epoch: 209, batch: 4, loss: 1.6746360063552856, acc: 34.375, f1: 10.743243243243244, r: 0.16059267137925642
06/02/2019 03:51:16 step: 6907, epoch: 209, batch: 9, loss: 1.7911157608032227, acc: 32.8125, f1: 8.644237175216523, r: 0.08117983295772649
06/02/2019 03:51:16 step: 6912, epoch: 209, batch: 14, loss: 1.3306010961532593, acc: 50.0, f1: 14.613957471100328, r: 0.25927269513528794
06/02/2019 03:51:16 step: 6917, epoch: 209, batch: 19, loss: 1.3262001276016235, acc: 51.5625, f1: 17.981366459627328, r: 0.28726953567848024
06/02/2019 03:51:17 step: 6922, epoch: 209, batch: 24, loss: 1.4669015407562256, acc: 50.0, f1: 14.71264367816092, r: 0.22373647390931306
06/02/2019 03:51:17 step: 6927, epoch: 209, batch: 29, loss: 1.3981339931488037, acc: 53.125, f1: 16.48578811369509, r: 0.23851755618741957
06/02/2019 03:51:17 *** evaluating ***
06/02/2019 03:51:17 step: 210, epoch: 209, acc: 57.26495726495726, f1: 17.246218334244578, r: 0.24657563398541396
06/02/2019 03:51:17 *** epoch: 211 ***
06/02/2019 03:51:17 *** training ***
06/02/2019 03:51:18 step: 6935, epoch: 210, batch: 4, loss: 1.5464487075805664, acc: 34.375, f1: 10.864197530864196, r: 0.1906120068490365
06/02/2019 03:51:18 step: 6940, epoch: 210, batch: 9, loss: 1.3530354499816895, acc: 46.875, f1: 15.23302646720368, r: 0.28180515047412347
06/02/2019 03:51:18 step: 6945, epoch: 210, batch: 14, loss: 1.4250907897949219, acc: 53.125, f1: 17.083333333333332, r: 0.27794440844257345
06/02/2019 03:51:19 step: 6950, epoch: 210, batch: 19, loss: 1.1772286891937256, acc: 60.9375, f1: 19.79668271803103, r: 0.23188579447553243
06/02/2019 03:51:19 step: 6955, epoch: 210, batch: 24, loss: 1.5039011240005493, acc: 37.5, f1: 12.427409988385598, r: 0.2498821144259949
06/02/2019 03:51:19 step: 6960, epoch: 210, batch: 29, loss: 1.4662280082702637, acc: 46.875, f1: 15.740740740740739, r: 0.20063829116460913
06/02/2019 03:51:19 *** evaluating ***
06/02/2019 03:51:20 step: 211, epoch: 210, acc: 57.692307692307686, f1: 17.330964096956443, r: 0.27316135318795326
06/02/2019 03:51:20 *** epoch: 212 ***
06/02/2019 03:51:20 *** training ***
06/02/2019 03:51:20 step: 6968, epoch: 211, batch: 4, loss: 1.576845407485962, acc: 37.5, f1: 10.446428571428571, r: 0.20404131258317476
06/02/2019 03:51:20 step: 6973, epoch: 211, batch: 9, loss: 1.3363113403320312, acc: 46.875, f1: 14.404761904761903, r: 0.2421651177845504
06/02/2019 03:51:20 step: 6978, epoch: 211, batch: 14, loss: 1.7869927883148193, acc: 31.25, f1: 10.459810459810459, r: 0.15814753657752612
06/02/2019 03:51:21 step: 6983, epoch: 211, batch: 19, loss: 1.5131115913391113, acc: 45.3125, f1: 11.013071895424837, r: 0.3249574966559555
06/02/2019 03:51:21 step: 6988, epoch: 211, batch: 24, loss: 1.4793752431869507, acc: 40.625, f1: 11.833046471600687, r: 0.2746962959446322
06/02/2019 03:51:21 step: 6993, epoch: 211, batch: 29, loss: 1.3583382368087769, acc: 50.0, f1: 17.261904761904763, r: 0.20283917742221175
06/02/2019 03:51:21 *** evaluating ***
06/02/2019 03:51:22 step: 212, epoch: 211, acc: 57.692307692307686, f1: 17.46124246124246, r: 0.2743918593904289
06/02/2019 03:51:22 *** epoch: 213 ***
06/02/2019 03:51:22 *** training ***
06/02/2019 03:51:22 step: 7001, epoch: 212, batch: 4, loss: 1.203720211982727, acc: 62.5, f1: 16.273522395971376, r: 0.24016061378430414
06/02/2019 03:51:22 step: 7006, epoch: 212, batch: 9, loss: 1.246934175491333, acc: 53.125, f1: 17.92717086834734, r: 0.22509909627579358
06/02/2019 03:51:23 step: 7011, epoch: 212, batch: 14, loss: 1.5025429725646973, acc: 48.4375, f1: 17.84089854265293, r: 0.326902357191288
06/02/2019 03:51:23 step: 7016, epoch: 212, batch: 19, loss: 1.364555835723877, acc: 46.875, f1: 15.734265734265731, r: 0.2278964489694555
06/02/2019 03:51:23 step: 7021, epoch: 212, batch: 24, loss: 1.535698413848877, acc: 39.0625, f1: 11.407474382157925, r: 0.23180501059115508
06/02/2019 03:51:24 step: 7026, epoch: 212, batch: 29, loss: 1.30793035030365, acc: 59.375, f1: 12.760416666666666, r: 0.25818341768878716
06/02/2019 03:51:24 *** evaluating ***
06/02/2019 03:51:24 step: 213, epoch: 212, acc: 57.26495726495726, f1: 17.301957966588827, r: 0.2731149123501882
06/02/2019 03:51:24 *** epoch: 214 ***
06/02/2019 03:51:24 *** training ***
06/02/2019 03:51:24 step: 7034, epoch: 213, batch: 4, loss: 1.5179435014724731, acc: 45.3125, f1: 14.76032273374466, r: 0.20091502354572086
06/02/2019 03:51:24 step: 7039, epoch: 213, batch: 9, loss: 1.4772793054580688, acc: 45.3125, f1: 16.494208494208493, r: 0.3212642451490204
06/02/2019 03:51:25 step: 7044, epoch: 213, batch: 14, loss: 1.368058443069458, acc: 45.3125, f1: 13.862541952429591, r: 0.21083901804899055
06/02/2019 03:51:25 step: 7049, epoch: 213, batch: 19, loss: 1.3809043169021606, acc: 46.875, f1: 20.057457786116323, r: 0.22856473779988368
06/02/2019 03:51:25 step: 7054, epoch: 213, batch: 24, loss: 1.4596363306045532, acc: 40.625, f1: 11.87675070028011, r: 0.18915500991077036
06/02/2019 03:51:26 step: 7059, epoch: 213, batch: 29, loss: 1.4892363548278809, acc: 40.625, f1: 14.458069620253166, r: 0.22781731508327116
06/02/2019 03:51:26 *** evaluating ***
06/02/2019 03:51:26 step: 214, epoch: 213, acc: 58.119658119658126, f1: 17.69759450171821, r: 0.2640902760757801
06/02/2019 03:51:26 *** epoch: 215 ***
06/02/2019 03:51:26 *** training ***
06/02/2019 03:51:26 step: 7067, epoch: 214, batch: 4, loss: 1.2378448247909546, acc: 59.375, f1: 16.519607843137255, r: 0.2774376804595235
06/02/2019 03:51:26 step: 7072, epoch: 214, batch: 9, loss: 1.333277940750122, acc: 53.125, f1: 17.20951371004393, r: 0.20132017985265843
06/02/2019 03:51:27 step: 7077, epoch: 214, batch: 14, loss: 1.3003783226013184, acc: 54.6875, f1: 19.585561497326207, r: 0.18484450537548697
06/02/2019 03:51:27 step: 7082, epoch: 214, batch: 19, loss: 1.413140892982483, acc: 39.0625, f1: 12.619047619047619, r: 0.20156175185743289
06/02/2019 03:51:27 step: 7087, epoch: 214, batch: 24, loss: 1.6435859203338623, acc: 34.375, f1: 11.294642857142856, r: 0.22758444715062948
06/02/2019 03:51:28 step: 7092, epoch: 214, batch: 29, loss: 1.4038201570510864, acc: 50.0, f1: 15.238095238095239, r: 0.26008103960470014
06/02/2019 03:51:28 *** evaluating ***
06/02/2019 03:51:28 step: 215, epoch: 214, acc: 57.26495726495726, f1: 17.252050708426548, r: 0.28226245683761725
06/02/2019 03:51:28 *** epoch: 216 ***
06/02/2019 03:51:28 *** training ***
06/02/2019 03:51:28 step: 7100, epoch: 215, batch: 4, loss: 1.6380858421325684, acc: 31.25, f1: 9.046052631578947, r: 0.17129856578161157
06/02/2019 03:51:29 step: 7105, epoch: 215, batch: 9, loss: 1.4790582656860352, acc: 43.75, f1: 15.837061006947748, r: 0.3056005381526389
06/02/2019 03:51:29 step: 7110, epoch: 215, batch: 14, loss: 1.4664546251296997, acc: 43.75, f1: 17.614549193496565, r: 0.19339743222597852
06/02/2019 03:51:29 step: 7115, epoch: 215, batch: 19, loss: 1.2939239740371704, acc: 46.875, f1: 22.76094276094276, r: 0.2193058463504218
06/02/2019 03:51:30 step: 7120, epoch: 215, batch: 24, loss: 1.2616727352142334, acc: 53.125, f1: 17.14975845410628, r: 0.21223922866782122
06/02/2019 03:51:30 step: 7125, epoch: 215, batch: 29, loss: 1.482722282409668, acc: 39.0625, f1: 12.899159663865545, r: 0.1960629823003795
06/02/2019 03:51:30 *** evaluating ***
06/02/2019 03:51:30 step: 216, epoch: 215, acc: 57.26495726495726, f1: 17.246218334244578, r: 0.26628541704750963
06/02/2019 03:51:30 *** epoch: 217 ***
06/02/2019 03:51:30 *** training ***
06/02/2019 03:51:31 step: 7133, epoch: 216, batch: 4, loss: 1.4272605180740356, acc: 45.3125, f1: 16.81910569105691, r: 0.22900678739928898
06/02/2019 03:51:31 step: 7138, epoch: 216, batch: 9, loss: 1.4703764915466309, acc: 40.625, f1: 12.430555555555554, r: 0.25071142708550787
06/02/2019 03:51:31 step: 7143, epoch: 216, batch: 14, loss: 1.4976556301116943, acc: 45.3125, f1: 13.893534002229654, r: 0.21234832255265695
06/02/2019 03:51:32 step: 7148, epoch: 216, batch: 19, loss: 1.4638561010360718, acc: 39.0625, f1: 15.209965842877235, r: 0.23759841651927815
06/02/2019 03:51:32 step: 7153, epoch: 216, batch: 24, loss: 1.3538941144943237, acc: 50.0, f1: 18.13250041098142, r: 0.2548402929914193
06/02/2019 03:51:32 step: 7158, epoch: 216, batch: 29, loss: 1.594221830368042, acc: 42.1875, f1: 10.773809523809524, r: 0.26351181183751266
06/02/2019 03:51:32 *** evaluating ***
06/02/2019 03:51:32 step: 217, epoch: 216, acc: 57.692307692307686, f1: 17.50142694063927, r: 0.2563353476363208
06/02/2019 03:51:32 *** epoch: 218 ***
06/02/2019 03:51:32 *** training ***
06/02/2019 03:51:33 step: 7166, epoch: 217, batch: 4, loss: 1.4332605600357056, acc: 48.4375, f1: 16.78387248007501, r: 0.22029860172153812
06/02/2019 03:51:33 step: 7171, epoch: 217, batch: 9, loss: 1.3726813793182373, acc: 46.875, f1: 14.313725490196077, r: 0.238929021681174
06/02/2019 03:51:33 step: 7176, epoch: 217, batch: 14, loss: 1.5999755859375, acc: 39.0625, f1: 14.498644986449863, r: 0.1616607461284693
06/02/2019 03:51:34 step: 7181, epoch: 217, batch: 19, loss: 1.4191492795944214, acc: 43.75, f1: 13.348765432098764, r: 0.2290660321997643
06/02/2019 03:51:34 step: 7186, epoch: 217, batch: 24, loss: 1.3875653743743896, acc: 40.625, f1: 17.584541062801932, r: 0.19677021283575158
06/02/2019 03:51:34 step: 7191, epoch: 217, batch: 29, loss: 1.4866245985031128, acc: 32.8125, f1: 11.729452054794521, r: 0.2783099901615076
06/02/2019 03:51:35 *** evaluating ***
06/02/2019 03:51:35 step: 218, epoch: 217, acc: 56.41025641025641, f1: 16.892655367231637, r: 0.26185903982209036
06/02/2019 03:51:35 *** epoch: 219 ***
06/02/2019 03:51:35 *** training ***
06/02/2019 03:51:35 step: 7199, epoch: 218, batch: 4, loss: 1.4192383289337158, acc: 50.0, f1: 16.195652173913043, r: 0.26148087949148224
06/02/2019 03:51:35 step: 7204, epoch: 218, batch: 9, loss: 1.3010989427566528, acc: 48.4375, f1: 16.88659399502773, r: 0.2586227161394059
06/02/2019 03:51:36 step: 7209, epoch: 218, batch: 14, loss: 1.3934246301651, acc: 54.6875, f1: 14.792299898682879, r: 0.17003252085278908
06/02/2019 03:51:36 step: 7214, epoch: 218, batch: 19, loss: 1.6286509037017822, acc: 40.625, f1: 10.755336617405584, r: 0.17909377758751044
06/02/2019 03:51:36 step: 7219, epoch: 218, batch: 24, loss: 1.362976312637329, acc: 54.6875, f1: 13.232600732600734, r: 0.27279570205499876
06/02/2019 03:51:37 step: 7224, epoch: 218, batch: 29, loss: 1.3587274551391602, acc: 51.5625, f1: 14.82213438735178, r: 0.22292747001163205
06/02/2019 03:51:37 *** evaluating ***
06/02/2019 03:51:37 step: 219, epoch: 218, acc: 58.119658119658126, f1: 17.621692482837133, r: 0.2621485451219429
06/02/2019 03:51:37 *** epoch: 220 ***
06/02/2019 03:51:37 *** training ***
06/02/2019 03:51:37 step: 7232, epoch: 219, batch: 4, loss: 1.5189415216445923, acc: 46.875, f1: 16.542359915853893, r: 0.1264467819371058
06/02/2019 03:51:37 step: 7237, epoch: 219, batch: 9, loss: 1.5971111059188843, acc: 34.375, f1: 11.726508785332316, r: 0.189950421575179
06/02/2019 03:51:38 step: 7242, epoch: 219, batch: 14, loss: 1.5010056495666504, acc: 46.875, f1: 17.184448462929474, r: 0.2528217542831031
06/02/2019 03:51:38 step: 7247, epoch: 219, batch: 19, loss: 1.3506286144256592, acc: 48.4375, f1: 17.77083675817853, r: 0.1747236369954978
06/02/2019 03:51:38 step: 7252, epoch: 219, batch: 24, loss: 1.4547935724258423, acc: 48.4375, f1: 19.074074074074073, r: 0.34893269239219155
06/02/2019 03:51:39 step: 7257, epoch: 219, batch: 29, loss: 1.3883490562438965, acc: 43.75, f1: 12.763157894736842, r: 0.28881131110351144
06/02/2019 03:51:39 *** evaluating ***
06/02/2019 03:51:39 step: 220, epoch: 219, acc: 57.692307692307686, f1: 17.439862542955325, r: 0.262500862318408
06/02/2019 03:51:39 *** epoch: 221 ***
06/02/2019 03:51:39 *** training ***
06/02/2019 03:51:39 step: 7265, epoch: 220, batch: 4, loss: 1.3890514373779297, acc: 48.4375, f1: 15.574339329336, r: 0.2683359376833762
06/02/2019 03:51:40 step: 7270, epoch: 220, batch: 9, loss: 1.4148622751235962, acc: 48.4375, f1: 15.091036414565828, r: 0.22515643328907614
06/02/2019 03:51:40 step: 7275, epoch: 220, batch: 14, loss: 1.516255497932434, acc: 46.875, f1: 12.012987012987011, r: 0.20720515473002535
06/02/2019 03:51:40 step: 7280, epoch: 220, batch: 19, loss: 1.596242904663086, acc: 35.9375, f1: 13.569638381668456, r: 0.16423853290297916
06/02/2019 03:51:41 step: 7285, epoch: 220, batch: 24, loss: 1.4670954942703247, acc: 45.3125, f1: 14.76032273374466, r: 0.20879462609303837
06/02/2019 03:51:41 step: 7290, epoch: 220, batch: 29, loss: 1.4433220624923706, acc: 48.4375, f1: 14.098837209302328, r: 0.1745474816524196
06/02/2019 03:51:41 *** evaluating ***
06/02/2019 03:51:41 step: 221, epoch: 220, acc: 57.26495726495726, f1: 17.287059716424604, r: 0.26384677181856453
06/02/2019 03:51:41 *** epoch: 222 ***
06/02/2019 03:51:41 *** training ***
06/02/2019 03:51:41 step: 7298, epoch: 221, batch: 4, loss: 1.446953535079956, acc: 43.75, f1: 10.679271708683475, r: 0.20637068043726048
06/02/2019 03:51:42 step: 7303, epoch: 221, batch: 9, loss: 1.6351019144058228, acc: 35.9375, f1: 9.85576923076923, r: 0.2300332009025119
06/02/2019 03:51:42 step: 7308, epoch: 221, batch: 14, loss: 1.423504114151001, acc: 39.0625, f1: 12.661251791686572, r: 0.2261444954358914
06/02/2019 03:51:42 step: 7313, epoch: 221, batch: 19, loss: 1.599252700805664, acc: 39.0625, f1: 11.011904761904761, r: 0.25092723828669006
06/02/2019 03:51:43 step: 7318, epoch: 221, batch: 24, loss: 1.5158495903015137, acc: 39.0625, f1: 12.20112517580872, r: 0.28692703836427585
06/02/2019 03:51:43 step: 7323, epoch: 221, batch: 29, loss: 1.4733195304870605, acc: 48.4375, f1: 17.38955823293173, r: 0.2319258721205641
06/02/2019 03:51:43 *** evaluating ***
06/02/2019 03:51:43 step: 222, epoch: 221, acc: 57.26495726495726, f1: 17.182130584192436, r: 0.2811485653097542
06/02/2019 03:51:43 *** epoch: 223 ***
06/02/2019 03:51:43 *** training ***
06/02/2019 03:51:44 step: 7331, epoch: 222, batch: 4, loss: 1.2744568586349487, acc: 48.4375, f1: 18.621399176954732, r: 0.2097915450754138
06/02/2019 03:51:44 step: 7336, epoch: 222, batch: 9, loss: 1.4213995933532715, acc: 54.6875, f1: 13.512845849802371, r: 0.14575696027915352
06/02/2019 03:51:44 step: 7341, epoch: 222, batch: 14, loss: 1.4676555395126343, acc: 40.625, f1: 12.208333333333334, r: 0.22395807232841045
06/02/2019 03:51:45 step: 7346, epoch: 222, batch: 19, loss: 1.5797175168991089, acc: 39.0625, f1: 11.119081779053083, r: 0.19668723949782432
06/02/2019 03:51:45 step: 7351, epoch: 222, batch: 24, loss: 1.472198724746704, acc: 40.625, f1: 15.099457504520794, r: 0.266299098607228
06/02/2019 03:51:45 step: 7356, epoch: 222, batch: 29, loss: 1.6474542617797852, acc: 37.5, f1: 11.569664902998237, r: 0.20536315745688685
06/02/2019 03:51:46 *** evaluating ***
06/02/2019 03:51:46 step: 223, epoch: 222, acc: 58.119658119658126, f1: 17.51736111111111, r: 0.2909501671342481
06/02/2019 03:51:46 *** epoch: 224 ***
06/02/2019 03:51:46 *** training ***
06/02/2019 03:51:46 step: 7364, epoch: 223, batch: 4, loss: 1.6199090480804443, acc: 39.0625, f1: 11.041666666666666, r: 0.24361318810998725
06/02/2019 03:51:46 step: 7369, epoch: 223, batch: 9, loss: 1.53519868850708, acc: 37.5, f1: 15.60201316298877, r: 0.3065544983141818
06/02/2019 03:51:47 step: 7374, epoch: 223, batch: 14, loss: 1.4510449171066284, acc: 50.0, f1: 15.928270042194093, r: 0.17186593128739258
06/02/2019 03:51:47 step: 7379, epoch: 223, batch: 19, loss: 1.4052619934082031, acc: 46.875, f1: 14.653558052434459, r: 0.09822577073362476
06/02/2019 03:51:47 step: 7384, epoch: 223, batch: 24, loss: 1.4008654356002808, acc: 45.3125, f1: 20.790725928925788, r: 0.28243416189363624
06/02/2019 03:51:48 step: 7389, epoch: 223, batch: 29, loss: 1.2302085161209106, acc: 53.125, f1: 19.04761904761905, r: 0.30481094697379907
06/02/2019 03:51:48 *** evaluating ***
06/02/2019 03:51:48 step: 224, epoch: 223, acc: 59.82905982905983, f1: 18.637093819575572, r: 0.2762618231989958
06/02/2019 03:51:48 *** epoch: 225 ***
06/02/2019 03:51:48 *** training ***
06/02/2019 03:51:48 step: 7397, epoch: 224, batch: 4, loss: 1.252718210220337, acc: 46.875, f1: 20.694444444444446, r: 0.17325134619374505
06/02/2019 03:51:49 step: 7402, epoch: 224, batch: 9, loss: 1.3965682983398438, acc: 43.75, f1: 16.541353383458645, r: 0.2164933453508623
06/02/2019 03:51:49 step: 7407, epoch: 224, batch: 14, loss: 1.542423963546753, acc: 40.625, f1: 11.615210843373495, r: 0.19667135723655957
06/02/2019 03:51:49 step: 7412, epoch: 224, batch: 19, loss: 1.5770299434661865, acc: 43.75, f1: 17.15357812918789, r: 0.24528129458787257
06/02/2019 03:51:50 step: 7417, epoch: 224, batch: 24, loss: 1.2078299522399902, acc: 57.8125, f1: 23.1990231990232, r: 0.2772765074738412
06/02/2019 03:51:50 step: 7422, epoch: 224, batch: 29, loss: 1.284496545791626, acc: 51.5625, f1: 17.36263736263736, r: 0.21133588935481223
06/02/2019 03:51:50 *** evaluating ***
06/02/2019 03:51:50 step: 225, epoch: 224, acc: 58.54700854700855, f1: 17.493576609100074, r: 0.2816290782059413
06/02/2019 03:51:50 *** epoch: 226 ***
06/02/2019 03:51:50 *** training ***
06/02/2019 03:51:51 step: 7430, epoch: 225, batch: 4, loss: 1.403061866760254, acc: 48.4375, f1: 14.591503267973858, r: 0.2442109735774755
06/02/2019 03:51:51 step: 7435, epoch: 225, batch: 9, loss: 1.555694341659546, acc: 40.625, f1: 16.52350813743219, r: 0.22629284774452807
06/02/2019 03:51:51 step: 7440, epoch: 225, batch: 14, loss: 1.4504534006118774, acc: 42.1875, f1: 13.54895104895105, r: 0.22895402561051284
06/02/2019 03:51:52 step: 7445, epoch: 225, batch: 19, loss: 1.3860218524932861, acc: 46.875, f1: 13.580246913580247, r: 0.30320107966763876
06/02/2019 03:51:52 step: 7450, epoch: 225, batch: 24, loss: 1.2929832935333252, acc: 50.0, f1: 16.0250146570256, r: 0.2461066995528423
06/02/2019 03:51:52 step: 7455, epoch: 225, batch: 29, loss: 1.4102901220321655, acc: 46.875, f1: 14.117647058823529, r: 0.24418403786948395
06/02/2019 03:51:52 *** evaluating ***
06/02/2019 03:51:53 step: 226, epoch: 225, acc: 56.837606837606835, f1: 17.039057002111193, r: 0.26066687401398253
06/02/2019 03:51:53 *** epoch: 227 ***
06/02/2019 03:51:53 *** training ***
06/02/2019 03:51:53 step: 7463, epoch: 226, batch: 4, loss: 1.4361586570739746, acc: 48.4375, f1: 20.816395663956637, r: 0.2845841712150144
06/02/2019 03:51:53 step: 7468, epoch: 226, batch: 9, loss: 1.5115517377853394, acc: 39.0625, f1: 10.82236842105263, r: 0.1836556836319789
06/02/2019 03:51:53 step: 7473, epoch: 226, batch: 14, loss: 1.5612353086471558, acc: 40.625, f1: 10.74074074074074, r: 0.2725050700256209
06/02/2019 03:51:54 step: 7478, epoch: 226, batch: 19, loss: 1.3707000017166138, acc: 45.3125, f1: 13.605442176870747, r: 0.17846101223913513
06/02/2019 03:51:54 step: 7483, epoch: 226, batch: 24, loss: 1.6485142707824707, acc: 48.4375, f1: 15.312500000000002, r: 0.18771472873877976
06/02/2019 03:51:54 step: 7488, epoch: 226, batch: 29, loss: 1.4059892892837524, acc: 48.4375, f1: 17.246898759503804, r: 0.220952309666447
06/02/2019 03:51:54 *** evaluating ***
06/02/2019 03:51:55 step: 227, epoch: 226, acc: 57.26495726495726, f1: 17.246218334244578, r: 0.29598389702591943
06/02/2019 03:51:55 *** epoch: 228 ***
06/02/2019 03:51:55 *** training ***
06/02/2019 03:51:55 step: 7496, epoch: 227, batch: 4, loss: 1.376907467842102, acc: 46.875, f1: 14.974892395982781, r: 0.3072540672168569
06/02/2019 03:51:55 step: 7501, epoch: 227, batch: 9, loss: 1.4637690782546997, acc: 43.75, f1: 14.371772805507742, r: 0.3093101238471009
06/02/2019 03:51:56 step: 7506, epoch: 227, batch: 14, loss: 1.4452059268951416, acc: 45.3125, f1: 16.29887727448703, r: 0.24567505977914325
06/02/2019 03:51:56 step: 7511, epoch: 227, batch: 19, loss: 1.4446327686309814, acc: 50.0, f1: 15.069686411149824, r: 0.2034043173169932
06/02/2019 03:51:56 step: 7516, epoch: 227, batch: 24, loss: 1.3527143001556396, acc: 48.4375, f1: 15.906362545018007, r: 0.29455087180928763
06/02/2019 03:51:57 step: 7521, epoch: 227, batch: 29, loss: 1.366822361946106, acc: 46.875, f1: 15.731292517006803, r: 0.2591169107681311
06/02/2019 03:51:57 *** evaluating ***
06/02/2019 03:51:57 step: 228, epoch: 227, acc: 59.401709401709404, f1: 17.760082954158545, r: 0.2609204754117368
06/02/2019 03:51:57 *** epoch: 229 ***
06/02/2019 03:51:57 *** training ***
06/02/2019 03:51:57 step: 7529, epoch: 228, batch: 4, loss: 1.4822195768356323, acc: 39.0625, f1: 12.812500000000002, r: 0.23907087229261095
06/02/2019 03:51:58 step: 7534, epoch: 228, batch: 9, loss: 1.5359047651290894, acc: 39.0625, f1: 15.907335907335906, r: 0.32681267077090465
06/02/2019 03:51:58 step: 7539, epoch: 228, batch: 14, loss: 1.262107253074646, acc: 51.5625, f1: 17.034632034632033, r: 0.2137712192588879
06/02/2019 03:51:58 step: 7544, epoch: 228, batch: 19, loss: 1.3904385566711426, acc: 42.1875, f1: 12.361923326835607, r: 0.2289939742611714
06/02/2019 03:51:58 step: 7549, epoch: 228, batch: 24, loss: 1.4961304664611816, acc: 48.4375, f1: 13.404392764857882, r: 0.2176780604317102
06/02/2019 03:51:59 step: 7554, epoch: 228, batch: 29, loss: 1.3466291427612305, acc: 43.75, f1: 17.042606516290725, r: 0.23504458938677253
06/02/2019 03:51:59 *** evaluating ***
06/02/2019 03:51:59 step: 229, epoch: 228, acc: 58.119658119658126, f1: 17.503010839020472, r: 0.2919001355737966
06/02/2019 03:51:59 *** epoch: 230 ***
06/02/2019 03:51:59 *** training ***
06/02/2019 03:51:59 step: 7562, epoch: 229, batch: 4, loss: 1.5486221313476562, acc: 39.0625, f1: 13.571428571428571, r: 0.26195158134403124
06/02/2019 03:52:00 step: 7567, epoch: 229, batch: 9, loss: 1.389916181564331, acc: 48.4375, f1: 12.12121212121212, r: 0.20561546158724534
06/02/2019 03:52:00 step: 7572, epoch: 229, batch: 14, loss: 1.5740364789962769, acc: 32.8125, f1: 7.317073170731707, r: 0.22474456844161658
06/02/2019 03:52:00 step: 7577, epoch: 229, batch: 19, loss: 1.1737817525863647, acc: 59.375, f1: 18.96103896103896, r: 0.19070376016206914
06/02/2019 03:52:01 step: 7582, epoch: 229, batch: 24, loss: 1.4249701499938965, acc: 50.0, f1: 16.019736842105264, r: 0.258997305309897
06/02/2019 03:52:01 step: 7587, epoch: 229, batch: 29, loss: 1.6168200969696045, acc: 32.8125, f1: 12.491228070175438, r: 0.25181992619400717
06/02/2019 03:52:01 *** evaluating ***
06/02/2019 03:52:01 step: 230, epoch: 229, acc: 58.97435897435898, f1: 17.73178529362275, r: 0.30513678795415805
06/02/2019 03:52:01 *** epoch: 231 ***
06/02/2019 03:52:01 *** training ***
06/02/2019 03:52:02 step: 7595, epoch: 230, batch: 4, loss: 1.1640545129776, acc: 51.5625, f1: 15.873015873015875, r: 0.14913962010230458
06/02/2019 03:52:02 step: 7600, epoch: 230, batch: 9, loss: 1.4771091938018799, acc: 51.5625, f1: 15.306122448979592, r: 0.21478525732862713
06/02/2019 03:52:02 step: 7605, epoch: 230, batch: 14, loss: 1.107742190361023, acc: 62.5, f1: 20.6430398246255, r: 0.2007056092389031
06/02/2019 03:52:03 step: 7610, epoch: 230, batch: 19, loss: 1.544430136680603, acc: 37.5, f1: 11.340852130325816, r: 0.22767325703860772
06/02/2019 03:52:03 step: 7615, epoch: 230, batch: 24, loss: 1.3420069217681885, acc: 43.75, f1: 18.23695040069309, r: 0.23065684967080238
06/02/2019 03:52:03 step: 7620, epoch: 230, batch: 29, loss: 1.6092976331710815, acc: 40.625, f1: 9.106984969053936, r: 0.13251040748016912
06/02/2019 03:52:03 *** evaluating ***
06/02/2019 03:52:04 step: 231, epoch: 230, acc: 56.837606837606835, f1: 17.099073672022, r: 0.26624624501186417
06/02/2019 03:52:04 *** epoch: 232 ***
06/02/2019 03:52:04 *** training ***
06/02/2019 03:52:04 step: 7628, epoch: 231, batch: 4, loss: 1.3558293581008911, acc: 48.4375, f1: 14.864158829676072, r: 0.30368594085135375
06/02/2019 03:52:04 step: 7633, epoch: 231, batch: 9, loss: 1.3332648277282715, acc: 51.5625, f1: 17.96119929453263, r: 0.21114596018520587
06/02/2019 03:52:04 step: 7638, epoch: 231, batch: 14, loss: 1.5381311178207397, acc: 43.75, f1: 12.033715925394548, r: 0.2513707857091076
06/02/2019 03:52:05 step: 7643, epoch: 231, batch: 19, loss: 1.1938649415969849, acc: 51.5625, f1: 18.73840445269017, r: 0.217835985880063
06/02/2019 03:52:05 step: 7648, epoch: 231, batch: 24, loss: 1.533639669418335, acc: 39.0625, f1: 11.47316538882804, r: 0.28179541960782123
06/02/2019 03:52:05 step: 7653, epoch: 231, batch: 29, loss: 1.4945156574249268, acc: 48.4375, f1: 16.202090592334496, r: 0.24649667450978496
06/02/2019 03:52:05 *** evaluating ***
06/02/2019 03:52:06 step: 232, epoch: 231, acc: 56.837606837606835, f1: 17.099073672022, r: 0.2990034781735479
06/02/2019 03:52:06 *** epoch: 233 ***
06/02/2019 03:52:06 *** training ***
06/02/2019 03:52:06 step: 7661, epoch: 232, batch: 4, loss: 1.2728615999221802, acc: 53.125, f1: 10.469924812030074, r: 0.248113354055616
06/02/2019 03:52:06 step: 7666, epoch: 232, batch: 9, loss: 1.3699238300323486, acc: 48.4375, f1: 17.5, r: 0.249808499704508
06/02/2019 03:52:07 step: 7671, epoch: 232, batch: 14, loss: 1.4533060789108276, acc: 46.875, f1: 16.02848101265823, r: 0.23093673509400836
06/02/2019 03:52:07 step: 7676, epoch: 232, batch: 19, loss: 1.4790910482406616, acc: 42.1875, f1: 12.396807925151348, r: 0.2460531699375308
06/02/2019 03:52:07 step: 7681, epoch: 232, batch: 24, loss: 1.7712223529815674, acc: 29.6875, f1: 8.092105263157896, r: 0.21053316463608404
06/02/2019 03:52:08 step: 7686, epoch: 232, batch: 29, loss: 1.4288651943206787, acc: 43.75, f1: 16.29016064257028, r: 0.2433112194583832
06/02/2019 03:52:08 *** evaluating ***
06/02/2019 03:52:08 step: 233, epoch: 232, acc: 58.54700854700855, f1: 17.41823123301365, r: 0.3015564250818446
06/02/2019 03:52:08 *** epoch: 234 ***
06/02/2019 03:52:08 *** training ***
06/02/2019 03:52:08 step: 7694, epoch: 233, batch: 4, loss: 1.3399467468261719, acc: 45.3125, f1: 16.278659611992946, r: 0.276899345646282
06/02/2019 03:52:09 step: 7699, epoch: 233, batch: 9, loss: 1.5655364990234375, acc: 42.1875, f1: 13.594377510040163, r: 0.2204223333019305
06/02/2019 03:52:09 step: 7704, epoch: 233, batch: 14, loss: 1.3046553134918213, acc: 54.6875, f1: 16.96428571428571, r: 0.2574762653605634
06/02/2019 03:52:09 step: 7709, epoch: 233, batch: 19, loss: 1.2565996646881104, acc: 42.1875, f1: 30.867208672086722, r: 0.28216407914887015
06/02/2019 03:53:26 {'input_path': 'data/word2vec_temp', 'output_path': 'save/cnn_7', 'gpu': True, 'seed': 20000125, 'display_per_batch': 5, 'optimizer': 'adagrad', 'lr': 0.01, 'lr_decay': 0, 'weight_decay': 0.0001, 'momentum': 0.985, 'num_epochs': 300, 'batch_size': 64, 'num_labels': 8, 'embedding_size': 300, 'type': 'cnn', 'cnn': {'max_length': 512, 'conv_1': {'size': 512, 'kernel_size': 3}, 'max_pool_1': {'kernel_size': 2, 'stride': 2}, 'fc': {'hidden_size': 512, 'dropout': 0.5}, 'loss': 'cross_entropy'}}
06/02/2019 03:53:26 Loading Train Data
06/02/2019 03:53:26 load data from data/word2vec_temp/train_text.npy, data/word2vec_temp/train_label.npy, training: True
06/02/2019 03:53:50 loaded. total len: 2342
06/02/2019 03:53:50 Train: length: 2108, total batch: 33, batch size: 64
06/02/2019 03:53:50 Dev: length: 234, total batch: 4, batch size: 64
06/02/2019 03:53:50 Loading model cnn
06/02/2019 03:53:58 *** epoch: 1 ***
06/02/2019 03:53:58 *** training ***
06/02/2019 03:53:59 step: 5, epoch: 0, batch: 4, loss: 59.41958236694336, acc: 9.375, f1: 7.245863404330224, r: -0.02659925024456294
06/02/2019 03:53:59 step: 10, epoch: 0, batch: 9, loss: 11.847235679626465, acc: 25.0, f1: 10.972222222222221, r: -0.02071716749618272
06/02/2019 03:53:59 step: 15, epoch: 0, batch: 14, loss: 2.241121292114258, acc: 12.5, f1: 6.411655571319437, r: 0.025300526111779038
06/02/2019 03:54:00 step: 20, epoch: 0, batch: 19, loss: 1.9068176746368408, acc: 46.875, f1: 9.419152276295133, r: 0.05522846248064669
06/02/2019 03:54:00 step: 25, epoch: 0, batch: 24, loss: 2.002110719680786, acc: 42.1875, f1: 7.584269662921349, r: 0.0049286620575981
06/02/2019 03:54:00 step: 30, epoch: 0, batch: 29, loss: 2.041226625442505, acc: 39.0625, f1: 11.375281820045089, r: -0.055282146460085335
06/02/2019 03:54:00 *** evaluating ***
06/02/2019 03:54:01 step: 1, epoch: 0, acc: 42.30769230769231, f1: 9.780090109418579, r: 0.04491290717590394
06/02/2019 03:54:01 *** epoch: 2 ***
06/02/2019 03:54:01 *** training ***
06/02/2019 03:54:01 step: 38, epoch: 1, batch: 4, loss: 1.9459677934646606, acc: 37.5, f1: 11.21029075245659, r: 0.06208641600360619
06/02/2019 03:54:01 step: 43, epoch: 1, batch: 9, loss: 1.7676113843917847, acc: 50.0, f1: 20.7977207977208, r: 0.17721486311736423
06/02/2019 03:54:01 step: 48, epoch: 1, batch: 14, loss: 1.8760629892349243, acc: 35.9375, f1: 14.37613019891501, r: 0.0728373268246863
06/02/2019 03:54:01 step: 53, epoch: 1, batch: 19, loss: 1.9819782972335815, acc: 43.75, f1: 11.792207792207792, r: 0.14599301456000835
06/02/2019 03:54:02 step: 58, epoch: 1, batch: 24, loss: 2.8090009689331055, acc: 34.375, f1: 9.74426807760141, r: -0.05587698063368185
06/02/2019 03:54:02 step: 63, epoch: 1, batch: 29, loss: 1.9336795806884766, acc: 35.9375, f1: 11.13871635610766, r: -0.04844185539575477
06/02/2019 03:54:02 *** evaluating ***
06/02/2019 03:54:02 step: 2, epoch: 1, acc: 39.743589743589745, f1: 13.930396374029272, r: 0.1719741687785021
06/02/2019 03:54:02 *** epoch: 3 ***
06/02/2019 03:54:02 *** training ***
06/02/2019 03:54:02 step: 71, epoch: 2, batch: 4, loss: 1.867556095123291, acc: 26.5625, f1: 11.814387917329094, r: 0.11267933741594764
06/02/2019 03:54:02 step: 76, epoch: 2, batch: 9, loss: 1.87349271774292, acc: 42.1875, f1: 10.392441860465116, r: 0.07644198407082112
06/02/2019 03:54:03 step: 81, epoch: 2, batch: 14, loss: 1.9194471836090088, acc: 39.0625, f1: 9.060077519379844, r: 0.03665319759360928
06/02/2019 03:54:03 step: 86, epoch: 2, batch: 19, loss: 1.671055793762207, acc: 45.3125, f1: 9.006211180124224, r: 0.11244943879550004
06/02/2019 03:54:03 step: 91, epoch: 2, batch: 24, loss: 1.7668592929840088, acc: 43.75, f1: 11.232517482517483, r: 0.093459851416553
06/02/2019 03:54:03 step: 96, epoch: 2, batch: 29, loss: 1.947911024093628, acc: 37.5, f1: 9.811046511627907, r: 0.03898709859120401
06/02/2019 03:54:03 *** evaluating ***
06/02/2019 03:54:04 step: 3, epoch: 2, acc: 48.717948717948715, f1: 11.740671829427738, r: 0.1958439743202967
06/02/2019 03:54:04 *** epoch: 4 ***
06/02/2019 03:54:04 *** training ***
06/02/2019 03:54:04 step: 104, epoch: 3, batch: 4, loss: 1.7080788612365723, acc: 34.375, f1: 10.40787623066104, r: 0.1711979905966553
06/02/2019 03:54:04 step: 109, epoch: 3, batch: 9, loss: 1.7204620838165283, acc: 56.25, f1: 14.903381642512079, r: 0.009479445147114768
06/02/2019 03:54:04 step: 114, epoch: 3, batch: 14, loss: 1.7021054029464722, acc: 46.875, f1: 11.60361377752682, r: 0.14848763031758694
06/02/2019 03:54:04 step: 119, epoch: 3, batch: 19, loss: 1.7473154067993164, acc: 35.9375, f1: 11.805555555555555, r: 0.16775561008837112
06/02/2019 03:54:05 step: 124, epoch: 3, batch: 24, loss: 1.7090723514556885, acc: 46.875, f1: 12.698412698412698, r: 0.1028670271058309
06/02/2019 03:54:05 step: 129, epoch: 3, batch: 29, loss: 1.7491997480392456, acc: 32.8125, f1: 6.325301204819277, r: 0.12277251692659508
06/02/2019 03:54:05 *** evaluating ***
06/02/2019 03:54:05 step: 4, epoch: 3, acc: 51.28205128205128, f1: 13.792502179598953, r: 0.21649977418027153
06/02/2019 03:54:05 *** epoch: 5 ***
06/02/2019 03:54:05 *** training ***
06/02/2019 03:54:05 step: 137, epoch: 4, batch: 4, loss: 1.7159619331359863, acc: 42.1875, f1: 9.815486993345433, r: 0.1534525328556998
06/02/2019 03:54:06 step: 142, epoch: 4, batch: 9, loss: 1.7614272832870483, acc: 32.8125, f1: 8.75, r: 0.19798398611326096
06/02/2019 03:54:06 step: 147, epoch: 4, batch: 14, loss: 1.7223433256149292, acc: 46.875, f1: 15.360562829783072, r: 0.09189838415974265
06/02/2019 03:54:06 step: 152, epoch: 4, batch: 19, loss: 1.4744371175765991, acc: 48.4375, f1: 10.091277890466532, r: 0.2274442243451622
06/02/2019 03:54:06 step: 157, epoch: 4, batch: 24, loss: 1.5183649063110352, acc: 48.4375, f1: 16.00985221674877, r: 0.1513757883502087
06/02/2019 03:54:06 step: 162, epoch: 4, batch: 29, loss: 1.6207518577575684, acc: 43.75, f1: 13.61607142857143, r: 0.10978451047521008
06/02/2019 03:54:07 *** evaluating ***
06/02/2019 03:54:07 step: 5, epoch: 4, acc: 52.13675213675214, f1: 14.308149405772497, r: 0.22794866638412767
06/02/2019 03:54:07 *** epoch: 6 ***
06/02/2019 03:54:07 *** training ***
06/02/2019 03:54:07 step: 170, epoch: 5, batch: 4, loss: 1.727131962776184, acc: 43.75, f1: 12.653061224489795, r: 0.06652690083786418
06/02/2019 03:54:07 step: 175, epoch: 5, batch: 9, loss: 1.7941666841506958, acc: 28.125, f1: 7.895279323850753, r: 0.17053222001472015
06/02/2019 03:54:07 step: 180, epoch: 5, batch: 14, loss: 1.8399971723556519, acc: 34.375, f1: 9.027777777777777, r: 0.11080161909260466
06/02/2019 03:54:08 step: 185, epoch: 5, batch: 19, loss: 1.4883352518081665, acc: 48.4375, f1: 15.684315684315683, r: 0.23760193036654315
06/02/2019 03:54:08 step: 190, epoch: 5, batch: 24, loss: 1.6548573970794678, acc: 37.5, f1: 11.057334326135516, r: 0.23724913547256754
06/02/2019 03:54:08 step: 195, epoch: 5, batch: 29, loss: 1.931155800819397, acc: 40.625, f1: 7.647058823529412, r: 0.0190536023812418
06/02/2019 03:54:08 *** evaluating ***
06/02/2019 03:54:08 step: 6, epoch: 5, acc: 51.28205128205128, f1: 13.792502179598953, r: 0.22721005228043684
06/02/2019 03:54:08 *** epoch: 7 ***
06/02/2019 03:54:08 *** training ***
06/02/2019 03:54:08 step: 203, epoch: 6, batch: 4, loss: 1.9060255289077759, acc: 37.5, f1: 9.54595791805094, r: 0.10749248658557807
06/02/2019 03:54:09 step: 208, epoch: 6, batch: 9, loss: 1.5734434127807617, acc: 46.875, f1: 13.255813953488374, r: 0.19204053429033366
06/02/2019 03:54:09 step: 213, epoch: 6, batch: 14, loss: 1.7739280462265015, acc: 32.8125, f1: 7.317073170731707, r: 0.0958276846894877
06/02/2019 03:54:09 step: 218, epoch: 6, batch: 19, loss: 1.543080449104309, acc: 53.125, f1: 19.226638023630503, r: 0.1619641058010181
06/02/2019 03:54:09 step: 223, epoch: 6, batch: 24, loss: 1.6763694286346436, acc: 31.25, f1: 9.23103418582623, r: 0.1425782028373297
06/02/2019 03:54:10 step: 228, epoch: 6, batch: 29, loss: 1.6676136255264282, acc: 39.0625, f1: 11.954711954711954, r: 0.15047911661849048
06/02/2019 03:54:10 *** evaluating ***
06/02/2019 03:54:10 step: 7, epoch: 6, acc: 52.56410256410257, f1: 14.524256173108805, r: 0.24799877692669292
06/02/2019 03:54:10 *** epoch: 8 ***
06/02/2019 03:54:10 *** training ***
06/02/2019 03:54:10 step: 236, epoch: 7, batch: 4, loss: 1.8132061958312988, acc: 40.625, f1: 8.823529411764707, r: 0.1538239833558186
06/02/2019 03:54:10 step: 241, epoch: 7, batch: 9, loss: 1.5528159141540527, acc: 40.625, f1: 10.904977375565611, r: 0.2696521370026334
06/02/2019 03:54:10 step: 246, epoch: 7, batch: 14, loss: 1.6730952262878418, acc: 42.1875, f1: 10.657225853304286, r: 0.16859127885836436
06/02/2019 03:54:11 step: 251, epoch: 7, batch: 19, loss: 1.8326542377471924, acc: 42.1875, f1: 15.69295683219734, r: 0.031114119792226922
06/02/2019 03:54:11 step: 256, epoch: 7, batch: 24, loss: 1.5778789520263672, acc: 37.5, f1: 10.040650406504065, r: 0.15952390319927076
06/02/2019 03:54:11 step: 261, epoch: 7, batch: 29, loss: 1.6201962232589722, acc: 39.0625, f1: 12.551510989010989, r: 0.19464777861939248
06/02/2019 03:54:11 *** evaluating ***
06/02/2019 03:54:11 step: 8, epoch: 7, acc: 55.55555555555556, f1: 16.31343796540268, r: 0.2657626975810153
06/02/2019 03:54:11 *** epoch: 9 ***
06/02/2019 03:54:11 *** training ***
06/02/2019 03:54:11 step: 269, epoch: 8, batch: 4, loss: 1.7035526037216187, acc: 37.5, f1: 11.838624338624337, r: 0.1780094707606022
06/02/2019 03:54:12 step: 274, epoch: 8, batch: 9, loss: 1.7039694786071777, acc: 37.5, f1: 10.278745644599303, r: 0.23162957413891036
06/02/2019 03:54:12 step: 279, epoch: 8, batch: 14, loss: 1.5167765617370605, acc: 35.9375, f1: 13.881987577639752, r: 0.2285429482595588
06/02/2019 03:54:12 step: 284, epoch: 8, batch: 19, loss: 1.55559241771698, acc: 40.625, f1: 13.87218045112782, r: 0.21058232660469603
06/02/2019 03:54:12 step: 289, epoch: 8, batch: 24, loss: 1.6142897605895996, acc: 46.875, f1: 9.740259740259742, r: 0.18986356525529824
06/02/2019 03:54:12 step: 294, epoch: 8, batch: 29, loss: 1.861270546913147, acc: 29.6875, f1: 8.852813852813854, r: 0.15908481073277522
06/02/2019 03:54:13 *** evaluating ***
06/02/2019 03:54:13 step: 9, epoch: 8, acc: 50.85470085470085, f1: 13.45520421607378, r: 0.271284635240211
06/02/2019 03:54:13 *** epoch: 10 ***
06/02/2019 03:54:13 *** training ***
06/02/2019 03:54:13 step: 302, epoch: 9, batch: 4, loss: 1.8544028997421265, acc: 29.6875, f1: 8.766233766233766, r: 0.15928169631350733
06/02/2019 03:54:13 step: 307, epoch: 9, batch: 9, loss: 1.4191064834594727, acc: 42.1875, f1: 12.291813277034953, r: 0.17247374091965031
06/02/2019 03:54:13 step: 312, epoch: 9, batch: 14, loss: 1.5530712604522705, acc: 45.3125, f1: 17.781037960755548, r: 0.20208845676582918
06/02/2019 03:54:14 step: 317, epoch: 9, batch: 19, loss: 1.476062536239624, acc: 48.4375, f1: 16.791208791208792, r: 0.25482154606074914
06/02/2019 03:54:14 step: 322, epoch: 9, batch: 24, loss: 1.5354875326156616, acc: 46.875, f1: 15.780730897009967, r: 0.19353625363380883
06/02/2019 03:54:14 step: 327, epoch: 9, batch: 29, loss: 1.449582576751709, acc: 50.0, f1: 16.3265306122449, r: 0.216039881019782
06/02/2019 03:54:14 *** evaluating ***
06/02/2019 03:54:14 step: 10, epoch: 9, acc: 56.41025641025641, f1: 16.767038777908343, r: 0.23985566731643793
06/02/2019 03:54:14 *** epoch: 11 ***
06/02/2019 03:54:14 *** training ***
06/02/2019 03:54:14 step: 335, epoch: 10, batch: 4, loss: 1.519334316253662, acc: 42.1875, f1: 9.940476190476192, r: 0.24974412197822204
06/02/2019 03:54:15 step: 340, epoch: 10, batch: 9, loss: 1.7103285789489746, acc: 34.375, f1: 10.197368421052632, r: 0.2790868623591581
06/02/2019 03:54:15 step: 345, epoch: 10, batch: 14, loss: 1.3711600303649902, acc: 57.8125, f1: 18.12467260345731, r: 0.2695626284879757
06/02/2019 03:54:15 step: 350, epoch: 10, batch: 19, loss: 1.7232065200805664, acc: 35.9375, f1: 12.694805194805195, r: 0.2607690636818957
06/02/2019 03:54:15 step: 355, epoch: 10, batch: 24, loss: 1.5286802053451538, acc: 40.625, f1: 15.208333333333332, r: 0.31393802114843017
06/02/2019 03:54:15 step: 360, epoch: 10, batch: 29, loss: 1.5001248121261597, acc: 51.5625, f1: 19.07975065869803, r: 0.19269044792644918
06/02/2019 03:54:15 *** evaluating ***
06/02/2019 03:54:16 step: 11, epoch: 10, acc: 55.98290598290598, f1: 16.536281179138324, r: 0.2886641691088736
06/02/2019 03:54:16 *** epoch: 12 ***
06/02/2019 03:54:16 *** training ***
06/02/2019 03:54:16 step: 368, epoch: 11, batch: 4, loss: 1.580178141593933, acc: 46.875, f1: 20.172155097528233, r: 0.28413108332820775
06/02/2019 03:54:16 step: 373, epoch: 11, batch: 9, loss: 1.5849919319152832, acc: 29.6875, f1: 13.988095238095239, r: 0.2405833263365226
06/02/2019 03:54:16 step: 378, epoch: 11, batch: 14, loss: 1.6313008069992065, acc: 40.625, f1: 13.142857142857146, r: 0.2711969189220488
06/02/2019 03:54:16 step: 383, epoch: 11, batch: 19, loss: 1.6315560340881348, acc: 43.75, f1: 8.79120879120879, r: 0.11585775841602163
06/02/2019 03:54:17 step: 388, epoch: 11, batch: 24, loss: 1.5092428922653198, acc: 42.1875, f1: 13.227513227513226, r: 0.2600278474183524
06/02/2019 03:54:17 step: 393, epoch: 11, batch: 29, loss: 1.489254355430603, acc: 48.4375, f1: 19.32013769363167, r: 0.3119859791583104
06/02/2019 03:54:17 *** evaluating ***
06/02/2019 03:54:17 step: 12, epoch: 11, acc: 55.55555555555556, f1: 16.37414383561644, r: 0.28979321557111615
06/02/2019 03:54:17 *** epoch: 13 ***
06/02/2019 03:54:17 *** training ***
06/02/2019 03:54:17 step: 401, epoch: 12, batch: 4, loss: 1.7505115270614624, acc: 35.9375, f1: 10.205314009661837, r: 0.27293350001069494
06/02/2019 03:54:17 step: 406, epoch: 12, batch: 9, loss: 1.5996761322021484, acc: 40.625, f1: 12.07843137254902, r: 0.1948930021955962
06/02/2019 03:54:18 step: 411, epoch: 12, batch: 14, loss: 1.5890140533447266, acc: 42.1875, f1: 10.686274509803923, r: 0.24515711631513978
06/02/2019 03:54:18 step: 416, epoch: 12, batch: 19, loss: 1.4879695177078247, acc: 50.0, f1: 18.17921101268243, r: 0.2178475217281798
06/02/2019 03:54:18 step: 421, epoch: 12, batch: 24, loss: 1.4207344055175781, acc: 46.875, f1: 13.255813953488374, r: 0.21980561055031828
06/02/2019 03:54:18 step: 426, epoch: 12, batch: 29, loss: 1.573708176612854, acc: 35.9375, f1: 10.892857142857142, r: 0.19209033767572598
06/02/2019 03:54:18 *** evaluating ***
06/02/2019 03:54:18 step: 13, epoch: 12, acc: 56.41025641025641, f1: 16.821969696969695, r: 0.27431074951863416
06/02/2019 03:54:18 *** epoch: 14 ***
06/02/2019 03:54:18 *** training ***
06/02/2019 03:54:19 step: 434, epoch: 13, batch: 4, loss: 1.5297157764434814, acc: 40.625, f1: 14.737793851717901, r: 0.23829282201418
06/02/2019 03:54:19 step: 439, epoch: 13, batch: 9, loss: 1.5439013242721558, acc: 45.3125, f1: 13.18181818181818, r: 0.19498005367623575
06/02/2019 03:54:19 step: 444, epoch: 13, batch: 14, loss: 1.5079693794250488, acc: 48.4375, f1: 16.009852216748772, r: 0.21467729607680808
06/02/2019 03:54:19 step: 449, epoch: 13, batch: 19, loss: 1.3338210582733154, acc: 46.875, f1: 14.294996751137102, r: 0.3076432314643965
06/02/2019 03:54:19 step: 454, epoch: 13, batch: 24, loss: 1.5211211442947388, acc: 42.1875, f1: 11.542305129913391, r: 0.23029975401610975
06/02/2019 03:54:20 step: 459, epoch: 13, batch: 29, loss: 1.4217373132705688, acc: 51.5625, f1: 16.666666666666664, r: 0.2862744316950112
06/02/2019 03:54:20 *** evaluating ***
06/02/2019 03:54:20 step: 14, epoch: 13, acc: 54.700854700854705, f1: 15.9543264194427, r: 0.272317501158285
06/02/2019 03:54:20 *** epoch: 15 ***
06/02/2019 03:54:20 *** training ***
06/02/2019 03:54:20 step: 467, epoch: 14, batch: 4, loss: 1.664290428161621, acc: 35.9375, f1: 12.422839506172838, r: 0.2619996280054496
06/02/2019 03:54:20 step: 472, epoch: 14, batch: 9, loss: 1.6041983366012573, acc: 32.8125, f1: 9.911816578483243, r: 0.26048268703242006
06/02/2019 03:54:20 step: 477, epoch: 14, batch: 14, loss: 1.4136347770690918, acc: 53.125, f1: 18.098251959011456, r: 0.280553946070296
06/02/2019 03:54:21 step: 482, epoch: 14, batch: 19, loss: 1.3217582702636719, acc: 51.5625, f1: 16.14100185528757, r: 0.29555103956760226
06/02/2019 03:54:21 step: 487, epoch: 14, batch: 24, loss: 1.6182597875595093, acc: 42.1875, f1: 14.48481831757093, r: 0.22711480033884723
06/02/2019 03:54:21 step: 492, epoch: 14, batch: 29, loss: 1.5798180103302002, acc: 42.1875, f1: 12.040385774562989, r: 0.20590095157870983
06/02/2019 03:54:21 *** evaluating ***
06/02/2019 03:54:21 step: 15, epoch: 14, acc: 56.41025641025641, f1: 16.827616827616826, r: 0.28354351079413825
06/02/2019 03:54:21 *** epoch: 16 ***
06/02/2019 03:54:21 *** training ***
06/02/2019 03:54:21 step: 500, epoch: 15, batch: 4, loss: 1.4092835187911987, acc: 43.75, f1: 14.707792207792208, r: 0.28745328210892307
06/02/2019 03:54:22 step: 505, epoch: 15, batch: 9, loss: 1.3485727310180664, acc: 48.4375, f1: 17.290249433106574, r: 0.25298623103262463
06/02/2019 03:54:22 step: 510, epoch: 15, batch: 14, loss: 1.4070961475372314, acc: 45.3125, f1: 16.684303350970016, r: 0.3354517359482304
06/02/2019 03:54:22 step: 515, epoch: 15, batch: 19, loss: 1.5581549406051636, acc: 37.5, f1: 21.07983193277311, r: 0.27923932126617035
06/02/2019 03:54:22 step: 520, epoch: 15, batch: 24, loss: 1.2280313968658447, acc: 56.25, f1: 21.928904428904428, r: 0.3422661686753464
06/02/2019 03:54:22 step: 525, epoch: 15, batch: 29, loss: 1.4815503358840942, acc: 45.3125, f1: 17.04059829059829, r: 0.2927837473451529
06/02/2019 03:54:22 *** evaluating ***
06/02/2019 03:54:23 step: 16, epoch: 15, acc: 54.700854700854705, f1: 15.876462782000239, r: 0.3197998786443118
06/02/2019 03:54:23 *** epoch: 17 ***
06/02/2019 03:54:23 *** training ***
06/02/2019 03:54:23 step: 533, epoch: 16, batch: 4, loss: 1.4878064393997192, acc: 40.625, f1: 9.705284552845528, r: 0.24363098764126567
06/02/2019 03:54:23 step: 538, epoch: 16, batch: 9, loss: 1.5290182828903198, acc: 43.75, f1: 12.914639540026537, r: 0.1881106758815935
06/02/2019 03:54:23 step: 543, epoch: 16, batch: 14, loss: 1.4375392198562622, acc: 43.75, f1: 16.22237411711096, r: 0.2719237267389727
06/02/2019 03:54:23 step: 548, epoch: 16, batch: 19, loss: 1.5127065181732178, acc: 45.3125, f1: 14.867663981588034, r: 0.19043072013385173
06/02/2019 03:54:24 step: 553, epoch: 16, batch: 24, loss: 1.3429498672485352, acc: 43.75, f1: 13.825363825363826, r: 0.34979774873399644
06/02/2019 03:54:24 step: 558, epoch: 16, batch: 29, loss: 1.4781426191329956, acc: 50.0, f1: 17.4655908875178, r: 0.24618905200131933
06/02/2019 03:54:24 *** evaluating ***
06/02/2019 03:54:24 step: 17, epoch: 16, acc: 57.692307692307686, f1: 17.322882807311874, r: 0.29238438691555524
06/02/2019 03:54:24 *** epoch: 18 ***
06/02/2019 03:54:24 *** training ***
06/02/2019 03:54:24 step: 566, epoch: 17, batch: 4, loss: 1.369589924812317, acc: 45.3125, f1: 16.276140085663897, r: 0.3244455662639509
06/02/2019 03:54:24 step: 571, epoch: 17, batch: 9, loss: 1.3961780071258545, acc: 43.75, f1: 20.939822082679225, r: 0.38707772340047497
06/02/2019 03:54:25 step: 576, epoch: 17, batch: 14, loss: 1.449271559715271, acc: 46.875, f1: 20.021854895804477, r: 0.22018734975680634
06/02/2019 03:54:25 step: 581, epoch: 17, batch: 19, loss: 1.3630861043930054, acc: 54.6875, f1: 18.45174973488865, r: 0.30922046189728797
06/02/2019 03:54:25 step: 586, epoch: 17, batch: 24, loss: 1.536947250366211, acc: 35.9375, f1: 12.916666666666664, r: 0.2712367899170804
06/02/2019 03:54:25 step: 591, epoch: 17, batch: 29, loss: 1.5324894189834595, acc: 40.625, f1: 14.975845410628022, r: 0.22890566931170972
06/02/2019 03:54:25 *** evaluating ***
06/02/2019 03:54:25 step: 18, epoch: 17, acc: 55.55555555555556, f1: 16.393596632971327, r: 0.2969342646471016
06/02/2019 03:54:25 *** epoch: 19 ***
06/02/2019 03:54:25 *** training ***
06/02/2019 03:54:26 step: 599, epoch: 18, batch: 4, loss: 1.4109177589416504, acc: 40.625, f1: 11.510530137981117, r: 0.31716001076209066
06/02/2019 03:54:26 step: 604, epoch: 18, batch: 9, loss: 1.263316035270691, acc: 46.875, f1: 16.542359915853893, r: 0.28979412494727985
06/02/2019 03:54:26 step: 609, epoch: 18, batch: 14, loss: 1.2440015077590942, acc: 54.6875, f1: 15.753968253968253, r: 0.3111011979230943
06/02/2019 03:54:26 step: 614, epoch: 18, batch: 19, loss: 1.5316872596740723, acc: 39.0625, f1: 13.588922493032083, r: 0.31786009554227634
06/02/2019 03:54:26 step: 619, epoch: 18, batch: 24, loss: 1.3801535367965698, acc: 46.875, f1: 11.60361377752682, r: 0.16994686507303552
06/02/2019 03:54:26 step: 624, epoch: 18, batch: 29, loss: 1.4722861051559448, acc: 32.8125, f1: 10.134310134310134, r: 0.3395917971003385
06/02/2019 03:54:27 *** evaluating ***
06/02/2019 03:54:27 step: 19, epoch: 18, acc: 56.837606837606835, f1: 16.87016185784659, r: 0.3183102898281244
06/02/2019 03:54:27 *** epoch: 20 ***
06/02/2019 03:54:27 *** training ***
06/02/2019 03:54:27 step: 632, epoch: 19, batch: 4, loss: 1.4543565511703491, acc: 43.75, f1: 13.055555555555557, r: 0.3199121956719176
06/02/2019 03:54:27 step: 637, epoch: 19, batch: 9, loss: 1.6603132486343384, acc: 40.625, f1: 17.447767556081487, r: 0.22116691371306801
06/02/2019 03:54:27 step: 642, epoch: 19, batch: 14, loss: 1.2317230701446533, acc: 48.4375, f1: 16.72077922077922, r: 0.377843507435585
06/02/2019 03:54:27 step: 647, epoch: 19, batch: 19, loss: 1.3138062953948975, acc: 45.3125, f1: 15.737451737451735, r: 0.27807358277601396
06/02/2019 03:54:28 step: 652, epoch: 19, batch: 24, loss: 1.3367551565170288, acc: 43.75, f1: 16.870290203623536, r: 0.3284218535090864
06/02/2019 03:54:28 step: 657, epoch: 19, batch: 29, loss: 1.5069538354873657, acc: 43.75, f1: 13.84493670886076, r: 0.28173560172826845
06/02/2019 03:54:28 *** evaluating ***
06/02/2019 03:54:28 step: 20, epoch: 19, acc: 57.26495726495726, f1: 17.31990530231859, r: 0.2904650448675376
06/02/2019 03:54:28 *** epoch: 21 ***
06/02/2019 03:54:28 *** training ***
06/02/2019 03:54:28 step: 665, epoch: 20, batch: 4, loss: 1.2722357511520386, acc: 50.0, f1: 19.25327969848518, r: 0.3025151143892587
06/02/2019 03:54:29 step: 670, epoch: 20, batch: 9, loss: 1.3644846677780151, acc: 43.75, f1: 18.47222222222222, r: 0.3453593169833462
06/02/2019 03:54:29 step: 675, epoch: 20, batch: 14, loss: 1.3722642660140991, acc: 40.625, f1: 12.558869701726847, r: 0.27769873224352726
06/02/2019 03:54:29 step: 680, epoch: 20, batch: 19, loss: 1.568186640739441, acc: 37.5, f1: 18.103864734299517, r: 0.4067359137973427
06/02/2019 03:54:29 step: 685, epoch: 20, batch: 24, loss: 1.4362534284591675, acc: 40.625, f1: 13.720238095238097, r: 0.35896062876137524
06/02/2019 03:54:29 step: 690, epoch: 20, batch: 29, loss: 1.3712141513824463, acc: 42.1875, f1: 14.7575493612079, r: 0.26974838564294856
06/02/2019 03:54:29 *** evaluating ***
06/02/2019 03:54:30 step: 21, epoch: 20, acc: 56.837606837606835, f1: 17.039057002111193, r: 0.30177756349260176
06/02/2019 03:54:30 *** epoch: 22 ***
06/02/2019 03:54:30 *** training ***
06/02/2019 03:54:30 step: 698, epoch: 21, batch: 4, loss: 1.2303862571716309, acc: 45.3125, f1: 21.63368178998805, r: 0.27987607115462676
06/02/2019 03:54:30 step: 703, epoch: 21, batch: 9, loss: 1.2892860174179077, acc: 45.3125, f1: 20.66774923917781, r: 0.36068559817878776
06/02/2019 03:54:30 step: 708, epoch: 21, batch: 14, loss: 1.2217686176300049, acc: 51.5625, f1: 18.60902255639098, r: 0.2646509927638919
06/02/2019 03:54:30 step: 713, epoch: 21, batch: 19, loss: 1.377544641494751, acc: 37.5, f1: 14.821428571428571, r: 0.4528397283733019
06/02/2019 03:54:31 step: 718, epoch: 21, batch: 24, loss: 1.2135869264602661, acc: 54.6875, f1: 27.84026149975269, r: 0.28911559578604346
06/02/2019 03:54:31 step: 723, epoch: 21, batch: 29, loss: 1.2643628120422363, acc: 51.5625, f1: 16.34908536585366, r: 0.3875150261965274
06/02/2019 03:54:31 *** evaluating ***
06/02/2019 03:54:31 step: 22, epoch: 21, acc: 55.12820512820513, f1: 16.819284583546327, r: 0.31068687752358515
06/02/2019 03:54:31 *** epoch: 23 ***
06/02/2019 03:54:31 *** training ***
06/02/2019 03:54:31 step: 731, epoch: 22, batch: 4, loss: 1.4924360513687134, acc: 43.75, f1: 16.924342105263154, r: 0.32264819312616044
06/02/2019 03:54:31 step: 736, epoch: 22, batch: 9, loss: 1.2004503011703491, acc: 48.4375, f1: 18.90538033395176, r: 0.3575740444612583
06/02/2019 03:54:32 step: 741, epoch: 22, batch: 14, loss: 1.336686134338379, acc: 50.0, f1: 19.405949741315595, r: 0.40820335932063806
06/02/2019 03:54:32 step: 746, epoch: 22, batch: 19, loss: 1.339614748954773, acc: 50.0, f1: 16.71245421245421, r: 0.36797264563236703
06/02/2019 03:54:32 step: 751, epoch: 22, batch: 24, loss: 1.3787603378295898, acc: 43.75, f1: 15.854700854700853, r: 0.3100320934102731
06/02/2019 03:54:32 step: 756, epoch: 22, batch: 29, loss: 1.2921301126480103, acc: 48.4375, f1: 18.240274599542335, r: 0.3639069306180759
06/02/2019 03:54:32 *** evaluating ***
06/02/2019 03:54:32 step: 23, epoch: 22, acc: 56.837606837606835, f1: 17.099073672022, r: 0.2952825412384842
06/02/2019 03:54:32 *** epoch: 24 ***
06/02/2019 03:54:32 *** training ***
06/02/2019 03:54:33 step: 764, epoch: 23, batch: 4, loss: 1.4493138790130615, acc: 42.1875, f1: 17.13196033562166, r: 0.2654259720102766
06/02/2019 03:54:33 step: 769, epoch: 23, batch: 9, loss: 1.1073553562164307, acc: 56.25, f1: 16.7960088691796, r: 0.3555595172375904
06/02/2019 03:54:33 step: 774, epoch: 23, batch: 14, loss: 1.5246304273605347, acc: 46.875, f1: 20.159602302459444, r: 0.319715690111744
06/02/2019 03:54:33 step: 779, epoch: 23, batch: 19, loss: 1.053144097328186, acc: 59.375, f1: 33.723215024406386, r: 0.4026252311780992
06/02/2019 03:54:33 step: 784, epoch: 23, batch: 24, loss: 1.1675946712493896, acc: 50.0, f1: 20.579136403543774, r: 0.3607334420056997
06/02/2019 03:54:34 step: 789, epoch: 23, batch: 29, loss: 1.449181318283081, acc: 45.3125, f1: 19.769901460042302, r: 0.2925277036205604
06/02/2019 03:54:34 *** evaluating ***
06/02/2019 03:54:34 step: 24, epoch: 23, acc: 54.27350427350427, f1: 15.719914802981894, r: 0.29994820635711217
06/02/2019 03:54:34 *** epoch: 25 ***
06/02/2019 03:54:34 *** training ***
06/02/2019 03:54:34 step: 797, epoch: 24, batch: 4, loss: 1.3199933767318726, acc: 43.75, f1: 15.132783882783885, r: 0.3495680316754754
06/02/2019 03:54:34 step: 802, epoch: 24, batch: 9, loss: 1.310664415359497, acc: 53.125, f1: 20.346283783783786, r: 0.39595854777296924
06/02/2019 03:54:34 step: 807, epoch: 24, batch: 14, loss: 1.2350285053253174, acc: 53.125, f1: 24.294755152721986, r: 0.3490739827846467
06/02/2019 03:54:34 step: 812, epoch: 24, batch: 19, loss: 1.263026237487793, acc: 45.3125, f1: 18.020309573104605, r: 0.36911309868407355
06/02/2019 03:54:35 step: 817, epoch: 24, batch: 24, loss: 1.161757469177246, acc: 51.5625, f1: 26.0595542564852, r: 0.3442347104368928
06/02/2019 03:54:35 step: 822, epoch: 24, batch: 29, loss: 1.3389664888381958, acc: 50.0, f1: 14.812467214547997, r: 0.26351691984930914
06/02/2019 03:54:35 *** evaluating ***
06/02/2019 03:54:35 step: 25, epoch: 24, acc: 56.837606837606835, f1: 17.803908434296364, r: 0.29148267942955186
06/02/2019 03:54:35 *** epoch: 26 ***
06/02/2019 03:54:35 *** training ***
06/02/2019 03:54:35 step: 830, epoch: 25, batch: 4, loss: 1.2245548963546753, acc: 50.0, f1: 24.293272864701436, r: 0.3001955781528652
06/02/2019 03:54:36 step: 835, epoch: 25, batch: 9, loss: 1.4072301387786865, acc: 48.4375, f1: 15.32738095238095, r: 0.2487673995686931
06/02/2019 03:54:36 step: 840, epoch: 25, batch: 14, loss: 1.2473695278167725, acc: 50.0, f1: 20.978868924074405, r: 0.3264819667547242
06/02/2019 03:54:36 step: 845, epoch: 25, batch: 19, loss: 1.1408318281173706, acc: 54.6875, f1: 21.688871473354233, r: 0.3733677816015712
06/02/2019 03:54:36 step: 850, epoch: 25, batch: 24, loss: 1.1954138278961182, acc: 53.125, f1: 25.253105937210567, r: 0.38516951810923805
06/02/2019 03:54:36 step: 855, epoch: 25, batch: 29, loss: 1.3745460510253906, acc: 43.75, f1: 18.235595390524967, r: 0.2718892293667279
06/02/2019 03:54:37 *** evaluating ***
06/02/2019 03:54:37 step: 26, epoch: 25, acc: 57.692307692307686, f1: 17.98501206395943, r: 0.28967570750010685
06/02/2019 03:54:37 *** epoch: 27 ***
06/02/2019 03:54:37 *** training ***
06/02/2019 03:54:37 step: 863, epoch: 26, batch: 4, loss: 1.2018835544586182, acc: 46.875, f1: 21.321699134199136, r: 0.4105268983284698
06/02/2019 03:54:37 step: 868, epoch: 26, batch: 9, loss: 1.3589363098144531, acc: 48.4375, f1: 16.707410236822, r: 0.30677053961543443
06/02/2019 03:54:37 step: 873, epoch: 26, batch: 14, loss: 1.2107707262039185, acc: 56.25, f1: 30.701754385964907, r: 0.34536394445432134
06/02/2019 03:54:38 step: 878, epoch: 26, batch: 19, loss: 1.2957737445831299, acc: 45.3125, f1: 14.560439560439558, r: 0.2839963780084062
06/02/2019 03:54:38 step: 883, epoch: 26, batch: 24, loss: 1.1100307703018188, acc: 59.375, f1: 24.430821371610843, r: 0.3939733326790534
06/02/2019 03:54:38 step: 888, epoch: 26, batch: 29, loss: 1.4090207815170288, acc: 46.875, f1: 20.815656565656564, r: 0.3599136552206291
06/02/2019 03:54:39 *** evaluating ***
06/02/2019 03:54:39 step: 27, epoch: 26, acc: 57.692307692307686, f1: 18.083538771851188, r: 0.3234808644376904
06/02/2019 03:54:39 *** epoch: 28 ***
06/02/2019 03:54:39 *** training ***
06/02/2019 03:54:39 step: 896, epoch: 27, batch: 4, loss: 1.2457133531570435, acc: 46.875, f1: 25.303177298083916, r: 0.33816537322681123
06/02/2019 03:54:39 step: 901, epoch: 27, batch: 9, loss: 1.2346550226211548, acc: 50.0, f1: 30.923821548821547, r: 0.33567894173871116
06/02/2019 03:54:40 step: 906, epoch: 27, batch: 14, loss: 1.4661213159561157, acc: 39.0625, f1: 20.118911408957178, r: 0.3593630241631759
06/02/2019 03:54:40 step: 911, epoch: 27, batch: 19, loss: 1.206856608390808, acc: 53.125, f1: 23.971088435374146, r: 0.33053851368630327
06/02/2019 03:54:40 step: 916, epoch: 27, batch: 24, loss: 1.237781047821045, acc: 53.125, f1: 22.700405885337393, r: 0.3174230457668509
06/02/2019 03:54:40 step: 921, epoch: 27, batch: 29, loss: 1.434546709060669, acc: 42.1875, f1: 19.401392632524708, r: 0.40573227200548384
06/02/2019 03:54:41 *** evaluating ***
06/02/2019 03:54:41 step: 28, epoch: 27, acc: 51.70940170940172, f1: 18.10070306038048, r: 0.21785363092134102
06/02/2019 03:54:41 *** epoch: 29 ***
06/02/2019 03:54:41 *** training ***
06/02/2019 03:54:41 step: 929, epoch: 28, batch: 4, loss: 1.1087944507598877, acc: 60.9375, f1: 27.678338549817422, r: 0.3824724520326462
06/02/2019 03:54:41 step: 934, epoch: 28, batch: 9, loss: 1.1750141382217407, acc: 56.25, f1: 29.637681159420286, r: 0.37994066287574396
06/02/2019 03:54:42 step: 939, epoch: 28, batch: 14, loss: 1.0914721488952637, acc: 57.8125, f1: 32.524380122419345, r: 0.390491247299432
06/02/2019 03:54:42 step: 944, epoch: 28, batch: 19, loss: 1.0778963565826416, acc: 53.125, f1: 24.851325757575758, r: 0.39795980302819933
06/02/2019 03:54:42 step: 949, epoch: 28, batch: 24, loss: 1.2546193599700928, acc: 56.25, f1: 25.76776165011459, r: 0.33152110358806647
06/02/2019 03:54:42 step: 954, epoch: 28, batch: 29, loss: 1.3288593292236328, acc: 39.0625, f1: 15.516163793103448, r: 0.365786128427729
06/02/2019 03:54:43 *** evaluating ***
06/02/2019 03:54:43 step: 29, epoch: 28, acc: 57.692307692307686, f1: 18.330834582708643, r: 0.3135903995920816
06/02/2019 03:54:43 *** epoch: 30 ***
06/02/2019 03:54:43 *** training ***
06/02/2019 03:54:43 step: 962, epoch: 29, batch: 4, loss: 1.2724131345748901, acc: 46.875, f1: 22.270212285456186, r: 0.4202169795193356
06/02/2019 03:54:43 step: 967, epoch: 29, batch: 9, loss: 1.234299659729004, acc: 48.4375, f1: 23.48673300165838, r: 0.3980457198843908
06/02/2019 03:54:43 step: 972, epoch: 29, batch: 14, loss: 1.2337790727615356, acc: 43.75, f1: 20.629458937077064, r: 0.35876603182334216
06/02/2019 03:54:44 step: 977, epoch: 29, batch: 19, loss: 1.013205885887146, acc: 57.8125, f1: 32.66886109282422, r: 0.4035297755458058
06/02/2019 03:54:44 step: 982, epoch: 29, batch: 24, loss: 1.2265114784240723, acc: 46.875, f1: 26.96267696267696, r: 0.41804203291375197
06/02/2019 03:54:44 step: 987, epoch: 29, batch: 29, loss: 1.2438620328903198, acc: 45.3125, f1: 17.748044965786907, r: 0.38832186363035226
06/02/2019 03:54:44 *** evaluating ***
06/02/2019 03:54:45 step: 30, epoch: 29, acc: 56.837606837606835, f1: 17.58585555513883, r: 0.3398644699181177
06/02/2019 03:54:45 *** epoch: 31 ***
06/02/2019 03:54:45 *** training ***
06/02/2019 03:54:45 step: 995, epoch: 30, batch: 4, loss: 1.1993979215621948, acc: 40.625, f1: 16.0941783583293, r: 0.3114096647837685
06/02/2019 03:54:45 step: 1000, epoch: 30, batch: 9, loss: 1.1077226400375366, acc: 50.0, f1: 20.0, r: 0.3787068711229493
06/02/2019 03:54:45 step: 1005, epoch: 30, batch: 14, loss: 1.2949469089508057, acc: 48.4375, f1: 25.817403884630775, r: 0.31037655500475214
06/02/2019 03:54:46 step: 1010, epoch: 30, batch: 19, loss: 1.3250691890716553, acc: 42.1875, f1: 19.30409421087387, r: 0.3561556007042484
06/02/2019 03:54:46 step: 1015, epoch: 30, batch: 24, loss: 1.136399507522583, acc: 50.0, f1: 26.309067688378036, r: 0.3146739596260176
06/02/2019 03:54:46 step: 1020, epoch: 30, batch: 29, loss: 1.2485734224319458, acc: 48.4375, f1: 20.971974648445237, r: 0.3250374053331688
06/02/2019 03:54:46 *** evaluating ***
06/02/2019 03:54:46 step: 31, epoch: 30, acc: 56.837606837606835, f1: 19.044754091157827, r: 0.322180510322775
06/02/2019 03:54:46 *** epoch: 32 ***
06/02/2019 03:54:46 *** training ***
06/02/2019 03:54:47 step: 1028, epoch: 31, batch: 4, loss: 0.9924697875976562, acc: 53.125, f1: 22.437148772552497, r: 0.3510682532287171
06/02/2019 03:54:47 step: 1033, epoch: 31, batch: 9, loss: 1.2902793884277344, acc: 48.4375, f1: 20.621543778801843, r: 0.34877684479196125
06/02/2019 03:54:47 step: 1038, epoch: 31, batch: 14, loss: 1.1488276720046997, acc: 51.5625, f1: 21.463509316770185, r: 0.43078699448934055
06/02/2019 03:54:48 step: 1043, epoch: 31, batch: 19, loss: 1.1332290172576904, acc: 56.25, f1: 20.872803666921314, r: 0.36595661855744466
06/02/2019 03:54:48 step: 1048, epoch: 31, batch: 24, loss: 1.1127617359161377, acc: 48.4375, f1: 35.57823129251701, r: 0.3436591463591597
06/02/2019 03:54:48 step: 1053, epoch: 31, batch: 29, loss: 1.1896274089813232, acc: 51.5625, f1: 21.46990740740741, r: 0.39281217399858775
06/02/2019 03:54:48 *** evaluating ***
06/02/2019 03:54:49 step: 32, epoch: 31, acc: 58.97435897435898, f1: 17.860857990223668, r: 0.327819602907493
06/02/2019 03:54:49 *** epoch: 33 ***
06/02/2019 03:54:49 *** training ***
06/02/2019 03:54:49 step: 1061, epoch: 32, batch: 4, loss: 1.237483024597168, acc: 53.125, f1: 23.53134454198284, r: 0.3515814963206151
06/02/2019 03:54:49 step: 1066, epoch: 32, batch: 9, loss: 1.5980415344238281, acc: 42.1875, f1: 17.286478227654698, r: 0.14045356516027777
06/02/2019 03:54:49 step: 1071, epoch: 32, batch: 14, loss: 0.9616254568099976, acc: 56.25, f1: 20.075224071702948, r: 0.40714473414159325
06/02/2019 03:54:50 step: 1076, epoch: 32, batch: 19, loss: 1.1966723203659058, acc: 50.0, f1: 25.341269841269842, r: 0.34913591044451797
06/02/2019 03:54:50 step: 1081, epoch: 32, batch: 24, loss: 1.020979881286621, acc: 59.375, f1: 29.47110423116615, r: 0.5116271465361574
06/02/2019 03:54:50 step: 1086, epoch: 32, batch: 29, loss: 1.1141939163208008, acc: 51.5625, f1: 28.090380324739805, r: 0.3169228665912121
06/02/2019 03:54:50 *** evaluating ***
06/02/2019 03:54:50 step: 33, epoch: 32, acc: 57.26495726495726, f1: 18.02357836338419, r: 0.3167962908384755
06/02/2019 03:54:50 *** epoch: 34 ***
06/02/2019 03:54:50 *** training ***
06/02/2019 03:54:51 step: 1094, epoch: 33, batch: 4, loss: 1.1750978231430054, acc: 51.5625, f1: 21.13837312113174, r: 0.2978673495360843
06/02/2019 03:54:51 step: 1099, epoch: 33, batch: 9, loss: 1.205804467201233, acc: 46.875, f1: 20.45548654244306, r: 0.3536066467256014
06/02/2019 03:54:51 step: 1104, epoch: 33, batch: 14, loss: 0.9341123104095459, acc: 62.5, f1: 25.213032581453636, r: 0.4152421413085383
06/02/2019 03:54:52 step: 1109, epoch: 33, batch: 19, loss: 1.0145084857940674, acc: 62.5, f1: 39.12786588968569, r: 0.398639177744277
06/02/2019 03:54:52 step: 1114, epoch: 33, batch: 24, loss: 1.1032534837722778, acc: 53.125, f1: 23.777327935222672, r: 0.37743739226779943
06/02/2019 03:54:52 step: 1119, epoch: 33, batch: 29, loss: 1.1829830408096313, acc: 50.0, f1: 24.082561582561585, r: 0.40924287104028445
06/02/2019 03:54:52 *** evaluating ***
06/02/2019 03:54:52 step: 34, epoch: 33, acc: 58.54700854700855, f1: 18.19697182379664, r: 0.3308432144577025
06/02/2019 03:54:52 *** epoch: 35 ***
06/02/2019 03:54:52 *** training ***
06/02/2019 03:54:53 step: 1127, epoch: 34, batch: 4, loss: 1.0312620401382446, acc: 59.375, f1: 24.681133367574045, r: 0.3861077644172608
06/02/2019 03:54:53 step: 1132, epoch: 34, batch: 9, loss: 0.9996832609176636, acc: 59.375, f1: 26.829697396199858, r: 0.48474661680149567
06/02/2019 03:54:53 step: 1137, epoch: 34, batch: 14, loss: 1.198834776878357, acc: 46.875, f1: 28.83080808080808, r: 0.4412634416496903
06/02/2019 03:54:53 step: 1142, epoch: 34, batch: 19, loss: 1.0128504037857056, acc: 59.375, f1: 37.332710862122624, r: 0.32428465840494725
06/02/2019 03:54:54 step: 1147, epoch: 34, batch: 24, loss: 1.1061762571334839, acc: 53.125, f1: 24.66642228739003, r: 0.39505422228548387
06/02/2019 03:54:54 step: 1152, epoch: 34, batch: 29, loss: 1.143417477607727, acc: 48.4375, f1: 24.307110514007068, r: 0.40401890661003126
06/02/2019 03:54:54 *** evaluating ***
06/02/2019 03:54:54 step: 35, epoch: 34, acc: 55.55555555555556, f1: 17.491046646969288, r: 0.301732790536974
06/02/2019 03:54:54 *** epoch: 36 ***
06/02/2019 03:54:54 *** training ***
06/02/2019 03:54:55 step: 1160, epoch: 35, batch: 4, loss: 1.1108486652374268, acc: 53.125, f1: 21.127232142857146, r: 0.38448018760174413
06/02/2019 03:54:55 step: 1165, epoch: 35, batch: 9, loss: 1.254825234413147, acc: 53.125, f1: 31.70634920634921, r: 0.40534928327978514
06/02/2019 03:54:55 step: 1170, epoch: 35, batch: 14, loss: 1.2332178354263306, acc: 46.875, f1: 20.548752834467123, r: 0.3930078226329592
06/02/2019 03:54:55 step: 1175, epoch: 35, batch: 19, loss: 1.2308961153030396, acc: 48.4375, f1: 26.671556432189192, r: 0.2986785620237406
06/02/2019 03:54:56 step: 1180, epoch: 35, batch: 24, loss: 1.1483784914016724, acc: 54.6875, f1: 24.934440559440556, r: 0.45893940884476303
06/02/2019 03:54:56 step: 1185, epoch: 35, batch: 29, loss: 1.0259101390838623, acc: 51.5625, f1: 22.58852258852259, r: 0.41624350811852556
06/02/2019 03:54:56 *** evaluating ***
06/02/2019 03:54:56 step: 36, epoch: 35, acc: 58.119658119658126, f1: 18.291513739883307, r: 0.3093422832537947
06/02/2019 03:54:56 *** epoch: 37 ***
06/02/2019 03:54:56 *** training ***
06/02/2019 03:54:56 step: 1193, epoch: 36, batch: 4, loss: 0.9035380482673645, acc: 65.625, f1: 39.82621437354014, r: 0.45609192967274254
06/02/2019 03:54:57 step: 1198, epoch: 36, batch: 9, loss: 1.2251038551330566, acc: 45.3125, f1: 33.094220594220594, r: 0.4217595139244326
06/02/2019 03:54:57 step: 1203, epoch: 36, batch: 14, loss: 1.0662745237350464, acc: 57.8125, f1: 29.555380852550662, r: 0.4344342165385469
06/02/2019 03:54:57 step: 1208, epoch: 36, batch: 19, loss: 1.2365466356277466, acc: 51.5625, f1: 25.420361247947454, r: 0.4209586817904884
06/02/2019 03:54:58 step: 1213, epoch: 36, batch: 24, loss: 1.134402871131897, acc: 57.8125, f1: 37.63586956521739, r: 0.39674984137644587
06/02/2019 03:54:58 step: 1218, epoch: 36, batch: 29, loss: 1.018297791481018, acc: 57.8125, f1: 25.27103331451157, r: 0.3832349710519923
06/02/2019 03:54:58 *** evaluating ***
06/02/2019 03:54:58 step: 37, epoch: 36, acc: 56.41025641025641, f1: 18.98094477632539, r: 0.33815516976902577
06/02/2019 03:54:58 *** epoch: 38 ***
06/02/2019 03:54:58 *** training ***
06/02/2019 03:54:59 step: 1226, epoch: 37, batch: 4, loss: 0.818666934967041, acc: 62.5, f1: 39.25112417848496, r: 0.4763083124954965
06/02/2019 03:54:59 step: 1231, epoch: 37, batch: 9, loss: 0.9733564853668213, acc: 57.8125, f1: 26.359912665331386, r: 0.4267911887585679
06/02/2019 03:54:59 step: 1236, epoch: 37, batch: 14, loss: 1.1136153936386108, acc: 54.6875, f1: 36.67061714348922, r: 0.4289840877223089
06/02/2019 03:54:59 step: 1241, epoch: 37, batch: 19, loss: 0.9941363334655762, acc: 54.6875, f1: 30.905282331511835, r: 0.4579410481347606
06/02/2019 03:55:00 step: 1246, epoch: 37, batch: 24, loss: 0.8829519748687744, acc: 65.625, f1: 30.360848357452774, r: 0.4284893831241483
06/02/2019 03:55:00 step: 1251, epoch: 37, batch: 29, loss: 1.0932576656341553, acc: 46.875, f1: 31.459705314152757, r: 0.43630319992348965
06/02/2019 03:55:00 *** evaluating ***
06/02/2019 03:55:00 step: 38, epoch: 37, acc: 57.692307692307686, f1: 20.160006226806217, r: 0.34870461692367616
06/02/2019 03:55:00 *** epoch: 39 ***
06/02/2019 03:55:00 *** training ***
06/02/2019 03:55:01 step: 1259, epoch: 38, batch: 4, loss: 1.0511857271194458, acc: 54.6875, f1: 29.25086918730986, r: 0.4608799097727129
06/02/2019 03:55:01 step: 1264, epoch: 38, batch: 9, loss: 1.067811369895935, acc: 50.0, f1: 23.773567119155352, r: 0.5218421419615532
06/02/2019 03:55:01 step: 1269, epoch: 38, batch: 14, loss: 0.8991411328315735, acc: 57.8125, f1: 28.764665286404416, r: 0.45765875514355087
06/02/2019 03:55:01 step: 1274, epoch: 38, batch: 19, loss: 1.3163200616836548, acc: 40.625, f1: 19.180301807420452, r: 0.3848191938700085
06/02/2019 03:55:02 step: 1279, epoch: 38, batch: 24, loss: 0.752468466758728, acc: 64.0625, f1: 30.796932536062975, r: 0.4604969324699829
06/02/2019 03:55:02 step: 1284, epoch: 38, batch: 29, loss: 1.1402966976165771, acc: 46.875, f1: 25.804988662131517, r: 0.4478420017193184
06/02/2019 03:55:02 *** evaluating ***
06/02/2019 03:55:02 step: 39, epoch: 38, acc: 47.43589743589743, f1: 18.9733062767654, r: 0.3144643861153042
06/02/2019 03:55:02 *** epoch: 40 ***
06/02/2019 03:55:02 *** training ***
06/02/2019 03:55:03 step: 1292, epoch: 39, batch: 4, loss: 1.0613396167755127, acc: 53.125, f1: 29.677158999192905, r: 0.36254835918341155
06/02/2019 03:55:03 step: 1297, epoch: 39, batch: 9, loss: 1.0121158361434937, acc: 54.6875, f1: 36.869300911854104, r: 0.38350331645442964
06/02/2019 03:55:03 step: 1302, epoch: 39, batch: 14, loss: 0.9931468963623047, acc: 62.5, f1: 28.177966101694913, r: 0.33237066962916323
06/02/2019 03:55:03 step: 1307, epoch: 39, batch: 19, loss: 0.8597500920295715, acc: 67.1875, f1: 29.479316979316977, r: 0.40788269961800233
06/02/2019 03:55:04 step: 1312, epoch: 39, batch: 24, loss: 1.1573166847229004, acc: 50.0, f1: 22.78567992853707, r: 0.38206809198343156
06/02/2019 03:55:04 step: 1317, epoch: 39, batch: 29, loss: 1.3599975109100342, acc: 39.0625, f1: 16.18073316283035, r: 0.36812791121892074
06/02/2019 03:55:04 *** evaluating ***
06/02/2019 03:55:04 step: 40, epoch: 39, acc: 35.8974358974359, f1: 16.100375738056897, r: 0.2753341136829302
06/02/2019 03:55:04 *** epoch: 41 ***
06/02/2019 03:55:04 *** training ***
06/02/2019 03:55:05 step: 1325, epoch: 40, batch: 4, loss: 1.0282551050186157, acc: 53.125, f1: 21.7775666885574, r: 0.39478263534573105
06/02/2019 03:55:05 step: 1330, epoch: 40, batch: 9, loss: 0.8997558355331421, acc: 65.625, f1: 41.1481766080043, r: 0.4387339263184071
06/02/2019 03:55:05 step: 1335, epoch: 40, batch: 14, loss: 1.0709526538848877, acc: 54.6875, f1: 24.548277809147372, r: 0.46697673932751294
06/02/2019 03:55:06 step: 1340, epoch: 40, batch: 19, loss: 0.872136652469635, acc: 56.25, f1: 31.230212573496157, r: 0.4940950876679147
06/02/2019 03:55:06 step: 1345, epoch: 40, batch: 24, loss: 1.0599663257598877, acc: 56.25, f1: 30.84249755620723, r: 0.4548331421752932
06/02/2019 03:55:06 step: 1350, epoch: 40, batch: 29, loss: 0.9541014432907104, acc: 62.5, f1: 41.419547872340424, r: 0.5117763906320344
06/02/2019 03:55:06 *** evaluating ***
06/02/2019 03:55:07 step: 41, epoch: 40, acc: 55.98290598290598, f1: 21.59878133562344, r: 0.35266579972930756
06/02/2019 03:55:07 *** epoch: 42 ***
06/02/2019 03:55:07 *** training ***
06/02/2019 03:55:07 step: 1358, epoch: 41, batch: 4, loss: 0.8148695826530457, acc: 65.625, f1: 27.637084133886137, r: 0.46688002880226437
06/02/2019 03:55:07 step: 1363, epoch: 41, batch: 9, loss: 0.8257601261138916, acc: 62.5, f1: 32.71284271284271, r: 0.4898036574598086
06/02/2019 03:55:07 step: 1368, epoch: 41, batch: 14, loss: 0.7902542352676392, acc: 67.1875, f1: 24.889441680486453, r: 0.3554467928737572
06/02/2019 03:55:08 step: 1373, epoch: 41, batch: 19, loss: 1.0512405633926392, acc: 50.0, f1: 28.080167884959366, r: 0.4600645929467413
06/02/2019 03:55:08 step: 1378, epoch: 41, batch: 24, loss: 1.0781919956207275, acc: 57.8125, f1: 42.88220551378446, r: 0.4643404196551463
06/02/2019 03:55:08 step: 1383, epoch: 41, batch: 29, loss: 0.877419114112854, acc: 65.625, f1: 34.37359878037844, r: 0.38392246328192803
06/02/2019 03:55:08 *** evaluating ***
06/02/2019 03:55:09 step: 42, epoch: 41, acc: 55.12820512820513, f1: 20.13601195384041, r: 0.32092952852827866
06/02/2019 03:55:09 *** epoch: 43 ***
06/02/2019 03:55:09 *** training ***
06/02/2019 03:55:09 step: 1391, epoch: 42, batch: 4, loss: 0.8676058650016785, acc: 65.625, f1: 40.19240019240019, r: 0.465635792205791
06/02/2019 03:55:09 step: 1396, epoch: 42, batch: 9, loss: 0.9360045194625854, acc: 59.375, f1: 31.05324787378282, r: 0.4276663438094222
06/02/2019 03:55:10 step: 1401, epoch: 42, batch: 14, loss: 0.9679703712463379, acc: 62.5, f1: 31.376502213788736, r: 0.45489016483829153
06/02/2019 03:55:10 step: 1406, epoch: 42, batch: 19, loss: 0.9932963252067566, acc: 59.375, f1: 44.27777777777777, r: 0.42686754587947323
06/02/2019 03:55:10 step: 1411, epoch: 42, batch: 24, loss: 0.9634780883789062, acc: 53.125, f1: 37.71978021978022, r: 0.46903167395367623
06/02/2019 03:55:10 step: 1416, epoch: 42, batch: 29, loss: 1.0441824197769165, acc: 50.0, f1: 27.77983704000534, r: 0.41390172094095334
06/02/2019 03:55:11 *** evaluating ***
06/02/2019 03:55:11 step: 43, epoch: 42, acc: 58.119658119658126, f1: 19.791871808000842, r: 0.35435135912860466
06/02/2019 03:55:11 *** epoch: 44 ***
06/02/2019 03:55:11 *** training ***
06/02/2019 03:55:11 step: 1424, epoch: 43, batch: 4, loss: 0.8981093764305115, acc: 60.9375, f1: 28.37234893957583, r: 0.46538044767117487
06/02/2019 03:55:11 step: 1429, epoch: 43, batch: 9, loss: 0.778856635093689, acc: 68.75, f1: 38.689479222220626, r: 0.4803458744603798
06/02/2019 03:55:12 step: 1434, epoch: 43, batch: 14, loss: 0.958784818649292, acc: 56.25, f1: 31.62698412698413, r: 0.4984447532658046
06/02/2019 03:55:12 step: 1439, epoch: 43, batch: 19, loss: 0.9894675016403198, acc: 59.375, f1: 24.075596816976123, r: 0.4580610186178582
06/02/2019 03:55:12 step: 1444, epoch: 43, batch: 24, loss: 0.7808355689048767, acc: 67.1875, f1: 35.4531490015361, r: 0.47224977896040005
06/02/2019 03:55:12 step: 1449, epoch: 43, batch: 29, loss: 0.7692407369613647, acc: 70.3125, f1: 50.69264069264069, r: 0.5061432860044921
06/02/2019 03:55:13 *** evaluating ***
06/02/2019 03:55:13 step: 44, epoch: 43, acc: 58.119658119658126, f1: 22.707683083945025, r: 0.34638255079626107
06/02/2019 03:55:13 *** epoch: 45 ***
06/02/2019 03:55:13 *** training ***
06/02/2019 03:55:13 step: 1457, epoch: 44, batch: 4, loss: 0.9715020060539246, acc: 59.375, f1: 31.658803872426162, r: 0.4374271909150798
06/02/2019 03:55:13 step: 1462, epoch: 44, batch: 9, loss: 0.7783053517341614, acc: 71.875, f1: 40.190030557677616, r: 0.5467505886688103
06/02/2019 03:55:13 step: 1467, epoch: 44, batch: 14, loss: 0.9665444493293762, acc: 56.25, f1: 24.929113268525654, r: 0.33047767262164035
06/02/2019 03:55:14 step: 1472, epoch: 44, batch: 19, loss: 0.8206375241279602, acc: 71.875, f1: 41.458832749400706, r: 0.4301225731799669
06/02/2019 03:55:14 step: 1477, epoch: 44, batch: 24, loss: 0.8974043726921082, acc: 60.9375, f1: 37.10127957098734, r: 0.4760780319854725
06/02/2019 03:55:14 step: 1482, epoch: 44, batch: 29, loss: 0.8988949060440063, acc: 68.75, f1: 40.306360306360304, r: 0.439075950379806
06/02/2019 03:55:14 *** evaluating ***
06/02/2019 03:55:15 step: 45, epoch: 44, acc: 58.97435897435898, f1: 22.230573607242892, r: 0.36829284129644224
06/02/2019 03:55:15 *** epoch: 46 ***
06/02/2019 03:55:15 *** training ***
06/02/2019 03:55:15 step: 1490, epoch: 45, batch: 4, loss: 1.0326581001281738, acc: 54.6875, f1: 24.35699893327012, r: 0.42154738839317984
06/02/2019 03:55:15 step: 1495, epoch: 45, batch: 9, loss: 0.8911846876144409, acc: 59.375, f1: 28.102598566308245, r: 0.4921431000109738
06/02/2019 03:55:15 step: 1500, epoch: 45, batch: 14, loss: 0.96197110414505, acc: 54.6875, f1: 30.456105456105455, r: 0.5654144719148717
06/02/2019 03:55:16 step: 1505, epoch: 45, batch: 19, loss: 0.8699512481689453, acc: 64.0625, f1: 39.25239061141698, r: 0.4767273690362956
06/02/2019 03:55:16 step: 1510, epoch: 45, batch: 24, loss: 0.7376649975776672, acc: 68.75, f1: 39.1173777203189, r: 0.513162143558944
06/02/2019 03:55:16 step: 1515, epoch: 45, batch: 29, loss: 0.8813355565071106, acc: 59.375, f1: 34.55774853801169, r: 0.5051490802132391
06/02/2019 03:55:16 *** evaluating ***
06/02/2019 03:55:16 step: 46, epoch: 45, acc: 55.98290598290598, f1: 19.691107428887047, r: 0.3552775028477024
06/02/2019 03:55:16 *** epoch: 47 ***
06/02/2019 03:55:16 *** training ***
06/02/2019 03:55:17 step: 1523, epoch: 46, batch: 4, loss: 0.7975375652313232, acc: 62.5, f1: 29.040060468631896, r: 0.4306304963308939
06/02/2019 03:55:17 step: 1528, epoch: 46, batch: 9, loss: 1.10513174533844, acc: 53.125, f1: 36.504467754467754, r: 0.45252867114592754
06/02/2019 03:55:17 step: 1533, epoch: 46, batch: 14, loss: 0.7745457887649536, acc: 62.5, f1: 30.94393377412245, r: 0.5030268816485669
06/02/2019 03:55:18 step: 1538, epoch: 46, batch: 19, loss: 1.0215991735458374, acc: 54.6875, f1: 44.63533292104721, r: 0.4007209078731882
06/02/2019 03:55:18 step: 1543, epoch: 46, batch: 24, loss: 1.0785057544708252, acc: 46.875, f1: 21.76177536231884, r: 0.4577718826004479
06/02/2019 03:55:18 step: 1548, epoch: 46, batch: 29, loss: 0.9543972611427307, acc: 53.125, f1: 24.28973356392711, r: 0.45431664375510117
06/02/2019 03:55:18 *** evaluating ***
06/02/2019 03:55:18 step: 47, epoch: 46, acc: 58.97435897435898, f1: 21.174048699412086, r: 0.3500978373386248
06/02/2019 03:55:18 *** epoch: 48 ***
06/02/2019 03:55:18 *** training ***
06/02/2019 03:55:19 step: 1556, epoch: 47, batch: 4, loss: 0.7769246697425842, acc: 64.0625, f1: 29.238816738816737, r: 0.5440901560854929
06/02/2019 03:55:19 step: 1561, epoch: 47, batch: 9, loss: 0.8955509066581726, acc: 59.375, f1: 32.33603786342123, r: 0.4587648753341483
06/02/2019 03:55:19 step: 1566, epoch: 47, batch: 14, loss: 0.9350548982620239, acc: 54.6875, f1: 29.630475067385447, r: 0.4807976379609055
06/02/2019 03:55:19 step: 1571, epoch: 47, batch: 19, loss: 0.7649825811386108, acc: 65.625, f1: 36.48015873015873, r: 0.44581693346808976
06/02/2019 03:55:20 step: 1576, epoch: 47, batch: 24, loss: 1.0682872533798218, acc: 65.625, f1: 30.758843030505794, r: 0.2802875029038619
06/02/2019 03:55:20 step: 1581, epoch: 47, batch: 29, loss: 1.0368375778198242, acc: 54.6875, f1: 29.73057644110276, r: 0.40119222972013346
06/02/2019 03:55:20 *** evaluating ***
06/02/2019 03:55:20 step: 48, epoch: 47, acc: 50.85470085470085, f1: 18.746521995469546, r: 0.3230354209447443
06/02/2019 03:55:20 *** epoch: 49 ***
06/02/2019 03:55:20 *** training ***
06/02/2019 03:55:21 step: 1589, epoch: 48, batch: 4, loss: 0.9676451086997986, acc: 60.9375, f1: 35.04273504273504, r: 0.503428998252406
06/02/2019 03:55:21 step: 1594, epoch: 48, batch: 9, loss: 0.8015333414077759, acc: 71.875, f1: 50.92462272265228, r: 0.467989509611371
06/02/2019 03:55:21 step: 1599, epoch: 48, batch: 14, loss: 0.8590865731239319, acc: 67.1875, f1: 31.802721088435376, r: 0.4250728172549309
06/02/2019 03:55:21 step: 1604, epoch: 48, batch: 19, loss: 0.8619061708450317, acc: 64.0625, f1: 48.797155225726655, r: 0.5196195006547099
06/02/2019 03:55:22 step: 1609, epoch: 48, batch: 24, loss: 0.9264692068099976, acc: 62.5, f1: 42.14707952146376, r: 0.494582816525454
06/02/2019 03:55:22 step: 1614, epoch: 48, batch: 29, loss: 0.8717599511146545, acc: 62.5, f1: 35.81138438281295, r: 0.38358459158096914
06/02/2019 03:55:22 *** evaluating ***
06/02/2019 03:55:22 step: 49, epoch: 48, acc: 59.401709401709404, f1: 21.094032858578238, r: 0.36137119300312004
06/02/2019 03:55:22 *** epoch: 50 ***
06/02/2019 03:55:22 *** training ***
06/02/2019 03:55:22 step: 1622, epoch: 49, batch: 4, loss: 0.9207457304000854, acc: 60.9375, f1: 40.00872512500419, r: 0.46097234745004556
06/02/2019 03:55:23 step: 1627, epoch: 49, batch: 9, loss: 0.8362205028533936, acc: 68.75, f1: 45.73669467787115, r: 0.3543093164814042
06/02/2019 03:55:23 step: 1632, epoch: 49, batch: 14, loss: 0.847970724105835, acc: 67.1875, f1: 32.00757575757576, r: 0.49533887699467927
06/02/2019 03:55:23 step: 1637, epoch: 49, batch: 19, loss: 0.8347600698471069, acc: 62.5, f1: 44.96794871794872, r: 0.5043591798050754
06/02/2019 03:55:24 step: 1642, epoch: 49, batch: 24, loss: 0.8138338327407837, acc: 59.375, f1: 28.703703703703702, r: 0.5880479987640522
06/02/2019 03:55:24 step: 1647, epoch: 49, batch: 29, loss: 0.8670525550842285, acc: 59.375, f1: 35.86745042844724, r: 0.4422944813884422
06/02/2019 03:55:24 *** evaluating ***
06/02/2019 03:55:24 step: 50, epoch: 49, acc: 55.55555555555556, f1: 21.93878724477683, r: 0.35728533639285526
06/02/2019 03:55:24 *** epoch: 51 ***
06/02/2019 03:55:24 *** training ***
06/02/2019 03:55:24 step: 1655, epoch: 50, batch: 4, loss: 0.805314838886261, acc: 60.9375, f1: 27.106410746116627, r: 0.46890536519476833
06/02/2019 03:55:25 step: 1660, epoch: 50, batch: 9, loss: 0.9407346844673157, acc: 53.125, f1: 26.851851851851855, r: 0.526114349617762
06/02/2019 03:55:25 step: 1665, epoch: 50, batch: 14, loss: 0.9851470589637756, acc: 48.4375, f1: 26.897866918028207, r: 0.4959378402290152
06/02/2019 03:55:25 step: 1670, epoch: 50, batch: 19, loss: 0.7315129041671753, acc: 67.1875, f1: 38.077336197636946, r: 0.4242064208382805
06/02/2019 03:55:26 step: 1675, epoch: 50, batch: 24, loss: 0.7465226650238037, acc: 62.5, f1: 30.02503156153411, r: 0.5165621281109659
06/02/2019 03:55:26 step: 1680, epoch: 50, batch: 29, loss: 0.726448118686676, acc: 73.4375, f1: 40.62171524128046, r: 0.5266951338724862
06/02/2019 03:55:26 *** evaluating ***
06/02/2019 03:55:26 step: 51, epoch: 50, acc: 59.82905982905983, f1: 21.403591492776886, r: 0.36861039801062057
06/02/2019 03:55:26 *** epoch: 52 ***
06/02/2019 03:55:26 *** training ***
06/02/2019 03:55:27 step: 1688, epoch: 51, batch: 4, loss: 0.6754292845726013, acc: 75.0, f1: 61.570905285191, r: 0.5004824780405388
06/02/2019 03:55:27 step: 1693, epoch: 51, batch: 9, loss: 0.6757425665855408, acc: 70.3125, f1: 37.96387520525451, r: 0.40796715121145666
06/02/2019 03:55:27 step: 1698, epoch: 51, batch: 14, loss: 0.7357102632522583, acc: 68.75, f1: 34.70022624434389, r: 0.49240738951307933
06/02/2019 03:55:28 step: 1703, epoch: 51, batch: 19, loss: 0.6374387741088867, acc: 73.4375, f1: 48.6984649122807, r: 0.5636946263972844
06/02/2019 03:55:28 step: 1708, epoch: 51, batch: 24, loss: 5.5609869956970215, acc: 43.75, f1: 23.837535014005606, r: 0.19671171296497053
06/02/2019 03:55:28 step: 1713, epoch: 51, batch: 29, loss: 0.9944295883178711, acc: 59.375, f1: 51.01731601731602, r: 0.5313290418803176
06/02/2019 03:55:28 *** evaluating ***
06/02/2019 03:55:28 step: 52, epoch: 51, acc: 56.41025641025641, f1: 21.241627930224418, r: 0.37587529788990304
06/02/2019 03:55:28 *** epoch: 53 ***
06/02/2019 03:55:28 *** training ***
06/02/2019 03:55:29 step: 1721, epoch: 52, batch: 4, loss: 0.6563685536384583, acc: 71.875, f1: 48.80898077663577, r: 0.4736567141469909
06/02/2019 03:55:29 step: 1726, epoch: 52, batch: 9, loss: 0.8082091808319092, acc: 56.25, f1: 32.20521541950114, r: 0.5511167201075917
06/02/2019 03:55:29 step: 1731, epoch: 52, batch: 14, loss: 0.8392026424407959, acc: 62.5, f1: 41.262497984196095, r: 0.5342213057044534
06/02/2019 03:55:30 step: 1736, epoch: 52, batch: 19, loss: 0.759116530418396, acc: 68.75, f1: 35.98312090568644, r: 0.4465320503747967
06/02/2019 03:55:30 step: 1741, epoch: 52, batch: 24, loss: 0.6395857334136963, acc: 67.1875, f1: 38.1289206143197, r: 0.5228403972085944
06/02/2019 03:55:30 step: 1746, epoch: 52, batch: 29, loss: 0.7899956703186035, acc: 60.9375, f1: 38.10820656291892, r: 0.4965828852019125
06/02/2019 03:55:30 *** evaluating ***
06/02/2019 03:55:30 step: 53, epoch: 52, acc: 54.27350427350427, f1: 22.37903817914831, r: 0.37133448309871525
06/02/2019 03:55:30 *** epoch: 54 ***
06/02/2019 03:55:30 *** training ***
06/02/2019 03:55:31 step: 1754, epoch: 53, batch: 4, loss: 0.8378015160560608, acc: 59.375, f1: 31.992190815720228, r: 0.4452706676553007
06/02/2019 03:55:31 step: 1759, epoch: 53, batch: 9, loss: 0.711361825466156, acc: 65.625, f1: 31.698085357624834, r: 0.5414121115610836
06/02/2019 03:55:31 step: 1764, epoch: 53, batch: 14, loss: 0.7122049927711487, acc: 65.625, f1: 42.006802721088434, r: 0.5815569438990968
06/02/2019 03:55:31 step: 1769, epoch: 53, batch: 19, loss: 0.6663737297058105, acc: 73.4375, f1: 39.852607709750565, r: 0.46467516299250494
06/02/2019 03:55:32 step: 1774, epoch: 53, batch: 24, loss: 0.6336466073989868, acc: 71.875, f1: 43.50840336134454, r: 0.6256583561234248
06/02/2019 03:55:32 step: 1779, epoch: 53, batch: 29, loss: 0.5474323034286499, acc: 76.5625, f1: 52.01299641977608, r: 0.5508195168716435
06/02/2019 03:55:32 *** evaluating ***
06/02/2019 03:55:32 step: 54, epoch: 53, acc: 59.401709401709404, f1: 22.24699148134982, r: 0.36988369072530647
06/02/2019 03:55:32 *** epoch: 55 ***
06/02/2019 03:55:32 *** training ***
06/02/2019 03:55:33 step: 1787, epoch: 54, batch: 4, loss: 0.721673309803009, acc: 67.1875, f1: 37.966089466089464, r: 0.4876718073349856
06/02/2019 03:55:33 step: 1792, epoch: 54, batch: 9, loss: 0.6977260112762451, acc: 65.625, f1: 42.090395480225986, r: 0.4397438605164416
06/02/2019 03:55:33 step: 1797, epoch: 54, batch: 14, loss: 0.726051390171051, acc: 67.1875, f1: 49.028797289666855, r: 0.5063851818835959
06/02/2019 03:55:33 step: 1802, epoch: 54, batch: 19, loss: 0.6930744051933289, acc: 67.1875, f1: 46.876528389133426, r: 0.5312090497902051
06/02/2019 03:55:34 step: 1807, epoch: 54, batch: 24, loss: 0.7002604007720947, acc: 67.1875, f1: 33.55750605326877, r: 0.5309614889026437
06/02/2019 03:55:34 step: 1812, epoch: 54, batch: 29, loss: 0.9000681638717651, acc: 53.125, f1: 31.840277777777782, r: 0.6035824240676911
06/02/2019 03:55:34 *** evaluating ***
06/02/2019 03:55:34 step: 55, epoch: 54, acc: 58.119658119658126, f1: 21.240392333226023, r: 0.37724808305382174
06/02/2019 03:55:34 *** epoch: 56 ***
06/02/2019 03:55:34 *** training ***
06/02/2019 03:55:34 step: 1820, epoch: 55, batch: 4, loss: 0.7550526261329651, acc: 64.0625, f1: 44.34319082824386, r: 0.4648540805631381
06/02/2019 03:55:35 step: 1825, epoch: 55, batch: 9, loss: 0.7491806149482727, acc: 68.75, f1: 39.97002997002997, r: 0.5393222027560804
06/02/2019 03:55:35 step: 1830, epoch: 55, batch: 14, loss: 0.7414205074310303, acc: 64.0625, f1: 36.00201708590413, r: 0.6320292758496016
06/02/2019 03:55:35 step: 1835, epoch: 55, batch: 19, loss: 0.7112041711807251, acc: 68.75, f1: 44.476663121824416, r: 0.43916834561623314
06/02/2019 03:55:35 step: 1840, epoch: 55, batch: 24, loss: 0.812865138053894, acc: 67.1875, f1: 43.64239926739927, r: 0.5419853402926037
06/02/2019 03:55:36 step: 1845, epoch: 55, batch: 29, loss: 0.7035154104232788, acc: 71.875, f1: 52.853863852569624, r: 0.5178075727524679
06/02/2019 03:55:36 *** evaluating ***
06/02/2019 03:55:36 step: 56, epoch: 55, acc: 59.401709401709404, f1: 24.553508690398733, r: 0.38408060381147596
06/02/2019 03:55:36 *** epoch: 57 ***
06/02/2019 03:55:36 *** training ***
06/02/2019 03:55:36 step: 1853, epoch: 56, batch: 4, loss: 0.8172836303710938, acc: 67.1875, f1: 45.72532300860474, r: 0.5488077253379315
06/02/2019 03:55:37 step: 1858, epoch: 56, batch: 9, loss: 0.742204487323761, acc: 68.75, f1: 40.96967654986523, r: 0.5329373720092229
06/02/2019 03:55:37 step: 1863, epoch: 56, batch: 14, loss: 0.74154132604599, acc: 62.5, f1: 42.29133439659755, r: 0.5261289520581711
06/02/2019 03:55:37 step: 1868, epoch: 56, batch: 19, loss: 0.6830244660377502, acc: 70.3125, f1: 45.17006802721088, r: 0.4551145130361488
06/02/2019 03:55:37 step: 1873, epoch: 56, batch: 24, loss: 0.6043007373809814, acc: 70.3125, f1: 40.8390022675737, r: 0.39983643724227846
06/02/2019 03:55:38 step: 1878, epoch: 56, batch: 29, loss: 0.7312208414077759, acc: 67.1875, f1: 40.39587362991618, r: 0.5451800145941826
06/02/2019 03:55:38 *** evaluating ***
06/02/2019 03:55:38 step: 57, epoch: 56, acc: 57.692307692307686, f1: 21.651608410229098, r: 0.3791688278664596
06/02/2019 03:55:38 *** epoch: 58 ***
06/02/2019 03:55:38 *** training ***
06/02/2019 03:55:38 step: 1886, epoch: 57, batch: 4, loss: 0.5814774632453918, acc: 76.5625, f1: 46.10943341961461, r: 0.625456495074338
06/02/2019 03:55:38 step: 1891, epoch: 57, batch: 9, loss: 0.8150062561035156, acc: 62.5, f1: 38.23194349510138, r: 0.43109266905353316
06/02/2019 03:55:39 step: 1896, epoch: 57, batch: 14, loss: 0.866224467754364, acc: 64.0625, f1: 36.65353641456583, r: 0.4793792705509168
06/02/2019 03:55:39 step: 1901, epoch: 57, batch: 19, loss: 0.716350793838501, acc: 70.3125, f1: 48.69376321957354, r: 0.5106388116773037
06/02/2019 03:55:39 step: 1906, epoch: 57, batch: 24, loss: 0.552834689617157, acc: 76.5625, f1: 38.46938775510204, r: 0.5347275439535454
06/02/2019 03:55:40 step: 1911, epoch: 57, batch: 29, loss: 0.6384115219116211, acc: 71.875, f1: 55.774031351671105, r: 0.5108631184362281
06/02/2019 03:55:40 *** evaluating ***
06/02/2019 03:55:40 step: 58, epoch: 57, acc: 57.692307692307686, f1: 21.260917908048437, r: 0.37260103358917473
06/02/2019 03:55:40 *** epoch: 59 ***
06/02/2019 03:55:40 *** training ***
06/02/2019 03:55:40 step: 1919, epoch: 58, batch: 4, loss: 0.6416381597518921, acc: 78.125, f1: 59.07946733069886, r: 0.5352502492783172
06/02/2019 03:55:41 step: 1924, epoch: 58, batch: 9, loss: 0.6087945699691772, acc: 76.5625, f1: 48.115358819584166, r: 0.46960879236826425
06/02/2019 03:55:41 step: 1929, epoch: 58, batch: 14, loss: 0.9012870788574219, acc: 60.9375, f1: 37.909946236559136, r: 0.5526301412283939
06/02/2019 03:55:41 step: 1934, epoch: 58, batch: 19, loss: 0.7384120225906372, acc: 67.1875, f1: 43.45996694058235, r: 0.4670670489977495
06/02/2019 03:55:41 step: 1939, epoch: 58, batch: 24, loss: 0.6680117249488831, acc: 65.625, f1: 37.67704216073782, r: 0.6099011957719429
06/02/2019 03:55:42 step: 1944, epoch: 58, batch: 29, loss: 0.7442863583564758, acc: 68.75, f1: 38.69539225142674, r: 0.5339771566033101
06/02/2019 03:55:42 *** evaluating ***
06/02/2019 03:55:42 step: 59, epoch: 58, acc: 59.401709401709404, f1: 24.62796889767914, r: 0.3785345600147372
06/02/2019 03:55:42 *** epoch: 60 ***
06/02/2019 03:55:42 *** training ***
06/02/2019 03:55:42 step: 1952, epoch: 59, batch: 4, loss: 0.7075089812278748, acc: 65.625, f1: 43.27537593984962, r: 0.5310644437403722
06/02/2019 03:55:43 step: 1957, epoch: 59, batch: 9, loss: 0.6509537696838379, acc: 70.3125, f1: 46.20455709165386, r: 0.5605395728882298
06/02/2019 03:55:43 step: 1962, epoch: 59, batch: 14, loss: 0.7487282752990723, acc: 57.8125, f1: 34.78961477063944, r: 0.5260736449559942
06/02/2019 03:55:43 step: 1967, epoch: 59, batch: 19, loss: 0.5930215120315552, acc: 67.1875, f1: 38.702107279693486, r: 0.5449905826759459
06/02/2019 03:55:43 step: 1972, epoch: 59, batch: 24, loss: 0.5939600467681885, acc: 75.0, f1: 50.00739426205264, r: 0.4940738602486552
06/02/2019 03:55:44 step: 1977, epoch: 59, batch: 29, loss: 0.7039933800697327, acc: 67.1875, f1: 38.16856452726017, r: 0.5542969207136135
06/02/2019 03:55:44 *** evaluating ***
06/02/2019 03:55:44 step: 60, epoch: 59, acc: 60.256410256410255, f1: 27.666383729069434, r: 0.3837239276986587
06/02/2019 03:55:44 *** epoch: 61 ***
06/02/2019 03:55:44 *** training ***
06/02/2019 03:55:44 step: 1985, epoch: 60, batch: 4, loss: 0.6918637752532959, acc: 71.875, f1: 48.54925622343655, r: 0.43737036353063663
06/02/2019 03:55:45 step: 1990, epoch: 60, batch: 9, loss: 0.6761173605918884, acc: 70.3125, f1: 62.72486772486773, r: 0.5102786067665323
06/02/2019 03:55:45 step: 1995, epoch: 60, batch: 14, loss: 0.7147554159164429, acc: 65.625, f1: 48.772602263472294, r: 0.5210303201133953
06/02/2019 03:55:45 step: 2000, epoch: 60, batch: 19, loss: 0.7232609987258911, acc: 59.375, f1: 37.021421107628, r: 0.6000368096728915
06/02/2019 03:55:45 step: 2005, epoch: 60, batch: 24, loss: 0.6001195311546326, acc: 78.125, f1: 49.867388642059694, r: 0.48636373407188516
06/02/2019 03:55:46 step: 2010, epoch: 60, batch: 29, loss: 0.7399864196777344, acc: 59.375, f1: 32.25806451612903, r: 0.5797736380398488
06/02/2019 03:55:46 *** evaluating ***
06/02/2019 03:55:46 step: 61, epoch: 60, acc: 59.82905982905983, f1: 25.857207084753565, r: 0.38179674472019987
06/02/2019 03:55:46 *** epoch: 62 ***
06/02/2019 03:55:46 *** training ***
06/02/2019 03:55:46 step: 2018, epoch: 61, batch: 4, loss: 0.6657857894897461, acc: 71.875, f1: 45.225974462365585, r: 0.613513201125812
06/02/2019 03:55:47 step: 2023, epoch: 61, batch: 9, loss: 0.584671676158905, acc: 78.125, f1: 66.87861730234613, r: 0.5372379812734475
06/02/2019 03:55:47 step: 2028, epoch: 61, batch: 14, loss: 0.8073294162750244, acc: 62.5, f1: 54.93930905695612, r: 0.5257288739829475
06/02/2019 03:55:47 step: 2033, epoch: 61, batch: 19, loss: 0.8022169470787048, acc: 64.0625, f1: 39.70078418848911, r: 0.5186236857032981
06/02/2019 03:55:47 step: 2038, epoch: 61, batch: 24, loss: 0.6459680199623108, acc: 73.4375, f1: 53.47003873319663, r: 0.5379267303340852
06/02/2019 03:55:48 step: 2043, epoch: 61, batch: 29, loss: 0.7828125357627869, acc: 64.0625, f1: 40.37873482726424, r: 0.5907018116928592
06/02/2019 03:55:48 *** evaluating ***
06/02/2019 03:55:48 step: 62, epoch: 61, acc: 57.692307692307686, f1: 24.714404629776435, r: 0.36557763004430865
06/02/2019 03:55:48 *** epoch: 63 ***
06/02/2019 03:55:48 *** training ***
06/02/2019 03:55:48 step: 2051, epoch: 62, batch: 4, loss: 0.5317754745483398, acc: 71.875, f1: 52.64250527797325, r: 0.5517801691510137
06/02/2019 03:55:48 step: 2056, epoch: 62, batch: 9, loss: 0.6482561826705933, acc: 73.4375, f1: 53.27602505021859, r: 0.42751490621366645
06/02/2019 03:55:49 step: 2061, epoch: 62, batch: 14, loss: 0.6623296141624451, acc: 70.3125, f1: 39.72873359870264, r: 0.55393662874261
06/02/2019 03:55:49 step: 2066, epoch: 62, batch: 19, loss: 0.6286993026733398, acc: 68.75, f1: 63.68665648077412, r: 0.617779581064257
06/02/2019 03:55:49 step: 2071, epoch: 62, batch: 24, loss: 0.4821659326553345, acc: 79.6875, f1: 51.63165266106443, r: 0.45717471799892023
06/02/2019 03:55:50 step: 2076, epoch: 62, batch: 29, loss: 0.620594322681427, acc: 71.875, f1: 40.29431216931217, r: 0.597373512737712
06/02/2019 03:55:50 *** evaluating ***
06/02/2019 03:55:50 step: 63, epoch: 62, acc: 60.68376068376068, f1: 26.478066180935034, r: 0.39289742802717964
06/02/2019 03:55:50 *** epoch: 64 ***
06/02/2019 03:55:50 *** training ***
06/02/2019 03:55:50 step: 2084, epoch: 63, batch: 4, loss: 0.491254597902298, acc: 84.375, f1: 59.1520979020979, r: 0.5076033262852855
06/02/2019 03:55:50 step: 2089, epoch: 63, batch: 9, loss: 0.5551610589027405, acc: 76.5625, f1: 70.23809523809524, r: 0.5019903397953358
06/02/2019 03:55:51 step: 2094, epoch: 63, batch: 14, loss: 0.6823369860649109, acc: 68.75, f1: 50.17857142857143, r: 0.4743641722191825
06/02/2019 03:55:51 step: 2099, epoch: 63, batch: 19, loss: 0.6154583096504211, acc: 76.5625, f1: 67.62718130730553, r: 0.5972604683767989
06/02/2019 03:55:51 step: 2104, epoch: 63, batch: 24, loss: 0.5899272561073303, acc: 78.125, f1: 70.00234576589257, r: 0.575449738649887
06/02/2019 03:55:52 step: 2109, epoch: 63, batch: 29, loss: 0.705830991268158, acc: 62.5, f1: 46.43741169316616, r: 0.5348354359534532
06/02/2019 03:55:52 *** evaluating ***
06/02/2019 03:55:52 step: 64, epoch: 63, acc: 59.401709401709404, f1: 24.73002765180069, r: 0.39323612013987147
06/02/2019 03:55:52 *** epoch: 65 ***
06/02/2019 03:55:52 *** training ***
06/02/2019 03:55:52 step: 2117, epoch: 64, batch: 4, loss: 0.48875123262405396, acc: 79.6875, f1: 58.394689554073864, r: 0.5941130159572349
06/02/2019 03:55:52 step: 2122, epoch: 64, batch: 9, loss: 0.6857720017433167, acc: 60.9375, f1: 28.72562893081761, r: 0.5220976737200894
06/02/2019 03:55:53 step: 2127, epoch: 64, batch: 14, loss: 0.544871985912323, acc: 75.0, f1: 57.61179996474114, r: 0.49366129899661704
06/02/2019 03:55:53 step: 2132, epoch: 64, batch: 19, loss: 0.5193117260932922, acc: 78.125, f1: 58.515251196172244, r: 0.5362801351373543
06/02/2019 03:55:53 step: 2137, epoch: 64, batch: 24, loss: 0.569709300994873, acc: 78.125, f1: 63.25483563188481, r: 0.5280111072453392
06/02/2019 03:55:53 step: 2142, epoch: 64, batch: 29, loss: 0.531288206577301, acc: 79.6875, f1: 60.412224591329064, r: 0.5566413160358397
06/02/2019 03:55:54 *** evaluating ***
06/02/2019 03:55:54 step: 65, epoch: 64, acc: 52.56410256410257, f1: 24.6480532720669, r: 0.35503689942573513
06/02/2019 03:55:54 *** epoch: 66 ***
06/02/2019 03:55:54 *** training ***
06/02/2019 03:55:54 step: 2150, epoch: 65, batch: 4, loss: 0.6050936579704285, acc: 73.4375, f1: 45.78869047619048, r: 0.515769300213581
06/02/2019 03:55:54 step: 2155, epoch: 65, batch: 9, loss: 0.4984734058380127, acc: 75.0, f1: 51.66125541125541, r: 0.5308301557149101
06/02/2019 03:55:55 step: 2160, epoch: 65, batch: 14, loss: 0.5840038061141968, acc: 73.4375, f1: 45.35919540229885, r: 0.5369465554314266
06/02/2019 03:55:55 step: 2165, epoch: 65, batch: 19, loss: 0.5413841009140015, acc: 71.875, f1: 35.76092644415626, r: 0.519103632997665
06/02/2019 03:55:55 step: 2170, epoch: 65, batch: 24, loss: 0.6479455828666687, acc: 65.625, f1: 45.74021411843426, r: 0.5692393338906626
06/02/2019 03:55:55 step: 2175, epoch: 65, batch: 29, loss: 0.515332043170929, acc: 75.0, f1: 56.880977028492566, r: 0.5877470174611583
06/02/2019 03:55:56 *** evaluating ***
06/02/2019 03:55:56 step: 66, epoch: 65, acc: 59.401709401709404, f1: 26.073727651317892, r: 0.3820755067151478
06/02/2019 03:55:56 *** epoch: 67 ***
06/02/2019 03:55:56 *** training ***
06/02/2019 03:55:56 step: 2183, epoch: 66, batch: 4, loss: 0.6480838656425476, acc: 67.1875, f1: 38.59055624061219, r: 0.5810491254232818
06/02/2019 03:55:56 step: 2188, epoch: 66, batch: 9, loss: 0.6120951175689697, acc: 75.0, f1: 66.1904761904762, r: 0.6061046694144531
06/02/2019 03:55:56 step: 2193, epoch: 66, batch: 14, loss: 0.6308639645576477, acc: 70.3125, f1: 47.733879876737014, r: 0.5141005091185239
06/02/2019 03:55:57 step: 2198, epoch: 66, batch: 19, loss: 0.5271511077880859, acc: 73.4375, f1: 48.692810457516345, r: 0.4996508992399963
06/02/2019 03:55:57 step: 2203, epoch: 66, batch: 24, loss: 0.6709058880805969, acc: 62.5, f1: 43.20286883215945, r: 0.5211802096266838
06/02/2019 03:55:57 step: 2208, epoch: 66, batch: 29, loss: 0.6946725845336914, acc: 65.625, f1: 40.03498128001303, r: 0.6434252279315907
06/02/2019 03:55:57 *** evaluating ***
06/02/2019 03:55:58 step: 67, epoch: 66, acc: 58.54700854700855, f1: 23.799074741360545, r: 0.3803546390159413
06/02/2019 03:55:58 *** epoch: 68 ***
06/02/2019 03:55:58 *** training ***
06/02/2019 03:55:58 step: 2216, epoch: 67, batch: 4, loss: 0.6316301822662354, acc: 70.3125, f1: 58.18826958701101, r: 0.6232632699584237
06/02/2019 03:55:58 step: 2221, epoch: 67, batch: 9, loss: 0.5172500610351562, acc: 71.875, f1: 44.536297036297036, r: 0.49713271669746595
06/02/2019 03:55:58 step: 2226, epoch: 67, batch: 14, loss: 0.5648082494735718, acc: 75.0, f1: 38.45496093174112, r: 0.4909084290288787
06/02/2019 03:55:59 step: 2231, epoch: 67, batch: 19, loss: 0.6727303266525269, acc: 70.3125, f1: 57.33525733525734, r: 0.5057117375401431
06/02/2019 03:55:59 step: 2236, epoch: 67, batch: 24, loss: 0.6372038125991821, acc: 70.3125, f1: 36.35467541717542, r: 0.6087061482532284
06/02/2019 03:55:59 step: 2241, epoch: 67, batch: 29, loss: 0.6969292759895325, acc: 68.75, f1: 41.98962544344526, r: 0.5090920063607267
06/02/2019 03:55:59 *** evaluating ***
06/02/2019 03:56:00 step: 68, epoch: 67, acc: 58.119658119658126, f1: 24.587114337568057, r: 0.37542572672537294
06/02/2019 03:56:00 *** epoch: 69 ***
06/02/2019 03:56:00 *** training ***
06/02/2019 03:56:00 step: 2249, epoch: 68, batch: 4, loss: 0.5729678869247437, acc: 75.0, f1: 40.772792022792025, r: 0.5796218184951019
06/02/2019 03:56:00 step: 2254, epoch: 68, batch: 9, loss: 0.6065939664840698, acc: 73.4375, f1: 48.26370468611848, r: 0.5255329735734854
06/02/2019 03:56:01 step: 2259, epoch: 68, batch: 14, loss: 0.6342202425003052, acc: 75.0, f1: 66.44209956709956, r: 0.5811683634106002
06/02/2019 03:56:01 step: 2264, epoch: 68, batch: 19, loss: 0.5608921647071838, acc: 70.3125, f1: 38.43614718614718, r: 0.5014070019203936
06/02/2019 03:56:01 step: 2269, epoch: 68, batch: 24, loss: 0.5063182711601257, acc: 76.5625, f1: 55.61482168625026, r: 0.5525169040170832
06/02/2019 03:56:01 step: 2274, epoch: 68, batch: 29, loss: 0.5767818689346313, acc: 71.875, f1: 44.330227743271216, r: 0.47659947074587716
06/02/2019 03:56:02 *** evaluating ***
06/02/2019 03:56:02 step: 69, epoch: 68, acc: 58.54700854700855, f1: 24.01415885824174, r: 0.38167108294344715
06/02/2019 03:56:02 *** epoch: 70 ***
06/02/2019 03:56:02 *** training ***
06/02/2019 03:56:02 step: 2282, epoch: 69, batch: 4, loss: 0.6426486968994141, acc: 71.875, f1: 55.788451557817545, r: 0.5483753158380342
06/02/2019 03:56:02 step: 2287, epoch: 69, batch: 9, loss: 0.7200055718421936, acc: 65.625, f1: 48.57717542446606, r: 0.5306569970168302
06/02/2019 03:56:03 step: 2292, epoch: 69, batch: 14, loss: 0.5188769698143005, acc: 79.6875, f1: 64.93987493987495, r: 0.5179245080824106
06/02/2019 03:56:03 step: 2297, epoch: 69, batch: 19, loss: 0.644779622554779, acc: 73.4375, f1: 65.7936507936508, r: 0.5301764459530807
06/02/2019 03:56:03 step: 2302, epoch: 69, batch: 24, loss: 0.5306035876274109, acc: 75.0, f1: 53.52288345497175, r: 0.5472261704551461
06/02/2019 03:56:03 step: 2307, epoch: 69, batch: 29, loss: 0.5242642164230347, acc: 78.125, f1: 50.79674993936454, r: 0.5365746826429203
06/02/2019 03:56:04 *** evaluating ***
06/02/2019 03:56:04 step: 70, epoch: 69, acc: 60.256410256410255, f1: 24.974158982779674, r: 0.38907898421645126
06/02/2019 03:56:04 *** epoch: 71 ***
06/02/2019 03:56:04 *** training ***
06/02/2019 03:56:04 step: 2315, epoch: 70, batch: 4, loss: 0.5833256840705872, acc: 71.875, f1: 61.625675911390196, r: 0.4852992823718721
06/02/2019 03:56:04 step: 2320, epoch: 70, batch: 9, loss: 0.5613815784454346, acc: 75.0, f1: 55.37278244631187, r: 0.6757820921288258
06/02/2019 03:56:05 step: 2325, epoch: 70, batch: 14, loss: 0.5311015248298645, acc: 78.125, f1: 57.873563218390814, r: 0.6435408539847306
06/02/2019 03:56:05 step: 2330, epoch: 70, batch: 19, loss: 0.5170396566390991, acc: 75.0, f1: 45.48805786050684, r: 0.5852367521162597
06/02/2019 03:56:05 step: 2335, epoch: 70, batch: 24, loss: 0.5214493870735168, acc: 76.5625, f1: 45.76708173690932, r: 0.5402289635988498
06/02/2019 03:56:06 step: 2340, epoch: 70, batch: 29, loss: 0.6688646674156189, acc: 65.625, f1: 60.10585776863937, r: 0.6411022885959983
06/02/2019 03:56:06 *** evaluating ***
06/02/2019 03:56:06 step: 71, epoch: 70, acc: 60.256410256410255, f1: 24.95601810597603, r: 0.3943739543982165
06/02/2019 03:56:06 *** epoch: 72 ***
06/02/2019 03:56:06 *** training ***
06/02/2019 03:56:06 step: 2348, epoch: 71, batch: 4, loss: 0.9000977873802185, acc: 56.25, f1: 38.75904528478058, r: 0.45905335519003926
06/02/2019 03:56:06 step: 2353, epoch: 71, batch: 9, loss: 0.5731921792030334, acc: 68.75, f1: 49.503367003367, r: 0.6503954520363597
06/02/2019 03:56:07 step: 2358, epoch: 71, batch: 14, loss: 0.6466390490531921, acc: 67.1875, f1: 59.7316101089686, r: 0.5752963237111713
06/02/2019 03:56:07 step: 2363, epoch: 71, batch: 19, loss: 0.5652577877044678, acc: 71.875, f1: 51.822660098522164, r: 0.4965415347291967
06/02/2019 03:56:07 step: 2368, epoch: 71, batch: 24, loss: 0.5976922512054443, acc: 71.875, f1: 49.99214224084802, r: 0.5427252103325338
06/02/2019 03:56:08 step: 2373, epoch: 71, batch: 29, loss: 0.6459036469459534, acc: 76.5625, f1: 57.664311332789595, r: 0.5477999139000707
06/02/2019 03:56:08 *** evaluating ***
06/02/2019 03:56:08 step: 72, epoch: 71, acc: 56.837606837606835, f1: 25.63007185572847, r: 0.39227125109220046
06/02/2019 03:56:08 *** epoch: 73 ***
06/02/2019 03:56:08 *** training ***
06/02/2019 03:56:08 step: 2381, epoch: 72, batch: 4, loss: 0.5181233882904053, acc: 75.0, f1: 56.07142857142857, r: 0.6222988183610402
06/02/2019 03:56:08 step: 2386, epoch: 72, batch: 9, loss: 0.4425494074821472, acc: 81.25, f1: 64.22459893048128, r: 0.5252099529526376
06/02/2019 03:56:09 step: 2391, epoch: 72, batch: 14, loss: 0.6060047149658203, acc: 65.625, f1: 31.879800307219664, r: 0.5085079204045911
06/02/2019 03:56:09 step: 2396, epoch: 72, batch: 19, loss: 0.43967536091804504, acc: 81.25, f1: 57.16093117408907, r: 0.6218300757352518
06/02/2019 03:56:09 step: 2401, epoch: 72, batch: 24, loss: 0.5072522163391113, acc: 79.6875, f1: 46.61940836940837, r: 0.5563701798576793
06/02/2019 03:56:10 step: 2406, epoch: 72, batch: 29, loss: 0.5295991897583008, acc: 81.25, f1: 67.84623283275575, r: 0.5222116924754311
06/02/2019 03:56:10 *** evaluating ***
06/02/2019 03:56:10 step: 73, epoch: 72, acc: 57.26495726495726, f1: 22.563757281258216, r: 0.3656932457465608
06/02/2019 03:56:10 *** epoch: 74 ***
06/02/2019 03:56:10 *** training ***
06/02/2019 03:56:10 step: 2414, epoch: 73, batch: 4, loss: 0.5695812702178955, acc: 71.875, f1: 53.68224709238534, r: 0.5829148692472867
06/02/2019 03:56:11 step: 2419, epoch: 73, batch: 9, loss: 0.6006036400794983, acc: 68.75, f1: 57.27322485943176, r: 0.5938093643409522
06/02/2019 03:56:11 step: 2424, epoch: 73, batch: 14, loss: 0.541235625743866, acc: 71.875, f1: 57.28334812185744, r: 0.5885537422172274
06/02/2019 03:56:11 step: 2429, epoch: 73, batch: 19, loss: 0.546036958694458, acc: 71.875, f1: 50.34790640394089, r: 0.6303960232271268
06/02/2019 03:56:11 step: 2434, epoch: 73, batch: 24, loss: 0.4814845323562622, acc: 81.25, f1: 60.59948979591836, r: 0.571368895237057
06/02/2019 03:56:12 step: 2439, epoch: 73, batch: 29, loss: 0.47375524044036865, acc: 78.125, f1: 64.79684265010353, r: 0.6548098524191474
06/02/2019 03:56:12 *** evaluating ***
06/02/2019 03:56:12 step: 74, epoch: 73, acc: 60.256410256410255, f1: 26.34362773449461, r: 0.3970649109902052
06/02/2019 03:56:12 *** epoch: 75 ***
06/02/2019 03:56:12 *** training ***
06/02/2019 03:56:12 step: 2447, epoch: 74, batch: 4, loss: 0.6647158265113831, acc: 70.3125, f1: 69.9134199134199, r: 0.5947592137629386
06/02/2019 03:56:12 step: 2452, epoch: 74, batch: 9, loss: 0.5057644844055176, acc: 75.0, f1: 55.25539275539275, r: 0.6290802423658163
06/02/2019 03:56:13 step: 2457, epoch: 74, batch: 14, loss: 0.5183814167976379, acc: 70.3125, f1: 61.53817082388511, r: 0.6997131306042914
06/02/2019 03:56:13 step: 2462, epoch: 74, batch: 19, loss: 0.3765011727809906, acc: 84.375, f1: 72.20861678004536, r: 0.6309075968947498
06/02/2019 03:56:13 step: 2467, epoch: 74, batch: 24, loss: 0.5115000605583191, acc: 78.125, f1: 56.38157894736842, r: 0.6309142131077349
06/02/2019 03:56:14 step: 2472, epoch: 74, batch: 29, loss: 0.5237861275672913, acc: 78.125, f1: 55.977805263519556, r: 0.5343145843848537
06/02/2019 03:56:14 *** evaluating ***
06/02/2019 03:56:14 step: 75, epoch: 74, acc: 58.97435897435898, f1: 21.755689485952644, r: 0.3843125849828176
06/02/2019 03:56:14 *** epoch: 76 ***
06/02/2019 03:56:14 *** training ***
06/02/2019 03:56:14 step: 2480, epoch: 75, batch: 4, loss: 0.6637834310531616, acc: 62.5, f1: 45.68128395522969, r: 0.5634348854603513
06/02/2019 03:56:14 step: 2485, epoch: 75, batch: 9, loss: 0.4282836318016052, acc: 78.125, f1: 49.18245832038936, r: 0.6439510355641339
06/02/2019 03:56:15 step: 2490, epoch: 75, batch: 14, loss: 0.47805726528167725, acc: 82.8125, f1: 65.65837671100829, r: 0.5096552800348263
06/02/2019 03:56:15 step: 2495, epoch: 75, batch: 19, loss: 0.5598863959312439, acc: 75.0, f1: 68.92734553775743, r: 0.5723154369506214
06/02/2019 03:56:15 step: 2500, epoch: 75, batch: 24, loss: 0.6072652339935303, acc: 75.0, f1: 56.08333333333333, r: 0.5838497228637071
06/02/2019 03:56:15 step: 2505, epoch: 75, batch: 29, loss: 0.4936281442642212, acc: 81.25, f1: 69.5190985485103, r: 0.577584982420732
06/02/2019 03:56:16 *** evaluating ***
06/02/2019 03:56:16 step: 76, epoch: 75, acc: 59.401709401709404, f1: 22.859308807134894, r: 0.366785551844212
06/02/2019 03:56:16 *** epoch: 77 ***
06/02/2019 03:56:16 *** training ***
06/02/2019 03:56:16 step: 2513, epoch: 76, batch: 4, loss: 0.6478525400161743, acc: 57.8125, f1: 36.045751633986924, r: 0.7148681054706503
06/02/2019 03:56:16 step: 2518, epoch: 76, batch: 9, loss: 0.4315316379070282, acc: 82.8125, f1: 52.75221199590947, r: 0.5184890802078869
06/02/2019 03:56:16 step: 2523, epoch: 76, batch: 14, loss: 0.6364852786064148, acc: 68.75, f1: 43.2463054187192, r: 0.49924625161200303
06/02/2019 03:56:17 step: 2528, epoch: 76, batch: 19, loss: 0.4421456456184387, acc: 79.6875, f1: 51.7102181232616, r: 0.6785931382582979
06/02/2019 03:56:17 step: 2533, epoch: 76, batch: 24, loss: 0.6183540225028992, acc: 68.75, f1: 51.556972789115655, r: 0.682996667982488
06/02/2019 03:56:17 step: 2538, epoch: 76, batch: 29, loss: 0.6406290531158447, acc: 73.4375, f1: 56.96428571428571, r: 0.5995583064526224
06/02/2019 03:56:17 *** evaluating ***
06/02/2019 03:56:18 step: 77, epoch: 76, acc: 59.82905982905983, f1: 22.705245927818588, r: 0.36901440052201856
06/02/2019 03:56:18 *** epoch: 78 ***
06/02/2019 03:56:18 *** training ***
06/02/2019 03:56:18 step: 2546, epoch: 77, batch: 4, loss: 0.5990740060806274, acc: 70.3125, f1: 55.05061115355232, r: 0.6379997347592152
06/02/2019 03:56:18 step: 2551, epoch: 77, batch: 9, loss: 0.5528849959373474, acc: 75.0, f1: 66.463380673907, r: 0.6507609289917609
06/02/2019 03:56:18 step: 2556, epoch: 77, batch: 14, loss: 0.5497739315032959, acc: 75.0, f1: 42.004487797641126, r: 0.49721934996387207
06/02/2019 03:56:19 step: 2561, epoch: 77, batch: 19, loss: 0.47490379214286804, acc: 79.6875, f1: 66.4914795349578, r: 0.5970222018574332
06/02/2019 03:56:19 step: 2566, epoch: 77, batch: 24, loss: 0.5122284889221191, acc: 71.875, f1: 66.30577909291937, r: 0.546784905473414
06/02/2019 03:56:19 step: 2571, epoch: 77, batch: 29, loss: 0.4590280055999756, acc: 76.5625, f1: 62.87314925970387, r: 0.5830236722960858
06/02/2019 03:56:19 *** evaluating ***
06/02/2019 03:56:19 step: 78, epoch: 77, acc: 58.54700854700855, f1: 22.82025995314073, r: 0.3813021900324387
06/02/2019 03:56:19 *** epoch: 79 ***
06/02/2019 03:56:19 *** training ***
06/02/2019 03:56:20 step: 2579, epoch: 78, batch: 4, loss: 0.6844872236251831, acc: 65.625, f1: 61.371340883225436, r: 0.6463650597794178
06/02/2019 03:56:20 step: 2584, epoch: 78, batch: 9, loss: 0.5763037204742432, acc: 71.875, f1: 53.718520090548964, r: 0.5782924777366945
06/02/2019 03:56:20 step: 2589, epoch: 78, batch: 14, loss: 0.47648370265960693, acc: 73.4375, f1: 65.23586561322409, r: 0.547045130026746
06/02/2019 03:56:21 step: 2594, epoch: 78, batch: 19, loss: 0.5634776949882507, acc: 78.125, f1: 58.61751152073733, r: 0.5502797654936813
06/02/2019 03:56:21 step: 2599, epoch: 78, batch: 24, loss: 0.4953871965408325, acc: 73.4375, f1: 66.28733871716327, r: 0.5258764390014606
06/02/2019 03:56:21 step: 2604, epoch: 78, batch: 29, loss: 0.5120706558227539, acc: 71.875, f1: 58.899190110826936, r: 0.5476134033767049
06/02/2019 03:56:21 *** evaluating ***
06/02/2019 03:56:21 step: 79, epoch: 78, acc: 56.837606837606835, f1: 21.4165280857354, r: 0.380583278053328
06/02/2019 03:56:21 *** epoch: 80 ***
06/02/2019 03:56:21 *** training ***
06/02/2019 03:56:22 step: 2612, epoch: 79, batch: 4, loss: 0.4432050585746765, acc: 81.25, f1: 67.14770118573746, r: 0.5663590317994132
06/02/2019 03:56:22 step: 2617, epoch: 79, batch: 9, loss: 0.44040536880493164, acc: 81.25, f1: 64.63868566294443, r: 0.49786519891452113
06/02/2019 03:56:22 step: 2622, epoch: 79, batch: 14, loss: 0.47429296374320984, acc: 76.5625, f1: 55.595238095238095, r: 0.6290674200949784
06/02/2019 03:56:23 step: 2627, epoch: 79, batch: 19, loss: 0.5091410875320435, acc: 81.25, f1: 66.27133872416891, r: 0.5589186586424995
06/02/2019 03:56:23 step: 2632, epoch: 79, batch: 24, loss: 0.6360043287277222, acc: 70.3125, f1: 52.4222732513268, r: 0.5836966937844723
06/02/2019 03:56:23 step: 2637, epoch: 79, batch: 29, loss: 0.5274251699447632, acc: 78.125, f1: 65.48437130562255, r: 0.6032732943876116
06/02/2019 03:56:23 *** evaluating ***
06/02/2019 03:56:23 step: 80, epoch: 79, acc: 61.111111111111114, f1: 24.04958686351816, r: 0.3714736973017445
06/02/2019 03:56:23 *** epoch: 81 ***
06/02/2019 03:56:23 *** training ***
06/02/2019 03:56:24 step: 2645, epoch: 80, batch: 4, loss: 0.5058223009109497, acc: 78.125, f1: 48.90522875816993, r: 0.5521217683096977
06/02/2019 03:56:24 step: 2650, epoch: 80, batch: 9, loss: 0.5411841869354248, acc: 68.75, f1: 46.4693371144984, r: 0.5536641159353729
06/02/2019 03:56:24 step: 2655, epoch: 80, batch: 14, loss: 0.5212543606758118, acc: 75.0, f1: 51.64140271493213, r: 0.4453685873533444
06/02/2019 03:56:25 step: 2660, epoch: 80, batch: 19, loss: 0.4756565988063812, acc: 73.4375, f1: 49.88226059654631, r: 0.535407668098608
06/02/2019 03:56:25 step: 2665, epoch: 80, batch: 24, loss: 0.538495659828186, acc: 75.0, f1: 53.858695652173914, r: 0.5386969861579323
06/02/2019 03:56:25 step: 2670, epoch: 80, batch: 29, loss: 0.33581244945526123, acc: 87.5, f1: 81.35190918472651, r: 0.5894034816115883
06/02/2019 03:56:25 *** evaluating ***
06/02/2019 03:56:25 step: 81, epoch: 80, acc: 60.68376068376068, f1: 27.298566690357738, r: 0.37515617913673877
06/02/2019 03:56:25 *** epoch: 82 ***
06/02/2019 03:56:25 *** training ***
06/02/2019 03:56:26 step: 2678, epoch: 81, batch: 4, loss: 0.4450938105583191, acc: 78.125, f1: 59.117032392894465, r: 0.5329130037225196
06/02/2019 03:56:26 step: 2683, epoch: 81, batch: 9, loss: 0.4577189087867737, acc: 78.125, f1: 57.61086591004623, r: 0.6362557169187193
06/02/2019 03:56:26 step: 2688, epoch: 81, batch: 14, loss: 0.31409937143325806, acc: 84.375, f1: 74.56597222222221, r: 0.6776257683510816
06/02/2019 03:56:26 step: 2693, epoch: 81, batch: 19, loss: 0.5229609608650208, acc: 75.0, f1: 57.256235827664405, r: 0.5227719590287241
06/02/2019 03:56:27 step: 2698, epoch: 81, batch: 24, loss: 0.5081074833869934, acc: 75.0, f1: 54.77276016789695, r: 0.5328529835287086
06/02/2019 03:56:27 step: 2703, epoch: 81, batch: 29, loss: 0.6684366464614868, acc: 65.625, f1: 39.46046207665398, r: 0.4727532774241548
06/02/2019 03:56:27 *** evaluating ***
06/02/2019 03:56:27 step: 82, epoch: 81, acc: 55.98290598290598, f1: 22.916671330648843, r: 0.379436371632934
06/02/2019 03:56:27 *** epoch: 83 ***
06/02/2019 03:56:27 *** training ***
06/02/2019 03:56:28 step: 2711, epoch: 82, batch: 4, loss: 0.44702088832855225, acc: 76.5625, f1: 51.235431235431236, r: 0.6410036435338792
06/02/2019 03:56:28 step: 2716, epoch: 82, batch: 9, loss: 0.4117897152900696, acc: 78.125, f1: 54.214472309299886, r: 0.6595588180571123
06/02/2019 03:56:28 step: 2721, epoch: 82, batch: 14, loss: 0.6106359362602234, acc: 67.1875, f1: 49.2220572439502, r: 0.5204870438259049
06/02/2019 03:56:28 step: 2726, epoch: 82, batch: 19, loss: 0.625583291053772, acc: 67.1875, f1: 53.13763906816644, r: 0.5606367547504809
06/02/2019 03:56:29 step: 2731, epoch: 82, batch: 24, loss: 0.5268986821174622, acc: 76.5625, f1: 61.114324272219, r: 0.5333613507559525
06/02/2019 03:56:29 step: 2736, epoch: 82, batch: 29, loss: 0.44887083768844604, acc: 78.125, f1: 53.64586157043462, r: 0.5136351357765586
06/02/2019 03:56:29 *** evaluating ***
06/02/2019 03:56:29 step: 83, epoch: 82, acc: 58.54700854700855, f1: 24.49185999185999, r: 0.36857923230547784
06/02/2019 03:56:29 *** epoch: 84 ***
06/02/2019 03:56:29 *** training ***
06/02/2019 03:56:29 step: 2744, epoch: 83, batch: 4, loss: 0.3598383069038391, acc: 81.25, f1: 56.463178294573645, r: 0.6168898243690573
06/02/2019 03:56:30 step: 2749, epoch: 83, batch: 9, loss: 0.46241194009780884, acc: 76.5625, f1: 57.93877551020409, r: 0.5876697595521985
06/02/2019 03:56:30 step: 2754, epoch: 83, batch: 14, loss: 0.470034658908844, acc: 87.5, f1: 81.81139122315592, r: 0.6028771484566063
06/02/2019 03:56:30 step: 2759, epoch: 83, batch: 19, loss: 0.4499497413635254, acc: 76.5625, f1: 65.0201926063995, r: 0.6228281185167498
06/02/2019 03:56:30 step: 2764, epoch: 83, batch: 24, loss: 0.45535793900489807, acc: 81.25, f1: 70.27389277389278, r: 0.6311580177979338
06/02/2019 03:56:31 step: 2769, epoch: 83, batch: 29, loss: 0.42273658514022827, acc: 81.25, f1: 69.47102425876011, r: 0.6735779327999663
06/02/2019 03:56:31 *** evaluating ***
06/02/2019 03:56:31 step: 84, epoch: 83, acc: 61.111111111111114, f1: 26.830761696236326, r: 0.3830203096359063
06/02/2019 03:56:31 *** epoch: 85 ***
06/02/2019 03:56:31 *** training ***
06/02/2019 03:56:31 step: 2777, epoch: 84, batch: 4, loss: 0.4071398973464966, acc: 79.6875, f1: 53.361800285256464, r: 0.5239423727375443
06/02/2019 03:56:32 step: 2782, epoch: 84, batch: 9, loss: 0.39513006806373596, acc: 78.125, f1: 65.41353383458647, r: 0.6703883441125921
06/02/2019 03:56:32 step: 2787, epoch: 84, batch: 14, loss: 0.37940508127212524, acc: 87.5, f1: 78.65919193764461, r: 0.6261964078059458
06/02/2019 03:56:32 step: 2792, epoch: 84, batch: 19, loss: 0.3524876832962036, acc: 84.375, f1: 75.6544556072858, r: 0.6485728728127858
06/02/2019 03:56:32 step: 2797, epoch: 84, batch: 24, loss: 0.3853793442249298, acc: 82.8125, f1: 50.86737858915278, r: 0.5690939158060422
06/02/2019 03:56:33 step: 2802, epoch: 84, batch: 29, loss: 0.5674387216567993, acc: 70.3125, f1: 49.11172161172161, r: 0.543885174000109
06/02/2019 03:56:33 *** evaluating ***
06/02/2019 03:56:33 step: 85, epoch: 84, acc: 59.401709401709404, f1: 22.522875816993466, r: 0.36861863437238895
06/02/2019 03:56:33 *** epoch: 86 ***
06/02/2019 03:56:33 *** training ***
06/02/2019 03:56:33 step: 2810, epoch: 85, batch: 4, loss: 0.3353714048862457, acc: 84.375, f1: 67.30218403150735, r: 0.6249633525891521
06/02/2019 03:56:34 step: 2815, epoch: 85, batch: 9, loss: 0.338144987821579, acc: 90.625, f1: 86.5918243207414, r: 0.5923017485285024
06/02/2019 03:56:34 step: 2820, epoch: 85, batch: 14, loss: 0.38569578528404236, acc: 82.8125, f1: 66.07764575849683, r: 0.6380122558954676
06/02/2019 03:56:34 step: 2825, epoch: 85, batch: 19, loss: 0.4681573510169983, acc: 78.125, f1: 68.02197802197803, r: 0.6325179159847716
06/02/2019 03:56:34 step: 2830, epoch: 85, batch: 24, loss: 0.3357641100883484, acc: 87.5, f1: 67.47009607130232, r: 0.6647614208669933
06/02/2019 03:56:35 step: 2835, epoch: 85, batch: 29, loss: 0.506349503993988, acc: 79.6875, f1: 67.04274272638118, r: 0.6629758198271729
06/02/2019 03:56:35 *** evaluating ***
06/02/2019 03:56:35 step: 86, epoch: 85, acc: 59.401709401709404, f1: 24.151484242435796, r: 0.37747173636708836
06/02/2019 03:56:35 *** epoch: 87 ***
06/02/2019 03:56:35 *** training ***
06/02/2019 03:56:35 step: 2843, epoch: 86, batch: 4, loss: 0.48125410079956055, acc: 79.6875, f1: 55.5921052631579, r: 0.5679807069681613
06/02/2019 03:56:36 step: 2848, epoch: 86, batch: 9, loss: 0.46619656682014465, acc: 81.25, f1: 69.99616858237549, r: 0.55677703292585
06/02/2019 03:56:36 step: 2853, epoch: 86, batch: 14, loss: 0.43272465467453003, acc: 82.8125, f1: 71.86560150375941, r: 0.6610109326434598
06/02/2019 03:56:36 step: 2858, epoch: 86, batch: 19, loss: 0.5519464612007141, acc: 76.5625, f1: 69.72668650793649, r: 0.6686914080306403
06/02/2019 03:56:36 step: 2863, epoch: 86, batch: 24, loss: 0.42219075560569763, acc: 78.125, f1: 73.38674253768592, r: 0.5271935541738944
06/02/2019 03:56:37 step: 2868, epoch: 86, batch: 29, loss: 0.5309802889823914, acc: 76.5625, f1: 59.58435960591133, r: 0.6284499418931275
06/02/2019 03:56:37 *** evaluating ***
06/02/2019 03:56:37 step: 87, epoch: 86, acc: 58.54700854700855, f1: 24.3180349062702, r: 0.3882020744359126
06/02/2019 03:56:37 *** epoch: 88 ***
06/02/2019 03:56:37 *** training ***
06/02/2019 03:56:37 step: 2876, epoch: 87, batch: 4, loss: 0.6001471281051636, acc: 71.875, f1: 56.65554683411826, r: 0.5959562146907439
06/02/2019 03:56:37 step: 2881, epoch: 87, batch: 9, loss: 0.39064666628837585, acc: 85.9375, f1: 79.54735740450026, r: 0.638341100555572
06/02/2019 03:56:38 step: 2886, epoch: 87, batch: 14, loss: 0.629912793636322, acc: 67.1875, f1: 43.52794536618066, r: 0.549896927365374
06/02/2019 03:56:38 step: 2891, epoch: 87, batch: 19, loss: 0.4591733515262604, acc: 79.6875, f1: 51.907260079172914, r: 0.5714524752591941
06/02/2019 03:56:38 step: 2896, epoch: 87, batch: 24, loss: 0.42987966537475586, acc: 84.375, f1: 63.706507405153175, r: 0.5199355603926364
06/02/2019 03:56:39 step: 2901, epoch: 87, batch: 29, loss: 0.3959566354751587, acc: 79.6875, f1: 68.84696322196322, r: 0.64984502066162
06/02/2019 03:56:39 *** evaluating ***
06/02/2019 03:56:39 step: 88, epoch: 87, acc: 59.82905982905983, f1: 23.547125043485355, r: 0.3563397732936149
06/02/2019 03:56:39 *** epoch: 89 ***
06/02/2019 03:56:39 *** training ***
06/02/2019 03:56:39 step: 2909, epoch: 88, batch: 4, loss: 0.42505428194999695, acc: 84.375, f1: 80.15862512503051, r: 0.636865204548541
06/02/2019 03:56:39 step: 2914, epoch: 88, batch: 9, loss: 0.4649946987628937, acc: 78.125, f1: 63.712556600487645, r: 0.5972021230772182
06/02/2019 03:56:40 step: 2919, epoch: 88, batch: 14, loss: 0.3637552559375763, acc: 87.5, f1: 70.04026767359387, r: 0.5614805433230001
06/02/2019 03:56:40 step: 2924, epoch: 88, batch: 19, loss: 0.5798609852790833, acc: 76.5625, f1: 65.12012497306614, r: 0.6103919590152176
06/02/2019 03:56:40 step: 2929, epoch: 88, batch: 24, loss: 0.4362323582172394, acc: 82.8125, f1: 61.25783124921056, r: 0.7100228432924099
06/02/2019 03:56:41 step: 2934, epoch: 88, batch: 29, loss: 0.5530948042869568, acc: 78.125, f1: 58.135609124613175, r: 0.5958077450242069
06/02/2019 03:56:41 *** evaluating ***
06/02/2019 03:56:41 step: 89, epoch: 88, acc: 58.97435897435898, f1: 24.49506907747192, r: 0.38580378875557125
06/02/2019 03:56:41 *** epoch: 90 ***
06/02/2019 03:56:41 *** training ***
06/02/2019 03:56:41 step: 2942, epoch: 89, batch: 4, loss: 0.4075239896774292, acc: 82.8125, f1: 70.96414002664002, r: 0.633363218634172
06/02/2019 03:56:41 step: 2947, epoch: 89, batch: 9, loss: 0.3537636399269104, acc: 85.9375, f1: 83.05281882868091, r: 0.5779368026228878
06/02/2019 03:56:42 step: 2952, epoch: 89, batch: 14, loss: 0.47037404775619507, acc: 79.6875, f1: 65.80434762805584, r: 0.62747385438051
06/02/2019 03:56:42 step: 2957, epoch: 89, batch: 19, loss: 0.62169349193573, acc: 70.3125, f1: 63.450834879406315, r: 0.5270257973219561
06/02/2019 03:56:42 step: 2962, epoch: 89, batch: 24, loss: 0.28133243322372437, acc: 90.625, f1: 75.73983393655526, r: 0.5905818260438219
06/02/2019 03:56:42 step: 2967, epoch: 89, batch: 29, loss: 0.5266023278236389, acc: 76.5625, f1: 51.48099027409372, r: 0.5534576836323754
06/02/2019 03:56:43 *** evaluating ***
06/02/2019 03:56:43 step: 90, epoch: 89, acc: 59.82905982905983, f1: 24.53434065934066, r: 0.3720861959442081
06/02/2019 03:56:43 *** epoch: 91 ***
06/02/2019 03:56:43 *** training ***
06/02/2019 03:56:43 step: 2975, epoch: 90, batch: 4, loss: 0.3947030007839203, acc: 81.25, f1: 56.03174603174603, r: 0.5575169606323424
06/02/2019 03:56:43 step: 2980, epoch: 90, batch: 9, loss: 0.4100259840488434, acc: 81.25, f1: 54.25492660832448, r: 0.4881721167847206
06/02/2019 03:56:44 step: 2985, epoch: 90, batch: 14, loss: 0.3189457654953003, acc: 85.9375, f1: 74.85414824124503, r: 0.5251455737286513
06/02/2019 03:56:44 step: 2990, epoch: 90, batch: 19, loss: 0.5238581895828247, acc: 79.6875, f1: 68.84662956091526, r: 0.5431126350193598
06/02/2019 03:56:44 step: 2995, epoch: 90, batch: 24, loss: 0.4268987476825714, acc: 71.875, f1: 42.920585161964475, r: 0.550231760687933
06/02/2019 03:56:44 step: 3000, epoch: 90, batch: 29, loss: 0.36166447401046753, acc: 82.8125, f1: 77.70328355854672, r: 0.7304865602727527
06/02/2019 03:56:44 *** evaluating ***
06/02/2019 03:56:45 step: 91, epoch: 90, acc: 58.97435897435898, f1: 24.160554547730374, r: 0.3523743624393277
06/02/2019 03:56:45 *** epoch: 92 ***
06/02/2019 03:56:45 *** training ***
06/02/2019 03:56:45 step: 3008, epoch: 91, batch: 4, loss: 0.4854555130004883, acc: 79.6875, f1: 75.74582203614462, r: 0.6706987218150372
06/02/2019 03:56:45 step: 3013, epoch: 91, batch: 9, loss: 0.4051057696342468, acc: 84.375, f1: 79.72593718338399, r: 0.6610163805673591
06/02/2019 03:56:45 step: 3018, epoch: 91, batch: 14, loss: 0.4316650331020355, acc: 84.375, f1: 69.07196969696969, r: 0.618313139940908
06/02/2019 03:56:46 step: 3023, epoch: 91, batch: 19, loss: 0.41880810260772705, acc: 81.25, f1: 68.86672430830039, r: 0.6322413120461643
06/02/2019 03:56:46 step: 3028, epoch: 91, batch: 24, loss: 0.383463591337204, acc: 84.375, f1: 57.5775927081708, r: 0.5372040525932958
06/02/2019 03:56:46 step: 3033, epoch: 91, batch: 29, loss: 0.46929770708084106, acc: 82.8125, f1: 76.9167869167869, r: 0.6152917530865814
06/02/2019 03:56:46 *** evaluating ***
06/02/2019 03:56:46 step: 92, epoch: 91, acc: 58.97435897435898, f1: 22.326775481386093, r: 0.3614819067791102
06/02/2019 03:56:46 *** epoch: 93 ***
06/02/2019 03:56:46 *** training ***
06/02/2019 03:56:47 step: 3041, epoch: 92, batch: 4, loss: 0.4333072006702423, acc: 79.6875, f1: 77.11760461760461, r: 0.6946962368647714
06/02/2019 03:56:47 step: 3046, epoch: 92, batch: 9, loss: 0.37311995029449463, acc: 85.9375, f1: 80.01765351090687, r: 0.581753435553574
06/02/2019 03:56:47 step: 3051, epoch: 92, batch: 14, loss: 0.5523167252540588, acc: 76.5625, f1: 68.58294042254583, r: 0.5054371928148215
06/02/2019 03:56:48 step: 3056, epoch: 92, batch: 19, loss: 0.24804700911045074, acc: 93.75, f1: 94.39100954761346, r: 0.6825238136744248
06/02/2019 03:56:48 step: 3061, epoch: 92, batch: 24, loss: 0.4369956851005554, acc: 82.8125, f1: 48.56311360448808, r: 0.5662818447482948
06/02/2019 03:56:48 step: 3066, epoch: 92, batch: 29, loss: 0.40196317434310913, acc: 84.375, f1: 53.094871230464456, r: 0.5285205812041368
06/02/2019 03:56:48 *** evaluating ***
06/02/2019 03:56:48 step: 93, epoch: 92, acc: 59.82905982905983, f1: 24.48075837206272, r: 0.364181482519018
06/02/2019 03:56:48 *** epoch: 94 ***
06/02/2019 03:56:48 *** training ***
06/02/2019 03:56:49 step: 3074, epoch: 93, batch: 4, loss: 0.3484603762626648, acc: 87.5, f1: 75.36858974358975, r: 0.6896784899179802
06/02/2019 03:56:49 step: 3079, epoch: 93, batch: 9, loss: 0.4121147692203522, acc: 78.125, f1: 69.39374351139058, r: 0.5701152718490373
06/02/2019 03:56:49 step: 3084, epoch: 93, batch: 14, loss: 0.47732433676719666, acc: 73.4375, f1: 44.83225108225108, r: 0.5811809599431395
06/02/2019 03:56:49 step: 3089, epoch: 93, batch: 19, loss: 0.4228673577308655, acc: 81.25, f1: 50.570175438596486, r: 0.5543378105372961
06/02/2019 03:56:50 step: 3094, epoch: 93, batch: 24, loss: 0.46873363852500916, acc: 81.25, f1: 73.89125848694583, r: 0.6303934066513436
06/02/2019 03:56:50 step: 3099, epoch: 93, batch: 29, loss: 0.4144204556941986, acc: 81.25, f1: 64.22230028665498, r: 0.6407399701218772
06/02/2019 03:56:50 *** evaluating ***
06/02/2019 03:56:50 step: 94, epoch: 93, acc: 58.97435897435898, f1: 25.926792469825262, r: 0.38170709308391537
06/02/2019 03:56:50 *** epoch: 95 ***
06/02/2019 03:56:50 *** training ***
06/02/2019 03:56:50 step: 3107, epoch: 94, batch: 4, loss: 0.30808576941490173, acc: 82.8125, f1: 66.29761904761904, r: 0.5580497158072583
06/02/2019 03:56:51 step: 3112, epoch: 94, batch: 9, loss: 0.4863132834434509, acc: 78.125, f1: 63.50281793678019, r: 0.5277899636290974
06/02/2019 03:56:51 step: 3117, epoch: 94, batch: 14, loss: 0.4724109470844269, acc: 81.25, f1: 70.36184210526315, r: 0.642887521110652
06/02/2019 03:56:51 step: 3122, epoch: 94, batch: 19, loss: 0.3814488649368286, acc: 79.6875, f1: 63.11535366834907, r: 0.5750755616610296
06/02/2019 03:56:51 step: 3127, epoch: 94, batch: 24, loss: 0.5306358933448792, acc: 76.5625, f1: 54.01244588744589, r: 0.674845783104095
06/02/2019 03:56:52 step: 3132, epoch: 94, batch: 29, loss: 0.35657361149787903, acc: 84.375, f1: 68.57142857142857, r: 0.6025176265271196
06/02/2019 03:56:52 *** evaluating ***
06/02/2019 03:56:52 step: 95, epoch: 94, acc: 59.401709401709404, f1: 24.7251031627911, r: 0.3847991113643553
06/02/2019 03:56:52 *** epoch: 96 ***
06/02/2019 03:56:52 *** training ***
06/02/2019 03:56:52 step: 3140, epoch: 95, batch: 4, loss: 0.31626662611961365, acc: 87.5, f1: 79.27536231884058, r: 0.6257350841038525
06/02/2019 03:56:53 step: 3145, epoch: 95, batch: 9, loss: 0.48604023456573486, acc: 79.6875, f1: 64.2443783068783, r: 0.5687527466823534
06/02/2019 03:56:53 step: 3150, epoch: 95, batch: 14, loss: 0.44878190755844116, acc: 81.25, f1: 78.28598484848486, r: 0.6884413310302484
06/02/2019 03:56:53 step: 3155, epoch: 95, batch: 19, loss: 0.4058975279331207, acc: 85.9375, f1: 63.495450785773365, r: 0.6001058869090575
06/02/2019 03:56:53 step: 3160, epoch: 95, batch: 24, loss: 0.429637610912323, acc: 81.25, f1: 65.3631907308378, r: 0.6299725521252498
06/02/2019 03:56:54 step: 3165, epoch: 95, batch: 29, loss: 0.3974854350090027, acc: 79.6875, f1: 70.10243140318329, r: 0.5833144563604994
06/02/2019 03:56:54 *** evaluating ***
06/02/2019 03:56:54 step: 96, epoch: 95, acc: 60.256410256410255, f1: 27.232795821031115, r: 0.35648608038150825
06/02/2019 03:56:54 *** epoch: 97 ***
06/02/2019 03:56:54 *** training ***
06/02/2019 03:56:54 step: 3173, epoch: 96, batch: 4, loss: 0.3515140116214752, acc: 84.375, f1: 72.94076751946608, r: 0.6462391318786678
06/02/2019 03:56:54 step: 3178, epoch: 96, batch: 9, loss: 0.4234272539615631, acc: 76.5625, f1: 70.49517481411168, r: 0.6459573816868501
06/02/2019 03:56:55 step: 3183, epoch: 96, batch: 14, loss: 0.37587201595306396, acc: 82.8125, f1: 72.06081081081082, r: 0.6562565052564663
06/02/2019 03:56:55 step: 3188, epoch: 96, batch: 19, loss: 0.2651975154876709, acc: 92.1875, f1: 81.59656669860752, r: 0.5930290547511157
06/02/2019 03:56:55 step: 3193, epoch: 96, batch: 24, loss: 0.3044666349887848, acc: 85.9375, f1: 55.98693703956863, r: 0.5158681236010745
06/02/2019 03:56:55 step: 3198, epoch: 96, batch: 29, loss: 0.22960630059242249, acc: 89.0625, f1: 86.77452177452177, r: 0.5720923838616473
06/02/2019 03:56:56 *** evaluating ***
06/02/2019 03:56:56 step: 97, epoch: 96, acc: 59.401709401709404, f1: 25.8656191369606, r: 0.381977955559351
06/02/2019 03:56:56 *** epoch: 98 ***
06/02/2019 03:56:56 *** training ***
06/02/2019 03:56:56 step: 3206, epoch: 97, batch: 4, loss: 0.35501328110694885, acc: 76.5625, f1: 66.99941927990709, r: 0.6579914039082171
06/02/2019 03:56:56 step: 3211, epoch: 97, batch: 9, loss: 0.3786279559135437, acc: 85.9375, f1: 73.23778145137837, r: 0.7061181346272386
06/02/2019 03:56:57 step: 3216, epoch: 97, batch: 14, loss: 0.374225378036499, acc: 79.6875, f1: 58.34800838574423, r: 0.6682899615892848
06/02/2019 03:56:57 step: 3221, epoch: 97, batch: 19, loss: 0.40612760186195374, acc: 81.25, f1: 75.21006783842121, r: 0.686463195797207
06/02/2019 03:56:57 step: 3226, epoch: 97, batch: 24, loss: 0.41296517848968506, acc: 84.375, f1: 59.866771159874624, r: 0.6011377552129229
06/02/2019 03:56:57 step: 3231, epoch: 97, batch: 29, loss: 0.40263551473617554, acc: 81.25, f1: 62.0587886992321, r: 0.5840532021512177
06/02/2019 03:56:57 *** evaluating ***
06/02/2019 03:56:58 step: 98, epoch: 97, acc: 59.401709401709404, f1: 26.02644974073546, r: 0.385952660403225
06/02/2019 03:56:58 *** epoch: 99 ***
06/02/2019 03:56:58 *** training ***
06/02/2019 03:56:58 step: 3239, epoch: 98, batch: 4, loss: 0.46778979897499084, acc: 82.8125, f1: 77.56875746205014, r: 0.6591083765382874
06/02/2019 03:56:58 step: 3244, epoch: 98, batch: 9, loss: 0.3662654757499695, acc: 82.8125, f1: 64.39016602809706, r: 0.6320279342108239
06/02/2019 03:56:58 step: 3249, epoch: 98, batch: 14, loss: 0.35397210717201233, acc: 79.6875, f1: 71.13095238095238, r: 0.6393443182801317
06/02/2019 03:56:59 step: 3254, epoch: 98, batch: 19, loss: 0.46893325448036194, acc: 79.6875, f1: 64.13039710833827, r: 0.6220063964045346
06/02/2019 03:56:59 step: 3259, epoch: 98, batch: 24, loss: 0.2916918396949768, acc: 89.0625, f1: 79.72431077694236, r: 0.6687190650661633
06/02/2019 03:56:59 step: 3264, epoch: 98, batch: 29, loss: 0.30863291025161743, acc: 87.5, f1: 80.4511822262511, r: 0.5835486944789422
06/02/2019 03:56:59 *** evaluating ***
06/02/2019 03:56:59 step: 99, epoch: 98, acc: 59.401709401709404, f1: 25.69799765451939, r: 0.38411451994014395
06/02/2019 03:56:59 *** epoch: 100 ***
06/02/2019 03:56:59 *** training ***
06/02/2019 03:57:00 step: 3272, epoch: 99, batch: 4, loss: 0.3856772780418396, acc: 79.6875, f1: 64.3248814677386, r: 0.5394441142969053
06/02/2019 03:57:00 step: 3277, epoch: 99, batch: 9, loss: 0.31727781891822815, acc: 82.8125, f1: 46.68894830659537, r: 0.5685286479153447
06/02/2019 03:57:00 step: 3282, epoch: 99, batch: 14, loss: 0.4022444784641266, acc: 78.125, f1: 70.67515448430363, r: 0.6209857487970091
06/02/2019 03:57:00 step: 3287, epoch: 99, batch: 19, loss: 0.3395218849182129, acc: 85.9375, f1: 68.40201850294365, r: 0.6838779104522607
06/02/2019 03:57:01 step: 3292, epoch: 99, batch: 24, loss: 0.3767836093902588, acc: 81.25, f1: 74.09722222222221, r: 0.7127363615788319
06/02/2019 03:57:01 step: 3297, epoch: 99, batch: 29, loss: 0.3802044093608856, acc: 84.375, f1: 73.72113997113998, r: 0.5906679050841469
06/02/2019 03:57:01 *** evaluating ***
06/02/2019 03:57:01 step: 100, epoch: 99, acc: 58.54700854700855, f1: 25.180430085576276, r: 0.3688995598309756
06/02/2019 03:57:01 *** epoch: 101 ***
06/02/2019 03:57:01 *** training ***
06/02/2019 03:57:02 step: 3305, epoch: 100, batch: 4, loss: 0.4672129154205322, acc: 75.0, f1: 54.340222575516705, r: 0.5406191691130677
06/02/2019 03:57:02 step: 3310, epoch: 100, batch: 9, loss: 0.3941529095172882, acc: 82.8125, f1: 74.50216450216452, r: 0.6064798672987108
06/02/2019 03:57:02 step: 3315, epoch: 100, batch: 14, loss: 0.49518805742263794, acc: 82.8125, f1: 64.51982799808886, r: 0.6674493047424404
06/02/2019 03:57:02 step: 3320, epoch: 100, batch: 19, loss: 0.46813255548477173, acc: 82.8125, f1: 59.13157962011402, r: 0.6186565901075076
06/02/2019 03:57:03 step: 3325, epoch: 100, batch: 24, loss: 0.40711164474487305, acc: 85.9375, f1: 75.64516129032258, r: 0.5405432872242648
06/02/2019 03:57:03 step: 3330, epoch: 100, batch: 29, loss: 0.29109132289886475, acc: 87.5, f1: 73.02055644160907, r: 0.7127952159210824
06/02/2019 03:57:03 *** evaluating ***
06/02/2019 03:57:03 step: 101, epoch: 100, acc: 59.82905982905983, f1: 25.238948099002446, r: 0.3749645802839499
06/02/2019 03:57:03 *** epoch: 102 ***
06/02/2019 03:57:03 *** training ***
06/02/2019 03:57:03 step: 3338, epoch: 101, batch: 4, loss: 0.27778160572052, acc: 90.625, f1: 79.57091927680162, r: 0.742387927292628
06/02/2019 03:57:04 step: 3343, epoch: 101, batch: 9, loss: 0.3614915609359741, acc: 82.8125, f1: 53.12927796445259, r: 0.5782360083168676
06/02/2019 03:57:04 step: 3348, epoch: 101, batch: 14, loss: 0.3383788466453552, acc: 87.5, f1: 65.74817726133514, r: 0.635530285917039
06/02/2019 03:57:04 step: 3353, epoch: 101, batch: 19, loss: 0.3746088743209839, acc: 76.5625, f1: 59.061715203322706, r: 0.6418388460649244
06/02/2019 03:57:05 step: 3358, epoch: 101, batch: 24, loss: 0.38120487332344055, acc: 84.375, f1: 66.87636165577342, r: 0.5725291121224824
06/02/2019 03:57:05 step: 3363, epoch: 101, batch: 29, loss: 0.40161988139152527, acc: 89.0625, f1: 79.54273612518293, r: 0.6797640810968968
06/02/2019 03:57:05 *** evaluating ***
06/02/2019 03:57:05 step: 102, epoch: 101, acc: 58.54700854700855, f1: 22.383079750411177, r: 0.35701003396180975
06/02/2019 03:57:05 *** epoch: 103 ***
06/02/2019 03:57:05 *** training ***
06/02/2019 03:57:05 step: 3371, epoch: 102, batch: 4, loss: 0.30654436349868774, acc: 85.9375, f1: 76.87157287157288, r: 0.6318207542383231
06/02/2019 03:57:06 step: 3376, epoch: 102, batch: 9, loss: 0.2792069613933563, acc: 89.0625, f1: 78.218655883406, r: 0.6253541376129137
06/02/2019 03:57:06 step: 3381, epoch: 102, batch: 14, loss: 0.3252529203891754, acc: 87.5, f1: 69.53658143099138, r: 0.6245095598335484
06/02/2019 03:57:06 step: 3386, epoch: 102, batch: 19, loss: 0.3283945322036743, acc: 87.5, f1: 69.83654572940287, r: 0.7071807236845058
06/02/2019 03:57:06 step: 3391, epoch: 102, batch: 24, loss: 0.3021244406700134, acc: 82.8125, f1: 75.2421191204725, r: 0.7546236836140064
06/02/2019 03:57:07 step: 3396, epoch: 102, batch: 29, loss: 0.41331300139427185, acc: 78.125, f1: 60.19825268817205, r: 0.6351380537815873
06/02/2019 03:57:07 *** evaluating ***
06/02/2019 03:57:07 step: 103, epoch: 102, acc: 58.97435897435898, f1: 24.277108640745006, r: 0.3574517493270121
06/02/2019 03:57:07 *** epoch: 104 ***
06/02/2019 03:57:07 *** training ***
06/02/2019 03:57:07 step: 3404, epoch: 103, batch: 4, loss: 0.4394908845424652, acc: 75.0, f1: 51.429196966142776, r: 0.5349059926612475
06/02/2019 03:57:07 step: 3409, epoch: 103, batch: 9, loss: 0.32942071557044983, acc: 87.5, f1: 74.94551067180119, r: 0.6441238336565267
06/02/2019 03:57:08 step: 3414, epoch: 103, batch: 14, loss: 0.24204561114311218, acc: 92.1875, f1: 80.08021390374331, r: 0.7458379542238396
06/02/2019 03:57:08 step: 3419, epoch: 103, batch: 19, loss: 0.44205033779144287, acc: 79.6875, f1: 57.07457919363169, r: 0.6356161063986692
06/02/2019 03:57:08 step: 3424, epoch: 103, batch: 24, loss: 0.3781746029853821, acc: 84.375, f1: 67.76643990929705, r: 0.5899994079027601
06/02/2019 03:57:09 step: 3429, epoch: 103, batch: 29, loss: 0.34409740567207336, acc: 84.375, f1: 69.39226619029576, r: 0.5824689379692357
06/02/2019 03:57:09 *** evaluating ***
06/02/2019 03:57:09 step: 104, epoch: 103, acc: 59.82905982905983, f1: 25.313436276137892, r: 0.3728289349124947
06/02/2019 03:57:09 *** epoch: 105 ***
06/02/2019 03:57:09 *** training ***
06/02/2019 03:57:09 step: 3437, epoch: 104, batch: 4, loss: 0.22819927334785461, acc: 92.1875, f1: 92.38970588235293, r: 0.7064060189753751
06/02/2019 03:57:09 step: 3442, epoch: 104, batch: 9, loss: 0.38628894090652466, acc: 81.25, f1: 65.00345592782567, r: 0.6032657225143809
06/02/2019 03:57:10 step: 3447, epoch: 104, batch: 14, loss: 0.38465583324432373, acc: 81.25, f1: 65.51371516348475, r: 0.6031681485934841
06/02/2019 03:57:10 step: 3452, epoch: 104, batch: 19, loss: 0.31464314460754395, acc: 90.625, f1: 78.4546271338724, r: 0.7121473360915191
06/02/2019 03:57:10 step: 3457, epoch: 104, batch: 24, loss: 0.4427306652069092, acc: 76.5625, f1: 59.67940272288098, r: 0.5314921070780594
06/02/2019 03:57:11 step: 3462, epoch: 104, batch: 29, loss: 0.39232879877090454, acc: 85.9375, f1: 80.98970956113814, r: 0.6732134626797763
06/02/2019 03:57:11 *** evaluating ***
06/02/2019 03:57:11 step: 105, epoch: 104, acc: 59.82905982905983, f1: 25.26542536410957, r: 0.36716905300733155
06/02/2019 03:57:11 *** epoch: 106 ***
06/02/2019 03:57:11 *** training ***
06/02/2019 03:57:11 step: 3470, epoch: 105, batch: 4, loss: 0.3113624155521393, acc: 87.5, f1: 84.16650955253897, r: 0.5220552738888045
06/02/2019 03:57:11 step: 3475, epoch: 105, batch: 9, loss: 0.33198198676109314, acc: 82.8125, f1: 64.59016860332649, r: 0.7233827389937829
06/02/2019 03:57:12 step: 3480, epoch: 105, batch: 14, loss: 0.39804351329803467, acc: 79.6875, f1: 55.097624798711756, r: 0.588419026481858
06/02/2019 03:57:12 step: 3485, epoch: 105, batch: 19, loss: 0.3340705633163452, acc: 85.9375, f1: 73.43238923580137, r: 0.7197313840661671
06/02/2019 03:57:12 step: 3490, epoch: 105, batch: 24, loss: 0.2818096876144409, acc: 89.0625, f1: 79.6210552732292, r: 0.5878962175288533
06/02/2019 03:57:13 step: 3495, epoch: 105, batch: 29, loss: 0.3197234272956848, acc: 85.9375, f1: 80.29243047001094, r: 0.6546335547772609
06/02/2019 03:57:13 *** evaluating ***
06/02/2019 03:57:13 step: 106, epoch: 105, acc: 56.41025641025641, f1: 24.50235596487471, r: 0.3707093957379991
06/02/2019 03:57:13 *** epoch: 107 ***
06/02/2019 03:57:13 *** training ***
06/02/2019 03:57:13 step: 3503, epoch: 106, batch: 4, loss: 0.373760849237442, acc: 81.25, f1: 76.37340919949615, r: 0.6793908002307868
06/02/2019 03:57:13 step: 3508, epoch: 106, batch: 9, loss: 0.44913387298583984, acc: 79.6875, f1: 70.36255411255411, r: 0.7352698057930539
06/02/2019 03:57:14 step: 3513, epoch: 106, batch: 14, loss: 0.29229235649108887, acc: 90.625, f1: 86.74655388471177, r: 0.7260258359944692
06/02/2019 03:57:14 step: 3518, epoch: 106, batch: 19, loss: 0.4064447283744812, acc: 79.6875, f1: 61.16808844750021, r: 0.5734217581331362
06/02/2019 03:57:14 step: 3523, epoch: 106, batch: 24, loss: 0.34358248114585876, acc: 82.8125, f1: 69.71392692808301, r: 0.6886634359103964
06/02/2019 03:57:15 step: 3528, epoch: 106, batch: 29, loss: 0.5579373240470886, acc: 71.875, f1: 57.51775814275815, r: 0.625241780593362
06/02/2019 03:57:15 *** evaluating ***
06/02/2019 03:57:15 step: 107, epoch: 106, acc: 58.54700854700855, f1: 24.265270990923703, r: 0.36004180013505466
06/02/2019 03:57:15 *** epoch: 108 ***
06/02/2019 03:57:15 *** training ***
06/02/2019 03:57:15 step: 3536, epoch: 107, batch: 4, loss: 0.28781652450561523, acc: 85.9375, f1: 74.48928056070913, r: 0.7214112784859256
06/02/2019 03:57:15 step: 3541, epoch: 107, batch: 9, loss: 0.28235259652137756, acc: 89.0625, f1: 82.91600212652845, r: 0.653296541659573
06/02/2019 03:57:16 step: 3546, epoch: 107, batch: 14, loss: 0.3062460422515869, acc: 85.9375, f1: 73.61897282651191, r: 0.5422506565659339
06/02/2019 03:57:16 step: 3551, epoch: 107, batch: 19, loss: 0.2547939121723175, acc: 92.1875, f1: 77.67361111111111, r: 0.704472714728598
06/02/2019 03:57:16 step: 3556, epoch: 107, batch: 24, loss: 0.33711013197898865, acc: 84.375, f1: 67.13688610240334, r: 0.5840538725331544
06/02/2019 03:57:17 step: 3561, epoch: 107, batch: 29, loss: 0.31605786085128784, acc: 84.375, f1: 75.12080694346704, r: 0.6349695048566438
06/02/2019 03:57:17 *** evaluating ***
06/02/2019 03:57:17 step: 108, epoch: 107, acc: 59.82905982905983, f1: 25.313436276137892, r: 0.37051150907824243
06/02/2019 03:57:17 *** epoch: 109 ***
06/02/2019 03:57:17 *** training ***
06/02/2019 03:57:17 step: 3569, epoch: 108, batch: 4, loss: 0.2957075238227844, acc: 89.0625, f1: 83.46930846930847, r: 0.6785379889186038
06/02/2019 03:57:17 step: 3574, epoch: 108, batch: 9, loss: 0.31893444061279297, acc: 84.375, f1: 62.160364145658264, r: 0.6663817892924941
06/02/2019 03:57:18 step: 3579, epoch: 108, batch: 14, loss: 0.3883676528930664, acc: 79.6875, f1: 62.30914588057445, r: 0.6072310168238563
06/02/2019 03:57:18 step: 3584, epoch: 108, batch: 19, loss: 0.3276206851005554, acc: 82.8125, f1: 64.80592236894758, r: 0.6679733943365241
06/02/2019 03:57:18 step: 3589, epoch: 108, batch: 24, loss: 0.39255720376968384, acc: 81.25, f1: 68.92605663819413, r: 0.6382717387640935
06/02/2019 03:57:19 step: 3594, epoch: 108, batch: 29, loss: 0.32338544726371765, acc: 82.8125, f1: 77.81828132874372, r: 0.607096188735397
06/02/2019 03:57:19 *** evaluating ***
06/02/2019 03:57:19 step: 109, epoch: 108, acc: 59.401709401709404, f1: 24.445498198252174, r: 0.3532937028707972
06/02/2019 03:57:19 *** epoch: 110 ***
06/02/2019 03:57:19 *** training ***
06/02/2019 03:57:19 step: 3602, epoch: 109, batch: 4, loss: 0.3986509144306183, acc: 81.25, f1: 75.46761571761571, r: 0.6464860934396581
06/02/2019 03:57:19 step: 3607, epoch: 109, batch: 9, loss: 0.2761622667312622, acc: 87.5, f1: 79.90445933994322, r: 0.5639392838845824
06/02/2019 03:57:20 step: 3612, epoch: 109, batch: 14, loss: 0.33498919010162354, acc: 84.375, f1: 68.42891337136861, r: 0.6671799247915203
06/02/2019 03:57:20 step: 3617, epoch: 109, batch: 19, loss: 0.3324251174926758, acc: 85.9375, f1: 70.40925617896062, r: 0.7392924446675939
06/02/2019 03:57:20 step: 3622, epoch: 109, batch: 24, loss: 0.272907555103302, acc: 87.5, f1: 79.8117969546541, r: 0.6032762927175493
06/02/2019 03:57:20 step: 3627, epoch: 109, batch: 29, loss: 0.2932400703430176, acc: 89.0625, f1: 81.50326402321082, r: 0.6519093017183026
06/02/2019 03:57:21 *** evaluating ***
06/02/2019 03:57:21 step: 110, epoch: 109, acc: 58.97435897435898, f1: 24.08315597446032, r: 0.3653681526885685
06/02/2019 03:57:21 *** epoch: 111 ***
06/02/2019 03:57:21 *** training ***
06/02/2019 03:57:21 step: 3635, epoch: 110, batch: 4, loss: 0.3995681703090668, acc: 82.8125, f1: 75.63023361078281, r: 0.6114578748895504
06/02/2019 03:57:21 step: 3640, epoch: 110, batch: 9, loss: 0.28508567810058594, acc: 92.1875, f1: 93.06552660589928, r: 0.6697263282222027
06/02/2019 03:57:22 step: 3645, epoch: 110, batch: 14, loss: 0.331429123878479, acc: 87.5, f1: 70.61958874458874, r: 0.7473050747004003
06/02/2019 03:57:22 step: 3650, epoch: 110, batch: 19, loss: 0.2150137573480606, acc: 93.75, f1: 79.44688644688645, r: 0.6580860515357216
06/02/2019 03:57:22 step: 3655, epoch: 110, batch: 24, loss: 0.3508705496788025, acc: 87.5, f1: 71.97916666666666, r: 0.6859393801496172
06/02/2019 03:57:23 step: 3660, epoch: 110, batch: 29, loss: 0.20320048928260803, acc: 93.75, f1: 76.04299307029338, r: 0.6141173646299509
06/02/2019 03:57:23 *** evaluating ***
06/02/2019 03:57:23 step: 111, epoch: 110, acc: 58.97435897435898, f1: 24.192412412726284, r: 0.366953173295492
06/02/2019 03:57:23 *** epoch: 112 ***
06/02/2019 03:57:23 *** training ***
06/02/2019 03:57:23 step: 3668, epoch: 111, batch: 4, loss: 0.2995178699493408, acc: 90.625, f1: 65.67743764172336, r: 0.6391337397724819
06/02/2019 03:57:23 step: 3673, epoch: 111, batch: 9, loss: 0.31555798649787903, acc: 84.375, f1: 84.91171328671328, r: 0.7384687122252968
06/02/2019 03:57:24 step: 3678, epoch: 111, batch: 14, loss: 0.2512928247451782, acc: 90.625, f1: 79.94274897807507, r: 0.6364238806366911
06/02/2019 03:57:24 step: 3683, epoch: 111, batch: 19, loss: 0.35513070225715637, acc: 82.8125, f1: 59.589134589134595, r: 0.599131903721793
06/02/2019 03:57:24 step: 3688, epoch: 111, batch: 24, loss: 0.4479750692844391, acc: 81.25, f1: 75.95325309611025, r: 0.5922158265615944
06/02/2019 03:57:25 step: 3693, epoch: 111, batch: 29, loss: 0.3259826898574829, acc: 85.9375, f1: 85.41182501708818, r: 0.6035573376758933
06/02/2019 03:57:25 *** evaluating ***
06/02/2019 03:57:25 step: 112, epoch: 111, acc: 59.82905982905983, f1: 24.5373600857347, r: 0.3714205695891233
06/02/2019 03:57:25 *** epoch: 113 ***
06/02/2019 03:57:25 *** training ***
06/02/2019 03:57:25 step: 3701, epoch: 112, batch: 4, loss: 0.32193490862846375, acc: 89.0625, f1: 75.703197945845, r: 0.6839245720576285
06/02/2019 03:57:25 step: 3706, epoch: 112, batch: 9, loss: 0.4137830138206482, acc: 82.8125, f1: 74.29710442170598, r: 0.5907824816475559
06/02/2019 03:57:26 step: 3711, epoch: 112, batch: 14, loss: 0.3386470377445221, acc: 87.5, f1: 81.72938172938173, r: 0.6292211743023335
06/02/2019 03:57:26 step: 3716, epoch: 112, batch: 19, loss: 0.3387155532836914, acc: 82.8125, f1: 76.41748366013073, r: 0.679087303667067
06/02/2019 03:57:26 step: 3721, epoch: 112, batch: 24, loss: 0.22220966219902039, acc: 95.3125, f1: 84.33404558404558, r: 0.7193637980159309
06/02/2019 03:57:26 step: 3726, epoch: 112, batch: 29, loss: 0.3595084547996521, acc: 87.5, f1: 76.66223103394897, r: 0.5844318522957486
06/02/2019 03:57:27 *** evaluating ***
06/02/2019 03:57:27 step: 113, epoch: 112, acc: 60.68376068376068, f1: 26.53658413848631, r: 0.3630777940483868
06/02/2019 03:57:27 *** epoch: 114 ***
06/02/2019 03:57:27 *** training ***
06/02/2019 03:57:27 step: 3734, epoch: 113, batch: 4, loss: 0.48224788904190063, acc: 73.4375, f1: 56.45622895622895, r: 0.5999048329921819
06/02/2019 03:57:27 step: 3739, epoch: 113, batch: 9, loss: 0.2902560234069824, acc: 85.9375, f1: 84.05708874458875, r: 0.7587153922981437
06/02/2019 03:57:28 step: 3744, epoch: 113, batch: 14, loss: 0.2692989110946655, acc: 89.0625, f1: 67.85256410256409, r: 0.7017192194594793
06/02/2019 03:57:28 step: 3749, epoch: 113, batch: 19, loss: 0.29455339908599854, acc: 82.8125, f1: 75.96371882086167, r: 0.6175154806999568
06/02/2019 03:57:28 step: 3754, epoch: 113, batch: 24, loss: 0.30723661184310913, acc: 89.0625, f1: 89.65935342241961, r: 0.5779228021549426
06/02/2019 03:57:29 step: 3759, epoch: 113, batch: 29, loss: 0.2929973006248474, acc: 85.9375, f1: 82.28420730883784, r: 0.6253195607232147
06/02/2019 03:57:29 *** evaluating ***
06/02/2019 03:57:29 step: 114, epoch: 113, acc: 58.97435897435898, f1: 24.478315582300635, r: 0.37621588611479684
06/02/2019 03:57:29 *** epoch: 115 ***
06/02/2019 03:57:29 *** training ***
06/02/2019 03:57:29 step: 3767, epoch: 114, batch: 4, loss: 0.36843451857566833, acc: 85.9375, f1: 81.64423652228531, r: 0.5913828126576626
06/02/2019 03:57:29 step: 3772, epoch: 114, batch: 9, loss: 0.24683916568756104, acc: 87.5, f1: 70.15962581479823, r: 0.6602230443968901
06/02/2019 03:57:30 step: 3777, epoch: 114, batch: 14, loss: 0.3060108423233032, acc: 84.375, f1: 52.00490414133593, r: 0.5029319445656489
06/02/2019 03:57:30 step: 3782, epoch: 114, batch: 19, loss: 0.20315812528133392, acc: 90.625, f1: 74.76895520112808, r: 0.5554829125187895
06/02/2019 03:57:30 step: 3787, epoch: 114, batch: 24, loss: 0.3138688802719116, acc: 84.375, f1: 64.39747327502428, r: 0.5439182434269161
06/02/2019 03:57:30 step: 3792, epoch: 114, batch: 29, loss: 0.34120482206344604, acc: 89.0625, f1: 81.1219545957918, r: 0.7540784397891819
06/02/2019 03:57:31 *** evaluating ***
06/02/2019 03:57:31 step: 115, epoch: 114, acc: 58.119658119658126, f1: 26.08506535359581, r: 0.3764208980446253
06/02/2019 03:57:31 *** epoch: 116 ***
06/02/2019 03:57:31 *** training ***
06/02/2019 03:57:31 step: 3800, epoch: 115, batch: 4, loss: 0.3955785036087036, acc: 82.8125, f1: 67.8129117259552, r: 0.6692855604106209
06/02/2019 03:57:31 step: 3805, epoch: 115, batch: 9, loss: 0.2185865342617035, acc: 92.1875, f1: 87.12433862433862, r: 0.6926887077869088
06/02/2019 03:57:32 step: 3810, epoch: 115, batch: 14, loss: 0.2831772267818451, acc: 87.5, f1: 82.47581254724112, r: 0.6543608496913381
06/02/2019 03:57:32 step: 3815, epoch: 115, batch: 19, loss: 0.2266746163368225, acc: 92.1875, f1: 83.21207430340559, r: 0.6509375376153023
06/02/2019 03:57:32 step: 3820, epoch: 115, batch: 24, loss: 0.23526661098003387, acc: 92.1875, f1: 72.56989247311827, r: 0.5757401485321174
06/02/2019 03:57:32 step: 3825, epoch: 115, batch: 29, loss: 0.22103199362754822, acc: 92.1875, f1: 93.46657283603096, r: 0.6171402300621135
06/02/2019 03:57:33 *** evaluating ***
06/02/2019 03:57:33 step: 116, epoch: 115, acc: 60.68376068376068, f1: 26.500848801026837, r: 0.3686237466718463
06/02/2019 03:57:33 *** epoch: 117 ***
06/02/2019 03:57:33 *** training ***
06/02/2019 03:57:33 step: 3833, epoch: 116, batch: 4, loss: 0.2582365870475769, acc: 90.625, f1: 73.23953823953823, r: 0.6852621021192509
06/02/2019 03:57:33 step: 3838, epoch: 116, batch: 9, loss: 0.4080442488193512, acc: 82.8125, f1: 59.4614840596803, r: 0.49749292988048555
06/02/2019 03:57:34 step: 3843, epoch: 116, batch: 14, loss: 0.2637362480163574, acc: 89.0625, f1: 85.9264122315593, r: 0.7335247407006169
06/02/2019 03:57:34 step: 3848, epoch: 116, batch: 19, loss: 0.3463301658630371, acc: 81.25, f1: 64.89709884846663, r: 0.5682954804933835
06/02/2019 03:57:34 step: 3853, epoch: 116, batch: 24, loss: 0.2761618494987488, acc: 90.625, f1: 89.17479426817289, r: 0.6582659386374502
06/02/2019 03:57:34 step: 3858, epoch: 116, batch: 29, loss: 0.3720819354057312, acc: 81.25, f1: 67.9995004995005, r: 0.6668475358111396
06/02/2019 03:57:35 *** evaluating ***
06/02/2019 03:57:35 step: 117, epoch: 116, acc: 60.68376068376068, f1: 26.350740296118445, r: 0.378214727438895
06/02/2019 03:57:35 *** epoch: 118 ***
06/02/2019 03:57:35 *** training ***
06/02/2019 03:57:35 step: 3866, epoch: 117, batch: 4, loss: 0.3265998661518097, acc: 82.8125, f1: 71.54573453080914, r: 0.5837575670736743
06/02/2019 03:57:35 step: 3871, epoch: 117, batch: 9, loss: 0.2499081790447235, acc: 89.0625, f1: 87.73584054834055, r: 0.7307640691828254
06/02/2019 03:57:35 step: 3876, epoch: 117, batch: 14, loss: 0.38451218605041504, acc: 84.375, f1: 72.36239637555427, r: 0.645028733232755
06/02/2019 03:57:36 step: 3881, epoch: 117, batch: 19, loss: 0.29380086064338684, acc: 85.9375, f1: 69.52651515151516, r: 0.5909699230751848
06/02/2019 03:57:36 step: 3886, epoch: 117, batch: 24, loss: 0.39077028632164, acc: 85.9375, f1: 73.3723544973545, r: 0.6791563615041522
06/02/2019 03:57:36 step: 3891, epoch: 117, batch: 29, loss: 0.25686514377593994, acc: 89.0625, f1: 73.40773809523809, r: 0.7395393538175037
06/02/2019 03:57:36 *** evaluating ***
06/02/2019 03:57:37 step: 118, epoch: 117, acc: 60.68376068376068, f1: 24.860758848994145, r: 0.3770647591226841
06/02/2019 03:57:37 *** epoch: 119 ***
06/02/2019 03:57:37 *** training ***
06/02/2019 03:57:37 step: 3899, epoch: 118, batch: 4, loss: 0.28077930212020874, acc: 85.9375, f1: 80.68685972412679, r: 0.6209423261076306
06/02/2019 03:57:37 step: 3904, epoch: 118, batch: 9, loss: 0.19759654998779297, acc: 93.75, f1: 92.25572047000618, r: 0.6626144651724132
06/02/2019 03:57:37 step: 3909, epoch: 118, batch: 14, loss: 0.33218127489089966, acc: 89.0625, f1: 74.37996031746032, r: 0.643091509037721
06/02/2019 03:57:38 step: 3914, epoch: 118, batch: 19, loss: 0.2767564058303833, acc: 87.5, f1: 78.0473208961581, r: 0.6883168975980167
06/02/2019 03:57:38 step: 3919, epoch: 118, batch: 24, loss: 0.20427630841732025, acc: 87.5, f1: 70.97773168553569, r: 0.5545996337039742
06/02/2019 03:57:38 step: 3924, epoch: 118, batch: 29, loss: 0.2981020212173462, acc: 87.5, f1: 85.66643772893772, r: 0.775824813825024
06/02/2019 03:57:38 *** evaluating ***
06/02/2019 03:57:38 step: 119, epoch: 118, acc: 60.256410256410255, f1: 25.794029685423126, r: 0.3639691262797999
06/02/2019 03:57:38 *** epoch: 120 ***
06/02/2019 03:57:38 *** training ***
06/02/2019 03:57:39 step: 3932, epoch: 119, batch: 4, loss: 0.35317209362983704, acc: 81.25, f1: 75.20394420394422, r: 0.6561722298420174
06/02/2019 03:57:39 step: 3937, epoch: 119, batch: 9, loss: 0.381817102432251, acc: 79.6875, f1: 53.96825396825397, r: 0.6137996862737463
06/02/2019 03:57:39 step: 3942, epoch: 119, batch: 14, loss: 0.2714165449142456, acc: 85.9375, f1: 69.55240429505136, r: 0.6354567479434226
06/02/2019 03:57:39 step: 3947, epoch: 119, batch: 19, loss: 0.3618118166923523, acc: 81.25, f1: 57.590303202186355, r: 0.6380692816933496
06/02/2019 03:57:40 step: 3952, epoch: 119, batch: 24, loss: 0.1781577467918396, acc: 95.3125, f1: 92.35124747573448, r: 0.6210175340576853
06/02/2019 03:57:40 step: 3957, epoch: 119, batch: 29, loss: 0.3265855312347412, acc: 85.9375, f1: 61.465305658046574, r: 0.5951508040665096
06/02/2019 03:57:40 *** evaluating ***
06/02/2019 03:57:40 step: 120, epoch: 119, acc: 60.256410256410255, f1: 24.760860196921573, r: 0.37672444595558163
06/02/2019 03:57:40 *** epoch: 121 ***
06/02/2019 03:57:40 *** training ***
06/02/2019 03:57:41 step: 3965, epoch: 120, batch: 4, loss: 0.19010677933692932, acc: 93.75, f1: 85.06349206349206, r: 0.4666760716390384
06/02/2019 03:57:41 step: 3970, epoch: 120, batch: 9, loss: 0.22085434198379517, acc: 92.1875, f1: 73.32799066568411, r: 0.6655368466268519
06/02/2019 03:57:41 step: 3975, epoch: 120, batch: 14, loss: 0.31250613927841187, acc: 89.0625, f1: 70.52227342549922, r: 0.5314842956963693
06/02/2019 03:57:41 step: 3980, epoch: 120, batch: 19, loss: 0.24781838059425354, acc: 89.0625, f1: 77.46888460531639, r: 0.5507503450076786
06/02/2019 03:57:42 step: 3985, epoch: 120, batch: 24, loss: 0.27849847078323364, acc: 87.5, f1: 72.56592635975527, r: 0.5879822928549958
06/02/2019 03:57:42 step: 3990, epoch: 120, batch: 29, loss: 0.24196229875087738, acc: 89.0625, f1: 79.572940287226, r: 0.6548835212952849
06/02/2019 03:57:42 *** evaluating ***
06/02/2019 03:57:42 step: 121, epoch: 120, acc: 59.401709401709404, f1: 25.567583839801266, r: 0.36904993382475915
06/02/2019 03:57:42 *** epoch: 122 ***
06/02/2019 03:57:42 *** training ***
06/02/2019 03:57:43 step: 3998, epoch: 121, batch: 4, loss: 0.25967681407928467, acc: 89.0625, f1: 81.48938363224077, r: 0.5743433748871363
06/02/2019 03:57:43 step: 4003, epoch: 121, batch: 9, loss: 0.34232765436172485, acc: 87.5, f1: 79.9527701303506, r: 0.5989393805186474
06/02/2019 03:57:43 step: 4008, epoch: 121, batch: 14, loss: 0.32207977771759033, acc: 85.9375, f1: 84.08011031366294, r: 0.6642732714473473
06/02/2019 03:57:43 step: 4013, epoch: 121, batch: 19, loss: 0.29101207852363586, acc: 90.625, f1: 85.76178451178451, r: 0.6771791446531444
06/02/2019 03:57:44 step: 4018, epoch: 121, batch: 24, loss: 0.2225906103849411, acc: 89.0625, f1: 82.32931628517368, r: 0.6412462509679272
06/02/2019 03:57:44 step: 4023, epoch: 121, batch: 29, loss: 0.1943063884973526, acc: 90.625, f1: 71.75720529757797, r: 0.5954669581477136
06/02/2019 03:57:44 *** evaluating ***
06/02/2019 03:57:44 step: 122, epoch: 121, acc: 59.401709401709404, f1: 25.29233995968614, r: 0.3692485815722073
06/02/2019 03:57:44 *** epoch: 123 ***
06/02/2019 03:57:44 *** training ***
06/02/2019 03:57:44 step: 4031, epoch: 122, batch: 4, loss: 0.23969227075576782, acc: 89.0625, f1: 82.547018807523, r: 0.5586633513008223
06/02/2019 03:57:45 step: 4036, epoch: 122, batch: 9, loss: 0.2945711016654968, acc: 89.0625, f1: 73.96664008506113, r: 0.6588984790719146
06/02/2019 03:57:45 step: 4041, epoch: 122, batch: 14, loss: 0.35720038414001465, acc: 84.375, f1: 73.94949343626168, r: 0.6037557188916444
06/02/2019 03:57:45 step: 4046, epoch: 122, batch: 19, loss: 0.21286849677562714, acc: 92.1875, f1: 90.2845348665782, r: 0.7448096769826502
06/02/2019 03:57:46 step: 4051, epoch: 122, batch: 24, loss: 0.15938766300678253, acc: 92.1875, f1: 75.91301265517292, r: 0.621520338971908
06/02/2019 03:57:46 step: 4056, epoch: 122, batch: 29, loss: 0.2748633027076721, acc: 89.0625, f1: 81.36600221483943, r: 0.6567481536599967
06/02/2019 03:57:46 *** evaluating ***
06/02/2019 03:57:46 step: 123, epoch: 122, acc: 59.401709401709404, f1: 24.62937147425596, r: 0.366082850884312
06/02/2019 03:57:46 *** epoch: 124 ***
06/02/2019 03:57:46 *** training ***
06/02/2019 03:57:46 step: 4064, epoch: 123, batch: 4, loss: 0.305688738822937, acc: 89.0625, f1: 84.360851470114, r: 0.6185005845728482
06/02/2019 03:57:47 step: 4069, epoch: 123, batch: 9, loss: 0.25689518451690674, acc: 89.0625, f1: 63.3735627552117, r: 0.5996107161396569
06/02/2019 03:57:47 step: 4074, epoch: 123, batch: 14, loss: 0.35949790477752686, acc: 85.9375, f1: 56.50487588652482, r: 0.6363886054971517
06/02/2019 03:57:47 step: 4079, epoch: 123, batch: 19, loss: 0.2116602212190628, acc: 93.75, f1: 94.83789322857025, r: 0.7639143222988601
06/02/2019 03:57:48 step: 4084, epoch: 123, batch: 24, loss: 0.30543267726898193, acc: 87.5, f1: 80.55723724896656, r: 0.6943251354895635
06/02/2019 03:57:48 step: 4089, epoch: 123, batch: 29, loss: 0.3982395529747009, acc: 79.6875, f1: 69.68359436797975, r: 0.5864216745252171
06/02/2019 03:57:48 *** evaluating ***
06/02/2019 03:57:48 step: 124, epoch: 123, acc: 59.401709401709404, f1: 24.28526357758247, r: 0.3609318469750627
06/02/2019 03:57:48 *** epoch: 125 ***
06/02/2019 03:57:48 *** training ***
06/02/2019 03:57:48 step: 4097, epoch: 124, batch: 4, loss: 0.37205228209495544, acc: 79.6875, f1: 75.41578045610304, r: 0.6779047279737556
06/02/2019 03:57:49 step: 4102, epoch: 124, batch: 9, loss: 0.22397470474243164, acc: 89.0625, f1: 85.34260080843931, r: 0.639050634413911
06/02/2019 03:57:49 step: 4107, epoch: 124, batch: 14, loss: 0.2881152033805847, acc: 87.5, f1: 83.22278911564625, r: 0.7338625106484225
06/02/2019 03:57:49 step: 4112, epoch: 124, batch: 19, loss: 0.13258185982704163, acc: 93.75, f1: 88.15608127308894, r: 0.5440177908315819
06/02/2019 03:57:49 step: 4117, epoch: 124, batch: 24, loss: 0.36670517921447754, acc: 84.375, f1: 67.11682424094076, r: 0.5358014745397034
06/02/2019 03:57:50 step: 4122, epoch: 124, batch: 29, loss: 0.290621817111969, acc: 90.625, f1: 81.27769334684228, r: 0.731028562671001
06/02/2019 03:57:50 *** evaluating ***
06/02/2019 03:57:50 step: 125, epoch: 124, acc: 60.256410256410255, f1: 24.566681821626126, r: 0.3678345103347932
06/02/2019 03:57:50 *** epoch: 126 ***
06/02/2019 03:57:50 *** training ***
06/02/2019 03:57:50 step: 4130, epoch: 125, batch: 4, loss: 0.3183850049972534, acc: 87.5, f1: 73.71247412008282, r: 0.6996934982511811
06/02/2019 03:57:51 step: 4135, epoch: 125, batch: 9, loss: 0.2509676218032837, acc: 93.75, f1: 91.6735282435221, r: 0.5667127997427913
06/02/2019 03:57:51 step: 4140, epoch: 125, batch: 14, loss: 0.2398909628391266, acc: 87.5, f1: 77.5819369258407, r: 0.6354648194570349
06/02/2019 03:57:51 step: 4145, epoch: 125, batch: 19, loss: 0.31209298968315125, acc: 89.0625, f1: 82.61837740022116, r: 0.594509839946093
06/02/2019 03:57:51 step: 4150, epoch: 125, batch: 24, loss: 0.4069902300834656, acc: 82.8125, f1: 67.83385093167702, r: 0.6236701764440075
06/02/2019 03:57:52 step: 4155, epoch: 125, batch: 29, loss: 0.31358957290649414, acc: 87.5, f1: 64.6344714013887, r: 0.5422186649112599
06/02/2019 03:57:52 *** evaluating ***
06/02/2019 03:57:52 step: 126, epoch: 125, acc: 59.82905982905983, f1: 26.043034605146403, r: 0.3671172460346979
06/02/2019 03:57:52 *** epoch: 127 ***
06/02/2019 03:57:52 *** training ***
06/02/2019 03:57:52 step: 4163, epoch: 126, batch: 4, loss: 0.27092698216438293, acc: 92.1875, f1: 89.6909986565159, r: 0.6325053215035363
06/02/2019 03:57:52 step: 4168, epoch: 126, batch: 9, loss: 0.1818893551826477, acc: 95.3125, f1: 93.83081674673987, r: 0.7134204851186987
06/02/2019 03:57:53 step: 4173, epoch: 126, batch: 14, loss: 0.19255484640598297, acc: 93.75, f1: 90.12058702791461, r: 0.6487103437957008
06/02/2019 03:57:53 step: 4178, epoch: 126, batch: 19, loss: 0.31567415595054626, acc: 89.0625, f1: 87.56286605551311, r: 0.7237502275118812
06/02/2019 03:57:53 step: 4183, epoch: 126, batch: 24, loss: 0.43330687284469604, acc: 79.6875, f1: 67.26190476190477, r: 0.6645470695939262
06/02/2019 03:57:54 step: 4188, epoch: 126, batch: 29, loss: 0.40623849630355835, acc: 82.8125, f1: 72.95296033302031, r: 0.5765886839956621
06/02/2019 03:57:54 *** evaluating ***
06/02/2019 03:57:54 step: 127, epoch: 126, acc: 60.256410256410255, f1: 27.721088435374153, r: 0.3749820810035691
06/02/2019 03:57:54 *** epoch: 128 ***
06/02/2019 03:57:54 *** training ***
06/02/2019 03:57:54 step: 4196, epoch: 127, batch: 4, loss: 0.18059125542640686, acc: 95.3125, f1: 94.78163559796214, r: 0.6603366384253897
06/02/2019 03:57:54 step: 4201, epoch: 127, batch: 9, loss: 0.1808248609304428, acc: 93.75, f1: 87.97688886875505, r: 0.6423018037646693
06/02/2019 03:57:55 step: 4206, epoch: 127, batch: 14, loss: 0.18100234866142273, acc: 92.1875, f1: 73.70276915113871, r: 0.6322457160484933
06/02/2019 03:57:55 step: 4211, epoch: 127, batch: 19, loss: 0.2270573526620865, acc: 90.625, f1: 83.13852813852813, r: 0.5397694468625182
06/02/2019 03:57:55 step: 4216, epoch: 127, batch: 24, loss: 0.232487291097641, acc: 90.625, f1: 84.52529674076989, r: 0.7506214087757743
06/02/2019 03:57:56 step: 4221, epoch: 127, batch: 29, loss: 0.22305448353290558, acc: 93.75, f1: 91.55089529109685, r: 0.7270741757176692
06/02/2019 03:57:56 *** evaluating ***
06/02/2019 03:57:56 step: 128, epoch: 127, acc: 59.401709401709404, f1: 25.148255499118665, r: 0.3806706450437526
06/02/2019 03:57:56 *** epoch: 129 ***
06/02/2019 03:57:56 *** training ***
06/02/2019 03:57:56 step: 4229, epoch: 128, batch: 4, loss: 0.19463664293289185, acc: 93.75, f1: 85.91720779220779, r: 0.6771765849623641
06/02/2019 03:57:56 step: 4234, epoch: 128, batch: 9, loss: 0.23516780138015747, acc: 89.0625, f1: 83.84305006587616, r: 0.728131901339089
06/02/2019 03:57:57 step: 4239, epoch: 128, batch: 14, loss: 0.2491844892501831, acc: 92.1875, f1: 80.6231884057971, r: 0.6660059708365382
06/02/2019 03:57:57 step: 4244, epoch: 128, batch: 19, loss: 0.14847296476364136, acc: 96.875, f1: 91.61765981438111, r: 0.5489355144640746
06/02/2019 03:57:57 step: 4249, epoch: 128, batch: 24, loss: 0.2859545052051544, acc: 90.625, f1: 91.56929696443373, r: 0.6740796375423911
06/02/2019 03:57:57 step: 4254, epoch: 128, batch: 29, loss: 0.32119742035865784, acc: 82.8125, f1: 70.65113681017657, r: 0.5887484101951288
06/02/2019 03:57:58 *** evaluating ***
06/02/2019 03:57:58 step: 129, epoch: 128, acc: 61.111111111111114, f1: 26.541240771497254, r: 0.37782479949263736
06/02/2019 03:57:58 *** epoch: 130 ***
06/02/2019 03:57:58 *** training ***
06/02/2019 03:57:58 step: 4262, epoch: 129, batch: 4, loss: 0.21033895015716553, acc: 93.75, f1: 88.45133636615448, r: 0.6574983550715108
06/02/2019 03:57:58 step: 4267, epoch: 129, batch: 9, loss: 0.27074944972991943, acc: 89.0625, f1: 74.45582399626517, r: 0.7231398904462113
06/02/2019 03:57:59 step: 4272, epoch: 129, batch: 14, loss: 0.3168795704841614, acc: 81.25, f1: 74.10724896019015, r: 0.7306204503013154
06/02/2019 03:57:59 step: 4277, epoch: 129, batch: 19, loss: 0.39308393001556396, acc: 81.25, f1: 73.63324175824177, r: 0.5850435933112023
06/02/2019 03:57:59 step: 4282, epoch: 129, batch: 24, loss: 0.24744591116905212, acc: 90.625, f1: 72.53062456866805, r: 0.6831938984107976
06/02/2019 03:57:59 step: 4287, epoch: 129, batch: 29, loss: 0.33620941638946533, acc: 87.5, f1: 81.94313909774436, r: 0.7013301347880545
06/02/2019 03:58:00 *** evaluating ***
06/02/2019 03:58:00 step: 130, epoch: 129, acc: 60.256410256410255, f1: 26.66604229104229, r: 0.3765841486514742
06/02/2019 03:58:00 *** epoch: 131 ***
06/02/2019 03:58:00 *** training ***
06/02/2019 03:58:00 step: 4295, epoch: 130, batch: 4, loss: 0.2508155405521393, acc: 92.1875, f1: 74.05357142857143, r: 0.6099204551076507
06/02/2019 03:58:00 step: 4300, epoch: 130, batch: 9, loss: 0.16941313445568085, acc: 93.75, f1: 87.80284147557327, r: 0.7363642580930905
06/02/2019 03:58:01 step: 4305, epoch: 130, batch: 14, loss: 0.24982403218746185, acc: 92.1875, f1: 83.7839366515837, r: 0.6313721737215923
06/02/2019 03:58:01 step: 4310, epoch: 130, batch: 19, loss: 0.30108433961868286, acc: 85.9375, f1: 73.70883885788199, r: 0.7313637288856399
06/02/2019 03:58:01 step: 4315, epoch: 130, batch: 24, loss: 0.2853136360645294, acc: 90.625, f1: 79.17356349654486, r: 0.5014900922313786
06/02/2019 03:58:01 step: 4320, epoch: 130, batch: 29, loss: 0.2578667104244232, acc: 87.5, f1: 78.4584603071998, r: 0.6267033249368076
06/02/2019 03:58:02 *** evaluating ***
06/02/2019 03:58:02 step: 131, epoch: 130, acc: 59.82905982905983, f1: 24.485412304990884, r: 0.3725556616363178
06/02/2019 03:58:02 *** epoch: 132 ***
06/02/2019 03:58:02 *** training ***
06/02/2019 03:58:02 step: 4328, epoch: 131, batch: 4, loss: 0.26385897397994995, acc: 93.75, f1: 87.6892859857121, r: 0.6589831460477723
06/02/2019 03:58:02 step: 4333, epoch: 131, batch: 9, loss: 0.3038342595100403, acc: 87.5, f1: 80.33969539232697, r: 0.5593858130359777
06/02/2019 03:58:03 step: 4338, epoch: 131, batch: 14, loss: 0.18160520493984222, acc: 92.1875, f1: 86.9059526644666, r: 0.5805558894607801
06/02/2019 03:58:03 step: 4343, epoch: 131, batch: 19, loss: 0.20740610361099243, acc: 92.1875, f1: 74.77496048924621, r: 0.6214514864437376
06/02/2019 03:58:03 step: 4348, epoch: 131, batch: 24, loss: 0.14762252569198608, acc: 95.3125, f1: 69.688013136289, r: 0.6925656821261954
06/02/2019 03:58:03 step: 4353, epoch: 131, batch: 29, loss: 0.22300860285758972, acc: 93.75, f1: 78.63468768074031, r: 0.6510958737731335
06/02/2019 03:58:04 *** evaluating ***
06/02/2019 03:58:04 step: 132, epoch: 131, acc: 59.401709401709404, f1: 24.450164569729786, r: 0.36768344236965333
06/02/2019 03:58:04 *** epoch: 133 ***
06/02/2019 03:58:04 *** training ***
06/02/2019 03:58:04 step: 4361, epoch: 132, batch: 4, loss: 0.2939772605895996, acc: 90.625, f1: 87.93428793428794, r: 0.5991622362118272
06/02/2019 03:58:04 step: 4366, epoch: 132, batch: 9, loss: 0.24414831399917603, acc: 92.1875, f1: 82.81588586851744, r: 0.6539151197311004
06/02/2019 03:58:04 step: 4371, epoch: 132, batch: 14, loss: 0.36228594183921814, acc: 81.25, f1: 53.939393939393945, r: 0.6995375623065824
06/02/2019 03:58:05 step: 4376, epoch: 132, batch: 19, loss: 0.30833548307418823, acc: 85.9375, f1: 82.40070440969993, r: 0.5927881579402686
06/02/2019 03:58:05 step: 4381, epoch: 132, batch: 24, loss: 0.21729284524917603, acc: 93.75, f1: 79.7108551825533, r: 0.5808537055718991
06/02/2019 03:58:05 step: 4386, epoch: 132, batch: 29, loss: 0.3261237144470215, acc: 82.8125, f1: 70.4626103063603, r: 0.6354446827574096
06/02/2019 03:58:05 *** evaluating ***
06/02/2019 03:58:06 step: 133, epoch: 132, acc: 59.401709401709404, f1: 24.500552086073466, r: 0.36442294723155555
06/02/2019 03:58:06 *** epoch: 134 ***
06/02/2019 03:58:06 *** training ***
06/02/2019 03:58:06 step: 4394, epoch: 133, batch: 4, loss: 0.3745913505554199, acc: 85.9375, f1: 82.2693179836037, r: 0.5973235797435312
06/02/2019 03:58:06 step: 4399, epoch: 133, batch: 9, loss: 0.32619279623031616, acc: 87.5, f1: 82.18040462938424, r: 0.6396202108226541
06/02/2019 03:58:06 step: 4404, epoch: 133, batch: 14, loss: 0.2915118932723999, acc: 84.375, f1: 71.25699970527558, r: 0.6975743344201862
06/02/2019 03:58:07 step: 4409, epoch: 133, batch: 19, loss: 0.2541254162788391, acc: 89.0625, f1: 72.17532467532467, r: 0.7954565561154237
06/02/2019 03:58:07 step: 4414, epoch: 133, batch: 24, loss: 0.21564044058322906, acc: 92.1875, f1: 78.97041062801932, r: 0.7197737859176022
06/02/2019 03:58:07 step: 4419, epoch: 133, batch: 29, loss: 0.3396607041358948, acc: 84.375, f1: 80.72420634920634, r: 0.7140159069210167
06/02/2019 03:58:07 *** evaluating ***
06/02/2019 03:58:08 step: 134, epoch: 133, acc: 58.54700854700855, f1: 26.23699015471167, r: 0.3736009216287175
06/02/2019 03:58:08 *** epoch: 135 ***
06/02/2019 03:58:08 *** training ***
06/02/2019 03:58:08 step: 4427, epoch: 134, batch: 4, loss: 0.2854822874069214, acc: 92.1875, f1: 76.81227106227107, r: 0.6802813759803271
06/02/2019 03:58:08 step: 4432, epoch: 134, batch: 9, loss: 0.14381490647792816, acc: 96.875, f1: 97.48814677386108, r: 0.6060226395042048
06/02/2019 03:58:08 step: 4437, epoch: 134, batch: 14, loss: 0.2134483903646469, acc: 90.625, f1: 81.61623366214468, r: 0.7244198411790527
06/02/2019 03:58:09 step: 4442, epoch: 134, batch: 19, loss: 0.2562607526779175, acc: 92.1875, f1: 90.81632653061226, r: 0.6818839406825071
06/02/2019 03:58:09 step: 4447, epoch: 134, batch: 24, loss: 0.24975086748600006, acc: 87.5, f1: 81.61331422200988, r: 0.6697608495559485
06/02/2019 03:58:09 step: 4452, epoch: 134, batch: 29, loss: 0.2752884030342102, acc: 85.9375, f1: 71.69483294483294, r: 0.7012868599206564
06/02/2019 03:58:10 *** evaluating ***
06/02/2019 03:58:10 step: 135, epoch: 134, acc: 59.401709401709404, f1: 24.336486067829355, r: 0.3720153029439272
06/02/2019 03:58:10 *** epoch: 136 ***
06/02/2019 03:58:10 *** training ***
06/02/2019 03:58:10 step: 4460, epoch: 135, batch: 4, loss: 0.33607637882232666, acc: 85.9375, f1: 81.5550062963856, r: 0.6271296171448689
06/02/2019 03:58:10 step: 4465, epoch: 135, batch: 9, loss: 0.1662181317806244, acc: 95.3125, f1: 91.25000000000001, r: 0.7386702163053085
06/02/2019 03:58:11 step: 4470, epoch: 135, batch: 14, loss: 0.3106134533882141, acc: 87.5, f1: 82.1885521885522, r: 0.676470614964708
06/02/2019 03:58:11 step: 4475, epoch: 135, batch: 19, loss: 0.2751208245754242, acc: 87.5, f1: 79.5827664399093, r: 0.628666819104089
06/02/2019 03:58:11 step: 4480, epoch: 135, batch: 24, loss: 0.25464409589767456, acc: 87.5, f1: 80.2486559139785, r: 0.7609975293865993
06/02/2019 03:58:11 step: 4485, epoch: 135, batch: 29, loss: 0.191202312707901, acc: 96.875, f1: 97.63034188034189, r: 0.6271078628112771
06/02/2019 03:58:12 *** evaluating ***
06/02/2019 03:58:12 step: 136, epoch: 135, acc: 59.401709401709404, f1: 24.086415472337805, r: 0.3674512276290123
06/02/2019 03:58:12 *** epoch: 137 ***
06/02/2019 03:58:12 *** training ***
06/02/2019 03:58:12 step: 4493, epoch: 136, batch: 4, loss: 0.40993183851242065, acc: 84.375, f1: 72.8291847041847, r: 0.6874125106920203
06/02/2019 03:58:12 step: 4498, epoch: 136, batch: 9, loss: 0.2144652009010315, acc: 93.75, f1: 84.028298741352, r: 0.6255671277325067
06/02/2019 03:58:13 step: 4503, epoch: 136, batch: 14, loss: 0.20088322460651398, acc: 92.1875, f1: 91.28686196700903, r: 0.8080563507196274
06/02/2019 03:58:13 step: 4508, epoch: 136, batch: 19, loss: 0.32991063594818115, acc: 84.375, f1: 82.1453331151607, r: 0.6862738716513406
06/02/2019 03:58:13 step: 4513, epoch: 136, batch: 24, loss: 0.2433311641216278, acc: 90.625, f1: 85.69709250560315, r: 0.6674805931025275
06/02/2019 03:58:13 step: 4518, epoch: 136, batch: 29, loss: 0.20794366300106049, acc: 90.625, f1: 76.42974431151771, r: 0.5596431706522182
06/02/2019 03:58:14 *** evaluating ***
06/02/2019 03:58:14 step: 137, epoch: 136, acc: 59.401709401709404, f1: 24.404317904317903, r: 0.37423482758369353
06/02/2019 03:58:14 *** epoch: 138 ***
06/02/2019 03:58:14 *** training ***
06/02/2019 03:58:14 step: 4526, epoch: 137, batch: 4, loss: 0.25514546036720276, acc: 89.0625, f1: 66.81998556998558, r: 0.6608342314560975
06/02/2019 03:58:14 step: 4531, epoch: 137, batch: 9, loss: 0.21406038105487823, acc: 93.75, f1: 85.63357938357939, r: 0.6914856830455487
06/02/2019 03:58:15 step: 4536, epoch: 137, batch: 14, loss: 0.2581974267959595, acc: 90.625, f1: 84.34952144629564, r: 0.6803893205121373
06/02/2019 03:58:15 step: 4541, epoch: 137, batch: 19, loss: 0.17550864815711975, acc: 92.1875, f1: 85.323534175575, r: 0.6414318572232646
06/02/2019 03:58:15 step: 4546, epoch: 137, batch: 24, loss: 0.26300495862960815, acc: 93.75, f1: 87.91186411671586, r: 0.6573275761318776
06/02/2019 03:58:15 step: 4551, epoch: 137, batch: 29, loss: 0.2808777689933777, acc: 89.0625, f1: 74.50148809523809, r: 0.7650377822497898
06/02/2019 03:58:15 *** evaluating ***
06/02/2019 03:58:16 step: 138, epoch: 137, acc: 59.82905982905983, f1: 24.591422752505228, r: 0.36702496100412046
06/02/2019 03:58:16 *** epoch: 139 ***
06/02/2019 03:58:16 *** training ***
06/02/2019 03:58:16 step: 4559, epoch: 138, batch: 4, loss: 0.20655283331871033, acc: 93.75, f1: 89.20361990950225, r: 0.6040255816648028
06/02/2019 03:58:16 step: 4564, epoch: 138, batch: 9, loss: 0.20605860650539398, acc: 93.75, f1: 82.83736378563964, r: 0.7450848491600268
06/02/2019 03:58:16 step: 4569, epoch: 138, batch: 14, loss: 0.31826457381248474, acc: 85.9375, f1: 82.96333874458874, r: 0.7521656306404108
06/02/2019 03:58:17 step: 4574, epoch: 138, batch: 19, loss: 0.22771932184696198, acc: 89.0625, f1: 84.5863791423002, r: 0.658428606263652
06/02/2019 03:58:17 step: 4579, epoch: 138, batch: 24, loss: 0.2760332226753235, acc: 85.9375, f1: 66.92442156114925, r: 0.7037707779645639
06/02/2019 03:58:17 step: 4584, epoch: 138, batch: 29, loss: 0.14879894256591797, acc: 93.75, f1: 89.70965865206185, r: 0.744234789454493
06/02/2019 03:58:17 *** evaluating ***
06/02/2019 03:58:17 step: 139, epoch: 138, acc: 59.401709401709404, f1: 25.04893667993513, r: 0.3642020551866979
06/02/2019 03:58:17 *** epoch: 140 ***
06/02/2019 03:58:17 *** training ***
06/02/2019 03:58:18 step: 4592, epoch: 139, batch: 4, loss: 0.24936869740486145, acc: 87.5, f1: 86.6480140443274, r: 0.6096148608250532
06/02/2019 03:58:18 step: 4597, epoch: 139, batch: 9, loss: 0.4192693531513214, acc: 82.8125, f1: 74.37728937728937, r: 0.6556646054919124
06/02/2019 03:58:18 step: 4602, epoch: 139, batch: 14, loss: 0.15461695194244385, acc: 93.75, f1: 78.99076872128161, r: 0.6525796594389055
06/02/2019 03:58:19 step: 4607, epoch: 139, batch: 19, loss: 0.3422369062900543, acc: 82.8125, f1: 76.68189014736329, r: 0.675141333604207
06/02/2019 03:58:19 step: 4612, epoch: 139, batch: 24, loss: 0.3150431215763092, acc: 85.9375, f1: 73.74588455627253, r: 0.6728967046920723
06/02/2019 03:58:19 step: 4617, epoch: 139, batch: 29, loss: 0.25965672731399536, acc: 90.625, f1: 81.14515250544663, r: 0.6587048280931271
06/02/2019 03:58:19 *** evaluating ***
06/02/2019 03:58:19 step: 140, epoch: 139, acc: 59.82905982905983, f1: 25.35798142218697, r: 0.3802882219108964
06/02/2019 03:58:19 *** epoch: 141 ***
06/02/2019 03:58:19 *** training ***
06/02/2019 03:58:20 step: 4625, epoch: 140, batch: 4, loss: 0.30200278759002686, acc: 84.375, f1: 61.34154040404041, r: 0.5739509883898248
06/02/2019 03:58:20 step: 4630, epoch: 140, batch: 9, loss: 0.1811520904302597, acc: 95.3125, f1: 80.80126912425048, r: 0.6289960956776052
06/02/2019 03:58:20 step: 4635, epoch: 140, batch: 14, loss: 0.21634450554847717, acc: 90.625, f1: 84.83495670995671, r: 0.6343698846913951
06/02/2019 03:58:21 step: 4640, epoch: 140, batch: 19, loss: 0.25343406200408936, acc: 89.0625, f1: 83.05400459597962, r: 0.7121801984599933
06/02/2019 03:58:21 step: 4645, epoch: 140, batch: 24, loss: 0.2966805100440979, acc: 87.5, f1: 83.80768523625666, r: 0.575443456206283
06/02/2019 03:58:21 step: 4650, epoch: 140, batch: 29, loss: 0.1859985888004303, acc: 96.875, f1: 91.63883735312307, r: 0.7119427651069292
06/02/2019 03:58:21 *** evaluating ***
06/02/2019 03:58:21 step: 141, epoch: 140, acc: 58.97435897435898, f1: 24.238494981360404, r: 0.35458114323869666
06/02/2019 03:58:21 *** epoch: 142 ***
06/02/2019 03:58:21 *** training ***
06/02/2019 03:58:22 step: 4658, epoch: 141, batch: 4, loss: 0.30319759249687195, acc: 85.9375, f1: 84.40118152524168, r: 0.6397463228327283
06/02/2019 03:58:22 step: 4663, epoch: 141, batch: 9, loss: 0.23774242401123047, acc: 92.1875, f1: 85.66964285714286, r: 0.7974440540661438
06/02/2019 03:58:22 step: 4668, epoch: 141, batch: 14, loss: 0.15262262523174286, acc: 95.3125, f1: 75.93963413635547, r: 0.7110504513967943
06/02/2019 03:58:23 step: 4673, epoch: 141, batch: 19, loss: 0.2814486026763916, acc: 87.5, f1: 62.93029675638372, r: 0.5211993183932242
06/02/2019 03:58:23 step: 4678, epoch: 141, batch: 24, loss: 0.15205112099647522, acc: 95.3125, f1: 83.24440052700923, r: 0.7300855242879377
06/02/2019 03:58:23 step: 4683, epoch: 141, batch: 29, loss: 0.2174394279718399, acc: 89.0625, f1: 63.207895748218334, r: 0.6634543267414058
06/02/2019 03:58:23 *** evaluating ***
06/02/2019 03:58:23 step: 142, epoch: 141, acc: 59.82905982905983, f1: 24.81379974607066, r: 0.37509189172050234
06/02/2019 03:58:23 *** epoch: 143 ***
06/02/2019 03:58:23 *** training ***
06/02/2019 03:58:24 step: 4691, epoch: 142, batch: 4, loss: 0.29946640133857727, acc: 89.0625, f1: 71.82472968061424, r: 0.6094120018435171
06/02/2019 03:58:24 step: 4696, epoch: 142, batch: 9, loss: 0.30125436186790466, acc: 85.9375, f1: 64.19622884559017, r: 0.6330196465875939
06/02/2019 03:58:24 step: 4701, epoch: 142, batch: 14, loss: 0.2687284052371979, acc: 87.5, f1: 66.08387920385546, r: 0.6098526849229291
06/02/2019 03:58:25 step: 4706, epoch: 142, batch: 19, loss: 0.21417412161827087, acc: 89.0625, f1: 92.30287256603046, r: 0.6888002485892375
06/02/2019 03:58:25 step: 4711, epoch: 142, batch: 24, loss: 0.19742600619792938, acc: 85.9375, f1: 78.62806661125988, r: 0.6239259964384548
06/02/2019 03:58:25 step: 4716, epoch: 142, batch: 29, loss: 0.344698041677475, acc: 85.9375, f1: 70.34798534798536, r: 0.6572424686108519
06/02/2019 03:58:25 *** evaluating ***
06/02/2019 03:58:25 step: 143, epoch: 142, acc: 59.82905982905983, f1: 25.25185618615902, r: 0.3817854421917063
06/02/2019 03:58:25 *** epoch: 144 ***
06/02/2019 03:58:25 *** training ***
06/02/2019 03:58:26 step: 4724, epoch: 143, batch: 4, loss: 0.3055565655231476, acc: 87.5, f1: 82.21205723668777, r: 0.6193048109706045
06/02/2019 03:58:26 step: 4729, epoch: 143, batch: 9, loss: 0.206632599234581, acc: 92.1875, f1: 84.43688586545728, r: 0.6098913564878659
06/02/2019 03:58:26 step: 4734, epoch: 143, batch: 14, loss: 0.2446894645690918, acc: 89.0625, f1: 70.19627192982456, r: 0.7415816881078233
06/02/2019 03:58:26 step: 4739, epoch: 143, batch: 19, loss: 0.3051261901855469, acc: 85.9375, f1: 72.65993265993266, r: 0.6562509668779536
06/02/2019 03:58:27 step: 4744, epoch: 143, batch: 24, loss: 0.30793297290802, acc: 85.9375, f1: 81.69932079414839, r: 0.6408292485306294
06/02/2019 03:58:27 step: 4749, epoch: 143, batch: 29, loss: 0.21822884678840637, acc: 92.1875, f1: 85.34722222222221, r: 0.7172260771203637
06/02/2019 03:58:27 *** evaluating ***
06/02/2019 03:58:27 step: 144, epoch: 143, acc: 59.401709401709404, f1: 25.973344078565596, r: 0.38522125079402014
06/02/2019 03:58:27 *** epoch: 145 ***
06/02/2019 03:58:27 *** training ***
06/02/2019 03:58:28 step: 4757, epoch: 144, batch: 4, loss: 0.29551970958709717, acc: 85.9375, f1: 84.92788461538461, r: 0.7220662664232231
06/02/2019 03:58:28 step: 4762, epoch: 144, batch: 9, loss: 0.23144499957561493, acc: 95.3125, f1: 93.17797816801139, r: 0.6302991064861851
06/02/2019 03:58:28 step: 4767, epoch: 144, batch: 14, loss: 0.24152827262878418, acc: 89.0625, f1: 84.54167269134373, r: 0.678091975250675
06/02/2019 03:58:28 step: 4772, epoch: 144, batch: 19, loss: 0.2685855031013489, acc: 90.625, f1: 88.18327330932372, r: 0.635331906169135
06/02/2019 03:58:29 step: 4777, epoch: 144, batch: 24, loss: 0.16950678825378418, acc: 98.4375, f1: 99.17874396135265, r: 0.7165258793474395
06/02/2019 03:58:29 step: 4782, epoch: 144, batch: 29, loss: 0.21331456303596497, acc: 84.375, f1: 82.4814570989686, r: 0.6110113292055271
06/02/2019 03:58:29 *** evaluating ***
06/02/2019 03:58:29 step: 145, epoch: 144, acc: 58.97435897435898, f1: 24.42702881311129, r: 0.3703248794873003
06/02/2019 03:58:29 *** epoch: 146 ***
06/02/2019 03:58:29 *** training ***
06/02/2019 03:58:29 step: 4790, epoch: 145, batch: 4, loss: 0.17511500418186188, acc: 93.75, f1: 90.64625850340136, r: 0.6629901861972903
06/02/2019 03:58:30 step: 4795, epoch: 145, batch: 9, loss: 0.2579326331615448, acc: 90.625, f1: 76.68876262626263, r: 0.687773270459582
06/02/2019 03:58:30 step: 4800, epoch: 145, batch: 14, loss: 0.24206066131591797, acc: 89.0625, f1: 83.80333951762523, r: 0.6270671683046785
06/02/2019 03:58:30 step: 4805, epoch: 145, batch: 19, loss: 0.20342138409614563, acc: 92.1875, f1: 89.83806566104701, r: 0.6146793664697735
06/02/2019 03:58:31 step: 4810, epoch: 145, batch: 24, loss: 0.3010384142398834, acc: 84.375, f1: 63.13568376068376, r: 0.665401315759101
06/02/2019 03:58:31 step: 4815, epoch: 145, batch: 29, loss: 0.18843728303909302, acc: 92.1875, f1: 80.9375, r: 0.7176821288654637
06/02/2019 03:58:31 *** evaluating ***
06/02/2019 03:58:31 step: 146, epoch: 145, acc: 59.401709401709404, f1: 24.44488248324565, r: 0.37243898815373566
06/02/2019 03:58:31 *** epoch: 147 ***
06/02/2019 03:58:31 *** training ***
06/02/2019 03:58:31 step: 4823, epoch: 146, batch: 4, loss: 0.14472615718841553, acc: 96.875, f1: 95.16945516945518, r: 0.6054791880997432
06/02/2019 03:58:32 step: 4828, epoch: 146, batch: 9, loss: 0.23229604959487915, acc: 87.5, f1: 76.51785714285715, r: 0.7183107539452687
06/02/2019 03:58:32 step: 4833, epoch: 146, batch: 14, loss: 0.2843627333641052, acc: 87.5, f1: 70.3602552131964, r: 0.6511965226587052
06/02/2019 03:58:32 step: 4838, epoch: 146, batch: 19, loss: 0.26172831654548645, acc: 89.0625, f1: 80.85838181426416, r: 0.6950078827438982
06/02/2019 03:58:33 step: 4843, epoch: 146, batch: 24, loss: 0.23189933598041534, acc: 87.5, f1: 82.26243310989075, r: 0.572229272258196
06/02/2019 03:58:33 step: 4848, epoch: 146, batch: 29, loss: 0.16202767193317413, acc: 92.1875, f1: 85.53685897435896, r: 0.677488552351743
06/02/2019 03:58:33 *** evaluating ***
06/02/2019 03:58:33 step: 147, epoch: 146, acc: 60.256410256410255, f1: 25.87598960179605, r: 0.3726937938671859
06/02/2019 03:58:33 *** epoch: 148 ***
06/02/2019 03:58:33 *** training ***
06/02/2019 03:58:34 step: 4856, epoch: 147, batch: 4, loss: 0.22950349748134613, acc: 92.1875, f1: 88.05163090877376, r: 0.6054894404608344
06/02/2019 03:58:34 step: 4861, epoch: 147, batch: 9, loss: 0.3455456495285034, acc: 84.375, f1: 56.16402116402116, r: 0.5854456824946367
06/02/2019 03:58:34 step: 4866, epoch: 147, batch: 14, loss: 0.10342235863208771, acc: 98.4375, f1: 98.53846153846155, r: 0.7457220456568294
06/02/2019 03:58:35 step: 4871, epoch: 147, batch: 19, loss: 0.23970620334148407, acc: 92.1875, f1: 85.4721888755502, r: 0.661251088361488
06/02/2019 03:58:35 step: 4876, epoch: 147, batch: 24, loss: 0.24059705436229706, acc: 85.9375, f1: 68.32837301587303, r: 0.6971168081571966
06/02/2019 03:58:35 step: 4881, epoch: 147, batch: 29, loss: 0.19628125429153442, acc: 90.625, f1: 85.21210887384862, r: 0.6552305992416032
06/02/2019 03:58:35 *** evaluating ***
06/02/2019 03:58:35 step: 148, epoch: 147, acc: 58.119658119658126, f1: 24.758311800429894, r: 0.372567070498414
06/02/2019 03:58:35 *** epoch: 149 ***
06/02/2019 03:58:35 *** training ***
06/02/2019 03:58:36 step: 4889, epoch: 148, batch: 4, loss: 0.2347707450389862, acc: 92.1875, f1: 90.78949432210302, r: 0.7163285119273185
06/02/2019 03:58:36 step: 4894, epoch: 148, batch: 9, loss: 0.2740766406059265, acc: 87.5, f1: 74.43721395881006, r: 0.6862352209310966
06/02/2019 03:58:36 step: 4899, epoch: 148, batch: 14, loss: 0.1632828712463379, acc: 92.1875, f1: 74.76967476967478, r: 0.6374681458674619
06/02/2019 03:58:37 step: 4904, epoch: 148, batch: 19, loss: 0.17428386211395264, acc: 90.625, f1: 84.5825505731166, r: 0.7157149127450391
06/02/2019 03:58:37 step: 4909, epoch: 148, batch: 24, loss: 0.15148383378982544, acc: 95.3125, f1: 79.7904335306351, r: 0.7777596703400342
06/02/2019 03:58:37 step: 4914, epoch: 148, batch: 29, loss: 0.2063513994216919, acc: 93.75, f1: 88.43049719887955, r: 0.6755089985115678
06/02/2019 03:58:37 *** evaluating ***
06/02/2019 03:58:37 step: 149, epoch: 148, acc: 59.82905982905983, f1: 24.780797171041076, r: 0.37324540856056754
06/02/2019 03:58:37 *** epoch: 150 ***
06/02/2019 03:58:37 *** training ***
06/02/2019 03:58:38 step: 4922, epoch: 149, batch: 4, loss: 0.2058611661195755, acc: 89.0625, f1: 77.64426961788834, r: 0.6127278894168922
06/02/2019 03:58:38 step: 4927, epoch: 149, batch: 9, loss: 0.23269568383693695, acc: 89.0625, f1: 69.5927750410509, r: 0.6648158422664282
06/02/2019 03:58:38 step: 4932, epoch: 149, batch: 14, loss: 0.20258091390132904, acc: 89.0625, f1: 79.64175654853621, r: 0.5665178607409015
06/02/2019 03:58:38 step: 4937, epoch: 149, batch: 19, loss: 0.26858463883399963, acc: 87.5, f1: 82.26617826617826, r: 0.6403976062511212
06/02/2019 03:58:39 step: 4942, epoch: 149, batch: 24, loss: 0.37873905897140503, acc: 81.25, f1: 65.42839105339107, r: 0.5798689708760765
06/02/2019 03:58:39 step: 4947, epoch: 149, batch: 29, loss: 0.22806578874588013, acc: 93.75, f1: 88.8983343700325, r: 0.6761195535474973
06/02/2019 03:58:39 *** evaluating ***
06/02/2019 03:58:39 step: 150, epoch: 149, acc: 58.97435897435898, f1: 24.8109243697479, r: 0.3718233212211574
06/02/2019 03:58:39 *** epoch: 151 ***
06/02/2019 03:58:39 *** training ***
06/02/2019 03:58:40 step: 4955, epoch: 150, batch: 4, loss: 0.30048227310180664, acc: 82.8125, f1: 76.06728588871445, r: 0.698997768553045
06/02/2019 03:58:40 step: 4960, epoch: 150, batch: 9, loss: 0.40074801445007324, acc: 81.25, f1: 65.53907203907204, r: 0.6688187646482431
06/02/2019 03:58:40 step: 4965, epoch: 150, batch: 14, loss: 0.24012047052383423, acc: 89.0625, f1: 67.41666666666666, r: 0.5552818791284526
06/02/2019 03:58:40 step: 4970, epoch: 150, batch: 19, loss: 0.2393472045660019, acc: 92.1875, f1: 75.73678861788618, r: 0.7286516161011855
06/02/2019 03:58:41 step: 4975, epoch: 150, batch: 24, loss: 0.1960199773311615, acc: 92.1875, f1: 85.39101110529683, r: 0.6090985901353441
06/02/2019 03:58:41 step: 4980, epoch: 150, batch: 29, loss: 0.3812613785266876, acc: 81.25, f1: 76.16973175079343, r: 0.5859921556239028
06/02/2019 03:58:41 *** evaluating ***
06/02/2019 03:58:41 step: 151, epoch: 150, acc: 59.401709401709404, f1: 25.207716816378646, r: 0.3734201730616628
06/02/2019 03:58:41 *** epoch: 152 ***
06/02/2019 03:58:41 *** training ***
06/02/2019 03:58:41 step: 4988, epoch: 151, batch: 4, loss: 0.20912513136863708, acc: 89.0625, f1: 71.98101826334586, r: 0.6580886736387237
06/02/2019 03:58:42 step: 4993, epoch: 151, batch: 9, loss: 0.26949331164360046, acc: 93.75, f1: 93.88744588744589, r: 0.604668099204659
06/02/2019 03:58:42 step: 4998, epoch: 151, batch: 14, loss: 0.24170070886611938, acc: 89.0625, f1: 71.43568840579711, r: 0.6408584471374489
06/02/2019 03:58:42 step: 5003, epoch: 151, batch: 19, loss: 0.29205676913261414, acc: 89.0625, f1: 71.66851609440984, r: 0.6327477025796024
06/02/2019 03:58:42 step: 5008, epoch: 151, batch: 24, loss: 0.22440940141677856, acc: 90.625, f1: 83.29231797212093, r: 0.6439706573979146
06/02/2019 03:58:43 step: 5013, epoch: 151, batch: 29, loss: 0.31669044494628906, acc: 84.375, f1: 72.72274091822963, r: 0.5663867386262954
06/02/2019 03:58:43 *** evaluating ***
06/02/2019 03:58:43 step: 152, epoch: 151, acc: 58.97435897435898, f1: 24.382249323809198, r: 0.3657319106269953
06/02/2019 03:58:43 *** epoch: 153 ***
06/02/2019 03:58:43 *** training ***
06/02/2019 03:58:43 step: 5021, epoch: 152, batch: 4, loss: 0.22000855207443237, acc: 90.625, f1: 68.3838383838384, r: 0.6708828401587088
06/02/2019 03:58:44 step: 5026, epoch: 152, batch: 9, loss: 0.2462824583053589, acc: 90.625, f1: 84.37263794406651, r: 0.6729123217836094
06/02/2019 03:58:44 step: 5031, epoch: 152, batch: 14, loss: 0.24476096034049988, acc: 92.1875, f1: 86.27152673128562, r: 0.7434338846796404
06/02/2019 03:58:44 step: 5036, epoch: 152, batch: 19, loss: 0.17316487431526184, acc: 90.625, f1: 84.15804002760525, r: 0.6191937393239075
06/02/2019 03:58:44 step: 5041, epoch: 152, batch: 24, loss: 0.3346083462238312, acc: 85.9375, f1: 60.378953816453816, r: 0.511926266706259
06/02/2019 03:58:45 step: 5046, epoch: 152, batch: 29, loss: 0.14892011880874634, acc: 93.75, f1: 78.86281785441449, r: 0.5674547859665732
06/02/2019 03:58:45 *** evaluating ***
06/02/2019 03:58:45 step: 153, epoch: 152, acc: 59.401709401709404, f1: 24.574939487351458, r: 0.36359550430650084
06/02/2019 03:58:45 *** epoch: 154 ***
06/02/2019 03:58:45 *** training ***
06/02/2019 03:58:45 step: 5054, epoch: 153, batch: 4, loss: 0.25318533182144165, acc: 85.9375, f1: 61.21486912968723, r: 0.5802739665703478
06/02/2019 03:58:46 step: 5059, epoch: 153, batch: 9, loss: 0.21799138188362122, acc: 93.75, f1: 74.90726817042606, r: 0.642613943102377
06/02/2019 03:58:46 step: 5064, epoch: 153, batch: 14, loss: 0.09581444412469864, acc: 95.3125, f1: 91.50213827512474, r: 0.5267403119171166
06/02/2019 03:58:46 step: 5069, epoch: 153, batch: 19, loss: 0.2976255416870117, acc: 85.9375, f1: 79.64285714285715, r: 0.75702654255163
06/02/2019 03:58:46 step: 5074, epoch: 153, batch: 24, loss: 0.1981092393398285, acc: 90.625, f1: 70.8058608058608, r: 0.6281253106782217
06/02/2019 03:58:47 step: 5079, epoch: 153, batch: 29, loss: 0.26513218879699707, acc: 84.375, f1: 78.0164835164835, r: 0.5755454546105406
06/02/2019 03:58:47 *** evaluating ***
06/02/2019 03:58:47 step: 154, epoch: 153, acc: 59.401709401709404, f1: 24.391946289005112, r: 0.37427775993559076
06/02/2019 03:58:47 *** epoch: 155 ***
06/02/2019 03:58:47 *** training ***
06/02/2019 03:58:47 step: 5087, epoch: 154, batch: 4, loss: 0.1910674273967743, acc: 92.1875, f1: 86.24604990737713, r: 0.6579432150217236
06/02/2019 03:58:47 step: 5092, epoch: 154, batch: 9, loss: 0.19786179065704346, acc: 90.625, f1: 74.63943463943464, r: 0.5961533714537023
06/02/2019 03:58:48 step: 5097, epoch: 154, batch: 14, loss: 0.22439131140708923, acc: 89.0625, f1: 71.70680679219578, r: 0.6563714431367542
06/02/2019 03:58:48 step: 5102, epoch: 154, batch: 19, loss: 0.1980271190404892, acc: 92.1875, f1: 65.12545669460563, r: 0.7020951235266527
06/02/2019 03:58:48 step: 5107, epoch: 154, batch: 24, loss: 0.25893568992614746, acc: 89.0625, f1: 78.43502751977984, r: 0.6389808428474139
06/02/2019 03:58:49 step: 5112, epoch: 154, batch: 29, loss: 0.13168619573116302, acc: 95.3125, f1: 93.78233555062823, r: 0.7529685373834746
06/02/2019 03:58:49 *** evaluating ***
06/02/2019 03:58:49 step: 155, epoch: 154, acc: 58.97435897435898, f1: 25.805588351042896, r: 0.3730583185300979
06/02/2019 03:58:49 *** epoch: 156 ***
06/02/2019 03:58:49 *** training ***
06/02/2019 03:58:49 step: 5120, epoch: 155, batch: 4, loss: 0.2489321529865265, acc: 92.1875, f1: 93.44897379241552, r: 0.6259425883402862
06/02/2019 03:58:49 step: 5125, epoch: 155, batch: 9, loss: 0.21491849422454834, acc: 89.0625, f1: 74.22321428571428, r: 0.616185675468556
06/02/2019 03:58:50 step: 5130, epoch: 155, batch: 14, loss: 0.1452237069606781, acc: 95.3125, f1: 95.96314796314796, r: 0.7547590323463044
06/02/2019 03:58:50 step: 5135, epoch: 155, batch: 19, loss: 0.19787441194057465, acc: 92.1875, f1: 77.88586413586414, r: 0.6987805303587757
06/02/2019 03:58:50 step: 5140, epoch: 155, batch: 24, loss: 0.2533441185951233, acc: 89.0625, f1: 75.92015865907491, r: 0.6080984529876637
06/02/2019 03:58:51 step: 5145, epoch: 155, batch: 29, loss: 0.7606321573257446, acc: 82.8125, f1: 75.46024454269136, r: 0.7129421876388808
06/02/2019 03:58:51 *** evaluating ***
06/02/2019 03:58:51 step: 156, epoch: 155, acc: 59.401709401709404, f1: 24.722585855180043, r: 0.3633938602375361
06/02/2019 03:58:51 *** epoch: 157 ***
06/02/2019 03:58:51 *** training ***
06/02/2019 03:58:51 step: 5153, epoch: 156, batch: 4, loss: 0.23172378540039062, acc: 90.625, f1: 87.51816239316238, r: 0.6143790738584883
06/02/2019 03:58:51 step: 5158, epoch: 156, batch: 9, loss: 0.20774570107460022, acc: 92.1875, f1: 80.57076006228549, r: 0.6670476819443472
06/02/2019 03:58:52 step: 5163, epoch: 156, batch: 14, loss: 0.26027795672416687, acc: 89.0625, f1: 74.91797713780473, r: 0.6408716495720526
06/02/2019 03:58:52 step: 5168, epoch: 156, batch: 19, loss: 0.27524131536483765, acc: 87.5, f1: 79.99472252669186, r: 0.5770004214226606
06/02/2019 03:58:52 step: 5173, epoch: 156, batch: 24, loss: 0.20834459364414215, acc: 92.1875, f1: 89.92930505535547, r: 0.5858999366385542
06/02/2019 03:58:53 step: 5178, epoch: 156, batch: 29, loss: 0.28897637128829956, acc: 87.5, f1: 58.947179869311014, r: 0.6565441262208758
06/02/2019 03:58:53 *** evaluating ***
06/02/2019 03:58:53 step: 157, epoch: 156, acc: 58.97435897435898, f1: 24.629468682100264, r: 0.3662040780698466
06/02/2019 03:58:53 *** epoch: 158 ***
06/02/2019 03:58:53 *** training ***
06/02/2019 03:58:53 step: 5186, epoch: 157, batch: 4, loss: 0.2547873854637146, acc: 89.0625, f1: 73.6156462585034, r: 0.7148400154853497
06/02/2019 03:58:53 step: 5191, epoch: 157, batch: 9, loss: 0.20893841981887817, acc: 92.1875, f1: 91.52059232180936, r: 0.7119984045569498
06/02/2019 03:58:54 step: 5196, epoch: 157, batch: 14, loss: 0.2620057761669159, acc: 87.5, f1: 72.01244918699187, r: 0.7091824521412496
06/02/2019 03:58:54 step: 5201, epoch: 157, batch: 19, loss: 0.27908390760421753, acc: 85.9375, f1: 81.16374518013862, r: 0.6672551647638663
06/02/2019 03:58:54 step: 5206, epoch: 157, batch: 24, loss: 0.22309011220932007, acc: 90.625, f1: 86.36363636363636, r: 0.7621021278553618
06/02/2019 03:58:55 step: 5211, epoch: 157, batch: 29, loss: 0.1832057535648346, acc: 92.1875, f1: 87.7042903904606, r: 0.6175224777568007
06/02/2019 03:58:55 *** evaluating ***
06/02/2019 03:58:55 step: 158, epoch: 157, acc: 58.54700854700855, f1: 24.861671449198468, r: 0.36030599967851723
06/02/2019 03:58:55 *** epoch: 159 ***
06/02/2019 03:58:55 *** training ***
06/02/2019 03:58:55 step: 5219, epoch: 158, batch: 4, loss: 0.30355367064476013, acc: 89.0625, f1: 73.39881528046422, r: 0.7050728935531446
06/02/2019 03:58:56 step: 5224, epoch: 158, batch: 9, loss: 0.22261512279510498, acc: 89.0625, f1: 72.83145095645095, r: 0.5928704560851367
06/02/2019 03:58:56 step: 5229, epoch: 158, batch: 14, loss: 0.256917268037796, acc: 87.5, f1: 83.41096593376228, r: 0.7442493808326733
06/02/2019 03:58:56 step: 5234, epoch: 158, batch: 19, loss: 0.20640377700328827, acc: 89.0625, f1: 59.50799136859045, r: 0.5607334652969952
06/02/2019 03:58:56 step: 5239, epoch: 158, batch: 24, loss: 0.18151938915252686, acc: 92.1875, f1: 87.2233951051685, r: 0.6394175102391182
06/02/2019 03:58:57 step: 5244, epoch: 158, batch: 29, loss: 0.2282407283782959, acc: 90.625, f1: 78.39052287581698, r: 0.695482586755344
06/02/2019 03:58:57 *** evaluating ***
06/02/2019 03:58:57 step: 159, epoch: 158, acc: 58.97435897435898, f1: 25.133752202414172, r: 0.3637025078883591
06/02/2019 03:58:57 *** epoch: 160 ***
06/02/2019 03:58:57 *** training ***
06/02/2019 03:58:57 step: 5252, epoch: 159, batch: 4, loss: 0.09182456135749817, acc: 98.4375, f1: 99.09700722394221, r: 0.7183652531878388
06/02/2019 03:58:58 step: 5257, epoch: 159, batch: 9, loss: 0.24202600121498108, acc: 90.625, f1: 79.3760521885522, r: 0.6990772005096553
06/02/2019 03:58:58 step: 5262, epoch: 159, batch: 14, loss: 0.24686658382415771, acc: 92.1875, f1: 76.28745158350421, r: 0.7177902000454415
06/02/2019 03:58:58 step: 5267, epoch: 159, batch: 19, loss: 0.1779787540435791, acc: 89.0625, f1: 75.39594061333193, r: 0.6019150769749486
06/02/2019 03:58:58 step: 5272, epoch: 159, batch: 24, loss: 0.28604328632354736, acc: 87.5, f1: 80.56232492997198, r: 0.7304455779309657
06/02/2019 03:58:59 step: 5277, epoch: 159, batch: 29, loss: 0.21200287342071533, acc: 92.1875, f1: 72.03198877068557, r: 0.6789734315772147
06/02/2019 03:58:59 *** evaluating ***
06/02/2019 03:58:59 step: 160, epoch: 159, acc: 58.97435897435898, f1: 24.771686614781917, r: 0.36015708392692314
06/02/2019 03:58:59 *** epoch: 161 ***
06/02/2019 03:58:59 *** training ***
06/02/2019 03:58:59 step: 5285, epoch: 160, batch: 4, loss: 0.2538348436355591, acc: 90.625, f1: 83.46944750671457, r: 0.6362742717588197
06/02/2019 03:59:00 step: 5290, epoch: 160, batch: 9, loss: 0.21257692575454712, acc: 90.625, f1: 76.64334282099937, r: 0.7207780154568256
06/02/2019 03:59:00 step: 5295, epoch: 160, batch: 14, loss: 0.23061269521713257, acc: 89.0625, f1: 80.51438119107293, r: 0.6407026540802413
06/02/2019 03:59:00 step: 5300, epoch: 160, batch: 19, loss: 0.15064314007759094, acc: 90.625, f1: 88.51091765377478, r: 0.6323140981276968
06/02/2019 03:59:00 step: 5305, epoch: 160, batch: 24, loss: 0.3147367835044861, acc: 85.9375, f1: 73.27380952380952, r: 0.7370255517973338
06/02/2019 03:59:01 step: 5310, epoch: 160, batch: 29, loss: 0.19703692197799683, acc: 93.75, f1: 86.94991789819375, r: 0.7253580392121256
06/02/2019 03:59:01 *** evaluating ***
06/02/2019 03:59:01 step: 161, epoch: 160, acc: 59.82905982905983, f1: 24.599362118661194, r: 0.3627501933807334
06/02/2019 03:59:01 *** epoch: 162 ***
06/02/2019 03:59:01 *** training ***
06/02/2019 03:59:01 step: 5318, epoch: 161, batch: 4, loss: 0.14949250221252441, acc: 95.3125, f1: 91.19571813251413, r: 0.7273987983319159
06/02/2019 03:59:01 step: 5323, epoch: 161, batch: 9, loss: 0.20848853886127472, acc: 89.0625, f1: 82.31486167341431, r: 0.7433987556893941
06/02/2019 03:59:02 step: 5328, epoch: 161, batch: 14, loss: 0.28206774592399597, acc: 89.0625, f1: 80.62581459758879, r: 0.7307296260461837
06/02/2019 03:59:02 step: 5333, epoch: 161, batch: 19, loss: 0.28905192017555237, acc: 84.375, f1: 71.76339285714286, r: 0.6410463129561214
06/02/2019 03:59:02 step: 5338, epoch: 161, batch: 24, loss: 0.13992972671985626, acc: 96.875, f1: 81.75824175824175, r: 0.6112789549630155
06/02/2019 03:59:03 step: 5343, epoch: 161, batch: 29, loss: 0.23791128396987915, acc: 89.0625, f1: 74.80798771121351, r: 0.5779993553000295
06/02/2019 03:59:03 *** evaluating ***
06/02/2019 03:59:03 step: 162, epoch: 161, acc: 58.54700854700855, f1: 24.25035723737439, r: 0.37080927214068315
06/02/2019 03:59:03 *** epoch: 163 ***
06/02/2019 03:59:03 *** training ***
06/02/2019 03:59:03 step: 5351, epoch: 162, batch: 4, loss: 0.2908599376678467, acc: 90.625, f1: 81.88031078523439, r: 0.6746954467426305
06/02/2019 03:59:03 step: 5356, epoch: 162, batch: 9, loss: 0.12262895703315735, acc: 95.3125, f1: 82.6984126984127, r: 0.6848499480952079
06/02/2019 03:59:04 step: 5361, epoch: 162, batch: 14, loss: 0.14288689196109772, acc: 96.875, f1: 98.26366030283081, r: 0.7352957460640966
06/02/2019 03:59:04 step: 5366, epoch: 162, batch: 19, loss: 0.2286977618932724, acc: 89.0625, f1: 70.3384238310709, r: 0.6527934700022467
06/02/2019 03:59:04 step: 5371, epoch: 162, batch: 24, loss: 0.04236321151256561, acc: 100.0, f1: 100.0, r: 0.6044339015004291
06/02/2019 03:59:04 step: 5376, epoch: 162, batch: 29, loss: 0.21649420261383057, acc: 89.0625, f1: 72.14814814814814, r: 0.6797684350509463
06/02/2019 03:59:05 *** evaluating ***
06/02/2019 03:59:05 step: 163, epoch: 162, acc: 59.82905982905983, f1: 25.4984900003259, r: 0.3726215388593946
06/02/2019 03:59:05 *** epoch: 164 ***
06/02/2019 03:59:05 *** training ***
06/02/2019 03:59:05 step: 5384, epoch: 163, batch: 4, loss: 0.27686652541160583, acc: 87.5, f1: 73.59343725271145, r: 0.6136279700381974
06/02/2019 03:59:05 step: 5389, epoch: 163, batch: 9, loss: 0.2180870622396469, acc: 92.1875, f1: 77.76041666666667, r: 0.6229619696145037
06/02/2019 03:59:06 step: 5394, epoch: 163, batch: 14, loss: 0.21525810658931732, acc: 90.625, f1: 84.8878952723241, r: 0.609836557027558
06/02/2019 03:59:06 step: 5399, epoch: 163, batch: 19, loss: 0.23811763525009155, acc: 87.5, f1: 73.29412546517811, r: 0.6415871883107652
06/02/2019 03:59:06 step: 5404, epoch: 163, batch: 24, loss: 0.23636078834533691, acc: 93.75, f1: 88.10083594566353, r: 0.6846051893643639
06/02/2019 03:59:06 step: 5409, epoch: 163, batch: 29, loss: 0.2269468605518341, acc: 92.1875, f1: 75.57142857142858, r: 0.6894981265981338
06/02/2019 03:59:07 *** evaluating ***
06/02/2019 03:59:07 step: 164, epoch: 163, acc: 59.401709401709404, f1: 24.397525199361336, r: 0.373561178248218
06/02/2019 03:59:07 *** epoch: 165 ***
06/02/2019 03:59:07 *** training ***
06/02/2019 03:59:07 step: 5417, epoch: 164, batch: 4, loss: 0.16699153184890747, acc: 95.3125, f1: 80.98086124401914, r: 0.6630357029860764
06/02/2019 03:59:07 step: 5422, epoch: 164, batch: 9, loss: 0.23006975650787354, acc: 89.0625, f1: 66.2380831643002, r: 0.6889187766105124
06/02/2019 03:59:07 step: 5427, epoch: 164, batch: 14, loss: 0.19966500997543335, acc: 93.75, f1: 95.22792022792022, r: 0.603264048459205
06/02/2019 03:59:08 step: 5432, epoch: 164, batch: 19, loss: 0.1402769386768341, acc: 93.75, f1: 94.68542590499376, r: 0.69374057397665
06/02/2019 03:59:08 step: 5437, epoch: 164, batch: 24, loss: 0.1413394808769226, acc: 95.3125, f1: 96.02610874788293, r: 0.7629190450027793
06/02/2019 03:59:08 step: 5442, epoch: 164, batch: 29, loss: 0.2521713674068451, acc: 92.1875, f1: 89.24189814814815, r: 0.7131422448955996
06/02/2019 03:59:08 *** evaluating ***
06/02/2019 03:59:09 step: 165, epoch: 164, acc: 59.401709401709404, f1: 24.926764593431262, r: 0.3679632410108865
06/02/2019 03:59:09 *** epoch: 166 ***
06/02/2019 03:59:09 *** training ***
06/02/2019 03:59:09 step: 5450, epoch: 165, batch: 4, loss: 0.29408079385757446, acc: 81.25, f1: 71.51515151515152, r: 0.5761035107177138
06/02/2019 03:59:09 step: 5455, epoch: 165, batch: 9, loss: 0.18448802828788757, acc: 93.75, f1: 91.22083441755572, r: 0.6305783389579481
06/02/2019 03:59:09 step: 5460, epoch: 165, batch: 14, loss: 0.17764335870742798, acc: 93.75, f1: 88.63082885612677, r: 0.7487022404394785
06/02/2019 03:59:10 step: 5465, epoch: 165, batch: 19, loss: 0.2635513246059418, acc: 87.5, f1: 82.21754973749654, r: 0.6588675629675986
06/02/2019 03:59:10 step: 5470, epoch: 165, batch: 24, loss: 0.15366685390472412, acc: 95.3125, f1: 93.7230276726075, r: 0.6549899043442122
06/02/2019 03:59:10 step: 5475, epoch: 165, batch: 29, loss: 0.3019644021987915, acc: 89.0625, f1: 81.91427853192559, r: 0.7515151454400684
06/02/2019 03:59:10 *** evaluating ***
06/02/2019 03:59:10 step: 166, epoch: 165, acc: 59.401709401709404, f1: 24.482133879192702, r: 0.35507959667059874
06/02/2019 03:59:10 *** epoch: 167 ***
06/02/2019 03:59:10 *** training ***
06/02/2019 03:59:11 step: 5483, epoch: 166, batch: 4, loss: 0.21243996918201447, acc: 92.1875, f1: 74.19696969696969, r: 0.6552753283866548
06/02/2019 03:59:11 step: 5488, epoch: 166, batch: 9, loss: 0.23960819840431213, acc: 90.625, f1: 92.02821049368364, r: 0.7196888769713604
06/02/2019 03:59:11 step: 5493, epoch: 166, batch: 14, loss: 0.18497753143310547, acc: 92.1875, f1: 88.53298895315702, r: 0.5928632667733365
06/02/2019 03:59:12 step: 5498, epoch: 166, batch: 19, loss: 0.16505984961986542, acc: 93.75, f1: 79.47480307912639, r: 0.6774537389865282
06/02/2019 03:59:12 step: 5503, epoch: 166, batch: 24, loss: 0.21975108981132507, acc: 89.0625, f1: 87.49178140155584, r: 0.6042548429411122
06/02/2019 03:59:12 step: 5508, epoch: 166, batch: 29, loss: 0.11708629131317139, acc: 93.75, f1: 85.64863187814007, r: 0.533067195792759
06/02/2019 03:59:12 *** evaluating ***
06/02/2019 03:59:12 step: 167, epoch: 166, acc: 58.54700854700855, f1: 24.49032184860485, r: 0.36567686609880734
06/02/2019 03:59:12 *** epoch: 168 ***
06/02/2019 03:59:12 *** training ***
06/02/2019 03:59:13 step: 5516, epoch: 167, batch: 4, loss: 0.25239914655685425, acc: 89.0625, f1: 82.8943623426382, r: 0.7393592663320914
06/02/2019 03:59:13 step: 5521, epoch: 167, batch: 9, loss: 0.21021191775798798, acc: 90.625, f1: 82.76742515187895, r: 0.6022639638570642
06/02/2019 03:59:13 step: 5526, epoch: 167, batch: 14, loss: 0.14656120538711548, acc: 90.625, f1: 76.22957228868559, r: 0.6371663930935817
06/02/2019 03:59:13 step: 5531, epoch: 167, batch: 19, loss: 0.2153048813343048, acc: 89.0625, f1: 64.52457264957265, r: 0.5903487036824913
06/02/2019 03:59:14 step: 5536, epoch: 167, batch: 24, loss: 0.10161355882883072, acc: 95.3125, f1: 95.22727272727273, r: 0.7925716503751583
06/02/2019 03:59:14 step: 5541, epoch: 167, batch: 29, loss: 0.21660193800926208, acc: 90.625, f1: 85.50264550264552, r: 0.626606574986375
06/02/2019 03:59:14 *** evaluating ***
06/02/2019 03:59:14 step: 168, epoch: 167, acc: 59.82905982905983, f1: 24.4356924170357, r: 0.35037146148308185
06/02/2019 03:59:14 *** epoch: 169 ***
06/02/2019 03:59:14 *** training ***
06/02/2019 03:59:15 step: 5549, epoch: 168, batch: 4, loss: 0.1681356579065323, acc: 92.1875, f1: 86.47834864940128, r: 0.7688604027785559
06/02/2019 03:59:15 step: 5554, epoch: 168, batch: 9, loss: 0.2276548147201538, acc: 87.5, f1: 69.89072521681219, r: 0.5970453060799691
06/02/2019 03:59:15 step: 5559, epoch: 168, batch: 14, loss: 0.1319095641374588, acc: 95.3125, f1: 91.65013227513228, r: 0.6916923995622914
06/02/2019 03:59:15 step: 5564, epoch: 168, batch: 19, loss: 0.19851408898830414, acc: 90.625, f1: 84.46353597669388, r: 0.7439504497093562
06/02/2019 03:59:16 step: 5569, epoch: 168, batch: 24, loss: 0.12399580329656601, acc: 96.875, f1: 96.96145124716553, r: 0.685781703199643
06/02/2019 03:59:16 step: 5574, epoch: 168, batch: 29, loss: 0.22084231674671173, acc: 87.5, f1: 71.72375541125542, r: 0.5794852071420941
06/02/2019 03:59:16 *** evaluating ***
06/02/2019 03:59:16 step: 169, epoch: 168, acc: 60.256410256410255, f1: 24.52822942715625, r: 0.3613544865190577
06/02/2019 03:59:16 *** epoch: 170 ***
06/02/2019 03:59:16 *** training ***
06/02/2019 03:59:16 step: 5582, epoch: 169, batch: 4, loss: 0.17523828148841858, acc: 90.625, f1: 60.877192982456144, r: 0.5360072669916269
06/02/2019 03:59:17 step: 5587, epoch: 169, batch: 9, loss: 0.22821973264217377, acc: 87.5, f1: 73.52941176470588, r: 0.7130696345134373
06/02/2019 03:59:17 step: 5592, epoch: 169, batch: 14, loss: 0.1810227930545807, acc: 92.1875, f1: 76.42621870882739, r: 0.6850096405225333
06/02/2019 03:59:17 step: 5597, epoch: 169, batch: 19, loss: 0.18870137631893158, acc: 89.0625, f1: 52.35027580772261, r: 0.6902760491405873
06/02/2019 03:59:18 step: 5602, epoch: 169, batch: 24, loss: 0.12700682878494263, acc: 93.75, f1: 92.7010743321719, r: 0.7307665765784763
06/02/2019 03:59:18 step: 5607, epoch: 169, batch: 29, loss: 0.14757560193538666, acc: 93.75, f1: 89.70845481049562, r: 0.6930057238585327
06/02/2019 03:59:18 *** evaluating ***
06/02/2019 03:59:18 step: 170, epoch: 169, acc: 56.837606837606835, f1: 24.433823118500154, r: 0.38496661807860605
06/02/2019 03:59:18 *** epoch: 171 ***
06/02/2019 03:59:18 *** training ***
06/02/2019 03:59:18 step: 5615, epoch: 170, batch: 4, loss: 0.2118155062198639, acc: 90.625, f1: 77.93786605551311, r: 0.6993664610429692
06/02/2019 03:59:19 step: 5620, epoch: 170, batch: 9, loss: 0.09187920391559601, acc: 98.4375, f1: 94.48621553884713, r: 0.6703220245010666
06/02/2019 03:59:19 step: 5625, epoch: 170, batch: 14, loss: 0.33807018399238586, acc: 87.5, f1: 74.47257483895416, r: 0.6954192327685268
06/02/2019 03:59:19 step: 5630, epoch: 170, batch: 19, loss: 0.1421462744474411, acc: 93.75, f1: 95.20909645909646, r: 0.7752349278282316
06/02/2019 03:59:20 step: 5635, epoch: 170, batch: 24, loss: 0.16902564465999603, acc: 92.1875, f1: 91.17874084979347, r: 0.7132383664787834
06/02/2019 03:59:20 step: 5640, epoch: 170, batch: 29, loss: 0.23489797115325928, acc: 89.0625, f1: 74.5752086078173, r: 0.7109668230474415
06/02/2019 03:59:20 *** evaluating ***
06/02/2019 03:59:20 step: 171, epoch: 170, acc: 60.68376068376068, f1: 26.701734998773325, r: 0.37511597688294185
06/02/2019 03:59:20 *** epoch: 172 ***
06/02/2019 03:59:20 *** training ***
06/02/2019 03:59:20 step: 5648, epoch: 171, batch: 4, loss: 0.20264261960983276, acc: 89.0625, f1: 83.5529038573535, r: 0.6035396791372705
06/02/2019 03:59:21 step: 5653, epoch: 171, batch: 9, loss: 0.2035416215658188, acc: 93.75, f1: 78.97700746965452, r: 0.6508615952675367
06/02/2019 03:59:21 step: 5658, epoch: 171, batch: 14, loss: 0.11879733204841614, acc: 96.875, f1: 93.60119047619047, r: 0.7832721322159296
06/02/2019 03:59:21 step: 5663, epoch: 171, batch: 19, loss: 0.23308366537094116, acc: 84.375, f1: 72.01344943137312, r: 0.6258060290732136
06/02/2019 03:59:22 step: 5668, epoch: 171, batch: 24, loss: 0.24040451645851135, acc: 87.5, f1: 73.8031462585034, r: 0.657034366611624
06/02/2019 03:59:22 step: 5673, epoch: 171, batch: 29, loss: 0.17071086168289185, acc: 92.1875, f1: 78.20586820586823, r: 0.694281457869449
06/02/2019 03:59:22 *** evaluating ***
06/02/2019 03:59:22 step: 172, epoch: 171, acc: 59.401709401709404, f1: 24.686630829487978, r: 0.3615027588895981
06/02/2019 03:59:22 *** epoch: 173 ***
06/02/2019 03:59:22 *** training ***
06/02/2019 03:59:22 step: 5681, epoch: 172, batch: 4, loss: 0.2303234338760376, acc: 90.625, f1: 84.18010752688173, r: 0.6689319479881074
06/02/2019 03:59:23 step: 5686, epoch: 172, batch: 9, loss: 0.16779427230358124, acc: 95.3125, f1: 84.00764921353156, r: 0.7384459846539242
06/02/2019 03:59:23 step: 5691, epoch: 172, batch: 14, loss: 0.1761968433856964, acc: 90.625, f1: 89.11322541837248, r: 0.7804775505967014
06/02/2019 03:59:23 step: 5696, epoch: 172, batch: 19, loss: 0.2728997468948364, acc: 89.0625, f1: 80.90024468285337, r: 0.6754007174579656
06/02/2019 03:59:24 step: 5701, epoch: 172, batch: 24, loss: 0.19308817386627197, acc: 93.75, f1: 89.81047857739586, r: 0.6478221896776878
06/02/2019 03:59:24 step: 5706, epoch: 172, batch: 29, loss: 0.3901703357696533, acc: 79.6875, f1: 71.9623015873016, r: 0.6771601397519768
06/02/2019 03:59:24 *** evaluating ***
06/02/2019 03:59:24 step: 173, epoch: 172, acc: 59.401709401709404, f1: 25.06827363812658, r: 0.37293370321084507
06/02/2019 03:59:24 *** epoch: 174 ***
06/02/2019 03:59:24 *** training ***
06/02/2019 03:59:24 step: 5714, epoch: 173, batch: 4, loss: 0.26483234763145447, acc: 87.5, f1: 76.80581323438467, r: 0.5950829040009882
06/02/2019 03:59:25 step: 5719, epoch: 173, batch: 9, loss: 0.23804554343223572, acc: 90.625, f1: 86.50925925925925, r: 0.713875223581013
06/02/2019 03:59:25 step: 5724, epoch: 173, batch: 14, loss: 0.25571101903915405, acc: 92.1875, f1: 88.28948245965054, r: 0.6381453908574888
06/02/2019 03:59:25 step: 5729, epoch: 173, batch: 19, loss: 0.21940886974334717, acc: 92.1875, f1: 92.82967032967032, r: 0.6075387123822912
06/02/2019 03:59:25 step: 5734, epoch: 173, batch: 24, loss: 0.2693650424480438, acc: 89.0625, f1: 71.99135423400129, r: 0.7172076461304505
06/02/2019 03:59:26 step: 5739, epoch: 173, batch: 29, loss: 0.15842479467391968, acc: 95.3125, f1: 94.16502084704848, r: 0.6607044267179664
06/02/2019 03:59:26 *** evaluating ***
06/02/2019 03:59:26 step: 174, epoch: 173, acc: 59.82905982905983, f1: 25.403360492200775, r: 0.36225709501933767
06/02/2019 03:59:26 *** epoch: 175 ***
06/02/2019 03:59:26 *** training ***
06/02/2019 03:59:26 step: 5747, epoch: 174, batch: 4, loss: 0.11966270208358765, acc: 96.875, f1: 98.60544217687075, r: 0.6735515416901448
06/02/2019 03:59:27 step: 5752, epoch: 174, batch: 9, loss: 0.15663792192935944, acc: 93.75, f1: 94.62952506430769, r: 0.694108749311502
06/02/2019 03:59:27 step: 5757, epoch: 174, batch: 14, loss: 0.24032358825206757, acc: 89.0625, f1: 73.56442577030813, r: 0.6292656079401514
06/02/2019 03:59:27 step: 5762, epoch: 174, batch: 19, loss: 0.2680759131908417, acc: 92.1875, f1: 89.9461598450569, r: 0.7220823123084454
06/02/2019 03:59:27 step: 5767, epoch: 174, batch: 24, loss: 0.15632197260856628, acc: 95.3125, f1: 90.92627748773927, r: 0.6381812348473693
06/02/2019 03:59:28 step: 5772, epoch: 174, batch: 29, loss: 0.21081097424030304, acc: 92.1875, f1: 83.91559923858061, r: 0.6335879215573051
06/02/2019 03:59:28 *** evaluating ***
06/02/2019 03:59:28 step: 175, epoch: 174, acc: 58.54700854700855, f1: 24.567413903151973, r: 0.36361560907222235
06/02/2019 03:59:28 *** epoch: 176 ***
06/02/2019 03:59:28 *** training ***
06/02/2019 03:59:28 step: 5780, epoch: 175, batch: 4, loss: 0.1240726113319397, acc: 95.3125, f1: 73.27380952380953, r: 0.639415087487381
06/02/2019 03:59:29 step: 5785, epoch: 175, batch: 9, loss: 0.13919222354888916, acc: 96.875, f1: 97.28325123152709, r: 0.7259541453203409
06/02/2019 03:59:29 step: 5790, epoch: 175, batch: 14, loss: 0.2045697569847107, acc: 90.625, f1: 60.65828351542637, r: 0.5788940874243327
06/02/2019 03:59:29 step: 5795, epoch: 175, batch: 19, loss: 0.1843673437833786, acc: 92.1875, f1: 78.87391434561248, r: 0.6784680760005725
06/02/2019 03:59:29 step: 5800, epoch: 175, batch: 24, loss: 0.24295279383659363, acc: 89.0625, f1: 75.8742026023132, r: 0.6654382681873748
06/02/2019 03:59:30 step: 5805, epoch: 175, batch: 29, loss: 0.15976779162883759, acc: 93.75, f1: 95.41005291005291, r: 0.7383891405088282
06/02/2019 03:59:30 *** evaluating ***
06/02/2019 03:59:30 step: 176, epoch: 175, acc: 58.97435897435898, f1: 24.269734626523874, r: 0.3696773844511299
06/02/2019 03:59:30 *** epoch: 177 ***
06/02/2019 03:59:30 *** training ***
06/02/2019 03:59:30 step: 5813, epoch: 176, batch: 4, loss: 0.24895837903022766, acc: 89.0625, f1: 58.920497491926064, r: 0.4807214811181743
06/02/2019 03:59:31 step: 5818, epoch: 176, batch: 9, loss: 0.25407132506370544, acc: 87.5, f1: 71.37486727109368, r: 0.6859717053776746
06/02/2019 03:59:31 step: 5823, epoch: 176, batch: 14, loss: 0.29901695251464844, acc: 84.375, f1: 68.01587301587301, r: 0.7221130961787743
06/02/2019 03:59:31 step: 5828, epoch: 176, batch: 19, loss: 0.14308574795722961, acc: 95.3125, f1: 96.01056086770372, r: 0.709553139628608
06/02/2019 03:59:31 step: 5833, epoch: 176, batch: 24, loss: 0.12681524455547333, acc: 96.875, f1: 82.6984126984127, r: 0.6827930610711284
06/02/2019 03:59:32 step: 5838, epoch: 176, batch: 29, loss: 0.1901436597108841, acc: 92.1875, f1: 92.74482727189864, r: 0.6584533928902601
06/02/2019 03:59:32 *** evaluating ***
06/02/2019 03:59:32 step: 177, epoch: 176, acc: 58.119658119658126, f1: 24.936602820404016, r: 0.36915563883397884
06/02/2019 03:59:32 *** epoch: 178 ***
06/02/2019 03:59:32 *** training ***
06/02/2019 03:59:32 step: 5846, epoch: 177, batch: 4, loss: 0.17587406933307648, acc: 90.625, f1: 79.22514619883042, r: 0.7411917163495453
06/02/2019 03:59:33 step: 5851, epoch: 177, batch: 9, loss: 0.12087280303239822, acc: 96.875, f1: 85.9857978279031, r: 0.7282046080053786
06/02/2019 03:59:33 step: 5856, epoch: 177, batch: 14, loss: 0.19232381880283356, acc: 93.75, f1: 86.99792960662525, r: 0.614688312143704
06/02/2019 03:59:33 step: 5861, epoch: 177, batch: 19, loss: 0.15417440235614777, acc: 93.75, f1: 88.29158040027606, r: 0.6252124396323725
06/02/2019 03:59:33 step: 5866, epoch: 177, batch: 24, loss: 0.21845099329948425, acc: 89.0625, f1: 89.07621381886088, r: 0.7376527425307339
06/02/2019 03:59:34 step: 5871, epoch: 177, batch: 29, loss: 0.2785010039806366, acc: 89.0625, f1: 85.10941043083899, r: 0.6829772078310227
06/02/2019 03:59:34 *** evaluating ***
06/02/2019 03:59:34 step: 178, epoch: 177, acc: 59.401709401709404, f1: 24.49773610240103, r: 0.362121255359893
06/02/2019 03:59:34 *** epoch: 179 ***
06/02/2019 03:59:34 *** training ***
06/02/2019 03:59:34 step: 5879, epoch: 178, batch: 4, loss: 0.19609791040420532, acc: 90.625, f1: 73.39778016469745, r: 0.7732336532756245
06/02/2019 03:59:34 step: 5884, epoch: 178, batch: 9, loss: 0.20100325345993042, acc: 95.3125, f1: 94.11954505122206, r: 0.6852416062769578
06/02/2019 03:59:35 step: 5889, epoch: 178, batch: 14, loss: 0.1946328729391098, acc: 93.75, f1: 90.71293081468845, r: 0.7905233875548724
06/02/2019 03:59:35 step: 5894, epoch: 178, batch: 19, loss: 0.14401951432228088, acc: 93.75, f1: 94.82421041006319, r: 0.6894409458786043
06/02/2019 03:59:35 step: 5899, epoch: 178, batch: 24, loss: 0.23408567905426025, acc: 90.625, f1: 85.05779255118229, r: 0.6119556111135314
06/02/2019 03:59:36 step: 5904, epoch: 178, batch: 29, loss: 0.18375632166862488, acc: 90.625, f1: 77.43660968660969, r: 0.5922006145214492
06/02/2019 03:59:36 *** evaluating ***
06/02/2019 03:59:36 step: 179, epoch: 178, acc: 58.54700854700855, f1: 24.291630844262425, r: 0.36589801840654385
06/02/2019 03:59:36 *** epoch: 180 ***
06/02/2019 03:59:36 *** training ***
06/02/2019 03:59:36 step: 5912, epoch: 179, batch: 4, loss: 0.12414047867059708, acc: 95.3125, f1: 90.6575963718821, r: 0.6767706742665955
06/02/2019 03:59:36 step: 5917, epoch: 179, batch: 9, loss: 0.100184865295887, acc: 95.3125, f1: 85.1194234006734, r: 0.6604450513621147
06/02/2019 03:59:37 step: 5922, epoch: 179, batch: 14, loss: 0.1731170415878296, acc: 92.1875, f1: 86.44616467276566, r: 0.6565833187759557
06/02/2019 03:59:37 step: 5927, epoch: 179, batch: 19, loss: 0.16600137948989868, acc: 92.1875, f1: 79.71311379350755, r: 0.709141328340988
06/02/2019 03:59:37 step: 5932, epoch: 179, batch: 24, loss: 0.1988397240638733, acc: 95.3125, f1: 82.05856255545696, r: 0.6460972481840132
06/02/2019 03:59:38 step: 5937, epoch: 179, batch: 29, loss: 0.14299461245536804, acc: 95.3125, f1: 95.65456182472988, r: 0.6394194252186597
06/02/2019 03:59:38 *** evaluating ***
06/02/2019 03:59:38 step: 180, epoch: 179, acc: 58.97435897435898, f1: 25.083791208791204, r: 0.3619722268197862
06/02/2019 03:59:38 *** epoch: 181 ***
06/02/2019 03:59:38 *** training ***
06/02/2019 03:59:38 step: 5945, epoch: 180, batch: 4, loss: 0.17434602975845337, acc: 93.75, f1: 90.92152961980548, r: 0.7139664368594592
06/02/2019 03:59:39 step: 5950, epoch: 180, batch: 9, loss: 0.2513969838619232, acc: 87.5, f1: 86.39631714754869, r: 0.6590216523048865
06/02/2019 03:59:39 step: 5955, epoch: 180, batch: 14, loss: 0.18412531912326813, acc: 92.1875, f1: 82.04286135320619, r: 0.642364403131758
06/02/2019 03:59:39 step: 5960, epoch: 180, batch: 19, loss: 0.22701263427734375, acc: 89.0625, f1: 69.56357157168618, r: 0.5943573745727255
06/02/2019 03:59:40 step: 5965, epoch: 180, batch: 24, loss: 0.2702223062515259, acc: 87.5, f1: 76.13520408163265, r: 0.7057336293249274
06/02/2019 03:59:40 step: 5970, epoch: 180, batch: 29, loss: 0.2712869346141815, acc: 87.5, f1: 70.37831141604727, r: 0.6453135551149096
06/02/2019 03:59:40 *** evaluating ***
06/02/2019 03:59:40 step: 181, epoch: 180, acc: 59.401709401709404, f1: 25.10900788542002, r: 0.35778352326115564
06/02/2019 03:59:40 *** epoch: 182 ***
06/02/2019 03:59:40 *** training ***
06/02/2019 03:59:40 step: 5978, epoch: 181, batch: 4, loss: 0.1748594045639038, acc: 92.1875, f1: 86.48100907029479, r: 0.6814320101243284
06/02/2019 03:59:41 step: 5983, epoch: 181, batch: 9, loss: 0.19403322041034698, acc: 92.1875, f1: 88.10522810522811, r: 0.5770518630983389
06/02/2019 03:59:41 step: 5988, epoch: 181, batch: 14, loss: 0.377957284450531, acc: 84.375, f1: 70.28532870638134, r: 0.7126611131625182
06/02/2019 03:59:41 step: 5993, epoch: 181, batch: 19, loss: 0.17188423871994019, acc: 92.1875, f1: 75.07229562224292, r: 0.6329442761305605
06/02/2019 03:59:42 step: 5998, epoch: 181, batch: 24, loss: 0.26505351066589355, acc: 89.0625, f1: 76.07142857142857, r: 0.677036309836677
06/02/2019 03:59:42 step: 6003, epoch: 181, batch: 29, loss: 0.12173064798116684, acc: 95.3125, f1: 91.26070328628963, r: 0.6634670556725859
06/02/2019 03:59:42 *** evaluating ***
06/02/2019 03:59:42 step: 182, epoch: 181, acc: 60.256410256410255, f1: 25.3543205765428, r: 0.37283398087253494
06/02/2019 03:59:42 *** epoch: 183 ***
06/02/2019 03:59:42 *** training ***
06/02/2019 03:59:42 step: 6011, epoch: 182, batch: 4, loss: 0.1881961226463318, acc: 92.1875, f1: 77.51698242020822, r: 0.6006920422635108
06/02/2019 03:59:43 step: 6016, epoch: 182, batch: 9, loss: 0.17383521795272827, acc: 90.625, f1: 87.60328469344863, r: 0.7326620245716686
06/02/2019 03:59:43 step: 6021, epoch: 182, batch: 14, loss: 0.20334261655807495, acc: 89.0625, f1: 88.74050477824062, r: 0.7793436660092146
06/02/2019 03:59:43 step: 6026, epoch: 182, batch: 19, loss: 0.24286025762557983, acc: 92.1875, f1: 80.72210197210197, r: 0.676354925299359
06/02/2019 03:59:43 step: 6031, epoch: 182, batch: 24, loss: 0.054494597017765045, acc: 98.4375, f1: 98.6986098787341, r: 0.6493757966655035
06/02/2019 03:59:44 step: 6036, epoch: 182, batch: 29, loss: 0.17108997702598572, acc: 93.75, f1: 88.73641410587224, r: 0.6826725387431646
06/02/2019 03:59:44 *** evaluating ***
06/02/2019 03:59:44 step: 183, epoch: 182, acc: 59.401709401709404, f1: 24.74704572533425, r: 0.35383138344521187
06/02/2019 03:59:44 *** epoch: 184 ***
06/02/2019 03:59:44 *** training ***
06/02/2019 03:59:44 step: 6044, epoch: 183, batch: 4, loss: 0.16007864475250244, acc: 93.75, f1: 78.49591386537199, r: 0.5946175255315114
06/02/2019 03:59:45 step: 6049, epoch: 183, batch: 9, loss: 0.2442202866077423, acc: 89.0625, f1: 74.21062271062272, r: 0.5897817840683368
06/02/2019 03:59:45 step: 6054, epoch: 183, batch: 14, loss: 0.20569448173046112, acc: 89.0625, f1: 86.35045031596755, r: 0.6411458144969135
06/02/2019 03:59:45 step: 6059, epoch: 183, batch: 19, loss: 0.23390616476535797, acc: 87.5, f1: 78.44844497542401, r: 0.6287196791912953
06/02/2019 03:59:45 step: 6064, epoch: 183, batch: 24, loss: 0.17188730835914612, acc: 92.1875, f1: 87.02734724391578, r: 0.8219799918944801
06/02/2019 03:59:46 step: 6069, epoch: 183, batch: 29, loss: 0.19021043181419373, acc: 92.1875, f1: 81.94500674763833, r: 0.6574190201239343
06/02/2019 03:59:46 *** evaluating ***
06/02/2019 03:59:46 step: 184, epoch: 183, acc: 59.82905982905983, f1: 24.604086676375836, r: 0.3724798852649529
06/02/2019 03:59:46 *** epoch: 185 ***
06/02/2019 03:59:46 *** training ***
06/02/2019 03:59:46 step: 6077, epoch: 184, batch: 4, loss: 0.1918502002954483, acc: 90.625, f1: 80.3202947845805, r: 0.72908975979357
06/02/2019 03:59:47 step: 6082, epoch: 184, batch: 9, loss: 0.1543578952550888, acc: 95.3125, f1: 96.42918985776129, r: 0.5812645489186392
06/02/2019 03:59:47 step: 6087, epoch: 184, batch: 14, loss: 0.23343360424041748, acc: 85.9375, f1: 73.57019171304886, r: 0.6348868785442566
06/02/2019 03:59:47 step: 6092, epoch: 184, batch: 19, loss: 0.18649637699127197, acc: 93.75, f1: 89.02661064425772, r: 0.6957359846239543
06/02/2019 03:59:47 step: 6097, epoch: 184, batch: 24, loss: 0.1416652947664261, acc: 93.75, f1: 91.36049771925174, r: 0.6804186313964349
06/02/2019 03:59:48 step: 6102, epoch: 184, batch: 29, loss: 0.13882353901863098, acc: 93.75, f1: 90.07768692318258, r: 0.5291631626835152
06/02/2019 03:59:48 *** evaluating ***
06/02/2019 03:59:48 step: 185, epoch: 184, acc: 58.97435897435898, f1: 25.00967823026647, r: 0.3683337793068468
06/02/2019 03:59:48 *** epoch: 186 ***
06/02/2019 03:59:48 *** training ***
06/02/2019 03:59:48 step: 6110, epoch: 185, batch: 4, loss: 0.24542784690856934, acc: 87.5, f1: 76.81848774876065, r: 0.6959134867789352
06/02/2019 03:59:49 step: 6115, epoch: 185, batch: 9, loss: 0.19805914163589478, acc: 89.0625, f1: 72.36848296630906, r: 0.6112907772849342
06/02/2019 03:59:49 step: 6120, epoch: 185, batch: 14, loss: 0.24049973487854004, acc: 93.75, f1: 82.09156989427146, r: 0.695394327645924
06/02/2019 03:59:49 step: 6125, epoch: 185, batch: 19, loss: 0.227000892162323, acc: 90.625, f1: 78.12515684798294, r: 0.7488218405136199
06/02/2019 03:59:49 step: 6130, epoch: 185, batch: 24, loss: 0.2533630132675171, acc: 90.625, f1: 83.68245525160418, r: 0.7058206359279529
06/02/2019 03:59:50 step: 6135, epoch: 185, batch: 29, loss: 0.11772456765174866, acc: 95.3125, f1: 83.23833573833573, r: 0.7061285296429884
06/02/2019 03:59:50 *** evaluating ***
06/02/2019 03:59:50 step: 186, epoch: 185, acc: 60.256410256410255, f1: 25.495141863785935, r: 0.3568279493967115
06/02/2019 03:59:50 *** epoch: 187 ***
06/02/2019 03:59:50 *** training ***
06/02/2019 03:59:50 step: 6143, epoch: 186, batch: 4, loss: 0.13215461373329163, acc: 93.75, f1: 91.30240130240131, r: 0.6925147060676169
06/02/2019 03:59:51 step: 6148, epoch: 186, batch: 9, loss: 0.10195617377758026, acc: 93.75, f1: 86.66644811609406, r: 0.715718560154972
06/02/2019 03:59:51 step: 6153, epoch: 186, batch: 14, loss: 0.11303810775279999, acc: 96.875, f1: 96.02467707730867, r: 0.6909850900876057
06/02/2019 03:59:51 step: 6158, epoch: 186, batch: 19, loss: 0.28201615810394287, acc: 87.5, f1: 64.0974330592452, r: 0.6484416564005027
06/02/2019 03:59:52 step: 6163, epoch: 186, batch: 24, loss: 0.0874163955450058, acc: 98.4375, f1: 94.28571428571428, r: 0.6913667214854878
06/02/2019 03:59:52 step: 6168, epoch: 186, batch: 29, loss: 0.14329533278942108, acc: 93.75, f1: 84.37379949481041, r: 0.7131966314348912
06/02/2019 03:59:52 *** evaluating ***
06/02/2019 03:59:52 step: 187, epoch: 186, acc: 59.401709401709404, f1: 24.4214069980199, r: 0.3691611188569347
06/02/2019 03:59:52 *** epoch: 188 ***
06/02/2019 03:59:52 *** training ***
06/02/2019 03:59:52 step: 6176, epoch: 187, batch: 4, loss: 0.15667974948883057, acc: 92.1875, f1: 87.36653417504482, r: 0.636372200809926
06/02/2019 03:59:53 step: 6181, epoch: 187, batch: 9, loss: 0.19952689111232758, acc: 93.75, f1: 76.71916160081054, r: 0.6546684761727731
06/02/2019 03:59:53 step: 6186, epoch: 187, batch: 14, loss: 0.19626206159591675, acc: 92.1875, f1: 78.0593487394958, r: 0.6715441899141165
06/02/2019 03:59:53 step: 6191, epoch: 187, batch: 19, loss: 0.10273555666208267, acc: 96.875, f1: 95.60710122529909, r: 0.6835324835727015
06/02/2019 03:59:53 step: 6196, epoch: 187, batch: 24, loss: 0.11385054886341095, acc: 93.75, f1: 88.18287037037037, r: 0.7368862975016793
06/02/2019 03:59:54 step: 6201, epoch: 187, batch: 29, loss: 0.18173138797283173, acc: 93.75, f1: 89.83948429385771, r: 0.7888678771820451
06/02/2019 03:59:54 *** evaluating ***
06/02/2019 03:59:54 step: 188, epoch: 187, acc: 59.401709401709404, f1: 24.456573672776194, r: 0.3708191321146668
06/02/2019 03:59:54 *** epoch: 189 ***
06/02/2019 03:59:54 *** training ***
06/02/2019 03:59:54 step: 6209, epoch: 188, batch: 4, loss: 0.07866141945123672, acc: 100.0, f1: 100.0, r: 0.6742071802494534
06/02/2019 03:59:55 step: 6214, epoch: 188, batch: 9, loss: 0.18846657872200012, acc: 87.5, f1: 81.91249641831038, r: 0.6664615287773978
06/02/2019 03:59:55 step: 6219, epoch: 188, batch: 14, loss: 0.09612370282411575, acc: 95.3125, f1: 79.5495284856987, r: 0.5888865219585453
06/02/2019 03:59:55 step: 6224, epoch: 188, batch: 19, loss: 0.18466439843177795, acc: 89.0625, f1: 79.27318295739349, r: 0.7026835750890348
06/02/2019 03:59:55 step: 6229, epoch: 188, batch: 24, loss: 0.2798110842704773, acc: 87.5, f1: 80.00076312576313, r: 0.7379922987715514
06/02/2019 03:59:56 step: 6234, epoch: 188, batch: 29, loss: 0.14880691468715668, acc: 93.75, f1: 91.26277231540391, r: 0.6999901720964389
06/02/2019 03:59:56 *** evaluating ***
06/02/2019 03:59:56 step: 189, epoch: 188, acc: 59.82905982905983, f1: 24.437899581337845, r: 0.363585201494051
06/02/2019 03:59:56 *** epoch: 190 ***
06/02/2019 03:59:56 *** training ***
06/02/2019 03:59:56 step: 6242, epoch: 189, batch: 4, loss: 0.2251368761062622, acc: 87.5, f1: 74.80098605098607, r: 0.6600330467437158
06/02/2019 03:59:56 step: 6247, epoch: 189, batch: 9, loss: 0.12698109447956085, acc: 96.875, f1: 94.15562835303614, r: 0.6822713271004581
06/02/2019 03:59:57 step: 6252, epoch: 189, batch: 14, loss: 0.21330514550209045, acc: 89.0625, f1: 73.64928606995302, r: 0.7050162468865769
06/02/2019 03:59:57 step: 6257, epoch: 189, batch: 19, loss: 0.07621046900749207, acc: 95.3125, f1: 80.54535805295684, r: 0.6856699212186166
06/02/2019 03:59:57 step: 6262, epoch: 189, batch: 24, loss: 0.1922059953212738, acc: 90.625, f1: 82.17792845452419, r: 0.6094428586479593
06/02/2019 03:59:57 step: 6267, epoch: 189, batch: 29, loss: 0.15029148757457733, acc: 93.75, f1: 89.79795950869597, r: 0.7125275304401443
06/02/2019 03:59:58 *** evaluating ***
06/02/2019 03:59:58 step: 190, epoch: 189, acc: 59.401709401709404, f1: 25.038055826030515, r: 0.360867454610109
06/02/2019 03:59:58 *** epoch: 191 ***
06/02/2019 03:59:58 *** training ***
06/02/2019 03:59:58 step: 6275, epoch: 190, batch: 4, loss: 0.1753179430961609, acc: 95.3125, f1: 82.22547895161632, r: 0.7091664343914267
06/02/2019 03:59:58 step: 6280, epoch: 190, batch: 9, loss: 0.13024495542049408, acc: 93.75, f1: 80.90993046501521, r: 0.6629936860243303
06/02/2019 03:59:59 step: 6285, epoch: 190, batch: 14, loss: 0.1454627513885498, acc: 93.75, f1: 92.42110377985782, r: 0.6809244923355336
06/02/2019 03:59:59 step: 6290, epoch: 190, batch: 19, loss: 0.1283789724111557, acc: 93.75, f1: 75.25510204081633, r: 0.6185075948748018
06/02/2019 03:59:59 step: 6295, epoch: 190, batch: 24, loss: 0.18141606450080872, acc: 93.75, f1: 89.34462349132133, r: 0.6192807175167699
06/02/2019 03:59:59 step: 6300, epoch: 190, batch: 29, loss: 0.1701449602842331, acc: 92.1875, f1: 81.8228690195323, r: 0.6951463568478623
06/02/2019 04:00:00 *** evaluating ***
06/02/2019 04:00:00 step: 191, epoch: 190, acc: 60.256410256410255, f1: 25.74019753411896, r: 0.3798356373947793
06/02/2019 04:00:00 *** epoch: 192 ***
06/02/2019 04:00:00 *** training ***
06/02/2019 04:00:00 step: 6308, epoch: 191, batch: 4, loss: 0.19194826483726501, acc: 90.625, f1: 80.16164994425864, r: 0.6991766814614366
06/02/2019 04:00:00 step: 6313, epoch: 191, batch: 9, loss: 0.17300067842006683, acc: 93.75, f1: 90.41186987615559, r: 0.6309217661869833
06/02/2019 04:00:01 step: 6318, epoch: 191, batch: 14, loss: 0.1559448540210724, acc: 93.75, f1: 80.47992577597842, r: 0.7319065349031819
06/02/2019 04:00:01 step: 6323, epoch: 191, batch: 19, loss: 0.17939411103725433, acc: 92.1875, f1: 87.31643625192012, r: 0.6464419040857599
06/02/2019 04:00:01 step: 6328, epoch: 191, batch: 24, loss: 0.16152867674827576, acc: 92.1875, f1: 83.00384582145017, r: 0.6379908707573254
06/02/2019 04:00:01 step: 6333, epoch: 191, batch: 29, loss: 0.18837153911590576, acc: 93.75, f1: 92.53239801667794, r: 0.5255144336037562
06/02/2019 04:00:02 *** evaluating ***
06/02/2019 04:00:02 step: 192, epoch: 191, acc: 59.82905982905983, f1: 24.59698996655518, r: 0.357798291145191
06/02/2019 04:00:02 *** epoch: 193 ***
06/02/2019 04:00:02 *** training ***
06/02/2019 04:00:02 step: 6341, epoch: 192, batch: 4, loss: 0.1944531351327896, acc: 92.1875, f1: 80.68314093314093, r: 0.7315573686395159
06/02/2019 04:00:02 step: 6346, epoch: 192, batch: 9, loss: 0.2269870936870575, acc: 90.625, f1: 78.90794739851344, r: 0.6103732031008575
06/02/2019 04:00:02 step: 6351, epoch: 192, batch: 14, loss: 0.20648746192455292, acc: 90.625, f1: 84.88859941139577, r: 0.705187658159115
06/02/2019 04:00:03 step: 6356, epoch: 192, batch: 19, loss: 0.1491977870464325, acc: 96.875, f1: 94.93650793650794, r: 0.7453373700187458
06/02/2019 04:00:03 step: 6361, epoch: 192, batch: 24, loss: 0.15534789860248566, acc: 95.3125, f1: 87.5, r: 0.7279746301809209
06/02/2019 04:00:03 step: 6366, epoch: 192, batch: 29, loss: 0.1726168990135193, acc: 93.75, f1: 82.67857142857142, r: 0.7266342032443758
06/02/2019 04:00:03 *** evaluating ***
06/02/2019 04:00:04 step: 193, epoch: 192, acc: 60.256410256410255, f1: 25.840955753750556, r: 0.34683878871844837
06/02/2019 04:00:04 *** epoch: 194 ***
06/02/2019 04:00:04 *** training ***
06/02/2019 04:00:04 step: 6374, epoch: 193, batch: 4, loss: 0.10894685238599777, acc: 95.3125, f1: 86.6894586894587, r: 0.6147537523774266
06/02/2019 04:00:04 step: 6379, epoch: 193, batch: 9, loss: 0.1716320514678955, acc: 93.75, f1: 81.03967071180188, r: 0.7058532364892057
06/02/2019 04:00:04 step: 6384, epoch: 193, batch: 14, loss: 0.18060016632080078, acc: 92.1875, f1: 83.95959109596653, r: 0.6593428009907719
06/02/2019 04:00:05 step: 6389, epoch: 193, batch: 19, loss: 0.24761800467967987, acc: 90.625, f1: 75.85882058970515, r: 0.628560306274227
06/02/2019 04:00:05 step: 6394, epoch: 193, batch: 24, loss: 0.14788226783275604, acc: 95.3125, f1: 92.41666666666667, r: 0.7058290430505714
06/02/2019 04:00:05 step: 6399, epoch: 193, batch: 29, loss: 0.17293938994407654, acc: 93.75, f1: 84.00890904760874, r: 0.6780729886183958
06/02/2019 04:00:05 *** evaluating ***
06/02/2019 04:00:05 step: 194, epoch: 193, acc: 58.97435897435898, f1: 24.330425322494538, r: 0.35161179647256247
06/02/2019 04:00:05 *** epoch: 195 ***
06/02/2019 04:00:05 *** training ***
06/02/2019 04:00:06 step: 6407, epoch: 194, batch: 4, loss: 0.14351309835910797, acc: 95.3125, f1: 96.13138184566755, r: 0.6778375242658781
06/02/2019 04:00:06 step: 6412, epoch: 194, batch: 9, loss: 0.17429916560649872, acc: 93.75, f1: 92.70664869721472, r: 0.6113348231753496
06/02/2019 04:00:06 step: 6417, epoch: 194, batch: 14, loss: 0.21881839632987976, acc: 90.625, f1: 86.01724664224663, r: 0.7421163385927404
06/02/2019 04:00:07 step: 6422, epoch: 194, batch: 19, loss: 0.16612520813941956, acc: 92.1875, f1: 85.93188557474272, r: 0.7314768005189081
06/02/2019 04:00:07 step: 6427, epoch: 194, batch: 24, loss: 0.22943845391273499, acc: 90.625, f1: 74.03325123152709, r: 0.5817547898961564
06/02/2019 04:00:07 step: 6432, epoch: 194, batch: 29, loss: 0.15799188613891602, acc: 93.75, f1: 76.56366621883863, r: 0.6399622944078794
06/02/2019 04:00:07 *** evaluating ***
06/02/2019 04:00:07 step: 195, epoch: 194, acc: 59.401709401709404, f1: 25.80449853177126, r: 0.3672061732821498
06/02/2019 04:00:07 *** epoch: 196 ***
06/02/2019 04:00:07 *** training ***
06/02/2019 04:00:08 step: 6440, epoch: 195, batch: 4, loss: 0.18717782199382782, acc: 92.1875, f1: 84.60317460317461, r: 0.7515193878949569
06/02/2019 04:00:08 step: 6445, epoch: 195, batch: 9, loss: 0.14933009445667267, acc: 93.75, f1: 93.41514383531191, r: 0.6721153360505479
06/02/2019 04:00:08 step: 6450, epoch: 195, batch: 14, loss: 0.074718177318573, acc: 98.4375, f1: 99.09700722394221, r: 0.8042888401657505
06/02/2019 04:00:08 step: 6455, epoch: 195, batch: 19, loss: 0.1200031191110611, acc: 95.3125, f1: 89.61853558627753, r: 0.6488637945793949
06/02/2019 04:00:09 step: 6460, epoch: 195, batch: 24, loss: 0.12432339787483215, acc: 93.75, f1: 91.9111821548975, r: 0.7756496925621001
06/02/2019 04:00:09 step: 6465, epoch: 195, batch: 29, loss: 0.335691899061203, acc: 84.375, f1: 55.470172153742126, r: 0.5810634007029932
06/02/2019 04:00:09 *** evaluating ***
06/02/2019 04:00:09 step: 196, epoch: 195, acc: 58.97435897435898, f1: 24.378558451101124, r: 0.3539408679782168
06/02/2019 04:00:09 *** epoch: 197 ***
06/02/2019 04:00:09 *** training ***
06/02/2019 04:00:10 step: 6473, epoch: 196, batch: 4, loss: 0.16016940772533417, acc: 93.75, f1: 95.74569100150495, r: 0.6574834018744203
06/02/2019 04:00:10 step: 6478, epoch: 196, batch: 9, loss: 0.19086682796478271, acc: 92.1875, f1: 79.8645438974452, r: 0.6594411132645726
06/02/2019 04:00:10 step: 6483, epoch: 196, batch: 14, loss: 0.15538005530834198, acc: 92.1875, f1: 90.21778221778222, r: 0.6593220645425376
06/02/2019 04:00:10 step: 6488, epoch: 196, batch: 19, loss: 0.18089927732944489, acc: 90.625, f1: 87.08117862529627, r: 0.7208081053699251
06/02/2019 04:00:11 step: 6493, epoch: 196, batch: 24, loss: 0.11762020736932755, acc: 96.875, f1: 96.37896825396825, r: 0.7964922858218654
06/02/2019 04:00:11 step: 6498, epoch: 196, batch: 29, loss: 0.21086913347244263, acc: 90.625, f1: 86.85631828488971, r: 0.6296550546368579
06/02/2019 04:00:11 *** evaluating ***
06/02/2019 04:00:11 step: 197, epoch: 196, acc: 58.97435897435898, f1: 24.871204634484695, r: 0.35719942512109293
06/02/2019 04:00:11 *** epoch: 198 ***
06/02/2019 04:00:11 *** training ***
06/02/2019 04:00:11 step: 6506, epoch: 197, batch: 4, loss: 0.09486464411020279, acc: 96.875, f1: 81.47509241330752, r: 0.6766517558470051
06/02/2019 04:00:12 step: 6511, epoch: 197, batch: 9, loss: 0.14570173621177673, acc: 95.3125, f1: 93.48727785428554, r: 0.5831498020867152
06/02/2019 04:00:12 step: 6516, epoch: 197, batch: 14, loss: 0.155999094247818, acc: 93.75, f1: 88.6111463218828, r: 0.6693407355864812
06/02/2019 04:00:12 step: 6521, epoch: 197, batch: 19, loss: 0.1854795515537262, acc: 93.75, f1: 80.63215610385423, r: 0.6199820301543086
06/02/2019 04:00:13 step: 6526, epoch: 197, batch: 24, loss: 0.177814781665802, acc: 90.625, f1: 81.5278131454602, r: 0.6362615835819936
06/02/2019 04:00:13 step: 6531, epoch: 197, batch: 29, loss: 0.22745266556739807, acc: 89.0625, f1: 81.73766073766073, r: 0.7439217772584671
06/02/2019 04:00:13 *** evaluating ***
06/02/2019 04:00:13 step: 198, epoch: 197, acc: 59.82905982905983, f1: 25.200700155342957, r: 0.35899777947170586
06/02/2019 04:00:13 *** epoch: 199 ***
06/02/2019 04:00:13 *** training ***
06/02/2019 04:00:13 step: 6539, epoch: 198, batch: 4, loss: 0.09780924767255783, acc: 96.875, f1: 95.06756548773356, r: 0.6897473531850778
06/02/2019 04:00:14 step: 6544, epoch: 198, batch: 9, loss: 0.14471501111984253, acc: 96.875, f1: 94.93314567206193, r: 0.6536217273274363
06/02/2019 04:00:14 step: 6549, epoch: 198, batch: 14, loss: 0.1558992862701416, acc: 92.1875, f1: 92.10664335664336, r: 0.763972761588348
06/02/2019 04:00:14 step: 6554, epoch: 198, batch: 19, loss: 0.22954483330249786, acc: 89.0625, f1: 74.50929304190174, r: 0.6807272053988419
06/02/2019 04:00:15 step: 6559, epoch: 198, batch: 24, loss: 0.18991509079933167, acc: 95.3125, f1: 77.35681114551085, r: 0.609969759699215
06/02/2019 04:00:15 step: 6564, epoch: 198, batch: 29, loss: 0.07355968654155731, acc: 98.4375, f1: 98.93362750505608, r: 0.6458816850979915
06/02/2019 04:00:15 *** evaluating ***
06/02/2019 04:00:15 step: 199, epoch: 198, acc: 59.82905982905983, f1: 24.656843156843156, r: 0.3601738395690635
06/02/2019 04:00:15 *** epoch: 200 ***
06/02/2019 04:00:15 *** training ***
06/02/2019 04:00:16 step: 6572, epoch: 199, batch: 4, loss: 0.1207350566983223, acc: 93.75, f1: 77.23224555522691, r: 0.5611172644444093
06/02/2019 04:00:16 step: 6577, epoch: 199, batch: 9, loss: 0.10767438262701035, acc: 98.4375, f1: 99.20694459329118, r: 0.6638330537743079
06/02/2019 04:00:16 step: 6582, epoch: 199, batch: 14, loss: 0.21062318980693817, acc: 90.625, f1: 84.11989795918366, r: 0.7460500588989768
06/02/2019 04:00:16 step: 6587, epoch: 199, batch: 19, loss: 0.14411307871341705, acc: 95.3125, f1: 92.0809349380778, r: 0.7191675689716932
06/02/2019 04:00:17 step: 6592, epoch: 199, batch: 24, loss: 0.18126758933067322, acc: 92.1875, f1: 90.63547563547564, r: 0.6746389511381725
06/02/2019 04:00:17 step: 6597, epoch: 199, batch: 29, loss: 0.28516215085983276, acc: 85.9375, f1: 78.94509476031214, r: 0.6792408492029985
06/02/2019 04:00:17 *** evaluating ***
06/02/2019 04:00:17 step: 200, epoch: 199, acc: 59.401709401709404, f1: 23.117810760667904, r: 0.36257609263027585
06/02/2019 04:00:17 *** epoch: 201 ***
06/02/2019 04:00:17 *** training ***
06/02/2019 04:00:18 step: 6605, epoch: 200, batch: 4, loss: 0.17914386093616486, acc: 92.1875, f1: 76.04710701484896, r: 0.6161504043790551
06/02/2019 04:00:18 step: 6610, epoch: 200, batch: 9, loss: 0.13236893713474274, acc: 95.3125, f1: 93.240536059685, r: 0.7344444816662544
06/02/2019 04:00:18 step: 6615, epoch: 200, batch: 14, loss: 0.15918555855751038, acc: 95.3125, f1: 92.19466936572199, r: 0.771306329383783
06/02/2019 04:00:18 step: 6620, epoch: 200, batch: 19, loss: 0.20442301034927368, acc: 89.0625, f1: 79.82961065667833, r: 0.610825236667802
06/02/2019 04:00:19 step: 6625, epoch: 200, batch: 24, loss: 0.15986865758895874, acc: 90.625, f1: 82.55219329110955, r: 0.6083624388952683
06/02/2019 04:00:19 step: 6630, epoch: 200, batch: 29, loss: 0.12498629093170166, acc: 93.75, f1: 91.89811066126856, r: 0.7669295526867126
06/02/2019 04:00:19 *** evaluating ***
06/02/2019 04:00:19 step: 201, epoch: 200, acc: 58.54700854700855, f1: 24.915839618896385, r: 0.3628366412137858
06/02/2019 04:00:19 *** epoch: 202 ***
06/02/2019 04:00:19 *** training ***
06/02/2019 04:00:19 step: 6638, epoch: 201, batch: 4, loss: 0.1294078528881073, acc: 93.75, f1: 80.58465608465607, r: 0.6678740382955312
06/02/2019 04:00:20 step: 6643, epoch: 201, batch: 9, loss: 0.15286961197853088, acc: 96.875, f1: 94.11483253588517, r: 0.7285562495143959
06/02/2019 04:00:20 step: 6648, epoch: 201, batch: 14, loss: 0.1795026808977127, acc: 92.1875, f1: 77.85418090681249, r: 0.6734564568136966
06/02/2019 04:00:20 step: 6653, epoch: 201, batch: 19, loss: 0.15087705850601196, acc: 95.3125, f1: 93.64642607567136, r: 0.7331029646535473
06/02/2019 04:00:21 step: 6658, epoch: 201, batch: 24, loss: 0.09815342724323273, acc: 96.875, f1: 91.94444444444444, r: 0.5695731151445981
06/02/2019 04:00:21 step: 6663, epoch: 201, batch: 29, loss: 0.173402339220047, acc: 93.75, f1: 94.87774725274724, r: 0.7833073267380902
06/02/2019 04:00:21 *** evaluating ***
06/02/2019 04:00:21 step: 202, epoch: 201, acc: 59.401709401709404, f1: 24.615525651508538, r: 0.365174282806938
06/02/2019 04:00:21 *** epoch: 203 ***
06/02/2019 04:00:21 *** training ***
06/02/2019 04:00:21 step: 6671, epoch: 202, batch: 4, loss: 0.15214534103870392, acc: 93.75, f1: 88.9416384708422, r: 0.6014487524874554
06/02/2019 04:00:22 step: 6676, epoch: 202, batch: 9, loss: 0.10960865020751953, acc: 96.875, f1: 94.7904335306351, r: 0.788949934265625
06/02/2019 04:00:22 step: 6681, epoch: 202, batch: 14, loss: 0.16757944226264954, acc: 93.75, f1: 93.65744277508983, r: 0.707322036819751
06/02/2019 04:00:22 step: 6686, epoch: 202, batch: 19, loss: 0.22037774324417114, acc: 92.1875, f1: 86.45652173913044, r: 0.7494839075805193
06/02/2019 04:00:22 step: 6691, epoch: 202, batch: 24, loss: 0.11355054378509521, acc: 96.875, f1: 90.07936507936509, r: 0.6176375362279177
06/02/2019 04:00:23 step: 6696, epoch: 202, batch: 29, loss: 0.2601226568222046, acc: 89.0625, f1: 84.79166666666667, r: 0.6929457923319096
06/02/2019 04:00:23 *** evaluating ***
06/02/2019 04:00:23 step: 203, epoch: 202, acc: 58.97435897435898, f1: 24.96069943128767, r: 0.36989335625349173
06/02/2019 04:00:23 *** epoch: 204 ***
06/02/2019 04:00:23 *** training ***
06/02/2019 04:00:23 step: 6704, epoch: 203, batch: 4, loss: 0.1818070262670517, acc: 90.625, f1: 86.02721088435372, r: 0.6367793496124253
06/02/2019 04:00:24 step: 6709, epoch: 203, batch: 9, loss: 0.12048868834972382, acc: 93.75, f1: 91.76430976430977, r: 0.7817558400135054
06/02/2019 04:00:24 step: 6714, epoch: 203, batch: 14, loss: 0.20090380311012268, acc: 89.0625, f1: 74.45942945942944, r: 0.6807686936932038
06/02/2019 04:00:24 step: 6719, epoch: 203, batch: 19, loss: 0.1445058286190033, acc: 95.3125, f1: 95.46382189239333, r: 0.6896496481203014
06/02/2019 04:00:24 step: 6724, epoch: 203, batch: 24, loss: 0.0984073206782341, acc: 96.875, f1: 96.55122655122655, r: 0.685391328695077
06/02/2019 04:00:25 step: 6729, epoch: 203, batch: 29, loss: 0.16818645596504211, acc: 93.75, f1: 91.76206759837446, r: 0.6657532006515481
06/02/2019 04:00:25 *** evaluating ***
06/02/2019 04:00:25 step: 204, epoch: 203, acc: 59.82905982905983, f1: 24.49064129822457, r: 0.36639834639604646
06/02/2019 04:00:25 *** epoch: 205 ***
06/02/2019 04:00:25 *** training ***
06/02/2019 04:00:25 step: 6737, epoch: 204, batch: 4, loss: 0.31425344944000244, acc: 82.8125, f1: 75.26334776334778, r: 0.6229481746218957
06/02/2019 04:00:26 step: 6742, epoch: 204, batch: 9, loss: 0.16579782962799072, acc: 93.75, f1: 92.0032223415682, r: 0.7131799031453221
06/02/2019 04:00:26 step: 6747, epoch: 204, batch: 14, loss: 0.22370164096355438, acc: 90.625, f1: 84.61952963003382, r: 0.745412797598613
06/02/2019 04:00:26 step: 6752, epoch: 204, batch: 19, loss: 0.1275731325149536, acc: 95.3125, f1: 92.29587542087542, r: 0.77562301185009
06/02/2019 04:00:26 step: 6757, epoch: 204, batch: 24, loss: 0.16301380097866058, acc: 90.625, f1: 74.39522342064716, r: 0.7248464700600751
06/02/2019 04:00:27 step: 6762, epoch: 204, batch: 29, loss: 0.06732486188411713, acc: 100.0, f1: 100.0, r: 0.7124497609011592
06/02/2019 04:00:27 *** evaluating ***
06/02/2019 04:00:27 step: 205, epoch: 204, acc: 59.82905982905983, f1: 25.068535913420405, r: 0.3617636915793418
06/02/2019 04:00:27 *** epoch: 206 ***
06/02/2019 04:00:27 *** training ***
06/02/2019 04:00:27 step: 6770, epoch: 205, batch: 4, loss: 0.14521543681621552, acc: 92.1875, f1: 85.59174024219615, r: 0.6112064977947639
06/02/2019 04:00:28 step: 6775, epoch: 205, batch: 9, loss: 0.20231115818023682, acc: 92.1875, f1: 89.62593565854435, r: 0.7762169042523202
06/02/2019 04:00:28 step: 6780, epoch: 205, batch: 14, loss: 0.14576634764671326, acc: 93.75, f1: 89.79606950792349, r: 0.7297246044859982
06/02/2019 04:00:28 step: 6785, epoch: 205, batch: 19, loss: 0.11972019821405411, acc: 95.3125, f1: 72.1808176100629, r: 0.7076084083461465
06/02/2019 04:00:28 step: 6790, epoch: 205, batch: 24, loss: 0.18452297151088715, acc: 92.1875, f1: 79.19179894179894, r: 0.699729857056323
06/02/2019 04:00:29 step: 6795, epoch: 205, batch: 29, loss: 0.13064876198768616, acc: 92.1875, f1: 75.94012204424104, r: 0.6661871043150914
06/02/2019 04:00:29 *** evaluating ***
06/02/2019 04:00:29 step: 206, epoch: 205, acc: 60.256410256410255, f1: 25.075458558462003, r: 0.36311575428162596
06/02/2019 04:00:29 *** epoch: 207 ***
06/02/2019 04:00:29 *** training ***
06/02/2019 04:00:29 step: 6803, epoch: 206, batch: 4, loss: 0.08081516623497009, acc: 96.875, f1: 92.06691682301438, r: 0.7302901663958793
06/02/2019 04:00:30 step: 6808, epoch: 206, batch: 9, loss: 0.17672112584114075, acc: 92.1875, f1: 90.63827938446319, r: 0.708557889742574
06/02/2019 04:00:30 step: 6813, epoch: 206, batch: 14, loss: 0.16818147897720337, acc: 96.875, f1: 84.80113636363636, r: 0.6512553372953079
06/02/2019 04:00:30 step: 6818, epoch: 206, batch: 19, loss: 0.12949781119823456, acc: 95.3125, f1: 92.77777777777779, r: 0.7558552389025841
06/02/2019 04:00:31 step: 6823, epoch: 206, batch: 24, loss: 0.09925752878189087, acc: 96.875, f1: 90.6961506961507, r: 0.6632408949367371
06/02/2019 04:00:31 step: 6828, epoch: 206, batch: 29, loss: 0.12210721522569656, acc: 93.75, f1: 75.80979046496289, r: 0.594488640415187
06/02/2019 04:00:31 *** evaluating ***
06/02/2019 04:00:31 step: 207, epoch: 206, acc: 59.82905982905983, f1: 25.17244030479325, r: 0.3562160559575681
06/02/2019 04:00:31 *** epoch: 208 ***
06/02/2019 04:00:31 *** training ***
06/02/2019 04:00:31 step: 6836, epoch: 207, batch: 4, loss: 0.19980573654174805, acc: 92.1875, f1: 86.63548752834467, r: 0.6554242429343508
06/02/2019 04:00:32 step: 6841, epoch: 207, batch: 9, loss: 0.07127446681261063, acc: 96.875, f1: 86.60714285714286, r: 0.7753158056060612
06/02/2019 04:00:32 step: 6846, epoch: 207, batch: 14, loss: 0.13283473253250122, acc: 93.75, f1: 88.59768907563026, r: 0.7824691863855497
06/02/2019 04:00:32 step: 6851, epoch: 207, batch: 19, loss: 0.2638048827648163, acc: 84.375, f1: 71.67821067821068, r: 0.5890898328267701
06/02/2019 04:00:33 step: 6856, epoch: 207, batch: 24, loss: 0.11075295507907867, acc: 95.3125, f1: 91.40563946859346, r: 0.5706921160271633
06/02/2019 04:00:33 step: 6861, epoch: 207, batch: 29, loss: 0.11191990226507187, acc: 96.875, f1: 96.20696763553907, r: 0.7002217038074094
06/02/2019 04:00:33 *** evaluating ***
06/02/2019 04:00:33 step: 208, epoch: 207, acc: 58.97435897435898, f1: 25.912077754646905, r: 0.3652127512914065
06/02/2019 04:00:33 *** epoch: 209 ***
06/02/2019 04:00:33 *** training ***
06/02/2019 04:00:34 step: 6869, epoch: 208, batch: 4, loss: 0.05325707793235779, acc: 100.0, f1: 100.0, r: 0.7589837030222351
06/02/2019 04:00:34 step: 6874, epoch: 208, batch: 9, loss: 0.06863753497600555, acc: 98.4375, f1: 96.70995670995671, r: 0.6986499024125141
06/02/2019 04:00:34 step: 6879, epoch: 208, batch: 14, loss: 0.22894835472106934, acc: 90.625, f1: 82.17223910840931, r: 0.6903494552881673
06/02/2019 04:00:34 step: 6884, epoch: 208, batch: 19, loss: 0.16078868508338928, acc: 93.75, f1: 90.47565196382361, r: 0.7075230391058551
06/02/2019 04:00:35 step: 6889, epoch: 208, batch: 24, loss: 0.10841422528028488, acc: 95.3125, f1: 95.09697539302803, r: 0.8080778050399812
06/02/2019 04:00:35 step: 6894, epoch: 208, batch: 29, loss: 0.19757679104804993, acc: 90.625, f1: 78.41804612337296, r: 0.6326346685914495
06/02/2019 04:00:35 *** evaluating ***
06/02/2019 04:00:35 step: 209, epoch: 208, acc: 58.54700854700855, f1: 24.928537511870847, r: 0.3658448746826831
06/02/2019 04:00:35 *** epoch: 210 ***
06/02/2019 04:00:35 *** training ***
06/02/2019 04:00:36 step: 6902, epoch: 209, batch: 4, loss: 0.2794913053512573, acc: 84.375, f1: 81.62313523200619, r: 0.7147065290710182
06/02/2019 04:00:36 step: 6907, epoch: 209, batch: 9, loss: 0.12348777055740356, acc: 95.3125, f1: 91.51035745863332, r: 0.7188324765561079
06/02/2019 04:00:36 step: 6912, epoch: 209, batch: 14, loss: 0.12204664945602417, acc: 95.3125, f1: 95.86880552397795, r: 0.7003487588227146
06/02/2019 04:00:37 step: 6917, epoch: 209, batch: 19, loss: 0.15841135382652283, acc: 90.625, f1: 77.31601731601731, r: 0.5562382631839029
06/02/2019 04:00:37 step: 6922, epoch: 209, batch: 24, loss: 0.19535139203071594, acc: 90.625, f1: 68.75957882550175, r: 0.6992979685457694
06/02/2019 04:00:37 step: 6927, epoch: 209, batch: 29, loss: 0.150252103805542, acc: 90.625, f1: 70.25413109043797, r: 0.7382407989759306
06/02/2019 04:00:37 *** evaluating ***
06/02/2019 04:00:37 step: 210, epoch: 209, acc: 59.401709401709404, f1: 25.082173473089338, r: 0.3514116377460733
06/02/2019 04:00:37 *** epoch: 211 ***
06/02/2019 04:00:37 *** training ***
06/02/2019 04:00:38 step: 6935, epoch: 210, batch: 4, loss: 0.2227569818496704, acc: 89.0625, f1: 82.59242827565808, r: 0.6553491845830937
06/02/2019 04:00:38 step: 6940, epoch: 210, batch: 9, loss: 0.23035795986652374, acc: 87.5, f1: 80.66658223573117, r: 0.7449264094701675
06/02/2019 04:00:38 step: 6945, epoch: 210, batch: 14, loss: 0.13639622926712036, acc: 96.875, f1: 95.29445397369926, r: 0.7299253693208527
06/02/2019 04:00:39 step: 6950, epoch: 210, batch: 19, loss: 0.24031764268875122, acc: 89.0625, f1: 60.12896825396825, r: 0.6518057019573023
06/02/2019 04:00:39 step: 6955, epoch: 210, batch: 24, loss: 0.17950968444347382, acc: 90.625, f1: 67.66802131691837, r: 0.5755235312201638
06/02/2019 04:00:39 step: 6960, epoch: 210, batch: 29, loss: 0.05311144143342972, acc: 98.4375, f1: 95.71428571428571, r: 0.7729012205955841
06/02/2019 04:00:39 *** evaluating ***
06/02/2019 04:00:40 step: 211, epoch: 210, acc: 60.256410256410255, f1: 25.12200497914784, r: 0.3607428194624336
06/02/2019 04:00:40 *** epoch: 212 ***
06/02/2019 04:00:40 *** training ***
06/02/2019 04:00:40 step: 6968, epoch: 211, batch: 4, loss: 0.11692976951599121, acc: 96.875, f1: 95.91649159663865, r: 0.7187748857856319
06/02/2019 04:00:40 step: 6973, epoch: 211, batch: 9, loss: 0.24886170029640198, acc: 87.5, f1: 72.34037712811298, r: 0.6893611900198516
06/02/2019 04:00:40 step: 6978, epoch: 211, batch: 14, loss: 0.22920435667037964, acc: 90.625, f1: 76.3814560912084, r: 0.6343872481633516
06/02/2019 04:00:41 step: 6983, epoch: 211, batch: 19, loss: 0.10217554867267609, acc: 96.875, f1: 97.05847953216374, r: 0.7472694562347988
06/02/2019 04:00:41 step: 6988, epoch: 211, batch: 24, loss: 0.1662055104970932, acc: 92.1875, f1: 84.28236251765665, r: 0.6696950705844122
06/02/2019 04:00:41 step: 6993, epoch: 211, batch: 29, loss: 0.20678164064884186, acc: 89.0625, f1: 72.99138361638362, r: 0.6289575753437194
06/02/2019 04:00:41 *** evaluating ***
06/02/2019 04:00:42 step: 212, epoch: 211, acc: 58.119658119658126, f1: 24.654266664309706, r: 0.36030629352233806
06/02/2019 04:00:42 *** epoch: 213 ***
06/02/2019 04:00:42 *** training ***
06/02/2019 04:00:42 step: 7001, epoch: 212, batch: 4, loss: 0.11201132088899612, acc: 95.3125, f1: 72.61904761904762, r: 0.6235532361611045
06/02/2019 04:00:42 step: 7006, epoch: 212, batch: 9, loss: 0.06726190447807312, acc: 96.875, f1: 93.36796536796538, r: 0.6710934795074642
06/02/2019 04:00:42 step: 7011, epoch: 212, batch: 14, loss: 0.16308808326721191, acc: 95.3125, f1: 96.28990844458085, r: 0.6415847749700228
06/02/2019 04:00:43 step: 7016, epoch: 212, batch: 19, loss: 0.13421501219272614, acc: 92.1875, f1: 86.08405483405484, r: 0.5742748623664387
06/02/2019 04:00:43 step: 7021, epoch: 212, batch: 24, loss: 0.13185225427150726, acc: 96.875, f1: 95.78373015873017, r: 0.7518370652689312
06/02/2019 04:00:43 step: 7026, epoch: 212, batch: 29, loss: 0.28865545988082886, acc: 85.9375, f1: 65.37591162591163, r: 0.6275014936736044
06/02/2019 04:00:43 *** evaluating ***
06/02/2019 04:00:44 step: 213, epoch: 212, acc: 59.82905982905983, f1: 25.99361469636384, r: 0.36571567913746966
06/02/2019 04:00:44 *** epoch: 214 ***
06/02/2019 04:00:44 *** training ***
06/02/2019 04:00:44 step: 7034, epoch: 213, batch: 4, loss: 0.10765645653009415, acc: 96.875, f1: 96.36211741474901, r: 0.6914712923549116
06/02/2019 04:00:44 step: 7039, epoch: 213, batch: 9, loss: 0.14356295764446259, acc: 95.3125, f1: 92.97252747252747, r: 0.6930364130782269
06/02/2019 04:00:44 step: 7044, epoch: 213, batch: 14, loss: 0.15240854024887085, acc: 90.625, f1: 71.74506779769938, r: 0.5781613981582362
06/02/2019 04:00:45 step: 7049, epoch: 213, batch: 19, loss: 0.10427290946245193, acc: 96.875, f1: 93.79251700680273, r: 0.7648310909753868
06/02/2019 04:00:45 step: 7054, epoch: 213, batch: 24, loss: 0.11141744256019592, acc: 96.875, f1: 97.39363999568081, r: 0.6402483968033514
06/02/2019 04:00:45 step: 7059, epoch: 213, batch: 29, loss: 0.2528451085090637, acc: 85.9375, f1: 59.019543118380334, r: 0.7027178548514257
06/02/2019 04:00:45 *** evaluating ***
06/02/2019 04:00:46 step: 214, epoch: 213, acc: 57.692307692307686, f1: 24.81611450615096, r: 0.3604097300349722
06/02/2019 04:00:46 *** epoch: 215 ***
06/02/2019 04:00:46 *** training ***
06/02/2019 04:00:46 step: 7067, epoch: 214, batch: 4, loss: 0.17753827571868896, acc: 92.1875, f1: 76.28168680407487, r: 0.7532799797545351
06/02/2019 04:00:46 step: 7072, epoch: 214, batch: 9, loss: 0.08086860179901123, acc: 95.3125, f1: 90.64713064713065, r: 0.6764904345385117
06/02/2019 04:00:46 step: 7077, epoch: 214, batch: 14, loss: 0.12520600855350494, acc: 93.75, f1: 87.23544973544972, r: 0.5562505849490782
06/02/2019 04:00:47 step: 7082, epoch: 214, batch: 19, loss: 0.07711663842201233, acc: 96.875, f1: 85.3204134366925, r: 0.6737336868196282
06/02/2019 04:00:47 step: 7087, epoch: 214, batch: 24, loss: 0.22723935544490814, acc: 90.625, f1: 85.0944647655174, r: 0.698004567838059
06/02/2019 04:00:47 step: 7092, epoch: 214, batch: 29, loss: 0.15344122052192688, acc: 93.75, f1: 93.75673854447439, r: 0.7136155070508577
06/02/2019 04:00:47 *** evaluating ***
06/02/2019 04:00:48 step: 215, epoch: 214, acc: 60.256410256410255, f1: 24.79238618524333, r: 0.37040168387076244
06/02/2019 04:00:48 *** epoch: 216 ***
06/02/2019 04:00:48 *** training ***
06/02/2019 04:00:48 step: 7100, epoch: 215, batch: 4, loss: 0.2367786467075348, acc: 89.0625, f1: 88.16049006030636, r: 0.6839347184921142
06/02/2019 04:00:48 step: 7105, epoch: 215, batch: 9, loss: 0.14010785520076752, acc: 95.3125, f1: 93.64635364635365, r: 0.6517738028217517
06/02/2019 04:00:48 step: 7110, epoch: 215, batch: 14, loss: 0.20998698472976685, acc: 93.75, f1: 80.0154494032045, r: 0.6813097802538521
06/02/2019 04:00:49 step: 7115, epoch: 215, batch: 19, loss: 0.15501326322555542, acc: 90.625, f1: 74.21209096665487, r: 0.49875119041669536
06/02/2019 04:00:49 step: 7120, epoch: 215, batch: 24, loss: 0.06892409920692444, acc: 98.4375, f1: 82.85714285714285, r: 0.5931221910178405
06/02/2019 04:00:49 step: 7125, epoch: 215, batch: 29, loss: 0.2389964908361435, acc: 90.625, f1: 77.96661193212918, r: 0.5588745682410983
06/02/2019 04:00:49 *** evaluating ***
06/02/2019 04:00:49 step: 216, epoch: 215, acc: 59.82905982905983, f1: 25.393265321625137, r: 0.3595143077271606
06/02/2019 04:00:49 *** epoch: 217 ***
06/02/2019 04:00:49 *** training ***
06/02/2019 04:00:50 step: 7133, epoch: 216, batch: 4, loss: 0.09386592358350754, acc: 96.875, f1: 92.5138174194778, r: 0.6118381004992903
06/02/2019 04:00:50 step: 7138, epoch: 216, batch: 9, loss: 0.12332335859537125, acc: 96.875, f1: 97.6219674045761, r: 0.7730056839512064
06/02/2019 04:00:50 step: 7143, epoch: 216, batch: 14, loss: 0.09500229358673096, acc: 98.4375, f1: 98.66666666666667, r: 0.7346187558880306
06/02/2019 04:00:51 step: 7148, epoch: 216, batch: 19, loss: 0.11209996044635773, acc: 96.875, f1: 97.21800476644361, r: 0.5505219148426423
06/02/2019 04:00:51 step: 7153, epoch: 216, batch: 24, loss: 0.1939125955104828, acc: 92.1875, f1: 88.60963188694281, r: 0.6027400163556698
06/02/2019 04:00:51 step: 7158, epoch: 216, batch: 29, loss: 0.1395321786403656, acc: 98.4375, f1: 98.79862700228833, r: 0.7761920373076151
06/02/2019 04:00:51 *** evaluating ***
06/02/2019 04:00:51 step: 217, epoch: 216, acc: 58.54700854700855, f1: 24.820110435276984, r: 0.34711711671477075
06/02/2019 04:00:51 *** epoch: 218 ***
06/02/2019 04:00:51 *** training ***
06/02/2019 04:00:52 step: 7166, epoch: 217, batch: 4, loss: 0.16129131615161896, acc: 90.625, f1: 75.5719696969697, r: 0.6443948348518691
06/02/2019 04:00:52 step: 7171, epoch: 217, batch: 9, loss: 0.13619428873062134, acc: 95.3125, f1: 80.2756734006734, r: 0.7142590804876374
06/02/2019 04:00:52 step: 7176, epoch: 217, batch: 14, loss: 0.11706025153398514, acc: 96.875, f1: 83.26565768426234, r: 0.5950498918964837
06/02/2019 04:00:53 step: 7181, epoch: 217, batch: 19, loss: 0.16847065091133118, acc: 89.0625, f1: 78.65351347705936, r: 0.7519775345603821
06/02/2019 04:00:53 step: 7186, epoch: 217, batch: 24, loss: 0.17106793820858002, acc: 90.625, f1: 66.01568863943325, r: 0.5695442834208974
06/02/2019 04:00:53 step: 7191, epoch: 217, batch: 29, loss: 0.08127555251121521, acc: 96.875, f1: 93.39105339105338, r: 0.7267182715703211
06/02/2019 04:00:53 *** evaluating ***
06/02/2019 04:00:53 step: 218, epoch: 217, acc: 60.256410256410255, f1: 24.96028491443701, r: 0.3605849350579516
06/02/2019 04:00:53 *** epoch: 219 ***
06/02/2019 04:00:53 *** training ***
06/02/2019 04:00:54 step: 7199, epoch: 218, batch: 4, loss: 0.2069803774356842, acc: 92.1875, f1: 86.53137537066108, r: 0.7120119105276218
06/02/2019 04:00:54 step: 7204, epoch: 218, batch: 9, loss: 0.11610221117734909, acc: 96.875, f1: 96.97027368744072, r: 0.6769442355488747
06/02/2019 04:00:54 step: 7209, epoch: 218, batch: 14, loss: 0.05689670145511627, acc: 98.4375, f1: 95.10204081632652, r: 0.720429781720638
06/02/2019 04:00:54 step: 7214, epoch: 218, batch: 19, loss: 0.2044702023267746, acc: 90.625, f1: 87.05357142857143, r: 0.7528048399650101
06/02/2019 04:00:55 step: 7219, epoch: 218, batch: 24, loss: 0.0779154896736145, acc: 98.4375, f1: 93.33333333333333, r: 0.7332447004478184
06/02/2019 04:00:55 step: 7224, epoch: 218, batch: 29, loss: 0.15561649203300476, acc: 90.625, f1: 76.3301282051282, r: 0.6204520772414129
06/02/2019 04:00:55 *** evaluating ***
06/02/2019 04:00:55 step: 219, epoch: 218, acc: 58.54700854700855, f1: 24.749425015812744, r: 0.3675146680217496
06/02/2019 04:00:55 *** epoch: 220 ***
06/02/2019 04:00:55 *** training ***
06/02/2019 04:00:56 step: 7232, epoch: 219, batch: 4, loss: 0.12264932692050934, acc: 93.75, f1: 88.32594856124268, r: 0.6201966048932354
06/02/2019 04:00:56 step: 7237, epoch: 219, batch: 9, loss: 0.07132648676633835, acc: 98.4375, f1: 99.28337428337429, r: 0.7123705768258433
06/02/2019 04:00:56 step: 7242, epoch: 219, batch: 14, loss: 0.1083347499370575, acc: 95.3125, f1: 79.76293156558812, r: 0.6904510458660762
06/02/2019 04:00:56 step: 7247, epoch: 219, batch: 19, loss: 0.07588997483253479, acc: 95.3125, f1: 90.46140556344639, r: 0.6481028478886197
06/02/2019 04:00:57 step: 7252, epoch: 219, batch: 24, loss: 0.20118075609207153, acc: 90.625, f1: 85.37836067441332, r: 0.7893446500071718
06/02/2019 04:00:57 step: 7257, epoch: 219, batch: 29, loss: 0.1660793572664261, acc: 93.75, f1: 76.40980920670155, r: 0.7212334898411066
06/02/2019 04:00:57 *** evaluating ***
06/02/2019 04:00:57 step: 220, epoch: 219, acc: 60.256410256410255, f1: 25.124016803161187, r: 0.3549044961311856
06/02/2019 04:00:57 *** epoch: 221 ***
06/02/2019 04:00:57 *** training ***
06/02/2019 04:00:57 step: 7265, epoch: 220, batch: 4, loss: 0.13344743847846985, acc: 95.3125, f1: 95.37892578846947, r: 0.8225882447382535
06/02/2019 04:00:58 step: 7270, epoch: 220, batch: 9, loss: 0.16721473634243011, acc: 92.1875, f1: 90.09027524053099, r: 0.7282298419809289
06/02/2019 04:00:58 step: 7275, epoch: 220, batch: 14, loss: 0.06489214301109314, acc: 96.875, f1: 97.02247070668123, r: 0.6668760384405829
06/02/2019 04:00:58 step: 7280, epoch: 220, batch: 19, loss: 0.08517582714557648, acc: 96.875, f1: 83.11111111111111, r: 0.7539018525594087
06/02/2019 04:00:59 step: 7285, epoch: 220, batch: 24, loss: 0.15850196778774261, acc: 93.75, f1: 92.65615337043907, r: 0.6432672954782751
06/02/2019 04:00:59 step: 7290, epoch: 220, batch: 29, loss: 0.04434332624077797, acc: 96.875, f1: 93.30357142857143, r: 0.7621066613466403
06/02/2019 04:00:59 *** evaluating ***
06/02/2019 04:00:59 step: 221, epoch: 220, acc: 59.82905982905983, f1: 24.78225309271061, r: 0.3531103479461549
06/02/2019 04:00:59 *** epoch: 222 ***
06/02/2019 04:00:59 *** training ***
06/02/2019 04:00:59 step: 7298, epoch: 221, batch: 4, loss: 0.10464021563529968, acc: 96.875, f1: 94.62297153310138, r: 0.744213702563448
06/02/2019 04:01:00 step: 7303, epoch: 221, batch: 9, loss: 0.1539754718542099, acc: 95.3125, f1: 91.30036630036629, r: 0.7830282613220919
06/02/2019 04:01:00 step: 7308, epoch: 221, batch: 14, loss: 0.12187503278255463, acc: 93.75, f1: 77.3735119047619, r: 0.6534292973942659
06/02/2019 04:01:00 step: 7313, epoch: 221, batch: 19, loss: 0.1183290034532547, acc: 93.75, f1: 94.83828042058724, r: 0.7856064513119106
06/02/2019 04:01:00 step: 7318, epoch: 221, batch: 24, loss: 0.19461262226104736, acc: 90.625, f1: 84.699554223442, r: 0.7303687282581229
06/02/2019 04:01:01 step: 7323, epoch: 221, batch: 29, loss: 0.20232689380645752, acc: 89.0625, f1: 74.89546376764422, r: 0.7310882680022303
06/02/2019 04:01:01 *** evaluating ***
06/02/2019 04:01:01 step: 222, epoch: 221, acc: 60.256410256410255, f1: 26.145691323087938, r: 0.36680841247615775
06/02/2019 04:01:01 *** epoch: 223 ***
06/02/2019 04:01:01 *** training ***
06/02/2019 04:01:01 step: 7331, epoch: 222, batch: 4, loss: 0.11811444163322449, acc: 95.3125, f1: 93.71626045054037, r: 0.5529408633897351
06/02/2019 04:01:02 step: 7336, epoch: 222, batch: 9, loss: 0.1037294790148735, acc: 96.875, f1: 96.39135491374297, r: 0.7629830683782987
06/02/2019 04:01:02 step: 7341, epoch: 222, batch: 14, loss: 0.08359643816947937, acc: 98.4375, f1: 99.24633936261843, r: 0.7614052767312125
06/02/2019 04:01:02 step: 7346, epoch: 222, batch: 19, loss: 0.20967556536197662, acc: 87.5, f1: 77.31944444444444, r: 0.6286382977110576
06/02/2019 04:01:02 step: 7351, epoch: 222, batch: 24, loss: 0.167527973651886, acc: 90.625, f1: 77.32142763547547, r: 0.6727721796377759
06/02/2019 04:01:03 step: 7356, epoch: 222, batch: 29, loss: 0.14439089596271515, acc: 92.1875, f1: 79.64965696423498, r: 0.6673013388681259
06/02/2019 04:01:03 *** evaluating ***
06/02/2019 04:01:03 step: 223, epoch: 222, acc: 56.41025641025641, f1: 24.812825085303004, r: 0.3487501326857748
06/02/2019 04:01:03 *** epoch: 224 ***
06/02/2019 04:01:03 *** training ***
06/02/2019 04:01:03 step: 7364, epoch: 223, batch: 4, loss: 0.16769102215766907, acc: 92.1875, f1: 87.3982843137255, r: 0.7531686741381196
06/02/2019 04:01:04 step: 7369, epoch: 223, batch: 9, loss: 0.16509780287742615, acc: 93.75, f1: 92.58854177056942, r: 0.6459146722216775
06/02/2019 04:01:04 step: 7374, epoch: 223, batch: 14, loss: 0.2173510491847992, acc: 92.1875, f1: 78.88582826747721, r: 0.6662020108532667
06/02/2019 04:01:04 step: 7379, epoch: 223, batch: 19, loss: 0.1253933161497116, acc: 95.3125, f1: 90.69808910486876, r: 0.5469180998990264
06/02/2019 04:01:04 step: 7384, epoch: 223, batch: 24, loss: 0.14386078715324402, acc: 92.1875, f1: 93.06246390895903, r: 0.6375832784990263
06/02/2019 04:01:05 step: 7389, epoch: 223, batch: 29, loss: 0.2059815227985382, acc: 90.625, f1: 83.95918367346938, r: 0.5764038396996696
06/02/2019 04:01:05 *** evaluating ***
06/02/2019 04:01:05 step: 224, epoch: 223, acc: 60.256410256410255, f1: 26.187219405380457, r: 0.3645863853633054
06/02/2019 04:01:05 *** epoch: 225 ***
06/02/2019 04:01:05 *** training ***
06/02/2019 04:01:05 step: 7397, epoch: 224, batch: 4, loss: 0.14326059818267822, acc: 93.75, f1: 90.54771396344195, r: 0.5558939014077006
06/02/2019 04:01:06 step: 7402, epoch: 224, batch: 9, loss: 0.17857125401496887, acc: 92.1875, f1: 67.66697790227202, r: 0.6026867681882475
06/02/2019 04:01:06 step: 7407, epoch: 224, batch: 14, loss: 0.14883503317832947, acc: 95.3125, f1: 94.15178571428572, r: 0.7465206123456287
06/02/2019 04:01:06 step: 7412, epoch: 224, batch: 19, loss: 0.13166463375091553, acc: 95.3125, f1: 89.19258179462261, r: 0.6655569842257149
06/02/2019 04:01:07 step: 7417, epoch: 224, batch: 24, loss: 0.06402457505464554, acc: 96.875, f1: 94.73304473304474, r: 0.5640643304436421
06/02/2019 04:01:07 step: 7422, epoch: 224, batch: 29, loss: 0.09553144127130508, acc: 96.875, f1: 92.14285714285715, r: 0.6157172493727979
06/02/2019 04:01:07 *** evaluating ***
06/02/2019 04:01:07 step: 225, epoch: 224, acc: 58.119658119658126, f1: 24.605772199396778, r: 0.357110578289904
06/02/2019 04:01:07 *** epoch: 226 ***
06/02/2019 04:01:07 *** training ***
06/02/2019 04:01:07 step: 7430, epoch: 225, batch: 4, loss: 0.0821356400847435, acc: 96.875, f1: 94.96918767507003, r: 0.7936898020714542
06/02/2019 04:01:08 step: 7435, epoch: 225, batch: 9, loss: 0.16732501983642578, acc: 92.1875, f1: 86.62651709775334, r: 0.64053422222028
06/02/2019 04:01:08 step: 7440, epoch: 225, batch: 14, loss: 0.17216646671295166, acc: 92.1875, f1: 89.130291005291, r: 0.6905060877114212
06/02/2019 04:01:08 step: 7445, epoch: 225, batch: 19, loss: 0.1575809121131897, acc: 95.3125, f1: 91.23719015908249, r: 0.7149309674758502
06/02/2019 04:01:09 step: 7450, epoch: 225, batch: 24, loss: 0.06470517069101334, acc: 98.4375, f1: 98.08018068887634, r: 0.6158087072518076
06/02/2019 04:01:09 step: 7455, epoch: 225, batch: 29, loss: 0.11115781217813492, acc: 98.4375, f1: 94.61697722567288, r: 0.6776531433073115
06/02/2019 04:01:09 *** evaluating ***
06/02/2019 04:01:09 step: 226, epoch: 225, acc: 59.82905982905983, f1: 25.34442454374209, r: 0.34382143765783546
06/02/2019 04:01:09 *** epoch: 227 ***
06/02/2019 04:01:09 *** training ***
06/02/2019 04:01:10 step: 7463, epoch: 226, batch: 4, loss: 0.10882315039634705, acc: 95.3125, f1: 93.30316742081448, r: 0.8107938578821895
06/02/2019 04:01:10 step: 7468, epoch: 226, batch: 9, loss: 0.21715505421161652, acc: 92.1875, f1: 82.81133378116137, r: 0.6407431915597916
06/02/2019 04:01:10 step: 7473, epoch: 226, batch: 14, loss: 0.18352994322776794, acc: 90.625, f1: 90.41880641145347, r: 0.7653518835013668
06/02/2019 04:01:10 step: 7478, epoch: 226, batch: 19, loss: 0.11910226941108704, acc: 95.3125, f1: 93.94474393530997, r: 0.6003542846763544
06/02/2019 04:01:11 step: 7483, epoch: 226, batch: 24, loss: 0.14305326342582703, acc: 96.875, f1: 97.67482517482517, r: 0.7177382910997883
06/02/2019 04:01:11 step: 7488, epoch: 226, batch: 29, loss: 0.23556950688362122, acc: 89.0625, f1: 85.41601255886971, r: 0.594506717311822
06/02/2019 04:01:11 *** evaluating ***
06/02/2019 04:01:11 step: 227, epoch: 226, acc: 59.82905982905983, f1: 24.848157481616127, r: 0.3597009408517984
06/02/2019 04:01:11 *** epoch: 228 ***
06/02/2019 04:01:11 *** training ***
06/02/2019 04:01:12 step: 7496, epoch: 227, batch: 4, loss: 0.23142790794372559, acc: 85.9375, f1: 66.27331577792408, r: 0.6375238714722647
06/02/2019 04:01:12 step: 7501, epoch: 227, batch: 9, loss: 0.1294977068901062, acc: 95.3125, f1: 96.4829598506069, r: 0.7835701048121868
06/02/2019 04:01:12 step: 7506, epoch: 227, batch: 14, loss: 0.10374413430690765, acc: 96.875, f1: 96.36893718526372, r: 0.6891702554189857
06/02/2019 04:01:12 step: 7511, epoch: 227, batch: 19, loss: 0.1995951533317566, acc: 92.1875, f1: 77.56176999101527, r: 0.7696152185664026
06/02/2019 04:01:13 step: 7516, epoch: 227, batch: 24, loss: 0.12491529434919357, acc: 95.3125, f1: 80.69349845201239, r: 0.6375523751925452
06/02/2019 04:01:13 step: 7521, epoch: 227, batch: 29, loss: 0.10334932059049606, acc: 95.3125, f1: 90.62238228624783, r: 0.6785671050221678
06/02/2019 04:01:13 *** evaluating ***
06/02/2019 04:01:13 step: 228, epoch: 227, acc: 59.401709401709404, f1: 25.082173473089338, r: 0.35924253409243306
06/02/2019 04:01:13 *** epoch: 229 ***
06/02/2019 04:01:13 *** training ***
06/02/2019 04:01:14 step: 7529, epoch: 228, batch: 4, loss: 0.14312393963336945, acc: 95.3125, f1: 96.6032417195208, r: 0.7811713255339534
06/02/2019 04:01:14 step: 7534, epoch: 228, batch: 9, loss: 0.0939866155385971, acc: 98.4375, f1: 98.44026940801133, r: 0.6723527124377259
06/02/2019 04:01:14 step: 7539, epoch: 228, batch: 14, loss: 0.06743529438972473, acc: 98.4375, f1: 98.36734693877551, r: 0.68274948343704
06/02/2019 04:01:14 step: 7544, epoch: 228, batch: 19, loss: 0.11795222759246826, acc: 95.3125, f1: 91.27250409165303, r: 0.7291667099132676
06/02/2019 04:01:15 step: 7549, epoch: 228, batch: 24, loss: 0.22504711151123047, acc: 87.5, f1: 83.97810522810522, r: 0.71290598177566
06/02/2019 04:01:15 step: 7554, epoch: 228, batch: 29, loss: 0.10906998813152313, acc: 93.75, f1: 95.52154195011339, r: 0.6729997052422273
06/02/2019 04:01:15 *** evaluating ***
06/02/2019 04:01:15 step: 229, epoch: 228, acc: 60.68376068376068, f1: 26.0866587623012, r: 0.37166208013656804
06/02/2019 04:01:15 *** epoch: 230 ***
06/02/2019 04:01:15 *** training ***
06/02/2019 04:01:16 step: 7562, epoch: 229, batch: 4, loss: 0.22634010016918182, acc: 85.9375, f1: 71.87677110471228, r: 0.6456037179888039
06/02/2019 04:01:16 step: 7567, epoch: 229, batch: 9, loss: 0.13048091530799866, acc: 92.1875, f1: 86.20847364856716, r: 0.6515382999697341
06/02/2019 04:01:16 step: 7572, epoch: 229, batch: 14, loss: 0.16523662209510803, acc: 89.0625, f1: 88.1277385228753, r: 0.6806065576582923
06/02/2019 04:01:16 step: 7577, epoch: 229, batch: 19, loss: 0.05617167800664902, acc: 98.4375, f1: 97.38775510204081, r: 0.6309583626948889
06/02/2019 04:01:17 step: 7582, epoch: 229, batch: 24, loss: 0.18601404130458832, acc: 89.0625, f1: 89.04464816229523, r: 0.7824160950567844
06/02/2019 04:01:17 step: 7587, epoch: 229, batch: 29, loss: 0.14629626274108887, acc: 96.875, f1: 96.65704665704665, r: 0.7086009652933069
06/02/2019 04:01:17 *** evaluating ***
06/02/2019 04:01:17 step: 230, epoch: 229, acc: 61.53846153846154, f1: 27.81784188034188, r: 0.36132450332309735
06/02/2019 04:01:17 *** epoch: 231 ***
06/02/2019 04:01:17 *** training ***
06/02/2019 04:01:17 step: 7595, epoch: 230, batch: 4, loss: 0.12727288901805878, acc: 93.75, f1: 80.83162451583503, r: 0.5990225693627728
06/02/2019 04:01:18 step: 7600, epoch: 230, batch: 9, loss: 0.23489654064178467, acc: 93.75, f1: 89.8248494800219, r: 0.6547201504535022
06/02/2019 04:01:18 step: 7605, epoch: 230, batch: 14, loss: 0.07535393536090851, acc: 98.4375, f1: 97.73242630385488, r: 0.6269998632398169
06/02/2019 04:01:18 step: 7610, epoch: 230, batch: 19, loss: 0.10514871031045914, acc: 96.875, f1: 93.11658456486043, r: 0.7833372829746796
06/02/2019 04:01:19 step: 7615, epoch: 230, batch: 24, loss: 0.10356254875659943, acc: 96.875, f1: 84.50396825396825, r: 0.7106000147671292
06/02/2019 04:01:19 step: 7620, epoch: 230, batch: 29, loss: 0.1481037735939026, acc: 92.1875, f1: 79.1396103896104, r: 0.7351769118985632
06/02/2019 04:01:19 *** evaluating ***
06/02/2019 04:01:19 step: 231, epoch: 230, acc: 59.401709401709404, f1: 25.180276199035458, r: 0.34610737267522423
06/02/2019 04:01:19 *** epoch: 232 ***
06/02/2019 04:01:19 *** training ***
06/02/2019 04:01:19 step: 7628, epoch: 231, batch: 4, loss: 0.0760916918516159, acc: 98.4375, f1: 97.28070175438597, r: 0.7490682500287951
06/02/2019 04:01:20 step: 7633, epoch: 231, batch: 9, loss: 0.24383509159088135, acc: 85.9375, f1: 61.54154194951158, r: 0.6015596893433965
06/02/2019 04:01:20 step: 7638, epoch: 231, batch: 14, loss: 0.16906121373176575, acc: 90.625, f1: 90.94491129785247, r: 0.7886973968845138
06/02/2019 04:01:20 step: 7643, epoch: 231, batch: 19, loss: 0.07991622388362885, acc: 98.4375, f1: 99.23521913913127, r: 0.6203336629760963
06/02/2019 04:01:21 step: 7648, epoch: 231, batch: 24, loss: 0.17691877484321594, acc: 90.625, f1: 78.26642862590015, r: 0.7056795677736108
06/02/2019 04:01:21 step: 7653, epoch: 231, batch: 29, loss: 0.12966176867485046, acc: 93.75, f1: 90.50905160831331, r: 0.6670836925527357
06/02/2019 04:01:21 *** evaluating ***
06/02/2019 04:01:21 step: 232, epoch: 231, acc: 59.82905982905983, f1: 24.831897831897834, r: 0.35884770297508284
06/02/2019 04:01:21 *** epoch: 233 ***
06/02/2019 04:01:21 *** training ***
06/02/2019 04:01:21 step: 7661, epoch: 232, batch: 4, loss: 0.07883895188570023, acc: 96.875, f1: 94.55980964741511, r: 0.65926090705488
06/02/2019 04:01:22 step: 7666, epoch: 232, batch: 9, loss: 0.208457350730896, acc: 89.0625, f1: 76.03459469592192, r: 0.6335559927370508
06/02/2019 04:01:22 step: 7671, epoch: 232, batch: 14, loss: 0.13391752541065216, acc: 95.3125, f1: 81.27250409165303, r: 0.7171514028634853
06/02/2019 04:01:22 step: 7676, epoch: 232, batch: 19, loss: 0.11193278431892395, acc: 93.75, f1: 92.81498015873015, r: 0.7719230419330423
06/02/2019 04:01:23 step: 7681, epoch: 232, batch: 24, loss: 0.11068865656852722, acc: 98.4375, f1: 97.94941900205059, r: 0.7663404133020841
06/02/2019 04:01:23 step: 7686, epoch: 232, batch: 29, loss: 0.09716169536113739, acc: 96.875, f1: 84.18968313278124, r: 0.588640171530288
06/02/2019 04:01:23 *** evaluating ***
06/02/2019 04:01:23 step: 233, epoch: 232, acc: 60.256410256410255, f1: 25.222738394717194, r: 0.352602941119843
06/02/2019 04:01:23 *** epoch: 234 ***
06/02/2019 04:01:23 *** training ***
06/02/2019 04:01:23 step: 7694, epoch: 233, batch: 4, loss: 0.11479756981134415, acc: 95.3125, f1: 83.71428571428572, r: 0.62584705197061
06/02/2019 04:01:24 step: 7699, epoch: 233, batch: 9, loss: 0.1817885935306549, acc: 92.1875, f1: 88.65345647260541, r: 0.7411083120616379
06/02/2019 04:01:24 step: 7704, epoch: 233, batch: 14, loss: 0.130729079246521, acc: 93.75, f1: 81.50533676849466, r: 0.6999566646620288
06/02/2019 04:01:24 step: 7709, epoch: 233, batch: 19, loss: 0.05024725943803787, acc: 100.0, f1: 100.0, r: 0.5763651427267522
06/02/2019 04:01:25 step: 7714, epoch: 233, batch: 24, loss: 0.09220626950263977, acc: 96.875, f1: 96.82539682539682, r: 0.6570486842062638
06/02/2019 04:01:25 step: 7719, epoch: 233, batch: 29, loss: 0.13278831541538239, acc: 93.75, f1: 89.37756920950196, r: 0.6643525954545352
06/02/2019 04:01:25 *** evaluating ***
06/02/2019 04:01:25 step: 234, epoch: 233, acc: 60.256410256410255, f1: 26.097153318418393, r: 0.3581269645599661
06/02/2019 04:01:25 *** epoch: 235 ***
06/02/2019 04:01:25 *** training ***
06/02/2019 04:01:25 step: 7727, epoch: 234, batch: 4, loss: 0.20843075215816498, acc: 90.625, f1: 84.21124197439987, r: 0.7299702688204288
06/02/2019 04:01:26 step: 7732, epoch: 234, batch: 9, loss: 0.081168532371521, acc: 98.4375, f1: 99.21115921115921, r: 0.7028239935527435
06/02/2019 04:01:26 step: 7737, epoch: 234, batch: 14, loss: 0.11928992718458176, acc: 93.75, f1: 88.25280112044818, r: 0.7570921842508492
06/02/2019 04:01:26 step: 7742, epoch: 234, batch: 19, loss: 0.13344494998455048, acc: 93.75, f1: 84.20948839305838, r: 0.7304657408587948
06/02/2019 04:01:26 step: 7747, epoch: 234, batch: 24, loss: 0.14898186922073364, acc: 90.625, f1: 77.22055047283011, r: 0.666238066559712
06/02/2019 04:01:27 step: 7752, epoch: 234, batch: 29, loss: 0.1359121948480606, acc: 93.75, f1: 91.83417910992662, r: 0.6777267792420715
06/02/2019 04:01:27 *** evaluating ***
06/02/2019 04:01:27 step: 235, epoch: 234, acc: 59.82905982905983, f1: 25.085570576288397, r: 0.35699392957235865
06/02/2019 04:01:27 *** epoch: 236 ***
06/02/2019 04:01:27 *** training ***
06/02/2019 04:01:27 step: 7760, epoch: 235, batch: 4, loss: 0.1432168036699295, acc: 95.3125, f1: 92.05919209789178, r: 0.7448292849553526
06/02/2019 04:01:28 step: 7765, epoch: 235, batch: 9, loss: 0.11623113602399826, acc: 93.75, f1: 93.50566787922779, r: 0.7163918176945333
06/02/2019 04:01:28 step: 7770, epoch: 235, batch: 14, loss: 0.1218734160065651, acc: 93.75, f1: 91.00733781584846, r: 0.6738851508399697
06/02/2019 04:01:28 step: 7775, epoch: 235, batch: 19, loss: 0.13704299926757812, acc: 95.3125, f1: 91.9064269064269, r: 0.7174232334880922
06/02/2019 04:01:28 step: 7780, epoch: 235, batch: 24, loss: 0.08436684310436249, acc: 96.875, f1: 93.4740391883249, r: 0.6677922844454616
06/02/2019 04:01:29 step: 7785, epoch: 235, batch: 29, loss: 0.129456028342247, acc: 93.75, f1: 92.5792957042957, r: 0.7692556099545252
06/02/2019 04:01:29 *** evaluating ***
06/02/2019 04:01:29 step: 236, epoch: 235, acc: 59.401709401709404, f1: 25.123236182397303, r: 0.34971801746116415
06/02/2019 04:01:29 *** epoch: 237 ***
06/02/2019 04:01:29 *** training ***
06/02/2019 04:01:29 step: 7793, epoch: 236, batch: 4, loss: 0.18585827946662903, acc: 89.0625, f1: 85.46479044834308, r: 0.7595637955306969
06/02/2019 04:01:30 step: 7798, epoch: 236, batch: 9, loss: 0.12625253200531006, acc: 95.3125, f1: 95.81916329284749, r: 0.7676315813221177
06/02/2019 04:01:30 step: 7803, epoch: 236, batch: 14, loss: 0.15175597369670868, acc: 93.75, f1: 91.40179995443152, r: 0.6573212536080073
06/02/2019 04:01:30 step: 7808, epoch: 236, batch: 19, loss: 0.16551755368709564, acc: 93.75, f1: 92.14855320118478, r: 0.6766069299000457
06/02/2019 04:01:31 step: 7813, epoch: 236, batch: 24, loss: 0.11699749529361725, acc: 92.1875, f1: 72.81647116324535, r: 0.6598275376716598
06/02/2019 04:01:31 step: 7818, epoch: 236, batch: 29, loss: 0.08821482211351395, acc: 95.3125, f1: 93.8045634920635, r: 0.6853569898855647
06/02/2019 04:01:31 *** evaluating ***
06/02/2019 04:01:31 step: 237, epoch: 236, acc: 58.97435897435898, f1: 24.901319400595114, r: 0.3518400933939702
06/02/2019 04:01:31 *** epoch: 238 ***
06/02/2019 04:01:31 *** training ***
06/02/2019 04:01:31 step: 7826, epoch: 237, batch: 4, loss: 0.15870393812656403, acc: 92.1875, f1: 91.03673888557609, r: 0.7087757522904821
06/02/2019 04:01:32 step: 7831, epoch: 237, batch: 9, loss: 0.13048890233039856, acc: 95.3125, f1: 80.80172855782612, r: 0.8126343457241112
06/02/2019 04:01:32 step: 7836, epoch: 237, batch: 14, loss: 0.157631978392601, acc: 95.3125, f1: 78.66874389051807, r: 0.6290894862557713
06/02/2019 04:01:32 step: 7841, epoch: 237, batch: 19, loss: 0.15490297973155975, acc: 92.1875, f1: 85.29220779220779, r: 0.6496296145381542
06/02/2019 04:01:33 step: 7846, epoch: 237, batch: 24, loss: 0.13884340226650238, acc: 93.75, f1: 86.82277908664716, r: 0.6729500137433257
06/02/2019 04:01:33 step: 7851, epoch: 237, batch: 29, loss: 0.12283105403184891, acc: 95.3125, f1: 91.13505814225003, r: 0.5929126193913704
06/02/2019 04:01:33 *** evaluating ***
06/02/2019 04:01:33 step: 238, epoch: 237, acc: 58.54700854700855, f1: 24.917818601655643, r: 0.3483724022866609
06/02/2019 04:01:33 *** epoch: 239 ***
06/02/2019 04:01:33 *** training ***
06/02/2019 04:01:34 step: 7859, epoch: 238, batch: 4, loss: 0.06065654754638672, acc: 98.4375, f1: 99.39620471535365, r: 0.7881511344830421
06/02/2019 04:01:34 step: 7864, epoch: 238, batch: 9, loss: 0.2473020851612091, acc: 85.9375, f1: 69.95308857808857, r: 0.7548747427539435
06/02/2019 04:01:34 step: 7869, epoch: 238, batch: 14, loss: 0.17466333508491516, acc: 93.75, f1: 88.93939393939394, r: 0.6702411484836815
06/02/2019 04:01:34 step: 7874, epoch: 238, batch: 19, loss: 0.1483639031648636, acc: 93.75, f1: 92.29878364389234, r: 0.7561057706043619
06/02/2019 04:01:35 step: 7879, epoch: 238, batch: 24, loss: 0.09835325926542282, acc: 96.875, f1: 97.1687722708131, r: 0.695022757345384
06/02/2019 04:01:35 step: 7884, epoch: 238, batch: 29, loss: 0.14086002111434937, acc: 92.1875, f1: 88.7696916373387, r: 0.7210839632465351
06/02/2019 04:01:35 *** evaluating ***
06/02/2019 04:01:35 step: 239, epoch: 238, acc: 59.401709401709404, f1: 24.679372434274395, r: 0.3548895268718563
06/02/2019 04:01:35 *** epoch: 240 ***
06/02/2019 04:01:35 *** training ***
06/02/2019 04:01:36 step: 7892, epoch: 239, batch: 4, loss: 0.09262706339359283, acc: 95.3125, f1: 91.94848262563282, r: 0.7770879516115408
06/02/2019 04:01:36 step: 7897, epoch: 239, batch: 9, loss: 0.10710100829601288, acc: 95.3125, f1: 90.17384731670445, r: 0.6839323444310187
06/02/2019 04:01:36 step: 7902, epoch: 239, batch: 14, loss: 0.15996527671813965, acc: 89.0625, f1: 80.02470355731226, r: 0.7343683521928563
06/02/2019 04:01:36 step: 7907, epoch: 239, batch: 19, loss: 0.16853487491607666, acc: 90.625, f1: 72.24435286935287, r: 0.7487040703251827
06/02/2019 04:01:37 step: 7912, epoch: 239, batch: 24, loss: 0.12133728712797165, acc: 95.3125, f1: 89.7274294333118, r: 0.5732527353158567
06/02/2019 04:01:37 step: 7917, epoch: 239, batch: 29, loss: 0.14233925938606262, acc: 93.75, f1: 92.69772023641994, r: 0.7121285730261679
06/02/2019 04:01:37 *** evaluating ***
06/02/2019 04:01:37 step: 240, epoch: 239, acc: 59.82905982905983, f1: 25.661433804290944, r: 0.3566721660375955
06/02/2019 04:01:37 *** epoch: 241 ***
06/02/2019 04:01:37 *** training ***
06/02/2019 04:01:38 step: 7925, epoch: 240, batch: 4, loss: 0.09552697092294693, acc: 96.875, f1: 84.85861964122834, r: 0.7583362456268987
06/02/2019 04:01:38 step: 7930, epoch: 240, batch: 9, loss: 0.20016348361968994, acc: 90.625, f1: 63.99794854662335, r: 0.5923748130465456
06/02/2019 04:01:38 step: 7935, epoch: 240, batch: 14, loss: 0.13559716939926147, acc: 95.3125, f1: 80.89959481060184, r: 0.6219100623633136
06/02/2019 04:01:38 step: 7940, epoch: 240, batch: 19, loss: 0.10720590502023697, acc: 95.3125, f1: 93.77551020408163, r: 0.6691657180507685
06/02/2019 04:01:39 step: 7945, epoch: 240, batch: 24, loss: 0.1341080367565155, acc: 95.3125, f1: 95.86218247982954, r: 0.7737682757909472
06/02/2019 04:01:39 step: 7950, epoch: 240, batch: 29, loss: 0.11340001225471497, acc: 95.3125, f1: 92.08089417312311, r: 0.6459649883865098
06/02/2019 04:01:39 *** evaluating ***
06/02/2019 04:01:39 step: 241, epoch: 240, acc: 60.256410256410255, f1: 24.969017094017097, r: 0.36357129149752826
06/02/2019 04:01:39 *** epoch: 242 ***
06/02/2019 04:01:39 *** training ***
06/02/2019 04:01:40 step: 7958, epoch: 241, batch: 4, loss: 0.16135114431381226, acc: 92.1875, f1: 77.29333166833166, r: 0.7963439813781124
06/02/2019 04:01:40 step: 7963, epoch: 241, batch: 9, loss: 0.13009557127952576, acc: 96.875, f1: 96.88854489164086, r: 0.5106862200002995
06/02/2019 04:01:40 step: 7968, epoch: 241, batch: 14, loss: 0.1210881844162941, acc: 90.625, f1: 92.07393483709274, r: 0.7805658157831649
06/02/2019 04:01:40 step: 7973, epoch: 241, batch: 19, loss: 0.1681111752986908, acc: 95.3125, f1: 94.08050443764729, r: 0.6953680337224015
06/02/2019 04:01:41 step: 7978, epoch: 241, batch: 24, loss: 0.09797897934913635, acc: 95.3125, f1: 94.3982010806738, r: 0.6767620547923482
06/02/2019 04:01:41 step: 7983, epoch: 241, batch: 29, loss: 0.16998642683029175, acc: 90.625, f1: 76.67560887388474, r: 0.7365271065700182
06/02/2019 04:01:41 *** evaluating ***
06/02/2019 04:01:41 step: 242, epoch: 241, acc: 57.692307692307686, f1: 24.42691109513539, r: 0.3430326041848429
06/02/2019 04:01:41 *** epoch: 243 ***
06/02/2019 04:01:41 *** training ***
06/02/2019 04:01:41 step: 7991, epoch: 242, batch: 4, loss: 0.22969649732112885, acc: 89.0625, f1: 81.2483164983165, r: 0.7312509726969465
06/02/2019 04:01:42 step: 7996, epoch: 242, batch: 9, loss: 0.12873107194900513, acc: 96.875, f1: 95.56935817805383, r: 0.6384968784482655
06/02/2019 04:01:42 step: 8001, epoch: 242, batch: 14, loss: 0.09917601943016052, acc: 96.875, f1: 84.73404255319149, r: 0.7144403022356446
06/02/2019 04:01:42 step: 8006, epoch: 242, batch: 19, loss: 0.15199199318885803, acc: 92.1875, f1: 66.96047430830039, r: 0.6587240676581694
06/02/2019 04:01:43 step: 8011, epoch: 242, batch: 24, loss: 0.15549227595329285, acc: 90.625, f1: 80.7564330079858, r: 0.6610232505885685
06/02/2019 04:01:43 step: 8016, epoch: 242, batch: 29, loss: 0.09049690514802933, acc: 95.3125, f1: 82.72594364699628, r: 0.7477137827749064
06/02/2019 04:01:43 *** evaluating ***
06/02/2019 04:01:43 step: 243, epoch: 242, acc: 59.401709401709404, f1: 26.101009486194528, r: 0.35688339961827253
06/02/2019 04:01:43 *** epoch: 244 ***
06/02/2019 04:01:43 *** training ***
06/02/2019 04:01:43 step: 8024, epoch: 243, batch: 4, loss: 0.1342240571975708, acc: 95.3125, f1: 83.56366459627328, r: 0.7210467458745787
06/02/2019 04:01:44 step: 8029, epoch: 243, batch: 9, loss: 0.1395368129014969, acc: 95.3125, f1: 88.89827327327328, r: 0.6934314229700016
06/02/2019 04:01:44 step: 8034, epoch: 243, batch: 14, loss: 0.13270901143550873, acc: 93.75, f1: 83.51749318938553, r: 0.7549492752931697
06/02/2019 04:01:44 step: 8039, epoch: 243, batch: 19, loss: 0.031162746250629425, acc: 100.0, f1: 100.0, r: 0.6088349518661103
06/02/2019 04:01:45 step: 8044, epoch: 243, batch: 24, loss: 0.06619122624397278, acc: 98.4375, f1: 97.69944341372913, r: 0.6910844959206017
06/02/2019 04:01:45 step: 8049, epoch: 243, batch: 29, loss: 0.12634119391441345, acc: 92.1875, f1: 74.66200595070565, r: 0.6207936236025112
06/02/2019 04:01:45 *** evaluating ***
06/02/2019 04:01:45 step: 244, epoch: 243, acc: 58.97435897435898, f1: 24.970426646699202, r: 0.3579390862483532
06/02/2019 04:01:45 *** epoch: 245 ***
06/02/2019 04:01:45 *** training ***
06/02/2019 04:01:45 step: 8057, epoch: 244, batch: 4, loss: 0.1590292751789093, acc: 92.1875, f1: 86.1001460334052, r: 0.6286196686920472
06/02/2019 04:01:46 step: 8062, epoch: 244, batch: 9, loss: 0.1472269892692566, acc: 92.1875, f1: 84.22505017332604, r: 0.6992665064433004
06/02/2019 04:01:46 step: 8067, epoch: 244, batch: 14, loss: 0.11732300370931625, acc: 95.3125, f1: 92.18765730393638, r: 0.7552456452154159
06/02/2019 04:01:46 step: 8072, epoch: 244, batch: 19, loss: 0.10489775985479355, acc: 95.3125, f1: 93.32358674463939, r: 0.8042866873235888
06/02/2019 04:01:47 step: 8077, epoch: 244, batch: 24, loss: 0.1431064009666443, acc: 92.1875, f1: 85.76822997308173, r: 0.6268597698637215
06/02/2019 04:01:47 step: 8082, epoch: 244, batch: 29, loss: 0.09318871796131134, acc: 95.3125, f1: 96.46927914852444, r: 0.7709715619215068
06/02/2019 04:01:47 *** evaluating ***
06/02/2019 04:01:47 step: 245, epoch: 244, acc: 58.97435897435898, f1: 24.970426646699202, r: 0.3415886019042901
06/02/2019 04:01:47 *** epoch: 246 ***
06/02/2019 04:01:47 *** training ***
06/02/2019 04:01:47 step: 8090, epoch: 245, batch: 4, loss: 0.07958579808473587, acc: 98.4375, f1: 93.65079365079366, r: 0.6663292834799972
06/02/2019 04:01:48 step: 8095, epoch: 245, batch: 9, loss: 0.09926392883062363, acc: 96.875, f1: 95.83860583860584, r: 0.7721470936141932
06/02/2019 04:01:48 step: 8100, epoch: 245, batch: 14, loss: 0.18090194463729858, acc: 92.1875, f1: 88.20470717022442, r: 0.6095545740712882
06/02/2019 04:01:48 step: 8105, epoch: 245, batch: 19, loss: 0.1706744134426117, acc: 90.625, f1: 87.78997659225098, r: 0.7420748054618692
06/02/2019 04:01:48 step: 8110, epoch: 245, batch: 24, loss: 0.14130379259586334, acc: 95.3125, f1: 88.90798424633012, r: 0.6445941506924298
06/02/2019 04:01:49 step: 8115, epoch: 245, batch: 29, loss: 0.1380605548620224, acc: 93.75, f1: 88.3816198544195, r: 0.6604778597545052
06/02/2019 04:01:49 *** evaluating ***
06/02/2019 04:01:49 step: 246, epoch: 245, acc: 58.119658119658126, f1: 24.58890710244174, r: 0.34956077234539384
06/02/2019 04:01:49 *** epoch: 247 ***
06/02/2019 04:01:49 *** training ***
06/02/2019 04:01:49 step: 8123, epoch: 246, batch: 4, loss: 0.1501653641462326, acc: 93.75, f1: 87.45633737365914, r: 0.5936133836205173
06/02/2019 04:01:49 step: 8128, epoch: 246, batch: 9, loss: 0.15245606005191803, acc: 93.75, f1: 91.00568655555868, r: 0.6737708081191013
06/02/2019 04:01:50 step: 8133, epoch: 246, batch: 14, loss: 0.07650000602006912, acc: 96.875, f1: 94.55691367456073, r: 0.7610694893350518
06/02/2019 04:01:50 step: 8138, epoch: 246, batch: 19, loss: 0.11090371757745743, acc: 95.3125, f1: 94.3268921095008, r: 0.7706569227805437
06/02/2019 04:01:50 step: 8143, epoch: 246, batch: 24, loss: 0.10043807327747345, acc: 98.4375, f1: 98.1111111111111, r: 0.770909314115488
06/02/2019 04:01:51 step: 8148, epoch: 246, batch: 29, loss: 0.0743979811668396, acc: 98.4375, f1: 95.55555555555556, r: 0.6381395021944438
06/02/2019 04:01:51 *** evaluating ***
06/02/2019 04:01:51 step: 247, epoch: 246, acc: 58.54700854700855, f1: 26.03138721603398, r: 0.35539547549980727
06/02/2019 04:01:51 *** epoch: 248 ***
06/02/2019 04:01:51 *** training ***
06/02/2019 04:01:51 step: 8156, epoch: 247, batch: 4, loss: 0.1563834846019745, acc: 95.3125, f1: 89.99999999999999, r: 0.6784187074268377
06/02/2019 04:01:51 step: 8161, epoch: 247, batch: 9, loss: 0.07730775326490402, acc: 96.875, f1: 84.46115288220551, r: 0.7675276404915334
06/02/2019 04:01:52 step: 8166, epoch: 247, batch: 14, loss: 0.16340327262878418, acc: 92.1875, f1: 85.7520325203252, r: 0.6044248566813255
06/02/2019 04:01:52 step: 8171, epoch: 247, batch: 19, loss: 0.19028335809707642, acc: 92.1875, f1: 86.38187112451818, r: 0.6049711428638334
06/02/2019 04:01:52 step: 8176, epoch: 247, batch: 24, loss: 0.14920549094676971, acc: 95.3125, f1: 93.87960829493088, r: 0.7479253629029138
06/02/2019 04:01:52 step: 8181, epoch: 247, batch: 29, loss: 0.11721424013376236, acc: 96.875, f1: 86.36363636363636, r: 0.8096799895428793
06/02/2019 04:01:53 *** evaluating ***
06/02/2019 04:01:53 step: 248, epoch: 247, acc: 58.97435897435898, f1: 24.727414321843476, r: 0.35194334026426743
06/02/2019 04:01:53 *** epoch: 249 ***
06/02/2019 04:01:53 *** training ***
06/02/2019 04:01:53 step: 8189, epoch: 248, batch: 4, loss: 0.06348461657762527, acc: 100.0, f1: 100.0, r: 0.7584631113837098
06/02/2019 04:01:53 step: 8194, epoch: 248, batch: 9, loss: 0.11638442426919937, acc: 93.75, f1: 89.91125541125541, r: 0.8027713792165049
06/02/2019 04:01:53 step: 8199, epoch: 248, batch: 14, loss: 0.14417506754398346, acc: 92.1875, f1: 86.02819628429384, r: 0.7137455127450313
06/02/2019 04:01:54 step: 8204, epoch: 248, batch: 19, loss: 0.0524119958281517, acc: 98.4375, f1: 99.37395937395938, r: 0.7166006701450893
06/02/2019 04:01:54 step: 8209, epoch: 248, batch: 24, loss: 0.1265920102596283, acc: 95.3125, f1: 91.34021132376395, r: 0.7517660409934284
06/02/2019 04:01:54 step: 8214, epoch: 248, batch: 29, loss: 0.20301377773284912, acc: 87.5, f1: 70.41474670217865, r: 0.6018197596132107
06/02/2019 04:01:54 *** evaluating ***
06/02/2019 04:01:55 step: 249, epoch: 248, acc: 58.119658119658126, f1: 24.595994238151103, r: 0.350320452540239
06/02/2019 04:01:55 *** epoch: 250 ***
06/02/2019 04:01:55 *** training ***
06/02/2019 04:01:55 step: 8222, epoch: 249, batch: 4, loss: 0.11918947845697403, acc: 93.75, f1: 85.69922830792395, r: 0.6278171636984636
06/02/2019 04:01:55 step: 8227, epoch: 249, batch: 9, loss: 0.09926865249872208, acc: 95.3125, f1: 90.05127050820329, r: 0.7033635086979995
06/02/2019 04:01:55 step: 8232, epoch: 249, batch: 14, loss: 0.20598673820495605, acc: 90.625, f1: 79.86453587063342, r: 0.6690244500769053
06/02/2019 04:01:56 step: 8237, epoch: 249, batch: 19, loss: 0.11474640667438507, acc: 96.875, f1: 91.3298652794451, r: 0.6803221191729405
06/02/2019 04:01:56 step: 8242, epoch: 249, batch: 24, loss: 0.14850148558616638, acc: 93.75, f1: 94.17210240739652, r: 0.7094955355428124
06/02/2019 04:01:56 step: 8247, epoch: 249, batch: 29, loss: 0.10814067721366882, acc: 95.3125, f1: 92.90421455938697, r: 0.6664995194493971
06/02/2019 04:01:56 *** evaluating ***
06/02/2019 04:01:56 step: 250, epoch: 249, acc: 58.119658119658126, f1: 24.595994238151103, r: 0.35187722197874854
06/02/2019 04:01:56 *** epoch: 251 ***
06/02/2019 04:01:56 *** training ***
06/02/2019 04:01:57 step: 8255, epoch: 250, batch: 4, loss: 0.1444700062274933, acc: 92.1875, f1: 87.36003492306014, r: 0.6743026312533272
06/02/2019 04:01:57 step: 8260, epoch: 250, batch: 9, loss: 0.08847517520189285, acc: 95.3125, f1: 82.36658456486042, r: 0.8263541425853962
06/02/2019 04:01:57 step: 8265, epoch: 250, batch: 14, loss: 0.12120421975851059, acc: 95.3125, f1: 83.10862383230804, r: 0.7172897051881589
06/02/2019 04:01:58 step: 8270, epoch: 250, batch: 19, loss: 0.1847362369298935, acc: 89.0625, f1: 73.25924075924077, r: 0.6042174753636733
06/02/2019 04:01:58 step: 8275, epoch: 250, batch: 24, loss: 0.04502492398023605, acc: 98.4375, f1: 97.88359788359789, r: 0.6664412166376269
06/02/2019 04:01:58 step: 8280, epoch: 250, batch: 29, loss: 0.10460536181926727, acc: 93.75, f1: 78.04915514592933, r: 0.6329011792258787
06/02/2019 04:01:58 *** evaluating ***
06/02/2019 04:01:58 step: 251, epoch: 250, acc: 59.401709401709404, f1: 25.016641976507657, r: 0.34989455353056387
06/02/2019 04:01:58 *** epoch: 252 ***
06/02/2019 04:01:58 *** training ***
06/02/2019 04:01:59 step: 8288, epoch: 251, batch: 4, loss: 0.13801175355911255, acc: 95.3125, f1: 80.42328042328043, r: 0.6976519788858382
06/02/2019 04:01:59 step: 8293, epoch: 251, batch: 9, loss: 0.07028429210186005, acc: 98.4375, f1: 96.95652173913044, r: 0.7809176959928441
06/02/2019 04:01:59 step: 8298, epoch: 251, batch: 14, loss: 0.06664364784955978, acc: 98.4375, f1: 99.12121212121212, r: 0.8561626974842755
06/02/2019 04:01:59 step: 8303, epoch: 251, batch: 19, loss: 0.1292053610086441, acc: 93.75, f1: 88.06382275132275, r: 0.7438403552864934
06/02/2019 04:02:00 step: 8308, epoch: 251, batch: 24, loss: 0.0890972912311554, acc: 96.875, f1: 94.54715219421102, r: 0.5568068038374231
06/02/2019 04:02:00 step: 8313, epoch: 251, batch: 29, loss: 0.05972793325781822, acc: 98.4375, f1: 98.61853832442068, r: 0.7952196811959809
06/02/2019 04:02:00 *** evaluating ***
06/02/2019 04:02:00 step: 252, epoch: 251, acc: 58.97435897435898, f1: 24.833959144303975, r: 0.3701631988874111
06/02/2019 04:02:00 *** epoch: 253 ***
06/02/2019 04:02:00 *** training ***
06/02/2019 04:02:01 step: 8321, epoch: 252, batch: 4, loss: 0.09100303798913956, acc: 96.875, f1: 96.15178571428571, r: 0.7057740315294945
06/02/2019 04:02:01 step: 8326, epoch: 252, batch: 9, loss: 0.2119295597076416, acc: 89.0625, f1: 82.13468915994491, r: 0.6915101721710596
06/02/2019 04:02:01 step: 8331, epoch: 252, batch: 14, loss: 0.11036878824234009, acc: 95.3125, f1: 86.93602693602693, r: 0.7190977635328424
06/02/2019 04:02:01 step: 8336, epoch: 252, batch: 19, loss: 0.14954297244548798, acc: 92.1875, f1: 75.19444444444444, r: 0.7876150293701609
06/02/2019 04:02:02 step: 8341, epoch: 252, batch: 24, loss: 0.060625605285167694, acc: 98.4375, f1: 93.65079365079366, r: 0.6335050641347815
06/02/2019 04:02:02 step: 8346, epoch: 252, batch: 29, loss: 0.11242273449897766, acc: 95.3125, f1: 94.7463768115942, r: 0.7317089764590649
06/02/2019 04:02:02 *** evaluating ***
06/02/2019 04:02:02 step: 253, epoch: 252, acc: 58.97435897435898, f1: 24.41322228887882, r: 0.359364601697785
06/02/2019 04:02:02 *** epoch: 254 ***
06/02/2019 04:02:02 *** training ***
06/02/2019 04:02:02 step: 8354, epoch: 253, batch: 4, loss: 0.11531922966241837, acc: 93.75, f1: 92.77375307987553, r: 0.6605890309579056
06/02/2019 04:02:03 step: 8359, epoch: 253, batch: 9, loss: 0.12353667616844177, acc: 95.3125, f1: 84.58686361947233, r: 0.7584061265729293
06/02/2019 04:02:03 step: 8364, epoch: 253, batch: 14, loss: 0.10250674188137054, acc: 96.875, f1: 89.26681783824641, r: 0.5663644163937104
06/02/2019 04:02:03 step: 8369, epoch: 253, batch: 19, loss: 0.13518789410591125, acc: 95.3125, f1: 95.25414230019493, r: 0.7126234515203556
06/02/2019 04:02:04 step: 8374, epoch: 253, batch: 24, loss: 0.16216938197612762, acc: 93.75, f1: 94.3019943019943, r: 0.7575516245555337
06/02/2019 04:02:04 step: 8379, epoch: 253, batch: 29, loss: 0.11160743981599808, acc: 93.75, f1: 92.51783500605183, r: 0.7806086333090009
06/02/2019 04:02:04 *** evaluating ***
06/02/2019 04:02:04 step: 254, epoch: 253, acc: 58.54700854700855, f1: 24.30286786384347, r: 0.3556645885629822
06/02/2019 04:02:04 *** epoch: 255 ***
06/02/2019 04:02:04 *** training ***
06/02/2019 04:02:04 step: 8387, epoch: 254, batch: 4, loss: 0.1351037621498108, acc: 93.75, f1: 90.63834539444295, r: 0.778957058733717
06/02/2019 04:02:05 step: 8392, epoch: 254, batch: 9, loss: 0.141585573554039, acc: 92.1875, f1: 89.96160064380075, r: 0.6501628019395165
06/02/2019 04:02:05 step: 8397, epoch: 254, batch: 14, loss: 0.13808557391166687, acc: 93.75, f1: 82.14966740576497, r: 0.8282997426297651
06/02/2019 04:02:05 step: 8402, epoch: 254, batch: 19, loss: 0.04512219876050949, acc: 98.4375, f1: 98.16207184628237, r: 0.7352048642831932
06/02/2019 04:02:06 step: 8407, epoch: 254, batch: 24, loss: 0.07047660648822784, acc: 98.4375, f1: 94.77726574500768, r: 0.676742229912366
06/02/2019 04:02:06 step: 8412, epoch: 254, batch: 29, loss: 0.09294259548187256, acc: 96.875, f1: 91.31944444444444, r: 0.7330088072005242
06/02/2019 04:02:06 *** evaluating ***
06/02/2019 04:02:06 step: 255, epoch: 254, acc: 58.54700854700855, f1: 24.341444270015696, r: 0.3598833916099329
06/02/2019 04:02:06 *** epoch: 256 ***
06/02/2019 04:02:06 *** training ***
06/02/2019 04:02:07 step: 8420, epoch: 255, batch: 4, loss: 0.08424228429794312, acc: 96.875, f1: 97.29844623461645, r: 0.6415644610524183
06/02/2019 04:02:07 step: 8425, epoch: 255, batch: 9, loss: 0.12823760509490967, acc: 93.75, f1: 88.82967269595177, r: 0.7938700478349618
06/02/2019 04:02:07 step: 8430, epoch: 255, batch: 14, loss: 0.1916281133890152, acc: 93.75, f1: 79.25326403488762, r: 0.6732024553601311
06/02/2019 04:02:07 step: 8435, epoch: 255, batch: 19, loss: 0.16493426263332367, acc: 93.75, f1: 81.09234515484516, r: 0.703098848549334
06/02/2019 04:02:08 step: 8440, epoch: 255, batch: 24, loss: 0.13058483600616455, acc: 96.875, f1: 96.25492302184033, r: 0.6194390380990448
06/02/2019 04:02:08 step: 8445, epoch: 255, batch: 29, loss: 0.15482671558856964, acc: 93.75, f1: 88.3909533962839, r: 0.648396373217533
06/02/2019 04:02:08 *** evaluating ***
06/02/2019 04:02:08 step: 256, epoch: 255, acc: 58.119658119658126, f1: 24.54022477231563, r: 0.3620814231097618
06/02/2019 04:02:08 *** epoch: 257 ***
06/02/2019 04:02:08 *** training ***
06/02/2019 04:02:09 step: 8453, epoch: 256, batch: 4, loss: 0.1310175657272339, acc: 93.75, f1: 68.06386347642506, r: 0.6977689014038513
06/02/2019 04:02:09 step: 8458, epoch: 256, batch: 9, loss: 0.16086673736572266, acc: 92.1875, f1: 92.6547619047619, r: 0.7667554281707493
06/02/2019 04:02:09 step: 8463, epoch: 256, batch: 14, loss: 0.1576996147632599, acc: 92.1875, f1: 88.68276460381723, r: 0.798790008817816
06/02/2019 04:02:09 step: 8468, epoch: 256, batch: 19, loss: 0.14502201974391937, acc: 95.3125, f1: 88.56720689018826, r: 0.5935012053538422
06/02/2019 04:02:10 step: 8473, epoch: 256, batch: 24, loss: 0.09734022617340088, acc: 98.4375, f1: 98.80745341614906, r: 0.6989262279437224
06/02/2019 04:02:10 step: 8478, epoch: 256, batch: 29, loss: 0.15485522150993347, acc: 92.1875, f1: 78.86243386243386, r: 0.667956964900467
06/02/2019 04:02:10 *** evaluating ***
06/02/2019 04:02:10 step: 257, epoch: 256, acc: 59.82905982905983, f1: 24.504848093083385, r: 0.3534405626475392
06/02/2019 04:02:10 *** epoch: 258 ***
06/02/2019 04:02:10 *** training ***
06/02/2019 04:02:10 step: 8486, epoch: 257, batch: 4, loss: 0.11768422275781631, acc: 95.3125, f1: 94.79914070891515, r: 0.70390875875273
06/02/2019 04:02:11 step: 8491, epoch: 257, batch: 9, loss: 0.15813617408275604, acc: 93.75, f1: 92.18769325912184, r: 0.6087829410085378
06/02/2019 04:02:11 step: 8496, epoch: 257, batch: 14, loss: 0.0945238396525383, acc: 96.875, f1: 94.39290867862296, r: 0.6940680288592671
06/02/2019 04:02:11 step: 8501, epoch: 257, batch: 19, loss: 0.1327105164527893, acc: 93.75, f1: 89.27700266709554, r: 0.721636504237867
06/02/2019 04:02:11 step: 8506, epoch: 257, batch: 24, loss: 0.19595541059970856, acc: 90.625, f1: 75.28348104958889, r: 0.652076109986341
06/02/2019 04:02:12 step: 8511, epoch: 257, batch: 29, loss: 0.14715182781219482, acc: 93.75, f1: 91.27513227513228, r: 0.7614548378506651
06/02/2019 04:02:12 *** evaluating ***
06/02/2019 04:02:12 step: 258, epoch: 257, acc: 59.401709401709404, f1: 24.74403760766213, r: 0.34390117064392206
06/02/2019 04:02:12 *** epoch: 259 ***
06/02/2019 04:02:12 *** training ***
06/02/2019 04:02:12 step: 8519, epoch: 258, batch: 4, loss: 0.13708063960075378, acc: 93.75, f1: 90.7484000075516, r: 0.6833519570214611
06/02/2019 04:02:13 step: 8524, epoch: 258, batch: 9, loss: 0.09866677969694138, acc: 96.875, f1: 90.64872325741892, r: 0.7393948087920436
06/02/2019 04:02:13 step: 8529, epoch: 258, batch: 14, loss: 0.1052260547876358, acc: 96.875, f1: 96.06277794568456, r: 0.8063856781674852
06/02/2019 04:02:13 step: 8534, epoch: 258, batch: 19, loss: 0.15725164115428925, acc: 93.75, f1: 96.85645686846168, r: 0.6690691935561571
06/02/2019 04:02:13 step: 8539, epoch: 258, batch: 24, loss: 0.045136019587516785, acc: 98.4375, f1: 96.8602825745683, r: 0.6656675384590215
06/02/2019 04:02:14 step: 8544, epoch: 258, batch: 29, loss: 0.12692147493362427, acc: 93.75, f1: 92.62295081967213, r: 0.6883869960488365
06/02/2019 04:02:14 *** evaluating ***
06/02/2019 04:02:14 step: 259, epoch: 258, acc: 59.401709401709404, f1: 25.016641976507657, r: 0.3451573055740426
06/02/2019 04:02:14 *** epoch: 260 ***
06/02/2019 04:02:14 *** training ***
06/02/2019 04:02:14 step: 8552, epoch: 259, batch: 4, loss: 0.061837971210479736, acc: 98.4375, f1: 96.66048237476808, r: 0.6802927722249771
06/02/2019 04:02:14 step: 8557, epoch: 259, batch: 9, loss: 0.09755802899599075, acc: 96.875, f1: 92.13594581519109, r: 0.767505899531429
06/02/2019 04:02:15 step: 8562, epoch: 259, batch: 14, loss: 0.12408575415611267, acc: 96.875, f1: 97.15993046501521, r: 0.7898988831513754
06/02/2019 04:02:15 step: 8567, epoch: 259, batch: 19, loss: 0.1178693026304245, acc: 93.75, f1: 90.89995682792559, r: 0.6215960378398986
06/02/2019 04:02:15 step: 8572, epoch: 259, batch: 24, loss: 0.13474176824092865, acc: 95.3125, f1: 82.23484848484848, r: 0.7423449679914564
06/02/2019 04:02:16 step: 8577, epoch: 259, batch: 29, loss: 0.1149822399020195, acc: 95.3125, f1: 94.84942202047466, r: 0.7169799593241429
06/02/2019 04:02:16 *** evaluating ***
06/02/2019 04:02:16 step: 260, epoch: 259, acc: 58.119658119658126, f1: 24.099306431273646, r: 0.3457055402739756
06/02/2019 04:02:16 *** epoch: 261 ***
06/02/2019 04:02:16 *** training ***
06/02/2019 04:02:16 step: 8585, epoch: 260, batch: 4, loss: 0.12862733006477356, acc: 95.3125, f1: 92.85023152947682, r: 0.766453425677565
06/02/2019 04:02:16 step: 8590, epoch: 260, batch: 9, loss: 0.14351972937583923, acc: 96.875, f1: 85.41666666666667, r: 0.6909741767792622
06/02/2019 04:02:17 step: 8595, epoch: 260, batch: 14, loss: 0.16847138106822968, acc: 96.875, f1: 98.45238095238095, r: 0.6817559328226296
06/02/2019 04:02:17 step: 8600, epoch: 260, batch: 19, loss: 0.07046329230070114, acc: 96.875, f1: 84.18803418803418, r: 0.6740774555694159
06/02/2019 04:02:17 step: 8605, epoch: 260, batch: 24, loss: 0.1519646942615509, acc: 93.75, f1: 86.2756283068783, r: 0.7415244075084877
06/02/2019 04:02:17 step: 8610, epoch: 260, batch: 29, loss: 0.1016581803560257, acc: 96.875, f1: 93.26530612244898, r: 0.6807836490019865
06/02/2019 04:02:18 *** evaluating ***
06/02/2019 04:02:18 step: 261, epoch: 260, acc: 57.26495726495726, f1: 24.54521190351468, r: 0.3532403753178618
06/02/2019 04:02:18 *** epoch: 262 ***
06/02/2019 04:02:18 *** training ***
06/02/2019 04:02:18 step: 8618, epoch: 261, batch: 4, loss: 0.19290249049663544, acc: 92.1875, f1: 79.99339945768517, r: 0.6459846882695547
06/02/2019 04:02:18 step: 8623, epoch: 261, batch: 9, loss: 0.17023734748363495, acc: 92.1875, f1: 75.67816516527036, r: 0.5690020571061551
06/02/2019 04:02:18 step: 8628, epoch: 261, batch: 14, loss: 0.11534183472394943, acc: 95.3125, f1: 84.93828369905955, r: 0.6663907500414781
06/02/2019 04:02:19 step: 8633, epoch: 261, batch: 19, loss: 0.11868799477815628, acc: 95.3125, f1: 83.20397603485839, r: 0.6506290519527734
06/02/2019 04:02:19 step: 8638, epoch: 261, batch: 24, loss: 0.0509725883603096, acc: 96.875, f1: 96.27609664555477, r: 0.705731177518855
06/02/2019 04:02:19 step: 8643, epoch: 261, batch: 29, loss: 0.18487155437469482, acc: 93.75, f1: 90.95114781634939, r: 0.7769565358813815
06/02/2019 04:02:19 *** evaluating ***
06/02/2019 04:02:20 step: 262, epoch: 261, acc: 59.401709401709404, f1: 24.92175374694034, r: 0.36054459161101904
06/02/2019 04:02:20 *** epoch: 263 ***
06/02/2019 04:02:20 *** training ***
06/02/2019 04:02:20 step: 8651, epoch: 262, batch: 4, loss: 0.1575661301612854, acc: 93.75, f1: 93.51190476190476, r: 0.7551987777131378
06/02/2019 04:02:20 step: 8656, epoch: 262, batch: 9, loss: 0.14554983377456665, acc: 95.3125, f1: 91.85234795672656, r: 0.6881837235802085
06/02/2019 04:02:21 step: 8661, epoch: 262, batch: 14, loss: 0.10708250850439072, acc: 95.3125, f1: 95.36454693687398, r: 0.6048850970216605
06/02/2019 04:02:21 step: 8666, epoch: 262, batch: 19, loss: 0.07855246961116791, acc: 96.875, f1: 96.81722218668031, r: 0.6973656621522734
06/02/2019 04:02:21 step: 8671, epoch: 262, batch: 24, loss: 0.18039445579051971, acc: 90.625, f1: 67.4980574980575, r: 0.7218175560383528
06/02/2019 04:02:21 step: 8676, epoch: 262, batch: 29, loss: 0.13159975409507751, acc: 93.75, f1: 92.29909583170452, r: 0.711891520817265
06/02/2019 04:02:22 *** evaluating ***
06/02/2019 04:02:22 step: 263, epoch: 262, acc: 58.54700854700855, f1: 25.144052456815423, r: 0.34905349033645017
06/02/2019 04:02:22 *** epoch: 264 ***
06/02/2019 04:02:22 *** training ***
06/02/2019 04:02:22 step: 8684, epoch: 263, batch: 4, loss: 0.15842945873737335, acc: 90.625, f1: 85.23796137645463, r: 0.6714137257535845
06/02/2019 04:02:22 step: 8689, epoch: 263, batch: 9, loss: 0.11602726578712463, acc: 93.75, f1: 80.03103816681403, r: 0.7642010243209779
06/02/2019 04:02:23 step: 8694, epoch: 263, batch: 14, loss: 0.1345665454864502, acc: 95.3125, f1: 92.10884353741496, r: 0.6916955290646908
06/02/2019 04:02:23 step: 8699, epoch: 263, batch: 19, loss: 0.16721047461032867, acc: 92.1875, f1: 87.11971942144146, r: 0.7087066970424162
06/02/2019 04:02:23 step: 8704, epoch: 263, batch: 24, loss: 0.07684418559074402, acc: 96.875, f1: 95.01680672268907, r: 0.702733084491293
06/02/2019 04:02:23 step: 8709, epoch: 263, batch: 29, loss: 0.08073592185974121, acc: 98.4375, f1: 97.92358803986711, r: 0.7522916759332343
06/02/2019 04:02:23 *** evaluating ***
06/02/2019 04:02:24 step: 264, epoch: 263, acc: 59.82905982905983, f1: 25.21391843450667, r: 0.365176766246325
06/02/2019 04:02:24 *** epoch: 265 ***
06/02/2019 04:02:24 *** training ***
06/02/2019 04:02:24 step: 8717, epoch: 264, batch: 4, loss: 0.10373541712760925, acc: 95.3125, f1: 92.4393090569561, r: 0.7327091693727157
06/02/2019 04:02:24 step: 8722, epoch: 264, batch: 9, loss: 0.10928910225629807, acc: 96.875, f1: 80.40229885057471, r: 0.6595210594444058
06/02/2019 04:02:24 step: 8727, epoch: 264, batch: 14, loss: 0.07436849921941757, acc: 96.875, f1: 97.29760289461782, r: 0.6668312046348451
06/02/2019 04:02:25 step: 8732, epoch: 264, batch: 19, loss: 0.1380995661020279, acc: 93.75, f1: 79.29408396105899, r: 0.6674116156070958
06/02/2019 04:02:25 step: 8737, epoch: 264, batch: 24, loss: 0.1291113942861557, acc: 96.875, f1: 95.84415584415584, r: 0.7876667079287218
06/02/2019 04:02:25 step: 8742, epoch: 264, batch: 29, loss: 0.10453387349843979, acc: 96.875, f1: 97.27570738440303, r: 0.7103154163643632
06/02/2019 04:02:25 *** evaluating ***
06/02/2019 04:02:25 step: 265, epoch: 264, acc: 58.119658119658126, f1: 24.735010095540083, r: 0.35571176752914896
06/02/2019 04:02:25 *** epoch: 266 ***
06/02/2019 04:02:25 *** training ***
06/02/2019 04:02:26 step: 8750, epoch: 265, batch: 4, loss: 0.10630422830581665, acc: 95.3125, f1: 93.59634551495016, r: 0.7057009840654933
06/02/2019 04:02:26 step: 8755, epoch: 265, batch: 9, loss: 0.046365171670913696, acc: 100.0, f1: 100.0, r: 0.8004414990539829
06/02/2019 04:02:26 step: 8760, epoch: 265, batch: 14, loss: 0.07699695229530334, acc: 98.4375, f1: 98.90903729913018, r: 0.7049556639383096
06/02/2019 04:02:27 step: 8765, epoch: 265, batch: 19, loss: 0.08907939493656158, acc: 98.4375, f1: 96.82539682539682, r: 0.7936293868588091
06/02/2019 04:02:27 step: 8770, epoch: 265, batch: 24, loss: 0.16063909232616425, acc: 89.0625, f1: 75.38690476190476, r: 0.6315714527706973
06/02/2019 04:02:27 step: 8775, epoch: 265, batch: 29, loss: 0.09170953929424286, acc: 96.875, f1: 95.2729809872667, r: 0.6414269766141567
06/02/2019 04:02:27 *** evaluating ***
06/02/2019 04:02:27 step: 266, epoch: 265, acc: 57.692307692307686, f1: 24.402984977800678, r: 0.358355611058745
06/02/2019 04:02:27 *** epoch: 267 ***
06/02/2019 04:02:27 *** training ***
06/02/2019 04:02:28 step: 8783, epoch: 266, batch: 4, loss: 0.09841349720954895, acc: 95.3125, f1: 60.03267973856209, r: 0.5939609097744107
06/02/2019 04:02:28 step: 8788, epoch: 266, batch: 9, loss: 0.04851873219013214, acc: 98.4375, f1: 98.57142857142858, r: 0.7174832381464101
06/02/2019 04:02:28 step: 8793, epoch: 266, batch: 14, loss: 0.0790497362613678, acc: 96.875, f1: 90.71428571428572, r: 0.7809124563510668
06/02/2019 04:02:28 step: 8798, epoch: 266, batch: 19, loss: 0.12181026488542557, acc: 92.1875, f1: 77.41777356103732, r: 0.6708831440347398
06/02/2019 04:02:29 step: 8803, epoch: 266, batch: 24, loss: 0.15533334016799927, acc: 90.625, f1: 85.41043083900227, r: 0.662162238900566
06/02/2019 04:02:29 step: 8808, epoch: 266, batch: 29, loss: 0.14788629114627838, acc: 95.3125, f1: 91.45942166523281, r: 0.6654246668648423
06/02/2019 04:02:29 *** evaluating ***
06/02/2019 04:02:29 step: 267, epoch: 266, acc: 58.97435897435898, f1: 24.836674795811305, r: 0.34862755165286335
06/02/2019 04:02:29 *** epoch: 268 ***
06/02/2019 04:02:29 *** training ***
06/02/2019 04:02:30 step: 8816, epoch: 267, batch: 4, loss: 0.12696649134159088, acc: 93.75, f1: 71.12455197132617, r: 0.7279452536943636
06/02/2019 04:02:30 step: 8821, epoch: 267, batch: 9, loss: 0.13667170703411102, acc: 93.75, f1: 80.56763285024155, r: 0.6627012379225573
06/02/2019 04:02:30 step: 8826, epoch: 267, batch: 14, loss: 0.18159711360931396, acc: 92.1875, f1: 78.97753835692126, r: 0.6980756823781028
06/02/2019 04:02:30 step: 8831, epoch: 267, batch: 19, loss: 0.1027199998497963, acc: 96.875, f1: 82.4249699879952, r: 0.620053267326281
06/02/2019 04:02:31 step: 8836, epoch: 267, batch: 24, loss: 0.24140143394470215, acc: 85.9375, f1: 81.39757738694237, r: 0.7093027547071155
06/02/2019 04:02:31 step: 8841, epoch: 267, batch: 29, loss: 0.09976023435592651, acc: 95.3125, f1: 93.72619047619047, r: 0.7336975498708718
06/02/2019 04:02:31 *** evaluating ***
06/02/2019 04:02:31 step: 268, epoch: 267, acc: 58.97435897435898, f1: 25.34796227868517, r: 0.3493327824444534
06/02/2019 04:02:31 *** epoch: 269 ***
06/02/2019 04:02:31 *** training ***
06/02/2019 04:02:31 step: 8849, epoch: 268, batch: 4, loss: 0.10161035507917404, acc: 95.3125, f1: 92.89891395154554, r: 0.7138508010747907
06/02/2019 04:02:32 step: 8854, epoch: 268, batch: 9, loss: 0.11697262525558472, acc: 92.1875, f1: 93.29371299198885, r: 0.7283367859443648
06/02/2019 04:02:32 step: 8859, epoch: 268, batch: 14, loss: 0.13874654471874237, acc: 93.75, f1: 89.74525474525474, r: 0.6476273713895407
06/02/2019 04:02:32 step: 8864, epoch: 268, batch: 19, loss: 0.1763877272605896, acc: 92.1875, f1: 87.13500873676637, r: 0.7268678679451208
06/02/2019 04:02:33 step: 8869, epoch: 268, batch: 24, loss: 0.042752161622047424, acc: 98.4375, f1: 97.87581699346406, r: 0.7490023262603462
06/02/2019 04:02:33 step: 8874, epoch: 268, batch: 29, loss: 0.08659840375185013, acc: 95.3125, f1: 81.26291567530133, r: 0.673982765151776
06/02/2019 04:02:33 *** evaluating ***
06/02/2019 04:02:33 step: 269, epoch: 268, acc: 60.256410256410255, f1: 27.220158104947263, r: 0.365676477711178
06/02/2019 04:02:33 *** epoch: 270 ***
06/02/2019 04:02:33 *** training ***
06/02/2019 04:02:33 step: 8882, epoch: 269, batch: 4, loss: 0.11053062975406647, acc: 96.875, f1: 94.45502645502646, r: 0.6491032046434969
06/02/2019 04:02:34 step: 8887, epoch: 269, batch: 9, loss: 0.10250925272703171, acc: 96.875, f1: 95.23388488905731, r: 0.692024539426167
06/02/2019 04:02:34 step: 8892, epoch: 269, batch: 14, loss: 0.08445452898740768, acc: 96.875, f1: 96.8542667581789, r: 0.6906284452570892
06/02/2019 04:02:34 step: 8897, epoch: 269, batch: 19, loss: 0.11626949161291122, acc: 95.3125, f1: 88.4426021218474, r: 0.7353392982665589
06/02/2019 04:02:34 step: 8902, epoch: 269, batch: 24, loss: 0.0938405841588974, acc: 96.875, f1: 93.82721055334792, r: 0.7927566226004155
06/02/2019 04:02:35 step: 8907, epoch: 269, batch: 29, loss: 0.12603498995304108, acc: 96.875, f1: 98.26839826839827, r: 0.7333983367115215
06/02/2019 04:02:35 *** evaluating ***
06/02/2019 04:02:35 step: 270, epoch: 269, acc: 60.256410256410255, f1: 26.45923697253485, r: 0.35212980419264567
06/02/2019 04:02:35 *** epoch: 271 ***
06/02/2019 04:02:35 *** training ***
06/02/2019 04:02:35 step: 8915, epoch: 270, batch: 4, loss: 0.11071201413869858, acc: 96.875, f1: 96.02197802197801, r: 0.6796112586911235
06/02/2019 04:02:35 step: 8920, epoch: 270, batch: 9, loss: 0.0998106598854065, acc: 95.3125, f1: 83.35912528783224, r: 0.6375340932885161
06/02/2019 04:02:36 step: 8925, epoch: 270, batch: 14, loss: 0.14854274690151215, acc: 93.75, f1: 90.18285477962897, r: 0.7656362251841216
06/02/2019 04:02:36 step: 8930, epoch: 270, batch: 19, loss: 0.07902758568525314, acc: 96.875, f1: 94.84902733023034, r: 0.6798023705971392
06/02/2019 04:02:36 step: 8935, epoch: 270, batch: 24, loss: 0.15078075230121613, acc: 95.3125, f1: 93.0751787894645, r: 0.6511981656174337
06/02/2019 04:02:36 step: 8940, epoch: 270, batch: 29, loss: 0.12844529747962952, acc: 93.75, f1: 68.04245933278192, r: 0.5703071917423593
06/02/2019 04:02:37 *** evaluating ***
06/02/2019 04:02:37 step: 271, epoch: 270, acc: 59.401709401709404, f1: 25.043242283966105, r: 0.35280030142366103
06/02/2019 04:02:37 *** epoch: 272 ***
06/02/2019 04:02:37 *** training ***
06/02/2019 04:02:37 step: 8948, epoch: 271, batch: 4, loss: 0.08804769068956375, acc: 93.75, f1: 75.05859486651804, r: 0.6391926250516443
06/02/2019 04:02:37 step: 8953, epoch: 271, batch: 9, loss: 0.07594792544841766, acc: 98.4375, f1: 96.52173913043478, r: 0.6439148794496271
06/02/2019 04:02:38 step: 8958, epoch: 271, batch: 14, loss: 0.07390794903039932, acc: 96.875, f1: 97.63034188034189, r: 0.7157507389450822
06/02/2019 04:02:38 step: 8963, epoch: 271, batch: 19, loss: 0.13933677971363068, acc: 93.75, f1: 80.77011555272423, r: 0.7987536290473769
06/02/2019 04:02:38 step: 8968, epoch: 271, batch: 24, loss: 0.1305796504020691, acc: 93.75, f1: 91.50058275058275, r: 0.7796616235811199
06/02/2019 04:02:38 step: 8973, epoch: 271, batch: 29, loss: 0.09642282873392105, acc: 95.3125, f1: 90.14081156938299, r: 0.6221187090993718
06/02/2019 04:02:38 *** evaluating ***
06/02/2019 04:02:39 step: 272, epoch: 271, acc: 58.119658119658126, f1: 24.410219928427214, r: 0.34735234148573413
06/02/2019 04:02:39 *** epoch: 273 ***
06/02/2019 04:02:39 *** training ***
06/02/2019 04:02:39 step: 8981, epoch: 272, batch: 4, loss: 0.08244186639785767, acc: 98.4375, f1: 83.67346938775509, r: 0.5650337766217667
06/02/2019 04:02:39 step: 8986, epoch: 272, batch: 9, loss: 0.08847957104444504, acc: 98.4375, f1: 97.46657283603096, r: 0.6391344888036095
06/02/2019 04:02:39 step: 8991, epoch: 272, batch: 14, loss: 0.17408640682697296, acc: 95.3125, f1: 89.23226433430516, r: 0.6592988904608733
06/02/2019 04:02:40 step: 8996, epoch: 272, batch: 19, loss: 0.12873025238513947, acc: 95.3125, f1: 96.57046657046656, r: 0.6484213786576226
06/02/2019 04:02:40 step: 9001, epoch: 272, batch: 24, loss: 0.1574835479259491, acc: 90.625, f1: 82.72372742200328, r: 0.6975231143895052
06/02/2019 04:02:40 step: 9006, epoch: 272, batch: 29, loss: 0.08380154520273209, acc: 95.3125, f1: 96.70690094219506, r: 0.6258333935772388
06/02/2019 04:02:40 *** evaluating ***
06/02/2019 04:02:40 step: 273, epoch: 272, acc: 59.82905982905983, f1: 26.82459699231851, r: 0.35217838097382914
06/02/2019 04:02:40 *** epoch: 274 ***
06/02/2019 04:02:40 *** training ***
06/02/2019 04:02:41 step: 9014, epoch: 273, batch: 4, loss: 0.07928518950939178, acc: 98.4375, f1: 98.89992360580597, r: 0.6910444415735326
06/02/2019 04:02:41 step: 9019, epoch: 273, batch: 9, loss: 0.13199356198310852, acc: 96.875, f1: 95.2832512315271, r: 0.7184415509740801
06/02/2019 04:02:41 step: 9024, epoch: 273, batch: 14, loss: 0.10468537360429764, acc: 93.75, f1: 94.10218253968254, r: 0.7794613192675294
06/02/2019 04:02:42 step: 9029, epoch: 273, batch: 19, loss: 0.12880516052246094, acc: 92.1875, f1: 80.63934558349453, r: 0.692481772903052
06/02/2019 04:02:42 step: 9034, epoch: 273, batch: 24, loss: 0.1353510469198227, acc: 95.3125, f1: 90.66326345896239, r: 0.571325106914551
06/02/2019 04:02:42 step: 9039, epoch: 273, batch: 29, loss: 0.12784966826438904, acc: 92.1875, f1: 87.66750622259096, r: 0.7206765414431232
06/02/2019 04:02:42 *** evaluating ***
06/02/2019 04:02:42 step: 274, epoch: 273, acc: 58.97435897435898, f1: 24.706218670872577, r: 0.35265182090495856
06/02/2019 04:02:42 *** epoch: 275 ***
06/02/2019 04:02:42 *** training ***
06/02/2019 04:02:43 step: 9047, epoch: 274, batch: 4, loss: 0.1253630518913269, acc: 95.3125, f1: 94.90950226244344, r: 0.74767063233316
06/02/2019 04:02:43 step: 9052, epoch: 274, batch: 9, loss: 0.08952182531356812, acc: 96.875, f1: 94.84452657066394, r: 0.7693193019897101
06/02/2019 04:02:43 step: 9057, epoch: 274, batch: 14, loss: 0.08772007375955582, acc: 96.875, f1: 96.07142857142857, r: 0.7217927848030139
06/02/2019 04:02:44 step: 9062, epoch: 274, batch: 19, loss: 0.1432131975889206, acc: 95.3125, f1: 94.45391414141415, r: 0.7861423462872147
06/02/2019 04:02:44 step: 9067, epoch: 274, batch: 24, loss: 0.11072932183742523, acc: 98.4375, f1: 96.95652173913044, r: 0.7446990997223248
06/02/2019 04:02:44 step: 9072, epoch: 274, batch: 29, loss: 0.15289971232414246, acc: 90.625, f1: 84.84729556158128, r: 0.706897754524537
06/02/2019 04:02:44 *** evaluating ***
06/02/2019 04:02:44 step: 275, epoch: 274, acc: 58.54700854700855, f1: 26.66462600859375, r: 0.3603336985952542
06/02/2019 04:02:44 *** epoch: 276 ***
06/02/2019 04:02:44 *** training ***
06/02/2019 04:02:45 step: 9080, epoch: 275, batch: 4, loss: 0.09208645671606064, acc: 95.3125, f1: 96.87363570215864, r: 0.4718051485777372
06/02/2019 04:02:45 step: 9085, epoch: 275, batch: 9, loss: 0.07991103082895279, acc: 95.3125, f1: 92.09671907040328, r: 0.7533182956329034
06/02/2019 04:02:45 step: 9090, epoch: 275, batch: 14, loss: 0.0797504335641861, acc: 96.875, f1: 83.65902964959568, r: 0.6367939893684904
06/02/2019 04:02:46 step: 9095, epoch: 275, batch: 19, loss: 0.1034618392586708, acc: 93.75, f1: 84.44285435454785, r: 0.5892524646162308
06/02/2019 04:02:46 step: 9100, epoch: 275, batch: 24, loss: 0.13000227510929108, acc: 95.3125, f1: 89.57240038872692, r: 0.6327731779482975
06/02/2019 04:02:46 step: 9105, epoch: 275, batch: 29, loss: 0.11916320025920868, acc: 95.3125, f1: 93.99355877616746, r: 0.7965424702391527
06/02/2019 04:02:46 *** evaluating ***
06/02/2019 04:02:46 step: 276, epoch: 275, acc: 58.97435897435898, f1: 24.824070260164643, r: 0.3553924426490187
06/02/2019 04:02:46 *** epoch: 277 ***
06/02/2019 04:02:46 *** training ***
06/02/2019 04:02:47 step: 9113, epoch: 276, batch: 4, loss: 0.039586879312992096, acc: 100.0, f1: 100.0, r: 0.7411382382205419
06/02/2019 04:02:47 step: 9118, epoch: 276, batch: 9, loss: 0.10875216126441956, acc: 96.875, f1: 96.37773079633544, r: 0.6279989716889388
06/02/2019 04:02:47 step: 9123, epoch: 276, batch: 14, loss: 0.1142573207616806, acc: 95.3125, f1: 77.20115565185988, r: 0.6316047368070647
06/02/2019 04:02:48 step: 9128, epoch: 276, batch: 19, loss: 0.1509421467781067, acc: 93.75, f1: 77.56720830848738, r: 0.6414121091878033
06/02/2019 04:02:48 step: 9133, epoch: 276, batch: 24, loss: 0.07737907767295837, acc: 96.875, f1: 92.91716686674671, r: 0.6606640783023572
06/02/2019 04:02:48 step: 9138, epoch: 276, batch: 29, loss: 0.11036975681781769, acc: 96.875, f1: 95.51587301587301, r: 0.7410415890074475
06/02/2019 04:02:48 *** evaluating ***
06/02/2019 04:02:49 step: 277, epoch: 276, acc: 59.401709401709404, f1: 24.922522936800494, r: 0.35706468468105174
06/02/2019 04:02:49 *** epoch: 278 ***
06/02/2019 04:02:49 *** training ***
06/02/2019 04:02:49 step: 9146, epoch: 277, batch: 4, loss: 0.09543381631374359, acc: 96.875, f1: 92.67540635465164, r: 0.7625392513560828
06/02/2019 04:02:49 step: 9151, epoch: 277, batch: 9, loss: 0.23450596630573273, acc: 90.625, f1: 70.35056446821153, r: 0.6097418774606724
06/02/2019 04:02:49 step: 9156, epoch: 277, batch: 14, loss: 0.12662780284881592, acc: 95.3125, f1: 94.85460191981932, r: 0.7648698344632024
06/02/2019 04:02:50 step: 9161, epoch: 277, batch: 19, loss: 0.07547306269407272, acc: 98.4375, f1: 97.1188475390156, r: 0.7001595646584656
06/02/2019 04:02:50 step: 9166, epoch: 277, batch: 24, loss: 0.10970878601074219, acc: 95.3125, f1: 84.20449006233869, r: 0.6982524090094677
06/02/2019 04:02:50 step: 9171, epoch: 277, batch: 29, loss: 0.11155404150485992, acc: 95.3125, f1: 96.04761904761905, r: 0.8167110232489085
06/02/2019 04:02:50 *** evaluating ***
06/02/2019 04:02:51 step: 278, epoch: 277, acc: 56.837606837606835, f1: 24.39507528467126, r: 0.3331569903020142
06/02/2019 04:02:51 *** epoch: 279 ***
06/02/2019 04:02:51 *** training ***
06/02/2019 04:02:51 step: 9179, epoch: 278, batch: 4, loss: 0.12222585082054138, acc: 96.875, f1: 83.14814814814815, r: 0.7329654926961278
06/02/2019 04:02:51 step: 9184, epoch: 278, batch: 9, loss: 0.1451294869184494, acc: 93.75, f1: 88.62179487179486, r: 0.7223042009786564
06/02/2019 04:02:52 step: 9189, epoch: 278, batch: 14, loss: 0.08644096553325653, acc: 98.4375, f1: 85.0, r: 0.6890920611894539
06/02/2019 04:02:52 step: 9194, epoch: 278, batch: 19, loss: 0.0980018749833107, acc: 96.875, f1: 94.26170468187276, r: 0.6732604030720694
06/02/2019 04:02:52 step: 9199, epoch: 278, batch: 24, loss: 0.09419173747301102, acc: 96.875, f1: 96.78571428571429, r: 0.6869587708359193
06/02/2019 04:02:52 step: 9204, epoch: 278, batch: 29, loss: 0.11987738311290741, acc: 95.3125, f1: 88.55637116506682, r: 0.6526743497971573
06/02/2019 04:02:53 *** evaluating ***
06/02/2019 04:02:53 step: 279, epoch: 278, acc: 57.692307692307686, f1: 24.510681578988706, r: 0.33138398170121675
06/02/2019 04:02:53 *** epoch: 280 ***
06/02/2019 04:02:53 *** training ***
06/02/2019 04:02:53 step: 9212, epoch: 279, batch: 4, loss: 0.12211016565561295, acc: 96.875, f1: 92.44047619047619, r: 0.7520427542045359
06/02/2019 04:02:53 step: 9217, epoch: 279, batch: 9, loss: 0.1263016164302826, acc: 95.3125, f1: 96.29391076759497, r: 0.8041573695975119
06/02/2019 04:02:54 step: 9222, epoch: 279, batch: 14, loss: 0.04648236557841301, acc: 100.0, f1: 100.0, r: 0.8066056196336764
06/02/2019 04:02:54 step: 9227, epoch: 279, batch: 19, loss: 0.08623964339494705, acc: 96.875, f1: 96.34920634920636, r: 0.7719429486886983
06/02/2019 04:02:54 step: 9232, epoch: 279, batch: 24, loss: 0.03609251230955124, acc: 98.4375, f1: 93.93939393939394, r: 0.7503501495712972
06/02/2019 04:02:54 step: 9237, epoch: 279, batch: 29, loss: 0.1956489235162735, acc: 93.75, f1: 84.10499955130794, r: 0.6917115353680695
06/02/2019 04:02:55 *** evaluating ***
06/02/2019 04:02:55 step: 280, epoch: 279, acc: 59.401709401709404, f1: 25.163498483285125, r: 0.3430606222010745
06/02/2019 04:02:55 *** epoch: 281 ***
06/02/2019 04:02:55 *** training ***
06/02/2019 04:02:55 step: 9245, epoch: 280, batch: 4, loss: 0.15861746668815613, acc: 93.75, f1: 86.32953912559177, r: 0.7725434698768744
06/02/2019 04:02:55 step: 9250, epoch: 280, batch: 9, loss: 0.052461229264736176, acc: 98.4375, f1: 98.99749373433583, r: 0.6752887379520357
06/02/2019 04:02:56 step: 9255, epoch: 280, batch: 14, loss: 0.17208918929100037, acc: 90.625, f1: 88.7584825084825, r: 0.7374037027316911
06/02/2019 04:02:56 step: 9260, epoch: 280, batch: 19, loss: 0.11149394512176514, acc: 93.75, f1: 82.14362026862027, r: 0.798580834316021
06/02/2019 04:02:56 step: 9265, epoch: 280, batch: 24, loss: 0.08512561023235321, acc: 98.4375, f1: 98.14921920185078, r: 0.6915859450881781
06/02/2019 04:02:56 step: 9270, epoch: 280, batch: 29, loss: 0.06865660846233368, acc: 96.875, f1: 93.75661375661376, r: 0.6636961806028021
06/02/2019 04:02:57 *** evaluating ***
06/02/2019 04:02:57 step: 281, epoch: 280, acc: 58.119658119658126, f1: 24.734497377477478, r: 0.35238696533068004
06/02/2019 04:02:57 *** epoch: 282 ***
06/02/2019 04:02:57 *** training ***
06/02/2019 04:02:57 step: 9278, epoch: 281, batch: 4, loss: 0.13388530910015106, acc: 93.75, f1: 77.45816293430032, r: 0.6294919063135938
06/02/2019 04:02:57 step: 9283, epoch: 281, batch: 9, loss: 0.19567424058914185, acc: 90.625, f1: 67.15116902616903, r: 0.6198184172204474
06/02/2019 04:02:58 step: 9288, epoch: 281, batch: 14, loss: 0.08836816996335983, acc: 98.4375, f1: 99.09876994275972, r: 0.7081391520669316
06/02/2019 04:02:58 step: 9293, epoch: 281, batch: 19, loss: 0.08184854686260223, acc: 95.3125, f1: 95.20691195581547, r: 0.550343150797422
06/02/2019 04:02:58 step: 9298, epoch: 281, batch: 24, loss: 0.13854841887950897, acc: 93.75, f1: 60.25345622119816, r: 0.5207003535733723
06/02/2019 04:02:58 step: 9303, epoch: 281, batch: 29, loss: 0.08450345695018768, acc: 98.4375, f1: 86.76470588235294, r: 0.7155006706756207
06/02/2019 04:02:58 *** evaluating ***
06/02/2019 04:02:59 step: 282, epoch: 281, acc: 59.401709401709404, f1: 24.989044544149184, r: 0.35628844624191813
06/02/2019 04:02:59 *** epoch: 283 ***
06/02/2019 04:02:59 *** training ***
06/02/2019 04:02:59 step: 9311, epoch: 282, batch: 4, loss: 0.18498118221759796, acc: 90.625, f1: 78.65151515151516, r: 0.7635068429253744
06/02/2019 04:02:59 step: 9316, epoch: 282, batch: 9, loss: 0.1267777681350708, acc: 95.3125, f1: 91.90883190883191, r: 0.7136778161479577
06/02/2019 04:02:59 step: 9321, epoch: 282, batch: 14, loss: 0.15357741713523865, acc: 92.1875, f1: 89.74684735554301, r: 0.6482641159920988
06/02/2019 04:03:00 step: 9326, epoch: 282, batch: 19, loss: 0.10529505461454391, acc: 96.875, f1: 96.16750208855473, r: 0.7943632060299033
06/02/2019 04:03:00 step: 9331, epoch: 282, batch: 24, loss: 0.10592526942491531, acc: 95.3125, f1: 87.81845238095238, r: 0.7513102561999934
06/02/2019 04:03:00 step: 9336, epoch: 282, batch: 29, loss: 0.07053077965974808, acc: 98.4375, f1: 97.73242630385488, r: 0.6606415545297237
06/02/2019 04:03:00 *** evaluating ***
06/02/2019 04:03:00 step: 283, epoch: 282, acc: 59.82905982905983, f1: 24.958914850063838, r: 0.35435133646625244
06/02/2019 04:03:00 *** epoch: 284 ***
06/02/2019 04:03:00 *** training ***
06/02/2019 04:03:01 step: 9344, epoch: 283, batch: 4, loss: 0.06689310073852539, acc: 98.4375, f1: 97.46031746031747, r: 0.7360516671711523
06/02/2019 04:03:01 step: 9349, epoch: 283, batch: 9, loss: 0.1612730771303177, acc: 90.625, f1: 84.47503032220641, r: 0.6994870148971769
06/02/2019 04:03:01 step: 9354, epoch: 283, batch: 14, loss: 0.07449007779359818, acc: 98.4375, f1: 98.7468671679198, r: 0.783773692332943
06/02/2019 04:03:02 step: 9359, epoch: 283, batch: 19, loss: 0.03884395584464073, acc: 100.0, f1: 100.0, r: 0.7895841688987459
06/02/2019 04:03:02 step: 9364, epoch: 283, batch: 24, loss: 0.07268808782100677, acc: 96.875, f1: 94.4368858654573, r: 0.68729164008578
06/02/2019 04:03:02 step: 9369, epoch: 283, batch: 29, loss: 0.1032300814986229, acc: 96.875, f1: 92.95238095238095, r: 0.7461170478045558
06/02/2019 04:03:02 *** evaluating ***
06/02/2019 04:03:02 step: 284, epoch: 283, acc: 56.837606837606835, f1: 24.440177894257783, r: 0.34114337799258737
06/02/2019 04:03:02 *** epoch: 285 ***
06/02/2019 04:03:02 *** training ***
06/02/2019 04:03:03 step: 9377, epoch: 284, batch: 4, loss: 0.13004307448863983, acc: 95.3125, f1: 87.31538992408558, r: 0.6706882805410069
06/02/2019 04:03:03 step: 9382, epoch: 284, batch: 9, loss: 0.143307164311409, acc: 93.75, f1: 91.03378866122493, r: 0.6850614440755977
06/02/2019 04:03:03 step: 9387, epoch: 284, batch: 14, loss: 0.06661215424537659, acc: 96.875, f1: 96.10366466821961, r: 0.7998258210656339
06/02/2019 04:03:03 step: 9392, epoch: 284, batch: 19, loss: 0.1597367823123932, acc: 92.1875, f1: 68.49103414394497, r: 0.5818791920082774
06/02/2019 04:03:04 step: 9397, epoch: 284, batch: 24, loss: 0.03964429348707199, acc: 100.0, f1: 100.0, r: 0.7275576530319272
06/02/2019 04:03:04 step: 9402, epoch: 284, batch: 29, loss: 0.12291395664215088, acc: 92.1875, f1: 73.81664078674949, r: 0.6389885727846044
06/02/2019 04:03:04 *** evaluating ***
06/02/2019 04:03:04 step: 285, epoch: 284, acc: 58.97435897435898, f1: 24.484892547290812, r: 0.32917180338645846
06/02/2019 04:03:04 *** epoch: 286 ***
06/02/2019 04:03:04 *** training ***
06/02/2019 04:03:04 step: 9410, epoch: 285, batch: 4, loss: 0.15848682820796967, acc: 90.625, f1: 89.71073044602456, r: 0.7801561425090239
06/02/2019 04:03:05 step: 9415, epoch: 285, batch: 9, loss: 0.1170186698436737, acc: 96.875, f1: 97.32142857142857, r: 0.6709098899767276
06/02/2019 04:03:05 step: 9420, epoch: 285, batch: 14, loss: 0.15784499049186707, acc: 93.75, f1: 86.29329004329004, r: 0.6208138324526289
06/02/2019 04:03:05 step: 9425, epoch: 285, batch: 19, loss: 0.09958852827548981, acc: 93.75, f1: 91.49293563579278, r: 0.746249802839438
06/02/2019 04:03:06 step: 9430, epoch: 285, batch: 24, loss: 0.058310896158218384, acc: 96.875, f1: 81.56565656565657, r: 0.6537293692185082
06/02/2019 04:03:06 step: 9435, epoch: 285, batch: 29, loss: 0.09595517814159393, acc: 95.3125, f1: 93.83747412008282, r: 0.8502707628357598
06/02/2019 04:03:06 *** evaluating ***
06/02/2019 04:03:06 step: 286, epoch: 285, acc: 59.401709401709404, f1: 26.932571141941324, r: 0.3618908480028502
06/02/2019 04:03:06 *** epoch: 287 ***
06/02/2019 04:03:06 *** training ***
06/02/2019 04:03:06 step: 9443, epoch: 286, batch: 4, loss: 0.13950422406196594, acc: 95.3125, f1: 91.32870169344332, r: 0.7071234645019401
06/02/2019 04:03:07 step: 9448, epoch: 286, batch: 9, loss: 0.07816144078969955, acc: 96.875, f1: 91.34270101483216, r: 0.7944229946189281
06/02/2019 04:03:07 step: 9453, epoch: 286, batch: 14, loss: 0.15038251876831055, acc: 93.75, f1: 90.07674734061541, r: 0.6148514525479497
06/02/2019 04:03:07 step: 9458, epoch: 286, batch: 19, loss: 0.1818334311246872, acc: 90.625, f1: 88.40728876443163, r: 0.6839064620839601
06/02/2019 04:03:08 step: 9463, epoch: 286, batch: 24, loss: 0.1043558195233345, acc: 93.75, f1: 89.86887508626641, r: 0.6359781496034029
06/02/2019 04:03:08 step: 9468, epoch: 286, batch: 29, loss: 0.158661887049675, acc: 92.1875, f1: 78.85164717079611, r: 0.6795267452632421
06/02/2019 04:03:08 *** evaluating ***
06/02/2019 04:03:08 step: 287, epoch: 286, acc: 60.68376068376068, f1: 26.251010746679064, r: 0.3583145775574991
06/02/2019 04:03:08 *** epoch: 288 ***
06/02/2019 04:03:08 *** training ***
06/02/2019 04:03:08 step: 9476, epoch: 287, batch: 4, loss: 0.04631883651018143, acc: 100.0, f1: 100.0, r: 0.7953748265753028
06/02/2019 04:03:09 step: 9481, epoch: 287, batch: 9, loss: 0.07659582793712616, acc: 98.4375, f1: 94.87179487179486, r: 0.7240224344242754
06/02/2019 04:03:09 step: 9486, epoch: 287, batch: 14, loss: 0.1755041480064392, acc: 93.75, f1: 91.60791589363016, r: 0.751707097468145
06/02/2019 04:03:09 step: 9491, epoch: 287, batch: 19, loss: 0.1166149452328682, acc: 95.3125, f1: 83.1817031652558, r: 0.8241524221616692
06/02/2019 04:03:09 step: 9496, epoch: 287, batch: 24, loss: 0.11427187919616699, acc: 93.75, f1: 77.02296217398165, r: 0.7093572217482348
06/02/2019 04:03:10 step: 9501, epoch: 287, batch: 29, loss: 0.11542273312807083, acc: 95.3125, f1: 92.16047947505749, r: 0.6980320369444808
06/02/2019 04:03:10 *** evaluating ***
06/02/2019 04:03:10 step: 288, epoch: 287, acc: 59.82905982905983, f1: 25.16725468624203, r: 0.34598245393335675
06/02/2019 04:03:10 *** epoch: 289 ***
06/02/2019 04:03:10 *** training ***
06/02/2019 04:03:10 step: 9509, epoch: 288, batch: 4, loss: 0.09390558302402496, acc: 93.75, f1: 92.12301587301587, r: 0.6976881961692788
06/02/2019 04:03:10 step: 9514, epoch: 288, batch: 9, loss: 0.1000571995973587, acc: 98.4375, f1: 86.76470588235294, r: 0.7386890798096495
06/02/2019 04:03:11 step: 9519, epoch: 288, batch: 14, loss: 0.10594731569290161, acc: 95.3125, f1: 88.95735716101849, r: 0.811215047828163
06/02/2019 04:03:11 step: 9524, epoch: 288, batch: 19, loss: 0.1735900342464447, acc: 90.625, f1: 86.18347338935575, r: 0.7437973532805132
06/02/2019 04:03:11 step: 9529, epoch: 288, batch: 24, loss: 0.1126570999622345, acc: 95.3125, f1: 93.93181818181819, r: 0.7313896132060528
06/02/2019 04:03:11 step: 9534, epoch: 288, batch: 29, loss: 0.0877244621515274, acc: 96.875, f1: 97.14997412008282, r: 0.7452631306618617
06/02/2019 04:03:12 *** evaluating ***
06/02/2019 04:03:12 step: 289, epoch: 288, acc: 58.54700854700855, f1: 24.79677470073092, r: 0.3409412217926268
06/02/2019 04:03:12 *** epoch: 290 ***
06/02/2019 04:03:12 *** training ***
06/02/2019 04:03:12 step: 9542, epoch: 289, batch: 4, loss: 0.1101784035563469, acc: 93.75, f1: 78.58806566104703, r: 0.6038498908870706
06/02/2019 04:03:12 step: 9547, epoch: 289, batch: 9, loss: 0.044580571353435516, acc: 100.0, f1: 100.0, r: 0.6622359118709088
06/02/2019 04:03:13 step: 9552, epoch: 289, batch: 14, loss: 0.09614767134189606, acc: 95.3125, f1: 91.00388382303275, r: 0.7376010491259379
06/02/2019 04:03:13 step: 9557, epoch: 289, batch: 19, loss: 0.07792668044567108, acc: 98.4375, f1: 96.9047619047619, r: 0.8359742970247577
06/02/2019 04:03:13 step: 9562, epoch: 289, batch: 24, loss: 0.0815388560295105, acc: 95.3125, f1: 89.9330282448114, r: 0.6663607125050754
06/02/2019 04:03:13 step: 9567, epoch: 289, batch: 29, loss: 0.10286106169223785, acc: 96.875, f1: 80.1360544217687, r: 0.6455926886849623
06/02/2019 04:03:13 *** evaluating ***
06/02/2019 04:03:14 step: 290, epoch: 289, acc: 58.119658119658126, f1: 25.013763197586727, r: 0.3451137146668915
06/02/2019 04:03:14 *** epoch: 291 ***
06/02/2019 04:03:14 *** training ***
06/02/2019 04:03:14 step: 9575, epoch: 290, batch: 4, loss: 0.14483442902565002, acc: 90.625, f1: 65.1569264069264, r: 0.593171500726508
06/02/2019 04:03:14 step: 9580, epoch: 290, batch: 9, loss: 0.03883209824562073, acc: 100.0, f1: 100.0, r: 0.6938032273409932
06/02/2019 04:03:14 step: 9585, epoch: 290, batch: 14, loss: 0.10255103558301926, acc: 95.3125, f1: 89.13868229284051, r: 0.7120259451287589
06/02/2019 04:03:15 step: 9590, epoch: 290, batch: 19, loss: 0.14357203245162964, acc: 92.1875, f1: 86.78073089700997, r: 0.7537844631040693
06/02/2019 04:03:15 step: 9595, epoch: 290, batch: 24, loss: 0.09089694172143936, acc: 95.3125, f1: 87.26340996168582, r: 0.7348542977057702
06/02/2019 04:03:15 step: 9600, epoch: 290, batch: 29, loss: 0.12039312720298767, acc: 93.75, f1: 87.9082432945899, r: 0.628698130238984
06/02/2019 04:03:15 *** evaluating ***
06/02/2019 04:03:16 step: 291, epoch: 290, acc: 58.119658119658126, f1: 24.7444542544973, r: 0.3412779272945767
06/02/2019 04:03:16 *** epoch: 292 ***
06/02/2019 04:03:16 *** training ***
06/02/2019 04:03:16 step: 9608, epoch: 291, batch: 4, loss: 0.13027848303318024, acc: 95.3125, f1: 82.3466897626129, r: 0.6814068454316762
06/02/2019 04:03:16 step: 9613, epoch: 291, batch: 9, loss: 0.18098516762256622, acc: 90.625, f1: 71.86956521739131, r: 0.6377894641293975
06/02/2019 04:03:16 step: 9618, epoch: 291, batch: 14, loss: 0.14433258771896362, acc: 96.875, f1: 93.55844977287103, r: 0.7119497722489616
06/02/2019 04:03:17 step: 9623, epoch: 291, batch: 19, loss: 0.11639609187841415, acc: 96.875, f1: 95.69690237937509, r: 0.7132751606046372
06/02/2019 04:03:17 step: 9628, epoch: 291, batch: 24, loss: 0.13349002599716187, acc: 96.875, f1: 96.23015873015873, r: 0.7399368253146356
06/02/2019 04:03:17 step: 9633, epoch: 291, batch: 29, loss: 0.16372404992580414, acc: 92.1875, f1: 90.7437641723356, r: 0.653353770083573
06/02/2019 04:03:17 *** evaluating ***
06/02/2019 04:03:17 step: 292, epoch: 291, acc: 58.119658119658126, f1: 25.30701936492965, r: 0.34066187887086313
06/02/2019 04:03:17 *** epoch: 293 ***
06/02/2019 04:03:17 *** training ***
06/02/2019 04:03:18 step: 9641, epoch: 292, batch: 4, loss: 0.10261280834674835, acc: 93.75, f1: 86.16787153732966, r: 0.6247259337073409
06/02/2019 04:03:18 step: 9646, epoch: 292, batch: 9, loss: 0.05489188805222511, acc: 100.0, f1: 100.0, r: 0.7998460326681619
06/02/2019 04:03:18 step: 9651, epoch: 292, batch: 14, loss: 0.06998079270124435, acc: 98.4375, f1: 98.63056333644569, r: 0.683212823491009
06/02/2019 04:03:19 step: 9656, epoch: 292, batch: 19, loss: 0.14047682285308838, acc: 93.75, f1: 95.99905303030303, r: 0.6784365231978947
06/02/2019 04:03:19 step: 9661, epoch: 292, batch: 24, loss: 0.10170067101716995, acc: 95.3125, f1: 94.11499493414388, r: 0.7084012260698168
06/02/2019 04:03:19 step: 9666, epoch: 292, batch: 29, loss: 0.062419719994068146, acc: 96.875, f1: 96.4047619047619, r: 0.8003262845974235
06/02/2019 04:03:19 *** evaluating ***
06/02/2019 04:03:19 step: 293, epoch: 292, acc: 57.692307692307686, f1: 25.724953421523335, r: 0.3516149185244297
06/02/2019 04:03:19 *** epoch: 294 ***
06/02/2019 04:03:19 *** training ***
06/02/2019 04:03:20 step: 9674, epoch: 293, batch: 4, loss: 0.21015803515911102, acc: 92.1875, f1: 68.59848484848486, r: 0.6478605431553535
06/02/2019 04:03:20 step: 9679, epoch: 293, batch: 9, loss: 0.11697956174612045, acc: 95.3125, f1: 94.14688759516345, r: 0.772463558503898
06/02/2019 04:03:20 step: 9684, epoch: 293, batch: 14, loss: 0.15587756037712097, acc: 92.1875, f1: 87.88429020722148, r: 0.6420947162186252
06/02/2019 04:03:20 step: 9689, epoch: 293, batch: 19, loss: 0.07107004523277283, acc: 98.4375, f1: 98.70370370370371, r: 0.7889558857467659
06/02/2019 04:03:21 step: 9694, epoch: 293, batch: 24, loss: 0.13491643965244293, acc: 93.75, f1: 89.7686925647452, r: 0.7606137550480522
06/02/2019 04:03:21 step: 9699, epoch: 293, batch: 29, loss: 0.09571647644042969, acc: 96.875, f1: 94.65825123152709, r: 0.7924135689675074
06/02/2019 04:03:21 *** evaluating ***
06/02/2019 04:03:21 step: 294, epoch: 293, acc: 58.119658119658126, f1: 24.654266664309706, r: 0.33652782881886706
06/02/2019 04:03:21 *** epoch: 295 ***
06/02/2019 04:03:21 *** training ***
06/02/2019 04:03:22 step: 9707, epoch: 294, batch: 4, loss: 0.07809486985206604, acc: 96.875, f1: 96.60946529149294, r: 0.7472322898317914
06/02/2019 04:03:22 step: 9712, epoch: 294, batch: 9, loss: 0.044820550829172134, acc: 100.0, f1: 100.0, r: 0.6364765150852055
06/02/2019 04:03:22 step: 9717, epoch: 294, batch: 14, loss: 0.06545424461364746, acc: 98.4375, f1: 97.1951219512195, r: 0.800335235020514
06/02/2019 04:03:22 step: 9722, epoch: 294, batch: 19, loss: 0.03724712133407593, acc: 98.4375, f1: 99.15343915343915, r: 0.6373293723536154
06/02/2019 04:03:23 step: 9727, epoch: 294, batch: 24, loss: 0.13878431916236877, acc: 95.3125, f1: 93.57641969831411, r: 0.761749947118666
06/02/2019 04:03:23 step: 9732, epoch: 294, batch: 29, loss: 0.06769926100969315, acc: 96.875, f1: 96.94444444444446, r: 0.7935397631464526
06/02/2019 04:03:23 *** evaluating ***
06/02/2019 04:03:23 step: 295, epoch: 294, acc: 59.82905982905983, f1: 25.837704281539963, r: 0.3434865493231034
06/02/2019 04:03:23 *** epoch: 296 ***
06/02/2019 04:03:23 *** training ***
06/02/2019 04:03:23 step: 9740, epoch: 295, batch: 4, loss: 0.09522862732410431, acc: 96.875, f1: 85.76367389060889, r: 0.6816142842367376
06/02/2019 04:03:24 step: 9745, epoch: 295, batch: 9, loss: 0.09805308282375336, acc: 95.3125, f1: 94.78325123152709, r: 0.7739975675168198
06/02/2019 04:03:24 step: 9750, epoch: 295, batch: 14, loss: 0.10537998378276825, acc: 95.3125, f1: 95.10416666666667, r: 0.7639339781645968
06/02/2019 04:03:24 step: 9755, epoch: 295, batch: 19, loss: 0.059219907969236374, acc: 100.0, f1: 100.0, r: 0.6032728530300357
06/02/2019 04:03:25 step: 9760, epoch: 295, batch: 24, loss: 0.12268766760826111, acc: 96.875, f1: 81.63956639566395, r: 0.6824791017771393
06/02/2019 04:03:25 step: 9765, epoch: 295, batch: 29, loss: 0.046543486416339874, acc: 98.4375, f1: 95.17543859649122, r: 0.7538587824478362
06/02/2019 04:03:25 *** evaluating ***
06/02/2019 04:03:25 step: 296, epoch: 295, acc: 57.692307692307686, f1: 24.522114410412286, r: 0.3433347993415705
06/02/2019 04:03:25 *** epoch: 297 ***
06/02/2019 04:03:25 *** training ***
06/02/2019 04:03:25 step: 9773, epoch: 296, batch: 4, loss: 0.06014969199895859, acc: 98.4375, f1: 86.53846153846155, r: 0.7104502113205122
06/02/2019 04:03:26 step: 9778, epoch: 296, batch: 9, loss: 0.03985445946455002, acc: 100.0, f1: 100.0, r: 0.7364705867755476
06/02/2019 04:03:26 step: 9783, epoch: 296, batch: 14, loss: 0.10583917796611786, acc: 95.3125, f1: 90.65362160189746, r: 0.7284337385471858
06/02/2019 04:03:26 step: 9788, epoch: 296, batch: 19, loss: 0.06781807541847229, acc: 96.875, f1: 96.46464646464646, r: 0.6963641747840204
06/02/2019 04:03:27 step: 9793, epoch: 296, batch: 24, loss: 0.16727812588214874, acc: 92.1875, f1: 84.8420013166557, r: 0.6648558955279311
06/02/2019 04:03:27 step: 9798, epoch: 296, batch: 29, loss: 0.04092341661453247, acc: 98.4375, f1: 97.96918767507003, r: 0.7740695237306686
06/02/2019 04:03:27 *** evaluating ***
06/02/2019 04:03:27 step: 297, epoch: 296, acc: 58.119658119658126, f1: 25.38851029463487, r: 0.3377473332567001
06/02/2019 04:03:27 *** epoch: 298 ***
06/02/2019 04:03:27 *** training ***
06/02/2019 04:03:27 step: 9806, epoch: 297, batch: 4, loss: 0.07654449343681335, acc: 98.4375, f1: 98.24046920821115, r: 0.6553429703627446
06/02/2019 04:03:28 step: 9811, epoch: 297, batch: 9, loss: 0.07501925528049469, acc: 96.875, f1: 95.06944444444446, r: 0.7741886879064888
06/02/2019 04:03:28 step: 9816, epoch: 297, batch: 14, loss: 0.11248724162578583, acc: 95.3125, f1: 92.00083542188806, r: 0.7945761497664934
06/02/2019 04:03:28 step: 9821, epoch: 297, batch: 19, loss: 0.08088269829750061, acc: 96.875, f1: 94.56549232158989, r: 0.7731066104107146
06/02/2019 04:03:28 step: 9826, epoch: 297, batch: 24, loss: 0.09281472116708755, acc: 95.3125, f1: 94.20077220077219, r: 0.768064462709244
06/02/2019 04:03:29 step: 9831, epoch: 297, batch: 29, loss: 0.024531647562980652, acc: 100.0, f1: 100.0, r: 0.5898328936115529
06/02/2019 04:03:29 *** evaluating ***
06/02/2019 04:03:29 step: 298, epoch: 297, acc: 58.119658119658126, f1: 24.70095850412465, r: 0.33848561392794346
06/02/2019 04:03:29 *** epoch: 299 ***
06/02/2019 04:03:29 *** training ***
06/02/2019 04:03:29 step: 9839, epoch: 298, batch: 4, loss: 0.12264291942119598, acc: 90.625, f1: 85.90122681820183, r: 0.758933576166263
06/02/2019 04:03:30 step: 9844, epoch: 298, batch: 9, loss: 0.12830716371536255, acc: 95.3125, f1: 95.21294578437436, r: 0.7037732293521021
06/02/2019 04:03:30 step: 9849, epoch: 298, batch: 14, loss: 0.06885141134262085, acc: 98.4375, f1: 97.67080745341615, r: 0.755114438124412
06/02/2019 04:03:30 step: 9854, epoch: 298, batch: 19, loss: 0.05868525058031082, acc: 98.4375, f1: 92.38095238095238, r: 0.6952135732475759
06/02/2019 04:03:30 step: 9859, epoch: 298, batch: 24, loss: 0.10994145274162292, acc: 95.3125, f1: 79.50937950937951, r: 0.5919775875758029
06/02/2019 04:03:31 step: 9864, epoch: 298, batch: 29, loss: 0.14900264143943787, acc: 95.3125, f1: 96.27173091458806, r: 0.7700862083082451
06/02/2019 04:03:31 *** evaluating ***
06/02/2019 04:03:31 step: 299, epoch: 298, acc: 58.97435897435898, f1: 24.291940684797826, r: 0.3501391531267624
06/02/2019 04:03:31 *** epoch: 300 ***
06/02/2019 04:03:31 *** training ***
06/02/2019 04:03:31 step: 9872, epoch: 299, batch: 4, loss: 0.09910619258880615, acc: 96.875, f1: 91.75824175824177, r: 0.6483665412739643
06/02/2019 04:03:31 step: 9877, epoch: 299, batch: 9, loss: 0.07887747138738632, acc: 98.4375, f1: 94.44444444444444, r: 0.7948157352975285
06/02/2019 04:03:32 step: 9882, epoch: 299, batch: 14, loss: 0.04239225387573242, acc: 96.875, f1: 96.88970449840015, r: 0.7032680809856756
06/02/2019 04:03:32 step: 9887, epoch: 299, batch: 19, loss: 0.0850672721862793, acc: 95.3125, f1: 96.13005458593695, r: 0.6817287448059315
06/02/2019 04:03:32 step: 9892, epoch: 299, batch: 24, loss: 0.07485141605138779, acc: 96.875, f1: 95.8407605466429, r: 0.7531591362191093
06/02/2019 04:03:33 step: 9897, epoch: 299, batch: 29, loss: 0.10146088153123856, acc: 96.875, f1: 96.86011904761905, r: 0.7828184394628646
06/02/2019 04:03:33 *** evaluating ***
06/02/2019 04:03:33 step: 300, epoch: 299, acc: 58.119658119658126, f1: 25.396436575112702, r: 0.34111663158505995
06/02/2019 04:03:33 
*** Best acc model ***
epoch: 230
acc: 61.53846153846154
f1: 27.81784188034188
corr: 0.36132450332309735
06/02/2019 04:03:33 Loading Test Data
06/02/2019 04:03:33 load data from data/word2vec_temp/test_text.npy, data/word2vec_temp/test_label.npy, training: False
06/02/2019 04:03:55 loaded. total len: 2228
06/02/2019 04:03:55 Test: length: 2228, total batch: 35, batch size: 64
06/02/2019 04:03:55 
*** Test Result ***
acc: 58.119658119658126
f1: 25.396436575112702
corr: 0.34111663158505995
