06/01/2019 10:35:53 {'input_path': 'data/word2vec_temp', 'output_path': 'save/cnn_1', 'gpu': True, 'seed': 20000125, 'display_per_batch': 5, 'optimizer': 'adagrad', 'lr': 0.01, 'lr_decay': 0, 'weight_decay': 0.0001, 'momentum': 0.985, 'num_epochs': 300, 'batch_size': 64, 'num_labels': 8, 'embedding_size': 300, 'type': 'cnn', 'cnn': {'max_length': 512, 'conv_1': {'size': 512, 'kernel_size': 3, 'dropout': 0.5}, 'max_pool_1': {'kernel_size': 2, 'stride': 2}, 'fc': {'hidden_size': 2000, 'dropout': 0.5}, 'loss': 'cross_entropy'}}
06/01/2019 10:35:53 Loading Train Data
06/01/2019 10:35:53 load data from data/word2vec_temp/train_text.npy, data/word2vec_temp/train_label.npy, training: True
06/01/2019 10:36:24 loaded. total len: 2342
06/01/2019 10:36:24 Train: length: 2108, total batch: 33, batch size: 64
06/01/2019 10:36:24 Dev: length: 234, total batch: 4, batch size: 64
06/01/2019 10:36:24 Loading model cnn
06/01/2019 10:36:49 *** epoch: 1 ***
06/01/2019 10:36:49 *** training ***
06/01/2019 10:36:50 step: 5, epoch: 0, batch: 4, loss: 6.057499885559082, acc: 14.0625, f1: 5.853174603174603, r: -0.005301868282609886
06/01/2019 10:36:51 step: 10, epoch: 0, batch: 9, loss: 2.18990421295166, acc: 26.5625, f1: 15.334765485141427, r: -0.029368051503312592
06/01/2019 10:36:51 step: 15, epoch: 0, batch: 14, loss: 1.8732134103775024, acc: 35.9375, f1: 13.698308270676693, r: 0.03145363532193877
06/01/2019 10:36:52 step: 20, epoch: 0, batch: 19, loss: 2.081179141998291, acc: 29.6875, f1: 11.881980785753338, r: -0.005143841935557919
06/01/2019 10:36:53 step: 25, epoch: 0, batch: 24, loss: 2.025313138961792, acc: 29.6875, f1: 9.921267816004658, r: -0.007752181676923157
06/01/2019 10:36:54 step: 30, epoch: 0, batch: 29, loss: 1.7637591361999512, acc: 32.8125, f1: 14.26829875105737, r: -0.023305380504707682
06/01/2019 10:36:54 *** evaluating ***
06/01/2019 10:36:54 step: 1, epoch: 0, acc: 36.324786324786324, f1: 11.478716803176967, r: -0.025267779339948795
06/01/2019 10:36:54 *** epoch: 2 ***
06/01/2019 10:36:54 *** training ***
06/01/2019 10:36:55 step: 38, epoch: 1, batch: 4, loss: 1.8283129930496216, acc: 31.25, f1: 12.617554858934168, r: -0.007593809284637376
06/01/2019 10:36:56 step: 43, epoch: 1, batch: 9, loss: 1.765223741531372, acc: 34.375, f1: 15.934065934065936, r: 0.04885776197489872
06/01/2019 10:36:56 step: 48, epoch: 1, batch: 14, loss: 1.6936452388763428, acc: 35.9375, f1: 11.600429645542427, r: 0.01167068214715498
06/01/2019 10:36:57 step: 53, epoch: 1, batch: 19, loss: 1.5798444747924805, acc: 46.875, f1: 18.614718614718615, r: 0.019404647230705254
06/01/2019 10:36:58 step: 58, epoch: 1, batch: 24, loss: 1.6726821660995483, acc: 35.9375, f1: 16.745730550284634, r: 0.07869780546132771
06/01/2019 10:36:58 step: 63, epoch: 1, batch: 29, loss: 1.738433599472046, acc: 32.8125, f1: 11.28205128205128, r: 0.07120843125288714
06/01/2019 10:36:59 *** evaluating ***
06/01/2019 10:36:59 step: 2, epoch: 1, acc: 41.452991452991455, f1: 11.44553854132547, r: 0.0205731744390746
06/01/2019 10:36:59 *** epoch: 3 ***
06/01/2019 10:36:59 *** training ***
06/01/2019 10:37:00 step: 71, epoch: 2, batch: 4, loss: 1.5974498987197876, acc: 46.875, f1: 16.690555820990603, r: 0.18174039698765962
06/01/2019 10:37:00 step: 76, epoch: 2, batch: 9, loss: 1.7000460624694824, acc: 35.9375, f1: 14.123268036311515, r: 0.11975938249108399
06/01/2019 10:37:01 step: 81, epoch: 2, batch: 14, loss: 1.7084890604019165, acc: 43.75, f1: 15.000808015513895, r: 0.10338336283440167
06/01/2019 10:37:02 step: 86, epoch: 2, batch: 19, loss: 1.6009912490844727, acc: 39.0625, f1: 11.601397315683029, r: 0.0672179181462382
06/01/2019 10:37:03 step: 91, epoch: 2, batch: 24, loss: 1.5052796602249146, acc: 51.5625, f1: 19.816387816387817, r: 0.115115495661216
06/01/2019 10:37:03 step: 96, epoch: 2, batch: 29, loss: 1.7127134799957275, acc: 35.9375, f1: 15.959203839638622, r: 0.05857555912683705
06/01/2019 10:37:04 *** evaluating ***
06/01/2019 10:37:04 step: 3, epoch: 2, acc: 43.162393162393165, f1: 11.787850622101157, r: 0.06601324410725232
06/01/2019 10:37:04 *** epoch: 4 ***
06/01/2019 10:37:04 *** training ***
06/01/2019 10:37:05 step: 104, epoch: 3, batch: 4, loss: 1.6282968521118164, acc: 46.875, f1: 17.652464494569756, r: 0.24104067099770346
06/01/2019 10:37:05 step: 109, epoch: 3, batch: 9, loss: 1.4351314306259155, acc: 50.0, f1: 16.138253638253637, r: 0.2876201503220661
06/01/2019 10:37:06 step: 114, epoch: 3, batch: 14, loss: 1.4423145055770874, acc: 46.875, f1: 20.953748006379584, r: 0.20424794255012466
06/01/2019 10:37:07 step: 119, epoch: 3, batch: 19, loss: 1.527111291885376, acc: 48.4375, f1: 19.954648526077097, r: 0.26181794258742097
06/01/2019 10:37:07 step: 124, epoch: 3, batch: 24, loss: 1.4179033041000366, acc: 48.4375, f1: 18.972332015810274, r: 0.32729016336557853
06/01/2019 10:37:08 step: 129, epoch: 3, batch: 29, loss: 1.6287015676498413, acc: 37.5, f1: 14.440039201710618, r: 0.2124928958489765
06/01/2019 10:37:08 *** evaluating ***
06/01/2019 10:37:09 step: 4, epoch: 3, acc: 42.30769230769231, f1: 14.684377549174851, r: 0.09282305548813043
06/01/2019 10:37:09 *** epoch: 5 ***
06/01/2019 10:37:09 *** training ***
06/01/2019 10:37:09 step: 137, epoch: 4, batch: 4, loss: 1.3529651165008545, acc: 51.5625, f1: 21.4438928724643, r: 0.2207200343264205
06/01/2019 10:37:10 step: 142, epoch: 4, batch: 9, loss: 1.3569082021713257, acc: 57.8125, f1: 28.02282203203862, r: 0.2608441870916006
06/01/2019 10:37:11 step: 147, epoch: 4, batch: 14, loss: 1.3255860805511475, acc: 54.6875, f1: 27.862318840579707, r: 0.19257929473695595
06/01/2019 10:37:12 step: 152, epoch: 4, batch: 19, loss: 1.3541536331176758, acc: 54.6875, f1: 22.147078009285906, r: 0.2674012996624112
06/01/2019 10:37:12 step: 157, epoch: 4, batch: 24, loss: 1.401192307472229, acc: 53.125, f1: 22.872496961166906, r: 0.39481876848728087
06/01/2019 10:37:13 step: 162, epoch: 4, batch: 29, loss: 1.3587841987609863, acc: 57.8125, f1: 20.728428564249462, r: 0.2721450609591321
06/01/2019 10:37:13 *** evaluating ***
06/01/2019 10:37:14 step: 5, epoch: 4, acc: 47.008547008547005, f1: 14.406670515610426, r: 0.13951264754729867
06/01/2019 10:37:14 *** epoch: 6 ***
06/01/2019 10:37:14 *** training ***
06/01/2019 10:37:14 step: 170, epoch: 5, batch: 4, loss: 1.322094202041626, acc: 57.8125, f1: 21.14661654135338, r: 0.30092756076493615
06/01/2019 10:37:15 step: 175, epoch: 5, batch: 9, loss: 1.0958364009857178, acc: 67.1875, f1: 26.787138787138787, r: 0.3366894571383172
06/01/2019 10:37:16 step: 180, epoch: 5, batch: 14, loss: 1.1445235013961792, acc: 56.25, f1: 23.95703933747412, r: 0.3154112088550819
06/01/2019 10:37:16 step: 185, epoch: 5, batch: 19, loss: 1.069207787513733, acc: 65.625, f1: 30.317468615340953, r: 0.32893320858257896
06/01/2019 10:37:17 step: 190, epoch: 5, batch: 24, loss: 1.2844812870025635, acc: 48.4375, f1: 22.097114707952148, r: 0.3774845596756755
06/01/2019 10:37:18 step: 195, epoch: 5, batch: 29, loss: 1.2423059940338135, acc: 60.9375, f1: 22.72686990428926, r: 0.34180203130697423
06/01/2019 10:37:18 *** evaluating ***
06/01/2019 10:37:18 step: 6, epoch: 5, acc: 48.717948717948715, f1: 15.410994169317128, r: 0.1845803410987592
06/01/2019 10:37:18 *** epoch: 7 ***
06/01/2019 10:37:18 *** training ***
06/01/2019 10:37:19 step: 203, epoch: 6, batch: 4, loss: 0.9096968173980713, acc: 75.0, f1: 42.09294107253291, r: 0.3706153729760827
06/01/2019 10:37:20 step: 208, epoch: 6, batch: 9, loss: 0.9014917016029358, acc: 70.3125, f1: 31.88195841716968, r: 0.4089007987485579
06/01/2019 10:37:21 step: 213, epoch: 6, batch: 14, loss: 1.1779088973999023, acc: 62.5, f1: 38.032218677379966, r: 0.4841496927107389
06/01/2019 10:37:21 step: 218, epoch: 6, batch: 19, loss: 1.0197666883468628, acc: 67.1875, f1: 37.62024182713838, r: 0.4454486903903868
06/01/2019 10:37:22 step: 223, epoch: 6, batch: 24, loss: 1.1343450546264648, acc: 62.5, f1: 35.001665001665, r: 0.3934500529674066
06/01/2019 10:37:23 step: 228, epoch: 6, batch: 29, loss: 1.1630574464797974, acc: 62.5, f1: 31.722334682860996, r: 0.47842902312920343
06/01/2019 10:37:23 *** evaluating ***
06/01/2019 10:37:23 step: 7, epoch: 6, acc: 52.991452991452995, f1: 16.43430490035621, r: 0.23211756971992242
06/01/2019 10:37:23 *** epoch: 8 ***
06/01/2019 10:37:23 *** training ***
06/01/2019 10:37:24 step: 236, epoch: 7, batch: 4, loss: 0.9491993188858032, acc: 73.4375, f1: 44.10725580554802, r: 0.47305264498148547
06/01/2019 10:37:25 step: 241, epoch: 7, batch: 9, loss: 0.6599303483963013, acc: 81.25, f1: 55.23499224251104, r: 0.5174214845797912
06/01/2019 10:37:26 step: 246, epoch: 7, batch: 14, loss: 0.8911218047142029, acc: 65.625, f1: 32.31292517006803, r: 0.46057064188801267
06/01/2019 10:37:26 step: 251, epoch: 7, batch: 19, loss: 0.7434557676315308, acc: 79.6875, f1: 61.00607760635811, r: 0.5238167565472687
06/01/2019 10:37:27 step: 256, epoch: 7, batch: 24, loss: 0.7865747213363647, acc: 75.0, f1: 43.44431738212526, r: 0.6080176666329119
06/01/2019 10:37:28 step: 261, epoch: 7, batch: 29, loss: 0.9218120574951172, acc: 67.1875, f1: 42.1029341029341, r: 0.5401893758159966
06/01/2019 10:37:28 *** evaluating ***
06/01/2019 10:37:28 step: 8, epoch: 7, acc: 54.700854700854705, f1: 18.990746302936294, r: 0.24934412853025967
06/01/2019 10:37:28 *** epoch: 9 ***
06/01/2019 10:37:28 *** training ***
06/01/2019 10:37:29 step: 269, epoch: 8, batch: 4, loss: 0.5117931365966797, acc: 84.375, f1: 61.34190969031614, r: 0.5275954757896237
06/01/2019 10:37:30 step: 274, epoch: 8, batch: 9, loss: 0.6697076559066772, acc: 85.9375, f1: 60.65693701723114, r: 0.5840990287104408
06/01/2019 10:37:30 step: 279, epoch: 8, batch: 14, loss: 0.5218866467475891, acc: 85.9375, f1: 64.27675371223758, r: 0.44050565092724286
06/01/2019 10:37:31 step: 284, epoch: 8, batch: 19, loss: 0.586645781993866, acc: 81.25, f1: 59.87339711480666, r: 0.5429376294649247
06/01/2019 10:37:32 step: 289, epoch: 8, batch: 24, loss: 0.5553330183029175, acc: 84.375, f1: 49.88162878787878, r: 0.544692139262191
06/01/2019 10:37:33 step: 294, epoch: 8, batch: 29, loss: 0.4651120603084564, acc: 85.9375, f1: 70.42240355535114, r: 0.6646246950611284
06/01/2019 10:37:33 *** evaluating ***
06/01/2019 10:37:33 step: 9, epoch: 8, acc: 50.85470085470085, f1: 19.59660317547072, r: 0.25425751677644026
06/01/2019 10:37:33 *** epoch: 10 ***
06/01/2019 10:37:33 *** training ***
06/01/2019 10:37:34 step: 302, epoch: 9, batch: 4, loss: 0.335340678691864, acc: 89.0625, f1: 86.72000514105778, r: 0.6247592293884046
06/01/2019 10:37:35 step: 307, epoch: 9, batch: 9, loss: 0.4179903268814087, acc: 89.0625, f1: 86.65163034910934, r: 0.6488762981510173
06/01/2019 10:37:35 step: 312, epoch: 9, batch: 14, loss: 0.2963202893733978, acc: 95.3125, f1: 82.71132376395535, r: 0.6165362836431274
06/01/2019 10:37:36 step: 317, epoch: 9, batch: 19, loss: 0.4049152135848999, acc: 85.9375, f1: 67.96497892086127, r: 0.7322623417055797
06/01/2019 10:37:37 step: 322, epoch: 9, batch: 24, loss: 0.3824782371520996, acc: 89.0625, f1: 62.21626199887069, r: 0.6819496270137273
06/01/2019 10:37:37 step: 327, epoch: 9, batch: 29, loss: 0.3188043236732483, acc: 92.1875, f1: 78.5312429569529, r: 0.7197261578695757
06/01/2019 10:37:38 *** evaluating ***
06/01/2019 10:37:38 step: 10, epoch: 9, acc: 55.55555555555556, f1: 21.551058974083574, r: 0.2844683391602361
06/01/2019 10:37:38 *** epoch: 11 ***
06/01/2019 10:37:38 *** training ***
06/01/2019 10:37:39 step: 335, epoch: 10, batch: 4, loss: 0.20324841141700745, acc: 95.3125, f1: 97.76199272812573, r: 0.7459464383649814
06/01/2019 10:37:39 step: 340, epoch: 10, batch: 9, loss: 0.2964739203453064, acc: 90.625, f1: 74.31536338737344, r: 0.7294161006521267
06/01/2019 10:37:40 step: 345, epoch: 10, batch: 14, loss: 0.23045672476291656, acc: 95.3125, f1: 81.2758669901527, r: 0.6146791481816463
06/01/2019 10:37:41 step: 350, epoch: 10, batch: 19, loss: 0.292024165391922, acc: 92.1875, f1: 87.86120899972893, r: 0.6434568916557052
06/01/2019 10:37:42 step: 355, epoch: 10, batch: 24, loss: 0.3641211986541748, acc: 89.0625, f1: 74.75503663003663, r: 0.6697398415424077
06/01/2019 10:37:42 step: 360, epoch: 10, batch: 29, loss: 0.2635871171951294, acc: 92.1875, f1: 91.92630095516466, r: 0.6819113356102456
06/01/2019 10:37:43 *** evaluating ***
06/01/2019 10:37:43 step: 11, epoch: 10, acc: 58.119658119658126, f1: 22.834445062071524, r: 0.27372123152861144
06/01/2019 10:37:43 *** epoch: 12 ***
06/01/2019 10:37:43 *** training ***
06/01/2019 10:37:44 step: 368, epoch: 11, batch: 4, loss: 0.10926420241594315, acc: 98.4375, f1: 96.95652173913044, r: 0.7604795992595772
06/01/2019 10:37:44 step: 373, epoch: 11, batch: 9, loss: 0.18911348283290863, acc: 96.875, f1: 98.27835288704854, r: 0.6775286944938956
06/01/2019 10:37:45 step: 378, epoch: 11, batch: 14, loss: 0.12794998288154602, acc: 96.875, f1: 97.51275510204081, r: 0.6852920999199004
06/01/2019 10:37:46 step: 383, epoch: 11, batch: 19, loss: 0.24858418107032776, acc: 93.75, f1: 92.22222222222221, r: 0.7987682870600497
06/01/2019 10:37:47 step: 388, epoch: 11, batch: 24, loss: 0.24217665195465088, acc: 95.3125, f1: 96.27833023418762, r: 0.6503093701636409
06/01/2019 10:37:47 step: 393, epoch: 11, batch: 29, loss: 0.2426249384880066, acc: 92.1875, f1: 76.73118160307601, r: 0.7273192261154956
06/01/2019 10:37:48 *** evaluating ***
06/01/2019 10:37:48 step: 12, epoch: 11, acc: 56.41025641025641, f1: 21.059561884455274, r: 0.29580204187161985
06/01/2019 10:37:48 *** epoch: 13 ***
06/01/2019 10:37:48 *** training ***
06/01/2019 10:37:49 step: 401, epoch: 12, batch: 4, loss: 0.07384602725505829, acc: 100.0, f1: 100.0, r: 0.7609067145389796
06/01/2019 10:37:49 step: 406, epoch: 12, batch: 9, loss: 0.17421424388885498, acc: 96.875, f1: 93.4920634920635, r: 0.7380744168320702
06/01/2019 10:37:50 step: 411, epoch: 12, batch: 14, loss: 0.09363862872123718, acc: 98.4375, f1: 98.17219817219816, r: 0.6691050106569351
06/01/2019 10:37:51 step: 416, epoch: 12, batch: 19, loss: 0.1813202202320099, acc: 95.3125, f1: 83.6802086802087, r: 0.7759051080492523
06/01/2019 10:37:52 step: 421, epoch: 12, batch: 24, loss: 0.12657684087753296, acc: 98.4375, f1: 99.18367346938776, r: 0.6552312618544632
06/01/2019 10:37:52 step: 426, epoch: 12, batch: 29, loss: 0.11028581857681274, acc: 100.0, f1: 100.0, r: 0.6486301091985197
06/01/2019 10:37:53 *** evaluating ***
06/01/2019 10:37:53 step: 13, epoch: 12, acc: 59.82905982905983, f1: 23.606071696420088, r: 0.28947242225235764
06/01/2019 10:37:53 *** epoch: 14 ***
06/01/2019 10:37:53 *** training ***
06/01/2019 10:37:54 step: 434, epoch: 13, batch: 4, loss: 0.08699566125869751, acc: 98.4375, f1: 91.66666666666666, r: 0.7477852778145673
06/01/2019 10:37:54 step: 439, epoch: 13, batch: 9, loss: 0.08664768189191818, acc: 100.0, f1: 100.0, r: 0.7859070009561401
06/01/2019 10:37:55 step: 444, epoch: 13, batch: 14, loss: 0.098362036049366, acc: 98.4375, f1: 98.36363636363636, r: 0.7720806941694107
06/01/2019 10:37:56 step: 449, epoch: 13, batch: 19, loss: 0.08111917972564697, acc: 98.4375, f1: 99.18992884510126, r: 0.7121608445951727
06/01/2019 10:37:57 step: 454, epoch: 13, batch: 24, loss: 0.0715603157877922, acc: 100.0, f1: 100.0, r: 0.7656176284284926
06/01/2019 10:37:57 step: 459, epoch: 13, batch: 29, loss: 0.07764240354299545, acc: 100.0, f1: 100.0, r: 0.7046181328594137
06/01/2019 10:37:58 *** evaluating ***
06/01/2019 10:37:58 step: 14, epoch: 13, acc: 58.54700854700855, f1: 23.66211973400018, r: 0.28942252175772076
06/01/2019 10:37:58 *** epoch: 15 ***
06/01/2019 10:37:58 *** training ***
06/01/2019 10:37:59 step: 467, epoch: 14, batch: 4, loss: 0.047512516379356384, acc: 100.0, f1: 100.0, r: 0.8125943599135519
06/01/2019 10:37:59 step: 472, epoch: 14, batch: 9, loss: 0.06524313986301422, acc: 98.4375, f1: 96.52173913043478, r: 0.7210256552818514
06/01/2019 10:38:00 step: 477, epoch: 14, batch: 14, loss: 0.06037994846701622, acc: 100.0, f1: 100.0, r: 0.7281969522488726
06/01/2019 10:38:01 step: 482, epoch: 14, batch: 19, loss: 0.04487837851047516, acc: 100.0, f1: 100.0, r: 0.6924140603008573
06/01/2019 10:38:01 step: 487, epoch: 14, batch: 24, loss: 0.0744098499417305, acc: 100.0, f1: 100.0, r: 0.6776983479511851
06/01/2019 10:38:02 step: 492, epoch: 14, batch: 29, loss: 0.10203410685062408, acc: 98.4375, f1: 99.22067268252665, r: 0.7710395223323542
06/01/2019 10:38:02 *** evaluating ***
06/01/2019 10:38:03 step: 15, epoch: 14, acc: 52.991452991452995, f1: 18.435576325062062, r: 0.27281941215318667
06/01/2019 10:38:03 *** epoch: 16 ***
06/01/2019 10:38:03 *** training ***
06/01/2019 10:38:03 step: 500, epoch: 15, batch: 4, loss: 0.0815449059009552, acc: 98.4375, f1: 97.22222222222221, r: 0.7022728630802071
06/01/2019 10:38:04 step: 505, epoch: 15, batch: 9, loss: 0.04025456681847572, acc: 100.0, f1: 100.0, r: 0.7994956733343394
06/01/2019 10:38:05 step: 510, epoch: 15, batch: 14, loss: 0.040038153529167175, acc: 100.0, f1: 100.0, r: 0.7947061484537387
06/01/2019 10:38:06 step: 515, epoch: 15, batch: 19, loss: 0.051430556923151016, acc: 98.4375, f1: 99.29118773946361, r: 0.7576354022101495
06/01/2019 10:38:06 step: 520, epoch: 15, batch: 24, loss: 0.03481200709939003, acc: 100.0, f1: 100.0, r: 0.7848953078351073
06/01/2019 10:38:07 step: 525, epoch: 15, batch: 29, loss: 0.03043293207883835, acc: 100.0, f1: 100.0, r: 0.6697842055713041
06/01/2019 10:38:07 *** evaluating ***
06/01/2019 10:38:08 step: 16, epoch: 15, acc: 58.54700854700855, f1: 25.568285877807984, r: 0.2833486657854498
06/01/2019 10:38:08 *** epoch: 17 ***
06/01/2019 10:38:08 *** training ***
06/01/2019 10:38:08 step: 533, epoch: 16, batch: 4, loss: 0.06399551033973694, acc: 98.4375, f1: 98.63523573200992, r: 0.7957018162649347
06/01/2019 10:38:09 step: 538, epoch: 16, batch: 9, loss: 0.02729830890893936, acc: 100.0, f1: 100.0, r: 0.7452384615295289
06/01/2019 10:38:10 step: 543, epoch: 16, batch: 14, loss: 0.04102105647325516, acc: 98.4375, f1: 99.38536306460834, r: 0.8054961494396716
06/01/2019 10:38:10 step: 548, epoch: 16, batch: 19, loss: 0.03000733256340027, acc: 100.0, f1: 100.0, r: 0.67063810191991
06/01/2019 10:38:11 step: 553, epoch: 16, batch: 24, loss: 0.05047241225838661, acc: 98.4375, f1: 99.20141969831411, r: 0.7671550752515433
06/01/2019 10:38:12 step: 558, epoch: 16, batch: 29, loss: 0.051790330559015274, acc: 98.4375, f1: 96.70995670995671, r: 0.7417437076149354
06/01/2019 10:38:12 *** evaluating ***
06/01/2019 10:38:13 step: 17, epoch: 16, acc: 57.692307692307686, f1: 23.61810390453638, r: 0.2797445412576504
06/01/2019 10:38:13 *** epoch: 18 ***
06/01/2019 10:38:13 *** training ***
06/01/2019 10:38:13 step: 566, epoch: 17, batch: 4, loss: 0.044239792972803116, acc: 100.0, f1: 100.0, r: 0.6344249459887769
06/01/2019 10:38:14 step: 571, epoch: 17, batch: 9, loss: 0.03184884041547775, acc: 100.0, f1: 100.0, r: 0.7962098428372194
06/01/2019 10:38:15 step: 576, epoch: 17, batch: 14, loss: 0.013471581041812897, acc: 100.0, f1: 100.0, r: 0.6592002025265343
06/01/2019 10:38:15 step: 581, epoch: 17, batch: 19, loss: 0.03658895194530487, acc: 100.0, f1: 100.0, r: 0.6780016546308765
06/01/2019 10:38:16 step: 586, epoch: 17, batch: 24, loss: 0.019466184079647064, acc: 100.0, f1: 100.0, r: 0.6024766994988898
06/01/2019 10:38:17 step: 591, epoch: 17, batch: 29, loss: 0.023180320858955383, acc: 100.0, f1: 100.0, r: 0.730829260530319
06/01/2019 10:38:17 *** evaluating ***
06/01/2019 10:38:17 step: 18, epoch: 17, acc: 57.692307692307686, f1: 26.458654340010273, r: 0.28089147985231916
06/01/2019 10:38:17 *** epoch: 19 ***
06/01/2019 10:38:17 *** training ***
06/01/2019 10:38:18 step: 599, epoch: 18, batch: 4, loss: 0.021757595241069794, acc: 100.0, f1: 100.0, r: 0.6744024272598308
06/01/2019 10:38:19 step: 604, epoch: 18, batch: 9, loss: 0.04587670415639877, acc: 98.4375, f1: 97.59288330716902, r: 0.6867763914745579
06/01/2019 10:38:20 step: 609, epoch: 18, batch: 14, loss: 0.018147867172956467, acc: 100.0, f1: 100.0, r: 0.7669950832780229
06/01/2019 10:38:20 step: 614, epoch: 18, batch: 19, loss: 0.08802841603755951, acc: 96.875, f1: 98.42335972850678, r: 0.7351150394149756
06/01/2019 10:38:21 step: 619, epoch: 18, batch: 24, loss: 0.07263769209384918, acc: 96.875, f1: 94.56439393939394, r: 0.7569394664300237
06/01/2019 10:38:22 step: 624, epoch: 18, batch: 29, loss: 0.040892962366342545, acc: 100.0, f1: 100.0, r: 0.8045064768481001
06/01/2019 10:38:22 *** evaluating ***
06/01/2019 10:38:22 step: 19, epoch: 18, acc: 57.26495726495726, f1: 25.52380250656113, r: 0.2648058047870991
06/01/2019 10:38:22 *** epoch: 20 ***
06/01/2019 10:38:22 *** training ***
06/01/2019 10:38:23 step: 632, epoch: 19, batch: 4, loss: 0.016201090067625046, acc: 100.0, f1: 100.0, r: 0.7427426689839837
06/01/2019 10:38:24 step: 637, epoch: 19, batch: 9, loss: 0.024186283349990845, acc: 100.0, f1: 100.0, r: 0.694139886789234
06/01/2019 10:38:24 step: 642, epoch: 19, batch: 14, loss: 0.028307847678661346, acc: 100.0, f1: 100.0, r: 0.706499802799901
06/01/2019 10:38:25 step: 647, epoch: 19, batch: 19, loss: 0.03321849927306175, acc: 100.0, f1: 100.0, r: 0.7983373981518566
06/01/2019 10:38:26 step: 652, epoch: 19, batch: 24, loss: 0.0316564179956913, acc: 100.0, f1: 100.0, r: 0.6515757789934279
06/01/2019 10:38:27 step: 657, epoch: 19, batch: 29, loss: 0.017204590141773224, acc: 100.0, f1: 100.0, r: 0.6524564570267398
06/01/2019 10:38:27 *** evaluating ***
06/01/2019 10:38:27 step: 20, epoch: 19, acc: 56.41025641025641, f1: 22.592556085868615, r: 0.26963988624168767
06/01/2019 10:38:27 *** epoch: 21 ***
06/01/2019 10:38:27 *** training ***
06/01/2019 10:38:28 step: 665, epoch: 20, batch: 4, loss: 0.01784275472164154, acc: 100.0, f1: 100.0, r: 0.7477942538523381
06/01/2019 10:38:29 step: 670, epoch: 20, batch: 9, loss: 0.04691407456994057, acc: 100.0, f1: 100.0, r: 0.7473858779024801
06/01/2019 10:38:29 step: 675, epoch: 20, batch: 14, loss: 0.026252660900354385, acc: 100.0, f1: 100.0, r: 0.7159993292727641
06/01/2019 10:38:30 step: 680, epoch: 20, batch: 19, loss: 0.0744766965508461, acc: 98.4375, f1: 98.4006734006734, r: 0.8031187170869679
06/01/2019 10:38:31 step: 685, epoch: 20, batch: 24, loss: 0.03026682883501053, acc: 100.0, f1: 100.0, r: 0.6977689915962172
06/01/2019 10:38:32 step: 690, epoch: 20, batch: 29, loss: 0.03580712899565697, acc: 100.0, f1: 100.0, r: 0.7597268567120626
06/01/2019 10:38:32 *** evaluating ***
06/01/2019 10:38:32 step: 21, epoch: 20, acc: 58.97435897435898, f1: 25.297619047619047, r: 0.27349265451285204
06/01/2019 10:38:32 *** epoch: 22 ***
06/01/2019 10:38:32 *** training ***
06/01/2019 10:38:33 step: 698, epoch: 21, batch: 4, loss: 0.011105194687843323, acc: 100.0, f1: 100.0, r: 0.6633417107221057
06/01/2019 10:38:34 step: 703, epoch: 21, batch: 9, loss: 0.031922027468681335, acc: 100.0, f1: 100.0, r: 0.7916482959667198
06/01/2019 10:38:34 step: 708, epoch: 21, batch: 14, loss: 0.012136273086071014, acc: 100.0, f1: 100.0, r: 0.6853972317791067
06/01/2019 10:38:35 step: 713, epoch: 21, batch: 19, loss: 0.0168452188372612, acc: 100.0, f1: 100.0, r: 0.6814456672624566
06/01/2019 10:38:36 step: 718, epoch: 21, batch: 24, loss: 0.025802146643400192, acc: 100.0, f1: 100.0, r: 0.8343845373127174
06/01/2019 10:38:36 step: 723, epoch: 21, batch: 29, loss: 0.015569765120744705, acc: 100.0, f1: 100.0, r: 0.733428309476003
06/01/2019 10:38:37 *** evaluating ***
06/01/2019 10:38:37 step: 22, epoch: 21, acc: 57.26495726495726, f1: 22.6130148005148, r: 0.27947819757718306
06/01/2019 10:38:37 *** epoch: 23 ***
06/01/2019 10:38:37 *** training ***
06/01/2019 10:38:38 step: 731, epoch: 22, batch: 4, loss: 0.018657170236110687, acc: 100.0, f1: 100.0, r: 0.7808294836935699
06/01/2019 10:38:38 step: 736, epoch: 22, batch: 9, loss: 0.030328068882226944, acc: 98.4375, f1: 98.99355877616746, r: 0.8007852247156149
06/01/2019 10:38:39 step: 741, epoch: 22, batch: 14, loss: 0.03119003027677536, acc: 98.4375, f1: 99.09876994275972, r: 0.7032929112915853
06/01/2019 10:38:40 step: 746, epoch: 22, batch: 19, loss: 0.01132204756140709, acc: 100.0, f1: 100.0, r: 0.6906857161158285
06/01/2019 10:38:41 step: 751, epoch: 22, batch: 24, loss: 0.030098162591457367, acc: 98.4375, f1: 99.26314819931841, r: 0.6570938014655313
06/01/2019 10:38:41 step: 756, epoch: 22, batch: 29, loss: 0.03916214779019356, acc: 100.0, f1: 100.0, r: 0.6074558241032514
06/01/2019 10:38:42 *** evaluating ***
06/01/2019 10:38:42 step: 23, epoch: 22, acc: 58.97435897435898, f1: 23.64352528363861, r: 0.2923167289196212
06/01/2019 10:38:42 *** epoch: 24 ***
06/01/2019 10:38:42 *** training ***
06/01/2019 10:38:43 step: 764, epoch: 23, batch: 4, loss: 0.016324400901794434, acc: 100.0, f1: 100.0, r: 0.8250777224641209
06/01/2019 10:38:43 step: 769, epoch: 23, batch: 9, loss: 0.012323737144470215, acc: 100.0, f1: 100.0, r: 0.6792751391584019
06/01/2019 10:38:44 step: 774, epoch: 23, batch: 14, loss: 0.030201055109500885, acc: 98.4375, f1: 98.32967032967034, r: 0.6502999698818035
06/01/2019 10:38:45 step: 779, epoch: 23, batch: 19, loss: 0.016351405531167984, acc: 100.0, f1: 100.0, r: 0.7475784833580138
06/01/2019 10:38:46 step: 784, epoch: 23, batch: 24, loss: 0.011352188885211945, acc: 100.0, f1: 100.0, r: 0.7115514005788129
06/01/2019 10:38:46 step: 789, epoch: 23, batch: 29, loss: 0.02112603187561035, acc: 100.0, f1: 100.0, r: 0.8099368409057701
06/01/2019 10:38:47 *** evaluating ***
06/01/2019 10:38:47 step: 24, epoch: 23, acc: 58.97435897435898, f1: 24.35191165059181, r: 0.291604981046574
06/01/2019 10:38:47 *** epoch: 25 ***
06/01/2019 10:38:47 *** training ***
06/01/2019 10:38:48 step: 797, epoch: 24, batch: 4, loss: 0.009981945157051086, acc: 100.0, f1: 100.0, r: 0.6627436298325773
06/01/2019 10:38:48 step: 802, epoch: 24, batch: 9, loss: 0.017079051584005356, acc: 100.0, f1: 100.0, r: 0.6670786483641445
06/01/2019 10:38:49 step: 807, epoch: 24, batch: 14, loss: 0.008293330669403076, acc: 100.0, f1: 100.0, r: 0.753626799878444
06/01/2019 10:38:50 step: 812, epoch: 24, batch: 19, loss: 0.028290163725614548, acc: 100.0, f1: 100.0, r: 0.7753635534093329
06/01/2019 10:38:51 step: 817, epoch: 24, batch: 24, loss: 0.022190354764461517, acc: 100.0, f1: 100.0, r: 0.8176326920859868
06/01/2019 10:38:51 step: 822, epoch: 24, batch: 29, loss: 0.012181274592876434, acc: 100.0, f1: 100.0, r: 0.7821473402232239
06/01/2019 10:38:52 *** evaluating ***
06/01/2019 10:38:52 step: 25, epoch: 24, acc: 59.82905982905983, f1: 23.838168525668525, r: 0.2953384911376638
06/01/2019 10:38:52 *** epoch: 26 ***
06/01/2019 10:38:52 *** training ***
06/01/2019 10:38:53 step: 830, epoch: 25, batch: 4, loss: 0.008923597633838654, acc: 100.0, f1: 100.0, r: 0.7256503364520052
06/01/2019 10:38:53 step: 835, epoch: 25, batch: 9, loss: 0.00976436585187912, acc: 100.0, f1: 100.0, r: 0.7814421038136291
06/01/2019 10:38:54 step: 840, epoch: 25, batch: 14, loss: 0.025676313787698746, acc: 100.0, f1: 100.0, r: 0.6111177374530516
06/01/2019 10:38:55 step: 845, epoch: 25, batch: 19, loss: 0.05093963444232941, acc: 98.4375, f1: 99.11483253588517, r: 0.7663827559779904
06/01/2019 10:38:55 step: 850, epoch: 25, batch: 24, loss: 0.013784784823656082, acc: 100.0, f1: 100.0, r: 0.7362331957258482
06/01/2019 10:38:56 step: 855, epoch: 25, batch: 29, loss: 0.009328708052635193, acc: 100.0, f1: 100.0, r: 0.6659994847288567
06/01/2019 10:38:57 *** evaluating ***
06/01/2019 10:38:57 step: 26, epoch: 25, acc: 58.54700854700855, f1: 23.88673979635078, r: 0.28217136958417266
06/01/2019 10:38:57 *** epoch: 27 ***
06/01/2019 10:38:57 *** training ***
06/01/2019 10:38:58 step: 863, epoch: 26, batch: 4, loss: 0.015492461621761322, acc: 100.0, f1: 100.0, r: 0.7190067433563689
06/01/2019 10:38:58 step: 868, epoch: 26, batch: 9, loss: 0.0250077024102211, acc: 98.4375, f1: 99.17935428139509, r: 0.7522374630843419
06/01/2019 10:38:59 step: 873, epoch: 26, batch: 14, loss: 0.024098996073007584, acc: 100.0, f1: 100.0, r: 0.7390706088871531
06/01/2019 10:39:00 step: 878, epoch: 26, batch: 19, loss: 0.02095804363489151, acc: 100.0, f1: 100.0, r: 0.7896859125088318
06/01/2019 10:39:01 step: 883, epoch: 26, batch: 24, loss: 0.0135786272585392, acc: 100.0, f1: 100.0, r: 0.670281782459638
06/01/2019 10:39:01 step: 888, epoch: 26, batch: 29, loss: 0.003011271357536316, acc: 100.0, f1: 100.0, r: 0.8038832513056872
06/01/2019 10:39:02 *** evaluating ***
06/01/2019 10:39:02 step: 27, epoch: 26, acc: 58.119658119658126, f1: 23.939676593141932, r: 0.2891651647710257
06/01/2019 10:39:02 *** epoch: 28 ***
06/01/2019 10:39:02 *** training ***
06/01/2019 10:39:03 step: 896, epoch: 27, batch: 4, loss: 0.040198251605033875, acc: 98.4375, f1: 96.76470588235294, r: 0.8081753629107897
06/01/2019 10:39:03 step: 901, epoch: 27, batch: 9, loss: 0.03455844521522522, acc: 98.4375, f1: 99.06142167011733, r: 0.7224571398372636
06/01/2019 10:39:04 step: 906, epoch: 27, batch: 14, loss: 0.04164287447929382, acc: 98.4375, f1: 98.8795518207283, r: 0.6053279971394427
06/01/2019 10:39:05 step: 911, epoch: 27, batch: 19, loss: 0.014934666454792023, acc: 100.0, f1: 100.0, r: 0.7609103147700461
06/01/2019 10:39:05 step: 916, epoch: 27, batch: 24, loss: 0.024829011410474777, acc: 100.0, f1: 100.0, r: 0.7298619619249684
06/01/2019 10:39:06 step: 921, epoch: 27, batch: 29, loss: 0.011016331613063812, acc: 100.0, f1: 100.0, r: 0.8038269155592503
06/01/2019 10:39:07 *** evaluating ***
06/01/2019 10:39:07 step: 28, epoch: 27, acc: 58.119658119658126, f1: 24.661560241241194, r: 0.2811200127842466
06/01/2019 10:39:07 *** epoch: 29 ***
06/01/2019 10:39:07 *** training ***
06/01/2019 10:39:08 step: 929, epoch: 28, batch: 4, loss: 0.014796707779169083, acc: 100.0, f1: 100.0, r: 0.8066973521762779
06/01/2019 10:39:08 step: 934, epoch: 28, batch: 9, loss: 0.01924486644566059, acc: 100.0, f1: 100.0, r: 0.7033354815717481
06/01/2019 10:39:09 step: 939, epoch: 28, batch: 14, loss: 0.026317916810512543, acc: 100.0, f1: 100.0, r: 0.7943957754317841
06/01/2019 10:39:10 step: 944, epoch: 28, batch: 19, loss: 0.023795686662197113, acc: 100.0, f1: 100.0, r: 0.6742408636565963
06/01/2019 10:39:11 step: 949, epoch: 28, batch: 24, loss: 0.014906592667102814, acc: 100.0, f1: 100.0, r: 0.8268721867956944
06/01/2019 10:39:11 step: 954, epoch: 28, batch: 29, loss: 0.01041494682431221, acc: 100.0, f1: 100.0, r: 0.746398666770012
06/01/2019 10:39:12 *** evaluating ***
06/01/2019 10:39:12 step: 29, epoch: 28, acc: 58.119658119658126, f1: 23.52017195767196, r: 0.2824787998887822
06/01/2019 10:39:12 *** epoch: 30 ***
06/01/2019 10:39:12 *** training ***
06/01/2019 10:39:13 step: 962, epoch: 29, batch: 4, loss: 0.0093921460211277, acc: 100.0, f1: 100.0, r: 0.6754815091329003
06/01/2019 10:39:13 step: 967, epoch: 29, batch: 9, loss: 0.011100295931100845, acc: 100.0, f1: 100.0, r: 0.6450752393865115
06/01/2019 10:39:14 step: 972, epoch: 29, batch: 14, loss: 0.009937554597854614, acc: 100.0, f1: 100.0, r: 0.7682540345220781
06/01/2019 10:39:15 step: 977, epoch: 29, batch: 19, loss: 0.01602734997868538, acc: 100.0, f1: 100.0, r: 0.7567579054403811
06/01/2019 10:39:15 step: 982, epoch: 29, batch: 24, loss: 0.017937984317541122, acc: 100.0, f1: 100.0, r: 0.7429704484045792
06/01/2019 10:39:16 step: 987, epoch: 29, batch: 29, loss: 0.008455231785774231, acc: 100.0, f1: 100.0, r: 0.7367400182701992
06/01/2019 10:39:16 *** evaluating ***
06/01/2019 10:39:17 step: 30, epoch: 29, acc: 57.26495726495726, f1: 23.696741407248112, r: 0.2858073806666143
06/01/2019 10:39:17 *** epoch: 31 ***
06/01/2019 10:39:17 *** training ***
06/01/2019 10:39:17 step: 995, epoch: 30, batch: 4, loss: 0.011938147246837616, acc: 100.0, f1: 100.0, r: 0.7535294969628418
06/01/2019 10:39:18 step: 1000, epoch: 30, batch: 9, loss: 0.014968566596508026, acc: 100.0, f1: 100.0, r: 0.6722157794976964
06/01/2019 10:39:19 step: 1005, epoch: 30, batch: 14, loss: 0.013379167765378952, acc: 100.0, f1: 100.0, r: 0.7214697127431728
06/01/2019 10:39:20 step: 1010, epoch: 30, batch: 19, loss: 0.005833268165588379, acc: 100.0, f1: 100.0, r: 0.7484172307803412
06/01/2019 10:39:20 step: 1015, epoch: 30, batch: 24, loss: 0.005908973515033722, acc: 100.0, f1: 100.0, r: 0.7490543171773602
06/01/2019 10:39:21 step: 1020, epoch: 30, batch: 29, loss: 0.020038317888975143, acc: 100.0, f1: 100.0, r: 0.743906170921111
06/01/2019 10:39:21 *** evaluating ***
06/01/2019 10:39:22 step: 31, epoch: 30, acc: 56.837606837606835, f1: 23.312073449727116, r: 0.2815737617378794
06/01/2019 10:39:22 *** epoch: 32 ***
06/01/2019 10:39:22 *** training ***
06/01/2019 10:39:22 step: 1028, epoch: 31, batch: 4, loss: 0.011840548366308212, acc: 100.0, f1: 100.0, r: 0.6893480510935858
06/01/2019 10:39:23 step: 1033, epoch: 31, batch: 9, loss: 0.017480306327342987, acc: 100.0, f1: 100.0, r: 0.7270317870133808
06/01/2019 10:39:24 step: 1038, epoch: 31, batch: 14, loss: 0.013394538313150406, acc: 100.0, f1: 100.0, r: 0.6688973389757887
06/01/2019 10:39:25 step: 1043, epoch: 31, batch: 19, loss: 0.009135313332080841, acc: 100.0, f1: 100.0, r: 0.8284853402676934
06/01/2019 10:39:25 step: 1048, epoch: 31, batch: 24, loss: 0.012830018997192383, acc: 100.0, f1: 100.0, r: 0.5183461721159528
06/01/2019 10:39:26 step: 1053, epoch: 31, batch: 29, loss: 0.009832121431827545, acc: 100.0, f1: 100.0, r: 0.760577959259598
06/01/2019 10:39:26 *** evaluating ***
06/01/2019 10:39:27 step: 32, epoch: 31, acc: 58.119658119658126, f1: 23.742159277504104, r: 0.2890010075925064
06/01/2019 10:39:27 *** epoch: 33 ***
06/01/2019 10:39:27 *** training ***
06/01/2019 10:39:27 step: 1061, epoch: 32, batch: 4, loss: 0.00480341911315918, acc: 100.0, f1: 100.0, r: 0.7349351068078386
06/01/2019 10:39:28 step: 1066, epoch: 32, batch: 9, loss: 0.00825829803943634, acc: 100.0, f1: 100.0, r: 0.6925666815670566
06/01/2019 10:39:29 step: 1071, epoch: 32, batch: 14, loss: 0.009350281208753586, acc: 100.0, f1: 100.0, r: 0.7641192960167265
06/01/2019 10:39:30 step: 1076, epoch: 32, batch: 19, loss: 0.004167288541793823, acc: 100.0, f1: 100.0, r: 0.8167003440528622
06/01/2019 10:39:30 step: 1081, epoch: 32, batch: 24, loss: 0.005379222333431244, acc: 100.0, f1: 100.0, r: 0.7142452240925362
06/01/2019 10:39:31 step: 1086, epoch: 32, batch: 29, loss: 0.010154780000448227, acc: 100.0, f1: 100.0, r: 0.6432496673840946
06/01/2019 10:39:31 *** evaluating ***
06/01/2019 10:39:32 step: 33, epoch: 32, acc: 59.401709401709404, f1: 24.570254196864084, r: 0.2813969695485508
06/01/2019 10:39:32 *** epoch: 34 ***
06/01/2019 10:39:32 *** training ***
06/01/2019 10:39:32 step: 1094, epoch: 33, batch: 4, loss: 0.03327331691980362, acc: 98.4375, f1: 99.13675123697232, r: 0.72154175088585
06/01/2019 10:39:33 step: 1099, epoch: 33, batch: 9, loss: 0.01662677899003029, acc: 100.0, f1: 100.0, r: 0.708521708464149
06/01/2019 10:39:34 step: 1104, epoch: 33, batch: 14, loss: 0.01170264557003975, acc: 100.0, f1: 100.0, r: 0.6558097305047247
06/01/2019 10:39:34 step: 1109, epoch: 33, batch: 19, loss: 0.0035792961716651917, acc: 100.0, f1: 100.0, r: 0.8446370202387024
06/01/2019 10:39:35 step: 1114, epoch: 33, batch: 24, loss: 0.00694701075553894, acc: 100.0, f1: 100.0, r: 0.8227973426769855
06/01/2019 10:39:36 step: 1119, epoch: 33, batch: 29, loss: 0.008270423859357834, acc: 100.0, f1: 100.0, r: 0.7112904436139191
06/01/2019 10:39:36 *** evaluating ***
06/01/2019 10:39:37 step: 34, epoch: 33, acc: 58.97435897435898, f1: 24.11366881382903, r: 0.2876909663609746
06/01/2019 10:39:37 *** epoch: 35 ***
06/01/2019 10:39:37 *** training ***
06/01/2019 10:39:37 step: 1127, epoch: 34, batch: 4, loss: 0.029887385666370392, acc: 98.4375, f1: 99.24845269672856, r: 0.8146545191339194
06/01/2019 10:39:38 step: 1132, epoch: 34, batch: 9, loss: 0.010559119284152985, acc: 100.0, f1: 100.0, r: 0.8107018173032619
06/01/2019 10:39:39 step: 1137, epoch: 34, batch: 14, loss: 0.004636704921722412, acc: 100.0, f1: 100.0, r: 0.8193247469436655
06/01/2019 10:39:40 step: 1142, epoch: 34, batch: 19, loss: 0.0135117769241333, acc: 100.0, f1: 100.0, r: 0.7808658392597817
06/01/2019 10:39:40 step: 1147, epoch: 34, batch: 24, loss: 0.012452412396669388, acc: 100.0, f1: 100.0, r: 0.6265941861494199
06/01/2019 10:39:41 step: 1152, epoch: 34, batch: 29, loss: 0.00591103732585907, acc: 100.0, f1: 100.0, r: 0.7969380616648123
06/01/2019 10:39:41 *** evaluating ***
06/01/2019 10:39:42 step: 35, epoch: 34, acc: 58.119658119658126, f1: 25.91815119367678, r: 0.28750501984882587
06/01/2019 10:39:42 *** epoch: 36 ***
06/01/2019 10:39:42 *** training ***
06/01/2019 10:39:42 step: 1160, epoch: 35, batch: 4, loss: 0.003280051052570343, acc: 100.0, f1: 100.0, r: 0.8354836812971091
06/01/2019 10:39:43 step: 1165, epoch: 35, batch: 9, loss: 0.008370175957679749, acc: 100.0, f1: 100.0, r: 0.7377347564704775
06/01/2019 10:39:44 step: 1170, epoch: 35, batch: 14, loss: 0.014457538723945618, acc: 100.0, f1: 100.0, r: 0.8166088534242878
06/01/2019 10:39:44 step: 1175, epoch: 35, batch: 19, loss: 0.007197454571723938, acc: 100.0, f1: 100.0, r: 0.6658486281063218
06/01/2019 10:39:45 step: 1180, epoch: 35, batch: 24, loss: 0.008107990026473999, acc: 100.0, f1: 100.0, r: 0.7445467656663267
06/01/2019 10:39:46 step: 1185, epoch: 35, batch: 29, loss: 0.022355470806360245, acc: 100.0, f1: 100.0, r: 0.808342893094146
06/01/2019 10:39:46 *** evaluating ***
06/01/2019 10:39:47 step: 36, epoch: 35, acc: 58.97435897435898, f1: 26.220696757032457, r: 0.2875526104700955
06/01/2019 10:39:47 *** epoch: 37 ***
06/01/2019 10:39:47 *** training ***
06/01/2019 10:39:47 step: 1193, epoch: 36, batch: 4, loss: 0.01493627205491066, acc: 100.0, f1: 100.0, r: 0.686515704275041
06/01/2019 10:39:48 step: 1198, epoch: 36, batch: 9, loss: 0.003316164016723633, acc: 100.0, f1: 100.0, r: 0.770476788649196
06/01/2019 10:39:49 step: 1203, epoch: 36, batch: 14, loss: 0.0030812323093414307, acc: 100.0, f1: 100.0, r: 0.6701635243370875
06/01/2019 10:39:49 step: 1208, epoch: 36, batch: 19, loss: 0.011225298047065735, acc: 100.0, f1: 100.0, r: 0.8239077108914005
06/01/2019 10:39:50 step: 1213, epoch: 36, batch: 24, loss: 0.011288747191429138, acc: 100.0, f1: 100.0, r: 0.6604401199148634
06/01/2019 10:39:51 step: 1218, epoch: 36, batch: 29, loss: 0.005598470568656921, acc: 100.0, f1: 100.0, r: 0.7216522913630458
06/01/2019 10:39:51 *** evaluating ***
06/01/2019 10:39:52 step: 37, epoch: 36, acc: 58.54700854700855, f1: 24.73838461331395, r: 0.29309373372729874
06/01/2019 10:39:52 *** epoch: 38 ***
06/01/2019 10:39:52 *** training ***
06/01/2019 10:39:52 step: 1226, epoch: 37, batch: 4, loss: 0.009178254753351212, acc: 100.0, f1: 100.0, r: 0.735909388561421
06/01/2019 10:39:53 step: 1231, epoch: 37, batch: 9, loss: 0.0063942670822143555, acc: 100.0, f1: 100.0, r: 0.7403478291450976
06/01/2019 10:39:54 step: 1236, epoch: 37, batch: 14, loss: 0.008854750543832779, acc: 100.0, f1: 100.0, r: 0.7327683034174198
06/01/2019 10:39:54 step: 1241, epoch: 37, batch: 19, loss: 0.009483110159635544, acc: 100.0, f1: 100.0, r: 0.7766136319804713
06/01/2019 10:39:55 step: 1246, epoch: 37, batch: 24, loss: 0.003916069865226746, acc: 100.0, f1: 100.0, r: 0.6656495426576293
06/01/2019 10:39:56 step: 1251, epoch: 37, batch: 29, loss: 0.012725327163934708, acc: 100.0, f1: 100.0, r: 0.6496031228177451
06/01/2019 10:39:56 *** evaluating ***
06/01/2019 10:39:57 step: 38, epoch: 37, acc: 59.82905982905983, f1: 25.825197546683754, r: 0.30336139818201613
06/01/2019 10:39:57 *** epoch: 39 ***
06/01/2019 10:39:57 *** training ***
06/01/2019 10:39:57 step: 1259, epoch: 38, batch: 4, loss: 0.003460697829723358, acc: 100.0, f1: 100.0, r: 0.8080835862717772
06/01/2019 10:39:58 step: 1264, epoch: 38, batch: 9, loss: 0.011641725897789001, acc: 100.0, f1: 100.0, r: 0.7591069110652781
06/01/2019 10:39:59 step: 1269, epoch: 38, batch: 14, loss: 0.015434414148330688, acc: 100.0, f1: 100.0, r: 0.7635115006990534
06/01/2019 10:39:59 step: 1274, epoch: 38, batch: 19, loss: 0.005073033273220062, acc: 100.0, f1: 100.0, r: 0.7048950458868122
06/01/2019 10:40:00 step: 1279, epoch: 38, batch: 24, loss: 0.005359560251235962, acc: 100.0, f1: 100.0, r: 0.743140507039936
06/01/2019 10:40:01 step: 1284, epoch: 38, batch: 29, loss: 0.003985412418842316, acc: 100.0, f1: 100.0, r: 0.7947510787453663
06/01/2019 10:40:01 *** evaluating ***
06/01/2019 10:40:01 step: 39, epoch: 38, acc: 59.82905982905983, f1: 23.338587535396044, r: 0.29204049866479204
06/01/2019 10:40:01 *** epoch: 40 ***
06/01/2019 10:40:01 *** training ***
06/01/2019 10:40:02 step: 1292, epoch: 39, batch: 4, loss: 0.013806939125061035, acc: 100.0, f1: 100.0, r: 0.7150636207460254
06/01/2019 10:40:03 step: 1297, epoch: 39, batch: 9, loss: 0.008955143392086029, acc: 100.0, f1: 100.0, r: 0.7346270878116674
06/01/2019 10:40:04 step: 1302, epoch: 39, batch: 14, loss: 0.006670132279396057, acc: 100.0, f1: 100.0, r: 0.7101108543952638
06/01/2019 10:40:04 step: 1307, epoch: 39, batch: 19, loss: 0.008177980780601501, acc: 100.0, f1: 100.0, r: 0.7040071183917467
06/01/2019 10:40:05 step: 1312, epoch: 39, batch: 24, loss: 0.009384915232658386, acc: 100.0, f1: 100.0, r: 0.6910034493468069
06/01/2019 10:40:06 step: 1317, epoch: 39, batch: 29, loss: 0.009201139211654663, acc: 100.0, f1: 100.0, r: 0.7007352427355308
06/01/2019 10:40:06 *** evaluating ***
06/01/2019 10:40:06 step: 40, epoch: 39, acc: 58.54700854700855, f1: 25.74000277378236, r: 0.30074119238699404
06/01/2019 10:40:06 *** epoch: 41 ***
06/01/2019 10:40:06 *** training ***
06/01/2019 10:40:07 step: 1325, epoch: 40, batch: 4, loss: 0.01006985828280449, acc: 100.0, f1: 100.0, r: 0.7718199180853154
06/01/2019 10:40:08 step: 1330, epoch: 40, batch: 9, loss: 0.008581787347793579, acc: 100.0, f1: 100.0, r: 0.6706856838073242
06/01/2019 10:40:09 step: 1335, epoch: 40, batch: 14, loss: 0.00681903213262558, acc: 100.0, f1: 100.0, r: 0.7265188379962624
06/01/2019 10:40:09 step: 1340, epoch: 40, batch: 19, loss: 0.008308075368404388, acc: 100.0, f1: 100.0, r: 0.8445173040506563
06/01/2019 10:40:10 step: 1345, epoch: 40, batch: 24, loss: 0.008301876485347748, acc: 100.0, f1: 100.0, r: 0.7969468397155314
06/01/2019 10:40:11 step: 1350, epoch: 40, batch: 29, loss: 0.005603291094303131, acc: 100.0, f1: 100.0, r: 0.7403369278268548
06/01/2019 10:40:11 *** evaluating ***
06/01/2019 10:40:11 step: 41, epoch: 40, acc: 58.54700854700855, f1: 25.319218824653607, r: 0.29216596126254263
06/01/2019 10:40:11 *** epoch: 42 ***
06/01/2019 10:40:11 *** training ***
06/01/2019 10:40:12 step: 1358, epoch: 41, batch: 4, loss: 0.007111843675374985, acc: 100.0, f1: 100.0, r: 0.7564848006388568
06/01/2019 10:40:13 step: 1363, epoch: 41, batch: 9, loss: 0.009303417056798935, acc: 100.0, f1: 100.0, r: 0.8624851797303673
06/01/2019 10:40:13 step: 1368, epoch: 41, batch: 14, loss: 0.005981624126434326, acc: 100.0, f1: 100.0, r: 0.7554475487545541
06/01/2019 10:40:14 step: 1373, epoch: 41, batch: 19, loss: 0.007078684866428375, acc: 100.0, f1: 100.0, r: 0.6561077967883855
06/01/2019 10:40:15 step: 1378, epoch: 41, batch: 24, loss: 0.026594925671815872, acc: 98.4375, f1: 99.15164369034994, r: 0.8040706068938192
06/01/2019 10:40:16 step: 1383, epoch: 41, batch: 29, loss: 0.03663305938243866, acc: 98.4375, f1: 99.13024085637822, r: 0.7911016088671063
06/01/2019 10:40:16 *** evaluating ***
06/01/2019 10:40:16 step: 42, epoch: 41, acc: 58.54700854700855, f1: 25.555595418751846, r: 0.27246980329482523
06/01/2019 10:40:16 *** epoch: 43 ***
06/01/2019 10:40:16 *** training ***
06/01/2019 10:40:17 step: 1391, epoch: 42, batch: 4, loss: 0.012031994760036469, acc: 100.0, f1: 100.0, r: 0.8404881888923013
06/01/2019 10:40:18 step: 1396, epoch: 42, batch: 9, loss: 0.017014849931001663, acc: 100.0, f1: 100.0, r: 0.670206925930603
06/01/2019 10:40:18 step: 1401, epoch: 42, batch: 14, loss: 0.004889167845249176, acc: 100.0, f1: 100.0, r: 0.7102327939941672
06/01/2019 10:40:19 step: 1406, epoch: 42, batch: 19, loss: 0.009569641202688217, acc: 100.0, f1: 100.0, r: 0.8019534352097976
06/01/2019 10:40:20 step: 1411, epoch: 42, batch: 24, loss: 0.008871287107467651, acc: 100.0, f1: 100.0, r: 0.7961834094444566
06/01/2019 10:40:21 step: 1416, epoch: 42, batch: 29, loss: 0.004208333790302277, acc: 100.0, f1: 100.0, r: 0.6907383813895982
06/01/2019 10:40:21 *** evaluating ***
06/01/2019 10:40:21 step: 43, epoch: 42, acc: 58.54700854700855, f1: 25.65386671685337, r: 0.2863477687851263
06/01/2019 10:40:21 *** epoch: 44 ***
06/01/2019 10:40:21 *** training ***
06/01/2019 10:40:22 step: 1424, epoch: 43, batch: 4, loss: 0.0056680068373680115, acc: 100.0, f1: 100.0, r: 0.7747040760674455
06/01/2019 10:40:22 step: 1429, epoch: 43, batch: 9, loss: 0.007103979587554932, acc: 100.0, f1: 100.0, r: 0.76971851194864
06/01/2019 10:40:23 step: 1434, epoch: 43, batch: 14, loss: 0.00713956356048584, acc: 100.0, f1: 100.0, r: 0.7980703482334759
06/01/2019 10:40:24 step: 1439, epoch: 43, batch: 19, loss: 0.026897232979536057, acc: 98.4375, f1: 99.17748917748918, r: 0.6951749945753296
06/01/2019 10:40:25 step: 1444, epoch: 43, batch: 24, loss: 0.00322163850069046, acc: 100.0, f1: 100.0, r: 0.7448563362864175
06/01/2019 10:40:25 step: 1449, epoch: 43, batch: 29, loss: 0.03196139261126518, acc: 100.0, f1: 100.0, r: 0.7322648985683827
06/01/2019 10:40:26 *** evaluating ***
06/01/2019 10:40:26 step: 44, epoch: 43, acc: 58.54700854700855, f1: 23.866762775178618, r: 0.29382810830652206
06/01/2019 10:40:26 *** epoch: 45 ***
06/01/2019 10:40:26 *** training ***
06/01/2019 10:40:27 step: 1457, epoch: 44, batch: 4, loss: 0.006521634757518768, acc: 100.0, f1: 100.0, r: 0.7740831448046501
06/01/2019 10:40:27 step: 1462, epoch: 44, batch: 9, loss: 0.009318191558122635, acc: 100.0, f1: 100.0, r: 0.7955256544268867
06/01/2019 10:40:28 step: 1467, epoch: 44, batch: 14, loss: 0.010261818766593933, acc: 100.0, f1: 100.0, r: 0.7747337786622581
06/01/2019 10:40:29 step: 1472, epoch: 44, batch: 19, loss: 0.01101711019873619, acc: 100.0, f1: 100.0, r: 0.8227889834306463
06/01/2019 10:40:29 step: 1477, epoch: 44, batch: 24, loss: 0.004694357514381409, acc: 100.0, f1: 100.0, r: 0.6978458737966532
06/01/2019 10:40:30 step: 1482, epoch: 44, batch: 29, loss: 0.00497475266456604, acc: 100.0, f1: 100.0, r: 0.7966523020375303
06/01/2019 10:40:30 *** evaluating ***
06/01/2019 10:40:30 step: 45, epoch: 44, acc: 58.97435897435898, f1: 25.070605522256724, r: 0.29193524383821023
06/01/2019 10:40:30 *** epoch: 46 ***
06/01/2019 10:40:30 *** training ***
06/01/2019 10:40:31 step: 1490, epoch: 45, batch: 4, loss: 0.00534270703792572, acc: 100.0, f1: 100.0, r: 0.7079919005424822
06/01/2019 10:40:32 step: 1495, epoch: 45, batch: 9, loss: 0.006323277950286865, acc: 100.0, f1: 100.0, r: 0.6660044910888316
06/01/2019 10:40:33 step: 1500, epoch: 45, batch: 14, loss: 0.002260163426399231, acc: 100.0, f1: 100.0, r: 0.7857331693707325
06/01/2019 10:40:33 step: 1505, epoch: 45, batch: 19, loss: 0.0038280226290225983, acc: 100.0, f1: 100.0, r: 0.7588116505668785
06/01/2019 10:40:34 step: 1510, epoch: 45, batch: 24, loss: 0.004883609712123871, acc: 100.0, f1: 100.0, r: 0.7870924334250998
06/01/2019 10:40:35 step: 1515, epoch: 45, batch: 29, loss: 0.004583239555358887, acc: 100.0, f1: 100.0, r: 0.7386705014871722
06/01/2019 10:40:35 *** evaluating ***
06/01/2019 10:40:35 step: 46, epoch: 45, acc: 58.54700854700855, f1: 24.641928504985646, r: 0.28712081441297266
06/01/2019 10:40:35 *** epoch: 47 ***
06/01/2019 10:40:35 *** training ***
06/01/2019 10:40:36 step: 1523, epoch: 46, batch: 4, loss: 0.025849346071481705, acc: 98.4375, f1: 98.98692810457517, r: 0.817507887748829
06/01/2019 10:40:37 step: 1528, epoch: 46, batch: 9, loss: 0.037973012775182724, acc: 98.4375, f1: 98.7780772686433, r: 0.681990750221247
06/01/2019 10:40:37 step: 1533, epoch: 46, batch: 14, loss: 0.0051961541175842285, acc: 100.0, f1: 100.0, r: 0.8580412873269635
06/01/2019 10:40:38 step: 1538, epoch: 46, batch: 19, loss: 0.004930213093757629, acc: 100.0, f1: 100.0, r: 0.8111338456961652
06/01/2019 10:40:39 step: 1543, epoch: 46, batch: 24, loss: 0.008988633751869202, acc: 100.0, f1: 100.0, r: 0.6350805626458818
06/01/2019 10:40:39 step: 1548, epoch: 46, batch: 29, loss: 0.03121032938361168, acc: 98.4375, f1: 99.09909909909909, r: 0.6845146159230834
06/01/2019 10:40:40 *** evaluating ***
06/01/2019 10:40:40 step: 47, epoch: 46, acc: 59.82905982905983, f1: 26.471377654775264, r: 0.293729531219261
06/01/2019 10:40:40 *** epoch: 48 ***
06/01/2019 10:40:40 *** training ***
06/01/2019 10:40:41 step: 1556, epoch: 47, batch: 4, loss: 0.004894539713859558, acc: 100.0, f1: 100.0, r: 0.7666505023316528
06/01/2019 10:40:41 step: 1561, epoch: 47, batch: 9, loss: 0.011633023619651794, acc: 100.0, f1: 100.0, r: 0.613840701487496
06/01/2019 10:40:42 step: 1566, epoch: 47, batch: 14, loss: 0.0027428939938545227, acc: 100.0, f1: 100.0, r: 0.7684550989789819
06/01/2019 10:40:43 step: 1571, epoch: 47, batch: 19, loss: 0.011899232864379883, acc: 100.0, f1: 100.0, r: 0.7373930571486869
06/01/2019 10:40:43 step: 1576, epoch: 47, batch: 24, loss: 0.007230695337057114, acc: 100.0, f1: 100.0, r: 0.8500887728144371
06/01/2019 10:40:44 step: 1581, epoch: 47, batch: 29, loss: 0.0020201876759529114, acc: 100.0, f1: 100.0, r: 0.7381658168592812
06/01/2019 10:40:45 *** evaluating ***
06/01/2019 10:40:45 step: 48, epoch: 47, acc: 59.82905982905983, f1: 27.54975150004235, r: 0.29476180824657583
06/01/2019 10:40:45 *** epoch: 49 ***
06/01/2019 10:40:45 *** training ***
06/01/2019 10:40:46 step: 1589, epoch: 48, batch: 4, loss: 0.012304086238145828, acc: 100.0, f1: 100.0, r: 0.7918105855687163
06/01/2019 10:40:46 step: 1594, epoch: 48, batch: 9, loss: 0.012239435687661171, acc: 100.0, f1: 100.0, r: 0.6785859073263638
06/01/2019 10:40:47 step: 1599, epoch: 48, batch: 14, loss: 0.004566900432109833, acc: 100.0, f1: 100.0, r: 0.6197639398688392
06/01/2019 10:40:48 step: 1604, epoch: 48, batch: 19, loss: 0.003187529742717743, acc: 100.0, f1: 100.0, r: 0.7965479165960639
06/01/2019 10:40:48 step: 1609, epoch: 48, batch: 24, loss: 0.005472064018249512, acc: 100.0, f1: 100.0, r: 0.7902924539518803
06/01/2019 10:40:49 step: 1614, epoch: 48, batch: 29, loss: 0.004451267421245575, acc: 100.0, f1: 100.0, r: 0.8469342660462091
06/01/2019 10:40:49 *** evaluating ***
06/01/2019 10:40:50 step: 49, epoch: 48, acc: 58.97435897435898, f1: 25.944744308832252, r: 0.29413296705466885
06/01/2019 10:40:50 *** epoch: 50 ***
06/01/2019 10:40:50 *** training ***
06/01/2019 10:40:50 step: 1622, epoch: 49, batch: 4, loss: 0.006349720060825348, acc: 100.0, f1: 100.0, r: 0.6342856036731298
06/01/2019 10:40:51 step: 1627, epoch: 49, batch: 9, loss: 0.006927553564310074, acc: 100.0, f1: 100.0, r: 0.7750659492648088
06/01/2019 10:40:52 step: 1632, epoch: 49, batch: 14, loss: 0.008504591882228851, acc: 100.0, f1: 100.0, r: 0.7390006633465062
06/01/2019 10:40:52 step: 1637, epoch: 49, batch: 19, loss: 0.01191207766532898, acc: 100.0, f1: 100.0, r: 0.8340922084594203
06/01/2019 10:40:53 step: 1642, epoch: 49, batch: 24, loss: 0.01442137360572815, acc: 100.0, f1: 100.0, r: 0.8847882383129969
06/01/2019 10:40:54 step: 1647, epoch: 49, batch: 29, loss: 0.006610557436943054, acc: 100.0, f1: 100.0, r: 0.6042384180926794
06/01/2019 10:40:54 *** evaluating ***
06/01/2019 10:40:54 step: 50, epoch: 49, acc: 59.401709401709404, f1: 27.12417464924245, r: 0.28426096358117253
06/01/2019 10:40:54 *** epoch: 51 ***
06/01/2019 10:40:54 *** training ***
06/01/2019 10:40:55 step: 1655, epoch: 50, batch: 4, loss: 0.0135725736618042, acc: 100.0, f1: 100.0, r: 0.696482047942542
06/01/2019 10:40:56 step: 1660, epoch: 50, batch: 9, loss: 0.006943024694919586, acc: 100.0, f1: 100.0, r: 0.7586608906276892
06/01/2019 10:40:56 step: 1665, epoch: 50, batch: 14, loss: 0.001184418797492981, acc: 100.0, f1: 100.0, r: 0.6998732671096692
06/01/2019 10:40:57 step: 1670, epoch: 50, batch: 19, loss: 0.007575437426567078, acc: 100.0, f1: 100.0, r: 0.7128173020385967
06/01/2019 10:40:58 step: 1675, epoch: 50, batch: 24, loss: 0.010742239654064178, acc: 100.0, f1: 100.0, r: 0.7876314455174445
06/01/2019 10:40:59 step: 1680, epoch: 50, batch: 29, loss: 0.0056414008140563965, acc: 100.0, f1: 100.0, r: 0.5985873614763789
06/01/2019 10:40:59 *** evaluating ***
06/01/2019 10:40:59 step: 51, epoch: 50, acc: 58.97435897435898, f1: 26.59455968141609, r: 0.29605021127121106
06/01/2019 10:40:59 *** epoch: 52 ***
06/01/2019 10:40:59 *** training ***
06/01/2019 10:41:00 step: 1688, epoch: 51, batch: 4, loss: 0.0035154372453689575, acc: 100.0, f1: 100.0, r: 0.7673219964337594
06/01/2019 10:41:01 step: 1693, epoch: 51, batch: 9, loss: 0.006424982100725174, acc: 100.0, f1: 100.0, r: 0.8128732122769139
06/01/2019 10:41:01 step: 1698, epoch: 51, batch: 14, loss: 0.0028762221336364746, acc: 100.0, f1: 100.0, r: 0.7960158648563924
06/01/2019 10:41:02 step: 1703, epoch: 51, batch: 19, loss: 0.006771780550479889, acc: 100.0, f1: 100.0, r: 0.7770528289330801
06/01/2019 10:41:03 step: 1708, epoch: 51, batch: 24, loss: 0.012251656502485275, acc: 100.0, f1: 100.0, r: 0.793518214202813
06/01/2019 10:41:03 step: 1713, epoch: 51, batch: 29, loss: 0.016714617609977722, acc: 100.0, f1: 100.0, r: 0.6645805346682396
06/01/2019 10:41:04 *** evaluating ***
06/01/2019 10:41:04 step: 52, epoch: 51, acc: 58.97435897435898, f1: 26.384818884818884, r: 0.2910420578648013
06/01/2019 10:41:04 *** epoch: 53 ***
06/01/2019 10:41:04 *** training ***
06/01/2019 10:41:05 step: 1721, epoch: 52, batch: 4, loss: 0.021962184458971024, acc: 98.4375, f1: 98.0952380952381, r: 0.6960258810598672
06/01/2019 10:41:05 step: 1726, epoch: 52, batch: 9, loss: 0.008064966648817062, acc: 100.0, f1: 100.0, r: 0.8135453422231077
06/01/2019 10:41:06 step: 1731, epoch: 52, batch: 14, loss: 0.004920586943626404, acc: 100.0, f1: 100.0, r: 0.798439243127085
06/01/2019 10:41:07 step: 1736, epoch: 52, batch: 19, loss: 0.00858684629201889, acc: 100.0, f1: 100.0, r: 0.830602130807898
06/01/2019 10:41:07 step: 1741, epoch: 52, batch: 24, loss: 0.00594903901219368, acc: 100.0, f1: 100.0, r: 0.7041176106660052
06/01/2019 10:41:08 step: 1746, epoch: 52, batch: 29, loss: 0.003109581768512726, acc: 100.0, f1: 100.0, r: 0.7351474064206163
06/01/2019 10:41:08 *** evaluating ***
06/01/2019 10:41:09 step: 53, epoch: 52, acc: 59.401709401709404, f1: 26.43471277399849, r: 0.29651269605593994
06/01/2019 10:41:09 *** epoch: 54 ***
06/01/2019 10:41:09 *** training ***
06/01/2019 10:41:09 step: 1754, epoch: 53, batch: 4, loss: 0.006178379058837891, acc: 100.0, f1: 100.0, r: 0.6396633351237705
06/01/2019 10:41:10 step: 1759, epoch: 53, batch: 9, loss: 0.015759944915771484, acc: 100.0, f1: 100.0, r: 0.7008941402482082
06/01/2019 10:41:11 step: 1764, epoch: 53, batch: 14, loss: 0.018162190914154053, acc: 98.4375, f1: 99.23215898825654, r: 0.8280145075090971
06/01/2019 10:41:11 step: 1769, epoch: 53, batch: 19, loss: 0.006083551794290543, acc: 100.0, f1: 100.0, r: 0.7721284974482985
06/01/2019 10:41:12 step: 1774, epoch: 53, batch: 24, loss: 0.006647452712059021, acc: 100.0, f1: 100.0, r: 0.7629252186118554
06/01/2019 10:41:13 step: 1779, epoch: 53, batch: 29, loss: 0.006026849150657654, acc: 100.0, f1: 100.0, r: 0.8061241532925676
06/01/2019 10:41:13 *** evaluating ***
06/01/2019 10:41:13 step: 54, epoch: 53, acc: 58.54700854700855, f1: 26.732498566506948, r: 0.28998440571269474
06/01/2019 10:41:13 *** epoch: 55 ***
06/01/2019 10:41:13 *** training ***
06/01/2019 10:41:14 step: 1787, epoch: 54, batch: 4, loss: 0.0028720051050186157, acc: 100.0, f1: 100.0, r: 0.7187419739724137
06/01/2019 10:41:15 step: 1792, epoch: 54, batch: 9, loss: 0.0019996538758277893, acc: 100.0, f1: 100.0, r: 0.7551820044673454
06/01/2019 10:41:16 step: 1797, epoch: 54, batch: 14, loss: 0.009105078876018524, acc: 100.0, f1: 100.0, r: 0.7104720046497766
06/01/2019 10:41:16 step: 1802, epoch: 54, batch: 19, loss: 0.006350893527269363, acc: 100.0, f1: 100.0, r: 0.7759964573021397
06/01/2019 10:41:17 step: 1807, epoch: 54, batch: 24, loss: 0.003999777138233185, acc: 100.0, f1: 100.0, r: 0.6580010288046603
06/01/2019 10:41:18 step: 1812, epoch: 54, batch: 29, loss: 0.0123092420399189, acc: 100.0, f1: 100.0, r: 0.6885981209781274
06/01/2019 10:41:18 *** evaluating ***
06/01/2019 10:41:18 step: 55, epoch: 54, acc: 59.401709401709404, f1: 27.085965945598627, r: 0.2877140906052013
06/01/2019 10:41:18 *** epoch: 56 ***
06/01/2019 10:41:18 *** training ***
06/01/2019 10:41:19 step: 1820, epoch: 55, batch: 4, loss: 0.006372347474098206, acc: 100.0, f1: 100.0, r: 0.7404723441238296
06/01/2019 10:41:20 step: 1825, epoch: 55, batch: 9, loss: 0.0036399811506271362, acc: 100.0, f1: 100.0, r: 0.7724433194605179
06/01/2019 10:41:20 step: 1830, epoch: 55, batch: 14, loss: 0.004586905241012573, acc: 100.0, f1: 100.0, r: 0.8356056518162851
06/01/2019 10:41:21 step: 1835, epoch: 55, batch: 19, loss: 0.007140904664993286, acc: 100.0, f1: 100.0, r: 0.6603980158210107
06/01/2019 10:41:22 step: 1840, epoch: 55, batch: 24, loss: 0.0067366138100624084, acc: 100.0, f1: 100.0, r: 0.6694859060146883
06/01/2019 10:41:22 step: 1845, epoch: 55, batch: 29, loss: 0.0038531795144081116, acc: 100.0, f1: 100.0, r: 0.6674488779342869
06/01/2019 10:41:23 *** evaluating ***
06/01/2019 10:41:23 step: 56, epoch: 55, acc: 61.111111111111114, f1: 26.045394863965914, r: 0.30242736455747393
06/01/2019 10:41:23 *** epoch: 57 ***
06/01/2019 10:41:23 *** training ***
06/01/2019 10:41:24 step: 1853, epoch: 56, batch: 4, loss: 0.003762364387512207, acc: 100.0, f1: 100.0, r: 0.8348360251486504
06/01/2019 10:41:24 step: 1858, epoch: 56, batch: 9, loss: 0.010227032005786896, acc: 100.0, f1: 100.0, r: 0.8593426558142678
06/01/2019 10:41:25 step: 1863, epoch: 56, batch: 14, loss: 0.022477485239505768, acc: 98.4375, f1: 98.59767891682785, r: 0.779861838442914
06/01/2019 10:41:26 step: 1868, epoch: 56, batch: 19, loss: 0.0013196244835853577, acc: 100.0, f1: 100.0, r: 0.7243538945066654
06/01/2019 10:41:26 step: 1873, epoch: 56, batch: 24, loss: 0.009181268513202667, acc: 100.0, f1: 100.0, r: 0.7932516941905214
06/01/2019 10:41:27 step: 1878, epoch: 56, batch: 29, loss: 0.003620326519012451, acc: 100.0, f1: 100.0, r: 0.7341951393886094
06/01/2019 10:41:27 *** evaluating ***
06/01/2019 10:41:27 step: 57, epoch: 56, acc: 61.53846153846154, f1: 28.129610172487563, r: 0.29815259901476576
06/01/2019 10:41:27 *** epoch: 58 ***
06/01/2019 10:41:27 *** training ***
06/01/2019 10:41:28 step: 1886, epoch: 57, batch: 4, loss: 0.0051576197147369385, acc: 100.0, f1: 100.0, r: 0.61190805058092
06/01/2019 10:41:29 step: 1891, epoch: 57, batch: 9, loss: 0.0026897192001342773, acc: 100.0, f1: 100.0, r: 0.7116618085686716
06/01/2019 10:41:29 step: 1896, epoch: 57, batch: 14, loss: 0.0027827247977256775, acc: 100.0, f1: 100.0, r: 0.7371264684631746
06/01/2019 10:41:30 step: 1901, epoch: 57, batch: 19, loss: 0.0017200112342834473, acc: 100.0, f1: 100.0, r: 0.7198911585963325
06/01/2019 10:41:31 step: 1906, epoch: 57, batch: 24, loss: 0.003631535917520523, acc: 100.0, f1: 100.0, r: 0.626054821046963
06/01/2019 10:41:32 step: 1911, epoch: 57, batch: 29, loss: 0.003839053213596344, acc: 100.0, f1: 100.0, r: 0.8184775828972836
06/01/2019 10:41:32 *** evaluating ***
06/01/2019 10:41:32 step: 58, epoch: 57, acc: 60.256410256410255, f1: 24.62041906923797, r: 0.3012311294538822
06/01/2019 10:41:32 *** epoch: 59 ***
06/01/2019 10:41:32 *** training ***
06/01/2019 10:41:33 step: 1919, epoch: 58, batch: 4, loss: 0.0072450414299964905, acc: 100.0, f1: 100.0, r: 0.8136535161865819
06/01/2019 10:41:33 step: 1924, epoch: 58, batch: 9, loss: 0.012615524232387543, acc: 100.0, f1: 100.0, r: 0.7895959400586476
06/01/2019 10:41:34 step: 1929, epoch: 58, batch: 14, loss: 0.009722623974084854, acc: 100.0, f1: 100.0, r: 0.7632737749643956
06/01/2019 10:41:35 step: 1934, epoch: 58, batch: 19, loss: 0.0024504996836185455, acc: 100.0, f1: 100.0, r: 0.6463504928715647
06/01/2019 10:41:36 step: 1939, epoch: 58, batch: 24, loss: 0.00930267944931984, acc: 100.0, f1: 100.0, r: 0.6431088094199453
06/01/2019 10:41:36 step: 1944, epoch: 58, batch: 29, loss: 0.01859797164797783, acc: 98.4375, f1: 99.0514075887393, r: 0.8313041008204209
06/01/2019 10:41:36 *** evaluating ***
06/01/2019 10:41:37 step: 59, epoch: 58, acc: 58.97435897435898, f1: 25.19374768408965, r: 0.2922872772890647
06/01/2019 10:41:37 *** epoch: 60 ***
06/01/2019 10:41:37 *** training ***
06/01/2019 10:41:37 step: 1952, epoch: 59, batch: 4, loss: 0.0015812069177627563, acc: 100.0, f1: 100.0, r: 0.7636215763014862
06/01/2019 10:41:38 step: 1957, epoch: 59, batch: 9, loss: 0.005625471472740173, acc: 100.0, f1: 100.0, r: 0.8219343971990662
06/01/2019 10:41:39 step: 1962, epoch: 59, batch: 14, loss: 0.012107685208320618, acc: 100.0, f1: 100.0, r: 0.7317156916545089
06/01/2019 10:41:39 step: 1967, epoch: 59, batch: 19, loss: 0.008473657071590424, acc: 100.0, f1: 100.0, r: 0.7106511276723155
06/01/2019 10:41:40 step: 1972, epoch: 59, batch: 24, loss: 0.003913626074790955, acc: 100.0, f1: 100.0, r: 0.6908893086035455
06/01/2019 10:41:41 step: 1977, epoch: 59, batch: 29, loss: 0.0033610910177230835, acc: 100.0, f1: 100.0, r: 0.7921481178999015
06/01/2019 10:41:41 *** evaluating ***
06/01/2019 10:41:41 step: 60, epoch: 59, acc: 61.53846153846154, f1: 27.784913509545518, r: 0.2987293447735419
06/01/2019 10:41:41 *** epoch: 61 ***
06/01/2019 10:41:41 *** training ***
06/01/2019 10:41:42 step: 1985, epoch: 60, batch: 4, loss: 0.00265386700630188, acc: 100.0, f1: 100.0, r: 0.6998940161497212
06/01/2019 10:41:43 step: 1990, epoch: 60, batch: 9, loss: 0.006580285727977753, acc: 100.0, f1: 100.0, r: 0.8418897538812949
06/01/2019 10:41:43 step: 1995, epoch: 60, batch: 14, loss: 0.004264310002326965, acc: 100.0, f1: 100.0, r: 0.727735621399564
06/01/2019 10:41:44 step: 2000, epoch: 60, batch: 19, loss: 0.006495926529169083, acc: 100.0, f1: 100.0, r: 0.7926826916657217
06/01/2019 10:41:45 step: 2005, epoch: 60, batch: 24, loss: 0.012772664427757263, acc: 100.0, f1: 100.0, r: 0.7501916392421141
06/01/2019 10:41:45 step: 2010, epoch: 60, batch: 29, loss: 0.00481298565864563, acc: 100.0, f1: 100.0, r: 0.7966005629885267
06/01/2019 10:41:46 *** evaluating ***
06/01/2019 10:41:46 step: 61, epoch: 60, acc: 61.53846153846154, f1: 26.758502446551624, r: 0.30146949161746434
06/01/2019 10:41:46 *** epoch: 62 ***
06/01/2019 10:41:46 *** training ***
06/01/2019 10:41:47 step: 2018, epoch: 61, batch: 4, loss: 0.014166036620736122, acc: 100.0, f1: 100.0, r: 0.7386481595673494
06/01/2019 10:41:47 step: 2023, epoch: 61, batch: 9, loss: 0.007159017026424408, acc: 100.0, f1: 100.0, r: 0.7675778873713098
06/01/2019 10:41:48 step: 2028, epoch: 61, batch: 14, loss: 0.0037243664264678955, acc: 100.0, f1: 100.0, r: 0.8414783913431466
06/01/2019 10:41:49 step: 2033, epoch: 61, batch: 19, loss: 0.0028567463159561157, acc: 100.0, f1: 100.0, r: 0.698778149169045
06/01/2019 10:41:49 step: 2038, epoch: 61, batch: 24, loss: 0.0040838271379470825, acc: 100.0, f1: 100.0, r: 0.7829495767024783
06/01/2019 10:41:50 step: 2043, epoch: 61, batch: 29, loss: 0.0037075430154800415, acc: 100.0, f1: 100.0, r: 0.7963457982418732
06/01/2019 10:41:50 *** evaluating ***
06/01/2019 10:41:51 step: 62, epoch: 61, acc: 61.111111111111114, f1: 27.562682170428275, r: 0.29902699728433896
06/01/2019 10:41:51 *** epoch: 63 ***
06/01/2019 10:41:51 *** training ***
06/01/2019 10:41:51 step: 2051, epoch: 62, batch: 4, loss: 0.006302684545516968, acc: 100.0, f1: 100.0, r: 0.7523434513564498
06/01/2019 10:41:52 step: 2056, epoch: 62, batch: 9, loss: 0.0061198025941848755, acc: 100.0, f1: 100.0, r: 0.7474482362554586
06/01/2019 10:41:53 step: 2061, epoch: 62, batch: 14, loss: 0.004244357347488403, acc: 100.0, f1: 100.0, r: 0.8110672401166744
06/01/2019 10:41:53 step: 2066, epoch: 62, batch: 19, loss: 0.003634631633758545, acc: 100.0, f1: 100.0, r: 0.7438850595078608
06/01/2019 10:41:54 step: 2071, epoch: 62, batch: 24, loss: 0.004881635308265686, acc: 100.0, f1: 100.0, r: 0.683306354562719
06/01/2019 10:41:55 step: 2076, epoch: 62, batch: 29, loss: 0.0042905062437057495, acc: 100.0, f1: 100.0, r: 0.7533431945985236
06/01/2019 10:41:55 *** evaluating ***
06/01/2019 10:41:55 step: 63, epoch: 62, acc: 59.82905982905983, f1: 27.84293341335041, r: 0.29305871841739817
06/01/2019 10:41:55 *** epoch: 64 ***
06/01/2019 10:41:55 *** training ***
06/01/2019 10:41:56 step: 2084, epoch: 63, batch: 4, loss: 0.008458081632852554, acc: 100.0, f1: 100.0, r: 0.7203614134857729
06/01/2019 10:41:57 step: 2089, epoch: 63, batch: 9, loss: 0.010611608624458313, acc: 100.0, f1: 100.0, r: 0.7885827227738393
06/01/2019 10:41:57 step: 2094, epoch: 63, batch: 14, loss: 0.0033880770206451416, acc: 100.0, f1: 100.0, r: 0.6761235881071355
06/01/2019 10:41:58 step: 2099, epoch: 63, batch: 19, loss: 0.005134213715791702, acc: 100.0, f1: 100.0, r: 0.7575990382153435
06/01/2019 10:41:59 step: 2104, epoch: 63, batch: 24, loss: 0.006347164511680603, acc: 100.0, f1: 100.0, r: 0.6728406819682198
06/01/2019 10:42:00 step: 2109, epoch: 63, batch: 29, loss: 0.0291762575507164, acc: 98.4375, f1: 99.13880445795338, r: 0.7536063263078111
06/01/2019 10:42:00 *** evaluating ***
06/01/2019 10:42:00 step: 64, epoch: 63, acc: 59.401709401709404, f1: 27.315599689488195, r: 0.2946974556639122
06/01/2019 10:42:00 *** epoch: 65 ***
06/01/2019 10:42:00 *** training ***
06/01/2019 10:42:01 step: 2117, epoch: 64, batch: 4, loss: 0.007043484598398209, acc: 100.0, f1: 100.0, r: 0.6803648036784049
06/01/2019 10:42:02 step: 2122, epoch: 64, batch: 9, loss: 0.005142010748386383, acc: 100.0, f1: 100.0, r: 0.7922706773367801
06/01/2019 10:42:03 step: 2127, epoch: 64, batch: 14, loss: 0.0033964961767196655, acc: 100.0, f1: 100.0, r: 0.623819734313168
06/01/2019 10:42:03 step: 2132, epoch: 64, batch: 19, loss: 0.006149128079414368, acc: 100.0, f1: 100.0, r: 0.8009478568099879
06/01/2019 10:42:04 step: 2137, epoch: 64, batch: 24, loss: 0.008151009678840637, acc: 100.0, f1: 100.0, r: 0.8126817010864957
06/01/2019 10:42:05 step: 2142, epoch: 64, batch: 29, loss: 0.007632825523614883, acc: 100.0, f1: 100.0, r: 0.7450952836983106
06/01/2019 10:42:05 *** evaluating ***
06/01/2019 10:42:05 step: 65, epoch: 64, acc: 60.68376068376068, f1: 27.638260308714855, r: 0.298226527954939
06/01/2019 10:42:05 *** epoch: 66 ***
06/01/2019 10:42:05 *** training ***
06/01/2019 10:42:06 step: 2150, epoch: 65, batch: 4, loss: 0.004235297441482544, acc: 100.0, f1: 100.0, r: 0.7846894775679831
06/01/2019 10:42:07 step: 2155, epoch: 65, batch: 9, loss: 0.0023925304412841797, acc: 100.0, f1: 100.0, r: 0.7532445867309281
06/01/2019 10:42:07 step: 2160, epoch: 65, batch: 14, loss: 0.004208892583847046, acc: 100.0, f1: 100.0, r: 0.701702380394673
06/01/2019 10:42:08 step: 2165, epoch: 65, batch: 19, loss: 0.015130478888750076, acc: 100.0, f1: 100.0, r: 0.6795652818472318
06/01/2019 10:42:09 step: 2170, epoch: 65, batch: 24, loss: 0.003017537295818329, acc: 100.0, f1: 100.0, r: 0.678089140523752
06/01/2019 10:42:10 step: 2175, epoch: 65, batch: 29, loss: 0.005123622715473175, acc: 100.0, f1: 100.0, r: 0.7083914748957406
06/01/2019 10:42:10 *** evaluating ***
06/01/2019 10:42:10 step: 66, epoch: 65, acc: 59.82905982905983, f1: 28.06230483996877, r: 0.29000643532155274
06/01/2019 10:42:10 *** epoch: 67 ***
06/01/2019 10:42:10 *** training ***
06/01/2019 10:42:11 step: 2183, epoch: 66, batch: 4, loss: 0.002407185733318329, acc: 100.0, f1: 100.0, r: 0.7374858096656767
06/01/2019 10:42:12 step: 2188, epoch: 66, batch: 9, loss: 0.002499878406524658, acc: 100.0, f1: 100.0, r: 0.791329852890706
06/01/2019 10:42:12 step: 2193, epoch: 66, batch: 14, loss: 0.003874830901622772, acc: 100.0, f1: 100.0, r: 0.6163937701839193
06/01/2019 10:42:13 step: 2198, epoch: 66, batch: 19, loss: 0.009260017424821854, acc: 100.0, f1: 100.0, r: 0.7797823879169231
06/01/2019 10:42:14 step: 2203, epoch: 66, batch: 24, loss: 0.004354901611804962, acc: 100.0, f1: 100.0, r: 0.6739187473586914
06/01/2019 10:42:14 step: 2208, epoch: 66, batch: 29, loss: 0.008781369775533676, acc: 100.0, f1: 100.0, r: 0.8141185557060958
06/01/2019 10:42:15 *** evaluating ***
06/01/2019 10:42:15 step: 67, epoch: 66, acc: 59.82905982905983, f1: 27.407633533962784, r: 0.2880181811133688
06/01/2019 10:42:15 *** epoch: 68 ***
06/01/2019 10:42:15 *** training ***
06/01/2019 10:42:16 step: 2216, epoch: 67, batch: 4, loss: 0.003943286836147308, acc: 100.0, f1: 100.0, r: 0.691147234923599
06/01/2019 10:42:16 step: 2221, epoch: 67, batch: 9, loss: 0.007637806236743927, acc: 100.0, f1: 100.0, r: 0.7206746935232476
06/01/2019 10:42:17 step: 2226, epoch: 67, batch: 14, loss: 0.0055879391729831696, acc: 100.0, f1: 100.0, r: 0.6847277889836739
06/01/2019 10:42:18 step: 2231, epoch: 67, batch: 19, loss: 0.003477446734905243, acc: 100.0, f1: 100.0, r: 0.6826228584997225
06/01/2019 10:42:18 step: 2236, epoch: 67, batch: 24, loss: 0.0062642693519592285, acc: 100.0, f1: 100.0, r: 0.7422859381996513
06/01/2019 10:42:19 step: 2241, epoch: 67, batch: 29, loss: 0.005425266921520233, acc: 100.0, f1: 100.0, r: 0.7547762612325015
06/01/2019 10:42:19 *** evaluating ***
06/01/2019 10:42:20 step: 68, epoch: 67, acc: 58.54700854700855, f1: 27.30090547971871, r: 0.2888907592487988
06/01/2019 10:42:20 *** epoch: 69 ***
06/01/2019 10:42:20 *** training ***
06/01/2019 10:42:20 step: 2249, epoch: 68, batch: 4, loss: 0.010763067752122879, acc: 100.0, f1: 100.0, r: 0.6854896085726702
06/01/2019 10:42:21 step: 2254, epoch: 68, batch: 9, loss: 0.010722063481807709, acc: 100.0, f1: 100.0, r: 0.5887678992806278
06/01/2019 10:42:22 step: 2259, epoch: 68, batch: 14, loss: 0.009715497493743896, acc: 100.0, f1: 100.0, r: 0.7977968356998814
06/01/2019 10:42:22 step: 2264, epoch: 68, batch: 19, loss: 0.018026914447546005, acc: 98.4375, f1: 98.93939393939395, r: 0.7937948869184859
06/01/2019 10:42:23 step: 2269, epoch: 68, batch: 24, loss: 0.004925474524497986, acc: 100.0, f1: 100.0, r: 0.7997428914586248
06/01/2019 10:42:24 step: 2274, epoch: 68, batch: 29, loss: 0.006232157349586487, acc: 100.0, f1: 100.0, r: 0.6465670762930914
06/01/2019 10:42:24 *** evaluating ***
06/01/2019 10:42:24 step: 69, epoch: 68, acc: 59.401709401709404, f1: 26.70497592993652, r: 0.3006551024756721
06/01/2019 10:42:24 *** epoch: 70 ***
06/01/2019 10:42:24 *** training ***
06/01/2019 10:42:25 step: 2282, epoch: 69, batch: 4, loss: 0.012424156069755554, acc: 100.0, f1: 100.0, r: 0.6637753766886252
06/01/2019 10:42:26 step: 2287, epoch: 69, batch: 9, loss: 0.008475881069898605, acc: 100.0, f1: 100.0, r: 0.8269757895261693
06/01/2019 10:42:26 step: 2292, epoch: 69, batch: 14, loss: 0.0024935975670814514, acc: 100.0, f1: 100.0, r: 0.7118181520403627
06/01/2019 10:42:27 step: 2297, epoch: 69, batch: 19, loss: 0.0025406628847122192, acc: 100.0, f1: 100.0, r: 0.781766731767096
06/01/2019 10:42:28 step: 2302, epoch: 69, batch: 24, loss: 0.006147328764200211, acc: 100.0, f1: 100.0, r: 0.7689674955385152
06/01/2019 10:42:29 step: 2307, epoch: 69, batch: 29, loss: 0.002144157886505127, acc: 100.0, f1: 100.0, r: 0.6671277877945376
06/01/2019 10:42:29 *** evaluating ***
06/01/2019 10:42:29 step: 70, epoch: 69, acc: 60.256410256410255, f1: 27.82402073732718, r: 0.2971784507760319
06/01/2019 10:42:29 *** epoch: 71 ***
06/01/2019 10:42:29 *** training ***
06/01/2019 10:42:30 step: 2315, epoch: 70, batch: 4, loss: 0.00508151575922966, acc: 100.0, f1: 100.0, r: 0.7910863468383268
06/01/2019 10:42:31 step: 2320, epoch: 70, batch: 9, loss: 0.005163546651601791, acc: 100.0, f1: 100.0, r: 0.8110851072511162
06/01/2019 10:42:31 step: 2325, epoch: 70, batch: 14, loss: 0.022163499146699905, acc: 98.4375, f1: 98.99567099567099, r: 0.6896747972147406
06/01/2019 10:42:32 step: 2330, epoch: 70, batch: 19, loss: 0.001384541392326355, acc: 100.0, f1: 100.0, r: 0.6685148401250794
06/01/2019 10:42:33 step: 2335, epoch: 70, batch: 24, loss: 0.006334543228149414, acc: 100.0, f1: 100.0, r: 0.7831448489676133
06/01/2019 10:42:34 step: 2340, epoch: 70, batch: 29, loss: 0.002926267683506012, acc: 100.0, f1: 100.0, r: 0.7113988428835525
06/01/2019 10:42:34 *** evaluating ***
06/01/2019 10:42:34 step: 71, epoch: 70, acc: 59.82905982905983, f1: 28.081082622326846, r: 0.28690755386980865
06/01/2019 10:42:34 *** epoch: 72 ***
06/01/2019 10:42:34 *** training ***
06/01/2019 10:42:35 step: 2348, epoch: 71, batch: 4, loss: 0.004026502370834351, acc: 100.0, f1: 100.0, r: 0.8080065171506524
06/01/2019 10:42:36 step: 2353, epoch: 71, batch: 9, loss: 0.0028857067227363586, acc: 100.0, f1: 100.0, r: 0.7905431860601916
06/01/2019 10:42:36 step: 2358, epoch: 71, batch: 14, loss: 0.004005610942840576, acc: 100.0, f1: 100.0, r: 0.6714056233161014
06/01/2019 10:42:37 step: 2363, epoch: 71, batch: 19, loss: 0.0052406564354896545, acc: 100.0, f1: 100.0, r: 0.684407413916921
06/01/2019 10:42:38 step: 2368, epoch: 71, batch: 24, loss: 0.006719324737787247, acc: 100.0, f1: 100.0, r: 0.8060622849962064
06/01/2019 10:42:39 step: 2373, epoch: 71, batch: 29, loss: 0.006042003631591797, acc: 100.0, f1: 100.0, r: 0.7047458085752397
06/01/2019 10:42:39 *** evaluating ***
06/01/2019 10:42:39 step: 72, epoch: 71, acc: 59.82905982905983, f1: 26.311409550045916, r: 0.2824683606037571
06/01/2019 10:42:39 *** epoch: 73 ***
06/01/2019 10:42:39 *** training ***
06/01/2019 10:42:40 step: 2381, epoch: 72, batch: 4, loss: 0.003995321691036224, acc: 100.0, f1: 100.0, r: 0.75540787836137
06/01/2019 10:42:40 step: 2386, epoch: 72, batch: 9, loss: 0.002375565469264984, acc: 100.0, f1: 100.0, r: 0.7877906032084524
06/01/2019 10:42:41 step: 2391, epoch: 72, batch: 14, loss: 0.007353115826845169, acc: 100.0, f1: 100.0, r: 0.7100460518346867
06/01/2019 10:42:42 step: 2396, epoch: 72, batch: 19, loss: 0.024768855422735214, acc: 98.4375, f1: 99.31647300068353, r: 0.6450426667292639
06/01/2019 10:42:42 step: 2401, epoch: 72, batch: 24, loss: 0.004799440503120422, acc: 100.0, f1: 100.0, r: 0.7268718343251881
06/01/2019 10:42:43 step: 2406, epoch: 72, batch: 29, loss: 0.0032198354601860046, acc: 100.0, f1: 100.0, r: 0.843800071528937
06/01/2019 10:42:43 *** evaluating ***
06/01/2019 10:42:44 step: 73, epoch: 72, acc: 61.53846153846154, f1: 29.256097560975608, r: 0.2980373484311585
06/01/2019 10:42:44 *** epoch: 74 ***
06/01/2019 10:42:44 *** training ***
06/01/2019 10:42:44 step: 2414, epoch: 73, batch: 4, loss: 0.0038874298334121704, acc: 100.0, f1: 100.0, r: 0.6513253435254953
06/01/2019 10:42:45 step: 2419, epoch: 73, batch: 9, loss: 0.005010098218917847, acc: 100.0, f1: 100.0, r: 0.8083497787444136
06/01/2019 10:42:46 step: 2424, epoch: 73, batch: 14, loss: 0.0050103142857551575, acc: 100.0, f1: 100.0, r: 0.651766498660341
06/01/2019 10:42:46 step: 2429, epoch: 73, batch: 19, loss: 0.011234484612941742, acc: 100.0, f1: 100.0, r: 0.5681175853749345
06/01/2019 10:42:47 step: 2434, epoch: 73, batch: 24, loss: 0.002595365047454834, acc: 100.0, f1: 100.0, r: 0.48767998489084724
06/01/2019 10:42:48 step: 2439, epoch: 73, batch: 29, loss: 0.00580964982509613, acc: 100.0, f1: 100.0, r: 0.8157759725132355
06/01/2019 10:42:48 *** evaluating ***
06/01/2019 10:42:48 step: 74, epoch: 73, acc: 62.39316239316239, f1: 29.067427320290474, r: 0.3078535427229812
06/01/2019 10:42:48 *** epoch: 75 ***
06/01/2019 10:42:48 *** training ***
06/01/2019 10:42:49 step: 2447, epoch: 74, batch: 4, loss: 0.008881375193595886, acc: 100.0, f1: 100.0, r: 0.6180597371850257
06/01/2019 10:42:50 step: 2452, epoch: 74, batch: 9, loss: 0.005531493574380875, acc: 100.0, f1: 100.0, r: 0.681367668685086
06/01/2019 10:42:50 step: 2457, epoch: 74, batch: 14, loss: 0.00444415956735611, acc: 100.0, f1: 100.0, r: 0.7106287686292105
06/01/2019 10:42:51 step: 2462, epoch: 74, batch: 19, loss: 0.00963887944817543, acc: 100.0, f1: 100.0, r: 0.6577786459473266
06/01/2019 10:42:52 step: 2467, epoch: 74, batch: 24, loss: 0.0017085820436477661, acc: 100.0, f1: 100.0, r: 0.7698929594694669
06/01/2019 10:42:52 step: 2472, epoch: 74, batch: 29, loss: 0.0035148337483406067, acc: 100.0, f1: 100.0, r: 0.7788280206592354
06/01/2019 10:42:53 *** evaluating ***
06/01/2019 10:42:53 step: 75, epoch: 74, acc: 61.965811965811966, f1: 27.69134610937989, r: 0.3134833329497015
06/01/2019 10:42:53 *** epoch: 76 ***
06/01/2019 10:42:53 *** training ***
06/01/2019 10:42:54 step: 2480, epoch: 75, batch: 4, loss: 0.0030782297253608704, acc: 100.0, f1: 100.0, r: 0.6772441861155752
06/01/2019 10:42:54 step: 2485, epoch: 75, batch: 9, loss: 0.008025579154491425, acc: 100.0, f1: 100.0, r: 0.7824767997378537
06/01/2019 10:42:55 step: 2490, epoch: 75, batch: 14, loss: 0.004233572632074356, acc: 100.0, f1: 100.0, r: 0.7673106690659289
06/01/2019 10:42:56 step: 2495, epoch: 75, batch: 19, loss: 0.02951568365097046, acc: 98.4375, f1: 98.80261248185775, r: 0.8028357653511675
06/01/2019 10:42:56 step: 2500, epoch: 75, batch: 24, loss: 0.007590293884277344, acc: 100.0, f1: 100.0, r: 0.7855628255760458
06/01/2019 10:42:57 step: 2505, epoch: 75, batch: 29, loss: 0.007340416312217712, acc: 100.0, f1: 100.0, r: 0.7665774647111462
06/01/2019 10:42:57 *** evaluating ***
06/01/2019 10:42:58 step: 76, epoch: 75, acc: 60.68376068376068, f1: 28.326962853537996, r: 0.28640647799819835
06/01/2019 10:42:58 *** epoch: 77 ***
06/01/2019 10:42:58 *** training ***
06/01/2019 10:42:58 step: 2513, epoch: 76, batch: 4, loss: 0.0017084404826164246, acc: 100.0, f1: 100.0, r: 0.7128292524621108
06/01/2019 10:42:59 step: 2518, epoch: 76, batch: 9, loss: 0.009954981505870819, acc: 100.0, f1: 100.0, r: 0.806416862122359
06/01/2019 10:43:00 step: 2523, epoch: 76, batch: 14, loss: 0.010058857500553131, acc: 100.0, f1: 100.0, r: 0.7021172144369384
06/01/2019 10:43:00 step: 2528, epoch: 76, batch: 19, loss: 0.013112634420394897, acc: 100.0, f1: 100.0, r: 0.784101719934255
06/01/2019 10:43:01 step: 2533, epoch: 76, batch: 24, loss: 0.008523084223270416, acc: 100.0, f1: 100.0, r: 0.6778028987120093
06/01/2019 10:43:02 step: 2538, epoch: 76, batch: 29, loss: 0.005653694272041321, acc: 100.0, f1: 100.0, r: 0.7462653436058252
06/01/2019 10:43:02 *** evaluating ***
06/01/2019 10:43:02 step: 77, epoch: 76, acc: 58.54700854700855, f1: 28.022304034577527, r: 0.2868082653529353
06/01/2019 10:43:02 *** epoch: 78 ***
06/01/2019 10:43:02 *** training ***
06/01/2019 10:43:03 step: 2546, epoch: 77, batch: 4, loss: 0.002033539116382599, acc: 100.0, f1: 100.0, r: 0.7385086173162034
06/01/2019 10:43:04 step: 2551, epoch: 77, batch: 9, loss: 0.0020758509635925293, acc: 100.0, f1: 100.0, r: 0.7165778328349451
06/01/2019 10:43:04 step: 2556, epoch: 77, batch: 14, loss: 0.028024856001138687, acc: 98.4375, f1: 98.90903729913018, r: 0.7306128343171198
06/01/2019 10:43:05 step: 2561, epoch: 77, batch: 19, loss: 0.0038618147373199463, acc: 100.0, f1: 100.0, r: 0.7452331520500877
06/01/2019 10:43:06 step: 2566, epoch: 77, batch: 24, loss: 0.006444394588470459, acc: 100.0, f1: 100.0, r: 0.6607829468586496
06/01/2019 10:43:06 step: 2571, epoch: 77, batch: 29, loss: 0.003816276788711548, acc: 100.0, f1: 100.0, r: 0.7876058062441552
06/01/2019 10:43:07 *** evaluating ***
06/01/2019 10:43:07 step: 78, epoch: 77, acc: 60.68376068376068, f1: 27.818508862066032, r: 0.297755087381336
06/01/2019 10:43:07 *** epoch: 79 ***
06/01/2019 10:43:07 *** training ***
06/01/2019 10:43:08 step: 2579, epoch: 78, batch: 4, loss: 0.0025180205702781677, acc: 100.0, f1: 100.0, r: 0.7904705781565473
06/01/2019 10:43:09 step: 2584, epoch: 78, batch: 9, loss: 0.011733070015907288, acc: 100.0, f1: 100.0, r: 0.7967827867302177
06/01/2019 10:43:09 step: 2589, epoch: 78, batch: 14, loss: 0.002311021089553833, acc: 100.0, f1: 100.0, r: 0.7997308077535037
06/01/2019 10:43:10 step: 2594, epoch: 78, batch: 19, loss: 0.0017855837941169739, acc: 100.0, f1: 100.0, r: 0.6581515667456478
06/01/2019 10:43:11 step: 2599, epoch: 78, batch: 24, loss: 0.021270878612995148, acc: 98.4375, f1: 99.1484593837535, r: 0.6893459159057548
06/01/2019 10:43:11 step: 2604, epoch: 78, batch: 29, loss: 0.004426151514053345, acc: 100.0, f1: 100.0, r: 0.6994412206480508
06/01/2019 10:43:12 *** evaluating ***
06/01/2019 10:43:12 step: 79, epoch: 78, acc: 55.98290598290598, f1: 21.046596150762813, r: 0.27597518569551205
06/01/2019 10:43:12 *** epoch: 80 ***
06/01/2019 10:43:12 *** training ***
06/01/2019 10:43:13 step: 2612, epoch: 79, batch: 4, loss: 0.010404668748378754, acc: 100.0, f1: 100.0, r: 0.7947062226440238
06/01/2019 10:43:14 step: 2617, epoch: 79, batch: 9, loss: 0.005253933370113373, acc: 100.0, f1: 100.0, r: 0.6193731332203928
06/01/2019 10:43:14 step: 2622, epoch: 79, batch: 14, loss: 0.0030101165175437927, acc: 100.0, f1: 100.0, r: 0.7344924397082284
06/01/2019 10:43:15 step: 2627, epoch: 79, batch: 19, loss: 0.0026310235261917114, acc: 100.0, f1: 100.0, r: 0.682883701340507
06/01/2019 10:43:16 step: 2632, epoch: 79, batch: 24, loss: 0.0022222399711608887, acc: 100.0, f1: 100.0, r: 0.7150357773816169
06/01/2019 10:43:16 step: 2637, epoch: 79, batch: 29, loss: 0.002011515200138092, acc: 100.0, f1: 100.0, r: 0.7371479152531042
06/01/2019 10:43:17 *** evaluating ***
06/01/2019 10:43:17 step: 80, epoch: 79, acc: 58.97435897435898, f1: 25.883356953656566, r: 0.2854462353121787
06/01/2019 10:43:17 *** epoch: 81 ***
06/01/2019 10:43:17 *** training ***
06/01/2019 10:43:18 step: 2645, epoch: 80, batch: 4, loss: 0.0027112290263175964, acc: 100.0, f1: 100.0, r: 0.778633127094973
06/01/2019 10:43:18 step: 2650, epoch: 80, batch: 9, loss: 0.0060614533722400665, acc: 100.0, f1: 100.0, r: 0.7958430788788537
06/01/2019 10:43:19 step: 2655, epoch: 80, batch: 14, loss: 0.004216879606246948, acc: 100.0, f1: 100.0, r: 0.684309144179629
06/01/2019 10:43:20 step: 2660, epoch: 80, batch: 19, loss: 0.00769197940826416, acc: 100.0, f1: 100.0, r: 0.7938898520282581
06/01/2019 10:43:20 step: 2665, epoch: 80, batch: 24, loss: 0.002958938479423523, acc: 100.0, f1: 100.0, r: 0.6785326372562269
06/01/2019 10:43:21 step: 2670, epoch: 80, batch: 29, loss: 0.005018509924411774, acc: 100.0, f1: 100.0, r: 0.7939785937864736
06/01/2019 10:43:21 *** evaluating ***
06/01/2019 10:43:22 step: 81, epoch: 80, acc: 59.82905982905983, f1: 27.7804387805938, r: 0.28491210299594133
06/01/2019 10:43:22 *** epoch: 82 ***
06/01/2019 10:43:22 *** training ***
06/01/2019 10:43:22 step: 2678, epoch: 81, batch: 4, loss: 0.0029145032167434692, acc: 100.0, f1: 100.0, r: 0.7817935318148119
06/01/2019 10:43:23 step: 2683, epoch: 81, batch: 9, loss: 0.0034071505069732666, acc: 100.0, f1: 100.0, r: 0.8011636036074635
06/01/2019 10:43:24 step: 2688, epoch: 81, batch: 14, loss: 0.007631257176399231, acc: 100.0, f1: 100.0, r: 0.817717564842912
06/01/2019 10:43:24 step: 2693, epoch: 81, batch: 19, loss: 0.006307043135166168, acc: 100.0, f1: 100.0, r: 0.7402242854295715
06/01/2019 10:43:25 step: 2698, epoch: 81, batch: 24, loss: 0.0025153979659080505, acc: 100.0, f1: 100.0, r: 0.7070722584841647
06/01/2019 10:43:26 step: 2703, epoch: 81, batch: 29, loss: 0.007912486791610718, acc: 100.0, f1: 100.0, r: 0.8050195447375317
06/01/2019 10:43:26 *** evaluating ***
06/01/2019 10:43:26 step: 82, epoch: 81, acc: 59.82905982905983, f1: 26.118901804753737, r: 0.2928527878424668
06/01/2019 10:43:26 *** epoch: 83 ***
06/01/2019 10:43:26 *** training ***
06/01/2019 10:43:27 step: 2711, epoch: 82, batch: 4, loss: 0.0010195672512054443, acc: 100.0, f1: 100.0, r: 0.6661123595272225
06/01/2019 10:43:28 step: 2716, epoch: 82, batch: 9, loss: 0.0023281648755073547, acc: 100.0, f1: 100.0, r: 0.792020228768295
06/01/2019 10:43:28 step: 2721, epoch: 82, batch: 14, loss: 0.004490561783313751, acc: 100.0, f1: 100.0, r: 0.6993768285811455
06/01/2019 10:43:29 step: 2726, epoch: 82, batch: 19, loss: 0.004368677735328674, acc: 100.0, f1: 100.0, r: 0.7010924282179136
06/01/2019 10:43:30 step: 2731, epoch: 82, batch: 24, loss: 0.003268130123615265, acc: 100.0, f1: 100.0, r: 0.7273692535169842
06/01/2019 10:43:31 step: 2736, epoch: 82, batch: 29, loss: 0.0046472325921058655, acc: 100.0, f1: 100.0, r: 0.7082053542318182
06/01/2019 10:43:31 *** evaluating ***
06/01/2019 10:43:31 step: 83, epoch: 82, acc: 60.68376068376068, f1: 25.76011657333189, r: 0.2881103977041952
06/01/2019 10:43:31 *** epoch: 84 ***
06/01/2019 10:43:31 *** training ***
06/01/2019 10:43:32 step: 2744, epoch: 83, batch: 4, loss: 0.0033980384469032288, acc: 100.0, f1: 100.0, r: 0.6102442002776032
06/01/2019 10:43:32 step: 2749, epoch: 83, batch: 9, loss: 0.0057244673371315, acc: 100.0, f1: 100.0, r: 0.7044805652146252
06/01/2019 10:43:33 step: 2754, epoch: 83, batch: 14, loss: 0.00579022616147995, acc: 100.0, f1: 100.0, r: 0.6195766484498796
06/01/2019 10:43:34 step: 2759, epoch: 83, batch: 19, loss: 0.007917620241641998, acc: 100.0, f1: 100.0, r: 0.7851563720530145
06/01/2019 10:43:35 step: 2764, epoch: 83, batch: 24, loss: 0.010707609355449677, acc: 100.0, f1: 100.0, r: 0.8346639319611755
06/01/2019 10:43:35 step: 2769, epoch: 83, batch: 29, loss: 0.006323732435703278, acc: 100.0, f1: 100.0, r: 0.7818149636833281
06/01/2019 10:43:36 *** evaluating ***
06/01/2019 10:43:36 step: 84, epoch: 83, acc: 60.68376068376068, f1: 27.403106502539853, r: 0.28347922409195025
06/01/2019 10:43:36 *** epoch: 85 ***
06/01/2019 10:43:36 *** training ***
06/01/2019 10:43:37 step: 2777, epoch: 84, batch: 4, loss: 0.0017437785863876343, acc: 100.0, f1: 100.0, r: 0.7577117362492068
06/01/2019 10:43:37 step: 2782, epoch: 84, batch: 9, loss: 0.00443798303604126, acc: 100.0, f1: 100.0, r: 0.7044696060001793
06/01/2019 10:43:38 step: 2787, epoch: 84, batch: 14, loss: 0.001836150884628296, acc: 100.0, f1: 100.0, r: 0.5792469179947781
06/01/2019 10:43:39 step: 2792, epoch: 84, batch: 19, loss: 0.008325450122356415, acc: 100.0, f1: 100.0, r: 0.697252345530036
06/01/2019 10:43:40 step: 2797, epoch: 84, batch: 24, loss: 0.0016622170805931091, acc: 100.0, f1: 100.0, r: 0.6166610740060214
06/01/2019 10:43:40 step: 2802, epoch: 84, batch: 29, loss: 0.004052869975566864, acc: 100.0, f1: 100.0, r: 0.6956674017247987
06/01/2019 10:43:41 *** evaluating ***
06/01/2019 10:43:41 step: 85, epoch: 84, acc: 60.256410256410255, f1: 27.328720858422106, r: 0.2958726684700575
06/01/2019 10:43:41 *** epoch: 86 ***
06/01/2019 10:43:41 *** training ***
06/01/2019 10:43:42 step: 2810, epoch: 85, batch: 4, loss: 0.0015743076801300049, acc: 100.0, f1: 100.0, r: 0.7461594566940581
06/01/2019 10:43:42 step: 2815, epoch: 85, batch: 9, loss: 0.00878899171948433, acc: 100.0, f1: 100.0, r: 0.6773063158577113
06/01/2019 10:43:43 step: 2820, epoch: 85, batch: 14, loss: 0.004305824637413025, acc: 100.0, f1: 100.0, r: 0.7396627253073517
06/01/2019 10:43:44 step: 2825, epoch: 85, batch: 19, loss: 0.00349290668964386, acc: 100.0, f1: 100.0, r: 0.8277518595132882
06/01/2019 10:43:45 step: 2830, epoch: 85, batch: 24, loss: 0.004454024136066437, acc: 100.0, f1: 100.0, r: 0.8264319953152957
06/01/2019 10:43:45 step: 2835, epoch: 85, batch: 29, loss: 0.0038160160183906555, acc: 100.0, f1: 100.0, r: 0.8611683569695318
06/01/2019 10:43:46 *** evaluating ***
06/01/2019 10:43:46 step: 86, epoch: 85, acc: 61.53846153846154, f1: 28.442118750334146, r: 0.29670306261257257
06/01/2019 10:43:46 *** epoch: 87 ***
06/01/2019 10:43:46 *** training ***
06/01/2019 10:43:47 step: 2843, epoch: 86, batch: 4, loss: 0.002231776714324951, acc: 100.0, f1: 100.0, r: 0.7403438355066578
06/01/2019 10:43:47 step: 2848, epoch: 86, batch: 9, loss: 0.005423322319984436, acc: 100.0, f1: 100.0, r: 0.7558063076024653
06/01/2019 10:43:48 step: 2853, epoch: 86, batch: 14, loss: 0.0033178627490997314, acc: 100.0, f1: 100.0, r: 0.7258505040851657
06/01/2019 10:43:49 step: 2858, epoch: 86, batch: 19, loss: 0.005658999085426331, acc: 100.0, f1: 100.0, r: 0.7858923496405541
06/01/2019 10:43:49 step: 2863, epoch: 86, batch: 24, loss: 0.013553023338317871, acc: 100.0, f1: 100.0, r: 0.6179489055995512
06/01/2019 10:43:50 step: 2868, epoch: 86, batch: 29, loss: 0.0030251964926719666, acc: 100.0, f1: 100.0, r: 0.7401727113261336
06/01/2019 10:43:50 *** evaluating ***
06/01/2019 10:43:51 step: 87, epoch: 86, acc: 60.68376068376068, f1: 27.852272727272727, r: 0.29867526444438175
06/01/2019 10:43:51 *** epoch: 88 ***
06/01/2019 10:43:51 *** training ***
06/01/2019 10:43:51 step: 2876, epoch: 87, batch: 4, loss: 0.00889294221997261, acc: 100.0, f1: 100.0, r: 0.8148740024331311
06/01/2019 10:43:52 step: 2881, epoch: 87, batch: 9, loss: 0.054579779505729675, acc: 98.4375, f1: 98.9329064959317, r: 0.7539457836220449
06/01/2019 10:43:53 step: 2886, epoch: 87, batch: 14, loss: 0.005286417901515961, acc: 100.0, f1: 100.0, r: 0.7681507764174584
06/01/2019 10:43:54 step: 2891, epoch: 87, batch: 19, loss: 0.004294589161872864, acc: 100.0, f1: 100.0, r: 0.6904066988636375
06/01/2019 10:43:54 step: 2896, epoch: 87, batch: 24, loss: 0.00426260381937027, acc: 100.0, f1: 100.0, r: 0.6893393998890465
06/01/2019 10:43:55 step: 2901, epoch: 87, batch: 29, loss: 0.0022493377327919006, acc: 100.0, f1: 100.0, r: 0.6513107734529336
06/01/2019 10:43:55 *** evaluating ***
06/01/2019 10:43:55 step: 88, epoch: 87, acc: 59.401709401709404, f1: 25.51222101743923, r: 0.2910954566455386
06/01/2019 10:43:55 *** epoch: 89 ***
06/01/2019 10:43:55 *** training ***
06/01/2019 10:43:56 step: 2909, epoch: 88, batch: 4, loss: 0.003363393247127533, acc: 100.0, f1: 100.0, r: 0.8219595715748039
06/01/2019 10:43:57 step: 2914, epoch: 88, batch: 9, loss: 0.0019692033529281616, acc: 100.0, f1: 100.0, r: 0.8158785835632552
06/01/2019 10:43:58 step: 2919, epoch: 88, batch: 14, loss: 0.003334149718284607, acc: 100.0, f1: 100.0, r: 0.7874776828040814
06/01/2019 10:43:58 step: 2924, epoch: 88, batch: 19, loss: 0.005209565162658691, acc: 100.0, f1: 100.0, r: 0.8060012212054283
06/01/2019 10:43:59 step: 2929, epoch: 88, batch: 24, loss: 0.001818835735321045, acc: 100.0, f1: 100.0, r: 0.7918494755225509
06/01/2019 10:44:00 step: 2934, epoch: 88, batch: 29, loss: 0.0023268386721611023, acc: 100.0, f1: 100.0, r: 0.8360615605697608
06/01/2019 10:44:00 *** evaluating ***
06/01/2019 10:44:00 step: 89, epoch: 88, acc: 60.68376068376068, f1: 28.13357881955443, r: 0.2975423766164992
06/01/2019 10:44:00 *** epoch: 90 ***
06/01/2019 10:44:00 *** training ***
06/01/2019 10:44:01 step: 2942, epoch: 89, batch: 4, loss: 0.0012803375720977783, acc: 100.0, f1: 100.0, r: 0.7381216267923365
06/01/2019 10:44:02 step: 2947, epoch: 89, batch: 9, loss: 0.006410844624042511, acc: 100.0, f1: 100.0, r: 0.8318841800598955
06/01/2019 10:44:02 step: 2952, epoch: 89, batch: 14, loss: 0.010855033993721008, acc: 100.0, f1: 100.0, r: 0.7909493529907486
06/01/2019 10:44:03 step: 2957, epoch: 89, batch: 19, loss: 0.00454297661781311, acc: 100.0, f1: 100.0, r: 0.7597506143380346
06/01/2019 10:44:04 step: 2962, epoch: 89, batch: 24, loss: 0.02188560739159584, acc: 98.4375, f1: 98.75607385811468, r: 0.6975937300624505
06/01/2019 10:44:05 step: 2967, epoch: 89, batch: 29, loss: 0.004964187741279602, acc: 100.0, f1: 100.0, r: 0.7589106045877272
06/01/2019 10:44:05 *** evaluating ***
06/01/2019 10:44:05 step: 90, epoch: 89, acc: 59.401709401709404, f1: 26.91126695537898, r: 0.297688902126376
06/01/2019 10:44:05 *** epoch: 91 ***
06/01/2019 10:44:05 *** training ***
06/01/2019 10:44:06 step: 2975, epoch: 90, batch: 4, loss: 0.003281623125076294, acc: 100.0, f1: 100.0, r: 0.7780648660999058
06/01/2019 10:44:07 step: 2980, epoch: 90, batch: 9, loss: 0.011137038469314575, acc: 100.0, f1: 100.0, r: 0.6519045861984075
06/01/2019 10:44:07 step: 2985, epoch: 90, batch: 14, loss: 0.004928387701511383, acc: 100.0, f1: 100.0, r: 0.7547634716004934
06/01/2019 10:44:08 step: 2990, epoch: 90, batch: 19, loss: 0.005748361349105835, acc: 100.0, f1: 100.0, r: 0.7923390022548926
06/01/2019 10:44:09 step: 2995, epoch: 90, batch: 24, loss: 0.0030602142214775085, acc: 100.0, f1: 100.0, r: 0.717378313099111
06/01/2019 10:44:09 step: 3000, epoch: 90, batch: 29, loss: 0.004053935408592224, acc: 100.0, f1: 100.0, r: 0.7409793975915884
06/01/2019 10:44:10 *** evaluating ***
06/01/2019 10:44:10 step: 91, epoch: 90, acc: 61.965811965811966, f1: 28.214449020950994, r: 0.29658601484553426
06/01/2019 10:44:10 *** epoch: 92 ***
06/01/2019 10:44:10 *** training ***
06/01/2019 10:44:11 step: 3008, epoch: 91, batch: 4, loss: 0.0017273277044296265, acc: 100.0, f1: 100.0, r: 0.7791014866583302
06/01/2019 10:44:11 step: 3013, epoch: 91, batch: 9, loss: 0.0016383379697799683, acc: 100.0, f1: 100.0, r: 0.6364382901424894
06/01/2019 10:44:12 step: 3018, epoch: 91, batch: 14, loss: 0.003651037812232971, acc: 100.0, f1: 100.0, r: 0.5386355371245791
06/01/2019 10:44:13 step: 3023, epoch: 91, batch: 19, loss: 0.0024711787700653076, acc: 100.0, f1: 100.0, r: 0.721240315910377
06/01/2019 10:44:13 step: 3028, epoch: 91, batch: 24, loss: 0.0027125701308250427, acc: 100.0, f1: 100.0, r: 0.7617312380327067
06/01/2019 10:44:14 step: 3033, epoch: 91, batch: 29, loss: 0.0029853209853172302, acc: 100.0, f1: 100.0, r: 0.6986815335352996
06/01/2019 10:44:14 *** evaluating ***
06/01/2019 10:44:15 step: 92, epoch: 91, acc: 60.68376068376068, f1: 28.521259506949903, r: 0.2978789042766006
06/01/2019 10:44:15 *** epoch: 93 ***
06/01/2019 10:44:15 *** training ***
06/01/2019 10:44:15 step: 3041, epoch: 92, batch: 4, loss: 0.004164651036262512, acc: 100.0, f1: 100.0, r: 0.7229739250934712
06/01/2019 10:44:16 step: 3046, epoch: 92, batch: 9, loss: 0.03456764668226242, acc: 98.4375, f1: 99.38438438438439, r: 0.7579448816254153
06/01/2019 10:44:17 step: 3051, epoch: 92, batch: 14, loss: 0.004904694855213165, acc: 100.0, f1: 100.0, r: 0.7991074218070385
06/01/2019 10:44:17 step: 3056, epoch: 92, batch: 19, loss: 0.009434722363948822, acc: 100.0, f1: 100.0, r: 0.6962364299189946
06/01/2019 10:44:18 step: 3061, epoch: 92, batch: 24, loss: 0.006365038454532623, acc: 100.0, f1: 100.0, r: 0.7017456959872859
06/01/2019 10:44:18 step: 3066, epoch: 92, batch: 29, loss: 0.00409042090177536, acc: 100.0, f1: 100.0, r: 0.6385712905504565
06/01/2019 10:44:18 *** evaluating ***
06/01/2019 10:44:19 step: 93, epoch: 92, acc: 61.965811965811966, f1: 28.749599259758384, r: 0.30007870806964676
06/01/2019 10:44:19 *** epoch: 94 ***
06/01/2019 10:44:19 *** training ***
06/01/2019 10:44:19 step: 3074, epoch: 93, batch: 4, loss: 0.002901718020439148, acc: 100.0, f1: 100.0, r: 0.7433159207956507
06/01/2019 10:44:19 step: 3079, epoch: 93, batch: 9, loss: 0.0015665888786315918, acc: 100.0, f1: 100.0, r: 0.8186785790557974
06/01/2019 10:44:20 step: 3084, epoch: 93, batch: 14, loss: 0.002917572855949402, acc: 100.0, f1: 100.0, r: 0.6640024641319111
06/01/2019 10:44:20 step: 3089, epoch: 93, batch: 19, loss: 0.003607913851737976, acc: 100.0, f1: 100.0, r: 0.6969561300481697
06/01/2019 10:44:21 step: 3094, epoch: 93, batch: 24, loss: 0.006214510649442673, acc: 100.0, f1: 100.0, r: 0.769942317501435
06/01/2019 10:44:21 step: 3099, epoch: 93, batch: 29, loss: 0.008771590888500214, acc: 100.0, f1: 100.0, r: 0.7628680346317379
06/01/2019 10:44:22 *** evaluating ***
06/01/2019 10:44:22 step: 94, epoch: 93, acc: 62.82051282051282, f1: 28.295625942684765, r: 0.311051903928082
06/01/2019 10:44:22 *** epoch: 95 ***
06/01/2019 10:44:22 *** training ***
06/01/2019 10:44:22 step: 3107, epoch: 94, batch: 4, loss: 0.001619592308998108, acc: 100.0, f1: 100.0, r: 0.6851938624158752
06/01/2019 10:44:23 step: 3112, epoch: 94, batch: 9, loss: 0.002062886953353882, acc: 100.0, f1: 100.0, r: 0.7387082503419344
06/01/2019 10:44:23 step: 3117, epoch: 94, batch: 14, loss: 0.003834255039691925, acc: 100.0, f1: 100.0, r: 0.8298399716556077
06/01/2019 10:44:24 step: 3122, epoch: 94, batch: 19, loss: 0.019153937697410583, acc: 98.4375, f1: 97.97979797979798, r: 0.767401420565027
06/01/2019 10:44:24 step: 3127, epoch: 94, batch: 24, loss: 0.0026195943355560303, acc: 100.0, f1: 100.0, r: 0.7875153898318098
06/01/2019 10:44:25 step: 3132, epoch: 94, batch: 29, loss: 0.0020361170172691345, acc: 100.0, f1: 100.0, r: 0.7412903124113643
06/01/2019 10:44:25 *** evaluating ***
06/01/2019 10:44:25 step: 95, epoch: 94, acc: 61.965811965811966, f1: 28.6780118030118, r: 0.30657895562436144
06/01/2019 10:44:25 *** epoch: 96 ***
06/01/2019 10:44:25 *** training ***
06/01/2019 10:44:26 step: 3140, epoch: 95, batch: 4, loss: 0.005755364894866943, acc: 100.0, f1: 100.0, r: 0.6800984858580037
06/01/2019 10:44:26 step: 3145, epoch: 95, batch: 9, loss: 0.0037911534309387207, acc: 100.0, f1: 100.0, r: 0.8506569763648593
06/01/2019 10:44:26 step: 3150, epoch: 95, batch: 14, loss: 0.002932153642177582, acc: 100.0, f1: 100.0, r: 0.7100081538186891
06/01/2019 10:44:27 step: 3155, epoch: 95, batch: 19, loss: 0.0060710906982421875, acc: 100.0, f1: 100.0, r: 0.7563891717968113
06/01/2019 10:44:27 step: 3160, epoch: 95, batch: 24, loss: 0.003075912594795227, acc: 100.0, f1: 100.0, r: 0.8370993923385524
06/01/2019 10:44:28 step: 3165, epoch: 95, batch: 29, loss: 0.0036275461316108704, acc: 100.0, f1: 100.0, r: 0.7870231838533456
06/01/2019 10:44:28 *** evaluating ***
06/01/2019 10:44:28 step: 96, epoch: 95, acc: 62.39316239316239, f1: 29.3665625620533, r: 0.31013271922957236
06/01/2019 10:44:28 *** epoch: 97 ***
06/01/2019 10:44:28 *** training ***
06/01/2019 10:44:29 step: 3173, epoch: 96, batch: 4, loss: 0.0012392327189445496, acc: 100.0, f1: 100.0, r: 0.6271291474068122
06/01/2019 10:44:29 step: 3178, epoch: 96, batch: 9, loss: 0.009888507425785065, acc: 100.0, f1: 100.0, r: 0.7063218325872277
06/01/2019 10:44:30 step: 3183, epoch: 96, batch: 14, loss: 0.0023468658328056335, acc: 100.0, f1: 100.0, r: 0.6987344916545473
06/01/2019 10:44:30 step: 3188, epoch: 96, batch: 19, loss: 0.0033970512449741364, acc: 100.0, f1: 100.0, r: 0.6884056760197649
06/01/2019 10:44:31 step: 3193, epoch: 96, batch: 24, loss: 0.002598673105239868, acc: 100.0, f1: 100.0, r: 0.7864042181340221
06/01/2019 10:44:31 step: 3198, epoch: 96, batch: 29, loss: 0.005637727677822113, acc: 100.0, f1: 100.0, r: 0.8391721076262374
06/01/2019 10:44:31 *** evaluating ***
06/01/2019 10:44:31 step: 97, epoch: 96, acc: 61.111111111111114, f1: 26.784323839701308, r: 0.30415820089004664
06/01/2019 10:44:31 *** epoch: 98 ***
06/01/2019 10:44:31 *** training ***
06/01/2019 10:44:32 step: 3206, epoch: 97, batch: 4, loss: 0.002631448209285736, acc: 100.0, f1: 100.0, r: 0.6912431904806391
06/01/2019 10:44:32 step: 3211, epoch: 97, batch: 9, loss: 0.01577107608318329, acc: 98.4375, f1: 98.93065998329156, r: 0.6958146854716803
06/01/2019 10:44:33 step: 3216, epoch: 97, batch: 14, loss: 0.0017567276954650879, acc: 100.0, f1: 100.0, r: 0.7823389542020343
06/01/2019 10:44:33 step: 3221, epoch: 97, batch: 19, loss: 0.002115543931722641, acc: 100.0, f1: 100.0, r: 0.7946711924539799
06/01/2019 10:44:34 step: 3226, epoch: 97, batch: 24, loss: 0.0035918205976486206, acc: 100.0, f1: 100.0, r: 0.7191240090666698
06/01/2019 10:44:34 step: 3231, epoch: 97, batch: 29, loss: 0.002019628882408142, acc: 100.0, f1: 100.0, r: 0.767113687040674
06/01/2019 10:44:34 *** evaluating ***
06/01/2019 10:44:35 step: 98, epoch: 97, acc: 62.82051282051282, f1: 28.253312816038186, r: 0.31622466364235574
06/01/2019 10:44:35 *** epoch: 99 ***
06/01/2019 10:44:35 *** training ***
06/01/2019 10:44:35 step: 3239, epoch: 98, batch: 4, loss: 0.0035564005374908447, acc: 100.0, f1: 100.0, r: 0.8078147870332177
06/01/2019 10:44:36 step: 3244, epoch: 98, batch: 9, loss: 0.0017773285508155823, acc: 100.0, f1: 100.0, r: 0.8194770908886472
06/01/2019 10:44:36 step: 3249, epoch: 98, batch: 14, loss: 0.004229146987199783, acc: 100.0, f1: 100.0, r: 0.6208386312482266
06/01/2019 10:44:36 step: 3254, epoch: 98, batch: 19, loss: 0.0032855570316314697, acc: 100.0, f1: 100.0, r: 0.6670547120316781
06/01/2019 10:44:37 step: 3259, epoch: 98, batch: 24, loss: 0.0024179592728614807, acc: 100.0, f1: 100.0, r: 0.7543709316442622
06/01/2019 10:44:37 step: 3264, epoch: 98, batch: 29, loss: 0.009172286838293076, acc: 100.0, f1: 100.0, r: 0.8216326536074084
06/01/2019 10:44:38 *** evaluating ***
06/01/2019 10:44:38 step: 99, epoch: 98, acc: 62.39316239316239, f1: 27.453494218200102, r: 0.3175000633302103
06/01/2019 10:44:38 *** epoch: 100 ***
06/01/2019 10:44:38 *** training ***
06/01/2019 10:44:38 step: 3272, epoch: 99, batch: 4, loss: 0.010927289724349976, acc: 100.0, f1: 100.0, r: 0.6235965297945534
06/01/2019 10:44:39 step: 3277, epoch: 99, batch: 9, loss: 0.012420523911714554, acc: 100.0, f1: 100.0, r: 0.6654886942190772
06/01/2019 10:44:39 step: 3282, epoch: 99, batch: 14, loss: 0.0050546228885650635, acc: 100.0, f1: 100.0, r: 0.7111449026077834
06/01/2019 10:44:40 step: 3287, epoch: 99, batch: 19, loss: 0.005165129899978638, acc: 100.0, f1: 100.0, r: 0.7773233592266193
06/01/2019 10:44:40 step: 3292, epoch: 99, batch: 24, loss: 0.003994166851043701, acc: 100.0, f1: 100.0, r: 0.6220060402319652
06/01/2019 10:44:41 step: 3297, epoch: 99, batch: 29, loss: 0.004143610596656799, acc: 100.0, f1: 100.0, r: 0.8149064714543368
06/01/2019 10:44:41 *** evaluating ***
06/01/2019 10:44:41 step: 100, epoch: 99, acc: 61.965811965811966, f1: 31.610435675069017, r: 0.2977136305210861
06/01/2019 10:44:41 *** epoch: 101 ***
06/01/2019 10:44:41 *** training ***
06/01/2019 10:44:41 step: 3305, epoch: 100, batch: 4, loss: 0.0018455684185028076, acc: 100.0, f1: 100.0, r: 0.7032896903296467
06/01/2019 10:44:42 step: 3310, epoch: 100, batch: 9, loss: 0.002477683126926422, acc: 100.0, f1: 100.0, r: 0.7443177286866731
06/01/2019 10:44:42 step: 3315, epoch: 100, batch: 14, loss: 0.019803635776042938, acc: 98.4375, f1: 99.31773879142301, r: 0.7564585432371553
06/01/2019 10:44:43 step: 3320, epoch: 100, batch: 19, loss: 0.0056119635701179504, acc: 100.0, f1: 100.0, r: 0.8007060624585157
06/01/2019 10:44:43 step: 3325, epoch: 100, batch: 24, loss: 0.002659514546394348, acc: 100.0, f1: 100.0, r: 0.7957534602947643
06/01/2019 10:44:44 step: 3330, epoch: 100, batch: 29, loss: 0.0010561421513557434, acc: 100.0, f1: 100.0, r: 0.8026627644729308
06/01/2019 10:44:44 *** evaluating ***
06/01/2019 10:44:44 step: 101, epoch: 100, acc: 63.24786324786324, f1: 30.69487681753238, r: 0.3063586010517658
06/01/2019 10:44:44 *** epoch: 102 ***
06/01/2019 10:44:44 *** training ***
06/01/2019 10:44:44 step: 3338, epoch: 101, batch: 4, loss: 0.0027308091521263123, acc: 100.0, f1: 100.0, r: 0.783239847961673
06/01/2019 10:44:45 step: 3343, epoch: 101, batch: 9, loss: 0.015306789427995682, acc: 100.0, f1: 100.0, r: 0.5981542765665914
06/01/2019 10:44:45 step: 3348, epoch: 101, batch: 14, loss: 0.003928549587726593, acc: 100.0, f1: 100.0, r: 0.8370208816436348
06/01/2019 10:44:46 step: 3353, epoch: 101, batch: 19, loss: 0.008182376623153687, acc: 100.0, f1: 100.0, r: 0.7330580325901122
06/01/2019 10:44:46 step: 3358, epoch: 101, batch: 24, loss: 0.007669001817703247, acc: 100.0, f1: 100.0, r: 0.792348715670468
06/01/2019 10:44:47 step: 3363, epoch: 101, batch: 29, loss: 0.002493254840373993, acc: 100.0, f1: 100.0, r: 0.7179278940797489
06/01/2019 10:44:47 *** evaluating ***
06/01/2019 10:44:47 step: 102, epoch: 101, acc: 62.82051282051282, f1: 29.062328954570333, r: 0.30776490758924924
06/01/2019 10:44:47 *** epoch: 103 ***
06/01/2019 10:44:47 *** training ***
06/01/2019 10:44:48 step: 3371, epoch: 102, batch: 4, loss: 0.003394298255443573, acc: 100.0, f1: 100.0, r: 0.6958186548314172
06/01/2019 10:44:48 step: 3376, epoch: 102, batch: 9, loss: 0.002821214497089386, acc: 100.0, f1: 100.0, r: 0.8424633069837981
06/01/2019 10:44:49 step: 3381, epoch: 102, batch: 14, loss: 0.0012784749269485474, acc: 100.0, f1: 100.0, r: 0.7290308248891474
06/01/2019 10:44:49 step: 3386, epoch: 102, batch: 19, loss: 0.004331618547439575, acc: 100.0, f1: 100.0, r: 0.7138624522722962
06/01/2019 10:44:50 step: 3391, epoch: 102, batch: 24, loss: 0.008930347859859467, acc: 100.0, f1: 100.0, r: 0.8213913566923426
06/01/2019 10:44:50 step: 3396, epoch: 102, batch: 29, loss: 0.004010103642940521, acc: 100.0, f1: 100.0, r: 0.8374159059844845
06/01/2019 10:44:50 *** evaluating ***
06/01/2019 10:44:51 step: 103, epoch: 102, acc: 61.965811965811966, f1: 28.11121908116859, r: 0.31243324774085923
06/01/2019 10:44:51 *** epoch: 104 ***
06/01/2019 10:44:51 *** training ***
06/01/2019 10:44:51 step: 3404, epoch: 103, batch: 4, loss: 0.006299465894699097, acc: 100.0, f1: 100.0, r: 0.7394307375651205
06/01/2019 10:44:52 step: 3409, epoch: 103, batch: 9, loss: 0.006104357540607452, acc: 100.0, f1: 100.0, r: 0.6717340987770807
06/01/2019 10:44:52 step: 3414, epoch: 103, batch: 14, loss: 0.0010676383972167969, acc: 100.0, f1: 100.0, r: 0.6744362625348416
06/01/2019 10:44:53 step: 3419, epoch: 103, batch: 19, loss: 0.001423366367816925, acc: 100.0, f1: 100.0, r: 0.7905782299367285
06/01/2019 10:44:53 step: 3424, epoch: 103, batch: 24, loss: 0.011368885636329651, acc: 100.0, f1: 100.0, r: 0.6712305247567553
06/01/2019 10:44:54 step: 3429, epoch: 103, batch: 29, loss: 0.0038514211773872375, acc: 100.0, f1: 100.0, r: 0.7236324212549753
06/01/2019 10:44:54 *** evaluating ***
06/01/2019 10:44:54 step: 104, epoch: 103, acc: 60.256410256410255, f1: 24.65087145969499, r: 0.2993548147911144
06/01/2019 10:44:54 *** epoch: 105 ***
06/01/2019 10:44:54 *** training ***
06/01/2019 10:44:55 step: 3437, epoch: 104, batch: 4, loss: 0.00589381530880928, acc: 100.0, f1: 100.0, r: 0.6720762495140692
06/01/2019 10:44:55 step: 3442, epoch: 104, batch: 9, loss: 0.0037966296076774597, acc: 100.0, f1: 100.0, r: 0.8081948147985667
06/01/2019 10:44:56 step: 3447, epoch: 104, batch: 14, loss: 0.002291768789291382, acc: 100.0, f1: 100.0, r: 0.8036328616968191
06/01/2019 10:44:56 step: 3452, epoch: 104, batch: 19, loss: 0.009345866739749908, acc: 100.0, f1: 100.0, r: 0.6302787391461697
06/01/2019 10:44:57 step: 3457, epoch: 104, batch: 24, loss: 0.00397074967622757, acc: 100.0, f1: 100.0, r: 0.8122825011713173
06/01/2019 10:44:57 step: 3462, epoch: 104, batch: 29, loss: 0.002119496464729309, acc: 100.0, f1: 100.0, r: 0.7913045606210596
06/01/2019 10:44:57 *** evaluating ***
06/01/2019 10:44:58 step: 105, epoch: 104, acc: 61.965811965811966, f1: 26.93490046914404, r: 0.30121409336267674
06/01/2019 10:44:58 *** epoch: 106 ***
06/01/2019 10:44:58 *** training ***
06/01/2019 10:44:58 step: 3470, epoch: 105, batch: 4, loss: 0.004598952829837799, acc: 100.0, f1: 100.0, r: 0.7097124339408056
06/01/2019 10:44:59 step: 3475, epoch: 105, batch: 9, loss: 0.00434468686580658, acc: 100.0, f1: 100.0, r: 0.7553110999316457
06/01/2019 10:44:59 step: 3480, epoch: 105, batch: 14, loss: 0.0026977136731147766, acc: 100.0, f1: 100.0, r: 0.8205428208453788
06/01/2019 10:45:00 step: 3485, epoch: 105, batch: 19, loss: 0.0038641467690467834, acc: 100.0, f1: 100.0, r: 0.6939356618249156
06/01/2019 10:45:00 step: 3490, epoch: 105, batch: 24, loss: 0.010532727465033531, acc: 100.0, f1: 100.0, r: 0.7926664106821719
06/01/2019 10:45:01 step: 3495, epoch: 105, batch: 29, loss: 0.012171734124422073, acc: 100.0, f1: 100.0, r: 0.8357718518137552
06/01/2019 10:45:01 *** evaluating ***
06/01/2019 10:45:01 step: 106, epoch: 105, acc: 61.965811965811966, f1: 28.43435938347831, r: 0.29512316149755274
06/01/2019 10:45:01 *** epoch: 107 ***
06/01/2019 10:45:01 *** training ***
06/01/2019 10:45:01 step: 3503, epoch: 106, batch: 4, loss: 0.006647981703281403, acc: 100.0, f1: 100.0, r: 0.7935752583124873
06/01/2019 10:45:02 step: 3508, epoch: 106, batch: 9, loss: 0.0029867738485336304, acc: 100.0, f1: 100.0, r: 0.7350535803705138
06/01/2019 10:45:02 step: 3513, epoch: 106, batch: 14, loss: 0.005980081856250763, acc: 100.0, f1: 100.0, r: 0.7551576576721976
06/01/2019 10:45:03 step: 3518, epoch: 106, batch: 19, loss: 0.0019415020942687988, acc: 100.0, f1: 100.0, r: 0.7165455980256604
06/01/2019 10:45:03 step: 3523, epoch: 106, batch: 24, loss: 0.005945775657892227, acc: 100.0, f1: 100.0, r: 0.6933225427465132
06/01/2019 10:45:04 step: 3528, epoch: 106, batch: 29, loss: 0.008062280714511871, acc: 100.0, f1: 100.0, r: 0.6848762354187101
06/01/2019 10:45:04 *** evaluating ***
06/01/2019 10:45:04 step: 107, epoch: 106, acc: 61.111111111111114, f1: 28.16601024338355, r: 0.2976637572474192
06/01/2019 10:45:04 *** epoch: 108 ***
06/01/2019 10:45:04 *** training ***
06/01/2019 10:45:05 step: 3536, epoch: 107, batch: 4, loss: 0.002134747803211212, acc: 100.0, f1: 100.0, r: 0.6590175950628239
06/01/2019 10:45:05 step: 3541, epoch: 107, batch: 9, loss: 0.003172174096107483, acc: 100.0, f1: 100.0, r: 0.7807090635938398
06/01/2019 10:45:06 step: 3546, epoch: 107, batch: 14, loss: 0.0024239271879196167, acc: 100.0, f1: 100.0, r: 0.7315739075624835
06/01/2019 10:45:06 step: 3551, epoch: 107, batch: 19, loss: 0.0014341026544570923, acc: 100.0, f1: 100.0, r: 0.7500297550966597
06/01/2019 10:45:07 step: 3556, epoch: 107, batch: 24, loss: 0.004038088023662567, acc: 100.0, f1: 100.0, r: 0.7326007424946013
06/01/2019 10:45:07 step: 3561, epoch: 107, batch: 29, loss: 0.0027984753251075745, acc: 100.0, f1: 100.0, r: 0.7738261272727376
06/01/2019 10:45:08 *** evaluating ***
06/01/2019 10:45:08 step: 108, epoch: 107, acc: 61.965811965811966, f1: 27.67771910459279, r: 0.30541959913067884
06/01/2019 10:45:08 *** epoch: 109 ***
06/01/2019 10:45:08 *** training ***
06/01/2019 10:45:08 step: 3569, epoch: 108, batch: 4, loss: 0.005580879747867584, acc: 100.0, f1: 100.0, r: 0.7788657351961312
06/01/2019 10:45:09 step: 3574, epoch: 108, batch: 9, loss: 0.0032640397548675537, acc: 100.0, f1: 100.0, r: 0.6875089299677024
06/01/2019 10:45:09 step: 3579, epoch: 108, batch: 14, loss: 0.00880681723356247, acc: 100.0, f1: 100.0, r: 0.8326195610064092
06/01/2019 10:45:10 step: 3584, epoch: 108, batch: 19, loss: 0.004023432731628418, acc: 100.0, f1: 100.0, r: 0.6701027815585632
06/01/2019 10:45:10 step: 3589, epoch: 108, batch: 24, loss: 0.0020420625805854797, acc: 100.0, f1: 100.0, r: 0.7208402916322455
06/01/2019 10:45:11 step: 3594, epoch: 108, batch: 29, loss: 0.004061862826347351, acc: 100.0, f1: 100.0, r: 0.6017667253933169
06/01/2019 10:45:11 *** evaluating ***
06/01/2019 10:45:11 step: 109, epoch: 108, acc: 61.53846153846154, f1: 27.15315934065934, r: 0.306815088096591
06/01/2019 10:45:11 *** epoch: 110 ***
06/01/2019 10:45:11 *** training ***
06/01/2019 10:45:12 step: 3602, epoch: 109, batch: 4, loss: 0.0026882588863372803, acc: 100.0, f1: 100.0, r: 0.721957796441366
06/01/2019 10:45:12 step: 3607, epoch: 109, batch: 9, loss: 0.0022446289658546448, acc: 100.0, f1: 100.0, r: 0.6640180564732964
06/01/2019 10:45:13 step: 3612, epoch: 109, batch: 14, loss: 0.003222160041332245, acc: 100.0, f1: 100.0, r: 0.6663658096284574
06/01/2019 10:45:13 step: 3617, epoch: 109, batch: 19, loss: 0.0013450086116790771, acc: 100.0, f1: 100.0, r: 0.7182104847616537
06/01/2019 10:45:14 step: 3622, epoch: 109, batch: 24, loss: 0.003543868660926819, acc: 100.0, f1: 100.0, r: 0.8163480764802247
06/01/2019 10:45:14 step: 3627, epoch: 109, batch: 29, loss: 0.002029549330472946, acc: 100.0, f1: 100.0, r: 0.6476859815089941
06/01/2019 10:45:14 *** evaluating ***
06/01/2019 10:45:14 step: 110, epoch: 109, acc: 58.97435897435898, f1: 26.283500440457374, r: 0.29337693937296694
06/01/2019 10:45:14 *** epoch: 111 ***
06/01/2019 10:45:14 *** training ***
06/01/2019 10:45:15 step: 3635, epoch: 110, batch: 4, loss: 0.0017888471484184265, acc: 100.0, f1: 100.0, r: 0.7472268775211572
06/01/2019 10:45:15 step: 3640, epoch: 110, batch: 9, loss: 0.0013548657298088074, acc: 100.0, f1: 100.0, r: 0.6163513821367473
06/01/2019 10:45:16 step: 3645, epoch: 110, batch: 14, loss: 0.0046911537647247314, acc: 100.0, f1: 100.0, r: 0.6718472154501858
06/01/2019 10:45:16 step: 3650, epoch: 110, batch: 19, loss: 0.018334418535232544, acc: 98.4375, f1: 99.21182266009852, r: 0.8134230347735684
06/01/2019 10:45:17 step: 3655, epoch: 110, batch: 24, loss: 0.003908790647983551, acc: 100.0, f1: 100.0, r: 0.8413772889944198
06/01/2019 10:45:17 step: 3660, epoch: 110, batch: 29, loss: 0.0016206353902816772, acc: 100.0, f1: 100.0, r: 0.6785003035034912
06/01/2019 10:45:18 *** evaluating ***
06/01/2019 10:45:18 step: 111, epoch: 110, acc: 60.256410256410255, f1: 28.15763264816318, r: 0.2941280975095321
06/01/2019 10:45:18 *** epoch: 112 ***
06/01/2019 10:45:18 *** training ***
06/01/2019 10:45:18 step: 3668, epoch: 111, batch: 4, loss: 0.0026507340371608734, acc: 100.0, f1: 100.0, r: 0.780671546863965
06/01/2019 10:45:19 step: 3673, epoch: 111, batch: 9, loss: 0.003944307565689087, acc: 100.0, f1: 100.0, r: 0.8107020186107787
06/01/2019 10:45:19 step: 3678, epoch: 111, batch: 14, loss: 0.0025007277727127075, acc: 100.0, f1: 100.0, r: 0.7241598565032499
06/01/2019 10:45:20 step: 3683, epoch: 111, batch: 19, loss: 0.0016849637031555176, acc: 100.0, f1: 100.0, r: 0.8169611939678649
06/01/2019 10:45:20 step: 3688, epoch: 111, batch: 24, loss: 0.004641436040401459, acc: 100.0, f1: 100.0, r: 0.6003634535798141
06/01/2019 10:45:21 step: 3693, epoch: 111, batch: 29, loss: 0.0023083239793777466, acc: 100.0, f1: 100.0, r: 0.8228312380562203
06/01/2019 10:45:21 *** evaluating ***
06/01/2019 10:45:21 step: 112, epoch: 111, acc: 60.68376068376068, f1: 27.204970575872217, r: 0.29436086416772383
06/01/2019 10:45:21 *** epoch: 113 ***
06/01/2019 10:45:21 *** training ***
06/01/2019 10:45:22 step: 3701, epoch: 112, batch: 4, loss: 0.0008981823921203613, acc: 100.0, f1: 100.0, r: 0.6874095121035823
06/01/2019 10:45:22 step: 3706, epoch: 112, batch: 9, loss: 0.002497240900993347, acc: 100.0, f1: 100.0, r: 0.7119930194506493
06/01/2019 10:45:23 step: 3711, epoch: 112, batch: 14, loss: 0.001600503921508789, acc: 100.0, f1: 100.0, r: 0.6972625996469345
06/01/2019 10:45:23 step: 3716, epoch: 112, batch: 19, loss: 0.005043327808380127, acc: 100.0, f1: 100.0, r: 0.7903403421392309
06/01/2019 10:45:24 step: 3721, epoch: 112, batch: 24, loss: 0.0041159167885780334, acc: 100.0, f1: 100.0, r: 0.7137219571516251
06/01/2019 10:45:24 step: 3726, epoch: 112, batch: 29, loss: 0.0017455816268920898, acc: 100.0, f1: 100.0, r: 0.6884451028345459
06/01/2019 10:45:24 *** evaluating ***
06/01/2019 10:45:25 step: 113, epoch: 112, acc: 60.68376068376068, f1: 27.846428188482086, r: 0.29531117340821844
06/01/2019 10:45:25 *** epoch: 114 ***
06/01/2019 10:45:25 *** training ***
06/01/2019 10:45:25 step: 3734, epoch: 113, batch: 4, loss: 0.0011093392968177795, acc: 100.0, f1: 100.0, r: 0.6891671935787927
06/01/2019 10:45:26 step: 3739, epoch: 113, batch: 9, loss: 0.0019302219152450562, acc: 100.0, f1: 100.0, r: 0.717887678733105
06/01/2019 10:45:26 step: 3744, epoch: 113, batch: 14, loss: 0.0017756298184394836, acc: 100.0, f1: 100.0, r: 0.8494542277681574
06/01/2019 10:45:27 step: 3749, epoch: 113, batch: 19, loss: 0.003487996757030487, acc: 100.0, f1: 100.0, r: 0.7101857469276377
06/01/2019 10:45:27 step: 3754, epoch: 113, batch: 24, loss: 0.009410001337528229, acc: 100.0, f1: 100.0, r: 0.8037024861277624
06/01/2019 10:45:28 step: 3759, epoch: 113, batch: 29, loss: 0.002840716391801834, acc: 100.0, f1: 100.0, r: 0.778530671167276
06/01/2019 10:45:28 *** evaluating ***
06/01/2019 10:45:28 step: 114, epoch: 113, acc: 59.82905982905983, f1: 26.89011915767134, r: 0.2988207987735068
06/01/2019 10:45:28 *** epoch: 115 ***
06/01/2019 10:45:28 *** training ***
06/01/2019 10:45:29 step: 3767, epoch: 114, batch: 4, loss: 0.024905264377593994, acc: 98.4375, f1: 97.20730397422128, r: 0.6587246238891455
06/01/2019 10:45:29 step: 3772, epoch: 114, batch: 9, loss: 0.004206046462059021, acc: 100.0, f1: 100.0, r: 0.8122348729878728
06/01/2019 10:45:30 step: 3777, epoch: 114, batch: 14, loss: 0.011497978121042252, acc: 100.0, f1: 100.0, r: 0.7092148298458135
06/01/2019 10:45:30 step: 3782, epoch: 114, batch: 19, loss: 0.009056299924850464, acc: 100.0, f1: 100.0, r: 0.6507261441885153
06/01/2019 10:45:31 step: 3787, epoch: 114, batch: 24, loss: 0.007164079695940018, acc: 100.0, f1: 100.0, r: 0.7292574962288538
06/01/2019 10:45:31 step: 3792, epoch: 114, batch: 29, loss: 0.003688901662826538, acc: 100.0, f1: 100.0, r: 0.6728617970220819
06/01/2019 10:45:31 *** evaluating ***
06/01/2019 10:45:32 step: 115, epoch: 114, acc: 60.68376068376068, f1: 27.713032581453632, r: 0.28985923293131777
06/01/2019 10:45:32 *** epoch: 116 ***
06/01/2019 10:45:32 *** training ***
06/01/2019 10:45:32 step: 3800, epoch: 115, batch: 4, loss: 0.015588540583848953, acc: 98.4375, f1: 99.23404255319149, r: 0.7495936799639544
06/01/2019 10:45:33 step: 3805, epoch: 115, batch: 9, loss: 0.0024461671710014343, acc: 100.0, f1: 100.0, r: 0.7329139872439581
06/01/2019 10:45:33 step: 3810, epoch: 115, batch: 14, loss: 0.008698582649230957, acc: 100.0, f1: 100.0, r: 0.5746589575890637
06/01/2019 10:45:34 step: 3815, epoch: 115, batch: 19, loss: 0.003062080591917038, acc: 100.0, f1: 100.0, r: 0.733472335236778
06/01/2019 10:45:34 step: 3820, epoch: 115, batch: 24, loss: 0.0021524950861930847, acc: 100.0, f1: 100.0, r: 0.774334990099776
06/01/2019 10:45:35 step: 3825, epoch: 115, batch: 29, loss: 0.010897167026996613, acc: 100.0, f1: 100.0, r: 0.6120788846519721
06/01/2019 10:45:35 *** evaluating ***
06/01/2019 10:45:35 step: 116, epoch: 115, acc: 61.111111111111114, f1: 26.844619916807268, r: 0.2921968541214861
06/01/2019 10:45:35 *** epoch: 117 ***
06/01/2019 10:45:35 *** training ***
06/01/2019 10:45:35 step: 3833, epoch: 116, batch: 4, loss: 0.0025152936577796936, acc: 100.0, f1: 100.0, r: 0.6537836611236763
06/01/2019 10:45:36 step: 3838, epoch: 116, batch: 9, loss: 0.0025570057332515717, acc: 100.0, f1: 100.0, r: 0.6047224926117616
06/01/2019 10:45:37 step: 3843, epoch: 116, batch: 14, loss: 0.0021317526698112488, acc: 100.0, f1: 100.0, r: 0.7278887042859704
06/01/2019 10:45:37 step: 3848, epoch: 116, batch: 19, loss: 0.0035781264305114746, acc: 100.0, f1: 100.0, r: 0.7224935712926907
06/01/2019 10:45:37 step: 3853, epoch: 116, batch: 24, loss: 0.004893586039543152, acc: 100.0, f1: 100.0, r: 0.81880205894711
06/01/2019 10:45:38 step: 3858, epoch: 116, batch: 29, loss: 0.0016480833292007446, acc: 100.0, f1: 100.0, r: 0.6899395170939406
06/01/2019 10:45:38 *** evaluating ***
06/01/2019 10:45:38 step: 117, epoch: 116, acc: 61.53846153846154, f1: 26.910677629280222, r: 0.2934336574458785
06/01/2019 10:45:38 *** epoch: 118 ***
06/01/2019 10:45:38 *** training ***
06/01/2019 10:45:39 step: 3866, epoch: 117, batch: 4, loss: 0.0030016228556632996, acc: 100.0, f1: 100.0, r: 0.7160695986577018
06/01/2019 10:45:39 step: 3871, epoch: 117, batch: 9, loss: 0.00221407413482666, acc: 100.0, f1: 100.0, r: 0.751544023569941
06/01/2019 10:45:40 step: 3876, epoch: 117, batch: 14, loss: 0.001285053789615631, acc: 100.0, f1: 100.0, r: 0.6851588108817204
06/01/2019 10:45:40 step: 3881, epoch: 117, batch: 19, loss: 0.005517266690731049, acc: 100.0, f1: 100.0, r: 0.760119876838165
06/01/2019 10:45:41 step: 3886, epoch: 117, batch: 24, loss: 0.01527709886431694, acc: 98.4375, f1: 99.16883116883118, r: 0.6450918663654454
06/01/2019 10:45:41 step: 3891, epoch: 117, batch: 29, loss: 0.029676031321287155, acc: 98.4375, f1: 98.73358348968104, r: 0.7860452146878883
06/01/2019 10:45:42 *** evaluating ***
06/01/2019 10:45:42 step: 118, epoch: 117, acc: 60.256410256410255, f1: 26.668911419387204, r: 0.28702681611780106
06/01/2019 10:45:42 *** epoch: 119 ***
06/01/2019 10:45:42 *** training ***
06/01/2019 10:45:42 step: 3899, epoch: 118, batch: 4, loss: 0.014306295663118362, acc: 100.0, f1: 100.0, r: 0.7814677021242021
06/01/2019 10:45:43 step: 3904, epoch: 118, batch: 9, loss: 0.0025280267000198364, acc: 100.0, f1: 100.0, r: 0.822069180688388
06/01/2019 10:45:43 step: 3909, epoch: 118, batch: 14, loss: 0.006651781499385834, acc: 100.0, f1: 100.0, r: 0.592940784789245
06/01/2019 10:45:44 step: 3914, epoch: 118, batch: 19, loss: 0.002328529953956604, acc: 100.0, f1: 100.0, r: 0.7453880263658804
06/01/2019 10:45:44 step: 3919, epoch: 118, batch: 24, loss: 0.002253606915473938, acc: 100.0, f1: 100.0, r: 0.8195880367840627
06/01/2019 10:45:45 step: 3924, epoch: 118, batch: 29, loss: 0.0023050159215927124, acc: 100.0, f1: 100.0, r: 0.7031116358950151
06/01/2019 10:45:45 *** evaluating ***
06/01/2019 10:45:45 step: 119, epoch: 118, acc: 59.82905982905983, f1: 25.653954817815794, r: 0.28762150904094347
06/01/2019 10:45:45 *** epoch: 120 ***
06/01/2019 10:45:45 *** training ***
06/01/2019 10:45:46 step: 3932, epoch: 119, batch: 4, loss: 0.005137227475643158, acc: 100.0, f1: 100.0, r: 0.6406836995518302
06/01/2019 10:45:46 step: 3937, epoch: 119, batch: 9, loss: 0.0022569894790649414, acc: 100.0, f1: 100.0, r: 0.808823379933774
06/01/2019 10:45:47 step: 3942, epoch: 119, batch: 14, loss: 0.00171608105301857, acc: 100.0, f1: 100.0, r: 0.7730455525150923
06/01/2019 10:45:47 step: 3947, epoch: 119, batch: 19, loss: 0.0016284584999084473, acc: 100.0, f1: 100.0, r: 0.8310767870484068
06/01/2019 10:45:48 step: 3952, epoch: 119, batch: 24, loss: 0.003570765256881714, acc: 100.0, f1: 100.0, r: 0.7601953633147218
06/01/2019 10:45:48 step: 3957, epoch: 119, batch: 29, loss: 0.0024072453379631042, acc: 100.0, f1: 100.0, r: 0.6935486658891362
06/01/2019 10:45:48 *** evaluating ***
06/01/2019 10:45:49 step: 120, epoch: 119, acc: 61.53846153846154, f1: 26.237878809880755, r: 0.298835770548292
06/01/2019 10:45:49 *** epoch: 121 ***
06/01/2019 10:45:49 *** training ***
06/01/2019 10:45:49 step: 3965, epoch: 120, batch: 4, loss: 0.0017866119742393494, acc: 100.0, f1: 100.0, r: 0.7001296535192516
06/01/2019 10:45:50 step: 3970, epoch: 120, batch: 9, loss: 0.0021307766437530518, acc: 100.0, f1: 100.0, r: 0.732822773417843
06/01/2019 10:45:50 step: 3975, epoch: 120, batch: 14, loss: 0.0013067051768302917, acc: 100.0, f1: 100.0, r: 0.6453872611146623
06/01/2019 10:45:51 step: 3980, epoch: 120, batch: 19, loss: 0.0014866366982460022, acc: 100.0, f1: 100.0, r: 0.7394211295575839
06/01/2019 10:45:51 step: 3985, epoch: 120, batch: 24, loss: 0.002912767231464386, acc: 100.0, f1: 100.0, r: 0.6137631422643592
06/01/2019 10:45:52 step: 3990, epoch: 120, batch: 29, loss: 0.0046790242195129395, acc: 100.0, f1: 100.0, r: 0.6408240064270633
06/01/2019 10:45:52 *** evaluating ***
06/01/2019 10:45:52 step: 121, epoch: 120, acc: 61.53846153846154, f1: 26.222627959675993, r: 0.2923923369275179
06/01/2019 10:45:52 *** epoch: 122 ***
06/01/2019 10:45:52 *** training ***
06/01/2019 10:45:53 step: 3998, epoch: 121, batch: 4, loss: 0.003567725419998169, acc: 100.0, f1: 100.0, r: 0.8157379561970328
06/01/2019 10:45:53 step: 4003, epoch: 121, batch: 9, loss: 0.002964407205581665, acc: 100.0, f1: 100.0, r: 0.7063018694402383
06/01/2019 10:45:54 step: 4008, epoch: 121, batch: 14, loss: 0.0008275210857391357, acc: 100.0, f1: 100.0, r: 0.8051412946078885
06/01/2019 10:45:54 step: 4013, epoch: 121, batch: 19, loss: 0.001008056104183197, acc: 100.0, f1: 100.0, r: 0.5951398118278188
06/01/2019 10:45:55 step: 4018, epoch: 121, batch: 24, loss: 0.014406323432922363, acc: 100.0, f1: 100.0, r: 0.8344606053983906
06/01/2019 10:45:55 step: 4023, epoch: 121, batch: 29, loss: 0.002225451171398163, acc: 100.0, f1: 100.0, r: 0.6170606194620004
06/01/2019 10:45:55 *** evaluating ***
06/01/2019 10:45:55 step: 122, epoch: 121, acc: 58.119658119658126, f1: 26.33658799777221, r: 0.27769009896210184
06/01/2019 10:45:55 *** epoch: 123 ***
06/01/2019 10:45:55 *** training ***
06/01/2019 10:45:56 step: 4031, epoch: 122, batch: 4, loss: 0.025123663246631622, acc: 98.4375, f1: 97.95186891961086, r: 0.7134718746738606
06/01/2019 10:45:56 step: 4036, epoch: 122, batch: 9, loss: 0.005029980093240738, acc: 100.0, f1: 100.0, r: 0.7322353237976427
06/01/2019 10:45:57 step: 4041, epoch: 122, batch: 14, loss: 0.0033337250351905823, acc: 100.0, f1: 100.0, r: 0.7465429691544448
06/01/2019 10:45:57 step: 4046, epoch: 122, batch: 19, loss: 0.011116225272417068, acc: 100.0, f1: 100.0, r: 0.7777342924616808
06/01/2019 10:45:58 step: 4051, epoch: 122, batch: 24, loss: 0.0036156848073005676, acc: 100.0, f1: 100.0, r: 0.8511248981941233
06/01/2019 10:45:58 step: 4056, epoch: 122, batch: 29, loss: 0.00368688628077507, acc: 100.0, f1: 100.0, r: 0.7334947498257636
06/01/2019 10:45:59 *** evaluating ***
06/01/2019 10:45:59 step: 123, epoch: 122, acc: 60.256410256410255, f1: 27.67086115536501, r: 0.28880798944594144
06/01/2019 10:45:59 *** epoch: 124 ***
06/01/2019 10:45:59 *** training ***
06/01/2019 10:45:59 step: 4064, epoch: 123, batch: 4, loss: 0.010331802070140839, acc: 100.0, f1: 100.0, r: 0.7663442078467474
06/01/2019 10:46:00 step: 4069, epoch: 123, batch: 9, loss: 0.007397890090942383, acc: 100.0, f1: 100.0, r: 0.7009565885953325
06/01/2019 10:46:00 step: 4074, epoch: 123, batch: 14, loss: 0.0018180683255195618, acc: 100.0, f1: 100.0, r: 0.7002249301228213
06/01/2019 10:46:01 step: 4079, epoch: 123, batch: 19, loss: 0.0023329704999923706, acc: 100.0, f1: 100.0, r: 0.6477047863231555
06/01/2019 10:46:01 step: 4084, epoch: 123, batch: 24, loss: 0.0015424787998199463, acc: 100.0, f1: 100.0, r: 0.6618993374530845
06/01/2019 10:46:02 step: 4089, epoch: 123, batch: 29, loss: 0.0014877542853355408, acc: 100.0, f1: 100.0, r: 0.7050242125884597
06/01/2019 10:46:02 *** evaluating ***
06/01/2019 10:46:02 step: 124, epoch: 123, acc: 59.82905982905983, f1: 26.917911995745015, r: 0.2891629860363686
06/01/2019 10:46:02 *** epoch: 125 ***
06/01/2019 10:46:02 *** training ***
06/01/2019 10:46:03 step: 4097, epoch: 124, batch: 4, loss: 0.0023533403873443604, acc: 100.0, f1: 100.0, r: 0.7904484227979085
06/01/2019 10:46:03 step: 4102, epoch: 124, batch: 9, loss: 0.00429505854845047, acc: 100.0, f1: 100.0, r: 0.69171942127466
06/01/2019 10:46:04 step: 4107, epoch: 124, batch: 14, loss: 0.002013295888900757, acc: 100.0, f1: 100.0, r: 0.70392466566913
06/01/2019 10:46:04 step: 4112, epoch: 124, batch: 19, loss: 0.006641887128353119, acc: 100.0, f1: 100.0, r: 0.719241516260885
06/01/2019 10:46:05 step: 4117, epoch: 124, batch: 24, loss: 0.0033855587244033813, acc: 100.0, f1: 100.0, r: 0.7465012594083809
06/01/2019 10:46:05 step: 4122, epoch: 124, batch: 29, loss: 0.0022340789437294006, acc: 100.0, f1: 100.0, r: 0.6935479470033561
06/01/2019 10:46:06 *** evaluating ***
06/01/2019 10:46:06 step: 125, epoch: 124, acc: 61.53846153846154, f1: 28.19123674275995, r: 0.30232468200027546
06/01/2019 10:46:06 *** epoch: 126 ***
06/01/2019 10:46:06 *** training ***
06/01/2019 10:46:06 step: 4130, epoch: 125, batch: 4, loss: 0.0016283392906188965, acc: 100.0, f1: 100.0, r: 0.767407670480302
06/01/2019 10:46:07 step: 4135, epoch: 125, batch: 9, loss: 0.00464479997754097, acc: 100.0, f1: 100.0, r: 0.7715538740454292
06/01/2019 10:46:07 step: 4140, epoch: 125, batch: 14, loss: 0.014015443623065948, acc: 100.0, f1: 100.0, r: 0.8029769137974794
06/01/2019 10:46:08 step: 4145, epoch: 125, batch: 19, loss: 0.008502773940563202, acc: 100.0, f1: 100.0, r: 0.6443594839996297
06/01/2019 10:46:08 step: 4150, epoch: 125, batch: 24, loss: 0.0018887966871261597, acc: 100.0, f1: 100.0, r: 0.7879706850879917
06/01/2019 10:46:09 step: 4155, epoch: 125, batch: 29, loss: 0.00996396690607071, acc: 100.0, f1: 100.0, r: 0.7205133989824126
06/01/2019 10:46:09 *** evaluating ***
06/01/2019 10:46:09 step: 126, epoch: 125, acc: 60.256410256410255, f1: 26.509699044358864, r: 0.2980437186643259
06/01/2019 10:46:09 *** epoch: 127 ***
06/01/2019 10:46:09 *** training ***
06/01/2019 10:46:10 step: 4163, epoch: 126, batch: 4, loss: 0.002099141478538513, acc: 100.0, f1: 100.0, r: 0.7993609540263823
06/01/2019 10:46:10 step: 4168, epoch: 126, batch: 9, loss: 0.0026333406567573547, acc: 100.0, f1: 100.0, r: 0.6933776370074851
06/01/2019 10:46:11 step: 4173, epoch: 126, batch: 14, loss: 0.0031462833285331726, acc: 100.0, f1: 100.0, r: 0.8327269308929599
06/01/2019 10:46:11 step: 4178, epoch: 126, batch: 19, loss: 0.007043391466140747, acc: 100.0, f1: 100.0, r: 0.7543939273377159
06/01/2019 10:46:12 step: 4183, epoch: 126, batch: 24, loss: 0.0075465478003025055, acc: 100.0, f1: 100.0, r: 0.8095887750228201
06/01/2019 10:46:12 step: 4188, epoch: 126, batch: 29, loss: 0.0023113787174224854, acc: 100.0, f1: 100.0, r: 0.7348763912325282
06/01/2019 10:46:12 *** evaluating ***
06/01/2019 10:46:13 step: 127, epoch: 126, acc: 61.53846153846154, f1: 29.396835733455106, r: 0.2917867327628007
06/01/2019 10:46:13 *** epoch: 128 ***
06/01/2019 10:46:13 *** training ***
06/01/2019 10:46:13 step: 4196, epoch: 127, batch: 4, loss: 0.00338628888130188, acc: 100.0, f1: 100.0, r: 0.7712347254466256
06/01/2019 10:46:14 step: 4201, epoch: 127, batch: 9, loss: 0.00830533355474472, acc: 100.0, f1: 100.0, r: 0.6877722353332594
06/01/2019 10:46:14 step: 4206, epoch: 127, batch: 14, loss: 0.000598907470703125, acc: 100.0, f1: 100.0, r: 0.6757820618773501
06/01/2019 10:46:15 step: 4211, epoch: 127, batch: 19, loss: 0.005981672555208206, acc: 100.0, f1: 100.0, r: 0.7352252790081326
06/01/2019 10:46:15 step: 4216, epoch: 127, batch: 24, loss: 0.0037047266960144043, acc: 100.0, f1: 100.0, r: 0.7627808086310017
06/01/2019 10:46:16 step: 4221, epoch: 127, batch: 29, loss: 0.007191851735115051, acc: 100.0, f1: 100.0, r: 0.806028510792218
06/01/2019 10:46:16 *** evaluating ***
06/01/2019 10:46:16 step: 128, epoch: 127, acc: 61.965811965811966, f1: 26.458944855181993, r: 0.3038073793786275
06/01/2019 10:46:16 *** epoch: 129 ***
06/01/2019 10:46:16 *** training ***
06/01/2019 10:46:17 step: 4229, epoch: 128, batch: 4, loss: 0.007885687053203583, acc: 100.0, f1: 100.0, r: 0.7150302396159866
06/01/2019 10:46:17 step: 4234, epoch: 128, batch: 9, loss: 0.002346150577068329, acc: 100.0, f1: 100.0, r: 0.8096122295346466
06/01/2019 10:46:18 step: 4239, epoch: 128, batch: 14, loss: 0.0022587552666664124, acc: 100.0, f1: 100.0, r: 0.7510300775594374
06/01/2019 10:46:18 step: 4244, epoch: 128, batch: 19, loss: 0.003830544650554657, acc: 100.0, f1: 100.0, r: 0.8270474113102292
06/01/2019 10:46:19 step: 4249, epoch: 128, batch: 24, loss: 0.0030471161007881165, acc: 100.0, f1: 100.0, r: 0.6583337793573697
06/01/2019 10:46:19 step: 4254, epoch: 128, batch: 29, loss: 0.0013599768280982971, acc: 100.0, f1: 100.0, r: 0.711981555382704
06/01/2019 10:46:19 *** evaluating ***
06/01/2019 10:46:19 step: 129, epoch: 128, acc: 56.837606837606835, f1: 25.45090963155191, r: 0.2748352700745477
06/01/2019 10:46:19 *** epoch: 130 ***
06/01/2019 10:46:19 *** training ***
06/01/2019 10:46:20 step: 4262, epoch: 129, batch: 4, loss: 0.001492030918598175, acc: 100.0, f1: 100.0, r: 0.687461794386809
06/01/2019 10:46:20 step: 4267, epoch: 129, batch: 9, loss: 0.001710645854473114, acc: 100.0, f1: 100.0, r: 0.6758135003541385
06/01/2019 10:46:21 step: 4272, epoch: 129, batch: 14, loss: 0.0018392279744148254, acc: 100.0, f1: 100.0, r: 0.8357121131468802
06/01/2019 10:46:21 step: 4277, epoch: 129, batch: 19, loss: 0.001161225140094757, acc: 100.0, f1: 100.0, r: 0.6640717856417884
06/01/2019 10:46:22 step: 4282, epoch: 129, batch: 24, loss: 0.0027521662414073944, acc: 100.0, f1: 100.0, r: 0.6690319501460278
06/01/2019 10:46:22 step: 4287, epoch: 129, batch: 29, loss: 0.006384119391441345, acc: 100.0, f1: 100.0, r: 0.7650415727650508
06/01/2019 10:46:23 *** evaluating ***
06/01/2019 10:46:23 step: 130, epoch: 129, acc: 61.965811965811966, f1: 27.406431990531253, r: 0.3026537616417584
06/01/2019 10:46:23 *** epoch: 131 ***
06/01/2019 10:46:23 *** training ***
06/01/2019 10:46:23 step: 4295, epoch: 130, batch: 4, loss: 0.004716150462627411, acc: 100.0, f1: 100.0, r: 0.7239569626530297
06/01/2019 10:46:24 step: 4300, epoch: 130, batch: 9, loss: 0.001423180103302002, acc: 100.0, f1: 100.0, r: 0.6211080143750854
06/01/2019 10:46:24 step: 4305, epoch: 130, batch: 14, loss: 0.003462359309196472, acc: 100.0, f1: 100.0, r: 0.820875197679427
06/01/2019 10:46:25 step: 4310, epoch: 130, batch: 19, loss: 0.0018038377165794373, acc: 100.0, f1: 100.0, r: 0.7333373505964936
06/01/2019 10:46:25 step: 4315, epoch: 130, batch: 24, loss: 0.0022971555590629578, acc: 100.0, f1: 100.0, r: 0.6530411240699955
06/01/2019 10:46:26 step: 4320, epoch: 130, batch: 29, loss: 0.0045457929372787476, acc: 100.0, f1: 100.0, r: 0.6288253800407866
06/01/2019 10:46:26 *** evaluating ***
06/01/2019 10:46:26 step: 131, epoch: 130, acc: 61.111111111111114, f1: 28.710741001381397, r: 0.2914482951706722
06/01/2019 10:46:26 *** epoch: 132 ***
06/01/2019 10:46:26 *** training ***
06/01/2019 10:46:26 step: 4328, epoch: 131, batch: 4, loss: 0.0045787617564201355, acc: 100.0, f1: 100.0, r: 0.7100399502784056
06/01/2019 10:46:27 step: 4333, epoch: 131, batch: 9, loss: 0.004909880459308624, acc: 100.0, f1: 100.0, r: 0.6987802620369912
06/01/2019 10:46:27 step: 4338, epoch: 131, batch: 14, loss: 0.001517169177532196, acc: 100.0, f1: 100.0, r: 0.7759844028153343
06/01/2019 10:46:28 step: 4343, epoch: 131, batch: 19, loss: 0.003395959734916687, acc: 100.0, f1: 100.0, r: 0.8664636284002434
06/01/2019 10:46:28 step: 4348, epoch: 131, batch: 24, loss: 0.006309770047664642, acc: 100.0, f1: 100.0, r: 0.7667934993063206
06/01/2019 10:46:29 step: 4353, epoch: 131, batch: 29, loss: 0.002185240387916565, acc: 100.0, f1: 100.0, r: 0.7326804447201993
06/01/2019 10:46:29 *** evaluating ***
06/01/2019 10:46:29 step: 132, epoch: 131, acc: 60.68376068376068, f1: 27.627341920374704, r: 0.29071187092950995
06/01/2019 10:46:29 *** epoch: 133 ***
06/01/2019 10:46:29 *** training ***
06/01/2019 10:46:30 step: 4361, epoch: 132, batch: 4, loss: 0.0031991489231586456, acc: 100.0, f1: 100.0, r: 0.65169028516143
06/01/2019 10:46:30 step: 4366, epoch: 132, batch: 9, loss: 0.002772718667984009, acc: 100.0, f1: 100.0, r: 0.6807078501467222
06/01/2019 10:46:31 step: 4371, epoch: 132, batch: 14, loss: 0.002093799412250519, acc: 100.0, f1: 100.0, r: 0.7565976858062265
06/01/2019 10:46:31 step: 4376, epoch: 132, batch: 19, loss: 0.004149273037910461, acc: 100.0, f1: 100.0, r: 0.7626488590347835
06/01/2019 10:46:32 step: 4381, epoch: 132, batch: 24, loss: 0.0021719038486480713, acc: 100.0, f1: 100.0, r: 0.7794704641931673
06/01/2019 10:46:32 step: 4386, epoch: 132, batch: 29, loss: 0.006028998643159866, acc: 100.0, f1: 100.0, r: 0.8198079815503542
06/01/2019 10:46:33 *** evaluating ***
06/01/2019 10:46:33 step: 133, epoch: 132, acc: 62.82051282051282, f1: 28.303887985737376, r: 0.3003634629760416
06/01/2019 10:46:33 *** epoch: 134 ***
06/01/2019 10:46:33 *** training ***
06/01/2019 10:46:33 step: 4394, epoch: 133, batch: 4, loss: 0.006418749690055847, acc: 100.0, f1: 100.0, r: 0.7298237865586789
06/01/2019 10:46:34 step: 4399, epoch: 133, batch: 9, loss: 0.002066686749458313, acc: 100.0, f1: 100.0, r: 0.6767213009413992
06/01/2019 10:46:34 step: 4404, epoch: 133, batch: 14, loss: 0.0017617568373680115, acc: 100.0, f1: 100.0, r: 0.8084185018492054
06/01/2019 10:46:35 step: 4409, epoch: 133, batch: 19, loss: 0.008831392973661423, acc: 100.0, f1: 100.0, r: 0.7547187871873376
06/01/2019 10:46:35 step: 4414, epoch: 133, batch: 24, loss: 0.001399412751197815, acc: 100.0, f1: 100.0, r: 0.7877549074089577
06/01/2019 10:46:36 step: 4419, epoch: 133, batch: 29, loss: 0.001852087676525116, acc: 100.0, f1: 100.0, r: 0.7110281061722029
06/01/2019 10:46:36 *** evaluating ***
06/01/2019 10:46:36 step: 134, epoch: 133, acc: 60.68376068376068, f1: 26.930317047452778, r: 0.29165522406941763
06/01/2019 10:46:36 *** epoch: 135 ***
06/01/2019 10:46:36 *** training ***
06/01/2019 10:46:37 step: 4427, epoch: 134, batch: 4, loss: 0.036939285695552826, acc: 96.875, f1: 97.76839826839827, r: 0.8119098324044856
06/01/2019 10:46:37 step: 4432, epoch: 134, batch: 9, loss: 0.004487313330173492, acc: 100.0, f1: 100.0, r: 0.779822494723567
06/01/2019 10:46:38 step: 4437, epoch: 134, batch: 14, loss: 0.0037901028990745544, acc: 100.0, f1: 100.0, r: 0.7053252294316482
06/01/2019 10:46:38 step: 4442, epoch: 134, batch: 19, loss: 0.001908518373966217, acc: 100.0, f1: 100.0, r: 0.8387734880783003
06/01/2019 10:46:39 step: 4447, epoch: 134, batch: 24, loss: 0.004606842994689941, acc: 100.0, f1: 100.0, r: 0.7476452281886358
06/01/2019 10:46:39 step: 4452, epoch: 134, batch: 29, loss: 0.0016994401812553406, acc: 100.0, f1: 100.0, r: 0.8364843271009134
06/01/2019 10:46:39 *** evaluating ***
06/01/2019 10:46:40 step: 135, epoch: 134, acc: 61.965811965811966, f1: 28.044577205882348, r: 0.29540672502702636
06/01/2019 10:46:40 *** epoch: 136 ***
06/01/2019 10:46:40 *** training ***
06/01/2019 10:46:40 step: 4460, epoch: 135, batch: 4, loss: 0.001243099570274353, acc: 100.0, f1: 100.0, r: 0.8107580213656214
06/01/2019 10:46:41 step: 4465, epoch: 135, batch: 9, loss: 0.002751462161540985, acc: 100.0, f1: 100.0, r: 0.7590924268814301
06/01/2019 10:46:41 step: 4470, epoch: 135, batch: 14, loss: 0.0041297972202301025, acc: 100.0, f1: 100.0, r: 0.6519847011606605
06/01/2019 10:46:42 step: 4475, epoch: 135, batch: 19, loss: 0.005784347653388977, acc: 100.0, f1: 100.0, r: 0.7667109966127057
06/01/2019 10:46:42 step: 4480, epoch: 135, batch: 24, loss: 0.0017358586192131042, acc: 100.0, f1: 100.0, r: 0.66261895523669
06/01/2019 10:46:43 step: 4485, epoch: 135, batch: 29, loss: 0.001335345208644867, acc: 100.0, f1: 100.0, r: 0.6176323560939396
06/01/2019 10:46:43 *** evaluating ***
06/01/2019 10:46:43 step: 136, epoch: 135, acc: 60.256410256410255, f1: 29.349269723247367, r: 0.2921363199588617
06/01/2019 10:46:43 *** epoch: 137 ***
06/01/2019 10:46:43 *** training ***
06/01/2019 10:46:44 step: 4493, epoch: 136, batch: 4, loss: 0.0017477795481681824, acc: 100.0, f1: 100.0, r: 0.7014339243296959
06/01/2019 10:46:44 step: 4498, epoch: 136, batch: 9, loss: 0.002263695001602173, acc: 100.0, f1: 100.0, r: 0.6782646139178737
06/01/2019 10:46:45 step: 4503, epoch: 136, batch: 14, loss: 0.0006198137998580933, acc: 100.0, f1: 100.0, r: 0.6429493244913288
06/01/2019 10:46:45 step: 4508, epoch: 136, batch: 19, loss: 0.008329205214977264, acc: 100.0, f1: 100.0, r: 0.6738218508820446
06/01/2019 10:46:46 step: 4513, epoch: 136, batch: 24, loss: 0.002550557255744934, acc: 100.0, f1: 100.0, r: 0.8272821703018727
06/01/2019 10:46:46 step: 4518, epoch: 136, batch: 29, loss: 0.0027133673429489136, acc: 100.0, f1: 100.0, r: 0.7883076382823391
06/01/2019 10:46:46 *** evaluating ***
06/01/2019 10:46:46 step: 137, epoch: 136, acc: 60.68376068376068, f1: 27.044312917474677, r: 0.2902423148549216
06/01/2019 10:46:46 *** epoch: 138 ***
06/01/2019 10:46:46 *** training ***
06/01/2019 10:46:47 step: 4526, epoch: 137, batch: 4, loss: 0.0014695003628730774, acc: 100.0, f1: 100.0, r: 0.7066568223991647
06/01/2019 10:46:47 step: 4531, epoch: 137, batch: 9, loss: 0.0029101967811584473, acc: 100.0, f1: 100.0, r: 0.7042656326826314
06/01/2019 10:46:48 step: 4536, epoch: 137, batch: 14, loss: 0.003465712070465088, acc: 100.0, f1: 100.0, r: 0.7216161840458786
06/01/2019 10:46:49 step: 4541, epoch: 137, batch: 19, loss: 0.009126447141170502, acc: 100.0, f1: 100.0, r: 0.7115817884779656
06/01/2019 10:46:49 step: 4546, epoch: 137, batch: 24, loss: 0.003934130072593689, acc: 100.0, f1: 100.0, r: 0.7829846123009341
06/01/2019 10:46:50 step: 4551, epoch: 137, batch: 29, loss: 0.001405268907546997, acc: 100.0, f1: 100.0, r: 0.8485604250504707
06/01/2019 10:46:50 *** evaluating ***
06/01/2019 10:46:50 step: 138, epoch: 137, acc: 61.53846153846154, f1: 27.87147348519375, r: 0.29492138429954523
06/01/2019 10:46:50 *** epoch: 139 ***
06/01/2019 10:46:50 *** training ***
06/01/2019 10:46:50 step: 4559, epoch: 138, batch: 4, loss: 0.003824058920145035, acc: 100.0, f1: 100.0, r: 0.7143461643269531
06/01/2019 10:46:51 step: 4564, epoch: 138, batch: 9, loss: 0.0019835978746414185, acc: 100.0, f1: 100.0, r: 0.5764382649359339
06/01/2019 10:46:51 step: 4569, epoch: 138, batch: 14, loss: 0.014324478805065155, acc: 98.4375, f1: 98.80548829701372, r: 0.6916459215992092
06/01/2019 10:46:52 step: 4574, epoch: 138, batch: 19, loss: 0.036150041967630386, acc: 98.4375, f1: 95.59748427672956, r: 0.7481135689742118
06/01/2019 10:46:53 step: 4579, epoch: 138, batch: 24, loss: 0.0010619908571243286, acc: 100.0, f1: 100.0, r: 0.7621640641472266
06/01/2019 10:46:53 step: 4584, epoch: 138, batch: 29, loss: 0.005519106984138489, acc: 100.0, f1: 100.0, r: 0.7491861957028354
06/01/2019 10:46:53 *** evaluating ***
06/01/2019 10:46:53 step: 139, epoch: 138, acc: 60.68376068376068, f1: 28.617459481494123, r: 0.30510941889847015
06/01/2019 10:46:53 *** epoch: 140 ***
06/01/2019 10:46:53 *** training ***
06/01/2019 10:46:54 step: 4592, epoch: 139, batch: 4, loss: 0.003067418932914734, acc: 100.0, f1: 100.0, r: 0.7209223596034868
06/01/2019 10:46:54 step: 4597, epoch: 139, batch: 9, loss: 0.00747419148683548, acc: 100.0, f1: 100.0, r: 0.7040720464223187
06/01/2019 10:46:55 step: 4602, epoch: 139, batch: 14, loss: 0.005561076104640961, acc: 100.0, f1: 100.0, r: 0.8135134810134032
06/01/2019 10:46:55 step: 4607, epoch: 139, batch: 19, loss: 0.003923200070858002, acc: 100.0, f1: 100.0, r: 0.789192756983308
06/01/2019 10:46:56 step: 4612, epoch: 139, batch: 24, loss: 0.0007904991507530212, acc: 100.0, f1: 100.0, r: 0.7174619276082881
06/01/2019 10:46:56 step: 4617, epoch: 139, batch: 29, loss: 0.014735240489244461, acc: 100.0, f1: 100.0, r: 0.7631333715419919
06/01/2019 10:46:57 *** evaluating ***
06/01/2019 10:46:57 step: 140, epoch: 139, acc: 59.401709401709404, f1: 28.76844596902086, r: 0.2926963165866539
06/01/2019 10:46:57 *** epoch: 141 ***
06/01/2019 10:46:57 *** training ***
06/01/2019 10:46:57 step: 4625, epoch: 140, batch: 4, loss: 0.005091220140457153, acc: 100.0, f1: 100.0, r: 0.7201541668337216
06/01/2019 10:46:58 step: 4630, epoch: 140, batch: 9, loss: 0.0020127445459365845, acc: 100.0, f1: 100.0, r: 0.5997911956984788
06/01/2019 10:46:58 step: 4635, epoch: 140, batch: 14, loss: 0.002661868929862976, acc: 100.0, f1: 100.0, r: 0.754742108390877
06/01/2019 10:46:59 step: 4640, epoch: 140, batch: 19, loss: 0.0011349990963935852, acc: 100.0, f1: 100.0, r: 0.7338786075401663
06/01/2019 10:46:59 step: 4645, epoch: 140, batch: 24, loss: 0.003931231796741486, acc: 100.0, f1: 100.0, r: 0.706271840631236
06/01/2019 10:47:00 step: 4650, epoch: 140, batch: 29, loss: 0.014326635748147964, acc: 100.0, f1: 100.0, r: 0.701759633885299
06/01/2019 10:47:00 *** evaluating ***
06/01/2019 10:47:00 step: 141, epoch: 140, acc: 61.111111111111114, f1: 28.85109114249037, r: 0.30703289845620013
06/01/2019 10:47:00 *** epoch: 142 ***
06/01/2019 10:47:00 *** training ***
06/01/2019 10:47:01 step: 4658, epoch: 141, batch: 4, loss: 0.0011877268552780151, acc: 100.0, f1: 100.0, r: 0.6602442484655092
06/01/2019 10:47:01 step: 4663, epoch: 141, batch: 9, loss: 0.015932973474264145, acc: 98.4375, f1: 97.65523230568823, r: 0.7047636796307625
06/01/2019 10:47:02 step: 4668, epoch: 141, batch: 14, loss: 0.003452576696872711, acc: 100.0, f1: 100.0, r: 0.7776948643591363
06/01/2019 10:47:02 step: 4673, epoch: 141, batch: 19, loss: 0.002223365008831024, acc: 100.0, f1: 100.0, r: 0.7653177732785915
06/01/2019 10:47:03 step: 4678, epoch: 141, batch: 24, loss: 0.002788018435239792, acc: 100.0, f1: 100.0, r: 0.7978025525033561
06/01/2019 10:47:03 step: 4683, epoch: 141, batch: 29, loss: 0.001867532730102539, acc: 100.0, f1: 100.0, r: 0.7874657385685023
06/01/2019 10:47:03 *** evaluating ***
06/01/2019 10:47:04 step: 142, epoch: 141, acc: 60.256410256410255, f1: 25.906880733944952, r: 0.29865198133274645
06/01/2019 10:47:04 *** epoch: 143 ***
06/01/2019 10:47:04 *** training ***
06/01/2019 10:47:04 step: 4691, epoch: 142, batch: 4, loss: 0.0008587539196014404, acc: 100.0, f1: 100.0, r: 0.8336475176699099
06/01/2019 10:47:05 step: 4696, epoch: 142, batch: 9, loss: 0.0025294050574302673, acc: 100.0, f1: 100.0, r: 0.7869941527944183
06/01/2019 10:47:05 step: 4701, epoch: 142, batch: 14, loss: 0.0020333603024482727, acc: 100.0, f1: 100.0, r: 0.8568269338359474
06/01/2019 10:47:06 step: 4706, epoch: 142, batch: 19, loss: 0.0026233941316604614, acc: 100.0, f1: 100.0, r: 0.7987511676511673
06/01/2019 10:47:06 step: 4711, epoch: 142, batch: 24, loss: 0.00932004302740097, acc: 100.0, f1: 100.0, r: 0.6771304785059431
06/01/2019 10:47:07 step: 4716, epoch: 142, batch: 29, loss: 0.012568723410367966, acc: 100.0, f1: 100.0, r: 0.8242497873880268
06/01/2019 10:47:07 *** evaluating ***
06/01/2019 10:47:07 step: 143, epoch: 142, acc: 33.76068376068376, f1: 14.258143816967344, r: 0.24924415578078013
06/01/2019 10:47:07 *** epoch: 144 ***
06/01/2019 10:47:07 *** training ***
06/01/2019 10:47:08 step: 4724, epoch: 143, batch: 4, loss: 22.863086700439453, acc: 40.625, f1: 8.253968253968253, r: 0.0751304689436525
06/01/2019 10:47:08 step: 4729, epoch: 143, batch: 9, loss: 0.11744430661201477, acc: 96.875, f1: 97.78947368421052, r: 0.6361021670567898
06/01/2019 10:47:09 step: 4734, epoch: 143, batch: 14, loss: 0.09558447450399399, acc: 98.4375, f1: 98.29573934837093, r: 0.7259235180499842
06/01/2019 10:47:09 step: 4739, epoch: 143, batch: 19, loss: 0.13485291600227356, acc: 96.875, f1: 96.68196386946387, r: 0.6485716739047556
06/01/2019 10:47:10 step: 4744, epoch: 143, batch: 24, loss: 0.0716235339641571, acc: 100.0, f1: 100.0, r: 0.8436904409993962
06/01/2019 10:47:10 step: 4749, epoch: 143, batch: 29, loss: 0.10096187144517899, acc: 95.3125, f1: 95.37439975990397, r: 0.7937968195177422
06/01/2019 10:47:10 *** evaluating ***
06/01/2019 10:47:10 step: 144, epoch: 143, acc: 56.837606837606835, f1: 22.46540528948938, r: 0.29133415442405414
06/01/2019 10:47:10 *** epoch: 145 ***
06/01/2019 10:47:10 *** training ***
06/01/2019 10:47:11 step: 4757, epoch: 144, batch: 4, loss: 0.024254348129034042, acc: 100.0, f1: 100.0, r: 0.6633862228381844
06/01/2019 10:47:11 step: 4762, epoch: 144, batch: 9, loss: 0.028276674449443817, acc: 98.4375, f1: 97.46657283603096, r: 0.6776107556347692
06/01/2019 10:47:12 step: 4767, epoch: 144, batch: 14, loss: 0.016103509813547134, acc: 100.0, f1: 100.0, r: 0.7508485415761963
06/01/2019 10:47:12 step: 4772, epoch: 144, batch: 19, loss: 0.019672952592372894, acc: 100.0, f1: 100.0, r: 0.7247256299455571
06/01/2019 10:47:13 step: 4777, epoch: 144, batch: 24, loss: 0.019837673753499985, acc: 100.0, f1: 100.0, r: 0.7272183240562141
06/01/2019 10:47:13 step: 4782, epoch: 144, batch: 29, loss: 0.0330437496304512, acc: 100.0, f1: 100.0, r: 0.7147238619239474
06/01/2019 10:47:14 *** evaluating ***
06/01/2019 10:47:14 step: 145, epoch: 144, acc: 57.692307692307686, f1: 23.95010656436488, r: 0.293520259158379
06/01/2019 10:47:14 *** epoch: 146 ***
06/01/2019 10:47:14 *** training ***
06/01/2019 10:47:14 step: 4790, epoch: 145, batch: 4, loss: 0.022817522287368774, acc: 100.0, f1: 100.0, r: 0.7591578759914107
06/01/2019 10:47:15 step: 4795, epoch: 145, batch: 9, loss: 0.01119224727153778, acc: 100.0, f1: 100.0, r: 0.7281558986980351
06/01/2019 10:47:15 step: 4800, epoch: 145, batch: 14, loss: 0.014848917722702026, acc: 100.0, f1: 100.0, r: 0.7092279620482702
06/01/2019 10:47:16 step: 4805, epoch: 145, batch: 19, loss: 0.031147312372922897, acc: 100.0, f1: 100.0, r: 0.8189216126767569
06/01/2019 10:47:16 step: 4810, epoch: 145, batch: 24, loss: 0.012181341648101807, acc: 100.0, f1: 100.0, r: 0.6942124419630121
06/01/2019 10:47:17 step: 4815, epoch: 145, batch: 29, loss: 0.00589192658662796, acc: 100.0, f1: 100.0, r: 0.7959536172089856
06/01/2019 10:47:17 *** evaluating ***
06/01/2019 10:47:17 step: 146, epoch: 145, acc: 58.119658119658126, f1: 24.390359707750424, r: 0.2988658616299726
06/01/2019 10:47:17 *** epoch: 147 ***
06/01/2019 10:47:17 *** training ***
06/01/2019 10:47:18 step: 4823, epoch: 146, batch: 4, loss: 0.04672694206237793, acc: 96.875, f1: 97.86442006269593, r: 0.7812095936509159
06/01/2019 10:47:18 step: 4828, epoch: 146, batch: 9, loss: 0.018928322941064835, acc: 100.0, f1: 100.0, r: 0.7604788072979571
06/01/2019 10:47:19 step: 4833, epoch: 146, batch: 14, loss: 0.010161612182855606, acc: 100.0, f1: 100.0, r: 0.7839605919556534
06/01/2019 10:47:19 step: 4838, epoch: 146, batch: 19, loss: 0.008897189050912857, acc: 100.0, f1: 100.0, r: 0.6756041452981187
06/01/2019 10:47:20 step: 4843, epoch: 146, batch: 24, loss: 0.013157177716493607, acc: 100.0, f1: 100.0, r: 0.7699189160105315
06/01/2019 10:47:20 step: 4848, epoch: 146, batch: 29, loss: 0.007898226380348206, acc: 100.0, f1: 100.0, r: 0.7923100212771024
06/01/2019 10:47:20 *** evaluating ***
06/01/2019 10:47:21 step: 147, epoch: 146, acc: 56.41025641025641, f1: 24.016232900289573, r: 0.2895368456491268
06/01/2019 10:47:21 *** epoch: 148 ***
06/01/2019 10:47:21 *** training ***
06/01/2019 10:47:21 step: 4856, epoch: 147, batch: 4, loss: 0.004280395805835724, acc: 100.0, f1: 100.0, r: 0.7607116362493631
06/01/2019 10:47:22 step: 4861, epoch: 147, batch: 9, loss: 0.005000066012144089, acc: 100.0, f1: 100.0, r: 0.7014281642358401
06/01/2019 10:47:22 step: 4866, epoch: 147, batch: 14, loss: 0.01363738626241684, acc: 100.0, f1: 100.0, r: 0.7209666287859202
06/01/2019 10:47:23 step: 4871, epoch: 147, batch: 19, loss: 0.004241496324539185, acc: 100.0, f1: 100.0, r: 0.779902308814173
06/01/2019 10:47:23 step: 4876, epoch: 147, batch: 24, loss: 0.019954431802034378, acc: 100.0, f1: 100.0, r: 0.7402231172843645
06/01/2019 10:47:24 step: 4881, epoch: 147, batch: 29, loss: 0.027508392930030823, acc: 100.0, f1: 100.0, r: 0.749138011528288
06/01/2019 10:47:24 *** evaluating ***
06/01/2019 10:47:24 step: 148, epoch: 147, acc: 58.54700854700855, f1: 25.39853133977288, r: 0.2957019545787632
06/01/2019 10:47:24 *** epoch: 149 ***
06/01/2019 10:47:24 *** training ***
06/01/2019 10:47:25 step: 4889, epoch: 148, batch: 4, loss: 0.00862034410238266, acc: 100.0, f1: 100.0, r: 0.7193147108058727
06/01/2019 10:47:25 step: 4894, epoch: 148, batch: 9, loss: 0.013196364045143127, acc: 100.0, f1: 100.0, r: 0.6848908651003575
06/01/2019 10:47:25 step: 4899, epoch: 148, batch: 14, loss: 0.023518696427345276, acc: 98.4375, f1: 98.32967032967034, r: 0.704728424760643
06/01/2019 10:47:26 step: 4904, epoch: 148, batch: 19, loss: 0.013683009892702103, acc: 100.0, f1: 100.0, r: 0.8315021272097902
06/01/2019 10:47:26 step: 4909, epoch: 148, batch: 24, loss: 0.014690268784761429, acc: 100.0, f1: 100.0, r: 0.7886393063268673
06/01/2019 10:47:27 step: 4914, epoch: 148, batch: 29, loss: 0.013234589248895645, acc: 100.0, f1: 100.0, r: 0.6361123140445881
06/01/2019 10:47:27 *** evaluating ***
06/01/2019 10:47:27 step: 149, epoch: 148, acc: 59.401709401709404, f1: 24.66326223935663, r: 0.2915643726286268
06/01/2019 10:47:27 *** epoch: 150 ***
06/01/2019 10:47:27 *** training ***
06/01/2019 10:47:28 step: 4922, epoch: 149, batch: 4, loss: 0.0068371109664440155, acc: 100.0, f1: 100.0, r: 0.7814829099727301
06/01/2019 10:47:28 step: 4927, epoch: 149, batch: 9, loss: 0.013927619904279709, acc: 100.0, f1: 100.0, r: 0.7140257173886828
06/01/2019 10:47:29 step: 4932, epoch: 149, batch: 14, loss: 0.007155146449804306, acc: 100.0, f1: 100.0, r: 0.6553295617647067
06/01/2019 10:47:29 step: 4937, epoch: 149, batch: 19, loss: 0.0045456066727638245, acc: 100.0, f1: 100.0, r: 0.8283867269612905
06/01/2019 10:47:30 step: 4942, epoch: 149, batch: 24, loss: 0.020270049571990967, acc: 98.4375, f1: 99.42438513867086, r: 0.7768470922395977
06/01/2019 10:47:30 step: 4947, epoch: 149, batch: 29, loss: 0.015095937997102737, acc: 100.0, f1: 100.0, r: 0.8027575613175149
06/01/2019 10:47:31 *** evaluating ***
06/01/2019 10:47:31 step: 150, epoch: 149, acc: 58.97435897435898, f1: 24.14705497531117, r: 0.2928217450094633
06/01/2019 10:47:31 *** epoch: 151 ***
06/01/2019 10:47:31 *** training ***
06/01/2019 10:47:31 step: 4955, epoch: 150, batch: 4, loss: 0.007077552378177643, acc: 100.0, f1: 100.0, r: 0.7654346003746337
06/01/2019 10:47:32 step: 4960, epoch: 150, batch: 9, loss: 0.013402063399553299, acc: 100.0, f1: 100.0, r: 0.7685922706509593
06/01/2019 10:47:32 step: 4965, epoch: 150, batch: 14, loss: 0.007357921451330185, acc: 100.0, f1: 100.0, r: 0.8100158038080304
06/01/2019 10:47:33 step: 4970, epoch: 150, batch: 19, loss: 0.004592329263687134, acc: 100.0, f1: 100.0, r: 0.7014407579404593
06/01/2019 10:47:33 step: 4975, epoch: 150, batch: 24, loss: 0.005322739481925964, acc: 100.0, f1: 100.0, r: 0.710973535356068
06/01/2019 10:47:34 step: 4980, epoch: 150, batch: 29, loss: 0.0010197237133979797, acc: 100.0, f1: 100.0, r: 0.780853143382572
06/01/2019 10:47:34 *** evaluating ***
06/01/2019 10:47:34 step: 151, epoch: 150, acc: 58.119658119658126, f1: 24.35574952683568, r: 0.29116485586506613
06/01/2019 10:47:34 *** epoch: 152 ***
06/01/2019 10:47:34 *** training ***
06/01/2019 10:47:35 step: 4988, epoch: 151, batch: 4, loss: 0.002303965389728546, acc: 100.0, f1: 100.0, r: 0.6812904572027507
06/01/2019 10:47:35 step: 4993, epoch: 151, batch: 9, loss: 0.006509214639663696, acc: 100.0, f1: 100.0, r: 0.7245234859218301
06/01/2019 10:47:36 step: 4998, epoch: 151, batch: 14, loss: 0.007101569324731827, acc: 100.0, f1: 100.0, r: 0.6456620747636445
06/01/2019 10:47:36 step: 5003, epoch: 151, batch: 19, loss: 0.022244684398174286, acc: 100.0, f1: 100.0, r: 0.7981184243099188
06/01/2019 10:47:37 step: 5008, epoch: 151, batch: 24, loss: 0.004794113337993622, acc: 100.0, f1: 100.0, r: 0.6506648479088274
06/01/2019 10:47:37 step: 5013, epoch: 151, batch: 29, loss: 0.003602307289838791, acc: 100.0, f1: 100.0, r: 0.7680564298507984
06/01/2019 10:47:37 *** evaluating ***
06/01/2019 10:47:38 step: 152, epoch: 151, acc: 58.97435897435898, f1: 25.076523708498254, r: 0.2891177906074711
06/01/2019 10:47:38 *** epoch: 153 ***
06/01/2019 10:47:38 *** training ***
06/01/2019 10:47:38 step: 5021, epoch: 152, batch: 4, loss: 0.002807091921567917, acc: 100.0, f1: 100.0, r: 0.7982010176618235
06/01/2019 10:47:39 step: 5026, epoch: 152, batch: 9, loss: 0.007390405982732773, acc: 100.0, f1: 100.0, r: 0.7221717151083481
06/01/2019 10:47:39 step: 5031, epoch: 152, batch: 14, loss: 0.002390846610069275, acc: 100.0, f1: 100.0, r: 0.7320718654557643
06/01/2019 10:47:40 step: 5036, epoch: 152, batch: 19, loss: 0.0037419572472572327, acc: 100.0, f1: 100.0, r: 0.7006296701841228
06/01/2019 10:47:40 step: 5041, epoch: 152, batch: 24, loss: 0.005898267030715942, acc: 100.0, f1: 100.0, r: 0.7858644523568458
06/01/2019 10:47:41 step: 5046, epoch: 152, batch: 29, loss: 0.007972776889801025, acc: 100.0, f1: 100.0, r: 0.7801532671045669
06/01/2019 10:47:41 *** evaluating ***
06/01/2019 10:47:41 step: 153, epoch: 152, acc: 58.97435897435898, f1: 24.663765872603143, r: 0.28772326546115434
06/01/2019 10:47:41 *** epoch: 154 ***
06/01/2019 10:47:41 *** training ***
06/01/2019 10:47:42 step: 5054, epoch: 153, batch: 4, loss: 0.02359246090054512, acc: 98.4375, f1: 99.34343434343434, r: 0.7269390812142296
06/01/2019 10:47:42 step: 5059, epoch: 153, batch: 9, loss: 0.004228539764881134, acc: 100.0, f1: 100.0, r: 0.7965054208261352
06/01/2019 10:47:43 step: 5064, epoch: 153, batch: 14, loss: 0.006489276885986328, acc: 100.0, f1: 100.0, r: 0.8453169933064903
06/01/2019 10:47:43 step: 5069, epoch: 153, batch: 19, loss: 0.0043351538479328156, acc: 100.0, f1: 100.0, r: 0.8120491298336759
06/01/2019 10:47:44 step: 5074, epoch: 153, batch: 24, loss: 0.00304378941655159, acc: 100.0, f1: 100.0, r: 0.61787468828963
06/01/2019 10:47:44 step: 5079, epoch: 153, batch: 29, loss: 0.01612037792801857, acc: 100.0, f1: 100.0, r: 0.7166344377700878
06/01/2019 10:47:44 *** evaluating ***
06/01/2019 10:47:44 step: 154, epoch: 153, acc: 59.82905982905983, f1: 25.839922618030407, r: 0.29171123972194646
06/01/2019 10:47:44 *** epoch: 155 ***
06/01/2019 10:47:44 *** training ***
06/01/2019 10:47:45 step: 5087, epoch: 154, batch: 4, loss: 0.005600914359092712, acc: 100.0, f1: 100.0, r: 0.6908760984439464
06/01/2019 10:47:45 step: 5092, epoch: 154, batch: 9, loss: 0.0029460936784744263, acc: 100.0, f1: 100.0, r: 0.6858218506238547
06/01/2019 10:47:46 step: 5097, epoch: 154, batch: 14, loss: 0.012853316962718964, acc: 100.0, f1: 100.0, r: 0.7452958666491153
06/01/2019 10:47:46 step: 5102, epoch: 154, batch: 19, loss: 0.0025729164481163025, acc: 100.0, f1: 100.0, r: 0.6462420284566093
06/01/2019 10:47:47 step: 5107, epoch: 154, batch: 24, loss: 0.004174180328845978, acc: 100.0, f1: 100.0, r: 0.7945704392300156
06/01/2019 10:47:47 step: 5112, epoch: 154, batch: 29, loss: 0.002342529594898224, acc: 100.0, f1: 100.0, r: 0.705373294282101
06/01/2019 10:47:48 *** evaluating ***
06/01/2019 10:47:48 step: 155, epoch: 154, acc: 59.401709401709404, f1: 24.727552432384364, r: 0.2920691533552232
06/01/2019 10:47:48 *** epoch: 156 ***
06/01/2019 10:47:48 *** training ***
06/01/2019 10:47:48 step: 5120, epoch: 155, batch: 4, loss: 0.006094388663768768, acc: 100.0, f1: 100.0, r: 0.7712950699009847
06/01/2019 10:47:49 step: 5125, epoch: 155, batch: 9, loss: 0.004384450614452362, acc: 100.0, f1: 100.0, r: 0.626113913649197
06/01/2019 10:47:49 step: 5130, epoch: 155, batch: 14, loss: 0.0017448291182518005, acc: 100.0, f1: 100.0, r: 0.7467128774918326
06/01/2019 10:47:50 step: 5135, epoch: 155, batch: 19, loss: 0.003100551664829254, acc: 100.0, f1: 100.0, r: 0.8380289288682109
06/01/2019 10:47:50 step: 5140, epoch: 155, batch: 24, loss: 0.003528803586959839, acc: 100.0, f1: 100.0, r: 0.6556785838238476
06/01/2019 10:47:51 step: 5145, epoch: 155, batch: 29, loss: 0.0029976777732372284, acc: 100.0, f1: 100.0, r: 0.6666680046041917
06/01/2019 10:47:51 *** evaluating ***
06/01/2019 10:47:51 step: 156, epoch: 155, acc: 58.97435897435898, f1: 24.594738418089655, r: 0.28945771509204393
06/01/2019 10:47:51 *** epoch: 157 ***
06/01/2019 10:47:51 *** training ***
06/01/2019 10:47:52 step: 5153, epoch: 156, batch: 4, loss: 0.004580356180667877, acc: 100.0, f1: 100.0, r: 0.6921329731865028
06/01/2019 10:47:52 step: 5158, epoch: 156, batch: 9, loss: 0.003947928547859192, acc: 100.0, f1: 100.0, r: 0.8157104530881407
06/01/2019 10:47:53 step: 5163, epoch: 156, batch: 14, loss: 0.0020623356103897095, acc: 100.0, f1: 100.0, r: 0.8362077794910987
06/01/2019 10:47:53 step: 5168, epoch: 156, batch: 19, loss: 0.0029564425349235535, acc: 100.0, f1: 100.0, r: 0.7505887715704231
06/01/2019 10:47:54 step: 5173, epoch: 156, batch: 24, loss: 0.005479373037815094, acc: 100.0, f1: 100.0, r: 0.6359622111430536
06/01/2019 10:47:54 step: 5178, epoch: 156, batch: 29, loss: 0.003514692187309265, acc: 100.0, f1: 100.0, r: 0.808676537854606
06/01/2019 10:47:54 *** evaluating ***
06/01/2019 10:47:55 step: 157, epoch: 156, acc: 58.97435897435898, f1: 24.45948415991807, r: 0.288458527743106
06/01/2019 10:47:55 *** epoch: 158 ***
06/01/2019 10:47:55 *** training ***
06/01/2019 10:47:55 step: 5186, epoch: 157, batch: 4, loss: 0.0020346716046333313, acc: 100.0, f1: 100.0, r: 0.7029065330565801
06/01/2019 10:47:56 step: 5191, epoch: 157, batch: 9, loss: 0.0019577518105506897, acc: 100.0, f1: 100.0, r: 0.7887784017170351
06/01/2019 10:47:56 step: 5196, epoch: 157, batch: 14, loss: 0.0037490949034690857, acc: 100.0, f1: 100.0, r: 0.757185205666546
06/01/2019 10:47:57 step: 5201, epoch: 157, batch: 19, loss: 0.0026903972029685974, acc: 100.0, f1: 100.0, r: 0.7912017283501
06/01/2019 10:47:57 step: 5206, epoch: 157, batch: 24, loss: 0.0019973814487457275, acc: 100.0, f1: 100.0, r: 0.773667289870109
06/01/2019 10:47:57 step: 5211, epoch: 157, batch: 29, loss: 0.002441857010126114, acc: 100.0, f1: 100.0, r: 0.7656049033116489
06/01/2019 10:47:58 *** evaluating ***
06/01/2019 10:47:58 step: 158, epoch: 157, acc: 58.97435897435898, f1: 25.175957955210173, r: 0.29217043996391
06/01/2019 10:47:58 *** epoch: 159 ***
06/01/2019 10:47:58 *** training ***
06/01/2019 10:47:58 step: 5219, epoch: 158, batch: 4, loss: 0.0021771490573883057, acc: 100.0, f1: 100.0, r: 0.6458776017750418
06/01/2019 10:47:59 step: 5224, epoch: 158, batch: 9, loss: 0.003969330340623856, acc: 100.0, f1: 100.0, r: 0.7967298368488656
06/01/2019 10:47:59 step: 5229, epoch: 158, batch: 14, loss: 0.005821492522954941, acc: 100.0, f1: 100.0, r: 0.7490764919515941
06/01/2019 10:48:00 step: 5234, epoch: 158, batch: 19, loss: 0.0048727914690971375, acc: 100.0, f1: 100.0, r: 0.7188556637938668
06/01/2019 10:48:00 step: 5239, epoch: 158, batch: 24, loss: 0.008703615516424179, acc: 100.0, f1: 100.0, r: 0.716102210600873
06/01/2019 10:48:01 step: 5244, epoch: 158, batch: 29, loss: 0.009480878710746765, acc: 100.0, f1: 100.0, r: 0.7153475419275406
06/01/2019 10:48:01 *** evaluating ***
06/01/2019 10:48:01 step: 159, epoch: 158, acc: 59.401709401709404, f1: 24.635575494565643, r: 0.294871991177308
06/01/2019 10:48:01 *** epoch: 160 ***
06/01/2019 10:48:01 *** training ***
06/01/2019 10:48:02 step: 5252, epoch: 159, batch: 4, loss: 0.002589695155620575, acc: 100.0, f1: 100.0, r: 0.781962894792546
06/01/2019 10:48:02 step: 5257, epoch: 159, batch: 9, loss: 0.005525410175323486, acc: 100.0, f1: 100.0, r: 0.8439785430840556
06/01/2019 10:48:03 step: 5262, epoch: 159, batch: 14, loss: 0.0022613704204559326, acc: 100.0, f1: 100.0, r: 0.6786703214304189
06/01/2019 10:48:03 step: 5267, epoch: 159, batch: 19, loss: 0.005182676017284393, acc: 100.0, f1: 100.0, r: 0.8111089683308508
06/01/2019 10:48:04 step: 5272, epoch: 159, batch: 24, loss: 0.010799083858728409, acc: 100.0, f1: 100.0, r: 0.6861535188516694
06/01/2019 10:48:04 step: 5277, epoch: 159, batch: 29, loss: 0.006047256290912628, acc: 100.0, f1: 100.0, r: 0.771607915907687
06/01/2019 10:48:04 *** evaluating ***
06/01/2019 10:48:05 step: 160, epoch: 159, acc: 60.256410256410255, f1: 26.062513583367604, r: 0.2964660246420634
06/01/2019 10:48:05 *** epoch: 161 ***
06/01/2019 10:48:05 *** training ***
06/01/2019 10:48:05 step: 5285, epoch: 160, batch: 4, loss: 0.004344940185546875, acc: 100.0, f1: 100.0, r: 0.8216942954232529
06/01/2019 10:48:06 step: 5290, epoch: 160, batch: 9, loss: 0.003324843943119049, acc: 100.0, f1: 100.0, r: 0.7832305230273361
06/01/2019 10:48:06 step: 5295, epoch: 160, batch: 14, loss: 0.002155601978302002, acc: 100.0, f1: 100.0, r: 0.7046299864206315
06/01/2019 10:48:07 step: 5300, epoch: 160, batch: 19, loss: 0.0014363527297973633, acc: 100.0, f1: 100.0, r: 0.7457341590251336
06/01/2019 10:48:07 step: 5305, epoch: 160, batch: 24, loss: 0.008677415549755096, acc: 100.0, f1: 100.0, r: 0.6756627876042365
06/01/2019 10:48:08 step: 5310, epoch: 160, batch: 29, loss: 0.005879592150449753, acc: 100.0, f1: 100.0, r: 0.6879881584832872
06/01/2019 10:48:08 *** evaluating ***
06/01/2019 10:48:08 step: 161, epoch: 160, acc: 59.401709401709404, f1: 24.22922541465621, r: 0.2943983445458688
06/01/2019 10:48:08 *** epoch: 162 ***
06/01/2019 10:48:08 *** training ***
06/01/2019 10:48:09 step: 5318, epoch: 161, batch: 4, loss: 0.0041000619530677795, acc: 100.0, f1: 100.0, r: 0.7495105121636065
06/01/2019 10:48:09 step: 5323, epoch: 161, batch: 9, loss: 0.003389619290828705, acc: 100.0, f1: 100.0, r: 0.6662951776704205
06/01/2019 10:48:10 step: 5328, epoch: 161, batch: 14, loss: 0.0014755502343177795, acc: 100.0, f1: 100.0, r: 0.8340192498217752
06/01/2019 10:48:10 step: 5333, epoch: 161, batch: 19, loss: 0.0033317357301712036, acc: 100.0, f1: 100.0, r: 0.7350185913600009
06/01/2019 10:48:11 step: 5338, epoch: 161, batch: 24, loss: 0.004927471280097961, acc: 100.0, f1: 100.0, r: 0.7155059989421135
06/01/2019 10:48:11 step: 5343, epoch: 161, batch: 29, loss: 0.003196675330400467, acc: 100.0, f1: 100.0, r: 0.740798743171093
06/01/2019 10:48:11 *** evaluating ***
06/01/2019 10:48:12 step: 162, epoch: 161, acc: 59.401709401709404, f1: 25.202005683380136, r: 0.28949270027931984
06/01/2019 10:48:12 *** epoch: 163 ***
06/01/2019 10:48:12 *** training ***
06/01/2019 10:48:12 step: 5351, epoch: 162, batch: 4, loss: 0.0027091875672340393, acc: 100.0, f1: 100.0, r: 0.7583164308867257
06/01/2019 10:48:13 step: 5356, epoch: 162, batch: 9, loss: 0.0027631819248199463, acc: 100.0, f1: 100.0, r: 0.7819479267063238
06/01/2019 10:48:13 step: 5361, epoch: 162, batch: 14, loss: 0.0030881986021995544, acc: 100.0, f1: 100.0, r: 0.763436207754759
06/01/2019 10:48:14 step: 5366, epoch: 162, batch: 19, loss: 0.006829015910625458, acc: 100.0, f1: 100.0, r: 0.6705855417015267
06/01/2019 10:48:14 step: 5371, epoch: 162, batch: 24, loss: 0.008717384189367294, acc: 100.0, f1: 100.0, r: 0.7138379698787373
06/01/2019 10:48:15 step: 5376, epoch: 162, batch: 29, loss: 0.0013220757246017456, acc: 100.0, f1: 100.0, r: 0.6895274922062806
06/01/2019 10:48:15 *** evaluating ***
06/01/2019 10:48:15 step: 163, epoch: 162, acc: 59.401709401709404, f1: 25.004479436750348, r: 0.2909744517363843
06/01/2019 10:48:15 *** epoch: 164 ***
06/01/2019 10:48:15 *** training ***
06/01/2019 10:48:16 step: 5384, epoch: 163, batch: 4, loss: 0.0026181787252426147, acc: 100.0, f1: 100.0, r: 0.7936319007027387
06/01/2019 10:48:16 step: 5389, epoch: 163, batch: 9, loss: 0.0017585605382919312, acc: 100.0, f1: 100.0, r: 0.7272112890415268
06/01/2019 10:48:17 step: 5394, epoch: 163, batch: 14, loss: 0.0049863457679748535, acc: 100.0, f1: 100.0, r: 0.7853005060419349
06/01/2019 10:48:17 step: 5399, epoch: 163, batch: 19, loss: 0.0032071582973003387, acc: 100.0, f1: 100.0, r: 0.5779751447539294
06/01/2019 10:48:18 step: 5404, epoch: 163, batch: 24, loss: 0.0025117099285125732, acc: 100.0, f1: 100.0, r: 0.6850756657913664
06/01/2019 10:48:18 step: 5409, epoch: 163, batch: 29, loss: 0.0014565512537956238, acc: 100.0, f1: 100.0, r: 0.7064921801113814
06/01/2019 10:48:18 *** evaluating ***
06/01/2019 10:48:19 step: 164, epoch: 163, acc: 59.82905982905983, f1: 25.046096420219467, r: 0.292102092497769
06/01/2019 10:48:19 *** epoch: 165 ***
06/01/2019 10:48:19 *** training ***
06/01/2019 10:48:19 step: 5417, epoch: 164, batch: 4, loss: 0.0023770183324813843, acc: 100.0, f1: 100.0, r: 0.6982884691128047
06/01/2019 10:48:20 step: 5422, epoch: 164, batch: 9, loss: 0.003030344843864441, acc: 100.0, f1: 100.0, r: 0.7214491904575132
06/01/2019 10:48:20 step: 5427, epoch: 164, batch: 14, loss: 0.016766399145126343, acc: 98.4375, f1: 99.26546755815048, r: 0.659130941860226
06/01/2019 10:48:21 step: 5432, epoch: 164, batch: 19, loss: 0.006731271743774414, acc: 100.0, f1: 100.0, r: 0.6422438230068258
06/01/2019 10:48:21 step: 5437, epoch: 164, batch: 24, loss: 0.0019247159361839294, acc: 100.0, f1: 100.0, r: 0.7658354738664369
06/01/2019 10:48:22 step: 5442, epoch: 164, batch: 29, loss: 0.011296894401311874, acc: 100.0, f1: 100.0, r: 0.6080558022904213
06/01/2019 10:48:22 *** evaluating ***
06/01/2019 10:48:22 step: 165, epoch: 164, acc: 57.692307692307686, f1: 24.584003162254064, r: 0.29501778712823157
06/01/2019 10:48:22 *** epoch: 166 ***
06/01/2019 10:48:22 *** training ***
06/01/2019 10:48:22 step: 5450, epoch: 165, batch: 4, loss: 0.0036260411143302917, acc: 100.0, f1: 100.0, r: 0.7746032773312423
06/01/2019 10:48:23 step: 5455, epoch: 165, batch: 9, loss: 0.0035359859466552734, acc: 100.0, f1: 100.0, r: 0.7152490235056664
06/01/2019 10:48:23 step: 5460, epoch: 165, batch: 14, loss: 0.001448117196559906, acc: 100.0, f1: 100.0, r: 0.649875268448451
06/01/2019 10:48:24 step: 5465, epoch: 165, batch: 19, loss: 0.003879830241203308, acc: 100.0, f1: 100.0, r: 0.8100058515714395
06/01/2019 10:48:24 step: 5470, epoch: 165, batch: 24, loss: 0.0037330053746700287, acc: 100.0, f1: 100.0, r: 0.7122845421608164
06/01/2019 10:48:25 step: 5475, epoch: 165, batch: 29, loss: 0.0028140172362327576, acc: 100.0, f1: 100.0, r: 0.8131963243222776
06/01/2019 10:48:25 *** evaluating ***
06/01/2019 10:48:25 step: 166, epoch: 165, acc: 58.54700854700855, f1: 25.69724393050654, r: 0.29886445273770856
06/01/2019 10:48:25 *** epoch: 167 ***
06/01/2019 10:48:25 *** training ***
06/01/2019 10:48:26 step: 5483, epoch: 166, batch: 4, loss: 0.0027703121304512024, acc: 100.0, f1: 100.0, r: 0.7624291899542381
06/01/2019 10:48:26 step: 5488, epoch: 166, batch: 9, loss: 0.002958279103040695, acc: 100.0, f1: 100.0, r: 0.8003597897746855
06/01/2019 10:48:27 step: 5493, epoch: 166, batch: 14, loss: 0.0024089887738227844, acc: 100.0, f1: 100.0, r: 0.7426460182916079
06/01/2019 10:48:27 step: 5498, epoch: 166, batch: 19, loss: 0.0055886730551719666, acc: 100.0, f1: 100.0, r: 0.6786335937267342
06/01/2019 10:48:28 step: 5503, epoch: 166, batch: 24, loss: 0.0008009746670722961, acc: 100.0, f1: 100.0, r: 0.6599431664532927
06/01/2019 10:48:28 step: 5508, epoch: 166, batch: 29, loss: 0.0015967823565006256, acc: 100.0, f1: 100.0, r: 0.7367636585996726
06/01/2019 10:48:28 *** evaluating ***
06/01/2019 10:48:29 step: 167, epoch: 166, acc: 56.41025641025641, f1: 24.386885954563493, r: 0.29525018982587303
06/01/2019 10:48:29 *** epoch: 168 ***
06/01/2019 10:48:29 *** training ***
06/01/2019 10:48:29 step: 5516, epoch: 167, batch: 4, loss: 0.002576105296611786, acc: 100.0, f1: 100.0, r: 0.6371955349682082
06/01/2019 10:48:30 step: 5521, epoch: 167, batch: 9, loss: 0.005884747952222824, acc: 100.0, f1: 100.0, r: 0.6951568085948655
06/01/2019 10:48:30 step: 5526, epoch: 167, batch: 14, loss: 0.002180442214012146, acc: 100.0, f1: 100.0, r: 0.7305220553990621
06/01/2019 10:48:31 step: 5531, epoch: 167, batch: 19, loss: 0.0019368082284927368, acc: 100.0, f1: 100.0, r: 0.7066118354445794
06/01/2019 10:48:31 step: 5536, epoch: 167, batch: 24, loss: 0.0020727813243865967, acc: 100.0, f1: 100.0, r: 0.7736769392801675
06/01/2019 10:48:31 step: 5541, epoch: 167, batch: 29, loss: 0.002980530261993408, acc: 100.0, f1: 100.0, r: 0.7702879072778961
06/01/2019 10:48:32 *** evaluating ***
06/01/2019 10:48:32 step: 168, epoch: 167, acc: 58.119658119658126, f1: 25.110766568617425, r: 0.2966840361558472
06/01/2019 10:48:32 *** epoch: 169 ***
06/01/2019 10:48:32 *** training ***
06/01/2019 10:48:32 step: 5549, epoch: 168, batch: 4, loss: 0.006552733480930328, acc: 100.0, f1: 100.0, r: 0.8238407307837199
06/01/2019 10:48:33 step: 5554, epoch: 168, batch: 9, loss: 0.0010205134749412537, acc: 100.0, f1: 100.0, r: 0.8019749122445858
06/01/2019 10:48:33 step: 5559, epoch: 168, batch: 14, loss: 0.0026921331882476807, acc: 100.0, f1: 100.0, r: 0.6951333595183034
06/01/2019 10:48:34 step: 5564, epoch: 168, batch: 19, loss: 0.0037940889596939087, acc: 100.0, f1: 100.0, r: 0.74240827723984
06/01/2019 10:48:34 step: 5569, epoch: 168, batch: 24, loss: 0.002600826323032379, acc: 100.0, f1: 100.0, r: 0.7636957037585419
06/01/2019 10:48:35 step: 5574, epoch: 168, batch: 29, loss: 0.0026580095291137695, acc: 100.0, f1: 100.0, r: 0.7236253101403747
06/01/2019 10:48:35 *** evaluating ***
06/01/2019 10:48:35 step: 169, epoch: 168, acc: 59.401709401709404, f1: 25.7206990287384, r: 0.29863884545886343
06/01/2019 10:48:35 *** epoch: 170 ***
06/01/2019 10:48:35 *** training ***
06/01/2019 10:48:36 step: 5582, epoch: 169, batch: 4, loss: 0.002107396721839905, acc: 100.0, f1: 100.0, r: 0.8364788009251019
06/01/2019 10:48:36 step: 5587, epoch: 169, batch: 9, loss: 0.0008937716484069824, acc: 100.0, f1: 100.0, r: 0.7668205653309574
06/01/2019 10:48:37 step: 5592, epoch: 169, batch: 14, loss: 0.002207353711128235, acc: 100.0, f1: 100.0, r: 0.6380393654475204
06/01/2019 10:48:37 step: 5597, epoch: 169, batch: 19, loss: 0.000998467206954956, acc: 100.0, f1: 100.0, r: 0.726895403925146
06/01/2019 10:48:38 step: 5602, epoch: 169, batch: 24, loss: 0.007494259625673294, acc: 100.0, f1: 100.0, r: 0.6584472851314263
06/01/2019 10:48:38 step: 5607, epoch: 169, batch: 29, loss: 0.0031344667077064514, acc: 100.0, f1: 100.0, r: 0.803801527196079
06/01/2019 10:48:39 *** evaluating ***
06/01/2019 10:48:39 step: 170, epoch: 169, acc: 58.97435897435898, f1: 25.4485655018879, r: 0.29609420832975103
06/01/2019 10:48:39 *** epoch: 171 ***
06/01/2019 10:48:39 *** training ***
06/01/2019 10:48:39 step: 5615, epoch: 170, batch: 4, loss: 0.0020562559366226196, acc: 100.0, f1: 100.0, r: 0.8127505245103519
06/01/2019 10:48:40 step: 5620, epoch: 170, batch: 9, loss: 0.0038265101611614227, acc: 100.0, f1: 100.0, r: 0.7407493215942232
06/01/2019 10:48:40 step: 5625, epoch: 170, batch: 14, loss: 0.0012119710445404053, acc: 100.0, f1: 100.0, r: 0.7465160267728074
06/01/2019 10:48:41 step: 5630, epoch: 170, batch: 19, loss: 0.004566878080368042, acc: 100.0, f1: 100.0, r: 0.644033495070144
06/01/2019 10:48:41 step: 5635, epoch: 170, batch: 24, loss: 0.0037640929222106934, acc: 100.0, f1: 100.0, r: 0.7554562207387927
06/01/2019 10:48:42 step: 5640, epoch: 170, batch: 29, loss: 0.0016438066959381104, acc: 100.0, f1: 100.0, r: 0.694629864509038
06/01/2019 10:48:42 *** evaluating ***
06/01/2019 10:48:42 step: 171, epoch: 170, acc: 58.119658119658126, f1: 25.107096199328787, r: 0.29357402421276957
06/01/2019 10:48:42 *** epoch: 172 ***
06/01/2019 10:48:42 *** training ***
06/01/2019 10:48:43 step: 5648, epoch: 171, batch: 4, loss: 0.0032187551259994507, acc: 100.0, f1: 100.0, r: 0.7884767718659413
06/01/2019 10:48:43 step: 5653, epoch: 171, batch: 9, loss: 0.005509994924068451, acc: 100.0, f1: 100.0, r: 0.8279051990027008
06/01/2019 10:48:44 step: 5658, epoch: 171, batch: 14, loss: 0.006188474595546722, acc: 100.0, f1: 100.0, r: 0.6933723472069184
06/01/2019 10:48:44 step: 5663, epoch: 171, batch: 19, loss: 0.0013495758175849915, acc: 100.0, f1: 100.0, r: 0.6728831175829949
06/01/2019 10:48:45 step: 5668, epoch: 171, batch: 24, loss: 0.0020919740200042725, acc: 100.0, f1: 100.0, r: 0.7776809963396818
06/01/2019 10:48:45 step: 5673, epoch: 171, batch: 29, loss: 0.004358232021331787, acc: 100.0, f1: 100.0, r: 0.7653103636894978
06/01/2019 10:48:45 *** evaluating ***
06/01/2019 10:48:45 step: 172, epoch: 171, acc: 58.54700854700855, f1: 25.249355097898253, r: 0.2937142356488992
06/01/2019 10:48:45 *** epoch: 173 ***
06/01/2019 10:48:45 *** training ***
06/01/2019 10:48:46 step: 5681, epoch: 172, batch: 4, loss: 0.0009458065032958984, acc: 100.0, f1: 100.0, r: 0.8497988309239636
06/01/2019 10:48:46 step: 5686, epoch: 172, batch: 9, loss: 0.00214974582195282, acc: 100.0, f1: 100.0, r: 0.6958527534655833
06/01/2019 10:48:47 step: 5691, epoch: 172, batch: 14, loss: 0.035980917513370514, acc: 98.4375, f1: 98.82086167800453, r: 0.6924532030699707
06/01/2019 10:48:47 step: 5696, epoch: 172, batch: 19, loss: 0.002445913851261139, acc: 100.0, f1: 100.0, r: 0.7060071669134896
06/01/2019 10:48:48 step: 5701, epoch: 172, batch: 24, loss: 0.0017981156706809998, acc: 100.0, f1: 100.0, r: 0.7269744864229953
06/01/2019 10:48:48 step: 5706, epoch: 172, batch: 29, loss: 0.004481174051761627, acc: 100.0, f1: 100.0, r: 0.6705363551501219
06/01/2019 10:48:49 *** evaluating ***
06/01/2019 10:48:49 step: 173, epoch: 172, acc: 58.97435897435898, f1: 25.594401783461063, r: 0.29781066860597033
06/01/2019 10:48:49 *** epoch: 174 ***
06/01/2019 10:48:49 *** training ***
06/01/2019 10:48:49 step: 5714, epoch: 173, batch: 4, loss: 0.002417527139186859, acc: 100.0, f1: 100.0, r: 0.6981493506208347
06/01/2019 10:48:50 step: 5719, epoch: 173, batch: 9, loss: 0.002505652606487274, acc: 100.0, f1: 100.0, r: 0.8123906861806047
06/01/2019 10:48:50 step: 5724, epoch: 173, batch: 14, loss: 0.0010028854012489319, acc: 100.0, f1: 100.0, r: 0.7132697944432637
06/01/2019 10:48:51 step: 5729, epoch: 173, batch: 19, loss: 0.007270544767379761, acc: 100.0, f1: 100.0, r: 0.6374817566074575
06/01/2019 10:48:51 step: 5734, epoch: 173, batch: 24, loss: 0.0014552250504493713, acc: 100.0, f1: 100.0, r: 0.6662248625956044
06/01/2019 10:48:52 step: 5739, epoch: 173, batch: 29, loss: 0.002249777317047119, acc: 100.0, f1: 100.0, r: 0.7238821414791716
06/01/2019 10:48:52 *** evaluating ***
06/01/2019 10:48:52 step: 174, epoch: 173, acc: 58.54700854700855, f1: 24.561678665457578, r: 0.29230627109254903
06/01/2019 10:48:52 *** epoch: 175 ***
06/01/2019 10:48:52 *** training ***
06/01/2019 10:48:53 step: 5747, epoch: 174, batch: 4, loss: 0.002411589026451111, acc: 100.0, f1: 100.0, r: 0.6904386141751008
06/01/2019 10:48:53 step: 5752, epoch: 174, batch: 9, loss: 0.003093395382165909, acc: 100.0, f1: 100.0, r: 0.7746662310782466
06/01/2019 10:48:54 step: 5757, epoch: 174, batch: 14, loss: 0.0020801499485969543, acc: 100.0, f1: 100.0, r: 0.5677080338658949
06/01/2019 10:48:54 step: 5762, epoch: 174, batch: 19, loss: 0.005461573600769043, acc: 100.0, f1: 100.0, r: 0.7436981468716657
06/01/2019 10:48:55 step: 5767, epoch: 174, batch: 24, loss: 0.002228371798992157, acc: 100.0, f1: 100.0, r: 0.7239678669031201
06/01/2019 10:48:55 step: 5772, epoch: 174, batch: 29, loss: 0.0007776692509651184, acc: 100.0, f1: 100.0, r: 0.7570136201509619
06/01/2019 10:48:55 *** evaluating ***
06/01/2019 10:48:56 step: 175, epoch: 174, acc: 59.401709401709404, f1: 25.582588634601155, r: 0.2992907850390151
06/01/2019 10:48:56 *** epoch: 176 ***
06/01/2019 10:48:56 *** training ***
06/01/2019 10:48:56 step: 5780, epoch: 175, batch: 4, loss: 0.002377055585384369, acc: 100.0, f1: 100.0, r: 0.8039062491206488
06/01/2019 10:48:56 step: 5785, epoch: 175, batch: 9, loss: 0.005278050899505615, acc: 100.0, f1: 100.0, r: 0.7458734392774976
06/01/2019 10:48:57 step: 5790, epoch: 175, batch: 14, loss: 0.0021707937121391296, acc: 100.0, f1: 100.0, r: 0.7119514636188584
06/01/2019 10:48:57 step: 5795, epoch: 175, batch: 19, loss: 0.0014724507927894592, acc: 100.0, f1: 100.0, r: 0.7282611696468375
06/01/2019 10:48:58 step: 5800, epoch: 175, batch: 24, loss: 0.009178787469863892, acc: 100.0, f1: 100.0, r: 0.6886074428655127
06/01/2019 10:48:58 step: 5805, epoch: 175, batch: 29, loss: 0.0026229768991470337, acc: 100.0, f1: 100.0, r: 0.7155904598632062
06/01/2019 10:48:59 *** evaluating ***
06/01/2019 10:48:59 step: 176, epoch: 175, acc: 58.54700854700855, f1: 24.70692857555442, r: 0.2989409613446182
06/01/2019 10:48:59 *** epoch: 177 ***
06/01/2019 10:48:59 *** training ***
06/01/2019 10:48:59 step: 5813, epoch: 176, batch: 4, loss: 0.0038882382214069366, acc: 100.0, f1: 100.0, r: 0.685408726329367
06/01/2019 10:49:00 step: 5818, epoch: 176, batch: 9, loss: 0.0020141974091529846, acc: 100.0, f1: 100.0, r: 0.8466855884002339
06/01/2019 10:49:00 step: 5823, epoch: 176, batch: 14, loss: 0.0029559247195720673, acc: 100.0, f1: 100.0, r: 0.6448693147889819
06/01/2019 10:49:01 step: 5828, epoch: 176, batch: 19, loss: 0.0033375397324562073, acc: 100.0, f1: 100.0, r: 0.7043347948680122
06/01/2019 10:49:01 step: 5833, epoch: 176, batch: 24, loss: 0.0009494796395301819, acc: 100.0, f1: 100.0, r: 0.5695444986990708
06/01/2019 10:49:02 step: 5838, epoch: 176, batch: 29, loss: 0.0009804293513298035, acc: 100.0, f1: 100.0, r: 0.773745525413764
06/01/2019 10:49:02 *** evaluating ***
06/01/2019 10:49:02 step: 177, epoch: 176, acc: 58.97435897435898, f1: 25.17963025370816, r: 0.29989948630774066
06/01/2019 10:49:02 *** epoch: 178 ***
06/01/2019 10:49:02 *** training ***
06/01/2019 10:49:03 step: 5846, epoch: 177, batch: 4, loss: 0.002659633755683899, acc: 100.0, f1: 100.0, r: 0.7642108334429587
06/01/2019 10:49:03 step: 5851, epoch: 177, batch: 9, loss: 0.0009370520710945129, acc: 100.0, f1: 100.0, r: 0.6782232909281102
06/01/2019 10:49:04 step: 5856, epoch: 177, batch: 14, loss: 0.0016905292868614197, acc: 100.0, f1: 100.0, r: 0.7769610622571421
06/01/2019 10:49:04 step: 5861, epoch: 177, batch: 19, loss: 0.00454285740852356, acc: 100.0, f1: 100.0, r: 0.7042008953747976
06/01/2019 10:49:05 step: 5866, epoch: 177, batch: 24, loss: 0.009160857647657394, acc: 100.0, f1: 100.0, r: 0.7129769430937027
06/01/2019 10:49:05 step: 5871, epoch: 177, batch: 29, loss: 0.0035725757479667664, acc: 100.0, f1: 100.0, r: 0.6841758273850235
06/01/2019 10:49:05 *** evaluating ***
06/01/2019 10:49:05 step: 178, epoch: 177, acc: 59.401709401709404, f1: 25.3984091383749, r: 0.29698670692176526
06/01/2019 10:49:05 *** epoch: 179 ***
06/01/2019 10:49:05 *** training ***
06/01/2019 10:49:06 step: 5879, epoch: 178, batch: 4, loss: 0.0027851760387420654, acc: 100.0, f1: 100.0, r: 0.6996427314888507
06/01/2019 10:49:06 step: 5884, epoch: 178, batch: 9, loss: 0.0021290257573127747, acc: 100.0, f1: 100.0, r: 0.7251875317855038
06/01/2019 10:49:07 step: 5889, epoch: 178, batch: 14, loss: 0.0020600929856300354, acc: 100.0, f1: 100.0, r: 0.6351521034218374
06/01/2019 10:49:07 step: 5894, epoch: 178, batch: 19, loss: 0.006647326052188873, acc: 100.0, f1: 100.0, r: 0.8356302526648174
06/01/2019 10:49:08 step: 5899, epoch: 178, batch: 24, loss: 0.0029786601662635803, acc: 100.0, f1: 100.0, r: 0.6628797449766484
06/01/2019 10:49:08 step: 5904, epoch: 178, batch: 29, loss: 0.0054639242589473724, acc: 100.0, f1: 100.0, r: 0.8045599313773908
06/01/2019 10:49:09 *** evaluating ***
06/01/2019 10:49:09 step: 179, epoch: 178, acc: 58.54700854700855, f1: 25.10499730234813, r: 0.2954296201244744
06/01/2019 10:49:09 *** epoch: 180 ***
06/01/2019 10:49:09 *** training ***
06/01/2019 10:49:09 step: 5912, epoch: 179, batch: 4, loss: 0.0013323575258255005, acc: 100.0, f1: 100.0, r: 0.6652143736400981
06/01/2019 10:49:10 step: 5917, epoch: 179, batch: 9, loss: 0.0033661648631095886, acc: 100.0, f1: 100.0, r: 0.7674203919336235
06/01/2019 10:49:10 step: 5922, epoch: 179, batch: 14, loss: 0.005080677568912506, acc: 100.0, f1: 100.0, r: 0.5781429031773456
06/01/2019 10:49:11 step: 5927, epoch: 179, batch: 19, loss: 0.003036871552467346, acc: 100.0, f1: 100.0, r: 0.6475588033233942
06/01/2019 10:49:11 step: 5932, epoch: 179, batch: 24, loss: 0.001879766583442688, acc: 100.0, f1: 100.0, r: 0.6913198721873324
06/01/2019 10:49:12 step: 5937, epoch: 179, batch: 29, loss: 0.006241589784622192, acc: 100.0, f1: 100.0, r: 0.7044775321322487
06/01/2019 10:49:12 *** evaluating ***
06/01/2019 10:49:12 step: 180, epoch: 179, acc: 58.54700854700855, f1: 25.030277475659794, r: 0.2972606992905615
06/01/2019 10:49:12 *** epoch: 181 ***
06/01/2019 10:49:12 *** training ***
06/01/2019 10:49:12 step: 5945, epoch: 180, batch: 4, loss: 0.005318000912666321, acc: 100.0, f1: 100.0, r: 0.6852237115824895
06/01/2019 10:49:13 step: 5950, epoch: 180, batch: 9, loss: 0.0038365423679351807, acc: 100.0, f1: 100.0, r: 0.7824778678826916
06/01/2019 10:49:13 step: 5955, epoch: 180, batch: 14, loss: 0.009640984237194061, acc: 100.0, f1: 100.0, r: 0.7854265039709314
06/01/2019 10:49:14 step: 5960, epoch: 180, batch: 19, loss: 0.002060241997241974, acc: 100.0, f1: 100.0, r: 0.6536422808534686
06/01/2019 10:49:14 step: 5965, epoch: 180, batch: 24, loss: 0.0036348477005958557, acc: 100.0, f1: 100.0, r: 0.6767145741539758
06/01/2019 10:49:15 step: 5970, epoch: 180, batch: 29, loss: 0.0013822242617607117, acc: 100.0, f1: 100.0, r: 0.7083712819477378
06/01/2019 10:49:15 *** evaluating ***
06/01/2019 10:49:15 step: 181, epoch: 180, acc: 58.54700854700855, f1: 25.030277475659794, r: 0.2981446991636584
06/01/2019 10:49:15 *** epoch: 182 ***
06/01/2019 10:49:15 *** training ***
06/01/2019 10:49:16 step: 5978, epoch: 181, batch: 4, loss: 0.001260913908481598, acc: 100.0, f1: 100.0, r: 0.7631714860466401
06/01/2019 10:49:16 step: 5983, epoch: 181, batch: 9, loss: 0.0022068172693252563, acc: 100.0, f1: 100.0, r: 0.7113720695239539
06/01/2019 10:49:17 step: 5988, epoch: 181, batch: 14, loss: 0.0012001171708106995, acc: 100.0, f1: 100.0, r: 0.7191624529139492
06/01/2019 10:49:17 step: 5993, epoch: 181, batch: 19, loss: 0.0018548518419265747, acc: 100.0, f1: 100.0, r: 0.7136778042664985
06/01/2019 10:49:18 step: 5998, epoch: 181, batch: 24, loss: 0.0012631416320800781, acc: 100.0, f1: 100.0, r: 0.580401792698987
06/01/2019 10:49:18 step: 6003, epoch: 181, batch: 29, loss: 0.0012508183717727661, acc: 100.0, f1: 100.0, r: 0.6757919747446473
06/01/2019 10:49:18 *** evaluating ***
06/01/2019 10:49:19 step: 182, epoch: 181, acc: 58.97435897435898, f1: 25.879912240023195, r: 0.29869369024682435
06/01/2019 10:49:19 *** epoch: 183 ***
06/01/2019 10:49:19 *** training ***
06/01/2019 10:49:19 step: 6011, epoch: 182, batch: 4, loss: 0.0018638968467712402, acc: 100.0, f1: 100.0, r: 0.8073414911426241
06/01/2019 10:49:20 step: 6016, epoch: 182, batch: 9, loss: 0.0006531849503517151, acc: 100.0, f1: 100.0, r: 0.7454661797093907
06/01/2019 10:49:20 step: 6021, epoch: 182, batch: 14, loss: 0.0029852762818336487, acc: 100.0, f1: 100.0, r: 0.7593774973851309
06/01/2019 10:49:21 step: 6026, epoch: 182, batch: 19, loss: 0.003864474594593048, acc: 100.0, f1: 100.0, r: 0.6890241238388324
06/01/2019 10:49:21 step: 6031, epoch: 182, batch: 24, loss: 0.0029665380716323853, acc: 100.0, f1: 100.0, r: 0.7146815845284156
06/01/2019 10:49:22 step: 6036, epoch: 182, batch: 29, loss: 0.0013775825500488281, acc: 100.0, f1: 100.0, r: 0.6944112489795855
06/01/2019 10:49:22 *** evaluating ***
06/01/2019 10:49:22 step: 183, epoch: 182, acc: 59.401709401709404, f1: 26.7208737317433, r: 0.3022878805782077
06/01/2019 10:49:22 *** epoch: 184 ***
06/01/2019 10:49:22 *** training ***
06/01/2019 10:49:22 step: 6044, epoch: 183, batch: 4, loss: 0.0015739798545837402, acc: 100.0, f1: 100.0, r: 0.7163693147954537
06/01/2019 10:49:23 step: 6049, epoch: 183, batch: 9, loss: 0.0010345354676246643, acc: 100.0, f1: 100.0, r: 0.7457775921268148
06/01/2019 10:49:23 step: 6054, epoch: 183, batch: 14, loss: 0.008756957948207855, acc: 100.0, f1: 100.0, r: 0.7843247298017306
06/01/2019 10:49:24 step: 6059, epoch: 183, batch: 19, loss: 0.0022039860486984253, acc: 100.0, f1: 100.0, r: 0.8058703511295078
06/01/2019 10:49:24 step: 6064, epoch: 183, batch: 24, loss: 0.0018680021166801453, acc: 100.0, f1: 100.0, r: 0.6590606597423678
06/01/2019 10:49:25 step: 6069, epoch: 183, batch: 29, loss: 0.004218794405460358, acc: 100.0, f1: 100.0, r: 0.7886874268867623
06/01/2019 10:49:25 *** evaluating ***
06/01/2019 10:49:25 step: 184, epoch: 183, acc: 58.97435897435898, f1: 25.24150165940898, r: 0.30380073323016815
06/01/2019 10:49:25 *** epoch: 185 ***
06/01/2019 10:49:25 *** training ***
06/01/2019 10:49:26 step: 6077, epoch: 184, batch: 4, loss: 0.0029925107955932617, acc: 100.0, f1: 100.0, r: 0.6105667761460264
06/01/2019 10:49:26 step: 6082, epoch: 184, batch: 9, loss: 0.004279248416423798, acc: 100.0, f1: 100.0, r: 0.8215034612822275
06/01/2019 10:49:27 step: 6087, epoch: 184, batch: 14, loss: 0.003430284559726715, acc: 100.0, f1: 100.0, r: 0.8335221262972585
06/01/2019 10:49:27 step: 6092, epoch: 184, batch: 19, loss: 0.003303375095129013, acc: 100.0, f1: 100.0, r: 0.7808955317314309
06/01/2019 10:49:28 step: 6097, epoch: 184, batch: 24, loss: 0.0027103647589683533, acc: 100.0, f1: 100.0, r: 0.7425204156645998
06/01/2019 10:49:28 step: 6102, epoch: 184, batch: 29, loss: 0.0019623637199401855, acc: 100.0, f1: 100.0, r: 0.7332843986776019
06/01/2019 10:49:28 *** evaluating ***
06/01/2019 10:49:29 step: 185, epoch: 184, acc: 59.401709401709404, f1: 25.453733808237487, r: 0.30658856886491453
06/01/2019 10:49:29 *** epoch: 186 ***
06/01/2019 10:49:29 *** training ***
06/01/2019 10:49:29 step: 6110, epoch: 185, batch: 4, loss: 0.001829683780670166, acc: 100.0, f1: 100.0, r: 0.7768411561968938
06/01/2019 10:49:29 step: 6115, epoch: 185, batch: 9, loss: 0.0023372843861579895, acc: 100.0, f1: 100.0, r: 0.7079155997043365
06/01/2019 10:49:30 step: 6120, epoch: 185, batch: 14, loss: 0.001445382833480835, acc: 100.0, f1: 100.0, r: 0.7143259824455408
06/01/2019 10:49:30 step: 6125, epoch: 185, batch: 19, loss: 0.001884356141090393, acc: 100.0, f1: 100.0, r: 0.6740037631760678
06/01/2019 10:49:31 step: 6130, epoch: 185, batch: 24, loss: 0.0023468881845474243, acc: 100.0, f1: 100.0, r: 0.6916516040607436
06/01/2019 10:49:31 step: 6135, epoch: 185, batch: 29, loss: 0.0016101598739624023, acc: 100.0, f1: 100.0, r: 0.7119833718010911
06/01/2019 10:49:32 *** evaluating ***
06/01/2019 10:49:32 step: 186, epoch: 185, acc: 59.401709401709404, f1: 26.033535993051665, r: 0.3038804681037452
06/01/2019 10:49:32 *** epoch: 187 ***
06/01/2019 10:49:32 *** training ***
06/01/2019 10:49:32 step: 6143, epoch: 186, batch: 4, loss: 0.006093055009841919, acc: 100.0, f1: 100.0, r: 0.7504509243594052
06/01/2019 10:49:33 step: 6148, epoch: 186, batch: 9, loss: 0.0018529966473579407, acc: 100.0, f1: 100.0, r: 0.8304932833549941
06/01/2019 10:49:33 step: 6153, epoch: 186, batch: 14, loss: 0.00328662246465683, acc: 100.0, f1: 100.0, r: 0.6902348714631439
06/01/2019 10:49:34 step: 6158, epoch: 186, batch: 19, loss: 0.0033245347440242767, acc: 100.0, f1: 100.0, r: 0.7221028806581463
06/01/2019 10:49:34 step: 6163, epoch: 186, batch: 24, loss: 0.0011307448148727417, acc: 100.0, f1: 100.0, r: 0.7043624103285017
06/01/2019 10:49:35 step: 6168, epoch: 186, batch: 29, loss: 0.004282429814338684, acc: 100.0, f1: 100.0, r: 0.7523503537511391
06/01/2019 10:49:35 *** evaluating ***
06/01/2019 10:49:35 step: 187, epoch: 186, acc: 59.401709401709404, f1: 25.438664004706652, r: 0.30528434362940027
06/01/2019 10:49:35 *** epoch: 188 ***
06/01/2019 10:49:35 *** training ***
06/01/2019 10:49:36 step: 6176, epoch: 187, batch: 4, loss: 0.00445624440908432, acc: 100.0, f1: 100.0, r: 0.6911728957363077
06/01/2019 10:49:36 step: 6181, epoch: 187, batch: 9, loss: 0.0016077980399131775, acc: 100.0, f1: 100.0, r: 0.8194214754291074
06/01/2019 10:49:37 step: 6186, epoch: 187, batch: 14, loss: 0.0007368028163909912, acc: 100.0, f1: 100.0, r: 0.7491112207261937
06/01/2019 10:49:37 step: 6191, epoch: 187, batch: 19, loss: 0.002407371997833252, acc: 100.0, f1: 100.0, r: 0.7673822359859785
06/01/2019 10:49:37 step: 6196, epoch: 187, batch: 24, loss: 0.0011131390929222107, acc: 100.0, f1: 100.0, r: 0.7967617270266258
06/01/2019 10:49:38 step: 6201, epoch: 187, batch: 29, loss: 0.007121481001377106, acc: 100.0, f1: 100.0, r: 0.7206315433842488
06/01/2019 10:49:38 *** evaluating ***
06/01/2019 10:49:38 step: 188, epoch: 187, acc: 58.97435897435898, f1: 25.586289781035642, r: 0.3040096836816134
06/01/2019 10:49:38 *** epoch: 189 ***
06/01/2019 10:49:38 *** training ***
06/01/2019 10:49:39 step: 6209, epoch: 188, batch: 4, loss: 0.0021440833806991577, acc: 100.0, f1: 100.0, r: 0.6424801280802744
06/01/2019 10:49:39 step: 6214, epoch: 188, batch: 9, loss: 0.001453980803489685, acc: 100.0, f1: 100.0, r: 0.8098413471990528
06/01/2019 10:49:40 step: 6219, epoch: 188, batch: 14, loss: 0.003528796136379242, acc: 100.0, f1: 100.0, r: 0.6835618663618465
06/01/2019 10:49:40 step: 6224, epoch: 188, batch: 19, loss: 0.0006412714719772339, acc: 100.0, f1: 100.0, r: 0.7163015627879424
06/01/2019 10:49:41 step: 6229, epoch: 188, batch: 24, loss: 0.002561718225479126, acc: 100.0, f1: 100.0, r: 0.7598251430486915
06/01/2019 10:49:41 step: 6234, epoch: 188, batch: 29, loss: 0.0035574883222579956, acc: 100.0, f1: 100.0, r: 0.739529100376712
06/01/2019 10:49:42 *** evaluating ***
06/01/2019 10:49:42 step: 189, epoch: 188, acc: 58.97435897435898, f1: 24.926518111759073, r: 0.3040793402909463
06/01/2019 10:49:42 *** epoch: 190 ***
06/01/2019 10:49:42 *** training ***
06/01/2019 10:49:42 step: 6242, epoch: 189, batch: 4, loss: 0.001760914921760559, acc: 100.0, f1: 100.0, r: 0.6497100386611981
06/01/2019 10:49:43 step: 6247, epoch: 189, batch: 9, loss: 0.0005224347114562988, acc: 100.0, f1: 100.0, r: 0.704518462576744
06/01/2019 10:49:43 step: 6252, epoch: 189, batch: 14, loss: 0.001699291169643402, acc: 100.0, f1: 100.0, r: 0.8042500092226362
06/01/2019 10:49:44 step: 6257, epoch: 189, batch: 19, loss: 0.0015274211764335632, acc: 100.0, f1: 100.0, r: 0.6909576563165186
06/01/2019 10:49:44 step: 6262, epoch: 189, batch: 24, loss: 0.001866322010755539, acc: 100.0, f1: 100.0, r: 0.6245099153443149
06/01/2019 10:49:45 step: 6267, epoch: 189, batch: 29, loss: 0.0017147809267044067, acc: 100.0, f1: 100.0, r: 0.6733626674316916
06/01/2019 10:49:45 *** evaluating ***
06/01/2019 10:49:45 step: 190, epoch: 189, acc: 59.401709401709404, f1: 24.664577533823056, r: 0.30794769806725797
06/01/2019 10:49:45 *** epoch: 191 ***
06/01/2019 10:49:45 *** training ***
06/01/2019 10:49:46 step: 6275, epoch: 190, batch: 4, loss: 0.007652979344129562, acc: 100.0, f1: 100.0, r: 0.7985303918226464
06/01/2019 10:49:46 step: 6280, epoch: 190, batch: 9, loss: 0.009874477982521057, acc: 100.0, f1: 100.0, r: 0.7215871839874892
06/01/2019 10:49:47 step: 6285, epoch: 190, batch: 14, loss: 0.0007911175489425659, acc: 100.0, f1: 100.0, r: 0.6177304145413586
06/01/2019 10:49:47 step: 6290, epoch: 190, batch: 19, loss: 0.001351773738861084, acc: 100.0, f1: 100.0, r: 0.6346558799829747
06/01/2019 10:49:48 step: 6295, epoch: 190, batch: 24, loss: 0.0015638470649719238, acc: 100.0, f1: 100.0, r: 0.6504578691866291
06/01/2019 10:49:48 step: 6300, epoch: 190, batch: 29, loss: 0.0011919960379600525, acc: 100.0, f1: 100.0, r: 0.7780274909567745
06/01/2019 10:49:48 *** evaluating ***
06/01/2019 10:49:48 step: 191, epoch: 190, acc: 58.119658119658126, f1: 24.275861716899453, r: 0.3072985144151495
06/01/2019 10:49:48 *** epoch: 192 ***
06/01/2019 10:49:48 *** training ***
06/01/2019 10:49:49 step: 6308, epoch: 191, batch: 4, loss: 0.0018016546964645386, acc: 100.0, f1: 100.0, r: 0.7754789683515756
06/01/2019 10:49:49 step: 6313, epoch: 191, batch: 9, loss: 0.005064524710178375, acc: 100.0, f1: 100.0, r: 0.7257005564604304
06/01/2019 10:49:50 step: 6318, epoch: 191, batch: 14, loss: 0.002694167196750641, acc: 100.0, f1: 100.0, r: 0.743337848856369
06/01/2019 10:49:50 step: 6323, epoch: 191, batch: 19, loss: 0.0026587210595607758, acc: 100.0, f1: 100.0, r: 0.7447581904723068
06/01/2019 10:49:51 step: 6328, epoch: 191, batch: 24, loss: 0.0023899823427200317, acc: 100.0, f1: 100.0, r: 0.7756667385797433
06/01/2019 10:49:52 step: 6333, epoch: 191, batch: 29, loss: 0.0029255077242851257, acc: 100.0, f1: 100.0, r: 0.740413742065965
06/01/2019 10:49:52 *** evaluating ***
06/01/2019 10:49:52 step: 192, epoch: 191, acc: 59.401709401709404, f1: 24.615193996421972, r: 0.3107413718872357
06/01/2019 10:49:52 *** epoch: 193 ***
06/01/2019 10:49:52 *** training ***
06/01/2019 10:49:52 step: 6341, epoch: 192, batch: 4, loss: 0.004405394196510315, acc: 100.0, f1: 100.0, r: 0.832790015461458
06/01/2019 10:49:53 step: 6346, epoch: 192, batch: 9, loss: 0.0014960616827011108, acc: 100.0, f1: 100.0, r: 0.696601317933526
06/01/2019 10:49:53 step: 6351, epoch: 192, batch: 14, loss: 0.0018353015184402466, acc: 100.0, f1: 100.0, r: 0.6990045525159697
06/01/2019 10:49:54 step: 6356, epoch: 192, batch: 19, loss: 0.0035129711031913757, acc: 100.0, f1: 100.0, r: 0.7639587695983625
06/01/2019 10:49:54 step: 6361, epoch: 192, batch: 24, loss: 0.002422276884317398, acc: 100.0, f1: 100.0, r: 0.8504528394887838
06/01/2019 10:49:55 step: 6366, epoch: 192, batch: 29, loss: 0.0010011643171310425, acc: 100.0, f1: 100.0, r: 0.7739491783921136
06/01/2019 10:49:55 *** evaluating ***
06/01/2019 10:49:55 step: 193, epoch: 192, acc: 58.54700854700855, f1: 25.597665787237844, r: 0.3083491774969477
06/01/2019 10:49:55 *** epoch: 194 ***
06/01/2019 10:49:55 *** training ***
06/01/2019 10:49:56 step: 6374, epoch: 193, batch: 4, loss: 0.003865934908390045, acc: 100.0, f1: 100.0, r: 0.8550121418286026
06/01/2019 10:49:56 step: 6379, epoch: 193, batch: 9, loss: 0.0017029643058776855, acc: 100.0, f1: 100.0, r: 0.770953484779962
06/01/2019 10:49:57 step: 6384, epoch: 193, batch: 14, loss: 0.004418537020683289, acc: 100.0, f1: 100.0, r: 0.6901036490883753
06/01/2019 10:49:57 step: 6389, epoch: 193, batch: 19, loss: 0.0009944215416908264, acc: 100.0, f1: 100.0, r: 0.7086807419467834
06/01/2019 10:49:58 step: 6394, epoch: 193, batch: 24, loss: 0.0033209100365638733, acc: 100.0, f1: 100.0, r: 0.7989449282509379
06/01/2019 10:49:58 step: 6399, epoch: 193, batch: 29, loss: 0.0026473701000213623, acc: 100.0, f1: 100.0, r: 0.7934997145895016
06/01/2019 10:49:58 *** evaluating ***
06/01/2019 10:49:59 step: 194, epoch: 193, acc: 59.82905982905983, f1: 26.916458102388873, r: 0.3066615092831947
06/01/2019 10:49:59 *** epoch: 195 ***
06/01/2019 10:49:59 *** training ***
06/01/2019 10:49:59 step: 6407, epoch: 194, batch: 4, loss: 0.0010608360171318054, acc: 100.0, f1: 100.0, r: 0.7581171044154739
06/01/2019 10:49:59 step: 6412, epoch: 194, batch: 9, loss: 0.002329207956790924, acc: 100.0, f1: 100.0, r: 0.8113810897223749
06/01/2019 10:50:00 step: 6417, epoch: 194, batch: 14, loss: 0.0013569220900535583, acc: 100.0, f1: 100.0, r: 0.7223571878042861
06/01/2019 10:50:00 step: 6422, epoch: 194, batch: 19, loss: 0.0009681656956672668, acc: 100.0, f1: 100.0, r: 0.6246412400582431
06/01/2019 10:50:01 step: 6427, epoch: 194, batch: 24, loss: 0.0019103512167930603, acc: 100.0, f1: 100.0, r: 0.790555931239373
06/01/2019 10:50:01 step: 6432, epoch: 194, batch: 29, loss: 0.00066395103931427, acc: 100.0, f1: 100.0, r: 0.6966404823728287
06/01/2019 10:50:02 *** evaluating ***
06/01/2019 10:50:02 step: 195, epoch: 194, acc: 58.54700854700855, f1: 25.084392440903002, r: 0.3045001939912199
06/01/2019 10:50:02 *** epoch: 196 ***
06/01/2019 10:50:02 *** training ***
06/01/2019 10:50:02 step: 6440, epoch: 195, batch: 4, loss: 0.0028850138187408447, acc: 100.0, f1: 100.0, r: 0.6596331882450249
06/01/2019 10:50:03 step: 6445, epoch: 195, batch: 9, loss: 0.0027840062975883484, acc: 100.0, f1: 100.0, r: 0.7292215586679952
06/01/2019 10:50:03 step: 6450, epoch: 195, batch: 14, loss: 0.0028928816318511963, acc: 100.0, f1: 100.0, r: 0.5873601007717363
06/01/2019 10:50:04 step: 6455, epoch: 195, batch: 19, loss: 0.002481698989868164, acc: 100.0, f1: 100.0, r: 0.6923353677783622
06/01/2019 10:50:04 step: 6460, epoch: 195, batch: 24, loss: 0.0027531199157238007, acc: 100.0, f1: 100.0, r: 0.6219826439864512
06/01/2019 10:50:05 step: 6465, epoch: 195, batch: 29, loss: 0.0021573826670646667, acc: 100.0, f1: 100.0, r: 0.7183440235322396
06/01/2019 10:50:05 *** evaluating ***
06/01/2019 10:50:05 step: 196, epoch: 195, acc: 57.692307692307686, f1: 24.10608540496882, r: 0.306024475541957
06/01/2019 10:50:05 *** epoch: 197 ***
06/01/2019 10:50:05 *** training ***
06/01/2019 10:50:06 step: 6473, epoch: 196, batch: 4, loss: 0.0019453614950180054, acc: 100.0, f1: 100.0, r: 0.6759398371366692
06/01/2019 10:50:06 step: 6478, epoch: 196, batch: 9, loss: 0.0012268796563148499, acc: 100.0, f1: 100.0, r: 0.6996790000512165
06/01/2019 10:50:07 step: 6483, epoch: 196, batch: 14, loss: 0.003987826406955719, acc: 100.0, f1: 100.0, r: 0.7610917025122266
06/01/2019 10:50:07 step: 6488, epoch: 196, batch: 19, loss: 0.002855844795703888, acc: 100.0, f1: 100.0, r: 0.8075480217670996
06/01/2019 10:50:08 step: 6493, epoch: 196, batch: 24, loss: 0.0029818788170814514, acc: 100.0, f1: 100.0, r: 0.8430420012365933
06/01/2019 10:50:08 step: 6498, epoch: 196, batch: 29, loss: 0.0010451152920722961, acc: 100.0, f1: 100.0, r: 0.6671716520838188
06/01/2019 10:50:08 *** evaluating ***
06/01/2019 10:50:09 step: 197, epoch: 196, acc: 58.97435897435898, f1: 23.72558480909848, r: 0.3074897826587024
06/01/2019 10:50:09 *** epoch: 198 ***
06/01/2019 10:50:09 *** training ***
06/01/2019 10:50:09 step: 6506, epoch: 197, batch: 4, loss: 0.0005553737282752991, acc: 100.0, f1: 100.0, r: 0.7595186006478749
06/01/2019 10:50:09 step: 6511, epoch: 197, batch: 9, loss: 0.0013565421104431152, acc: 100.0, f1: 100.0, r: 0.6797806008227858
06/01/2019 10:50:10 step: 6516, epoch: 197, batch: 14, loss: 0.0015557855367660522, acc: 100.0, f1: 100.0, r: 0.8199857378578614
06/01/2019 10:50:10 step: 6521, epoch: 197, batch: 19, loss: 0.003050379455089569, acc: 100.0, f1: 100.0, r: 0.6977041279192451
06/01/2019 10:50:11 step: 6526, epoch: 197, batch: 24, loss: 0.0037517137825489044, acc: 100.0, f1: 100.0, r: 0.7880686396097873
06/01/2019 10:50:11 step: 6531, epoch: 197, batch: 29, loss: 0.0006359443068504333, acc: 100.0, f1: 100.0, r: 0.8298718086605925
06/01/2019 10:50:12 *** evaluating ***
06/01/2019 10:50:12 step: 198, epoch: 197, acc: 58.97435897435898, f1: 24.356889204545453, r: 0.30807327332765727
06/01/2019 10:50:12 *** epoch: 199 ***
06/01/2019 10:50:12 *** training ***
06/01/2019 10:50:12 step: 6539, epoch: 198, batch: 4, loss: 0.001346580684185028, acc: 100.0, f1: 100.0, r: 0.681600282865891
06/01/2019 10:50:13 step: 6544, epoch: 198, batch: 9, loss: 0.0033338814973831177, acc: 100.0, f1: 100.0, r: 0.7835744842675878
06/01/2019 10:50:13 step: 6549, epoch: 198, batch: 14, loss: 0.0009980127215385437, acc: 100.0, f1: 100.0, r: 0.7035316079624975
06/01/2019 10:50:14 step: 6554, epoch: 198, batch: 19, loss: 0.00309554859995842, acc: 100.0, f1: 100.0, r: 0.6981869249845885
06/01/2019 10:50:14 step: 6559, epoch: 198, batch: 24, loss: 0.001372411847114563, acc: 100.0, f1: 100.0, r: 0.803550398950504
06/01/2019 10:50:15 step: 6564, epoch: 198, batch: 29, loss: 0.005434006452560425, acc: 100.0, f1: 100.0, r: 0.7222742785852991
06/01/2019 10:50:15 *** evaluating ***
06/01/2019 10:50:15 step: 199, epoch: 198, acc: 58.97435897435898, f1: 26.430825609559815, r: 0.3025941067672362
06/01/2019 10:50:15 *** epoch: 200 ***
06/01/2019 10:50:15 *** training ***
06/01/2019 10:50:16 step: 6572, epoch: 199, batch: 4, loss: 0.0013352930545806885, acc: 100.0, f1: 100.0, r: 0.6969623093772638
06/01/2019 10:50:16 step: 6577, epoch: 199, batch: 9, loss: 0.0016080588102340698, acc: 100.0, f1: 100.0, r: 0.7743412713397521
06/01/2019 10:50:17 step: 6582, epoch: 199, batch: 14, loss: 0.0047228336334228516, acc: 100.0, f1: 100.0, r: 0.6786862184812502
06/01/2019 10:50:17 step: 6587, epoch: 199, batch: 19, loss: 0.0010160654783248901, acc: 100.0, f1: 100.0, r: 0.7626652840543818
06/01/2019 10:50:18 step: 6592, epoch: 199, batch: 24, loss: 0.0015311017632484436, acc: 100.0, f1: 100.0, r: 0.8103744369147677
06/01/2019 10:50:18 step: 6597, epoch: 199, batch: 29, loss: 0.0016717761754989624, acc: 100.0, f1: 100.0, r: 0.751675282205142
06/01/2019 10:50:18 *** evaluating ***
06/01/2019 10:50:18 step: 200, epoch: 199, acc: 58.54700854700855, f1: 24.885028250812603, r: 0.29647273416205744
06/01/2019 10:50:18 *** epoch: 201 ***
06/01/2019 10:50:18 *** training ***
06/01/2019 10:50:19 step: 6605, epoch: 200, batch: 4, loss: 0.0008293688297271729, acc: 100.0, f1: 100.0, r: 0.7237629064708285
06/01/2019 10:50:19 step: 6610, epoch: 200, batch: 9, loss: 0.003827713429927826, acc: 100.0, f1: 100.0, r: 0.667504560753526
06/01/2019 10:50:20 step: 6615, epoch: 200, batch: 14, loss: 0.0019991472363471985, acc: 100.0, f1: 100.0, r: 0.7719598392861953
06/01/2019 10:50:20 step: 6620, epoch: 200, batch: 19, loss: 0.003681100904941559, acc: 100.0, f1: 100.0, r: 0.6481231151224517
06/01/2019 10:50:21 step: 6625, epoch: 200, batch: 24, loss: 0.00083160400390625, acc: 100.0, f1: 100.0, r: 0.7307028452692411
06/01/2019 10:50:21 step: 6630, epoch: 200, batch: 29, loss: 0.0011745765805244446, acc: 100.0, f1: 100.0, r: 0.7879619184767767
06/01/2019 10:50:22 *** evaluating ***
06/01/2019 10:50:22 step: 201, epoch: 200, acc: 58.97435897435898, f1: 26.71703296703296, r: 0.3012812437677835
06/01/2019 10:50:22 *** epoch: 202 ***
06/01/2019 10:50:22 *** training ***
06/01/2019 10:50:22 step: 6638, epoch: 201, batch: 4, loss: 0.001516588032245636, acc: 100.0, f1: 100.0, r: 0.6323358289099144
06/01/2019 10:50:23 step: 6643, epoch: 201, batch: 9, loss: 0.0006376728415489197, acc: 100.0, f1: 100.0, r: 0.7711629899272275
06/01/2019 10:50:23 step: 6648, epoch: 201, batch: 14, loss: 0.0025644227862358093, acc: 100.0, f1: 100.0, r: 0.6859195609692985
06/01/2019 10:50:24 step: 6653, epoch: 201, batch: 19, loss: 0.0016859807074069977, acc: 100.0, f1: 100.0, r: 0.7853052793858332
06/01/2019 10:50:24 step: 6658, epoch: 201, batch: 24, loss: 0.0019409358501434326, acc: 100.0, f1: 100.0, r: 0.7094632749049785
06/01/2019 10:50:25 step: 6663, epoch: 201, batch: 29, loss: 0.007994778454303741, acc: 100.0, f1: 100.0, r: 0.730417556048576
06/01/2019 10:50:25 *** evaluating ***
06/01/2019 10:50:25 step: 202, epoch: 201, acc: 60.256410256410255, f1: 26.0031436427157, r: 0.31192247506565757
06/01/2019 10:50:25 *** epoch: 203 ***
06/01/2019 10:50:25 *** training ***
06/01/2019 10:50:26 step: 6671, epoch: 202, batch: 4, loss: 0.0017586871981620789, acc: 100.0, f1: 100.0, r: 0.7091370347202247
06/01/2019 10:50:26 step: 6676, epoch: 202, batch: 9, loss: 0.0030284300446510315, acc: 100.0, f1: 100.0, r: 0.6910112224635571
06/01/2019 10:50:27 step: 6681, epoch: 202, batch: 14, loss: 0.0016513168811798096, acc: 100.0, f1: 100.0, r: 0.6775104113576081
06/01/2019 10:50:27 step: 6686, epoch: 202, batch: 19, loss: 0.0019697770476341248, acc: 100.0, f1: 100.0, r: 0.6346750337042748
06/01/2019 10:50:28 step: 6691, epoch: 202, batch: 24, loss: 0.005318976938724518, acc: 100.0, f1: 100.0, r: 0.810192489871892
06/01/2019 10:50:28 step: 6696, epoch: 202, batch: 29, loss: 0.006959039717912674, acc: 100.0, f1: 100.0, r: 0.7540561651453648
06/01/2019 10:50:28 *** evaluating ***
06/01/2019 10:50:28 step: 203, epoch: 202, acc: 59.82905982905983, f1: 26.815926031666315, r: 0.3065927441664085
06/01/2019 10:50:28 *** epoch: 204 ***
06/01/2019 10:50:28 *** training ***
06/01/2019 10:50:29 step: 6704, epoch: 203, batch: 4, loss: 0.009861722588539124, acc: 100.0, f1: 100.0, r: 0.7795126985145457
06/01/2019 10:50:29 step: 6709, epoch: 203, batch: 9, loss: 0.0009275078773498535, acc: 100.0, f1: 100.0, r: 0.7654560388900068
06/01/2019 10:50:30 step: 6714, epoch: 203, batch: 14, loss: 0.005126871168613434, acc: 100.0, f1: 100.0, r: 0.805073993291753
06/01/2019 10:50:30 step: 6719, epoch: 203, batch: 19, loss: 0.0012644752860069275, acc: 100.0, f1: 100.0, r: 0.6764478307816227
06/01/2019 10:50:31 step: 6724, epoch: 203, batch: 24, loss: 0.004141762852668762, acc: 100.0, f1: 100.0, r: 0.7096496827112397
06/01/2019 10:50:31 step: 6729, epoch: 203, batch: 29, loss: 0.0015474334359169006, acc: 100.0, f1: 100.0, r: 0.7663239595936508
06/01/2019 10:50:32 *** evaluating ***
06/01/2019 10:50:32 step: 204, epoch: 203, acc: 58.97435897435898, f1: 25.919405058343774, r: 0.3101048339955113
06/01/2019 10:50:32 *** epoch: 205 ***
06/01/2019 10:50:32 *** training ***
06/01/2019 10:50:32 step: 6737, epoch: 204, batch: 4, loss: 0.0027536675333976746, acc: 100.0, f1: 100.0, r: 0.8169031979064756
06/01/2019 10:50:33 step: 6742, epoch: 204, batch: 9, loss: 0.0027400925755500793, acc: 100.0, f1: 100.0, r: 0.7248720323004195
06/01/2019 10:50:33 step: 6747, epoch: 204, batch: 14, loss: 0.0027249380946159363, acc: 100.0, f1: 100.0, r: 0.6953476714921993
06/01/2019 10:50:34 step: 6752, epoch: 204, batch: 19, loss: 0.0009554177522659302, acc: 100.0, f1: 100.0, r: 0.7148203692281445
06/01/2019 10:50:34 step: 6757, epoch: 204, batch: 24, loss: 0.0024224668741226196, acc: 100.0, f1: 100.0, r: 0.7384686354735408
06/01/2019 10:50:35 step: 6762, epoch: 204, batch: 29, loss: 0.0010770633816719055, acc: 100.0, f1: 100.0, r: 0.7243294899564106
06/01/2019 10:50:35 *** evaluating ***
06/01/2019 10:50:35 step: 205, epoch: 204, acc: 58.97435897435898, f1: 26.27781961508177, r: 0.30768424500511243
06/01/2019 10:50:35 *** epoch: 206 ***
06/01/2019 10:50:35 *** training ***
06/01/2019 10:50:36 step: 6770, epoch: 205, batch: 4, loss: 0.0013409331440925598, acc: 100.0, f1: 100.0, r: 0.68949329865702
06/01/2019 10:50:36 step: 6775, epoch: 205, batch: 9, loss: 0.0009616091847419739, acc: 100.0, f1: 100.0, r: 0.8037635141251851
06/01/2019 10:50:37 step: 6780, epoch: 205, batch: 14, loss: 0.0014452412724494934, acc: 100.0, f1: 100.0, r: 0.6549236479076705
06/01/2019 10:50:37 step: 6785, epoch: 205, batch: 19, loss: 0.001491248607635498, acc: 100.0, f1: 100.0, r: 0.6757388444816627
06/01/2019 10:50:38 step: 6790, epoch: 205, batch: 24, loss: 0.0047239214181900024, acc: 100.0, f1: 100.0, r: 0.8278446046743186
06/01/2019 10:50:38 step: 6795, epoch: 205, batch: 29, loss: 0.0023771747946739197, acc: 100.0, f1: 100.0, r: 0.7643110538701139
06/01/2019 10:50:38 *** evaluating ***
06/01/2019 10:50:38 step: 206, epoch: 205, acc: 59.401709401709404, f1: 26.686176550813272, r: 0.30401502012229076
06/01/2019 10:50:38 *** epoch: 207 ***
06/01/2019 10:50:38 *** training ***
06/01/2019 10:50:39 step: 6803, epoch: 206, batch: 4, loss: 0.0024677738547325134, acc: 100.0, f1: 100.0, r: 0.8429617258254907
06/01/2019 10:50:39 step: 6808, epoch: 206, batch: 9, loss: 0.0015504807233810425, acc: 100.0, f1: 100.0, r: 0.7960566769855566
06/01/2019 10:50:40 step: 6813, epoch: 206, batch: 14, loss: 0.006173655390739441, acc: 100.0, f1: 100.0, r: 0.6895629722855583
06/01/2019 10:50:40 step: 6818, epoch: 206, batch: 19, loss: 0.0018936172127723694, acc: 100.0, f1: 100.0, r: 0.8002115037322224
06/01/2019 10:50:41 step: 6823, epoch: 206, batch: 24, loss: 0.001514822244644165, acc: 100.0, f1: 100.0, r: 0.8325516110563262
06/01/2019 10:50:41 step: 6828, epoch: 206, batch: 29, loss: 0.001405090093612671, acc: 100.0, f1: 100.0, r: 0.7760113939804472
06/01/2019 10:50:42 *** evaluating ***
06/01/2019 10:50:42 step: 207, epoch: 206, acc: 60.256410256410255, f1: 28.20826103720841, r: 0.3056946090575697
06/01/2019 10:50:42 *** epoch: 208 ***
06/01/2019 10:50:42 *** training ***
06/01/2019 10:50:42 step: 6836, epoch: 207, batch: 4, loss: 0.005072355270385742, acc: 100.0, f1: 100.0, r: 0.8054981690449878
06/01/2019 10:50:43 step: 6841, epoch: 207, batch: 9, loss: 0.0015525370836257935, acc: 100.0, f1: 100.0, r: 0.8308034276821632
06/01/2019 10:50:43 step: 6846, epoch: 207, batch: 14, loss: 0.0014296621084213257, acc: 100.0, f1: 100.0, r: 0.8226565817664157
06/01/2019 10:50:44 step: 6851, epoch: 207, batch: 19, loss: 0.0025519728660583496, acc: 100.0, f1: 100.0, r: 0.6513920252993233
06/01/2019 10:50:44 step: 6856, epoch: 207, batch: 24, loss: 0.001553259789943695, acc: 100.0, f1: 100.0, r: 0.6961008604449517
06/01/2019 10:50:45 step: 6861, epoch: 207, batch: 29, loss: 0.0020381510257720947, acc: 100.0, f1: 100.0, r: 0.7522337213842544
06/01/2019 10:50:45 *** evaluating ***
06/01/2019 10:50:45 step: 208, epoch: 207, acc: 58.97435897435898, f1: 26.482281171853227, r: 0.3071075677521967
06/01/2019 10:50:45 *** epoch: 209 ***
06/01/2019 10:50:45 *** training ***
06/01/2019 10:50:46 step: 6869, epoch: 208, batch: 4, loss: 0.0022326335310935974, acc: 100.0, f1: 100.0, r: 0.6326731325594853
06/01/2019 10:50:46 step: 6874, epoch: 208, batch: 9, loss: 0.0022272467613220215, acc: 100.0, f1: 100.0, r: 0.7369081924589138
06/01/2019 10:50:47 step: 6879, epoch: 208, batch: 14, loss: 0.0011707991361618042, acc: 100.0, f1: 100.0, r: 0.7309439043701129
06/01/2019 10:50:47 step: 6884, epoch: 208, batch: 19, loss: 0.00835757702589035, acc: 100.0, f1: 100.0, r: 0.781090917318841
06/01/2019 10:50:48 step: 6889, epoch: 208, batch: 24, loss: 0.004194781184196472, acc: 100.0, f1: 100.0, r: 0.7347994386441398
06/01/2019 10:50:48 step: 6894, epoch: 208, batch: 29, loss: 0.0014242306351661682, acc: 100.0, f1: 100.0, r: 0.6695513165401993
06/01/2019 10:50:48 *** evaluating ***
06/01/2019 10:50:48 step: 209, epoch: 208, acc: 59.401709401709404, f1: 28.007941527865626, r: 0.2967824154419771
06/01/2019 10:50:48 *** epoch: 210 ***
06/01/2019 10:50:48 *** training ***
06/01/2019 10:50:49 step: 6902, epoch: 209, batch: 4, loss: 0.002265632152557373, acc: 100.0, f1: 100.0, r: 0.7586574490163296
06/01/2019 10:50:49 step: 6907, epoch: 209, batch: 9, loss: 0.003983490169048309, acc: 100.0, f1: 100.0, r: 0.6506888010941642
06/01/2019 10:50:50 step: 6912, epoch: 209, batch: 14, loss: 0.0017290636897087097, acc: 100.0, f1: 100.0, r: 0.6528709252787802
06/01/2019 10:50:50 step: 6917, epoch: 209, batch: 19, loss: 0.001667998731136322, acc: 100.0, f1: 100.0, r: 0.7168681814942919
06/01/2019 10:50:51 step: 6922, epoch: 209, batch: 24, loss: 0.0037061460316181183, acc: 100.0, f1: 100.0, r: 0.6416656737971856
06/01/2019 10:50:51 step: 6927, epoch: 209, batch: 29, loss: 0.0029512494802474976, acc: 100.0, f1: 100.0, r: 0.7255371508853463
06/01/2019 10:50:52 *** evaluating ***
06/01/2019 10:50:52 step: 210, epoch: 209, acc: 58.54700854700855, f1: 25.661467745287638, r: 0.30225919504989796
06/01/2019 10:50:52 *** epoch: 211 ***
06/01/2019 10:50:52 *** training ***
06/01/2019 10:50:52 step: 6935, epoch: 210, batch: 4, loss: 0.00543598085641861, acc: 100.0, f1: 100.0, r: 0.7682721228247964
06/01/2019 10:50:53 step: 6940, epoch: 210, batch: 9, loss: 0.007645115256309509, acc: 100.0, f1: 100.0, r: 0.6882622707244618
06/01/2019 10:50:53 step: 6945, epoch: 210, batch: 14, loss: 0.0015735998749732971, acc: 100.0, f1: 100.0, r: 0.7314393322659614
06/01/2019 10:50:54 step: 6950, epoch: 210, batch: 19, loss: 0.0008875429630279541, acc: 100.0, f1: 100.0, r: 0.5917398806067513
06/01/2019 10:50:54 step: 6955, epoch: 210, batch: 24, loss: 0.0029793158173561096, acc: 100.0, f1: 100.0, r: 0.817338988420947
06/01/2019 10:50:55 step: 6960, epoch: 210, batch: 29, loss: 0.005316242575645447, acc: 100.0, f1: 100.0, r: 0.6857529966731922
06/01/2019 10:50:55 *** evaluating ***
06/01/2019 10:50:55 step: 211, epoch: 210, acc: 58.54700854700855, f1: 25.086618655496057, r: 0.30502779319410256
06/01/2019 10:50:55 *** epoch: 212 ***
06/01/2019 10:50:55 *** training ***
06/01/2019 10:50:56 step: 6968, epoch: 211, batch: 4, loss: 0.005001213401556015, acc: 100.0, f1: 100.0, r: 0.7755817950068745
06/01/2019 10:50:56 step: 6973, epoch: 211, batch: 9, loss: 0.001981683075428009, acc: 100.0, f1: 100.0, r: 0.7763699697345193
06/01/2019 10:50:57 step: 6978, epoch: 211, batch: 14, loss: 0.004066012799739838, acc: 100.0, f1: 100.0, r: 0.7608272479518502
06/01/2019 10:50:57 step: 6983, epoch: 211, batch: 19, loss: 0.0018256381154060364, acc: 100.0, f1: 100.0, r: 0.7052506668566336
06/01/2019 10:50:58 step: 6988, epoch: 211, batch: 24, loss: 0.0012425035238265991, acc: 100.0, f1: 100.0, r: 0.8469392904873775
06/01/2019 10:50:58 step: 6993, epoch: 211, batch: 29, loss: 0.0013369619846343994, acc: 100.0, f1: 100.0, r: 0.8024105542684514
06/01/2019 10:50:58 *** evaluating ***
06/01/2019 10:50:58 step: 212, epoch: 211, acc: 58.54700854700855, f1: 24.961482437897534, r: 0.3080277468760903
06/01/2019 10:50:58 *** epoch: 213 ***
06/01/2019 10:50:58 *** training ***
06/01/2019 10:50:59 step: 7001, epoch: 212, batch: 4, loss: 0.0014875009655952454, acc: 100.0, f1: 100.0, r: 0.721717588700657
06/01/2019 10:50:59 step: 7006, epoch: 212, batch: 9, loss: 0.0010166317224502563, acc: 100.0, f1: 100.0, r: 0.8150348887340818
06/01/2019 10:51:00 step: 7011, epoch: 212, batch: 14, loss: 0.001408107578754425, acc: 100.0, f1: 100.0, r: 0.5634158230471181
06/01/2019 10:51:00 step: 7016, epoch: 212, batch: 19, loss: 0.0009159445762634277, acc: 100.0, f1: 100.0, r: 0.8486654661441076
06/01/2019 10:51:01 step: 7021, epoch: 212, batch: 24, loss: 0.0014897361397743225, acc: 100.0, f1: 100.0, r: 0.7900897828179518
06/01/2019 10:51:01 step: 7026, epoch: 212, batch: 29, loss: 0.0022307634353637695, acc: 100.0, f1: 100.0, r: 0.7400460546341915
06/01/2019 10:51:01 *** evaluating ***
06/01/2019 10:51:02 step: 213, epoch: 212, acc: 58.97435897435898, f1: 24.982076358723386, r: 0.3048979342173189
06/01/2019 10:51:02 *** epoch: 214 ***
06/01/2019 10:51:02 *** training ***
06/01/2019 10:51:02 step: 7034, epoch: 213, batch: 4, loss: 0.001801908016204834, acc: 100.0, f1: 100.0, r: 0.7019186101690089
06/01/2019 10:51:03 step: 7039, epoch: 213, batch: 9, loss: 0.0024320855736732483, acc: 100.0, f1: 100.0, r: 0.7431158511925388
06/01/2019 10:51:03 step: 7044, epoch: 213, batch: 14, loss: 0.001806996762752533, acc: 100.0, f1: 100.0, r: 0.7366488652666591
06/01/2019 10:51:04 step: 7049, epoch: 213, batch: 19, loss: 0.0008580610156059265, acc: 100.0, f1: 100.0, r: 0.6679011218659778
06/01/2019 10:51:04 step: 7054, epoch: 213, batch: 24, loss: 0.002645350992679596, acc: 100.0, f1: 100.0, r: 0.7742428778655595
06/01/2019 10:51:05 step: 7059, epoch: 213, batch: 29, loss: 0.0018818974494934082, acc: 100.0, f1: 100.0, r: 0.7040372634011794
06/01/2019 10:51:05 *** evaluating ***
06/01/2019 10:51:05 step: 214, epoch: 213, acc: 60.256410256410255, f1: 26.48449724448305, r: 0.3093656977270784
06/01/2019 10:51:05 *** epoch: 215 ***
06/01/2019 10:51:05 *** training ***
06/01/2019 10:51:05 step: 7067, epoch: 214, batch: 4, loss: 0.0010520890355110168, acc: 100.0, f1: 100.0, r: 0.8021903728002154
06/01/2019 10:51:06 step: 7072, epoch: 214, batch: 9, loss: 0.001416333019733429, acc: 100.0, f1: 100.0, r: 0.7482099894811096
06/01/2019 10:51:06 step: 7077, epoch: 214, batch: 14, loss: 0.00408935546875, acc: 100.0, f1: 100.0, r: 0.6854688328483701
06/01/2019 10:51:07 step: 7082, epoch: 214, batch: 19, loss: 0.0018824264407157898, acc: 100.0, f1: 100.0, r: 0.8201698909739514
06/01/2019 10:51:07 step: 7087, epoch: 214, batch: 24, loss: 0.000896260142326355, acc: 100.0, f1: 100.0, r: 0.8276090348788926
06/01/2019 10:51:08 step: 7092, epoch: 214, batch: 29, loss: 0.003713589161634445, acc: 100.0, f1: 100.0, r: 0.6628160332701402
06/01/2019 10:51:08 *** evaluating ***
06/01/2019 10:51:08 step: 215, epoch: 214, acc: 59.82905982905983, f1: 25.306904510239388, r: 0.3076937835229258
06/01/2019 10:51:08 *** epoch: 216 ***
06/01/2019 10:51:08 *** training ***
06/01/2019 10:51:09 step: 7100, epoch: 215, batch: 4, loss: 0.0010253265500068665, acc: 100.0, f1: 100.0, r: 0.742952177009911
06/01/2019 10:51:09 step: 7105, epoch: 215, batch: 9, loss: 0.0028308145701885223, acc: 100.0, f1: 100.0, r: 0.822711289353956
06/01/2019 10:51:10 step: 7110, epoch: 215, batch: 14, loss: 0.0018949061632156372, acc: 100.0, f1: 100.0, r: 0.6934653696469097
06/01/2019 10:51:10 step: 7115, epoch: 215, batch: 19, loss: 0.0028427541255950928, acc: 100.0, f1: 100.0, r: 0.6095745983212127
06/01/2019 10:51:11 step: 7120, epoch: 215, batch: 24, loss: 0.000781916081905365, acc: 100.0, f1: 100.0, r: 0.6123895563278817
06/01/2019 10:51:11 step: 7125, epoch: 215, batch: 29, loss: 0.0026390403509140015, acc: 100.0, f1: 100.0, r: 0.7976642040416299
06/01/2019 10:51:11 *** evaluating ***
06/01/2019 10:51:12 step: 216, epoch: 215, acc: 59.82905982905983, f1: 26.424891485132445, r: 0.30385426219347206
06/01/2019 10:51:12 *** epoch: 217 ***
06/01/2019 10:51:12 *** training ***
06/01/2019 10:51:12 step: 7133, epoch: 216, batch: 4, loss: 0.0013652369379997253, acc: 100.0, f1: 100.0, r: 0.7997722758797118
06/01/2019 10:51:13 step: 7138, epoch: 216, batch: 9, loss: 0.000860266387462616, acc: 100.0, f1: 100.0, r: 0.8191899538432175
06/01/2019 10:51:13 step: 7143, epoch: 216, batch: 14, loss: 0.0010266229510307312, acc: 100.0, f1: 100.0, r: 0.8483010559877392
06/01/2019 10:51:14 step: 7148, epoch: 216, batch: 19, loss: 0.0014519058167934418, acc: 100.0, f1: 100.0, r: 0.7735225126900241
06/01/2019 10:51:14 step: 7153, epoch: 216, batch: 24, loss: 0.0006132498383522034, acc: 100.0, f1: 100.0, r: 0.7041792241806626
06/01/2019 10:51:15 step: 7158, epoch: 216, batch: 29, loss: 0.001050502061843872, acc: 100.0, f1: 100.0, r: 0.5748643763512854
06/01/2019 10:51:15 *** evaluating ***
06/01/2019 10:51:15 step: 217, epoch: 216, acc: 60.68376068376068, f1: 25.220713449925825, r: 0.30972243610262395
06/01/2019 10:51:15 *** epoch: 218 ***
06/01/2019 10:51:15 *** training ***
06/01/2019 10:51:15 step: 7166, epoch: 217, batch: 4, loss: 0.011963322758674622, acc: 100.0, f1: 100.0, r: 0.7827382817412457
06/01/2019 10:51:16 step: 7171, epoch: 217, batch: 9, loss: 0.0016456767916679382, acc: 100.0, f1: 100.0, r: 0.7192028706990651
06/01/2019 10:51:16 step: 7176, epoch: 217, batch: 14, loss: 0.0016109123826026917, acc: 100.0, f1: 100.0, r: 0.7949637171210635
06/01/2019 10:51:17 step: 7181, epoch: 217, batch: 19, loss: 0.0012708976864814758, acc: 100.0, f1: 100.0, r: 0.8114031179763213
06/01/2019 10:51:17 step: 7186, epoch: 217, batch: 24, loss: 0.002587798982858658, acc: 100.0, f1: 100.0, r: 0.8106898260888231
06/01/2019 10:51:18 step: 7191, epoch: 217, batch: 29, loss: 0.0014713555574417114, acc: 100.0, f1: 100.0, r: 0.7024270253960511
06/01/2019 10:51:18 *** evaluating ***
06/01/2019 10:51:18 step: 218, epoch: 217, acc: 59.401709401709404, f1: 26.255650343808234, r: 0.30727118933649
06/01/2019 10:51:18 *** epoch: 219 ***
06/01/2019 10:51:18 *** training ***
06/01/2019 10:51:19 step: 7199, epoch: 218, batch: 4, loss: 0.0014433935284614563, acc: 100.0, f1: 100.0, r: 0.8620282672590206
06/01/2019 10:51:19 step: 7204, epoch: 218, batch: 9, loss: 0.0011287778615951538, acc: 100.0, f1: 100.0, r: 0.662509168233129
06/01/2019 10:51:20 step: 7209, epoch: 218, batch: 14, loss: 0.004493020474910736, acc: 100.0, f1: 100.0, r: 0.7284223578328922
06/01/2019 10:51:20 step: 7214, epoch: 218, batch: 19, loss: 0.0015055611729621887, acc: 100.0, f1: 100.0, r: 0.7280714393176025
06/01/2019 10:51:21 step: 7219, epoch: 218, batch: 24, loss: 0.0014287084341049194, acc: 100.0, f1: 100.0, r: 0.7162183552142724
06/01/2019 10:51:21 step: 7224, epoch: 218, batch: 29, loss: 0.0016259178519248962, acc: 100.0, f1: 100.0, r: 0.8188855266977078
06/01/2019 10:51:21 *** evaluating ***
06/01/2019 10:51:21 step: 219, epoch: 218, acc: 59.82905982905983, f1: 26.644755565056787, r: 0.3057078968984401
06/01/2019 10:51:21 *** epoch: 220 ***
06/01/2019 10:51:21 *** training ***
06/01/2019 10:51:22 step: 7232, epoch: 219, batch: 4, loss: 0.0016076937317848206, acc: 100.0, f1: 100.0, r: 0.7026535060362701
06/01/2019 10:51:22 step: 7237, epoch: 219, batch: 9, loss: 0.0005771741271018982, acc: 100.0, f1: 100.0, r: 0.7459997210988386
06/01/2019 10:51:23 step: 7242, epoch: 219, batch: 14, loss: 0.002131648361682892, acc: 100.0, f1: 100.0, r: 0.7701362531117779
06/01/2019 10:51:23 step: 7247, epoch: 219, batch: 19, loss: 0.0038305148482322693, acc: 100.0, f1: 100.0, r: 0.681546137950146
06/01/2019 10:51:24 step: 7252, epoch: 219, batch: 24, loss: 0.0009889006614685059, acc: 100.0, f1: 100.0, r: 0.6715701756972655
06/01/2019 10:51:24 step: 7257, epoch: 219, batch: 29, loss: 0.002157732844352722, acc: 100.0, f1: 100.0, r: 0.7976057091689303
06/01/2019 10:51:24 *** evaluating ***
06/01/2019 10:51:25 step: 220, epoch: 219, acc: 58.54700854700855, f1: 27.3316140776699, r: 0.29251705043305665
06/01/2019 10:51:25 *** epoch: 221 ***
06/01/2019 10:51:25 *** training ***
06/01/2019 10:51:25 step: 7265, epoch: 220, batch: 4, loss: 0.0017673224210739136, acc: 100.0, f1: 100.0, r: 0.7964730436092381
06/01/2019 10:51:26 step: 7270, epoch: 220, batch: 9, loss: 0.0014548972249031067, acc: 100.0, f1: 100.0, r: 0.6905241518016039
06/01/2019 10:51:26 step: 7275, epoch: 220, batch: 14, loss: 0.0006583034992218018, acc: 100.0, f1: 100.0, r: 0.7857523908820099
06/01/2019 10:51:27 step: 7280, epoch: 220, batch: 19, loss: 0.000874832272529602, acc: 100.0, f1: 100.0, r: 0.691036463254748
06/01/2019 10:51:27 step: 7285, epoch: 220, batch: 24, loss: 0.0012853965163230896, acc: 100.0, f1: 100.0, r: 0.7778578208705103
06/01/2019 10:51:28 step: 7290, epoch: 220, batch: 29, loss: 0.0009734407067298889, acc: 100.0, f1: 100.0, r: 0.7056495268666724
06/01/2019 10:51:28 *** evaluating ***
06/01/2019 10:51:28 step: 221, epoch: 220, acc: 57.692307692307686, f1: 25.195100096162253, r: 0.29709886400850116
06/01/2019 10:51:28 *** epoch: 222 ***
06/01/2019 10:51:28 *** training ***
06/01/2019 10:51:28 step: 7298, epoch: 221, batch: 4, loss: 0.0011894181370735168, acc: 100.0, f1: 100.0, r: 0.7857013701604544
06/01/2019 10:51:29 step: 7303, epoch: 221, batch: 9, loss: 0.0026263445615768433, acc: 100.0, f1: 100.0, r: 0.7459510701915253
06/01/2019 10:51:29 step: 7308, epoch: 221, batch: 14, loss: 0.0015542954206466675, acc: 100.0, f1: 100.0, r: 0.6829750008786627
06/01/2019 10:51:30 step: 7313, epoch: 221, batch: 19, loss: 0.0013315528631210327, acc: 100.0, f1: 100.0, r: 0.6584445213570604
06/01/2019 10:51:30 step: 7318, epoch: 221, batch: 24, loss: 0.002113655209541321, acc: 100.0, f1: 100.0, r: 0.6736830728882361
06/01/2019 10:51:31 step: 7323, epoch: 221, batch: 29, loss: 0.0023188963532447815, acc: 100.0, f1: 100.0, r: 0.8124066730437455
06/01/2019 10:51:31 *** evaluating ***
06/01/2019 10:51:31 step: 222, epoch: 221, acc: 58.97435897435898, f1: 27.533438608721934, r: 0.290591431720749
06/01/2019 10:51:31 *** epoch: 223 ***
06/01/2019 10:51:31 *** training ***
06/01/2019 10:51:32 step: 7331, epoch: 222, batch: 4, loss: 0.003228895366191864, acc: 100.0, f1: 100.0, r: 0.7217090083156303
06/01/2019 10:51:32 step: 7336, epoch: 222, batch: 9, loss: 0.0012503862380981445, acc: 100.0, f1: 100.0, r: 0.7122158000928879
06/01/2019 10:51:33 step: 7341, epoch: 222, batch: 14, loss: 0.0027275756001472473, acc: 100.0, f1: 100.0, r: 0.820024830392825
06/01/2019 10:51:33 step: 7346, epoch: 222, batch: 19, loss: 0.002776890993118286, acc: 100.0, f1: 100.0, r: 0.6186609730877852
06/01/2019 10:51:34 step: 7351, epoch: 222, batch: 24, loss: 0.0009019896388053894, acc: 100.0, f1: 100.0, r: 0.665999949752887
06/01/2019 10:51:34 step: 7356, epoch: 222, batch: 29, loss: 0.0051474496722221375, acc: 100.0, f1: 100.0, r: 0.7654809041502278
06/01/2019 10:51:34 *** evaluating ***
06/01/2019 10:51:34 step: 223, epoch: 222, acc: 59.82905982905983, f1: 26.46218868948572, r: 0.3045004364777816
06/01/2019 10:51:34 *** epoch: 224 ***
06/01/2019 10:51:34 *** training ***
06/01/2019 10:51:35 step: 7364, epoch: 223, batch: 4, loss: 0.0008150041103363037, acc: 100.0, f1: 100.0, r: 0.8109947493012482
06/01/2019 10:51:35 step: 7369, epoch: 223, batch: 9, loss: 0.0009788796305656433, acc: 100.0, f1: 100.0, r: 0.6953707199793911
06/01/2019 10:51:36 step: 7374, epoch: 223, batch: 14, loss: 0.0031566396355628967, acc: 100.0, f1: 100.0, r: 0.6563932766929818
06/01/2019 10:51:36 step: 7379, epoch: 223, batch: 19, loss: 0.0006368979811668396, acc: 100.0, f1: 100.0, r: 0.6904512766149202
06/01/2019 10:51:37 step: 7384, epoch: 223, batch: 24, loss: 0.0015887990593910217, acc: 100.0, f1: 100.0, r: 0.7919349577170458
06/01/2019 10:51:37 step: 7389, epoch: 223, batch: 29, loss: 0.002505362033843994, acc: 100.0, f1: 100.0, r: 0.8217980176697726
06/01/2019 10:51:38 *** evaluating ***
06/01/2019 10:51:38 step: 224, epoch: 223, acc: 59.401709401709404, f1: 25.606714312883494, r: 0.30647490010786793
06/01/2019 10:51:38 *** epoch: 225 ***
06/01/2019 10:51:38 *** training ***
06/01/2019 10:51:38 step: 7397, epoch: 224, batch: 4, loss: 0.0005372315645217896, acc: 100.0, f1: 100.0, r: 0.5683880551249129
06/01/2019 10:51:39 step: 7402, epoch: 224, batch: 9, loss: 0.0018683746457099915, acc: 100.0, f1: 100.0, r: 0.7960230209470678
06/01/2019 10:51:39 step: 7407, epoch: 224, batch: 14, loss: 0.0039057135581970215, acc: 100.0, f1: 100.0, r: 0.7561727416589353
06/01/2019 10:51:40 step: 7412, epoch: 224, batch: 19, loss: 0.004125021398067474, acc: 100.0, f1: 100.0, r: 0.7158758616237059
06/01/2019 10:51:40 step: 7417, epoch: 224, batch: 24, loss: 0.005796968936920166, acc: 100.0, f1: 100.0, r: 0.7420331637768944
06/01/2019 10:51:41 step: 7422, epoch: 224, batch: 29, loss: 0.002084188163280487, acc: 100.0, f1: 100.0, r: 0.7314624887880395
06/01/2019 10:51:41 *** evaluating ***
06/01/2019 10:51:41 step: 225, epoch: 224, acc: 59.401709401709404, f1: 25.240164361934564, r: 0.3036533918744178
06/01/2019 10:51:41 *** epoch: 226 ***
06/01/2019 10:51:41 *** training ***
06/01/2019 10:51:42 step: 7430, epoch: 225, batch: 4, loss: 0.001366741955280304, acc: 100.0, f1: 100.0, r: 0.659706148537815
06/01/2019 10:51:42 step: 7435, epoch: 225, batch: 9, loss: 0.0012364387512207031, acc: 100.0, f1: 100.0, r: 0.6973735334924105
06/01/2019 10:51:43 step: 7440, epoch: 225, batch: 14, loss: 0.0072443559765815735, acc: 100.0, f1: 100.0, r: 0.7821787088575813
06/01/2019 10:51:43 step: 7445, epoch: 225, batch: 19, loss: 0.0030956491827964783, acc: 100.0, f1: 100.0, r: 0.7796768477298139
06/01/2019 10:51:44 step: 7450, epoch: 225, batch: 24, loss: 0.0016336441040039062, acc: 100.0, f1: 100.0, r: 0.7863782088890248
06/01/2019 10:51:44 step: 7455, epoch: 225, batch: 29, loss: 0.0024728775024414062, acc: 100.0, f1: 100.0, r: 0.681905566295381
06/01/2019 10:51:45 *** evaluating ***
06/01/2019 10:51:45 step: 226, epoch: 225, acc: 59.401709401709404, f1: 25.577023593720018, r: 0.3031609112745722
06/01/2019 10:51:45 *** epoch: 227 ***
06/01/2019 10:51:45 *** training ***
06/01/2019 10:51:45 step: 7463, epoch: 226, batch: 4, loss: 0.003105759620666504, acc: 100.0, f1: 100.0, r: 0.7735317163824166
06/01/2019 10:51:46 step: 7468, epoch: 226, batch: 9, loss: 0.0006559938192367554, acc: 100.0, f1: 100.0, r: 0.6151362194250983
06/01/2019 10:51:46 step: 7473, epoch: 226, batch: 14, loss: 0.001325748860836029, acc: 100.0, f1: 100.0, r: 0.777450517687988
06/01/2019 10:51:47 step: 7478, epoch: 226, batch: 19, loss: 0.000448077917098999, acc: 100.0, f1: 100.0, r: 0.6561189168797612
06/01/2019 10:51:47 step: 7483, epoch: 226, batch: 24, loss: 0.0009312406182289124, acc: 100.0, f1: 100.0, r: 0.7891116003522473
06/01/2019 10:51:48 step: 7488, epoch: 226, batch: 29, loss: 0.00221870094537735, acc: 100.0, f1: 100.0, r: 0.8075253101246218
06/01/2019 10:51:48 *** evaluating ***
06/01/2019 10:51:48 step: 227, epoch: 226, acc: 59.401709401709404, f1: 26.280938167501745, r: 0.2989343464807906
06/01/2019 10:51:48 *** epoch: 228 ***
06/01/2019 10:51:48 *** training ***
06/01/2019 10:51:49 step: 7496, epoch: 227, batch: 4, loss: 0.0015110000967979431, acc: 100.0, f1: 100.0, r: 0.7970794293748367
06/01/2019 10:51:49 step: 7501, epoch: 227, batch: 9, loss: 0.001110576093196869, acc: 100.0, f1: 100.0, r: 0.8564108800128484
06/01/2019 10:51:50 step: 7506, epoch: 227, batch: 14, loss: 0.003721103072166443, acc: 100.0, f1: 100.0, r: 0.8025620868838049
06/01/2019 10:51:50 step: 7511, epoch: 227, batch: 19, loss: 0.003708917647600174, acc: 100.0, f1: 100.0, r: 0.8027961226834845
06/01/2019 10:51:51 step: 7516, epoch: 227, batch: 24, loss: 0.000819869339466095, acc: 100.0, f1: 100.0, r: 0.7016412182329558
06/01/2019 10:51:51 step: 7521, epoch: 227, batch: 29, loss: 0.0014347657561302185, acc: 100.0, f1: 100.0, r: 0.705762487033934
06/01/2019 10:51:52 *** evaluating ***
06/01/2019 10:51:52 step: 228, epoch: 227, acc: 59.401709401709404, f1: 25.52935056665575, r: 0.30069401174268295
06/01/2019 10:51:52 *** epoch: 229 ***
06/01/2019 10:51:52 *** training ***
06/01/2019 10:51:52 step: 7529, epoch: 228, batch: 4, loss: 0.0028108954429626465, acc: 100.0, f1: 100.0, r: 0.7107859192414363
06/01/2019 10:51:53 step: 7534, epoch: 228, batch: 9, loss: 0.00223521888256073, acc: 100.0, f1: 100.0, r: 0.6375419853777621
06/01/2019 10:51:53 step: 7539, epoch: 228, batch: 14, loss: 0.0037676841020584106, acc: 100.0, f1: 100.0, r: 0.749149342946034
06/01/2019 10:51:54 step: 7544, epoch: 228, batch: 19, loss: 0.0013028010725975037, acc: 100.0, f1: 100.0, r: 0.7532665715449841
06/01/2019 10:51:54 step: 7549, epoch: 228, batch: 24, loss: 0.003120817244052887, acc: 100.0, f1: 100.0, r: 0.7833528304175545
06/01/2019 10:51:55 step: 7554, epoch: 228, batch: 29, loss: 0.004797615110874176, acc: 100.0, f1: 100.0, r: 0.7036855816558073
06/01/2019 10:51:55 *** evaluating ***
06/01/2019 10:51:55 step: 229, epoch: 228, acc: 58.97435897435898, f1: 25.475487555471606, r: 0.2979631201867458
06/01/2019 10:51:55 *** epoch: 230 ***
06/01/2019 10:51:55 *** training ***
06/01/2019 10:51:56 step: 7562, epoch: 229, batch: 4, loss: 0.0019394531846046448, acc: 100.0, f1: 100.0, r: 0.7684569835812192
06/01/2019 10:51:56 step: 7567, epoch: 229, batch: 9, loss: 0.0006969049572944641, acc: 100.0, f1: 100.0, r: 0.8173451343066873
06/01/2019 10:51:57 step: 7572, epoch: 229, batch: 14, loss: 0.0017881840467453003, acc: 100.0, f1: 100.0, r: 0.7006959965182066
06/01/2019 10:51:57 step: 7577, epoch: 229, batch: 19, loss: 0.006931878626346588, acc: 100.0, f1: 100.0, r: 0.7360107098604441
06/01/2019 10:51:58 step: 7582, epoch: 229, batch: 24, loss: 0.015331447124481201, acc: 100.0, f1: 100.0, r: 0.7279342203300734
06/01/2019 10:51:58 step: 7587, epoch: 229, batch: 29, loss: 0.0014274120330810547, acc: 100.0, f1: 100.0, r: 0.6768057816215778
06/01/2019 10:51:59 *** evaluating ***
06/01/2019 10:51:59 step: 230, epoch: 229, acc: 58.97435897435898, f1: 26.131568812428675, r: 0.2983610680723476
06/01/2019 10:51:59 *** epoch: 231 ***
06/01/2019 10:51:59 *** training ***
06/01/2019 10:51:59 step: 7595, epoch: 230, batch: 4, loss: 0.0014197081327438354, acc: 100.0, f1: 100.0, r: 0.6976826847587954
06/01/2019 10:52:00 step: 7600, epoch: 230, batch: 9, loss: 0.0009589865803718567, acc: 100.0, f1: 100.0, r: 0.8349674873939747
06/01/2019 10:52:00 step: 7605, epoch: 230, batch: 14, loss: 0.0019784122705459595, acc: 100.0, f1: 100.0, r: 0.7919616828316143
06/01/2019 10:52:01 step: 7610, epoch: 230, batch: 19, loss: 0.0010465532541275024, acc: 100.0, f1: 100.0, r: 0.7247907974781501
06/01/2019 10:52:01 step: 7615, epoch: 230, batch: 24, loss: 0.0008147135376930237, acc: 100.0, f1: 100.0, r: 0.8173078008251211
06/01/2019 10:52:02 step: 7620, epoch: 230, batch: 29, loss: 0.0013212710618972778, acc: 100.0, f1: 100.0, r: 0.7326578361381336
06/01/2019 10:52:02 *** evaluating ***
06/01/2019 10:52:02 step: 231, epoch: 230, acc: 58.54700854700855, f1: 26.06273883966174, r: 0.2975697098059515
06/01/2019 10:52:02 *** epoch: 232 ***
06/01/2019 10:52:02 *** training ***
06/01/2019 10:52:03 step: 7628, epoch: 231, batch: 4, loss: 0.036174606531858444, acc: 98.4375, f1: 97.71428571428571, r: 0.8117965087666459
06/01/2019 10:52:03 step: 7633, epoch: 231, batch: 9, loss: 0.0011680126190185547, acc: 100.0, f1: 100.0, r: 0.7478603179938577
06/01/2019 10:52:04 step: 7638, epoch: 231, batch: 14, loss: 0.004168305546045303, acc: 100.0, f1: 100.0, r: 0.6818589117169928
06/01/2019 10:52:04 step: 7643, epoch: 231, batch: 19, loss: 0.0015610754489898682, acc: 100.0, f1: 100.0, r: 0.6893822122366425
06/01/2019 10:52:05 step: 7648, epoch: 231, batch: 24, loss: 0.001843877136707306, acc: 100.0, f1: 100.0, r: 0.8192298220059618
06/01/2019 10:52:05 step: 7653, epoch: 231, batch: 29, loss: 0.00222720205783844, acc: 100.0, f1: 100.0, r: 0.6775265532305551
06/01/2019 10:52:05 *** evaluating ***
06/01/2019 10:52:06 step: 232, epoch: 231, acc: 59.401709401709404, f1: 27.284099990290755, r: 0.2970479557076788
06/01/2019 10:52:06 *** epoch: 233 ***
06/01/2019 10:52:06 *** training ***
06/01/2019 10:52:06 step: 7661, epoch: 232, batch: 4, loss: 0.001286625862121582, acc: 100.0, f1: 100.0, r: 0.705071136615064
06/01/2019 10:52:07 step: 7666, epoch: 232, batch: 9, loss: 0.0026889443397521973, acc: 100.0, f1: 100.0, r: 0.6882538120918142
06/01/2019 10:52:07 step: 7671, epoch: 232, batch: 14, loss: 0.0015279874205589294, acc: 100.0, f1: 100.0, r: 0.6308029457269347
06/01/2019 10:52:08 step: 7676, epoch: 232, batch: 19, loss: 0.0026064813137054443, acc: 100.0, f1: 100.0, r: 0.7192210468555033
06/01/2019 10:52:08 step: 7681, epoch: 232, batch: 24, loss: 0.005441244691610336, acc: 100.0, f1: 100.0, r: 0.7643642316751038
06/01/2019 10:52:09 step: 7686, epoch: 232, batch: 29, loss: 0.005430035293102264, acc: 100.0, f1: 100.0, r: 0.7123396707290406
06/01/2019 10:52:09 *** evaluating ***
06/01/2019 10:52:09 step: 233, epoch: 232, acc: 58.54700854700855, f1: 22.891469117884213, r: 0.29689338138063837
06/01/2019 10:52:09 *** epoch: 234 ***
06/01/2019 10:52:09 *** training ***
06/01/2019 10:52:10 step: 7694, epoch: 233, batch: 4, loss: 0.0030979588627815247, acc: 100.0, f1: 100.0, r: 0.6394939109113659
06/01/2019 10:52:10 step: 7699, epoch: 233, batch: 9, loss: 0.0029968395829200745, acc: 100.0, f1: 100.0, r: 0.8394435661128121
06/01/2019 10:52:11 step: 7704, epoch: 233, batch: 14, loss: 0.0022310838103294373, acc: 100.0, f1: 100.0, r: 0.8133364133836719
06/01/2019 10:52:11 step: 7709, epoch: 233, batch: 19, loss: 0.0006511062383651733, acc: 100.0, f1: 100.0, r: 0.7509966999185607
06/01/2019 10:52:12 step: 7714, epoch: 233, batch: 24, loss: 0.0015224441885948181, acc: 100.0, f1: 100.0, r: 0.7628333226347043
06/01/2019 10:52:12 step: 7719, epoch: 233, batch: 29, loss: 0.0027121827006340027, acc: 100.0, f1: 100.0, r: 0.7869232215551538
06/01/2019 10:52:12 *** evaluating ***
06/01/2019 10:52:13 step: 234, epoch: 233, acc: 57.692307692307686, f1: 23.650396825396825, r: 0.29409965697154183
06/01/2019 10:52:13 *** epoch: 235 ***
06/01/2019 10:52:13 *** training ***
06/01/2019 10:52:13 step: 7727, epoch: 234, batch: 4, loss: 0.0009377449750900269, acc: 100.0, f1: 100.0, r: 0.5698270774206354
06/01/2019 10:52:14 step: 7732, epoch: 234, batch: 9, loss: 0.003015216439962387, acc: 100.0, f1: 100.0, r: 0.7031595786817549
06/01/2019 10:52:14 step: 7737, epoch: 234, batch: 14, loss: 0.001062631607055664, acc: 100.0, f1: 100.0, r: 0.7725864692773934
06/01/2019 10:52:15 step: 7742, epoch: 234, batch: 19, loss: 0.000507749617099762, acc: 100.0, f1: 100.0, r: 0.7559702653147253
06/01/2019 10:52:15 step: 7747, epoch: 234, batch: 24, loss: 0.0013164207339286804, acc: 100.0, f1: 100.0, r: 0.7827117317775177
06/01/2019 10:52:16 step: 7752, epoch: 234, batch: 29, loss: 0.0020159557461738586, acc: 100.0, f1: 100.0, r: 0.6880338079103336
06/01/2019 10:52:16 *** evaluating ***
06/01/2019 10:52:16 step: 235, epoch: 234, acc: 58.119658119658126, f1: 24.10244486718805, r: 0.29217083189292137
06/01/2019 10:52:16 *** epoch: 236 ***
06/01/2019 10:52:16 *** training ***
06/01/2019 10:52:17 step: 7760, epoch: 235, batch: 4, loss: 0.0013068392872810364, acc: 100.0, f1: 100.0, r: 0.6766049582978136
06/01/2019 10:52:17 step: 7765, epoch: 235, batch: 9, loss: 0.001933962106704712, acc: 100.0, f1: 100.0, r: 0.6941240593378603
06/01/2019 10:52:18 step: 7770, epoch: 235, batch: 14, loss: 0.0029745399951934814, acc: 100.0, f1: 100.0, r: 0.7793266710333075
06/01/2019 10:52:18 step: 7775, epoch: 235, batch: 19, loss: 0.0013280585408210754, acc: 100.0, f1: 100.0, r: 0.7265404213718175
06/01/2019 10:52:19 step: 7780, epoch: 235, batch: 24, loss: 0.001390412449836731, acc: 100.0, f1: 100.0, r: 0.8076661900691496
06/01/2019 10:52:19 step: 7785, epoch: 235, batch: 29, loss: 0.0032191500067710876, acc: 100.0, f1: 100.0, r: 0.7127734062776082
06/01/2019 10:52:19 *** evaluating ***
06/01/2019 10:52:20 step: 236, epoch: 235, acc: 58.97435897435898, f1: 25.629452517002594, r: 0.2913570440709783
06/01/2019 10:52:20 *** epoch: 237 ***
06/01/2019 10:52:20 *** training ***
06/01/2019 10:52:20 step: 7793, epoch: 236, batch: 4, loss: 0.0007529184222221375, acc: 100.0, f1: 100.0, r: 0.6946629436492338
06/01/2019 10:52:21 step: 7798, epoch: 236, batch: 9, loss: 0.0013124942779541016, acc: 100.0, f1: 100.0, r: 0.7862348383643926
06/01/2019 10:52:21 step: 7803, epoch: 236, batch: 14, loss: 0.0011152401566505432, acc: 100.0, f1: 100.0, r: 0.644211830042478
06/01/2019 10:52:22 step: 7808, epoch: 236, batch: 19, loss: 0.0012187212705612183, acc: 100.0, f1: 100.0, r: 0.8119078266765187
06/01/2019 10:52:22 step: 7813, epoch: 236, batch: 24, loss: 0.0010715499520301819, acc: 100.0, f1: 100.0, r: 0.7264402687989507
06/01/2019 10:52:22 step: 7818, epoch: 236, batch: 29, loss: 0.0019026696681976318, acc: 100.0, f1: 100.0, r: 0.6728262478654059
06/01/2019 10:52:23 *** evaluating ***
06/01/2019 10:52:23 step: 237, epoch: 236, acc: 58.54700854700855, f1: 25.49043715846995, r: 0.2874500086678503
06/01/2019 10:52:23 *** epoch: 238 ***
06/01/2019 10:52:23 *** training ***
06/01/2019 10:52:23 step: 7826, epoch: 237, batch: 4, loss: 0.0008773952722549438, acc: 100.0, f1: 100.0, r: 0.8233152565539624
06/01/2019 10:52:24 step: 7831, epoch: 237, batch: 9, loss: 0.0025582164525985718, acc: 100.0, f1: 100.0, r: 0.686922244279615
06/01/2019 10:52:24 step: 7836, epoch: 237, batch: 14, loss: 0.003958657383918762, acc: 100.0, f1: 100.0, r: 0.7279449062129183
06/01/2019 10:52:25 step: 7841, epoch: 237, batch: 19, loss: 0.0006024092435836792, acc: 100.0, f1: 100.0, r: 0.7035288827638129
06/01/2019 10:52:25 step: 7846, epoch: 237, batch: 24, loss: 0.0027932748198509216, acc: 100.0, f1: 100.0, r: 0.7346024336766528
06/01/2019 10:52:26 step: 7851, epoch: 237, batch: 29, loss: 0.001155652105808258, acc: 100.0, f1: 100.0, r: 0.7557302733047357
06/01/2019 10:52:26 *** evaluating ***
06/01/2019 10:52:26 step: 238, epoch: 237, acc: 58.119658119658126, f1: 24.91132063917253, r: 0.29145104121703136
06/01/2019 10:52:26 *** epoch: 239 ***
06/01/2019 10:52:26 *** training ***
06/01/2019 10:52:27 step: 7859, epoch: 238, batch: 4, loss: 0.0006282702088356018, acc: 100.0, f1: 100.0, r: 0.6770307898301067
06/01/2019 10:52:27 step: 7864, epoch: 238, batch: 9, loss: 0.001192249357700348, acc: 100.0, f1: 100.0, r: 0.7177556891667953
06/01/2019 10:52:28 step: 7869, epoch: 238, batch: 14, loss: 0.001826561987400055, acc: 100.0, f1: 100.0, r: 0.8187257428427677
06/01/2019 10:52:28 step: 7874, epoch: 238, batch: 19, loss: 0.0036370009183883667, acc: 100.0, f1: 100.0, r: 0.7173742893330947
06/01/2019 10:52:29 step: 7879, epoch: 238, batch: 24, loss: 0.0016154944896697998, acc: 100.0, f1: 100.0, r: 0.5745283608335463
06/01/2019 10:52:29 step: 7884, epoch: 238, batch: 29, loss: 0.009189382195472717, acc: 100.0, f1: 100.0, r: 0.8554712068830268
06/01/2019 10:52:30 *** evaluating ***
06/01/2019 10:52:30 step: 239, epoch: 238, acc: 58.119658119658126, f1: 23.73769568191879, r: 0.29291146330128703
06/01/2019 10:52:30 *** epoch: 240 ***
06/01/2019 10:52:30 *** training ***
06/01/2019 10:52:30 step: 7892, epoch: 239, batch: 4, loss: 0.0032057464122772217, acc: 100.0, f1: 100.0, r: 0.7689763284716976
06/01/2019 10:52:31 step: 7897, epoch: 239, batch: 9, loss: 0.001439116895198822, acc: 100.0, f1: 100.0, r: 0.8094977959976293
06/01/2019 10:52:31 step: 7902, epoch: 239, batch: 14, loss: 0.004272304475307465, acc: 100.0, f1: 100.0, r: 0.8007732475182998
06/01/2019 10:52:32 step: 7907, epoch: 239, batch: 19, loss: 0.003431856632232666, acc: 100.0, f1: 100.0, r: 0.7484136541606653
06/01/2019 10:52:32 step: 7912, epoch: 239, batch: 24, loss: 0.00041318684816360474, acc: 100.0, f1: 100.0, r: 0.8453625870248268
06/01/2019 10:52:33 step: 7917, epoch: 239, batch: 29, loss: 0.0010158196091651917, acc: 100.0, f1: 100.0, r: 0.8012107021321645
06/01/2019 10:52:33 *** evaluating ***
06/01/2019 10:52:33 step: 240, epoch: 239, acc: 58.54700854700855, f1: 26.459176788124157, r: 0.28832183529578725
06/01/2019 10:52:33 *** epoch: 241 ***
06/01/2019 10:52:33 *** training ***
06/01/2019 10:52:34 step: 7925, epoch: 240, batch: 4, loss: 0.0014079585671424866, acc: 100.0, f1: 100.0, r: 0.7756865667833376
06/01/2019 10:52:34 step: 7930, epoch: 240, batch: 9, loss: 0.0021607279777526855, acc: 100.0, f1: 100.0, r: 0.7968751918265318
06/01/2019 10:52:35 step: 7935, epoch: 240, batch: 14, loss: 0.0017807558178901672, acc: 100.0, f1: 100.0, r: 0.5958904393706478
06/01/2019 10:52:35 step: 7940, epoch: 240, batch: 19, loss: 0.001775979995727539, acc: 100.0, f1: 100.0, r: 0.6986709854894144
06/01/2019 10:52:36 step: 7945, epoch: 240, batch: 24, loss: 0.0018999725580215454, acc: 100.0, f1: 100.0, r: 0.7568136648100878
06/01/2019 10:52:36 step: 7950, epoch: 240, batch: 29, loss: 0.0008316710591316223, acc: 100.0, f1: 100.0, r: 0.7581034921166211
06/01/2019 10:52:37 *** evaluating ***
06/01/2019 10:52:37 step: 241, epoch: 240, acc: 58.54700854700855, f1: 23.703279856529836, r: 0.2982378298181906
06/01/2019 10:52:37 *** epoch: 242 ***
06/01/2019 10:52:37 *** training ***
06/01/2019 10:52:37 step: 7958, epoch: 241, batch: 4, loss: 0.006842620670795441, acc: 100.0, f1: 100.0, r: 0.7797187960334719
06/01/2019 10:52:38 step: 7963, epoch: 241, batch: 9, loss: 0.002151601016521454, acc: 100.0, f1: 100.0, r: 0.8010871731972987
06/01/2019 10:52:38 step: 7968, epoch: 241, batch: 14, loss: 0.003042079508304596, acc: 100.0, f1: 100.0, r: 0.6344749827321701
06/01/2019 10:52:39 step: 7973, epoch: 241, batch: 19, loss: 0.0037781931459903717, acc: 100.0, f1: 100.0, r: 0.7584895919506943
06/01/2019 10:52:39 step: 7978, epoch: 241, batch: 24, loss: 0.003097809851169586, acc: 100.0, f1: 100.0, r: 0.7204103399697217
06/01/2019 10:52:40 step: 7983, epoch: 241, batch: 29, loss: 0.0009682998061180115, acc: 100.0, f1: 100.0, r: 0.6870755888354234
06/01/2019 10:52:40 *** evaluating ***
06/01/2019 10:52:40 step: 242, epoch: 241, acc: 58.97435897435898, f1: 26.519645780827805, r: 0.28961510864386436
06/01/2019 10:52:40 *** epoch: 243 ***
06/01/2019 10:52:40 *** training ***
06/01/2019 10:52:41 step: 7991, epoch: 242, batch: 4, loss: 0.0010679438710212708, acc: 100.0, f1: 100.0, r: 0.6811685854422935
06/01/2019 10:52:41 step: 7996, epoch: 242, batch: 9, loss: 0.0008451789617538452, acc: 100.0, f1: 100.0, r: 0.5938376072543433
06/01/2019 10:52:42 step: 8001, epoch: 242, batch: 14, loss: 0.0011140406131744385, acc: 100.0, f1: 100.0, r: 0.6826254447923548
06/01/2019 10:52:42 step: 8006, epoch: 242, batch: 19, loss: 0.0025202035903930664, acc: 100.0, f1: 100.0, r: 0.8366365337344182
06/01/2019 10:52:43 step: 8011, epoch: 242, batch: 24, loss: 0.0010572746396064758, acc: 100.0, f1: 100.0, r: 0.7273899667426598
06/01/2019 10:52:43 step: 8016, epoch: 242, batch: 29, loss: 0.0008030533790588379, acc: 100.0, f1: 100.0, r: 0.7370907529299917
06/01/2019 10:52:43 *** evaluating ***
06/01/2019 10:52:44 step: 243, epoch: 242, acc: 57.692307692307686, f1: 24.39152121672042, r: 0.29264693006829257
06/01/2019 10:52:44 *** epoch: 244 ***
06/01/2019 10:52:44 *** training ***
06/01/2019 10:52:44 step: 8024, epoch: 243, batch: 4, loss: 0.0013039633631706238, acc: 100.0, f1: 100.0, r: 0.7561355680125061
06/01/2019 10:52:45 step: 8029, epoch: 243, batch: 9, loss: 0.0011238232254981995, acc: 100.0, f1: 100.0, r: 0.8286154328836098
06/01/2019 10:52:45 step: 8034, epoch: 243, batch: 14, loss: 0.0009032264351844788, acc: 100.0, f1: 100.0, r: 0.7986204231590228
06/01/2019 10:52:46 step: 8039, epoch: 243, batch: 19, loss: 0.001021057367324829, acc: 100.0, f1: 100.0, r: 0.7355525441266916
06/01/2019 10:52:46 step: 8044, epoch: 243, batch: 24, loss: 0.0012988746166229248, acc: 100.0, f1: 100.0, r: 0.7908985267733711
06/01/2019 10:52:47 step: 8049, epoch: 243, batch: 29, loss: 0.0009814724326133728, acc: 100.0, f1: 100.0, r: 0.6576593673839843
06/01/2019 10:52:47 *** evaluating ***
06/01/2019 10:52:47 step: 244, epoch: 243, acc: 59.82905982905983, f1: 26.867473182152597, r: 0.3001971073826558
06/01/2019 10:52:47 *** epoch: 245 ***
06/01/2019 10:52:47 *** training ***
06/01/2019 10:52:48 step: 8057, epoch: 244, batch: 4, loss: 0.0010425299406051636, acc: 100.0, f1: 100.0, r: 0.7026369264437333
06/01/2019 10:52:48 step: 8062, epoch: 244, batch: 9, loss: 0.0014386475086212158, acc: 100.0, f1: 100.0, r: 0.7239645037489588
06/01/2019 10:52:49 step: 8067, epoch: 244, batch: 14, loss: 0.0011588484048843384, acc: 100.0, f1: 100.0, r: 0.6431506555397316
06/01/2019 10:52:49 step: 8072, epoch: 244, batch: 19, loss: 0.002088412642478943, acc: 100.0, f1: 100.0, r: 0.7412167354897031
06/01/2019 10:52:50 step: 8077, epoch: 244, batch: 24, loss: 0.0019508376717567444, acc: 100.0, f1: 100.0, r: 0.7779509939830374
06/01/2019 10:52:50 step: 8082, epoch: 244, batch: 29, loss: 0.001595139503479004, acc: 100.0, f1: 100.0, r: 0.6769302891068713
06/01/2019 10:52:50 *** evaluating ***
06/01/2019 10:52:51 step: 245, epoch: 244, acc: 58.54700854700855, f1: 26.020054704749256, r: 0.2942905627823392
06/01/2019 10:52:51 *** epoch: 246 ***
06/01/2019 10:52:51 *** training ***
06/01/2019 10:52:51 step: 8090, epoch: 245, batch: 4, loss: 0.0011106804013252258, acc: 100.0, f1: 100.0, r: 0.7170547880988913
06/01/2019 10:52:52 step: 8095, epoch: 245, batch: 9, loss: 0.0019778162240982056, acc: 100.0, f1: 100.0, r: 0.7564415707812584
06/01/2019 10:52:52 step: 8100, epoch: 245, batch: 14, loss: 0.002536296844482422, acc: 100.0, f1: 100.0, r: 0.8161000533778844
06/01/2019 10:52:53 step: 8105, epoch: 245, batch: 19, loss: 0.0016489699482917786, acc: 100.0, f1: 100.0, r: 0.7178060472843745
06/01/2019 10:52:53 step: 8110, epoch: 245, batch: 24, loss: 0.0017872080206871033, acc: 100.0, f1: 100.0, r: 0.7062513398381479
06/01/2019 10:52:54 step: 8115, epoch: 245, batch: 29, loss: 0.0027170851826667786, acc: 100.0, f1: 100.0, r: 0.7110059429651714
06/01/2019 10:52:54 *** evaluating ***
06/01/2019 10:52:54 step: 246, epoch: 245, acc: 58.54700854700855, f1: 24.905654776130884, r: 0.29503425438449754
06/01/2019 10:52:54 *** epoch: 247 ***
06/01/2019 10:52:54 *** training ***
06/01/2019 10:52:55 step: 8123, epoch: 246, batch: 4, loss: 0.0014784112572669983, acc: 100.0, f1: 100.0, r: 0.7659952190043607
06/01/2019 10:52:55 step: 8128, epoch: 246, batch: 9, loss: 0.002435259521007538, acc: 100.0, f1: 100.0, r: 0.7560113519996634
06/01/2019 10:52:56 step: 8133, epoch: 246, batch: 14, loss: 0.0011979788541793823, acc: 100.0, f1: 100.0, r: 0.6984330887938127
06/01/2019 10:52:56 step: 8138, epoch: 246, batch: 19, loss: 0.001805238425731659, acc: 100.0, f1: 100.0, r: 0.7424693619450407
06/01/2019 10:52:57 step: 8143, epoch: 246, batch: 24, loss: 0.0043090954422950745, acc: 100.0, f1: 100.0, r: 0.7125935843119517
06/01/2019 10:52:57 step: 8148, epoch: 246, batch: 29, loss: 0.0009746849536895752, acc: 100.0, f1: 100.0, r: 0.8143511576801747
06/01/2019 10:52:57 *** evaluating ***
06/01/2019 10:52:58 step: 247, epoch: 246, acc: 58.119658119658126, f1: 26.224258714168634, r: 0.28935701328530644
06/01/2019 10:52:58 *** epoch: 248 ***
06/01/2019 10:52:58 *** training ***
06/01/2019 10:52:58 step: 8156, epoch: 247, batch: 4, loss: 0.0011379867792129517, acc: 100.0, f1: 100.0, r: 0.6812957089146114
06/01/2019 10:52:59 step: 8161, epoch: 247, batch: 9, loss: 0.0005578920245170593, acc: 100.0, f1: 100.0, r: 0.7224638415787157
06/01/2019 10:52:59 step: 8166, epoch: 247, batch: 14, loss: 0.01274297758936882, acc: 100.0, f1: 100.0, r: 0.7797614898255006
06/01/2019 10:53:00 step: 8171, epoch: 247, batch: 19, loss: 0.0012602359056472778, acc: 100.0, f1: 100.0, r: 0.761120859962639
06/01/2019 10:53:00 step: 8176, epoch: 247, batch: 24, loss: 0.001565679907798767, acc: 100.0, f1: 100.0, r: 0.8059929807652985
06/01/2019 10:53:01 step: 8181, epoch: 247, batch: 29, loss: 0.0010636448860168457, acc: 100.0, f1: 100.0, r: 0.7763030848621095
06/01/2019 10:53:01 *** evaluating ***
06/01/2019 10:53:01 step: 248, epoch: 247, acc: 58.119658119658126, f1: 26.26907618483357, r: 0.2891996857585105
06/01/2019 10:53:01 *** epoch: 249 ***
06/01/2019 10:53:01 *** training ***
06/01/2019 10:53:02 step: 8189, epoch: 248, batch: 4, loss: 0.009492956101894379, acc: 100.0, f1: 100.0, r: 0.7815252849435749
06/01/2019 10:53:02 step: 8194, epoch: 248, batch: 9, loss: 0.0025784149765968323, acc: 100.0, f1: 100.0, r: 0.6926798264975418
06/01/2019 10:53:03 step: 8199, epoch: 248, batch: 14, loss: 0.0013111382722854614, acc: 100.0, f1: 100.0, r: 0.6177470891864183
06/01/2019 10:53:03 step: 8204, epoch: 248, batch: 19, loss: 0.0029203295707702637, acc: 100.0, f1: 100.0, r: 0.6852174233545221
06/01/2019 10:53:04 step: 8209, epoch: 248, batch: 24, loss: 0.00246574729681015, acc: 100.0, f1: 100.0, r: 0.6792534931081768
06/01/2019 10:53:04 step: 8214, epoch: 248, batch: 29, loss: 0.006129715591669083, acc: 100.0, f1: 100.0, r: 0.7240679047491658
06/01/2019 10:53:04 *** evaluating ***
06/01/2019 10:53:05 step: 249, epoch: 248, acc: 58.97435897435898, f1: 24.71491636568486, r: 0.2979329530915264
06/01/2019 10:53:05 *** epoch: 250 ***
06/01/2019 10:53:05 *** training ***
06/01/2019 10:53:05 step: 8222, epoch: 249, batch: 4, loss: 0.0021464601159095764, acc: 100.0, f1: 100.0, r: 0.7090097990423307
06/01/2019 10:53:06 step: 8227, epoch: 249, batch: 9, loss: 0.004659339785575867, acc: 100.0, f1: 100.0, r: 0.7068536915817067
06/01/2019 10:53:06 step: 8232, epoch: 249, batch: 14, loss: 0.002850279211997986, acc: 100.0, f1: 100.0, r: 0.6857653368691919
06/01/2019 10:53:07 step: 8237, epoch: 249, batch: 19, loss: 0.002069823443889618, acc: 100.0, f1: 100.0, r: 0.7740903200938736
06/01/2019 10:53:07 step: 8242, epoch: 249, batch: 24, loss: 0.0019877105951309204, acc: 100.0, f1: 100.0, r: 0.6939286698337676
06/01/2019 10:53:08 step: 8247, epoch: 249, batch: 29, loss: 0.008059203624725342, acc: 100.0, f1: 100.0, r: 0.7913303054281402
06/01/2019 10:53:08 *** evaluating ***
06/01/2019 10:53:08 step: 250, epoch: 249, acc: 58.54700854700855, f1: 25.76519230809963, r: 0.2973003681197762
06/01/2019 10:53:08 *** epoch: 251 ***
06/01/2019 10:53:08 *** training ***
06/01/2019 10:53:09 step: 8255, epoch: 250, batch: 4, loss: 0.009953364729881287, acc: 100.0, f1: 100.0, r: 0.8292096928340607
06/01/2019 10:53:09 step: 8260, epoch: 250, batch: 9, loss: 0.02647731825709343, acc: 98.4375, f1: 98.92156862745098, r: 0.7575765298604562
06/01/2019 10:53:10 step: 8265, epoch: 250, batch: 14, loss: 0.002028215676546097, acc: 100.0, f1: 100.0, r: 0.7217120971661903
06/01/2019 10:53:10 step: 8270, epoch: 250, batch: 19, loss: 0.005602240562438965, acc: 100.0, f1: 100.0, r: 0.7198866167447366
06/01/2019 10:53:11 step: 8275, epoch: 250, batch: 24, loss: 0.006273508071899414, acc: 100.0, f1: 100.0, r: 0.6697471875732075
06/01/2019 10:53:11 step: 8280, epoch: 250, batch: 29, loss: 0.0007557794451713562, acc: 100.0, f1: 100.0, r: 0.8099910824735368
06/01/2019 10:53:11 *** evaluating ***
06/01/2019 10:53:12 step: 251, epoch: 250, acc: 59.82905982905983, f1: 27.008394827256478, r: 0.2964953885624129
06/01/2019 10:53:12 *** epoch: 252 ***
06/01/2019 10:53:12 *** training ***
06/01/2019 10:53:12 step: 8288, epoch: 251, batch: 4, loss: 0.0026005804538726807, acc: 100.0, f1: 100.0, r: 0.6903620465698073
06/01/2019 10:53:13 step: 8293, epoch: 251, batch: 9, loss: 0.0034817755222320557, acc: 100.0, f1: 100.0, r: 0.614851577309627
06/01/2019 10:53:13 step: 8298, epoch: 251, batch: 14, loss: 0.0027683228254318237, acc: 100.0, f1: 100.0, r: 0.7712323778167616
06/01/2019 10:53:14 step: 8303, epoch: 251, batch: 19, loss: 0.0050301700830459595, acc: 100.0, f1: 100.0, r: 0.612505103241064
06/01/2019 10:53:14 step: 8308, epoch: 251, batch: 24, loss: 0.0018564462661743164, acc: 100.0, f1: 100.0, r: 0.7696373720182949
06/01/2019 10:53:15 step: 8313, epoch: 251, batch: 29, loss: 0.00611935555934906, acc: 100.0, f1: 100.0, r: 0.7230400091425219
06/01/2019 10:53:15 *** evaluating ***
06/01/2019 10:53:15 step: 252, epoch: 251, acc: 58.119658119658126, f1: 24.92021628350255, r: 0.29730335484746684
06/01/2019 10:53:15 *** epoch: 253 ***
06/01/2019 10:53:15 *** training ***
06/01/2019 10:53:15 step: 8321, epoch: 252, batch: 4, loss: 0.0009382143616676331, acc: 100.0, f1: 100.0, r: 0.666703514531117
06/01/2019 10:53:16 step: 8326, epoch: 252, batch: 9, loss: 0.0026077404618263245, acc: 100.0, f1: 100.0, r: 0.6801759482957203
06/01/2019 10:53:16 step: 8331, epoch: 252, batch: 14, loss: 0.0020730197429656982, acc: 100.0, f1: 100.0, r: 0.7283847029466
06/01/2019 10:53:17 step: 8336, epoch: 252, batch: 19, loss: 0.009010251611471176, acc: 100.0, f1: 100.0, r: 0.7098777616089188
06/01/2019 10:53:17 step: 8341, epoch: 252, batch: 24, loss: 0.0008769035339355469, acc: 100.0, f1: 100.0, r: 0.7674165683506257
06/01/2019 10:53:18 step: 8346, epoch: 252, batch: 29, loss: 0.0012255311012268066, acc: 100.0, f1: 100.0, r: 0.7546470581025009
06/01/2019 10:53:18 *** evaluating ***
06/01/2019 10:53:18 step: 253, epoch: 252, acc: 57.26495726495726, f1: 23.564405312548892, r: 0.29710107853264345
06/01/2019 10:53:18 *** epoch: 254 ***
06/01/2019 10:53:18 *** training ***
06/01/2019 10:53:19 step: 8354, epoch: 253, batch: 4, loss: 0.0015859007835388184, acc: 100.0, f1: 100.0, r: 0.8023999486977296
06/01/2019 10:53:19 step: 8359, epoch: 253, batch: 9, loss: 0.002034693956375122, acc: 100.0, f1: 100.0, r: 0.7215053313076089
06/01/2019 10:53:20 step: 8364, epoch: 253, batch: 14, loss: 0.0011013299226760864, acc: 100.0, f1: 100.0, r: 0.7937821003066539
06/01/2019 10:53:20 step: 8369, epoch: 253, batch: 19, loss: 0.0007410421967506409, acc: 100.0, f1: 100.0, r: 0.6092602361615866
06/01/2019 10:53:21 step: 8374, epoch: 253, batch: 24, loss: 0.001127384603023529, acc: 100.0, f1: 100.0, r: 0.8150571844675194
06/01/2019 10:53:21 step: 8379, epoch: 253, batch: 29, loss: 0.0007435455918312073, acc: 100.0, f1: 100.0, r: 0.8260824045334499
06/01/2019 10:53:21 *** evaluating ***
06/01/2019 10:53:22 step: 254, epoch: 253, acc: 58.119658119658126, f1: 24.605647265493815, r: 0.29606469266897956
06/01/2019 10:53:22 *** epoch: 255 ***
06/01/2019 10:53:22 *** training ***
06/01/2019 10:53:22 step: 8387, epoch: 254, batch: 4, loss: 0.0018234774470329285, acc: 100.0, f1: 100.0, r: 0.7525037159492848
06/01/2019 10:53:23 step: 8392, epoch: 254, batch: 9, loss: 0.0010947659611701965, acc: 100.0, f1: 100.0, r: 0.8355526072561397
06/01/2019 10:53:23 step: 8397, epoch: 254, batch: 14, loss: 0.0014386624097824097, acc: 100.0, f1: 100.0, r: 0.7165848410970657
06/01/2019 10:53:24 step: 8402, epoch: 254, batch: 19, loss: 0.004135455936193466, acc: 100.0, f1: 100.0, r: 0.7900737970970969
06/01/2019 10:53:24 step: 8407, epoch: 254, batch: 24, loss: 0.003698192536830902, acc: 100.0, f1: 100.0, r: 0.7926898220297623
06/01/2019 10:53:25 step: 8412, epoch: 254, batch: 29, loss: 0.0019273832440376282, acc: 100.0, f1: 100.0, r: 0.6620168087994487
06/01/2019 10:53:25 *** evaluating ***
06/01/2019 10:53:25 step: 255, epoch: 254, acc: 57.26495726495726, f1: 24.690742149245125, r: 0.29209315759387994
06/01/2019 10:53:25 *** epoch: 256 ***
06/01/2019 10:53:25 *** training ***
06/01/2019 10:53:25 step: 8420, epoch: 255, batch: 4, loss: 0.002805657684803009, acc: 100.0, f1: 100.0, r: 0.6835234095536705
06/01/2019 10:53:26 step: 8425, epoch: 255, batch: 9, loss: 0.006247390061616898, acc: 100.0, f1: 100.0, r: 0.8225921585076081
06/01/2019 10:53:26 step: 8430, epoch: 255, batch: 14, loss: 0.0027536749839782715, acc: 100.0, f1: 100.0, r: 0.7742132429375232
06/01/2019 10:53:27 step: 8435, epoch: 255, batch: 19, loss: 0.0015961304306983948, acc: 100.0, f1: 100.0, r: 0.6849059174244477
06/01/2019 10:53:27 step: 8440, epoch: 255, batch: 24, loss: 0.001072131097316742, acc: 100.0, f1: 100.0, r: 0.8129227163815167
06/01/2019 10:53:28 step: 8445, epoch: 255, batch: 29, loss: 0.0020050033926963806, acc: 100.0, f1: 100.0, r: 0.8111667373469715
06/01/2019 10:53:28 *** evaluating ***
06/01/2019 10:53:28 step: 256, epoch: 255, acc: 58.119658119658126, f1: 25.094978779976472, r: 0.29111916936685606
06/01/2019 10:53:28 *** epoch: 257 ***
06/01/2019 10:53:28 *** training ***
06/01/2019 10:53:29 step: 8453, epoch: 256, batch: 4, loss: 0.025910820811986923, acc: 98.4375, f1: 99.3337875690817, r: 0.7414146684173823
06/01/2019 10:53:29 step: 8458, epoch: 256, batch: 9, loss: 0.003982052206993103, acc: 100.0, f1: 100.0, r: 0.8392577554088018
06/01/2019 10:53:30 step: 8463, epoch: 256, batch: 14, loss: 0.00033858418464660645, acc: 100.0, f1: 100.0, r: 0.7363710802115814
06/01/2019 10:53:30 step: 8468, epoch: 256, batch: 19, loss: 0.0013093426823616028, acc: 100.0, f1: 100.0, r: 0.6863975855665544
06/01/2019 10:53:31 step: 8473, epoch: 256, batch: 24, loss: 0.002834007143974304, acc: 100.0, f1: 100.0, r: 0.7256758815834272
06/01/2019 10:53:31 step: 8478, epoch: 256, batch: 29, loss: 0.0006154254078865051, acc: 100.0, f1: 100.0, r: 0.7176216586544957
06/01/2019 10:53:31 *** evaluating ***
06/01/2019 10:53:32 step: 257, epoch: 256, acc: 58.54700854700855, f1: 25.484333643415724, r: 0.3014738873977313
06/01/2019 10:53:32 *** epoch: 258 ***
06/01/2019 10:53:32 *** training ***
06/01/2019 10:53:32 step: 8486, epoch: 257, batch: 4, loss: 0.02915891259908676, acc: 98.4375, f1: 98.424543946932, r: 0.7896747292888826
06/01/2019 10:53:33 step: 8491, epoch: 257, batch: 9, loss: 0.0013848915696144104, acc: 100.0, f1: 100.0, r: 0.6924379674840652
06/01/2019 10:53:33 step: 8496, epoch: 257, batch: 14, loss: 0.002012297511100769, acc: 100.0, f1: 100.0, r: 0.7048870446238137
06/01/2019 10:53:34 step: 8501, epoch: 257, batch: 19, loss: 0.00411553680896759, acc: 100.0, f1: 100.0, r: 0.6680093026612357
06/01/2019 10:53:34 step: 8506, epoch: 257, batch: 24, loss: 0.0011332407593727112, acc: 100.0, f1: 100.0, r: 0.6811987608572707
06/01/2019 10:53:35 step: 8511, epoch: 257, batch: 29, loss: 0.0046955905854702, acc: 100.0, f1: 100.0, r: 0.7202350130109667
06/01/2019 10:53:35 *** evaluating ***
06/01/2019 10:53:35 step: 258, epoch: 257, acc: 58.54700854700855, f1: 26.952910154920495, r: 0.29965210649517193
06/01/2019 10:53:35 *** epoch: 259 ***
06/01/2019 10:53:35 *** training ***
06/01/2019 10:53:36 step: 8519, epoch: 258, batch: 4, loss: 0.00148068368434906, acc: 100.0, f1: 100.0, r: 0.72897062495096
06/01/2019 10:53:36 step: 8524, epoch: 258, batch: 9, loss: 0.0014999806880950928, acc: 100.0, f1: 100.0, r: 0.7275966956076362
06/01/2019 10:53:37 step: 8529, epoch: 258, batch: 14, loss: 0.001192428171634674, acc: 100.0, f1: 100.0, r: 0.6861487409627489
06/01/2019 10:53:37 step: 8534, epoch: 258, batch: 19, loss: 0.0014788061380386353, acc: 100.0, f1: 100.0, r: 0.7069568459445319
06/01/2019 10:53:38 step: 8539, epoch: 258, batch: 24, loss: 0.0016548484563827515, acc: 100.0, f1: 100.0, r: 0.6505273232946561
06/01/2019 10:53:38 step: 8544, epoch: 258, batch: 29, loss: 0.0007757395505905151, acc: 100.0, f1: 100.0, r: 0.7996096624372094
06/01/2019 10:53:38 *** evaluating ***
06/01/2019 10:53:39 step: 259, epoch: 258, acc: 60.256410256410255, f1: 25.180366465572156, r: 0.3083797434179994
06/01/2019 10:53:39 *** epoch: 260 ***
06/01/2019 10:53:39 *** training ***
06/01/2019 10:53:39 step: 8552, epoch: 259, batch: 4, loss: 0.0012651458382606506, acc: 100.0, f1: 100.0, r: 0.6910037859951635
06/01/2019 10:53:40 step: 8557, epoch: 259, batch: 9, loss: 0.003640606999397278, acc: 100.0, f1: 100.0, r: 0.7788623981513916
06/01/2019 10:53:40 step: 8562, epoch: 259, batch: 14, loss: 0.0029109492897987366, acc: 100.0, f1: 100.0, r: 0.7435293049016422
06/01/2019 10:53:41 step: 8567, epoch: 259, batch: 19, loss: 0.0018904879689216614, acc: 100.0, f1: 100.0, r: 0.7078173466346446
06/01/2019 10:53:41 step: 8572, epoch: 259, batch: 24, loss: 0.009441465139389038, acc: 100.0, f1: 100.0, r: 0.7131704817579506
06/01/2019 10:53:42 step: 8577, epoch: 259, batch: 29, loss: 0.002088680863380432, acc: 100.0, f1: 100.0, r: 0.7438670001478186
06/01/2019 10:53:42 *** evaluating ***
06/01/2019 10:53:42 step: 260, epoch: 259, acc: 60.256410256410255, f1: 25.73396984260073, r: 0.30481255500586535
06/01/2019 10:53:42 *** epoch: 261 ***
06/01/2019 10:53:42 *** training ***
06/01/2019 10:53:43 step: 8585, epoch: 260, batch: 4, loss: 0.0011134520173072815, acc: 100.0, f1: 100.0, r: 0.7177769967043696
06/01/2019 10:53:43 step: 8590, epoch: 260, batch: 9, loss: 0.003131873905658722, acc: 100.0, f1: 100.0, r: 0.6816732094818078
06/01/2019 10:53:44 step: 8595, epoch: 260, batch: 14, loss: 0.02537452057003975, acc: 98.4375, f1: 98.3451536643026, r: 0.7557178921138961
06/01/2019 10:53:44 step: 8600, epoch: 260, batch: 19, loss: 0.001543983817100525, acc: 100.0, f1: 100.0, r: 0.6841776052103873
06/01/2019 10:53:45 step: 8605, epoch: 260, batch: 24, loss: 0.0008008256554603577, acc: 100.0, f1: 100.0, r: 0.7087613395092626
06/01/2019 10:53:45 step: 8610, epoch: 260, batch: 29, loss: 0.00872347503900528, acc: 100.0, f1: 100.0, r: 0.6251420131350126
06/01/2019 10:53:45 *** evaluating ***
06/01/2019 10:53:46 step: 261, epoch: 260, acc: 58.119658119658126, f1: 25.257797270955162, r: 0.3060512105469681
06/01/2019 10:53:46 *** epoch: 262 ***
06/01/2019 10:53:46 *** training ***
06/01/2019 10:53:46 step: 8618, epoch: 261, batch: 4, loss: 0.0014283359050750732, acc: 100.0, f1: 100.0, r: 0.724103770332314
06/01/2019 10:53:47 step: 8623, epoch: 261, batch: 9, loss: 0.0060068219900131226, acc: 100.0, f1: 100.0, r: 0.6920509886470047
06/01/2019 10:53:47 step: 8628, epoch: 261, batch: 14, loss: 0.0005504265427589417, acc: 100.0, f1: 100.0, r: 0.7175774950574358
06/01/2019 10:53:48 step: 8633, epoch: 261, batch: 19, loss: 0.0021112263202667236, acc: 100.0, f1: 100.0, r: 0.6381373935825184
06/01/2019 10:53:48 step: 8638, epoch: 261, batch: 24, loss: 0.00251036137342453, acc: 100.0, f1: 100.0, r: 0.6897803722675369
06/01/2019 10:53:49 step: 8643, epoch: 261, batch: 29, loss: 0.0032954253256320953, acc: 100.0, f1: 100.0, r: 0.7080856581517461
06/01/2019 10:53:49 *** evaluating ***
06/01/2019 10:53:49 step: 262, epoch: 261, acc: 58.97435897435898, f1: 26.476168406962554, r: 0.300863225864019
06/01/2019 10:53:49 *** epoch: 263 ***
06/01/2019 10:53:49 *** training ***
06/01/2019 10:53:50 step: 8651, epoch: 262, batch: 4, loss: 0.0011842027306556702, acc: 100.0, f1: 100.0, r: 0.7156798108797835
06/01/2019 10:53:50 step: 8656, epoch: 262, batch: 9, loss: 0.0010698884725570679, acc: 100.0, f1: 100.0, r: 0.7535341908311042
06/01/2019 10:53:51 step: 8661, epoch: 262, batch: 14, loss: 0.001445382833480835, acc: 100.0, f1: 100.0, r: 0.6433930463144125
06/01/2019 10:53:51 step: 8666, epoch: 262, batch: 19, loss: 0.007088474929332733, acc: 100.0, f1: 100.0, r: 0.7425504028236302
06/01/2019 10:53:52 step: 8671, epoch: 262, batch: 24, loss: 0.0026495158672332764, acc: 100.0, f1: 100.0, r: 0.6754615659334957
06/01/2019 10:53:52 step: 8676, epoch: 262, batch: 29, loss: 0.0013357028365135193, acc: 100.0, f1: 100.0, r: 0.6679377824671295
06/01/2019 10:53:52 *** evaluating ***
06/01/2019 10:53:53 step: 263, epoch: 262, acc: 57.692307692307686, f1: 24.76987832484437, r: 0.30493656738133185
06/01/2019 10:53:53 *** epoch: 264 ***
06/01/2019 10:53:53 *** training ***
06/01/2019 10:53:53 step: 8684, epoch: 263, batch: 4, loss: 0.0028430745005607605, acc: 100.0, f1: 100.0, r: 0.8215731560276315
06/01/2019 10:53:53 step: 8689, epoch: 263, batch: 9, loss: 0.0005841851234436035, acc: 100.0, f1: 100.0, r: 0.7014150334887564
06/01/2019 10:53:54 step: 8694, epoch: 263, batch: 14, loss: 0.00093802809715271, acc: 100.0, f1: 100.0, r: 0.6457785471897394
06/01/2019 10:53:55 step: 8699, epoch: 263, batch: 19, loss: 0.0014602243900299072, acc: 100.0, f1: 100.0, r: 0.6919202142290484
06/01/2019 10:53:55 step: 8704, epoch: 263, batch: 24, loss: 0.0012364014983177185, acc: 100.0, f1: 100.0, r: 0.7175982414454957
06/01/2019 10:53:56 step: 8709, epoch: 263, batch: 29, loss: 0.0019368678331375122, acc: 100.0, f1: 100.0, r: 0.8324700234033763
06/01/2019 10:53:56 *** evaluating ***
06/01/2019 10:53:56 step: 264, epoch: 263, acc: 61.111111111111114, f1: 25.318584612258398, r: 0.2973344851366679
06/01/2019 10:53:56 *** epoch: 265 ***
06/01/2019 10:53:56 *** training ***
06/01/2019 10:53:57 step: 8717, epoch: 264, batch: 4, loss: 0.0013267993927001953, acc: 100.0, f1: 100.0, r: 0.6933669255007816
06/01/2019 10:53:57 step: 8722, epoch: 264, batch: 9, loss: 0.0018161162734031677, acc: 100.0, f1: 100.0, r: 0.6745156690013124
06/01/2019 10:53:57 step: 8727, epoch: 264, batch: 14, loss: 0.0011756494641304016, acc: 100.0, f1: 100.0, r: 0.677793568842846
06/01/2019 10:53:58 step: 8732, epoch: 264, batch: 19, loss: 0.005956739187240601, acc: 100.0, f1: 100.0, r: 0.6650641526589988
06/01/2019 10:53:59 step: 8737, epoch: 264, batch: 24, loss: 0.001857168972492218, acc: 100.0, f1: 100.0, r: 0.775036981886861
06/01/2019 10:53:59 step: 8742, epoch: 264, batch: 29, loss: 0.008642764762043953, acc: 100.0, f1: 100.0, r: 0.6838430513912469
06/01/2019 10:53:59 *** evaluating ***
06/01/2019 10:53:59 step: 265, epoch: 264, acc: 58.97435897435898, f1: 25.429730731227533, r: 0.29713817672279547
06/01/2019 10:53:59 *** epoch: 266 ***
06/01/2019 10:53:59 *** training ***
06/01/2019 10:54:00 step: 8750, epoch: 265, batch: 4, loss: 0.0017278939485549927, acc: 100.0, f1: 100.0, r: 0.77102514090941
06/01/2019 10:54:00 step: 8755, epoch: 265, batch: 9, loss: 0.010848283767700195, acc: 100.0, f1: 100.0, r: 0.8006261180105109
06/01/2019 10:54:01 step: 8760, epoch: 265, batch: 14, loss: 0.009456336498260498, acc: 100.0, f1: 100.0, r: 0.6128915476604623
06/01/2019 10:54:01 step: 8765, epoch: 265, batch: 19, loss: 0.0009274929761886597, acc: 100.0, f1: 100.0, r: 0.669341043154765
06/01/2019 10:54:02 step: 8770, epoch: 265, batch: 24, loss: 0.0026812106370925903, acc: 100.0, f1: 100.0, r: 0.7859515118059093
06/01/2019 10:54:02 step: 8775, epoch: 265, batch: 29, loss: 0.0018449872732162476, acc: 100.0, f1: 100.0, r: 0.839490486268326
06/01/2019 10:54:03 *** evaluating ***
06/01/2019 10:54:03 step: 266, epoch: 265, acc: 58.54700854700855, f1: 25.622461721434842, r: 0.2963548960274785
06/01/2019 10:54:03 *** epoch: 267 ***
06/01/2019 10:54:03 *** training ***
06/01/2019 10:54:03 step: 8783, epoch: 266, batch: 4, loss: 0.002640634775161743, acc: 100.0, f1: 100.0, r: 0.6479496598027047
06/01/2019 10:54:04 step: 8788, epoch: 266, batch: 9, loss: 0.0023370832204818726, acc: 100.0, f1: 100.0, r: 0.8449641770391556
06/01/2019 10:54:04 step: 8793, epoch: 266, batch: 14, loss: 0.003453262150287628, acc: 100.0, f1: 100.0, r: 0.8188681371335241
06/01/2019 10:54:05 step: 8798, epoch: 266, batch: 19, loss: 0.0018717944622039795, acc: 100.0, f1: 100.0, r: 0.7963915510630422
06/01/2019 10:54:05 step: 8803, epoch: 266, batch: 24, loss: 0.0034545212984085083, acc: 100.0, f1: 100.0, r: 0.8064029135303443
06/01/2019 10:54:06 step: 8808, epoch: 266, batch: 29, loss: 0.0018850713968276978, acc: 100.0, f1: 100.0, r: 0.7441708201958422
06/01/2019 10:54:06 *** evaluating ***
06/01/2019 10:54:06 step: 267, epoch: 266, acc: 60.256410256410255, f1: 24.073832427220275, r: 0.3035729955476213
06/01/2019 10:54:06 *** epoch: 268 ***
06/01/2019 10:54:06 *** training ***
06/01/2019 10:54:07 step: 8816, epoch: 267, batch: 4, loss: 0.012539323419332504, acc: 100.0, f1: 100.0, r: 0.8225863265147672
06/01/2019 10:54:07 step: 8821, epoch: 267, batch: 9, loss: 0.0014647617936134338, acc: 100.0, f1: 100.0, r: 0.7934405913904699
06/01/2019 10:54:08 step: 8826, epoch: 267, batch: 14, loss: 0.0024199485778808594, acc: 100.0, f1: 100.0, r: 0.8166079954969045
06/01/2019 10:54:08 step: 8831, epoch: 267, batch: 19, loss: 0.001607269048690796, acc: 100.0, f1: 100.0, r: 0.7624503172514603
06/01/2019 10:54:09 step: 8836, epoch: 267, batch: 24, loss: 0.0011369585990905762, acc: 100.0, f1: 100.0, r: 0.7529542034376235
06/01/2019 10:54:09 step: 8841, epoch: 267, batch: 29, loss: 0.0010312646627426147, acc: 100.0, f1: 100.0, r: 0.8388111174707913
06/01/2019 10:54:10 *** evaluating ***
06/01/2019 10:54:10 step: 268, epoch: 267, acc: 60.256410256410255, f1: 24.459086993970715, r: 0.30457184631051226
06/01/2019 10:54:10 *** epoch: 269 ***
06/01/2019 10:54:10 *** training ***
06/01/2019 10:54:10 step: 8849, epoch: 268, batch: 4, loss: 0.0036885440349578857, acc: 100.0, f1: 100.0, r: 0.6840161969210153
06/01/2019 10:54:11 step: 8854, epoch: 268, batch: 9, loss: 0.001737114042043686, acc: 100.0, f1: 100.0, r: 0.7307311956110891
06/01/2019 10:54:11 step: 8859, epoch: 268, batch: 14, loss: 0.001638360321521759, acc: 100.0, f1: 100.0, r: 0.7000767889615521
06/01/2019 10:54:12 step: 8864, epoch: 268, batch: 19, loss: 0.000806167721748352, acc: 100.0, f1: 100.0, r: 0.7165455803003575
06/01/2019 10:54:12 step: 8869, epoch: 268, batch: 24, loss: 0.002087235450744629, acc: 100.0, f1: 100.0, r: 0.7934807964214533
06/01/2019 10:54:13 step: 8874, epoch: 268, batch: 29, loss: 0.0021000653505325317, acc: 100.0, f1: 100.0, r: 0.6923469618030788
06/01/2019 10:54:13 *** evaluating ***
06/01/2019 10:54:13 step: 269, epoch: 268, acc: 60.256410256410255, f1: 24.466748663557176, r: 0.3029597322222432
06/01/2019 10:54:13 *** epoch: 270 ***
06/01/2019 10:54:13 *** training ***
06/01/2019 10:54:14 step: 8882, epoch: 269, batch: 4, loss: 0.00237293541431427, acc: 100.0, f1: 100.0, r: 0.701871166223888
06/01/2019 10:54:14 step: 8887, epoch: 269, batch: 9, loss: 0.0009298920631408691, acc: 100.0, f1: 100.0, r: 0.8275248307628557
06/01/2019 10:54:15 step: 8892, epoch: 269, batch: 14, loss: 0.005391985177993774, acc: 100.0, f1: 100.0, r: 0.7555157975686847
06/01/2019 10:54:15 step: 8897, epoch: 269, batch: 19, loss: 0.0019280165433883667, acc: 100.0, f1: 100.0, r: 0.6394430808010848
06/01/2019 10:54:16 step: 8902, epoch: 269, batch: 24, loss: 0.003739975392818451, acc: 100.0, f1: 100.0, r: 0.7805871134871853
06/01/2019 10:54:16 step: 8907, epoch: 269, batch: 29, loss: 0.001689910888671875, acc: 100.0, f1: 100.0, r: 0.7046875927117946
06/01/2019 10:54:17 *** evaluating ***
06/01/2019 10:54:17 step: 270, epoch: 269, acc: 59.82905982905983, f1: 27.015858273133812, r: 0.2991489710299524
06/01/2019 10:54:17 *** epoch: 271 ***
06/01/2019 10:54:17 *** training ***
06/01/2019 10:54:17 step: 8915, epoch: 270, batch: 4, loss: 0.004522733390331268, acc: 100.0, f1: 100.0, r: 0.7914517813070774
06/01/2019 10:54:18 step: 8920, epoch: 270, batch: 9, loss: 0.0021170377731323242, acc: 100.0, f1: 100.0, r: 0.7353237070601449
06/01/2019 10:54:18 step: 8925, epoch: 270, batch: 14, loss: 0.00280635803937912, acc: 100.0, f1: 100.0, r: 0.8055009532618989
06/01/2019 10:54:19 step: 8930, epoch: 270, batch: 19, loss: 0.0021922364830970764, acc: 100.0, f1: 100.0, r: 0.824294776245576
06/01/2019 10:54:19 step: 8935, epoch: 270, batch: 24, loss: 0.000947117805480957, acc: 100.0, f1: 100.0, r: 0.6960683579410724
06/01/2019 10:54:20 step: 8940, epoch: 270, batch: 29, loss: 0.0015906542539596558, acc: 100.0, f1: 100.0, r: 0.661198277956109
06/01/2019 10:54:20 *** evaluating ***
06/01/2019 10:54:20 step: 271, epoch: 270, acc: 58.97435897435898, f1: 26.830143654392817, r: 0.29791692145786663
06/01/2019 10:54:20 *** epoch: 272 ***
06/01/2019 10:54:20 *** training ***
06/01/2019 10:54:21 step: 8948, epoch: 271, batch: 4, loss: 0.0014329776167869568, acc: 100.0, f1: 100.0, r: 0.7647869881519381
06/01/2019 10:54:21 step: 8953, epoch: 271, batch: 9, loss: 0.002057686448097229, acc: 100.0, f1: 100.0, r: 0.7813603413982079
06/01/2019 10:54:22 step: 8958, epoch: 271, batch: 14, loss: 0.0030115582048892975, acc: 100.0, f1: 100.0, r: 0.8259381619115005
06/01/2019 10:54:22 step: 8963, epoch: 271, batch: 19, loss: 0.0014480501413345337, acc: 100.0, f1: 100.0, r: 0.738051717472013
06/01/2019 10:54:23 step: 8968, epoch: 271, batch: 24, loss: 0.001625344157218933, acc: 100.0, f1: 100.0, r: 0.686080227639677
06/01/2019 10:54:23 step: 8973, epoch: 271, batch: 29, loss: 0.006003454327583313, acc: 100.0, f1: 100.0, r: 0.8372919841307889
06/01/2019 10:54:23 *** evaluating ***
06/01/2019 10:54:24 step: 272, epoch: 271, acc: 59.401709401709404, f1: 26.05640990285855, r: 0.2984095716635053
06/01/2019 10:54:24 *** epoch: 273 ***
06/01/2019 10:54:24 *** training ***
06/01/2019 10:54:24 step: 8981, epoch: 272, batch: 4, loss: 0.0010107681155204773, acc: 100.0, f1: 100.0, r: 0.7427836123157614
06/01/2019 10:54:25 step: 8986, epoch: 272, batch: 9, loss: 0.0015976428985595703, acc: 100.0, f1: 100.0, r: 0.7149866593200307
06/01/2019 10:54:25 step: 8991, epoch: 272, batch: 14, loss: 0.0009739696979522705, acc: 100.0, f1: 100.0, r: 0.68238826234824
06/01/2019 10:54:26 step: 8996, epoch: 272, batch: 19, loss: 0.0017672628164291382, acc: 100.0, f1: 100.0, r: 0.6684854619597664
06/01/2019 10:54:26 step: 9001, epoch: 272, batch: 24, loss: 0.0033422037959098816, acc: 100.0, f1: 100.0, r: 0.7031462053815856
06/01/2019 10:54:27 step: 9006, epoch: 272, batch: 29, loss: 0.0012854188680648804, acc: 100.0, f1: 100.0, r: 0.8076421369728845
06/01/2019 10:54:27 *** evaluating ***
06/01/2019 10:54:27 step: 273, epoch: 272, acc: 58.54700854700855, f1: 27.37921425050751, r: 0.2910337102099902
06/01/2019 10:54:27 *** epoch: 274 ***
06/01/2019 10:54:27 *** training ***
06/01/2019 10:54:28 step: 9014, epoch: 273, batch: 4, loss: 0.00524003803730011, acc: 100.0, f1: 100.0, r: 0.7418004292803907
06/01/2019 10:54:28 step: 9019, epoch: 273, batch: 9, loss: 0.0006512627005577087, acc: 100.0, f1: 100.0, r: 0.7146044869240404
06/01/2019 10:54:29 step: 9024, epoch: 273, batch: 14, loss: 0.00473591685295105, acc: 100.0, f1: 100.0, r: 0.8133434951372143
06/01/2019 10:54:29 step: 9029, epoch: 273, batch: 19, loss: 0.0013223141431808472, acc: 100.0, f1: 100.0, r: 0.8170276220669956
06/01/2019 10:54:30 step: 9034, epoch: 273, batch: 24, loss: 0.0012820661067962646, acc: 100.0, f1: 100.0, r: 0.841738168009202
06/01/2019 10:54:30 step: 9039, epoch: 273, batch: 29, loss: 0.0008532106876373291, acc: 100.0, f1: 100.0, r: 0.678551120722119
06/01/2019 10:54:30 *** evaluating ***
06/01/2019 10:54:31 step: 274, epoch: 273, acc: 60.256410256410255, f1: 26.276347455199655, r: 0.30160929978158385
06/01/2019 10:54:31 *** epoch: 275 ***
06/01/2019 10:54:31 *** training ***
06/01/2019 10:54:31 step: 9047, epoch: 274, batch: 4, loss: 0.0013501569628715515, acc: 100.0, f1: 100.0, r: 0.7319445408802594
06/01/2019 10:54:32 step: 9052, epoch: 274, batch: 9, loss: 0.001992851495742798, acc: 100.0, f1: 100.0, r: 0.8254119061261438
06/01/2019 10:54:32 step: 9057, epoch: 274, batch: 14, loss: 0.0016158297657966614, acc: 100.0, f1: 100.0, r: 0.7179503666264481
06/01/2019 10:54:33 step: 9062, epoch: 274, batch: 19, loss: 0.0018438547849655151, acc: 100.0, f1: 100.0, r: 0.778014250284148
06/01/2019 10:54:33 step: 9067, epoch: 274, batch: 24, loss: 0.0024843886494636536, acc: 100.0, f1: 100.0, r: 0.6911258874174997
06/01/2019 10:54:34 step: 9072, epoch: 274, batch: 29, loss: 0.00026760995388031006, acc: 100.0, f1: 100.0, r: 0.7094563649958607
06/01/2019 10:54:34 *** evaluating ***
06/01/2019 10:54:34 step: 275, epoch: 274, acc: 58.97435897435898, f1: 26.206935019772576, r: 0.2993045020536298
06/01/2019 10:54:34 *** epoch: 276 ***
06/01/2019 10:54:34 *** training ***
06/01/2019 10:54:35 step: 9080, epoch: 275, batch: 4, loss: 0.0013296008110046387, acc: 100.0, f1: 100.0, r: 0.8321865226995978
06/01/2019 10:54:35 step: 9085, epoch: 275, batch: 9, loss: 0.002315238118171692, acc: 100.0, f1: 100.0, r: 0.6579293891731146
06/01/2019 10:54:36 step: 9090, epoch: 275, batch: 14, loss: 0.021980859339237213, acc: 98.4375, f1: 99.15966386554622, r: 0.8257005145026597
06/01/2019 10:54:36 step: 9095, epoch: 275, batch: 19, loss: 0.001926872879266739, acc: 100.0, f1: 100.0, r: 0.6748568627417496
06/01/2019 10:54:37 step: 9100, epoch: 275, batch: 24, loss: 0.003845013678073883, acc: 100.0, f1: 100.0, r: 0.7102847159162201
06/01/2019 10:54:37 step: 9105, epoch: 275, batch: 29, loss: 0.0012809783220291138, acc: 100.0, f1: 100.0, r: 0.7332578050425962
06/01/2019 10:54:37 *** evaluating ***
06/01/2019 10:54:38 step: 276, epoch: 275, acc: 59.82905982905983, f1: 27.22793834796402, r: 0.302542016054844
06/01/2019 10:54:38 *** epoch: 277 ***
06/01/2019 10:54:38 *** training ***
06/01/2019 10:54:38 step: 9113, epoch: 276, batch: 4, loss: 0.001446053385734558, acc: 100.0, f1: 100.0, r: 0.8037476525834863
06/01/2019 10:54:39 step: 9118, epoch: 276, batch: 9, loss: 0.0013881698250770569, acc: 100.0, f1: 100.0, r: 0.7457434100532586
06/01/2019 10:54:39 step: 9123, epoch: 276, batch: 14, loss: 0.0013280659914016724, acc: 100.0, f1: 100.0, r: 0.6789359742534978
06/01/2019 10:54:40 step: 9128, epoch: 276, batch: 19, loss: 0.00228118896484375, acc: 100.0, f1: 100.0, r: 0.605332861014221
06/01/2019 10:54:40 step: 9133, epoch: 276, batch: 24, loss: 0.0019110366702079773, acc: 100.0, f1: 100.0, r: 0.677372788879662
06/01/2019 10:54:41 step: 9138, epoch: 276, batch: 29, loss: 0.0005793645977973938, acc: 100.0, f1: 100.0, r: 0.703050868422667
06/01/2019 10:54:41 *** evaluating ***
06/01/2019 10:54:41 step: 277, epoch: 276, acc: 58.54700854700855, f1: 27.25130985403674, r: 0.3006686908107631
06/01/2019 10:54:41 *** epoch: 278 ***
06/01/2019 10:54:41 *** training ***
06/01/2019 10:54:41 step: 9146, epoch: 277, batch: 4, loss: 0.0012574493885040283, acc: 100.0, f1: 100.0, r: 0.7142367517941406
06/01/2019 10:54:42 step: 9151, epoch: 277, batch: 9, loss: 0.0008987411856651306, acc: 100.0, f1: 100.0, r: 0.7264912618749085
06/01/2019 10:54:42 step: 9156, epoch: 277, batch: 14, loss: 0.0015982091426849365, acc: 100.0, f1: 100.0, r: 0.6890180504951974
06/01/2019 10:54:43 step: 9161, epoch: 277, batch: 19, loss: 0.0014804229140281677, acc: 100.0, f1: 100.0, r: 0.7492569679515848
06/01/2019 10:54:43 step: 9166, epoch: 277, batch: 24, loss: 0.002500206232070923, acc: 100.0, f1: 100.0, r: 0.6933645472133956
06/01/2019 10:54:44 step: 9171, epoch: 277, batch: 29, loss: 0.011486819013953209, acc: 100.0, f1: 100.0, r: 0.8283313396887078
06/01/2019 10:54:44 *** evaluating ***
06/01/2019 10:54:44 step: 278, epoch: 277, acc: 60.256410256410255, f1: 25.326224429165606, r: 0.30161856483495497
06/01/2019 10:54:44 *** epoch: 279 ***
06/01/2019 10:54:44 *** training ***
06/01/2019 10:54:45 step: 9179, epoch: 278, batch: 4, loss: 0.0010410994291305542, acc: 100.0, f1: 100.0, r: 0.7975510846899059
06/01/2019 10:54:45 step: 9184, epoch: 278, batch: 9, loss: 0.0057695843279361725, acc: 100.0, f1: 100.0, r: 0.7535688355756192
06/01/2019 10:54:46 step: 9189, epoch: 278, batch: 14, loss: 0.001627996563911438, acc: 100.0, f1: 100.0, r: 0.7507189977924482
06/01/2019 10:54:46 step: 9194, epoch: 278, batch: 19, loss: 0.0023192167282104492, acc: 100.0, f1: 100.0, r: 0.8111851540568639
06/01/2019 10:54:47 step: 9199, epoch: 278, batch: 24, loss: 0.0027700215578079224, acc: 100.0, f1: 100.0, r: 0.8388140595983273
06/01/2019 10:54:47 step: 9204, epoch: 278, batch: 29, loss: 0.0016552135348320007, acc: 100.0, f1: 100.0, r: 0.8395578334691363
06/01/2019 10:54:48 *** evaluating ***
06/01/2019 10:54:48 step: 279, epoch: 278, acc: 60.256410256410255, f1: 25.404969426708558, r: 0.30624965188441894
06/01/2019 10:54:48 *** epoch: 280 ***
06/01/2019 10:54:48 *** training ***
06/01/2019 10:54:48 step: 9212, epoch: 279, batch: 4, loss: 0.0015566051006317139, acc: 100.0, f1: 100.0, r: 0.7644299336497873
06/01/2019 10:54:49 step: 9217, epoch: 279, batch: 9, loss: 0.0014109089970588684, acc: 100.0, f1: 100.0, r: 0.7093297751061242
06/01/2019 10:54:49 step: 9222, epoch: 279, batch: 14, loss: 0.0014995485544204712, acc: 100.0, f1: 100.0, r: 0.7497786845604898
06/01/2019 10:54:50 step: 9227, epoch: 279, batch: 19, loss: 0.0013670623302459717, acc: 100.0, f1: 100.0, r: 0.6938249529476848
06/01/2019 10:54:50 step: 9232, epoch: 279, batch: 24, loss: 0.0011524856090545654, acc: 100.0, f1: 100.0, r: 0.8280952815468203
06/01/2019 10:54:51 step: 9237, epoch: 279, batch: 29, loss: 0.016658809036016464, acc: 98.4375, f1: 96.0, r: 0.6216412032734736
06/01/2019 10:54:51 *** evaluating ***
06/01/2019 10:54:51 step: 280, epoch: 279, acc: 61.111111111111114, f1: 26.86656603125505, r: 0.31116028538777063
06/01/2019 10:54:51 *** epoch: 281 ***
06/01/2019 10:54:51 *** training ***
06/01/2019 10:54:52 step: 9245, epoch: 280, batch: 4, loss: 0.0014366358518600464, acc: 100.0, f1: 100.0, r: 0.801974374025066
06/01/2019 10:54:52 step: 9250, epoch: 280, batch: 9, loss: 0.003285318613052368, acc: 100.0, f1: 100.0, r: 0.7085552276809558
06/01/2019 10:54:53 step: 9255, epoch: 280, batch: 14, loss: 0.0011868104338645935, acc: 100.0, f1: 100.0, r: 0.7191360386613312
06/01/2019 10:54:53 step: 9260, epoch: 280, batch: 19, loss: 0.01552625373005867, acc: 98.4375, f1: 99.3661100803958, r: 0.7835733547950142
06/01/2019 10:54:54 step: 9265, epoch: 280, batch: 24, loss: 0.0017107948660850525, acc: 100.0, f1: 100.0, r: 0.7619003025269354
06/01/2019 10:54:54 step: 9270, epoch: 280, batch: 29, loss: 0.0013577118515968323, acc: 100.0, f1: 100.0, r: 0.6544781248070594
06/01/2019 10:54:55 *** evaluating ***
06/01/2019 10:54:55 step: 281, epoch: 280, acc: 59.82905982905983, f1: 28.42027095654037, r: 0.2947825136531579
06/01/2019 10:54:55 *** epoch: 282 ***
06/01/2019 10:54:55 *** training ***
06/01/2019 10:54:55 step: 9278, epoch: 281, batch: 4, loss: 0.001118391752243042, acc: 100.0, f1: 100.0, r: 0.8124875577638726
06/01/2019 10:54:56 step: 9283, epoch: 281, batch: 9, loss: 0.0025850459933280945, acc: 100.0, f1: 100.0, r: 0.6641779372614638
06/01/2019 10:54:56 step: 9288, epoch: 281, batch: 14, loss: 0.0008456632494926453, acc: 100.0, f1: 100.0, r: 0.8108943681028117
06/01/2019 10:54:57 step: 9293, epoch: 281, batch: 19, loss: 0.0014866366982460022, acc: 100.0, f1: 100.0, r: 0.7570088635136468
06/01/2019 10:54:57 step: 9298, epoch: 281, batch: 24, loss: 0.0021947622299194336, acc: 100.0, f1: 100.0, r: 0.6833399547764395
06/01/2019 10:54:58 step: 9303, epoch: 281, batch: 29, loss: 0.0031830966472625732, acc: 100.0, f1: 100.0, r: 0.8161240765715401
06/01/2019 10:54:58 *** evaluating ***
06/01/2019 10:54:58 step: 282, epoch: 281, acc: 60.68376068376068, f1: 27.244708994708994, r: 0.30430774185744935
06/01/2019 10:54:58 *** epoch: 283 ***
06/01/2019 10:54:58 *** training ***
06/01/2019 10:54:59 step: 9311, epoch: 282, batch: 4, loss: 0.0028337612748146057, acc: 100.0, f1: 100.0, r: 0.836151894361388
06/01/2019 10:54:59 step: 9316, epoch: 282, batch: 9, loss: 0.0014565736055374146, acc: 100.0, f1: 100.0, r: 0.761386939162442
06/01/2019 10:55:00 step: 9321, epoch: 282, batch: 14, loss: 0.0004906579852104187, acc: 100.0, f1: 100.0, r: 0.73690833228692
06/01/2019 10:55:00 step: 9326, epoch: 282, batch: 19, loss: 0.0010034218430519104, acc: 100.0, f1: 100.0, r: 0.8047491534740118
06/01/2019 10:55:01 step: 9331, epoch: 282, batch: 24, loss: 0.002933800220489502, acc: 100.0, f1: 100.0, r: 0.7096846806922239
06/01/2019 10:55:01 step: 9336, epoch: 282, batch: 29, loss: 0.001789391040802002, acc: 100.0, f1: 100.0, r: 0.7175535539196504
06/01/2019 10:55:02 *** evaluating ***
06/01/2019 10:55:02 step: 283, epoch: 282, acc: 61.111111111111114, f1: 26.570126570126572, r: 0.30499898636096473
06/01/2019 10:55:02 *** epoch: 284 ***
06/01/2019 10:55:02 *** training ***
06/01/2019 10:55:02 step: 9344, epoch: 283, batch: 4, loss: 0.001924850046634674, acc: 100.0, f1: 100.0, r: 0.7058297708559179
06/01/2019 10:55:03 step: 9349, epoch: 283, batch: 9, loss: 0.00132809579372406, acc: 100.0, f1: 100.0, r: 0.8070025828485762
06/01/2019 10:55:03 step: 9354, epoch: 283, batch: 14, loss: 0.003023594617843628, acc: 100.0, f1: 100.0, r: 0.6529963570548629
06/01/2019 10:55:04 step: 9359, epoch: 283, batch: 19, loss: 0.008472234010696411, acc: 100.0, f1: 100.0, r: 0.6713957622133081
06/01/2019 10:55:04 step: 9364, epoch: 283, batch: 24, loss: 0.006413862109184265, acc: 100.0, f1: 100.0, r: 0.7016668279236358
06/01/2019 10:55:05 step: 9369, epoch: 283, batch: 29, loss: 0.0015096589922904968, acc: 100.0, f1: 100.0, r: 0.7676823128257878
06/01/2019 10:55:05 *** evaluating ***
06/01/2019 10:55:05 step: 284, epoch: 283, acc: 59.401709401709404, f1: 27.366833448889217, r: 0.2937534891331274
06/01/2019 10:55:05 *** epoch: 285 ***
06/01/2019 10:55:05 *** training ***
06/01/2019 10:55:06 step: 9377, epoch: 284, batch: 4, loss: 0.0010138079524040222, acc: 100.0, f1: 100.0, r: 0.7128187460754789
06/01/2019 10:55:06 step: 9382, epoch: 284, batch: 9, loss: 0.000984877347946167, acc: 100.0, f1: 100.0, r: 0.725345497795952
06/01/2019 10:55:07 step: 9387, epoch: 284, batch: 14, loss: 0.009667754173278809, acc: 100.0, f1: 100.0, r: 0.7723129190942094
06/01/2019 10:55:07 step: 9392, epoch: 284, batch: 19, loss: 0.0007256120443344116, acc: 100.0, f1: 100.0, r: 0.7276677367786646
06/01/2019 10:55:08 step: 9397, epoch: 284, batch: 24, loss: 0.002888064831495285, acc: 100.0, f1: 100.0, r: 0.7128167578782838
06/01/2019 10:55:08 step: 9402, epoch: 284, batch: 29, loss: 0.007738009095191956, acc: 100.0, f1: 100.0, r: 0.5282314133838588
06/01/2019 10:55:09 *** evaluating ***
06/01/2019 10:55:09 step: 285, epoch: 284, acc: 58.54700854700855, f1: 26.785403500884676, r: 0.28578826227435067
06/01/2019 10:55:09 *** epoch: 286 ***
06/01/2019 10:55:09 *** training ***
06/01/2019 10:55:09 step: 9410, epoch: 285, batch: 4, loss: 0.0016894638538360596, acc: 100.0, f1: 100.0, r: 0.6982768925858844
06/01/2019 10:55:10 step: 9415, epoch: 285, batch: 9, loss: 0.002860456705093384, acc: 100.0, f1: 100.0, r: 0.7889767049656753
06/01/2019 10:55:10 step: 9420, epoch: 285, batch: 14, loss: 0.000993870198726654, acc: 100.0, f1: 100.0, r: 0.7182525447080761
06/01/2019 10:55:11 step: 9425, epoch: 285, batch: 19, loss: 0.034504808485507965, acc: 98.4375, f1: 99.22222222222223, r: 0.7683613597191683
06/01/2019 10:55:11 step: 9430, epoch: 285, batch: 24, loss: 0.0006719455122947693, acc: 100.0, f1: 100.0, r: 0.8053927570804146
06/01/2019 10:55:12 step: 9435, epoch: 285, batch: 29, loss: 0.0006681308150291443, acc: 100.0, f1: 100.0, r: 0.7134283130189503
06/01/2019 10:55:12 *** evaluating ***
06/01/2019 10:55:12 step: 286, epoch: 285, acc: 60.68376068376068, f1: 25.235571618639664, r: 0.3013221830550997
06/01/2019 10:55:12 *** epoch: 287 ***
06/01/2019 10:55:12 *** training ***
06/01/2019 10:55:13 step: 9443, epoch: 286, batch: 4, loss: 0.0007628276944160461, acc: 100.0, f1: 100.0, r: 0.7749693607292363
06/01/2019 10:55:13 step: 9448, epoch: 286, batch: 9, loss: 0.0011813268065452576, acc: 100.0, f1: 100.0, r: 0.763243951926585
06/01/2019 10:55:14 step: 9453, epoch: 286, batch: 14, loss: 0.0008053630590438843, acc: 100.0, f1: 100.0, r: 0.7975766154602372
06/01/2019 10:55:14 step: 9458, epoch: 286, batch: 19, loss: 0.0013705715537071228, acc: 100.0, f1: 100.0, r: 0.7363782744342329
06/01/2019 10:55:15 step: 9463, epoch: 286, batch: 24, loss: 0.001135110855102539, acc: 100.0, f1: 100.0, r: 0.6411848941281578
06/01/2019 10:55:15 step: 9468, epoch: 286, batch: 29, loss: 0.005233775824308395, acc: 100.0, f1: 100.0, r: 0.7020345055469188
06/01/2019 10:55:16 *** evaluating ***
06/01/2019 10:55:16 step: 287, epoch: 286, acc: 59.401709401709404, f1: 26.565342994768837, r: 0.29091155962861687
06/01/2019 10:55:16 *** epoch: 288 ***
06/01/2019 10:55:16 *** training ***
06/01/2019 10:55:16 step: 9476, epoch: 287, batch: 4, loss: 0.0011171996593475342, acc: 100.0, f1: 100.0, r: 0.8022653890339678
06/01/2019 10:55:17 step: 9481, epoch: 287, batch: 9, loss: 0.001400083303451538, acc: 100.0, f1: 100.0, r: 0.7142984346591571
06/01/2019 10:55:17 step: 9486, epoch: 287, batch: 14, loss: 0.0031730756163597107, acc: 100.0, f1: 100.0, r: 0.7087983976010291
06/01/2019 10:55:18 step: 9491, epoch: 287, batch: 19, loss: 0.001653127372264862, acc: 100.0, f1: 100.0, r: 0.7538578847783011
06/01/2019 10:55:18 step: 9496, epoch: 287, batch: 24, loss: 0.0013292208313941956, acc: 100.0, f1: 100.0, r: 0.7570483991911859
06/01/2019 10:55:19 step: 9501, epoch: 287, batch: 29, loss: 0.01073283702135086, acc: 100.0, f1: 100.0, r: 0.7863402643768993
06/01/2019 10:55:19 *** evaluating ***
06/01/2019 10:55:19 step: 288, epoch: 287, acc: 59.401709401709404, f1: 26.636576007921036, r: 0.2875664809738902
06/01/2019 10:55:19 *** epoch: 289 ***
06/01/2019 10:55:19 *** training ***
06/01/2019 10:55:20 step: 9509, epoch: 288, batch: 4, loss: 0.001126408576965332, acc: 100.0, f1: 100.0, r: 0.6941593960583706
06/01/2019 10:55:20 step: 9514, epoch: 288, batch: 9, loss: 0.0011638253927230835, acc: 100.0, f1: 100.0, r: 0.7108044132665341
06/01/2019 10:55:21 step: 9519, epoch: 288, batch: 14, loss: 0.0013039186596870422, acc: 100.0, f1: 100.0, r: 0.723471093784922
06/01/2019 10:55:21 step: 9524, epoch: 288, batch: 19, loss: 0.0014065727591514587, acc: 100.0, f1: 100.0, r: 0.7079184856440044
06/01/2019 10:55:22 step: 9529, epoch: 288, batch: 24, loss: 0.0012383311986923218, acc: 100.0, f1: 100.0, r: 0.7377383332107255
06/01/2019 10:55:22 step: 9534, epoch: 288, batch: 29, loss: 0.0018882602453231812, acc: 100.0, f1: 100.0, r: 0.7285372451778773
06/01/2019 10:55:23 *** evaluating ***
06/01/2019 10:55:23 step: 289, epoch: 288, acc: 58.97435897435898, f1: 25.905822075176914, r: 0.29503931145230566
06/01/2019 10:55:23 *** epoch: 290 ***
06/01/2019 10:55:23 *** training ***
06/01/2019 10:55:23 step: 9542, epoch: 289, batch: 4, loss: 0.0009094029664993286, acc: 100.0, f1: 100.0, r: 0.7734165880191407
06/01/2019 10:55:24 step: 9547, epoch: 289, batch: 9, loss: 0.004030268639326096, acc: 100.0, f1: 100.0, r: 0.6757313924374515
06/01/2019 10:55:24 step: 9552, epoch: 289, batch: 14, loss: 0.0013691186904907227, acc: 100.0, f1: 100.0, r: 0.7735522475695435
06/01/2019 10:55:25 step: 9557, epoch: 289, batch: 19, loss: 0.0009597763419151306, acc: 100.0, f1: 100.0, r: 0.7356110752672211
06/01/2019 10:55:25 step: 9562, epoch: 289, batch: 24, loss: 0.0016640573740005493, acc: 100.0, f1: 100.0, r: 0.7527264227135263
06/01/2019 10:55:26 step: 9567, epoch: 289, batch: 29, loss: 0.0011431872844696045, acc: 100.0, f1: 100.0, r: 0.6886975630832046
06/01/2019 10:55:26 *** evaluating ***
06/01/2019 10:55:26 step: 290, epoch: 289, acc: 59.82905982905983, f1: 26.624640859628368, r: 0.2958371317185883
06/01/2019 10:55:26 *** epoch: 291 ***
06/01/2019 10:55:26 *** training ***
06/01/2019 10:55:27 step: 9575, epoch: 290, batch: 4, loss: 0.0031495466828346252, acc: 100.0, f1: 100.0, r: 0.8347876508966591
06/01/2019 10:55:27 step: 9580, epoch: 290, batch: 9, loss: 0.002814777195453644, acc: 100.0, f1: 100.0, r: 0.6814367568456648
06/01/2019 10:55:28 step: 9585, epoch: 290, batch: 14, loss: 0.006667658686637878, acc: 100.0, f1: 100.0, r: 0.7315500932806421
06/01/2019 10:55:28 step: 9590, epoch: 290, batch: 19, loss: 0.0024810805916786194, acc: 100.0, f1: 100.0, r: 0.7425209604377646
06/01/2019 10:55:29 step: 9595, epoch: 290, batch: 24, loss: 0.0015227347612380981, acc: 100.0, f1: 100.0, r: 0.7813813540688749
06/01/2019 10:55:29 step: 9600, epoch: 290, batch: 29, loss: 0.0017575696110725403, acc: 100.0, f1: 100.0, r: 0.8206276394896599
06/01/2019 10:55:30 *** evaluating ***
06/01/2019 10:55:30 step: 291, epoch: 290, acc: 59.401709401709404, f1: 26.663760251147878, r: 0.29390416233278427
06/01/2019 10:55:30 *** epoch: 292 ***
06/01/2019 10:55:30 *** training ***
06/01/2019 10:55:30 step: 9608, epoch: 291, batch: 4, loss: 0.0010441020131111145, acc: 100.0, f1: 100.0, r: 0.5785392129520351
06/01/2019 10:55:31 step: 9613, epoch: 291, batch: 9, loss: 0.0031375139951705933, acc: 100.0, f1: 100.0, r: 0.8086575452386684
06/01/2019 10:55:31 step: 9618, epoch: 291, batch: 14, loss: 0.0012383535504341125, acc: 100.0, f1: 100.0, r: 0.7560524233245767
06/01/2019 10:55:32 step: 9623, epoch: 291, batch: 19, loss: 0.0025300681591033936, acc: 100.0, f1: 100.0, r: 0.6587901890025827
06/01/2019 10:55:32 step: 9628, epoch: 291, batch: 24, loss: 0.003091204911470413, acc: 100.0, f1: 100.0, r: 0.7997413498048969
06/01/2019 10:55:33 step: 9633, epoch: 291, batch: 29, loss: 0.0007292106747627258, acc: 100.0, f1: 100.0, r: 0.6991971038995373
06/01/2019 10:55:33 *** evaluating ***
06/01/2019 10:55:33 step: 292, epoch: 291, acc: 59.82905982905983, f1: 26.637455111931985, r: 0.2982958796041365
06/01/2019 10:55:33 *** epoch: 293 ***
06/01/2019 10:55:33 *** training ***
06/01/2019 10:55:34 step: 9641, epoch: 292, batch: 4, loss: 0.0007047206163406372, acc: 100.0, f1: 100.0, r: 0.8370626223557435
06/01/2019 10:55:34 step: 9646, epoch: 292, batch: 9, loss: 0.002348650246858597, acc: 100.0, f1: 100.0, r: 0.7924568990366573
06/01/2019 10:55:35 step: 9651, epoch: 292, batch: 14, loss: 0.0029798299074172974, acc: 100.0, f1: 100.0, r: 0.7843017462461327
06/01/2019 10:55:35 step: 9656, epoch: 292, batch: 19, loss: 0.0038893036544322968, acc: 100.0, f1: 100.0, r: 0.5642186395597354
06/01/2019 10:55:36 step: 9661, epoch: 292, batch: 24, loss: 0.003460921347141266, acc: 100.0, f1: 100.0, r: 0.7147540301457107
06/01/2019 10:55:36 step: 9666, epoch: 292, batch: 29, loss: 0.0013843849301338196, acc: 100.0, f1: 100.0, r: 0.6163599165797462
06/01/2019 10:55:36 *** evaluating ***
06/01/2019 10:55:37 step: 293, epoch: 292, acc: 61.111111111111114, f1: 26.566707786082976, r: 0.2964291742508547
06/01/2019 10:55:37 *** epoch: 294 ***
06/01/2019 10:55:37 *** training ***
06/01/2019 10:55:37 step: 9674, epoch: 293, batch: 4, loss: 0.0007707849144935608, acc: 100.0, f1: 100.0, r: 0.7215921326051643
06/01/2019 10:55:38 step: 9679, epoch: 293, batch: 9, loss: 0.00341125950217247, acc: 100.0, f1: 100.0, r: 0.6826155661269911
06/01/2019 10:55:38 step: 9684, epoch: 293, batch: 14, loss: 0.0010523051023483276, acc: 100.0, f1: 100.0, r: 0.6711904148946772
06/01/2019 10:55:39 step: 9689, epoch: 293, batch: 19, loss: 0.0009703561663627625, acc: 100.0, f1: 100.0, r: 0.7087061117013447
06/01/2019 10:55:39 step: 9694, epoch: 293, batch: 24, loss: 0.0024986639618873596, acc: 100.0, f1: 100.0, r: 0.7575206699260666
06/01/2019 10:55:40 step: 9699, epoch: 293, batch: 29, loss: 0.0014453157782554626, acc: 100.0, f1: 100.0, r: 0.7616114105465583
06/01/2019 10:55:40 *** evaluating ***
06/01/2019 10:55:40 step: 294, epoch: 293, acc: 59.82905982905983, f1: 25.75577455164611, r: 0.3003412358567998
06/01/2019 10:55:40 *** epoch: 295 ***
06/01/2019 10:55:40 *** training ***
06/01/2019 10:55:41 step: 9707, epoch: 294, batch: 4, loss: 0.002043120563030243, acc: 100.0, f1: 100.0, r: 0.7001466506987281
06/01/2019 10:55:41 step: 9712, epoch: 294, batch: 9, loss: 0.0006140768527984619, acc: 100.0, f1: 100.0, r: 0.5959699699781649
06/01/2019 10:55:42 step: 9717, epoch: 294, batch: 14, loss: 0.0014188513159751892, acc: 100.0, f1: 100.0, r: 0.7253445939417105
06/01/2019 10:55:42 step: 9722, epoch: 294, batch: 19, loss: 0.010160524398088455, acc: 100.0, f1: 100.0, r: 0.8305602066543746
06/01/2019 10:55:43 step: 9727, epoch: 294, batch: 24, loss: 0.0022402554750442505, acc: 100.0, f1: 100.0, r: 0.7921776764990015
06/01/2019 10:55:43 step: 9732, epoch: 294, batch: 29, loss: 0.0012214258313179016, acc: 100.0, f1: 100.0, r: 0.794977057494314
06/01/2019 10:55:43 *** evaluating ***
06/01/2019 10:55:44 step: 295, epoch: 294, acc: 60.256410256410255, f1: 26.888085469741817, r: 0.29078386142446555
06/01/2019 10:55:44 *** epoch: 296 ***
06/01/2019 10:55:44 *** training ***
06/01/2019 10:55:44 step: 9740, epoch: 295, batch: 4, loss: 0.0012019723653793335, acc: 100.0, f1: 100.0, r: 0.7530793475589724
06/01/2019 10:55:44 step: 9745, epoch: 295, batch: 9, loss: 0.0006509944796562195, acc: 100.0, f1: 100.0, r: 0.6624287749334857
06/01/2019 10:55:45 step: 9750, epoch: 295, batch: 14, loss: 0.0017404407262802124, acc: 100.0, f1: 100.0, r: 0.7920341262637163
06/01/2019 10:55:45 step: 9755, epoch: 295, batch: 19, loss: 0.003916289657354355, acc: 100.0, f1: 100.0, r: 0.7732382431959338
06/01/2019 10:55:46 step: 9760, epoch: 295, batch: 24, loss: 0.0011339113116264343, acc: 100.0, f1: 100.0, r: 0.7318288550757017
06/01/2019 10:55:46 step: 9765, epoch: 295, batch: 29, loss: 0.0011250823736190796, acc: 100.0, f1: 100.0, r: 0.7941488991960631
06/01/2019 10:55:47 *** evaluating ***
06/01/2019 10:55:47 step: 296, epoch: 295, acc: 60.256410256410255, f1: 26.16023300629703, r: 0.29826338698716054
06/01/2019 10:55:47 *** epoch: 297 ***
06/01/2019 10:55:47 *** training ***
06/01/2019 10:55:47 step: 9773, epoch: 296, batch: 4, loss: 0.0005012825131416321, acc: 100.0, f1: 100.0, r: 0.6968123157250916
06/01/2019 10:55:48 step: 9778, epoch: 296, batch: 9, loss: 0.004145406186580658, acc: 100.0, f1: 100.0, r: 0.6357984770383365
06/01/2019 10:55:48 step: 9783, epoch: 296, batch: 14, loss: 0.0008186846971511841, acc: 100.0, f1: 100.0, r: 0.7724129856134133
06/01/2019 10:55:49 step: 9788, epoch: 296, batch: 19, loss: 0.0018526837229728699, acc: 100.0, f1: 100.0, r: 0.7938119172386215
06/01/2019 10:55:49 step: 9793, epoch: 296, batch: 24, loss: 0.0023291409015655518, acc: 100.0, f1: 100.0, r: 0.726809790766304
06/01/2019 10:55:50 step: 9798, epoch: 296, batch: 29, loss: 0.00085478276014328, acc: 100.0, f1: 100.0, r: 0.7003767551564176
06/01/2019 10:55:50 *** evaluating ***
06/01/2019 10:55:50 step: 297, epoch: 296, acc: 59.82905982905983, f1: 26.958490937718665, r: 0.2936814468131074
06/01/2019 10:55:50 *** epoch: 298 ***
06/01/2019 10:55:50 *** training ***
06/01/2019 10:55:51 step: 9806, epoch: 297, batch: 4, loss: 0.0025274716317653656, acc: 100.0, f1: 100.0, r: 0.751741883854492
06/01/2019 10:55:51 step: 9811, epoch: 297, batch: 9, loss: 0.006114162504673004, acc: 100.0, f1: 100.0, r: 0.8144619969035868
06/01/2019 10:55:52 step: 9816, epoch: 297, batch: 14, loss: 0.0027168989181518555, acc: 100.0, f1: 100.0, r: 0.6798425599914887
06/01/2019 10:55:52 step: 9821, epoch: 297, batch: 19, loss: 0.0012650415301322937, acc: 100.0, f1: 100.0, r: 0.7013330604777676
06/01/2019 10:55:53 step: 9826, epoch: 297, batch: 24, loss: 0.0029505863785743713, acc: 100.0, f1: 100.0, r: 0.7229946712309441
06/01/2019 10:55:53 step: 9831, epoch: 297, batch: 29, loss: 0.08878792822360992, acc: 95.3125, f1: 84.9136035080082, r: 0.7561476357126641
06/01/2019 10:55:53 *** evaluating ***
06/01/2019 10:55:54 step: 298, epoch: 297, acc: 60.256410256410255, f1: 24.59292412617221, r: 0.28259140136056715
06/01/2019 10:55:54 *** epoch: 299 ***
06/01/2019 10:55:54 *** training ***
06/01/2019 10:55:54 step: 9839, epoch: 298, batch: 4, loss: 0.06961829215288162, acc: 96.875, f1: 94.8268921095008, r: 0.7838416071623987
06/01/2019 10:55:55 step: 9844, epoch: 298, batch: 9, loss: 0.007691383361816406, acc: 100.0, f1: 100.0, r: 0.717770333828002
06/01/2019 10:55:55 step: 9849, epoch: 298, batch: 14, loss: 0.03689877688884735, acc: 98.4375, f1: 99.2372234935164, r: 0.7816946362415397
06/01/2019 10:55:56 step: 9854, epoch: 298, batch: 19, loss: 0.0038441941142082214, acc: 100.0, f1: 100.0, r: 0.7564447659429705
06/01/2019 10:55:56 step: 9859, epoch: 298, batch: 24, loss: 0.006177898496389389, acc: 100.0, f1: 100.0, r: 0.7421043601054668
06/01/2019 10:55:56 step: 9864, epoch: 298, batch: 29, loss: 0.009939629584550858, acc: 100.0, f1: 100.0, r: 0.7967402968184765
06/01/2019 10:55:57 *** evaluating ***
06/01/2019 10:55:57 step: 299, epoch: 298, acc: 59.82905982905983, f1: 25.313915208458553, r: 0.28746055108236934
06/01/2019 10:55:57 *** epoch: 300 ***
06/01/2019 10:55:57 *** training ***
06/01/2019 10:55:57 step: 9872, epoch: 299, batch: 4, loss: 0.003995724022388458, acc: 100.0, f1: 100.0, r: 0.6763276959520584
06/01/2019 10:55:58 step: 9877, epoch: 299, batch: 9, loss: 0.0032404959201812744, acc: 100.0, f1: 100.0, r: 0.7925759775863682
06/01/2019 10:55:58 step: 9882, epoch: 299, batch: 14, loss: 0.005645029246807098, acc: 100.0, f1: 100.0, r: 0.7007308652123797
06/01/2019 10:55:59 step: 9887, epoch: 299, batch: 19, loss: 0.0062532201409339905, acc: 100.0, f1: 100.0, r: 0.8219043867999218
06/01/2019 10:55:59 step: 9892, epoch: 299, batch: 24, loss: 0.008711151778697968, acc: 100.0, f1: 100.0, r: 0.7463053154334723
06/01/2019 10:56:00 step: 9897, epoch: 299, batch: 29, loss: 0.0053402818739414215, acc: 100.0, f1: 100.0, r: 0.8057174573671747
06/01/2019 10:56:00 *** evaluating ***
06/01/2019 10:56:00 step: 300, epoch: 299, acc: 60.256410256410255, f1: 25.008285372803517, r: 0.29433942082148074
06/01/2019 10:56:00 
*** Best acc model ***
epoch: 101
acc: 63.24786324786324
f1: 30.69487681753238
corr: 0.3063586010517658
06/01/2019 10:56:00 Loading Test Data
06/01/2019 10:56:00 load data from data/word2vec_temp/test_text.npy, data/word2vec_temp/test_label.npy, training: False
06/01/2019 10:56:24 loaded. total len: 2228
06/01/2019 10:56:24 Test: length: 2228, total batch: 35, batch size: 64
06/01/2019 10:56:25 
*** Test Result ***
acc: 60.256410256410255
f1: 25.008285372803517
corr: 0.29433942082148074
