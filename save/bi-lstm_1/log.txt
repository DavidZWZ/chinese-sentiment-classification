06/01/2019 10:43:47 {'input_path': 'data/word2vec_temp', 'output_path': 'save/bi-lstm_1', 'gpu': True, 'seed': 20000125, 'display_per_batch': 5, 'optimizer': 'adagrad', 'lr': 0.01, 'lr_decay': 0, 'weight_decay': 0.0001, 'momentum': 0.985, 'num_epochs': 300, 'batch_size': 64, 'num_labels': 8, 'embedding_size': 300, 'type': 'rnn', 'rnn': {'type': 'lstm', 'bidirectional': True, 'rnn_hidden_size': 256, 'mlp_hidden_size': 512, 'dropout': 0.5, 'p_coefficient': 1, 'num_layers': 1, 'param_da': 350, 'param_r': 30, 'loss': 'cross_entropy'}}
06/01/2019 10:43:47 Loading Train Data
06/01/2019 10:43:47 load data from data/word2vec_temp/train_text.npy, data/word2vec_temp/train_label.npy, training: True
06/01/2019 10:44:09 loaded. total len: 2342
06/01/2019 10:44:09 Train: length: 2108, total batch: 33, batch size: 64
06/01/2019 10:44:09 Dev: length: 234, total batch: 4, batch size: 64
06/01/2019 10:44:09 Loading model rnn
06/01/2019 10:44:18 *** epoch: 1 ***
06/01/2019 10:44:18 *** training ***
06/01/2019 10:44:19 step: 5, epoch: 0, batch: 4, loss: 11.989387512207031, acc: 10.9375, f1: 8.71764290131637, r: 0.05917257262317982
06/01/2019 10:44:21 step: 10, epoch: 0, batch: 9, loss: 7.011277675628662, acc: 26.5625, f1: 16.94871794871795, r: 0.09034828894381997
06/01/2019 10:44:22 step: 15, epoch: 0, batch: 14, loss: 4.945207595825195, acc: 50.0, f1: 13.821069527751964, r: 0.055953360466142035
06/01/2019 10:44:23 step: 20, epoch: 0, batch: 19, loss: 5.254104137420654, acc: 40.625, f1: 14.263531499556345, r: 0.07304211310430826
06/01/2019 10:44:24 step: 25, epoch: 0, batch: 24, loss: 4.274068832397461, acc: 43.75, f1: 7.6923076923076925, r: 0.03142385760362834
06/01/2019 10:44:25 step: 30, epoch: 0, batch: 29, loss: 4.7981061935424805, acc: 18.75, f1: 4.10958904109589, r: 0.0497328702508842
06/01/2019 10:44:26 *** evaluating ***
06/01/2019 10:44:26 step: 1, epoch: 0, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.28680897692913215
06/01/2019 10:44:26 *** epoch: 2 ***
06/01/2019 10:44:26 *** training ***
06/01/2019 10:44:27 step: 38, epoch: 1, batch: 4, loss: 4.130321502685547, acc: 34.375, f1: 7.698863636363637, r: 0.14294187252514426
06/01/2019 10:44:28 step: 43, epoch: 1, batch: 9, loss: 3.8988218307495117, acc: 40.625, f1: 12.343250553233341, r: 0.1145312879467386
06/01/2019 10:44:29 step: 48, epoch: 1, batch: 14, loss: 4.03261661529541, acc: 31.25, f1: 7.334785766158316, r: 0.08204420024130492
06/01/2019 10:44:31 step: 53, epoch: 1, batch: 19, loss: 3.7544026374816895, acc: 40.625, f1: 10.476190476190478, r: 0.18977511739171538
06/01/2019 10:44:32 step: 58, epoch: 1, batch: 24, loss: 3.511274814605713, acc: 51.5625, f1: 9.821428571428571, r: 0.10806582855783473
06/01/2019 10:44:33 step: 63, epoch: 1, batch: 29, loss: 4.1380767822265625, acc: 29.6875, f1: 10.769980506822613, r: 0.15126029506250885
06/01/2019 10:44:34 *** evaluating ***
06/01/2019 10:44:34 step: 2, epoch: 1, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.2895927492721508
06/01/2019 10:44:34 *** epoch: 3 ***
06/01/2019 10:44:34 *** training ***
06/01/2019 10:44:35 step: 71, epoch: 2, batch: 4, loss: 3.8992996215820312, acc: 39.0625, f1: 13.329005734069025, r: 0.24063865590180566
06/01/2019 10:44:36 step: 76, epoch: 2, batch: 9, loss: 3.8306615352630615, acc: 43.75, f1: 12.209302325581394, r: 0.20990748587965738
06/01/2019 10:44:37 step: 81, epoch: 2, batch: 14, loss: 3.249397039413452, acc: 48.4375, f1: 15.279958137100994, r: 0.2567014747504268
06/01/2019 10:44:38 step: 86, epoch: 2, batch: 19, loss: 3.8829023838043213, acc: 50.0, f1: 14.919786096256685, r: 0.15899181611218177
06/01/2019 10:44:39 step: 91, epoch: 2, batch: 24, loss: 3.1638851165771484, acc: 40.625, f1: 11.79193899782135, r: 0.23093161476947072
06/01/2019 10:44:40 step: 96, epoch: 2, batch: 29, loss: 3.4807095527648926, acc: 56.25, f1: 19.288728149487643, r: 0.19708077127592621
06/01/2019 10:44:41 *** evaluating ***
06/01/2019 10:44:41 step: 3, epoch: 2, acc: 55.55555555555556, f1: 15.841307421656637, r: 0.30065200087332805
06/01/2019 10:44:41 *** epoch: 4 ***
06/01/2019 10:44:41 *** training ***
06/01/2019 10:44:42 step: 104, epoch: 3, batch: 4, loss: 3.00006365776062, acc: 51.5625, f1: 17.743993371996687, r: 0.27405555488182504
06/01/2019 10:44:43 step: 109, epoch: 3, batch: 9, loss: 4.1677021980285645, acc: 51.5625, f1: 17.473056638385692, r: 0.3255437394409774
06/01/2019 10:44:44 step: 114, epoch: 3, batch: 14, loss: 3.3203535079956055, acc: 46.875, f1: 16.117216117216117, r: 0.19764165113979199
06/01/2019 10:44:45 step: 119, epoch: 3, batch: 19, loss: 3.3965845108032227, acc: 48.4375, f1: 15.414082919079586, r: 0.19985622346570397
06/01/2019 10:44:46 step: 124, epoch: 3, batch: 24, loss: 3.4448812007904053, acc: 46.875, f1: 15.10989010989011, r: 0.2454862841626471
06/01/2019 10:44:47 step: 129, epoch: 3, batch: 29, loss: 3.4708402156829834, acc: 43.75, f1: 19.227216748768473, r: 0.29672583958545784
06/01/2019 10:44:48 *** evaluating ***
06/01/2019 10:44:48 step: 4, epoch: 3, acc: 50.0, f1: 13.146964856230031, r: 0.3288712924258361
06/01/2019 10:44:48 *** epoch: 5 ***
06/01/2019 10:44:48 *** training ***
06/01/2019 10:44:49 step: 137, epoch: 4, batch: 4, loss: 3.2077627182006836, acc: 46.875, f1: 12.924575424575425, r: 0.14438309880398792
06/01/2019 10:44:51 step: 142, epoch: 4, batch: 9, loss: 2.9918761253356934, acc: 50.0, f1: 14.670790985417586, r: 0.29258141665093257
06/01/2019 10:44:52 step: 147, epoch: 4, batch: 14, loss: 3.6198337078094482, acc: 54.6875, f1: 19.523809523809526, r: 0.28075222210721523
06/01/2019 10:44:53 step: 152, epoch: 4, batch: 19, loss: 3.1740102767944336, acc: 54.6875, f1: 20.365142104272536, r: 0.27658734147198094
06/01/2019 10:44:54 step: 157, epoch: 4, batch: 24, loss: 2.7862792015075684, acc: 56.25, f1: 17.428250761584096, r: 0.2157804827687574
06/01/2019 10:44:55 step: 162, epoch: 4, batch: 29, loss: 2.963435173034668, acc: 53.125, f1: 13.92241379310345, r: 0.2361558752690466
06/01/2019 10:44:55 *** evaluating ***
06/01/2019 10:44:56 step: 5, epoch: 4, acc: 52.991452991452995, f1: 15.033497754546122, r: 0.3264548481450666
06/01/2019 10:44:56 *** epoch: 6 ***
06/01/2019 10:44:56 *** training ***
06/01/2019 10:44:57 step: 170, epoch: 5, batch: 4, loss: 2.845970630645752, acc: 50.0, f1: 15.369961692205198, r: 0.20739873008104143
06/01/2019 10:44:58 step: 175, epoch: 5, batch: 9, loss: 3.9351162910461426, acc: 43.75, f1: 19.549192811452937, r: 0.23497520506382527
06/01/2019 10:44:59 step: 180, epoch: 5, batch: 14, loss: 2.7302799224853516, acc: 43.75, f1: 16.784786305603273, r: 0.2618262222995058
06/01/2019 10:45:00 step: 185, epoch: 5, batch: 19, loss: 3.2445015907287598, acc: 39.0625, f1: 12.587719298245615, r: 0.2418082601494946
06/01/2019 10:45:01 step: 190, epoch: 5, batch: 24, loss: 2.849172830581665, acc: 51.5625, f1: 12.635869565217392, r: 0.26568758200584675
06/01/2019 10:45:02 step: 195, epoch: 5, batch: 29, loss: 2.8400721549987793, acc: 48.4375, f1: 15.625, r: 0.336745694805087
06/01/2019 10:45:03 *** evaluating ***
06/01/2019 10:45:03 step: 6, epoch: 5, acc: 54.700854700854705, f1: 15.928238428238428, r: 0.3572796900545278
06/01/2019 10:45:03 *** epoch: 7 ***
06/01/2019 10:45:03 *** training ***
06/01/2019 10:45:04 step: 203, epoch: 6, batch: 4, loss: 2.70068621635437, acc: 51.5625, f1: 21.16071428571428, r: 0.3201321760537924
06/01/2019 10:45:05 step: 208, epoch: 6, batch: 9, loss: 2.6299872398376465, acc: 64.0625, f1: 27.43764172335601, r: 0.3253132059614981
06/01/2019 10:45:06 step: 213, epoch: 6, batch: 14, loss: 2.6894211769104004, acc: 39.0625, f1: 10.833963214915595, r: 0.2510964997933911
06/01/2019 10:45:07 step: 218, epoch: 6, batch: 19, loss: 3.0722386837005615, acc: 37.5, f1: 18.858946102985374, r: 0.30795325085455383
06/01/2019 10:45:08 step: 223, epoch: 6, batch: 24, loss: 2.8437323570251465, acc: 45.3125, f1: 17.687074829931973, r: 0.28865646098891556
06/01/2019 10:45:10 step: 228, epoch: 6, batch: 29, loss: 2.787057876586914, acc: 51.5625, f1: 18.137857552659618, r: 0.2158795632437487
06/01/2019 10:45:10 *** evaluating ***
06/01/2019 10:45:11 step: 7, epoch: 6, acc: 56.41025641025641, f1: 18.803344393385277, r: 0.353797135317083
06/01/2019 10:45:11 *** epoch: 8 ***
06/01/2019 10:45:11 *** training ***
06/01/2019 10:45:12 step: 236, epoch: 7, batch: 4, loss: 2.562770366668701, acc: 51.5625, f1: 25.057137479497726, r: 0.27907657200802904
06/01/2019 10:45:13 step: 241, epoch: 7, batch: 9, loss: 2.631929397583008, acc: 53.125, f1: 22.61904761904762, r: 0.2654144277817174
06/01/2019 10:45:14 step: 246, epoch: 7, batch: 14, loss: 2.472705364227295, acc: 54.6875, f1: 28.649193548387096, r: 0.2533303917687994
06/01/2019 10:45:15 step: 251, epoch: 7, batch: 19, loss: 2.5623693466186523, acc: 45.3125, f1: 21.697513013302487, r: 0.229074311039474
06/01/2019 10:45:16 step: 256, epoch: 7, batch: 24, loss: 2.8709280490875244, acc: 37.5, f1: 13.020833333333334, r: 0.27994894280248067
06/01/2019 10:45:17 step: 261, epoch: 7, batch: 29, loss: 2.849952220916748, acc: 46.875, f1: 29.296565503462052, r: 0.2587918293407119
06/01/2019 10:45:18 *** evaluating ***
06/01/2019 10:45:19 step: 8, epoch: 7, acc: 58.54700854700855, f1: 21.4363414223121, r: 0.375886433096261
06/01/2019 10:45:19 *** epoch: 9 ***
06/01/2019 10:45:19 *** training ***
06/01/2019 10:45:20 step: 269, epoch: 8, batch: 4, loss: 2.567730665206909, acc: 51.5625, f1: 30.31788958562414, r: 0.3253883087759279
06/01/2019 10:45:21 step: 274, epoch: 8, batch: 9, loss: 3.1125476360321045, acc: 45.3125, f1: 22.45232519237203, r: 0.21250608947430372
06/01/2019 10:45:22 step: 279, epoch: 8, batch: 14, loss: 2.3809196949005127, acc: 53.125, f1: 27.146121541501977, r: 0.3632511471338715
06/01/2019 10:45:23 step: 284, epoch: 8, batch: 19, loss: 2.618044376373291, acc: 48.4375, f1: 19.126712328767127, r: 0.2723166156609998
06/01/2019 10:45:24 step: 289, epoch: 8, batch: 24, loss: 2.376784324645996, acc: 46.875, f1: 19.5578231292517, r: 0.3514321866364979
06/01/2019 10:45:26 step: 294, epoch: 8, batch: 29, loss: 2.471987724304199, acc: 57.8125, f1: 25.128998968008258, r: 0.4057790282832671
06/01/2019 10:45:26 *** evaluating ***
06/01/2019 10:45:27 step: 9, epoch: 8, acc: 58.119658119658126, f1: 22.120352899253426, r: 0.3678510148746662
06/01/2019 10:45:27 *** epoch: 10 ***
06/01/2019 10:45:27 *** training ***
06/01/2019 10:45:27 step: 302, epoch: 9, batch: 4, loss: 2.117023468017578, acc: 60.9375, f1: 29.506172839506167, r: 0.3842843268320455
06/01/2019 10:45:29 step: 307, epoch: 9, batch: 9, loss: 2.599262237548828, acc: 40.625, f1: 16.894013690352367, r: 0.3839583387217455
06/01/2019 10:45:30 step: 312, epoch: 9, batch: 14, loss: 2.497666835784912, acc: 50.0, f1: 28.42928216062544, r: 0.3783393784648477
06/01/2019 10:45:31 step: 317, epoch: 9, batch: 19, loss: 2.3831043243408203, acc: 56.25, f1: 26.05042016806723, r: 0.30734800954427083
06/01/2019 10:45:32 step: 322, epoch: 9, batch: 24, loss: 2.600605010986328, acc: 50.0, f1: 23.171900339083003, r: 0.2899682406003285
06/01/2019 10:45:33 step: 327, epoch: 9, batch: 29, loss: 2.4067928791046143, acc: 54.6875, f1: 26.170868884415043, r: 0.386593512000879
06/01/2019 10:45:34 *** evaluating ***
06/01/2019 10:45:34 step: 10, epoch: 9, acc: 58.54700854700855, f1: 23.62576738012278, r: 0.3658817093205409
06/01/2019 10:45:34 *** epoch: 11 ***
06/01/2019 10:45:34 *** training ***
06/01/2019 10:45:35 step: 335, epoch: 10, batch: 4, loss: 2.2654387950897217, acc: 51.5625, f1: 27.15461062235256, r: 0.3939528225675142
06/01/2019 10:45:36 step: 340, epoch: 10, batch: 9, loss: 2.469073534011841, acc: 45.3125, f1: 20.21047008547009, r: 0.29792276671277856
06/01/2019 10:45:37 step: 345, epoch: 10, batch: 14, loss: 2.3186068534851074, acc: 60.9375, f1: 33.25182790825272, r: 0.40897851900802673
06/01/2019 10:45:39 step: 350, epoch: 10, batch: 19, loss: 2.0310840606689453, acc: 62.5, f1: 30.95881595881596, r: 0.39186768573918956
06/01/2019 10:45:40 step: 355, epoch: 10, batch: 24, loss: 2.3908133506774902, acc: 43.75, f1: 17.402630817225745, r: 0.3017472870306117
06/01/2019 10:45:41 step: 360, epoch: 10, batch: 29, loss: 2.360123872756958, acc: 45.3125, f1: 19.95093795093795, r: 0.2975005206121832
06/01/2019 10:45:41 *** evaluating ***
06/01/2019 10:45:42 step: 11, epoch: 10, acc: 59.401709401709404, f1: 24.21395998823183, r: 0.3939252449837795
06/01/2019 10:45:42 *** epoch: 12 ***
06/01/2019 10:45:42 *** training ***
06/01/2019 10:45:43 step: 368, epoch: 11, batch: 4, loss: 2.046677827835083, acc: 50.0, f1: 28.628153455739664, r: 0.39393237990095786
06/01/2019 10:45:44 step: 373, epoch: 11, batch: 9, loss: 3.1237268447875977, acc: 48.4375, f1: 22.33766233766234, r: 0.3818269259695539
06/01/2019 10:45:45 step: 378, epoch: 11, batch: 14, loss: 2.9040684700012207, acc: 60.9375, f1: 37.25425183335933, r: 0.3895296053791567
06/01/2019 10:45:46 step: 383, epoch: 11, batch: 19, loss: 2.445406913757324, acc: 45.3125, f1: 27.218066112075327, r: 0.36989563702945455
06/01/2019 10:45:47 step: 388, epoch: 11, batch: 24, loss: 2.2531542778015137, acc: 48.4375, f1: 25.290808543096873, r: 0.2319721542453841
06/01/2019 10:45:48 step: 393, epoch: 11, batch: 29, loss: 2.203761100769043, acc: 53.125, f1: 25.500731938344956, r: 0.3796557424113381
06/01/2019 10:45:49 *** evaluating ***
06/01/2019 10:45:49 step: 12, epoch: 11, acc: 55.55555555555556, f1: 21.428764609291992, r: 0.3991398061987455
06/01/2019 10:45:49 *** epoch: 13 ***
06/01/2019 10:45:49 *** training ***
06/01/2019 10:45:50 step: 401, epoch: 12, batch: 4, loss: 2.171335220336914, acc: 51.5625, f1: 24.5778725469801, r: 0.33077766673836595
06/01/2019 10:45:51 step: 406, epoch: 12, batch: 9, loss: 1.848589539527893, acc: 57.8125, f1: 29.260067663429005, r: 0.3541807722750406
06/01/2019 10:45:52 step: 411, epoch: 12, batch: 14, loss: 2.936202049255371, acc: 50.0, f1: 21.65295920012901, r: 0.37689295732346223
06/01/2019 10:45:53 step: 416, epoch: 12, batch: 19, loss: 2.0759220123291016, acc: 62.5, f1: 44.17989417989418, r: 0.4538536730524791
06/01/2019 10:45:55 step: 421, epoch: 12, batch: 24, loss: 2.2732250690460205, acc: 57.8125, f1: 27.719298245614034, r: 0.4534764866382089
06/01/2019 10:45:56 step: 426, epoch: 12, batch: 29, loss: 2.0307974815368652, acc: 60.9375, f1: 35.076578593819974, r: 0.3977474058869049
06/01/2019 10:45:57 *** evaluating ***
06/01/2019 10:45:57 step: 13, epoch: 12, acc: 58.54700854700855, f1: 22.03025400057736, r: 0.38685220499275774
06/01/2019 10:45:57 *** epoch: 14 ***
06/01/2019 10:45:57 *** training ***
06/01/2019 10:45:58 step: 434, epoch: 13, batch: 4, loss: 1.6344833374023438, acc: 68.75, f1: 53.02068302068302, r: 0.4757694097384718
06/01/2019 10:45:59 step: 439, epoch: 13, batch: 9, loss: 2.1364355087280273, acc: 43.75, f1: 30.233825782830948, r: 0.378921984505504
06/01/2019 10:46:00 step: 444, epoch: 13, batch: 14, loss: 2.7757768630981445, acc: 57.8125, f1: 41.66311300639659, r: 0.41123378522171167
06/01/2019 10:46:02 step: 449, epoch: 13, batch: 19, loss: 1.7543599605560303, acc: 56.25, f1: 33.4047619047619, r: 0.34250074796636165
06/01/2019 10:46:03 step: 454, epoch: 13, batch: 24, loss: 2.2795674800872803, acc: 50.0, f1: 20.582497212931997, r: 0.29118580335026323
06/01/2019 10:46:04 step: 459, epoch: 13, batch: 29, loss: 2.1960415840148926, acc: 43.75, f1: 24.3522178304787, r: 0.4400382164687063
06/01/2019 10:46:05 *** evaluating ***
06/01/2019 10:46:05 step: 14, epoch: 13, acc: 56.837606837606835, f1: 21.19046472926323, r: 0.40090011694179567
06/01/2019 10:46:05 *** epoch: 15 ***
06/01/2019 10:46:05 *** training ***
06/01/2019 10:46:06 step: 467, epoch: 14, batch: 4, loss: 1.9055185317993164, acc: 56.25, f1: 29.41287878787879, r: 0.3274829665211214
06/01/2019 10:46:07 step: 472, epoch: 14, batch: 9, loss: 1.8121778964996338, acc: 56.25, f1: 23.06134259259259, r: 0.45042157814378375
06/01/2019 10:46:08 step: 477, epoch: 14, batch: 14, loss: 1.712145447731018, acc: 60.9375, f1: 39.146939296193025, r: 0.4302047496385204
06/01/2019 10:46:09 step: 482, epoch: 14, batch: 19, loss: 1.7791541814804077, acc: 57.8125, f1: 48.269307723089234, r: 0.37766020228863323
06/01/2019 10:46:10 step: 487, epoch: 14, batch: 24, loss: 1.9961113929748535, acc: 54.6875, f1: 24.33760683760684, r: 0.36523624066440674
06/01/2019 10:46:11 step: 492, epoch: 14, batch: 29, loss: 2.7604877948760986, acc: 42.1875, f1: 17.75107148241477, r: 0.34761208763332246
06/01/2019 10:46:12 *** evaluating ***
06/01/2019 10:46:12 step: 15, epoch: 14, acc: 58.97435897435898, f1: 25.737164602683183, r: 0.3967053976251704
06/01/2019 10:46:12 *** epoch: 16 ***
06/01/2019 10:46:12 *** training ***
06/01/2019 10:46:13 step: 500, epoch: 15, batch: 4, loss: 1.778517246246338, acc: 53.125, f1: 22.587301587301592, r: 0.3304595373813755
06/01/2019 10:46:14 step: 505, epoch: 15, batch: 9, loss: 1.9274852275848389, acc: 50.0, f1: 25.585764809902745, r: 0.3824570640845175
06/01/2019 10:46:16 step: 510, epoch: 15, batch: 14, loss: 1.8793468475341797, acc: 50.0, f1: 20.432692307692307, r: 0.42192105779124556
06/01/2019 10:46:17 step: 515, epoch: 15, batch: 19, loss: 1.5338658094406128, acc: 64.0625, f1: 39.87219130076273, r: 0.4102078206790255
06/01/2019 10:46:18 step: 520, epoch: 15, batch: 24, loss: 1.575157642364502, acc: 60.9375, f1: 38.53408029878618, r: 0.4335930258396063
06/01/2019 10:46:19 step: 525, epoch: 15, batch: 29, loss: 1.6614034175872803, acc: 59.375, f1: 27.906536527654545, r: 0.31876755739592105
06/01/2019 10:46:19 *** evaluating ***
06/01/2019 10:46:20 step: 16, epoch: 15, acc: 55.98290598290598, f1: 22.116281572904704, r: 0.3982096061639466
06/01/2019 10:46:20 *** epoch: 17 ***
06/01/2019 10:46:20 *** training ***
06/01/2019 10:46:21 step: 533, epoch: 16, batch: 4, loss: 1.4578304290771484, acc: 53.125, f1: 21.693376068376068, r: 0.5296650419083234
06/01/2019 10:46:22 step: 538, epoch: 16, batch: 9, loss: 2.3032875061035156, acc: 57.8125, f1: 38.42092659113936, r: 0.4317733939847892
06/01/2019 10:46:23 step: 543, epoch: 16, batch: 14, loss: 1.4782980680465698, acc: 54.6875, f1: 31.373959373959377, r: 0.4849159418779694
06/01/2019 10:46:24 step: 548, epoch: 16, batch: 19, loss: 1.5008330345153809, acc: 53.125, f1: 26.242109345744396, r: 0.4551625112666233
06/01/2019 10:46:25 step: 553, epoch: 16, batch: 24, loss: 1.8666441440582275, acc: 50.0, f1: 23.619505494505493, r: 0.36834423222390766
06/01/2019 10:46:27 step: 558, epoch: 16, batch: 29, loss: 2.549592971801758, acc: 57.8125, f1: 28.008658008658006, r: 0.44095435286174295
06/01/2019 10:46:27 *** evaluating ***
06/01/2019 10:46:28 step: 17, epoch: 16, acc: 58.54700854700855, f1: 27.639172460729135, r: 0.3916306397798745
06/01/2019 10:46:28 *** epoch: 18 ***
06/01/2019 10:46:28 *** training ***
06/01/2019 10:46:29 step: 566, epoch: 17, batch: 4, loss: 2.3494744300842285, acc: 53.125, f1: 31.22448979591837, r: 0.35845716301835845
06/01/2019 10:46:30 step: 571, epoch: 17, batch: 9, loss: 1.4181742668151855, acc: 59.375, f1: 38.258636788048555, r: 0.5928070404119175
06/01/2019 10:46:31 step: 576, epoch: 17, batch: 14, loss: 1.2623846530914307, acc: 67.1875, f1: 38.70639164756812, r: 0.3819657838134675
06/01/2019 10:46:32 step: 581, epoch: 17, batch: 19, loss: 1.2717353105545044, acc: 59.375, f1: 41.761128364389236, r: 0.5408530074508886
06/01/2019 10:46:33 step: 586, epoch: 17, batch: 24, loss: 1.4384491443634033, acc: 60.9375, f1: 34.84832875457876, r: 0.5026229831383009
06/01/2019 10:46:34 step: 591, epoch: 17, batch: 29, loss: 1.511897325515747, acc: 56.25, f1: 35.199029771398195, r: 0.5123157281132903
06/01/2019 10:46:35 *** evaluating ***
06/01/2019 10:46:35 step: 18, epoch: 17, acc: 56.837606837606835, f1: 28.41934518543236, r: 0.3784213114451307
06/01/2019 10:46:35 *** epoch: 19 ***
06/01/2019 10:46:35 *** training ***
06/01/2019 10:46:36 step: 599, epoch: 18, batch: 4, loss: 1.108691692352295, acc: 68.75, f1: 47.52039627039627, r: 0.6617203924752934
06/01/2019 10:46:38 step: 604, epoch: 18, batch: 9, loss: 1.5304303169250488, acc: 46.875, f1: 25.398113790970932, r: 0.42707253244843546
06/01/2019 10:46:39 step: 609, epoch: 18, batch: 14, loss: 1.2825798988342285, acc: 60.9375, f1: 44.9324888798573, r: 0.5425366544422199
06/01/2019 10:46:40 step: 614, epoch: 18, batch: 19, loss: 1.2235050201416016, acc: 67.1875, f1: 40.19871794871794, r: 0.5083649527332834
06/01/2019 10:46:41 step: 619, epoch: 18, batch: 24, loss: 1.4257981777191162, acc: 60.9375, f1: 38.23830409356725, r: 0.47346784825436566
06/01/2019 10:46:42 step: 624, epoch: 18, batch: 29, loss: 1.1523324251174927, acc: 60.9375, f1: 35.021929824561404, r: 0.4880082632084804
06/01/2019 10:46:43 *** evaluating ***
06/01/2019 10:46:43 step: 19, epoch: 18, acc: 57.26495726495726, f1: 25.59201024159664, r: 0.3865840886000537
06/01/2019 10:46:43 *** epoch: 20 ***
06/01/2019 10:46:43 *** training ***
06/01/2019 10:46:44 step: 632, epoch: 19, batch: 4, loss: 1.218015193939209, acc: 67.1875, f1: 46.671876480060625, r: 0.5536830009435038
06/01/2019 10:46:45 step: 637, epoch: 19, batch: 9, loss: 1.323936104774475, acc: 53.125, f1: 32.62310606060606, r: 0.42258336297171606
06/01/2019 10:46:47 step: 642, epoch: 19, batch: 14, loss: 0.9813810586929321, acc: 68.75, f1: 62.44747487197875, r: 0.5202771181587278
06/01/2019 10:46:48 step: 647, epoch: 19, batch: 19, loss: 0.9306821823120117, acc: 78.125, f1: 56.09083850931677, r: 0.4738228149249219
06/01/2019 10:46:49 step: 652, epoch: 19, batch: 24, loss: 1.2838853597640991, acc: 57.8125, f1: 40.42132982225149, r: 0.40168881952147156
06/01/2019 10:46:50 step: 657, epoch: 19, batch: 29, loss: 1.1100027561187744, acc: 68.75, f1: 39.684129429892145, r: 0.5474747711331353
06/01/2019 10:46:50 *** evaluating ***
06/01/2019 10:46:51 step: 20, epoch: 19, acc: 56.41025641025641, f1: 24.78225478225478, r: 0.373207048221634
06/01/2019 10:46:51 *** epoch: 21 ***
06/01/2019 10:46:51 *** training ***
06/01/2019 10:46:52 step: 665, epoch: 20, batch: 4, loss: 0.9985933303833008, acc: 68.75, f1: 51.268430817303, r: 0.49485206450363556
06/01/2019 10:46:53 step: 670, epoch: 20, batch: 9, loss: 1.9306904077529907, acc: 59.375, f1: 34.784883853206836, r: 0.5362813693892935
06/01/2019 10:46:54 step: 675, epoch: 20, batch: 14, loss: 1.026964783668518, acc: 62.5, f1: 24.93468211964896, r: 0.540284778496698
06/01/2019 10:46:55 step: 680, epoch: 20, batch: 19, loss: 1.0594946146011353, acc: 60.9375, f1: 52.01149425287356, r: 0.5177713841539657
06/01/2019 10:46:57 step: 685, epoch: 20, batch: 24, loss: 0.9751678109169006, acc: 70.3125, f1: 40.50986754791103, r: 0.5177537839627937
06/01/2019 10:46:58 step: 690, epoch: 20, batch: 29, loss: 1.5982319116592407, acc: 54.6875, f1: 28.458862673926195, r: 0.5029277703255681
06/01/2019 10:46:58 *** evaluating ***
06/01/2019 10:46:58 step: 21, epoch: 20, acc: 58.119658119658126, f1: 26.920338454622573, r: 0.3905867604979261
06/01/2019 10:46:58 *** epoch: 22 ***
06/01/2019 10:46:58 *** training ***
06/01/2019 10:47:00 step: 698, epoch: 21, batch: 4, loss: 0.99290531873703, acc: 65.625, f1: 41.57823129251702, r: 0.5374658631304071
06/01/2019 10:47:01 step: 703, epoch: 21, batch: 9, loss: 1.0431339740753174, acc: 65.625, f1: 52.68382352941177, r: 0.5628385172450078
06/01/2019 10:47:02 step: 708, epoch: 21, batch: 14, loss: 1.0122261047363281, acc: 68.75, f1: 56.237595487970296, r: 0.5001597503839009
06/01/2019 10:47:03 step: 713, epoch: 21, batch: 19, loss: 1.2672168016433716, acc: 54.6875, f1: 36.864295125164695, r: 0.5144203439927324
06/01/2019 10:47:04 step: 718, epoch: 21, batch: 24, loss: 1.2201833724975586, acc: 57.8125, f1: 41.8024083196497, r: 0.47532458684997514
06/01/2019 10:47:05 step: 723, epoch: 21, batch: 29, loss: 0.9424089789390564, acc: 67.1875, f1: 49.5970695970696, r: 0.48475869203135813
06/01/2019 10:47:06 *** evaluating ***
06/01/2019 10:47:06 step: 22, epoch: 21, acc: 55.98290598290598, f1: 26.832602988576436, r: 0.37453896115844953
06/01/2019 10:47:06 *** epoch: 23 ***
06/01/2019 10:47:06 *** training ***
06/01/2019 10:47:07 step: 731, epoch: 22, batch: 4, loss: 1.1161010265350342, acc: 64.0625, f1: 48.728070175438596, r: 0.5808739995210664
06/01/2019 10:47:08 step: 736, epoch: 22, batch: 9, loss: 1.1016114950180054, acc: 67.1875, f1: 40.028209868672704, r: 0.5224010212939518
06/01/2019 10:47:09 step: 741, epoch: 22, batch: 14, loss: 0.9514151215553284, acc: 67.1875, f1: 46.27517351234498, r: 0.5478037166023718
06/01/2019 10:47:11 step: 746, epoch: 22, batch: 19, loss: 1.042360782623291, acc: 65.625, f1: 31.07627666451196, r: 0.49541962490116537
06/01/2019 10:47:12 step: 751, epoch: 22, batch: 24, loss: 0.9915434122085571, acc: 62.5, f1: 53.630179344465056, r: 0.5444239327668133
06/01/2019 10:47:13 step: 756, epoch: 22, batch: 29, loss: 0.8726032376289368, acc: 76.5625, f1: 63.73098166201613, r: 0.5115150337074372
06/01/2019 10:47:13 *** evaluating ***
06/01/2019 10:47:14 step: 23, epoch: 22, acc: 55.55555555555556, f1: 24.561945745813343, r: 0.38037975181119504
06/01/2019 10:47:14 *** epoch: 24 ***
06/01/2019 10:47:14 *** training ***
06/01/2019 10:47:15 step: 764, epoch: 23, batch: 4, loss: 1.1193737983703613, acc: 54.6875, f1: 28.745446265938067, r: 0.5065464384089111
06/01/2019 10:47:16 step: 769, epoch: 23, batch: 9, loss: 0.8341527581214905, acc: 76.5625, f1: 58.88888888888889, r: 0.4778190225691935
06/01/2019 10:47:17 step: 774, epoch: 23, batch: 14, loss: 0.684100329875946, acc: 79.6875, f1: 64.80128893662727, r: 0.5831682561077643
06/01/2019 10:47:18 step: 779, epoch: 23, batch: 19, loss: 0.8857552409172058, acc: 71.875, f1: 54.12719737842892, r: 0.5465553154930827
06/01/2019 10:47:19 step: 784, epoch: 23, batch: 24, loss: 0.9577913284301758, acc: 62.5, f1: 36.400330536486145, r: 0.4886974953868993
06/01/2019 10:47:20 step: 789, epoch: 23, batch: 29, loss: 0.9858355522155762, acc: 65.625, f1: 49.272125723738625, r: 0.5020721316173218
06/01/2019 10:47:21 *** evaluating ***
06/01/2019 10:47:21 step: 24, epoch: 23, acc: 55.98290598290598, f1: 26.82744917097611, r: 0.3708829785817245
06/01/2019 10:47:21 *** epoch: 25 ***
06/01/2019 10:47:21 *** training ***
06/01/2019 10:47:22 step: 797, epoch: 24, batch: 4, loss: 1.0635372400283813, acc: 59.375, f1: 47.48535777947542, r: 0.48573462245787774
06/01/2019 10:47:24 step: 802, epoch: 24, batch: 9, loss: 1.0163226127624512, acc: 62.5, f1: 39.62741464510332, r: 0.5366538723736856
06/01/2019 10:47:25 step: 807, epoch: 24, batch: 14, loss: 1.548912525177002, acc: 76.5625, f1: 64.20460491889062, r: 0.5801497042016132
06/01/2019 10:47:26 step: 812, epoch: 24, batch: 19, loss: 0.9459418058395386, acc: 68.75, f1: 46.28312863606981, r: 0.5613794677682488
06/01/2019 10:47:27 step: 817, epoch: 24, batch: 24, loss: 0.9192022681236267, acc: 73.4375, f1: 61.055179229698254, r: 0.5484340535568158
06/01/2019 10:47:28 step: 822, epoch: 24, batch: 29, loss: 0.9306618571281433, acc: 65.625, f1: 41.0663082437276, r: 0.5867442339201763
06/01/2019 10:47:29 *** evaluating ***
06/01/2019 10:47:29 step: 25, epoch: 24, acc: 57.692307692307686, f1: 28.926544766708695, r: 0.3814510763877916
06/01/2019 10:47:29 *** epoch: 26 ***
06/01/2019 10:47:29 *** training ***
06/01/2019 10:47:30 step: 830, epoch: 25, batch: 4, loss: 0.8783698081970215, acc: 73.4375, f1: 56.90351033499643, r: 0.5118556893810862
06/01/2019 10:47:31 step: 835, epoch: 25, batch: 9, loss: 0.9739894270896912, acc: 65.625, f1: 55.98118035092826, r: 0.5585670695701156
06/01/2019 10:47:32 step: 840, epoch: 25, batch: 14, loss: 0.7931007742881775, acc: 68.75, f1: 48.456876769846694, r: 0.5336586961321961
06/01/2019 10:47:34 step: 845, epoch: 25, batch: 19, loss: 0.916591227054596, acc: 70.3125, f1: 51.05520840814959, r: 0.6212746107197534
06/01/2019 10:47:35 step: 850, epoch: 25, batch: 24, loss: 0.969636082649231, acc: 71.875, f1: 50.770083574961625, r: 0.617427546509511
06/01/2019 10:47:36 step: 855, epoch: 25, batch: 29, loss: 0.710347056388855, acc: 79.6875, f1: 75.6730393306748, r: 0.6426498777729521
06/01/2019 10:47:37 *** evaluating ***
06/01/2019 10:47:37 step: 26, epoch: 25, acc: 55.98290598290598, f1: 25.286817170871434, r: 0.36809194693124864
06/01/2019 10:47:37 *** epoch: 27 ***
06/01/2019 10:47:37 *** training ***
06/01/2019 10:47:38 step: 863, epoch: 26, batch: 4, loss: 1.122844934463501, acc: 50.0, f1: 29.99535799168152, r: 0.5404669254436644
06/01/2019 10:47:39 step: 868, epoch: 26, batch: 9, loss: 0.8902626633644104, acc: 71.875, f1: 65.00639572068143, r: 0.5993734322456792
06/01/2019 10:47:40 step: 873, epoch: 26, batch: 14, loss: 0.90860515832901, acc: 68.75, f1: 47.614193404634584, r: 0.5828425545875096
06/01/2019 10:47:41 step: 878, epoch: 26, batch: 19, loss: 1.003147840499878, acc: 65.625, f1: 48.101962963225944, r: 0.5937248360703575
06/01/2019 10:47:42 step: 883, epoch: 26, batch: 24, loss: 0.8596599102020264, acc: 75.0, f1: 57.687295936792914, r: 0.4816338436645803
06/01/2019 10:47:44 step: 888, epoch: 26, batch: 29, loss: 0.7533896565437317, acc: 75.0, f1: 69.65053465053465, r: 0.5467496939293667
06/01/2019 10:47:44 *** evaluating ***
06/01/2019 10:47:44 step: 27, epoch: 26, acc: 55.12820512820513, f1: 25.305893213529302, r: 0.37216486462675646
06/01/2019 10:47:44 *** epoch: 28 ***
06/01/2019 10:47:44 *** training ***
06/01/2019 10:47:46 step: 896, epoch: 27, batch: 4, loss: 1.0647846460342407, acc: 57.8125, f1: 44.387278721012215, r: 0.4995042651565992
06/01/2019 10:47:47 step: 901, epoch: 27, batch: 9, loss: 1.2243525981903076, acc: 76.5625, f1: 71.53061224489797, r: 0.5047651827011285
06/01/2019 10:47:48 step: 906, epoch: 27, batch: 14, loss: 1.5085296630859375, acc: 65.625, f1: 55.82298136645962, r: 0.5921819489161738
06/01/2019 10:47:49 step: 911, epoch: 27, batch: 19, loss: 0.7964757084846497, acc: 71.875, f1: 61.87270467590834, r: 0.5604750730343053
06/01/2019 10:47:50 step: 916, epoch: 27, batch: 24, loss: 0.8499557375907898, acc: 68.75, f1: 40.96682851695672, r: 0.5785765204489669
06/01/2019 10:47:51 step: 921, epoch: 27, batch: 29, loss: 0.7632254362106323, acc: 78.125, f1: 73.73511904761905, r: 0.6401574807075038
06/01/2019 10:47:52 *** evaluating ***
06/01/2019 10:47:52 step: 28, epoch: 27, acc: 54.27350427350427, f1: 26.007574082264483, r: 0.3512875015606784
06/01/2019 10:47:52 *** epoch: 29 ***
06/01/2019 10:47:52 *** training ***
06/01/2019 10:47:53 step: 929, epoch: 28, batch: 4, loss: 0.7898017168045044, acc: 68.75, f1: 57.1575965018588, r: 0.4968383413041662
06/01/2019 10:47:54 step: 934, epoch: 28, batch: 9, loss: 0.7829224467277527, acc: 78.125, f1: 80.85480656909228, r: 0.6341303978901406
06/01/2019 10:47:55 step: 939, epoch: 28, batch: 14, loss: 0.7707480788230896, acc: 73.4375, f1: 53.71207408943258, r: 0.5372656425834562
06/01/2019 10:47:56 step: 944, epoch: 28, batch: 19, loss: 0.7545645833015442, acc: 75.0, f1: 51.910279263220446, r: 0.5999556277032945
06/01/2019 10:47:57 step: 949, epoch: 28, batch: 24, loss: 0.7318932414054871, acc: 76.5625, f1: 61.493484145897824, r: 0.5340171340078599
06/01/2019 10:47:58 step: 954, epoch: 28, batch: 29, loss: 0.8355770707130432, acc: 71.875, f1: 59.09319660067417, r: 0.6474134357547789
06/01/2019 10:47:59 *** evaluating ***
06/01/2019 10:47:59 step: 29, epoch: 28, acc: 57.692307692307686, f1: 26.05424547826003, r: 0.36253219243102763
06/01/2019 10:47:59 *** epoch: 30 ***
06/01/2019 10:47:59 *** training ***
06/01/2019 10:48:00 step: 962, epoch: 29, batch: 4, loss: 0.7609506249427795, acc: 76.5625, f1: 67.4094424094424, r: 0.5165092989493905
06/01/2019 10:48:02 step: 967, epoch: 29, batch: 9, loss: 0.7853900194168091, acc: 76.5625, f1: 73.78514206100412, r: 0.5805554109858485
06/01/2019 10:48:03 step: 972, epoch: 29, batch: 14, loss: 0.857208251953125, acc: 67.1875, f1: 45.24040575340745, r: 0.5142399168080394
06/01/2019 10:48:04 step: 977, epoch: 29, batch: 19, loss: 0.7914827466011047, acc: 71.875, f1: 59.04761904761904, r: 0.6724051192507485
06/01/2019 10:48:05 step: 982, epoch: 29, batch: 24, loss: 0.9597151279449463, acc: 67.1875, f1: 49.943284936479124, r: 0.5205604016739263
06/01/2019 10:48:06 step: 987, epoch: 29, batch: 29, loss: 0.7006932497024536, acc: 76.5625, f1: 59.73732713083626, r: 0.5820988579352058
06/01/2019 10:48:07 *** evaluating ***
06/01/2019 10:48:07 step: 30, epoch: 29, acc: 52.56410256410257, f1: 28.03118006908575, r: 0.3576697940254636
06/01/2019 10:48:07 *** epoch: 31 ***
06/01/2019 10:48:07 *** training ***
06/01/2019 10:48:08 step: 995, epoch: 30, batch: 4, loss: 1.0347528457641602, acc: 75.0, f1: 73.77344877344876, r: 0.5541043637291674
06/01/2019 10:48:09 step: 1000, epoch: 30, batch: 9, loss: 0.7405661344528198, acc: 71.875, f1: 59.151927437641724, r: 0.4945386760154043
06/01/2019 10:48:10 step: 1005, epoch: 30, batch: 14, loss: 0.8929179310798645, acc: 65.625, f1: 56.205357142857146, r: 0.6224823886483293
06/01/2019 10:48:11 step: 1010, epoch: 30, batch: 19, loss: 0.64732825756073, acc: 84.375, f1: 79.62558380759545, r: 0.6575210358939797
06/01/2019 10:48:13 step: 1015, epoch: 30, batch: 24, loss: 0.7765396237373352, acc: 76.5625, f1: 74.55511647629874, r: 0.5305796695556012
06/01/2019 10:48:14 step: 1020, epoch: 30, batch: 29, loss: 0.6854881644248962, acc: 79.6875, f1: 64.02597402597402, r: 0.5864385201407308
06/01/2019 10:48:14 *** evaluating ***
06/01/2019 10:48:15 step: 31, epoch: 30, acc: 57.26495726495726, f1: 27.86426565743249, r: 0.34610001197240564
06/01/2019 10:48:15 *** epoch: 32 ***
06/01/2019 10:48:15 *** training ***
06/01/2019 10:48:16 step: 1028, epoch: 31, batch: 4, loss: 0.7758040428161621, acc: 71.875, f1: 53.46858850129199, r: 0.6599437204971292
06/01/2019 10:48:17 step: 1033, epoch: 31, batch: 9, loss: 0.6412374973297119, acc: 81.25, f1: 65.91178406846609, r: 0.5421263564949033
06/01/2019 10:48:18 step: 1038, epoch: 31, batch: 14, loss: 0.5554804801940918, acc: 82.8125, f1: 77.29767769176635, r: 0.6149006491581613
06/01/2019 10:48:19 step: 1043, epoch: 31, batch: 19, loss: 0.5857630968093872, acc: 81.25, f1: 67.50391275391276, r: 0.6658092684108622
06/01/2019 10:48:20 step: 1048, epoch: 31, batch: 24, loss: 0.7118102312088013, acc: 76.5625, f1: 73.17444316436253, r: 0.6096426221062172
06/01/2019 10:48:21 step: 1053, epoch: 31, batch: 29, loss: 0.8641541004180908, acc: 67.1875, f1: 48.051394223929435, r: 0.5162776291359096
06/01/2019 10:48:22 *** evaluating ***
06/01/2019 10:48:22 step: 32, epoch: 31, acc: 56.41025641025641, f1: 27.2916782916924, r: 0.349142547535226
06/01/2019 10:48:22 *** epoch: 33 ***
06/01/2019 10:48:22 *** training ***
06/01/2019 10:48:24 step: 1061, epoch: 32, batch: 4, loss: 0.5777172446250916, acc: 85.9375, f1: 79.4467120181406, r: 0.6006296097636652
06/01/2019 10:48:25 step: 1066, epoch: 32, batch: 9, loss: 0.6635257601737976, acc: 78.125, f1: 58.148476782017156, r: 0.5689757006000129
06/01/2019 10:48:26 step: 1071, epoch: 32, batch: 14, loss: 0.5511862635612488, acc: 84.375, f1: 75.42071821232241, r: 0.5862868887382662
06/01/2019 10:48:27 step: 1076, epoch: 32, batch: 19, loss: 0.8305766582489014, acc: 67.1875, f1: 63.40567936816184, r: 0.6098847275660485
06/01/2019 10:48:28 step: 1081, epoch: 32, batch: 24, loss: 0.5447142720222473, acc: 81.25, f1: 54.627192982456144, r: 0.5487201343356196
06/01/2019 10:48:29 step: 1086, epoch: 32, batch: 29, loss: 0.8068717122077942, acc: 75.0, f1: 58.77509337068161, r: 0.6776301789602878
06/01/2019 10:48:30 *** evaluating ***
06/01/2019 10:48:30 step: 33, epoch: 32, acc: 58.119658119658126, f1: 28.249708322530466, r: 0.34850297438163386
06/01/2019 10:48:30 *** epoch: 34 ***
06/01/2019 10:48:30 *** training ***
06/01/2019 10:48:31 step: 1094, epoch: 33, batch: 4, loss: 0.5120848417282104, acc: 85.9375, f1: 84.03076786270063, r: 0.6731820385315392
06/01/2019 10:48:32 step: 1099, epoch: 33, batch: 9, loss: 0.7145313024520874, acc: 70.3125, f1: 65.13068385248836, r: 0.581975713879778
06/01/2019 10:48:33 step: 1104, epoch: 33, batch: 14, loss: 0.5392730236053467, acc: 82.8125, f1: 71.53643457991285, r: 0.5815863912563204
06/01/2019 10:48:34 step: 1109, epoch: 33, batch: 19, loss: 0.6037459373474121, acc: 78.125, f1: 67.35062039409866, r: 0.7388183736444912
06/01/2019 10:48:35 step: 1114, epoch: 33, batch: 24, loss: 0.6784154176712036, acc: 84.375, f1: 71.77637721755369, r: 0.7193749501015637
06/01/2019 10:48:36 step: 1119, epoch: 33, batch: 29, loss: 0.5857003331184387, acc: 82.8125, f1: 64.9920634920635, r: 0.6862426521533029
06/01/2019 10:48:37 *** evaluating ***
06/01/2019 10:48:37 step: 34, epoch: 33, acc: 57.692307692307686, f1: 29.09569332699692, r: 0.3543791654634908
06/01/2019 10:48:37 *** epoch: 35 ***
06/01/2019 10:48:37 *** training ***
06/01/2019 10:48:39 step: 1127, epoch: 34, batch: 4, loss: 0.5333376526832581, acc: 84.375, f1: 72.72865774152959, r: 0.7108277304282266
06/01/2019 10:48:40 step: 1132, epoch: 34, batch: 9, loss: 0.5813732147216797, acc: 79.6875, f1: 71.68098818474759, r: 0.5829809891385381
06/01/2019 10:48:41 step: 1137, epoch: 34, batch: 14, loss: 0.6982510089874268, acc: 73.4375, f1: 54.815490851513026, r: 0.5786848196321671
06/01/2019 10:48:42 step: 1142, epoch: 34, batch: 19, loss: 0.6386611461639404, acc: 78.125, f1: 68.32108944206612, r: 0.5432669827457187
06/01/2019 10:48:43 step: 1147, epoch: 34, batch: 24, loss: 0.5337990522384644, acc: 89.0625, f1: 78.98624286764448, r: 0.5888409948859742
06/01/2019 10:48:44 step: 1152, epoch: 34, batch: 29, loss: 0.42304494976997375, acc: 89.0625, f1: 87.75510204081633, r: 0.630365009683701
06/01/2019 10:48:45 *** evaluating ***
06/01/2019 10:48:45 step: 35, epoch: 34, acc: 55.12820512820513, f1: 27.83000453806799, r: 0.344795001285413
06/01/2019 10:48:45 *** epoch: 36 ***
06/01/2019 10:48:45 *** training ***
06/01/2019 10:48:46 step: 1160, epoch: 35, batch: 4, loss: 0.4924471080303192, acc: 89.0625, f1: 89.75737025272629, r: 0.6336096926005401
06/01/2019 10:48:48 step: 1165, epoch: 35, batch: 9, loss: 0.4601542055606842, acc: 95.3125, f1: 95.17739268499147, r: 0.7067827041805594
06/01/2019 10:48:49 step: 1170, epoch: 35, batch: 14, loss: 0.6339485049247742, acc: 81.25, f1: 71.47850444525636, r: 0.6184583437813357
06/01/2019 10:48:50 step: 1175, epoch: 35, batch: 19, loss: 0.591367244720459, acc: 87.5, f1: 78.87305749805749, r: 0.6968971970188358
06/01/2019 10:48:51 step: 1180, epoch: 35, batch: 24, loss: 0.5899829268455505, acc: 82.8125, f1: 61.46544631838749, r: 0.5961830346574295
06/01/2019 10:48:52 step: 1185, epoch: 35, batch: 29, loss: 0.7191787362098694, acc: 78.125, f1: 55.6074168797954, r: 0.652244461532113
06/01/2019 10:48:52 *** evaluating ***
06/01/2019 10:48:53 step: 36, epoch: 35, acc: 53.41880341880342, f1: 26.88458491780874, r: 0.33166563761468115
06/01/2019 10:48:53 *** epoch: 37 ***
06/01/2019 10:48:53 *** training ***
06/01/2019 10:48:54 step: 1193, epoch: 36, batch: 4, loss: 0.479343444108963, acc: 85.9375, f1: 87.20757345491388, r: 0.7227838159132577
06/01/2019 10:48:55 step: 1198, epoch: 36, batch: 9, loss: 0.4577854871749878, acc: 87.5, f1: 87.28599131960479, r: 0.6687248423912887
06/01/2019 10:48:56 step: 1203, epoch: 36, batch: 14, loss: 0.6310838460922241, acc: 79.6875, f1: 70.01259157509158, r: 0.6322872658963897
06/01/2019 10:48:57 step: 1208, epoch: 36, batch: 19, loss: 0.5435147285461426, acc: 82.8125, f1: 79.81487372088569, r: 0.6751873859343774
06/01/2019 10:48:58 step: 1213, epoch: 36, batch: 24, loss: 0.4342765808105469, acc: 87.5, f1: 72.14619938884644, r: 0.7439490068771402
06/01/2019 10:48:59 step: 1218, epoch: 36, batch: 29, loss: 0.5522006154060364, acc: 82.8125, f1: 77.13629311373673, r: 0.640659344176306
06/01/2019 10:49:00 *** evaluating ***
06/01/2019 10:49:00 step: 37, epoch: 36, acc: 53.84615384615385, f1: 25.951030732399555, r: 0.33609141638831397
06/01/2019 10:49:00 *** epoch: 38 ***
06/01/2019 10:49:00 *** training ***
06/01/2019 10:49:01 step: 1226, epoch: 37, batch: 4, loss: 0.46028923988342285, acc: 90.625, f1: 80.16119491274772, r: 0.5883409652657299
06/01/2019 10:49:02 step: 1231, epoch: 37, batch: 9, loss: 0.6730537414550781, acc: 82.8125, f1: 76.6400568031003, r: 0.6536944993843539
06/01/2019 10:49:03 step: 1236, epoch: 37, batch: 14, loss: 0.6818630695343018, acc: 70.3125, f1: 59.73290598290598, r: 0.6667518850738816
06/01/2019 10:49:04 step: 1241, epoch: 37, batch: 19, loss: 0.5276582837104797, acc: 81.25, f1: 74.44444444444444, r: 0.6065903956420409
06/01/2019 10:49:06 step: 1246, epoch: 37, batch: 24, loss: 1.2118147611618042, acc: 87.5, f1: 89.23553077357424, r: 0.6875293533991875
06/01/2019 10:49:07 step: 1251, epoch: 37, batch: 29, loss: 0.462809681892395, acc: 84.375, f1: 70.43188462925728, r: 0.6068631628895629
06/01/2019 10:49:08 *** evaluating ***
06/01/2019 10:49:08 step: 38, epoch: 37, acc: 50.0, f1: 26.451289460065553, r: 0.3439810888688561
06/01/2019 10:49:08 *** epoch: 39 ***
06/01/2019 10:49:08 *** training ***
06/01/2019 10:49:09 step: 1259, epoch: 38, batch: 4, loss: 0.5097118020057678, acc: 87.5, f1: 64.94708599149388, r: 0.747634727814492
06/01/2019 10:49:10 step: 1264, epoch: 38, batch: 9, loss: 0.5495153665542603, acc: 79.6875, f1: 63.071575317338024, r: 0.6127066411610961
06/01/2019 10:49:11 step: 1269, epoch: 38, batch: 14, loss: 0.4704444706439972, acc: 85.9375, f1: 68.37606837606837, r: 0.687028534323409
06/01/2019 10:49:12 step: 1274, epoch: 38, batch: 19, loss: 0.5795713663101196, acc: 79.6875, f1: 56.32052158367947, r: 0.5692414609165489
06/01/2019 10:49:13 step: 1279, epoch: 38, batch: 24, loss: 0.3211619257926941, acc: 92.1875, f1: 92.4122292595199, r: 0.6355700970378675
06/01/2019 10:49:14 step: 1284, epoch: 38, batch: 29, loss: 0.3804670572280884, acc: 90.625, f1: 79.88267770876466, r: 0.6966830769072722
06/01/2019 10:49:15 *** evaluating ***
06/01/2019 10:49:15 step: 39, epoch: 38, acc: 56.41025641025641, f1: 27.933754861335963, r: 0.3398245633246754
06/01/2019 10:49:15 *** epoch: 40 ***
06/01/2019 10:49:15 *** training ***
06/01/2019 10:49:16 step: 1292, epoch: 39, batch: 4, loss: 0.3727654814720154, acc: 90.625, f1: 63.927644472694524, r: 0.6390776471299104
06/01/2019 10:49:17 step: 1297, epoch: 39, batch: 9, loss: 0.46197250485420227, acc: 92.1875, f1: 81.68603455368162, r: 0.7910004938023484
06/01/2019 10:49:18 step: 1302, epoch: 39, batch: 14, loss: 0.5555768609046936, acc: 81.25, f1: 75.84663120567376, r: 0.6354613189150768
06/01/2019 10:49:19 step: 1307, epoch: 39, batch: 19, loss: 0.5969281196594238, acc: 78.125, f1: 76.65055497551396, r: 0.6827323248952066
06/01/2019 10:49:21 step: 1312, epoch: 39, batch: 24, loss: 0.4772149920463562, acc: 85.9375, f1: 81.38938007073018, r: 0.7250756543912545
06/01/2019 10:49:22 step: 1317, epoch: 39, batch: 29, loss: 0.5890278220176697, acc: 82.8125, f1: 62.761990100699784, r: 0.5049765441521883
06/01/2019 10:49:23 *** evaluating ***
06/01/2019 10:49:23 step: 40, epoch: 39, acc: 56.837606837606835, f1: 27.64628455649582, r: 0.32552648158320285
06/01/2019 10:49:23 *** epoch: 41 ***
06/01/2019 10:49:23 *** training ***
06/01/2019 10:49:24 step: 1325, epoch: 40, batch: 4, loss: 0.5116170644760132, acc: 87.5, f1: 83.98422670750793, r: 0.6049382543702245
06/01/2019 10:49:25 step: 1330, epoch: 40, batch: 9, loss: 0.5491213202476501, acc: 85.9375, f1: 73.6619683297288, r: 0.644959017155401
06/01/2019 10:49:26 step: 1335, epoch: 40, batch: 14, loss: 0.41845178604125977, acc: 85.9375, f1: 83.10584152689415, r: 0.6408646001275121
06/01/2019 10:49:27 step: 1340, epoch: 40, batch: 19, loss: 0.39098891615867615, acc: 89.0625, f1: 86.3095238095238, r: 0.6746041629825333
06/01/2019 10:49:28 step: 1345, epoch: 40, batch: 24, loss: 0.4997415542602539, acc: 84.375, f1: 83.06122448979593, r: 0.6451464762892528
06/01/2019 10:49:30 step: 1350, epoch: 40, batch: 29, loss: 0.5215898156166077, acc: 89.0625, f1: 81.85560675883255, r: 0.6747084796759566
06/01/2019 10:49:30 *** evaluating ***
06/01/2019 10:49:31 step: 41, epoch: 40, acc: 58.54700854700855, f1: 28.25399650304923, r: 0.3197574605300561
06/01/2019 10:49:31 *** epoch: 42 ***
06/01/2019 10:49:31 *** training ***
06/01/2019 10:49:32 step: 1358, epoch: 41, batch: 4, loss: 0.3073628842830658, acc: 93.75, f1: 83.49206349206348, r: 0.7183444172472789
06/01/2019 10:49:33 step: 1363, epoch: 41, batch: 9, loss: 0.3852154612541199, acc: 90.625, f1: 91.1672140120416, r: 0.7209000664267095
06/01/2019 10:49:34 step: 1368, epoch: 41, batch: 14, loss: 0.5265786647796631, acc: 85.9375, f1: 87.79183761988855, r: 0.626455204890428
06/01/2019 10:49:35 step: 1373, epoch: 41, batch: 19, loss: 0.5366074442863464, acc: 85.9375, f1: 81.32956284312107, r: 0.6964839664687328
06/01/2019 10:49:36 step: 1378, epoch: 41, batch: 24, loss: 0.35331621766090393, acc: 92.1875, f1: 80.51041666666666, r: 0.6959722627547669
06/01/2019 10:49:37 step: 1383, epoch: 41, batch: 29, loss: 0.40399840474128723, acc: 92.1875, f1: 90.53900901358529, r: 0.6525780519780792
06/01/2019 10:49:38 *** evaluating ***
06/01/2019 10:49:38 step: 42, epoch: 41, acc: 56.41025641025641, f1: 27.10493953679033, r: 0.3225903498825237
06/01/2019 10:49:38 *** epoch: 43 ***
06/01/2019 10:49:38 *** training ***
06/01/2019 10:49:39 step: 1391, epoch: 42, batch: 4, loss: 0.5431559085845947, acc: 84.375, f1: 72.8681857358328, r: 0.6716938166272038
06/01/2019 10:49:40 step: 1396, epoch: 42, batch: 9, loss: 0.32148680090904236, acc: 92.1875, f1: 91.0106434918465, r: 0.6802479495395238
06/01/2019 10:49:41 step: 1401, epoch: 42, batch: 14, loss: 0.352668821811676, acc: 93.75, f1: 93.72484658198944, r: 0.7125172375632229
06/01/2019 10:49:42 step: 1406, epoch: 42, batch: 19, loss: 0.2933424115180969, acc: 93.75, f1: 93.81332952761524, r: 0.6788482133234982
06/01/2019 10:49:44 step: 1411, epoch: 42, batch: 24, loss: 0.2774753272533417, acc: 96.875, f1: 90.90583601861798, r: 0.7006386761179596
06/01/2019 10:49:45 step: 1416, epoch: 42, batch: 29, loss: 0.3845328688621521, acc: 85.9375, f1: 85.14472455648927, r: 0.6713739815364456
06/01/2019 10:49:45 *** evaluating ***
06/01/2019 10:49:45 step: 43, epoch: 42, acc: 55.98290598290598, f1: 28.307157188205572, r: 0.3261264557872904
06/01/2019 10:49:45 *** epoch: 44 ***
06/01/2019 10:49:45 *** training ***
06/01/2019 10:49:47 step: 1424, epoch: 43, batch: 4, loss: 0.6300215721130371, acc: 87.5, f1: 85.27777777777777, r: 0.7334557756097474
06/01/2019 10:49:48 step: 1429, epoch: 43, batch: 9, loss: 0.33229005336761475, acc: 92.1875, f1: 92.6655719759168, r: 0.6617051432287964
06/01/2019 10:49:49 step: 1434, epoch: 43, batch: 14, loss: 0.41245967149734497, acc: 87.5, f1: 81.97058979667676, r: 0.6734914077423082
06/01/2019 10:49:50 step: 1439, epoch: 43, batch: 19, loss: 0.36856749653816223, acc: 87.5, f1: 88.55866355866355, r: 0.7941035078398032
06/01/2019 10:49:51 step: 1444, epoch: 43, batch: 24, loss: 0.28463250398635864, acc: 96.875, f1: 96.5223162435706, r: 0.6493024820798768
06/01/2019 10:49:52 step: 1449, epoch: 43, batch: 29, loss: 0.4065288305282593, acc: 85.9375, f1: 87.0180443104288, r: 0.60829356661194
06/01/2019 10:49:53 *** evaluating ***
06/01/2019 10:49:53 step: 44, epoch: 43, acc: 54.700854700854705, f1: 26.779898993521282, r: 0.3118379603719786
06/01/2019 10:49:53 *** epoch: 45 ***
06/01/2019 10:49:53 *** training ***
06/01/2019 10:49:54 step: 1457, epoch: 44, batch: 4, loss: 0.3941497802734375, acc: 92.1875, f1: 85.20664869721475, r: 0.6799856467047772
06/01/2019 10:49:55 step: 1462, epoch: 44, batch: 9, loss: 0.25996121764183044, acc: 95.3125, f1: 80.61876032464268, r: 0.7575766320751665
06/01/2019 10:49:57 step: 1467, epoch: 44, batch: 14, loss: 0.3353137969970703, acc: 93.75, f1: 80.76839826839827, r: 0.8011547753722015
06/01/2019 10:49:58 step: 1472, epoch: 44, batch: 19, loss: 0.45167243480682373, acc: 82.8125, f1: 72.17319042021929, r: 0.7078190440788787
06/01/2019 10:49:59 step: 1477, epoch: 44, batch: 24, loss: 0.47833263874053955, acc: 87.5, f1: 86.80714427866609, r: 0.6156364259119566
06/01/2019 10:50:00 step: 1482, epoch: 44, batch: 29, loss: 0.3652603030204773, acc: 90.625, f1: 89.55555762856274, r: 0.6158648941508335
06/01/2019 10:50:00 *** evaluating ***
06/01/2019 10:50:00 step: 45, epoch: 44, acc: 53.41880341880342, f1: 26.040673059166696, r: 0.31551154199140324
06/01/2019 10:50:00 *** epoch: 46 ***
06/01/2019 10:50:00 *** training ***
06/01/2019 10:50:01 step: 1490, epoch: 45, batch: 4, loss: 0.3564348518848419, acc: 90.625, f1: 86.07948655118467, r: 0.642223611444014
06/01/2019 10:50:03 step: 1495, epoch: 45, batch: 9, loss: 0.2863527536392212, acc: 93.75, f1: 93.7911877394636, r: 0.748280424095374
06/01/2019 10:50:04 step: 1500, epoch: 45, batch: 14, loss: 0.2787821292877197, acc: 93.75, f1: 88.90096618357488, r: 0.7653822154999248
06/01/2019 10:50:05 step: 1505, epoch: 45, batch: 19, loss: 0.30642277002334595, acc: 92.1875, f1: 90.70750677893535, r: 0.6306878933753977
06/01/2019 10:50:06 step: 1510, epoch: 45, batch: 24, loss: 0.44650766253471375, acc: 82.8125, f1: 80.90922833779976, r: 0.6414602314532086
06/01/2019 10:50:07 step: 1515, epoch: 45, batch: 29, loss: 0.9122156500816345, acc: 93.75, f1: 96.71428571428571, r: 0.7516650649082439
06/01/2019 10:50:08 *** evaluating ***
06/01/2019 10:50:08 step: 46, epoch: 45, acc: 55.55555555555556, f1: 27.288541928151055, r: 0.3176890739862755
06/01/2019 10:50:08 *** epoch: 47 ***
06/01/2019 10:50:08 *** training ***
06/01/2019 10:50:09 step: 1523, epoch: 46, batch: 4, loss: 0.2973773777484894, acc: 95.3125, f1: 92.109375, r: 0.7858199179313737
06/01/2019 10:50:10 step: 1528, epoch: 46, batch: 9, loss: 0.36640870571136475, acc: 93.75, f1: 91.33131676685684, r: 0.6665895914502362
06/01/2019 10:50:12 step: 1533, epoch: 46, batch: 14, loss: 0.2925046384334564, acc: 90.625, f1: 82.89351851851852, r: 0.777191178674917
06/01/2019 10:50:13 step: 1538, epoch: 46, batch: 19, loss: 0.35671114921569824, acc: 87.5, f1: 87.86562942483073, r: 0.7391912970027328
06/01/2019 10:50:14 step: 1543, epoch: 46, batch: 24, loss: 0.25542885065078735, acc: 92.1875, f1: 84.75228712174525, r: 0.6569670865757579
06/01/2019 10:50:15 step: 1548, epoch: 46, batch: 29, loss: 0.340611070394516, acc: 89.0625, f1: 88.69669337754443, r: 0.683927977008798
06/01/2019 10:50:16 *** evaluating ***
06/01/2019 10:50:16 step: 47, epoch: 46, acc: 55.98290598290598, f1: 27.663087993389183, r: 0.3144999106039386
06/01/2019 10:50:16 *** epoch: 48 ***
06/01/2019 10:50:16 *** training ***
06/01/2019 10:50:18 step: 1556, epoch: 47, batch: 4, loss: 0.3218696713447571, acc: 93.75, f1: 93.61352932781503, r: 0.6316835216700672
06/01/2019 10:50:19 step: 1561, epoch: 47, batch: 9, loss: 0.2841598689556122, acc: 93.75, f1: 83.73015873015872, r: 0.6503935281240691
06/01/2019 10:50:20 step: 1566, epoch: 47, batch: 14, loss: 0.3951700031757355, acc: 92.1875, f1: 89.49732905982906, r: 0.7304932184234025
06/01/2019 10:50:21 step: 1571, epoch: 47, batch: 19, loss: 0.21847529709339142, acc: 95.3125, f1: 91.96752321752322, r: 0.8041184332067831
06/01/2019 10:50:22 step: 1576, epoch: 47, batch: 24, loss: 0.590401291847229, acc: 92.1875, f1: 95.01439591481964, r: 0.7580349657221135
06/01/2019 10:50:23 step: 1581, epoch: 47, batch: 29, loss: 0.3081147372722626, acc: 96.875, f1: 93.52093342484558, r: 0.6317996498350857
06/01/2019 10:50:24 *** evaluating ***
06/01/2019 10:50:24 step: 48, epoch: 47, acc: 55.12820512820513, f1: 26.601645351645352, r: 0.3198245005326253
06/01/2019 10:50:24 *** epoch: 49 ***
06/01/2019 10:50:24 *** training ***
06/01/2019 10:50:25 step: 1589, epoch: 48, batch: 4, loss: 0.424364298582077, acc: 87.5, f1: 73.00595238095238, r: 0.673652487270228
06/01/2019 10:50:26 step: 1594, epoch: 48, batch: 9, loss: 0.2659302055835724, acc: 95.3125, f1: 94.80844155844156, r: 0.7605434535797398
06/01/2019 10:50:27 step: 1599, epoch: 48, batch: 14, loss: 0.3761509656906128, acc: 89.0625, f1: 85.89736652236653, r: 0.7243060264373893
06/01/2019 10:50:28 step: 1604, epoch: 48, batch: 19, loss: 0.2445746511220932, acc: 95.3125, f1: 81.54761904761905, r: 0.6894888556914182
06/01/2019 10:50:29 step: 1609, epoch: 48, batch: 24, loss: 0.29927852749824524, acc: 92.1875, f1: 80.33371969477446, r: 0.6661400690141871
06/01/2019 10:50:31 step: 1614, epoch: 48, batch: 29, loss: 0.2969965636730194, acc: 92.1875, f1: 92.72262264589631, r: 0.6434750865719473
06/01/2019 10:50:31 *** evaluating ***
06/01/2019 10:50:32 step: 49, epoch: 48, acc: 56.837606837606835, f1: 26.47184930092913, r: 0.3135310612786831
06/01/2019 10:50:32 *** epoch: 50 ***
06/01/2019 10:50:32 *** training ***
06/01/2019 10:50:33 step: 1622, epoch: 49, batch: 4, loss: 0.2922082543373108, acc: 92.1875, f1: 90.9013605442177, r: 0.65275955640952
06/01/2019 10:50:34 step: 1627, epoch: 49, batch: 9, loss: 0.2472926676273346, acc: 92.1875, f1: 91.59729933249096, r: 0.6746665296273091
06/01/2019 10:50:35 step: 1632, epoch: 49, batch: 14, loss: 0.3038426339626312, acc: 95.3125, f1: 93.11260168403025, r: 0.7003110552697989
06/01/2019 10:50:36 step: 1637, epoch: 49, batch: 19, loss: 0.28831160068511963, acc: 93.75, f1: 91.46074193224088, r: 0.6435607908764888
06/01/2019 10:50:37 step: 1642, epoch: 49, batch: 24, loss: 0.3155399262905121, acc: 92.1875, f1: 86.39708244971402, r: 0.6404933468470336
06/01/2019 10:50:38 step: 1647, epoch: 49, batch: 29, loss: 0.3185504972934723, acc: 92.1875, f1: 77.12654908021041, r: 0.6971068939745126
06/01/2019 10:50:39 *** evaluating ***
06/01/2019 10:50:39 step: 50, epoch: 49, acc: 55.55555555555556, f1: 27.88398899921505, r: 0.3145734395758775
06/01/2019 10:50:39 *** epoch: 51 ***
06/01/2019 10:50:39 *** training ***
06/01/2019 10:50:40 step: 1655, epoch: 50, batch: 4, loss: 0.2238069474697113, acc: 95.3125, f1: 95.01990762860328, r: 0.6727961427512335
06/01/2019 10:50:42 step: 1660, epoch: 50, batch: 9, loss: 0.35733455419540405, acc: 89.0625, f1: 91.31378229122589, r: 0.5416855203921589
06/01/2019 10:50:43 step: 1665, epoch: 50, batch: 14, loss: 0.21071191132068634, acc: 96.875, f1: 97.77284826974268, r: 0.7618218385715644
06/01/2019 10:50:44 step: 1670, epoch: 50, batch: 19, loss: 0.19565220177173615, acc: 96.875, f1: 94.89923007246377, r: 0.6877826629375999
06/01/2019 10:50:45 step: 1675, epoch: 50, batch: 24, loss: 0.21429234743118286, acc: 92.1875, f1: 90.05268009137978, r: 0.7506901922554332
06/01/2019 10:50:46 step: 1680, epoch: 50, batch: 29, loss: 0.328367680311203, acc: 90.625, f1: 85.45728808176824, r: 0.6777174209961419
06/01/2019 10:50:47 *** evaluating ***
06/01/2019 10:50:47 step: 51, epoch: 50, acc: 57.692307692307686, f1: 29.279465487940058, r: 0.31098992691191096
06/01/2019 10:50:47 *** epoch: 52 ***
06/01/2019 10:50:47 *** training ***
06/01/2019 10:50:48 step: 1688, epoch: 51, batch: 4, loss: 0.2275780588388443, acc: 93.75, f1: 94.62184873949579, r: 0.7880785454757722
06/01/2019 10:50:49 step: 1693, epoch: 51, batch: 9, loss: 0.2188052088022232, acc: 95.3125, f1: 95.69291991118618, r: 0.6776232618381496
06/01/2019 10:50:50 step: 1698, epoch: 51, batch: 14, loss: 0.3155129551887512, acc: 89.0625, f1: 81.18261398992297, r: 0.674029028005014
06/01/2019 10:50:52 step: 1703, epoch: 51, batch: 19, loss: 0.3027185797691345, acc: 90.625, f1: 79.59677419354838, r: 0.7211011548936386
06/01/2019 10:50:53 step: 1708, epoch: 51, batch: 24, loss: 0.33682525157928467, acc: 87.5, f1: 84.87730580898281, r: 0.6407918621879097
06/01/2019 10:50:54 step: 1713, epoch: 51, batch: 29, loss: 0.8130669593811035, acc: 95.3125, f1: 97.76470588235294, r: 0.693088091086002
06/01/2019 10:50:55 *** evaluating ***
06/01/2019 10:50:55 step: 52, epoch: 51, acc: 55.55555555555556, f1: 25.98125641444137, r: 0.31110186464429634
06/01/2019 10:50:55 *** epoch: 53 ***
06/01/2019 10:50:55 *** training ***
06/01/2019 10:50:56 step: 1721, epoch: 52, batch: 4, loss: 0.2982112467288971, acc: 95.3125, f1: 95.98172272861714, r: 0.7572336954969
06/01/2019 10:50:57 step: 1726, epoch: 52, batch: 9, loss: 0.1769980788230896, acc: 98.4375, f1: 97.95918367346938, r: 0.7453794963084663
06/01/2019 10:50:58 step: 1731, epoch: 52, batch: 14, loss: 0.2951161861419678, acc: 90.625, f1: 87.82952069716777, r: 0.7629546477041708
06/01/2019 10:50:59 step: 1736, epoch: 52, batch: 19, loss: 0.3814445734024048, acc: 89.0625, f1: 84.7035988308367, r: 0.7410649355898061
06/01/2019 10:51:00 step: 1741, epoch: 52, batch: 24, loss: 0.16914165019989014, acc: 96.875, f1: 97.45149911816577, r: 0.5685430739560521
06/01/2019 10:51:01 step: 1746, epoch: 52, batch: 29, loss: 0.2573055624961853, acc: 93.75, f1: 92.2545155993432, r: 0.796029610581369
06/01/2019 10:51:02 *** evaluating ***
06/01/2019 10:51:02 step: 53, epoch: 52, acc: 56.41025641025641, f1: 27.808324779201964, r: 0.3082737966031443
06/01/2019 10:51:02 *** epoch: 54 ***
06/01/2019 10:51:02 *** training ***
06/01/2019 10:51:04 step: 1754, epoch: 53, batch: 4, loss: 0.28199681639671326, acc: 93.75, f1: 90.80312049062049, r: 0.6938186064706592
06/01/2019 10:51:05 step: 1759, epoch: 53, batch: 9, loss: 0.17442572116851807, acc: 95.3125, f1: 83.11178983804571, r: 0.5993364175038505
06/01/2019 10:51:06 step: 1764, epoch: 53, batch: 14, loss: 0.2503220736980438, acc: 93.75, f1: 89.69015801313938, r: 0.6375372283513849
06/01/2019 10:51:07 step: 1769, epoch: 53, batch: 19, loss: 0.23687005043029785, acc: 93.75, f1: 94.98520634396039, r: 0.6621269438449323
06/01/2019 10:51:08 step: 1774, epoch: 53, batch: 24, loss: 0.1935068815946579, acc: 98.4375, f1: 96.42857142857143, r: 0.7639050620871132
06/01/2019 10:51:09 step: 1779, epoch: 53, batch: 29, loss: 0.15584984421730042, acc: 100.0, f1: 100.0, r: 0.6826011635970083
06/01/2019 10:51:10 *** evaluating ***
06/01/2019 10:51:10 step: 54, epoch: 53, acc: 52.13675213675214, f1: 27.26308818100668, r: 0.309683217241199
06/01/2019 10:51:10 *** epoch: 55 ***
06/01/2019 10:51:10 *** training ***
06/01/2019 10:51:11 step: 1787, epoch: 54, batch: 4, loss: 0.27023133635520935, acc: 95.3125, f1: 94.73042699429506, r: 0.6157257624291765
06/01/2019 10:51:12 step: 1792, epoch: 54, batch: 9, loss: 0.2710164487361908, acc: 93.75, f1: 80.75054178787349, r: 0.7871975526096662
06/01/2019 10:51:14 step: 1797, epoch: 54, batch: 14, loss: 0.2597067654132843, acc: 93.75, f1: 78.26747337240813, r: 0.6928639350508331
06/01/2019 10:51:15 step: 1802, epoch: 54, batch: 19, loss: 0.23769892752170563, acc: 92.1875, f1: 83.1638032388971, r: 0.7542039390928811
06/01/2019 10:51:16 step: 1807, epoch: 54, batch: 24, loss: 0.23419532179832458, acc: 95.3125, f1: 82.78155741062659, r: 0.6245721147232304
06/01/2019 10:51:17 step: 1812, epoch: 54, batch: 29, loss: 0.24620665609836578, acc: 95.3125, f1: 80.75626647055219, r: 0.7272676049103177
06/01/2019 10:51:17 *** evaluating ***
06/01/2019 10:51:18 step: 55, epoch: 54, acc: 57.26495726495726, f1: 28.088973212966273, r: 0.30395300513562656
06/01/2019 10:51:18 *** epoch: 56 ***
06/01/2019 10:51:18 *** training ***
06/01/2019 10:51:19 step: 1820, epoch: 55, batch: 4, loss: 0.23028498888015747, acc: 96.875, f1: 96.16477272727273, r: 0.791253510651594
06/01/2019 10:51:20 step: 1825, epoch: 55, batch: 9, loss: 0.3459300100803375, acc: 92.1875, f1: 92.18687474989996, r: 0.6834644794224908
06/01/2019 10:51:21 step: 1830, epoch: 55, batch: 14, loss: 0.24356044828891754, acc: 92.1875, f1: 92.48629385964912, r: 0.7540673755081295
06/01/2019 10:51:22 step: 1835, epoch: 55, batch: 19, loss: 0.19797740876674652, acc: 96.875, f1: 91.94971694971694, r: 0.7706713152483374
06/01/2019 10:51:23 step: 1840, epoch: 55, batch: 24, loss: 0.21994459629058838, acc: 96.875, f1: 83.43260188087774, r: 0.6941079553008289
06/01/2019 10:51:24 step: 1845, epoch: 55, batch: 29, loss: 0.2369852513074875, acc: 95.3125, f1: 92.0074211502783, r: 0.7535196241472324
06/01/2019 10:51:25 *** evaluating ***
06/01/2019 10:51:25 step: 56, epoch: 55, acc: 54.700854700854705, f1: 27.573879063402075, r: 0.30209982319104056
06/01/2019 10:51:25 *** epoch: 57 ***
06/01/2019 10:51:25 *** training ***
06/01/2019 10:51:26 step: 1853, epoch: 56, batch: 4, loss: 0.19798769056797028, acc: 93.75, f1: 87.26874269376488, r: 0.6782077003645149
06/01/2019 10:51:27 step: 1858, epoch: 56, batch: 9, loss: 0.2296857386827469, acc: 95.3125, f1: 95.3788010930868, r: 0.6825651624659043
06/01/2019 10:51:28 step: 1863, epoch: 56, batch: 14, loss: 0.13574980199337006, acc: 96.875, f1: 97.18137254901961, r: 0.7688635378015611
06/01/2019 10:51:29 step: 1868, epoch: 56, batch: 19, loss: 0.21141023933887482, acc: 96.875, f1: 85.06944444444444, r: 0.7418205272734132
06/01/2019 10:51:30 step: 1873, epoch: 56, batch: 24, loss: 0.12421490252017975, acc: 96.875, f1: 89.77272727272728, r: 0.6103347903336879
06/01/2019 10:51:31 step: 1878, epoch: 56, batch: 29, loss: 0.2190467268228531, acc: 93.75, f1: 91.42895786839792, r: 0.6674719816819275
06/01/2019 10:51:32 *** evaluating ***
06/01/2019 10:51:32 step: 57, epoch: 56, acc: 55.98290598290598, f1: 26.661878140346452, r: 0.2962843784796714
06/01/2019 10:51:32 *** epoch: 58 ***
06/01/2019 10:51:32 *** training ***
06/01/2019 10:51:33 step: 1886, epoch: 57, batch: 4, loss: 0.19800958037376404, acc: 96.875, f1: 81.70859538784067, r: 0.7644652856248356
06/01/2019 10:51:34 step: 1891, epoch: 57, batch: 9, loss: 0.17130333185195923, acc: 95.3125, f1: 83.47272544283413, r: 0.73259169792341
06/01/2019 10:51:35 step: 1896, epoch: 57, batch: 14, loss: 0.25201180577278137, acc: 96.875, f1: 97.44714222975092, r: 0.7647310266878505
06/01/2019 10:51:36 step: 1901, epoch: 57, batch: 19, loss: 0.25452902913093567, acc: 95.3125, f1: 92.39924863387978, r: 0.7462408585074457
06/01/2019 10:51:38 step: 1906, epoch: 57, batch: 24, loss: 0.22478584945201874, acc: 95.3125, f1: 93.08025308025309, r: 0.7034172336927431
06/01/2019 10:51:39 step: 1911, epoch: 57, batch: 29, loss: 0.16313718259334564, acc: 98.4375, f1: 97.92008757525998, r: 0.6424913145725167
06/01/2019 10:51:39 *** evaluating ***
06/01/2019 10:51:40 step: 58, epoch: 57, acc: 56.41025641025641, f1: 26.9702881152461, r: 0.29527298677508856
06/01/2019 10:51:40 *** epoch: 59 ***
06/01/2019 10:51:40 *** training ***
06/01/2019 10:51:41 step: 1919, epoch: 58, batch: 4, loss: 0.13035663962364197, acc: 98.4375, f1: 99.24762531740808, r: 0.6359779262263698
06/01/2019 10:51:42 step: 1924, epoch: 58, batch: 9, loss: 0.1584695279598236, acc: 98.4375, f1: 99.23404255319149, r: 0.7767610557130044
06/01/2019 10:51:43 step: 1929, epoch: 58, batch: 14, loss: 0.14414843916893005, acc: 96.875, f1: 97.31509625126647, r: 0.6483072893375674
06/01/2019 10:51:44 step: 1934, epoch: 58, batch: 19, loss: 0.14715085923671722, acc: 100.0, f1: 100.0, r: 0.7586190386920244
06/01/2019 10:51:45 step: 1939, epoch: 58, batch: 24, loss: 0.22717499732971191, acc: 93.75, f1: 77.49450549450549, r: 0.7555577146690701
06/01/2019 10:51:46 step: 1944, epoch: 58, batch: 29, loss: 0.4595201909542084, acc: 100.0, f1: 100.0, r: 0.7654504118001643
06/01/2019 10:51:47 *** evaluating ***
06/01/2019 10:51:47 step: 59, epoch: 58, acc: 53.41880341880342, f1: 27.18556500879031, r: 0.2959738199309646
06/01/2019 10:51:47 *** epoch: 60 ***
06/01/2019 10:51:47 *** training ***
06/01/2019 10:51:48 step: 1952, epoch: 59, batch: 4, loss: 0.21549691259860992, acc: 95.3125, f1: 97.5654761904762, r: 0.7368120482898104
06/01/2019 10:51:49 step: 1957, epoch: 59, batch: 9, loss: 0.16312867403030396, acc: 100.0, f1: 100.0, r: 0.655806838958056
06/01/2019 10:51:50 step: 1962, epoch: 59, batch: 14, loss: 0.8515193462371826, acc: 96.875, f1: 86.55626780626781, r: 0.7862095572376249
06/01/2019 10:51:52 step: 1967, epoch: 59, batch: 19, loss: 0.16571085155010223, acc: 96.875, f1: 96.53524808011394, r: 0.6711818452325945
06/01/2019 10:51:53 step: 1972, epoch: 59, batch: 24, loss: 0.1680203080177307, acc: 96.875, f1: 96.59715284715284, r: 0.7394921155483236
06/01/2019 10:51:54 step: 1977, epoch: 59, batch: 29, loss: 0.22954139113426208, acc: 92.1875, f1: 89.13947101797045, r: 0.7065968299797092
06/01/2019 10:51:54 *** evaluating ***
06/01/2019 10:51:54 step: 60, epoch: 59, acc: 55.98290598290598, f1: 27.484661283723145, r: 0.29344588931644455
06/01/2019 10:51:54 *** epoch: 61 ***
06/01/2019 10:51:54 *** training ***
06/01/2019 10:51:55 step: 1985, epoch: 60, batch: 4, loss: 0.13588280975818634, acc: 96.875, f1: 98.73563218390805, r: 0.7453865591056609
06/01/2019 10:51:56 step: 1990, epoch: 60, batch: 9, loss: 0.17208963632583618, acc: 98.4375, f1: 98.06763285024155, r: 0.7587197002962994
06/01/2019 10:51:57 step: 1995, epoch: 60, batch: 14, loss: 0.12409912049770355, acc: 96.875, f1: 96.2962962962963, r: 0.8060167943532013
06/01/2019 10:51:59 step: 2000, epoch: 60, batch: 19, loss: 0.17531324923038483, acc: 100.0, f1: 100.0, r: 0.7297321744719777
06/01/2019 10:52:00 step: 2005, epoch: 60, batch: 24, loss: 0.13473868370056152, acc: 98.4375, f1: 94.7454844006568, r: 0.6731779183637907
06/01/2019 10:52:01 step: 2010, epoch: 60, batch: 29, loss: 0.24533452093601227, acc: 95.3125, f1: 94.96392496392497, r: 0.617309055685729
06/01/2019 10:52:01 *** evaluating ***
06/01/2019 10:52:02 step: 61, epoch: 60, acc: 56.41025641025641, f1: 27.877790296268557, r: 0.2927254611513366
06/01/2019 10:52:02 *** epoch: 62 ***
06/01/2019 10:52:02 *** training ***
06/01/2019 10:52:03 step: 2018, epoch: 61, batch: 4, loss: 0.14502084255218506, acc: 96.875, f1: 96.83484504913076, r: 0.6769399877027256
06/01/2019 10:52:04 step: 2023, epoch: 61, batch: 9, loss: 0.151089608669281, acc: 95.3125, f1: 79.58294428882665, r: 0.669517305741117
06/01/2019 10:52:05 step: 2028, epoch: 61, batch: 14, loss: 0.14905230700969696, acc: 98.4375, f1: 99.24762531740808, r: 0.6840891466236521
06/01/2019 10:52:06 step: 2033, epoch: 61, batch: 19, loss: 0.11229794472455978, acc: 100.0, f1: 100.0, r: 0.6366406447919478
06/01/2019 10:52:07 step: 2038, epoch: 61, batch: 24, loss: 0.15491332113742828, acc: 95.3125, f1: 95.30296092796092, r: 0.8128887699458409
06/01/2019 10:52:08 step: 2043, epoch: 61, batch: 29, loss: 0.1930641084909439, acc: 95.3125, f1: 91.37626262626262, r: 0.7472369992780932
06/01/2019 10:52:09 *** evaluating ***
06/01/2019 10:52:09 step: 62, epoch: 61, acc: 56.41025641025641, f1: 27.696108922524022, r: 0.28960977330437565
06/01/2019 10:52:09 *** epoch: 63 ***
06/01/2019 10:52:09 *** training ***
06/01/2019 10:52:10 step: 2051, epoch: 62, batch: 4, loss: 0.5087214708328247, acc: 95.3125, f1: 95.84013315471115, r: 0.7407681399049323
06/01/2019 10:52:11 step: 2056, epoch: 62, batch: 9, loss: 0.17378933727741241, acc: 98.4375, f1: 99.28698752228165, r: 0.6849385941267423
06/01/2019 10:52:12 step: 2061, epoch: 62, batch: 14, loss: 0.17524857819080353, acc: 93.75, f1: 93.0945990252794, r: 0.6879902582322408
06/01/2019 10:52:13 step: 2066, epoch: 62, batch: 19, loss: 0.2096502184867859, acc: 95.3125, f1: 93.79310344827586, r: 0.6825246197996616
06/01/2019 10:52:14 step: 2071, epoch: 62, batch: 24, loss: 0.16625238955020905, acc: 98.4375, f1: 98.64135864135865, r: 0.6781947397147535
06/01/2019 10:52:15 step: 2076, epoch: 62, batch: 29, loss: 0.19054758548736572, acc: 98.4375, f1: 94.55782312925169, r: 0.6360259750452771
06/01/2019 10:52:16 *** evaluating ***
06/01/2019 10:52:16 step: 63, epoch: 62, acc: 55.12820512820513, f1: 26.679903877484524, r: 0.28321453909269173
06/01/2019 10:52:16 *** epoch: 64 ***
06/01/2019 10:52:16 *** training ***
06/01/2019 10:52:18 step: 2084, epoch: 63, batch: 4, loss: 0.16878797113895416, acc: 96.875, f1: 90.92261904761905, r: 0.8004905669156123
06/01/2019 10:52:18 step: 2089, epoch: 63, batch: 9, loss: 0.4841404855251312, acc: 95.3125, f1: 93.89189819614347, r: 0.7886341088089929
06/01/2019 10:52:20 step: 2094, epoch: 63, batch: 14, loss: 0.13545098900794983, acc: 98.4375, f1: 99.22027290448344, r: 0.6814278724733855
06/01/2019 10:52:21 step: 2099, epoch: 63, batch: 19, loss: 0.13626420497894287, acc: 98.4375, f1: 98.67434153148439, r: 0.6966386375142332
06/01/2019 10:52:22 step: 2104, epoch: 63, batch: 24, loss: 0.16996081173419952, acc: 96.875, f1: 94.27952999381571, r: 0.679752141919701
06/01/2019 10:52:23 step: 2109, epoch: 63, batch: 29, loss: 0.11950032413005829, acc: 96.875, f1: 83.83405212673506, r: 0.672116310315073
06/01/2019 10:52:23 *** evaluating ***
06/01/2019 10:52:24 step: 64, epoch: 63, acc: 55.55555555555556, f1: 30.171655859155855, r: 0.29195048350716285
06/01/2019 10:52:24 *** epoch: 65 ***
06/01/2019 10:52:24 *** training ***
06/01/2019 10:52:25 step: 2117, epoch: 64, batch: 4, loss: 0.14280913770198822, acc: 96.875, f1: 97.84688995215312, r: 0.680633164924158
06/01/2019 10:52:26 step: 2122, epoch: 64, batch: 9, loss: 0.21156585216522217, acc: 96.875, f1: 92.32748538011695, r: 0.7584159131478553
06/01/2019 10:52:27 step: 2127, epoch: 64, batch: 14, loss: 0.10693295300006866, acc: 100.0, f1: 100.0, r: 0.6822544858087255
06/01/2019 10:52:28 step: 2132, epoch: 64, batch: 19, loss: 0.10734723508358002, acc: 98.4375, f1: 97.94941900205059, r: 0.6855340083050733
06/01/2019 10:52:29 step: 2137, epoch: 64, batch: 24, loss: 0.13685189187526703, acc: 96.875, f1: 94.60371854888882, r: 0.766446960315491
06/01/2019 10:52:30 step: 2142, epoch: 64, batch: 29, loss: 0.18814553320407867, acc: 95.3125, f1: 94.71780303030303, r: 0.7286119143598244
06/01/2019 10:52:31 *** evaluating ***
06/01/2019 10:52:31 step: 65, epoch: 64, acc: 55.98290598290598, f1: 28.783763215270064, r: 0.2873813199728181
06/01/2019 10:52:31 *** epoch: 66 ***
06/01/2019 10:52:31 *** training ***
06/01/2019 10:52:32 step: 2150, epoch: 65, batch: 4, loss: 0.17714637517929077, acc: 93.75, f1: 81.66208791208791, r: 0.7256659768706559
06/01/2019 10:52:33 step: 2155, epoch: 65, batch: 9, loss: 0.47888267040252686, acc: 98.4375, f1: 98.35600907029477, r: 0.8113570053973307
06/01/2019 10:52:34 step: 2160, epoch: 65, batch: 14, loss: 0.13019388914108276, acc: 96.875, f1: 83.33333333333333, r: 0.6961075650688147
06/01/2019 10:52:35 step: 2165, epoch: 65, batch: 19, loss: 0.13787832856178284, acc: 96.875, f1: 96.60130718954248, r: 0.6401673192459352
06/01/2019 10:52:36 step: 2170, epoch: 65, batch: 24, loss: 0.17801256477832794, acc: 90.625, f1: 79.93055555555554, r: 0.7215482672603655
06/01/2019 10:52:37 step: 2175, epoch: 65, batch: 29, loss: 0.1437361240386963, acc: 96.875, f1: 87.90627362055933, r: 0.6033193223336999
06/01/2019 10:52:38 *** evaluating ***
06/01/2019 10:52:39 step: 66, epoch: 65, acc: 53.84615384615385, f1: 28.53555708134916, r: 0.2827966062180842
06/01/2019 10:52:39 *** epoch: 67 ***
06/01/2019 10:52:39 *** training ***
06/01/2019 10:52:40 step: 2183, epoch: 66, batch: 4, loss: 0.10376360267400742, acc: 100.0, f1: 100.0, r: 0.8056668348179317
06/01/2019 10:52:41 step: 2188, epoch: 66, batch: 9, loss: 0.11938637495040894, acc: 98.4375, f1: 99.19078742608154, r: 0.7170549347878825
06/01/2019 10:52:42 step: 2193, epoch: 66, batch: 14, loss: 0.13336192071437836, acc: 98.4375, f1: 98.86128364389234, r: 0.7731853598308821
06/01/2019 10:52:43 step: 2198, epoch: 66, batch: 19, loss: 0.7801264524459839, acc: 96.875, f1: 97.10609438870308, r: 0.7596554829422979
06/01/2019 10:52:44 step: 2203, epoch: 66, batch: 24, loss: 0.1040874794125557, acc: 98.4375, f1: 94.6969696969697, r: 0.7049221418559526
06/01/2019 10:52:45 step: 2208, epoch: 66, batch: 29, loss: 0.1584426313638687, acc: 96.875, f1: 98.1896428415953, r: 0.6223490420175086
06/01/2019 10:52:46 *** evaluating ***
06/01/2019 10:52:46 step: 67, epoch: 66, acc: 57.26495726495726, f1: 28.437091990599665, r: 0.2897199888982928
06/01/2019 10:52:46 *** epoch: 68 ***
06/01/2019 10:52:46 *** training ***
06/01/2019 10:52:47 step: 2216, epoch: 67, batch: 4, loss: 0.12229454517364502, acc: 96.875, f1: 94.1951219512195, r: 0.8149518740519253
06/01/2019 10:52:48 step: 2221, epoch: 67, batch: 9, loss: 0.14355520904064178, acc: 96.875, f1: 94.16666666666667, r: 0.7246966894439753
06/01/2019 10:52:49 step: 2226, epoch: 67, batch: 14, loss: 0.14026837050914764, acc: 98.4375, f1: 99.30300807043287, r: 0.8235352798341548
06/01/2019 10:52:50 step: 2231, epoch: 67, batch: 19, loss: 0.16217899322509766, acc: 98.4375, f1: 96.86274509803921, r: 0.6707667474258934
06/01/2019 10:52:52 step: 2236, epoch: 67, batch: 24, loss: 0.14096519351005554, acc: 100.0, f1: 100.0, r: 0.7485751276618134
06/01/2019 10:52:53 step: 2241, epoch: 67, batch: 29, loss: 0.14556071162223816, acc: 96.875, f1: 97.51540832049307, r: 0.8280062801995808
06/01/2019 10:52:53 *** evaluating ***
06/01/2019 10:52:54 step: 68, epoch: 67, acc: 55.98290598290598, f1: 28.52968460111317, r: 0.28411109627181963
06/01/2019 10:52:54 *** epoch: 69 ***
06/01/2019 10:52:54 *** training ***
06/01/2019 10:52:55 step: 2249, epoch: 68, batch: 4, loss: 0.17373506724834442, acc: 95.3125, f1: 77.10416666666666, r: 0.7387634162454121
06/01/2019 10:52:56 step: 2254, epoch: 68, batch: 9, loss: 0.1857849359512329, acc: 90.625, f1: 73.10656524250498, r: 0.7635412203228917
06/01/2019 10:52:57 step: 2259, epoch: 68, batch: 14, loss: 0.17632608115673065, acc: 95.3125, f1: 83.66459627329192, r: 0.6527176747734014
06/01/2019 10:52:58 step: 2264, epoch: 68, batch: 19, loss: 0.13309958577156067, acc: 96.875, f1: 93.8288042489723, r: 0.6882259533640334
06/01/2019 10:52:59 step: 2269, epoch: 68, batch: 24, loss: 0.16986967623233795, acc: 96.875, f1: 95.90187590187591, r: 0.6738877782006519
06/01/2019 10:53:00 step: 2274, epoch: 68, batch: 29, loss: 0.12855856120586395, acc: 96.875, f1: 97.89115646258504, r: 0.7005324388710269
06/01/2019 10:53:01 *** evaluating ***
06/01/2019 10:53:01 step: 69, epoch: 68, acc: 58.119658119658126, f1: 29.482645832693024, r: 0.2862119212549611
06/01/2019 10:53:01 *** epoch: 70 ***
06/01/2019 10:53:01 *** training ***
06/01/2019 10:53:02 step: 2282, epoch: 69, batch: 4, loss: 0.2171739637851715, acc: 95.3125, f1: 84.22661994090565, r: 0.7215403795656798
06/01/2019 10:53:03 step: 2287, epoch: 69, batch: 9, loss: 0.15748706459999084, acc: 96.875, f1: 95.85290720061609, r: 0.6534650819073066
06/01/2019 10:53:04 step: 2292, epoch: 69, batch: 14, loss: 0.09874261170625687, acc: 100.0, f1: 100.0, r: 0.7046057073727217
06/01/2019 10:53:05 step: 2297, epoch: 69, batch: 19, loss: 0.7814946174621582, acc: 96.875, f1: 96.50531876918683, r: 0.6679731602162241
06/01/2019 10:53:06 step: 2302, epoch: 69, batch: 24, loss: 0.1088770404458046, acc: 95.3125, f1: 92.05152224824357, r: 0.6092075912174909
06/01/2019 10:53:08 step: 2307, epoch: 69, batch: 29, loss: 0.13115628063678741, acc: 96.875, f1: 98.1654151022111, r: 0.7255618340612229
06/01/2019 10:53:08 *** evaluating ***
06/01/2019 10:53:09 step: 70, epoch: 69, acc: 56.41025641025641, f1: 28.51807900794633, r: 0.28443878605905665
06/01/2019 10:53:09 *** epoch: 71 ***
06/01/2019 10:53:09 *** training ***
06/01/2019 10:53:10 step: 2315, epoch: 70, batch: 4, loss: 0.09393516927957535, acc: 98.4375, f1: 97.95918367346938, r: 0.7568677278427591
06/01/2019 10:53:11 step: 2320, epoch: 70, batch: 9, loss: 0.14723898470401764, acc: 98.4375, f1: 98.4077841662981, r: 0.6889024480410159
06/01/2019 10:53:12 step: 2325, epoch: 70, batch: 14, loss: 0.09261550009250641, acc: 100.0, f1: 100.0, r: 0.7072248055341643
06/01/2019 10:53:13 step: 2330, epoch: 70, batch: 19, loss: 0.14617100358009338, acc: 95.3125, f1: 93.00529970760235, r: 0.7349283560563125
06/01/2019 10:53:14 step: 2335, epoch: 70, batch: 24, loss: 0.7894821166992188, acc: 96.875, f1: 98.20532915360502, r: 0.7736381419103407
06/01/2019 10:53:15 step: 2340, epoch: 70, batch: 29, loss: 0.15799753367900848, acc: 98.4375, f1: 98.46041055718476, r: 0.7354218034487365
06/01/2019 10:53:16 *** evaluating ***
06/01/2019 10:53:16 step: 71, epoch: 70, acc: 58.119658119658126, f1: 28.769620910040828, r: 0.27949126524788126
06/01/2019 10:53:16 *** epoch: 72 ***
06/01/2019 10:53:16 *** training ***
06/01/2019 10:53:17 step: 2348, epoch: 71, batch: 4, loss: 0.20025207102298737, acc: 95.3125, f1: 96.63149350649351, r: 0.761927314540211
06/01/2019 10:53:18 step: 2353, epoch: 71, batch: 9, loss: 0.671576201915741, acc: 100.0, f1: 100.0, r: 0.7770624367196569
06/01/2019 10:53:19 step: 2358, epoch: 71, batch: 14, loss: 0.0903719887137413, acc: 100.0, f1: 100.0, r: 0.6880365922304139
06/01/2019 10:53:20 step: 2363, epoch: 71, batch: 19, loss: 0.08966612070798874, acc: 100.0, f1: 100.0, r: 0.5983761621958199
06/01/2019 10:53:22 step: 2368, epoch: 71, batch: 24, loss: 0.06845596432685852, acc: 100.0, f1: 100.0, r: 0.6835236561084456
06/01/2019 10:53:23 step: 2373, epoch: 71, batch: 29, loss: 0.15268957614898682, acc: 96.875, f1: 98.39285714285715, r: 0.6899845139963768
06/01/2019 10:53:23 *** evaluating ***
06/01/2019 10:53:24 step: 72, epoch: 71, acc: 58.54700854700855, f1: 29.41512547623623, r: 0.2799090466665006
06/01/2019 10:53:24 *** epoch: 73 ***
06/01/2019 10:53:24 *** training ***
06/01/2019 10:53:25 step: 2381, epoch: 72, batch: 4, loss: 0.6385973691940308, acc: 100.0, f1: 100.0, r: 0.7044622939560402
06/01/2019 10:53:26 step: 2386, epoch: 72, batch: 9, loss: 0.12014339119195938, acc: 95.3125, f1: 94.45196480981343, r: 0.7952786243600287
06/01/2019 10:53:27 step: 2391, epoch: 72, batch: 14, loss: 0.16385027766227722, acc: 95.3125, f1: 96.20651204281891, r: 0.7257218334230061
06/01/2019 10:53:28 step: 2396, epoch: 72, batch: 19, loss: 0.13239246606826782, acc: 100.0, f1: 100.0, r: 0.7992867968877796
06/01/2019 10:53:29 step: 2401, epoch: 72, batch: 24, loss: 0.1204221323132515, acc: 96.875, f1: 97.61347647292034, r: 0.7314836701976528
06/01/2019 10:53:30 step: 2406, epoch: 72, batch: 29, loss: 0.09663057327270508, acc: 100.0, f1: 100.0, r: 0.7785458924767654
06/01/2019 10:53:31 *** evaluating ***
06/01/2019 10:53:31 step: 73, epoch: 72, acc: 57.692307692307686, f1: 28.158835203012412, r: 0.2783990114272285
06/01/2019 10:53:31 *** epoch: 74 ***
06/01/2019 10:53:31 *** training ***
06/01/2019 10:53:33 step: 2414, epoch: 73, batch: 4, loss: 0.06964827328920364, acc: 100.0, f1: 100.0, r: 0.6812760733826286
06/01/2019 10:53:34 step: 2419, epoch: 73, batch: 9, loss: 0.8130187392234802, acc: 98.4375, f1: 97.75132275132276, r: 0.781642176543331
06/01/2019 10:53:35 step: 2424, epoch: 73, batch: 14, loss: 0.0705142393708229, acc: 100.0, f1: 100.0, r: 0.8166028084742037
06/01/2019 10:53:36 step: 2429, epoch: 73, batch: 19, loss: 0.10914697498083115, acc: 96.875, f1: 98.02197802197803, r: 0.6816341328688337
06/01/2019 10:53:37 step: 2434, epoch: 73, batch: 24, loss: 0.1431267112493515, acc: 98.4375, f1: 98.92023685127134, r: 0.6808020894790321
06/01/2019 10:53:38 step: 2439, epoch: 73, batch: 29, loss: 0.04994005337357521, acc: 100.0, f1: 100.0, r: 0.7443724164099184
06/01/2019 10:53:39 *** evaluating ***
06/01/2019 10:53:40 step: 74, epoch: 73, acc: 55.98290598290598, f1: 28.636756755002562, r: 0.2770244325506266
06/01/2019 10:53:40 *** epoch: 75 ***
06/01/2019 10:53:40 *** training ***
06/01/2019 10:53:41 step: 2447, epoch: 74, batch: 4, loss: 0.07139579951763153, acc: 100.0, f1: 100.0, r: 0.7926238949612628
06/01/2019 10:53:42 step: 2452, epoch: 74, batch: 9, loss: 0.07307247072458267, acc: 100.0, f1: 100.0, r: 0.7951663267845643
06/01/2019 10:53:43 step: 2457, epoch: 74, batch: 14, loss: 0.13783934712409973, acc: 96.875, f1: 95.00811688311688, r: 0.7691609486638394
06/01/2019 10:53:44 step: 2462, epoch: 74, batch: 19, loss: 0.07273872196674347, acc: 100.0, f1: 100.0, r: 0.7086922376772351
06/01/2019 10:53:45 step: 2467, epoch: 74, batch: 24, loss: 0.14310114085674286, acc: 98.4375, f1: 95.37037037037037, r: 0.7928931288356298
06/01/2019 10:53:46 step: 2472, epoch: 74, batch: 29, loss: 0.07773842662572861, acc: 100.0, f1: 100.0, r: 0.7591301152673311
06/01/2019 10:53:47 *** evaluating ***
06/01/2019 10:53:47 step: 75, epoch: 74, acc: 48.29059829059829, f1: 26.23226853508988, r: 0.2799546826638523
06/01/2019 10:53:47 *** epoch: 76 ***
06/01/2019 10:53:47 *** training ***
06/01/2019 10:53:48 step: 2480, epoch: 75, batch: 4, loss: 0.06336790323257446, acc: 100.0, f1: 100.0, r: 0.7113153279970778
06/01/2019 10:53:49 step: 2485, epoch: 75, batch: 9, loss: 0.08811590075492859, acc: 100.0, f1: 100.0, r: 0.7968048753606612
06/01/2019 10:53:51 step: 2490, epoch: 75, batch: 14, loss: 0.10581441968679428, acc: 100.0, f1: 100.0, r: 0.7540934915036347
06/01/2019 10:53:52 step: 2495, epoch: 75, batch: 19, loss: 0.1563721001148224, acc: 93.75, f1: 91.68012050878184, r: 0.8021813228296342
06/01/2019 10:53:53 step: 2500, epoch: 75, batch: 24, loss: 0.14415857195854187, acc: 95.3125, f1: 92.33630952380953, r: 0.7677134030043713
06/01/2019 10:53:54 step: 2505, epoch: 75, batch: 29, loss: 0.10122014582157135, acc: 100.0, f1: 100.0, r: 0.6472961750427259
06/01/2019 10:53:54 *** evaluating ***
06/01/2019 10:53:55 step: 76, epoch: 75, acc: 55.98290598290598, f1: 30.35541613492671, r: 0.27563179634505053
06/01/2019 10:53:55 *** epoch: 77 ***
06/01/2019 10:53:55 *** training ***
06/01/2019 10:53:56 step: 2513, epoch: 76, batch: 4, loss: 0.09450254589319229, acc: 98.4375, f1: 96.9047619047619, r: 0.8059000015796592
06/01/2019 10:53:57 step: 2518, epoch: 76, batch: 9, loss: 0.07151265442371368, acc: 100.0, f1: 100.0, r: 0.6770794134547159
06/01/2019 10:53:58 step: 2523, epoch: 76, batch: 14, loss: 0.13537649810314178, acc: 98.4375, f1: 98.49498327759196, r: 0.8134521177928058
06/01/2019 10:53:59 step: 2528, epoch: 76, batch: 19, loss: 0.12569387257099152, acc: 96.875, f1: 83.93472664601423, r: 0.6112105381359142
06/01/2019 10:54:00 step: 2533, epoch: 76, batch: 24, loss: 0.10034462809562683, acc: 100.0, f1: 100.0, r: 0.7839292113393412
06/01/2019 10:54:01 step: 2538, epoch: 76, batch: 29, loss: 1.2947719097137451, acc: 100.0, f1: 100.0, r: 0.7016597850115147
06/01/2019 10:54:02 *** evaluating ***
06/01/2019 10:54:02 step: 77, epoch: 76, acc: 58.119658119658126, f1: 29.215264383581214, r: 0.2815752984578247
06/01/2019 10:54:02 *** epoch: 78 ***
06/01/2019 10:54:02 *** training ***
06/01/2019 10:54:03 step: 2546, epoch: 77, batch: 4, loss: 0.16145174205303192, acc: 96.875, f1: 97.01335701335701, r: 0.6632362145085486
06/01/2019 10:54:04 step: 2551, epoch: 77, batch: 9, loss: 0.07335590571165085, acc: 100.0, f1: 100.0, r: 0.7628918087052936
06/01/2019 10:54:05 step: 2556, epoch: 77, batch: 14, loss: 0.0667002946138382, acc: 100.0, f1: 100.0, r: 0.7035118604311283
06/01/2019 10:54:06 step: 2561, epoch: 77, batch: 19, loss: 0.07803478091955185, acc: 100.0, f1: 100.0, r: 0.8121515161811983
06/01/2019 10:54:07 step: 2566, epoch: 77, batch: 24, loss: 0.10569573938846588, acc: 98.4375, f1: 99.16891284815813, r: 0.7225881078843556
06/01/2019 10:54:08 step: 2571, epoch: 77, batch: 29, loss: 0.08682966977357864, acc: 100.0, f1: 100.0, r: 0.7932405182097879
06/01/2019 10:54:09 *** evaluating ***
06/01/2019 10:54:09 step: 78, epoch: 77, acc: 56.837606837606835, f1: 28.010351046535252, r: 0.27828218910375946
06/01/2019 10:54:09 *** epoch: 79 ***
06/01/2019 10:54:09 *** training ***
06/01/2019 10:54:11 step: 2579, epoch: 78, batch: 4, loss: 0.09309066087007523, acc: 98.4375, f1: 95.0, r: 0.7807933852896857
06/01/2019 10:54:12 step: 2584, epoch: 78, batch: 9, loss: 0.08278157562017441, acc: 98.4375, f1: 97.24489795918367, r: 0.8059588793995608
06/01/2019 10:54:13 step: 2589, epoch: 78, batch: 14, loss: 0.1076052337884903, acc: 98.4375, f1: 99.11483253588517, r: 0.7961164486460053
06/01/2019 10:54:14 step: 2594, epoch: 78, batch: 19, loss: 0.0785422995686531, acc: 98.4375, f1: 95.43010752688173, r: 0.7401013499669461
06/01/2019 10:54:15 step: 2599, epoch: 78, batch: 24, loss: 0.1061861664056778, acc: 98.4375, f1: 93.82716049382717, r: 0.6071954179498702
06/01/2019 10:54:16 step: 2604, epoch: 78, batch: 29, loss: 0.0877753421664238, acc: 98.4375, f1: 97.93650793650795, r: 0.7911161282285153
06/01/2019 10:54:16 *** evaluating ***
06/01/2019 10:54:17 step: 79, epoch: 78, acc: 58.119658119658126, f1: 31.767752715121134, r: 0.2789638399497862
06/01/2019 10:54:17 *** epoch: 80 ***
06/01/2019 10:54:17 *** training ***
06/01/2019 10:54:18 step: 2612, epoch: 79, batch: 4, loss: 0.07591593265533447, acc: 100.0, f1: 100.0, r: 0.6975617470614895
06/01/2019 10:54:19 step: 2617, epoch: 79, batch: 9, loss: 0.10487814247608185, acc: 98.4375, f1: 99.24762531740808, r: 0.7160261893700831
06/01/2019 10:54:20 step: 2622, epoch: 79, batch: 14, loss: 0.059767842292785645, acc: 98.4375, f1: 94.66666666666667, r: 0.7001679414337172
06/01/2019 10:54:21 step: 2627, epoch: 79, batch: 19, loss: 0.0809851661324501, acc: 98.4375, f1: 99.18367346938776, r: 0.6897760356971332
06/01/2019 10:54:22 step: 2632, epoch: 79, batch: 24, loss: 0.12284893542528152, acc: 98.4375, f1: 86.53846153846155, r: 0.764426787617099
06/01/2019 10:54:23 step: 2637, epoch: 79, batch: 29, loss: 0.0722811371088028, acc: 100.0, f1: 100.0, r: 0.7873550526000721
06/01/2019 10:54:24 *** evaluating ***
06/01/2019 10:54:24 step: 80, epoch: 79, acc: 58.54700854700855, f1: 29.064742431994972, r: 0.2763805520707302
06/01/2019 10:54:24 *** epoch: 81 ***
06/01/2019 10:54:24 *** training ***
06/01/2019 10:54:25 step: 2645, epoch: 80, batch: 4, loss: 0.09901083260774612, acc: 98.4375, f1: 96.04395604395604, r: 0.7192819443591767
06/01/2019 10:54:26 step: 2650, epoch: 80, batch: 9, loss: 0.7810145616531372, acc: 98.4375, f1: 94.6969696969697, r: 0.7041335389664025
06/01/2019 10:54:27 step: 2655, epoch: 80, batch: 14, loss: 0.07634542137384415, acc: 100.0, f1: 100.0, r: 0.6860137561477466
06/01/2019 10:54:28 step: 2660, epoch: 80, batch: 19, loss: 0.08563050627708435, acc: 96.875, f1: 95.21428571428572, r: 0.8299224649011996
06/01/2019 10:54:29 step: 2665, epoch: 80, batch: 24, loss: 0.09131801873445511, acc: 98.4375, f1: 97.47899159663865, r: 0.7316523245846416
06/01/2019 10:54:31 step: 2670, epoch: 80, batch: 29, loss: 0.10440981388092041, acc: 98.4375, f1: 99.22727711774365, r: 0.7300040333607404
06/01/2019 10:54:31 *** evaluating ***
06/01/2019 10:54:31 step: 81, epoch: 80, acc: 57.692307692307686, f1: 29.03346130097497, r: 0.27855596688172407
06/01/2019 10:54:31 *** epoch: 82 ***
06/01/2019 10:54:31 *** training ***
06/01/2019 10:54:33 step: 2678, epoch: 81, batch: 4, loss: 0.12291043996810913, acc: 98.4375, f1: 99.03703703703704, r: 0.8159916943544432
06/01/2019 10:54:34 step: 2683, epoch: 81, batch: 9, loss: 0.08698195219039917, acc: 98.4375, f1: 99.0514075887393, r: 0.7677893084437575
06/01/2019 10:54:35 step: 2688, epoch: 81, batch: 14, loss: 0.06364124268293381, acc: 100.0, f1: 100.0, r: 0.7209202146774651
06/01/2019 10:54:36 step: 2693, epoch: 81, batch: 19, loss: 0.06772995740175247, acc: 98.4375, f1: 96.66048237476808, r: 0.6986445226439268
06/01/2019 10:54:37 step: 2698, epoch: 81, batch: 24, loss: 0.0926179364323616, acc: 98.4375, f1: 99.35897435897436, r: 0.8276071826905131
06/01/2019 10:54:38 step: 2703, epoch: 81, batch: 29, loss: 0.07806433737277985, acc: 98.4375, f1: 98.53854585312386, r: 0.7219494727074723
06/01/2019 10:54:39 *** evaluating ***
06/01/2019 10:54:39 step: 82, epoch: 81, acc: 58.54700854700855, f1: 31.883387445887447, r: 0.2782849269874625
06/01/2019 10:54:39 *** epoch: 83 ***
06/01/2019 10:54:39 *** training ***
06/01/2019 10:54:40 step: 2711, epoch: 82, batch: 4, loss: 0.04574594646692276, acc: 100.0, f1: 100.0, r: 0.678244228651677
06/01/2019 10:54:41 step: 2716, epoch: 82, batch: 9, loss: 0.12931612133979797, acc: 98.4375, f1: 98.80952380952381, r: 0.7622847770297039
06/01/2019 10:54:42 step: 2721, epoch: 82, batch: 14, loss: 0.08785389363765717, acc: 100.0, f1: 100.0, r: 0.6654523678864787
06/01/2019 10:54:43 step: 2726, epoch: 82, batch: 19, loss: 0.07999669760465622, acc: 100.0, f1: 100.0, r: 0.7572694964412021
06/01/2019 10:54:44 step: 2731, epoch: 82, batch: 24, loss: 0.07068340480327606, acc: 98.4375, f1: 98.93081761006289, r: 0.7735114513061565
06/01/2019 10:54:46 step: 2736, epoch: 82, batch: 29, loss: 0.06508457660675049, acc: 100.0, f1: 100.0, r: 0.6271533371513954
06/01/2019 10:54:46 *** evaluating ***
06/01/2019 10:54:47 step: 83, epoch: 82, acc: 54.700854700854705, f1: 26.25725070559105, r: 0.27070610927801053
06/01/2019 10:54:47 *** epoch: 84 ***
06/01/2019 10:54:47 *** training ***
06/01/2019 10:54:48 step: 2744, epoch: 83, batch: 4, loss: 0.07459527999162674, acc: 100.0, f1: 100.0, r: 0.7795501803770484
06/01/2019 10:54:49 step: 2749, epoch: 83, batch: 9, loss: 0.05585411563515663, acc: 100.0, f1: 100.0, r: 0.7623518696822229
06/01/2019 10:54:50 step: 2754, epoch: 83, batch: 14, loss: 0.12170056998729706, acc: 98.4375, f1: 98.00453514739228, r: 0.6861333978060825
06/01/2019 10:54:51 step: 2759, epoch: 83, batch: 19, loss: 0.0860244408249855, acc: 98.4375, f1: 98.8994708994709, r: 0.7288958751924843
06/01/2019 10:54:52 step: 2764, epoch: 83, batch: 24, loss: 0.1135452464222908, acc: 100.0, f1: 100.0, r: 0.74256149266374
06/01/2019 10:54:53 step: 2769, epoch: 83, batch: 29, loss: 0.08279795944690704, acc: 100.0, f1: 100.0, r: 0.7059659046829133
06/01/2019 10:54:54 *** evaluating ***
06/01/2019 10:54:54 step: 84, epoch: 83, acc: 59.401709401709404, f1: 29.544344544344547, r: 0.2769453771673531
06/01/2019 10:54:54 *** epoch: 85 ***
06/01/2019 10:54:54 *** training ***
06/01/2019 10:54:55 step: 2777, epoch: 84, batch: 4, loss: 0.10210604965686798, acc: 98.4375, f1: 97.03703703703704, r: 0.8495910403607886
06/01/2019 10:54:56 step: 2782, epoch: 84, batch: 9, loss: 0.0646170973777771, acc: 100.0, f1: 100.0, r: 0.6878248084021956
06/01/2019 10:54:57 step: 2787, epoch: 84, batch: 14, loss: 0.0852782353758812, acc: 98.4375, f1: 98.97857852177614, r: 0.6919777593
06/01/2019 10:54:58 step: 2792, epoch: 84, batch: 19, loss: 0.774360716342926, acc: 98.4375, f1: 94.61697722567288, r: 0.6748940643667836
06/01/2019 10:55:00 step: 2797, epoch: 84, batch: 24, loss: 0.0988907590508461, acc: 96.875, f1: 97.52628324056894, r: 0.7191958903344066
06/01/2019 10:55:01 step: 2802, epoch: 84, batch: 29, loss: 0.044470228254795074, acc: 100.0, f1: 100.0, r: 0.7469717823202184
06/01/2019 10:55:01 *** evaluating ***
06/01/2019 10:55:02 step: 85, epoch: 84, acc: 57.692307692307686, f1: 29.288632119514467, r: 0.2735795300752939
06/01/2019 10:55:02 *** epoch: 86 ***
06/01/2019 10:55:02 *** training ***
06/01/2019 10:55:03 step: 2810, epoch: 85, batch: 4, loss: 0.05718695744872093, acc: 98.4375, f1: 95.40229885057472, r: 0.7245590106876931
06/01/2019 10:55:04 step: 2815, epoch: 85, batch: 9, loss: 0.12341160327196121, acc: 96.875, f1: 97.35518449804165, r: 0.6766071312572752
06/01/2019 10:55:05 step: 2820, epoch: 85, batch: 14, loss: 0.06430047750473022, acc: 98.4375, f1: 99.23404255319149, r: 0.807535679447013
06/01/2019 10:55:06 step: 2825, epoch: 85, batch: 19, loss: 0.11209336668252945, acc: 98.4375, f1: 98.02102659245516, r: 0.70721742840416
06/01/2019 10:55:07 step: 2830, epoch: 85, batch: 24, loss: 0.08317147195339203, acc: 100.0, f1: 100.0, r: 0.6092388809584383
06/01/2019 10:55:08 step: 2835, epoch: 85, batch: 29, loss: 0.07186073064804077, acc: 98.4375, f1: 97.28070175438597, r: 0.7652604679844432
06/01/2019 10:55:09 *** evaluating ***
06/01/2019 10:55:09 step: 86, epoch: 85, acc: 55.98290598290598, f1: 28.233518061057726, r: 0.2766705552271976
06/01/2019 10:55:09 *** epoch: 87 ***
06/01/2019 10:55:09 *** training ***
06/01/2019 10:55:10 step: 2843, epoch: 86, batch: 4, loss: 0.08553067594766617, acc: 100.0, f1: 100.0, r: 0.6422777270943227
06/01/2019 10:55:11 step: 2848, epoch: 86, batch: 9, loss: 0.06587997078895569, acc: 98.4375, f1: 99.17287014061208, r: 0.6997142698418397
06/01/2019 10:55:12 step: 2853, epoch: 86, batch: 14, loss: 0.08678025752305984, acc: 96.875, f1: 91.14599686028257, r: 0.6632206809441682
06/01/2019 10:55:13 step: 2858, epoch: 86, batch: 19, loss: 0.06782075762748718, acc: 98.4375, f1: 98.94419306184012, r: 0.7068303409493536
06/01/2019 10:55:15 step: 2863, epoch: 86, batch: 24, loss: 0.1254439651966095, acc: 93.75, f1: 93.26909770631308, r: 0.6664903232820989
06/01/2019 10:55:16 step: 2868, epoch: 86, batch: 29, loss: 0.07334484159946442, acc: 98.4375, f1: 99.15164369034994, r: 0.7947649955532478
06/01/2019 10:55:17 *** evaluating ***
06/01/2019 10:55:17 step: 87, epoch: 86, acc: 55.98290598290598, f1: 28.00591837026214, r: 0.27668681562069164
06/01/2019 10:55:17 *** epoch: 88 ***
06/01/2019 10:55:17 *** training ***
06/01/2019 10:55:18 step: 2876, epoch: 87, batch: 4, loss: 0.0691697895526886, acc: 100.0, f1: 100.0, r: 0.8168764986661815
06/01/2019 10:55:19 step: 2881, epoch: 87, batch: 9, loss: 0.05776238813996315, acc: 100.0, f1: 100.0, r: 0.8303682671306921
06/01/2019 10:55:20 step: 2886, epoch: 87, batch: 14, loss: 0.08280426263809204, acc: 100.0, f1: 100.0, r: 0.7965738088318751
06/01/2019 10:55:21 step: 2891, epoch: 87, batch: 19, loss: 0.05889735370874405, acc: 100.0, f1: 100.0, r: 0.6481018148395944
06/01/2019 10:55:22 step: 2896, epoch: 87, batch: 24, loss: 0.05645408853888512, acc: 100.0, f1: 100.0, r: 0.7209721879813398
06/01/2019 10:55:23 step: 2901, epoch: 87, batch: 29, loss: 0.07927210628986359, acc: 98.4375, f1: 87.09677419354838, r: 0.8081528533689576
06/01/2019 10:55:24 *** evaluating ***
06/01/2019 10:55:24 step: 88, epoch: 87, acc: 58.54700854700855, f1: 29.335466195473693, r: 0.27532629316923557
06/01/2019 10:55:24 *** epoch: 89 ***
06/01/2019 10:55:24 *** training ***
06/01/2019 10:55:26 step: 2909, epoch: 88, batch: 4, loss: 0.07367295026779175, acc: 98.4375, f1: 98.3201581027668, r: 0.7541782000459409
06/01/2019 10:55:27 step: 2914, epoch: 88, batch: 9, loss: 0.09033463895320892, acc: 100.0, f1: 100.0, r: 0.6976013745791476
06/01/2019 10:55:28 step: 2919, epoch: 88, batch: 14, loss: 0.07482206076383591, acc: 96.875, f1: 92.26088313309408, r: 0.6185792179434662
06/01/2019 10:55:29 step: 2924, epoch: 88, batch: 19, loss: 0.058053355664014816, acc: 100.0, f1: 100.0, r: 0.8028475351194326
06/01/2019 10:55:30 step: 2929, epoch: 88, batch: 24, loss: 0.07998625934123993, acc: 98.4375, f1: 98.36601307189542, r: 0.8640470721198859
06/01/2019 10:55:31 step: 2934, epoch: 88, batch: 29, loss: 0.12815025448799133, acc: 95.3125, f1: 79.74358974358974, r: 0.5757495874265092
06/01/2019 10:55:32 *** evaluating ***
06/01/2019 10:55:32 step: 89, epoch: 88, acc: 54.700854700854705, f1: 31.21736337555617, r: 0.26933279135409016
06/01/2019 10:55:32 *** epoch: 90 ***
06/01/2019 10:55:32 *** training ***
06/01/2019 10:55:33 step: 2942, epoch: 89, batch: 4, loss: 0.4012119770050049, acc: 96.875, f1: 96.48322937940425, r: 0.6730280629263719
06/01/2019 10:55:34 step: 2947, epoch: 89, batch: 9, loss: 0.03529992327094078, acc: 100.0, f1: 100.0, r: 0.7805425116460531
06/01/2019 10:55:35 step: 2952, epoch: 89, batch: 14, loss: 0.08707304298877716, acc: 98.4375, f1: 90.47619047619048, r: 0.5966551816684429
06/01/2019 10:55:36 step: 2957, epoch: 89, batch: 19, loss: 0.07222894579172134, acc: 98.4375, f1: 96.85131195335276, r: 0.7145483058378471
06/01/2019 10:55:38 step: 2962, epoch: 89, batch: 24, loss: 0.08380948007106781, acc: 96.875, f1: 97.3881673881674, r: 0.609964811261998
06/01/2019 10:55:39 step: 2967, epoch: 89, batch: 29, loss: 0.11064665019512177, acc: 98.4375, f1: 97.95321637426902, r: 0.8007149539220824
06/01/2019 10:55:39 *** evaluating ***
06/01/2019 10:55:40 step: 90, epoch: 89, acc: 58.97435897435898, f1: 29.384078583077333, r: 0.27475998907183924
06/01/2019 10:55:40 *** epoch: 91 ***
06/01/2019 10:55:40 *** training ***
06/01/2019 10:55:41 step: 2975, epoch: 90, batch: 4, loss: 0.03832957148551941, acc: 100.0, f1: 100.0, r: 0.6661624886492202
06/01/2019 10:55:42 step: 2980, epoch: 90, batch: 9, loss: 0.06786278635263443, acc: 98.4375, f1: 95.43010752688173, r: 0.7895399774064261
06/01/2019 10:55:43 step: 2985, epoch: 90, batch: 14, loss: 0.08506748825311661, acc: 98.4375, f1: 99.24489795918367, r: 0.7680276726463179
06/01/2019 10:55:44 step: 2990, epoch: 90, batch: 19, loss: 0.0960509330034256, acc: 96.875, f1: 94.62669683257919, r: 0.7113997788817585
06/01/2019 10:55:45 step: 2995, epoch: 90, batch: 24, loss: 0.08849935233592987, acc: 98.4375, f1: 98.30316742081448, r: 0.8233547123725569
06/01/2019 10:55:46 step: 3000, epoch: 90, batch: 29, loss: 0.132663294672966, acc: 98.4375, f1: 96.82539682539682, r: 0.7649779618369256
06/01/2019 10:55:47 *** evaluating ***
06/01/2019 10:55:47 step: 91, epoch: 90, acc: 58.119658119658126, f1: 30.62294332918029, r: 0.270007044568187
06/01/2019 10:55:47 *** epoch: 92 ***
06/01/2019 10:55:47 *** training ***
06/01/2019 10:55:48 step: 3008, epoch: 91, batch: 4, loss: 0.04349145665764809, acc: 100.0, f1: 100.0, r: 0.782514017663579
06/01/2019 10:55:49 step: 3013, epoch: 91, batch: 9, loss: 0.081067755818367, acc: 100.0, f1: 100.0, r: 0.6617867637881373
06/01/2019 10:55:50 step: 3018, epoch: 91, batch: 14, loss: 0.054455358535051346, acc: 98.4375, f1: 87.0, r: 0.7463275662199341
06/01/2019 10:55:52 step: 3023, epoch: 91, batch: 19, loss: 0.06395239382982254, acc: 98.4375, f1: 99.02818270165209, r: 0.6812977442250243
06/01/2019 10:55:53 step: 3028, epoch: 91, batch: 24, loss: 0.07783177495002747, acc: 100.0, f1: 100.0, r: 0.6804773950740829
06/01/2019 10:55:54 step: 3033, epoch: 91, batch: 29, loss: 0.06139342486858368, acc: 98.4375, f1: 97.64172335600907, r: 0.6964073384345478
06/01/2019 10:55:55 *** evaluating ***
06/01/2019 10:55:55 step: 92, epoch: 91, acc: 57.26495726495726, f1: 31.72654120570787, r: 0.2716422377436374
06/01/2019 10:55:55 *** epoch: 93 ***
06/01/2019 10:55:55 *** training ***
06/01/2019 10:55:56 step: 3041, epoch: 92, batch: 4, loss: 0.08200813829898834, acc: 98.4375, f1: 99.24633936261843, r: 0.7580388823653068
06/01/2019 10:55:57 step: 3046, epoch: 92, batch: 9, loss: 0.06563732028007507, acc: 100.0, f1: 100.0, r: 0.665002973635888
06/01/2019 10:55:58 step: 3051, epoch: 92, batch: 14, loss: 0.07706454396247864, acc: 98.4375, f1: 97.03703703703704, r: 0.751744985604607
06/01/2019 10:55:59 step: 3056, epoch: 92, batch: 19, loss: 0.0591437853872776, acc: 100.0, f1: 100.0, r: 0.738934303076516
06/01/2019 10:56:00 step: 3061, epoch: 92, batch: 24, loss: 0.04347281903028488, acc: 100.0, f1: 100.0, r: 0.7547906410245002
06/01/2019 10:56:01 step: 3066, epoch: 92, batch: 29, loss: 0.09622181951999664, acc: 100.0, f1: 100.0, r: 0.8277495803584771
06/01/2019 10:56:02 *** evaluating ***
06/01/2019 10:56:02 step: 93, epoch: 92, acc: 57.692307692307686, f1: 27.826738020424198, r: 0.2719046113011435
06/01/2019 10:56:02 *** epoch: 94 ***
06/01/2019 10:56:02 *** training ***
06/01/2019 10:56:04 step: 3074, epoch: 93, batch: 4, loss: 0.07321145385503769, acc: 96.875, f1: 92.4449042289727, r: 0.6664078236387116
06/01/2019 10:56:05 step: 3079, epoch: 93, batch: 9, loss: 0.055856384336948395, acc: 100.0, f1: 100.0, r: 0.6761390016305322
06/01/2019 10:56:06 step: 3084, epoch: 93, batch: 14, loss: 0.7350708246231079, acc: 98.4375, f1: 99.24465733235077, r: 0.788928497250709
06/01/2019 10:56:07 step: 3089, epoch: 93, batch: 19, loss: 0.08458993583917618, acc: 98.4375, f1: 98.62098685628098, r: 0.6929276488949814
06/01/2019 10:56:08 step: 3094, epoch: 93, batch: 24, loss: 0.06329557299613953, acc: 100.0, f1: 100.0, r: 0.6512795238317323
06/01/2019 10:56:09 step: 3099, epoch: 93, batch: 29, loss: 0.09805799275636673, acc: 100.0, f1: 100.0, r: 0.7781572168944786
06/01/2019 10:56:10 *** evaluating ***
06/01/2019 10:56:10 step: 94, epoch: 93, acc: 58.119658119658126, f1: 29.016537534174713, r: 0.27559153216373833
06/01/2019 10:56:10 *** epoch: 95 ***
06/01/2019 10:56:10 *** training ***
06/01/2019 10:56:11 step: 3107, epoch: 94, batch: 4, loss: 0.061195001006126404, acc: 98.4375, f1: 98.14921920185078, r: 0.6431139711597466
06/01/2019 10:56:12 step: 3112, epoch: 94, batch: 9, loss: 0.10159517079591751, acc: 98.4375, f1: 97.06896551724138, r: 0.7645733010703012
06/01/2019 10:56:13 step: 3117, epoch: 94, batch: 14, loss: 0.04261302947998047, acc: 100.0, f1: 100.0, r: 0.8590411442854986
06/01/2019 10:56:14 step: 3122, epoch: 94, batch: 19, loss: 0.05173703283071518, acc: 100.0, f1: 100.0, r: 0.7720762600767063
06/01/2019 10:56:15 step: 3127, epoch: 94, batch: 24, loss: 0.048905614763498306, acc: 100.0, f1: 100.0, r: 0.7084591621624665
06/01/2019 10:56:16 step: 3132, epoch: 94, batch: 29, loss: 0.07952921837568283, acc: 96.875, f1: 96.08033689666343, r: 0.7303840849107922
06/01/2019 10:56:17 *** evaluating ***
06/01/2019 10:56:17 step: 95, epoch: 94, acc: 57.26495726495726, f1: 29.059132903404787, r: 0.27012963956353286
06/01/2019 10:56:17 *** epoch: 96 ***
06/01/2019 10:56:17 *** training ***
06/01/2019 10:56:18 step: 3140, epoch: 95, batch: 4, loss: 0.06275265663862228, acc: 100.0, f1: 100.0, r: 0.8126088408859009
06/01/2019 10:56:20 step: 3145, epoch: 95, batch: 9, loss: 0.06614357978105545, acc: 98.4375, f1: 98.44026940801133, r: 0.7198020954906847
06/01/2019 10:56:20 step: 3150, epoch: 95, batch: 14, loss: 0.06168622896075249, acc: 100.0, f1: 100.0, r: 0.7630341672775235
06/01/2019 10:56:21 step: 3155, epoch: 95, batch: 19, loss: 0.03697827830910683, acc: 100.0, f1: 100.0, r: 0.8346618817069033
06/01/2019 10:56:22 step: 3160, epoch: 95, batch: 24, loss: 0.05665824934840202, acc: 100.0, f1: 100.0, r: 0.7036069735608894
06/01/2019 10:56:23 step: 3165, epoch: 95, batch: 29, loss: 0.04978601634502411, acc: 100.0, f1: 100.0, r: 0.8061376642815323
06/01/2019 10:56:24 *** evaluating ***
06/01/2019 10:56:24 step: 96, epoch: 95, acc: 56.41025641025641, f1: 30.813738721218265, r: 0.26992563897730915
06/01/2019 10:56:24 *** epoch: 97 ***
06/01/2019 10:56:24 *** training ***
06/01/2019 10:56:25 step: 3173, epoch: 96, batch: 4, loss: 0.07629947364330292, acc: 96.875, f1: 91.76593521421108, r: 0.7249362593414868
06/01/2019 10:56:27 step: 3178, epoch: 96, batch: 9, loss: 0.024328775703907013, acc: 100.0, f1: 100.0, r: 0.8207658795149565
06/01/2019 10:56:27 step: 3183, epoch: 96, batch: 14, loss: 0.05827935039997101, acc: 98.4375, f1: 99.28167370027835, r: 0.7114657701968523
06/01/2019 10:56:29 step: 3188, epoch: 96, batch: 19, loss: 0.07589326053857803, acc: 98.4375, f1: 94.48621553884712, r: 0.7096428441317143
06/01/2019 10:56:30 step: 3193, epoch: 96, batch: 24, loss: 0.07917729020118713, acc: 98.4375, f1: 98.42118665648077, r: 0.6912744037036042
06/01/2019 10:56:31 step: 3198, epoch: 96, batch: 29, loss: 0.07192885875701904, acc: 100.0, f1: 100.0, r: 0.7083403000247659
06/01/2019 10:56:31 *** evaluating ***
06/01/2019 10:56:32 step: 97, epoch: 96, acc: 54.27350427350427, f1: 27.961299020746782, r: 0.2683234320253227
06/01/2019 10:56:32 *** epoch: 98 ***
06/01/2019 10:56:32 *** training ***
06/01/2019 10:56:33 step: 3206, epoch: 97, batch: 4, loss: 0.09248010069131851, acc: 96.875, f1: 96.59700722394221, r: 0.8186409583303482
06/01/2019 10:56:34 step: 3211, epoch: 97, batch: 9, loss: 0.03516359254717827, acc: 100.0, f1: 100.0, r: 0.7276862399732
06/01/2019 10:56:35 step: 3216, epoch: 97, batch: 14, loss: 0.06807485222816467, acc: 100.0, f1: 100.0, r: 0.7090496896282843
06/01/2019 10:56:36 step: 3221, epoch: 97, batch: 19, loss: 0.039512909948825836, acc: 98.4375, f1: 94.77726574500768, r: 0.6191448817437449
06/01/2019 10:56:38 step: 3226, epoch: 97, batch: 24, loss: 0.06031497195363045, acc: 98.4375, f1: 97.22222222222221, r: 0.783932749730812
06/01/2019 10:56:38 step: 3231, epoch: 97, batch: 29, loss: 0.05317172035574913, acc: 100.0, f1: 100.0, r: 0.7483074218032991
06/01/2019 10:56:39 *** evaluating ***
06/01/2019 10:56:39 step: 98, epoch: 97, acc: 57.692307692307686, f1: 28.288344971367728, r: 0.26903560223385253
06/01/2019 10:56:39 *** epoch: 99 ***
06/01/2019 10:56:39 *** training ***
06/01/2019 10:56:40 step: 3239, epoch: 98, batch: 4, loss: 0.09017450362443924, acc: 96.875, f1: 97.8567388493859, r: 0.7152334166686027
06/01/2019 10:56:42 step: 3244, epoch: 98, batch: 9, loss: 0.11646480858325958, acc: 96.875, f1: 92.90229885057471, r: 0.7755456772877388
06/01/2019 10:56:43 step: 3249, epoch: 98, batch: 14, loss: 0.6421250104904175, acc: 96.875, f1: 94.16298261125847, r: 0.7780483129457457
06/01/2019 10:56:44 step: 3254, epoch: 98, batch: 19, loss: 0.03784538060426712, acc: 100.0, f1: 100.0, r: 0.6660604809157152
06/01/2019 10:56:45 step: 3259, epoch: 98, batch: 24, loss: 0.05583071708679199, acc: 98.4375, f1: 95.4954954954955, r: 0.7647198532715505
06/01/2019 10:56:46 step: 3264, epoch: 98, batch: 29, loss: 0.06961797922849655, acc: 98.4375, f1: 94.87179487179486, r: 0.725170239775958
06/01/2019 10:56:46 *** evaluating ***
06/01/2019 10:56:47 step: 99, epoch: 98, acc: 57.26495726495726, f1: 28.83517845985629, r: 0.26959837992914215
06/01/2019 10:56:47 *** epoch: 100 ***
06/01/2019 10:56:47 *** training ***
06/01/2019 10:56:48 step: 3272, epoch: 99, batch: 4, loss: 0.1051979511976242, acc: 95.3125, f1: 91.78872359296888, r: 0.7899008522031902
06/01/2019 10:56:49 step: 3277, epoch: 99, batch: 9, loss: 0.36334311962127686, acc: 100.0, f1: 100.0, r: 0.7480363663667086
06/01/2019 10:56:50 step: 3282, epoch: 99, batch: 14, loss: 0.05396193265914917, acc: 98.4375, f1: 94.61697722567288, r: 0.6766278384480446
06/01/2019 10:56:51 step: 3287, epoch: 99, batch: 19, loss: 0.05209670960903168, acc: 100.0, f1: 100.0, r: 0.720222032724839
06/01/2019 10:56:52 step: 3292, epoch: 99, batch: 24, loss: 0.03599688038229942, acc: 100.0, f1: 100.0, r: 0.6607880553403614
06/01/2019 10:56:53 step: 3297, epoch: 99, batch: 29, loss: 0.0658012181520462, acc: 100.0, f1: 100.0, r: 0.7228240337810545
06/01/2019 10:56:54 *** evaluating ***
06/01/2019 10:56:54 step: 100, epoch: 99, acc: 58.54700854700855, f1: 28.84545511317509, r: 0.2687302282305786
06/01/2019 10:56:54 *** epoch: 101 ***
06/01/2019 10:56:54 *** training ***
06/01/2019 10:56:55 step: 3305, epoch: 100, batch: 4, loss: 0.07310248911380768, acc: 98.4375, f1: 97.61904761904762, r: 0.8434353951009254
06/01/2019 10:56:56 step: 3310, epoch: 100, batch: 9, loss: 0.038636431097984314, acc: 100.0, f1: 100.0, r: 0.7944119416654609
06/01/2019 10:56:57 step: 3315, epoch: 100, batch: 14, loss: 0.05090775713324547, acc: 98.4375, f1: 95.10204081632652, r: 0.659113150884295
06/01/2019 10:56:58 step: 3320, epoch: 100, batch: 19, loss: 0.034020911902189255, acc: 100.0, f1: 100.0, r: 0.60260236537156
06/01/2019 10:57:00 step: 3325, epoch: 100, batch: 24, loss: 0.04727741330862045, acc: 100.0, f1: 100.0, r: 0.8306758651303792
06/01/2019 10:57:01 step: 3330, epoch: 100, batch: 29, loss: 0.3654635548591614, acc: 100.0, f1: 100.0, r: 0.797120489773438
06/01/2019 10:57:01 *** evaluating ***
06/01/2019 10:57:02 step: 101, epoch: 100, acc: 57.26495726495726, f1: 28.983097731482577, r: 0.265494578210834
06/01/2019 10:57:02 *** epoch: 102 ***
06/01/2019 10:57:02 *** training ***
06/01/2019 10:57:03 step: 3338, epoch: 101, batch: 4, loss: 0.04083459824323654, acc: 100.0, f1: 100.0, r: 0.7768380010524885
06/01/2019 10:57:04 step: 3343, epoch: 101, batch: 9, loss: 0.10233432054519653, acc: 96.875, f1: 97.81204906204907, r: 0.7731462171774331
06/01/2019 10:57:05 step: 3348, epoch: 101, batch: 14, loss: 0.058557718992233276, acc: 100.0, f1: 100.0, r: 0.8274672925155317
06/01/2019 10:57:06 step: 3353, epoch: 101, batch: 19, loss: 0.06706054508686066, acc: 98.4375, f1: 95.0, r: 0.8055336224531339
06/01/2019 10:57:07 step: 3358, epoch: 101, batch: 24, loss: 0.7436552047729492, acc: 100.0, f1: 100.0, r: 0.8053142578842357
06/01/2019 10:57:09 step: 3363, epoch: 101, batch: 29, loss: 0.0548962727189064, acc: 98.4375, f1: 96.6137566137566, r: 0.7300388703586376
06/01/2019 10:57:09 *** evaluating ***
06/01/2019 10:57:10 step: 102, epoch: 101, acc: 58.119658119658126, f1: 28.15637473646718, r: 0.26785938069935883
06/01/2019 10:57:10 *** epoch: 103 ***
06/01/2019 10:57:10 *** training ***
06/01/2019 10:57:10 step: 3371, epoch: 102, batch: 4, loss: 0.04247388616204262, acc: 100.0, f1: 100.0, r: 0.7005442949189112
06/01/2019 10:57:12 step: 3376, epoch: 102, batch: 9, loss: 0.072637178003788, acc: 96.875, f1: 98.30043859649123, r: 0.8330454741903257
06/01/2019 10:57:13 step: 3381, epoch: 102, batch: 14, loss: 0.0354120247066021, acc: 100.0, f1: 100.0, r: 0.7197860632079881
06/01/2019 10:57:14 step: 3386, epoch: 102, batch: 19, loss: 0.04454633593559265, acc: 100.0, f1: 100.0, r: 0.6806154734983441
06/01/2019 10:57:15 step: 3391, epoch: 102, batch: 24, loss: 0.031315140426158905, acc: 100.0, f1: 100.0, r: 0.6048609909226879
06/01/2019 10:57:16 step: 3396, epoch: 102, batch: 29, loss: 0.030343778431415558, acc: 100.0, f1: 100.0, r: 0.8261297671897226
06/01/2019 10:57:17 *** evaluating ***
06/01/2019 10:57:17 step: 103, epoch: 102, acc: 57.692307692307686, f1: 29.4507843641507, r: 0.26836889256166174
06/01/2019 10:57:17 *** epoch: 104 ***
06/01/2019 10:57:17 *** training ***
06/01/2019 10:57:18 step: 3404, epoch: 103, batch: 4, loss: 0.03370637819170952, acc: 100.0, f1: 100.0, r: 0.6332325745309834
06/01/2019 10:57:19 step: 3409, epoch: 103, batch: 9, loss: 0.029580172151327133, acc: 100.0, f1: 100.0, r: 0.730488043312413
06/01/2019 10:57:21 step: 3414, epoch: 103, batch: 14, loss: 0.09824340790510178, acc: 95.3125, f1: 93.5205879391926, r: 0.6408419303081223
06/01/2019 10:57:22 step: 3419, epoch: 103, batch: 19, loss: 0.7172783613204956, acc: 100.0, f1: 100.0, r: 0.7784103287449001
06/01/2019 10:57:23 step: 3424, epoch: 103, batch: 24, loss: 0.031683389097452164, acc: 100.0, f1: 100.0, r: 0.7243922738371261
06/01/2019 10:57:24 step: 3429, epoch: 103, batch: 29, loss: 0.05349083989858627, acc: 100.0, f1: 100.0, r: 0.8147924004486357
06/01/2019 10:57:24 *** evaluating ***
06/01/2019 10:57:24 step: 104, epoch: 103, acc: 53.41880341880342, f1: 28.42812460203021, r: 0.2641531518882141
06/01/2019 10:57:24 *** epoch: 105 ***
06/01/2019 10:57:24 *** training ***
06/01/2019 10:57:26 step: 3437, epoch: 104, batch: 4, loss: 0.033991921693086624, acc: 100.0, f1: 100.0, r: 0.8002690142651339
06/01/2019 10:57:26 step: 3442, epoch: 104, batch: 9, loss: 0.05794339254498482, acc: 100.0, f1: 100.0, r: 0.5802054625842609
06/01/2019 10:57:28 step: 3447, epoch: 104, batch: 14, loss: 0.05158725380897522, acc: 100.0, f1: 100.0, r: 0.8370513088174706
06/01/2019 10:57:29 step: 3452, epoch: 104, batch: 19, loss: 0.03514048457145691, acc: 100.0, f1: 100.0, r: 0.7310520987577014
06/01/2019 10:57:30 step: 3457, epoch: 104, batch: 24, loss: 0.03555046394467354, acc: 100.0, f1: 100.0, r: 0.6008425252495629
06/01/2019 10:57:31 step: 3462, epoch: 104, batch: 29, loss: 0.04910608008503914, acc: 100.0, f1: 100.0, r: 0.7203714999422793
06/01/2019 10:57:32 *** evaluating ***
06/01/2019 10:57:32 step: 105, epoch: 104, acc: 56.41025641025641, f1: 27.794044584241327, r: 0.2674092268790323
06/01/2019 10:57:32 *** epoch: 106 ***
06/01/2019 10:57:32 *** training ***
06/01/2019 10:57:33 step: 3470, epoch: 105, batch: 4, loss: 0.05620034784078598, acc: 100.0, f1: 100.0, r: 0.7684504593908298
06/01/2019 10:57:34 step: 3475, epoch: 105, batch: 9, loss: 0.06626665592193604, acc: 98.4375, f1: 98.36907278767744, r: 0.7012900432398136
06/01/2019 10:57:36 step: 3480, epoch: 105, batch: 14, loss: 0.06919354945421219, acc: 98.4375, f1: 99.27272727272727, r: 0.8142201412417337
06/01/2019 10:57:37 step: 3485, epoch: 105, batch: 19, loss: 0.07659139484167099, acc: 98.4375, f1: 99.1951219512195, r: 0.8272440681141393
06/01/2019 10:57:38 step: 3490, epoch: 105, batch: 24, loss: 0.33856093883514404, acc: 100.0, f1: 100.0, r: 0.7558066560812263
06/01/2019 10:57:39 step: 3495, epoch: 105, batch: 29, loss: 0.034327197819948196, acc: 100.0, f1: 100.0, r: 0.7477452057641295
06/01/2019 10:57:39 *** evaluating ***
06/01/2019 10:57:40 step: 106, epoch: 105, acc: 56.41025641025641, f1: 30.805817657951806, r: 0.2670774617344123
06/01/2019 10:57:40 *** epoch: 107 ***
06/01/2019 10:57:40 *** training ***
06/01/2019 10:57:41 step: 3503, epoch: 106, batch: 4, loss: 0.36451447010040283, acc: 98.4375, f1: 99.0454076367389, r: 0.8332365854258184
06/01/2019 10:57:42 step: 3508, epoch: 106, batch: 9, loss: 0.055093370378017426, acc: 100.0, f1: 100.0, r: 0.6841129122503807
06/01/2019 10:57:43 step: 3513, epoch: 106, batch: 14, loss: 0.03277720510959625, acc: 98.4375, f1: 94.66666666666667, r: 0.6803416394510374
06/01/2019 10:57:44 step: 3518, epoch: 106, batch: 19, loss: 0.07847105711698532, acc: 100.0, f1: 100.0, r: 0.7880808244787126
06/01/2019 10:57:45 step: 3523, epoch: 106, batch: 24, loss: 0.04613029211759567, acc: 100.0, f1: 100.0, r: 0.7086489931042994
06/01/2019 10:57:46 step: 3528, epoch: 106, batch: 29, loss: 0.0686749666929245, acc: 100.0, f1: 100.0, r: 0.8122233073850883
06/01/2019 10:57:47 *** evaluating ***
06/01/2019 10:57:47 step: 107, epoch: 106, acc: 58.54700854700855, f1: 31.158237607652893, r: 0.26892745500537
06/01/2019 10:57:47 *** epoch: 108 ***
06/01/2019 10:57:47 *** training ***
06/01/2019 10:57:48 step: 3536, epoch: 107, batch: 4, loss: 0.3466114401817322, acc: 100.0, f1: 100.0, r: 0.717554255891216
06/01/2019 10:57:50 step: 3541, epoch: 107, batch: 9, loss: 0.09322984516620636, acc: 96.875, f1: 83.50095186816145, r: 0.6423668876808957
06/01/2019 10:57:51 step: 3546, epoch: 107, batch: 14, loss: 0.06064673140645027, acc: 98.4375, f1: 97.67080745341616, r: 0.796642442934557
06/01/2019 10:57:52 step: 3551, epoch: 107, batch: 19, loss: 0.06130450218915939, acc: 100.0, f1: 100.0, r: 0.760148915167822
06/01/2019 10:57:53 step: 3556, epoch: 107, batch: 24, loss: 0.046757884323596954, acc: 100.0, f1: 100.0, r: 0.7930506109066076
06/01/2019 10:57:54 step: 3561, epoch: 107, batch: 29, loss: 0.039054181426763535, acc: 100.0, f1: 100.0, r: 0.7237372322568192
06/01/2019 10:57:55 *** evaluating ***
06/01/2019 10:57:55 step: 108, epoch: 107, acc: 57.26495726495726, f1: 31.2765326225762, r: 0.26919288584710804
06/01/2019 10:57:55 *** epoch: 109 ***
06/01/2019 10:57:55 *** training ***
06/01/2019 10:57:56 step: 3569, epoch: 108, batch: 4, loss: 0.34033700823783875, acc: 100.0, f1: 100.0, r: 0.8329455634470175
06/01/2019 10:57:57 step: 3574, epoch: 108, batch: 9, loss: 0.03296235576272011, acc: 100.0, f1: 100.0, r: 0.7819521982802251
06/01/2019 10:57:58 step: 3579, epoch: 108, batch: 14, loss: 0.030009780079126358, acc: 100.0, f1: 100.0, r: 0.7848214692641625
06/01/2019 10:58:00 step: 3584, epoch: 108, batch: 19, loss: 0.043732982128858566, acc: 100.0, f1: 100.0, r: 0.6838883030925186
06/01/2019 10:58:01 step: 3589, epoch: 108, batch: 24, loss: 0.07808549702167511, acc: 100.0, f1: 100.0, r: 0.6818547846750916
06/01/2019 10:58:02 step: 3594, epoch: 108, batch: 29, loss: 0.0655764639377594, acc: 98.4375, f1: 96.95652173913044, r: 0.7497452875356888
06/01/2019 10:58:02 *** evaluating ***
06/01/2019 10:58:03 step: 109, epoch: 108, acc: 56.837606837606835, f1: 29.91215893818869, r: 0.27012277954418873
06/01/2019 10:58:03 *** epoch: 110 ***
06/01/2019 10:58:03 *** training ***
06/01/2019 10:58:04 step: 3602, epoch: 109, batch: 4, loss: 0.03440963104367256, acc: 100.0, f1: 100.0, r: 0.6700972310145494
06/01/2019 10:58:05 step: 3607, epoch: 109, batch: 9, loss: 0.05754294991493225, acc: 98.4375, f1: 98.94736842105263, r: 0.7763956098191012
06/01/2019 10:58:06 step: 3612, epoch: 109, batch: 14, loss: 0.03447499871253967, acc: 98.4375, f1: 98.38056680161942, r: 0.7993353198167094
06/01/2019 10:58:07 step: 3617, epoch: 109, batch: 19, loss: 0.030650313943624496, acc: 100.0, f1: 100.0, r: 0.726043147834823
06/01/2019 10:58:08 step: 3622, epoch: 109, batch: 24, loss: 0.022614091634750366, acc: 100.0, f1: 100.0, r: 0.718077577040494
06/01/2019 10:58:09 step: 3627, epoch: 109, batch: 29, loss: 0.06976848840713501, acc: 98.4375, f1: 99.10627007401202, r: 0.684306998145652
06/01/2019 10:58:10 *** evaluating ***
06/01/2019 10:58:10 step: 110, epoch: 109, acc: 55.55555555555556, f1: 28.762569476930967, r: 0.2691846091900933
06/01/2019 10:58:10 *** epoch: 111 ***
06/01/2019 10:58:10 *** training ***
06/01/2019 10:58:11 step: 3635, epoch: 110, batch: 4, loss: 0.0449240617454052, acc: 100.0, f1: 100.0, r: 0.5713102753547041
06/01/2019 10:58:12 step: 3640, epoch: 110, batch: 9, loss: 0.026822011917829514, acc: 100.0, f1: 100.0, r: 0.7963232124352825
06/01/2019 10:58:13 step: 3645, epoch: 110, batch: 14, loss: 0.07369326800107956, acc: 98.4375, f1: 85.71428571428572, r: 0.7385443911081014
06/01/2019 10:58:15 step: 3650, epoch: 110, batch: 19, loss: 0.01738719455897808, acc: 100.0, f1: 100.0, r: 0.7857123758860004
06/01/2019 10:58:16 step: 3655, epoch: 110, batch: 24, loss: 0.6990803480148315, acc: 100.0, f1: 100.0, r: 0.7769510410975765
06/01/2019 10:58:17 step: 3660, epoch: 110, batch: 29, loss: 0.0330306738615036, acc: 100.0, f1: 100.0, r: 0.6080607870227525
06/01/2019 10:58:17 *** evaluating ***
06/01/2019 10:58:18 step: 111, epoch: 110, acc: 58.54700854700855, f1: 29.978579975173098, r: 0.27022220044613393
06/01/2019 10:58:18 *** epoch: 112 ***
06/01/2019 10:58:18 *** training ***
06/01/2019 10:58:19 step: 3668, epoch: 111, batch: 4, loss: 0.027164757251739502, acc: 100.0, f1: 100.0, r: 0.7488485439957416
06/01/2019 10:58:20 step: 3673, epoch: 111, batch: 9, loss: 0.030586395412683487, acc: 100.0, f1: 100.0, r: 0.7488734991732937
06/01/2019 10:58:21 step: 3678, epoch: 111, batch: 14, loss: 0.027999257668852806, acc: 100.0, f1: 100.0, r: 0.789543609350987
06/01/2019 10:58:22 step: 3683, epoch: 111, batch: 19, loss: 0.031102443113923073, acc: 100.0, f1: 100.0, r: 0.7047160292554211
06/01/2019 10:58:23 step: 3688, epoch: 111, batch: 24, loss: 0.04581454396247864, acc: 98.4375, f1: 96.68202764976958, r: 0.6904970165152258
06/01/2019 10:58:24 step: 3693, epoch: 111, batch: 29, loss: 0.32438167929649353, acc: 100.0, f1: 100.0, r: 0.7690225552401603
06/01/2019 10:58:25 *** evaluating ***
06/01/2019 10:58:25 step: 112, epoch: 111, acc: 60.68376068376068, f1: 33.74202441789379, r: 0.27403387750014907
06/01/2019 10:58:25 *** epoch: 113 ***
06/01/2019 10:58:25 *** training ***
06/01/2019 10:58:27 step: 3701, epoch: 112, batch: 4, loss: 0.35544607043266296, acc: 98.4375, f1: 99.02136293113738, r: 0.689427856846946
06/01/2019 10:58:27 step: 3706, epoch: 112, batch: 9, loss: 0.05216893553733826, acc: 100.0, f1: 100.0, r: 0.8053127650146124
06/01/2019 10:58:28 step: 3711, epoch: 112, batch: 14, loss: 0.05154059827327728, acc: 98.4375, f1: 97.78325123152709, r: 0.7566017735349906
06/01/2019 10:58:29 step: 3716, epoch: 112, batch: 19, loss: 0.04255988076329231, acc: 98.4375, f1: 95.17543859649122, r: 0.7627385879179043
06/01/2019 10:58:31 step: 3721, epoch: 112, batch: 24, loss: 0.06096668541431427, acc: 96.875, f1: 89.65554475758557, r: 0.6893623836227389
06/01/2019 10:58:32 step: 3726, epoch: 112, batch: 29, loss: 0.7193204760551453, acc: 100.0, f1: 100.0, r: 0.6805947533278378
06/01/2019 10:58:33 *** evaluating ***
06/01/2019 10:58:33 step: 113, epoch: 112, acc: 61.111111111111114, f1: 34.11317065728831, r: 0.2686276917328143
06/01/2019 10:58:33 *** epoch: 114 ***
06/01/2019 10:58:33 *** training ***
06/01/2019 10:58:34 step: 3734, epoch: 113, batch: 4, loss: 0.03787669539451599, acc: 100.0, f1: 100.0, r: 0.7586821737556303
06/01/2019 10:58:35 step: 3739, epoch: 113, batch: 9, loss: 0.08765114843845367, acc: 100.0, f1: 100.0, r: 0.7351303555138817
06/01/2019 10:58:36 step: 3744, epoch: 113, batch: 14, loss: 0.041571710258722305, acc: 100.0, f1: 100.0, r: 0.7537782742137462
06/01/2019 10:58:37 step: 3749, epoch: 113, batch: 19, loss: 0.0457635261118412, acc: 100.0, f1: 100.0, r: 0.7016420675509275
06/01/2019 10:58:38 step: 3754, epoch: 113, batch: 24, loss: 0.038869116455316544, acc: 100.0, f1: 100.0, r: 0.8013026757854435
06/01/2019 10:58:40 step: 3759, epoch: 113, batch: 29, loss: 0.06617889553308487, acc: 100.0, f1: 100.0, r: 0.6373416651134783
06/01/2019 10:58:40 *** evaluating ***
06/01/2019 10:58:41 step: 114, epoch: 113, acc: 54.700854700854705, f1: 27.944696715842156, r: 0.267149562096374
06/01/2019 10:58:41 *** epoch: 115 ***
06/01/2019 10:58:41 *** training ***
06/01/2019 10:58:42 step: 3767, epoch: 114, batch: 4, loss: 0.043481532484292984, acc: 100.0, f1: 100.0, r: 0.7182849952824236
06/01/2019 10:58:43 step: 3772, epoch: 114, batch: 9, loss: 0.061802007257938385, acc: 100.0, f1: 100.0, r: 0.8403167995122263
06/01/2019 10:58:44 step: 3777, epoch: 114, batch: 14, loss: 0.04161820560693741, acc: 100.0, f1: 100.0, r: 0.7115431259372127
06/01/2019 10:58:45 step: 3782, epoch: 114, batch: 19, loss: 0.07638765871524811, acc: 98.4375, f1: 96.19047619047619, r: 0.6865283746427047
06/01/2019 10:58:46 step: 3787, epoch: 114, batch: 24, loss: 0.6363191604614258, acc: 96.875, f1: 95.80016622033429, r: 0.688370456183799
06/01/2019 10:58:47 step: 3792, epoch: 114, batch: 29, loss: 0.050448283553123474, acc: 98.4375, f1: 96.65024630541872, r: 0.6993659245739678
06/01/2019 10:58:48 *** evaluating ***
06/01/2019 10:58:48 step: 115, epoch: 114, acc: 58.54700854700855, f1: 29.402368464868466, r: 0.2699072912949961
06/01/2019 10:58:48 *** epoch: 116 ***
06/01/2019 10:58:48 *** training ***
06/01/2019 10:58:49 step: 3800, epoch: 115, batch: 4, loss: 0.04517282918095589, acc: 100.0, f1: 100.0, r: 0.6757613616980819
06/01/2019 10:58:50 step: 3805, epoch: 115, batch: 9, loss: 0.03557584807276726, acc: 100.0, f1: 100.0, r: 0.6185399074244529
06/01/2019 10:58:51 step: 3810, epoch: 115, batch: 14, loss: 0.02533002384006977, acc: 100.0, f1: 100.0, r: 0.7066221941568387
06/01/2019 10:58:52 step: 3815, epoch: 115, batch: 19, loss: 0.07416567206382751, acc: 98.4375, f1: 96.3718820861678, r: 0.7357499200243381
06/01/2019 10:58:53 step: 3820, epoch: 115, batch: 24, loss: 0.03195502236485481, acc: 100.0, f1: 100.0, r: 0.7832525486009252
06/01/2019 10:58:55 step: 3825, epoch: 115, batch: 29, loss: 0.059291765093803406, acc: 98.4375, f1: 96.82539682539682, r: 0.8071343944342647
06/01/2019 10:58:55 *** evaluating ***
06/01/2019 10:58:56 step: 116, epoch: 115, acc: 56.41025641025641, f1: 28.99592616056031, r: 0.26709304057671934
06/01/2019 10:58:56 *** epoch: 117 ***
06/01/2019 10:58:56 *** training ***
06/01/2019 10:58:57 step: 3833, epoch: 116, batch: 4, loss: 0.043194495141506195, acc: 100.0, f1: 100.0, r: 0.6994429403006541
06/01/2019 10:58:58 step: 3838, epoch: 116, batch: 9, loss: 0.02867068722844124, acc: 100.0, f1: 100.0, r: 0.7623368223380806
06/01/2019 10:58:59 step: 3843, epoch: 116, batch: 14, loss: 0.06900040805339813, acc: 100.0, f1: 100.0, r: 0.8246215746217792
06/01/2019 10:59:00 step: 3848, epoch: 116, batch: 19, loss: 0.028630655258893967, acc: 100.0, f1: 100.0, r: 0.7248570620559318
06/01/2019 10:59:01 step: 3853, epoch: 116, batch: 24, loss: 0.025842269882559776, acc: 100.0, f1: 100.0, r: 0.7172587920539799
06/01/2019 10:59:02 step: 3858, epoch: 116, batch: 29, loss: 0.11486809700727463, acc: 98.4375, f1: 96.04938271604938, r: 0.6361112716830767
06/01/2019 10:59:03 *** evaluating ***
06/01/2019 10:59:03 step: 117, epoch: 116, acc: 55.98290598290598, f1: 29.12933215506836, r: 0.2657054632491095
06/01/2019 10:59:03 *** epoch: 118 ***
06/01/2019 10:59:03 *** training ***
06/01/2019 10:59:04 step: 3866, epoch: 117, batch: 4, loss: 0.06685665994882584, acc: 96.875, f1: 84.57142857142857, r: 0.6599349042372639
06/01/2019 10:59:05 step: 3871, epoch: 117, batch: 9, loss: 0.028197823092341423, acc: 100.0, f1: 100.0, r: 0.6614462431425502
06/01/2019 10:59:06 step: 3876, epoch: 117, batch: 14, loss: 0.09076036512851715, acc: 98.4375, f1: 98.06763285024155, r: 0.8046210625253893
06/01/2019 10:59:07 step: 3881, epoch: 117, batch: 19, loss: 0.047891661524772644, acc: 100.0, f1: 100.0, r: 0.7897149616534058
06/01/2019 10:59:08 step: 3886, epoch: 117, batch: 24, loss: 0.04614106938242912, acc: 98.4375, f1: 97.67907162865147, r: 0.6800243772922079
06/01/2019 10:59:10 step: 3891, epoch: 117, batch: 29, loss: 0.03240306302905083, acc: 100.0, f1: 100.0, r: 0.8341404051814815
06/01/2019 10:59:10 *** evaluating ***
06/01/2019 10:59:11 step: 118, epoch: 117, acc: 57.26495726495726, f1: 28.81361560277245, r: 0.2723451327930346
06/01/2019 10:59:11 *** epoch: 119 ***
06/01/2019 10:59:11 *** training ***
06/01/2019 10:59:12 step: 3899, epoch: 118, batch: 4, loss: 0.02353491820394993, acc: 100.0, f1: 100.0, r: 0.5928118504229227
06/01/2019 10:59:13 step: 3904, epoch: 118, batch: 9, loss: 0.33528101444244385, acc: 100.0, f1: 100.0, r: 0.6601071792759565
06/01/2019 10:59:14 step: 3909, epoch: 118, batch: 14, loss: 0.10176871716976166, acc: 98.4375, f1: 98.26839826839827, r: 0.7890512145495108
06/01/2019 10:59:15 step: 3914, epoch: 118, batch: 19, loss: 0.7467853426933289, acc: 98.4375, f1: 95.45454545454545, r: 0.8265629846942637
06/01/2019 10:59:16 step: 3919, epoch: 118, batch: 24, loss: 0.04902827739715576, acc: 100.0, f1: 100.0, r: 0.6975267602217196
06/01/2019 10:59:17 step: 3924, epoch: 118, batch: 29, loss: 0.039356399327516556, acc: 100.0, f1: 100.0, r: 0.7481654085168126
06/01/2019 10:59:18 *** evaluating ***
06/01/2019 10:59:18 step: 119, epoch: 118, acc: 55.55555555555556, f1: 29.11653328895052, r: 0.26533451191622803
06/01/2019 10:59:18 *** epoch: 120 ***
06/01/2019 10:59:18 *** training ***
06/01/2019 10:59:19 step: 3932, epoch: 119, batch: 4, loss: 0.05373373627662659, acc: 98.4375, f1: 99.34167215273206, r: 0.7925044021262654
06/01/2019 10:59:20 step: 3937, epoch: 119, batch: 9, loss: 0.06021616607904434, acc: 98.4375, f1: 98.44322344322343, r: 0.8218096093468805
06/01/2019 10:59:21 step: 3942, epoch: 119, batch: 14, loss: 0.03227820619940758, acc: 100.0, f1: 100.0, r: 0.6730733633159166
06/01/2019 10:59:22 step: 3947, epoch: 119, batch: 19, loss: 0.8813420534133911, acc: 96.875, f1: 93.5337977386495, r: 0.7291154925934343
06/01/2019 10:59:23 step: 3952, epoch: 119, batch: 24, loss: 0.02375822141766548, acc: 100.0, f1: 100.0, r: 0.7035960658137784
06/01/2019 10:59:25 step: 3957, epoch: 119, batch: 29, loss: 0.029637297615408897, acc: 100.0, f1: 100.0, r: 0.8130171953942271
06/01/2019 10:59:25 *** evaluating ***
06/01/2019 10:59:26 step: 120, epoch: 119, acc: 59.82905982905983, f1: 30.570652173913043, r: 0.2691106356788897
06/01/2019 10:59:26 *** epoch: 121 ***
06/01/2019 10:59:26 *** training ***
06/01/2019 10:59:27 step: 3965, epoch: 120, batch: 4, loss: 0.022788003087043762, acc: 100.0, f1: 100.0, r: 0.6722905031961895
06/01/2019 10:59:28 step: 3970, epoch: 120, batch: 9, loss: 0.040044404566287994, acc: 100.0, f1: 100.0, r: 0.8030545011014091
06/01/2019 10:59:29 step: 3975, epoch: 120, batch: 14, loss: 0.0567353330552578, acc: 100.0, f1: 100.0, r: 0.7992672364768536
06/01/2019 10:59:30 step: 3980, epoch: 120, batch: 19, loss: 0.05331806465983391, acc: 98.4375, f1: 99.05284147557329, r: 0.7910224429787552
06/01/2019 10:59:31 step: 3985, epoch: 120, batch: 24, loss: 0.582306444644928, acc: 98.4375, f1: 97.38775510204081, r: 0.7156043249782176
06/01/2019 10:59:32 step: 3990, epoch: 120, batch: 29, loss: 0.02964065410196781, acc: 100.0, f1: 100.0, r: 0.7285873038918983
06/01/2019 10:59:33 *** evaluating ***
06/01/2019 10:59:33 step: 121, epoch: 120, acc: 60.68376068376068, f1: 31.545627298773525, r: 0.2710931148178639
06/01/2019 10:59:33 *** epoch: 122 ***
06/01/2019 10:59:33 *** training ***
06/01/2019 10:59:35 step: 3998, epoch: 121, batch: 4, loss: 0.02802926115691662, acc: 100.0, f1: 100.0, r: 0.8587628508419415
06/01/2019 10:59:36 step: 4003, epoch: 121, batch: 9, loss: 0.035880208015441895, acc: 98.4375, f1: 99.23215898825654, r: 0.7508343918160624
06/01/2019 10:59:36 step: 4008, epoch: 121, batch: 14, loss: 0.042502351105213165, acc: 98.4375, f1: 99.16582406471183, r: 0.8452192776578922
06/01/2019 10:59:37 step: 4013, epoch: 121, batch: 19, loss: 0.03398613631725311, acc: 100.0, f1: 100.0, r: 0.6741373156618983
06/01/2019 10:59:39 step: 4018, epoch: 121, batch: 24, loss: 0.08909161388874054, acc: 98.4375, f1: 87.24489795918367, r: 0.7878935306938277
06/01/2019 10:59:40 step: 4023, epoch: 121, batch: 29, loss: 0.02205124869942665, acc: 100.0, f1: 100.0, r: 0.8031257401099708
06/01/2019 10:59:41 *** evaluating ***
06/01/2019 10:59:41 step: 122, epoch: 121, acc: 55.98290598290598, f1: 28.7857580908267, r: 0.2674223696274975
06/01/2019 10:59:41 *** epoch: 123 ***
06/01/2019 10:59:41 *** training ***
06/01/2019 10:59:42 step: 4031, epoch: 122, batch: 4, loss: 0.026030808687210083, acc: 100.0, f1: 100.0, r: 0.6632104512852923
06/01/2019 10:59:43 step: 4036, epoch: 122, batch: 9, loss: 0.02466617152094841, acc: 100.0, f1: 100.0, r: 0.820133346278548
06/01/2019 10:59:44 step: 4041, epoch: 122, batch: 14, loss: 0.04692404344677925, acc: 98.4375, f1: 98.52579852579852, r: 0.823145001338467
06/01/2019 10:59:45 step: 4046, epoch: 122, batch: 19, loss: 0.03589716926217079, acc: 100.0, f1: 100.0, r: 0.7161762959745822
06/01/2019 10:59:46 step: 4051, epoch: 122, batch: 24, loss: 0.045527271926403046, acc: 98.4375, f1: 93.93939393939394, r: 0.6797441265209767
06/01/2019 10:59:47 step: 4056, epoch: 122, batch: 29, loss: 0.7173349261283875, acc: 100.0, f1: 100.0, r: 0.780229736281269
06/01/2019 10:59:48 *** evaluating ***
06/01/2019 10:59:48 step: 123, epoch: 122, acc: 58.119658119658126, f1: 29.449583333119843, r: 0.2687460429763008
06/01/2019 10:59:48 *** epoch: 124 ***
06/01/2019 10:59:48 *** training ***
06/01/2019 10:59:50 step: 4064, epoch: 123, batch: 4, loss: 0.031787045300006866, acc: 100.0, f1: 100.0, r: 0.7860128977360386
06/01/2019 10:59:51 step: 4069, epoch: 123, batch: 9, loss: 0.02803756855428219, acc: 100.0, f1: 100.0, r: 0.6460758080191902
06/01/2019 10:59:52 step: 4074, epoch: 123, batch: 14, loss: 0.032893840223550797, acc: 100.0, f1: 100.0, r: 0.8152446579960735
06/01/2019 10:59:53 step: 4079, epoch: 123, batch: 19, loss: 0.02939492091536522, acc: 100.0, f1: 100.0, r: 0.766844793785259
06/01/2019 10:59:54 step: 4084, epoch: 123, batch: 24, loss: 0.04425263777375221, acc: 100.0, f1: 100.0, r: 0.6999961817369678
06/01/2019 10:59:55 step: 4089, epoch: 123, batch: 29, loss: 0.04690811410546303, acc: 98.4375, f1: 97.979797979798, r: 0.6915161723834241
06/01/2019 10:59:56 *** evaluating ***
06/01/2019 10:59:56 step: 124, epoch: 123, acc: 55.55555555555556, f1: 30.08535969834625, r: 0.26602380214830845
06/01/2019 10:59:56 *** epoch: 125 ***
06/01/2019 10:59:56 *** training ***
06/01/2019 10:59:57 step: 4097, epoch: 124, batch: 4, loss: 0.0172499381005764, acc: 100.0, f1: 100.0, r: 0.7061865845186439
06/01/2019 10:59:58 step: 4102, epoch: 124, batch: 9, loss: 0.04852091148495674, acc: 100.0, f1: 100.0, r: 0.6453948320882535
06/01/2019 11:00:00 step: 4107, epoch: 124, batch: 14, loss: 0.028561025857925415, acc: 100.0, f1: 100.0, r: 0.7835145403873097
06/01/2019 11:00:01 step: 4112, epoch: 124, batch: 19, loss: 0.03305768594145775, acc: 100.0, f1: 100.0, r: 0.7578575991465452
06/01/2019 11:00:02 step: 4117, epoch: 124, batch: 24, loss: 0.023191645741462708, acc: 100.0, f1: 100.0, r: 0.7148833133804926
06/01/2019 11:00:03 step: 4122, epoch: 124, batch: 29, loss: 0.0373741053044796, acc: 100.0, f1: 100.0, r: 0.7205662171176762
06/01/2019 11:00:04 *** evaluating ***
06/01/2019 11:00:04 step: 125, epoch: 124, acc: 57.26495726495726, f1: 30.48019088496794, r: 0.2668974820738318
06/01/2019 11:00:04 *** epoch: 126 ***
06/01/2019 11:00:04 *** training ***
06/01/2019 11:00:05 step: 4130, epoch: 125, batch: 4, loss: 0.04236365482211113, acc: 98.4375, f1: 97.93650793650795, r: 0.7818797560638387
06/01/2019 11:00:06 step: 4135, epoch: 125, batch: 9, loss: 0.058370087295770645, acc: 98.4375, f1: 97.25490196078431, r: 0.7359708360743209
06/01/2019 11:00:07 step: 4140, epoch: 125, batch: 14, loss: 0.02435317449271679, acc: 100.0, f1: 100.0, r: 0.7139426995008982
06/01/2019 11:00:09 step: 4145, epoch: 125, batch: 19, loss: 0.01342291571199894, acc: 100.0, f1: 100.0, r: 0.8360581460518547
06/01/2019 11:00:10 step: 4150, epoch: 125, batch: 24, loss: 0.04038405418395996, acc: 98.4375, f1: 98.8422035480859, r: 0.6335515423411805
06/01/2019 11:00:11 step: 4155, epoch: 125, batch: 29, loss: 0.048645906150341034, acc: 98.4375, f1: 94.28571428571428, r: 0.6841676228933392
06/01/2019 11:00:12 *** evaluating ***
06/01/2019 11:00:12 step: 126, epoch: 125, acc: 58.97435897435898, f1: 30.320178165503286, r: 0.2669141643271619
06/01/2019 11:00:12 *** epoch: 127 ***
06/01/2019 11:00:12 *** training ***
06/01/2019 11:00:13 step: 4163, epoch: 126, batch: 4, loss: 0.34575048089027405, acc: 98.4375, f1: 95.23809523809523, r: 0.7200156524786744
06/01/2019 11:00:14 step: 4168, epoch: 126, batch: 9, loss: 0.05638663098216057, acc: 98.4375, f1: 98.18007662835248, r: 0.7286232069467331
06/01/2019 11:00:15 step: 4173, epoch: 126, batch: 14, loss: 0.0396605059504509, acc: 100.0, f1: 100.0, r: 0.7711930054588338
06/01/2019 11:00:16 step: 4178, epoch: 126, batch: 19, loss: 0.021062804386019707, acc: 100.0, f1: 100.0, r: 0.6593993871012348
06/01/2019 11:00:17 step: 4183, epoch: 126, batch: 24, loss: 0.038075320422649384, acc: 100.0, f1: 100.0, r: 0.835814778157297
06/01/2019 11:00:18 step: 4188, epoch: 126, batch: 29, loss: 0.05152560770511627, acc: 100.0, f1: 100.0, r: 0.8106348496844211
06/01/2019 11:00:19 *** evaluating ***
06/01/2019 11:00:19 step: 127, epoch: 126, acc: 58.97435897435898, f1: 32.98275654890651, r: 0.26820443039099395
06/01/2019 11:00:19 *** epoch: 128 ***
06/01/2019 11:00:19 *** training ***
06/01/2019 11:00:21 step: 4196, epoch: 127, batch: 4, loss: 0.04094037413597107, acc: 100.0, f1: 100.0, r: 0.7573907704009839
06/01/2019 11:00:22 step: 4201, epoch: 127, batch: 9, loss: 0.054877765476703644, acc: 100.0, f1: 100.0, r: 0.7997693074135044
06/01/2019 11:00:23 step: 4206, epoch: 127, batch: 14, loss: 0.04704277962446213, acc: 100.0, f1: 100.0, r: 0.7947071143481714
06/01/2019 11:00:24 step: 4211, epoch: 127, batch: 19, loss: 0.5703637599945068, acc: 100.0, f1: 100.0, r: 0.762672661318353
06/01/2019 11:00:25 step: 4216, epoch: 127, batch: 24, loss: 0.026978030800819397, acc: 100.0, f1: 100.0, r: 0.6744409864508579
06/01/2019 11:00:26 step: 4221, epoch: 127, batch: 29, loss: 0.05722979083657265, acc: 100.0, f1: 100.0, r: 0.6623719972397459
06/01/2019 11:00:27 *** evaluating ***
06/01/2019 11:00:27 step: 128, epoch: 127, acc: 57.692307692307686, f1: 31.92113435810915, r: 0.2689576769605177
06/01/2019 11:00:27 *** epoch: 129 ***
06/01/2019 11:00:27 *** training ***
06/01/2019 11:00:28 step: 4229, epoch: 128, batch: 4, loss: 0.031075093895196915, acc: 100.0, f1: 100.0, r: 0.7142143438308387
06/01/2019 11:00:29 step: 4234, epoch: 128, batch: 9, loss: 0.7302107214927673, acc: 98.4375, f1: 98.43260188087774, r: 0.78013552295279
06/01/2019 11:00:30 step: 4239, epoch: 128, batch: 14, loss: 0.02167586237192154, acc: 100.0, f1: 100.0, r: 0.6624024774716225
06/01/2019 11:00:31 step: 4244, epoch: 128, batch: 19, loss: 0.03817198798060417, acc: 100.0, f1: 100.0, r: 0.7052131296649731
06/01/2019 11:00:33 step: 4249, epoch: 128, batch: 24, loss: 0.07314102351665497, acc: 96.875, f1: 97.91666666666667, r: 0.7968381196840325
06/01/2019 11:00:34 step: 4254, epoch: 128, batch: 29, loss: 0.03565356135368347, acc: 100.0, f1: 100.0, r: 0.7638139552543047
06/01/2019 11:00:35 *** evaluating ***
06/01/2019 11:00:35 step: 129, epoch: 128, acc: 58.97435897435898, f1: 31.414917641697826, r: 0.2692531093830692
06/01/2019 11:00:35 *** epoch: 130 ***
06/01/2019 11:00:35 *** training ***
06/01/2019 11:00:36 step: 4262, epoch: 129, batch: 4, loss: 0.021595878526568413, acc: 100.0, f1: 100.0, r: 0.700656809117159
06/01/2019 11:00:37 step: 4267, epoch: 129, batch: 9, loss: 0.03482455760240555, acc: 100.0, f1: 100.0, r: 0.8186226177083984
06/01/2019 11:00:38 step: 4272, epoch: 129, batch: 14, loss: 0.04597875103354454, acc: 100.0, f1: 100.0, r: 0.7461574712575851
06/01/2019 11:00:39 step: 4277, epoch: 129, batch: 19, loss: 0.03906017541885376, acc: 98.4375, f1: 98.25396825396825, r: 0.7906944095809464
06/01/2019 11:00:40 step: 4282, epoch: 129, batch: 24, loss: 0.02915947325527668, acc: 100.0, f1: 100.0, r: 0.8111847197975792
06/01/2019 11:00:42 step: 4287, epoch: 129, batch: 29, loss: 0.06265006959438324, acc: 98.4375, f1: 98.92156862745098, r: 0.7682655969234636
06/01/2019 11:00:42 *** evaluating ***
06/01/2019 11:00:42 step: 130, epoch: 129, acc: 57.692307692307686, f1: 31.45374515074723, r: 0.26563301863906713
06/01/2019 11:00:42 *** epoch: 131 ***
06/01/2019 11:00:42 *** training ***
06/01/2019 11:00:44 step: 4295, epoch: 130, batch: 4, loss: 0.06576820462942123, acc: 98.4375, f1: 99.05018611218071, r: 0.7386666821592424
06/01/2019 11:00:45 step: 4300, epoch: 130, batch: 9, loss: 0.03467497602105141, acc: 100.0, f1: 100.0, r: 0.7329944857938641
06/01/2019 11:00:46 step: 4305, epoch: 130, batch: 14, loss: 0.04686235263943672, acc: 100.0, f1: 100.0, r: 0.741336269152728
06/01/2019 11:00:47 step: 4310, epoch: 130, batch: 19, loss: 0.019877277314662933, acc: 100.0, f1: 100.0, r: 0.7468146336664923
06/01/2019 11:00:48 step: 4315, epoch: 130, batch: 24, loss: 0.01954684592783451, acc: 100.0, f1: 100.0, r: 0.7565052277395634
06/01/2019 11:00:49 step: 4320, epoch: 130, batch: 29, loss: 0.03870011493563652, acc: 100.0, f1: 100.0, r: 0.6995013400794315
06/01/2019 11:00:50 *** evaluating ***
06/01/2019 11:00:50 step: 131, epoch: 130, acc: 58.54700854700855, f1: 32.09183673469388, r: 0.2647012660919813
06/01/2019 11:00:50 *** epoch: 132 ***
06/01/2019 11:00:50 *** training ***
06/01/2019 11:00:51 step: 4328, epoch: 131, batch: 4, loss: 0.3458952307701111, acc: 98.4375, f1: 98.88181993445151, r: 0.7447331363500913
06/01/2019 11:00:52 step: 4333, epoch: 131, batch: 9, loss: 0.012294675223529339, acc: 100.0, f1: 100.0, r: 0.6896118549968445
06/01/2019 11:00:53 step: 4338, epoch: 131, batch: 14, loss: 0.032205648720264435, acc: 98.4375, f1: 99.17748917748918, r: 0.81760716619976
06/01/2019 11:00:55 step: 4343, epoch: 131, batch: 19, loss: 0.04045239835977554, acc: 100.0, f1: 100.0, r: 0.7157002911394554
06/01/2019 11:00:56 step: 4348, epoch: 131, batch: 24, loss: 0.0405658558011055, acc: 100.0, f1: 100.0, r: 0.792767351862908
06/01/2019 11:00:57 step: 4353, epoch: 131, batch: 29, loss: 0.04295659810304642, acc: 100.0, f1: 100.0, r: 0.6485436135828654
06/01/2019 11:00:58 *** evaluating ***
06/01/2019 11:00:58 step: 132, epoch: 131, acc: 57.26495726495726, f1: 31.295516435546944, r: 0.2668075261617669
06/01/2019 11:00:58 *** epoch: 133 ***
06/01/2019 11:00:58 *** training ***
06/01/2019 11:00:59 step: 4361, epoch: 132, batch: 4, loss: 0.014890979044139385, acc: 100.0, f1: 100.0, r: 0.7418500791882174
06/01/2019 11:01:00 step: 4366, epoch: 132, batch: 9, loss: 0.5648646354675293, acc: 100.0, f1: 100.0, r: 0.8565337585297618
06/01/2019 11:01:01 step: 4371, epoch: 132, batch: 14, loss: 0.016257183626294136, acc: 100.0, f1: 100.0, r: 0.695914594989987
06/01/2019 11:01:02 step: 4376, epoch: 132, batch: 19, loss: 0.06257150322198868, acc: 98.4375, f1: 97.46031746031747, r: 0.7001420241011387
06/01/2019 11:01:03 step: 4381, epoch: 132, batch: 24, loss: 0.03903947025537491, acc: 100.0, f1: 100.0, r: 0.7048065371103283
06/01/2019 11:01:04 step: 4386, epoch: 132, batch: 29, loss: 0.024574672803282738, acc: 100.0, f1: 100.0, r: 0.7415758376778737
06/01/2019 11:01:05 *** evaluating ***
06/01/2019 11:01:05 step: 133, epoch: 132, acc: 57.692307692307686, f1: 32.58531886935874, r: 0.26421630313466926
06/01/2019 11:01:05 *** epoch: 134 ***
06/01/2019 11:01:05 *** training ***
06/01/2019 11:01:06 step: 4394, epoch: 133, batch: 4, loss: 0.017243411391973495, acc: 100.0, f1: 100.0, r: 0.7929298024574518
06/01/2019 11:01:08 step: 4399, epoch: 133, batch: 9, loss: 0.028203599154949188, acc: 100.0, f1: 100.0, r: 0.8031361361817951
06/01/2019 11:01:09 step: 4404, epoch: 133, batch: 14, loss: 0.01591837778687477, acc: 100.0, f1: 100.0, r: 0.756062500547088
06/01/2019 11:01:10 step: 4409, epoch: 133, batch: 19, loss: 0.02053954266011715, acc: 100.0, f1: 100.0, r: 0.6765081592596823
06/01/2019 11:01:11 step: 4414, epoch: 133, batch: 24, loss: 0.030907895416021347, acc: 100.0, f1: 100.0, r: 0.73198234272123
06/01/2019 11:01:12 step: 4419, epoch: 133, batch: 29, loss: 0.053310245275497437, acc: 98.4375, f1: 97.11399711399712, r: 0.7222069083046903
06/01/2019 11:01:13 *** evaluating ***
06/01/2019 11:01:13 step: 134, epoch: 133, acc: 58.119658119658126, f1: 31.6919751352015, r: 0.2691745554080553
06/01/2019 11:01:13 *** epoch: 135 ***
06/01/2019 11:01:13 *** training ***
06/01/2019 11:01:14 step: 4427, epoch: 134, batch: 4, loss: 0.051363155245780945, acc: 98.4375, f1: 95.23809523809523, r: 0.7019654918989524
06/01/2019 11:01:15 step: 4432, epoch: 134, batch: 9, loss: 0.024083804339170456, acc: 100.0, f1: 100.0, r: 0.8132684654888325
06/01/2019 11:01:16 step: 4437, epoch: 134, batch: 14, loss: 0.04778747633099556, acc: 98.4375, f1: 99.23404255319149, r: 0.8134091797439104
06/01/2019 11:01:18 step: 4442, epoch: 134, batch: 19, loss: 0.016608858481049538, acc: 100.0, f1: 100.0, r: 0.8160533149353478
06/01/2019 11:01:19 step: 4447, epoch: 134, batch: 24, loss: 0.07029734551906586, acc: 100.0, f1: 100.0, r: 0.6766153144011929
06/01/2019 11:01:20 step: 4452, epoch: 134, batch: 29, loss: 0.038160957396030426, acc: 98.4375, f1: 98.81521986785145, r: 0.7214160319947402
06/01/2019 11:01:20 *** evaluating ***
06/01/2019 11:01:21 step: 135, epoch: 134, acc: 58.54700854700855, f1: 32.27002834445286, r: 0.2681985484838667
06/01/2019 11:01:21 *** epoch: 136 ***
06/01/2019 11:01:21 *** training ***
06/01/2019 11:01:22 step: 4460, epoch: 135, batch: 4, loss: 0.01426376961171627, acc: 100.0, f1: 100.0, r: 0.7902425056769041
06/01/2019 11:01:23 step: 4465, epoch: 135, batch: 9, loss: 0.35841014981269836, acc: 98.4375, f1: 97.03703703703704, r: 0.7636027986458083
06/01/2019 11:01:24 step: 4470, epoch: 135, batch: 14, loss: 0.699388861656189, acc: 98.4375, f1: 96.36363636363636, r: 0.7487281374418809
06/01/2019 11:01:25 step: 4475, epoch: 135, batch: 19, loss: 0.02177070453763008, acc: 100.0, f1: 100.0, r: 0.7024094128194855
06/01/2019 11:01:27 step: 4480, epoch: 135, batch: 24, loss: 0.08749344944953918, acc: 98.4375, f1: 98.0952380952381, r: 0.694543166880674
06/01/2019 11:01:28 step: 4485, epoch: 135, batch: 29, loss: 0.019213691353797913, acc: 100.0, f1: 100.0, r: 0.8179864419205434
06/01/2019 11:01:28 *** evaluating ***
06/01/2019 11:01:28 step: 136, epoch: 135, acc: 59.82905982905983, f1: 32.799585332833416, r: 0.2678592444582344
06/01/2019 11:01:28 *** epoch: 137 ***
06/01/2019 11:01:28 *** training ***
06/01/2019 11:01:30 step: 4493, epoch: 136, batch: 4, loss: 0.02562682144343853, acc: 100.0, f1: 100.0, r: 0.7700899761718798
06/01/2019 11:01:31 step: 4498, epoch: 136, batch: 9, loss: 0.5518932342529297, acc: 100.0, f1: 100.0, r: 0.6766668052019532
06/01/2019 11:01:32 step: 4503, epoch: 136, batch: 14, loss: 0.04185850918292999, acc: 96.875, f1: 96.55834181179804, r: 0.6883039679257041
06/01/2019 11:01:33 step: 4508, epoch: 136, batch: 19, loss: 0.05784864351153374, acc: 100.0, f1: 100.0, r: 0.761573611869904
06/01/2019 11:01:34 step: 4513, epoch: 136, batch: 24, loss: 0.041313499212265015, acc: 100.0, f1: 100.0, r: 0.6908552023618348
06/01/2019 11:01:35 step: 4518, epoch: 136, batch: 29, loss: 0.028135567903518677, acc: 100.0, f1: 100.0, r: 0.7812462013351902
06/01/2019 11:01:36 *** evaluating ***
06/01/2019 11:01:36 step: 137, epoch: 136, acc: 58.54700854700855, f1: 31.510684850876768, r: 0.2672501527738781
06/01/2019 11:01:36 *** epoch: 138 ***
06/01/2019 11:01:36 *** training ***
06/01/2019 11:01:37 step: 4526, epoch: 137, batch: 4, loss: 0.03251606598496437, acc: 100.0, f1: 100.0, r: 0.7515174591390361
06/01/2019 11:01:38 step: 4531, epoch: 137, batch: 9, loss: 0.03058786690235138, acc: 100.0, f1: 100.0, r: 0.7246176753971726
06/01/2019 11:01:39 step: 4536, epoch: 137, batch: 14, loss: 0.02644423022866249, acc: 100.0, f1: 100.0, r: 0.7178724410515065
06/01/2019 11:01:41 step: 4541, epoch: 137, batch: 19, loss: 0.060412175953388214, acc: 98.4375, f1: 97.94832826747721, r: 0.8016565935458695
06/01/2019 11:01:42 step: 4546, epoch: 137, batch: 24, loss: 0.02695763297379017, acc: 100.0, f1: 100.0, r: 0.6961241650124953
06/01/2019 11:01:43 step: 4551, epoch: 137, batch: 29, loss: 0.017047321423888206, acc: 100.0, f1: 100.0, r: 0.6830491572032942
06/01/2019 11:01:43 *** evaluating ***
06/01/2019 11:01:44 step: 138, epoch: 137, acc: 57.26495726495726, f1: 32.00214106632118, r: 0.26282483750040575
06/01/2019 11:01:44 *** epoch: 139 ***
06/01/2019 11:01:44 *** training ***
06/01/2019 11:01:45 step: 4559, epoch: 138, batch: 4, loss: 0.025430796667933464, acc: 100.0, f1: 100.0, r: 0.7737256508860948
06/01/2019 11:01:46 step: 4564, epoch: 138, batch: 9, loss: 0.034172043204307556, acc: 100.0, f1: 100.0, r: 0.8029343382248434
06/01/2019 11:01:47 step: 4569, epoch: 138, batch: 14, loss: 0.01495588943362236, acc: 100.0, f1: 100.0, r: 0.7410336176987269
06/01/2019 11:01:48 step: 4574, epoch: 138, batch: 19, loss: 0.07955995947122574, acc: 96.875, f1: 95.29203949689125, r: 0.6569731912820331
06/01/2019 11:01:49 step: 4579, epoch: 138, batch: 24, loss: 0.5858160257339478, acc: 98.4375, f1: 97.49835418038182, r: 0.6892522844104968
06/01/2019 11:01:50 step: 4584, epoch: 138, batch: 29, loss: 0.017718318849802017, acc: 100.0, f1: 100.0, r: 0.6897844725345831
06/01/2019 11:01:51 *** evaluating ***
06/01/2019 11:01:51 step: 139, epoch: 138, acc: 58.119658119658126, f1: 31.37023938190885, r: 0.26848662131067913
06/01/2019 11:01:51 *** epoch: 140 ***
06/01/2019 11:01:51 *** training ***
06/01/2019 11:01:52 step: 4592, epoch: 139, batch: 4, loss: 0.055083490908145905, acc: 98.4375, f1: 95.58823529411764, r: 0.7605063566997962
06/01/2019 11:01:54 step: 4597, epoch: 139, batch: 9, loss: 0.02010476216673851, acc: 100.0, f1: 100.0, r: 0.7643848895183313
06/01/2019 11:01:54 step: 4602, epoch: 139, batch: 14, loss: 0.5855252146720886, acc: 100.0, f1: 100.0, r: 0.721537104429881
06/01/2019 11:01:55 step: 4607, epoch: 139, batch: 19, loss: 0.7799350619316101, acc: 98.4375, f1: 98.32967032967032, r: 0.6943138422676202
06/01/2019 11:01:57 step: 4612, epoch: 139, batch: 24, loss: 0.02281981147825718, acc: 100.0, f1: 100.0, r: 0.68991428569385
06/01/2019 11:01:58 step: 4617, epoch: 139, batch: 29, loss: 0.03283531591296196, acc: 100.0, f1: 100.0, r: 0.6625550670093114
06/01/2019 11:01:59 *** evaluating ***
06/01/2019 11:01:59 step: 140, epoch: 139, acc: 58.97435897435898, f1: 32.519133187450024, r: 0.2627678163270812
06/01/2019 11:01:59 *** epoch: 141 ***
06/01/2019 11:01:59 *** training ***
06/01/2019 11:02:00 step: 4625, epoch: 140, batch: 4, loss: 0.023159757256507874, acc: 100.0, f1: 100.0, r: 0.7580823537498752
06/01/2019 11:02:01 step: 4630, epoch: 140, batch: 9, loss: 0.02689243294298649, acc: 98.4375, f1: 85.0, r: 0.8403832812571527
06/01/2019 11:02:02 step: 4635, epoch: 140, batch: 14, loss: 0.3748544752597809, acc: 100.0, f1: 100.0, r: 0.7937605617578044
06/01/2019 11:02:03 step: 4640, epoch: 140, batch: 19, loss: 0.02045351080596447, acc: 100.0, f1: 100.0, r: 0.7175366146135935
06/01/2019 11:02:04 step: 4645, epoch: 140, batch: 24, loss: 0.02193429507315159, acc: 100.0, f1: 100.0, r: 0.7020806387434475
06/01/2019 11:02:06 step: 4650, epoch: 140, batch: 29, loss: 0.028892861679196358, acc: 100.0, f1: 100.0, r: 0.7205561619132815
06/01/2019 11:02:06 *** evaluating ***
06/01/2019 11:02:06 step: 141, epoch: 140, acc: 55.55555555555556, f1: 30.235686024072123, r: 0.2622722032070163
06/01/2019 11:02:06 *** epoch: 142 ***
06/01/2019 11:02:06 *** training ***
06/01/2019 11:02:08 step: 4658, epoch: 141, batch: 4, loss: 0.3262401819229126, acc: 100.0, f1: 100.0, r: 0.7133562225696511
06/01/2019 11:02:09 step: 4663, epoch: 141, batch: 9, loss: 0.037637848407030106, acc: 100.0, f1: 100.0, r: 0.650296836394173
06/01/2019 11:02:10 step: 4668, epoch: 141, batch: 14, loss: 0.693064272403717, acc: 100.0, f1: 100.0, r: 0.7101191476538932
06/01/2019 11:02:11 step: 4673, epoch: 141, batch: 19, loss: 0.03287385404109955, acc: 100.0, f1: 100.0, r: 0.7733584597076645
06/01/2019 11:02:12 step: 4678, epoch: 141, batch: 24, loss: 0.03435785323381424, acc: 100.0, f1: 100.0, r: 0.6328662682225478
06/01/2019 11:02:13 step: 4683, epoch: 141, batch: 29, loss: 0.032626211643218994, acc: 98.4375, f1: 96.42857142857143, r: 0.8343622420939747
06/01/2019 11:02:14 *** evaluating ***
06/01/2019 11:02:14 step: 142, epoch: 141, acc: 57.26495726495726, f1: 29.87783803636099, r: 0.26514182110835477
06/01/2019 11:02:14 *** epoch: 143 ***
06/01/2019 11:02:14 *** training ***
06/01/2019 11:02:15 step: 4691, epoch: 142, batch: 4, loss: 0.07706813514232635, acc: 96.875, f1: 97.21574721574723, r: 0.6262624688506742
06/01/2019 11:02:17 step: 4696, epoch: 142, batch: 9, loss: 0.039371900260448456, acc: 98.4375, f1: 98.08018068887634, r: 0.7610501403943143
06/01/2019 11:02:18 step: 4701, epoch: 142, batch: 14, loss: 0.07490218430757523, acc: 96.875, f1: 96.93139097744361, r: 0.801378982124656
06/01/2019 11:02:19 step: 4706, epoch: 142, batch: 19, loss: 0.015243493020534515, acc: 100.0, f1: 100.0, r: 0.8216700584413458
06/01/2019 11:02:20 step: 4711, epoch: 142, batch: 24, loss: 0.054892171174287796, acc: 98.4375, f1: 98.7436676798379, r: 0.666398510497842
06/01/2019 11:02:21 step: 4716, epoch: 142, batch: 29, loss: 0.028462760150432587, acc: 100.0, f1: 100.0, r: 0.7490620858214261
06/01/2019 11:02:21 *** evaluating ***
06/01/2019 11:02:22 step: 143, epoch: 142, acc: 58.119658119658126, f1: 31.956262849119994, r: 0.2666244258057341
06/01/2019 11:02:22 *** epoch: 144 ***
06/01/2019 11:02:22 *** training ***
06/01/2019 11:02:23 step: 4724, epoch: 143, batch: 4, loss: 0.02532661333680153, acc: 100.0, f1: 100.0, r: 0.7675365511142196
06/01/2019 11:02:24 step: 4729, epoch: 143, batch: 9, loss: 0.05729738995432854, acc: 100.0, f1: 100.0, r: 0.7980118001072461
06/01/2019 11:02:25 step: 4734, epoch: 143, batch: 14, loss: 0.00840708427131176, acc: 100.0, f1: 100.0, r: 0.6982886127468572
06/01/2019 11:02:26 step: 4739, epoch: 143, batch: 19, loss: 0.02594560571014881, acc: 100.0, f1: 100.0, r: 0.7542441825153836
06/01/2019 11:02:28 step: 4744, epoch: 143, batch: 24, loss: 0.01569889672100544, acc: 100.0, f1: 100.0, r: 0.8028831881571321
06/01/2019 11:02:29 step: 4749, epoch: 143, batch: 29, loss: 0.019859276711940765, acc: 100.0, f1: 100.0, r: 0.7673461395979196
06/01/2019 11:02:29 *** evaluating ***
06/01/2019 11:02:30 step: 144, epoch: 143, acc: 54.700854700854705, f1: 30.57422492824052, r: 0.2616130416347863
06/01/2019 11:02:30 *** epoch: 145 ***
06/01/2019 11:02:30 *** training ***
06/01/2019 11:02:31 step: 4757, epoch: 144, batch: 4, loss: 0.02811143733561039, acc: 98.4375, f1: 97.75132275132276, r: 0.7621476024783094
06/01/2019 11:02:32 step: 4762, epoch: 144, batch: 9, loss: 0.03684084862470627, acc: 98.4375, f1: 98.94416893297073, r: 0.6717694617340584
06/01/2019 11:02:33 step: 4767, epoch: 144, batch: 14, loss: 0.037901632487773895, acc: 98.4375, f1: 95.91836734693878, r: 0.7617576500019062
06/01/2019 11:02:34 step: 4772, epoch: 144, batch: 19, loss: 0.05123011767864227, acc: 98.4375, f1: 98.58585858585859, r: 0.7401234701082844
06/01/2019 11:02:35 step: 4777, epoch: 144, batch: 24, loss: 0.037288304418325424, acc: 100.0, f1: 100.0, r: 0.8174689213613224
06/01/2019 11:02:36 step: 4782, epoch: 144, batch: 29, loss: 0.028693053871393204, acc: 100.0, f1: 100.0, r: 0.7916841172456466
06/01/2019 11:02:37 *** evaluating ***
06/01/2019 11:02:37 step: 145, epoch: 144, acc: 58.54700854700855, f1: 29.703044150176655, r: 0.2658190430119895
06/01/2019 11:02:37 *** epoch: 146 ***
06/01/2019 11:02:37 *** training ***
06/01/2019 11:02:38 step: 4790, epoch: 145, batch: 4, loss: 0.013714756816625595, acc: 100.0, f1: 100.0, r: 0.6876308811541213
06/01/2019 11:02:39 step: 4795, epoch: 145, batch: 9, loss: 0.02208239771425724, acc: 100.0, f1: 100.0, r: 0.7824674255228771
06/01/2019 11:02:40 step: 4800, epoch: 145, batch: 14, loss: 0.07653573900461197, acc: 98.4375, f1: 98.60681114551085, r: 0.7525568922224612
06/01/2019 11:02:42 step: 4805, epoch: 145, batch: 19, loss: 0.040166258811950684, acc: 100.0, f1: 100.0, r: 0.7024717414403547
06/01/2019 11:02:42 step: 4810, epoch: 145, batch: 24, loss: 0.7007262110710144, acc: 98.4375, f1: 98.43260188087774, r: 0.754442779398036
06/01/2019 11:02:44 step: 4815, epoch: 145, batch: 29, loss: 0.022095732390880585, acc: 100.0, f1: 100.0, r: 0.7777862528776048
06/01/2019 11:02:44 *** evaluating ***
06/01/2019 11:02:45 step: 146, epoch: 145, acc: 57.26495726495726, f1: 30.408276034938915, r: 0.2669373163029926
06/01/2019 11:02:45 *** epoch: 147 ***
06/01/2019 11:02:45 *** training ***
06/01/2019 11:02:46 step: 4823, epoch: 146, batch: 4, loss: 0.032021015882492065, acc: 100.0, f1: 100.0, r: 0.8009768654258779
06/01/2019 11:02:47 step: 4828, epoch: 146, batch: 9, loss: 0.017255859449505806, acc: 100.0, f1: 100.0, r: 0.6656350561046182
06/01/2019 11:02:48 step: 4833, epoch: 146, batch: 14, loss: 0.5851092338562012, acc: 100.0, f1: 100.0, r: 0.7011885949574307
06/01/2019 11:02:49 step: 4838, epoch: 146, batch: 19, loss: 0.022975675761699677, acc: 100.0, f1: 100.0, r: 0.6871889303178292
06/01/2019 11:02:50 step: 4843, epoch: 146, batch: 24, loss: 0.02119557000696659, acc: 100.0, f1: 100.0, r: 0.6920930165268354
06/01/2019 11:02:51 step: 4848, epoch: 146, batch: 29, loss: 0.028933364897966385, acc: 100.0, f1: 100.0, r: 0.6788366643142073
06/01/2019 11:02:52 *** evaluating ***
06/01/2019 11:02:52 step: 147, epoch: 146, acc: 58.54700854700855, f1: 29.264976152300093, r: 0.26407437602295386
06/01/2019 11:02:52 *** epoch: 148 ***
06/01/2019 11:02:52 *** training ***
06/01/2019 11:02:53 step: 4856, epoch: 147, batch: 4, loss: 0.020604226738214493, acc: 100.0, f1: 100.0, r: 0.6581002042451011
06/01/2019 11:02:54 step: 4861, epoch: 147, batch: 9, loss: 0.029236668720841408, acc: 100.0, f1: 100.0, r: 0.7043139322430549
06/01/2019 11:02:55 step: 4866, epoch: 147, batch: 14, loss: 0.022221367806196213, acc: 100.0, f1: 100.0, r: 0.7388140295314309
06/01/2019 11:02:56 step: 4871, epoch: 147, batch: 19, loss: 0.03177677467465401, acc: 100.0, f1: 100.0, r: 0.6785306329907204
06/01/2019 11:02:57 step: 4876, epoch: 147, batch: 24, loss: 0.014290042221546173, acc: 100.0, f1: 100.0, r: 0.7423522174775832
06/01/2019 11:02:58 step: 4881, epoch: 147, batch: 29, loss: 0.05653270706534386, acc: 100.0, f1: 100.0, r: 0.7926355350540537
06/01/2019 11:02:59 *** evaluating ***
06/01/2019 11:03:00 step: 148, epoch: 147, acc: 57.26495726495726, f1: 29.36655774758198, r: 0.2628193428415787
06/01/2019 11:03:00 *** epoch: 149 ***
06/01/2019 11:03:00 *** training ***
06/01/2019 11:03:01 step: 4889, epoch: 148, batch: 4, loss: 0.028103195130825043, acc: 100.0, f1: 100.0, r: 0.7773157246403737
06/01/2019 11:03:02 step: 4894, epoch: 148, batch: 9, loss: 0.02014201506972313, acc: 100.0, f1: 100.0, r: 0.6649872589993678
06/01/2019 11:03:03 step: 4899, epoch: 148, batch: 14, loss: 0.01873430423438549, acc: 100.0, f1: 100.0, r: 0.6464340563084435
06/01/2019 11:03:04 step: 4904, epoch: 148, batch: 19, loss: 0.03578696399927139, acc: 100.0, f1: 100.0, r: 0.7938718768794988
06/01/2019 11:03:05 step: 4909, epoch: 148, batch: 24, loss: 0.6844907402992249, acc: 100.0, f1: 100.0, r: 0.7333754317618661
06/01/2019 11:03:06 step: 4914, epoch: 148, batch: 29, loss: 0.028028493747115135, acc: 100.0, f1: 100.0, r: 0.7167293863517402
06/01/2019 11:03:07 *** evaluating ***
06/01/2019 11:03:07 step: 149, epoch: 148, acc: 57.26495726495726, f1: 30.182131694520898, r: 0.2652364359175981
06/01/2019 11:03:07 *** epoch: 150 ***
06/01/2019 11:03:07 *** training ***
06/01/2019 11:03:08 step: 4922, epoch: 149, batch: 4, loss: 0.014410097151994705, acc: 100.0, f1: 100.0, r: 0.6868528463815335
06/01/2019 11:03:09 step: 4927, epoch: 149, batch: 9, loss: 0.014657615683972836, acc: 100.0, f1: 100.0, r: 0.7144338157515135
06/01/2019 11:03:10 step: 4932, epoch: 149, batch: 14, loss: 0.05200888216495514, acc: 98.4375, f1: 96.70995670995671, r: 0.6604335951097088
06/01/2019 11:03:11 step: 4937, epoch: 149, batch: 19, loss: 0.01921040751039982, acc: 100.0, f1: 100.0, r: 0.6976846769667442
06/01/2019 11:03:12 step: 4942, epoch: 149, batch: 24, loss: 0.01722659543156624, acc: 100.0, f1: 100.0, r: 0.7729581329429115
06/01/2019 11:03:14 step: 4947, epoch: 149, batch: 29, loss: 0.057013604789972305, acc: 100.0, f1: 100.0, r: 0.7418417017081901
06/01/2019 11:03:14 *** evaluating ***
06/01/2019 11:03:14 step: 150, epoch: 149, acc: 58.119658119658126, f1: 32.14673663548419, r: 0.2634106806202963
06/01/2019 11:03:14 *** epoch: 151 ***
06/01/2019 11:03:14 *** training ***
06/01/2019 11:03:16 step: 4955, epoch: 150, batch: 4, loss: 0.011722663417458534, acc: 100.0, f1: 100.0, r: 0.7503443698495554
06/01/2019 11:03:17 step: 4960, epoch: 150, batch: 9, loss: 0.03634900972247124, acc: 98.4375, f1: 97.47899159663865, r: 0.7680279736493151
06/01/2019 11:03:18 step: 4965, epoch: 150, batch: 14, loss: 0.02887742593884468, acc: 100.0, f1: 100.0, r: 0.7984522292471733
06/01/2019 11:03:19 step: 4970, epoch: 150, batch: 19, loss: 0.01830308325588703, acc: 100.0, f1: 100.0, r: 0.7534943272673499
06/01/2019 11:03:20 step: 4975, epoch: 150, batch: 24, loss: 0.08146966248750687, acc: 98.4375, f1: 96.6604823747681, r: 0.6894916964073532
06/01/2019 11:03:21 step: 4980, epoch: 150, batch: 29, loss: 0.6632294058799744, acc: 100.0, f1: 100.0, r: 0.7430796891287813
06/01/2019 11:03:22 *** evaluating ***
06/01/2019 11:03:22 step: 151, epoch: 150, acc: 58.54700854700855, f1: 29.935433172040316, r: 0.2616067782504908
06/01/2019 11:03:22 *** epoch: 152 ***
06/01/2019 11:03:22 *** training ***
06/01/2019 11:03:23 step: 4988, epoch: 151, batch: 4, loss: 0.02131098508834839, acc: 100.0, f1: 100.0, r: 0.8450595293509673
06/01/2019 11:03:24 step: 4993, epoch: 151, batch: 9, loss: 0.0204994548112154, acc: 100.0, f1: 100.0, r: 0.7869153834625611
06/01/2019 11:03:25 step: 4998, epoch: 151, batch: 14, loss: 0.023392319679260254, acc: 100.0, f1: 100.0, r: 0.7949357849404941
06/01/2019 11:03:27 step: 5003, epoch: 151, batch: 19, loss: 0.034396763890981674, acc: 98.4375, f1: 99.11111111111111, r: 0.6749419732632729
06/01/2019 11:03:28 step: 5008, epoch: 151, batch: 24, loss: 0.046771883964538574, acc: 98.4375, f1: 99.19078742608154, r: 0.6947760130642603
06/01/2019 11:03:29 step: 5013, epoch: 151, batch: 29, loss: 0.019545376300811768, acc: 100.0, f1: 100.0, r: 0.8209533686801119
06/01/2019 11:03:29 *** evaluating ***
06/01/2019 11:03:29 step: 152, epoch: 151, acc: 56.837606837606835, f1: 28.817106526084856, r: 0.2625916508693568
06/01/2019 11:03:29 *** epoch: 153 ***
06/01/2019 11:03:29 *** training ***
06/01/2019 11:03:31 step: 5021, epoch: 152, batch: 4, loss: 0.32232120633125305, acc: 100.0, f1: 100.0, r: 0.6372278397996732
06/01/2019 11:03:32 step: 5026, epoch: 152, batch: 9, loss: 0.017550596967339516, acc: 100.0, f1: 100.0, r: 0.720491829677007
06/01/2019 11:03:33 step: 5031, epoch: 152, batch: 14, loss: 0.018790114670991898, acc: 100.0, f1: 100.0, r: 0.6918572254365036
06/01/2019 11:03:34 step: 5036, epoch: 152, batch: 19, loss: 0.017266539856791496, acc: 100.0, f1: 100.0, r: 0.804964603498822
06/01/2019 11:03:35 step: 5041, epoch: 152, batch: 24, loss: 0.03254031389951706, acc: 98.4375, f1: 95.55555555555556, r: 0.7041691167299748
06/01/2019 11:03:36 step: 5046, epoch: 152, batch: 29, loss: 0.620385468006134, acc: 100.0, f1: 100.0, r: 0.7220895840035956
06/01/2019 11:03:37 *** evaluating ***
06/01/2019 11:03:37 step: 153, epoch: 152, acc: 57.692307692307686, f1: 30.15421359237822, r: 0.2657109065252029
06/01/2019 11:03:37 *** epoch: 154 ***
06/01/2019 11:03:37 *** training ***
06/01/2019 11:03:38 step: 5054, epoch: 153, batch: 4, loss: 0.012754872441291809, acc: 100.0, f1: 100.0, r: 0.8124540810502127
06/01/2019 11:03:39 step: 5059, epoch: 153, batch: 9, loss: 0.014759603887796402, acc: 100.0, f1: 100.0, r: 0.7229924011991669
06/01/2019 11:03:40 step: 5064, epoch: 153, batch: 14, loss: 0.01785966381430626, acc: 100.0, f1: 100.0, r: 0.7234059194234951
06/01/2019 11:03:41 step: 5069, epoch: 153, batch: 19, loss: 0.9381390810012817, acc: 98.4375, f1: 85.09316770186335, r: 0.604533916588113
06/01/2019 11:03:43 step: 5074, epoch: 153, batch: 24, loss: 0.021439801901578903, acc: 100.0, f1: 100.0, r: 0.809798896269331
06/01/2019 11:03:44 step: 5079, epoch: 153, batch: 29, loss: 0.01730099879205227, acc: 100.0, f1: 100.0, r: 0.8009752039228358
06/01/2019 11:03:44 *** evaluating ***
06/01/2019 11:03:45 step: 154, epoch: 153, acc: 58.54700854700855, f1: 29.842688091739923, r: 0.2653282618356767
06/01/2019 11:03:45 *** epoch: 155 ***
06/01/2019 11:03:45 *** training ***
06/01/2019 11:03:46 step: 5087, epoch: 154, batch: 4, loss: 0.020211780443787575, acc: 100.0, f1: 100.0, r: 0.8549704175888483
06/01/2019 11:03:47 step: 5092, epoch: 154, batch: 9, loss: 0.019341059029102325, acc: 100.0, f1: 100.0, r: 0.732221526093958
06/01/2019 11:03:48 step: 5097, epoch: 154, batch: 14, loss: 0.03683104366064072, acc: 98.4375, f1: 98.96825396825398, r: 0.7864994773614806
06/01/2019 11:03:49 step: 5102, epoch: 154, batch: 19, loss: 0.026274340227246284, acc: 100.0, f1: 100.0, r: 0.6233887234325856
06/01/2019 11:03:50 step: 5107, epoch: 154, batch: 24, loss: 0.04780637100338936, acc: 96.875, f1: 96.08585858585859, r: 0.760577673002163
06/01/2019 11:03:51 step: 5112, epoch: 154, batch: 29, loss: 0.038178496062755585, acc: 100.0, f1: 100.0, r: 0.6997196961607737
06/01/2019 11:03:52 *** evaluating ***
06/01/2019 11:03:52 step: 155, epoch: 154, acc: 57.692307692307686, f1: 28.950754454629724, r: 0.2638749874613175
06/01/2019 11:03:52 *** epoch: 156 ***
06/01/2019 11:03:52 *** training ***
06/01/2019 11:03:54 step: 5120, epoch: 155, batch: 4, loss: 0.035941798239946365, acc: 100.0, f1: 100.0, r: 0.6443959346916653
06/01/2019 11:03:55 step: 5125, epoch: 155, batch: 9, loss: 0.018213920295238495, acc: 100.0, f1: 100.0, r: 0.7442099399283775
06/01/2019 11:03:56 step: 5130, epoch: 155, batch: 14, loss: 0.034616611897945404, acc: 98.4375, f1: 98.44322344322345, r: 0.8254167133096973
06/01/2019 11:03:57 step: 5135, epoch: 155, batch: 19, loss: 0.014889375306665897, acc: 100.0, f1: 100.0, r: 0.6902072804872484
06/01/2019 11:03:58 step: 5140, epoch: 155, batch: 24, loss: 0.06454862654209137, acc: 96.875, f1: 91.08465608465607, r: 0.776722652306632
06/01/2019 11:03:59 step: 5145, epoch: 155, batch: 29, loss: 0.01979147642850876, acc: 100.0, f1: 100.0, r: 0.7544930752719663
06/01/2019 11:04:00 *** evaluating ***
06/01/2019 11:04:00 step: 156, epoch: 155, acc: 57.692307692307686, f1: 28.258623381785142, r: 0.26526307601142857
06/01/2019 11:04:00 *** epoch: 157 ***
06/01/2019 11:04:00 *** training ***
06/01/2019 11:04:01 step: 5153, epoch: 156, batch: 4, loss: 0.010814893990755081, acc: 100.0, f1: 100.0, r: 0.6943263505527966
06/01/2019 11:04:02 step: 5158, epoch: 156, batch: 9, loss: 0.02977161668241024, acc: 100.0, f1: 100.0, r: 0.693826782596884
06/01/2019 11:04:03 step: 5163, epoch: 156, batch: 14, loss: 0.04743010550737381, acc: 100.0, f1: 100.0, r: 0.8063564782460838
06/01/2019 11:04:04 step: 5168, epoch: 156, batch: 19, loss: 0.042856957763433456, acc: 100.0, f1: 100.0, r: 0.7608659680122235
06/01/2019 11:04:05 step: 5173, epoch: 156, batch: 24, loss: 0.33010801672935486, acc: 98.4375, f1: 98.55266684534976, r: 0.6676812452889439
06/01/2019 11:04:06 step: 5178, epoch: 156, batch: 29, loss: 0.013138506561517715, acc: 100.0, f1: 100.0, r: 0.661691420139868
06/01/2019 11:04:07 *** evaluating ***
06/01/2019 11:04:07 step: 157, epoch: 156, acc: 56.837606837606835, f1: 29.185668620616916, r: 0.2620640106611466
06/01/2019 11:04:07 *** epoch: 158 ***
06/01/2019 11:04:07 *** training ***
06/01/2019 11:04:08 step: 5186, epoch: 157, batch: 4, loss: 0.026049405336380005, acc: 100.0, f1: 100.0, r: 0.7244442345834381
06/01/2019 11:04:10 step: 5191, epoch: 157, batch: 9, loss: 0.031054913997650146, acc: 98.4375, f1: 93.71980676328504, r: 0.613041161433155
06/01/2019 11:04:11 step: 5196, epoch: 157, batch: 14, loss: 0.022520527243614197, acc: 100.0, f1: 100.0, r: 0.8307935679377769
06/01/2019 11:04:12 step: 5201, epoch: 157, batch: 19, loss: 0.01963174343109131, acc: 100.0, f1: 100.0, r: 0.7027796683992248
06/01/2019 11:04:13 step: 5206, epoch: 157, batch: 24, loss: 0.04977511614561081, acc: 98.4375, f1: 97.95321637426902, r: 0.7665113656311425
06/01/2019 11:04:14 step: 5211, epoch: 157, batch: 29, loss: 0.051696039736270905, acc: 100.0, f1: 100.0, r: 0.7846136118338999
06/01/2019 11:04:15 *** evaluating ***
06/01/2019 11:04:15 step: 158, epoch: 157, acc: 57.26495726495726, f1: 31.17075660090868, r: 0.26969540695618893
06/01/2019 11:04:15 *** epoch: 159 ***
06/01/2019 11:04:15 *** training ***
06/01/2019 11:04:16 step: 5219, epoch: 158, batch: 4, loss: 0.012722088024020195, acc: 100.0, f1: 100.0, r: 0.588499331117002
06/01/2019 11:04:17 step: 5224, epoch: 158, batch: 9, loss: 0.05577108636498451, acc: 96.875, f1: 98.06339557099435, r: 0.6971223411778981
06/01/2019 11:04:18 step: 5229, epoch: 158, batch: 14, loss: 0.06021103262901306, acc: 100.0, f1: 100.0, r: 0.7729308762973384
06/01/2019 11:04:19 step: 5234, epoch: 158, batch: 19, loss: 0.014212432317435741, acc: 100.0, f1: 100.0, r: 0.8081803531495175
06/01/2019 11:04:20 step: 5239, epoch: 158, batch: 24, loss: 0.020192887634038925, acc: 100.0, f1: 100.0, r: 0.7286187632357705
06/01/2019 11:04:22 step: 5244, epoch: 158, batch: 29, loss: 0.02980877086520195, acc: 100.0, f1: 100.0, r: 0.7929490354901825
06/01/2019 11:04:22 *** evaluating ***
06/01/2019 11:04:23 step: 159, epoch: 158, acc: 56.837606837606835, f1: 28.843773680222274, r: 0.26759804515242813
06/01/2019 11:04:23 *** epoch: 160 ***
06/01/2019 11:04:23 *** training ***
06/01/2019 11:04:24 step: 5252, epoch: 159, batch: 4, loss: 0.018200937658548355, acc: 100.0, f1: 100.0, r: 0.548501483637162
06/01/2019 11:04:25 step: 5257, epoch: 159, batch: 9, loss: 0.039462678134441376, acc: 98.4375, f1: 97.47899159663866, r: 0.7530922907460148
06/01/2019 11:04:26 step: 5262, epoch: 159, batch: 14, loss: 0.04076215997338295, acc: 98.4375, f1: 98.12987012987013, r: 0.7413311831349874
06/01/2019 11:04:27 step: 5267, epoch: 159, batch: 19, loss: 0.06978671252727509, acc: 98.4375, f1: 99.24489795918367, r: 0.781633620583049
06/01/2019 11:04:28 step: 5272, epoch: 159, batch: 24, loss: 0.02790927141904831, acc: 100.0, f1: 100.0, r: 0.7438371701879921
06/01/2019 11:04:29 step: 5277, epoch: 159, batch: 29, loss: 0.04045433923602104, acc: 100.0, f1: 100.0, r: 0.7707180671316048
06/01/2019 11:04:30 *** evaluating ***
06/01/2019 11:04:30 step: 160, epoch: 159, acc: 57.26495726495726, f1: 27.718141929371875, r: 0.26588527812812257
06/01/2019 11:04:30 *** epoch: 161 ***
06/01/2019 11:04:30 *** training ***
06/01/2019 11:04:31 step: 5285, epoch: 160, batch: 4, loss: 0.019708245992660522, acc: 100.0, f1: 100.0, r: 0.8087781915536341
06/01/2019 11:04:32 step: 5290, epoch: 160, batch: 9, loss: 0.01854730024933815, acc: 100.0, f1: 100.0, r: 0.7735639190608458
06/01/2019 11:04:33 step: 5295, epoch: 160, batch: 14, loss: 0.5978493690490723, acc: 100.0, f1: 100.0, r: 0.5973325872672803
06/01/2019 11:04:34 step: 5300, epoch: 160, batch: 19, loss: 0.02627253159880638, acc: 100.0, f1: 100.0, r: 0.8560514788342802
06/01/2019 11:04:36 step: 5305, epoch: 160, batch: 24, loss: 0.042133282870054245, acc: 98.4375, f1: 96.260162601626, r: 0.6343426893751108
06/01/2019 11:04:37 step: 5310, epoch: 160, batch: 29, loss: 0.04306935891509056, acc: 98.4375, f1: 98.01587301587301, r: 0.792920591720692
06/01/2019 11:04:37 *** evaluating ***
06/01/2019 11:04:38 step: 161, epoch: 160, acc: 58.54700854700855, f1: 28.85629043148752, r: 0.2686671316989728
06/01/2019 11:04:38 *** epoch: 162 ***
06/01/2019 11:04:38 *** training ***
06/01/2019 11:04:39 step: 5318, epoch: 161, batch: 4, loss: 0.041191019117832184, acc: 100.0, f1: 100.0, r: 0.6173272517173063
06/01/2019 11:04:40 step: 5323, epoch: 161, batch: 9, loss: 0.015149397775530815, acc: 100.0, f1: 100.0, r: 0.8175095316515644
06/01/2019 11:04:41 step: 5328, epoch: 161, batch: 14, loss: 0.07449793815612793, acc: 98.4375, f1: 97.95321637426902, r: 0.8303694690752239
06/01/2019 11:04:42 step: 5333, epoch: 161, batch: 19, loss: 0.031200403347611427, acc: 100.0, f1: 100.0, r: 0.6152496712666883
06/01/2019 11:04:43 step: 5338, epoch: 161, batch: 24, loss: 0.6041932106018066, acc: 100.0, f1: 100.0, r: 0.5756916543150499
06/01/2019 11:04:44 step: 5343, epoch: 161, batch: 29, loss: 0.05360502749681473, acc: 98.4375, f1: 98.46041055718476, r: 0.7893370216882641
06/01/2019 11:04:45 *** evaluating ***
06/01/2019 11:04:45 step: 162, epoch: 161, acc: 58.119658119658126, f1: 32.911711576095136, r: 0.26682323370978217
06/01/2019 11:04:45 *** epoch: 163 ***
06/01/2019 11:04:45 *** training ***
06/01/2019 11:04:46 step: 5351, epoch: 162, batch: 4, loss: 0.014932682737708092, acc: 100.0, f1: 100.0, r: 0.7620388488277218
06/01/2019 11:04:47 step: 5356, epoch: 162, batch: 9, loss: 0.015353471040725708, acc: 100.0, f1: 100.0, r: 0.7463392682925356
06/01/2019 11:04:48 step: 5361, epoch: 162, batch: 14, loss: 0.0355636402964592, acc: 100.0, f1: 100.0, r: 0.6287874334332635
06/01/2019 11:04:49 step: 5366, epoch: 162, batch: 19, loss: 0.03490366041660309, acc: 98.4375, f1: 98.12987012987013, r: 0.6610526776080846
06/01/2019 11:04:50 step: 5371, epoch: 162, batch: 24, loss: 0.042324163019657135, acc: 100.0, f1: 100.0, r: 0.671706229576076
06/01/2019 11:04:51 step: 5376, epoch: 162, batch: 29, loss: 0.02264079824090004, acc: 100.0, f1: 100.0, r: 0.6806213256688354
06/01/2019 11:04:52 *** evaluating ***
06/01/2019 11:04:52 step: 163, epoch: 162, acc: 58.119658119658126, f1: 31.592422425267593, r: 0.2687808086584558
06/01/2019 11:04:52 *** epoch: 164 ***
06/01/2019 11:04:52 *** training ***
06/01/2019 11:04:53 step: 5384, epoch: 163, batch: 4, loss: 0.05905956029891968, acc: 100.0, f1: 100.0, r: 0.7374324045118343
06/01/2019 11:04:54 step: 5389, epoch: 163, batch: 9, loss: 0.02593669295310974, acc: 100.0, f1: 100.0, r: 0.7167741676921402
06/01/2019 11:04:56 step: 5394, epoch: 163, batch: 14, loss: 0.5734908580780029, acc: 100.0, f1: 100.0, r: 0.6600881615302727
06/01/2019 11:04:57 step: 5399, epoch: 163, batch: 19, loss: 0.008321559987962246, acc: 100.0, f1: 100.0, r: 0.746590392012587
06/01/2019 11:04:58 step: 5404, epoch: 163, batch: 24, loss: 0.009843100793659687, acc: 100.0, f1: 100.0, r: 0.773945232846612
06/01/2019 11:04:59 step: 5409, epoch: 163, batch: 29, loss: 0.014911786653101444, acc: 100.0, f1: 100.0, r: 0.6828324161729084
06/01/2019 11:04:59 *** evaluating ***
06/01/2019 11:05:00 step: 164, epoch: 163, acc: 58.119658119658126, f1: 29.545511662764202, r: 0.2724927919750875
06/01/2019 11:05:00 *** epoch: 165 ***
06/01/2019 11:05:00 *** training ***
06/01/2019 11:05:01 step: 5417, epoch: 164, batch: 4, loss: 0.06196368485689163, acc: 100.0, f1: 100.0, r: 0.7037240654777406
06/01/2019 11:05:02 step: 5422, epoch: 164, batch: 9, loss: 0.01630576327443123, acc: 100.0, f1: 100.0, r: 0.5694245112123776
06/01/2019 11:05:03 step: 5427, epoch: 164, batch: 14, loss: 0.014836737886071205, acc: 100.0, f1: 100.0, r: 0.7642319293256198
06/01/2019 11:05:04 step: 5432, epoch: 164, batch: 19, loss: 0.10572672635316849, acc: 98.4375, f1: 97.09677419354838, r: 0.7247035422369369
06/01/2019 11:05:05 step: 5437, epoch: 164, batch: 24, loss: 0.007967477664351463, acc: 100.0, f1: 100.0, r: 0.7344069534004942
06/01/2019 11:05:06 step: 5442, epoch: 164, batch: 29, loss: 0.3143821358680725, acc: 100.0, f1: 100.0, r: 0.6712982856668313
06/01/2019 11:05:07 *** evaluating ***
06/01/2019 11:05:07 step: 165, epoch: 164, acc: 58.54700854700855, f1: 32.48473348180645, r: 0.2650380016186318
06/01/2019 11:05:07 *** epoch: 166 ***
06/01/2019 11:05:07 *** training ***
06/01/2019 11:05:08 step: 5450, epoch: 165, batch: 4, loss: 0.013710723258554935, acc: 100.0, f1: 100.0, r: 0.759797488589576
06/01/2019 11:05:09 step: 5455, epoch: 165, batch: 9, loss: 0.017374863848090172, acc: 100.0, f1: 100.0, r: 0.6684866777000549
06/01/2019 11:05:10 step: 5460, epoch: 165, batch: 14, loss: 0.02075481414794922, acc: 100.0, f1: 100.0, r: 0.7050900912180358
06/01/2019 11:05:11 step: 5465, epoch: 165, batch: 19, loss: 0.013854313641786575, acc: 100.0, f1: 100.0, r: 0.7931699040158667
06/01/2019 11:05:12 step: 5470, epoch: 165, batch: 24, loss: 0.038132984191179276, acc: 100.0, f1: 100.0, r: 0.685772790547875
06/01/2019 11:05:13 step: 5475, epoch: 165, batch: 29, loss: 0.02237892709672451, acc: 100.0, f1: 100.0, r: 0.7875917747793527
06/01/2019 11:05:14 *** evaluating ***
06/01/2019 11:05:15 step: 166, epoch: 165, acc: 59.82905982905983, f1: 31.4373859626983, r: 0.27044743505501395
06/01/2019 11:05:15 *** epoch: 167 ***
06/01/2019 11:05:15 *** training ***
06/01/2019 11:05:16 step: 5483, epoch: 166, batch: 4, loss: 0.006665304768830538, acc: 100.0, f1: 100.0, r: 0.7330973814065836
06/01/2019 11:05:17 step: 5488, epoch: 166, batch: 9, loss: 0.04100652411580086, acc: 100.0, f1: 100.0, r: 0.7174903475599643
06/01/2019 11:05:18 step: 5493, epoch: 166, batch: 14, loss: 0.022965064272284508, acc: 100.0, f1: 100.0, r: 0.7616332664586015
06/01/2019 11:05:19 step: 5498, epoch: 166, batch: 19, loss: 0.042221710085868835, acc: 100.0, f1: 100.0, r: 0.6611149475641644
06/01/2019 11:05:20 step: 5503, epoch: 166, batch: 24, loss: 0.029929429292678833, acc: 100.0, f1: 100.0, r: 0.7087597020643477
06/01/2019 11:05:21 step: 5508, epoch: 166, batch: 29, loss: 0.005543146748095751, acc: 100.0, f1: 100.0, r: 0.6876043265542084
06/01/2019 11:05:22 *** evaluating ***
06/01/2019 11:05:22 step: 167, epoch: 166, acc: 58.54700854700855, f1: 31.457839186785087, r: 0.26990603157426385
06/01/2019 11:05:22 *** epoch: 168 ***
06/01/2019 11:05:22 *** training ***
06/01/2019 11:05:23 step: 5516, epoch: 167, batch: 4, loss: 0.035548336803913116, acc: 98.4375, f1: 98.57293868921776, r: 0.7565253353108073
06/01/2019 11:05:24 step: 5521, epoch: 167, batch: 9, loss: 0.018700120970606804, acc: 100.0, f1: 100.0, r: 0.6920188425366336
06/01/2019 11:05:25 step: 5526, epoch: 167, batch: 14, loss: 0.013223028741776943, acc: 100.0, f1: 100.0, r: 0.6893554302442565
06/01/2019 11:05:27 step: 5531, epoch: 167, batch: 19, loss: 0.0389232411980629, acc: 100.0, f1: 100.0, r: 0.6208721178763339
06/01/2019 11:05:27 step: 5536, epoch: 167, batch: 24, loss: 0.013160230591893196, acc: 100.0, f1: 100.0, r: 0.6740933754243498
06/01/2019 11:05:29 step: 5541, epoch: 167, batch: 29, loss: 0.01472171489149332, acc: 100.0, f1: 100.0, r: 0.7564405701086078
06/01/2019 11:05:29 *** evaluating ***
06/01/2019 11:05:29 step: 168, epoch: 167, acc: 58.119658119658126, f1: 32.578540625889715, r: 0.2683210864060648
06/01/2019 11:05:29 *** epoch: 169 ***
06/01/2019 11:05:29 *** training ***
06/01/2019 11:05:31 step: 5549, epoch: 168, batch: 4, loss: 0.025458334013819695, acc: 98.4375, f1: 97.64172335600908, r: 0.7347589078787549
06/01/2019 11:05:32 step: 5554, epoch: 168, batch: 9, loss: 0.01385206077247858, acc: 100.0, f1: 100.0, r: 0.6781942717915524
06/01/2019 11:05:33 step: 5559, epoch: 168, batch: 14, loss: 0.010694201104342937, acc: 100.0, f1: 100.0, r: 0.671720536316011
06/01/2019 11:05:34 step: 5564, epoch: 168, batch: 19, loss: 0.012471111491322517, acc: 100.0, f1: 100.0, r: 0.7154915709727134
06/01/2019 11:05:35 step: 5569, epoch: 168, batch: 24, loss: 0.02171795628964901, acc: 100.0, f1: 100.0, r: 0.7388932608786013
06/01/2019 11:05:36 step: 5574, epoch: 168, batch: 29, loss: 0.011748463846743107, acc: 100.0, f1: 100.0, r: 0.7232093429132095
06/01/2019 11:05:37 *** evaluating ***
06/01/2019 11:05:37 step: 169, epoch: 168, acc: 58.119658119658126, f1: 31.63800479290343, r: 0.2679732685561372
06/01/2019 11:05:37 *** epoch: 170 ***
06/01/2019 11:05:37 *** training ***
06/01/2019 11:05:38 step: 5582, epoch: 169, batch: 4, loss: 0.01828523725271225, acc: 100.0, f1: 100.0, r: 0.7719366235106839
06/01/2019 11:05:39 step: 5587, epoch: 169, batch: 9, loss: 0.05685923993587494, acc: 100.0, f1: 100.0, r: 0.7975104558551778
06/01/2019 11:05:40 step: 5592, epoch: 169, batch: 14, loss: 0.015090093947947025, acc: 100.0, f1: 100.0, r: 0.6984410585269453
06/01/2019 11:05:41 step: 5597, epoch: 169, batch: 19, loss: 0.00622618617489934, acc: 100.0, f1: 100.0, r: 0.7105912604902632
06/01/2019 11:05:42 step: 5602, epoch: 169, batch: 24, loss: 0.013606949709355831, acc: 100.0, f1: 100.0, r: 0.7744960849547363
06/01/2019 11:05:44 step: 5607, epoch: 169, batch: 29, loss: 0.028153598308563232, acc: 100.0, f1: 100.0, r: 0.7687009710514262
06/01/2019 11:05:44 *** evaluating ***
06/01/2019 11:05:45 step: 170, epoch: 169, acc: 58.54700854700855, f1: 29.30866450395826, r: 0.2680350356113495
06/01/2019 11:05:45 *** epoch: 171 ***
06/01/2019 11:05:45 *** training ***
06/01/2019 11:05:46 step: 5615, epoch: 170, batch: 4, loss: 0.06947753578424454, acc: 96.875, f1: 97.601664284137, r: 0.7058720861158249
06/01/2019 11:05:47 step: 5620, epoch: 170, batch: 9, loss: 0.011855538003146648, acc: 100.0, f1: 100.0, r: 0.7411159406501069
06/01/2019 11:05:48 step: 5625, epoch: 170, batch: 14, loss: 0.011926688253879547, acc: 100.0, f1: 100.0, r: 0.7093919890685894
06/01/2019 11:05:49 step: 5630, epoch: 170, batch: 19, loss: 0.596776008605957, acc: 100.0, f1: 100.0, r: 0.7761092513859835
06/01/2019 11:05:50 step: 5635, epoch: 170, batch: 24, loss: 0.008195919916033745, acc: 100.0, f1: 100.0, r: 0.8219762877479455
06/01/2019 11:05:51 step: 5640, epoch: 170, batch: 29, loss: 0.05162213370203972, acc: 100.0, f1: 100.0, r: 0.6767372225444169
06/01/2019 11:05:52 *** evaluating ***
06/01/2019 11:05:52 step: 171, epoch: 170, acc: 56.837606837606835, f1: 30.596704565438078, r: 0.26790647103438525
06/01/2019 11:05:52 *** epoch: 172 ***
06/01/2019 11:05:52 *** training ***
06/01/2019 11:05:53 step: 5648, epoch: 171, batch: 4, loss: 0.02833581529557705, acc: 100.0, f1: 100.0, r: 0.7087977621543098
06/01/2019 11:05:54 step: 5653, epoch: 171, batch: 9, loss: 0.01102232001721859, acc: 100.0, f1: 100.0, r: 0.7905524081866214
06/01/2019 11:05:55 step: 5658, epoch: 171, batch: 14, loss: 0.04289703071117401, acc: 98.4375, f1: 99.10600255427842, r: 0.730086592712961
06/01/2019 11:05:56 step: 5663, epoch: 171, batch: 19, loss: 0.02164079248905182, acc: 100.0, f1: 100.0, r: 0.8630591635502467
06/01/2019 11:05:58 step: 5668, epoch: 171, batch: 24, loss: 0.023644421249628067, acc: 100.0, f1: 100.0, r: 0.7897743593605737
06/01/2019 11:05:59 step: 5673, epoch: 171, batch: 29, loss: 0.015033232979476452, acc: 100.0, f1: 100.0, r: 0.8106336195377001
06/01/2019 11:05:59 *** evaluating ***
06/01/2019 11:05:59 step: 172, epoch: 171, acc: 58.54700854700855, f1: 31.70479962808555, r: 0.2665597124585242
06/01/2019 11:05:59 *** epoch: 173 ***
06/01/2019 11:05:59 *** training ***
06/01/2019 11:06:01 step: 5681, epoch: 172, batch: 4, loss: 0.02681584283709526, acc: 100.0, f1: 100.0, r: 0.7817449806176907
06/01/2019 11:06:02 step: 5686, epoch: 172, batch: 9, loss: 0.019850317388772964, acc: 100.0, f1: 100.0, r: 0.7087892156983888
06/01/2019 11:06:03 step: 5691, epoch: 172, batch: 14, loss: 0.07586701214313507, acc: 98.4375, f1: 98.17219817219816, r: 0.7063746609742982
06/01/2019 11:06:04 step: 5696, epoch: 172, batch: 19, loss: 0.017231453210115433, acc: 100.0, f1: 100.0, r: 0.7858150908046885
06/01/2019 11:06:05 step: 5701, epoch: 172, batch: 24, loss: 0.6278887987136841, acc: 100.0, f1: 100.0, r: 0.6563476093301398
06/01/2019 11:06:06 step: 5706, epoch: 172, batch: 29, loss: 0.05408402532339096, acc: 98.4375, f1: 97.73242630385488, r: 0.6235625559624297
06/01/2019 11:06:06 *** evaluating ***
06/01/2019 11:06:07 step: 173, epoch: 172, acc: 57.692307692307686, f1: 31.043600696614664, r: 0.2678083352338157
06/01/2019 11:06:07 *** epoch: 174 ***
06/01/2019 11:06:07 *** training ***
06/01/2019 11:06:08 step: 5714, epoch: 173, batch: 4, loss: 0.02999698370695114, acc: 98.4375, f1: 94.28571428571428, r: 0.7636562732047619
06/01/2019 11:06:09 step: 5719, epoch: 173, batch: 9, loss: 0.036261703819036484, acc: 100.0, f1: 100.0, r: 0.7819299630936316
06/01/2019 11:06:10 step: 5724, epoch: 173, batch: 14, loss: 0.34204891324043274, acc: 98.4375, f1: 95.0, r: 0.8251764855041688
06/01/2019 11:06:11 step: 5729, epoch: 173, batch: 19, loss: 0.0579853430390358, acc: 98.4375, f1: 98.86128364389234, r: 0.7487683456232603
06/01/2019 11:06:12 step: 5734, epoch: 173, batch: 24, loss: 0.012802871875464916, acc: 100.0, f1: 100.0, r: 0.8070362633698709
06/01/2019 11:06:13 step: 5739, epoch: 173, batch: 29, loss: 0.607073962688446, acc: 100.0, f1: 100.0, r: 0.7910842912478465
06/01/2019 11:06:14 *** evaluating ***
06/01/2019 11:06:14 step: 174, epoch: 173, acc: 56.837606837606835, f1: 31.191712476031736, r: 0.2645726924897416
06/01/2019 11:06:14 *** epoch: 175 ***
06/01/2019 11:06:14 *** training ***
06/01/2019 11:06:15 step: 5747, epoch: 174, batch: 4, loss: 0.011363537050783634, acc: 100.0, f1: 100.0, r: 0.8328728425730295
06/01/2019 11:06:16 step: 5752, epoch: 174, batch: 9, loss: 0.02544299326837063, acc: 100.0, f1: 100.0, r: 0.7097931231804806
06/01/2019 11:06:17 step: 5757, epoch: 174, batch: 14, loss: 0.012334722094237804, acc: 100.0, f1: 100.0, r: 0.6745404206336881
06/01/2019 11:06:18 step: 5762, epoch: 174, batch: 19, loss: 0.01220137532800436, acc: 100.0, f1: 100.0, r: 0.7760163511441021
06/01/2019 11:06:20 step: 5767, epoch: 174, batch: 24, loss: 0.03886120393872261, acc: 100.0, f1: 100.0, r: 0.7703734362968969
06/01/2019 11:06:21 step: 5772, epoch: 174, batch: 29, loss: 0.01634056493639946, acc: 100.0, f1: 100.0, r: 0.7343351411323913
06/01/2019 11:06:21 *** evaluating ***
06/01/2019 11:06:21 step: 175, epoch: 174, acc: 58.97435897435898, f1: 30.233463775369028, r: 0.269236441579442
06/01/2019 11:06:21 *** epoch: 176 ***
06/01/2019 11:06:21 *** training ***
06/01/2019 11:06:23 step: 5780, epoch: 175, batch: 4, loss: 0.03247744217514992, acc: 98.4375, f1: 98.8795518207283, r: 0.7323914539531597
06/01/2019 11:06:24 step: 5785, epoch: 175, batch: 9, loss: 0.022938691079616547, acc: 100.0, f1: 100.0, r: 0.6932669615121795
06/01/2019 11:06:25 step: 5790, epoch: 175, batch: 14, loss: 0.019130488857626915, acc: 100.0, f1: 100.0, r: 0.7761461745072389
06/01/2019 11:06:26 step: 5795, epoch: 175, batch: 19, loss: 0.013233261182904243, acc: 100.0, f1: 100.0, r: 0.8335261496056455
06/01/2019 11:06:27 step: 5800, epoch: 175, batch: 24, loss: 0.009932068176567554, acc: 100.0, f1: 100.0, r: 0.7588127120552484
06/01/2019 11:06:28 step: 5805, epoch: 175, batch: 29, loss: 0.6042377352714539, acc: 100.0, f1: 100.0, r: 0.840855356504561
06/01/2019 11:06:29 *** evaluating ***
06/01/2019 11:06:29 step: 176, epoch: 175, acc: 57.692307692307686, f1: 32.31943742802924, r: 0.2652694580388557
06/01/2019 11:06:29 *** epoch: 177 ***
06/01/2019 11:06:29 *** training ***
06/01/2019 11:06:30 step: 5813, epoch: 176, batch: 4, loss: 0.014788100495934486, acc: 100.0, f1: 100.0, r: 0.7937569801323333
06/01/2019 11:06:31 step: 5818, epoch: 176, batch: 9, loss: 0.008295237086713314, acc: 100.0, f1: 100.0, r: 0.7964128275441918
06/01/2019 11:06:32 step: 5823, epoch: 176, batch: 14, loss: 0.02206406183540821, acc: 100.0, f1: 100.0, r: 0.7611798943353906
06/01/2019 11:06:33 step: 5828, epoch: 176, batch: 19, loss: 0.011766836047172546, acc: 100.0, f1: 100.0, r: 0.7349348489170913
06/01/2019 11:06:34 step: 5833, epoch: 176, batch: 24, loss: 0.02537899650633335, acc: 98.4375, f1: 97.8609625668449, r: 0.707025698654671
06/01/2019 11:06:35 step: 5838, epoch: 176, batch: 29, loss: 0.01403292641043663, acc: 100.0, f1: 100.0, r: 0.6889653103663359
06/01/2019 11:06:36 *** evaluating ***
06/01/2019 11:06:37 step: 177, epoch: 176, acc: 58.119658119658126, f1: 31.704464725722737, r: 0.2711437591734377
06/01/2019 11:06:37 *** epoch: 178 ***
06/01/2019 11:06:37 *** training ***
06/01/2019 11:06:37 step: 5846, epoch: 177, batch: 4, loss: 0.016530418768525124, acc: 100.0, f1: 100.0, r: 0.7748792495448668
06/01/2019 11:06:38 step: 5851, epoch: 177, batch: 9, loss: 0.03189420700073242, acc: 100.0, f1: 100.0, r: 0.8562496627299436
06/01/2019 11:06:40 step: 5856, epoch: 177, batch: 14, loss: 0.03605850040912628, acc: 100.0, f1: 100.0, r: 0.8403382976686802
06/01/2019 11:06:41 step: 5861, epoch: 177, batch: 19, loss: 0.012860052287578583, acc: 100.0, f1: 100.0, r: 0.6163024286286822
06/01/2019 11:06:42 step: 5866, epoch: 177, batch: 24, loss: 0.023346342146396637, acc: 100.0, f1: 100.0, r: 0.8201672062767466
06/01/2019 11:06:43 step: 5871, epoch: 177, batch: 29, loss: 0.010670788586139679, acc: 100.0, f1: 100.0, r: 0.6410813500016015
06/01/2019 11:06:44 *** evaluating ***
06/01/2019 11:06:44 step: 178, epoch: 177, acc: 59.401709401709404, f1: 31.887238381251915, r: 0.2690209292163816
06/01/2019 11:06:44 *** epoch: 179 ***
06/01/2019 11:06:44 *** training ***
06/01/2019 11:06:45 step: 5879, epoch: 178, batch: 4, loss: 0.022264352068305016, acc: 98.4375, f1: 97.88359788359789, r: 0.73431729400825
06/01/2019 11:06:46 step: 5884, epoch: 178, batch: 9, loss: 0.041880469769239426, acc: 100.0, f1: 100.0, r: 0.8308756036970364
06/01/2019 11:06:47 step: 5889, epoch: 178, batch: 14, loss: 0.5794114470481873, acc: 100.0, f1: 100.0, r: 0.6944146055393025
06/01/2019 11:06:49 step: 5894, epoch: 178, batch: 19, loss: 0.030514558777213097, acc: 100.0, f1: 100.0, r: 0.6867118506756096
06/01/2019 11:06:50 step: 5899, epoch: 178, batch: 24, loss: 0.06187157705426216, acc: 98.4375, f1: 98.1111111111111, r: 0.7649954042218505
06/01/2019 11:06:51 step: 5904, epoch: 178, batch: 29, loss: 0.01861604116857052, acc: 100.0, f1: 100.0, r: 0.6812856627490855
06/01/2019 11:06:51 *** evaluating ***
06/01/2019 11:06:52 step: 179, epoch: 178, acc: 58.119658119658126, f1: 32.35970472047205, r: 0.26635254616181775
06/01/2019 11:06:52 *** epoch: 180 ***
06/01/2019 11:06:52 *** training ***
06/01/2019 11:06:53 step: 5912, epoch: 179, batch: 4, loss: 0.019333530217409134, acc: 100.0, f1: 100.0, r: 0.7045683886311765
06/01/2019 11:06:54 step: 5917, epoch: 179, batch: 9, loss: 0.04692964628338814, acc: 100.0, f1: 100.0, r: 0.7385911185305469
06/01/2019 11:06:55 step: 5922, epoch: 179, batch: 14, loss: 0.01649496518075466, acc: 100.0, f1: 100.0, r: 0.7235879512504853
06/01/2019 11:06:56 step: 5927, epoch: 179, batch: 19, loss: 0.01855062134563923, acc: 100.0, f1: 100.0, r: 0.7008322819162491
06/01/2019 11:06:57 step: 5932, epoch: 179, batch: 24, loss: 0.320818156003952, acc: 100.0, f1: 100.0, r: 0.8448575551058466
06/01/2019 11:06:58 step: 5937, epoch: 179, batch: 29, loss: 0.6140454411506653, acc: 98.4375, f1: 97.47899159663866, r: 0.7634459230714199
06/01/2019 11:06:59 *** evaluating ***
06/01/2019 11:06:59 step: 180, epoch: 179, acc: 58.119658119658126, f1: 32.37893642305407, r: 0.2672454204554043
06/01/2019 11:06:59 *** epoch: 181 ***
06/01/2019 11:06:59 *** training ***
06/01/2019 11:07:01 step: 5945, epoch: 180, batch: 4, loss: 0.008320329710841179, acc: 100.0, f1: 100.0, r: 0.7019820153422565
06/01/2019 11:07:02 step: 5950, epoch: 180, batch: 9, loss: 0.01767256110906601, acc: 100.0, f1: 100.0, r: 0.678707034764429
06/01/2019 11:07:03 step: 5955, epoch: 180, batch: 14, loss: 0.01611505076289177, acc: 100.0, f1: 100.0, r: 0.6557747596266843
06/01/2019 11:07:04 step: 5960, epoch: 180, batch: 19, loss: 0.03546900674700737, acc: 98.4375, f1: 97.11399711399712, r: 0.782842476373402
06/01/2019 11:07:05 step: 5965, epoch: 180, batch: 24, loss: 0.020413337275385857, acc: 100.0, f1: 100.0, r: 0.8410993185787838
06/01/2019 11:07:06 step: 5970, epoch: 180, batch: 29, loss: 0.015065348707139492, acc: 100.0, f1: 100.0, r: 0.7809218757397225
06/01/2019 11:07:07 *** evaluating ***
06/01/2019 11:07:07 step: 181, epoch: 180, acc: 55.55555555555556, f1: 29.985498382642383, r: 0.2666160875515195
06/01/2019 11:07:07 *** epoch: 182 ***
06/01/2019 11:07:07 *** training ***
06/01/2019 11:07:08 step: 5978, epoch: 181, batch: 4, loss: 0.01632227748632431, acc: 100.0, f1: 100.0, r: 0.7388587904469515
06/01/2019 11:07:09 step: 5983, epoch: 181, batch: 9, loss: 0.04520653188228607, acc: 98.4375, f1: 95.10204081632652, r: 0.7156022051347953
06/01/2019 11:07:10 step: 5988, epoch: 181, batch: 14, loss: 0.021506644785404205, acc: 100.0, f1: 100.0, r: 0.6446432395704625
06/01/2019 11:07:11 step: 5993, epoch: 181, batch: 19, loss: 0.01946801319718361, acc: 100.0, f1: 100.0, r: 0.7052554071054933
06/01/2019 11:07:13 step: 5998, epoch: 181, batch: 24, loss: 0.016219450160861015, acc: 100.0, f1: 100.0, r: 0.6589451283606944
06/01/2019 11:07:14 step: 6003, epoch: 181, batch: 29, loss: 0.008913976140320301, acc: 100.0, f1: 100.0, r: 0.6635461477953823
06/01/2019 11:07:14 *** evaluating ***
06/01/2019 11:07:14 step: 182, epoch: 181, acc: 58.54700854700855, f1: 31.712250738927022, r: 0.2672786596100247
06/01/2019 11:07:14 *** epoch: 183 ***
06/01/2019 11:07:14 *** training ***
06/01/2019 11:07:16 step: 6011, epoch: 182, batch: 4, loss: 0.005503098480403423, acc: 100.0, f1: 100.0, r: 0.7206209740004313
06/01/2019 11:07:17 step: 6016, epoch: 182, batch: 9, loss: 0.021867569535970688, acc: 100.0, f1: 100.0, r: 0.7852175560015695
06/01/2019 11:07:18 step: 6021, epoch: 182, batch: 14, loss: 0.019992714747786522, acc: 100.0, f1: 100.0, r: 0.764306248758885
06/01/2019 11:07:19 step: 6026, epoch: 182, batch: 19, loss: 0.020272603258490562, acc: 100.0, f1: 100.0, r: 0.7102677816823008
06/01/2019 11:07:20 step: 6031, epoch: 182, batch: 24, loss: 0.029148980975151062, acc: 100.0, f1: 100.0, r: 0.7275637297080012
06/01/2019 11:07:21 step: 6036, epoch: 182, batch: 29, loss: 0.04960820451378822, acc: 100.0, f1: 100.0, r: 0.771238858554144
06/01/2019 11:07:22 *** evaluating ***
06/01/2019 11:07:22 step: 183, epoch: 182, acc: 58.119658119658126, f1: 29.135131190452267, r: 0.2699198091377808
06/01/2019 11:07:22 *** epoch: 184 ***
06/01/2019 11:07:22 *** training ***
06/01/2019 11:07:23 step: 6044, epoch: 183, batch: 4, loss: 0.01633155345916748, acc: 100.0, f1: 100.0, r: 0.8229060711290097
06/01/2019 11:07:25 step: 6049, epoch: 183, batch: 9, loss: 0.02828475460410118, acc: 98.4375, f1: 97.65523230568823, r: 0.7080124929898352
06/01/2019 11:07:26 step: 6054, epoch: 183, batch: 14, loss: 0.011492178775370121, acc: 100.0, f1: 100.0, r: 0.7573903480493516
06/01/2019 11:07:27 step: 6059, epoch: 183, batch: 19, loss: 0.024769321084022522, acc: 100.0, f1: 100.0, r: 0.7138177037916061
06/01/2019 11:07:28 step: 6064, epoch: 183, batch: 24, loss: 0.016307363286614418, acc: 100.0, f1: 100.0, r: 0.7607877052853989
06/01/2019 11:07:29 step: 6069, epoch: 183, batch: 29, loss: 0.020409300923347473, acc: 100.0, f1: 100.0, r: 0.7912990765515956
06/01/2019 11:07:30 *** evaluating ***
06/01/2019 11:07:30 step: 184, epoch: 183, acc: 58.119658119658126, f1: 31.754953946528065, r: 0.26757008108267466
06/01/2019 11:07:30 *** epoch: 185 ***
06/01/2019 11:07:30 *** training ***
06/01/2019 11:07:31 step: 6077, epoch: 184, batch: 4, loss: 0.009875690564513206, acc: 100.0, f1: 100.0, r: 0.7468584071548642
06/01/2019 11:07:32 step: 6082, epoch: 184, batch: 9, loss: 0.017426075413823128, acc: 100.0, f1: 100.0, r: 0.8054663678489047
06/01/2019 11:07:33 step: 6087, epoch: 184, batch: 14, loss: 0.026699386537075043, acc: 100.0, f1: 100.0, r: 0.7656607129513281
06/01/2019 11:07:34 step: 6092, epoch: 184, batch: 19, loss: 0.009630042128264904, acc: 100.0, f1: 100.0, r: 0.7659735702691751
06/01/2019 11:07:35 step: 6097, epoch: 184, batch: 24, loss: 0.01718004420399666, acc: 100.0, f1: 100.0, r: 0.8045060338863922
06/01/2019 11:07:36 step: 6102, epoch: 184, batch: 29, loss: 0.012732209637761116, acc: 100.0, f1: 100.0, r: 0.7267050073393938
06/01/2019 11:07:37 *** evaluating ***
06/01/2019 11:07:37 step: 185, epoch: 184, acc: 59.82905982905983, f1: 33.01239963004669, r: 0.267009738209719
06/01/2019 11:07:37 *** epoch: 186 ***
06/01/2019 11:07:37 *** training ***
06/01/2019 11:07:38 step: 6110, epoch: 185, batch: 4, loss: 0.602687656879425, acc: 100.0, f1: 100.0, r: 0.7321132726315486
06/01/2019 11:07:40 step: 6115, epoch: 185, batch: 9, loss: 0.01908981427550316, acc: 100.0, f1: 100.0, r: 0.6975173003780448
06/01/2019 11:07:41 step: 6120, epoch: 185, batch: 14, loss: 0.014492407441139221, acc: 100.0, f1: 100.0, r: 0.6616057416121819
06/01/2019 11:07:42 step: 6125, epoch: 185, batch: 19, loss: 0.021672889590263367, acc: 100.0, f1: 100.0, r: 0.7088372772094269
06/01/2019 11:07:43 step: 6130, epoch: 185, batch: 24, loss: 0.03779474273324013, acc: 100.0, f1: 100.0, r: 0.7936240309601218
06/01/2019 11:07:44 step: 6135, epoch: 185, batch: 29, loss: 0.058150406926870346, acc: 98.4375, f1: 97.81818181818181, r: 0.6876965537934875
06/01/2019 11:07:45 *** evaluating ***
06/01/2019 11:07:45 step: 186, epoch: 185, acc: 58.54700854700855, f1: 32.87427831600763, r: 0.26389499280877526
06/01/2019 11:07:45 *** epoch: 187 ***
06/01/2019 11:07:45 *** training ***
06/01/2019 11:07:46 step: 6143, epoch: 186, batch: 4, loss: 0.059439707547426224, acc: 98.4375, f1: 97.25274725274726, r: 0.8410940376512402
06/01/2019 11:07:47 step: 6148, epoch: 186, batch: 9, loss: 0.011920075863599777, acc: 100.0, f1: 100.0, r: 0.6379503162613775
06/01/2019 11:07:48 step: 6153, epoch: 186, batch: 14, loss: 0.02040567435324192, acc: 100.0, f1: 100.0, r: 0.6552826011977166
06/01/2019 11:07:49 step: 6158, epoch: 186, batch: 19, loss: 0.019742295145988464, acc: 100.0, f1: 100.0, r: 0.7962931280986957
06/01/2019 11:07:50 step: 6163, epoch: 186, batch: 24, loss: 0.013447172939777374, acc: 100.0, f1: 100.0, r: 0.8384959461575318
06/01/2019 11:07:52 step: 6168, epoch: 186, batch: 29, loss: 0.015417387709021568, acc: 100.0, f1: 100.0, r: 0.6752865655805274
06/01/2019 11:07:52 *** evaluating ***
06/01/2019 11:07:53 step: 187, epoch: 186, acc: 57.692307692307686, f1: 31.598572845874518, r: 0.2662944678947483
06/01/2019 11:07:53 *** epoch: 188 ***
06/01/2019 11:07:53 *** training ***
06/01/2019 11:07:54 step: 6176, epoch: 187, batch: 4, loss: 0.6022655367851257, acc: 100.0, f1: 100.0, r: 0.8084709619086499
06/01/2019 11:07:55 step: 6181, epoch: 187, batch: 9, loss: 0.007850723341107368, acc: 100.0, f1: 100.0, r: 0.7308738159386081
06/01/2019 11:07:56 step: 6186, epoch: 187, batch: 14, loss: 0.5539811253547668, acc: 100.0, f1: 100.0, r: 0.7418773963693267
06/01/2019 11:07:57 step: 6191, epoch: 187, batch: 19, loss: 0.011662773787975311, acc: 100.0, f1: 100.0, r: 0.7715747332588271
06/01/2019 11:07:58 step: 6196, epoch: 187, batch: 24, loss: 0.014044421724975109, acc: 100.0, f1: 100.0, r: 0.7753306003632583
06/01/2019 11:07:59 step: 6201, epoch: 187, batch: 29, loss: 0.014127828180789948, acc: 100.0, f1: 100.0, r: 0.6621913707590266
06/01/2019 11:08:00 *** evaluating ***
06/01/2019 11:08:00 step: 188, epoch: 187, acc: 58.119658119658126, f1: 31.23753447294727, r: 0.26909098539300047
06/01/2019 11:08:00 *** epoch: 189 ***
06/01/2019 11:08:00 *** training ***
06/01/2019 11:08:01 step: 6209, epoch: 188, batch: 4, loss: 0.026735944673419, acc: 100.0, f1: 100.0, r: 0.8395175294404327
06/01/2019 11:08:02 step: 6214, epoch: 188, batch: 9, loss: 0.01852390542626381, acc: 100.0, f1: 100.0, r: 0.7585809537466879
06/01/2019 11:08:03 step: 6219, epoch: 188, batch: 14, loss: 0.5764777660369873, acc: 98.4375, f1: 98.62098685628098, r: 0.6911827137850524
06/01/2019 11:08:05 step: 6224, epoch: 188, batch: 19, loss: 0.028807764872908592, acc: 98.4375, f1: 97.94832826747721, r: 0.7487541340342441
06/01/2019 11:08:06 step: 6229, epoch: 188, batch: 24, loss: 0.018434645608067513, acc: 100.0, f1: 100.0, r: 0.6947665303459556
06/01/2019 11:08:07 step: 6234, epoch: 188, batch: 29, loss: 0.01087575126439333, acc: 100.0, f1: 100.0, r: 0.7683515252169679
06/01/2019 11:08:08 *** evaluating ***
06/01/2019 11:08:08 step: 189, epoch: 188, acc: 57.692307692307686, f1: 31.545376714971407, r: 0.26533674981885064
06/01/2019 11:08:08 *** epoch: 190 ***
06/01/2019 11:08:08 *** training ***
06/01/2019 11:08:09 step: 6242, epoch: 189, batch: 4, loss: 0.010814051143825054, acc: 100.0, f1: 100.0, r: 0.7146082374545679
06/01/2019 11:08:10 step: 6247, epoch: 189, batch: 9, loss: 0.019024178385734558, acc: 100.0, f1: 100.0, r: 0.824924071247291
06/01/2019 11:08:11 step: 6252, epoch: 189, batch: 14, loss: 0.05601373687386513, acc: 98.4375, f1: 95.37037037037037, r: 0.7896293132804549
06/01/2019 11:08:12 step: 6257, epoch: 189, batch: 19, loss: 0.02451947331428528, acc: 100.0, f1: 100.0, r: 0.711230453078338
06/01/2019 11:08:13 step: 6262, epoch: 189, batch: 24, loss: 0.018619192764163017, acc: 100.0, f1: 100.0, r: 0.7198921167870076
06/01/2019 11:08:14 step: 6267, epoch: 189, batch: 29, loss: 0.00931917130947113, acc: 100.0, f1: 100.0, r: 0.6987382228456053
06/01/2019 11:08:15 *** evaluating ***
06/01/2019 11:08:16 step: 190, epoch: 189, acc: 55.12820512820513, f1: 30.01507731793324, r: 0.2649302094499298
06/01/2019 11:08:16 *** epoch: 191 ***
06/01/2019 11:08:16 *** training ***
06/01/2019 11:08:16 step: 6275, epoch: 190, batch: 4, loss: 0.016071168705821037, acc: 100.0, f1: 100.0, r: 0.8103686936846797
06/01/2019 11:08:18 step: 6280, epoch: 190, batch: 9, loss: 0.03400528430938721, acc: 100.0, f1: 100.0, r: 0.7198498088340745
06/01/2019 11:08:19 step: 6285, epoch: 190, batch: 14, loss: 0.026652144268155098, acc: 98.4375, f1: 94.82993197278911, r: 0.6151656623594296
06/01/2019 11:08:20 step: 6290, epoch: 190, batch: 19, loss: 0.07300560921430588, acc: 98.4375, f1: 98.27998088867655, r: 0.712654000062494
06/01/2019 11:08:21 step: 6295, epoch: 190, batch: 24, loss: 0.007126000709831715, acc: 100.0, f1: 100.0, r: 0.6622030630882747
06/01/2019 11:08:22 step: 6300, epoch: 190, batch: 29, loss: 0.02248126082122326, acc: 100.0, f1: 100.0, r: 0.8019222102732649
06/01/2019 11:08:23 *** evaluating ***
06/01/2019 11:08:23 step: 191, epoch: 190, acc: 58.54700854700855, f1: 31.256783965762292, r: 0.26794000604085844
06/01/2019 11:08:23 *** epoch: 192 ***
06/01/2019 11:08:23 *** training ***
06/01/2019 11:08:24 step: 6308, epoch: 191, batch: 4, loss: 0.03243144601583481, acc: 100.0, f1: 100.0, r: 0.8359172089247621
06/01/2019 11:08:25 step: 6313, epoch: 191, batch: 9, loss: 0.01684173010289669, acc: 100.0, f1: 100.0, r: 0.8033548382092913
06/01/2019 11:08:26 step: 6318, epoch: 191, batch: 14, loss: 0.019832110032439232, acc: 100.0, f1: 100.0, r: 0.7277593243835939
06/01/2019 11:08:27 step: 6323, epoch: 191, batch: 19, loss: 0.6015570163726807, acc: 98.4375, f1: 97.8021978021978, r: 0.6581198881790684
06/01/2019 11:08:29 step: 6328, epoch: 191, batch: 24, loss: 0.016509033739566803, acc: 100.0, f1: 100.0, r: 0.7637696278982873
06/01/2019 11:08:30 step: 6333, epoch: 191, batch: 29, loss: 0.02917935512959957, acc: 98.4375, f1: 96.76470588235294, r: 0.7488353864447548
06/01/2019 11:08:30 *** evaluating ***
06/01/2019 11:08:31 step: 192, epoch: 191, acc: 57.26495726495726, f1: 31.073116426057602, r: 0.26687900206903853
06/01/2019 11:08:31 *** epoch: 193 ***
06/01/2019 11:08:31 *** training ***
06/01/2019 11:08:32 step: 6341, epoch: 192, batch: 4, loss: 0.00768996961414814, acc: 100.0, f1: 100.0, r: 0.8301173553726281
06/01/2019 11:08:33 step: 6346, epoch: 192, batch: 9, loss: 0.01626112312078476, acc: 100.0, f1: 100.0, r: 0.725223065350948
06/01/2019 11:08:34 step: 6351, epoch: 192, batch: 14, loss: 0.0072563448920845985, acc: 100.0, f1: 100.0, r: 0.7142565958141662
06/01/2019 11:08:35 step: 6356, epoch: 192, batch: 19, loss: 0.32354745268821716, acc: 100.0, f1: 100.0, r: 0.7958999845240773
06/01/2019 11:08:36 step: 6361, epoch: 192, batch: 24, loss: 0.04340940713882446, acc: 98.4375, f1: 98.17219817219816, r: 0.6592397154859085
06/01/2019 11:08:37 step: 6366, epoch: 192, batch: 29, loss: 0.05512656271457672, acc: 100.0, f1: 100.0, r: 0.7799600544851408
06/01/2019 11:08:38 *** evaluating ***
06/01/2019 11:08:38 step: 193, epoch: 192, acc: 59.82905982905983, f1: 32.51263772390533, r: 0.2649243782095863
06/01/2019 11:08:38 *** epoch: 194 ***
06/01/2019 11:08:38 *** training ***
06/01/2019 11:08:39 step: 6374, epoch: 193, batch: 4, loss: 0.015319238416850567, acc: 100.0, f1: 100.0, r: 0.7353285322038416
06/01/2019 11:08:41 step: 6379, epoch: 193, batch: 9, loss: 0.027214031666517258, acc: 98.4375, f1: 98.14814814814814, r: 0.7626692543010374
06/01/2019 11:08:42 step: 6384, epoch: 193, batch: 14, loss: 0.31786400079727173, acc: 100.0, f1: 100.0, r: 0.7484770845167137
06/01/2019 11:08:43 step: 6389, epoch: 193, batch: 19, loss: 0.017605699598789215, acc: 100.0, f1: 100.0, r: 0.7969322419155977
06/01/2019 11:08:44 step: 6394, epoch: 193, batch: 24, loss: 0.014483772218227386, acc: 100.0, f1: 100.0, r: 0.7246784479841882
06/01/2019 11:08:45 step: 6399, epoch: 193, batch: 29, loss: 0.02435382828116417, acc: 98.4375, f1: 97.77777777777779, r: 0.822574579187136
06/01/2019 11:08:46 *** evaluating ***
06/01/2019 11:08:46 step: 194, epoch: 193, acc: 58.119658119658126, f1: 28.04220809941556, r: 0.2692113347579955
06/01/2019 11:08:46 *** epoch: 195 ***
06/01/2019 11:08:46 *** training ***
06/01/2019 11:08:47 step: 6407, epoch: 194, batch: 4, loss: 0.02358151227235794, acc: 100.0, f1: 100.0, r: 0.715926610615764
06/01/2019 11:08:48 step: 6412, epoch: 194, batch: 9, loss: 0.5536057949066162, acc: 100.0, f1: 100.0, r: 0.8484099408342799
06/01/2019 11:08:49 step: 6417, epoch: 194, batch: 14, loss: 0.019659627228975296, acc: 100.0, f1: 100.0, r: 0.8076611397072595
06/01/2019 11:08:51 step: 6422, epoch: 194, batch: 19, loss: 0.0082254558801651, acc: 100.0, f1: 100.0, r: 0.6976640288613135
06/01/2019 11:08:52 step: 6427, epoch: 194, batch: 24, loss: 0.02028457447886467, acc: 100.0, f1: 100.0, r: 0.8258278767481231
06/01/2019 11:08:53 step: 6432, epoch: 194, batch: 29, loss: 0.0329340323805809, acc: 100.0, f1: 100.0, r: 0.7323244885063219
06/01/2019 11:08:54 *** evaluating ***
06/01/2019 11:08:54 step: 195, epoch: 194, acc: 58.97435897435898, f1: 31.143720965689404, r: 0.26707768296918555
06/01/2019 11:08:54 *** epoch: 196 ***
06/01/2019 11:08:54 *** training ***
06/01/2019 11:08:55 step: 6440, epoch: 195, batch: 4, loss: 0.05584563687443733, acc: 100.0, f1: 100.0, r: 0.6982346215501792
06/01/2019 11:08:56 step: 6445, epoch: 195, batch: 9, loss: 0.032226528972387314, acc: 100.0, f1: 100.0, r: 0.8211444375756123
06/01/2019 11:08:57 step: 6450, epoch: 195, batch: 14, loss: 0.016596514731645584, acc: 100.0, f1: 100.0, r: 0.7050703864320962
06/01/2019 11:08:58 step: 6455, epoch: 195, batch: 19, loss: 0.04403683915734291, acc: 100.0, f1: 100.0, r: 0.7719569486799416
06/01/2019 11:08:59 step: 6460, epoch: 195, batch: 24, loss: 0.03947851061820984, acc: 100.0, f1: 100.0, r: 0.7178638277895026
06/01/2019 11:09:01 step: 6465, epoch: 195, batch: 29, loss: 0.024207185953855515, acc: 100.0, f1: 100.0, r: 0.6828917198399171
06/01/2019 11:09:01 *** evaluating ***
06/01/2019 11:09:02 step: 196, epoch: 195, acc: 58.97435897435898, f1: 31.341178598641285, r: 0.2712852407470269
06/01/2019 11:09:02 *** epoch: 197 ***
06/01/2019 11:09:02 *** training ***
06/01/2019 11:09:03 step: 6473, epoch: 196, batch: 4, loss: 0.01978539675474167, acc: 100.0, f1: 100.0, r: 0.7239399924520901
06/01/2019 11:09:04 step: 6478, epoch: 196, batch: 9, loss: 0.8424820899963379, acc: 100.0, f1: 100.0, r: 0.7808834638717453
06/01/2019 11:09:05 step: 6483, epoch: 196, batch: 14, loss: 0.010196923278272152, acc: 100.0, f1: 100.0, r: 0.8416412395836342
06/01/2019 11:09:06 step: 6488, epoch: 196, batch: 19, loss: 0.014507879503071308, acc: 100.0, f1: 100.0, r: 0.623117965954736
06/01/2019 11:09:08 step: 6493, epoch: 196, batch: 24, loss: 0.057283274829387665, acc: 100.0, f1: 100.0, r: 0.7083310897140015
06/01/2019 11:09:09 step: 6498, epoch: 196, batch: 29, loss: 0.04079541191458702, acc: 100.0, f1: 100.0, r: 0.741072935459354
06/01/2019 11:09:09 *** evaluating ***
06/01/2019 11:09:10 step: 197, epoch: 196, acc: 57.692307692307686, f1: 32.02684871325523, r: 0.26685917881494114
06/01/2019 11:09:10 *** epoch: 198 ***
06/01/2019 11:09:10 *** training ***
06/01/2019 11:09:11 step: 6506, epoch: 197, batch: 4, loss: 0.014412294141948223, acc: 100.0, f1: 100.0, r: 0.8236580209317577
06/01/2019 11:09:12 step: 6511, epoch: 197, batch: 9, loss: 0.016231924295425415, acc: 100.0, f1: 100.0, r: 0.819778698634806
06/01/2019 11:09:13 step: 6516, epoch: 197, batch: 14, loss: 0.021310703828930855, acc: 100.0, f1: 100.0, r: 0.8313414599128041
06/01/2019 11:09:14 step: 6521, epoch: 197, batch: 19, loss: 0.02522978000342846, acc: 98.4375, f1: 96.82539682539682, r: 0.7461188420983668
06/01/2019 11:09:15 step: 6526, epoch: 197, batch: 24, loss: 0.022710686549544334, acc: 100.0, f1: 100.0, r: 0.767860267614191
06/01/2019 11:09:17 step: 6531, epoch: 197, batch: 29, loss: 0.02202126570045948, acc: 100.0, f1: 100.0, r: 0.6869576711993999
06/01/2019 11:09:17 *** evaluating ***
06/01/2019 11:09:17 step: 198, epoch: 197, acc: 57.692307692307686, f1: 31.314289045115558, r: 0.26381863156226837
06/01/2019 11:09:17 *** epoch: 199 ***
06/01/2019 11:09:17 *** training ***
06/01/2019 11:09:18 step: 6539, epoch: 198, batch: 4, loss: 0.004821019247174263, acc: 100.0, f1: 100.0, r: 0.7282648280992485
06/01/2019 11:09:19 step: 6544, epoch: 198, batch: 9, loss: 0.0438372828066349, acc: 100.0, f1: 100.0, r: 0.8030323099807404
06/01/2019 11:09:21 step: 6549, epoch: 198, batch: 14, loss: 0.3077290654182434, acc: 100.0, f1: 100.0, r: 0.8402323902236464
06/01/2019 11:09:22 step: 6554, epoch: 198, batch: 19, loss: 0.013544073328375816, acc: 100.0, f1: 100.0, r: 0.7884961377342348
06/01/2019 11:09:23 step: 6559, epoch: 198, batch: 24, loss: 0.014693129807710648, acc: 100.0, f1: 100.0, r: 0.7565469273010151
06/01/2019 11:09:24 step: 6564, epoch: 198, batch: 29, loss: 0.008011966943740845, acc: 100.0, f1: 100.0, r: 0.778625720073649
06/01/2019 11:09:25 *** evaluating ***
06/01/2019 11:09:25 step: 199, epoch: 198, acc: 56.837606837606835, f1: 30.43127478109846, r: 0.2661810387611479
06/01/2019 11:09:25 *** epoch: 200 ***
06/01/2019 11:09:25 *** training ***
06/01/2019 11:09:26 step: 6572, epoch: 199, batch: 4, loss: 0.5988795757293701, acc: 100.0, f1: 100.0, r: 0.7306948929293953
06/01/2019 11:09:27 step: 6577, epoch: 199, batch: 9, loss: 0.0157141275703907, acc: 100.0, f1: 100.0, r: 0.758904752301238
06/01/2019 11:09:29 step: 6582, epoch: 199, batch: 14, loss: 0.0473535880446434, acc: 100.0, f1: 100.0, r: 0.8114770133272674
06/01/2019 11:09:30 step: 6587, epoch: 199, batch: 19, loss: 0.03738792985677719, acc: 98.4375, f1: 96.4625850340136, r: 0.6801140133318337
06/01/2019 11:09:31 step: 6592, epoch: 199, batch: 24, loss: 0.018396664410829544, acc: 100.0, f1: 100.0, r: 0.7791193991090695
06/01/2019 11:09:32 step: 6597, epoch: 199, batch: 29, loss: 0.016709795221686363, acc: 100.0, f1: 100.0, r: 0.6146439904214923
06/01/2019 11:09:32 *** evaluating ***
06/01/2019 11:09:33 step: 200, epoch: 199, acc: 56.837606837606835, f1: 31.96400042750437, r: 0.260987446303343
06/01/2019 11:09:33 *** epoch: 201 ***
06/01/2019 11:09:33 *** training ***
06/01/2019 11:09:34 step: 6605, epoch: 200, batch: 4, loss: 0.05550113320350647, acc: 100.0, f1: 100.0, r: 0.8355386910183048
06/01/2019 11:09:35 step: 6610, epoch: 200, batch: 9, loss: 0.018653780221939087, acc: 100.0, f1: 100.0, r: 0.7202118153494764
06/01/2019 11:09:36 step: 6615, epoch: 200, batch: 14, loss: 0.01817615143954754, acc: 100.0, f1: 100.0, r: 0.7107059133042672
06/01/2019 11:09:37 step: 6620, epoch: 200, batch: 19, loss: 0.02682923898100853, acc: 100.0, f1: 100.0, r: 0.7728464235338662
06/01/2019 11:09:38 step: 6625, epoch: 200, batch: 24, loss: 0.015066294930875301, acc: 100.0, f1: 100.0, r: 0.6901449294187686
06/01/2019 11:09:39 step: 6630, epoch: 200, batch: 29, loss: 0.5641210675239563, acc: 100.0, f1: 100.0, r: 0.7155712143383474
06/01/2019 11:09:40 *** evaluating ***
06/01/2019 11:09:40 step: 201, epoch: 200, acc: 57.26495726495726, f1: 31.329344306912592, r: 0.2660567947762468
06/01/2019 11:09:40 *** epoch: 202 ***
06/01/2019 11:09:40 *** training ***
06/01/2019 11:09:42 step: 6638, epoch: 201, batch: 4, loss: 0.007999615743756294, acc: 100.0, f1: 100.0, r: 0.8476250713694062
06/01/2019 11:09:43 step: 6643, epoch: 201, batch: 9, loss: 0.011661043390631676, acc: 100.0, f1: 100.0, r: 0.7761659477745495
06/01/2019 11:09:44 step: 6648, epoch: 201, batch: 14, loss: 0.01064865943044424, acc: 100.0, f1: 100.0, r: 0.7076653355728135
06/01/2019 11:09:45 step: 6653, epoch: 201, batch: 19, loss: 0.009143436327576637, acc: 100.0, f1: 100.0, r: 0.7537677630483037
06/01/2019 11:09:46 step: 6658, epoch: 201, batch: 24, loss: 0.06023937091231346, acc: 100.0, f1: 100.0, r: 0.7460672160237322
06/01/2019 11:09:47 step: 6663, epoch: 201, batch: 29, loss: 0.030291561037302017, acc: 100.0, f1: 100.0, r: 0.6618040032371854
06/01/2019 11:09:48 *** evaluating ***
06/01/2019 11:09:48 step: 202, epoch: 201, acc: 58.54700854700855, f1: 31.772302564497423, r: 0.26568078667507
06/01/2019 11:09:48 *** epoch: 203 ***
06/01/2019 11:09:48 *** training ***
06/01/2019 11:09:49 step: 6671, epoch: 202, batch: 4, loss: 0.018435398116707802, acc: 100.0, f1: 100.0, r: 0.8286215143817628
06/01/2019 11:09:51 step: 6676, epoch: 202, batch: 9, loss: 0.007565605454146862, acc: 100.0, f1: 100.0, r: 0.6989514250422113
06/01/2019 11:09:52 step: 6681, epoch: 202, batch: 14, loss: 0.009399183094501495, acc: 100.0, f1: 100.0, r: 0.6787641454495653
06/01/2019 11:09:53 step: 6686, epoch: 202, batch: 19, loss: 0.02549559995532036, acc: 100.0, f1: 100.0, r: 0.8062632427504307
06/01/2019 11:09:54 step: 6691, epoch: 202, batch: 24, loss: 0.3062556982040405, acc: 100.0, f1: 100.0, r: 0.7207825675138376
06/01/2019 11:09:55 step: 6696, epoch: 202, batch: 29, loss: 0.04085106775164604, acc: 98.4375, f1: 97.95186891961086, r: 0.6843320209648343
06/01/2019 11:09:56 *** evaluating ***
06/01/2019 11:09:56 step: 203, epoch: 202, acc: 55.98290598290598, f1: 30.678092530238803, r: 0.2642416416602923
06/01/2019 11:09:56 *** epoch: 204 ***
06/01/2019 11:09:56 *** training ***
06/01/2019 11:09:57 step: 6704, epoch: 203, batch: 4, loss: 0.029902180656790733, acc: 98.4375, f1: 98.02102659245516, r: 0.6524934419182667
06/01/2019 11:09:58 step: 6709, epoch: 203, batch: 9, loss: 0.0227799154818058, acc: 100.0, f1: 100.0, r: 0.7344118238017551
06/01/2019 11:09:59 step: 6714, epoch: 203, batch: 14, loss: 0.012202493846416473, acc: 100.0, f1: 100.0, r: 0.815646957275802
06/01/2019 11:10:00 step: 6719, epoch: 203, batch: 19, loss: 0.04502149671316147, acc: 100.0, f1: 100.0, r: 0.8517117696681316
06/01/2019 11:10:01 step: 6724, epoch: 203, batch: 24, loss: 0.049904730170965195, acc: 98.4375, f1: 98.57549857549857, r: 0.7881315431747806
06/01/2019 11:10:03 step: 6729, epoch: 203, batch: 29, loss: 0.014448141679167747, acc: 100.0, f1: 100.0, r: 0.6697297793057835
06/01/2019 11:10:03 *** evaluating ***
06/01/2019 11:10:04 step: 204, epoch: 203, acc: 55.55555555555556, f1: 29.80429630045999, r: 0.2641208208508431
06/01/2019 11:10:04 *** epoch: 205 ***
06/01/2019 11:10:04 *** training ***
06/01/2019 11:10:05 step: 6737, epoch: 204, batch: 4, loss: 0.011871138587594032, acc: 100.0, f1: 100.0, r: 0.699498245406704
06/01/2019 11:10:06 step: 6742, epoch: 204, batch: 9, loss: 0.5993783473968506, acc: 100.0, f1: 100.0, r: 0.610845701721154
06/01/2019 11:10:07 step: 6747, epoch: 204, batch: 14, loss: 0.008376909419894218, acc: 100.0, f1: 100.0, r: 0.7023984015084629
06/01/2019 11:10:08 step: 6752, epoch: 204, batch: 19, loss: 0.03321678191423416, acc: 100.0, f1: 100.0, r: 0.8087589832670989
06/01/2019 11:10:09 step: 6757, epoch: 204, batch: 24, loss: 0.013394802808761597, acc: 100.0, f1: 100.0, r: 0.83198210583223
06/01/2019 11:10:10 step: 6762, epoch: 204, batch: 29, loss: 0.01139735709875822, acc: 100.0, f1: 100.0, r: 0.8121121434482426
06/01/2019 11:10:11 *** evaluating ***
06/01/2019 11:10:11 step: 205, epoch: 204, acc: 57.26495726495726, f1: 28.085181328105858, r: 0.27002273876277944
06/01/2019 11:10:11 *** epoch: 206 ***
06/01/2019 11:10:11 *** training ***
06/01/2019 11:10:12 step: 6770, epoch: 205, batch: 4, loss: 0.010217389091849327, acc: 100.0, f1: 100.0, r: 0.8551369260863751
06/01/2019 11:10:13 step: 6775, epoch: 205, batch: 9, loss: 0.01824088580906391, acc: 100.0, f1: 100.0, r: 0.716950413184108
06/01/2019 11:10:14 step: 6780, epoch: 205, batch: 14, loss: 0.022739160805940628, acc: 100.0, f1: 100.0, r: 0.7816500260546447
06/01/2019 11:10:15 step: 6785, epoch: 205, batch: 19, loss: 0.007054025307297707, acc: 100.0, f1: 100.0, r: 0.6738579644564091
06/01/2019 11:10:17 step: 6790, epoch: 205, batch: 24, loss: 0.012722786515951157, acc: 100.0, f1: 100.0, r: 0.828504757746303
06/01/2019 11:10:18 step: 6795, epoch: 205, batch: 29, loss: 0.020226987078785896, acc: 100.0, f1: 100.0, r: 0.7738066205345377
06/01/2019 11:10:18 *** evaluating ***
06/01/2019 11:10:19 step: 206, epoch: 205, acc: 57.692307692307686, f1: 30.643989219002997, r: 0.26755853307389105
06/01/2019 11:10:19 *** epoch: 207 ***
06/01/2019 11:10:19 *** training ***
06/01/2019 11:10:20 step: 6803, epoch: 206, batch: 4, loss: 0.009956898167729378, acc: 100.0, f1: 100.0, r: 0.7059907244036464
06/01/2019 11:10:21 step: 6808, epoch: 206, batch: 9, loss: 0.010495766066014767, acc: 100.0, f1: 100.0, r: 0.6575147038813557
06/01/2019 11:10:22 step: 6813, epoch: 206, batch: 14, loss: 0.018569130450487137, acc: 100.0, f1: 100.0, r: 0.7198986819989613
06/01/2019 11:10:23 step: 6818, epoch: 206, batch: 19, loss: 0.03742792457342148, acc: 100.0, f1: 100.0, r: 0.812981652909161
06/01/2019 11:10:24 step: 6823, epoch: 206, batch: 24, loss: 0.010996945202350616, acc: 100.0, f1: 100.0, r: 0.8203284398250033
06/01/2019 11:10:25 step: 6828, epoch: 206, batch: 29, loss: 0.033060915768146515, acc: 100.0, f1: 100.0, r: 0.8002570794937773
06/01/2019 11:10:26 *** evaluating ***
06/01/2019 11:10:26 step: 207, epoch: 206, acc: 58.119658119658126, f1: 30.587497400877684, r: 0.26526050542997603
06/01/2019 11:10:26 *** epoch: 208 ***
06/01/2019 11:10:26 *** training ***
06/01/2019 11:10:27 step: 6836, epoch: 207, batch: 4, loss: 0.014766904525458813, acc: 100.0, f1: 100.0, r: 0.8573439507222923
06/01/2019 11:10:28 step: 6841, epoch: 207, batch: 9, loss: 0.010245945304632187, acc: 100.0, f1: 100.0, r: 0.779645754231689
06/01/2019 11:10:30 step: 6846, epoch: 207, batch: 14, loss: 0.03931143134832382, acc: 100.0, f1: 100.0, r: 0.7082888858607536
06/01/2019 11:10:30 step: 6851, epoch: 207, batch: 19, loss: 0.014159398153424263, acc: 100.0, f1: 100.0, r: 0.8281484458691444
06/01/2019 11:10:32 step: 6856, epoch: 207, batch: 24, loss: 0.010350081138312817, acc: 100.0, f1: 100.0, r: 0.7380882881763361
06/01/2019 11:10:33 step: 6861, epoch: 207, batch: 29, loss: 0.04540368169546127, acc: 100.0, f1: 100.0, r: 0.821689075635198
06/01/2019 11:10:33 *** evaluating ***
06/01/2019 11:10:34 step: 208, epoch: 207, acc: 55.55555555555556, f1: 29.913263571558367, r: 0.26624096214493087
06/01/2019 11:10:34 *** epoch: 209 ***
06/01/2019 11:10:34 *** training ***
06/01/2019 11:10:35 step: 6869, epoch: 208, batch: 4, loss: 0.014751331880688667, acc: 100.0, f1: 100.0, r: 0.8052588280263328
06/01/2019 11:10:36 step: 6874, epoch: 208, batch: 9, loss: 0.03315315395593643, acc: 100.0, f1: 100.0, r: 0.7606582611048804
06/01/2019 11:10:37 step: 6879, epoch: 208, batch: 14, loss: 0.01031416840851307, acc: 100.0, f1: 100.0, r: 0.7965719448421932
06/01/2019 11:10:38 step: 6884, epoch: 208, batch: 19, loss: 0.02609965205192566, acc: 100.0, f1: 100.0, r: 0.7166716807598897
06/01/2019 11:10:39 step: 6889, epoch: 208, batch: 24, loss: 0.0225609689950943, acc: 100.0, f1: 100.0, r: 0.6994269044513695
06/01/2019 11:10:40 step: 6894, epoch: 208, batch: 29, loss: 0.6010127663612366, acc: 100.0, f1: 100.0, r: 0.783246734413797
06/01/2019 11:10:41 *** evaluating ***
06/01/2019 11:10:41 step: 209, epoch: 208, acc: 57.692307692307686, f1: 32.369923878334355, r: 0.263151093368872
06/01/2019 11:10:41 *** epoch: 210 ***
06/01/2019 11:10:41 *** training ***
06/01/2019 11:10:42 step: 6902, epoch: 209, batch: 4, loss: 0.029464714229106903, acc: 100.0, f1: 100.0, r: 0.7800400082206385
06/01/2019 11:10:43 step: 6907, epoch: 209, batch: 9, loss: 0.010392722673714161, acc: 100.0, f1: 100.0, r: 0.7532845744496276
06/01/2019 11:10:44 step: 6912, epoch: 209, batch: 14, loss: 0.027811553329229355, acc: 100.0, f1: 100.0, r: 0.7159299384983171
06/01/2019 11:10:45 step: 6917, epoch: 209, batch: 19, loss: 0.5398073196411133, acc: 100.0, f1: 100.0, r: 0.8153472403591073
06/01/2019 11:10:47 step: 6922, epoch: 209, batch: 24, loss: 0.5961657762527466, acc: 100.0, f1: 100.0, r: 0.6886128356826445
06/01/2019 11:10:48 step: 6927, epoch: 209, batch: 29, loss: 0.03343204781413078, acc: 98.4375, f1: 94.39775910364145, r: 0.7209184793422307
06/01/2019 11:10:48 *** evaluating ***
06/01/2019 11:10:49 step: 210, epoch: 209, acc: 54.700854700854705, f1: 30.15911970351223, r: 0.2652980246601116
06/01/2019 11:10:49 *** epoch: 211 ***
06/01/2019 11:10:49 *** training ***
06/01/2019 11:10:50 step: 6935, epoch: 210, batch: 4, loss: 0.009691781364381313, acc: 100.0, f1: 100.0, r: 0.8335024959276572
06/01/2019 11:10:51 step: 6940, epoch: 210, batch: 9, loss: 0.014739749021828175, acc: 100.0, f1: 100.0, r: 0.7470504484753799
06/01/2019 11:10:52 step: 6945, epoch: 210, batch: 14, loss: 0.012552271597087383, acc: 100.0, f1: 100.0, r: 0.7513029030135342
06/01/2019 11:10:53 step: 6950, epoch: 210, batch: 19, loss: 0.0065845586359500885, acc: 100.0, f1: 100.0, r: 0.7419302366289653
06/01/2019 11:10:54 step: 6955, epoch: 210, batch: 24, loss: 0.03936854377388954, acc: 100.0, f1: 100.0, r: 0.8283236552611839
06/01/2019 11:10:55 step: 6960, epoch: 210, batch: 29, loss: 0.006456711795181036, acc: 100.0, f1: 100.0, r: 0.7870243431532526
06/01/2019 11:10:56 *** evaluating ***
06/01/2019 11:10:56 step: 211, epoch: 210, acc: 57.692307692307686, f1: 30.312161781193904, r: 0.2680927718634938
06/01/2019 11:10:56 *** epoch: 212 ***
06/01/2019 11:10:56 *** training ***
06/01/2019 11:10:57 step: 6968, epoch: 211, batch: 4, loss: 0.011866283603012562, acc: 100.0, f1: 100.0, r: 0.6614239592992768
06/01/2019 11:10:58 step: 6973, epoch: 211, batch: 9, loss: 0.01145410817116499, acc: 100.0, f1: 100.0, r: 0.777439841465296
06/01/2019 11:10:59 step: 6978, epoch: 211, batch: 14, loss: 0.017658429220318794, acc: 100.0, f1: 100.0, r: 0.779839278871721
06/01/2019 11:11:00 step: 6983, epoch: 211, batch: 19, loss: 0.03357455134391785, acc: 98.4375, f1: 98.44322344322345, r: 0.7841315086828436
06/01/2019 11:11:02 step: 6988, epoch: 211, batch: 24, loss: 0.01653471030294895, acc: 100.0, f1: 100.0, r: 0.7227838250904898
06/01/2019 11:11:03 step: 6993, epoch: 211, batch: 29, loss: 0.020687241107225418, acc: 100.0, f1: 100.0, r: 0.6776233452533379
06/01/2019 11:11:04 *** evaluating ***
06/01/2019 11:11:04 step: 212, epoch: 211, acc: 58.119658119658126, f1: 30.418401429184012, r: 0.26875102355152625
06/01/2019 11:11:04 *** epoch: 213 ***
06/01/2019 11:11:04 *** training ***
06/01/2019 11:11:05 step: 7001, epoch: 212, batch: 4, loss: 0.006920573301613331, acc: 100.0, f1: 100.0, r: 0.7984919525686718
06/01/2019 11:11:06 step: 7006, epoch: 212, batch: 9, loss: 0.023506224155426025, acc: 100.0, f1: 100.0, r: 0.7576342300891599
06/01/2019 11:11:07 step: 7011, epoch: 212, batch: 14, loss: 0.5897959470748901, acc: 100.0, f1: 100.0, r: 0.7012054380796108
06/01/2019 11:11:08 step: 7016, epoch: 212, batch: 19, loss: 0.006676604971289635, acc: 100.0, f1: 100.0, r: 0.7498221023038312
06/01/2019 11:11:09 step: 7021, epoch: 212, batch: 24, loss: 0.008908625692129135, acc: 100.0, f1: 100.0, r: 0.694832690600241
06/01/2019 11:11:11 step: 7026, epoch: 212, batch: 29, loss: 0.055278558284044266, acc: 96.875, f1: 94.45684523809523, r: 0.7600566855979463
06/01/2019 11:11:11 *** evaluating ***
06/01/2019 11:11:12 step: 213, epoch: 212, acc: 58.119658119658126, f1: 29.282967032967033, r: 0.26280065555478604
06/01/2019 11:11:12 *** epoch: 214 ***
06/01/2019 11:11:12 *** training ***
06/01/2019 11:11:13 step: 7034, epoch: 213, batch: 4, loss: 0.05185119807720184, acc: 98.4375, f1: 99.38490978676873, r: 0.7178277253147728
06/01/2019 11:11:14 step: 7039, epoch: 213, batch: 9, loss: 0.008906181901693344, acc: 100.0, f1: 100.0, r: 0.7220376758040541
06/01/2019 11:11:15 step: 7044, epoch: 213, batch: 14, loss: 0.016709839925169945, acc: 100.0, f1: 100.0, r: 0.7628058632058023
06/01/2019 11:11:16 step: 7049, epoch: 213, batch: 19, loss: 0.0428687259554863, acc: 98.4375, f1: 97.47474747474747, r: 0.8211569029689716
06/01/2019 11:11:18 step: 7054, epoch: 213, batch: 24, loss: 0.008125165477395058, acc: 100.0, f1: 100.0, r: 0.811789167223105
06/01/2019 11:11:19 step: 7059, epoch: 213, batch: 29, loss: 0.04816577583551407, acc: 98.4375, f1: 97.20730397422128, r: 0.6911805897112786
06/01/2019 11:11:19 *** evaluating ***
06/01/2019 11:11:20 step: 214, epoch: 213, acc: 58.97435897435898, f1: 29.469558612525336, r: 0.2687160853112676
06/01/2019 11:11:20 *** epoch: 215 ***
06/01/2019 11:11:20 *** training ***
06/01/2019 11:11:21 step: 7067, epoch: 214, batch: 4, loss: 0.01591699942946434, acc: 100.0, f1: 100.0, r: 0.7012686773078242
06/01/2019 11:11:22 step: 7072, epoch: 214, batch: 9, loss: 0.013657097704708576, acc: 100.0, f1: 100.0, r: 0.6703068215205612
06/01/2019 11:11:23 step: 7077, epoch: 214, batch: 14, loss: 0.5548175573348999, acc: 98.4375, f1: 98.10874704491727, r: 0.6699537987331554
06/01/2019 11:11:24 step: 7082, epoch: 214, batch: 19, loss: 0.8848455548286438, acc: 100.0, f1: 100.0, r: 0.6891500092646947
06/01/2019 11:11:25 step: 7087, epoch: 214, batch: 24, loss: 0.03199443221092224, acc: 98.4375, f1: 97.47899159663865, r: 0.7846990223064848
06/01/2019 11:11:26 step: 7092, epoch: 214, batch: 29, loss: 0.007047904189676046, acc: 100.0, f1: 100.0, r: 0.8271296702168904
06/01/2019 11:11:27 *** evaluating ***
06/01/2019 11:11:27 step: 215, epoch: 214, acc: 57.26495726495726, f1: 31.063632860981947, r: 0.2617591763069422
06/01/2019 11:11:27 *** epoch: 216 ***
06/01/2019 11:11:27 *** training ***
06/01/2019 11:11:28 step: 7100, epoch: 215, batch: 4, loss: 0.015890400856733322, acc: 100.0, f1: 100.0, r: 0.7777945979484053
06/01/2019 11:11:29 step: 7105, epoch: 215, batch: 9, loss: 0.014761948958039284, acc: 100.0, f1: 100.0, r: 0.6857004730052009
06/01/2019 11:11:30 step: 7110, epoch: 215, batch: 14, loss: 0.029564689844846725, acc: 100.0, f1: 100.0, r: 0.6319889570244606
06/01/2019 11:11:32 step: 7115, epoch: 215, batch: 19, loss: 0.013874486088752747, acc: 100.0, f1: 100.0, r: 0.6982111932000555
06/01/2019 11:11:33 step: 7120, epoch: 215, batch: 24, loss: 0.014464616775512695, acc: 100.0, f1: 100.0, r: 0.743020396469418
06/01/2019 11:11:34 step: 7125, epoch: 215, batch: 29, loss: 0.022471677511930466, acc: 100.0, f1: 100.0, r: 0.8138451443395047
06/01/2019 11:11:34 *** evaluating ***
06/01/2019 11:11:35 step: 216, epoch: 215, acc: 58.119658119658126, f1: 32.87612189397904, r: 0.26437239910945726
06/01/2019 11:11:35 *** epoch: 217 ***
06/01/2019 11:11:35 *** training ***
06/01/2019 11:11:36 step: 7133, epoch: 216, batch: 4, loss: 0.019290847703814507, acc: 100.0, f1: 100.0, r: 0.6781900526472007
06/01/2019 11:11:37 step: 7138, epoch: 216, batch: 9, loss: 0.00933942198753357, acc: 100.0, f1: 100.0, r: 0.7079501263541345
06/01/2019 11:11:38 step: 7143, epoch: 216, batch: 14, loss: 0.029752811416983604, acc: 100.0, f1: 100.0, r: 0.7117634442429069
06/01/2019 11:11:39 step: 7148, epoch: 216, batch: 19, loss: 0.6257961988449097, acc: 98.4375, f1: 96.39097744360903, r: 0.5993848285985436
06/01/2019 11:11:40 step: 7153, epoch: 216, batch: 24, loss: 0.03611030802130699, acc: 100.0, f1: 100.0, r: 0.834217973441047
06/01/2019 11:11:42 step: 7158, epoch: 216, batch: 29, loss: 0.02460572123527527, acc: 98.4375, f1: 99.0599876314162, r: 0.6537377029418744
06/01/2019 11:11:42 *** evaluating ***
06/01/2019 11:11:42 step: 217, epoch: 216, acc: 58.119658119658126, f1: 31.069099455021785, r: 0.2639914148964352
06/01/2019 11:11:42 *** epoch: 218 ***
06/01/2019 11:11:42 *** training ***
06/01/2019 11:11:43 step: 7166, epoch: 217, batch: 4, loss: 0.04611646384000778, acc: 98.4375, f1: 98.55500821018063, r: 0.6314451158231514
06/01/2019 11:11:45 step: 7171, epoch: 217, batch: 9, loss: 0.019180919975042343, acc: 100.0, f1: 100.0, r: 0.6724536504921612
06/01/2019 11:11:46 step: 7176, epoch: 217, batch: 14, loss: 0.3058284819126129, acc: 100.0, f1: 100.0, r: 0.7365251082635673
06/01/2019 11:11:47 step: 7181, epoch: 217, batch: 19, loss: 0.01458750944584608, acc: 100.0, f1: 100.0, r: 0.8005941742432442
06/01/2019 11:11:48 step: 7186, epoch: 217, batch: 24, loss: 0.010156748816370964, acc: 100.0, f1: 100.0, r: 0.7947727922465235
06/01/2019 11:11:49 step: 7191, epoch: 217, batch: 29, loss: 0.009916187264025211, acc: 100.0, f1: 100.0, r: 0.6588647281522781
06/01/2019 11:11:50 *** evaluating ***
06/01/2019 11:11:50 step: 218, epoch: 217, acc: 55.98290598290598, f1: 31.92029611737911, r: 0.25905252220616554
06/01/2019 11:11:50 *** epoch: 219 ***
06/01/2019 11:11:50 *** training ***
06/01/2019 11:11:51 step: 7199, epoch: 218, batch: 4, loss: 0.026885325089097023, acc: 98.4375, f1: 98.44322344322343, r: 0.7718151168151673
06/01/2019 11:11:52 step: 7204, epoch: 218, batch: 9, loss: 0.020755890756845474, acc: 100.0, f1: 100.0, r: 0.8388170223540804
06/01/2019 11:11:54 step: 7209, epoch: 218, batch: 14, loss: 0.012853989377617836, acc: 100.0, f1: 100.0, r: 0.8171585201626511
06/01/2019 11:11:55 step: 7214, epoch: 218, batch: 19, loss: 0.016260545700788498, acc: 100.0, f1: 100.0, r: 0.781593739403301
06/01/2019 11:11:56 step: 7219, epoch: 218, batch: 24, loss: 0.013017053715884686, acc: 100.0, f1: 100.0, r: 0.7098426697618317
06/01/2019 11:11:57 step: 7224, epoch: 218, batch: 29, loss: 0.009100669994950294, acc: 100.0, f1: 100.0, r: 0.8303545840697601
06/01/2019 11:11:58 *** evaluating ***
06/01/2019 11:11:58 step: 219, epoch: 218, acc: 56.41025641025641, f1: 27.017751694977033, r: 0.2652476565812415
06/01/2019 11:11:58 *** epoch: 220 ***
06/01/2019 11:11:58 *** training ***
06/01/2019 11:11:59 step: 7232, epoch: 219, batch: 4, loss: 0.0638897716999054, acc: 98.4375, f1: 98.4006734006734, r: 0.8007375969363393
06/01/2019 11:12:00 step: 7237, epoch: 219, batch: 9, loss: 0.011148113757371902, acc: 100.0, f1: 100.0, r: 0.6902065683766833
06/01/2019 11:12:01 step: 7242, epoch: 219, batch: 14, loss: 0.018996162340044975, acc: 100.0, f1: 100.0, r: 0.699917049665426
06/01/2019 11:12:02 step: 7247, epoch: 219, batch: 19, loss: 0.009247880429029465, acc: 100.0, f1: 100.0, r: 0.7570258855616212
06/01/2019 11:12:04 step: 7252, epoch: 219, batch: 24, loss: 0.012479868717491627, acc: 100.0, f1: 100.0, r: 0.7067942343625931
06/01/2019 11:12:05 step: 7257, epoch: 219, batch: 29, loss: 0.016057398170232773, acc: 100.0, f1: 100.0, r: 0.8360690604857862
06/01/2019 11:12:05 *** evaluating ***
06/01/2019 11:12:05 step: 220, epoch: 219, acc: 58.119658119658126, f1: 32.20141644005523, r: 0.2669147461757442
06/01/2019 11:12:05 *** epoch: 221 ***
06/01/2019 11:12:05 *** training ***
06/01/2019 11:12:06 step: 7265, epoch: 220, batch: 4, loss: 0.01632053032517433, acc: 100.0, f1: 100.0, r: 0.7144909089356781
06/01/2019 11:12:08 step: 7270, epoch: 220, batch: 9, loss: 0.017569979652762413, acc: 100.0, f1: 100.0, r: 0.7478115326072332
06/01/2019 11:12:09 step: 7275, epoch: 220, batch: 14, loss: 0.008596821688115597, acc: 100.0, f1: 100.0, r: 0.6782013971566467
06/01/2019 11:12:10 step: 7280, epoch: 220, batch: 19, loss: 0.009611236862838268, acc: 100.0, f1: 100.0, r: 0.6530179443658001
06/01/2019 11:12:11 step: 7285, epoch: 220, batch: 24, loss: 0.008760886266827583, acc: 100.0, f1: 100.0, r: 0.7969711050578727
06/01/2019 11:12:12 step: 7290, epoch: 220, batch: 29, loss: 0.01551915518939495, acc: 100.0, f1: 100.0, r: 0.6107737624432286
06/01/2019 11:12:13 *** evaluating ***
06/01/2019 11:12:13 step: 221, epoch: 220, acc: 58.119658119658126, f1: 32.22688631794776, r: 0.26780388765437535
06/01/2019 11:12:13 *** epoch: 222 ***
06/01/2019 11:12:13 *** training ***
06/01/2019 11:12:14 step: 7298, epoch: 221, batch: 4, loss: 0.019167743623256683, acc: 100.0, f1: 100.0, r: 0.7775092860381307
06/01/2019 11:12:15 step: 7303, epoch: 221, batch: 9, loss: 0.5995364189147949, acc: 100.0, f1: 100.0, r: 0.5917521574661379
06/01/2019 11:12:17 step: 7308, epoch: 221, batch: 14, loss: 0.004374776966869831, acc: 100.0, f1: 100.0, r: 0.7774212052237887
06/01/2019 11:12:18 step: 7313, epoch: 221, batch: 19, loss: 0.019492579624056816, acc: 100.0, f1: 100.0, r: 0.7231190809762104
06/01/2019 11:12:19 step: 7318, epoch: 221, batch: 24, loss: 0.019746337085962296, acc: 100.0, f1: 100.0, r: 0.6332818509138498
06/01/2019 11:12:20 step: 7323, epoch: 221, batch: 29, loss: 0.03003457374870777, acc: 100.0, f1: 100.0, r: 0.715818574037636
06/01/2019 11:12:20 *** evaluating ***
06/01/2019 11:12:21 step: 222, epoch: 221, acc: 59.82905982905983, f1: 31.73044645413066, r: 0.271073678929341
06/01/2019 11:12:21 *** epoch: 223 ***
06/01/2019 11:12:21 *** training ***
06/01/2019 11:12:22 step: 7331, epoch: 222, batch: 4, loss: 0.5482648015022278, acc: 100.0, f1: 100.0, r: 0.6904445065453205
06/01/2019 11:12:23 step: 7336, epoch: 222, batch: 9, loss: 0.06513865292072296, acc: 98.4375, f1: 92.38095238095238, r: 0.6521908924033932
06/01/2019 11:12:24 step: 7341, epoch: 222, batch: 14, loss: 0.029267651960253716, acc: 98.4375, f1: 99.11483253588517, r: 0.789132949286814
06/01/2019 11:12:25 step: 7346, epoch: 222, batch: 19, loss: 0.87608802318573, acc: 100.0, f1: 100.0, r: 0.7709652847681436
06/01/2019 11:12:26 step: 7351, epoch: 222, batch: 24, loss: 0.023165641352534294, acc: 98.4375, f1: 95.84415584415584, r: 0.6933700815259995
06/01/2019 11:12:27 step: 7356, epoch: 222, batch: 29, loss: 0.011963099241256714, acc: 100.0, f1: 100.0, r: 0.6600284857761876
06/01/2019 11:12:28 *** evaluating ***
06/01/2019 11:12:28 step: 223, epoch: 222, acc: 58.54700854700855, f1: 31.312194868759512, r: 0.26817475075029096
06/01/2019 11:12:28 *** epoch: 224 ***
06/01/2019 11:12:28 *** training ***
06/01/2019 11:12:29 step: 7364, epoch: 223, batch: 4, loss: 0.02002730779349804, acc: 100.0, f1: 100.0, r: 0.7993179232567239
06/01/2019 11:12:30 step: 7369, epoch: 223, batch: 9, loss: 0.016256485134363174, acc: 100.0, f1: 100.0, r: 0.808344246328177
06/01/2019 11:12:32 step: 7374, epoch: 223, batch: 14, loss: 0.04948161542415619, acc: 100.0, f1: 100.0, r: 0.7479017458174325
06/01/2019 11:12:33 step: 7379, epoch: 223, batch: 19, loss: 0.0064576491713523865, acc: 100.0, f1: 100.0, r: 0.77032817599917
06/01/2019 11:12:34 step: 7384, epoch: 223, batch: 24, loss: 0.019662290811538696, acc: 100.0, f1: 100.0, r: 0.7189346593474595
06/01/2019 11:12:35 step: 7389, epoch: 223, batch: 29, loss: 0.31288662552833557, acc: 100.0, f1: 100.0, r: 0.6840320420266947
06/01/2019 11:12:36 *** evaluating ***
06/01/2019 11:12:36 step: 224, epoch: 223, acc: 56.837606837606835, f1: 31.74999378989989, r: 0.26546209035767415
06/01/2019 11:12:36 *** epoch: 225 ***
06/01/2019 11:12:36 *** training ***
06/01/2019 11:12:37 step: 7397, epoch: 224, batch: 4, loss: 0.02495107240974903, acc: 100.0, f1: 100.0, r: 0.6434439423169653
06/01/2019 11:12:38 step: 7402, epoch: 224, batch: 9, loss: 0.3013502061367035, acc: 100.0, f1: 100.0, r: 0.7624300797627765
06/01/2019 11:12:39 step: 7407, epoch: 224, batch: 14, loss: 0.010583643801510334, acc: 100.0, f1: 100.0, r: 0.6767618012131482
06/01/2019 11:12:41 step: 7412, epoch: 224, batch: 19, loss: 0.03083529695868492, acc: 98.4375, f1: 97.3809523809524, r: 0.7624703764590789
06/01/2019 11:12:42 step: 7417, epoch: 224, batch: 24, loss: 0.009505627676844597, acc: 100.0, f1: 100.0, r: 0.7120407839342819
06/01/2019 11:12:43 step: 7422, epoch: 224, batch: 29, loss: 0.01380334421992302, acc: 100.0, f1: 100.0, r: 0.8173146447926801
06/01/2019 11:12:43 *** evaluating ***
06/01/2019 11:12:44 step: 225, epoch: 224, acc: 58.119658119658126, f1: 32.406067337574186, r: 0.26586513224185604
06/01/2019 11:12:44 *** epoch: 226 ***
06/01/2019 11:12:44 *** training ***
06/01/2019 11:12:45 step: 7430, epoch: 225, batch: 4, loss: 0.30902099609375, acc: 100.0, f1: 100.0, r: 0.6728061107851869
06/01/2019 11:12:46 step: 7435, epoch: 225, batch: 9, loss: 0.013696186244487762, acc: 100.0, f1: 100.0, r: 0.7670603186832325
06/01/2019 11:12:47 step: 7440, epoch: 225, batch: 14, loss: 0.012170765548944473, acc: 100.0, f1: 100.0, r: 0.7273848577459064
06/01/2019 11:12:48 step: 7445, epoch: 225, batch: 19, loss: 0.00759377796202898, acc: 100.0, f1: 100.0, r: 0.7961136490048312
06/01/2019 11:12:49 step: 7450, epoch: 225, batch: 24, loss: 0.02726837992668152, acc: 100.0, f1: 100.0, r: 0.7912159298378728
06/01/2019 11:12:50 step: 7455, epoch: 225, batch: 29, loss: 0.021009797230362892, acc: 100.0, f1: 100.0, r: 0.7163316178643286
06/01/2019 11:12:51 *** evaluating ***
06/01/2019 11:12:51 step: 226, epoch: 225, acc: 56.837606837606835, f1: 28.481766426174993, r: 0.26456322188673503
06/01/2019 11:12:51 *** epoch: 227 ***
06/01/2019 11:12:51 *** training ***
06/01/2019 11:12:52 step: 7463, epoch: 226, batch: 4, loss: 0.02694423496723175, acc: 100.0, f1: 100.0, r: 0.7624890588514787
06/01/2019 11:12:54 step: 7468, epoch: 226, batch: 9, loss: 0.018034929409623146, acc: 100.0, f1: 100.0, r: 0.7040957787851014
06/01/2019 11:12:55 step: 7473, epoch: 226, batch: 14, loss: 0.07227709889411926, acc: 96.875, f1: 92.79448621553885, r: 0.7540085957056661
06/01/2019 11:12:56 step: 7478, epoch: 226, batch: 19, loss: 0.011383824050426483, acc: 100.0, f1: 100.0, r: 0.7021264297561239
06/01/2019 11:12:57 step: 7483, epoch: 226, batch: 24, loss: 0.019636670127511024, acc: 98.4375, f1: 95.10204081632652, r: 0.7119612950715769
06/01/2019 11:12:58 step: 7488, epoch: 226, batch: 29, loss: 0.02205357700586319, acc: 100.0, f1: 100.0, r: 0.7636674872085655
06/01/2019 11:12:59 *** evaluating ***
06/01/2019 11:12:59 step: 227, epoch: 226, acc: 57.26495726495726, f1: 31.848021026592455, r: 0.2711507239255372
06/01/2019 11:12:59 *** epoch: 228 ***
06/01/2019 11:12:59 *** training ***
06/01/2019 11:13:00 step: 7496, epoch: 227, batch: 4, loss: 0.02077215164899826, acc: 100.0, f1: 100.0, r: 0.8029111396403846
06/01/2019 11:13:01 step: 7501, epoch: 227, batch: 9, loss: 0.008748716674745083, acc: 100.0, f1: 100.0, r: 0.8351959318314272
06/01/2019 11:13:03 step: 7506, epoch: 227, batch: 14, loss: 0.010097917169332504, acc: 100.0, f1: 100.0, r: 0.7772863675573382
06/01/2019 11:13:04 step: 7511, epoch: 227, batch: 19, loss: 0.024964865297079086, acc: 100.0, f1: 100.0, r: 0.8268258591457827
06/01/2019 11:13:05 step: 7516, epoch: 227, batch: 24, loss: 0.01707826741039753, acc: 100.0, f1: 100.0, r: 0.6520792547918839
06/01/2019 11:13:06 step: 7521, epoch: 227, batch: 29, loss: 0.02144889533519745, acc: 100.0, f1: 100.0, r: 0.8051552843094866
06/01/2019 11:13:06 *** evaluating ***
06/01/2019 11:13:07 step: 228, epoch: 227, acc: 54.700854700854705, f1: 29.86269701849412, r: 0.2614158519502176
06/01/2019 11:13:07 *** epoch: 229 ***
06/01/2019 11:13:07 *** training ***
06/01/2019 11:13:08 step: 7529, epoch: 228, batch: 4, loss: 0.02661493606865406, acc: 100.0, f1: 100.0, r: 0.7587241905561353
06/01/2019 11:13:09 step: 7534, epoch: 228, batch: 9, loss: 0.0049060373567044735, acc: 100.0, f1: 100.0, r: 0.6614965199609832
06/01/2019 11:13:10 step: 7539, epoch: 228, batch: 14, loss: 0.0191891398280859, acc: 100.0, f1: 100.0, r: 0.6832864875225962
06/01/2019 11:13:11 step: 7544, epoch: 228, batch: 19, loss: 0.02048606239259243, acc: 100.0, f1: 100.0, r: 0.7614953901509652
06/01/2019 11:13:12 step: 7549, epoch: 228, batch: 24, loss: 0.007211790420114994, acc: 100.0, f1: 100.0, r: 0.6527490906515253
06/01/2019 11:13:13 step: 7554, epoch: 228, batch: 29, loss: 0.017846671864390373, acc: 100.0, f1: 100.0, r: 0.743313852334548
06/01/2019 11:13:14 *** evaluating ***
06/01/2019 11:13:14 step: 229, epoch: 228, acc: 59.401709401709404, f1: 33.554956295016794, r: 0.2676025183001831
06/01/2019 11:13:14 *** epoch: 230 ***
06/01/2019 11:13:14 *** training ***
06/01/2019 11:13:15 step: 7562, epoch: 229, batch: 4, loss: 0.020159581676125526, acc: 98.4375, f1: 97.8937728937729, r: 0.7482960474636803
06/01/2019 11:13:16 step: 7567, epoch: 229, batch: 9, loss: 0.012745924293994904, acc: 100.0, f1: 100.0, r: 0.7190624729678862
06/01/2019 11:13:17 step: 7572, epoch: 229, batch: 14, loss: 0.020126352086663246, acc: 100.0, f1: 100.0, r: 0.805803418798539
06/01/2019 11:13:19 step: 7577, epoch: 229, batch: 19, loss: 0.03427475690841675, acc: 98.4375, f1: 96.66048237476808, r: 0.6773654945453446
06/01/2019 11:13:20 step: 7582, epoch: 229, batch: 24, loss: 0.020253077149391174, acc: 100.0, f1: 100.0, r: 0.7621577649199325
06/01/2019 11:13:21 step: 7587, epoch: 229, batch: 29, loss: 0.014737623743712902, acc: 100.0, f1: 100.0, r: 0.8219187378884909
06/01/2019 11:13:22 *** evaluating ***
06/01/2019 11:13:22 step: 230, epoch: 229, acc: 59.82905982905983, f1: 31.477657350819115, r: 0.27509034076759703
06/01/2019 11:13:22 *** epoch: 231 ***
06/01/2019 11:13:22 *** training ***
06/01/2019 11:13:23 step: 7595, epoch: 230, batch: 4, loss: 0.018355442211031914, acc: 100.0, f1: 100.0, r: 0.8019789738884147
06/01/2019 11:13:24 step: 7600, epoch: 230, batch: 9, loss: 0.015650948509573936, acc: 100.0, f1: 100.0, r: 0.6774777720911928
06/01/2019 11:13:25 step: 7605, epoch: 230, batch: 14, loss: 0.0528615340590477, acc: 100.0, f1: 100.0, r: 0.7534846897329037
06/01/2019 11:13:27 step: 7610, epoch: 230, batch: 19, loss: 0.006826194003224373, acc: 100.0, f1: 100.0, r: 0.7680819029868128
06/01/2019 11:13:28 step: 7615, epoch: 230, batch: 24, loss: 0.025211408734321594, acc: 100.0, f1: 100.0, r: 0.709650424416356
06/01/2019 11:13:29 step: 7620, epoch: 230, batch: 29, loss: 0.012764085084199905, acc: 100.0, f1: 100.0, r: 0.7423135849211759
06/01/2019 11:13:29 *** evaluating ***
06/01/2019 11:13:30 step: 231, epoch: 230, acc: 58.54700854700855, f1: 32.004146782010565, r: 0.26978435471635337
06/01/2019 11:13:30 *** epoch: 232 ***
06/01/2019 11:13:30 *** training ***
06/01/2019 11:13:31 step: 7628, epoch: 231, batch: 4, loss: 0.025128215551376343, acc: 100.0, f1: 100.0, r: 0.7440918318472826
06/01/2019 11:13:32 step: 7633, epoch: 231, batch: 9, loss: 0.007680190727114677, acc: 100.0, f1: 100.0, r: 0.8008125794910133
06/01/2019 11:13:33 step: 7638, epoch: 231, batch: 14, loss: 0.021735580638051033, acc: 100.0, f1: 100.0, r: 0.7305910523943073
06/01/2019 11:13:34 step: 7643, epoch: 231, batch: 19, loss: 0.01526128500699997, acc: 100.0, f1: 100.0, r: 0.6924233889149498
06/01/2019 11:13:35 step: 7648, epoch: 231, batch: 24, loss: 0.019778717309236526, acc: 100.0, f1: 100.0, r: 0.6859128393477107
06/01/2019 11:13:36 step: 7653, epoch: 231, batch: 29, loss: 0.01128307357430458, acc: 100.0, f1: 100.0, r: 0.692803143591622
06/01/2019 11:13:37 *** evaluating ***
06/01/2019 11:13:38 step: 232, epoch: 231, acc: 54.27350427350427, f1: 30.849669381865695, r: 0.260087326592034
06/01/2019 11:13:38 *** epoch: 233 ***
06/01/2019 11:13:38 *** training ***
06/01/2019 11:13:38 step: 7661, epoch: 232, batch: 4, loss: 0.009102579206228256, acc: 100.0, f1: 100.0, r: 0.7189354614455319
06/01/2019 11:13:40 step: 7666, epoch: 232, batch: 9, loss: 0.016256507486104965, acc: 100.0, f1: 100.0, r: 0.8448039884811226
06/01/2019 11:13:41 step: 7671, epoch: 232, batch: 14, loss: 0.025166472420096397, acc: 100.0, f1: 100.0, r: 0.6773430741908294
06/01/2019 11:13:42 step: 7676, epoch: 232, batch: 19, loss: 0.019174156710505486, acc: 100.0, f1: 100.0, r: 0.7115172358589688
06/01/2019 11:13:43 step: 7681, epoch: 232, batch: 24, loss: 0.588068962097168, acc: 100.0, f1: 100.0, r: 0.7301073117339951
06/01/2019 11:13:45 step: 7686, epoch: 232, batch: 29, loss: 0.007081677671521902, acc: 100.0, f1: 100.0, r: 0.7339805244549641
06/01/2019 11:13:45 *** evaluating ***
06/01/2019 11:13:45 step: 233, epoch: 232, acc: 57.692307692307686, f1: 30.835832258982165, r: 0.26259539914267843
06/01/2019 11:13:45 *** epoch: 234 ***
06/01/2019 11:13:45 *** training ***
06/01/2019 11:13:46 step: 7694, epoch: 233, batch: 4, loss: 0.01533933263272047, acc: 100.0, f1: 100.0, r: 0.7995302579005835
06/01/2019 11:13:48 step: 7699, epoch: 233, batch: 9, loss: 0.01190469041466713, acc: 100.0, f1: 100.0, r: 0.673209619574119
06/01/2019 11:13:48 step: 7704, epoch: 233, batch: 14, loss: 0.015265680849552155, acc: 100.0, f1: 100.0, r: 0.7050536393849203
06/01/2019 11:13:50 step: 7709, epoch: 233, batch: 19, loss: 0.022928722202777863, acc: 100.0, f1: 100.0, r: 0.79961504601127
06/01/2019 11:13:51 step: 7714, epoch: 233, batch: 24, loss: 0.015547205694019794, acc: 100.0, f1: 100.0, r: 0.7413073045828656
06/01/2019 11:13:52 step: 7719, epoch: 233, batch: 29, loss: 0.0178091861307621, acc: 100.0, f1: 100.0, r: 0.6659460782429552
06/01/2019 11:13:53 *** evaluating ***
06/01/2019 11:13:53 step: 234, epoch: 233, acc: 58.54700854700855, f1: 29.793327814361696, r: 0.2695756742744274
06/01/2019 11:13:53 *** epoch: 235 ***
06/01/2019 11:13:53 *** training ***
06/01/2019 11:13:54 step: 7727, epoch: 234, batch: 4, loss: 0.010010918602347374, acc: 100.0, f1: 100.0, r: 0.7816601183337831
06/01/2019 11:13:55 step: 7732, epoch: 234, batch: 9, loss: 0.005866150837391615, acc: 100.0, f1: 100.0, r: 0.7193153501307207
06/01/2019 11:13:56 step: 7737, epoch: 234, batch: 14, loss: 0.02602447010576725, acc: 100.0, f1: 100.0, r: 0.7631691508546188
06/01/2019 11:13:57 step: 7742, epoch: 234, batch: 19, loss: 0.013696710579097271, acc: 100.0, f1: 100.0, r: 0.6686812261610162
06/01/2019 11:13:58 step: 7747, epoch: 234, batch: 24, loss: 0.30305802822113037, acc: 100.0, f1: 100.0, r: 0.8249211143328246
06/01/2019 11:14:00 step: 7752, epoch: 234, batch: 29, loss: 0.012925874441862106, acc: 100.0, f1: 100.0, r: 0.7883551413510269
06/01/2019 11:14:00 *** evaluating ***
06/01/2019 11:14:01 step: 235, epoch: 234, acc: 58.119658119658126, f1: 31.149414311179015, r: 0.26826325903522025
06/01/2019 11:14:01 *** epoch: 236 ***
06/01/2019 11:14:01 *** training ***
06/01/2019 11:14:02 step: 7760, epoch: 235, batch: 4, loss: 0.008929076604545116, acc: 100.0, f1: 100.0, r: 0.6484764209469694
06/01/2019 11:14:03 step: 7765, epoch: 235, batch: 9, loss: 0.014141026884317398, acc: 100.0, f1: 100.0, r: 0.7886308332109876
06/01/2019 11:14:04 step: 7770, epoch: 235, batch: 14, loss: 0.019813915714621544, acc: 100.0, f1: 100.0, r: 0.7956123184335173
06/01/2019 11:14:05 step: 7775, epoch: 235, batch: 19, loss: 0.018397677689790726, acc: 100.0, f1: 100.0, r: 0.7153106554963333
06/01/2019 11:14:06 step: 7780, epoch: 235, batch: 24, loss: 0.0459778755903244, acc: 98.4375, f1: 97.94941900205059, r: 0.671914718143043
06/01/2019 11:14:07 step: 7785, epoch: 235, batch: 29, loss: 0.010311566293239594, acc: 100.0, f1: 100.0, r: 0.7439071196662729
06/01/2019 11:14:08 *** evaluating ***
06/01/2019 11:14:08 step: 236, epoch: 235, acc: 58.54700854700855, f1: 31.296944072208614, r: 0.2701311236284632
06/01/2019 11:14:08 *** epoch: 237 ***
06/01/2019 11:14:08 *** training ***
06/01/2019 11:14:09 step: 7793, epoch: 236, batch: 4, loss: 0.008534533903002739, acc: 100.0, f1: 100.0, r: 0.7061316509269564
06/01/2019 11:14:10 step: 7798, epoch: 236, batch: 9, loss: 0.01097851898521185, acc: 100.0, f1: 100.0, r: 0.7098279881454044
06/01/2019 11:14:12 step: 7803, epoch: 236, batch: 14, loss: 0.01839056797325611, acc: 100.0, f1: 100.0, r: 0.7831664404355579
06/01/2019 11:14:13 step: 7808, epoch: 236, batch: 19, loss: 0.009995869360864162, acc: 100.0, f1: 100.0, r: 0.7889304004513228
06/01/2019 11:14:14 step: 7813, epoch: 236, batch: 24, loss: 0.00760524021461606, acc: 100.0, f1: 100.0, r: 0.7706841154187671
06/01/2019 11:14:15 step: 7818, epoch: 236, batch: 29, loss: 0.02457571029663086, acc: 100.0, f1: 100.0, r: 0.8086676395843652
06/01/2019 11:14:16 *** evaluating ***
06/01/2019 11:14:16 step: 237, epoch: 236, acc: 55.98290598290598, f1: 30.852228102228107, r: 0.2605973039163172
06/01/2019 11:14:16 *** epoch: 238 ***
06/01/2019 11:14:16 *** training ***
06/01/2019 11:14:17 step: 7826, epoch: 237, batch: 4, loss: 0.006807796657085419, acc: 100.0, f1: 100.0, r: 0.7296945003091739
06/01/2019 11:14:18 step: 7831, epoch: 237, batch: 9, loss: 0.0029698489233851433, acc: 100.0, f1: 100.0, r: 0.7409073462651693
06/01/2019 11:14:19 step: 7836, epoch: 237, batch: 14, loss: 0.04104040935635567, acc: 100.0, f1: 100.0, r: 0.7678676677995823
06/01/2019 11:14:21 step: 7841, epoch: 237, batch: 19, loss: 0.00965930987149477, acc: 100.0, f1: 100.0, r: 0.7436141062835699
06/01/2019 11:14:22 step: 7846, epoch: 237, batch: 24, loss: 0.007777527440339327, acc: 100.0, f1: 100.0, r: 0.8224553653260024
06/01/2019 11:14:23 step: 7851, epoch: 237, batch: 29, loss: 0.006687137298285961, acc: 100.0, f1: 100.0, r: 0.6726637987077945
06/01/2019 11:14:23 *** evaluating ***
06/01/2019 11:14:24 step: 238, epoch: 237, acc: 57.26495726495726, f1: 31.09383376397941, r: 0.26452964643676785
06/01/2019 11:14:24 *** epoch: 239 ***
06/01/2019 11:14:24 *** training ***
06/01/2019 11:14:25 step: 7859, epoch: 238, batch: 4, loss: 0.016793619841337204, acc: 100.0, f1: 100.0, r: 0.7334875017561513
06/01/2019 11:14:26 step: 7864, epoch: 238, batch: 9, loss: 0.007621970027685165, acc: 100.0, f1: 100.0, r: 0.7561380576134856
06/01/2019 11:14:27 step: 7869, epoch: 238, batch: 14, loss: 0.02293119952082634, acc: 100.0, f1: 100.0, r: 0.762930371724831
06/01/2019 11:14:28 step: 7874, epoch: 238, batch: 19, loss: 0.5990139842033386, acc: 100.0, f1: 100.0, r: 0.7644844218734865
06/01/2019 11:14:30 step: 7879, epoch: 238, batch: 24, loss: 0.01024898886680603, acc: 100.0, f1: 100.0, r: 0.6499408272816121
06/01/2019 11:14:31 step: 7884, epoch: 238, batch: 29, loss: 0.0054312171414494514, acc: 100.0, f1: 100.0, r: 0.7781028977780362
06/01/2019 11:14:31 *** evaluating ***
06/01/2019 11:14:32 step: 239, epoch: 238, acc: 57.692307692307686, f1: 28.68580636769491, r: 0.2646998149096998
06/01/2019 11:14:32 *** epoch: 240 ***
06/01/2019 11:14:32 *** training ***
06/01/2019 11:14:33 step: 7892, epoch: 239, batch: 4, loss: 0.00774751091375947, acc: 100.0, f1: 100.0, r: 0.7601338420819955
06/01/2019 11:14:34 step: 7897, epoch: 239, batch: 9, loss: 0.05434824153780937, acc: 100.0, f1: 100.0, r: 0.707660615558962
06/01/2019 11:14:35 step: 7902, epoch: 239, batch: 14, loss: 0.018621880561113358, acc: 100.0, f1: 100.0, r: 0.7019478085367789
06/01/2019 11:14:36 step: 7907, epoch: 239, batch: 19, loss: 0.007191468495875597, acc: 100.0, f1: 100.0, r: 0.7002293516303569
06/01/2019 11:14:37 step: 7912, epoch: 239, batch: 24, loss: 0.0072353919968008995, acc: 100.0, f1: 100.0, r: 0.7664360116607595
06/01/2019 11:14:38 step: 7917, epoch: 239, batch: 29, loss: 0.008947470225393772, acc: 100.0, f1: 100.0, r: 0.578661347870365
06/01/2019 11:14:39 *** evaluating ***
06/01/2019 11:14:39 step: 240, epoch: 239, acc: 58.119658119658126, f1: 29.730458189254378, r: 0.268800301471452
06/01/2019 11:14:39 *** epoch: 241 ***
06/01/2019 11:14:39 *** training ***
06/01/2019 11:14:41 step: 7925, epoch: 240, batch: 4, loss: 0.008226064965128899, acc: 100.0, f1: 100.0, r: 0.790085659142537
06/01/2019 11:14:42 step: 7930, epoch: 240, batch: 9, loss: 0.012772691436111927, acc: 100.0, f1: 100.0, r: 0.837394844318859
06/01/2019 11:14:43 step: 7935, epoch: 240, batch: 14, loss: 0.015310563147068024, acc: 100.0, f1: 100.0, r: 0.7910348207693104
06/01/2019 11:14:44 step: 7940, epoch: 240, batch: 19, loss: 0.0063854362815618515, acc: 100.0, f1: 100.0, r: 0.7823276953314057
06/01/2019 11:14:45 step: 7945, epoch: 240, batch: 24, loss: 0.005842323414981365, acc: 100.0, f1: 100.0, r: 0.755475582326774
06/01/2019 11:14:46 step: 7950, epoch: 240, batch: 29, loss: 0.00914913509041071, acc: 100.0, f1: 100.0, r: 0.8229578183240647
06/01/2019 11:14:47 *** evaluating ***
06/01/2019 11:14:47 step: 241, epoch: 240, acc: 57.692307692307686, f1: 30.41731224480869, r: 0.2647123655517227
06/01/2019 11:14:47 *** epoch: 242 ***
06/01/2019 11:14:47 *** training ***
06/01/2019 11:14:48 step: 7958, epoch: 241, batch: 4, loss: 0.01890409365296364, acc: 100.0, f1: 100.0, r: 0.671552214425678
06/01/2019 11:14:49 step: 7963, epoch: 241, batch: 9, loss: 0.021261252462863922, acc: 100.0, f1: 100.0, r: 0.7709285135159597
06/01/2019 11:14:50 step: 7968, epoch: 241, batch: 14, loss: 0.0263981893658638, acc: 100.0, f1: 100.0, r: 0.71938105867711
06/01/2019 11:14:51 step: 7973, epoch: 241, batch: 19, loss: 0.02581671252846718, acc: 100.0, f1: 100.0, r: 0.6472040297962759
06/01/2019 11:14:52 step: 7978, epoch: 241, batch: 24, loss: 0.01177952066063881, acc: 100.0, f1: 100.0, r: 0.7111069048198397
06/01/2019 11:14:54 step: 7983, epoch: 241, batch: 29, loss: 1.1492568254470825, acc: 100.0, f1: 100.0, r: 0.7509854936120325
06/01/2019 11:14:54 *** evaluating ***
06/01/2019 11:14:55 step: 242, epoch: 241, acc: 58.54700854700855, f1: 28.915426822781555, r: 0.26819816066796603
06/01/2019 11:14:55 *** epoch: 243 ***
06/01/2019 11:14:55 *** training ***
06/01/2019 11:14:56 step: 7991, epoch: 242, batch: 4, loss: 0.007775808684527874, acc: 100.0, f1: 100.0, r: 0.6605054006726115
06/01/2019 11:14:57 step: 7996, epoch: 242, batch: 9, loss: 0.013388065621256828, acc: 100.0, f1: 100.0, r: 0.6922393947185732
06/01/2019 11:14:58 step: 8001, epoch: 242, batch: 14, loss: 0.021069251000881195, acc: 100.0, f1: 100.0, r: 0.7067907120443261
06/01/2019 11:14:59 step: 8006, epoch: 242, batch: 19, loss: 0.008117802441120148, acc: 100.0, f1: 100.0, r: 0.7816254354490323
06/01/2019 11:15:00 step: 8011, epoch: 242, batch: 24, loss: 0.020229721441864967, acc: 100.0, f1: 100.0, r: 0.6736169525755714
06/01/2019 11:15:01 step: 8016, epoch: 242, batch: 29, loss: 0.018896128982305527, acc: 100.0, f1: 100.0, r: 0.7156266804170034
06/01/2019 11:15:02 *** evaluating ***
06/01/2019 11:15:02 step: 243, epoch: 242, acc: 59.82905982905983, f1: 31.190216881427002, r: 0.26818626547241015
06/01/2019 11:15:02 *** epoch: 244 ***
06/01/2019 11:15:02 *** training ***
06/01/2019 11:15:03 step: 8024, epoch: 243, batch: 4, loss: 0.014613050036132336, acc: 100.0, f1: 100.0, r: 0.7708021509538263
06/01/2019 11:15:04 step: 8029, epoch: 243, batch: 9, loss: 0.013497774489223957, acc: 100.0, f1: 100.0, r: 0.7168065499805775
06/01/2019 11:15:06 step: 8034, epoch: 243, batch: 14, loss: 0.00653364323079586, acc: 100.0, f1: 100.0, r: 0.7866787961470021
06/01/2019 11:15:07 step: 8039, epoch: 243, batch: 19, loss: 0.015978816896677017, acc: 100.0, f1: 100.0, r: 0.7500918428432336
06/01/2019 11:15:08 step: 8044, epoch: 243, batch: 24, loss: 0.017580421641469002, acc: 100.0, f1: 100.0, r: 0.6833963375183085
06/01/2019 11:15:09 step: 8049, epoch: 243, batch: 29, loss: 0.010747172869741917, acc: 100.0, f1: 100.0, r: 0.6057637923047603
06/01/2019 11:15:09 *** evaluating ***
06/01/2019 11:15:10 step: 244, epoch: 243, acc: 59.401709401709404, f1: 31.413703838792294, r: 0.2648919531467395
06/01/2019 11:15:10 *** epoch: 245 ***
06/01/2019 11:15:10 *** training ***
06/01/2019 11:15:11 step: 8057, epoch: 244, batch: 4, loss: 0.008020218461751938, acc: 100.0, f1: 100.0, r: 0.802025767814064
06/01/2019 11:15:12 step: 8062, epoch: 244, batch: 9, loss: 0.025074588134884834, acc: 100.0, f1: 100.0, r: 0.7149419216515892
06/01/2019 11:15:13 step: 8067, epoch: 244, batch: 14, loss: 0.023197291418910027, acc: 100.0, f1: 100.0, r: 0.7761133295940847
06/01/2019 11:15:14 step: 8072, epoch: 244, batch: 19, loss: 0.0448145866394043, acc: 100.0, f1: 100.0, r: 0.6033996839847131
06/01/2019 11:15:15 step: 8077, epoch: 244, batch: 24, loss: 0.009598677046597004, acc: 100.0, f1: 100.0, r: 0.8297721729754346
06/01/2019 11:15:16 step: 8082, epoch: 244, batch: 29, loss: 0.012810666114091873, acc: 100.0, f1: 100.0, r: 0.859577205657913
06/01/2019 11:15:17 *** evaluating ***
06/01/2019 11:15:17 step: 245, epoch: 244, acc: 59.401709401709404, f1: 30.094765204256603, r: 0.2708059131840716
06/01/2019 11:15:17 *** epoch: 246 ***
06/01/2019 11:15:17 *** training ***
06/01/2019 11:15:18 step: 8090, epoch: 245, batch: 4, loss: 0.004529013764113188, acc: 100.0, f1: 100.0, r: 0.8310114914934972
06/01/2019 11:15:20 step: 8095, epoch: 245, batch: 9, loss: 0.004861518740653992, acc: 100.0, f1: 100.0, r: 0.7968835561026532
06/01/2019 11:15:21 step: 8100, epoch: 245, batch: 14, loss: 0.007318772375583649, acc: 100.0, f1: 100.0, r: 0.7179262659128105
06/01/2019 11:15:22 step: 8105, epoch: 245, batch: 19, loss: 0.02168537676334381, acc: 98.4375, f1: 87.3076923076923, r: 0.8015902551184289
06/01/2019 11:15:23 step: 8110, epoch: 245, batch: 24, loss: 0.06188897788524628, acc: 98.4375, f1: 98.1283422459893, r: 0.7843300190706435
06/01/2019 11:15:24 step: 8115, epoch: 245, batch: 29, loss: 0.02337418869137764, acc: 100.0, f1: 100.0, r: 0.7419158865841008
06/01/2019 11:15:25 *** evaluating ***
06/01/2019 11:15:25 step: 246, epoch: 245, acc: 59.401709401709404, f1: 31.598665782102316, r: 0.26613830507763997
06/01/2019 11:15:25 *** epoch: 247 ***
06/01/2019 11:15:25 *** training ***
06/01/2019 11:15:26 step: 8123, epoch: 246, batch: 4, loss: 0.02331034280359745, acc: 100.0, f1: 100.0, r: 0.7423331394784959
06/01/2019 11:15:27 step: 8128, epoch: 246, batch: 9, loss: 0.03421001881361008, acc: 98.4375, f1: 95.28985507246377, r: 0.7870540785848734
06/01/2019 11:15:28 step: 8133, epoch: 246, batch: 14, loss: 0.011228119023144245, acc: 100.0, f1: 100.0, r: 0.7836397628308561
06/01/2019 11:15:29 step: 8138, epoch: 246, batch: 19, loss: 0.006713496055454016, acc: 100.0, f1: 100.0, r: 0.8350927544243507
06/01/2019 11:15:31 step: 8143, epoch: 246, batch: 24, loss: 0.010655093938112259, acc: 100.0, f1: 100.0, r: 0.6929525860426047
06/01/2019 11:15:32 step: 8148, epoch: 246, batch: 29, loss: 0.006936144083738327, acc: 100.0, f1: 100.0, r: 0.7238330212476871
06/01/2019 11:15:33 *** evaluating ***
06/01/2019 11:15:33 step: 247, epoch: 246, acc: 58.119658119658126, f1: 29.70695540275998, r: 0.2651145931615576
06/01/2019 11:15:33 *** epoch: 248 ***
06/01/2019 11:15:33 *** training ***
06/01/2019 11:15:34 step: 8156, epoch: 247, batch: 4, loss: 0.02283763512969017, acc: 100.0, f1: 100.0, r: 0.6907925131112611
06/01/2019 11:15:35 step: 8161, epoch: 247, batch: 9, loss: 0.018022192642092705, acc: 100.0, f1: 100.0, r: 0.7093688059140393
06/01/2019 11:15:36 step: 8166, epoch: 247, batch: 14, loss: 0.6381511688232422, acc: 98.4375, f1: 98.02102659245516, r: 0.7253504987954416
06/01/2019 11:15:37 step: 8171, epoch: 247, batch: 19, loss: 0.006029944866895676, acc: 100.0, f1: 100.0, r: 0.830826220118013
06/01/2019 11:15:38 step: 8176, epoch: 247, batch: 24, loss: 0.007794518023729324, acc: 100.0, f1: 100.0, r: 0.791615815175878
06/01/2019 11:15:39 step: 8181, epoch: 247, batch: 29, loss: 0.03603268042206764, acc: 100.0, f1: 100.0, r: 0.8120001660191387
06/01/2019 11:15:40 *** evaluating ***
06/01/2019 11:15:40 step: 248, epoch: 247, acc: 57.692307692307686, f1: 31.76171283363064, r: 0.2632378384943764
06/01/2019 11:15:40 *** epoch: 249 ***
06/01/2019 11:15:40 *** training ***
06/01/2019 11:15:41 step: 8189, epoch: 248, batch: 4, loss: 0.0059622665867209435, acc: 100.0, f1: 100.0, r: 0.7568884548799563
06/01/2019 11:15:42 step: 8194, epoch: 248, batch: 9, loss: 0.013841483741998672, acc: 100.0, f1: 100.0, r: 0.8443511213257388
06/01/2019 11:15:44 step: 8199, epoch: 248, batch: 14, loss: 0.01180698536336422, acc: 100.0, f1: 100.0, r: 0.6591495921478133
06/01/2019 11:15:45 step: 8204, epoch: 248, batch: 19, loss: 0.004619584884494543, acc: 100.0, f1: 100.0, r: 0.717182318505812
06/01/2019 11:15:46 step: 8209, epoch: 248, batch: 24, loss: 0.007360254414379597, acc: 100.0, f1: 100.0, r: 0.7949159569013291
06/01/2019 11:15:47 step: 8214, epoch: 248, batch: 29, loss: 0.016400283202528954, acc: 100.0, f1: 100.0, r: 0.7966512716522085
06/01/2019 11:15:47 *** evaluating ***
06/01/2019 11:15:48 step: 249, epoch: 248, acc: 59.401709401709404, f1: 31.871001138374666, r: 0.26544257511143987
06/01/2019 11:15:48 *** epoch: 250 ***
06/01/2019 11:15:48 *** training ***
06/01/2019 11:15:49 step: 8222, epoch: 249, batch: 4, loss: 0.030064700171351433, acc: 98.4375, f1: 97.94941900205058, r: 0.6907198676421704
06/01/2019 11:15:50 step: 8227, epoch: 249, batch: 9, loss: 0.008799131959676743, acc: 100.0, f1: 100.0, r: 0.667144440669044
06/01/2019 11:15:51 step: 8232, epoch: 249, batch: 14, loss: 0.05351844057440758, acc: 100.0, f1: 100.0, r: 0.6813133006219266
06/01/2019 11:15:52 step: 8237, epoch: 249, batch: 19, loss: 0.021458473056554794, acc: 100.0, f1: 100.0, r: 0.7965191074053307
06/01/2019 11:15:53 step: 8242, epoch: 249, batch: 24, loss: 0.00602699164301157, acc: 100.0, f1: 100.0, r: 0.7945771071963228
06/01/2019 11:15:54 step: 8247, epoch: 249, batch: 29, loss: 0.003694996703416109, acc: 100.0, f1: 100.0, r: 0.7428924088136422
06/01/2019 11:15:55 *** evaluating ***
06/01/2019 11:15:55 step: 250, epoch: 249, acc: 58.54700854700855, f1: 29.994108941477364, r: 0.2604577357597122
06/01/2019 11:15:55 *** epoch: 251 ***
06/01/2019 11:15:55 *** training ***
06/01/2019 11:15:56 step: 8255, epoch: 250, batch: 4, loss: 0.024403655901551247, acc: 100.0, f1: 100.0, r: 0.6491662432411448
06/01/2019 11:15:58 step: 8260, epoch: 250, batch: 9, loss: 0.035841554403305054, acc: 100.0, f1: 100.0, r: 0.8512058047343165
06/01/2019 11:15:58 step: 8265, epoch: 250, batch: 14, loss: 0.010627243667840958, acc: 100.0, f1: 100.0, r: 0.806252876278431
06/01/2019 11:15:59 step: 8270, epoch: 250, batch: 19, loss: 0.05817882716655731, acc: 100.0, f1: 100.0, r: 0.6759791450422414
06/01/2019 11:16:01 step: 8275, epoch: 250, batch: 24, loss: 0.006072233431041241, acc: 100.0, f1: 100.0, r: 0.7011213700622915
06/01/2019 11:16:02 step: 8280, epoch: 250, batch: 29, loss: 0.00930764339864254, acc: 100.0, f1: 100.0, r: 0.7385548790689009
06/01/2019 11:16:02 *** evaluating ***
06/01/2019 11:16:03 step: 251, epoch: 250, acc: 58.54700854700855, f1: 29.082989336085312, r: 0.2663756666733774
06/01/2019 11:16:03 *** epoch: 252 ***
06/01/2019 11:16:03 *** training ***
06/01/2019 11:16:04 step: 8288, epoch: 251, batch: 4, loss: 0.008082844316959381, acc: 100.0, f1: 100.0, r: 0.6576126210965978
06/01/2019 11:16:05 step: 8293, epoch: 251, batch: 9, loss: 0.02428572066128254, acc: 100.0, f1: 100.0, r: 0.6889287167337451
06/01/2019 11:16:06 step: 8298, epoch: 251, batch: 14, loss: 0.009624908678233624, acc: 100.0, f1: 100.0, r: 0.7115042346363637
06/01/2019 11:16:07 step: 8303, epoch: 251, batch: 19, loss: 0.011683980002999306, acc: 100.0, f1: 100.0, r: 0.7057105829988868
06/01/2019 11:16:08 step: 8308, epoch: 251, batch: 24, loss: 0.012842779979109764, acc: 100.0, f1: 100.0, r: 0.6870050173516904
06/01/2019 11:16:09 step: 8313, epoch: 251, batch: 29, loss: 0.018925722688436508, acc: 100.0, f1: 100.0, r: 0.7850608679365443
06/01/2019 11:16:10 *** evaluating ***
06/01/2019 11:16:10 step: 252, epoch: 251, acc: 57.692307692307686, f1: 29.674303202415686, r: 0.2616525138908524
06/01/2019 11:16:10 *** epoch: 253 ***
06/01/2019 11:16:10 *** training ***
06/01/2019 11:16:11 step: 8321, epoch: 252, batch: 4, loss: 0.005722736939787865, acc: 100.0, f1: 100.0, r: 0.5949684853716513
06/01/2019 11:16:12 step: 8326, epoch: 252, batch: 9, loss: 0.006570456549525261, acc: 100.0, f1: 100.0, r: 0.7259209616769109
06/01/2019 11:16:14 step: 8331, epoch: 252, batch: 14, loss: 0.018650341778993607, acc: 100.0, f1: 100.0, r: 0.7160143862091694
06/01/2019 11:16:15 step: 8336, epoch: 252, batch: 19, loss: 0.02725817821919918, acc: 100.0, f1: 100.0, r: 0.6883785550001497
06/01/2019 11:16:16 step: 8341, epoch: 252, batch: 24, loss: 0.013830372132360935, acc: 100.0, f1: 100.0, r: 0.748448739865292
06/01/2019 11:16:17 step: 8346, epoch: 252, batch: 29, loss: 0.010522192344069481, acc: 100.0, f1: 100.0, r: 0.8289644514992236
06/01/2019 11:16:18 *** evaluating ***
06/01/2019 11:16:18 step: 253, epoch: 252, acc: 58.119658119658126, f1: 32.78746568392702, r: 0.2603478021700709
06/01/2019 11:16:18 *** epoch: 254 ***
06/01/2019 11:16:18 *** training ***
06/01/2019 11:16:19 step: 8354, epoch: 253, batch: 4, loss: 0.011911634355783463, acc: 100.0, f1: 100.0, r: 0.8148314559086165
06/01/2019 11:16:20 step: 8359, epoch: 253, batch: 9, loss: 0.6260738372802734, acc: 100.0, f1: 100.0, r: 0.6832424039793742
06/01/2019 11:16:22 step: 8364, epoch: 253, batch: 14, loss: 0.016364773735404015, acc: 100.0, f1: 100.0, r: 0.6827973451226693
06/01/2019 11:16:22 step: 8369, epoch: 253, batch: 19, loss: 0.013084790669381618, acc: 100.0, f1: 100.0, r: 0.7253708300069626
06/01/2019 11:16:23 step: 8374, epoch: 253, batch: 24, loss: 0.0053527578711509705, acc: 100.0, f1: 100.0, r: 0.8303191567760307
06/01/2019 11:16:25 step: 8379, epoch: 253, batch: 29, loss: 0.017806904390454292, acc: 100.0, f1: 100.0, r: 0.6483181269365474
06/01/2019 11:16:25 *** evaluating ***
06/01/2019 11:16:26 step: 254, epoch: 253, acc: 57.692307692307686, f1: 31.623290765184564, r: 0.2608230073120898
06/01/2019 11:16:26 *** epoch: 255 ***
06/01/2019 11:16:26 *** training ***
06/01/2019 11:16:26 step: 8387, epoch: 254, batch: 4, loss: 0.5368938446044922, acc: 100.0, f1: 100.0, r: 0.6830573314200947
06/01/2019 11:16:28 step: 8392, epoch: 254, batch: 9, loss: 0.022570379078388214, acc: 100.0, f1: 100.0, r: 0.8309409552972973
06/01/2019 11:16:29 step: 8397, epoch: 254, batch: 14, loss: 0.008740688674151897, acc: 100.0, f1: 100.0, r: 0.620102208351173
06/01/2019 11:16:30 step: 8402, epoch: 254, batch: 19, loss: 0.00999176874756813, acc: 100.0, f1: 100.0, r: 0.7783650788044438
06/01/2019 11:16:31 step: 8407, epoch: 254, batch: 24, loss: 0.01393614150583744, acc: 100.0, f1: 100.0, r: 0.7691531600842701
06/01/2019 11:16:32 step: 8412, epoch: 254, batch: 29, loss: 0.04225606843829155, acc: 100.0, f1: 100.0, r: 0.6661627441577004
06/01/2019 11:16:33 *** evaluating ***
06/01/2019 11:16:33 step: 255, epoch: 254, acc: 57.26495726495726, f1: 30.638072716531106, r: 0.26800843404941715
06/01/2019 11:16:33 *** epoch: 256 ***
06/01/2019 11:16:33 *** training ***
06/01/2019 11:16:34 step: 8420, epoch: 255, batch: 4, loss: 0.010468965396285057, acc: 100.0, f1: 100.0, r: 0.7504556550915027
06/01/2019 11:16:35 step: 8425, epoch: 255, batch: 9, loss: 0.010576721280813217, acc: 100.0, f1: 100.0, r: 0.7400288364143103
06/01/2019 11:16:37 step: 8430, epoch: 255, batch: 14, loss: 0.01336442306637764, acc: 100.0, f1: 100.0, r: 0.6455970090747379
06/01/2019 11:16:38 step: 8435, epoch: 255, batch: 19, loss: 0.006671044044196606, acc: 100.0, f1: 100.0, r: 0.8242878378144888
06/01/2019 11:16:39 step: 8440, epoch: 255, batch: 24, loss: 0.009265361353754997, acc: 100.0, f1: 100.0, r: 0.8266586026298066
06/01/2019 11:16:40 step: 8445, epoch: 255, batch: 29, loss: 0.008744007907807827, acc: 100.0, f1: 100.0, r: 0.7547116645408606
06/01/2019 11:16:40 *** evaluating ***
06/01/2019 11:16:41 step: 256, epoch: 255, acc: 58.54700854700855, f1: 32.65367139206149, r: 0.26421845083817597
06/01/2019 11:16:41 *** epoch: 257 ***
06/01/2019 11:16:41 *** training ***
06/01/2019 11:16:42 step: 8453, epoch: 256, batch: 4, loss: 0.011467509903013706, acc: 100.0, f1: 100.0, r: 0.75086546955584
06/01/2019 11:16:43 step: 8458, epoch: 256, batch: 9, loss: 0.020013980567455292, acc: 100.0, f1: 100.0, r: 0.7402079712347452
06/01/2019 11:16:44 step: 8463, epoch: 256, batch: 14, loss: 0.003011289518326521, acc: 100.0, f1: 100.0, r: 0.6474791922358478
06/01/2019 11:16:45 step: 8468, epoch: 256, batch: 19, loss: 0.02751363441348076, acc: 98.4375, f1: 99.23215898825654, r: 0.7350824403380405
06/01/2019 11:16:46 step: 8473, epoch: 256, batch: 24, loss: 0.027948884293437004, acc: 98.4375, f1: 98.20868786386028, r: 0.7157355215532991
06/01/2019 11:16:47 step: 8478, epoch: 256, batch: 29, loss: 0.006589507684111595, acc: 100.0, f1: 100.0, r: 0.6825952769994368
06/01/2019 11:16:48 *** evaluating ***
06/01/2019 11:16:48 step: 257, epoch: 256, acc: 56.41025641025641, f1: 31.118401764289455, r: 0.2634405655013117
06/01/2019 11:16:48 *** epoch: 258 ***
06/01/2019 11:16:48 *** training ***
06/01/2019 11:16:49 step: 8486, epoch: 257, batch: 4, loss: 0.0061301011592149734, acc: 100.0, f1: 100.0, r: 0.622777512459965
06/01/2019 11:16:50 step: 8491, epoch: 257, batch: 9, loss: 0.0073349205777049065, acc: 100.0, f1: 100.0, r: 0.839016854216606
06/01/2019 11:16:52 step: 8496, epoch: 257, batch: 14, loss: 0.0032593775540590286, acc: 100.0, f1: 100.0, r: 0.6920047615048044
06/01/2019 11:16:53 step: 8501, epoch: 257, batch: 19, loss: 0.008122540079057217, acc: 100.0, f1: 100.0, r: 0.6656017672066046
06/01/2019 11:16:54 step: 8506, epoch: 257, batch: 24, loss: 0.012279133312404156, acc: 100.0, f1: 100.0, r: 0.719080603943591
06/01/2019 11:16:55 step: 8511, epoch: 257, batch: 29, loss: 0.011942679062485695, acc: 100.0, f1: 100.0, r: 0.7621921857937541
06/01/2019 11:16:55 *** evaluating ***
06/01/2019 11:16:56 step: 258, epoch: 257, acc: 58.119658119658126, f1: 32.8387532683759, r: 0.2631587823446937
06/01/2019 11:16:56 *** epoch: 259 ***
06/01/2019 11:16:56 *** training ***
06/01/2019 11:16:57 step: 8519, epoch: 258, batch: 4, loss: 0.008247417397797108, acc: 100.0, f1: 100.0, r: 0.5611946786487745
06/01/2019 11:16:58 step: 8524, epoch: 258, batch: 9, loss: 0.006481559947133064, acc: 100.0, f1: 100.0, r: 0.7967836691505762
06/01/2019 11:16:59 step: 8529, epoch: 258, batch: 14, loss: 0.00840322021394968, acc: 100.0, f1: 100.0, r: 0.7883408393066
06/01/2019 11:17:00 step: 8534, epoch: 258, batch: 19, loss: 0.004815451800823212, acc: 100.0, f1: 100.0, r: 0.6780024645808568
06/01/2019 11:17:01 step: 8539, epoch: 258, batch: 24, loss: 0.00880449265241623, acc: 100.0, f1: 100.0, r: 0.7565948100057094
06/01/2019 11:17:02 step: 8544, epoch: 258, batch: 29, loss: 0.01256268285214901, acc: 100.0, f1: 100.0, r: 0.679556684643013
06/01/2019 11:17:03 *** evaluating ***
06/01/2019 11:17:03 step: 259, epoch: 258, acc: 58.97435897435898, f1: 31.10529457276033, r: 0.2698007884786515
06/01/2019 11:17:03 *** epoch: 260 ***
06/01/2019 11:17:03 *** training ***
06/01/2019 11:17:04 step: 8552, epoch: 259, batch: 4, loss: 0.013549141585826874, acc: 100.0, f1: 100.0, r: 0.6241949764623689
06/01/2019 11:17:05 step: 8557, epoch: 259, batch: 9, loss: 0.03161738067865372, acc: 100.0, f1: 100.0, r: 0.7615127762451391
06/01/2019 11:17:06 step: 8562, epoch: 259, batch: 14, loss: 0.013043884187936783, acc: 100.0, f1: 100.0, r: 0.7502360817910099
06/01/2019 11:17:07 step: 8567, epoch: 259, batch: 19, loss: 0.010604172013700008, acc: 100.0, f1: 100.0, r: 0.7810326796547846
06/01/2019 11:17:08 step: 8572, epoch: 259, batch: 24, loss: 0.00968678668141365, acc: 100.0, f1: 100.0, r: 0.6909907695018034
06/01/2019 11:17:09 step: 8577, epoch: 259, batch: 29, loss: 0.019452229142189026, acc: 100.0, f1: 100.0, r: 0.7121231478471058
06/01/2019 11:17:10 *** evaluating ***
06/01/2019 11:17:10 step: 260, epoch: 259, acc: 58.119658119658126, f1: 28.22276549745908, r: 0.2696588674102883
06/01/2019 11:17:10 *** epoch: 261 ***
06/01/2019 11:17:10 *** training ***
06/01/2019 11:17:11 step: 8585, epoch: 260, batch: 4, loss: 0.01786673255264759, acc: 100.0, f1: 100.0, r: 0.8104557777482295
06/01/2019 11:17:13 step: 8590, epoch: 260, batch: 9, loss: 0.017485711723566055, acc: 100.0, f1: 100.0, r: 0.671929972569019
06/01/2019 11:17:13 step: 8595, epoch: 260, batch: 14, loss: 0.01924903504550457, acc: 98.4375, f1: 97.61904761904762, r: 0.7362586013342148
06/01/2019 11:17:15 step: 8600, epoch: 260, batch: 19, loss: 0.061129190027713776, acc: 98.4375, f1: 97.55639097744361, r: 0.7623771462457786
06/01/2019 11:17:16 step: 8605, epoch: 260, batch: 24, loss: 0.01583864353597164, acc: 100.0, f1: 100.0, r: 0.8185607689696555
06/01/2019 11:17:17 step: 8610, epoch: 260, batch: 29, loss: 0.008802969008684158, acc: 100.0, f1: 100.0, r: 0.7838366009131197
06/01/2019 11:17:17 *** evaluating ***
06/01/2019 11:17:18 step: 261, epoch: 260, acc: 57.26495726495726, f1: 28.696407571596573, r: 0.2654794233844778
06/01/2019 11:17:18 *** epoch: 262 ***
06/01/2019 11:17:18 *** training ***
06/01/2019 11:17:19 step: 8618, epoch: 261, batch: 4, loss: 0.008695811964571476, acc: 100.0, f1: 100.0, r: 0.7229684778443536
06/01/2019 11:17:20 step: 8623, epoch: 261, batch: 9, loss: 0.017406506463885307, acc: 100.0, f1: 100.0, r: 0.795917820878874
06/01/2019 11:17:21 step: 8628, epoch: 261, batch: 14, loss: 0.004683614708483219, acc: 100.0, f1: 100.0, r: 0.7778094536234054
06/01/2019 11:17:22 step: 8633, epoch: 261, batch: 19, loss: 0.30721619725227356, acc: 100.0, f1: 100.0, r: 0.7193245719955581
06/01/2019 11:17:23 step: 8638, epoch: 261, batch: 24, loss: 0.01557985134422779, acc: 100.0, f1: 100.0, r: 0.7402331544187356
06/01/2019 11:17:24 step: 8643, epoch: 261, batch: 29, loss: 0.01100054383277893, acc: 100.0, f1: 100.0, r: 0.7433103229675039
06/01/2019 11:17:25 *** evaluating ***
06/01/2019 11:17:25 step: 262, epoch: 261, acc: 59.401709401709404, f1: 32.038288288288285, r: 0.26620257325410984
06/01/2019 11:17:25 *** epoch: 263 ***
06/01/2019 11:17:25 *** training ***
06/01/2019 11:17:26 step: 8651, epoch: 262, batch: 4, loss: 0.016722051426768303, acc: 100.0, f1: 100.0, r: 0.6940720053900987
06/01/2019 11:17:27 step: 8656, epoch: 262, batch: 9, loss: 0.014671550132334232, acc: 100.0, f1: 100.0, r: 0.8032282726545634
06/01/2019 11:17:28 step: 8661, epoch: 262, batch: 14, loss: 0.005826471373438835, acc: 100.0, f1: 100.0, r: 0.7606469338241499
06/01/2019 11:17:29 step: 8666, epoch: 262, batch: 19, loss: 0.026548633351922035, acc: 100.0, f1: 100.0, r: 0.6361696041539765
06/01/2019 11:17:30 step: 8671, epoch: 262, batch: 24, loss: 0.008347908966243267, acc: 100.0, f1: 100.0, r: 0.7537373448935828
06/01/2019 11:17:31 step: 8676, epoch: 262, batch: 29, loss: 0.006236156914383173, acc: 100.0, f1: 100.0, r: 0.7923069251647786
06/01/2019 11:17:32 *** evaluating ***
06/01/2019 11:17:33 step: 263, epoch: 262, acc: 57.26495726495726, f1: 31.466346327093376, r: 0.2625061116786124
06/01/2019 11:17:33 *** epoch: 264 ***
06/01/2019 11:17:33 *** training ***
06/01/2019 11:17:34 step: 8684, epoch: 263, batch: 4, loss: 0.02028888836503029, acc: 100.0, f1: 100.0, r: 0.7788379371233524
06/01/2019 11:17:35 step: 8689, epoch: 263, batch: 9, loss: 0.0055052959360182285, acc: 100.0, f1: 100.0, r: 0.7012468551639379
06/01/2019 11:17:36 step: 8694, epoch: 263, batch: 14, loss: 0.0066555095836520195, acc: 100.0, f1: 100.0, r: 0.7107497635950053
06/01/2019 11:17:37 step: 8699, epoch: 263, batch: 19, loss: 0.6198082566261292, acc: 100.0, f1: 100.0, r: 0.7811086642115218
06/01/2019 11:17:38 step: 8704, epoch: 263, batch: 24, loss: 0.009215185418725014, acc: 100.0, f1: 100.0, r: 0.731329900486996
06/01/2019 11:17:39 step: 8709, epoch: 263, batch: 29, loss: 0.011453780345618725, acc: 100.0, f1: 100.0, r: 0.7568352866297806
06/01/2019 11:17:40 *** evaluating ***
06/01/2019 11:17:40 step: 264, epoch: 263, acc: 58.119658119658126, f1: 31.857193575083482, r: 0.26517283807546793
06/01/2019 11:17:40 *** epoch: 265 ***
06/01/2019 11:17:40 *** training ***
06/01/2019 11:17:41 step: 8717, epoch: 264, batch: 4, loss: 0.006838661152869463, acc: 100.0, f1: 100.0, r: 0.6930787204265748
06/01/2019 11:17:42 step: 8722, epoch: 264, batch: 9, loss: 0.6220399141311646, acc: 100.0, f1: 100.0, r: 0.7686902824390746
06/01/2019 11:17:43 step: 8727, epoch: 264, batch: 14, loss: 0.010278495959937572, acc: 100.0, f1: 100.0, r: 0.8146402911823409
06/01/2019 11:17:44 step: 8732, epoch: 264, batch: 19, loss: 0.04958067089319229, acc: 100.0, f1: 100.0, r: 0.7382894157299343
06/01/2019 11:17:45 step: 8737, epoch: 264, batch: 24, loss: 0.012868323363363743, acc: 100.0, f1: 100.0, r: 0.7023168931339805
06/01/2019 11:17:46 step: 8742, epoch: 264, batch: 29, loss: 0.014112510718405247, acc: 100.0, f1: 100.0, r: 0.6985560426135126
06/01/2019 11:17:47 *** evaluating ***
06/01/2019 11:17:47 step: 265, epoch: 264, acc: 58.97435897435898, f1: 30.982591837103545, r: 0.2702907600689054
06/01/2019 11:17:47 *** epoch: 266 ***
06/01/2019 11:17:47 *** training ***
06/01/2019 11:17:48 step: 8750, epoch: 265, batch: 4, loss: 0.009941866621375084, acc: 100.0, f1: 100.0, r: 0.688087646462159
06/01/2019 11:17:50 step: 8755, epoch: 265, batch: 9, loss: 0.017214328050613403, acc: 100.0, f1: 100.0, r: 0.7249802040595883
06/01/2019 11:17:51 step: 8760, epoch: 265, batch: 14, loss: 0.011784880422055721, acc: 100.0, f1: 100.0, r: 0.7301159900281053
06/01/2019 11:17:52 step: 8765, epoch: 265, batch: 19, loss: 0.029858022928237915, acc: 100.0, f1: 100.0, r: 0.7253335023654782
06/01/2019 11:17:53 step: 8770, epoch: 265, batch: 24, loss: 0.01011268887668848, acc: 100.0, f1: 100.0, r: 0.7414196928008212
06/01/2019 11:17:54 step: 8775, epoch: 265, batch: 29, loss: 0.004144991282373667, acc: 100.0, f1: 100.0, r: 0.6553962035435771
06/01/2019 11:17:55 *** evaluating ***
06/01/2019 11:17:55 step: 266, epoch: 265, acc: 58.54700854700855, f1: 31.980604114441423, r: 0.2654527804714033
06/01/2019 11:17:55 *** epoch: 267 ***
06/01/2019 11:17:55 *** training ***
06/01/2019 11:17:56 step: 8783, epoch: 266, batch: 4, loss: 0.024247020483016968, acc: 100.0, f1: 100.0, r: 0.8331992710838513
06/01/2019 11:17:57 step: 8788, epoch: 266, batch: 9, loss: 0.00565446587279439, acc: 100.0, f1: 100.0, r: 0.6467767155823658
06/01/2019 11:17:58 step: 8793, epoch: 266, batch: 14, loss: 0.008185960352420807, acc: 100.0, f1: 100.0, r: 0.7838544341137987
06/01/2019 11:17:59 step: 8798, epoch: 266, batch: 19, loss: 0.012375651858747005, acc: 100.0, f1: 100.0, r: 0.6133116556859114
06/01/2019 11:18:00 step: 8803, epoch: 266, batch: 24, loss: 0.025134993717074394, acc: 100.0, f1: 100.0, r: 0.7107048440627741
06/01/2019 11:18:01 step: 8808, epoch: 266, batch: 29, loss: 0.015277535654604435, acc: 100.0, f1: 100.0, r: 0.605108460580232
06/01/2019 11:18:02 *** evaluating ***
06/01/2019 11:18:02 step: 267, epoch: 266, acc: 55.98290598290598, f1: 30.272814840892643, r: 0.26508822249346614
06/01/2019 11:18:02 *** epoch: 268 ***
06/01/2019 11:18:02 *** training ***
06/01/2019 11:18:03 step: 8816, epoch: 267, batch: 4, loss: 0.6201415657997131, acc: 100.0, f1: 100.0, r: 0.810901900091323
06/01/2019 11:18:04 step: 8821, epoch: 267, batch: 9, loss: 0.012399634346365929, acc: 100.0, f1: 100.0, r: 0.7892952987904398
06/01/2019 11:18:06 step: 8826, epoch: 267, batch: 14, loss: 0.014678074046969414, acc: 100.0, f1: 100.0, r: 0.8261694286384842
06/01/2019 11:18:07 step: 8831, epoch: 267, batch: 19, loss: 0.01352770533412695, acc: 100.0, f1: 100.0, r: 0.6876642299188844
06/01/2019 11:18:08 step: 8836, epoch: 267, batch: 24, loss: 0.01269591972231865, acc: 100.0, f1: 100.0, r: 0.6488117686226362
06/01/2019 11:18:09 step: 8841, epoch: 267, batch: 29, loss: 0.011672122403979301, acc: 100.0, f1: 100.0, r: 0.672635628016916
06/01/2019 11:18:09 *** evaluating ***
06/01/2019 11:18:10 step: 268, epoch: 267, acc: 57.26495726495726, f1: 28.2024063149929, r: 0.2688059583264055
06/01/2019 11:18:10 *** epoch: 269 ***
06/01/2019 11:18:10 *** training ***
06/01/2019 11:18:11 step: 8849, epoch: 268, batch: 4, loss: 0.012206979095935822, acc: 100.0, f1: 100.0, r: 0.6651352409141348
06/01/2019 11:18:12 step: 8854, epoch: 268, batch: 9, loss: 0.026534512639045715, acc: 100.0, f1: 100.0, r: 0.8494533176564434
06/01/2019 11:18:13 step: 8859, epoch: 268, batch: 14, loss: 1.1535515785217285, acc: 100.0, f1: 100.0, r: 0.8045846643069361
06/01/2019 11:18:14 step: 8864, epoch: 268, batch: 19, loss: 0.011317496187984943, acc: 100.0, f1: 100.0, r: 0.8334206013744041
06/01/2019 11:18:15 step: 8869, epoch: 268, batch: 24, loss: 0.012677946127951145, acc: 100.0, f1: 100.0, r: 0.7195721971778617
06/01/2019 11:18:16 step: 8874, epoch: 268, batch: 29, loss: 0.011919192038476467, acc: 100.0, f1: 100.0, r: 0.7065133217457631
06/01/2019 11:18:17 *** evaluating ***
06/01/2019 11:18:17 step: 269, epoch: 268, acc: 57.26495726495726, f1: 31.31648487702399, r: 0.2654617222249232
06/01/2019 11:18:17 *** epoch: 270 ***
06/01/2019 11:18:17 *** training ***
06/01/2019 11:18:18 step: 8882, epoch: 269, batch: 4, loss: 0.013760805130004883, acc: 100.0, f1: 100.0, r: 0.6580052324957318
06/01/2019 11:18:19 step: 8887, epoch: 269, batch: 9, loss: 0.02048652619123459, acc: 100.0, f1: 100.0, r: 0.8187391768018053
06/01/2019 11:18:20 step: 8892, epoch: 269, batch: 14, loss: 0.6228642463684082, acc: 100.0, f1: 100.0, r: 0.7133512726286446
06/01/2019 11:18:21 step: 8897, epoch: 269, batch: 19, loss: 0.006756301037967205, acc: 100.0, f1: 100.0, r: 0.8114458402430887
06/01/2019 11:18:23 step: 8902, epoch: 269, batch: 24, loss: 0.01282381359487772, acc: 100.0, f1: 100.0, r: 0.7946377695630378
06/01/2019 11:18:24 step: 8907, epoch: 269, batch: 29, loss: 0.3032315969467163, acc: 100.0, f1: 100.0, r: 0.8181856076554488
06/01/2019 11:18:24 *** evaluating ***
06/01/2019 11:18:24 step: 270, epoch: 269, acc: 58.54700854700855, f1: 29.09915805653792, r: 0.26699172846550817
06/01/2019 11:18:24 *** epoch: 271 ***
06/01/2019 11:18:24 *** training ***
06/01/2019 11:18:26 step: 8915, epoch: 270, batch: 4, loss: 0.004224349278956652, acc: 100.0, f1: 100.0, r: 0.8113639060928565
06/01/2019 11:18:27 step: 8920, epoch: 270, batch: 9, loss: 0.020464811474084854, acc: 100.0, f1: 100.0, r: 0.6965105953833179
06/01/2019 11:18:28 step: 8925, epoch: 270, batch: 14, loss: 0.6165277361869812, acc: 100.0, f1: 100.0, r: 0.7635956167916254
06/01/2019 11:18:29 step: 8930, epoch: 270, batch: 19, loss: 0.019982365891337395, acc: 100.0, f1: 100.0, r: 0.7568094988001005
06/01/2019 11:18:30 step: 8935, epoch: 270, batch: 24, loss: 0.5399404764175415, acc: 100.0, f1: 100.0, r: 0.6933148871202393
06/01/2019 11:18:31 step: 8940, epoch: 270, batch: 29, loss: 0.005011899396777153, acc: 100.0, f1: 100.0, r: 0.7087191387177202
06/01/2019 11:18:32 *** evaluating ***
06/01/2019 11:18:32 step: 271, epoch: 270, acc: 57.692307692307686, f1: 28.11459689878183, r: 0.2673662571869742
06/01/2019 11:18:32 *** epoch: 272 ***
06/01/2019 11:18:32 *** training ***
06/01/2019 11:18:33 step: 8948, epoch: 271, batch: 4, loss: 0.009859813377261162, acc: 100.0, f1: 100.0, r: 0.8239814193053231
06/01/2019 11:18:34 step: 8953, epoch: 271, batch: 9, loss: 0.024799469858407974, acc: 100.0, f1: 100.0, r: 0.7443384220054877
06/01/2019 11:18:35 step: 8958, epoch: 271, batch: 14, loss: 0.01370178535580635, acc: 100.0, f1: 100.0, r: 0.6958775095700747
06/01/2019 11:18:36 step: 8963, epoch: 271, batch: 19, loss: 0.0073069422505795956, acc: 100.0, f1: 100.0, r: 0.7535076073940574
06/01/2019 11:18:38 step: 8968, epoch: 271, batch: 24, loss: 0.016199924051761627, acc: 100.0, f1: 100.0, r: 0.7932386252384325
06/01/2019 11:18:39 step: 8973, epoch: 271, batch: 29, loss: 0.5476467609405518, acc: 100.0, f1: 100.0, r: 0.6710748938267826
06/01/2019 11:18:39 *** evaluating ***
06/01/2019 11:18:39 step: 272, epoch: 271, acc: 56.837606837606835, f1: 27.635778782453723, r: 0.26950973187011457
06/01/2019 11:18:39 *** epoch: 273 ***
06/01/2019 11:18:39 *** training ***
06/01/2019 11:18:40 step: 8981, epoch: 272, batch: 4, loss: 0.00464270357042551, acc: 100.0, f1: 100.0, r: 0.6253999896651965
06/01/2019 11:18:42 step: 8986, epoch: 272, batch: 9, loss: 0.0066818175837397575, acc: 100.0, f1: 100.0, r: 0.8007262131419794
06/01/2019 11:18:43 step: 8991, epoch: 272, batch: 14, loss: 0.005969389341771603, acc: 100.0, f1: 100.0, r: 0.6963794329984166
06/01/2019 11:18:44 step: 8996, epoch: 272, batch: 19, loss: 0.008142351172864437, acc: 100.0, f1: 100.0, r: 0.7714499980970593
06/01/2019 11:18:45 step: 9001, epoch: 272, batch: 24, loss: 0.6236692667007446, acc: 100.0, f1: 100.0, r: 0.7065670798131664
06/01/2019 11:18:46 step: 9006, epoch: 272, batch: 29, loss: 0.005429517012089491, acc: 100.0, f1: 100.0, r: 0.7522137242537309
06/01/2019 11:18:47 *** evaluating ***
06/01/2019 11:18:47 step: 273, epoch: 272, acc: 58.97435897435898, f1: 32.18086930674959, r: 0.2660706839007327
06/01/2019 11:18:47 *** epoch: 274 ***
06/01/2019 11:18:47 *** training ***
06/01/2019 11:18:48 step: 9014, epoch: 273, batch: 4, loss: 0.005331564228981733, acc: 100.0, f1: 100.0, r: 0.7120333879035754
06/01/2019 11:18:49 step: 9019, epoch: 273, batch: 9, loss: 0.005512045696377754, acc: 100.0, f1: 100.0, r: 0.7893448439749522
06/01/2019 11:18:50 step: 9024, epoch: 273, batch: 14, loss: 0.009770193137228489, acc: 100.0, f1: 100.0, r: 0.7695082084627289
06/01/2019 11:18:51 step: 9029, epoch: 273, batch: 19, loss: 0.009989971294999123, acc: 100.0, f1: 100.0, r: 0.8234384568977909
06/01/2019 11:18:52 step: 9034, epoch: 273, batch: 24, loss: 0.009041959419846535, acc: 100.0, f1: 100.0, r: 0.8191064644212969
06/01/2019 11:18:54 step: 9039, epoch: 273, batch: 29, loss: 0.6182087659835815, acc: 100.0, f1: 100.0, r: 0.6988222809662887
06/01/2019 11:18:54 *** evaluating ***
06/01/2019 11:18:54 step: 274, epoch: 273, acc: 58.119658119658126, f1: 31.100427350427353, r: 0.26673944687585527
06/01/2019 11:18:54 *** epoch: 275 ***
06/01/2019 11:18:54 *** training ***
06/01/2019 11:18:55 step: 9047, epoch: 274, batch: 4, loss: 0.5388377904891968, acc: 100.0, f1: 100.0, r: 0.7367351783898807
06/01/2019 11:18:57 step: 9052, epoch: 274, batch: 9, loss: 0.024881727993488312, acc: 100.0, f1: 100.0, r: 0.7956754591231833
06/01/2019 11:18:58 step: 9057, epoch: 274, batch: 14, loss: 0.008335107937455177, acc: 100.0, f1: 100.0, r: 0.6941011927087943
06/01/2019 11:18:59 step: 9062, epoch: 274, batch: 19, loss: 0.016786836087703705, acc: 98.4375, f1: 99.28698752228165, r: 0.6719376836291693
06/01/2019 11:19:00 step: 9067, epoch: 274, batch: 24, loss: 0.007523674052208662, acc: 100.0, f1: 100.0, r: 0.6278334032398873
06/01/2019 11:19:01 step: 9072, epoch: 274, batch: 29, loss: 0.007655291352421045, acc: 100.0, f1: 100.0, r: 0.759263318049536
06/01/2019 11:19:02 *** evaluating ***
06/01/2019 11:19:02 step: 275, epoch: 274, acc: 57.26495726495726, f1: 30.028606089145192, r: 0.263114715439902
06/01/2019 11:19:02 *** epoch: 276 ***
06/01/2019 11:19:02 *** training ***
06/01/2019 11:19:03 step: 9080, epoch: 275, batch: 4, loss: 0.010666844435036182, acc: 100.0, f1: 100.0, r: 0.791960229013332
06/01/2019 11:19:04 step: 9085, epoch: 275, batch: 9, loss: 0.012986126355826855, acc: 100.0, f1: 100.0, r: 0.711476266318796
06/01/2019 11:19:05 step: 9090, epoch: 275, batch: 14, loss: 0.0082011166960001, acc: 100.0, f1: 100.0, r: 0.664274588451957
06/01/2019 11:19:06 step: 9095, epoch: 275, batch: 19, loss: 0.012490460649132729, acc: 100.0, f1: 100.0, r: 0.7337066958343911
06/01/2019 11:19:07 step: 9100, epoch: 275, batch: 24, loss: 0.007645273115485907, acc: 100.0, f1: 100.0, r: 0.6215649020550027
06/01/2019 11:19:08 step: 9105, epoch: 275, batch: 29, loss: 0.005776642356067896, acc: 100.0, f1: 100.0, r: 0.8000587703426902
06/01/2019 11:19:09 *** evaluating ***
06/01/2019 11:19:10 step: 276, epoch: 275, acc: 57.692307692307686, f1: 30.716205837173575, r: 0.26048639055986333
06/01/2019 11:19:10 *** epoch: 277 ***
06/01/2019 11:19:10 *** training ***
06/01/2019 11:19:11 step: 9113, epoch: 276, batch: 4, loss: 0.018719371408224106, acc: 100.0, f1: 100.0, r: 0.7364320723722467
06/01/2019 11:19:12 step: 9118, epoch: 276, batch: 9, loss: 0.014982198365032673, acc: 100.0, f1: 100.0, r: 0.7650914766810095
06/01/2019 11:19:13 step: 9123, epoch: 276, batch: 14, loss: 0.012019278481602669, acc: 100.0, f1: 100.0, r: 0.8298650098879149
06/01/2019 11:19:14 step: 9128, epoch: 276, batch: 19, loss: 0.01954098790884018, acc: 100.0, f1: 100.0, r: 0.6469195173929149
06/01/2019 11:19:15 step: 9133, epoch: 276, batch: 24, loss: 0.5416741371154785, acc: 100.0, f1: 100.0, r: 0.5824467902789119
06/01/2019 11:19:16 step: 9138, epoch: 276, batch: 29, loss: 0.004708115942776203, acc: 100.0, f1: 100.0, r: 0.6935533822137201
06/01/2019 11:19:17 *** evaluating ***
06/01/2019 11:19:17 step: 277, epoch: 276, acc: 58.54700854700855, f1: 30.11947941508143, r: 0.26508845024721595
06/01/2019 11:19:17 *** epoch: 278 ***
06/01/2019 11:19:17 *** training ***
06/01/2019 11:19:18 step: 9146, epoch: 277, batch: 4, loss: 0.004467635881155729, acc: 100.0, f1: 100.0, r: 0.7971604870832356
06/01/2019 11:19:19 step: 9151, epoch: 277, batch: 9, loss: 0.005796186160296202, acc: 100.0, f1: 100.0, r: 0.6501603018514508
06/01/2019 11:19:20 step: 9156, epoch: 277, batch: 14, loss: 0.0055673979222774506, acc: 100.0, f1: 100.0, r: 0.658143810838982
06/01/2019 11:19:21 step: 9161, epoch: 277, batch: 19, loss: 0.013183743692934513, acc: 100.0, f1: 100.0, r: 0.7069357092704553
06/01/2019 11:19:22 step: 9166, epoch: 277, batch: 24, loss: 0.009812666103243828, acc: 100.0, f1: 100.0, r: 0.7055074592331286
06/01/2019 11:19:24 step: 9171, epoch: 277, batch: 29, loss: 0.013347534462809563, acc: 100.0, f1: 100.0, r: 0.7509444234376004
06/01/2019 11:19:24 *** evaluating ***
06/01/2019 11:19:24 step: 278, epoch: 277, acc: 58.54700854700855, f1: 32.089417704236176, r: 0.2677349237420198
06/01/2019 11:19:24 *** epoch: 279 ***
06/01/2019 11:19:24 *** training ***
06/01/2019 11:19:25 step: 9179, epoch: 278, batch: 4, loss: 0.014946963638067245, acc: 100.0, f1: 100.0, r: 0.7640520148944582
06/01/2019 11:19:26 step: 9184, epoch: 278, batch: 9, loss: 0.32230955362319946, acc: 98.4375, f1: 97.72727272727273, r: 0.7255920359387795
06/01/2019 11:19:28 step: 9189, epoch: 278, batch: 14, loss: 0.006919967010617256, acc: 100.0, f1: 100.0, r: 0.6613172371270808
06/01/2019 11:19:29 step: 9194, epoch: 278, batch: 19, loss: 0.00645857211202383, acc: 100.0, f1: 100.0, r: 0.7144740443739428
06/01/2019 11:19:30 step: 9199, epoch: 278, batch: 24, loss: 0.016563287004828453, acc: 100.0, f1: 100.0, r: 0.7977145678951842
06/01/2019 11:19:31 step: 9204, epoch: 278, batch: 29, loss: 0.008169401437044144, acc: 100.0, f1: 100.0, r: 0.8271394354414202
06/01/2019 11:19:31 *** evaluating ***
06/01/2019 11:19:32 step: 279, epoch: 278, acc: 58.54700854700855, f1: 29.334203607815724, r: 0.2671341796066286
06/01/2019 11:19:32 *** epoch: 280 ***
06/01/2019 11:19:32 *** training ***
06/01/2019 11:19:33 step: 9212, epoch: 279, batch: 4, loss: 0.00839301198720932, acc: 100.0, f1: 100.0, r: 0.8250779005214303
06/01/2019 11:19:34 step: 9217, epoch: 279, batch: 9, loss: 0.01059391163289547, acc: 100.0, f1: 100.0, r: 0.6493506879870155
06/01/2019 11:19:35 step: 9222, epoch: 279, batch: 14, loss: 0.004669128451496363, acc: 100.0, f1: 100.0, r: 0.693871104686236
06/01/2019 11:19:36 step: 9227, epoch: 279, batch: 19, loss: 0.009473162703216076, acc: 100.0, f1: 100.0, r: 0.8564220302901524
06/01/2019 11:19:37 step: 9232, epoch: 279, batch: 24, loss: 0.5367043018341064, acc: 100.0, f1: 100.0, r: 0.7425308468951901
06/01/2019 11:19:38 step: 9237, epoch: 279, batch: 29, loss: 0.0076997303403913975, acc: 100.0, f1: 100.0, r: 0.7218466855464569
06/01/2019 11:19:39 *** evaluating ***
06/01/2019 11:19:39 step: 280, epoch: 279, acc: 57.26495726495726, f1: 31.402016699365788, r: 0.2651162532050462
06/01/2019 11:19:39 *** epoch: 281 ***
06/01/2019 11:19:39 *** training ***
06/01/2019 11:19:40 step: 9245, epoch: 280, batch: 4, loss: 0.007086668163537979, acc: 100.0, f1: 100.0, r: 0.7635369515136897
06/01/2019 11:19:41 step: 9250, epoch: 280, batch: 9, loss: 0.0055155763402581215, acc: 100.0, f1: 100.0, r: 0.6415647342563114
06/01/2019 11:19:42 step: 9255, epoch: 280, batch: 14, loss: 0.01040400005877018, acc: 100.0, f1: 100.0, r: 0.6877002116914569
06/01/2019 11:19:43 step: 9260, epoch: 280, batch: 19, loss: 0.010153070092201233, acc: 100.0, f1: 100.0, r: 0.6730506878568142
06/01/2019 11:19:44 step: 9265, epoch: 280, batch: 24, loss: 0.02766541577875614, acc: 100.0, f1: 100.0, r: 0.6779943685922198
06/01/2019 11:19:45 step: 9270, epoch: 280, batch: 29, loss: 0.005383002571761608, acc: 100.0, f1: 100.0, r: 0.7172735185759047
06/01/2019 11:19:46 *** evaluating ***
06/01/2019 11:19:46 step: 281, epoch: 280, acc: 58.97435897435898, f1: 30.017995937113582, r: 0.2638198349568554
06/01/2019 11:19:46 *** epoch: 282 ***
06/01/2019 11:19:46 *** training ***
06/01/2019 11:19:47 step: 9278, epoch: 281, batch: 4, loss: 0.005923840682953596, acc: 100.0, f1: 100.0, r: 0.8257204828365634
06/01/2019 11:19:48 step: 9283, epoch: 281, batch: 9, loss: 0.005724097602069378, acc: 100.0, f1: 100.0, r: 0.80638278022905
06/01/2019 11:19:49 step: 9288, epoch: 281, batch: 14, loss: 0.0027188281528651714, acc: 100.0, f1: 100.0, r: 0.7209316618853456
06/01/2019 11:19:50 step: 9293, epoch: 281, batch: 19, loss: 0.016974467784166336, acc: 100.0, f1: 100.0, r: 0.676014209974251
06/01/2019 11:19:51 step: 9298, epoch: 281, batch: 24, loss: 0.01061742939054966, acc: 100.0, f1: 100.0, r: 0.7967411831305636
06/01/2019 11:19:53 step: 9303, epoch: 281, batch: 29, loss: 0.005239523947238922, acc: 100.0, f1: 100.0, r: 0.6715215305166904
06/01/2019 11:19:53 *** evaluating ***
06/01/2019 11:19:54 step: 282, epoch: 281, acc: 58.54700854700855, f1: 31.943001443001446, r: 0.26646623941639513
06/01/2019 11:19:54 *** epoch: 283 ***
06/01/2019 11:19:54 *** training ***
06/01/2019 11:19:55 step: 9311, epoch: 282, batch: 4, loss: 0.02234187349677086, acc: 100.0, f1: 100.0, r: 0.6912669226383468
06/01/2019 11:19:56 step: 9316, epoch: 282, batch: 9, loss: 0.005170401651412249, acc: 100.0, f1: 100.0, r: 0.79069217527743
06/01/2019 11:19:57 step: 9321, epoch: 282, batch: 14, loss: 0.004949362948536873, acc: 100.0, f1: 100.0, r: 0.6968068131001328
06/01/2019 11:19:58 step: 9326, epoch: 282, batch: 19, loss: 0.006151398178189993, acc: 100.0, f1: 100.0, r: 0.7997317651266058
06/01/2019 11:19:59 step: 9331, epoch: 282, batch: 24, loss: 0.005017101764678955, acc: 100.0, f1: 100.0, r: 0.7876441158383987
06/01/2019 11:20:00 step: 9336, epoch: 282, batch: 29, loss: 0.016080815345048904, acc: 100.0, f1: 100.0, r: 0.6962422880199695
06/01/2019 11:20:01 *** evaluating ***
06/01/2019 11:20:01 step: 283, epoch: 282, acc: 57.692307692307686, f1: 31.325735583750934, r: 0.26325328103450285
06/01/2019 11:20:01 *** epoch: 284 ***
06/01/2019 11:20:01 *** training ***
06/01/2019 11:20:02 step: 9344, epoch: 283, batch: 4, loss: 0.008332984521985054, acc: 100.0, f1: 100.0, r: 0.7213802926035313
06/01/2019 11:20:03 step: 9349, epoch: 283, batch: 9, loss: 0.012811443768441677, acc: 100.0, f1: 100.0, r: 0.6722395236154178
06/01/2019 11:20:04 step: 9354, epoch: 283, batch: 14, loss: 0.024999232962727547, acc: 100.0, f1: 100.0, r: 0.7894335119633399
06/01/2019 11:20:05 step: 9359, epoch: 283, batch: 19, loss: 0.008600620552897453, acc: 100.0, f1: 100.0, r: 0.7733672227074075
06/01/2019 11:20:06 step: 9364, epoch: 283, batch: 24, loss: 0.006750359199941158, acc: 100.0, f1: 100.0, r: 0.7723380551960524
06/01/2019 11:20:07 step: 9369, epoch: 283, batch: 29, loss: 0.006896710954606533, acc: 100.0, f1: 100.0, r: 0.7155479749646083
06/01/2019 11:20:08 *** evaluating ***
06/01/2019 11:20:08 step: 284, epoch: 283, acc: 58.97435897435898, f1: 29.868063431730167, r: 0.26710651590279266
06/01/2019 11:20:08 *** epoch: 285 ***
06/01/2019 11:20:08 *** training ***
06/01/2019 11:20:09 step: 9377, epoch: 284, batch: 4, loss: 0.016548562794923782, acc: 100.0, f1: 100.0, r: 0.6927728706150103
06/01/2019 11:20:10 step: 9382, epoch: 284, batch: 9, loss: 0.00910040270537138, acc: 100.0, f1: 100.0, r: 0.7575148310026132
06/01/2019 11:20:11 step: 9387, epoch: 284, batch: 14, loss: 0.03733687847852707, acc: 98.4375, f1: 98.88682745825604, r: 0.705604422144462
06/01/2019 11:20:13 step: 9392, epoch: 284, batch: 19, loss: 0.038370247930288315, acc: 98.4375, f1: 98.20868786386028, r: 0.7069406008371528
06/01/2019 11:20:14 step: 9397, epoch: 284, batch: 24, loss: 0.0061421883292496204, acc: 100.0, f1: 100.0, r: 0.708290685520732
06/01/2019 11:20:14 step: 9402, epoch: 284, batch: 29, loss: 0.6149685382843018, acc: 100.0, f1: 100.0, r: 0.8336198255994994
06/01/2019 11:20:15 *** evaluating ***
06/01/2019 11:20:15 step: 285, epoch: 284, acc: 58.119658119658126, f1: 31.304975582516718, r: 0.2645020639374362
06/01/2019 11:20:15 *** epoch: 286 ***
06/01/2019 11:20:15 *** training ***
06/01/2019 11:20:17 step: 9410, epoch: 285, batch: 4, loss: 0.014339451678097248, acc: 100.0, f1: 100.0, r: 0.7875248500789396
06/01/2019 11:20:18 step: 9415, epoch: 285, batch: 9, loss: 0.008933914825320244, acc: 100.0, f1: 100.0, r: 0.8137809269362729
06/01/2019 11:20:19 step: 9420, epoch: 285, batch: 14, loss: 0.02163669839501381, acc: 100.0, f1: 100.0, r: 0.7964340625036286
06/01/2019 11:20:20 step: 9425, epoch: 285, batch: 19, loss: 0.0570799820125103, acc: 98.4375, f1: 95.40229885057472, r: 0.749347061456642
06/01/2019 11:20:21 step: 9430, epoch: 285, batch: 24, loss: 0.007442569825798273, acc: 100.0, f1: 100.0, r: 0.7977615671668699
06/01/2019 11:20:22 step: 9435, epoch: 285, batch: 29, loss: 0.007676321547478437, acc: 100.0, f1: 100.0, r: 0.8192516308426102
06/01/2019 11:20:23 *** evaluating ***
06/01/2019 11:20:23 step: 286, epoch: 285, acc: 57.692307692307686, f1: 31.32712568526368, r: 0.26322930849393544
06/01/2019 11:20:23 *** epoch: 287 ***
06/01/2019 11:20:23 *** training ***
06/01/2019 11:20:24 step: 9443, epoch: 286, batch: 4, loss: 0.6192889213562012, acc: 100.0, f1: 100.0, r: 0.7888098002044844
06/01/2019 11:20:25 step: 9448, epoch: 286, batch: 9, loss: 0.013160270638763905, acc: 100.0, f1: 100.0, r: 0.6828883937147529
06/01/2019 11:20:26 step: 9453, epoch: 286, batch: 14, loss: 0.010967107489705086, acc: 100.0, f1: 100.0, r: 0.6379107640833536
06/01/2019 11:20:27 step: 9458, epoch: 286, batch: 19, loss: 0.009310356341302395, acc: 100.0, f1: 100.0, r: 0.6827262228129393
06/01/2019 11:20:28 step: 9463, epoch: 286, batch: 24, loss: 0.03364934027194977, acc: 100.0, f1: 100.0, r: 0.7515938036208751
06/01/2019 11:20:29 step: 9468, epoch: 286, batch: 29, loss: 0.004960182122886181, acc: 100.0, f1: 100.0, r: 0.6818180757269554
06/01/2019 11:20:30 *** evaluating ***
06/01/2019 11:20:30 step: 287, epoch: 286, acc: 58.119658119658126, f1: 31.600767618569474, r: 0.26715638799325725
06/01/2019 11:20:30 *** epoch: 288 ***
06/01/2019 11:20:30 *** training ***
06/01/2019 11:20:31 step: 9476, epoch: 287, batch: 4, loss: 0.5388154983520508, acc: 100.0, f1: 100.0, r: 0.7728284444772577
06/01/2019 11:20:32 step: 9481, epoch: 287, batch: 9, loss: 0.0076265414245426655, acc: 100.0, f1: 100.0, r: 0.7395752046719332
06/01/2019 11:20:33 step: 9486, epoch: 287, batch: 14, loss: 0.010951346717774868, acc: 100.0, f1: 100.0, r: 0.7773090264884327
06/01/2019 11:20:34 step: 9491, epoch: 287, batch: 19, loss: 0.01898414082825184, acc: 98.4375, f1: 98.62433862433862, r: 0.5394727799478038
06/01/2019 11:20:36 step: 9496, epoch: 287, batch: 24, loss: 0.011772768571972847, acc: 100.0, f1: 100.0, r: 0.7835262510177805
06/01/2019 11:20:37 step: 9501, epoch: 287, batch: 29, loss: 0.013036483898758888, acc: 100.0, f1: 100.0, r: 0.7751919193805841
06/01/2019 11:20:37 *** evaluating ***
06/01/2019 11:20:38 step: 288, epoch: 287, acc: 57.692307692307686, f1: 31.58384127541539, r: 0.26217028286306227
06/01/2019 11:20:38 *** epoch: 289 ***
06/01/2019 11:20:38 *** training ***
06/01/2019 11:20:39 step: 9509, epoch: 288, batch: 4, loss: 0.006007232703268528, acc: 100.0, f1: 100.0, r: 0.8027728971901339
06/01/2019 11:20:40 step: 9514, epoch: 288, batch: 9, loss: 0.023840826004743576, acc: 100.0, f1: 100.0, r: 0.6368099580274209
06/01/2019 11:20:41 step: 9519, epoch: 288, batch: 14, loss: 0.01332705095410347, acc: 100.0, f1: 100.0, r: 0.7808727507894435
06/01/2019 11:20:42 step: 9524, epoch: 288, batch: 19, loss: 0.014249398373067379, acc: 100.0, f1: 100.0, r: 0.7764356763305654
06/01/2019 11:20:43 step: 9529, epoch: 288, batch: 24, loss: 0.0031487601809203625, acc: 100.0, f1: 100.0, r: 0.7135185991380062
06/01/2019 11:20:44 step: 9534, epoch: 288, batch: 29, loss: 0.007846180349588394, acc: 100.0, f1: 100.0, r: 0.6745360613318762
06/01/2019 11:20:45 *** evaluating ***
06/01/2019 11:20:45 step: 289, epoch: 288, acc: 59.401709401709404, f1: 29.288675083344806, r: 0.2655028757923372
06/01/2019 11:20:45 *** epoch: 290 ***
06/01/2019 11:20:45 *** training ***
06/01/2019 11:20:46 step: 9542, epoch: 289, batch: 4, loss: 0.019221827387809753, acc: 100.0, f1: 100.0, r: 0.6401741583358397
06/01/2019 11:20:47 step: 9547, epoch: 289, batch: 9, loss: 0.008199779316782951, acc: 100.0, f1: 100.0, r: 0.6466445705972035
06/01/2019 11:20:49 step: 9552, epoch: 289, batch: 14, loss: 0.012974853627383709, acc: 100.0, f1: 100.0, r: 0.6881282983318575
06/01/2019 11:20:50 step: 9557, epoch: 289, batch: 19, loss: 0.007098175119608641, acc: 100.0, f1: 100.0, r: 0.8126637030453748
06/01/2019 11:20:51 step: 9562, epoch: 289, batch: 24, loss: 0.03390731289982796, acc: 100.0, f1: 100.0, r: 0.8194041085891326
06/01/2019 11:20:52 step: 9567, epoch: 289, batch: 29, loss: 0.6179052591323853, acc: 100.0, f1: 100.0, r: 0.7037773854634185
06/01/2019 11:20:52 *** evaluating ***
06/01/2019 11:20:52 step: 290, epoch: 289, acc: 58.54700854700855, f1: 31.566765224836974, r: 0.26664923438513294
06/01/2019 11:20:52 *** epoch: 291 ***
06/01/2019 11:20:52 *** training ***
06/01/2019 11:20:53 step: 9575, epoch: 290, batch: 4, loss: 0.01178597379475832, acc: 100.0, f1: 100.0, r: 0.6092590093036386
06/01/2019 11:20:54 step: 9580, epoch: 290, batch: 9, loss: 0.010399168357253075, acc: 100.0, f1: 100.0, r: 0.7067151594578082
06/01/2019 11:20:55 step: 9585, epoch: 290, batch: 14, loss: 0.02345980331301689, acc: 98.4375, f1: 98.29313543599258, r: 0.7415606867635467
06/01/2019 11:20:56 step: 9590, epoch: 290, batch: 19, loss: 0.00883814413100481, acc: 100.0, f1: 100.0, r: 0.770827552493533
06/01/2019 11:20:57 step: 9595, epoch: 290, batch: 24, loss: 0.022075239568948746, acc: 100.0, f1: 100.0, r: 0.7485835648451555
06/01/2019 11:20:58 step: 9600, epoch: 290, batch: 29, loss: 0.00992656871676445, acc: 100.0, f1: 100.0, r: 0.8044274742783593
06/01/2019 11:20:59 *** evaluating ***
06/01/2019 11:20:59 step: 291, epoch: 290, acc: 57.26495726495726, f1: 28.533695795632852, r: 0.26511913025101497
06/01/2019 11:20:59 *** epoch: 292 ***
06/01/2019 11:20:59 *** training ***
06/01/2019 11:21:00 step: 9608, epoch: 291, batch: 4, loss: 0.010197220370173454, acc: 100.0, f1: 100.0, r: 0.7760096124155089
06/01/2019 11:21:02 step: 9613, epoch: 291, batch: 9, loss: 0.007394404616206884, acc: 100.0, f1: 100.0, r: 0.7932685388269669
06/01/2019 11:21:03 step: 9618, epoch: 291, batch: 14, loss: 0.008390570059418678, acc: 100.0, f1: 100.0, r: 0.8174416695774136
06/01/2019 11:21:04 step: 9623, epoch: 291, batch: 19, loss: 0.008001653477549553, acc: 100.0, f1: 100.0, r: 0.7193562938488486
06/01/2019 11:21:05 step: 9628, epoch: 291, batch: 24, loss: 0.009665747173130512, acc: 100.0, f1: 100.0, r: 0.7825288165213986
06/01/2019 11:21:06 step: 9633, epoch: 291, batch: 29, loss: 0.024122120812535286, acc: 100.0, f1: 100.0, r: 0.7071044943845977
06/01/2019 11:21:07 *** evaluating ***
06/01/2019 11:21:07 step: 292, epoch: 291, acc: 57.26495726495726, f1: 27.843884288112925, r: 0.26669778120842796
06/01/2019 11:21:07 *** epoch: 293 ***
06/01/2019 11:21:07 *** training ***
06/01/2019 11:21:08 step: 9641, epoch: 292, batch: 4, loss: 0.006476140581071377, acc: 100.0, f1: 100.0, r: 0.8267215691079131
06/01/2019 11:21:09 step: 9646, epoch: 292, batch: 9, loss: 0.02301677316427231, acc: 100.0, f1: 100.0, r: 0.772457370863168
06/01/2019 11:21:10 step: 9651, epoch: 292, batch: 14, loss: 0.004485736135393381, acc: 100.0, f1: 100.0, r: 0.6675379121071126
06/01/2019 11:21:11 step: 9656, epoch: 292, batch: 19, loss: 0.009862980805337429, acc: 100.0, f1: 100.0, r: 0.818375476384349
06/01/2019 11:21:12 step: 9661, epoch: 292, batch: 24, loss: 0.014244360849261284, acc: 100.0, f1: 100.0, r: 0.7444179591378193
06/01/2019 11:21:14 step: 9666, epoch: 292, batch: 29, loss: 0.004986790474504232, acc: 100.0, f1: 100.0, r: 0.7130587322992441
06/01/2019 11:21:14 *** evaluating ***
06/01/2019 11:21:14 step: 293, epoch: 292, acc: 57.692307692307686, f1: 27.932560265620097, r: 0.2685822489781451
06/01/2019 11:21:14 *** epoch: 294 ***
06/01/2019 11:21:14 *** training ***
06/01/2019 11:21:15 step: 9674, epoch: 293, batch: 4, loss: 0.6247429847717285, acc: 100.0, f1: 100.0, r: 0.840746856496916
06/01/2019 11:21:17 step: 9679, epoch: 293, batch: 9, loss: 0.017876682803034782, acc: 100.0, f1: 100.0, r: 0.8108286381620599
06/01/2019 11:21:18 step: 9684, epoch: 293, batch: 14, loss: 0.30087172985076904, acc: 100.0, f1: 100.0, r: 0.7764177759850026
06/01/2019 11:21:19 step: 9689, epoch: 293, batch: 19, loss: 0.043111804872751236, acc: 98.4375, f1: 86.76470588235294, r: 0.7633428278335214
06/01/2019 11:21:20 step: 9694, epoch: 293, batch: 24, loss: 0.01394426915794611, acc: 100.0, f1: 100.0, r: 0.7887553252168842
06/01/2019 11:21:21 step: 9699, epoch: 293, batch: 29, loss: 0.010010297410190105, acc: 100.0, f1: 100.0, r: 0.6579971678359091
06/01/2019 11:21:22 *** evaluating ***
06/01/2019 11:21:22 step: 294, epoch: 293, acc: 56.41025641025641, f1: 27.177061906125942, r: 0.2679654743376152
06/01/2019 11:21:22 *** epoch: 295 ***
06/01/2019 11:21:22 *** training ***
06/01/2019 11:21:23 step: 9707, epoch: 294, batch: 4, loss: 0.00605600792914629, acc: 100.0, f1: 100.0, r: 0.7886058727166736
06/01/2019 11:21:25 step: 9712, epoch: 294, batch: 9, loss: 0.00852155964821577, acc: 100.0, f1: 100.0, r: 0.8165952229705719
06/01/2019 11:21:26 step: 9717, epoch: 294, batch: 14, loss: 0.008394340053200722, acc: 100.0, f1: 100.0, r: 0.670301537312036
06/01/2019 11:21:27 step: 9722, epoch: 294, batch: 19, loss: 0.016102630645036697, acc: 100.0, f1: 100.0, r: 0.7947435190225308
06/01/2019 11:21:28 step: 9727, epoch: 294, batch: 24, loss: 0.00733332009986043, acc: 100.0, f1: 100.0, r: 0.6727797418974768
06/01/2019 11:21:29 step: 9732, epoch: 294, batch: 29, loss: 0.6108106970787048, acc: 100.0, f1: 100.0, r: 0.670048502830332
06/01/2019 11:21:30 *** evaluating ***
06/01/2019 11:21:30 step: 295, epoch: 294, acc: 57.692307692307686, f1: 31.435078121385924, r: 0.2625232020053524
06/01/2019 11:21:30 *** epoch: 296 ***
06/01/2019 11:21:30 *** training ***
06/01/2019 11:21:31 step: 9740, epoch: 295, batch: 4, loss: 0.009184281341731548, acc: 100.0, f1: 100.0, r: 0.6895897271463226
06/01/2019 11:21:32 step: 9745, epoch: 295, batch: 9, loss: 0.009435821324586868, acc: 100.0, f1: 100.0, r: 0.7535534271476472
06/01/2019 11:21:33 step: 9750, epoch: 295, batch: 14, loss: 0.0102410688996315, acc: 100.0, f1: 100.0, r: 0.8017433438955046
06/01/2019 11:21:34 step: 9755, epoch: 295, batch: 19, loss: 0.013846302404999733, acc: 100.0, f1: 100.0, r: 0.8129950292594372
06/01/2019 11:21:35 step: 9760, epoch: 295, batch: 24, loss: 0.005156148225069046, acc: 100.0, f1: 100.0, r: 0.7090168951409388
06/01/2019 11:21:37 step: 9765, epoch: 295, batch: 29, loss: 0.014687328599393368, acc: 100.0, f1: 100.0, r: 0.7843912378844742
06/01/2019 11:21:37 *** evaluating ***
06/01/2019 11:21:38 step: 296, epoch: 295, acc: 58.97435897435898, f1: 31.729086082505486, r: 0.2660410011741251
06/01/2019 11:21:38 *** epoch: 297 ***
06/01/2019 11:21:38 *** training ***
06/01/2019 11:21:39 step: 9773, epoch: 296, batch: 4, loss: 0.009055768139660358, acc: 100.0, f1: 100.0, r: 0.6831399170811079
06/01/2019 11:21:40 step: 9778, epoch: 296, batch: 9, loss: 0.017380857840180397, acc: 100.0, f1: 100.0, r: 0.7001939314690745
06/01/2019 11:21:41 step: 9783, epoch: 296, batch: 14, loss: 0.032843805849552155, acc: 100.0, f1: 100.0, r: 0.7711364348939765
06/01/2019 11:21:42 step: 9788, epoch: 296, batch: 19, loss: 0.008921227417886257, acc: 100.0, f1: 100.0, r: 0.6005476617054429
06/01/2019 11:21:43 step: 9793, epoch: 296, batch: 24, loss: 0.004466047510504723, acc: 100.0, f1: 100.0, r: 0.7831182519841284
06/01/2019 11:21:44 step: 9798, epoch: 296, batch: 29, loss: 0.00750892935320735, acc: 100.0, f1: 100.0, r: 0.7361354741485858
06/01/2019 11:21:45 *** evaluating ***
06/01/2019 11:21:45 step: 297, epoch: 296, acc: 58.119658119658126, f1: 28.925907037635817, r: 0.26628560103675836
06/01/2019 11:21:45 *** epoch: 298 ***
06/01/2019 11:21:45 *** training ***
06/01/2019 11:21:46 step: 9806, epoch: 297, batch: 4, loss: 0.010325226001441479, acc: 100.0, f1: 100.0, r: 0.6904502866477328
06/01/2019 11:21:48 step: 9811, epoch: 297, batch: 9, loss: 0.611221194267273, acc: 100.0, f1: 100.0, r: 0.6292564639254677
06/01/2019 11:21:49 step: 9816, epoch: 297, batch: 14, loss: 0.010704644955694675, acc: 100.0, f1: 100.0, r: 0.663076261373024
06/01/2019 11:21:50 step: 9821, epoch: 297, batch: 19, loss: 0.01008558738976717, acc: 100.0, f1: 100.0, r: 0.7568352636663822
06/01/2019 11:21:51 step: 9826, epoch: 297, batch: 24, loss: 0.022531501948833466, acc: 100.0, f1: 100.0, r: 0.7266550776562261
06/01/2019 11:21:52 step: 9831, epoch: 297, batch: 29, loss: 0.007928742095828056, acc: 100.0, f1: 100.0, r: 0.7723108577475767
06/01/2019 11:21:52 *** evaluating ***
06/01/2019 11:21:53 step: 298, epoch: 297, acc: 57.692307692307686, f1: 31.316048591504032, r: 0.2670735742350991
06/01/2019 11:21:53 *** epoch: 299 ***
06/01/2019 11:21:53 *** training ***
06/01/2019 11:21:54 step: 9839, epoch: 298, batch: 4, loss: 0.010112077929079533, acc: 100.0, f1: 100.0, r: 0.7132307030990838
06/01/2019 11:21:55 step: 9844, epoch: 298, batch: 9, loss: 0.016764629632234573, acc: 100.0, f1: 100.0, r: 0.6598685932921677
06/01/2019 11:21:56 step: 9849, epoch: 298, batch: 14, loss: 0.006281421519815922, acc: 100.0, f1: 100.0, r: 0.7572254905070072
06/01/2019 11:21:57 step: 9854, epoch: 298, batch: 19, loss: 0.008605641312897205, acc: 100.0, f1: 100.0, r: 0.7037659522211699
06/01/2019 11:21:58 step: 9859, epoch: 298, batch: 24, loss: 0.00781851913779974, acc: 100.0, f1: 100.0, r: 0.7025389153567076
06/01/2019 11:21:59 step: 9864, epoch: 298, batch: 29, loss: 0.0090975072234869, acc: 100.0, f1: 100.0, r: 0.6945758157789812
06/01/2019 11:22:00 *** evaluating ***
06/01/2019 11:22:00 step: 299, epoch: 298, acc: 58.97435897435898, f1: 32.44678814737496, r: 0.2653907611683018
06/01/2019 11:22:00 *** epoch: 300 ***
06/01/2019 11:22:00 *** training ***
06/01/2019 11:22:01 step: 9872, epoch: 299, batch: 4, loss: 0.2954721450805664, acc: 100.0, f1: 100.0, r: 0.7967677139704377
06/01/2019 11:22:02 step: 9877, epoch: 299, batch: 9, loss: 0.007045682519674301, acc: 100.0, f1: 100.0, r: 0.7653894206827053
06/01/2019 11:22:03 step: 9882, epoch: 299, batch: 14, loss: 0.02063441276550293, acc: 100.0, f1: 100.0, r: 0.8190214101094887
06/01/2019 11:22:04 step: 9887, epoch: 299, batch: 19, loss: 0.020997140556573868, acc: 100.0, f1: 100.0, r: 0.7153397008180881
06/01/2019 11:22:05 step: 9892, epoch: 299, batch: 24, loss: 0.019749704748392105, acc: 100.0, f1: 100.0, r: 0.7311808022439912
06/01/2019 11:22:07 step: 9897, epoch: 299, batch: 29, loss: 0.01847616210579872, acc: 100.0, f1: 100.0, r: 0.6846265530713445
06/01/2019 11:22:07 *** evaluating ***
06/01/2019 11:22:07 step: 300, epoch: 299, acc: 58.119658119658126, f1: 30.841249280789185, r: 0.26931946335386275
06/01/2019 11:22:07 
*** Best acc model ***
epoch: 113
acc: 61.111111111111114
f1: 34.11317065728831
corr: 0.2686276917328143
06/01/2019 11:22:07 Loading Test Data
06/01/2019 11:22:07 load data from data/word2vec_temp/test_text.npy, data/word2vec_temp/test_label.npy, training: False
06/01/2019 11:22:27 loaded. total len: 2228
06/01/2019 11:22:27 Test: length: 2228, total batch: 35, batch size: 64
06/01/2019 11:22:27 
*** Test Result ***
acc: 58.119658119658126
f1: 30.841249280789185
corr: 0.26931946335386275
