06/02/2019 10:50:31 {'input_path': 'data/elmo_temp', 'output_path': 'save/cnn_6', 'gpu': True, 'seed': 20000125, 'display_per_batch': 5, 'optimizer': 'adagrad', 'lr': 0.01, 'lr_decay': 0, 'weight_decay': 0.0001, 'momentum': 0.985, 'num_epochs': 300, 'batch_size': 64, 'num_labels': 8, 'embedding_size': 1024, 'type': 'cnn', 'cnn': {'max_length': 512, 'conv_1': {'size': 512, 'kernel_size': 3, 'dropout': 0.9}, 'max_pool_1': {'kernel_size': 2, 'stride': 2}, 'fc': {'hidden_size': 512, 'dropout': 0.9}, 'loss': 'cross_entropy'}}
06/02/2019 10:50:31 Loading Train Data
06/02/2019 10:50:31 load data from data/elmo_temp/train_text.npy, data/elmo_temp/train_label.npy, training: True
06/02/2019 10:51:13 loaded. total len: 2342
06/02/2019 10:51:13 Train: length: 2108, total batch: 33, batch size: 64
06/02/2019 10:51:13 Dev: length: 234, total batch: 4, batch size: 64
06/02/2019 10:51:13 Loading model cnn
06/02/2019 10:51:23 *** epoch: 1 ***
06/02/2019 10:51:23 *** training ***
06/02/2019 10:51:24 step: 5, epoch: 0, batch: 4, loss: 2.1754987239837646, acc: 9.375, f1: 7.453677035330261, r: -0.07053191452114613
06/02/2019 10:51:25 step: 10, epoch: 0, batch: 9, loss: 1.982008934020996, acc: 23.4375, f1: 13.83426097711812, r: 0.006451360730895214
06/02/2019 10:51:25 step: 15, epoch: 0, batch: 14, loss: 1.7194141149520874, acc: 37.5, f1: 14.343434343434346, r: 0.09679327354746899
06/02/2019 10:51:26 step: 20, epoch: 0, batch: 19, loss: 1.5900578498840332, acc: 37.5, f1: 10.17857142857143, r: 0.2393205747831863
06/02/2019 10:51:26 step: 25, epoch: 0, batch: 24, loss: 1.8228633403778076, acc: 35.9375, f1: 13.202075702075703, r: 0.029414327866787958
06/02/2019 10:51:27 step: 30, epoch: 0, batch: 29, loss: 1.9417203664779663, acc: 35.9375, f1: 15.136035824936863, r: 0.05543204758705721
06/02/2019 10:51:27 *** evaluating ***
06/02/2019 10:51:27 step: 1, epoch: 0, acc: 45.2991452991453, f1: 8.716898716898717, r: 0.1111390524036416
06/02/2019 10:51:27 *** epoch: 2 ***
06/02/2019 10:51:27 *** training ***
06/02/2019 10:51:28 step: 38, epoch: 1, batch: 4, loss: 1.7422491312026978, acc: 31.25, f1: 11.407400488799636, r: 0.0692430624258944
06/02/2019 10:51:28 step: 43, epoch: 1, batch: 9, loss: 1.691521167755127, acc: 34.375, f1: 19.148946291803433, r: 0.09547434191511406
06/02/2019 10:51:29 step: 48, epoch: 1, batch: 14, loss: 1.7991812229156494, acc: 29.6875, f1: 11.96969696969697, r: 0.06749065036256316
06/02/2019 10:51:29 step: 53, epoch: 1, batch: 19, loss: 1.8824992179870605, acc: 28.125, f1: 8.005698005698006, r: 0.03581623344538537
06/02/2019 10:51:30 step: 58, epoch: 1, batch: 24, loss: 1.8216328620910645, acc: 29.6875, f1: 9.39220515792524, r: 0.07703544649593408
06/02/2019 10:51:30 step: 63, epoch: 1, batch: 29, loss: 1.798018217086792, acc: 40.625, f1: 11.315789473684209, r: 0.1017688407537681
06/02/2019 10:51:31 *** evaluating ***
06/02/2019 10:51:31 step: 2, epoch: 1, acc: 46.58119658119658, f1: 9.684899560078444, r: 0.18586335648084656
06/02/2019 10:51:31 *** epoch: 3 ***
06/02/2019 10:51:31 *** training ***
06/02/2019 10:51:32 step: 71, epoch: 2, batch: 4, loss: 1.902575969696045, acc: 37.5, f1: 12.58041958041958, r: 0.15820316338628165
06/02/2019 10:51:32 step: 76, epoch: 2, batch: 9, loss: 1.7460492849349976, acc: 43.75, f1: 16.020378874856487, r: 0.13424657680224436
06/02/2019 10:51:33 step: 81, epoch: 2, batch: 14, loss: 1.8427658081054688, acc: 37.5, f1: 16.668380638968873, r: 0.06407714036517885
06/02/2019 10:51:33 step: 86, epoch: 2, batch: 19, loss: 1.7074763774871826, acc: 35.9375, f1: 12.219469127363864, r: 0.11625022841149577
06/02/2019 10:51:34 step: 91, epoch: 2, batch: 24, loss: 2.060962200164795, acc: 40.625, f1: 14.532428355957766, r: 0.0685530022492936
06/02/2019 10:51:34 step: 96, epoch: 2, batch: 29, loss: 1.5584403276443481, acc: 39.0625, f1: 13.877551020408161, r: 0.18053823715025558
06/02/2019 10:51:35 *** evaluating ***
06/02/2019 10:51:35 step: 3, epoch: 2, acc: 52.56410256410257, f1: 15.690497810063029, r: 0.1535580181561834
06/02/2019 10:51:35 *** epoch: 4 ***
06/02/2019 10:51:35 *** training ***
06/02/2019 10:51:35 step: 104, epoch: 3, batch: 4, loss: 1.7825400829315186, acc: 42.1875, f1: 17.37927565392354, r: 0.08889630461177386
06/02/2019 10:51:36 step: 109, epoch: 3, batch: 9, loss: 1.8418179750442505, acc: 40.625, f1: 13.990026435952895, r: 0.10930617187814338
06/02/2019 10:51:36 step: 114, epoch: 3, batch: 14, loss: 1.746376872062683, acc: 34.375, f1: 12.422360248447204, r: 0.028625924870120386
06/02/2019 10:51:37 step: 119, epoch: 3, batch: 19, loss: 1.6590920686721802, acc: 32.8125, f1: 14.3146913937989, r: 0.15536503253094078
06/02/2019 10:51:37 step: 124, epoch: 3, batch: 24, loss: 1.6366382837295532, acc: 42.1875, f1: 13.032541391842777, r: 0.12588912379794964
06/02/2019 10:51:38 step: 129, epoch: 3, batch: 29, loss: 1.6546258926391602, acc: 40.625, f1: 18.08654817295823, r: 0.1645289127876596
06/02/2019 10:51:38 *** evaluating ***
06/02/2019 10:51:38 step: 4, epoch: 3, acc: 56.41025641025641, f1: 16.45768833849329, r: 0.20030128617995716
06/02/2019 10:51:38 *** epoch: 5 ***
06/02/2019 10:51:38 *** training ***
06/02/2019 10:51:39 step: 137, epoch: 4, batch: 4, loss: 1.574687123298645, acc: 39.0625, f1: 14.234215676176742, r: 0.09701271537171276
06/02/2019 10:51:39 step: 142, epoch: 4, batch: 9, loss: 1.7865196466445923, acc: 43.75, f1: 17.203478363227052, r: 0.10334250475642406
06/02/2019 10:51:39 step: 147, epoch: 4, batch: 14, loss: 1.708933711051941, acc: 37.5, f1: 11.333333333333334, r: 0.19407488335890827
06/02/2019 10:51:40 step: 152, epoch: 4, batch: 19, loss: 1.6585389375686646, acc: 35.9375, f1: 13.94673123486683, r: 0.1712991722761986
06/02/2019 10:51:40 step: 157, epoch: 4, batch: 24, loss: 1.6727195978164673, acc: 37.5, f1: 10.760869565217392, r: 0.12727248039868322
06/02/2019 10:51:41 step: 162, epoch: 4, batch: 29, loss: 1.5009644031524658, acc: 39.0625, f1: 14.487327188940094, r: 0.16965885564141198
06/02/2019 10:51:41 *** evaluating ***
06/02/2019 10:51:41 step: 5, epoch: 4, acc: 56.837606837606835, f1: 16.651812240047533, r: 0.21456799942744667
06/02/2019 10:51:41 *** epoch: 6 ***
06/02/2019 10:51:41 *** training ***
06/02/2019 10:51:42 step: 170, epoch: 5, batch: 4, loss: 1.5150474309921265, acc: 43.75, f1: 17.945782964348535, r: 0.19882052246227405
06/02/2019 10:51:42 step: 175, epoch: 5, batch: 9, loss: 1.516053557395935, acc: 48.4375, f1: 17.721307256819347, r: 0.1825909407260781
06/02/2019 10:51:42 step: 180, epoch: 5, batch: 14, loss: 1.6463556289672852, acc: 43.75, f1: 15.234172387490464, r: 0.21789181611112401
06/02/2019 10:51:43 step: 185, epoch: 5, batch: 19, loss: 1.57039475440979, acc: 50.0, f1: 17.600310137623573, r: 0.09498641990051199
06/02/2019 10:51:43 step: 190, epoch: 5, batch: 24, loss: 1.8186848163604736, acc: 37.5, f1: 16.696243714877255, r: 0.1316661681015207
06/02/2019 10:51:44 step: 195, epoch: 5, batch: 29, loss: 1.5507640838623047, acc: 45.3125, f1: 20.865587614356087, r: 0.10131693864763947
06/02/2019 10:51:44 *** evaluating ***
06/02/2019 10:51:44 step: 6, epoch: 5, acc: 50.85470085470085, f1: 15.001470588235295, r: 0.20098803689988304
06/02/2019 10:51:44 *** epoch: 7 ***
06/02/2019 10:51:44 *** training ***
06/02/2019 10:51:45 step: 203, epoch: 6, batch: 4, loss: 1.5769598484039307, acc: 46.875, f1: 14.671947480147809, r: 0.1758892447512694
06/02/2019 10:51:45 step: 208, epoch: 6, batch: 9, loss: 1.4398880004882812, acc: 51.5625, f1: 18.561643835616437, r: 0.2830339893619111
06/02/2019 10:51:45 step: 213, epoch: 6, batch: 14, loss: 1.4451991319656372, acc: 40.625, f1: 14.868116471734893, r: 0.2535410089680297
06/02/2019 10:51:46 step: 218, epoch: 6, batch: 19, loss: 1.479461669921875, acc: 50.0, f1: 21.664050235478808, r: 0.2512535461673494
06/02/2019 10:51:46 step: 223, epoch: 6, batch: 24, loss: 1.5547281503677368, acc: 45.3125, f1: 18.268472906403943, r: 0.18155126086357382
06/02/2019 10:51:47 step: 228, epoch: 6, batch: 29, loss: 1.5016125440597534, acc: 46.875, f1: 18.353174603174605, r: 0.2534301636341558
06/02/2019 10:51:47 *** evaluating ***
06/02/2019 10:51:47 step: 7, epoch: 6, acc: 55.12820512820513, f1: 16.369843391902215, r: 0.2013339425992079
06/02/2019 10:51:47 *** epoch: 8 ***
06/02/2019 10:51:47 *** training ***
06/02/2019 10:51:48 step: 236, epoch: 7, batch: 4, loss: 1.5477256774902344, acc: 51.5625, f1: 19.401041666666664, r: 0.23936435898097458
06/02/2019 10:51:48 step: 241, epoch: 7, batch: 9, loss: 1.494770884513855, acc: 46.875, f1: 17.272727272727273, r: 0.29746480535879716
06/02/2019 10:51:48 step: 246, epoch: 7, batch: 14, loss: 1.5513224601745605, acc: 39.0625, f1: 17.533811943749832, r: 0.2549509991545475
06/02/2019 10:51:49 step: 251, epoch: 7, batch: 19, loss: 1.6642550230026245, acc: 46.875, f1: 17.571269310399746, r: 0.24061414947604426
06/02/2019 10:51:49 step: 256, epoch: 7, batch: 24, loss: 1.4747653007507324, acc: 54.6875, f1: 22.57703081232493, r: 0.21533968651163296
06/02/2019 10:51:50 step: 261, epoch: 7, batch: 29, loss: 1.695458173751831, acc: 43.75, f1: 20.784313725490197, r: 0.14116161503301924
06/02/2019 10:51:50 *** evaluating ***
06/02/2019 10:51:50 step: 8, epoch: 7, acc: 55.98290598290598, f1: 16.434562684082465, r: 0.25179177319789253
06/02/2019 10:51:50 *** epoch: 9 ***
06/02/2019 10:51:50 *** training ***
06/02/2019 10:51:51 step: 269, epoch: 8, batch: 4, loss: 1.5393763780593872, acc: 48.4375, f1: 22.732426303854876, r: 0.29270954432787627
06/02/2019 10:51:51 step: 274, epoch: 8, batch: 9, loss: 1.541935682296753, acc: 48.4375, f1: 17.920524691358025, r: 0.19265977829578834
06/02/2019 10:51:51 step: 279, epoch: 8, batch: 14, loss: 1.4800750017166138, acc: 46.875, f1: 18.088235294117645, r: 0.1744833446687697
06/02/2019 10:51:52 step: 284, epoch: 8, batch: 19, loss: 1.3204452991485596, acc: 59.375, f1: 25.22263046456595, r: 0.24273937193076778
06/02/2019 10:51:52 step: 289, epoch: 8, batch: 24, loss: 1.7744605541229248, acc: 39.0625, f1: 15.52618135376756, r: 0.16598909716477842
06/02/2019 10:51:53 step: 294, epoch: 8, batch: 29, loss: 1.306658387184143, acc: 59.375, f1: 23.93527107812822, r: 0.25370987388776206
06/02/2019 10:51:53 *** evaluating ***
06/02/2019 10:51:53 step: 9, epoch: 8, acc: 55.98290598290598, f1: 16.571624087591243, r: 0.24209350521410963
06/02/2019 10:51:53 *** epoch: 10 ***
06/02/2019 10:51:53 *** training ***
06/02/2019 10:51:53 step: 302, epoch: 9, batch: 4, loss: 1.685394287109375, acc: 39.0625, f1: 17.275095171308354, r: 0.22207983444100532
06/02/2019 10:51:54 step: 307, epoch: 9, batch: 9, loss: 1.5129883289337158, acc: 48.4375, f1: 18.94648829431438, r: 0.289790610605147
06/02/2019 10:51:54 step: 312, epoch: 9, batch: 14, loss: 1.6826804876327515, acc: 32.8125, f1: 13.18853427895981, r: 0.2585990054478573
06/02/2019 10:51:55 step: 317, epoch: 9, batch: 19, loss: 1.5075286626815796, acc: 50.0, f1: 17.360646599777034, r: 0.18666909404834345
06/02/2019 10:51:55 step: 322, epoch: 9, batch: 24, loss: 1.476749300956726, acc: 46.875, f1: 19.71747096373218, r: 0.20528395026548726
06/02/2019 10:51:56 step: 327, epoch: 9, batch: 29, loss: 1.4287160634994507, acc: 45.3125, f1: 17.86853774605235, r: 0.2988667122260857
06/02/2019 10:51:56 *** evaluating ***
06/02/2019 10:51:56 step: 10, epoch: 9, acc: 54.27350427350427, f1: 16.137902509268052, r: 0.22622688952220385
06/02/2019 10:51:56 *** epoch: 11 ***
06/02/2019 10:51:56 *** training ***
06/02/2019 10:51:56 step: 335, epoch: 10, batch: 4, loss: 1.6822587251663208, acc: 39.0625, f1: 12.246376811594203, r: 0.17077481345632006
06/02/2019 10:51:57 step: 340, epoch: 10, batch: 9, loss: 1.5657347440719604, acc: 46.875, f1: 20.323029796714007, r: 0.22712906096119953
06/02/2019 10:51:57 step: 345, epoch: 10, batch: 14, loss: 1.459995985031128, acc: 45.3125, f1: 22.516339869281047, r: 0.25589877293767066
06/02/2019 10:51:58 step: 350, epoch: 10, batch: 19, loss: 1.4459677934646606, acc: 48.4375, f1: 20.587609112199274, r: 0.2206036399188121
06/02/2019 10:51:58 step: 355, epoch: 10, batch: 24, loss: 1.5533095598220825, acc: 48.4375, f1: 21.684024036965212, r: 0.2158207989522294
06/02/2019 10:51:58 step: 360, epoch: 10, batch: 29, loss: 1.4665006399154663, acc: 56.25, f1: 24.211212963916644, r: 0.20445566171940327
06/02/2019 10:51:58 *** evaluating ***
06/02/2019 10:51:59 step: 11, epoch: 10, acc: 56.41025641025641, f1: 16.884095974826273, r: 0.25539674110671684
06/02/2019 10:51:59 *** epoch: 12 ***
06/02/2019 10:51:59 *** training ***
06/02/2019 10:51:59 step: 368, epoch: 11, batch: 4, loss: 1.6747164726257324, acc: 34.375, f1: 15.40286023044644, r: 0.12766320797452368
06/02/2019 10:51:59 step: 373, epoch: 11, batch: 9, loss: 1.3368710279464722, acc: 54.6875, f1: 20.476190476190474, r: 0.2430742874952163
06/02/2019 10:52:00 step: 378, epoch: 11, batch: 14, loss: 1.1492544412612915, acc: 60.9375, f1: 24.251700680272112, r: 0.30527750259048114
06/02/2019 10:52:00 step: 383, epoch: 11, batch: 19, loss: 1.7009018659591675, acc: 39.0625, f1: 16.02777777777778, r: 0.2749421958205275
06/02/2019 10:52:00 step: 388, epoch: 11, batch: 24, loss: 1.504409670829773, acc: 40.625, f1: 19.12087912087912, r: 0.27661387538541043
06/02/2019 10:52:01 step: 393, epoch: 11, batch: 29, loss: 1.6223645210266113, acc: 51.5625, f1: 18.76509661835749, r: 0.24795454517713283
06/02/2019 10:52:01 *** evaluating ***
06/02/2019 10:52:01 step: 12, epoch: 11, acc: 55.98290598290598, f1: 16.677798982188296, r: 0.25580698388901957
06/02/2019 10:52:01 *** epoch: 13 ***
06/02/2019 10:52:01 *** training ***
06/02/2019 10:52:02 step: 401, epoch: 12, batch: 4, loss: 1.2996954917907715, acc: 50.0, f1: 18.345090525541654, r: 0.28265591359887066
06/02/2019 10:52:02 step: 406, epoch: 12, batch: 9, loss: 1.513339877128601, acc: 42.1875, f1: 19.53823953823954, r: 0.20219750195475172
06/02/2019 10:52:02 step: 411, epoch: 12, batch: 14, loss: 1.5795395374298096, acc: 50.0, f1: 19.236909323116226, r: 0.22347870324103528
06/02/2019 10:52:03 step: 416, epoch: 12, batch: 19, loss: 1.3885825872421265, acc: 51.5625, f1: 21.638655462184875, r: 0.3016689327761005
06/02/2019 10:52:03 step: 421, epoch: 12, batch: 24, loss: 1.2565253973007202, acc: 54.6875, f1: 23.9141810570382, r: 0.4138617953817711
06/02/2019 10:52:04 step: 426, epoch: 12, batch: 29, loss: 1.5874974727630615, acc: 50.0, f1: 17.769932906849746, r: 0.20094124587934137
06/02/2019 10:52:04 *** evaluating ***
06/02/2019 10:52:04 step: 13, epoch: 12, acc: 54.700854700854705, f1: 16.86565698733072, r: 0.26752521731027196
06/02/2019 10:52:04 *** epoch: 14 ***
06/02/2019 10:52:04 *** training ***
06/02/2019 10:52:05 step: 434, epoch: 13, batch: 4, loss: 1.2917137145996094, acc: 53.125, f1: 22.600151171579743, r: 0.27842846649887976
06/02/2019 10:52:05 step: 439, epoch: 13, batch: 9, loss: 1.6634470224380493, acc: 43.75, f1: 21.48706896551724, r: 0.21582565623522368
06/02/2019 10:52:05 step: 444, epoch: 13, batch: 14, loss: 1.4255090951919556, acc: 48.4375, f1: 16.57843137254902, r: 0.2116235036805039
06/02/2019 10:52:06 step: 449, epoch: 13, batch: 19, loss: 1.5022927522659302, acc: 50.0, f1: 18.907563025210084, r: 0.23650549727043274
06/02/2019 10:52:06 step: 454, epoch: 13, batch: 24, loss: 1.5697650909423828, acc: 42.1875, f1: 16.94700460829493, r: 0.28891777774678407
06/02/2019 10:52:07 step: 459, epoch: 13, batch: 29, loss: 1.6351871490478516, acc: 42.1875, f1: 17.573426573426573, r: 0.2039356686230031
06/02/2019 10:52:07 *** evaluating ***
06/02/2019 10:52:07 step: 14, epoch: 13, acc: 56.41025641025641, f1: 16.88350088704908, r: 0.2709686564041857
06/02/2019 10:52:07 *** epoch: 15 ***
06/02/2019 10:52:07 *** training ***
06/02/2019 10:52:08 step: 467, epoch: 14, batch: 4, loss: 1.4649821519851685, acc: 53.125, f1: 26.53917219061538, r: 0.26269780825980504
06/02/2019 10:52:08 step: 472, epoch: 14, batch: 9, loss: 1.374272108078003, acc: 50.0, f1: 29.28415670351154, r: 0.20460707104152862
06/02/2019 10:52:08 step: 477, epoch: 14, batch: 14, loss: 1.3838191032409668, acc: 48.4375, f1: 19.805499664654597, r: 0.18481863671466212
06/02/2019 10:52:09 step: 482, epoch: 14, batch: 19, loss: 1.3792788982391357, acc: 56.25, f1: 26.519777265745002, r: 0.3286687452250342
06/02/2019 10:52:09 step: 487, epoch: 14, batch: 24, loss: 1.3358885049819946, acc: 50.0, f1: 22.13054643651382, r: 0.2722176585492205
06/02/2019 10:52:10 step: 492, epoch: 14, batch: 29, loss: 1.3647764921188354, acc: 53.125, f1: 22.06597222222222, r: 0.3076096276021868
06/02/2019 10:52:10 *** evaluating ***
06/02/2019 10:52:10 step: 15, epoch: 14, acc: 56.837606837606835, f1: 17.100617828773167, r: 0.27852356190736544
06/02/2019 10:52:10 *** epoch: 16 ***
06/02/2019 10:52:10 *** training ***
06/02/2019 10:52:11 step: 500, epoch: 15, batch: 4, loss: 1.1751208305358887, acc: 64.0625, f1: 33.776397515527954, r: 0.28278402736904873
06/02/2019 10:52:11 step: 505, epoch: 15, batch: 9, loss: 1.339125394821167, acc: 48.4375, f1: 21.459587666484218, r: 0.23108345486613602
06/02/2019 10:52:11 step: 510, epoch: 15, batch: 14, loss: 1.2830466032028198, acc: 51.5625, f1: 34.78935433822652, r: 0.32086935999366306
06/02/2019 10:52:12 step: 515, epoch: 15, batch: 19, loss: 1.3624582290649414, acc: 54.6875, f1: 22.468736864228667, r: 0.2984778059326981
06/02/2019 10:52:12 step: 520, epoch: 15, batch: 24, loss: 1.302472472190857, acc: 59.375, f1: 21.703993621801835, r: 0.22962790025215274
06/02/2019 10:52:13 step: 525, epoch: 15, batch: 29, loss: 1.5975385904312134, acc: 46.875, f1: 19.16195856873823, r: 0.2963358923728012
06/02/2019 10:52:13 *** evaluating ***
06/02/2019 10:52:13 step: 16, epoch: 15, acc: 55.12820512820513, f1: 16.601575056252006, r: 0.27754069005461074
06/02/2019 10:52:13 *** epoch: 17 ***
06/02/2019 10:52:13 *** training ***
06/02/2019 10:52:13 step: 533, epoch: 16, batch: 4, loss: 1.441851258277893, acc: 53.125, f1: 23.187229437229437, r: 0.33676021155626956
06/02/2019 10:52:14 step: 538, epoch: 16, batch: 9, loss: 1.5462929010391235, acc: 46.875, f1: 25.109017966160824, r: 0.19976162124342467
06/02/2019 10:52:14 step: 543, epoch: 16, batch: 14, loss: 1.2732802629470825, acc: 53.125, f1: 22.88644688644689, r: 0.23725031116614712
06/02/2019 10:52:15 step: 548, epoch: 16, batch: 19, loss: 1.3044945001602173, acc: 54.6875, f1: 22.29689366786141, r: 0.31140059676956344
06/02/2019 10:52:15 step: 553, epoch: 16, batch: 24, loss: 1.1947392225265503, acc: 64.0625, f1: 29.257023623220807, r: 0.21851169938525022
06/02/2019 10:52:16 step: 558, epoch: 16, batch: 29, loss: 1.1805201768875122, acc: 64.0625, f1: 29.860078002492862, r: 0.332873386602531
06/02/2019 10:52:16 *** evaluating ***
06/02/2019 10:52:16 step: 17, epoch: 16, acc: 56.837606837606835, f1: 17.11137707687975, r: 0.2820696597822907
06/02/2019 10:52:16 *** epoch: 18 ***
06/02/2019 10:52:16 *** training ***
06/02/2019 10:52:17 step: 566, epoch: 17, batch: 4, loss: 1.3385100364685059, acc: 48.4375, f1: 15.714285714285714, r: 0.2948551110126529
06/02/2019 10:52:17 step: 571, epoch: 17, batch: 9, loss: 1.3915902376174927, acc: 48.4375, f1: 22.171162171162173, r: 0.2732204542631449
06/02/2019 10:52:17 step: 576, epoch: 17, batch: 14, loss: 1.2878154516220093, acc: 50.0, f1: 18.84057971014493, r: 0.31308133095673857
06/02/2019 10:52:18 step: 581, epoch: 17, batch: 19, loss: 1.3397068977355957, acc: 53.125, f1: 26.70068027210884, r: 0.3243848065803805
06/02/2019 10:52:18 step: 586, epoch: 17, batch: 24, loss: 1.2789490222930908, acc: 59.375, f1: 23.839742136939385, r: 0.33971050537889047
06/02/2019 10:52:19 step: 591, epoch: 17, batch: 29, loss: 1.497574806213379, acc: 42.1875, f1: 20.376921733053805, r: 0.32091851515594516
06/02/2019 10:52:19 *** evaluating ***
06/02/2019 10:52:19 step: 18, epoch: 17, acc: 55.12820512820513, f1: 17.42803006129563, r: 0.26226908667715254
06/02/2019 10:52:19 *** epoch: 19 ***
06/02/2019 10:52:19 *** training ***
06/02/2019 10:52:20 step: 599, epoch: 18, batch: 4, loss: 1.3964859247207642, acc: 48.4375, f1: 17.294429708222815, r: 0.29028773121260865
06/02/2019 10:52:20 step: 604, epoch: 18, batch: 9, loss: 1.2620171308517456, acc: 56.25, f1: 21.847200418628994, r: 0.32584121774517844
06/02/2019 10:52:20 step: 609, epoch: 18, batch: 14, loss: 1.3190412521362305, acc: 54.6875, f1: 20.752924649963152, r: 0.3247756061696402
06/02/2019 10:52:21 step: 614, epoch: 18, batch: 19, loss: 1.2883918285369873, acc: 46.875, f1: 21.21510673234811, r: 0.3324060423845431
06/02/2019 10:52:21 step: 619, epoch: 18, batch: 24, loss: 1.3078033924102783, acc: 56.25, f1: 26.51053864168619, r: 0.2745418391014839
06/02/2019 10:52:22 step: 624, epoch: 18, batch: 29, loss: 1.2449039220809937, acc: 51.5625, f1: 27.84023128850715, r: 0.35241879040698826
06/02/2019 10:52:22 *** evaluating ***
06/02/2019 10:52:22 step: 19, epoch: 18, acc: 54.27350427350427, f1: 17.034187014501974, r: 0.271644853540634
06/02/2019 10:52:22 *** epoch: 20 ***
06/02/2019 10:52:22 *** training ***
06/02/2019 10:52:23 step: 632, epoch: 19, batch: 4, loss: 1.4708061218261719, acc: 53.125, f1: 26.026272577996718, r: 0.2746767544888952
06/02/2019 10:52:23 step: 637, epoch: 19, batch: 9, loss: 1.3428092002868652, acc: 56.25, f1: 26.87908496732026, r: 0.3460036138107292
06/02/2019 10:52:23 step: 642, epoch: 19, batch: 14, loss: 1.3806591033935547, acc: 46.875, f1: 20.256795835743205, r: 0.33564621813108164
06/02/2019 10:52:24 step: 647, epoch: 19, batch: 19, loss: 1.21890127658844, acc: 57.8125, f1: 30.568586674404735, r: 0.30833952712040336
06/02/2019 10:52:24 step: 652, epoch: 19, batch: 24, loss: 1.191804051399231, acc: 59.375, f1: 23.70248538011696, r: 0.3625810432649189
06/02/2019 10:52:25 step: 657, epoch: 19, batch: 29, loss: 1.4774469137191772, acc: 45.3125, f1: 19.64886964886965, r: 0.24886733522323043
06/02/2019 10:52:25 *** evaluating ***
06/02/2019 10:52:25 step: 20, epoch: 19, acc: 55.12820512820513, f1: 17.62174828084086, r: 0.2761395486064116
06/02/2019 10:52:25 *** epoch: 21 ***
06/02/2019 10:52:25 *** training ***
06/02/2019 10:52:26 step: 665, epoch: 20, batch: 4, loss: 1.4570950269699097, acc: 39.0625, f1: 15.865384615384615, r: 0.2657020975826337
06/02/2019 10:52:26 step: 670, epoch: 20, batch: 9, loss: 1.2313252687454224, acc: 57.8125, f1: 39.69063816002592, r: 0.32405873859573014
06/02/2019 10:52:27 step: 675, epoch: 20, batch: 14, loss: 1.4038054943084717, acc: 48.4375, f1: 23.639455782312925, r: 0.23611301774394883
06/02/2019 10:52:27 step: 680, epoch: 20, batch: 19, loss: 1.3995305299758911, acc: 46.875, f1: 23.86123680241327, r: 0.31057747923691187
06/02/2019 10:52:28 step: 685, epoch: 20, batch: 24, loss: 1.2280945777893066, acc: 53.125, f1: 28.3963117861423, r: 0.33649252459245477
06/02/2019 10:52:28 step: 690, epoch: 20, batch: 29, loss: 1.017278790473938, acc: 64.0625, f1: 39.92123165055496, r: 0.3435431356711895
06/02/2019 10:52:28 *** evaluating ***
06/02/2019 10:52:28 step: 21, epoch: 20, acc: 57.692307692307686, f1: 17.90756024356118, r: 0.29954827176425103
06/02/2019 10:52:28 *** epoch: 22 ***
06/02/2019 10:52:28 *** training ***
06/02/2019 10:52:29 step: 698, epoch: 21, batch: 4, loss: 1.3403366804122925, acc: 45.3125, f1: 20.295454545454543, r: 0.2660891661012725
06/02/2019 10:52:29 step: 703, epoch: 21, batch: 9, loss: 1.218061923980713, acc: 57.8125, f1: 23.67191404297851, r: 0.37492701864403105
06/02/2019 10:52:30 step: 708, epoch: 21, batch: 14, loss: 1.2297694683074951, acc: 59.375, f1: 28.140911284422238, r: 0.2879117566590235
06/02/2019 10:52:30 step: 713, epoch: 21, batch: 19, loss: 1.2198220491409302, acc: 56.25, f1: 30.867698609634093, r: 0.3477804282012695
06/02/2019 10:52:31 step: 718, epoch: 21, batch: 24, loss: 1.2520508766174316, acc: 56.25, f1: 23.861283643892342, r: 0.34448183233562685
06/02/2019 10:52:31 step: 723, epoch: 21, batch: 29, loss: 1.336507797241211, acc: 57.8125, f1: 30.312339989759344, r: 0.2550933737893733
06/02/2019 10:52:31 *** evaluating ***
06/02/2019 10:52:32 step: 22, epoch: 21, acc: 56.837606837606835, f1: 17.824558417241697, r: 0.2919803551609347
06/02/2019 10:52:32 *** epoch: 23 ***
06/02/2019 10:52:32 *** training ***
06/02/2019 10:52:32 step: 731, epoch: 22, batch: 4, loss: 1.4809460639953613, acc: 51.5625, f1: 19.944407008086255, r: 0.27041675722909675
06/02/2019 10:52:32 step: 736, epoch: 22, batch: 9, loss: 1.0680278539657593, acc: 59.375, f1: 37.3469387755102, r: 0.3613908662964017
06/02/2019 10:52:33 step: 741, epoch: 22, batch: 14, loss: 1.3653955459594727, acc: 45.3125, f1: 23.891625615763548, r: 0.3795059892492083
06/02/2019 10:52:33 step: 746, epoch: 22, batch: 19, loss: 1.3145850896835327, acc: 53.125, f1: 25.023224043715842, r: 0.35035181387211967
06/02/2019 10:52:34 step: 751, epoch: 22, batch: 24, loss: 1.4083826541900635, acc: 46.875, f1: 31.13328664799253, r: 0.16550966143422202
06/02/2019 10:52:34 step: 756, epoch: 22, batch: 29, loss: 1.332901954650879, acc: 46.875, f1: 24.689655172413797, r: 0.2249027800490891
06/02/2019 10:52:34 *** evaluating ***
06/02/2019 10:52:35 step: 23, epoch: 22, acc: 56.41025641025641, f1: 17.745456087807295, r: 0.2988644502393548
06/02/2019 10:52:35 *** epoch: 24 ***
06/02/2019 10:52:35 *** training ***
06/02/2019 10:52:35 step: 764, epoch: 23, batch: 4, loss: 1.3320950269699097, acc: 50.0, f1: 22.879812618943053, r: 0.40616503230653633
06/02/2019 10:52:35 step: 769, epoch: 23, batch: 9, loss: 1.412739634513855, acc: 54.6875, f1: 34.019878756720864, r: 0.21832149169862325
06/02/2019 10:52:36 step: 774, epoch: 23, batch: 14, loss: 1.3640491962432861, acc: 51.5625, f1: 22.05593885281385, r: 0.35962146400685374
06/02/2019 10:52:36 step: 779, epoch: 23, batch: 19, loss: 1.412471055984497, acc: 51.5625, f1: 21.686872909698995, r: 0.23553803900401435
06/02/2019 10:52:37 step: 784, epoch: 23, batch: 24, loss: 1.2787913084030151, acc: 57.8125, f1: 32.16440422322775, r: 0.3470382771425416
06/02/2019 10:52:37 step: 789, epoch: 23, batch: 29, loss: 1.2741261720657349, acc: 53.125, f1: 27.61904761904762, r: 0.32026736415184853
06/02/2019 10:52:37 *** evaluating ***
06/02/2019 10:52:38 step: 24, epoch: 23, acc: 56.837606837606835, f1: 18.683504933504935, r: 0.2908800271071954
06/02/2019 10:52:38 *** epoch: 25 ***
06/02/2019 10:52:38 *** training ***
06/02/2019 10:52:38 step: 797, epoch: 24, batch: 4, loss: 1.1491279602050781, acc: 56.25, f1: 29.212091846348486, r: 0.30278809641074367
06/02/2019 10:52:38 step: 802, epoch: 24, batch: 9, loss: 1.2732853889465332, acc: 62.5, f1: 30.74691514536849, r: 0.3045667195279003
06/02/2019 10:52:39 step: 807, epoch: 24, batch: 14, loss: 1.506216287612915, acc: 50.0, f1: 24.111281053021873, r: 0.3102159278952081
06/02/2019 10:52:39 step: 812, epoch: 24, batch: 19, loss: 1.305928349494934, acc: 57.8125, f1: 21.535087719298247, r: 0.3475807087668925
06/02/2019 10:52:40 step: 817, epoch: 24, batch: 24, loss: 1.3898847103118896, acc: 48.4375, f1: 21.723258268824775, r: 0.3440026841118124
06/02/2019 10:52:40 step: 822, epoch: 24, batch: 29, loss: 1.1351662874221802, acc: 57.8125, f1: 27.535026549804876, r: 0.33821993029199837
06/02/2019 10:52:40 *** evaluating ***
06/02/2019 10:52:40 step: 25, epoch: 24, acc: 55.98290598290598, f1: 17.966228146149195, r: 0.3164762019211865
06/02/2019 10:52:40 *** epoch: 26 ***
06/02/2019 10:52:40 *** training ***
06/02/2019 10:52:41 step: 830, epoch: 25, batch: 4, loss: 1.0949163436889648, acc: 65.625, f1: 45.045772470319754, r: 0.36121761057389473
06/02/2019 10:52:41 step: 835, epoch: 25, batch: 9, loss: 1.3040729761123657, acc: 53.125, f1: 25.45195709862893, r: 0.39074854076688137
06/02/2019 10:52:41 step: 840, epoch: 25, batch: 14, loss: 1.226507306098938, acc: 54.6875, f1: 23.777168330662725, r: 0.36205810741620864
06/02/2019 10:52:42 step: 845, epoch: 25, batch: 19, loss: 1.2289621829986572, acc: 50.0, f1: 26.533302586571356, r: 0.3884485755200272
06/02/2019 10:52:42 step: 850, epoch: 25, batch: 24, loss: 0.9065455794334412, acc: 71.875, f1: 47.20311118479222, r: 0.40415200514234606
06/02/2019 10:52:43 step: 855, epoch: 25, batch: 29, loss: 1.337675929069519, acc: 50.0, f1: 22.82405766276734, r: 0.31585440486453975
06/02/2019 10:52:43 *** evaluating ***
06/02/2019 10:52:43 step: 26, epoch: 25, acc: 56.41025641025641, f1: 18.598631694895857, r: 0.3048585348868934
06/02/2019 10:52:43 *** epoch: 27 ***
06/02/2019 10:52:43 *** training ***
06/02/2019 10:52:44 step: 863, epoch: 26, batch: 4, loss: 1.0801295042037964, acc: 64.0625, f1: 30.444102353585112, r: 0.4489439651954309
06/02/2019 10:52:44 step: 868, epoch: 26, batch: 9, loss: 1.5073962211608887, acc: 39.0625, f1: 22.110904968047823, r: 0.29881545753759775
06/02/2019 10:52:44 step: 873, epoch: 26, batch: 14, loss: 0.9926058053970337, acc: 67.1875, f1: 33.111025160714604, r: 0.3845689406037785
06/02/2019 10:52:45 step: 878, epoch: 26, batch: 19, loss: 1.3898884057998657, acc: 48.4375, f1: 20.20779220779221, r: 0.30187585950506934
06/02/2019 10:52:45 step: 883, epoch: 26, batch: 24, loss: 1.254927635192871, acc: 60.9375, f1: 31.036908101248915, r: 0.4034775230207462
06/02/2019 10:52:46 step: 888, epoch: 26, batch: 29, loss: 1.2097989320755005, acc: 64.0625, f1: 30.905797101449274, r: 0.30600838354727133
06/02/2019 10:52:46 *** evaluating ***
06/02/2019 10:52:46 step: 27, epoch: 26, acc: 56.41025641025641, f1: 18.259573421845225, r: 0.30809455564157406
06/02/2019 10:52:46 *** epoch: 28 ***
06/02/2019 10:52:46 *** training ***
06/02/2019 10:52:47 step: 896, epoch: 27, batch: 4, loss: 1.2208832502365112, acc: 64.0625, f1: 35.39249461446258, r: 0.3438603673898788
06/02/2019 10:52:47 step: 901, epoch: 27, batch: 9, loss: 1.023179531097412, acc: 60.9375, f1: 31.446886446886452, r: 0.3814064180849489
06/02/2019 10:52:47 step: 906, epoch: 27, batch: 14, loss: 1.0643157958984375, acc: 60.9375, f1: 27.605633802816904, r: 0.39637504495464904
06/02/2019 10:52:48 step: 911, epoch: 27, batch: 19, loss: 1.2852526903152466, acc: 50.0, f1: 26.295716557344463, r: 0.4004806558466283
06/02/2019 10:52:48 step: 916, epoch: 27, batch: 24, loss: 1.3645573854446411, acc: 51.5625, f1: 24.998430141287283, r: 0.3404531146544899
06/02/2019 10:52:49 step: 921, epoch: 27, batch: 29, loss: 1.24380624294281, acc: 62.5, f1: 34.45476698141943, r: 0.22919326929806053
06/02/2019 10:52:49 *** evaluating ***
06/02/2019 10:52:49 step: 28, epoch: 27, acc: 55.12820512820513, f1: 17.87925696594427, r: 0.31186073861460245
06/02/2019 10:52:49 *** epoch: 29 ***
06/02/2019 10:52:49 *** training ***
06/02/2019 10:52:49 step: 929, epoch: 28, batch: 4, loss: 1.2199400663375854, acc: 54.6875, f1: 27.249999999999996, r: 0.4463552349581193
06/02/2019 10:52:50 step: 934, epoch: 28, batch: 9, loss: 1.2296817302703857, acc: 57.8125, f1: 29.11027568922306, r: 0.4025636921306438
06/02/2019 10:52:50 step: 939, epoch: 28, batch: 14, loss: 1.179826021194458, acc: 64.0625, f1: 35.610114523157996, r: 0.39886662169092135
06/02/2019 10:52:51 step: 944, epoch: 28, batch: 19, loss: 1.1871336698532104, acc: 53.125, f1: 34.60947469406116, r: 0.43954368693727075
06/02/2019 10:52:51 step: 949, epoch: 28, batch: 24, loss: 1.27946937084198, acc: 53.125, f1: 24.54573934837093, r: 0.3305352664045254
06/02/2019 10:52:52 step: 954, epoch: 28, batch: 29, loss: 1.1757631301879883, acc: 57.8125, f1: 31.56655844155844, r: 0.33320546761659164
06/02/2019 10:52:52 *** evaluating ***
06/02/2019 10:52:52 step: 29, epoch: 28, acc: 56.837606837606835, f1: 18.385903426791277, r: 0.30603528504062005
06/02/2019 10:52:52 *** epoch: 30 ***
06/02/2019 10:52:52 *** training ***
06/02/2019 10:52:52 step: 962, epoch: 29, batch: 4, loss: 1.2925821542739868, acc: 60.9375, f1: 48.163209850663996, r: 0.3992317756088153
06/02/2019 10:52:53 step: 967, epoch: 29, batch: 9, loss: 1.1847621202468872, acc: 59.375, f1: 38.63287250384025, r: 0.4615175577298676
06/02/2019 10:52:53 step: 972, epoch: 29, batch: 14, loss: 1.0279086828231812, acc: 60.9375, f1: 37.75946275946276, r: 0.4492741680941762
06/02/2019 10:52:53 step: 977, epoch: 29, batch: 19, loss: 1.3137092590332031, acc: 60.9375, f1: 39.358180805549225, r: 0.3994911504696594
06/02/2019 10:52:54 step: 982, epoch: 29, batch: 24, loss: 1.2413113117218018, acc: 59.375, f1: 27.727272727272727, r: 0.35821529092742177
06/02/2019 10:52:54 step: 987, epoch: 29, batch: 29, loss: 1.2263710498809814, acc: 56.25, f1: 29.30917667238422, r: 0.47371096448202454
06/02/2019 10:52:54 *** evaluating ***
06/02/2019 10:52:55 step: 30, epoch: 29, acc: 55.55555555555556, f1: 19.208732444026563, r: 0.30574376753094046
06/02/2019 10:52:55 *** epoch: 31 ***
06/02/2019 10:52:55 *** training ***
06/02/2019 10:52:55 step: 995, epoch: 30, batch: 4, loss: 1.1793618202209473, acc: 59.375, f1: 40.48776455026455, r: 0.3836417299538824
06/02/2019 10:52:55 step: 1000, epoch: 30, batch: 9, loss: 1.407448649406433, acc: 46.875, f1: 27.232941477127522, r: 0.4034631427836842
06/02/2019 10:52:56 step: 1005, epoch: 30, batch: 14, loss: 0.9789750576019287, acc: 62.5, f1: 27.650904203323556, r: 0.3814609155520713
06/02/2019 10:52:56 step: 1010, epoch: 30, batch: 19, loss: 0.8859312534332275, acc: 64.0625, f1: 41.287035360046026, r: 0.3962863800203736
06/02/2019 10:52:57 step: 1015, epoch: 30, batch: 24, loss: 1.3233546018600464, acc: 54.6875, f1: 32.73809523809524, r: 0.42049343006471007
06/02/2019 10:52:57 step: 1020, epoch: 30, batch: 29, loss: 1.012774109840393, acc: 60.9375, f1: 45.89936379410064, r: 0.35491489501363427
06/02/2019 10:52:57 *** evaluating ***
06/02/2019 10:52:57 step: 31, epoch: 30, acc: 57.26495726495726, f1: 19.78753459016617, r: 0.32317315430031984
06/02/2019 10:52:57 *** epoch: 32 ***
06/02/2019 10:52:57 *** training ***
06/02/2019 10:52:58 step: 1028, epoch: 31, batch: 4, loss: 1.4044533967971802, acc: 57.8125, f1: 37.97369297369298, r: 0.35035461437286153
06/02/2019 10:52:58 step: 1033, epoch: 31, batch: 9, loss: 1.2140791416168213, acc: 56.25, f1: 27.947670901391408, r: 0.43296522005815324
06/02/2019 10:52:59 step: 1038, epoch: 31, batch: 14, loss: 1.1226081848144531, acc: 59.375, f1: 40.88620902906617, r: 0.3396456498749977
06/02/2019 10:52:59 step: 1043, epoch: 31, batch: 19, loss: 1.021669626235962, acc: 67.1875, f1: 45.625575900858436, r: 0.5708543123510585
06/02/2019 10:53:00 step: 1048, epoch: 31, batch: 24, loss: 1.0285264253616333, acc: 60.9375, f1: 39.46351385710654, r: 0.3456416572830353
06/02/2019 10:53:00 step: 1053, epoch: 31, batch: 29, loss: 1.2136934995651245, acc: 57.8125, f1: 36.230509355509355, r: 0.4050501942519022
06/02/2019 10:53:00 *** evaluating ***
06/02/2019 10:53:00 step: 32, epoch: 31, acc: 57.26495726495726, f1: 19.28318511198946, r: 0.31989558961183384
06/02/2019 10:53:00 *** epoch: 33 ***
06/02/2019 10:53:00 *** training ***
06/02/2019 10:53:01 step: 1061, epoch: 32, batch: 4, loss: 1.1920527219772339, acc: 62.5, f1: 43.33642547928262, r: 0.37090245087088375
06/02/2019 10:53:01 step: 1066, epoch: 32, batch: 9, loss: 1.037760853767395, acc: 67.1875, f1: 51.21902315135887, r: 0.37011393631422784
06/02/2019 10:53:02 step: 1071, epoch: 32, batch: 14, loss: 1.2155345678329468, acc: 50.0, f1: 30.157232704402514, r: 0.3162831566812327
06/02/2019 10:53:02 step: 1076, epoch: 32, batch: 19, loss: 1.2425676584243774, acc: 56.25, f1: 36.85973560973561, r: 0.3997550626833278
06/02/2019 10:53:02 step: 1081, epoch: 32, batch: 24, loss: 1.1819114685058594, acc: 62.5, f1: 39.23905083413711, r: 0.33418203857868534
06/02/2019 10:53:03 step: 1086, epoch: 32, batch: 29, loss: 0.8751335740089417, acc: 64.0625, f1: 35.287981859410436, r: 0.4412686558290447
06/02/2019 10:53:03 *** evaluating ***
06/02/2019 10:53:03 step: 33, epoch: 32, acc: 57.692307692307686, f1: 19.035497780054644, r: 0.3351018990856044
06/02/2019 10:53:03 *** epoch: 34 ***
06/02/2019 10:53:03 *** training ***
06/02/2019 10:53:03 step: 1094, epoch: 33, batch: 4, loss: 1.068447470664978, acc: 59.375, f1: 42.967032967032964, r: 0.41140167850933645
06/02/2019 10:53:04 step: 1099, epoch: 33, batch: 9, loss: 1.0744708776474, acc: 67.1875, f1: 44.82065553494125, r: 0.451137304214453
06/02/2019 10:53:04 step: 1104, epoch: 33, batch: 14, loss: 1.130255937576294, acc: 54.6875, f1: 29.834235380754574, r: 0.4452736626903925
06/02/2019 10:53:05 step: 1109, epoch: 33, batch: 19, loss: 1.0891144275665283, acc: 57.8125, f1: 32.34894590778519, r: 0.45764829341469665
06/02/2019 10:53:05 step: 1114, epoch: 33, batch: 24, loss: 0.9988057613372803, acc: 64.0625, f1: 44.55555555555556, r: 0.4687095130680153
06/02/2019 10:53:06 step: 1119, epoch: 33, batch: 29, loss: 0.9944420456886292, acc: 65.625, f1: 46.20585468229845, r: 0.43804227161509063
06/02/2019 10:53:06 *** evaluating ***
06/02/2019 10:53:06 step: 34, epoch: 33, acc: 57.692307692307686, f1: 21.309961164096023, r: 0.33255296272217316
06/02/2019 10:53:06 *** epoch: 35 ***
06/02/2019 10:53:06 *** training ***
06/02/2019 10:53:07 step: 1127, epoch: 34, batch: 4, loss: 1.232286810874939, acc: 56.25, f1: 23.146853146853147, r: 0.35687047404807476
06/02/2019 10:53:07 step: 1132, epoch: 34, batch: 9, loss: 1.0864557027816772, acc: 54.6875, f1: 32.26360751239022, r: 0.41169304003014556
06/02/2019 10:53:07 step: 1137, epoch: 34, batch: 14, loss: 0.9629859924316406, acc: 68.75, f1: 43.849726922253154, r: 0.4867623882668818
06/02/2019 10:53:08 step: 1142, epoch: 34, batch: 19, loss: 0.9634097218513489, acc: 65.625, f1: 55.34775979357261, r: 0.4523846536573689
06/02/2019 10:53:08 step: 1147, epoch: 34, batch: 24, loss: 0.9897026419639587, acc: 70.3125, f1: 61.40630797773655, r: 0.5020460152089221
06/02/2019 10:53:09 step: 1152, epoch: 34, batch: 29, loss: 1.03018057346344, acc: 59.375, f1: 29.89810688543407, r: 0.43518038914988766
06/02/2019 10:53:09 *** evaluating ***
06/02/2019 10:53:09 step: 35, epoch: 34, acc: 55.55555555555556, f1: 21.368775393963546, r: 0.3197530455287036
06/02/2019 10:53:09 *** epoch: 36 ***
06/02/2019 10:53:09 *** training ***
06/02/2019 10:53:10 step: 1160, epoch: 35, batch: 4, loss: 1.026311993598938, acc: 67.1875, f1: 44.96927803379416, r: 0.3703281470297713
06/02/2019 10:53:10 step: 1165, epoch: 35, batch: 9, loss: 1.0687369108200073, acc: 60.9375, f1: 30.945068445068447, r: 0.4239358137939909
06/02/2019 10:53:11 step: 1170, epoch: 35, batch: 14, loss: 0.9898614883422852, acc: 64.0625, f1: 35.67045882796307, r: 0.5149658022215307
06/02/2019 10:53:11 step: 1175, epoch: 35, batch: 19, loss: 1.251530647277832, acc: 53.125, f1: 37.15795376121463, r: 0.4308686570482463
06/02/2019 10:53:11 step: 1180, epoch: 35, batch: 24, loss: 0.9578640460968018, acc: 62.5, f1: 30.612041467304625, r: 0.34779727805878063
06/02/2019 10:53:12 step: 1185, epoch: 35, batch: 29, loss: 1.2969683408737183, acc: 62.5, f1: 27.807614273779684, r: 0.26667888110359195
06/02/2019 10:53:12 *** evaluating ***
06/02/2019 10:53:12 step: 36, epoch: 35, acc: 57.692307692307686, f1: 23.0751643908652, r: 0.327673520152715
06/02/2019 10:53:12 *** epoch: 37 ***
06/02/2019 10:53:12 *** training ***
06/02/2019 10:53:13 step: 1193, epoch: 36, batch: 4, loss: 1.1036499738693237, acc: 62.5, f1: 40.62363163656267, r: 0.47185224672886383
06/02/2019 10:53:13 step: 1198, epoch: 36, batch: 9, loss: 0.6931644082069397, acc: 76.5625, f1: 58.365008794215356, r: 0.5441755518633616
06/02/2019 10:53:13 step: 1203, epoch: 36, batch: 14, loss: 1.0765069723129272, acc: 67.1875, f1: 31.87441643323996, r: 0.4247564086348367
06/02/2019 10:53:14 step: 1208, epoch: 36, batch: 19, loss: 1.0852406024932861, acc: 59.375, f1: 45.29235132683408, r: 0.5001685044258714
06/02/2019 10:53:14 step: 1213, epoch: 36, batch: 24, loss: 1.2328358888626099, acc: 57.8125, f1: 44.990723562152134, r: 0.40372790816400217
06/02/2019 10:53:15 step: 1218, epoch: 36, batch: 29, loss: 0.9028104543685913, acc: 57.8125, f1: 28.165509259259263, r: 0.45818231787478153
06/02/2019 10:53:15 *** evaluating ***
06/02/2019 10:53:15 step: 37, epoch: 36, acc: 58.119658119658126, f1: 22.896613855234545, r: 0.33993908994060384
06/02/2019 10:53:15 *** epoch: 38 ***
06/02/2019 10:53:15 *** training ***
06/02/2019 10:53:15 step: 1226, epoch: 37, batch: 4, loss: 1.2386926412582397, acc: 59.375, f1: 34.740320336624926, r: 0.49340629075837705
06/02/2019 10:53:16 step: 1231, epoch: 37, batch: 9, loss: 0.9773602485656738, acc: 70.3125, f1: 45.70245489500147, r: 0.4020925176404513
06/02/2019 10:53:16 step: 1236, epoch: 37, batch: 14, loss: 1.09074068069458, acc: 67.1875, f1: 34.41885964912281, r: 0.34375071868960744
06/02/2019 10:53:17 step: 1241, epoch: 37, batch: 19, loss: 0.8829902410507202, acc: 65.625, f1: 30.41873520485021, r: 0.4937990449308782
06/02/2019 10:53:17 step: 1246, epoch: 37, batch: 24, loss: 0.8644271492958069, acc: 64.0625, f1: 35.40994623655914, r: 0.39600798638464924
06/02/2019 10:53:17 step: 1251, epoch: 37, batch: 29, loss: 1.27146577835083, acc: 65.625, f1: 39.62573099415205, r: 0.38122908730414906
06/02/2019 10:53:18 *** evaluating ***
06/02/2019 10:53:18 step: 38, epoch: 37, acc: 58.54700854700855, f1: 25.01720300379033, r: 0.34035937057135923
06/02/2019 10:53:18 *** epoch: 39 ***
06/02/2019 10:53:18 *** training ***
06/02/2019 10:53:18 step: 1259, epoch: 38, batch: 4, loss: 1.013938546180725, acc: 62.5, f1: 56.45530939648587, r: 0.4895583712972627
06/02/2019 10:53:19 step: 1264, epoch: 38, batch: 9, loss: 0.861558735370636, acc: 70.3125, f1: 45.544499723604204, r: 0.36916298307672857
06/02/2019 10:53:19 step: 1269, epoch: 38, batch: 14, loss: 0.9863916039466858, acc: 60.9375, f1: 35.30612244897959, r: 0.43935414047063087
06/02/2019 10:53:20 step: 1274, epoch: 38, batch: 19, loss: 0.7607660889625549, acc: 71.875, f1: 54.19531943212067, r: 0.4655615742466526
06/02/2019 10:53:20 step: 1279, epoch: 38, batch: 24, loss: 0.8338487148284912, acc: 65.625, f1: 31.370967741935484, r: 0.4461798287190339
06/02/2019 10:53:21 step: 1284, epoch: 38, batch: 29, loss: 0.8302814364433289, acc: 70.3125, f1: 58.075187969924805, r: 0.5237810251559013
06/02/2019 10:53:21 *** evaluating ***
06/02/2019 10:53:21 step: 39, epoch: 38, acc: 58.119658119658126, f1: 25.64810801283366, r: 0.33715374427470163
06/02/2019 10:53:21 *** epoch: 40 ***
06/02/2019 10:53:21 *** training ***
06/02/2019 10:53:21 step: 1292, epoch: 39, batch: 4, loss: 0.9829972386360168, acc: 65.625, f1: 38.902116402116405, r: 0.4237858203456175
06/02/2019 10:53:22 step: 1297, epoch: 39, batch: 9, loss: 1.0352586507797241, acc: 67.1875, f1: 58.23669936201906, r: 0.504476699143559
06/02/2019 10:53:22 step: 1302, epoch: 39, batch: 14, loss: 0.8024886846542358, acc: 71.875, f1: 63.74840845429082, r: 0.5228190813408322
06/02/2019 10:53:23 step: 1307, epoch: 39, batch: 19, loss: 1.0433911085128784, acc: 57.8125, f1: 40.19543410584573, r: 0.3524106274293449
06/02/2019 10:53:23 step: 1312, epoch: 39, batch: 24, loss: 0.900097131729126, acc: 67.1875, f1: 55.27766662220445, r: 0.45068529752954345
06/02/2019 10:53:23 step: 1317, epoch: 39, batch: 29, loss: 0.9033403396606445, acc: 70.3125, f1: 52.01310364353843, r: 0.4955762904166505
06/02/2019 10:53:24 *** evaluating ***
06/02/2019 10:53:24 step: 40, epoch: 39, acc: 57.26495726495726, f1: 22.947180426776228, r: 0.3493593746553179
06/02/2019 10:53:24 *** epoch: 41 ***
06/02/2019 10:53:24 *** training ***
06/02/2019 10:53:24 step: 1325, epoch: 40, batch: 4, loss: 0.9579393267631531, acc: 67.1875, f1: 42.785303776683094, r: 0.4797445180042714
06/02/2019 10:53:25 step: 1330, epoch: 40, batch: 9, loss: 0.9708526730537415, acc: 64.0625, f1: 39.55357142857143, r: 0.45436169332705106
06/02/2019 10:53:25 step: 1335, epoch: 40, batch: 14, loss: 1.2631893157958984, acc: 56.25, f1: 44.90800865800866, r: 0.388143100238595
06/02/2019 10:53:26 step: 1340, epoch: 40, batch: 19, loss: 0.8394773602485657, acc: 67.1875, f1: 52.21647869674185, r: 0.6737851726417871
06/02/2019 10:53:26 step: 1345, epoch: 40, batch: 24, loss: 0.8026304244995117, acc: 67.1875, f1: 49.572649572649574, r: 0.474070714681514
06/02/2019 10:53:26 step: 1350, epoch: 40, batch: 29, loss: 0.9909919500350952, acc: 62.5, f1: 42.90674603174603, r: 0.4589280247376816
06/02/2019 10:53:27 *** evaluating ***
06/02/2019 10:53:27 step: 41, epoch: 40, acc: 57.26495726495726, f1: 24.40382322705349, r: 0.35132130595596905
06/02/2019 10:53:27 *** epoch: 42 ***
06/02/2019 10:53:27 *** training ***
06/02/2019 10:53:27 step: 1358, epoch: 41, batch: 4, loss: 0.8452945351600647, acc: 75.0, f1: 53.18024421359577, r: 0.47438749945965203
06/02/2019 10:53:28 step: 1363, epoch: 41, batch: 9, loss: 0.8958717584609985, acc: 70.3125, f1: 56.92793317793318, r: 0.5151435924346898
06/02/2019 10:53:28 step: 1368, epoch: 41, batch: 14, loss: 0.9933992624282837, acc: 62.5, f1: 40.61020607073239, r: 0.5320171111137527
06/02/2019 10:53:28 step: 1373, epoch: 41, batch: 19, loss: 0.9003337025642395, acc: 67.1875, f1: 55.453686525115096, r: 0.4493312662076831
06/02/2019 10:53:29 step: 1378, epoch: 41, batch: 24, loss: 0.9310536980628967, acc: 64.0625, f1: 50.557605193106994, r: 0.484466144810724
06/02/2019 10:53:29 step: 1383, epoch: 41, batch: 29, loss: 1.116506814956665, acc: 57.8125, f1: 37.65880706287683, r: 0.4011403727195776
06/02/2019 10:53:29 *** evaluating ***
06/02/2019 10:53:30 step: 42, epoch: 41, acc: 58.119658119658126, f1: 25.383554864253394, r: 0.34877854659952623
06/02/2019 10:53:30 *** epoch: 43 ***
06/02/2019 10:53:30 *** training ***
06/02/2019 10:53:30 step: 1391, epoch: 42, batch: 4, loss: 0.7995339632034302, acc: 70.3125, f1: 47.93425062332625, r: 0.41701336845283
06/02/2019 10:53:31 step: 1396, epoch: 42, batch: 9, loss: 0.9344888925552368, acc: 64.0625, f1: 45.97559640621037, r: 0.4811482086750574
06/02/2019 10:53:31 step: 1401, epoch: 42, batch: 14, loss: 0.9030932188034058, acc: 64.0625, f1: 60.115646258503396, r: 0.5714027150848375
06/02/2019 10:53:31 step: 1406, epoch: 42, batch: 19, loss: 0.8715615272521973, acc: 76.5625, f1: 60.00075895567698, r: 0.5095517819047819
06/02/2019 10:53:32 step: 1411, epoch: 42, batch: 24, loss: 0.9055861830711365, acc: 62.5, f1: 43.615619774049776, r: 0.3029540741206771
06/02/2019 10:53:32 step: 1416, epoch: 42, batch: 29, loss: 0.8949211239814758, acc: 73.4375, f1: 59.29705215419501, r: 0.42230935120091695
06/02/2019 10:53:32 *** evaluating ***
06/02/2019 10:53:33 step: 43, epoch: 42, acc: 58.54700854700855, f1: 24.678471368308504, r: 0.3503026073816524
06/02/2019 10:53:33 *** epoch: 44 ***
06/02/2019 10:53:33 *** training ***
06/02/2019 10:53:33 step: 1424, epoch: 43, batch: 4, loss: 0.7702509164810181, acc: 73.4375, f1: 71.13374203436315, r: 0.5312495973158926
06/02/2019 10:53:33 step: 1429, epoch: 43, batch: 9, loss: 0.864432692527771, acc: 67.1875, f1: 37.01480263157895, r: 0.47347224838507845
06/02/2019 10:53:34 step: 1434, epoch: 43, batch: 14, loss: 1.2114171981811523, acc: 65.625, f1: 53.3689770374553, r: 0.4698734241671675
06/02/2019 10:53:34 step: 1439, epoch: 43, batch: 19, loss: 0.7644970417022705, acc: 70.3125, f1: 63.93586005830903, r: 0.5564325287287282
06/02/2019 10:53:35 step: 1444, epoch: 43, batch: 24, loss: 0.7396246194839478, acc: 73.4375, f1: 53.55158069443784, r: 0.5759525857941589
06/02/2019 10:53:35 step: 1449, epoch: 43, batch: 29, loss: 0.7828854918479919, acc: 76.5625, f1: 48.19019274376418, r: 0.5501627936613428
06/02/2019 10:53:35 *** evaluating ***
06/02/2019 10:53:35 step: 44, epoch: 43, acc: 58.54700854700855, f1: 26.62573792160948, r: 0.34912727131181487
06/02/2019 10:53:35 *** epoch: 45 ***
06/02/2019 10:53:35 *** training ***
06/02/2019 10:53:36 step: 1457, epoch: 44, batch: 4, loss: 0.5317991971969604, acc: 82.8125, f1: 69.2248677248677, r: 0.5720234595920053
06/02/2019 10:53:36 step: 1462, epoch: 44, batch: 9, loss: 0.8282307386398315, acc: 70.3125, f1: 60.53841991341991, r: 0.6201500011571425
06/02/2019 10:53:36 step: 1467, epoch: 44, batch: 14, loss: 0.9625163674354553, acc: 65.625, f1: 44.7008547008547, r: 0.5494690700012925
06/02/2019 10:53:37 step: 1472, epoch: 44, batch: 19, loss: 0.5568742156028748, acc: 84.375, f1: 61.74045889542258, r: 0.48427686644961204
06/02/2019 10:53:37 step: 1477, epoch: 44, batch: 24, loss: 0.8107483983039856, acc: 79.6875, f1: 65.12512697386647, r: 0.4799671991890417
06/02/2019 10:53:38 step: 1482, epoch: 44, batch: 29, loss: 0.8709458708763123, acc: 62.5, f1: 47.333783631655976, r: 0.4964340668543532
06/02/2019 10:53:38 *** evaluating ***
06/02/2019 10:53:38 step: 45, epoch: 44, acc: 58.119658119658126, f1: 22.924330892771756, r: 0.35674664901710634
06/02/2019 10:53:38 *** epoch: 46 ***
06/02/2019 10:53:38 *** training ***
06/02/2019 10:53:39 step: 1490, epoch: 45, batch: 4, loss: 0.9383580684661865, acc: 65.625, f1: 37.95918367346939, r: 0.44238387716705135
06/02/2019 10:53:39 step: 1495, epoch: 45, batch: 9, loss: 1.0227882862091064, acc: 67.1875, f1: 52.257785176217375, r: 0.5193389456573736
06/02/2019 10:53:39 step: 1500, epoch: 45, batch: 14, loss: 0.5830069780349731, acc: 76.5625, f1: 64.91618796496846, r: 0.3928032614035728
06/02/2019 10:53:40 step: 1505, epoch: 45, batch: 19, loss: 0.6603299975395203, acc: 75.0, f1: 50.59855110875519, r: 0.4931289231681705
06/02/2019 10:53:40 step: 1510, epoch: 45, batch: 24, loss: 0.9423690438270569, acc: 64.0625, f1: 42.73601046698873, r: 0.5220775565936342
06/02/2019 10:53:41 step: 1515, epoch: 45, batch: 29, loss: 0.6745258569717407, acc: 79.6875, f1: 63.19784382284382, r: 0.6098772906640988
06/02/2019 10:53:41 *** evaluating ***
06/02/2019 10:53:41 step: 46, epoch: 45, acc: 60.68376068376068, f1: 27.234299516908212, r: 0.3601836670156042
06/02/2019 10:53:41 *** epoch: 47 ***
06/02/2019 10:53:41 *** training ***
06/02/2019 10:53:41 step: 1523, epoch: 46, batch: 4, loss: 0.6479894518852234, acc: 76.5625, f1: 45.923622543340855, r: 0.4883632473830735
06/02/2019 10:53:42 step: 1528, epoch: 46, batch: 9, loss: 0.8629658222198486, acc: 70.3125, f1: 43.20393374741201, r: 0.4756685101854996
06/02/2019 10:53:42 step: 1533, epoch: 46, batch: 14, loss: 0.850841760635376, acc: 68.75, f1: 49.0566576280862, r: 0.5058517399761296
06/02/2019 10:53:43 step: 1538, epoch: 46, batch: 19, loss: 0.9545193314552307, acc: 60.9375, f1: 46.959669332550696, r: 0.47468271838542764
06/02/2019 10:53:43 step: 1543, epoch: 46, batch: 24, loss: 0.7869336605072021, acc: 76.5625, f1: 39.77564102564102, r: 0.4060962092581594
06/02/2019 10:53:43 step: 1548, epoch: 46, batch: 29, loss: 0.856747567653656, acc: 67.1875, f1: 50.41594142657972, r: 0.5595014398557303
06/02/2019 10:53:44 *** evaluating ***
06/02/2019 10:53:44 step: 47, epoch: 46, acc: 60.256410256410255, f1: 28.934131651953553, r: 0.3637675886012207
06/02/2019 10:53:44 *** epoch: 48 ***
06/02/2019 10:53:44 *** training ***
06/02/2019 10:53:44 step: 1556, epoch: 47, batch: 4, loss: 0.7422918081283569, acc: 75.0, f1: 58.53106353106352, r: 0.5244040817755974
06/02/2019 10:53:45 step: 1561, epoch: 47, batch: 9, loss: 0.6835383176803589, acc: 70.3125, f1: 64.77599365645753, r: 0.4855074424025317
06/02/2019 10:53:45 step: 1566, epoch: 47, batch: 14, loss: 0.7523884177207947, acc: 75.0, f1: 51.614803537998775, r: 0.5531862657642108
06/02/2019 10:53:45 step: 1571, epoch: 47, batch: 19, loss: 0.8258674144744873, acc: 70.3125, f1: 42.01150479954828, r: 0.4494820957775909
06/02/2019 10:53:46 step: 1576, epoch: 47, batch: 24, loss: 0.6343345642089844, acc: 71.875, f1: 46.515151515151516, r: 0.6103359596421111
06/02/2019 10:53:46 step: 1581, epoch: 47, batch: 29, loss: 1.2424161434173584, acc: 56.25, f1: 45.64171122994652, r: 0.5004977214025578
06/02/2019 10:53:46 *** evaluating ***
06/02/2019 10:53:47 step: 48, epoch: 47, acc: 58.54700854700855, f1: 26.05988939492518, r: 0.35670555214594624
06/02/2019 10:53:47 *** epoch: 49 ***
06/02/2019 10:53:47 *** training ***
06/02/2019 10:53:47 step: 1589, epoch: 48, batch: 4, loss: 0.7827103734016418, acc: 71.875, f1: 42.59199134199134, r: 0.4475738176198743
06/02/2019 10:53:48 step: 1594, epoch: 48, batch: 9, loss: 1.0389361381530762, acc: 62.5, f1: 41.90901360544218, r: 0.4322161120740199
06/02/2019 10:53:48 step: 1599, epoch: 48, batch: 14, loss: 0.6867561340332031, acc: 75.0, f1: 66.72553528918579, r: 0.5513057994180133
06/02/2019 10:53:48 step: 1604, epoch: 48, batch: 19, loss: 0.6316274404525757, acc: 71.875, f1: 46.416666666666664, r: 0.48122778717377374
06/02/2019 10:53:49 step: 1609, epoch: 48, batch: 24, loss: 0.8366394639015198, acc: 67.1875, f1: 57.102075485394025, r: 0.481710806264466
06/02/2019 10:53:49 step: 1614, epoch: 48, batch: 29, loss: 0.5178830623626709, acc: 81.25, f1: 77.70050548955216, r: 0.5433980751676459
06/02/2019 10:53:50 *** evaluating ***
06/02/2019 10:53:50 step: 49, epoch: 48, acc: 59.401709401709404, f1: 27.771685970351477, r: 0.3653623069513653
06/02/2019 10:53:50 *** epoch: 50 ***
06/02/2019 10:53:50 *** training ***
06/02/2019 10:53:50 step: 1622, epoch: 49, batch: 4, loss: 0.6115829348564148, acc: 76.5625, f1: 65.46589079197776, r: 0.5974746236839802
06/02/2019 10:53:51 step: 1627, epoch: 49, batch: 9, loss: 0.6723241806030273, acc: 71.875, f1: 40.04741330939289, r: 0.4684085552586036
06/02/2019 10:53:51 step: 1632, epoch: 49, batch: 14, loss: 0.6187719702720642, acc: 76.5625, f1: 59.56376956376956, r: 0.5468834032038763
06/02/2019 10:53:51 step: 1637, epoch: 49, batch: 19, loss: 0.9540001153945923, acc: 62.5, f1: 44.086051079527486, r: 0.5353950790099142
06/02/2019 10:53:52 step: 1642, epoch: 49, batch: 24, loss: 0.6330875754356384, acc: 81.25, f1: 59.83745421245421, r: 0.5590597966897177
06/02/2019 10:53:52 step: 1647, epoch: 49, batch: 29, loss: 0.9845872521400452, acc: 68.75, f1: 47.2152981427175, r: 0.43246517091321446
06/02/2019 10:53:52 *** evaluating ***
06/02/2019 10:53:53 step: 50, epoch: 49, acc: 58.54700854700855, f1: 24.703589161583206, r: 0.3595078866297862
06/02/2019 10:53:53 *** epoch: 51 ***
06/02/2019 10:53:53 *** training ***
06/02/2019 10:53:53 step: 1655, epoch: 50, batch: 4, loss: 0.6769508719444275, acc: 75.0, f1: 70.96255596255597, r: 0.5374698181449972
06/02/2019 10:53:54 step: 1660, epoch: 50, batch: 9, loss: 1.0311145782470703, acc: 67.1875, f1: 43.8088798737886, r: 0.44918255389884726
06/02/2019 10:53:54 step: 1665, epoch: 50, batch: 14, loss: 0.7280827164649963, acc: 71.875, f1: 43.71622842685462, r: 0.4953685215089132
06/02/2019 10:53:54 step: 1670, epoch: 50, batch: 19, loss: 0.615047037601471, acc: 76.5625, f1: 71.14094625704533, r: 0.6035086435901942
06/02/2019 10:53:55 step: 1675, epoch: 50, batch: 24, loss: 0.8588639497756958, acc: 67.1875, f1: 53.55296150718352, r: 0.5915697474526825
06/02/2019 10:53:55 step: 1680, epoch: 50, batch: 29, loss: 0.7754085063934326, acc: 73.4375, f1: 58.18215752146323, r: 0.57350785393369
06/02/2019 10:53:56 *** evaluating ***
06/02/2019 10:53:56 step: 51, epoch: 50, acc: 58.119658119658126, f1: 25.652537821655468, r: 0.3703574932271644
06/02/2019 10:53:56 *** epoch: 52 ***
06/02/2019 10:53:56 *** training ***
06/02/2019 10:53:56 step: 1688, epoch: 51, batch: 4, loss: 0.6098381280899048, acc: 78.125, f1: 63.845710533017034, r: 0.5918724865280858
06/02/2019 10:53:56 step: 1693, epoch: 51, batch: 9, loss: 0.7109388113021851, acc: 79.6875, f1: 70.30045351473923, r: 0.4964049285437836
06/02/2019 10:53:57 step: 1698, epoch: 51, batch: 14, loss: 0.5689763426780701, acc: 81.25, f1: 55.801806102558, r: 0.5605054726044584
06/02/2019 10:53:57 step: 1703, epoch: 51, batch: 19, loss: 0.8361958265304565, acc: 78.125, f1: 65.12531328320802, r: 0.6136299987285823
06/02/2019 10:53:58 step: 1708, epoch: 51, batch: 24, loss: 0.9471834301948547, acc: 65.625, f1: 49.7541395386223, r: 0.5009015746225266
06/02/2019 10:53:58 step: 1713, epoch: 51, batch: 29, loss: 0.8980683088302612, acc: 67.1875, f1: 53.39220303506017, r: 0.526446417864926
06/02/2019 10:53:58 *** evaluating ***
06/02/2019 10:53:59 step: 52, epoch: 51, acc: 57.692307692307686, f1: 22.709868862697196, r: 0.3545052996650673
06/02/2019 10:53:59 *** epoch: 53 ***
06/02/2019 10:53:59 *** training ***
06/02/2019 10:53:59 step: 1721, epoch: 52, batch: 4, loss: 0.5267825722694397, acc: 82.8125, f1: 81.10859925223893, r: 0.5406598518037347
06/02/2019 10:54:00 step: 1726, epoch: 52, batch: 9, loss: 0.6933771371841431, acc: 76.5625, f1: 52.24900372761947, r: 0.522843299397915
06/02/2019 10:54:00 step: 1731, epoch: 52, batch: 14, loss: 0.47439369559288025, acc: 81.25, f1: 74.15178571428571, r: 0.6265514006057079
06/02/2019 10:54:00 step: 1736, epoch: 52, batch: 19, loss: 0.6302791833877563, acc: 79.6875, f1: 65.36885461255208, r: 0.5234126536098395
06/02/2019 10:54:01 step: 1741, epoch: 52, batch: 24, loss: 0.6883525252342224, acc: 78.125, f1: 53.79495504495505, r: 0.5414541197739005
06/02/2019 10:54:01 step: 1746, epoch: 52, batch: 29, loss: 0.8411325216293335, acc: 71.875, f1: 48.77795160617438, r: 0.5486625632023173
06/02/2019 10:54:02 *** evaluating ***
06/02/2019 10:54:02 step: 53, epoch: 52, acc: 58.54700854700855, f1: 27.590460363621975, r: 0.3576130271148953
06/02/2019 10:54:02 *** epoch: 54 ***
06/02/2019 10:54:02 *** training ***
06/02/2019 10:54:02 step: 1754, epoch: 53, batch: 4, loss: 0.6570922136306763, acc: 75.0, f1: 74.34498834498834, r: 0.5124940740309865
06/02/2019 10:54:03 step: 1759, epoch: 53, batch: 9, loss: 0.7333609461784363, acc: 73.4375, f1: 69.55203133885135, r: 0.5574786441043813
06/02/2019 10:54:03 step: 1764, epoch: 53, batch: 14, loss: 0.6469545960426331, acc: 75.0, f1: 66.97831625738603, r: 0.6694452119999235
06/02/2019 10:54:03 step: 1769, epoch: 53, batch: 19, loss: 0.6722936630249023, acc: 75.0, f1: 54.341372912801475, r: 0.4871840938796371
06/02/2019 10:54:04 step: 1774, epoch: 53, batch: 24, loss: 0.7480767965316772, acc: 71.875, f1: 65.25000000000001, r: 0.675188472606187
06/02/2019 10:54:04 step: 1779, epoch: 53, batch: 29, loss: 0.7143583297729492, acc: 70.3125, f1: 59.12653522947642, r: 0.5664358435393371
06/02/2019 10:54:05 *** evaluating ***
06/02/2019 10:54:05 step: 54, epoch: 53, acc: 56.837606837606835, f1: 25.864137087467952, r: 0.36310811954757527
06/02/2019 10:54:05 *** epoch: 55 ***
06/02/2019 10:54:05 *** training ***
06/02/2019 10:54:05 step: 1787, epoch: 54, batch: 4, loss: 0.5125283598899841, acc: 84.375, f1: 61.457596831836184, r: 0.6179736381475024
06/02/2019 10:54:06 step: 1792, epoch: 54, batch: 9, loss: 0.9227044582366943, acc: 70.3125, f1: 63.981801712779976, r: 0.5398437949447916
06/02/2019 10:54:06 step: 1797, epoch: 54, batch: 14, loss: 0.520429790019989, acc: 82.8125, f1: 66.31717847214215, r: 0.5684524639539896
06/02/2019 10:54:06 step: 1802, epoch: 54, batch: 19, loss: 0.8513512015342712, acc: 70.3125, f1: 43.29085266585266, r: 0.5196987830436588
06/02/2019 10:54:07 step: 1807, epoch: 54, batch: 24, loss: 0.5837149620056152, acc: 82.8125, f1: 70.27777777777777, r: 0.4519771327471427
06/02/2019 10:54:07 step: 1812, epoch: 54, batch: 29, loss: 0.9494155645370483, acc: 62.5, f1: 47.607115107115106, r: 0.546250131711632
06/02/2019 10:54:07 *** evaluating ***
06/02/2019 10:54:08 step: 55, epoch: 54, acc: 56.837606837606835, f1: 23.534926229149335, r: 0.35923527688809775
06/02/2019 10:54:08 *** epoch: 56 ***
06/02/2019 10:54:08 *** training ***
06/02/2019 10:54:08 step: 1820, epoch: 55, batch: 4, loss: 0.7738975882530212, acc: 68.75, f1: 66.0254329004329, r: 0.6210748073166267
06/02/2019 10:54:08 step: 1825, epoch: 55, batch: 9, loss: 0.8211811780929565, acc: 75.0, f1: 64.69965764083412, r: 0.658489193705073
06/02/2019 10:54:09 step: 1830, epoch: 55, batch: 14, loss: 0.59527587890625, acc: 81.25, f1: 69.96467817896388, r: 0.5331527381839058
06/02/2019 10:54:09 step: 1835, epoch: 55, batch: 19, loss: 0.5896473526954651, acc: 78.125, f1: 65.61932465293809, r: 0.544214340593731
06/02/2019 10:54:10 step: 1840, epoch: 55, batch: 24, loss: 0.5915467739105225, acc: 84.375, f1: 75.58982744708253, r: 0.4869671127663928
06/02/2019 10:54:10 step: 1845, epoch: 55, batch: 29, loss: 0.689771831035614, acc: 76.5625, f1: 70.06711915535445, r: 0.6528222623606351
06/02/2019 10:54:10 *** evaluating ***
06/02/2019 10:54:10 step: 56, epoch: 55, acc: 59.401709401709404, f1: 25.092769502104638, r: 0.3547405711516462
06/02/2019 10:54:10 *** epoch: 57 ***
06/02/2019 10:54:10 *** training ***
06/02/2019 10:54:11 step: 1853, epoch: 56, batch: 4, loss: 0.532488226890564, acc: 82.8125, f1: 82.4763206718094, r: 0.5624127429439152
06/02/2019 10:54:11 step: 1858, epoch: 56, batch: 9, loss: 0.5587812066078186, acc: 76.5625, f1: 60.471331389698726, r: 0.552539243701743
06/02/2019 10:54:12 step: 1863, epoch: 56, batch: 14, loss: 0.5250909328460693, acc: 81.25, f1: 60.55550433339481, r: 0.6428430444811908
06/02/2019 10:54:12 step: 1868, epoch: 56, batch: 19, loss: 0.7656618356704712, acc: 75.0, f1: 64.19870273690871, r: 0.48470792227745174
06/02/2019 10:54:12 step: 1873, epoch: 56, batch: 24, loss: 0.6538291573524475, acc: 79.6875, f1: 50.03839453155343, r: 0.4574635594784142
06/02/2019 10:54:13 step: 1878, epoch: 56, batch: 29, loss: 0.43025368452072144, acc: 84.375, f1: 78.3843537414966, r: 0.6869006750870454
06/02/2019 10:54:13 *** evaluating ***
06/02/2019 10:54:13 step: 57, epoch: 56, acc: 56.41025641025641, f1: 23.698853336664083, r: 0.34740626699882
06/02/2019 10:54:13 *** epoch: 58 ***
06/02/2019 10:54:13 *** training ***
06/02/2019 10:54:14 step: 1886, epoch: 57, batch: 4, loss: 0.7649975419044495, acc: 75.0, f1: 56.62698412698413, r: 0.49482330773995475
06/02/2019 10:54:14 step: 1891, epoch: 57, batch: 9, loss: 0.6155401468276978, acc: 81.25, f1: 65.9352094859791, r: 0.6490589961325792
06/02/2019 10:54:14 step: 1896, epoch: 57, batch: 14, loss: 0.5954465866088867, acc: 82.8125, f1: 50.047619047619044, r: 0.5137133568805345
06/02/2019 10:54:15 step: 1901, epoch: 57, batch: 19, loss: 0.36781176924705505, acc: 90.625, f1: 76.72342709288522, r: 0.5431347942664455
06/02/2019 10:54:15 step: 1906, epoch: 57, batch: 24, loss: 0.7245267033576965, acc: 76.5625, f1: 62.755492293535774, r: 0.5222649440820256
06/02/2019 10:54:16 step: 1911, epoch: 57, batch: 29, loss: 0.5064428448677063, acc: 82.8125, f1: 66.57212885154063, r: 0.6160943211839776
06/02/2019 10:54:16 *** evaluating ***
06/02/2019 10:54:16 step: 58, epoch: 57, acc: 57.692307692307686, f1: 26.87145001141421, r: 0.35635734705846167
06/02/2019 10:54:16 *** epoch: 59 ***
06/02/2019 10:54:16 *** training ***
06/02/2019 10:54:16 step: 1919, epoch: 58, batch: 4, loss: 0.6356532573699951, acc: 79.6875, f1: 58.00406154018162, r: 0.5187227209149764
06/02/2019 10:54:17 step: 1924, epoch: 58, batch: 9, loss: 0.8176225423812866, acc: 67.1875, f1: 50.15775890775891, r: 0.5444143774905107
06/02/2019 10:54:17 step: 1929, epoch: 58, batch: 14, loss: 0.7110172510147095, acc: 73.4375, f1: 59.59362833183588, r: 0.5803521461777307
06/02/2019 10:54:18 step: 1934, epoch: 58, batch: 19, loss: 0.483692467212677, acc: 82.8125, f1: 76.76173499702911, r: 0.49971736133147004
06/02/2019 10:54:18 step: 1939, epoch: 58, batch: 24, loss: 0.3878270387649536, acc: 87.5, f1: 60.03082062864671, r: 0.6192495677933508
06/02/2019 10:54:18 step: 1944, epoch: 58, batch: 29, loss: 0.5686333179473877, acc: 75.0, f1: 61.31093099178205, r: 0.523547995651536
06/02/2019 10:54:18 *** evaluating ***
06/02/2019 10:54:19 step: 59, epoch: 58, acc: 57.692307692307686, f1: 27.42330966869506, r: 0.3595521641855646
06/02/2019 10:54:19 *** epoch: 60 ***
06/02/2019 10:54:19 *** training ***
06/02/2019 10:54:19 step: 1952, epoch: 59, batch: 4, loss: 0.5108548402786255, acc: 84.375, f1: 67.97399172754048, r: 0.6055210216680326
06/02/2019 10:54:19 step: 1957, epoch: 59, batch: 9, loss: 0.5562968254089355, acc: 81.25, f1: 77.13040771864301, r: 0.6277546640968661
06/02/2019 10:54:20 step: 1962, epoch: 59, batch: 14, loss: 0.4903560280799866, acc: 85.9375, f1: 54.29446778711484, r: 0.5648264793990693
06/02/2019 10:54:20 step: 1967, epoch: 59, batch: 19, loss: 0.39247098565101624, acc: 90.625, f1: 87.05855547960812, r: 0.568832842726994
06/02/2019 10:54:21 step: 1972, epoch: 59, batch: 24, loss: 0.6360014081001282, acc: 84.375, f1: 79.552388346643, r: 0.6652080051927943
06/02/2019 10:54:21 step: 1977, epoch: 59, batch: 29, loss: 0.7456650137901306, acc: 71.875, f1: 49.583604954367665, r: 0.5814429295644478
06/02/2019 10:54:21 *** evaluating ***
06/02/2019 10:54:21 step: 60, epoch: 59, acc: 58.119658119658126, f1: 27.77620100826983, r: 0.3559362845458948
06/02/2019 10:54:21 *** epoch: 61 ***
06/02/2019 10:54:21 *** training ***
06/02/2019 10:54:22 step: 1985, epoch: 60, batch: 4, loss: 0.6110123991966248, acc: 76.5625, f1: 50.55251765778082, r: 0.5272253502449732
06/02/2019 10:54:22 step: 1990, epoch: 60, batch: 9, loss: 0.41898584365844727, acc: 87.5, f1: 73.55012180310719, r: 0.61108705905765
06/02/2019 10:54:23 step: 1995, epoch: 60, batch: 14, loss: 0.5753402709960938, acc: 76.5625, f1: 60.46877122584826, r: 0.4600670365489573
06/02/2019 10:54:23 step: 2000, epoch: 60, batch: 19, loss: 0.7209368944168091, acc: 78.125, f1: 61.03816526610644, r: 0.6144403996036036
06/02/2019 10:54:24 step: 2005, epoch: 60, batch: 24, loss: 0.6134583950042725, acc: 78.125, f1: 54.91351546448997, r: 0.49250581029609763
06/02/2019 10:54:24 step: 2010, epoch: 60, batch: 29, loss: 0.41129958629608154, acc: 85.9375, f1: 82.335377500754, r: 0.6531323933113584
06/02/2019 10:54:24 *** evaluating ***
06/02/2019 10:54:24 step: 61, epoch: 60, acc: 56.41025641025641, f1: 22.860448960460676, r: 0.3592351950452612
06/02/2019 10:54:24 *** epoch: 62 ***
06/02/2019 10:54:24 *** training ***
06/02/2019 10:54:25 step: 2018, epoch: 61, batch: 4, loss: 0.6005991101264954, acc: 81.25, f1: 80.27818218366998, r: 0.6771973697988255
06/02/2019 10:54:25 step: 2023, epoch: 61, batch: 9, loss: 0.5981835126876831, acc: 81.25, f1: 60.08765513373808, r: 0.49629676143324225
06/02/2019 10:54:26 step: 2028, epoch: 61, batch: 14, loss: 0.43553435802459717, acc: 82.8125, f1: 81.39431996574854, r: 0.5966752085848869
06/02/2019 10:54:26 step: 2033, epoch: 61, batch: 19, loss: 0.3340931236743927, acc: 89.0625, f1: 87.53333759486344, r: 0.6102765100038676
06/02/2019 10:54:26 step: 2038, epoch: 61, batch: 24, loss: 0.4674738347530365, acc: 84.375, f1: 69.46150950498776, r: 0.47111687206427183
06/02/2019 10:54:27 step: 2043, epoch: 61, batch: 29, loss: 0.5325667262077332, acc: 78.125, f1: 46.26213111507229, r: 0.5610443734426277
06/02/2019 10:54:27 *** evaluating ***
06/02/2019 10:54:27 step: 62, epoch: 61, acc: 55.55555555555556, f1: 23.23938043830087, r: 0.3555602063941126
06/02/2019 10:54:27 *** epoch: 63 ***
06/02/2019 10:54:27 *** training ***
06/02/2019 10:54:28 step: 2051, epoch: 62, batch: 4, loss: 0.6925114393234253, acc: 75.0, f1: 57.06125601439659, r: 0.4593945016859333
06/02/2019 10:54:28 step: 2056, epoch: 62, batch: 9, loss: 0.8351792693138123, acc: 73.4375, f1: 67.79839208410637, r: 0.5449366887408541
06/02/2019 10:54:29 step: 2061, epoch: 62, batch: 14, loss: 0.4270819425582886, acc: 89.0625, f1: 84.06323877068557, r: 0.7069147808510474
06/02/2019 10:54:29 step: 2066, epoch: 62, batch: 19, loss: 0.3953310549259186, acc: 87.5, f1: 81.90126405171998, r: 0.6530358667491727
06/02/2019 10:54:29 step: 2071, epoch: 62, batch: 24, loss: 0.7391201853752136, acc: 70.3125, f1: 60.12770562770563, r: 0.6430388080163681
06/02/2019 10:54:30 step: 2076, epoch: 62, batch: 29, loss: 0.34756314754486084, acc: 85.9375, f1: 68.81734006734007, r: 0.6831388715371617
06/02/2019 10:54:30 *** evaluating ***
06/02/2019 10:54:30 step: 63, epoch: 62, acc: 56.837606837606835, f1: 27.291772352568167, r: 0.3654327356435796
06/02/2019 10:54:30 *** epoch: 64 ***
06/02/2019 10:54:30 *** training ***
06/02/2019 10:54:31 step: 2084, epoch: 63, batch: 4, loss: 0.5305929183959961, acc: 82.8125, f1: 81.95887445887446, r: 0.7285022269997812
06/02/2019 10:54:31 step: 2089, epoch: 63, batch: 9, loss: 0.6404013633728027, acc: 75.0, f1: 62.367175130921936, r: 0.6012878486450806
06/02/2019 10:54:31 step: 2094, epoch: 63, batch: 14, loss: 0.4825652539730072, acc: 79.6875, f1: 48.36885865457294, r: 0.4552173191447978
06/02/2019 10:54:32 step: 2099, epoch: 63, batch: 19, loss: 0.6540822386741638, acc: 79.6875, f1: 55.8407738095238, r: 0.48058665559227365
06/02/2019 10:54:32 step: 2104, epoch: 63, batch: 24, loss: 0.5972320437431335, acc: 75.0, f1: 59.96259029927759, r: 0.6945189627613538
06/02/2019 10:54:33 step: 2109, epoch: 63, batch: 29, loss: 0.46952107548713684, acc: 84.375, f1: 70.97609925196132, r: 0.5847977615205201
06/02/2019 10:54:33 *** evaluating ***
06/02/2019 10:54:33 step: 64, epoch: 63, acc: 57.692307692307686, f1: 24.787761952155137, r: 0.3534309551153691
06/02/2019 10:54:33 *** epoch: 65 ***
06/02/2019 10:54:33 *** training ***
06/02/2019 10:54:33 step: 2117, epoch: 64, batch: 4, loss: 0.36318597197532654, acc: 85.9375, f1: 74.76113935406698, r: 0.7385869378332077
06/02/2019 10:54:34 step: 2122, epoch: 64, batch: 9, loss: 0.6318455934524536, acc: 76.5625, f1: 73.59254046446165, r: 0.5617433061785715
06/02/2019 10:54:34 step: 2127, epoch: 64, batch: 14, loss: 0.4200612008571625, acc: 89.0625, f1: 65.46243587040551, r: 0.5974662717781235
06/02/2019 10:54:34 step: 2132, epoch: 64, batch: 19, loss: 0.4228219985961914, acc: 82.8125, f1: 72.45379141930866, r: 0.6132816089348048
06/02/2019 10:54:35 step: 2137, epoch: 64, batch: 24, loss: 0.5105588436126709, acc: 78.125, f1: 58.920717592592595, r: 0.5594149968615705
06/02/2019 10:54:35 step: 2142, epoch: 64, batch: 29, loss: 0.4750728905200958, acc: 82.8125, f1: 72.70670995670996, r: 0.6066785487990074
06/02/2019 10:54:35 *** evaluating ***
06/02/2019 10:54:36 step: 65, epoch: 64, acc: 58.119658119658126, f1: 27.761102066657617, r: 0.35611678314832984
06/02/2019 10:54:36 *** epoch: 66 ***
06/02/2019 10:54:36 *** training ***
06/02/2019 10:54:36 step: 2150, epoch: 65, batch: 4, loss: 0.6732079982757568, acc: 73.4375, f1: 63.30409356725146, r: 0.6354046001718696
06/02/2019 10:54:37 step: 2155, epoch: 65, batch: 9, loss: 0.5188907384872437, acc: 81.25, f1: 63.87639640836571, r: 0.58483443950375
06/02/2019 10:54:37 step: 2160, epoch: 65, batch: 14, loss: 0.5833255052566528, acc: 84.375, f1: 83.64207221350078, r: 0.5796305306692975
06/02/2019 10:54:38 step: 2165, epoch: 65, batch: 19, loss: 0.479486346244812, acc: 81.25, f1: 68.83962780514506, r: 0.5705590957922024
06/02/2019 10:54:38 step: 2170, epoch: 65, batch: 24, loss: 0.399982213973999, acc: 89.0625, f1: 88.03942089656374, r: 0.6543435488391147
06/02/2019 10:54:39 step: 2175, epoch: 65, batch: 29, loss: 0.5588914155960083, acc: 81.25, f1: 70.34119203236851, r: 0.5975771058143857
06/02/2019 10:54:39 *** evaluating ***
06/02/2019 10:54:39 step: 66, epoch: 65, acc: 57.692307692307686, f1: 26.355448714279174, r: 0.35631715556729343
06/02/2019 10:54:39 *** epoch: 67 ***
06/02/2019 10:54:39 *** training ***
06/02/2019 10:54:39 step: 2183, epoch: 66, batch: 4, loss: 0.3694092333316803, acc: 84.375, f1: 74.44444444444444, r: 0.6529716561825936
06/02/2019 10:54:40 step: 2188, epoch: 66, batch: 9, loss: 0.5490331649780273, acc: 82.8125, f1: 76.51721014492753, r: 0.6610858498880215
06/02/2019 10:54:40 step: 2193, epoch: 66, batch: 14, loss: 0.5510625839233398, acc: 82.8125, f1: 68.68558002397177, r: 0.5860467085122781
06/02/2019 10:54:41 step: 2198, epoch: 66, batch: 19, loss: 0.48751604557037354, acc: 81.25, f1: 65.08501914751915, r: 0.7230103732439981
06/02/2019 10:54:41 step: 2203, epoch: 66, batch: 24, loss: 0.5512106418609619, acc: 82.8125, f1: 66.91899585921325, r: 0.6336714315967559
06/02/2019 10:54:41 step: 2208, epoch: 66, batch: 29, loss: 0.517419695854187, acc: 87.5, f1: 81.82283666154635, r: 0.6332191685750858
06/02/2019 10:54:42 *** evaluating ***
06/02/2019 10:54:42 step: 67, epoch: 66, acc: 57.692307692307686, f1: 27.442238926973218, r: 0.35663746401681434
06/02/2019 10:54:42 *** epoch: 68 ***
06/02/2019 10:54:42 *** training ***
06/02/2019 10:54:42 step: 2216, epoch: 67, batch: 4, loss: 0.3636435568332672, acc: 89.0625, f1: 82.85474787937844, r: 0.5688127244161041
06/02/2019 10:54:43 step: 2221, epoch: 67, batch: 9, loss: 0.4796786904335022, acc: 82.8125, f1: 77.65715901745314, r: 0.6862392107954876
06/02/2019 10:54:43 step: 2226, epoch: 67, batch: 14, loss: 0.43308281898498535, acc: 82.8125, f1: 80.7986699291047, r: 0.5826354732723251
06/02/2019 10:54:44 step: 2231, epoch: 67, batch: 19, loss: 0.3307605981826782, acc: 89.0625, f1: 87.4476320582878, r: 0.7641559001902564
06/02/2019 10:54:44 step: 2236, epoch: 67, batch: 24, loss: 0.45982176065444946, acc: 85.9375, f1: 76.86474589835933, r: 0.5839596313726472
06/02/2019 10:54:44 step: 2241, epoch: 67, batch: 29, loss: 0.404455304145813, acc: 89.0625, f1: 83.56417112299465, r: 0.6563910210433399
06/02/2019 10:54:45 *** evaluating ***
06/02/2019 10:54:45 step: 68, epoch: 67, acc: 58.54700854700855, f1: 27.81409800892397, r: 0.35395458995608087
06/02/2019 10:54:45 *** epoch: 69 ***
06/02/2019 10:54:45 *** training ***
06/02/2019 10:54:45 step: 2249, epoch: 68, batch: 4, loss: 0.5292097330093384, acc: 79.6875, f1: 62.10673822301729, r: 0.6449527041411107
06/02/2019 10:54:46 step: 2254, epoch: 68, batch: 9, loss: 0.5386741161346436, acc: 81.25, f1: 78.42268842268842, r: 0.6082931065252719
06/02/2019 10:54:46 step: 2259, epoch: 68, batch: 14, loss: 0.586698055267334, acc: 82.8125, f1: 72.73023829414805, r: 0.5169446273013019
06/02/2019 10:54:46 step: 2264, epoch: 68, batch: 19, loss: 0.607273280620575, acc: 75.0, f1: 73.09265010351966, r: 0.6984935529862701
06/02/2019 10:54:47 step: 2269, epoch: 68, batch: 24, loss: 0.5443106293678284, acc: 82.8125, f1: 66.92424242424242, r: 0.5129333359055017
06/02/2019 10:54:47 step: 2274, epoch: 68, batch: 29, loss: 0.500140905380249, acc: 84.375, f1: 70.06594304388423, r: 0.6115732132832953
06/02/2019 10:54:47 *** evaluating ***
06/02/2019 10:54:48 step: 69, epoch: 68, acc: 57.26495726495726, f1: 26.860538340801497, r: 0.3570176340742104
06/02/2019 10:54:48 *** epoch: 70 ***
06/02/2019 10:54:48 *** training ***
06/02/2019 10:54:48 step: 2282, epoch: 69, batch: 4, loss: 0.5374152660369873, acc: 79.6875, f1: 82.33672124976472, r: 0.7000008684486679
06/02/2019 10:54:49 step: 2287, epoch: 69, batch: 9, loss: 0.41191259026527405, acc: 85.9375, f1: 73.16269841269842, r: 0.6403653087463324
06/02/2019 10:54:49 step: 2292, epoch: 69, batch: 14, loss: 0.6159025430679321, acc: 79.6875, f1: 76.16853192890059, r: 0.5667611356964535
06/02/2019 10:54:49 step: 2297, epoch: 69, batch: 19, loss: 0.63451087474823, acc: 78.125, f1: 65.87358276643991, r: 0.5855740068034722
06/02/2019 10:54:50 step: 2302, epoch: 69, batch: 24, loss: 0.5861021876335144, acc: 78.125, f1: 64.91576515663738, r: 0.6743984340174175
06/02/2019 10:54:50 step: 2307, epoch: 69, batch: 29, loss: 0.5507442355155945, acc: 84.375, f1: 71.8682722130998, r: 0.5681480586555238
06/02/2019 10:54:50 *** evaluating ***
06/02/2019 10:54:51 step: 70, epoch: 69, acc: 58.119658119658126, f1: 27.62723054924754, r: 0.35537374940171745
06/02/2019 10:54:51 *** epoch: 71 ***
06/02/2019 10:54:51 *** training ***
06/02/2019 10:54:51 step: 2315, epoch: 70, batch: 4, loss: 0.36308571696281433, acc: 90.625, f1: 87.44584634059068, r: 0.6078556849743154
06/02/2019 10:54:51 step: 2320, epoch: 70, batch: 9, loss: 0.4218788743019104, acc: 84.375, f1: 60.65739703163637, r: 0.62363147848765
06/02/2019 10:54:52 step: 2325, epoch: 70, batch: 14, loss: 0.4184122383594513, acc: 89.0625, f1: 81.70107463275164, r: 0.5859519657451567
06/02/2019 10:54:52 step: 2330, epoch: 70, batch: 19, loss: 0.33832865953445435, acc: 90.625, f1: 87.70841670001334, r: 0.6233259715520735
06/02/2019 10:54:53 step: 2335, epoch: 70, batch: 24, loss: 0.6063327193260193, acc: 81.25, f1: 77.77514870682573, r: 0.6342896412610154
06/02/2019 10:54:53 step: 2340, epoch: 70, batch: 29, loss: 0.35771462321281433, acc: 87.5, f1: 85.59262790761842, r: 0.6191206929662091
06/02/2019 10:54:53 *** evaluating ***
06/02/2019 10:54:54 step: 71, epoch: 70, acc: 57.692307692307686, f1: 27.457006096695952, r: 0.35298897375324856
06/02/2019 10:54:54 *** epoch: 72 ***
06/02/2019 10:54:54 *** training ***
06/02/2019 10:54:54 step: 2348, epoch: 71, batch: 4, loss: 0.5369082093238831, acc: 84.375, f1: 74.49172692332746, r: 0.5870804725814274
06/02/2019 10:54:54 step: 2353, epoch: 71, batch: 9, loss: 0.6623727679252625, acc: 82.8125, f1: 72.79353087388802, r: 0.7341790326007352
06/02/2019 10:54:55 step: 2358, epoch: 71, batch: 14, loss: 0.376229852437973, acc: 89.0625, f1: 84.50828157349896, r: 0.7637549766541052
06/02/2019 10:54:55 step: 2363, epoch: 71, batch: 19, loss: 0.3619084358215332, acc: 85.9375, f1: 79.33093420329824, r: 0.7568454793749323
06/02/2019 10:54:56 step: 2368, epoch: 71, batch: 24, loss: 0.3289623260498047, acc: 87.5, f1: 85.99196125511915, r: 0.6664374006172399
06/02/2019 10:54:56 step: 2373, epoch: 71, batch: 29, loss: 0.48093166947364807, acc: 85.9375, f1: 83.3543041276661, r: 0.5524273411511008
06/02/2019 10:54:56 *** evaluating ***
06/02/2019 10:54:57 step: 72, epoch: 71, acc: 59.82905982905983, f1: 26.915833040833036, r: 0.3639584786810132
06/02/2019 10:54:57 *** epoch: 73 ***
06/02/2019 10:54:57 *** training ***
06/02/2019 10:54:57 step: 2381, epoch: 72, batch: 4, loss: 0.45987969636917114, acc: 84.375, f1: 70.67398913143595, r: 0.6708836130990645
06/02/2019 10:54:57 step: 2386, epoch: 72, batch: 9, loss: 0.4851200580596924, acc: 79.6875, f1: 73.77203188967894, r: 0.6709012051519431
06/02/2019 10:54:58 step: 2391, epoch: 72, batch: 14, loss: 0.37805426120758057, acc: 87.5, f1: 85.82417582417582, r: 0.5703272750474886
06/02/2019 10:54:58 step: 2396, epoch: 72, batch: 19, loss: 0.4501569867134094, acc: 84.375, f1: 75.27642796248935, r: 0.6074268439672943
06/02/2019 10:54:59 step: 2401, epoch: 72, batch: 24, loss: 0.3861783742904663, acc: 87.5, f1: 70.33029878618113, r: 0.6596604151356829
06/02/2019 10:54:59 step: 2406, epoch: 72, batch: 29, loss: 0.43783676624298096, acc: 84.375, f1: 74.30796382752459, r: 0.5983354337856488
06/02/2019 10:54:59 *** evaluating ***
06/02/2019 10:54:59 step: 73, epoch: 72, acc: 58.119658119658126, f1: 28.850938408253835, r: 0.35568078515308355
06/02/2019 10:54:59 *** epoch: 74 ***
06/02/2019 10:54:59 *** training ***
06/02/2019 10:55:00 step: 2414, epoch: 73, batch: 4, loss: 0.32123005390167236, acc: 90.625, f1: 88.25354609929079, r: 0.6662962090408733
06/02/2019 10:55:00 step: 2419, epoch: 73, batch: 9, loss: 0.4531328082084656, acc: 79.6875, f1: 62.53554971560228, r: 0.71535133150084
06/02/2019 10:55:00 step: 2424, epoch: 73, batch: 14, loss: 0.5430362820625305, acc: 81.25, f1: 74.0633186740436, r: 0.5862505259032447
06/02/2019 10:55:01 step: 2429, epoch: 73, batch: 19, loss: 0.4286767244338989, acc: 78.125, f1: 76.04205318491033, r: 0.6775778833695891
06/02/2019 10:55:01 step: 2434, epoch: 73, batch: 24, loss: 0.382413387298584, acc: 89.0625, f1: 72.13919413919415, r: 0.5813723158356976
06/02/2019 10:55:02 step: 2439, epoch: 73, batch: 29, loss: 0.49040165543556213, acc: 84.375, f1: 57.79745989304812, r: 0.6185778666132717
06/02/2019 10:55:02 *** evaluating ***
06/02/2019 10:55:02 step: 74, epoch: 73, acc: 59.401709401709404, f1: 27.8148452621343, r: 0.36382574457229183
06/02/2019 10:55:02 *** epoch: 75 ***
06/02/2019 10:55:02 *** training ***
06/02/2019 10:55:02 step: 2447, epoch: 74, batch: 4, loss: 0.35580724477767944, acc: 85.9375, f1: 84.0871578099839, r: 0.6903737647569793
06/02/2019 10:55:03 step: 2452, epoch: 74, batch: 9, loss: 0.42670154571533203, acc: 84.375, f1: 55.29251700680271, r: 0.4842553323480212
06/02/2019 10:55:03 step: 2457, epoch: 74, batch: 14, loss: 0.4139680564403534, acc: 84.375, f1: 77.98642533936652, r: 0.7201544404166447
06/02/2019 10:55:04 step: 2462, epoch: 74, batch: 19, loss: 0.41733694076538086, acc: 82.8125, f1: 70.87560738581146, r: 0.6169107517587611
06/02/2019 10:55:04 step: 2467, epoch: 74, batch: 24, loss: 0.3831498622894287, acc: 84.375, f1: 83.27651515151516, r: 0.5545066872277197
06/02/2019 10:55:04 step: 2472, epoch: 74, batch: 29, loss: 0.3362349569797516, acc: 92.1875, f1: 91.69091560069003, r: 0.6292582755041288
06/02/2019 10:55:05 *** evaluating ***
06/02/2019 10:55:05 step: 75, epoch: 74, acc: 57.692307692307686, f1: 26.011296926694893, r: 0.3531832499951978
06/02/2019 10:55:05 *** epoch: 76 ***
06/02/2019 10:55:05 *** training ***
06/02/2019 10:55:05 step: 2480, epoch: 75, batch: 4, loss: 0.3656899631023407, acc: 85.9375, f1: 71.93877551020408, r: 0.6165015108869695
06/02/2019 10:55:06 step: 2485, epoch: 75, batch: 9, loss: 0.4181281626224518, acc: 81.25, f1: 67.43672243672243, r: 0.6268427750876558
06/02/2019 10:55:06 step: 2490, epoch: 75, batch: 14, loss: 0.6541388630867004, acc: 73.4375, f1: 62.436323366555925, r: 0.5171031326238529
06/02/2019 10:55:07 step: 2495, epoch: 75, batch: 19, loss: 0.4165283441543579, acc: 84.375, f1: 78.28023926428183, r: 0.6271447635152185
06/02/2019 10:55:07 step: 2500, epoch: 75, batch: 24, loss: 0.5211734771728516, acc: 84.375, f1: 68.49368928173276, r: 0.6297034900683621
06/02/2019 10:55:08 step: 2505, epoch: 75, batch: 29, loss: 0.32005298137664795, acc: 89.0625, f1: 67.1189223790042, r: 0.5940863779034842
06/02/2019 10:55:08 *** evaluating ***
06/02/2019 10:55:08 step: 76, epoch: 75, acc: 57.26495726495726, f1: 27.109667541155336, r: 0.3452591578614072
06/02/2019 10:55:08 *** epoch: 77 ***
06/02/2019 10:55:08 *** training ***
06/02/2019 10:55:09 step: 2513, epoch: 76, batch: 4, loss: 0.5506357550621033, acc: 82.8125, f1: 77.60651909666689, r: 0.6384141911725707
06/02/2019 10:55:09 step: 2518, epoch: 76, batch: 9, loss: 0.30025315284729004, acc: 85.9375, f1: 86.81778230631672, r: 0.7475640741730962
06/02/2019 10:55:09 step: 2523, epoch: 76, batch: 14, loss: 0.3140430748462677, acc: 87.5, f1: 84.05702394510428, r: 0.7239595081917064
06/02/2019 10:55:10 step: 2528, epoch: 76, batch: 19, loss: 0.4614872932434082, acc: 87.5, f1: 85.57492071881607, r: 0.6496254559000534
06/02/2019 10:55:10 step: 2533, epoch: 76, batch: 24, loss: 0.4426769018173218, acc: 82.8125, f1: 70.74925074925076, r: 0.6338470205061038
06/02/2019 10:55:11 step: 2538, epoch: 76, batch: 29, loss: 0.4667215049266815, acc: 85.9375, f1: 82.53873319662793, r: 0.6039194357462233
06/02/2019 10:55:11 *** evaluating ***
06/02/2019 10:55:11 step: 77, epoch: 76, acc: 58.119658119658126, f1: 25.681563471975622, r: 0.35656412398775006
06/02/2019 10:55:11 *** epoch: 78 ***
06/02/2019 10:55:11 *** training ***
06/02/2019 10:55:12 step: 2546, epoch: 77, batch: 4, loss: 0.5071867108345032, acc: 82.8125, f1: 73.72020699606907, r: 0.584840134608636
06/02/2019 10:55:12 step: 2551, epoch: 77, batch: 9, loss: 0.34127897024154663, acc: 89.0625, f1: 85.11989192840257, r: 0.6457254975962321
06/02/2019 10:55:12 step: 2556, epoch: 77, batch: 14, loss: 0.3483790159225464, acc: 89.0625, f1: 64.28781092037435, r: 0.6043628852497831
06/02/2019 10:55:13 step: 2561, epoch: 77, batch: 19, loss: 0.4244880676269531, acc: 87.5, f1: 80.5891160993202, r: 0.675647007576674
06/02/2019 10:55:13 step: 2566, epoch: 77, batch: 24, loss: 0.5382925271987915, acc: 79.6875, f1: 73.24829931972789, r: 0.5070165148537907
06/02/2019 10:55:14 step: 2571, epoch: 77, batch: 29, loss: 0.37039220333099365, acc: 84.375, f1: 69.34960022577307, r: 0.6107413073267826
06/02/2019 10:55:14 *** evaluating ***
06/02/2019 10:55:14 step: 78, epoch: 77, acc: 58.119658119658126, f1: 28.8270471469111, r: 0.3593803040471979
06/02/2019 10:55:14 *** epoch: 79 ***
06/02/2019 10:55:14 *** training ***
06/02/2019 10:55:15 step: 2579, epoch: 78, batch: 4, loss: 0.3152991533279419, acc: 89.0625, f1: 83.73349339735896, r: 0.6133556160631758
06/02/2019 10:55:15 step: 2584, epoch: 78, batch: 9, loss: 0.2772963345050812, acc: 90.625, f1: 91.18369604362813, r: 0.6745285295539027
06/02/2019 10:55:15 step: 2589, epoch: 78, batch: 14, loss: 0.40709906816482544, acc: 85.9375, f1: 72.97282046459136, r: 0.6359715646863658
06/02/2019 10:55:16 step: 2594, epoch: 78, batch: 19, loss: 0.3935903310775757, acc: 90.625, f1: 76.48311184939092, r: 0.6400513534534739
06/02/2019 10:55:16 step: 2599, epoch: 78, batch: 24, loss: 0.4179655909538269, acc: 85.9375, f1: 83.11526846809105, r: 0.6837329453094696
06/02/2019 10:55:17 step: 2604, epoch: 78, batch: 29, loss: 0.43256890773773193, acc: 85.9375, f1: 75.40058414670216, r: 0.641311324211154
06/02/2019 10:55:17 *** evaluating ***
06/02/2019 10:55:17 step: 79, epoch: 78, acc: 58.97435897435898, f1: 28.900701442155047, r: 0.3531657919455862
06/02/2019 10:55:17 *** epoch: 80 ***
06/02/2019 10:55:17 *** training ***
06/02/2019 10:55:17 step: 2612, epoch: 79, batch: 4, loss: 0.4984426498413086, acc: 82.8125, f1: 70.17875693596733, r: 0.6030447737238259
06/02/2019 10:55:18 step: 2617, epoch: 79, batch: 9, loss: 0.40816399455070496, acc: 85.9375, f1: 84.83283056812468, r: 0.7217837767290585
06/02/2019 10:55:18 step: 2622, epoch: 79, batch: 14, loss: 0.48812803626060486, acc: 81.25, f1: 68.32792207792208, r: 0.6468552435973189
06/02/2019 10:55:19 step: 2627, epoch: 79, batch: 19, loss: 0.4136042296886444, acc: 85.9375, f1: 82.95891479121292, r: 0.5529101321439395
06/02/2019 10:55:19 step: 2632, epoch: 79, batch: 24, loss: 0.42422130703926086, acc: 85.9375, f1: 78.42851592851592, r: 0.7016360552527071
06/02/2019 10:55:19 step: 2637, epoch: 79, batch: 29, loss: 0.4364531636238098, acc: 87.5, f1: 76.87435500515996, r: 0.6987696019899275
06/02/2019 10:55:20 *** evaluating ***
06/02/2019 10:55:20 step: 80, epoch: 79, acc: 57.26495726495726, f1: 26.706066835336284, r: 0.3565711158026713
06/02/2019 10:55:20 *** epoch: 81 ***
06/02/2019 10:55:20 *** training ***
06/02/2019 10:55:20 step: 2645, epoch: 80, batch: 4, loss: 0.3416435420513153, acc: 87.5, f1: 87.71189120809615, r: 0.6979557753636696
06/02/2019 10:55:21 step: 2650, epoch: 80, batch: 9, loss: 0.22813040018081665, acc: 87.5, f1: 84.43826015254585, r: 0.6481000853565844
06/02/2019 10:55:21 step: 2655, epoch: 80, batch: 14, loss: 0.35433050990104675, acc: 89.0625, f1: 77.15180906713165, r: 0.6139290740315093
06/02/2019 10:55:21 step: 2660, epoch: 80, batch: 19, loss: 0.34604138135910034, acc: 90.625, f1: 84.49404761904762, r: 0.6294395086495502
06/02/2019 10:55:22 step: 2665, epoch: 80, batch: 24, loss: 0.4899497330188751, acc: 82.8125, f1: 72.06810754604872, r: 0.6387258504104462
06/02/2019 10:55:22 step: 2670, epoch: 80, batch: 29, loss: 0.32679980993270874, acc: 89.0625, f1: 85.27331704734493, r: 0.6114831751798242
06/02/2019 10:55:22 *** evaluating ***
06/02/2019 10:55:23 step: 81, epoch: 80, acc: 58.54700854700855, f1: 28.24702173632396, r: 0.3573109525295181
06/02/2019 10:55:23 *** epoch: 82 ***
06/02/2019 10:55:23 *** training ***
06/02/2019 10:55:23 step: 2678, epoch: 81, batch: 4, loss: 0.47824954986572266, acc: 82.8125, f1: 81.26458885941645, r: 0.6257936657066124
06/02/2019 10:55:24 step: 2683, epoch: 81, batch: 9, loss: 0.4147782027721405, acc: 87.5, f1: 76.69536812393956, r: 0.5609938705450774
06/02/2019 10:55:24 step: 2688, epoch: 81, batch: 14, loss: 0.3473452925682068, acc: 89.0625, f1: 64.3892675647334, r: 0.6321922229401958
06/02/2019 10:55:25 step: 2693, epoch: 81, batch: 19, loss: 0.3442642092704773, acc: 90.625, f1: 67.08589655958077, r: 0.5820638249143363
06/02/2019 10:55:25 step: 2698, epoch: 81, batch: 24, loss: 0.38455891609191895, acc: 87.5, f1: 85.92064312746582, r: 0.7580163006829986
06/02/2019 10:55:26 step: 2703, epoch: 81, batch: 29, loss: 0.4701070785522461, acc: 79.6875, f1: 60.64504558497694, r: 0.6470072278575469
06/02/2019 10:55:26 *** evaluating ***
06/02/2019 10:55:26 step: 82, epoch: 81, acc: 56.41025641025641, f1: 27.467311594385922, r: 0.3499137708109277
06/02/2019 10:55:26 *** epoch: 83 ***
06/02/2019 10:55:26 *** training ***
06/02/2019 10:55:26 step: 2711, epoch: 82, batch: 4, loss: 0.3790206015110016, acc: 87.5, f1: 75.74258563074352, r: 0.6741929649621123
06/02/2019 10:55:27 step: 2716, epoch: 82, batch: 9, loss: 0.6744993925094604, acc: 85.9375, f1: 77.63546176046177, r: 0.5922518703531279
06/02/2019 10:55:27 step: 2721, epoch: 82, batch: 14, loss: 0.3463176488876343, acc: 85.9375, f1: 66.77366780045352, r: 0.7266064634918021
06/02/2019 10:55:28 step: 2726, epoch: 82, batch: 19, loss: 0.44879022240638733, acc: 84.375, f1: 85.82825145868624, r: 0.6687077604234191
06/02/2019 10:55:28 step: 2731, epoch: 82, batch: 24, loss: 0.4112730622291565, acc: 82.8125, f1: 76.61248516838579, r: 0.7392476416459445
06/02/2019 10:55:28 step: 2736, epoch: 82, batch: 29, loss: 0.39482709765434265, acc: 89.0625, f1: 76.60249472749472, r: 0.6901913724745379
06/02/2019 10:55:29 *** evaluating ***
06/02/2019 10:55:29 step: 83, epoch: 82, acc: 56.41025641025641, f1: 27.303640481131453, r: 0.3531359716092857
06/02/2019 10:55:29 *** epoch: 84 ***
06/02/2019 10:55:29 *** training ***
06/02/2019 10:55:29 step: 2744, epoch: 83, batch: 4, loss: 0.45312437415122986, acc: 87.5, f1: 85.57900432900432, r: 0.699409780622525
06/02/2019 10:55:30 step: 2749, epoch: 83, batch: 9, loss: 0.27730128169059753, acc: 90.625, f1: 86.70377241805812, r: 0.6973801591605869
06/02/2019 10:55:30 step: 2754, epoch: 83, batch: 14, loss: 0.2983393371105194, acc: 89.0625, f1: 88.67464826839827, r: 0.7443722701506466
06/02/2019 10:55:31 step: 2759, epoch: 83, batch: 19, loss: 0.1915973722934723, acc: 93.75, f1: 91.73431253825018, r: 0.6188392610197813
06/02/2019 10:55:31 step: 2764, epoch: 83, batch: 24, loss: 0.5453932285308838, acc: 82.8125, f1: 77.11956521739131, r: 0.6790531221861391
06/02/2019 10:55:31 step: 2769, epoch: 83, batch: 29, loss: 0.5866889953613281, acc: 85.9375, f1: 81.11528764511269, r: 0.5107088696933589
06/02/2019 10:55:32 *** evaluating ***
06/02/2019 10:55:32 step: 84, epoch: 83, acc: 58.97435897435898, f1: 28.147279895359333, r: 0.35521101306816455
06/02/2019 10:55:32 *** epoch: 85 ***
06/02/2019 10:55:32 *** training ***
06/02/2019 10:55:32 step: 2777, epoch: 84, batch: 4, loss: 0.46021324396133423, acc: 89.0625, f1: 86.97538252837792, r: 0.6167593324703801
06/02/2019 10:55:33 step: 2782, epoch: 84, batch: 9, loss: 0.4117928743362427, acc: 87.5, f1: 76.47227856659906, r: 0.6653788820539221
06/02/2019 10:55:33 step: 2787, epoch: 84, batch: 14, loss: 0.2969694435596466, acc: 90.625, f1: 84.781438074121, r: 0.6053800669710373
06/02/2019 10:55:33 step: 2792, epoch: 84, batch: 19, loss: 0.3781619966030121, acc: 84.375, f1: 68.13044662309369, r: 0.6805089319182518
06/02/2019 10:55:34 step: 2797, epoch: 84, batch: 24, loss: 0.35816115140914917, acc: 85.9375, f1: 85.4503367003367, r: 0.7043155577122606
06/02/2019 10:55:34 step: 2802, epoch: 84, batch: 29, loss: 0.49073299765586853, acc: 85.9375, f1: 84.73920646149884, r: 0.6729428974017477
06/02/2019 10:55:34 *** evaluating ***
06/02/2019 10:55:35 step: 85, epoch: 84, acc: 58.119658119658126, f1: 27.142625827743377, r: 0.35549863526554565
06/02/2019 10:55:35 *** epoch: 86 ***
06/02/2019 10:55:35 *** training ***
06/02/2019 10:55:35 step: 2810, epoch: 85, batch: 4, loss: 0.5360719561576843, acc: 78.125, f1: 65.98214285714286, r: 0.6080829626427147
06/02/2019 10:55:36 step: 2815, epoch: 85, batch: 9, loss: 0.3414422869682312, acc: 90.625, f1: 77.98266045548654, r: 0.6908807619219407
06/02/2019 10:55:36 step: 2820, epoch: 85, batch: 14, loss: 0.44674789905548096, acc: 85.9375, f1: 73.02655677655677, r: 0.626478144167392
06/02/2019 10:55:37 step: 2825, epoch: 85, batch: 19, loss: 0.23673246800899506, acc: 89.0625, f1: 73.95622895622895, r: 0.6498963401196777
06/02/2019 10:55:37 step: 2830, epoch: 85, batch: 24, loss: 0.43868371844291687, acc: 81.25, f1: 67.93877032520325, r: 0.6746613160440535
06/02/2019 10:55:37 step: 2835, epoch: 85, batch: 29, loss: 0.36960506439208984, acc: 90.625, f1: 85.37445887445888, r: 0.6706752049251419
06/02/2019 10:55:38 *** evaluating ***
06/02/2019 10:55:38 step: 86, epoch: 85, acc: 59.401709401709404, f1: 28.15435310072155, r: 0.35361899900203553
06/02/2019 10:55:38 *** epoch: 87 ***
06/02/2019 10:55:38 *** training ***
06/02/2019 10:55:38 step: 2843, epoch: 86, batch: 4, loss: 0.3116573691368103, acc: 85.9375, f1: 73.9926951579494, r: 0.6462000600405653
06/02/2019 10:55:39 step: 2848, epoch: 86, batch: 9, loss: 0.5083036422729492, acc: 82.8125, f1: 67.56238859180036, r: 0.5763739187135465
06/02/2019 10:55:39 step: 2853, epoch: 86, batch: 14, loss: 0.21831190586090088, acc: 92.1875, f1: 94.32561294312445, r: 0.6438669927391374
06/02/2019 10:55:39 step: 2858, epoch: 86, batch: 19, loss: 0.2635591924190521, acc: 92.1875, f1: 79.19741809767386, r: 0.712752289122284
06/02/2019 10:55:40 step: 2863, epoch: 86, batch: 24, loss: 0.34279799461364746, acc: 89.0625, f1: 63.10786435786435, r: 0.5065272112880487
06/02/2019 10:55:40 step: 2868, epoch: 86, batch: 29, loss: 0.2753906548023224, acc: 92.1875, f1: 92.28565365025467, r: 0.6523346923387122
06/02/2019 10:55:41 *** evaluating ***
06/02/2019 10:55:41 step: 87, epoch: 86, acc: 58.119658119658126, f1: 28.53592191837908, r: 0.3500059876035367
06/02/2019 10:55:41 *** epoch: 88 ***
06/02/2019 10:55:41 *** training ***
06/02/2019 10:55:41 step: 2876, epoch: 87, batch: 4, loss: 0.27619415521621704, acc: 92.1875, f1: 85.35021265284423, r: 0.6323368298964899
06/02/2019 10:55:42 step: 2881, epoch: 87, batch: 9, loss: 0.26413461565971375, acc: 92.1875, f1: 83.02808302808303, r: 0.6680563596672016
06/02/2019 10:55:42 step: 2886, epoch: 87, batch: 14, loss: 0.31707507371902466, acc: 85.9375, f1: 61.27770665047604, r: 0.5972167954609193
06/02/2019 10:55:43 step: 2891, epoch: 87, batch: 19, loss: 0.25988492369651794, acc: 90.625, f1: 63.78678706838872, r: 0.6695220383380578
06/02/2019 10:55:43 step: 2896, epoch: 87, batch: 24, loss: 0.2907744348049164, acc: 89.0625, f1: 80.83380387728212, r: 0.5087570563127561
06/02/2019 10:55:43 step: 2901, epoch: 87, batch: 29, loss: 0.4224649667739868, acc: 79.6875, f1: 70.99758085052204, r: 0.6835152857545942
06/02/2019 10:55:44 *** evaluating ***
06/02/2019 10:55:44 step: 88, epoch: 87, acc: 58.54700854700855, f1: 30.250065909308788, r: 0.36332080451337473
06/02/2019 10:55:44 *** epoch: 89 ***
06/02/2019 10:55:44 *** training ***
06/02/2019 10:55:44 step: 2909, epoch: 88, batch: 4, loss: 0.21626028418540955, acc: 92.1875, f1: 84.06910132474043, r: 0.6784349334793628
06/02/2019 10:55:45 step: 2914, epoch: 88, batch: 9, loss: 0.15641099214553833, acc: 95.3125, f1: 94.82689210950079, r: 0.7389925453503177
06/02/2019 10:55:45 step: 2919, epoch: 88, batch: 14, loss: 0.1484902799129486, acc: 95.3125, f1: 84.53513499440236, r: 0.5971215598446337
06/02/2019 10:55:45 step: 2924, epoch: 88, batch: 19, loss: 0.33645308017730713, acc: 89.0625, f1: 87.42168960607498, r: 0.6420394261827527
06/02/2019 10:55:46 step: 2929, epoch: 88, batch: 24, loss: 0.3674776554107666, acc: 90.625, f1: 73.40747834276739, r: 0.6301430517411767
06/02/2019 10:55:46 step: 2934, epoch: 88, batch: 29, loss: 0.37579384446144104, acc: 84.375, f1: 70.97947191697192, r: 0.6353009028419494
06/02/2019 10:55:46 *** evaluating ***
06/02/2019 10:55:46 step: 89, epoch: 88, acc: 57.692307692307686, f1: 26.641317165510713, r: 0.3565280468087558
06/02/2019 10:55:46 *** epoch: 90 ***
06/02/2019 10:55:46 *** training ***
06/02/2019 10:55:47 step: 2942, epoch: 89, batch: 4, loss: 0.2939623296260834, acc: 89.0625, f1: 82.68953634085211, r: 0.6759592579799145
06/02/2019 10:55:47 step: 2947, epoch: 89, batch: 9, loss: 0.2728603482246399, acc: 92.1875, f1: 75.65779528185544, r: 0.6490179377481986
06/02/2019 10:55:48 step: 2952, epoch: 89, batch: 14, loss: 0.31625473499298096, acc: 92.1875, f1: 91.98283795057989, r: 0.6914812169125663
06/02/2019 10:55:48 step: 2957, epoch: 89, batch: 19, loss: 0.4508335292339325, acc: 84.375, f1: 62.95215201465202, r: 0.6099078483425175
06/02/2019 10:55:48 step: 2962, epoch: 89, batch: 24, loss: 0.4111008644104004, acc: 84.375, f1: 79.47211518640088, r: 0.5644166913089342
06/02/2019 10:55:49 step: 2967, epoch: 89, batch: 29, loss: 0.26195141673088074, acc: 92.1875, f1: 90.51700680272108, r: 0.6232289848683762
06/02/2019 10:55:49 *** evaluating ***
06/02/2019 10:55:49 step: 90, epoch: 89, acc: 59.401709401709404, f1: 36.24786350340511, r: 0.36080603980531306
06/02/2019 10:55:49 *** epoch: 91 ***
06/02/2019 10:55:49 *** training ***
06/02/2019 10:55:50 step: 2975, epoch: 90, batch: 4, loss: 0.22579827904701233, acc: 87.5, f1: 76.27435064935065, r: 0.7408913676350671
06/02/2019 10:55:50 step: 2980, epoch: 90, batch: 9, loss: 0.44092071056365967, acc: 87.5, f1: 70.89575777075777, r: 0.5982790626741684
06/02/2019 10:55:51 step: 2985, epoch: 90, batch: 14, loss: 0.523038923740387, acc: 81.25, f1: 70.36796536796537, r: 0.6940540136869062
06/02/2019 10:55:51 step: 2990, epoch: 90, batch: 19, loss: 0.44623804092407227, acc: 87.5, f1: 90.07697044334975, r: 0.664647606003283
06/02/2019 10:55:52 step: 2995, epoch: 90, batch: 24, loss: 0.5455557703971863, acc: 79.6875, f1: 61.00567228616009, r: 0.5635994336829078
06/02/2019 10:55:52 step: 3000, epoch: 90, batch: 29, loss: 0.3212370276451111, acc: 89.0625, f1: 88.88659020518776, r: 0.7073276956158026
06/02/2019 10:55:52 *** evaluating ***
06/02/2019 10:55:52 step: 91, epoch: 90, acc: 58.97435897435898, f1: 26.456896551724135, r: 0.3575296863901738
06/02/2019 10:55:52 *** epoch: 92 ***
06/02/2019 10:55:52 *** training ***
06/02/2019 10:55:53 step: 3008, epoch: 91, batch: 4, loss: 0.5225727558135986, acc: 81.25, f1: 63.06112365894975, r: 0.6410318548825121
06/02/2019 10:55:53 step: 3013, epoch: 91, batch: 9, loss: 0.2914181351661682, acc: 90.625, f1: 89.13565426170467, r: 0.5627890395618054
06/02/2019 10:55:54 step: 3018, epoch: 91, batch: 14, loss: 0.300090491771698, acc: 92.1875, f1: 79.27466720896726, r: 0.5904039095716475
06/02/2019 10:55:54 step: 3023, epoch: 91, batch: 19, loss: 0.3331613540649414, acc: 85.9375, f1: 83.67394179894181, r: 0.6852741195716255
06/02/2019 10:55:54 step: 3028, epoch: 91, batch: 24, loss: 0.4217003583908081, acc: 84.375, f1: 72.86393895320536, r: 0.6645279725650087
06/02/2019 10:55:55 step: 3033, epoch: 91, batch: 29, loss: 0.3489445745944977, acc: 92.1875, f1: 70.07491789819376, r: 0.6583013686124051
06/02/2019 10:55:55 *** evaluating ***
06/02/2019 10:55:55 step: 92, epoch: 91, acc: 57.692307692307686, f1: 27.18011445957693, r: 0.35086250892323834
06/02/2019 10:55:55 *** epoch: 93 ***
06/02/2019 10:55:55 *** training ***
06/02/2019 10:55:56 step: 3041, epoch: 92, batch: 4, loss: 0.3550282120704651, acc: 85.9375, f1: 84.74191385956091, r: 0.5290522915195187
06/02/2019 10:55:56 step: 3046, epoch: 92, batch: 9, loss: 0.35736000537872314, acc: 82.8125, f1: 67.97435123016518, r: 0.529956707250049
06/02/2019 10:55:57 step: 3051, epoch: 92, batch: 14, loss: 0.7095091938972473, acc: 82.8125, f1: 74.63369963369964, r: 0.7059029046818727
06/02/2019 10:55:57 step: 3056, epoch: 92, batch: 19, loss: 0.24258655309677124, acc: 92.1875, f1: 87.55357142857142, r: 0.5521139284071772
06/02/2019 10:55:58 step: 3061, epoch: 92, batch: 24, loss: 0.39839237928390503, acc: 79.6875, f1: 65.4173545752493, r: 0.5905806114035254
06/02/2019 10:55:58 step: 3066, epoch: 92, batch: 29, loss: 0.3781515657901764, acc: 85.9375, f1: 83.13725490196077, r: 0.5916547994684964
06/02/2019 10:55:58 *** evaluating ***
06/02/2019 10:55:59 step: 93, epoch: 92, acc: 58.97435897435898, f1: 27.872346951294325, r: 0.3571754831162402
06/02/2019 10:55:59 *** epoch: 94 ***
06/02/2019 10:55:59 *** training ***
06/02/2019 10:55:59 step: 3074, epoch: 93, batch: 4, loss: 0.4266447126865387, acc: 87.5, f1: 68.62718821111935, r: 0.5676703662167449
06/02/2019 10:55:59 step: 3079, epoch: 93, batch: 9, loss: 0.5460282564163208, acc: 85.9375, f1: 70.50496522285236, r: 0.5241554110190861
06/02/2019 10:56:00 step: 3084, epoch: 93, batch: 14, loss: 0.35730642080307007, acc: 89.0625, f1: 83.71362433862434, r: 0.686569145670438
06/02/2019 10:56:00 step: 3089, epoch: 93, batch: 19, loss: 0.3779471516609192, acc: 85.9375, f1: 81.71238483738483, r: 0.6891705105711696
06/02/2019 10:56:01 step: 3094, epoch: 93, batch: 24, loss: 0.3252785801887512, acc: 87.5, f1: 80.91280404501344, r: 0.5681118824113844
06/02/2019 10:56:01 step: 3099, epoch: 93, batch: 29, loss: 0.19877102971076965, acc: 92.1875, f1: 89.69388379508348, r: 0.7249341683196436
06/02/2019 10:56:01 *** evaluating ***
06/02/2019 10:56:02 step: 94, epoch: 93, acc: 58.54700854700855, f1: 28.185161101159423, r: 0.35890677143587685
06/02/2019 10:56:02 *** epoch: 95 ***
06/02/2019 10:56:02 *** training ***
06/02/2019 10:56:02 step: 3107, epoch: 94, batch: 4, loss: 0.2186921238899231, acc: 90.625, f1: 89.02556995733812, r: 0.6356983356676039
06/02/2019 10:56:02 step: 3112, epoch: 94, batch: 9, loss: 0.4202757477760315, acc: 89.0625, f1: 85.95738636363637, r: 0.6971059994701215
06/02/2019 10:56:03 step: 3117, epoch: 94, batch: 14, loss: 0.3364272713661194, acc: 87.5, f1: 78.94544192904849, r: 0.6124705475349909
06/02/2019 10:56:03 step: 3122, epoch: 94, batch: 19, loss: 0.29041969776153564, acc: 92.1875, f1: 91.3840920983778, r: 0.6086132070316447
06/02/2019 10:56:04 step: 3127, epoch: 94, batch: 24, loss: 0.5023707151412964, acc: 78.125, f1: 73.85117623604465, r: 0.613962223879515
06/02/2019 10:56:04 step: 3132, epoch: 94, batch: 29, loss: 0.22720009088516235, acc: 90.625, f1: 77.8130032206119, r: 0.6450534531964954
06/02/2019 10:56:04 *** evaluating ***
06/02/2019 10:56:05 step: 95, epoch: 94, acc: 57.26495726495726, f1: 23.66734849039956, r: 0.35448821227736443
06/02/2019 10:56:05 *** epoch: 96 ***
06/02/2019 10:56:05 *** training ***
06/02/2019 10:56:05 step: 3140, epoch: 95, batch: 4, loss: 0.3835536241531372, acc: 92.1875, f1: 93.26470588235294, r: 0.7782297393355623
06/02/2019 10:56:06 step: 3145, epoch: 95, batch: 9, loss: 0.2649587392807007, acc: 89.0625, f1: 91.96497457367022, r: 0.6524306303438068
06/02/2019 10:56:06 step: 3150, epoch: 95, batch: 14, loss: 0.3240377902984619, acc: 87.5, f1: 86.7957840032308, r: 0.6633651886605928
06/02/2019 10:56:06 step: 3155, epoch: 95, batch: 19, loss: 0.260516881942749, acc: 92.1875, f1: 78.72750094732854, r: 0.7117218908953953
06/02/2019 10:56:07 step: 3160, epoch: 95, batch: 24, loss: 0.5607775449752808, acc: 87.5, f1: 89.40134099616859, r: 0.6873936846690069
06/02/2019 10:56:07 step: 3165, epoch: 95, batch: 29, loss: 0.2704022526741028, acc: 93.75, f1: 92.76085434173669, r: 0.7643192506905302
06/02/2019 10:56:07 *** evaluating ***
06/02/2019 10:56:08 step: 96, epoch: 95, acc: 57.692307692307686, f1: 28.30480865805287, r: 0.35538397834181334
06/02/2019 10:56:08 *** epoch: 97 ***
06/02/2019 10:56:08 *** training ***
06/02/2019 10:56:08 step: 3173, epoch: 96, batch: 4, loss: 0.3292921185493469, acc: 95.3125, f1: 93.06475837088082, r: 0.5804513541361317
06/02/2019 10:56:08 step: 3178, epoch: 96, batch: 9, loss: 0.23472294211387634, acc: 92.1875, f1: 91.73659673659674, r: 0.6797655140361142
06/02/2019 10:56:09 step: 3183, epoch: 96, batch: 14, loss: 0.2786458134651184, acc: 92.1875, f1: 91.33768051871502, r: 0.5527164732731668
06/02/2019 10:56:09 step: 3188, epoch: 96, batch: 19, loss: 0.34462404251098633, acc: 87.5, f1: 81.26159554730982, r: 0.6897966189453799
06/02/2019 10:56:10 step: 3193, epoch: 96, batch: 24, loss: 0.34920477867126465, acc: 87.5, f1: 76.65622143883013, r: 0.6910686144284154
06/02/2019 10:56:10 step: 3198, epoch: 96, batch: 29, loss: 0.3514325022697449, acc: 87.5, f1: 86.21933621933621, r: 0.7729295151882761
06/02/2019 10:56:10 *** evaluating ***
06/02/2019 10:56:10 step: 97, epoch: 96, acc: 57.692307692307686, f1: 27.665782114934657, r: 0.3592941756014753
06/02/2019 10:56:10 *** epoch: 98 ***
06/02/2019 10:56:10 *** training ***
06/02/2019 10:56:11 step: 3206, epoch: 97, batch: 4, loss: 0.34449049830436707, acc: 92.1875, f1: 91.13027289768955, r: 0.6456215923309202
06/02/2019 10:56:11 step: 3211, epoch: 97, batch: 9, loss: 0.30200687050819397, acc: 90.625, f1: 92.26524700789408, r: 0.7317571423849393
06/02/2019 10:56:12 step: 3216, epoch: 97, batch: 14, loss: 0.4129560589790344, acc: 82.8125, f1: 72.53002756051536, r: 0.7326262070251365
06/02/2019 10:56:12 step: 3221, epoch: 97, batch: 19, loss: 0.32358747720718384, acc: 90.625, f1: 83.19215392303848, r: 0.7230229935309609
06/02/2019 10:56:12 step: 3226, epoch: 97, batch: 24, loss: 0.489435076713562, acc: 82.8125, f1: 66.65096807953951, r: 0.5086128312202876
06/02/2019 10:56:13 step: 3231, epoch: 97, batch: 29, loss: 0.32794520258903503, acc: 82.8125, f1: 82.90423861852435, r: 0.641779550125735
06/02/2019 10:56:13 *** evaluating ***
06/02/2019 10:56:13 step: 98, epoch: 97, acc: 58.54700854700855, f1: 27.495779901440276, r: 0.35267609738797406
06/02/2019 10:56:13 *** epoch: 99 ***
06/02/2019 10:56:13 *** training ***
06/02/2019 10:56:14 step: 3239, epoch: 98, batch: 4, loss: 0.21882474422454834, acc: 90.625, f1: 87.8626681671178, r: 0.6560899038317968
06/02/2019 10:56:14 step: 3244, epoch: 98, batch: 9, loss: 0.19053450226783752, acc: 93.75, f1: 80.91208791208791, r: 0.6438408679497101
06/02/2019 10:56:15 step: 3249, epoch: 98, batch: 14, loss: 0.6076694130897522, acc: 78.125, f1: 77.54599567099567, r: 0.6675035966238353
06/02/2019 10:56:15 step: 3254, epoch: 98, batch: 19, loss: 0.2863052189350128, acc: 90.625, f1: 89.04538649436608, r: 0.6370156730363499
06/02/2019 10:56:16 step: 3259, epoch: 98, batch: 24, loss: 0.3960101306438446, acc: 87.5, f1: 69.61625833908442, r: 0.6389342829450236
06/02/2019 10:56:16 step: 3264, epoch: 98, batch: 29, loss: 0.2949444055557251, acc: 93.75, f1: 89.95670995670996, r: 0.648992088400571
06/02/2019 10:56:16 *** evaluating ***
06/02/2019 10:56:17 step: 99, epoch: 98, acc: 58.54700854700855, f1: 27.66217331713547, r: 0.3476700868081022
06/02/2019 10:56:17 *** epoch: 100 ***
06/02/2019 10:56:17 *** training ***
06/02/2019 10:56:17 step: 3272, epoch: 99, batch: 4, loss: 0.490325391292572, acc: 89.0625, f1: 83.70208280092001, r: 0.7015485117806082
06/02/2019 10:56:17 step: 3277, epoch: 99, batch: 9, loss: 0.26108649373054504, acc: 90.625, f1: 85.64491064491064, r: 0.5965764398038026
06/02/2019 10:56:18 step: 3282, epoch: 99, batch: 14, loss: 0.21713098883628845, acc: 93.75, f1: 83.91537928302635, r: 0.5877924734673324
06/02/2019 10:56:18 step: 3287, epoch: 99, batch: 19, loss: 0.2698908746242523, acc: 87.5, f1: 67.13546798029557, r: 0.5685154846627511
06/02/2019 10:56:19 step: 3292, epoch: 99, batch: 24, loss: 0.23767107725143433, acc: 90.625, f1: 89.654952890247, r: 0.740154071490949
06/02/2019 10:56:19 step: 3297, epoch: 99, batch: 29, loss: 0.39397868514060974, acc: 87.5, f1: 71.11001317523056, r: 0.676480159560686
06/02/2019 10:56:19 *** evaluating ***
06/02/2019 10:56:20 step: 100, epoch: 99, acc: 57.26495726495726, f1: 28.330909907673394, r: 0.35054530225159486
06/02/2019 10:56:20 *** epoch: 101 ***
06/02/2019 10:56:20 *** training ***
06/02/2019 10:56:20 step: 3305, epoch: 100, batch: 4, loss: 0.36274638772010803, acc: 89.0625, f1: 85.26446476026308, r: 0.7051146144835353
06/02/2019 10:56:20 step: 3310, epoch: 100, batch: 9, loss: 0.339404821395874, acc: 89.0625, f1: 70.85393772893774, r: 0.6168780612947509
06/02/2019 10:56:21 step: 3315, epoch: 100, batch: 14, loss: 0.2804889976978302, acc: 90.625, f1: 86.9885030035406, r: 0.7452262269448982
06/02/2019 10:56:21 step: 3320, epoch: 100, batch: 19, loss: 0.20970118045806885, acc: 92.1875, f1: 90.3077634154437, r: 0.7174992466959151
06/02/2019 10:56:22 step: 3325, epoch: 100, batch: 24, loss: 0.18819886445999146, acc: 95.3125, f1: 86.34803921568628, r: 0.762617680893242
06/02/2019 10:56:22 step: 3330, epoch: 100, batch: 29, loss: 0.18506485223770142, acc: 95.3125, f1: 90.67560621141814, r: 0.5583557714060546
06/02/2019 10:56:22 *** evaluating ***
06/02/2019 10:56:22 step: 101, epoch: 100, acc: 57.692307692307686, f1: 27.442149962132746, r: 0.35317523638838144
06/02/2019 10:56:22 *** epoch: 102 ***
06/02/2019 10:56:22 *** training ***
06/02/2019 10:56:23 step: 3338, epoch: 101, batch: 4, loss: 0.27637478709220886, acc: 90.625, f1: 86.60133948177426, r: 0.6149280429810513
06/02/2019 10:56:23 step: 3343, epoch: 101, batch: 9, loss: 0.15722118318080902, acc: 93.75, f1: 89.30307787450646, r: 0.6226166165983186
06/02/2019 10:56:23 step: 3348, epoch: 101, batch: 14, loss: 0.43826669454574585, acc: 87.5, f1: 80.01877821426694, r: 0.5871220920979465
06/02/2019 10:56:24 step: 3353, epoch: 101, batch: 19, loss: 0.27254563570022583, acc: 93.75, f1: 77.46527777777777, r: 0.7207344666027464
06/02/2019 10:56:24 step: 3358, epoch: 101, batch: 24, loss: 0.2690357267856598, acc: 89.0625, f1: 87.12053571428571, r: 0.6928229423931453
06/02/2019 10:56:25 step: 3363, epoch: 101, batch: 29, loss: 0.30577635765075684, acc: 89.0625, f1: 82.8110227141704, r: 0.631976431493583
06/02/2019 10:56:25 *** evaluating ***
06/02/2019 10:56:25 step: 102, epoch: 101, acc: 58.97435897435898, f1: 28.206552883586376, r: 0.3554891497800552
06/02/2019 10:56:25 *** epoch: 103 ***
06/02/2019 10:56:25 *** training ***
06/02/2019 10:56:25 step: 3371, epoch: 102, batch: 4, loss: 0.14097058773040771, acc: 95.3125, f1: 92.16036414565826, r: 0.677471912007425
06/02/2019 10:56:26 step: 3376, epoch: 102, batch: 9, loss: 0.20630204677581787, acc: 92.1875, f1: 92.46064272703617, r: 0.6894133223151444
06/02/2019 10:56:26 step: 3381, epoch: 102, batch: 14, loss: 0.2767033576965332, acc: 93.75, f1: 90.0136476426799, r: 0.576007950359225
06/02/2019 10:56:26 step: 3386, epoch: 102, batch: 19, loss: 0.35727354884147644, acc: 87.5, f1: 79.44479986351907, r: 0.5939368709667131
06/02/2019 10:56:27 step: 3391, epoch: 102, batch: 24, loss: 0.16668333113193512, acc: 96.875, f1: 94.97354497354497, r: 0.7306569914436761
06/02/2019 10:56:27 step: 3396, epoch: 102, batch: 29, loss: 0.2507113516330719, acc: 90.625, f1: 91.92721423544788, r: 0.6757991944489415
06/02/2019 10:56:27 *** evaluating ***
06/02/2019 10:56:28 step: 103, epoch: 102, acc: 59.82905982905983, f1: 28.206307196326406, r: 0.36040394938419856
06/02/2019 10:56:28 *** epoch: 104 ***
06/02/2019 10:56:28 *** training ***
06/02/2019 10:56:28 step: 3404, epoch: 103, batch: 4, loss: 0.35717344284057617, acc: 89.0625, f1: 88.02437641723355, r: 0.7900630479924405
06/02/2019 10:56:28 step: 3409, epoch: 103, batch: 9, loss: 0.1860063076019287, acc: 95.3125, f1: 93.078231292517, r: 0.7966411633321224
06/02/2019 10:56:29 step: 3414, epoch: 103, batch: 14, loss: 0.3439449667930603, acc: 89.0625, f1: 71.3927441606013, r: 0.6151995893452561
06/02/2019 10:56:29 step: 3419, epoch: 103, batch: 19, loss: 0.32485929131507874, acc: 85.9375, f1: 71.84493994377715, r: 0.6736291894869081
06/02/2019 10:56:30 step: 3424, epoch: 103, batch: 24, loss: 0.5243934988975525, acc: 81.25, f1: 73.5551948051948, r: 0.6326899807880438
06/02/2019 10:56:30 step: 3429, epoch: 103, batch: 29, loss: 0.33045387268066406, acc: 89.0625, f1: 87.16346153846153, r: 0.7024614554466708
06/02/2019 10:56:30 *** evaluating ***
06/02/2019 10:56:30 step: 104, epoch: 103, acc: 58.97435897435898, f1: 28.91424655703469, r: 0.36064406247101327
06/02/2019 10:56:30 *** epoch: 105 ***
06/02/2019 10:56:30 *** training ***
06/02/2019 10:56:31 step: 3437, epoch: 104, batch: 4, loss: 0.312998503446579, acc: 89.0625, f1: 84.19871794871796, r: 0.6545375218077201
06/02/2019 10:56:31 step: 3442, epoch: 104, batch: 9, loss: 0.24699461460113525, acc: 90.625, f1: 88.8268736094823, r: 0.7597590100916747
06/02/2019 10:56:32 step: 3447, epoch: 104, batch: 14, loss: 0.3400267958641052, acc: 89.0625, f1: 75.48828622358033, r: 0.6889056583022719
06/02/2019 10:56:32 step: 3452, epoch: 104, batch: 19, loss: 0.3065309226512909, acc: 89.0625, f1: 83.6608383406413, r: 0.5955327679069098
06/02/2019 10:56:32 step: 3457, epoch: 104, batch: 24, loss: 0.4038587808609009, acc: 87.5, f1: 71.7704700457258, r: 0.6271787242410866
06/02/2019 10:56:33 step: 3462, epoch: 104, batch: 29, loss: 0.35127463936805725, acc: 85.9375, f1: 67.95830327080327, r: 0.692557276883083
06/02/2019 10:56:33 *** evaluating ***
06/02/2019 10:56:33 step: 105, epoch: 104, acc: 58.97435897435898, f1: 28.573147125778707, r: 0.36139731394874053
06/02/2019 10:56:33 *** epoch: 106 ***
06/02/2019 10:56:33 *** training ***
06/02/2019 10:56:34 step: 3470, epoch: 105, batch: 4, loss: 0.34546905755996704, acc: 89.0625, f1: 90.41639466884266, r: 0.7494635134434791
06/02/2019 10:56:34 step: 3475, epoch: 105, batch: 9, loss: 0.20302708446979523, acc: 93.75, f1: 92.97498986939982, r: 0.6617742728332708
06/02/2019 10:56:35 step: 3480, epoch: 105, batch: 14, loss: 0.22852984070777893, acc: 93.75, f1: 91.41631652661064, r: 0.7287828049233068
06/02/2019 10:56:35 step: 3485, epoch: 105, batch: 19, loss: 0.278412789106369, acc: 89.0625, f1: 80.74500768049155, r: 0.6534006088416466
06/02/2019 10:56:35 step: 3490, epoch: 105, batch: 24, loss: 0.20495915412902832, acc: 90.625, f1: 86.51537302853092, r: 0.7517304252647357
06/02/2019 10:56:36 step: 3495, epoch: 105, batch: 29, loss: 0.299389511346817, acc: 89.0625, f1: 88.36178107606678, r: 0.6455973069573687
06/02/2019 10:56:36 *** evaluating ***
06/02/2019 10:56:36 step: 106, epoch: 105, acc: 59.401709401709404, f1: 28.931234156627855, r: 0.3598323623004831
06/02/2019 10:56:36 *** epoch: 107 ***
06/02/2019 10:56:36 *** training ***
06/02/2019 10:56:37 step: 3503, epoch: 106, batch: 4, loss: 0.39225730299949646, acc: 87.5, f1: 82.7228760591671, r: 0.6316145436103541
06/02/2019 10:56:37 step: 3508, epoch: 106, batch: 9, loss: 0.3619943857192993, acc: 85.9375, f1: 75.86143939917524, r: 0.6498565937318606
06/02/2019 10:56:37 step: 3513, epoch: 106, batch: 14, loss: 0.22522862255573273, acc: 89.0625, f1: 91.90282173475451, r: 0.6454722055155373
06/02/2019 10:56:38 step: 3518, epoch: 106, batch: 19, loss: 0.10280267894268036, acc: 96.875, f1: 97.16707826747721, r: 0.7722326558880543
06/02/2019 10:56:38 step: 3523, epoch: 106, batch: 24, loss: 0.1720775067806244, acc: 95.3125, f1: 93.87237762237763, r: 0.7589927568235457
06/02/2019 10:56:39 step: 3528, epoch: 106, batch: 29, loss: 0.27021563053131104, acc: 89.0625, f1: 85.89853810028784, r: 0.6353961622851184
06/02/2019 10:56:39 *** evaluating ***
06/02/2019 10:56:39 step: 107, epoch: 106, acc: 59.82905982905983, f1: 27.926322043969105, r: 0.3629313134562861
06/02/2019 10:56:39 *** epoch: 108 ***
06/02/2019 10:56:39 *** training ***
06/02/2019 10:56:39 step: 3536, epoch: 107, batch: 4, loss: 0.30155470967292786, acc: 87.5, f1: 75.55148737414747, r: 0.5921072740407759
06/02/2019 10:56:40 step: 3541, epoch: 107, batch: 9, loss: 0.38866332173347473, acc: 84.375, f1: 88.3867243867244, r: 0.6516481630411374
06/02/2019 10:56:40 step: 3546, epoch: 107, batch: 14, loss: 0.343652606010437, acc: 87.5, f1: 76.7573696145125, r: 0.5948348802187865
06/02/2019 10:56:41 step: 3551, epoch: 107, batch: 19, loss: 0.24223747849464417, acc: 87.5, f1: 75.95238095238095, r: 0.7117774475513307
06/02/2019 10:56:41 step: 3556, epoch: 107, batch: 24, loss: 0.3425808846950531, acc: 84.375, f1: 74.06204906204907, r: 0.6255532623698786
06/02/2019 10:56:41 step: 3561, epoch: 107, batch: 29, loss: 0.22674386203289032, acc: 87.5, f1: 73.14915458937199, r: 0.705904517873297
06/02/2019 10:56:41 *** evaluating ***
06/02/2019 10:56:42 step: 108, epoch: 107, acc: 57.692307692307686, f1: 27.20057354925776, r: 0.3569537192324165
06/02/2019 10:56:42 *** epoch: 109 ***
06/02/2019 10:56:42 *** training ***
06/02/2019 10:56:42 step: 3569, epoch: 108, batch: 4, loss: 0.2109690010547638, acc: 90.625, f1: 87.67649281934996, r: 0.6757779560515413
06/02/2019 10:56:42 step: 3574, epoch: 108, batch: 9, loss: 0.16662314534187317, acc: 95.3125, f1: 95.36511875221552, r: 0.5380509302461223
06/02/2019 10:56:43 step: 3579, epoch: 108, batch: 14, loss: 0.19989457726478577, acc: 95.3125, f1: 93.23140245109033, r: 0.7274898615471445
06/02/2019 10:56:43 step: 3584, epoch: 108, batch: 19, loss: 0.22662676870822906, acc: 90.625, f1: 90.04254536150475, r: 0.5909281001304103
06/02/2019 10:56:44 step: 3589, epoch: 108, batch: 24, loss: 0.46320101618766785, acc: 87.5, f1: 80.16156462585035, r: 0.48280671089459043
06/02/2019 10:56:44 step: 3594, epoch: 108, batch: 29, loss: 0.3566133379936218, acc: 85.9375, f1: 83.09990662931838, r: 0.7316772120233157
06/02/2019 10:56:44 *** evaluating ***
06/02/2019 10:56:44 step: 109, epoch: 108, acc: 57.26495726495726, f1: 27.863863287250386, r: 0.3512637461351579
06/02/2019 10:56:44 *** epoch: 110 ***
06/02/2019 10:56:44 *** training ***
06/02/2019 10:56:45 step: 3602, epoch: 109, batch: 4, loss: 0.30680733919143677, acc: 93.75, f1: 82.453216374269, r: 0.7216261516667383
06/02/2019 10:56:45 step: 3607, epoch: 109, batch: 9, loss: 0.2969989776611328, acc: 87.5, f1: 87.11111111111111, r: 0.7796813108375468
06/02/2019 10:56:46 step: 3612, epoch: 109, batch: 14, loss: 0.11260806024074554, acc: 96.875, f1: 96.09177004134989, r: 0.7019063825480756
06/02/2019 10:56:46 step: 3617, epoch: 109, batch: 19, loss: 0.2585362195968628, acc: 95.3125, f1: 94.49885269557402, r: 0.6857981716376989
06/02/2019 10:56:47 step: 3622, epoch: 109, batch: 24, loss: 0.27445244789123535, acc: 87.5, f1: 69.37091893613633, r: 0.7085792753970123
06/02/2019 10:56:47 step: 3627, epoch: 109, batch: 29, loss: 0.2581285834312439, acc: 92.1875, f1: 87.8559063485534, r: 0.6404812334735627
06/02/2019 10:56:47 *** evaluating ***
06/02/2019 10:56:47 step: 110, epoch: 109, acc: 57.692307692307686, f1: 27.778266420768905, r: 0.3549913490179094
06/02/2019 10:56:47 *** epoch: 111 ***
06/02/2019 10:56:47 *** training ***
06/02/2019 10:56:48 step: 3635, epoch: 110, batch: 4, loss: 0.6318163871765137, acc: 85.9375, f1: 86.49546660769572, r: 0.7095810321045093
06/02/2019 10:56:48 step: 3640, epoch: 110, batch: 9, loss: 0.29395371675491333, acc: 87.5, f1: 78.16352298495156, r: 0.70097296724788
06/02/2019 10:56:49 step: 3645, epoch: 110, batch: 14, loss: 0.18473964929580688, acc: 98.4375, f1: 99.04655966947402, r: 0.7181409423218889
06/02/2019 10:56:49 step: 3650, epoch: 110, batch: 19, loss: 0.2333046942949295, acc: 93.75, f1: 91.24510188087774, r: 0.7686353540030838
06/02/2019 10:56:49 step: 3655, epoch: 110, batch: 24, loss: 0.13660433888435364, acc: 93.75, f1: 93.41787778941365, r: 0.6697568078583788
06/02/2019 10:56:50 step: 3660, epoch: 110, batch: 29, loss: 0.39454567432403564, acc: 87.5, f1: 85.04931972789116, r: 0.5623622078543794
06/02/2019 10:56:50 *** evaluating ***
06/02/2019 10:56:50 step: 111, epoch: 110, acc: 59.401709401709404, f1: 27.638265423696307, r: 0.35641310377196456
06/02/2019 10:56:50 *** epoch: 112 ***
06/02/2019 10:56:50 *** training ***
06/02/2019 10:56:51 step: 3668, epoch: 111, batch: 4, loss: 0.47662636637687683, acc: 82.8125, f1: 78.52171637885922, r: 0.5910945345991673
06/02/2019 10:56:51 step: 3673, epoch: 111, batch: 9, loss: 0.20756292343139648, acc: 96.875, f1: 91.26984126984128, r: 0.6710664100721111
06/02/2019 10:56:52 step: 3678, epoch: 111, batch: 14, loss: 0.19432777166366577, acc: 92.1875, f1: 80.97215284715284, r: 0.7146738029841808
06/02/2019 10:56:52 step: 3683, epoch: 111, batch: 19, loss: 0.20675620436668396, acc: 95.3125, f1: 92.93000232849857, r: 0.7097738645162455
06/02/2019 10:56:53 step: 3688, epoch: 111, batch: 24, loss: 0.16838335990905762, acc: 95.3125, f1: 83.53383458646616, r: 0.6191625157598581
06/02/2019 10:56:53 step: 3693, epoch: 111, batch: 29, loss: 0.2625161409378052, acc: 92.1875, f1: 94.61269123033829, r: 0.7364261340486573
06/02/2019 10:56:53 *** evaluating ***
06/02/2019 10:56:53 step: 112, epoch: 111, acc: 59.401709401709404, f1: 28.707430097956177, r: 0.35620133931667447
06/02/2019 10:56:53 *** epoch: 113 ***
06/02/2019 10:56:53 *** training ***
06/02/2019 10:56:54 step: 3701, epoch: 112, batch: 4, loss: 0.19488544762134552, acc: 95.3125, f1: 95.74456975772765, r: 0.7156881733818344
06/02/2019 10:56:54 step: 3706, epoch: 112, batch: 9, loss: 0.24932846426963806, acc: 89.0625, f1: 77.23958333333334, r: 0.7423580928236995
06/02/2019 10:56:55 step: 3711, epoch: 112, batch: 14, loss: 0.30585718154907227, acc: 89.0625, f1: 82.22222222222221, r: 0.5910464500957576
06/02/2019 10:56:55 step: 3716, epoch: 112, batch: 19, loss: 0.505125880241394, acc: 82.8125, f1: 81.09538327526133, r: 0.6649241585071882
06/02/2019 10:56:55 step: 3721, epoch: 112, batch: 24, loss: 0.2236572951078415, acc: 90.625, f1: 84.69120981726024, r: 0.6670613518422392
06/02/2019 10:56:56 step: 3726, epoch: 112, batch: 29, loss: 0.31989583373069763, acc: 89.0625, f1: 74.87469806763285, r: 0.6295445720553925
06/02/2019 10:56:56 *** evaluating ***
06/02/2019 10:56:56 step: 113, epoch: 112, acc: 57.692307692307686, f1: 29.238529973225674, r: 0.35419276678510214
06/02/2019 10:56:56 *** epoch: 114 ***
06/02/2019 10:56:56 *** training ***
06/02/2019 10:56:57 step: 3734, epoch: 113, batch: 4, loss: 0.33272895216941833, acc: 92.1875, f1: 89.70323461848886, r: 0.7613786494680974
06/02/2019 10:56:57 step: 3739, epoch: 113, batch: 9, loss: 0.2948288321495056, acc: 93.75, f1: 69.33333333333334, r: 0.6340056901269633
06/02/2019 10:56:57 step: 3744, epoch: 113, batch: 14, loss: 0.2623704671859741, acc: 90.625, f1: 88.52195423623994, r: 0.5604536548232075
06/02/2019 10:56:58 step: 3749, epoch: 113, batch: 19, loss: 0.18893423676490784, acc: 93.75, f1: 90.65338602284415, r: 0.6145195972511818
06/02/2019 10:56:58 step: 3754, epoch: 113, batch: 24, loss: 0.339637815952301, acc: 85.9375, f1: 88.23636988088815, r: 0.62408203547101
06/02/2019 10:56:59 step: 3759, epoch: 113, batch: 29, loss: 0.31265318393707275, acc: 89.0625, f1: 72.64972301918114, r: 0.5734072280398271
06/02/2019 10:56:59 *** evaluating ***
06/02/2019 10:56:59 step: 114, epoch: 113, acc: 59.401709401709404, f1: 28.403282153176622, r: 0.3493724156932581
06/02/2019 10:56:59 *** epoch: 115 ***
06/02/2019 10:56:59 *** training ***
06/02/2019 10:57:00 step: 3767, epoch: 114, batch: 4, loss: 0.24212363362312317, acc: 90.625, f1: 84.99881071309642, r: 0.593949771952602
06/02/2019 10:57:00 step: 3772, epoch: 114, batch: 9, loss: 0.1859789490699768, acc: 95.3125, f1: 93.59674329501915, r: 0.7861018630540684
06/02/2019 10:57:00 step: 3777, epoch: 114, batch: 14, loss: 0.24500727653503418, acc: 92.1875, f1: 65.38766788766789, r: 0.5639285011968659
06/02/2019 10:57:01 step: 3782, epoch: 114, batch: 19, loss: 0.25112730264663696, acc: 92.1875, f1: 89.80874316939891, r: 0.7185190285316477
06/02/2019 10:57:01 step: 3787, epoch: 114, batch: 24, loss: 0.4338855743408203, acc: 82.8125, f1: 82.66553383338196, r: 0.6102443714085366
06/02/2019 10:57:02 step: 3792, epoch: 114, batch: 29, loss: 0.2219243198633194, acc: 90.625, f1: 83.89064480761984, r: 0.7655944594929482
06/02/2019 10:57:02 *** evaluating ***
06/02/2019 10:57:02 step: 115, epoch: 114, acc: 58.119658119658126, f1: 27.706121686245528, r: 0.3479651849715524
06/02/2019 10:57:02 *** epoch: 116 ***
06/02/2019 10:57:02 *** training ***
06/02/2019 10:57:02 step: 3800, epoch: 115, batch: 4, loss: 0.36238664388656616, acc: 89.0625, f1: 84.89057239057239, r: 0.6890994115407177
06/02/2019 10:57:03 step: 3805, epoch: 115, batch: 9, loss: 0.25308364629745483, acc: 90.625, f1: 90.07920409543128, r: 0.6539955339666927
06/02/2019 10:57:03 step: 3810, epoch: 115, batch: 14, loss: 0.2754932641983032, acc: 92.1875, f1: 75.44467787114846, r: 0.7224808276957715
06/02/2019 10:57:04 step: 3815, epoch: 115, batch: 19, loss: 0.14823421835899353, acc: 96.875, f1: 96.34365634365635, r: 0.7281683844748916
06/02/2019 10:57:04 step: 3820, epoch: 115, batch: 24, loss: 0.2894566059112549, acc: 85.9375, f1: 74.66527827839855, r: 0.6291805318859504
06/02/2019 10:57:05 step: 3825, epoch: 115, batch: 29, loss: 0.20869682729244232, acc: 93.75, f1: 67.71869639794168, r: 0.6709476234333301
06/02/2019 10:57:05 *** evaluating ***
06/02/2019 10:57:05 step: 116, epoch: 115, acc: 58.97435897435898, f1: 28.795815295815295, r: 0.34706936498417507
06/02/2019 10:57:05 *** epoch: 117 ***
06/02/2019 10:57:05 *** training ***
06/02/2019 10:57:06 step: 3833, epoch: 116, batch: 4, loss: 0.3683438301086426, acc: 89.0625, f1: 78.78004973749655, r: 0.6932628649746064
06/02/2019 10:57:06 step: 3838, epoch: 116, batch: 9, loss: 0.3161677122116089, acc: 87.5, f1: 73.59634551495017, r: 0.5409898309399748
06/02/2019 10:57:06 step: 3843, epoch: 116, batch: 14, loss: 0.24123752117156982, acc: 90.625, f1: 88.73777706075843, r: 0.6534377352435038
06/02/2019 10:57:07 step: 3848, epoch: 116, batch: 19, loss: 0.31137746572494507, acc: 90.625, f1: 91.50399290150843, r: 0.7376628860684187
06/02/2019 10:57:07 step: 3853, epoch: 116, batch: 24, loss: 0.40348875522613525, acc: 92.1875, f1: 91.17460317460318, r: 0.731243842673715
06/02/2019 10:57:08 step: 3858, epoch: 116, batch: 29, loss: 0.3138315677642822, acc: 87.5, f1: 86.95856998926052, r: 0.7683731600960417
06/02/2019 10:57:08 *** evaluating ***
06/02/2019 10:57:08 step: 117, epoch: 116, acc: 57.692307692307686, f1: 26.982785489364435, r: 0.35066264310775175
06/02/2019 10:57:08 *** epoch: 118 ***
06/02/2019 10:57:08 *** training ***
06/02/2019 10:57:09 step: 3866, epoch: 117, batch: 4, loss: 0.17547748982906342, acc: 95.3125, f1: 89.47303921568628, r: 0.7651143070681955
06/02/2019 10:57:09 step: 3871, epoch: 117, batch: 9, loss: 0.33505529165267944, acc: 89.0625, f1: 85.509195698541, r: 0.6510978663128355
06/02/2019 10:57:10 step: 3876, epoch: 117, batch: 14, loss: 0.15229804813861847, acc: 93.75, f1: 92.8042328042328, r: 0.6665902290568266
06/02/2019 10:57:10 step: 3881, epoch: 117, batch: 19, loss: 0.4107060432434082, acc: 87.5, f1: 72.05636812929941, r: 0.6174792046458263
06/02/2019 10:57:11 step: 3886, epoch: 117, batch: 24, loss: 0.20231737196445465, acc: 93.75, f1: 83.46998608626515, r: 0.72730019531354
06/02/2019 10:57:11 step: 3891, epoch: 117, batch: 29, loss: 0.20098935067653656, acc: 95.3125, f1: 88.61111111111111, r: 0.7601693927917713
06/02/2019 10:57:11 *** evaluating ***
06/02/2019 10:57:12 step: 118, epoch: 117, acc: 57.26495726495726, f1: 28.360953303677427, r: 0.35212230396124833
06/02/2019 10:57:12 *** epoch: 119 ***
06/02/2019 10:57:12 *** training ***
06/02/2019 10:57:12 step: 3899, epoch: 118, batch: 4, loss: 0.23753045499324799, acc: 93.75, f1: 95.12577123872805, r: 0.6593281927782164
06/02/2019 10:57:12 step: 3904, epoch: 118, batch: 9, loss: 0.25036221742630005, acc: 93.75, f1: 91.41071482608658, r: 0.7227001653873326
06/02/2019 10:57:13 step: 3909, epoch: 118, batch: 14, loss: 0.20715869963169098, acc: 92.1875, f1: 81.36385836385836, r: 0.7246556537263318
06/02/2019 10:57:13 step: 3914, epoch: 118, batch: 19, loss: 0.21546050906181335, acc: 90.625, f1: 85.7698754789272, r: 0.778373023542843
06/02/2019 10:57:14 step: 3919, epoch: 118, batch: 24, loss: 0.1303698867559433, acc: 95.3125, f1: 96.93121693121694, r: 0.698789753799146
06/02/2019 10:57:14 step: 3924, epoch: 118, batch: 29, loss: 0.3911394476890564, acc: 85.9375, f1: 81.36600003409887, r: 0.5743023326609847
06/02/2019 10:57:14 *** evaluating ***
06/02/2019 10:57:14 step: 119, epoch: 118, acc: 57.26495726495726, f1: 26.49174709952161, r: 0.35102274952294293
06/02/2019 10:57:14 *** epoch: 120 ***
06/02/2019 10:57:14 *** training ***
06/02/2019 10:57:15 step: 3932, epoch: 119, batch: 4, loss: 0.2142278403043747, acc: 92.1875, f1: 90.68179559353145, r: 0.7953148696523734
06/02/2019 10:57:15 step: 3937, epoch: 119, batch: 9, loss: 0.32830604910850525, acc: 82.8125, f1: 60.3897745741111, r: 0.6751430733832264
06/02/2019 10:57:16 step: 3942, epoch: 119, batch: 14, loss: 0.07473292946815491, acc: 100.0, f1: 100.0, r: 0.656504873320591
06/02/2019 10:57:16 step: 3947, epoch: 119, batch: 19, loss: 0.3602210283279419, acc: 84.375, f1: 67.51488095238095, r: 0.7148654437783322
06/02/2019 10:57:17 step: 3952, epoch: 119, batch: 24, loss: 0.34260669350624084, acc: 85.9375, f1: 76.9908424908425, r: 0.6269846747053215
06/02/2019 10:57:17 step: 3957, epoch: 119, batch: 29, loss: 0.20666897296905518, acc: 93.75, f1: 86.17182356813693, r: 0.6086742973041935
06/02/2019 10:57:17 *** evaluating ***
06/02/2019 10:57:17 step: 120, epoch: 119, acc: 57.692307692307686, f1: 27.98964797798873, r: 0.3504314834582354
06/02/2019 10:57:17 *** epoch: 121 ***
06/02/2019 10:57:17 *** training ***
06/02/2019 10:57:18 step: 3965, epoch: 120, batch: 4, loss: 0.23957109451293945, acc: 89.0625, f1: 92.72843618513325, r: 0.722909423240906
06/02/2019 10:57:18 step: 3970, epoch: 120, batch: 9, loss: 0.35257065296173096, acc: 92.1875, f1: 91.56438447442945, r: 0.7730582911805001
06/02/2019 10:57:19 step: 3975, epoch: 120, batch: 14, loss: 0.24818463623523712, acc: 93.75, f1: 89.7432521395655, r: 0.6605364426732458
06/02/2019 10:57:19 step: 3980, epoch: 120, batch: 19, loss: 0.25721845030784607, acc: 90.625, f1: 75.73704676965546, r: 0.7558016425306251
06/02/2019 10:57:19 step: 3985, epoch: 120, batch: 24, loss: 0.27878451347351074, acc: 90.625, f1: 91.00442354800357, r: 0.6389074449372677
06/02/2019 10:57:20 step: 3990, epoch: 120, batch: 29, loss: 0.4295111298561096, acc: 85.9375, f1: 86.48239357541684, r: 0.5945340398124747
06/02/2019 10:57:20 *** evaluating ***
06/02/2019 10:57:21 step: 121, epoch: 120, acc: 57.26495726495726, f1: 27.62606518102529, r: 0.35054768479942194
06/02/2019 10:57:21 *** epoch: 122 ***
06/02/2019 10:57:21 *** training ***
06/02/2019 10:57:21 step: 3998, epoch: 121, batch: 4, loss: 0.39241349697113037, acc: 93.75, f1: 94.51146992420577, r: 0.7720832595688897
06/02/2019 10:57:21 step: 4003, epoch: 121, batch: 9, loss: 0.2875760495662689, acc: 85.9375, f1: 85.22541818145697, r: 0.739833844255143
06/02/2019 10:57:22 step: 4008, epoch: 121, batch: 14, loss: 0.28248652815818787, acc: 93.75, f1: 88.22373393801965, r: 0.6342950762533419
06/02/2019 10:57:22 step: 4013, epoch: 121, batch: 19, loss: 0.2795996069908142, acc: 90.625, f1: 73.9889395474502, r: 0.6232921988844697
06/02/2019 10:57:23 step: 4018, epoch: 121, batch: 24, loss: 0.33313190937042236, acc: 89.0625, f1: 87.45148529631288, r: 0.7207350531234499
06/02/2019 10:57:23 step: 4023, epoch: 121, batch: 29, loss: 0.4027652442455292, acc: 89.0625, f1: 75.34929356357928, r: 0.5837841979097484
06/02/2019 10:57:23 *** evaluating ***
06/02/2019 10:57:24 step: 122, epoch: 121, acc: 58.54700854700855, f1: 26.591495319116802, r: 0.3459253152399125
06/02/2019 10:57:24 *** epoch: 123 ***
06/02/2019 10:57:24 *** training ***
06/02/2019 10:57:24 step: 4031, epoch: 122, batch: 4, loss: 0.2387169450521469, acc: 93.75, f1: 88.8946608946609, r: 0.6347998919431644
06/02/2019 10:57:25 step: 4036, epoch: 122, batch: 9, loss: 0.2675759792327881, acc: 89.0625, f1: 66.62145780744162, r: 0.6208931956010011
06/02/2019 10:57:25 step: 4041, epoch: 122, batch: 14, loss: 0.1623757928609848, acc: 98.4375, f1: 98.07692307692307, r: 0.7945374753372182
06/02/2019 10:57:26 step: 4046, epoch: 122, batch: 19, loss: 0.25099262595176697, acc: 90.625, f1: 89.64806435394672, r: 0.6225144066139255
06/02/2019 10:57:26 step: 4051, epoch: 122, batch: 24, loss: 0.34635356068611145, acc: 85.9375, f1: 85.58414522802396, r: 0.7892789092996362
06/02/2019 10:57:27 step: 4056, epoch: 122, batch: 29, loss: 0.12884961068630219, acc: 95.3125, f1: 92.30990730990732, r: 0.6712921649584173
06/02/2019 10:57:27 *** evaluating ***
06/02/2019 10:57:27 step: 123, epoch: 122, acc: 58.54700854700855, f1: 28.08243472877619, r: 0.34402418062009027
06/02/2019 10:57:27 *** epoch: 124 ***
06/02/2019 10:57:27 *** training ***
06/02/2019 10:57:27 step: 4064, epoch: 123, batch: 4, loss: 0.2659842371940613, acc: 92.1875, f1: 74.72769567597153, r: 0.6527612428459868
06/02/2019 10:57:28 step: 4069, epoch: 123, batch: 9, loss: 0.18531584739685059, acc: 95.3125, f1: 96.38760486218115, r: 0.7858684381601901
06/02/2019 10:57:28 step: 4074, epoch: 123, batch: 14, loss: 0.22917430102825165, acc: 90.625, f1: 84.01923076923077, r: 0.7167409440760345
06/02/2019 10:57:29 step: 4079, epoch: 123, batch: 19, loss: 0.17564870417118073, acc: 92.1875, f1: 82.17130813678128, r: 0.781531726806924
06/02/2019 10:57:29 step: 4084, epoch: 123, batch: 24, loss: 0.306968092918396, acc: 92.1875, f1: 92.92735042735043, r: 0.7529076856455392
06/02/2019 10:57:29 step: 4089, epoch: 123, batch: 29, loss: 0.26754260063171387, acc: 85.9375, f1: 84.23816349282187, r: 0.6076360251999833
06/02/2019 10:57:30 *** evaluating ***
06/02/2019 10:57:30 step: 124, epoch: 123, acc: 57.26495726495726, f1: 26.133148202919156, r: 0.344631197271813
06/02/2019 10:57:30 *** epoch: 125 ***
06/02/2019 10:57:30 *** training ***
06/02/2019 10:57:30 step: 4097, epoch: 124, batch: 4, loss: 0.30680838227272034, acc: 90.625, f1: 92.122113997114, r: 0.7431866818537038
06/02/2019 10:57:31 step: 4102, epoch: 124, batch: 9, loss: 0.16152192652225494, acc: 95.3125, f1: 95.921742340347, r: 0.6974229829505894
06/02/2019 10:57:31 step: 4107, epoch: 124, batch: 14, loss: 0.28622496128082275, acc: 84.375, f1: 69.52380952380953, r: 0.6204733649460276
06/02/2019 10:57:32 step: 4112, epoch: 124, batch: 19, loss: 0.18283197283744812, acc: 93.75, f1: 69.14288668320927, r: 0.6621595089060066
06/02/2019 10:57:32 step: 4117, epoch: 124, batch: 24, loss: 0.3486323952674866, acc: 92.1875, f1: 95.23107523107524, r: 0.7147934994403832
06/02/2019 10:57:33 step: 4122, epoch: 124, batch: 29, loss: 0.34350821375846863, acc: 84.375, f1: 63.233607259031, r: 0.6061568752714628
06/02/2019 10:57:33 *** evaluating ***
06/02/2019 10:57:33 step: 125, epoch: 124, acc: 59.82905982905983, f1: 27.052062803867326, r: 0.3475083482052999
06/02/2019 10:57:33 *** epoch: 126 ***
06/02/2019 10:57:33 *** training ***
06/02/2019 10:57:34 step: 4130, epoch: 125, batch: 4, loss: 0.19654090702533722, acc: 93.75, f1: 92.9591836734694, r: 0.6994305130469751
06/02/2019 10:57:34 step: 4135, epoch: 125, batch: 9, loss: 0.1753719300031662, acc: 92.1875, f1: 92.19188219188219, r: 0.7849013338284029
06/02/2019 10:57:34 step: 4140, epoch: 125, batch: 14, loss: 0.21239827573299408, acc: 92.1875, f1: 89.39079832696855, r: 0.6680885975433163
06/02/2019 10:57:35 step: 4145, epoch: 125, batch: 19, loss: 0.3172667920589447, acc: 89.0625, f1: 66.88808373590982, r: 0.6784422569822212
06/02/2019 10:57:35 step: 4150, epoch: 125, batch: 24, loss: 0.17107029259204865, acc: 92.1875, f1: 92.02775644307903, r: 0.815123567485115
06/02/2019 10:57:36 step: 4155, epoch: 125, batch: 29, loss: 0.15009041130542755, acc: 92.1875, f1: 85.22614451185878, r: 0.6077470241902284
06/02/2019 10:57:36 *** evaluating ***
06/02/2019 10:57:36 step: 126, epoch: 125, acc: 57.692307692307686, f1: 28.385174026144167, r: 0.35026923022005635
06/02/2019 10:57:36 *** epoch: 127 ***
06/02/2019 10:57:36 *** training ***
06/02/2019 10:57:37 step: 4163, epoch: 126, batch: 4, loss: 0.3397197425365448, acc: 85.9375, f1: 83.35682957393483, r: 0.7198498327980569
06/02/2019 10:57:37 step: 4168, epoch: 126, batch: 9, loss: 0.24770799279212952, acc: 93.75, f1: 94.44334975369458, r: 0.7285824324877754
06/02/2019 10:57:37 step: 4173, epoch: 126, batch: 14, loss: 0.14022329449653625, acc: 95.3125, f1: 95.24305555555556, r: 0.8039576061931835
06/02/2019 10:57:38 step: 4178, epoch: 126, batch: 19, loss: 0.15853793919086456, acc: 95.3125, f1: 93.82395382395381, r: 0.7046078166931231
06/02/2019 10:57:38 step: 4183, epoch: 126, batch: 24, loss: 0.2651418447494507, acc: 90.625, f1: 76.54719184324448, r: 0.6993318166792528
06/02/2019 10:57:39 step: 4188, epoch: 126, batch: 29, loss: 0.13906338810920715, acc: 95.3125, f1: 97.12773998488285, r: 0.5538871565783575
06/02/2019 10:57:39 *** evaluating ***
06/02/2019 10:57:39 step: 127, epoch: 126, acc: 59.401709401709404, f1: 28.53884461838852, r: 0.3541185730984559
06/02/2019 10:57:39 *** epoch: 128 ***
06/02/2019 10:57:39 *** training ***
06/02/2019 10:57:39 step: 4196, epoch: 127, batch: 4, loss: 0.28252699971199036, acc: 92.1875, f1: 75.25661450350891, r: 0.6804456985555914
06/02/2019 10:57:40 step: 4201, epoch: 127, batch: 9, loss: 0.12335968762636185, acc: 95.3125, f1: 84.05442176870748, r: 0.5716815157499534
06/02/2019 10:57:40 step: 4206, epoch: 127, batch: 14, loss: 0.35405251383781433, acc: 90.625, f1: 82.27803081421503, r: 0.6926040167965396
06/02/2019 10:57:40 step: 4211, epoch: 127, batch: 19, loss: 0.08897875249385834, acc: 95.3125, f1: 96.61122120138515, r: 0.7211978686272942
06/02/2019 10:57:41 step: 4216, epoch: 127, batch: 24, loss: 0.2837088704109192, acc: 90.625, f1: 82.16970003370407, r: 0.7123805122524554
06/02/2019 10:57:41 step: 4221, epoch: 127, batch: 29, loss: 0.21312063932418823, acc: 90.625, f1: 78.00464147011462, r: 0.6566761211428302
06/02/2019 10:57:41 *** evaluating ***
06/02/2019 10:57:42 step: 128, epoch: 127, acc: 59.401709401709404, f1: 30.504208853923497, r: 0.3546860686950678
06/02/2019 10:57:42 *** epoch: 129 ***
06/02/2019 10:57:42 *** training ***
06/02/2019 10:57:42 step: 4229, epoch: 128, batch: 4, loss: 0.26580965518951416, acc: 90.625, f1: 83.49669917227975, r: 0.5675221558970814
06/02/2019 10:57:42 step: 4234, epoch: 128, batch: 9, loss: 0.3297521471977234, acc: 87.5, f1: 86.72845971233068, r: 0.7428614299319518
06/02/2019 10:57:43 step: 4239, epoch: 128, batch: 14, loss: 0.26593345403671265, acc: 90.625, f1: 90.89262732119874, r: 0.6592434509812973
06/02/2019 10:57:43 step: 4244, epoch: 128, batch: 19, loss: 0.26186925172805786, acc: 92.1875, f1: 91.42740429505135, r: 0.7579626200125475
06/02/2019 10:57:43 step: 4249, epoch: 128, batch: 24, loss: 0.2981143593788147, acc: 89.0625, f1: 89.9357647183734, r: 0.720315767560094
06/02/2019 10:57:44 step: 4254, epoch: 128, batch: 29, loss: 0.2815055847167969, acc: 93.75, f1: 82.87890913941335, r: 0.6720059664360899
06/02/2019 10:57:44 *** evaluating ***
06/02/2019 10:57:44 step: 129, epoch: 128, acc: 58.97435897435898, f1: 29.410814116002793, r: 0.3576557837246076
06/02/2019 10:57:44 *** epoch: 130 ***
06/02/2019 10:57:44 *** training ***
06/02/2019 10:57:45 step: 4262, epoch: 129, batch: 4, loss: 0.33981212973594666, acc: 92.1875, f1: 86.77524856096285, r: 0.6561982378150216
06/02/2019 10:57:45 step: 4267, epoch: 129, batch: 9, loss: 0.2275891900062561, acc: 90.625, f1: 75.58470341715022, r: 0.709117725521749
06/02/2019 10:57:45 step: 4272, epoch: 129, batch: 14, loss: 0.1477203667163849, acc: 93.75, f1: 90.56966022483263, r: 0.6859640148220424
06/02/2019 10:57:46 step: 4277, epoch: 129, batch: 19, loss: 0.16227488219738007, acc: 92.1875, f1: 91.09623015873017, r: 0.763543135407722
06/02/2019 10:57:46 step: 4282, epoch: 129, batch: 24, loss: 0.24369828402996063, acc: 87.5, f1: 90.47619047619048, r: 0.6429563232000327
06/02/2019 10:57:47 step: 4287, epoch: 129, batch: 29, loss: 0.3358404040336609, acc: 92.1875, f1: 88.05895691609979, r: 0.6001211779279987
06/02/2019 10:57:47 *** evaluating ***
06/02/2019 10:57:47 step: 130, epoch: 129, acc: 59.82905982905983, f1: 29.70073399492707, r: 0.34901325312574927
06/02/2019 10:57:47 *** epoch: 131 ***
06/02/2019 10:57:47 *** training ***
06/02/2019 10:57:47 step: 4295, epoch: 130, batch: 4, loss: 0.16024304926395416, acc: 93.75, f1: 94.92249685798073, r: 0.6827477464407881
06/02/2019 10:57:48 step: 4300, epoch: 130, batch: 9, loss: 0.09504923969507217, acc: 96.875, f1: 95.7078853046595, r: 0.7293874220036941
06/02/2019 10:57:48 step: 4305, epoch: 130, batch: 14, loss: 0.19742590188980103, acc: 95.3125, f1: 93.28481849062963, r: 0.6462140397991664
06/02/2019 10:57:49 step: 4310, epoch: 130, batch: 19, loss: 0.28630533814430237, acc: 90.625, f1: 87.17379928560052, r: 0.677232751960669
06/02/2019 10:57:49 step: 4315, epoch: 130, batch: 24, loss: 0.23487341403961182, acc: 90.625, f1: 80.4219640380828, r: 0.6960781126071964
06/02/2019 10:57:50 step: 4320, epoch: 130, batch: 29, loss: 0.15723085403442383, acc: 98.4375, f1: 97.60765550239235, r: 0.5722148212770896
06/02/2019 10:57:50 *** evaluating ***
06/02/2019 10:57:50 step: 131, epoch: 130, acc: 58.97435897435898, f1: 29.480817041058003, r: 0.35346814051265213
06/02/2019 10:57:50 *** epoch: 132 ***
06/02/2019 10:57:50 *** training ***
06/02/2019 10:57:50 step: 4328, epoch: 131, batch: 4, loss: 0.1086268201470375, acc: 96.875, f1: 96.796992481203, r: 0.5974847360499795
06/02/2019 10:57:51 step: 4333, epoch: 131, batch: 9, loss: 0.23543360829353333, acc: 89.0625, f1: 72.70361990950227, r: 0.6383283230759118
06/02/2019 10:57:51 step: 4338, epoch: 131, batch: 14, loss: 0.3107086420059204, acc: 92.1875, f1: 91.02934102934101, r: 0.6330558210346957
06/02/2019 10:57:52 step: 4343, epoch: 131, batch: 19, loss: 0.23235343396663666, acc: 93.75, f1: 87.51470588235296, r: 0.6936783357256734
06/02/2019 10:57:52 step: 4348, epoch: 131, batch: 24, loss: 0.1932993084192276, acc: 92.1875, f1: 88.81063977838171, r: 0.6118836321504566
06/02/2019 10:57:53 step: 4353, epoch: 131, batch: 29, loss: 0.21119874715805054, acc: 93.75, f1: 92.05524634096062, r: 0.6766747224586488
06/02/2019 10:57:53 *** evaluating ***
06/02/2019 10:57:53 step: 132, epoch: 131, acc: 58.119658119658126, f1: 27.313881280365887, r: 0.34887648371320107
06/02/2019 10:57:53 *** epoch: 133 ***
06/02/2019 10:57:53 *** training ***
06/02/2019 10:57:53 step: 4361, epoch: 132, batch: 4, loss: 0.16669148206710815, acc: 96.875, f1: 96.7821074899115, r: 0.6597827915389324
06/02/2019 10:57:54 step: 4366, epoch: 132, batch: 9, loss: 0.1225806251168251, acc: 96.875, f1: 97.9469696969697, r: 0.8056047316870948
06/02/2019 10:57:54 step: 4371, epoch: 132, batch: 14, loss: 0.15574049949645996, acc: 95.3125, f1: 95.5026455026455, r: 0.7122599205034901
06/02/2019 10:57:55 step: 4376, epoch: 132, batch: 19, loss: 0.15669555962085724, acc: 95.3125, f1: 92.38997717258587, r: 0.7835837824462811
06/02/2019 10:57:55 step: 4381, epoch: 132, batch: 24, loss: 0.280029296875, acc: 90.625, f1: 80.44217687074831, r: 0.5360007698700728
06/02/2019 10:57:56 step: 4386, epoch: 132, batch: 29, loss: 0.3255499601364136, acc: 87.5, f1: 87.34654234654235, r: 0.6924310846798958
06/02/2019 10:57:56 *** evaluating ***
06/02/2019 10:57:56 step: 133, epoch: 132, acc: 58.119658119658126, f1: 27.71443402768104, r: 0.3458457632665238
06/02/2019 10:57:56 *** epoch: 134 ***
06/02/2019 10:57:56 *** training ***
06/02/2019 10:57:57 step: 4394, epoch: 133, batch: 4, loss: 0.2198469489812851, acc: 92.1875, f1: 89.1919191919192, r: 0.704132477900414
06/02/2019 10:57:57 step: 4399, epoch: 133, batch: 9, loss: 0.15981489419937134, acc: 93.75, f1: 88.52534562211983, r: 0.6519476707348083
06/02/2019 10:57:57 step: 4404, epoch: 133, batch: 14, loss: 0.2987671196460724, acc: 90.625, f1: 92.75761368377138, r: 0.6200809199179592
06/02/2019 10:57:58 step: 4409, epoch: 133, batch: 19, loss: 0.20798873901367188, acc: 93.75, f1: 92.03773817219195, r: 0.6331887555093949
06/02/2019 10:57:58 step: 4414, epoch: 133, batch: 24, loss: 0.24916879832744598, acc: 90.625, f1: 89.18232315242281, r: 0.5953636141421758
06/02/2019 10:57:59 step: 4419, epoch: 133, batch: 29, loss: 0.08799618482589722, acc: 98.4375, f1: 99.18099918099918, r: 0.730721941828086
06/02/2019 10:57:59 *** evaluating ***
06/02/2019 10:57:59 step: 134, epoch: 133, acc: 57.26495726495726, f1: 27.58300633219612, r: 0.3462275949618878
06/02/2019 10:57:59 *** epoch: 135 ***
06/02/2019 10:57:59 *** training ***
06/02/2019 10:58:00 step: 4427, epoch: 134, batch: 4, loss: 0.1654093861579895, acc: 93.75, f1: 81.04218880534671, r: 0.6610124932963745
06/02/2019 10:58:00 step: 4432, epoch: 134, batch: 9, loss: 0.1189303770661354, acc: 96.875, f1: 97.85973346828611, r: 0.7942378390398498
06/02/2019 10:58:00 step: 4437, epoch: 134, batch: 14, loss: 0.15852808952331543, acc: 93.75, f1: 91.67891188964835, r: 0.6246769301170935
06/02/2019 10:58:01 step: 4442, epoch: 134, batch: 19, loss: 0.10267426073551178, acc: 95.3125, f1: 92.93777923068541, r: 0.711725973094583
06/02/2019 10:58:01 step: 4447, epoch: 134, batch: 24, loss: 0.2960503399372101, acc: 87.5, f1: 84.57008890512728, r: 0.6842842220238923
06/02/2019 10:58:02 step: 4452, epoch: 134, batch: 29, loss: 0.2816748321056366, acc: 92.1875, f1: 89.69232885758309, r: 0.6782771787393522
06/02/2019 10:58:02 *** evaluating ***
06/02/2019 10:58:02 step: 135, epoch: 134, acc: 59.401709401709404, f1: 29.47953411889197, r: 0.3489835273576222
06/02/2019 10:58:02 *** epoch: 136 ***
06/02/2019 10:58:02 *** training ***
06/02/2019 10:58:02 step: 4460, epoch: 135, batch: 4, loss: 0.16313496232032776, acc: 93.75, f1: 85.25297619047619, r: 0.7402468329199028
06/02/2019 10:58:03 step: 4465, epoch: 135, batch: 9, loss: 0.3365553617477417, acc: 89.0625, f1: 87.46023713128976, r: 0.720164472017001
06/02/2019 10:58:03 step: 4470, epoch: 135, batch: 14, loss: 0.22410592436790466, acc: 90.625, f1: 90.95918367346938, r: 0.6253003792588775
06/02/2019 10:58:03 step: 4475, epoch: 135, batch: 19, loss: 0.24006174504756927, acc: 90.625, f1: 84.59561834561835, r: 0.7212439118701841
06/02/2019 10:58:04 step: 4480, epoch: 135, batch: 24, loss: 0.3634231984615326, acc: 89.0625, f1: 83.53282732593078, r: 0.5287071298635332
06/02/2019 10:58:04 step: 4485, epoch: 135, batch: 29, loss: 0.18884839117527008, acc: 92.1875, f1: 87.74553571428572, r: 0.7723969362413718
06/02/2019 10:58:04 *** evaluating ***
06/02/2019 10:58:05 step: 136, epoch: 135, acc: 59.82905982905983, f1: 28.924045528617516, r: 0.3477729348519032
06/02/2019 10:58:05 *** epoch: 137 ***
06/02/2019 10:58:05 *** training ***
06/02/2019 10:58:05 step: 4493, epoch: 136, batch: 4, loss: 0.1502971351146698, acc: 95.3125, f1: 96.21279761904762, r: 0.8072882972756431
06/02/2019 10:58:06 step: 4498, epoch: 136, batch: 9, loss: 0.1935187578201294, acc: 95.3125, f1: 94.48361162646877, r: 0.6835887979359192
06/02/2019 10:58:06 step: 4503, epoch: 136, batch: 14, loss: 0.29875248670578003, acc: 90.625, f1: 91.23001998001999, r: 0.6716440957616886
06/02/2019 10:58:06 step: 4508, epoch: 136, batch: 19, loss: 0.21490967273712158, acc: 90.625, f1: 75.6047077922078, r: 0.699562009848858
06/02/2019 10:58:07 step: 4513, epoch: 136, batch: 24, loss: 0.1642182469367981, acc: 96.875, f1: 84.67080745341616, r: 0.7248533858826751
06/02/2019 10:58:07 step: 4518, epoch: 136, batch: 29, loss: 0.18897117674350739, acc: 95.3125, f1: 96.47584973166369, r: 0.7310625661582532
06/02/2019 10:58:08 *** evaluating ***
06/02/2019 10:58:08 step: 137, epoch: 136, acc: 59.401709401709404, f1: 29.771681584160024, r: 0.34981725021985277
06/02/2019 10:58:08 *** epoch: 138 ***
06/02/2019 10:58:08 *** training ***
06/02/2019 10:58:08 step: 4526, epoch: 137, batch: 4, loss: 0.2632996439933777, acc: 93.75, f1: 90.09951348960638, r: 0.64155330497869
06/02/2019 10:58:09 step: 4531, epoch: 137, batch: 9, loss: 0.22246241569519043, acc: 92.1875, f1: 94.08978118655537, r: 0.7538564208982548
06/02/2019 10:58:09 step: 4536, epoch: 137, batch: 14, loss: 0.17975427210330963, acc: 92.1875, f1: 92.43303062851935, r: 0.6605079567206674
06/02/2019 10:58:09 step: 4541, epoch: 137, batch: 19, loss: 0.23963740468025208, acc: 93.75, f1: 90.12455223635347, r: 0.658564389642378
06/02/2019 10:58:10 step: 4546, epoch: 137, batch: 24, loss: 0.3348047137260437, acc: 89.0625, f1: 85.97276721786226, r: 0.6955384243239991
06/02/2019 10:58:10 step: 4551, epoch: 137, batch: 29, loss: 0.11865205317735672, acc: 95.3125, f1: 93.69930835448076, r: 0.6950143714090828
06/02/2019 10:58:10 *** evaluating ***
06/02/2019 10:58:11 step: 138, epoch: 137, acc: 58.97435897435898, f1: 29.00114282566194, r: 0.34415691870524473
06/02/2019 10:58:11 *** epoch: 139 ***
06/02/2019 10:58:11 *** training ***
06/02/2019 10:58:11 step: 4559, epoch: 138, batch: 4, loss: 0.3098964989185333, acc: 87.5, f1: 84.91522366522366, r: 0.6638861655759765
06/02/2019 10:58:12 step: 4564, epoch: 138, batch: 9, loss: 0.12751352787017822, acc: 95.3125, f1: 94.96969300540728, r: 0.7387024601598852
06/02/2019 10:58:12 step: 4569, epoch: 138, batch: 14, loss: 0.33986103534698486, acc: 89.0625, f1: 74.38178780284044, r: 0.7591953919518685
06/02/2019 10:58:12 step: 4574, epoch: 138, batch: 19, loss: 0.19192683696746826, acc: 90.625, f1: 91.9827931172469, r: 0.7072220395946202
06/02/2019 10:58:13 step: 4579, epoch: 138, batch: 24, loss: 0.20661509037017822, acc: 92.1875, f1: 89.29914070891513, r: 0.6708042953570574
06/02/2019 10:58:13 step: 4584, epoch: 138, batch: 29, loss: 0.1706647276878357, acc: 92.1875, f1: 90.33613445378151, r: 0.7016596231307493
06/02/2019 10:58:14 *** evaluating ***
06/02/2019 10:58:14 step: 139, epoch: 138, acc: 59.82905982905983, f1: 29.472439353099734, r: 0.3506801236143585
06/02/2019 10:58:14 *** epoch: 140 ***
06/02/2019 10:58:14 *** training ***
06/02/2019 10:58:14 step: 4592, epoch: 139, batch: 4, loss: 0.08031583577394485, acc: 96.875, f1: 97.94695467321056, r: 0.7266320882990509
06/02/2019 10:58:14 step: 4597, epoch: 139, batch: 9, loss: 0.19458632171154022, acc: 93.75, f1: 94.36454311454311, r: 0.8140071544151017
06/02/2019 10:58:15 step: 4602, epoch: 139, batch: 14, loss: 0.14362961053848267, acc: 95.3125, f1: 92.84874355311698, r: 0.7675661701065949
06/02/2019 10:58:15 step: 4607, epoch: 139, batch: 19, loss: 0.23605787754058838, acc: 90.625, f1: 85.26540126540127, r: 0.6434083633868368
06/02/2019 10:58:16 step: 4612, epoch: 139, batch: 24, loss: 0.19572773575782776, acc: 93.75, f1: 78.29795711388003, r: 0.6703107163504657
06/02/2019 10:58:16 step: 4617, epoch: 139, batch: 29, loss: 0.25833845138549805, acc: 90.625, f1: 72.26461038961038, r: 0.672956754891865
06/02/2019 10:58:16 *** evaluating ***
06/02/2019 10:58:16 step: 140, epoch: 139, acc: 59.401709401709404, f1: 30.828156158920073, r: 0.3547702952398973
06/02/2019 10:58:16 *** epoch: 141 ***
06/02/2019 10:58:16 *** training ***
06/02/2019 10:58:17 step: 4625, epoch: 140, batch: 4, loss: 0.3541698157787323, acc: 89.0625, f1: 88.74226931804866, r: 0.7295987881219379
06/02/2019 10:58:17 step: 4630, epoch: 140, batch: 9, loss: 0.17668560147285461, acc: 93.75, f1: 89.50273643291905, r: 0.7265947969559873
06/02/2019 10:58:18 step: 4635, epoch: 140, batch: 14, loss: 0.17636507749557495, acc: 92.1875, f1: 86.19168616063027, r: 0.6634199959054283
06/02/2019 10:58:18 step: 4640, epoch: 140, batch: 19, loss: 0.1431068778038025, acc: 95.3125, f1: 96.16750208855471, r: 0.7539584395429693
06/02/2019 10:58:18 step: 4645, epoch: 140, batch: 24, loss: 0.28070566058158875, acc: 90.625, f1: 88.03069271154378, r: 0.7101258380950717
06/02/2019 10:58:19 step: 4650, epoch: 140, batch: 29, loss: 0.19337792694568634, acc: 93.75, f1: 91.26984126984128, r: 0.5467719299695788
06/02/2019 10:58:19 *** evaluating ***
06/02/2019 10:58:19 step: 141, epoch: 140, acc: 59.401709401709404, f1: 29.079973916171976, r: 0.35071185812762384
06/02/2019 10:58:19 *** epoch: 142 ***
06/02/2019 10:58:19 *** training ***
06/02/2019 10:58:20 step: 4658, epoch: 141, batch: 4, loss: 0.24194088578224182, acc: 90.625, f1: 91.86347278452541, r: 0.7367102748197821
06/02/2019 10:58:20 step: 4663, epoch: 141, batch: 9, loss: 0.039455194026231766, acc: 98.4375, f1: 98.2078853046595, r: 0.8360745873563608
06/02/2019 10:58:20 step: 4668, epoch: 141, batch: 14, loss: 0.10733919590711594, acc: 96.875, f1: 91.23920913394598, r: 0.5981562100955695
06/02/2019 10:58:21 step: 4673, epoch: 141, batch: 19, loss: 0.22443188726902008, acc: 92.1875, f1: 92.97453703703704, r: 0.6986048510683106
06/02/2019 10:58:21 step: 4678, epoch: 141, batch: 24, loss: 0.3392783999443054, acc: 89.0625, f1: 82.85336356764928, r: 0.6406232345994305
06/02/2019 10:58:22 step: 4683, epoch: 141, batch: 29, loss: 0.26141974329948425, acc: 89.0625, f1: 81.90080942360576, r: 0.5726750173856777
06/02/2019 10:58:22 *** evaluating ***
06/02/2019 10:58:22 step: 142, epoch: 141, acc: 59.401709401709404, f1: 28.594190992074285, r: 0.35787223731781637
06/02/2019 10:58:22 *** epoch: 143 ***
06/02/2019 10:58:22 *** training ***
06/02/2019 10:58:23 step: 4691, epoch: 142, batch: 4, loss: 0.117739737033844, acc: 96.875, f1: 95.95238095238095, r: 0.6654801257010289
06/02/2019 10:58:23 step: 4696, epoch: 142, batch: 9, loss: 0.2768970727920532, acc: 85.9375, f1: 76.15818089956021, r: 0.7216586123852566
06/02/2019 10:58:23 step: 4701, epoch: 142, batch: 14, loss: 0.07313273102045059, acc: 98.4375, f1: 95.37037037037037, r: 0.7764915017954147
06/02/2019 10:58:24 step: 4706, epoch: 142, batch: 19, loss: 0.12153065949678421, acc: 98.4375, f1: 96.68202764976958, r: 0.6929996725024576
06/02/2019 10:58:24 step: 4711, epoch: 142, batch: 24, loss: 0.272056519985199, acc: 90.625, f1: 90.3260614590664, r: 0.6865187459764814
06/02/2019 10:58:25 step: 4716, epoch: 142, batch: 29, loss: 0.048231448978185654, acc: 100.0, f1: 100.0, r: 0.7712464631992038
06/02/2019 10:58:25 *** evaluating ***
06/02/2019 10:58:25 step: 143, epoch: 142, acc: 59.82905982905983, f1: 29.618869076416242, r: 0.3549743610700842
06/02/2019 10:58:25 *** epoch: 144 ***
06/02/2019 10:58:25 *** training ***
06/02/2019 10:58:26 step: 4724, epoch: 143, batch: 4, loss: 0.2321254163980484, acc: 92.1875, f1: 87.14630799308219, r: 0.7196583880421653
06/02/2019 10:58:26 step: 4729, epoch: 143, batch: 9, loss: 0.1576554924249649, acc: 95.3125, f1: 84.73459845254342, r: 0.80552683548266
06/02/2019 10:58:26 step: 4734, epoch: 143, batch: 14, loss: 0.2799129784107208, acc: 93.75, f1: 94.30555555555556, r: 0.5499415679492973
06/02/2019 10:58:27 step: 4739, epoch: 143, batch: 19, loss: 0.1741638034582138, acc: 92.1875, f1: 94.47019826052085, r: 0.7036097183409034
06/02/2019 10:58:27 step: 4744, epoch: 143, batch: 24, loss: 0.06979469954967499, acc: 98.4375, f1: 98.62700228832952, r: 0.733432940197497
06/02/2019 10:58:28 step: 4749, epoch: 143, batch: 29, loss: 0.20611773431301117, acc: 90.625, f1: 89.49022085528773, r: 0.7213381774392016
06/02/2019 10:58:28 *** evaluating ***
06/02/2019 10:58:28 step: 144, epoch: 143, acc: 57.692307692307686, f1: 27.2489703279177, r: 0.352000450772014
06/02/2019 10:58:28 *** epoch: 145 ***
06/02/2019 10:58:28 *** training ***
06/02/2019 10:58:29 step: 4757, epoch: 144, batch: 4, loss: 0.2835276424884796, acc: 89.0625, f1: 87.07039365284047, r: 0.7187261280830218
06/02/2019 10:58:29 step: 4762, epoch: 144, batch: 9, loss: 0.2442377507686615, acc: 90.625, f1: 89.0801271746508, r: 0.558782176048255
06/02/2019 10:58:30 step: 4767, epoch: 144, batch: 14, loss: 0.32952404022216797, acc: 92.1875, f1: 89.54167525596097, r: 0.6780329733509272
06/02/2019 10:58:30 step: 4772, epoch: 144, batch: 19, loss: 0.112389475107193, acc: 92.1875, f1: 88.3064058956916, r: 0.6669970282986086
06/02/2019 10:58:30 step: 4777, epoch: 144, batch: 24, loss: 0.24676157534122467, acc: 89.0625, f1: 80.26350461133069, r: 0.7423453365369038
06/02/2019 10:58:31 step: 4782, epoch: 144, batch: 29, loss: 0.3235907256603241, acc: 89.0625, f1: 88.98643177014584, r: 0.6036959639764986
06/02/2019 10:58:31 *** evaluating ***
06/02/2019 10:58:31 step: 145, epoch: 144, acc: 58.97435897435898, f1: 29.936265368368076, r: 0.35134362075184133
06/02/2019 10:58:31 *** epoch: 146 ***
06/02/2019 10:58:31 *** training ***
06/02/2019 10:58:32 step: 4790, epoch: 145, batch: 4, loss: 0.1331256926059723, acc: 95.3125, f1: 95.12477106227107, r: 0.7507689151407847
06/02/2019 10:58:32 step: 4795, epoch: 145, batch: 9, loss: 0.2211553305387497, acc: 90.625, f1: 77.97273021762817, r: 0.5666255051066524
06/02/2019 10:58:33 step: 4800, epoch: 145, batch: 14, loss: 0.2293539047241211, acc: 92.1875, f1: 86.38568470866608, r: 0.610270934556247
06/02/2019 10:58:33 step: 4805, epoch: 145, batch: 19, loss: 0.10274042189121246, acc: 96.875, f1: 79.9583705025275, r: 0.6755311296891234
06/02/2019 10:58:34 step: 4810, epoch: 145, batch: 24, loss: 0.10455265641212463, acc: 96.875, f1: 97.30983302411875, r: 0.7028989772767983
06/02/2019 10:58:34 step: 4815, epoch: 145, batch: 29, loss: 0.10806412994861603, acc: 96.875, f1: 98.21311858076565, r: 0.8264775090686509
06/02/2019 10:58:34 *** evaluating ***
06/02/2019 10:58:34 step: 146, epoch: 145, acc: 60.256410256410255, f1: 31.185553301780484, r: 0.3569999906817662
06/02/2019 10:58:34 *** epoch: 147 ***
06/02/2019 10:58:34 *** training ***
06/02/2019 10:58:35 step: 4823, epoch: 146, batch: 4, loss: 0.10109009593725204, acc: 96.875, f1: 97.90336647479505, r: 0.665428538885364
06/02/2019 10:58:35 step: 4828, epoch: 146, batch: 9, loss: 0.1773400604724884, acc: 92.1875, f1: 84.78572258943102, r: 0.7444887047281872
06/02/2019 10:58:36 step: 4833, epoch: 146, batch: 14, loss: 0.1138220950961113, acc: 95.3125, f1: 95.95689033189035, r: 0.7508080076895859
06/02/2019 10:58:36 step: 4838, epoch: 146, batch: 19, loss: 0.14444121718406677, acc: 98.4375, f1: 99.19056429232192, r: 0.756083176574224
06/02/2019 10:58:36 step: 4843, epoch: 146, batch: 24, loss: 0.15235891938209534, acc: 95.3125, f1: 82.57312111447449, r: 0.519728741384693
06/02/2019 10:58:37 step: 4848, epoch: 146, batch: 29, loss: 0.03490003943443298, acc: 98.4375, f1: 96.73469387755101, r: 0.660181310137352
06/02/2019 10:58:37 *** evaluating ***
06/02/2019 10:58:37 step: 147, epoch: 146, acc: 58.97435897435898, f1: 29.889932523050017, r: 0.35254020630923394
06/02/2019 10:58:37 *** epoch: 148 ***
06/02/2019 10:58:37 *** training ***
06/02/2019 10:58:38 step: 4856, epoch: 147, batch: 4, loss: 0.1428612321615219, acc: 95.3125, f1: 89.2007992007992, r: 0.6709525936105187
06/02/2019 10:58:38 step: 4861, epoch: 147, batch: 9, loss: 0.19255484640598297, acc: 89.0625, f1: 92.12365591397848, r: 0.7025391194591872
06/02/2019 10:58:39 step: 4866, epoch: 147, batch: 14, loss: 0.2048284262418747, acc: 93.75, f1: 91.86121652092615, r: 0.716054651887409
06/02/2019 10:58:39 step: 4871, epoch: 147, batch: 19, loss: 0.288769394159317, acc: 92.1875, f1: 64.45289206917114, r: 0.6081073871888242
06/02/2019 10:58:39 step: 4876, epoch: 147, batch: 24, loss: 0.12183844298124313, acc: 95.3125, f1: 92.43750055212674, r: 0.6411725190346922
06/02/2019 10:58:40 step: 4881, epoch: 147, batch: 29, loss: 0.19415602087974548, acc: 93.75, f1: 93.1780279341255, r: 0.7281708510754169
06/02/2019 10:58:40 *** evaluating ***
06/02/2019 10:58:40 step: 148, epoch: 147, acc: 57.692307692307686, f1: 28.39645686988126, r: 0.350652086466854
06/02/2019 10:58:40 *** epoch: 149 ***
06/02/2019 10:58:40 *** training ***
06/02/2019 10:58:41 step: 4889, epoch: 148, batch: 4, loss: 0.1564296931028366, acc: 93.75, f1: 91.72575942819525, r: 0.6701139178316596
06/02/2019 10:58:41 step: 4894, epoch: 148, batch: 9, loss: 0.14060337841510773, acc: 96.875, f1: 95.22166354464491, r: 0.6775500036899226
06/02/2019 10:58:41 step: 4899, epoch: 148, batch: 14, loss: 0.17442607879638672, acc: 93.75, f1: 92.3003828274542, r: 0.7030870477014424
06/02/2019 10:58:42 step: 4904, epoch: 148, batch: 19, loss: 0.1507493257522583, acc: 93.75, f1: 96.34166563031131, r: 0.5883480749025469
06/02/2019 10:58:42 step: 4909, epoch: 148, batch: 24, loss: 0.27187246084213257, acc: 89.0625, f1: 87.17357910906298, r: 0.630471309390373
06/02/2019 10:58:43 step: 4914, epoch: 148, batch: 29, loss: 0.3209710419178009, acc: 85.9375, f1: 82.3676948051948, r: 0.7639077706480817
06/02/2019 10:58:43 *** evaluating ***
06/02/2019 10:58:43 step: 149, epoch: 148, acc: 58.119658119658126, f1: 27.48390836121624, r: 0.3505705314593064
06/02/2019 10:58:43 *** epoch: 150 ***
06/02/2019 10:58:43 *** training ***
06/02/2019 10:58:43 step: 4922, epoch: 149, batch: 4, loss: 0.25262901186943054, acc: 92.1875, f1: 89.19924812030075, r: 0.7516862051788274
06/02/2019 10:58:44 step: 4927, epoch: 149, batch: 9, loss: 0.2789439558982849, acc: 89.0625, f1: 85.78910220214567, r: 0.7216098409344124
06/02/2019 10:58:44 step: 4932, epoch: 149, batch: 14, loss: 0.15461167693138123, acc: 93.75, f1: 94.55887781469177, r: 0.7364557496632735
06/02/2019 10:58:44 step: 4937, epoch: 149, batch: 19, loss: 0.2834348976612091, acc: 92.1875, f1: 86.96632996632997, r: 0.5462125515742077
06/02/2019 10:58:45 step: 4942, epoch: 149, batch: 24, loss: 0.1215151771903038, acc: 93.75, f1: 95.02068557919621, r: 0.7210380063023756
06/02/2019 10:58:45 step: 4947, epoch: 149, batch: 29, loss: 0.12944355607032776, acc: 93.75, f1: 95.54931783192653, r: 0.7923998891778727
06/02/2019 10:58:46 *** evaluating ***
06/02/2019 10:58:46 step: 150, epoch: 149, acc: 58.54700854700855, f1: 28.79747810891098, r: 0.3452676081822525
06/02/2019 10:58:46 *** epoch: 151 ***
06/02/2019 10:58:46 *** training ***
06/02/2019 10:58:46 step: 4955, epoch: 150, batch: 4, loss: 0.16572576761245728, acc: 93.75, f1: 94.45364979847739, r: 0.7767803201784553
06/02/2019 10:58:47 step: 4960, epoch: 150, batch: 9, loss: 0.08440282940864563, acc: 95.3125, f1: 96.5581797235023, r: 0.7606250749343824
06/02/2019 10:58:47 step: 4965, epoch: 150, batch: 14, loss: 0.0657208114862442, acc: 96.875, f1: 92.16502084704851, r: 0.6968426764274052
06/02/2019 10:58:47 step: 4970, epoch: 150, batch: 19, loss: 0.17886784672737122, acc: 90.625, f1: 72.78502747252747, r: 0.7194155434191455
06/02/2019 10:58:48 step: 4975, epoch: 150, batch: 24, loss: 0.21925170719623566, acc: 87.5, f1: 80.86850347719913, r: 0.6302085035892514
06/02/2019 10:58:48 step: 4980, epoch: 150, batch: 29, loss: 0.14320167899131775, acc: 95.3125, f1: 93.26788626645154, r: 0.7044234393842029
06/02/2019 10:58:49 *** evaluating ***
06/02/2019 10:58:49 step: 151, epoch: 150, acc: 58.119658119658126, f1: 26.897004563635402, r: 0.3415018229831888
06/02/2019 10:58:49 *** epoch: 152 ***
06/02/2019 10:58:49 *** training ***
06/02/2019 10:58:49 step: 4988, epoch: 151, batch: 4, loss: 0.15963247418403625, acc: 90.625, f1: 77.15710215710216, r: 0.5511695419242088
06/02/2019 10:58:49 step: 4993, epoch: 151, batch: 9, loss: 0.11816282570362091, acc: 95.3125, f1: 93.3539892007634, r: 0.7877365231231612
06/02/2019 10:58:50 step: 4998, epoch: 151, batch: 14, loss: 0.201764315366745, acc: 93.75, f1: 97.07482993197279, r: 0.6826817505309625
06/02/2019 10:58:50 step: 5003, epoch: 151, batch: 19, loss: 0.12059472501277924, acc: 93.75, f1: 80.8089651319465, r: 0.6766469120329622
06/02/2019 10:58:51 step: 5008, epoch: 151, batch: 24, loss: 0.11111044883728027, acc: 96.875, f1: 82.574568288854, r: 0.6620574455685416
06/02/2019 10:58:51 step: 5013, epoch: 151, batch: 29, loss: 0.21206936240196228, acc: 92.1875, f1: 92.98602631935967, r: 0.5997927641880597
06/02/2019 10:58:51 *** evaluating ***
06/02/2019 10:58:52 step: 152, epoch: 151, acc: 58.97435897435898, f1: 28.957855604197068, r: 0.3534364922322276
06/02/2019 10:58:52 *** epoch: 153 ***
06/02/2019 10:58:52 *** training ***
06/02/2019 10:58:52 step: 5021, epoch: 152, batch: 4, loss: 0.13537809252738953, acc: 95.3125, f1: 95.48825982005705, r: 0.6907818587483776
06/02/2019 10:58:52 step: 5026, epoch: 152, batch: 9, loss: 0.16210193932056427, acc: 93.75, f1: 92.49890969972107, r: 0.6765832924885131
06/02/2019 10:58:53 step: 5031, epoch: 152, batch: 14, loss: 0.2680189609527588, acc: 85.9375, f1: 82.13220780889954, r: 0.764820649271496
06/02/2019 10:58:53 step: 5036, epoch: 152, batch: 19, loss: 0.07084929198026657, acc: 96.875, f1: 97.47244714349978, r: 0.7909512763927241
06/02/2019 10:58:54 step: 5041, epoch: 152, batch: 24, loss: 0.11316849291324615, acc: 96.875, f1: 97.55530417295122, r: 0.8131800876073687
06/02/2019 10:58:54 step: 5046, epoch: 152, batch: 29, loss: 0.27671343088150024, acc: 90.625, f1: 75.74458874458874, r: 0.682038523806805
06/02/2019 10:58:54 *** evaluating ***
06/02/2019 10:58:55 step: 153, epoch: 152, acc: 58.97435897435898, f1: 26.54642218936051, r: 0.35222303250696274
06/02/2019 10:58:55 *** epoch: 154 ***
06/02/2019 10:58:55 *** training ***
06/02/2019 10:58:55 step: 5054, epoch: 153, batch: 4, loss: 0.06952977925539017, acc: 98.4375, f1: 94.6969696969697, r: 0.7440828280652133
06/02/2019 10:58:55 step: 5059, epoch: 153, batch: 9, loss: 0.15904289484024048, acc: 96.875, f1: 84.19984387197502, r: 0.7507251751569938
06/02/2019 10:58:56 step: 5064, epoch: 153, batch: 14, loss: 0.1506446897983551, acc: 92.1875, f1: 92.96629081512803, r: 0.8096323429486614
06/02/2019 10:58:56 step: 5069, epoch: 153, batch: 19, loss: 0.16033945977687836, acc: 93.75, f1: 83.73620660385366, r: 0.7920492736110288
06/02/2019 10:58:57 step: 5074, epoch: 153, batch: 24, loss: 0.17909523844718933, acc: 92.1875, f1: 91.46894771894772, r: 0.8028938011182613
06/02/2019 10:58:57 step: 5079, epoch: 153, batch: 29, loss: 0.27009090781211853, acc: 92.1875, f1: 91.39598887954152, r: 0.7648624111541753
06/02/2019 10:58:57 *** evaluating ***
06/02/2019 10:58:57 step: 154, epoch: 153, acc: 59.82905982905983, f1: 29.96603485466834, r: 0.35211871070157724
06/02/2019 10:58:57 *** epoch: 155 ***
06/02/2019 10:58:57 *** training ***
06/02/2019 10:58:58 step: 5087, epoch: 154, batch: 4, loss: 0.14177367091178894, acc: 95.3125, f1: 90.9373235460192, r: 0.6666787308351853
06/02/2019 10:58:58 step: 5092, epoch: 154, batch: 9, loss: 0.1525314450263977, acc: 93.75, f1: 93.06788628822527, r: 0.6038179895265192
06/02/2019 10:58:59 step: 5097, epoch: 154, batch: 14, loss: 0.08247483521699905, acc: 98.4375, f1: 98.8422035480859, r: 0.699506463592822
06/02/2019 10:58:59 step: 5102, epoch: 154, batch: 19, loss: 0.06824231892824173, acc: 98.4375, f1: 99.14965986394557, r: 0.7721509548915235
06/02/2019 10:59:00 step: 5107, epoch: 154, batch: 24, loss: 0.16015812754631042, acc: 93.75, f1: 93.2511055644483, r: 0.7446464463936239
06/02/2019 10:59:00 step: 5112, epoch: 154, batch: 29, loss: 0.13766127824783325, acc: 95.3125, f1: 95.18612235717498, r: 0.8078922413346683
06/02/2019 10:59:00 *** evaluating ***
06/02/2019 10:59:00 step: 155, epoch: 154, acc: 57.26495726495726, f1: 26.3798049340218, r: 0.3398738378255877
06/02/2019 10:59:00 *** epoch: 156 ***
06/02/2019 10:59:00 *** training ***
06/02/2019 10:59:01 step: 5120, epoch: 155, batch: 4, loss: 0.13237424194812775, acc: 95.3125, f1: 94.64241286901385, r: 0.7014889646335736
06/02/2019 10:59:01 step: 5125, epoch: 155, batch: 9, loss: 0.08305767178535461, acc: 96.875, f1: 96.21393099653969, r: 0.7698156503267289
06/02/2019 10:59:02 step: 5130, epoch: 155, batch: 14, loss: 0.2706078886985779, acc: 90.625, f1: 88.46505982381385, r: 0.6730698929307449
06/02/2019 10:59:02 step: 5135, epoch: 155, batch: 19, loss: 0.09350466728210449, acc: 96.875, f1: 96.3509005134621, r: 0.7148366586767078
06/02/2019 10:59:02 step: 5140, epoch: 155, batch: 24, loss: 0.12130513042211533, acc: 96.875, f1: 84.76415094339622, r: 0.7509712770425445
06/02/2019 10:59:03 step: 5145, epoch: 155, batch: 29, loss: 0.23494499921798706, acc: 93.75, f1: 79.85029463290333, r: 0.6459639622126847
06/02/2019 10:59:03 *** evaluating ***
06/02/2019 10:59:03 step: 156, epoch: 155, acc: 57.692307692307686, f1: 24.935083166790484, r: 0.33450144364047985
06/02/2019 10:59:03 *** epoch: 157 ***
06/02/2019 10:59:03 *** training ***
06/02/2019 10:59:03 step: 5153, epoch: 156, batch: 4, loss: 0.06375802308320999, acc: 100.0, f1: 100.0, r: 0.6392464393722779
06/02/2019 10:59:04 step: 5158, epoch: 156, batch: 9, loss: 0.12199870496988297, acc: 95.3125, f1: 93.25649288055304, r: 0.7370562743887871
06/02/2019 10:59:04 step: 5163, epoch: 156, batch: 14, loss: 0.1867430955171585, acc: 92.1875, f1: 87.63125763125763, r: 0.6246899743121438
06/02/2019 10:59:05 step: 5168, epoch: 156, batch: 19, loss: 0.20147478580474854, acc: 93.75, f1: 87.99724479549042, r: 0.5979348514985339
06/02/2019 10:59:05 step: 5173, epoch: 156, batch: 24, loss: 0.23015472292900085, acc: 90.625, f1: 89.48059968746452, r: 0.7647499922864188
06/02/2019 10:59:05 step: 5178, epoch: 156, batch: 29, loss: 0.15097753703594208, acc: 93.75, f1: 88.95934131228249, r: 0.7066258807073528
06/02/2019 10:59:06 *** evaluating ***
06/02/2019 10:59:06 step: 157, epoch: 156, acc: 57.692307692307686, f1: 25.875378192451365, r: 0.3389784172080682
06/02/2019 10:59:06 *** epoch: 158 ***
06/02/2019 10:59:06 *** training ***
06/02/2019 10:59:06 step: 5186, epoch: 157, batch: 4, loss: 0.11810308694839478, acc: 96.875, f1: 96.93121693121694, r: 0.6697660855893827
06/02/2019 10:59:06 step: 5191, epoch: 157, batch: 9, loss: 0.0931200236082077, acc: 95.3125, f1: 84.7465034965035, r: 0.8136585433200181
06/02/2019 10:59:07 step: 5196, epoch: 157, batch: 14, loss: 0.15363430976867676, acc: 93.75, f1: 80.37752525252525, r: 0.5779693373041306
06/02/2019 10:59:07 step: 5201, epoch: 157, batch: 19, loss: 0.18225201964378357, acc: 93.75, f1: 94.53113845219109, r: 0.7766731899673716
06/02/2019 10:59:08 step: 5206, epoch: 157, batch: 24, loss: 0.1237078607082367, acc: 95.3125, f1: 93.28985570680322, r: 0.6069037645119119
06/02/2019 10:59:08 step: 5211, epoch: 157, batch: 29, loss: 0.166352316737175, acc: 95.3125, f1: 95.14132925897633, r: 0.7083623650162075
06/02/2019 10:59:08 *** evaluating ***
06/02/2019 10:59:09 step: 158, epoch: 157, acc: 57.26495726495726, f1: 28.759008458831524, r: 0.344756221431393
06/02/2019 10:59:09 *** epoch: 159 ***
06/02/2019 10:59:09 *** training ***
06/02/2019 10:59:09 step: 5219, epoch: 158, batch: 4, loss: 0.12221147865056992, acc: 95.3125, f1: 91.4373208875788, r: 0.5264452849747471
06/02/2019 10:59:09 step: 5224, epoch: 158, batch: 9, loss: 0.2809857130050659, acc: 90.625, f1: 77.22295369760808, r: 0.5242207225344185
06/02/2019 10:59:10 step: 5229, epoch: 158, batch: 14, loss: 0.17121052742004395, acc: 95.3125, f1: 93.2062527138515, r: 0.668434662912949
06/02/2019 10:59:10 step: 5234, epoch: 158, batch: 19, loss: 0.17380833625793457, acc: 92.1875, f1: 75.29270066812673, r: 0.7671602574230739
06/02/2019 10:59:11 step: 5239, epoch: 158, batch: 24, loss: 0.20076042413711548, acc: 93.75, f1: 79.73901098901099, r: 0.7601237713775557
06/02/2019 10:59:11 step: 5244, epoch: 158, batch: 29, loss: 0.28711387515068054, acc: 89.0625, f1: 85.08225108225108, r: 0.630181650086472
06/02/2019 10:59:11 *** evaluating ***
06/02/2019 10:59:12 step: 159, epoch: 158, acc: 55.98290598290598, f1: 25.231145001787205, r: 0.3467168203702331
06/02/2019 10:59:12 *** epoch: 160 ***
06/02/2019 10:59:12 *** training ***
06/02/2019 10:59:12 step: 5252, epoch: 159, batch: 4, loss: 0.2100975066423416, acc: 92.1875, f1: 87.50944943659962, r: 0.6879419613604512
06/02/2019 10:59:12 step: 5257, epoch: 159, batch: 9, loss: 0.11600640416145325, acc: 98.4375, f1: 98.38383838383838, r: 0.7253507446059596
06/02/2019 10:59:13 step: 5262, epoch: 159, batch: 14, loss: 0.1047292947769165, acc: 95.3125, f1: 95.96280262946931, r: 0.5692293839831887
06/02/2019 10:59:13 step: 5267, epoch: 159, batch: 19, loss: 0.12063300609588623, acc: 96.875, f1: 96.41841152045234, r: 0.6610349502658871
06/02/2019 10:59:14 step: 5272, epoch: 159, batch: 24, loss: 0.23406386375427246, acc: 89.0625, f1: 86.01781151332005, r: 0.6179603475470623
06/02/2019 10:59:14 step: 5277, epoch: 159, batch: 29, loss: 0.18754878640174866, acc: 95.3125, f1: 84.19289749798224, r: 0.7268899511913426
06/02/2019 10:59:15 *** evaluating ***
06/02/2019 10:59:15 step: 160, epoch: 159, acc: 57.692307692307686, f1: 31.315876780739227, r: 0.3424913383313547
06/02/2019 10:59:15 *** epoch: 161 ***
06/02/2019 10:59:15 *** training ***
06/02/2019 10:59:15 step: 5285, epoch: 160, batch: 4, loss: 0.11657443642616272, acc: 98.4375, f1: 99.15343915343915, r: 0.6927146561507875
06/02/2019 10:59:16 step: 5290, epoch: 160, batch: 9, loss: 0.17530502378940582, acc: 96.875, f1: 98.1068352496924, r: 0.6330894441926948
06/02/2019 10:59:16 step: 5295, epoch: 160, batch: 14, loss: 0.12194385379552841, acc: 96.875, f1: 83.46073913436342, r: 0.587371590593621
06/02/2019 10:59:17 step: 5300, epoch: 160, batch: 19, loss: 0.10529721528291702, acc: 98.4375, f1: 98.43175692232296, r: 0.7171463420837304
06/02/2019 10:59:17 step: 5305, epoch: 160, batch: 24, loss: 0.23136386275291443, acc: 92.1875, f1: 90.75436607391494, r: 0.6467753780557819
06/02/2019 10:59:17 step: 5310, epoch: 160, batch: 29, loss: 0.18516521155834198, acc: 96.875, f1: 94.70930232558139, r: 0.7493474250676053
06/02/2019 10:59:18 *** evaluating ***
06/02/2019 10:59:18 step: 161, epoch: 160, acc: 59.401709401709404, f1: 27.84193523867437, r: 0.3494199763445591
06/02/2019 10:59:18 *** epoch: 162 ***
06/02/2019 10:59:18 *** training ***
06/02/2019 10:59:18 step: 5318, epoch: 161, batch: 4, loss: 0.21618719398975372, acc: 92.1875, f1: 87.86641929499072, r: 0.6929644349052769
06/02/2019 10:59:19 step: 5323, epoch: 161, batch: 9, loss: 0.19671200215816498, acc: 95.3125, f1: 92.51155100211705, r: 0.6463730703139275
06/02/2019 10:59:19 step: 5328, epoch: 161, batch: 14, loss: 0.1394188404083252, acc: 95.3125, f1: 95.35567880395467, r: 0.755463457151714
06/02/2019 10:59:20 step: 5333, epoch: 161, batch: 19, loss: 0.15229758620262146, acc: 95.3125, f1: 97.16949716949716, r: 0.6703663026363482
06/02/2019 10:59:20 step: 5338, epoch: 161, batch: 24, loss: 0.18311062455177307, acc: 93.75, f1: 91.26447189045109, r: 0.5899967749105641
06/02/2019 10:59:21 step: 5343, epoch: 161, batch: 29, loss: 0.11916211992502213, acc: 95.3125, f1: 96.17287474430333, r: 0.6779570131045262
06/02/2019 10:59:21 *** evaluating ***
06/02/2019 10:59:21 step: 162, epoch: 161, acc: 58.119658119658126, f1: 28.002249807805363, r: 0.35200864048650515
06/02/2019 10:59:21 *** epoch: 163 ***
06/02/2019 10:59:21 *** training ***
06/02/2019 10:59:22 step: 5351, epoch: 162, batch: 4, loss: 0.19243457913398743, acc: 95.3125, f1: 94.05381944444444, r: 0.7682819330266641
06/02/2019 10:59:22 step: 5356, epoch: 162, batch: 9, loss: 0.18739858269691467, acc: 95.3125, f1: 82.96805977071632, r: 0.6615461109076295
06/02/2019 10:59:22 step: 5361, epoch: 162, batch: 14, loss: 0.22066465020179749, acc: 89.0625, f1: 86.18281304616707, r: 0.754681115709624
06/02/2019 10:59:23 step: 5366, epoch: 162, batch: 19, loss: 0.20354019105434418, acc: 96.875, f1: 92.45824706694272, r: 0.6868243050446101
06/02/2019 10:59:23 step: 5371, epoch: 162, batch: 24, loss: 0.2977409362792969, acc: 90.625, f1: 88.38868880885687, r: 0.6375530592027558
06/02/2019 10:59:23 step: 5376, epoch: 162, batch: 29, loss: 0.09396419674158096, acc: 98.4375, f1: 97.79158040027606, r: 0.7212528617298097
06/02/2019 10:59:24 *** evaluating ***
06/02/2019 10:59:24 step: 163, epoch: 162, acc: 60.256410256410255, f1: 29.4760871758652, r: 0.35898418327057746
06/02/2019 10:59:24 *** epoch: 164 ***
06/02/2019 10:59:24 *** training ***
06/02/2019 10:59:24 step: 5384, epoch: 163, batch: 4, loss: 0.09793426096439362, acc: 98.4375, f1: 98.35286664554957, r: 0.7225096569530692
06/02/2019 10:59:25 step: 5389, epoch: 163, batch: 9, loss: 0.23329795897006989, acc: 95.3125, f1: 87.78602350030921, r: 0.6034624768521661
06/02/2019 10:59:25 step: 5394, epoch: 163, batch: 14, loss: 0.23786407709121704, acc: 93.75, f1: 87.63477039506097, r: 0.6522289108653362
06/02/2019 10:59:26 step: 5399, epoch: 163, batch: 19, loss: 0.15036599338054657, acc: 93.75, f1: 81.17527173913044, r: 0.6338620597562801
06/02/2019 10:59:26 step: 5404, epoch: 163, batch: 24, loss: 0.12199901044368744, acc: 93.75, f1: 87.48475309856386, r: 0.778739645647216
06/02/2019 10:59:27 step: 5409, epoch: 163, batch: 29, loss: 0.13659431040287018, acc: 96.875, f1: 91.27107446752287, r: 0.6966948079078384
06/02/2019 10:59:27 *** evaluating ***
06/02/2019 10:59:27 step: 164, epoch: 163, acc: 57.692307692307686, f1: 27.223027079894855, r: 0.3476103758933802
06/02/2019 10:59:27 *** epoch: 165 ***
06/02/2019 10:59:27 *** training ***
06/02/2019 10:59:27 step: 5417, epoch: 164, batch: 4, loss: 0.06024923548102379, acc: 98.4375, f1: 99.25676259614553, r: 0.6731200559225907
06/02/2019 10:59:28 step: 5422, epoch: 164, batch: 9, loss: 0.0956023558974266, acc: 98.4375, f1: 98.02659802659804, r: 0.6941945767504483
06/02/2019 10:59:28 step: 5427, epoch: 164, batch: 14, loss: 0.1610240340232849, acc: 93.75, f1: 85.47301136363636, r: 0.7183181412775078
06/02/2019 10:59:29 step: 5432, epoch: 164, batch: 19, loss: 0.203502357006073, acc: 92.1875, f1: 83.7854609929078, r: 0.7057322984072087
06/02/2019 10:59:29 step: 5437, epoch: 164, batch: 24, loss: 0.10894094407558441, acc: 95.3125, f1: 94.0878147235906, r: 0.7467943965411613
06/02/2019 10:59:29 step: 5442, epoch: 164, batch: 29, loss: 0.21113350987434387, acc: 92.1875, f1: 83.94709513307896, r: 0.6486213519149209
06/02/2019 10:59:30 *** evaluating ***
06/02/2019 10:59:30 step: 165, epoch: 164, acc: 56.837606837606835, f1: 26.271097798697284, r: 0.3503775310880568
06/02/2019 10:59:30 *** epoch: 166 ***
06/02/2019 10:59:30 *** training ***
06/02/2019 10:59:30 step: 5450, epoch: 165, batch: 4, loss: 0.12154322117567062, acc: 95.3125, f1: 92.41598679098679, r: 0.789208911012401
06/02/2019 10:59:31 step: 5455, epoch: 165, batch: 9, loss: 0.0880064070224762, acc: 95.3125, f1: 92.88996717568146, r: 0.700685472647141
06/02/2019 10:59:31 step: 5460, epoch: 165, batch: 14, loss: 0.12242044508457184, acc: 96.875, f1: 95.7591342209882, r: 0.7715997868926603
06/02/2019 10:59:32 step: 5465, epoch: 165, batch: 19, loss: 0.10965478420257568, acc: 95.3125, f1: 95.44793868921776, r: 0.7363271364093149
06/02/2019 10:59:32 step: 5470, epoch: 165, batch: 24, loss: 0.09654523432254791, acc: 98.4375, f1: 99.28760894278136, r: 0.7066090904205405
06/02/2019 10:59:32 step: 5475, epoch: 165, batch: 29, loss: 0.16823579370975494, acc: 93.75, f1: 87.930669585842, r: 0.6188150355810349
06/02/2019 10:59:33 *** evaluating ***
06/02/2019 10:59:33 step: 166, epoch: 165, acc: 57.26495726495726, f1: 26.583458722581256, r: 0.34262389280083744
06/02/2019 10:59:33 *** epoch: 167 ***
06/02/2019 10:59:33 *** training ***
06/02/2019 10:59:33 step: 5483, epoch: 166, batch: 4, loss: 0.07475161552429199, acc: 98.4375, f1: 95.0, r: 0.761590948192395
06/02/2019 10:59:34 step: 5488, epoch: 166, batch: 9, loss: 0.1469261795282364, acc: 93.75, f1: 83.50028417163968, r: 0.757520693534278
06/02/2019 10:59:34 step: 5493, epoch: 166, batch: 14, loss: 0.14149799942970276, acc: 95.3125, f1: 95.63034188034187, r: 0.7490540134681104
06/02/2019 10:59:34 step: 5498, epoch: 166, batch: 19, loss: 0.05951806902885437, acc: 98.4375, f1: 99.31899641577061, r: 0.8075600038980356
06/02/2019 10:59:35 step: 5503, epoch: 166, batch: 24, loss: 0.2237570881843567, acc: 93.75, f1: 93.07422969187675, r: 0.6724826902778487
06/02/2019 10:59:35 step: 5508, epoch: 166, batch: 29, loss: 0.11659310758113861, acc: 95.3125, f1: 94.9344375431332, r: 0.6559407980172831
06/02/2019 10:59:35 *** evaluating ***
06/02/2019 10:59:35 step: 167, epoch: 166, acc: 57.26495726495726, f1: 28.836714785980476, r: 0.34890305715719827
06/02/2019 10:59:35 *** epoch: 168 ***
06/02/2019 10:59:35 *** training ***
06/02/2019 10:59:36 step: 5516, epoch: 167, batch: 4, loss: 0.10747234523296356, acc: 98.4375, f1: 99.00226757369614, r: 0.6555575871299099
06/02/2019 10:59:36 step: 5521, epoch: 167, batch: 9, loss: 0.1393471360206604, acc: 93.75, f1: 92.06691682301438, r: 0.7011557926704369
06/02/2019 10:59:37 step: 5526, epoch: 167, batch: 14, loss: 0.2617190480232239, acc: 93.75, f1: 93.37930087930089, r: 0.6414845264265726
06/02/2019 10:59:37 step: 5531, epoch: 167, batch: 19, loss: 0.2659640610218048, acc: 90.625, f1: 84.74303474303476, r: 0.5991042167716932
06/02/2019 10:59:37 step: 5536, epoch: 167, batch: 24, loss: 0.11515295505523682, acc: 95.3125, f1: 92.32620977646769, r: 0.5843300126196215
06/02/2019 10:59:38 step: 5541, epoch: 167, batch: 29, loss: 0.11114010214805603, acc: 96.875, f1: 98.5636645962733, r: 0.7965120044587728
06/02/2019 10:59:38 *** evaluating ***
06/02/2019 10:59:38 step: 168, epoch: 167, acc: 58.97435897435898, f1: 24.174656015183814, r: 0.34867782311738305
06/02/2019 10:59:38 *** epoch: 169 ***
06/02/2019 10:59:38 *** training ***
06/02/2019 10:59:39 step: 5549, epoch: 168, batch: 4, loss: 0.1723085641860962, acc: 92.1875, f1: 78.91742627450749, r: 0.6605542644741679
06/02/2019 10:59:39 step: 5554, epoch: 168, batch: 9, loss: 0.2242531180381775, acc: 92.1875, f1: 89.70020401054883, r: 0.6687292437564482
06/02/2019 10:59:39 step: 5559, epoch: 168, batch: 14, loss: 0.1135835200548172, acc: 96.875, f1: 96.58613445378151, r: 0.7516572977184822
06/02/2019 10:59:40 step: 5564, epoch: 168, batch: 19, loss: 0.08747869729995728, acc: 96.875, f1: 98.49537037037037, r: 0.772355189519735
06/02/2019 10:59:40 step: 5569, epoch: 168, batch: 24, loss: 0.1440020501613617, acc: 93.75, f1: 88.92290249433107, r: 0.6815198589147262
06/02/2019 10:59:41 step: 5574, epoch: 168, batch: 29, loss: 0.09462778270244598, acc: 95.3125, f1: 85.72804314329738, r: 0.7711396374557931
06/02/2019 10:59:41 *** evaluating ***
06/02/2019 10:59:41 step: 169, epoch: 168, acc: 58.54700854700855, f1: 29.35217062811402, r: 0.3541319302702958
06/02/2019 10:59:41 *** epoch: 170 ***
06/02/2019 10:59:41 *** training ***
06/02/2019 10:59:41 step: 5582, epoch: 169, batch: 4, loss: 0.11882752925157547, acc: 96.875, f1: 97.53334062936794, r: 0.5803735335421658
06/02/2019 10:59:42 step: 5587, epoch: 169, batch: 9, loss: 0.14930778741836548, acc: 96.875, f1: 93.33575464083938, r: 0.7624789124842131
06/02/2019 10:59:42 step: 5592, epoch: 169, batch: 14, loss: 0.13628053665161133, acc: 96.875, f1: 97.40973811339944, r: 0.7921342640412202
06/02/2019 10:59:43 step: 5597, epoch: 169, batch: 19, loss: 0.14185938239097595, acc: 93.75, f1: 89.74639664294837, r: 0.775632911711034
06/02/2019 10:59:43 step: 5602, epoch: 169, batch: 24, loss: 0.18838311731815338, acc: 92.1875, f1: 82.67560927444649, r: 0.7317681708117049
06/02/2019 10:59:43 step: 5607, epoch: 169, batch: 29, loss: 0.060418110340833664, acc: 96.875, f1: 98.01767676767676, r: 0.7467309652493257
06/02/2019 10:59:44 *** evaluating ***
06/02/2019 10:59:44 step: 170, epoch: 169, acc: 58.97435897435898, f1: 27.54354091684957, r: 0.34666086710690897
06/02/2019 10:59:44 *** epoch: 171 ***
06/02/2019 10:59:44 *** training ***
06/02/2019 10:59:44 step: 5615, epoch: 170, batch: 4, loss: 0.13586050271987915, acc: 95.3125, f1: 97.07352672869914, r: 0.6908639308643493
06/02/2019 10:59:44 step: 5620, epoch: 170, batch: 9, loss: 0.14969229698181152, acc: 95.3125, f1: 92.02523114508962, r: 0.6840414719880222
06/02/2019 10:59:45 step: 5625, epoch: 170, batch: 14, loss: 0.08055473864078522, acc: 95.3125, f1: 95.64384818254787, r: 0.750352641394044
06/02/2019 10:59:45 step: 5630, epoch: 170, batch: 19, loss: 0.2505015432834625, acc: 92.1875, f1: 88.35282651072124, r: 0.6480680388592223
06/02/2019 10:59:46 step: 5635, epoch: 170, batch: 24, loss: 0.16840755939483643, acc: 95.3125, f1: 80.0361846866406, r: 0.5810102694275074
06/02/2019 10:59:46 step: 5640, epoch: 170, batch: 29, loss: 0.07805345207452774, acc: 98.4375, f1: 99.17748917748919, r: 0.8222518381829711
06/02/2019 10:59:46 *** evaluating ***
06/02/2019 10:59:46 step: 171, epoch: 170, acc: 58.97435897435898, f1: 26.595075008630698, r: 0.3470170984628452
06/02/2019 10:59:46 *** epoch: 172 ***
06/02/2019 10:59:46 *** training ***
06/02/2019 10:59:47 step: 5648, epoch: 171, batch: 4, loss: 0.1079387366771698, acc: 95.3125, f1: 82.42881400208987, r: 0.825505817406228
06/02/2019 10:59:47 step: 5653, epoch: 171, batch: 9, loss: 0.12341590970754623, acc: 93.75, f1: 90.07796964939821, r: 0.6600743816073665
06/02/2019 10:59:48 step: 5658, epoch: 171, batch: 14, loss: 0.09819468855857849, acc: 98.4375, f1: 97.03703703703704, r: 0.7328175901125693
06/02/2019 10:59:48 step: 5663, epoch: 171, batch: 19, loss: 0.07973935455083847, acc: 95.3125, f1: 95.72210197210197, r: 0.8108563264544941
06/02/2019 10:59:49 step: 5668, epoch: 171, batch: 24, loss: 0.23164045810699463, acc: 92.1875, f1: 91.3450604122246, r: 0.6814197143011915
06/02/2019 10:59:49 step: 5673, epoch: 171, batch: 29, loss: 0.08674369752407074, acc: 95.3125, f1: 94.41659401336821, r: 0.8230023235330991
06/02/2019 10:59:49 *** evaluating ***
06/02/2019 10:59:50 step: 172, epoch: 171, acc: 59.401709401709404, f1: 28.636512858503405, r: 0.34524904513155097
06/02/2019 10:59:50 *** epoch: 173 ***
06/02/2019 10:59:50 *** training ***
06/02/2019 10:59:50 step: 5681, epoch: 172, batch: 4, loss: 0.17686933279037476, acc: 93.75, f1: 93.4714491857349, r: 0.6574244954891627
06/02/2019 10:59:50 step: 5686, epoch: 172, batch: 9, loss: 0.164306640625, acc: 96.875, f1: 97.00574334294342, r: 0.6374622493137384
06/02/2019 10:59:51 step: 5691, epoch: 172, batch: 14, loss: 0.24873220920562744, acc: 89.0625, f1: 85.2304292929293, r: 0.7387456875468447
06/02/2019 10:59:51 step: 5696, epoch: 172, batch: 19, loss: 0.20620964467525482, acc: 87.5, f1: 79.26330604837216, r: 0.5932923341391496
06/02/2019 10:59:52 step: 5701, epoch: 172, batch: 24, loss: 0.07946553826332092, acc: 98.4375, f1: 97.87581699346406, r: 0.7840008169510531
06/02/2019 10:59:52 step: 5706, epoch: 172, batch: 29, loss: 0.1662774384021759, acc: 93.75, f1: 89.95464582229289, r: 0.742616408391404
06/02/2019 10:59:52 *** evaluating ***
06/02/2019 10:59:52 step: 173, epoch: 172, acc: 58.97435897435898, f1: 29.517473267473264, r: 0.34266151419622054
06/02/2019 10:59:52 *** epoch: 174 ***
06/02/2019 10:59:52 *** training ***
06/02/2019 10:59:53 step: 5714, epoch: 173, batch: 4, loss: 0.1299513876438141, acc: 93.75, f1: 93.42159361693815, r: 0.7706622499018909
06/02/2019 10:59:53 step: 5719, epoch: 173, batch: 9, loss: 0.03489711880683899, acc: 100.0, f1: 100.0, r: 0.7132160431343463
06/02/2019 10:59:54 step: 5724, epoch: 173, batch: 14, loss: 0.10030000656843185, acc: 96.875, f1: 91.58668872954587, r: 0.5934649272765716
06/02/2019 10:59:54 step: 5729, epoch: 173, batch: 19, loss: 0.5135380029678345, acc: 89.0625, f1: 86.43546929261214, r: 0.6467242398568532
06/02/2019 10:59:55 step: 5734, epoch: 173, batch: 24, loss: 0.0908459946513176, acc: 96.875, f1: 96.44289490474888, r: 0.7574660834454183
06/02/2019 10:59:55 step: 5739, epoch: 173, batch: 29, loss: 0.11822731792926788, acc: 95.3125, f1: 92.10526315789473, r: 0.6691618800948992
06/02/2019 10:59:55 *** evaluating ***
06/02/2019 10:59:55 step: 174, epoch: 173, acc: 59.401709401709404, f1: 30.75262408157145, r: 0.3448509218284415
06/02/2019 10:59:55 *** epoch: 175 ***
06/02/2019 10:59:55 *** training ***
06/02/2019 10:59:56 step: 5747, epoch: 174, batch: 4, loss: 0.07980462163686752, acc: 98.4375, f1: 98.57549857549857, r: 0.7445383027872914
06/02/2019 10:59:56 step: 5752, epoch: 174, batch: 9, loss: 0.09254603832960129, acc: 96.875, f1: 94.5268044324648, r: 0.6214996293850176
06/02/2019 10:59:57 step: 5757, epoch: 174, batch: 14, loss: 0.07332104444503784, acc: 98.4375, f1: 98.36601307189542, r: 0.7572136499414922
06/02/2019 10:59:57 step: 5762, epoch: 174, batch: 19, loss: 0.1762545108795166, acc: 93.75, f1: 97.03525641025641, r: 0.6953238547232742
06/02/2019 10:59:57 step: 5767, epoch: 174, batch: 24, loss: 0.2134176790714264, acc: 95.3125, f1: 96.21781727044885, r: 0.6712056148979438
06/02/2019 10:59:58 step: 5772, epoch: 174, batch: 29, loss: 0.06965525448322296, acc: 95.3125, f1: 95.34403925504628, r: 0.6679584431720582
06/02/2019 10:59:58 *** evaluating ***
06/02/2019 10:59:58 step: 175, epoch: 174, acc: 58.119658119658126, f1: 28.833954068195833, r: 0.34457905911317366
06/02/2019 10:59:58 *** epoch: 176 ***
06/02/2019 10:59:58 *** training ***
06/02/2019 10:59:58 step: 5780, epoch: 175, batch: 4, loss: 0.10495802760124207, acc: 95.3125, f1: 91.69000933706816, r: 0.695692349132863
06/02/2019 10:59:59 step: 5785, epoch: 175, batch: 9, loss: 0.21444465219974518, acc: 90.625, f1: 87.82275352296463, r: 0.6684250000801151
06/02/2019 10:59:59 step: 5790, epoch: 175, batch: 14, loss: 0.1397581547498703, acc: 95.3125, f1: 95.43393114821686, r: 0.7668812309702576
06/02/2019 11:00:00 step: 5795, epoch: 175, batch: 19, loss: 0.23779504001140594, acc: 92.1875, f1: 90.0331286360698, r: 0.7196919613364676
06/02/2019 11:00:00 step: 5800, epoch: 175, batch: 24, loss: 0.0912412628531456, acc: 96.875, f1: 82.1047421047421, r: 0.6368519301650349
06/02/2019 11:00:00 step: 5805, epoch: 175, batch: 29, loss: 0.06626220047473907, acc: 98.4375, f1: 99.17935428139509, r: 0.7187830177792829
06/02/2019 11:00:00 *** evaluating ***
06/02/2019 11:00:01 step: 176, epoch: 175, acc: 58.54700854700855, f1: 28.047931148626283, r: 0.3409266782480443
06/02/2019 11:00:01 *** epoch: 177 ***
06/02/2019 11:00:01 *** training ***
06/02/2019 11:00:01 step: 5813, epoch: 176, batch: 4, loss: 0.11917966604232788, acc: 96.875, f1: 97.45791245791246, r: 0.6870084752513128
06/02/2019 11:00:02 step: 5818, epoch: 176, batch: 9, loss: 0.20164306461811066, acc: 90.625, f1: 90.06790986189196, r: 0.6618767141420662
06/02/2019 11:00:02 step: 5823, epoch: 176, batch: 14, loss: 0.11313825845718384, acc: 93.75, f1: 92.23957329220488, r: 0.7160213298341515
06/02/2019 11:00:02 step: 5828, epoch: 176, batch: 19, loss: 0.18320520222187042, acc: 92.1875, f1: 90.90009024791634, r: 0.7352742289031868
06/02/2019 11:00:03 step: 5833, epoch: 176, batch: 24, loss: 0.15671157836914062, acc: 93.75, f1: 91.94931117100928, r: 0.6296829971783947
06/02/2019 11:00:03 step: 5838, epoch: 176, batch: 29, loss: 0.21169045567512512, acc: 90.625, f1: 89.2524759191426, r: 0.6682926988162025
06/02/2019 11:00:03 *** evaluating ***
06/02/2019 11:00:04 step: 177, epoch: 176, acc: 58.97435897435898, f1: 27.90252596901058, r: 0.34372232345174153
06/02/2019 11:00:04 *** epoch: 178 ***
06/02/2019 11:00:04 *** training ***
06/02/2019 11:00:04 step: 5846, epoch: 177, batch: 4, loss: 0.273821085691452, acc: 93.75, f1: 94.80137425503558, r: 0.7815452270822516
06/02/2019 11:00:04 step: 5851, epoch: 177, batch: 9, loss: 0.11514227092266083, acc: 95.3125, f1: 89.31972789115646, r: 0.6717270660167571
06/02/2019 11:00:05 step: 5856, epoch: 177, batch: 14, loss: 0.1711909919977188, acc: 96.875, f1: 96.8055086635572, r: 0.6803585171552655
06/02/2019 11:00:05 step: 5861, epoch: 177, batch: 19, loss: 0.16254281997680664, acc: 96.875, f1: 95.50960735171262, r: 0.7739793297639167
06/02/2019 11:00:06 step: 5866, epoch: 177, batch: 24, loss: 0.19994157552719116, acc: 92.1875, f1: 93.04594861660078, r: 0.7757558033057372
06/02/2019 11:00:06 step: 5871, epoch: 177, batch: 29, loss: 0.12283976376056671, acc: 95.3125, f1: 92.34396555825127, r: 0.7000153622412822
06/02/2019 11:00:06 *** evaluating ***
06/02/2019 11:00:06 step: 178, epoch: 177, acc: 60.256410256410255, f1: 29.43080060709543, r: 0.34495551840604605
06/02/2019 11:00:06 *** epoch: 179 ***
06/02/2019 11:00:06 *** training ***
06/02/2019 11:00:07 step: 5879, epoch: 178, batch: 4, loss: 0.21546491980552673, acc: 92.1875, f1: 89.80990274093722, r: 0.6002273389154105
06/02/2019 11:00:07 step: 5884, epoch: 178, batch: 9, loss: 0.21277910470962524, acc: 93.75, f1: 88.57387057387058, r: 0.5935629835484516
06/02/2019 11:00:07 step: 5889, epoch: 178, batch: 14, loss: 0.06129194796085358, acc: 98.4375, f1: 99.2158439730572, r: 0.7491679858217146
06/02/2019 11:00:08 step: 5894, epoch: 178, batch: 19, loss: 0.183601975440979, acc: 93.75, f1: 94.5843228595786, r: 0.7777451123207385
06/02/2019 11:00:08 step: 5899, epoch: 178, batch: 24, loss: 0.11551928520202637, acc: 95.3125, f1: 96.18751424014582, r: 0.6711267783115588
06/02/2019 11:00:09 step: 5904, epoch: 178, batch: 29, loss: 0.161956325173378, acc: 93.75, f1: 92.73809523809524, r: 0.7161141371764359
06/02/2019 11:00:09 *** evaluating ***
06/02/2019 11:00:09 step: 179, epoch: 178, acc: 58.54700854700855, f1: 27.432023050692365, r: 0.34091811966596547
06/02/2019 11:00:09 *** epoch: 180 ***
06/02/2019 11:00:09 *** training ***
06/02/2019 11:00:09 step: 5912, epoch: 179, batch: 4, loss: 0.11583755910396576, acc: 93.75, f1: 93.29424606813201, r: 0.7045850246908661
06/02/2019 11:00:10 step: 5917, epoch: 179, batch: 9, loss: 0.058474745601415634, acc: 100.0, f1: 100.0, r: 0.8005024927720281
06/02/2019 11:00:10 step: 5922, epoch: 179, batch: 14, loss: 0.07480478286743164, acc: 96.875, f1: 94.88095238095238, r: 0.7066158387136874
06/02/2019 11:00:10 step: 5927, epoch: 179, batch: 19, loss: 0.27370986342430115, acc: 93.75, f1: 95.37058371735792, r: 0.7518838640763825
06/02/2019 11:00:11 step: 5932, epoch: 179, batch: 24, loss: 0.09593559801578522, acc: 98.4375, f1: 99.2372234935164, r: 0.6845190007205506
06/02/2019 11:00:11 step: 5937, epoch: 179, batch: 29, loss: 0.08665783703327179, acc: 98.4375, f1: 97.86096256684492, r: 0.6659945037614721
06/02/2019 11:00:11 *** evaluating ***
06/02/2019 11:00:12 step: 180, epoch: 179, acc: 56.41025641025641, f1: 26.89508673453627, r: 0.3424324582898537
06/02/2019 11:00:12 *** epoch: 181 ***
06/02/2019 11:00:12 *** training ***
06/02/2019 11:00:12 step: 5945, epoch: 180, batch: 4, loss: 0.08911390602588654, acc: 96.875, f1: 97.21818670971213, r: 0.7523864691672122
06/02/2019 11:00:12 step: 5950, epoch: 180, batch: 9, loss: 0.17117822170257568, acc: 95.3125, f1: 93.7481884057971, r: 0.7331684794828625
06/02/2019 11:00:13 step: 5955, epoch: 180, batch: 14, loss: 0.22509755194187164, acc: 90.625, f1: 82.71707459207458, r: 0.7294054001988555
06/02/2019 11:00:13 step: 5960, epoch: 180, batch: 19, loss: 0.24545425176620483, acc: 90.625, f1: 91.58665394469948, r: 0.7763649845224718
06/02/2019 11:00:14 step: 5965, epoch: 180, batch: 24, loss: 0.1513964980840683, acc: 95.3125, f1: 97.17687074829932, r: 0.7310307733658301
06/02/2019 11:00:14 step: 5970, epoch: 180, batch: 29, loss: 0.1244766041636467, acc: 95.3125, f1: 92.63392857142857, r: 0.6923704087275409
06/02/2019 11:00:14 *** evaluating ***
06/02/2019 11:00:15 step: 181, epoch: 180, acc: 58.97435897435898, f1: 28.726445958153274, r: 0.34156258741581624
06/02/2019 11:00:15 *** epoch: 182 ***
06/02/2019 11:00:15 *** training ***
06/02/2019 11:00:15 step: 5978, epoch: 181, batch: 4, loss: 0.14650489389896393, acc: 95.3125, f1: 73.01211349924586, r: 0.6197593088039071
06/02/2019 11:00:15 step: 5983, epoch: 181, batch: 9, loss: 0.307239830493927, acc: 87.5, f1: 69.09157169361251, r: 0.5460730794803381
06/02/2019 11:00:16 step: 5988, epoch: 181, batch: 14, loss: 0.14492447674274445, acc: 95.3125, f1: 88.70614035087718, r: 0.6761848417953977
06/02/2019 11:00:16 step: 5993, epoch: 181, batch: 19, loss: 0.08358028531074524, acc: 98.4375, f1: 97.47474747474747, r: 0.8090672160108939
06/02/2019 11:00:17 step: 5998, epoch: 181, batch: 24, loss: 0.11819832772016525, acc: 93.75, f1: 93.77392344497608, r: 0.7285624910302417
06/02/2019 11:00:17 step: 6003, epoch: 181, batch: 29, loss: 0.08339143544435501, acc: 98.4375, f1: 96.90866510538642, r: 0.7015793522078956
06/02/2019 11:00:17 *** evaluating ***
06/02/2019 11:00:18 step: 182, epoch: 181, acc: 58.119658119658126, f1: 28.223539953615855, r: 0.3375805013246393
06/02/2019 11:00:18 *** epoch: 183 ***
06/02/2019 11:00:18 *** training ***
06/02/2019 11:00:18 step: 6011, epoch: 182, batch: 4, loss: 0.10214568674564362, acc: 98.4375, f1: 98.35600907029477, r: 0.7603247711960431
06/02/2019 11:00:18 step: 6016, epoch: 182, batch: 9, loss: 0.28378015756607056, acc: 89.0625, f1: 78.7797619047619, r: 0.6776285942580913
06/02/2019 11:00:19 step: 6021, epoch: 182, batch: 14, loss: 0.018479034304618835, acc: 100.0, f1: 100.0, r: 0.7891620391491442
06/02/2019 11:00:19 step: 6026, epoch: 182, batch: 19, loss: 0.174455463886261, acc: 90.625, f1: 93.92107720281095, r: 0.6944179771239791
06/02/2019 11:00:20 step: 6031, epoch: 182, batch: 24, loss: 0.09923814237117767, acc: 100.0, f1: 100.0, r: 0.7204046263355183
06/02/2019 11:00:20 step: 6036, epoch: 182, batch: 29, loss: 0.19785770773887634, acc: 93.75, f1: 94.5959595959596, r: 0.7421960588831853
06/02/2019 11:00:20 *** evaluating ***
06/02/2019 11:00:20 step: 183, epoch: 182, acc: 57.692307692307686, f1: 29.05907364837813, r: 0.3411796994324412
06/02/2019 11:00:20 *** epoch: 184 ***
06/02/2019 11:00:20 *** training ***
06/02/2019 11:00:21 step: 6044, epoch: 183, batch: 4, loss: 0.09605112671852112, acc: 96.875, f1: 83.98268398268398, r: 0.6321007670387508
06/02/2019 11:00:21 step: 6049, epoch: 183, batch: 9, loss: 0.14681825041770935, acc: 96.875, f1: 96.28746360204161, r: 0.7457206320931771
06/02/2019 11:00:22 step: 6054, epoch: 183, batch: 14, loss: 0.2017168253660202, acc: 95.3125, f1: 95.15131562946941, r: 0.6575871035989954
06/02/2019 11:00:22 step: 6059, epoch: 183, batch: 19, loss: 0.07586851716041565, acc: 98.4375, f1: 99.16216216216216, r: 0.7609120228677867
06/02/2019 11:00:22 step: 6064, epoch: 183, batch: 24, loss: 0.13910232484340668, acc: 96.875, f1: 89.65367965367965, r: 0.691201001921766
06/02/2019 11:00:23 step: 6069, epoch: 183, batch: 29, loss: 0.13191059231758118, acc: 95.3125, f1: 92.19535324798484, r: 0.709328024013659
06/02/2019 11:00:23 *** evaluating ***
06/02/2019 11:00:23 step: 184, epoch: 183, acc: 58.54700854700855, f1: 26.148294234192942, r: 0.34401309110951983
06/02/2019 11:00:23 *** epoch: 185 ***
06/02/2019 11:00:23 *** training ***
06/02/2019 11:00:23 step: 6077, epoch: 184, batch: 4, loss: 0.16341513395309448, acc: 92.1875, f1: 79.84572165423229, r: 0.6073753759170594
06/02/2019 11:00:24 step: 6082, epoch: 184, batch: 9, loss: 0.1965712308883667, acc: 90.625, f1: 78.98636689397559, r: 0.7975357638852577
06/02/2019 11:00:24 step: 6087, epoch: 184, batch: 14, loss: 0.2182776927947998, acc: 92.1875, f1: 94.45033037138299, r: 0.6967276993516136
06/02/2019 11:00:24 step: 6092, epoch: 184, batch: 19, loss: 0.14522475004196167, acc: 95.3125, f1: 93.69416764374748, r: 0.7030951950119876
06/02/2019 11:00:25 step: 6097, epoch: 184, batch: 24, loss: 0.12435685098171234, acc: 95.3125, f1: 92.994708994709, r: 0.7142496816395297
06/02/2019 11:00:25 step: 6102, epoch: 184, batch: 29, loss: 0.11728234589099884, acc: 96.875, f1: 93.78191856452727, r: 0.7860883748943335
06/02/2019 11:00:25 *** evaluating ***
06/02/2019 11:00:26 step: 185, epoch: 184, acc: 59.82905982905983, f1: 31.340273761326397, r: 0.3387417956725493
06/02/2019 11:00:26 *** epoch: 186 ***
06/02/2019 11:00:26 *** training ***
06/02/2019 11:00:26 step: 6110, epoch: 185, batch: 4, loss: 0.22074122726917267, acc: 95.3125, f1: 94.05193571860238, r: 0.6753467345138245
06/02/2019 11:00:26 step: 6115, epoch: 185, batch: 9, loss: 0.06517855823040009, acc: 96.875, f1: 96.44742354419775, r: 0.7711046264534323
06/02/2019 11:00:27 step: 6120, epoch: 185, batch: 14, loss: 0.18597644567489624, acc: 96.875, f1: 96.23563218390805, r: 0.7635517605455561
06/02/2019 11:00:27 step: 6125, epoch: 185, batch: 19, loss: 0.04864970222115517, acc: 98.4375, f1: 98.31519831519832, r: 0.697316467428415
06/02/2019 11:00:27 step: 6130, epoch: 185, batch: 24, loss: 0.10000164806842804, acc: 96.875, f1: 96.49007077578506, r: 0.6615606863985737
06/02/2019 11:00:28 step: 6135, epoch: 185, batch: 29, loss: 0.2803695797920227, acc: 93.75, f1: 89.6023918429579, r: 0.6038888487807025
06/02/2019 11:00:28 *** evaluating ***
06/02/2019 11:00:28 step: 186, epoch: 185, acc: 59.82905982905983, f1: 30.298788813606897, r: 0.3356304645589339
06/02/2019 11:00:28 *** epoch: 187 ***
06/02/2019 11:00:28 *** training ***
06/02/2019 11:00:29 step: 6143, epoch: 186, batch: 4, loss: 0.10079382359981537, acc: 95.3125, f1: 94.9968671679198, r: 0.7419913316769317
06/02/2019 11:00:29 step: 6148, epoch: 186, batch: 9, loss: 0.34050771594047546, acc: 89.0625, f1: 89.08143939393939, r: 0.7187664106478145
06/02/2019 11:00:30 step: 6153, epoch: 186, batch: 14, loss: 0.2538333833217621, acc: 92.1875, f1: 88.77526697177727, r: 0.778901019783121
06/02/2019 11:00:30 step: 6158, epoch: 186, batch: 19, loss: 0.1838243305683136, acc: 95.3125, f1: 88.12925170068027, r: 0.6311898378312805
06/02/2019 11:00:30 step: 6163, epoch: 186, batch: 24, loss: 0.16577839851379395, acc: 93.75, f1: 92.06180724037866, r: 0.6855588175294305
06/02/2019 11:00:31 step: 6168, epoch: 186, batch: 29, loss: 0.23871660232543945, acc: 92.1875, f1: 91.46331198160466, r: 0.7167378004519481
06/02/2019 11:00:31 *** evaluating ***
06/02/2019 11:00:31 step: 187, epoch: 186, acc: 58.119658119658126, f1: 29.760272956483313, r: 0.3296703519979075
06/02/2019 11:00:31 *** epoch: 188 ***
06/02/2019 11:00:31 *** training ***
06/02/2019 11:00:32 step: 6176, epoch: 187, batch: 4, loss: 0.10842985659837723, acc: 98.4375, f1: 99.22027290448344, r: 0.741808379970831
06/02/2019 11:00:32 step: 6181, epoch: 187, batch: 9, loss: 0.11375173926353455, acc: 95.3125, f1: 95.9857817000674, r: 0.6371455693916809
06/02/2019 11:00:32 step: 6186, epoch: 187, batch: 14, loss: 0.23291093111038208, acc: 87.5, f1: 80.71211816114783, r: 0.6975437790348719
06/02/2019 11:00:33 step: 6191, epoch: 187, batch: 19, loss: 0.09557361155748367, acc: 98.4375, f1: 99.05998763141622, r: 0.6124598281811375
06/02/2019 11:00:33 step: 6196, epoch: 187, batch: 24, loss: 0.1610807180404663, acc: 93.75, f1: 93.4426136201941, r: 0.6962164364953809
06/02/2019 11:00:34 step: 6201, epoch: 187, batch: 29, loss: 0.11192905157804489, acc: 95.3125, f1: 81.90702450570872, r: 0.6319083004209869
06/02/2019 11:00:34 *** evaluating ***
06/02/2019 11:00:34 step: 188, epoch: 187, acc: 58.119658119658126, f1: 27.978757239726814, r: 0.32713694693971584
06/02/2019 11:00:34 *** epoch: 189 ***
06/02/2019 11:00:34 *** training ***
06/02/2019 11:00:35 step: 6209, epoch: 188, batch: 4, loss: 0.28698891401290894, acc: 90.625, f1: 90.78302675585283, r: 0.7524813574372714
06/02/2019 11:00:35 step: 6214, epoch: 188, batch: 9, loss: 0.08037012070417404, acc: 96.875, f1: 96.64366032290562, r: 0.7904745828026389
06/02/2019 11:00:35 step: 6219, epoch: 188, batch: 14, loss: 0.13686855137348175, acc: 92.1875, f1: 87.84904437965663, r: 0.678447284201874
06/02/2019 11:00:36 step: 6224, epoch: 188, batch: 19, loss: 0.14870436489582062, acc: 95.3125, f1: 94.34694434694435, r: 0.6497286208490497
06/02/2019 11:00:36 step: 6229, epoch: 188, batch: 24, loss: 0.41027915477752686, acc: 87.5, f1: 81.96055796055796, r: 0.6030223083699399
06/02/2019 11:00:37 step: 6234, epoch: 188, batch: 29, loss: 0.2303800880908966, acc: 89.0625, f1: 84.35200746965452, r: 0.7486672296294159
06/02/2019 11:00:37 *** evaluating ***
06/02/2019 11:00:37 step: 189, epoch: 188, acc: 59.401709401709404, f1: 29.224481074481073, r: 0.3421591406075
06/02/2019 11:00:37 *** epoch: 190 ***
06/02/2019 11:00:37 *** training ***
06/02/2019 11:00:37 step: 6242, epoch: 189, batch: 4, loss: 0.06313003599643707, acc: 98.4375, f1: 97.46031746031747, r: 0.7046061587890391
06/02/2019 11:00:38 step: 6247, epoch: 189, batch: 9, loss: 0.2793594002723694, acc: 92.1875, f1: 90.35745207173778, r: 0.6893306923084174
06/02/2019 11:00:38 step: 6252, epoch: 189, batch: 14, loss: 0.04783698171377182, acc: 100.0, f1: 100.0, r: 0.6904023371784769
06/02/2019 11:00:39 step: 6257, epoch: 189, batch: 19, loss: 0.09689035266637802, acc: 96.875, f1: 96.84814364617321, r: 0.6903113541412976
06/02/2019 11:00:39 step: 6262, epoch: 189, batch: 24, loss: 0.11597239971160889, acc: 95.3125, f1: 82.44493873784491, r: 0.6509727504555824
06/02/2019 11:00:39 step: 6267, epoch: 189, batch: 29, loss: 0.13883796334266663, acc: 96.875, f1: 98.24045839071415, r: 0.7136699280387104
06/02/2019 11:00:40 *** evaluating ***
06/02/2019 11:00:40 step: 190, epoch: 189, acc: 58.54700854700855, f1: 27.454761834046582, r: 0.3395346251196248
06/02/2019 11:00:40 *** epoch: 191 ***
06/02/2019 11:00:40 *** training ***
06/02/2019 11:00:40 step: 6275, epoch: 190, batch: 4, loss: 0.2588980495929718, acc: 90.625, f1: 89.5545634920635, r: 0.6922485550974319
06/02/2019 11:00:40 step: 6280, epoch: 190, batch: 9, loss: 0.06588974595069885, acc: 98.4375, f1: 99.29118773946361, r: 0.7819410597558987
06/02/2019 11:00:41 step: 6285, epoch: 190, batch: 14, loss: 0.20784175395965576, acc: 92.1875, f1: 87.98678861788618, r: 0.7292650315950937
06/02/2019 11:00:41 step: 6290, epoch: 190, batch: 19, loss: 0.1611180454492569, acc: 95.3125, f1: 90.51689051689053, r: 0.6166070541622147
06/02/2019 11:00:42 step: 6295, epoch: 190, batch: 24, loss: 0.11081908643245697, acc: 95.3125, f1: 83.05738304093569, r: 0.7032994471471956
06/02/2019 11:00:42 step: 6300, epoch: 190, batch: 29, loss: 0.18428358435630798, acc: 95.3125, f1: 95.12921648383254, r: 0.6957101998860514
06/02/2019 11:00:42 *** evaluating ***
06/02/2019 11:00:43 step: 191, epoch: 190, acc: 60.256410256410255, f1: 28.92200730622148, r: 0.3384361570043194
06/02/2019 11:00:43 *** epoch: 192 ***
06/02/2019 11:00:43 *** training ***
06/02/2019 11:00:43 step: 6308, epoch: 191, batch: 4, loss: 0.23514437675476074, acc: 96.875, f1: 95.75958651402063, r: 0.5822434049585834
06/02/2019 11:00:43 step: 6313, epoch: 191, batch: 9, loss: 0.08068707585334778, acc: 96.875, f1: 93.15476190476191, r: 0.7581618058504731
06/02/2019 11:00:44 step: 6318, epoch: 191, batch: 14, loss: 0.21617181599140167, acc: 92.1875, f1: 68.76344086021506, r: 0.6585882648450775
06/02/2019 11:00:44 step: 6323, epoch: 191, batch: 19, loss: 0.2185579538345337, acc: 90.625, f1: 87.52480158730158, r: 0.6987949837279487
06/02/2019 11:00:45 step: 6328, epoch: 191, batch: 24, loss: 0.07475665211677551, acc: 98.4375, f1: 99.13880445795338, r: 0.7643918846016858
06/02/2019 11:00:45 step: 6333, epoch: 191, batch: 29, loss: 0.10857050865888596, acc: 96.875, f1: 96.92212549355406, r: 0.7251349317961509
06/02/2019 11:00:45 *** evaluating ***
06/02/2019 11:00:46 step: 192, epoch: 191, acc: 57.26495726495726, f1: 28.610147870442155, r: 0.3362553507526133
06/02/2019 11:00:46 *** epoch: 193 ***
06/02/2019 11:00:46 *** training ***
06/02/2019 11:00:46 step: 6341, epoch: 192, batch: 4, loss: 0.22546648979187012, acc: 92.1875, f1: 88.23596014492753, r: 0.6577478975249839
06/02/2019 11:00:46 step: 6346, epoch: 192, batch: 9, loss: 0.13280600309371948, acc: 98.4375, f1: 85.71428571428572, r: 0.6388319830127643
06/02/2019 11:00:47 step: 6351, epoch: 192, batch: 14, loss: 0.12961941957473755, acc: 96.875, f1: 95.46007876834945, r: 0.6603902954031599
06/02/2019 11:00:47 step: 6356, epoch: 192, batch: 19, loss: 0.17047423124313354, acc: 93.75, f1: 93.16964285714285, r: 0.7491399114420292
06/02/2019 11:00:48 step: 6361, epoch: 192, batch: 24, loss: 0.13485455513000488, acc: 93.75, f1: 95.53410553410554, r: 0.7305224720656384
06/02/2019 11:00:48 step: 6366, epoch: 192, batch: 29, loss: 0.08938082307577133, acc: 98.4375, f1: 95.51282051282051, r: 0.7357147820440645
06/02/2019 11:00:48 *** evaluating ***
06/02/2019 11:00:49 step: 193, epoch: 192, acc: 58.54700854700855, f1: 28.44306643828848, r: 0.3344641988011443
06/02/2019 11:00:49 *** epoch: 194 ***
06/02/2019 11:00:49 *** training ***
06/02/2019 11:00:49 step: 6374, epoch: 193, batch: 4, loss: 0.21196600794792175, acc: 95.3125, f1: 96.35077954626826, r: 0.6165400928386332
06/02/2019 11:00:49 step: 6379, epoch: 193, batch: 9, loss: 0.1943366825580597, acc: 95.3125, f1: 73.80952380952381, r: 0.5969592227638028
06/02/2019 11:00:50 step: 6384, epoch: 193, batch: 14, loss: 0.11809565126895905, acc: 96.875, f1: 94.40940012368584, r: 0.6375508554657792
06/02/2019 11:00:50 step: 6389, epoch: 193, batch: 19, loss: 0.0747777447104454, acc: 96.875, f1: 84.90940766550523, r: 0.72117792574632
06/02/2019 11:00:51 step: 6394, epoch: 193, batch: 24, loss: 0.2881854176521301, acc: 93.75, f1: 94.6358543417367, r: 0.6864373589749508
06/02/2019 11:00:51 step: 6399, epoch: 193, batch: 29, loss: 0.1339225023984909, acc: 92.1875, f1: 77.79516091144, r: 0.8081048160657085
06/02/2019 11:00:51 *** evaluating ***
06/02/2019 11:00:52 step: 194, epoch: 193, acc: 58.119658119658126, f1: 29.111010706450656, r: 0.3374509791071884
06/02/2019 11:00:52 *** epoch: 195 ***
06/02/2019 11:00:52 *** training ***
06/02/2019 11:00:52 step: 6407, epoch: 194, batch: 4, loss: 0.04943830892443657, acc: 100.0, f1: 100.0, r: 0.7046659467421621
06/02/2019 11:00:52 step: 6412, epoch: 194, batch: 9, loss: 0.12290260195732117, acc: 95.3125, f1: 95.72496947496947, r: 0.7758099096568025
06/02/2019 11:00:53 step: 6417, epoch: 194, batch: 14, loss: 0.10005181282758713, acc: 95.3125, f1: 96.3022113022113, r: 0.6957358868056647
06/02/2019 11:00:53 step: 6422, epoch: 194, batch: 19, loss: 0.15301594138145447, acc: 92.1875, f1: 93.80333951762523, r: 0.743322444835093
06/02/2019 11:00:53 step: 6427, epoch: 194, batch: 24, loss: 0.1271565556526184, acc: 95.3125, f1: 92.6365469208211, r: 0.7186915799151733
06/02/2019 11:00:54 step: 6432, epoch: 194, batch: 29, loss: 0.11075453460216522, acc: 93.75, f1: 93.98062148836142, r: 0.6000556514159624
06/02/2019 11:00:54 *** evaluating ***
06/02/2019 11:00:54 step: 195, epoch: 194, acc: 58.119658119658126, f1: 29.384462908656456, r: 0.33323446554433855
06/02/2019 11:00:54 *** epoch: 196 ***
06/02/2019 11:00:54 *** training ***
06/02/2019 11:00:55 step: 6440, epoch: 195, batch: 4, loss: 0.2181311398744583, acc: 95.3125, f1: 96.70948839305838, r: 0.756045721828798
06/02/2019 11:00:55 step: 6445, epoch: 195, batch: 9, loss: 0.12078326940536499, acc: 96.875, f1: 95.08483853311441, r: 0.7330625702897551
06/02/2019 11:00:56 step: 6450, epoch: 195, batch: 14, loss: 0.0673459842801094, acc: 96.875, f1: 92.44047619047619, r: 0.7649879853212639
06/02/2019 11:00:56 step: 6455, epoch: 195, batch: 19, loss: 0.07104457169771194, acc: 98.4375, f1: 99.26962872793669, r: 0.6544602559260341
06/02/2019 11:00:56 step: 6460, epoch: 195, batch: 24, loss: 0.1531822383403778, acc: 95.3125, f1: 94.58879687140556, r: 0.7638937291955703
06/02/2019 11:00:57 step: 6465, epoch: 195, batch: 29, loss: 0.08950620889663696, acc: 96.875, f1: 97.47259118701244, r: 0.7810245130587365
06/02/2019 11:00:57 *** evaluating ***
06/02/2019 11:00:57 step: 196, epoch: 195, acc: 59.82905982905983, f1: 29.158987953933234, r: 0.3437617910992101
06/02/2019 11:00:57 *** epoch: 197 ***
06/02/2019 11:00:57 *** training ***
06/02/2019 11:00:58 step: 6473, epoch: 196, batch: 4, loss: 0.2481919676065445, acc: 92.1875, f1: 90.93294806849795, r: 0.7492683193526357
06/02/2019 11:00:58 step: 6478, epoch: 196, batch: 9, loss: 0.09567722678184509, acc: 98.4375, f1: 97.74891774891773, r: 0.6313917718096149
06/02/2019 11:00:58 step: 6483, epoch: 196, batch: 14, loss: 0.15202119946479797, acc: 96.875, f1: 96.62698412698413, r: 0.7924397172624237
06/02/2019 11:00:59 step: 6488, epoch: 196, batch: 19, loss: 0.23126836121082306, acc: 90.625, f1: 90.40959040959041, r: 0.622106027271139
06/02/2019 11:00:59 step: 6493, epoch: 196, batch: 24, loss: 0.18895527720451355, acc: 93.75, f1: 91.87094128954595, r: 0.6384337970652864
06/02/2019 11:01:00 step: 6498, epoch: 196, batch: 29, loss: 0.06363330036401749, acc: 100.0, f1: 100.0, r: 0.7743282299176162
06/02/2019 11:01:00 *** evaluating ***
06/02/2019 11:01:00 step: 197, epoch: 196, acc: 58.97435897435898, f1: 31.2129324405705, r: 0.33954573915626396
06/02/2019 11:01:00 *** epoch: 198 ***
06/02/2019 11:01:00 *** training ***
06/02/2019 11:01:00 step: 6506, epoch: 197, batch: 4, loss: 0.10603568702936172, acc: 98.4375, f1: 99.28698752228165, r: 0.6992836741553665
06/02/2019 11:01:01 step: 6511, epoch: 197, batch: 9, loss: 0.1928761601448059, acc: 96.875, f1: 95.75345289631004, r: 0.6122143016827182
06/02/2019 11:01:01 step: 6516, epoch: 197, batch: 14, loss: 0.2634918689727783, acc: 90.625, f1: 88.70419135396094, r: 0.7118520408380297
06/02/2019 11:01:02 step: 6521, epoch: 197, batch: 19, loss: 0.09686920046806335, acc: 98.4375, f1: 99.09937888198758, r: 0.7662982387492407
06/02/2019 11:01:02 step: 6526, epoch: 197, batch: 24, loss: 0.08021828532218933, acc: 96.875, f1: 95.403007255879, r: 0.8096736071855788
06/02/2019 11:01:03 step: 6531, epoch: 197, batch: 29, loss: 0.13418787717819214, acc: 95.3125, f1: 96.73945335710042, r: 0.7394443978810482
06/02/2019 11:01:03 *** evaluating ***
06/02/2019 11:01:03 step: 198, epoch: 197, acc: 58.54700854700855, f1: 27.87144206928608, r: 0.3369890049535303
06/02/2019 11:01:03 *** epoch: 199 ***
06/02/2019 11:01:03 *** training ***
06/02/2019 11:01:03 step: 6539, epoch: 198, batch: 4, loss: 0.10147318243980408, acc: 96.875, f1: 90.5952380952381, r: 0.6382257507857034
06/02/2019 11:01:04 step: 6544, epoch: 198, batch: 9, loss: 0.12965725362300873, acc: 96.875, f1: 96.85887497515404, r: 0.7695509008560858
06/02/2019 11:01:04 step: 6549, epoch: 198, batch: 14, loss: 0.29302889108657837, acc: 96.875, f1: 97.83036492713913, r: 0.7227505086680466
06/02/2019 11:01:05 step: 6554, epoch: 198, batch: 19, loss: 0.17424534261226654, acc: 95.3125, f1: 91.1289233642175, r: 0.6315844432312607
06/02/2019 11:01:05 step: 6559, epoch: 198, batch: 24, loss: 0.07352986931800842, acc: 96.875, f1: 95.52772868058133, r: 0.7529893959789904
06/02/2019 11:01:05 step: 6564, epoch: 198, batch: 29, loss: 0.18192729353904724, acc: 90.625, f1: 85.64935064935064, r: 0.7399010025113949
06/02/2019 11:01:06 *** evaluating ***
06/02/2019 11:01:06 step: 199, epoch: 198, acc: 58.119658119658126, f1: 28.816678043430066, r: 0.3374019343726026
06/02/2019 11:01:06 *** epoch: 200 ***
06/02/2019 11:01:06 *** training ***
06/02/2019 11:01:06 step: 6572, epoch: 199, batch: 4, loss: 0.23684385418891907, acc: 92.1875, f1: 91.1783455732354, r: 0.6840840944717302
06/02/2019 11:01:07 step: 6577, epoch: 199, batch: 9, loss: 0.08714108169078827, acc: 96.875, f1: 93.54620317026333, r: 0.644352693197035
06/02/2019 11:01:07 step: 6582, epoch: 199, batch: 14, loss: 0.0547829233109951, acc: 96.875, f1: 95.31053459119497, r: 0.5571791232877557
06/02/2019 11:01:07 step: 6587, epoch: 199, batch: 19, loss: 0.1671486794948578, acc: 95.3125, f1: 94.51173423298859, r: 0.7285802550291219
06/02/2019 11:01:08 step: 6592, epoch: 199, batch: 24, loss: 0.08750272542238235, acc: 96.875, f1: 93.19727891156462, r: 0.8110991933152807
06/02/2019 11:01:08 step: 6597, epoch: 199, batch: 29, loss: 0.16923391819000244, acc: 96.875, f1: 94.28848149429263, r: 0.6544190818241055
06/02/2019 11:01:08 *** evaluating ***
06/02/2019 11:01:09 step: 200, epoch: 199, acc: 59.401709401709404, f1: 28.762274774774777, r: 0.34086391223994644
06/02/2019 11:01:09 *** epoch: 201 ***
06/02/2019 11:01:09 *** training ***
06/02/2019 11:01:09 step: 6605, epoch: 200, batch: 4, loss: 0.0372617244720459, acc: 98.4375, f1: 97.35449735449735, r: 0.6446060683322465
06/02/2019 11:01:09 step: 6610, epoch: 200, batch: 9, loss: 0.183406800031662, acc: 93.75, f1: 93.83387689308744, r: 0.5565929925561359
06/02/2019 11:01:10 step: 6615, epoch: 200, batch: 14, loss: 0.12813366949558258, acc: 96.875, f1: 93.43137254901961, r: 0.7437163265337465
06/02/2019 11:01:10 step: 6620, epoch: 200, batch: 19, loss: 0.07256491482257843, acc: 96.875, f1: 94.39102564102564, r: 0.8347462150668455
06/02/2019 11:01:11 step: 6625, epoch: 200, batch: 24, loss: 0.10578933358192444, acc: 93.75, f1: 91.00738820566407, r: 0.7399314532823823
06/02/2019 11:01:11 step: 6630, epoch: 200, batch: 29, loss: 0.19303853809833527, acc: 92.1875, f1: 89.26228691766983, r: 0.6943630408080479
06/02/2019 11:01:11 *** evaluating ***
06/02/2019 11:01:11 step: 201, epoch: 200, acc: 58.119658119658126, f1: 28.309706612384133, r: 0.33608102012120294
06/02/2019 11:01:11 *** epoch: 202 ***
06/02/2019 11:01:11 *** training ***
06/02/2019 11:01:12 step: 6638, epoch: 201, batch: 4, loss: 0.29284071922302246, acc: 92.1875, f1: 82.42804814233386, r: 0.590211666736904
06/02/2019 11:01:12 step: 6643, epoch: 201, batch: 9, loss: 0.15915335714817047, acc: 95.3125, f1: 93.86243386243387, r: 0.8125014117627581
06/02/2019 11:01:12 step: 6648, epoch: 201, batch: 14, loss: 0.10586239397525787, acc: 96.875, f1: 80.0, r: 0.6197475144523767
06/02/2019 11:01:13 step: 6653, epoch: 201, batch: 19, loss: 0.09540072083473206, acc: 95.3125, f1: 93.91753311494088, r: 0.6575561671928655
06/02/2019 11:01:13 step: 6658, epoch: 201, batch: 24, loss: 0.14106597006320953, acc: 93.75, f1: 91.48860837438424, r: 0.7449104553122076
06/02/2019 11:01:14 step: 6663, epoch: 201, batch: 29, loss: 0.11506472527980804, acc: 96.875, f1: 98.06458953988746, r: 0.7187032885674169
06/02/2019 11:01:14 *** evaluating ***
06/02/2019 11:01:14 step: 202, epoch: 201, acc: 59.82905982905983, f1: 27.84493264178608, r: 0.33567343900355
06/02/2019 11:01:14 *** epoch: 203 ***
06/02/2019 11:01:14 *** training ***
06/02/2019 11:01:14 step: 6671, epoch: 202, batch: 4, loss: 0.2552989423274994, acc: 89.0625, f1: 90.10893868036725, r: 0.6274288062245779
06/02/2019 11:01:15 step: 6676, epoch: 202, batch: 9, loss: 0.0387411005795002, acc: 98.4375, f1: 97.60765550239235, r: 0.6453975288264164
06/02/2019 11:01:15 step: 6681, epoch: 202, batch: 14, loss: 0.0847560241818428, acc: 96.875, f1: 95.15621473052461, r: 0.7133860743908483
06/02/2019 11:01:15 step: 6686, epoch: 202, batch: 19, loss: 0.04159038886427879, acc: 98.4375, f1: 99.13675123697232, r: 0.7218772321916971
06/02/2019 11:01:16 step: 6691, epoch: 202, batch: 24, loss: 0.173043891787529, acc: 92.1875, f1: 93.12395937395937, r: 0.7572882259712219
06/02/2019 11:01:16 step: 6696, epoch: 202, batch: 29, loss: 0.08054216206073761, acc: 96.875, f1: 98.11063218390805, r: 0.7316038603029253
06/02/2019 11:01:16 *** evaluating ***
06/02/2019 11:01:16 step: 203, epoch: 202, acc: 58.54700854700855, f1: 28.499750910465195, r: 0.33847027508992183
06/02/2019 11:01:16 *** epoch: 204 ***
06/02/2019 11:01:16 *** training ***
06/02/2019 11:01:17 step: 6704, epoch: 203, batch: 4, loss: 0.21092408895492554, acc: 92.1875, f1: 79.49262611027316, r: 0.7208117446111653
06/02/2019 11:01:17 step: 6709, epoch: 203, batch: 9, loss: 0.09897466748952866, acc: 96.875, f1: 96.69984387197502, r: 0.7625359872436516
06/02/2019 11:01:18 step: 6714, epoch: 203, batch: 14, loss: 0.16932493448257446, acc: 93.75, f1: 94.0422077922078, r: 0.7664198459133404
06/02/2019 11:01:18 step: 6719, epoch: 203, batch: 19, loss: 0.18450427055358887, acc: 96.875, f1: 93.92987964416535, r: 0.6592331773005609
06/02/2019 11:01:18 step: 6724, epoch: 203, batch: 24, loss: 0.05388699844479561, acc: 96.875, f1: 96.9047619047619, r: 0.7267025421784689
06/02/2019 11:01:19 step: 6729, epoch: 203, batch: 29, loss: 0.1449456363916397, acc: 95.3125, f1: 88.3813598034495, r: 0.6269627411212679
06/02/2019 11:01:19 *** evaluating ***
06/02/2019 11:01:19 step: 204, epoch: 203, acc: 59.82905982905983, f1: 28.529252353071254, r: 0.33728944062554533
06/02/2019 11:01:19 *** epoch: 205 ***
06/02/2019 11:01:19 *** training ***
06/02/2019 11:01:20 step: 6737, epoch: 204, batch: 4, loss: 0.22258155047893524, acc: 92.1875, f1: 84.4013120122191, r: 0.6086550897581566
06/02/2019 11:01:20 step: 6742, epoch: 204, batch: 9, loss: 0.17352822422981262, acc: 95.3125, f1: 96.19594964422551, r: 0.7536791140862096
06/02/2019 11:01:21 step: 6747, epoch: 204, batch: 14, loss: 0.09036526083946228, acc: 96.875, f1: 93.66433140420347, r: 0.6914033412545945
06/02/2019 11:01:21 step: 6752, epoch: 204, batch: 19, loss: 0.16004787385463715, acc: 93.75, f1: 94.8455966821784, r: 0.6100483638814955
06/02/2019 11:01:21 step: 6757, epoch: 204, batch: 24, loss: 0.11770246177911758, acc: 95.3125, f1: 83.5831381733021, r: 0.5493789431489944
06/02/2019 11:01:22 step: 6762, epoch: 204, batch: 29, loss: 0.09010262787342072, acc: 96.875, f1: 96.53468224896797, r: 0.8323819534160195
06/02/2019 11:01:22 *** evaluating ***
06/02/2019 11:01:22 step: 205, epoch: 204, acc: 57.26495726495726, f1: 29.048020830543177, r: 0.3390577525291342
06/02/2019 11:01:22 *** epoch: 206 ***
06/02/2019 11:01:22 *** training ***
06/02/2019 11:01:23 step: 6770, epoch: 205, batch: 4, loss: 0.16418099403381348, acc: 95.3125, f1: 97.0755524431995, r: 0.7805424920757512
06/02/2019 11:01:23 step: 6775, epoch: 205, batch: 9, loss: 0.05506568029522896, acc: 98.4375, f1: 98.43260188087774, r: 0.7989033896048529
06/02/2019 11:01:23 step: 6780, epoch: 205, batch: 14, loss: 0.08528188616037369, acc: 98.4375, f1: 99.11483253588517, r: 0.7226644171662417
06/02/2019 11:01:24 step: 6785, epoch: 205, batch: 19, loss: 0.33151450753211975, acc: 92.1875, f1: 93.390619405657, r: 0.669192979144765
06/02/2019 11:01:24 step: 6790, epoch: 205, batch: 24, loss: 0.1077408492565155, acc: 95.3125, f1: 81.18824005451913, r: 0.7884143953801486
06/02/2019 11:01:25 step: 6795, epoch: 205, batch: 29, loss: 0.0712529644370079, acc: 96.875, f1: 97.49978144942739, r: 0.7501486861917023
06/02/2019 11:01:25 *** evaluating ***
06/02/2019 11:01:25 step: 206, epoch: 205, acc: 58.54700854700855, f1: 27.466256316216903, r: 0.34023030926598974
06/02/2019 11:01:25 *** epoch: 207 ***
06/02/2019 11:01:25 *** training ***
06/02/2019 11:01:25 step: 6803, epoch: 206, batch: 4, loss: 0.13797055184841156, acc: 95.3125, f1: 96.56323877068557, r: 0.8086221283483752
06/02/2019 11:01:26 step: 6808, epoch: 206, batch: 9, loss: 0.04520580545067787, acc: 100.0, f1: 100.0, r: 0.6896698217737339
06/02/2019 11:01:26 step: 6813, epoch: 206, batch: 14, loss: 0.1589280068874359, acc: 96.875, f1: 98.16027136807536, r: 0.6588101255518157
06/02/2019 11:01:27 step: 6818, epoch: 206, batch: 19, loss: 0.12020377814769745, acc: 96.875, f1: 97.36613446290865, r: 0.7388420044559765
06/02/2019 11:01:27 step: 6823, epoch: 206, batch: 24, loss: 0.1169503927230835, acc: 92.1875, f1: 86.07792207792208, r: 0.6644252721867946
06/02/2019 11:01:27 step: 6828, epoch: 206, batch: 29, loss: 0.14118975400924683, acc: 96.875, f1: 91.28205128205128, r: 0.6835367287824836
06/02/2019 11:01:28 *** evaluating ***
06/02/2019 11:01:28 step: 207, epoch: 206, acc: 58.97435897435898, f1: 28.21043039347993, r: 0.3362501459411873
06/02/2019 11:01:28 *** epoch: 208 ***
06/02/2019 11:01:28 *** training ***
06/02/2019 11:01:28 step: 6836, epoch: 207, batch: 4, loss: 0.038733892142772675, acc: 100.0, f1: 100.0, r: 0.6721388568151049
06/02/2019 11:01:28 step: 6841, epoch: 207, batch: 9, loss: 0.13008037209510803, acc: 95.3125, f1: 96.08613445378151, r: 0.7915081938157627
06/02/2019 11:01:29 step: 6846, epoch: 207, batch: 14, loss: 0.2311234176158905, acc: 93.75, f1: 93.95995796692661, r: 0.6970572527714007
06/02/2019 11:01:29 step: 6851, epoch: 207, batch: 19, loss: 0.08252131193876266, acc: 96.875, f1: 91.48877941981391, r: 0.6313935222746989
06/02/2019 11:01:30 step: 6856, epoch: 207, batch: 24, loss: 0.18369126319885254, acc: 95.3125, f1: 88.62914862914863, r: 0.5668415915837367
06/02/2019 11:01:30 step: 6861, epoch: 207, batch: 29, loss: 0.17456084489822388, acc: 93.75, f1: 90.95076579018703, r: 0.7008773728342883
06/02/2019 11:01:30 *** evaluating ***
06/02/2019 11:01:30 step: 208, epoch: 207, acc: 58.119658119658126, f1: 27.711536356682977, r: 0.3407743885989488
06/02/2019 11:01:30 *** epoch: 209 ***
06/02/2019 11:01:30 *** training ***
06/02/2019 11:01:31 step: 6869, epoch: 208, batch: 4, loss: 0.12162333726882935, acc: 96.875, f1: 96.43537414965986, r: 0.7532578702791807
06/02/2019 11:01:31 step: 6874, epoch: 208, batch: 9, loss: 0.06374894082546234, acc: 98.4375, f1: 98.24046920821115, r: 0.6497863571778832
06/02/2019 11:01:31 step: 6879, epoch: 208, batch: 14, loss: 0.06867612153291702, acc: 98.4375, f1: 98.17850637522768, r: 0.6995491238158159
06/02/2019 11:01:32 step: 6884, epoch: 208, batch: 19, loss: 0.14799895882606506, acc: 92.1875, f1: 79.5018638768639, r: 0.7242706920352724
06/02/2019 11:01:32 step: 6889, epoch: 208, batch: 24, loss: 0.05224272608757019, acc: 98.4375, f1: 97.55639097744361, r: 0.7547252961541825
06/02/2019 11:01:32 step: 6894, epoch: 208, batch: 29, loss: 0.1432410329580307, acc: 93.75, f1: 91.23695855153657, r: 0.6703543940112948
06/02/2019 11:01:33 *** evaluating ***
06/02/2019 11:01:33 step: 209, epoch: 208, acc: 59.401709401709404, f1: 30.09279973033697, r: 0.3388432737879967
06/02/2019 11:01:33 *** epoch: 210 ***
06/02/2019 11:01:33 *** training ***
06/02/2019 11:01:33 step: 6902, epoch: 209, batch: 4, loss: 0.14907541871070862, acc: 93.75, f1: 82.75869963369964, r: 0.675967908312313
06/02/2019 11:01:34 step: 6907, epoch: 209, batch: 9, loss: 0.1137818694114685, acc: 95.3125, f1: 82.61502813637868, r: 0.7824906165623863
06/02/2019 11:01:34 step: 6912, epoch: 209, batch: 14, loss: 0.13467347621917725, acc: 96.875, f1: 92.82622139764997, r: 0.6508704347855705
06/02/2019 11:01:34 step: 6917, epoch: 209, batch: 19, loss: 0.07936222106218338, acc: 98.4375, f1: 98.42118665648077, r: 0.6956819713307644
06/02/2019 11:01:35 step: 6922, epoch: 209, batch: 24, loss: 0.05945252627134323, acc: 96.875, f1: 94.04932704789233, r: 0.6674999914343038
06/02/2019 11:01:35 step: 6927, epoch: 209, batch: 29, loss: 0.18929114937782288, acc: 95.3125, f1: 95.01182033096927, r: 0.7224402060527997
06/02/2019 11:01:36 *** evaluating ***
06/02/2019 11:01:36 step: 210, epoch: 209, acc: 59.401709401709404, f1: 29.569287211740043, r: 0.34115651970656197
06/02/2019 11:01:36 *** epoch: 211 ***
06/02/2019 11:01:36 *** training ***
06/02/2019 11:01:36 step: 6935, epoch: 210, batch: 4, loss: 0.13239485025405884, acc: 93.75, f1: 92.0746076208261, r: 0.6883481448411163
06/02/2019 11:01:37 step: 6940, epoch: 210, batch: 9, loss: 0.11460981518030167, acc: 96.875, f1: 90.62736205593349, r: 0.6423508819799217
06/02/2019 11:01:37 step: 6945, epoch: 210, batch: 14, loss: 0.04754265770316124, acc: 98.4375, f1: 97.46657283603096, r: 0.6643622943652744
06/02/2019 11:01:37 step: 6950, epoch: 210, batch: 19, loss: 0.24789397418498993, acc: 90.625, f1: 89.36822755417957, r: 0.7429692988933034
06/02/2019 11:01:38 step: 6955, epoch: 210, batch: 24, loss: 0.1267457902431488, acc: 95.3125, f1: 95.01649144506288, r: 0.7042279578224574
06/02/2019 11:01:38 step: 6960, epoch: 210, batch: 29, loss: 0.05161779373884201, acc: 98.4375, f1: 85.2216748768473, r: 0.7314440049958668
06/02/2019 11:01:39 *** evaluating ***
06/02/2019 11:01:39 step: 211, epoch: 210, acc: 58.97435897435898, f1: 29.319538503098407, r: 0.3434336982597849
06/02/2019 11:01:39 *** epoch: 212 ***
06/02/2019 11:01:39 *** training ***
06/02/2019 11:01:39 step: 6968, epoch: 211, batch: 4, loss: 0.05589177832007408, acc: 96.875, f1: 96.49725274725274, r: 0.5853654460862427
06/02/2019 11:01:40 step: 6973, epoch: 211, batch: 9, loss: 0.17102524638175964, acc: 93.75, f1: 95.76719576719577, r: 0.594657214900687
06/02/2019 11:01:40 step: 6978, epoch: 211, batch: 14, loss: 0.12524622678756714, acc: 93.75, f1: 85.73268911021026, r: 0.7197532749521421
06/02/2019 11:01:40 step: 6983, epoch: 211, batch: 19, loss: 0.117969810962677, acc: 95.3125, f1: 93.58585858585859, r: 0.7654855684991948
06/02/2019 11:01:41 step: 6988, epoch: 211, batch: 24, loss: 0.08195578306913376, acc: 96.875, f1: 95.33416875522138, r: 0.7261084623949196
06/02/2019 11:01:41 step: 6993, epoch: 211, batch: 29, loss: 0.08059098571538925, acc: 96.875, f1: 92.060547162588, r: 0.6445439351292924
06/02/2019 11:01:42 *** evaluating ***
06/02/2019 11:01:42 step: 212, epoch: 211, acc: 60.256410256410255, f1: 30.633415663251988, r: 0.34599605479630746
06/02/2019 11:01:42 *** epoch: 213 ***
06/02/2019 11:01:42 *** training ***
06/02/2019 11:01:42 step: 7001, epoch: 212, batch: 4, loss: 0.134699746966362, acc: 96.875, f1: 94.23684367363965, r: 0.7821721177398075
06/02/2019 11:01:43 step: 7006, epoch: 212, batch: 9, loss: 0.08132796734571457, acc: 95.3125, f1: 95.98506069094304, r: 0.6992909296275338
06/02/2019 11:01:43 step: 7011, epoch: 212, batch: 14, loss: 0.21233198046684265, acc: 95.3125, f1: 93.90994593031945, r: 0.6197792403385131
06/02/2019 11:01:43 step: 7016, epoch: 212, batch: 19, loss: 0.15593719482421875, acc: 90.625, f1: 67.40832087983935, r: 0.6697468029626744
06/02/2019 11:01:44 step: 7021, epoch: 212, batch: 24, loss: 0.20041197538375854, acc: 92.1875, f1: 86.93467711225759, r: 0.6502927886834419
06/02/2019 11:01:44 step: 7026, epoch: 212, batch: 29, loss: 0.04762039706110954, acc: 100.0, f1: 100.0, r: 0.7206030138172275
06/02/2019 11:01:44 *** evaluating ***
06/02/2019 11:01:45 step: 213, epoch: 212, acc: 61.111111111111114, f1: 29.97114912339736, r: 0.34734865130022435
06/02/2019 11:01:45 *** epoch: 214 ***
06/02/2019 11:01:45 *** training ***
06/02/2019 11:01:45 step: 7034, epoch: 213, batch: 4, loss: 0.15937890112400055, acc: 95.3125, f1: 97.58686361947231, r: 0.7153195057496256
06/02/2019 11:01:45 step: 7039, epoch: 213, batch: 9, loss: 0.1753300130367279, acc: 95.3125, f1: 97.28862973760933, r: 0.6263809332141306
06/02/2019 11:01:46 step: 7044, epoch: 213, batch: 14, loss: 0.25566935539245605, acc: 92.1875, f1: 87.64418811002662, r: 0.6244785685425674
06/02/2019 11:01:46 step: 7049, epoch: 213, batch: 19, loss: 0.05039075016975403, acc: 98.4375, f1: 97.00680272108843, r: 0.6996345727143051
06/02/2019 11:01:47 step: 7054, epoch: 213, batch: 24, loss: 0.2871411442756653, acc: 92.1875, f1: 73.31668331668332, r: 0.5520401738042264
06/02/2019 11:01:47 step: 7059, epoch: 213, batch: 29, loss: 0.09429654479026794, acc: 96.875, f1: 95.53081838796125, r: 0.6891887674940194
06/02/2019 11:01:47 *** evaluating ***
06/02/2019 11:01:47 step: 214, epoch: 213, acc: 61.111111111111114, f1: 28.27561751173926, r: 0.3449061801917324
06/02/2019 11:01:47 *** epoch: 215 ***
06/02/2019 11:01:47 *** training ***
06/02/2019 11:01:48 step: 7067, epoch: 214, batch: 4, loss: 0.12273495644330978, acc: 96.875, f1: 85.23351648351648, r: 0.7064929372697175
06/02/2019 11:01:48 step: 7072, epoch: 214, batch: 9, loss: 0.1845431923866272, acc: 95.3125, f1: 95.09307425540597, r: 0.7356240594472776
06/02/2019 11:01:49 step: 7077, epoch: 214, batch: 14, loss: 0.11937461793422699, acc: 96.875, f1: 98.33333333333334, r: 0.6735925797328828
06/02/2019 11:01:49 step: 7082, epoch: 214, batch: 19, loss: 0.16201399266719818, acc: 93.75, f1: 85.9920634920635, r: 0.6768895227337651
06/02/2019 11:01:49 step: 7087, epoch: 214, batch: 24, loss: 0.06970158964395523, acc: 98.4375, f1: 94.97835497835497, r: 0.7651099049694402
06/02/2019 11:01:50 step: 7092, epoch: 214, batch: 29, loss: 0.09023283421993256, acc: 96.875, f1: 86.1517719568567, r: 0.7351918433294307
06/02/2019 11:01:50 *** evaluating ***
06/02/2019 11:01:50 step: 215, epoch: 214, acc: 60.256410256410255, f1: 30.73329071144605, r: 0.3451542606028778
06/02/2019 11:01:50 *** epoch: 216 ***
06/02/2019 11:01:50 *** training ***
06/02/2019 11:01:50 step: 7100, epoch: 215, batch: 4, loss: 0.09429284185171127, acc: 95.3125, f1: 95.41346153846155, r: 0.7825403284832436
06/02/2019 11:01:51 step: 7105, epoch: 215, batch: 9, loss: 0.06944763660430908, acc: 95.3125, f1: 94.73290598290598, r: 0.7954214362649185
06/02/2019 11:01:51 step: 7110, epoch: 215, batch: 14, loss: 0.04883038252592087, acc: 98.4375, f1: 98.80745341614906, r: 0.6568700413690353
06/02/2019 11:01:52 step: 7115, epoch: 215, batch: 19, loss: 0.10609780997037888, acc: 95.3125, f1: 94.78011643528885, r: 0.6966257110026053
06/02/2019 11:01:52 step: 7120, epoch: 215, batch: 24, loss: 0.03812480345368385, acc: 100.0, f1: 100.0, r: 0.8000634009997123
06/02/2019 11:01:52 step: 7125, epoch: 215, batch: 29, loss: 0.031476378440856934, acc: 100.0, f1: 100.0, r: 0.7215250402038821
06/02/2019 11:01:53 *** evaluating ***
06/02/2019 11:01:53 step: 216, epoch: 215, acc: 57.692307692307686, f1: 27.74426026151652, r: 0.33486687471690313
06/02/2019 11:01:53 *** epoch: 217 ***
06/02/2019 11:01:53 *** training ***
06/02/2019 11:01:53 step: 7133, epoch: 216, batch: 4, loss: 0.1962771713733673, acc: 92.1875, f1: 87.11309523809523, r: 0.7041936951056623
06/02/2019 11:01:53 step: 7138, epoch: 216, batch: 9, loss: 0.07268168777227402, acc: 98.4375, f1: 96.89223057644111, r: 0.7085313423860606
06/02/2019 11:01:54 step: 7143, epoch: 216, batch: 14, loss: 0.1288631558418274, acc: 93.75, f1: 93.90756302521008, r: 0.7155036982545668
06/02/2019 11:01:54 step: 7148, epoch: 216, batch: 19, loss: 0.045509736984968185, acc: 98.4375, f1: 99.14965986394557, r: 0.8126534092202299
06/02/2019 11:01:55 step: 7153, epoch: 216, batch: 24, loss: 0.055473487824201584, acc: 95.3125, f1: 96.00723172151744, r: 0.6859634922886313
06/02/2019 11:01:55 step: 7158, epoch: 216, batch: 29, loss: 0.09333103150129318, acc: 95.3125, f1: 82.75185061299284, r: 0.6293040601668359
06/02/2019 11:01:55 *** evaluating ***
06/02/2019 11:01:56 step: 217, epoch: 216, acc: 59.82905982905983, f1: 30.04798627298627, r: 0.3429624362071982
06/02/2019 11:01:56 *** epoch: 218 ***
06/02/2019 11:01:56 *** training ***
06/02/2019 11:01:56 step: 7166, epoch: 217, batch: 4, loss: 0.11079946160316467, acc: 96.875, f1: 95.8018252933507, r: 0.673500184927461
06/02/2019 11:01:56 step: 7171, epoch: 217, batch: 9, loss: 0.21083977818489075, acc: 90.625, f1: 87.60915071770334, r: 0.7416855808481913
06/02/2019 11:01:57 step: 7176, epoch: 217, batch: 14, loss: 0.07270780950784683, acc: 96.875, f1: 91.94076038903624, r: 0.760472769086363
06/02/2019 11:01:57 step: 7181, epoch: 217, batch: 19, loss: 0.033638134598731995, acc: 100.0, f1: 100.0, r: 0.680461714202332
06/02/2019 11:01:58 step: 7186, epoch: 217, batch: 24, loss: 0.24462077021598816, acc: 95.3125, f1: 93.49033319621556, r: 0.7172066687307528
06/02/2019 11:01:58 step: 7191, epoch: 217, batch: 29, loss: 0.09530887007713318, acc: 96.875, f1: 84.97837318274661, r: 0.7771298674242926
06/02/2019 11:01:58 *** evaluating ***
06/02/2019 11:01:58 step: 218, epoch: 217, acc: 59.82905982905983, f1: 28.539609712430973, r: 0.3430842920226545
06/02/2019 11:01:58 *** epoch: 219 ***
06/02/2019 11:01:58 *** training ***
06/02/2019 11:01:59 step: 7199, epoch: 218, batch: 4, loss: 0.06675058603286743, acc: 96.875, f1: 98.43304843304844, r: 0.6088403947504922
06/02/2019 11:01:59 step: 7204, epoch: 218, batch: 9, loss: 0.06851516664028168, acc: 96.875, f1: 96.8359819623906, r: 0.6841743336469918
06/02/2019 11:01:59 step: 7209, epoch: 218, batch: 14, loss: 0.23751288652420044, acc: 93.75, f1: 93.42803030303031, r: 0.7799706919026631
06/02/2019 11:02:00 step: 7214, epoch: 218, batch: 19, loss: 0.039950255304574966, acc: 98.4375, f1: 95.71428571428571, r: 0.743158282937551
06/02/2019 11:02:00 step: 7219, epoch: 218, batch: 24, loss: 0.15756379067897797, acc: 93.75, f1: 91.01229263635278, r: 0.6485872246270761
06/02/2019 11:02:01 step: 7224, epoch: 218, batch: 29, loss: 0.13827300071716309, acc: 93.75, f1: 93.03030303030303, r: 0.8108493564244135
06/02/2019 11:02:01 *** evaluating ***
06/02/2019 11:02:01 step: 219, epoch: 218, acc: 58.119658119658126, f1: 25.35683859351453, r: 0.3380284202890944
06/02/2019 11:02:01 *** epoch: 220 ***
06/02/2019 11:02:01 *** training ***
06/02/2019 11:02:01 step: 7232, epoch: 219, batch: 4, loss: 0.04374529421329498, acc: 100.0, f1: 100.0, r: 0.7907423704786798
06/02/2019 11:02:02 step: 7237, epoch: 219, batch: 9, loss: 0.0469789057970047, acc: 100.0, f1: 100.0, r: 0.6624293892227078
06/02/2019 11:02:02 step: 7242, epoch: 219, batch: 14, loss: 0.06146995723247528, acc: 98.4375, f1: 99.30994824611847, r: 0.6201240399636094
06/02/2019 11:02:02 step: 7247, epoch: 219, batch: 19, loss: 0.12599876523017883, acc: 95.3125, f1: 92.83424908424908, r: 0.7005631671137792
06/02/2019 11:02:03 step: 7252, epoch: 219, batch: 24, loss: 0.0737583115696907, acc: 96.875, f1: 85.54112554112554, r: 0.7467261844729051
06/02/2019 11:02:03 step: 7257, epoch: 219, batch: 29, loss: 0.046546727418899536, acc: 100.0, f1: 100.0, r: 0.6676340639057479
06/02/2019 11:02:03 *** evaluating ***
06/02/2019 11:02:04 step: 220, epoch: 219, acc: 59.401709401709404, f1: 28.1550709875518, r: 0.33847295925785753
06/02/2019 11:02:04 *** epoch: 221 ***
06/02/2019 11:02:04 *** training ***
06/02/2019 11:02:04 step: 7265, epoch: 220, batch: 4, loss: 0.0784096047282219, acc: 96.875, f1: 97.90565756082998, r: 0.6711128089348694
06/02/2019 11:02:04 step: 7270, epoch: 220, batch: 9, loss: 0.16710421442985535, acc: 93.75, f1: 88.69701726844585, r: 0.6617169185136222
06/02/2019 11:02:05 step: 7275, epoch: 220, batch: 14, loss: 0.14511169493198395, acc: 93.75, f1: 91.3388909940634, r: 0.5854990892338391
06/02/2019 11:02:05 step: 7280, epoch: 220, batch: 19, loss: 0.12723183631896973, acc: 96.875, f1: 95.66750208855473, r: 0.7648151325006259
06/02/2019 11:02:06 step: 7285, epoch: 220, batch: 24, loss: 0.08736798167228699, acc: 96.875, f1: 96.49908661536568, r: 0.7668867873416386
06/02/2019 11:02:06 step: 7290, epoch: 220, batch: 29, loss: 0.06485052406787872, acc: 98.4375, f1: 97.85714285714286, r: 0.7658242728968601
06/02/2019 11:02:06 *** evaluating ***
06/02/2019 11:02:07 step: 221, epoch: 220, acc: 58.54700854700855, f1: 29.262104272604645, r: 0.3381548310217757
06/02/2019 11:02:07 *** epoch: 222 ***
06/02/2019 11:02:07 *** training ***
06/02/2019 11:02:07 step: 7298, epoch: 221, batch: 4, loss: 0.11001121997833252, acc: 96.875, f1: 94.15646258503402, r: 0.6899719900135702
06/02/2019 11:02:07 step: 7303, epoch: 221, batch: 9, loss: 0.021613162010908127, acc: 100.0, f1: 100.0, r: 0.5938994623050615
06/02/2019 11:02:08 step: 7308, epoch: 221, batch: 14, loss: 0.0884675458073616, acc: 95.3125, f1: 95.82964927792514, r: 0.7785283673300047
06/02/2019 11:02:08 step: 7313, epoch: 221, batch: 19, loss: 0.22412411868572235, acc: 96.875, f1: 81.76618015627304, r: 0.7325424936914846
06/02/2019 11:02:09 step: 7318, epoch: 221, batch: 24, loss: 0.13770200312137604, acc: 96.875, f1: 98.06020066889633, r: 0.7322311912201411
06/02/2019 11:02:09 step: 7323, epoch: 221, batch: 29, loss: 0.15805011987686157, acc: 93.75, f1: 90.2725933975934, r: 0.7592841857708942
06/02/2019 11:02:09 *** evaluating ***
06/02/2019 11:02:10 step: 222, epoch: 221, acc: 58.97435897435898, f1: 30.134210865918178, r: 0.34312249894684704
06/02/2019 11:02:10 *** epoch: 223 ***
06/02/2019 11:02:10 *** training ***
06/02/2019 11:02:10 step: 7331, epoch: 222, batch: 4, loss: 0.06854274123907089, acc: 96.875, f1: 95.93073593073592, r: 0.642688643404424
06/02/2019 11:02:10 step: 7336, epoch: 222, batch: 9, loss: 0.23336340487003326, acc: 95.3125, f1: 94.5703338806787, r: 0.6199676593568201
06/02/2019 11:02:11 step: 7341, epoch: 222, batch: 14, loss: 0.13297335803508759, acc: 95.3125, f1: 91.75725943946142, r: 0.7435815698189311
06/02/2019 11:02:11 step: 7346, epoch: 222, batch: 19, loss: 0.07450737059116364, acc: 96.875, f1: 91.70330680534762, r: 0.6354955002684484
06/02/2019 11:02:12 step: 7351, epoch: 222, batch: 24, loss: 0.02477256953716278, acc: 100.0, f1: 100.0, r: 0.6389944795445169
06/02/2019 11:02:12 step: 7356, epoch: 222, batch: 29, loss: 0.06456831097602844, acc: 98.4375, f1: 98.4006734006734, r: 0.8079892814606444
06/02/2019 11:02:12 *** evaluating ***
06/02/2019 11:02:12 step: 223, epoch: 222, acc: 59.82905982905983, f1: 30.618174138789755, r: 0.34558976440956996
06/02/2019 11:02:12 *** epoch: 224 ***
06/02/2019 11:02:12 *** training ***
06/02/2019 11:02:13 step: 7364, epoch: 223, batch: 4, loss: 0.12769202888011932, acc: 96.875, f1: 97.13305562362166, r: 0.667226495185741
06/02/2019 11:02:13 step: 7369, epoch: 223, batch: 9, loss: 0.1281770020723343, acc: 95.3125, f1: 97.45566933066934, r: 0.7293018186751427
06/02/2019 11:02:13 step: 7374, epoch: 223, batch: 14, loss: 0.20761361718177795, acc: 92.1875, f1: 88.27200398079246, r: 0.6537399434142723
06/02/2019 11:02:14 step: 7379, epoch: 223, batch: 19, loss: 0.16086354851722717, acc: 93.75, f1: 89.49764521193092, r: 0.6315105466667843
06/02/2019 11:02:14 step: 7384, epoch: 223, batch: 24, loss: 0.11211524158716202, acc: 96.875, f1: 95.93253968253967, r: 0.7475805698574538
06/02/2019 11:02:15 step: 7389, epoch: 223, batch: 29, loss: 0.04186214879155159, acc: 98.4375, f1: 98.79062736205594, r: 0.834550914777822
06/02/2019 11:02:15 *** evaluating ***
06/02/2019 11:02:15 step: 224, epoch: 223, acc: 59.82905982905983, f1: 29.94956691385263, r: 0.3412642788596384
06/02/2019 11:02:15 *** epoch: 225 ***
06/02/2019 11:02:15 *** training ***
06/02/2019 11:02:16 step: 7397, epoch: 224, batch: 4, loss: 0.08616411685943604, acc: 95.3125, f1: 92.24161255411256, r: 0.7661328458485261
06/02/2019 11:02:16 step: 7402, epoch: 224, batch: 9, loss: 0.09543687850236893, acc: 95.3125, f1: 95.7977207977208, r: 0.7738730345502527
06/02/2019 11:02:16 step: 7407, epoch: 224, batch: 14, loss: 0.1944068819284439, acc: 89.0625, f1: 86.37101495829891, r: 0.6618248548521547
06/02/2019 11:02:17 step: 7412, epoch: 224, batch: 19, loss: 0.017146598547697067, acc: 100.0, f1: 100.0, r: 0.7527129208892175
06/02/2019 11:02:17 step: 7417, epoch: 224, batch: 24, loss: 0.05487033724784851, acc: 98.4375, f1: 97.84126984126985, r: 0.6723965608483937
06/02/2019 11:02:18 step: 7422, epoch: 224, batch: 29, loss: 0.11246539652347565, acc: 95.3125, f1: 95.60795526312769, r: 0.6899246273077683
06/02/2019 11:02:18 *** evaluating ***
06/02/2019 11:02:18 step: 225, epoch: 224, acc: 60.68376068376068, f1: 32.4248361287835, r: 0.33653534196734525
06/02/2019 11:02:18 *** epoch: 226 ***
06/02/2019 11:02:18 *** training ***
06/02/2019 11:02:18 step: 7430, epoch: 225, batch: 4, loss: 0.19759738445281982, acc: 92.1875, f1: 91.43443754313321, r: 0.6451499313386723
06/02/2019 11:02:19 step: 7435, epoch: 225, batch: 9, loss: 0.06817031651735306, acc: 96.875, f1: 97.15805283152223, r: 0.7340924821210344
06/02/2019 11:02:19 step: 7440, epoch: 225, batch: 14, loss: 0.11760397255420685, acc: 98.4375, f1: 98.0045351473923, r: 0.7225068115093991
06/02/2019 11:02:20 step: 7445, epoch: 225, batch: 19, loss: 0.16041763126850128, acc: 93.75, f1: 92.32178932178932, r: 0.6725217384248446
06/02/2019 11:02:20 step: 7450, epoch: 225, batch: 24, loss: 0.0632159411907196, acc: 98.4375, f1: 98.02197802197803, r: 0.7584174943822337
06/02/2019 11:02:20 step: 7455, epoch: 225, batch: 29, loss: 0.10608769208192825, acc: 96.875, f1: 95.28985507246377, r: 0.8213972855489263
06/02/2019 11:02:21 *** evaluating ***
06/02/2019 11:02:21 step: 226, epoch: 225, acc: 58.54700854700855, f1: 28.38433111603843, r: 0.3348596360344896
06/02/2019 11:02:21 *** epoch: 227 ***
06/02/2019 11:02:21 *** training ***
06/02/2019 11:02:21 step: 7463, epoch: 226, batch: 4, loss: 0.0695045068860054, acc: 98.4375, f1: 98.59767891682785, r: 0.8157226431889752
06/02/2019 11:02:21 step: 7468, epoch: 226, batch: 9, loss: 0.15689557790756226, acc: 93.75, f1: 86.76434676434675, r: 0.6786836835959185
06/02/2019 11:02:22 step: 7473, epoch: 226, batch: 14, loss: 0.17682312428951263, acc: 93.75, f1: 93.97267206477733, r: 0.7385833033241362
06/02/2019 11:02:22 step: 7478, epoch: 226, batch: 19, loss: 0.08667878806591034, acc: 98.4375, f1: 99.10934020860189, r: 0.7248986517189622
06/02/2019 11:02:23 step: 7483, epoch: 226, batch: 24, loss: 0.0995572879910469, acc: 96.875, f1: 98.29497016197783, r: 0.6400798903946048
06/02/2019 11:02:23 step: 7488, epoch: 226, batch: 29, loss: 0.07539574801921844, acc: 96.875, f1: 86.10625620655412, r: 0.7722984243692161
06/02/2019 11:02:23 *** evaluating ***
06/02/2019 11:02:23 step: 227, epoch: 226, acc: 58.54700854700855, f1: 30.096780791225235, r: 0.33266667420354545
06/02/2019 11:02:23 *** epoch: 228 ***
06/02/2019 11:02:23 *** training ***
06/02/2019 11:02:24 step: 7496, epoch: 227, batch: 4, loss: 0.05575069040060043, acc: 98.4375, f1: 99.15902964959568, r: 0.6799036879119675
06/02/2019 11:02:24 step: 7501, epoch: 227, batch: 9, loss: 0.21526724100112915, acc: 90.625, f1: 85.61180904849508, r: 0.6599386368379483
06/02/2019 11:02:24 step: 7506, epoch: 227, batch: 14, loss: 0.1746111363172531, acc: 93.75, f1: 79.15374677002585, r: 0.6757533267095707
06/02/2019 11:02:25 step: 7511, epoch: 227, batch: 19, loss: 0.09777576476335526, acc: 98.4375, f1: 99.08733679807327, r: 0.7271252382423018
06/02/2019 11:02:25 step: 7516, epoch: 227, batch: 24, loss: 0.09317230433225632, acc: 95.3125, f1: 97.66825649178591, r: 0.5863675676990651
06/02/2019 11:02:26 step: 7521, epoch: 227, batch: 29, loss: 0.037127383053302765, acc: 100.0, f1: 100.0, r: 0.7917621172587599
06/02/2019 11:02:26 *** evaluating ***
06/02/2019 11:02:26 step: 228, epoch: 227, acc: 58.97435897435898, f1: 29.722909365766508, r: 0.33582182704693825
06/02/2019 11:02:26 *** epoch: 229 ***
06/02/2019 11:02:26 *** training ***
06/02/2019 11:02:27 step: 7529, epoch: 228, batch: 4, loss: 0.2403900921344757, acc: 93.75, f1: 93.8036198219125, r: 0.7305358844258932
06/02/2019 11:02:27 step: 7534, epoch: 228, batch: 9, loss: 0.06259875744581223, acc: 96.875, f1: 96.28848149429263, r: 0.6866966900712983
06/02/2019 11:02:27 step: 7539, epoch: 228, batch: 14, loss: 0.10836099088191986, acc: 95.3125, f1: 95.32738923839626, r: 0.6059972499323935
06/02/2019 11:02:28 step: 7544, epoch: 228, batch: 19, loss: 0.10766469687223434, acc: 96.875, f1: 93.75093717198982, r: 0.7316317173146121
06/02/2019 11:02:28 step: 7549, epoch: 228, batch: 24, loss: 0.12607397139072418, acc: 95.3125, f1: 97.21628117213855, r: 0.6655881208891452
06/02/2019 11:02:29 step: 7554, epoch: 228, batch: 29, loss: 0.14239487051963806, acc: 92.1875, f1: 85.15873015873015, r: 0.8406967562868668
06/02/2019 11:02:29 *** evaluating ***
06/02/2019 11:02:29 step: 229, epoch: 228, acc: 58.97435897435898, f1: 30.68764568764569, r: 0.33842784020061695
06/02/2019 11:02:29 *** epoch: 230 ***
06/02/2019 11:02:29 *** training ***
06/02/2019 11:02:29 step: 7562, epoch: 229, batch: 4, loss: 0.06294910609722137, acc: 98.4375, f1: 99.09634551495017, r: 0.7207685235402083
06/02/2019 11:02:30 step: 7567, epoch: 229, batch: 9, loss: 0.20678967237472534, acc: 95.3125, f1: 96.18898097516518, r: 0.7309679541704044
06/02/2019 11:02:30 step: 7572, epoch: 229, batch: 14, loss: 0.146306574344635, acc: 95.3125, f1: 92.45769972156779, r: 0.74504985756999
06/02/2019 11:02:31 step: 7577, epoch: 229, batch: 19, loss: 0.042947761714458466, acc: 100.0, f1: 100.0, r: 0.7342757909780528
06/02/2019 11:02:31 step: 7582, epoch: 229, batch: 24, loss: 0.17158377170562744, acc: 93.75, f1: 90.38149350649351, r: 0.754574800083988
06/02/2019 11:02:32 step: 7587, epoch: 229, batch: 29, loss: 0.11549440026283264, acc: 95.3125, f1: 91.75427746856319, r: 0.6452252021146316
06/02/2019 11:02:32 *** evaluating ***
06/02/2019 11:02:32 step: 230, epoch: 229, acc: 58.54700854700855, f1: 30.161978921589437, r: 0.3374834423815163
06/02/2019 11:02:32 *** epoch: 231 ***
06/02/2019 11:02:32 *** training ***
06/02/2019 11:02:32 step: 7595, epoch: 230, batch: 4, loss: 0.23334065079689026, acc: 92.1875, f1: 91.26858648597779, r: 0.6771415389529302
06/02/2019 11:02:33 step: 7600, epoch: 230, batch: 9, loss: 0.1748373806476593, acc: 95.3125, f1: 92.35185185185185, r: 0.6773741440668551
06/02/2019 11:02:33 step: 7605, epoch: 230, batch: 14, loss: 0.03753455728292465, acc: 100.0, f1: 100.0, r: 0.8168295618472824
06/02/2019 11:02:34 step: 7610, epoch: 230, batch: 19, loss: 0.05512858182191849, acc: 98.4375, f1: 98.60742705570291, r: 0.7523531710584517
06/02/2019 11:02:34 step: 7615, epoch: 230, batch: 24, loss: 0.07330334186553955, acc: 98.4375, f1: 97.97843665768194, r: 0.7980276749944413
06/02/2019 11:02:35 step: 7620, epoch: 230, batch: 29, loss: 0.07853121310472488, acc: 98.4375, f1: 96.91609977324262, r: 0.6862717841685326
06/02/2019 11:02:35 *** evaluating ***
06/02/2019 11:02:35 step: 231, epoch: 230, acc: 58.119658119658126, f1: 28.860438342145656, r: 0.3370400477123149
06/02/2019 11:02:35 *** epoch: 232 ***
06/02/2019 11:02:35 *** training ***
06/02/2019 11:02:35 step: 7628, epoch: 231, batch: 4, loss: 0.039974089711904526, acc: 98.4375, f1: 95.10204081632652, r: 0.6237130427495059
06/02/2019 11:02:36 step: 7633, epoch: 231, batch: 9, loss: 0.1087394431233406, acc: 95.3125, f1: 96.64957264957266, r: 0.7494052889742391
06/02/2019 11:02:36 step: 7638, epoch: 231, batch: 14, loss: 0.23491787910461426, acc: 90.625, f1: 78.39603409933285, r: 0.7033372418523944
06/02/2019 11:02:36 step: 7643, epoch: 231, batch: 19, loss: 0.12410511821508408, acc: 93.75, f1: 94.19018671298306, r: 0.6830051085191937
06/02/2019 11:02:37 step: 7648, epoch: 231, batch: 24, loss: 0.033324725925922394, acc: 100.0, f1: 100.0, r: 0.7573595779365605
06/02/2019 11:02:37 step: 7653, epoch: 231, batch: 29, loss: 0.06412015110254288, acc: 95.3125, f1: 93.18518518518519, r: 0.7690052477677339
06/02/2019 11:02:38 *** evaluating ***
06/02/2019 11:02:38 step: 232, epoch: 231, acc: 58.97435897435898, f1: 29.82504287203958, r: 0.34481482404984914
06/02/2019 11:02:38 *** epoch: 233 ***
06/02/2019 11:02:38 *** training ***
06/02/2019 11:02:38 step: 7661, epoch: 232, batch: 4, loss: 0.08942049741744995, acc: 96.875, f1: 97.31888108630588, r: 0.8161082040369634
06/02/2019 11:02:39 step: 7666, epoch: 232, batch: 9, loss: 0.08262979239225388, acc: 96.875, f1: 96.22289972899729, r: 0.84275207055592
06/02/2019 11:02:39 step: 7671, epoch: 232, batch: 14, loss: 0.10062944144010544, acc: 96.875, f1: 97.29312354312354, r: 0.8010939230767892
06/02/2019 11:02:40 step: 7676, epoch: 232, batch: 19, loss: 0.07536634057760239, acc: 96.875, f1: 95.5159082266447, r: 0.7216908183704516
06/02/2019 11:02:40 step: 7681, epoch: 232, batch: 24, loss: 0.11466153711080551, acc: 95.3125, f1: 94.48724033441643, r: 0.6500807291388451
06/02/2019 11:02:40 step: 7686, epoch: 232, batch: 29, loss: 0.06664340198040009, acc: 95.3125, f1: 91.27643144902514, r: 0.6643133699762218
06/02/2019 11:02:41 *** evaluating ***
06/02/2019 11:02:41 step: 233, epoch: 232, acc: 58.97435897435898, f1: 30.05426225820963, r: 0.34268663839101343
06/02/2019 11:02:41 *** epoch: 234 ***
06/02/2019 11:02:41 *** training ***
06/02/2019 11:02:41 step: 7694, epoch: 233, batch: 4, loss: 0.03712437301874161, acc: 100.0, f1: 100.0, r: 0.6965795348325853
06/02/2019 11:02:41 step: 7699, epoch: 233, batch: 9, loss: 0.13363823294639587, acc: 93.75, f1: 90.93481397829223, r: 0.6481170684425678
06/02/2019 11:02:42 step: 7704, epoch: 233, batch: 14, loss: 0.0819135531783104, acc: 98.4375, f1: 98.3201581027668, r: 0.7855032826302336
06/02/2019 11:02:42 step: 7709, epoch: 233, batch: 19, loss: 0.03480113670229912, acc: 100.0, f1: 100.0, r: 0.687231817833191
06/02/2019 11:02:43 step: 7714, epoch: 233, batch: 24, loss: 0.2448779046535492, acc: 93.75, f1: 89.98878701278082, r: 0.7536367835664628
06/02/2019 11:02:43 step: 7719, epoch: 233, batch: 29, loss: 0.15227726101875305, acc: 95.3125, f1: 90.57326137867834, r: 0.6248053322977957
06/02/2019 11:02:43 *** evaluating ***
06/02/2019 11:02:43 step: 234, epoch: 233, acc: 60.256410256410255, f1: 29.73031760093754, r: 0.3430031102537217
06/02/2019 11:02:43 *** epoch: 235 ***
06/02/2019 11:02:43 *** training ***
06/02/2019 11:02:44 step: 7727, epoch: 234, batch: 4, loss: 0.039358291774988174, acc: 100.0, f1: 100.0, r: 0.7215146086355899
06/02/2019 11:02:44 step: 7732, epoch: 234, batch: 9, loss: 0.14874783158302307, acc: 96.875, f1: 98.5, r: 0.7677180576317131
06/02/2019 11:02:44 step: 7737, epoch: 234, batch: 14, loss: 0.10042652487754822, acc: 98.4375, f1: 97.89939192924267, r: 0.6246040640058111
06/02/2019 11:02:45 step: 7742, epoch: 234, batch: 19, loss: 0.23588936030864716, acc: 93.75, f1: 85.92884375972612, r: 0.7511864231771403
06/02/2019 11:02:45 step: 7747, epoch: 234, batch: 24, loss: 0.06256885826587677, acc: 95.3125, f1: 94.67745718977247, r: 0.6846693464789103
06/02/2019 11:02:45 step: 7752, epoch: 234, batch: 29, loss: 0.03827439621090889, acc: 100.0, f1: 100.0, r: 0.8050786689852674
06/02/2019 11:02:46 *** evaluating ***
06/02/2019 11:02:46 step: 235, epoch: 234, acc: 59.401709401709404, f1: 27.793723030908268, r: 0.3382199886076473
06/02/2019 11:02:46 *** epoch: 236 ***
06/02/2019 11:02:46 *** training ***
06/02/2019 11:02:46 step: 7760, epoch: 235, batch: 4, loss: 0.17574501037597656, acc: 92.1875, f1: 79.31696336009004, r: 0.6096323063261525
06/02/2019 11:02:47 step: 7765, epoch: 235, batch: 9, loss: 0.10565055906772614, acc: 95.3125, f1: 94.87417344560203, r: 0.6406973579942743
06/02/2019 11:02:47 step: 7770, epoch: 235, batch: 14, loss: 0.16022251546382904, acc: 95.3125, f1: 85.62163604180411, r: 0.5818380440632254
06/02/2019 11:02:47 step: 7775, epoch: 235, batch: 19, loss: 0.0762416198849678, acc: 96.875, f1: 93.24853434779604, r: 0.7215892961313977
06/02/2019 11:02:48 step: 7780, epoch: 235, batch: 24, loss: 0.09454133361577988, acc: 96.875, f1: 96.88888888888889, r: 0.7068536895898264
06/02/2019 11:02:48 step: 7785, epoch: 235, batch: 29, loss: 0.07716136425733566, acc: 96.875, f1: 98.01536098310292, r: 0.7209276974043957
06/02/2019 11:02:48 *** evaluating ***
06/02/2019 11:02:49 step: 236, epoch: 235, acc: 59.82905982905983, f1: 26.985815855127733, r: 0.33212707073582837
06/02/2019 11:02:49 *** epoch: 237 ***
06/02/2019 11:02:49 *** training ***
06/02/2019 11:02:49 step: 7793, epoch: 236, batch: 4, loss: 0.11857964098453522, acc: 95.3125, f1: 95.66919191919192, r: 0.756663582874348
06/02/2019 11:02:49 step: 7798, epoch: 236, batch: 9, loss: 0.12342264503240585, acc: 93.75, f1: 78.96417095266199, r: 0.7548943872344749
06/02/2019 11:02:50 step: 7803, epoch: 236, batch: 14, loss: 0.1825394332408905, acc: 93.75, f1: 95.23093447905478, r: 0.6821002584005929
06/02/2019 11:02:50 step: 7808, epoch: 236, batch: 19, loss: 0.09586715698242188, acc: 96.875, f1: 92.03723594701037, r: 0.6252839398390575
06/02/2019 11:02:50 step: 7813, epoch: 236, batch: 24, loss: 0.0364607572555542, acc: 100.0, f1: 100.0, r: 0.7933073627225544
06/02/2019 11:02:51 step: 7818, epoch: 236, batch: 29, loss: 0.06250983476638794, acc: 98.4375, f1: 99.22067268252665, r: 0.799266434116556
06/02/2019 11:02:51 *** evaluating ***
06/02/2019 11:02:51 step: 237, epoch: 236, acc: 58.54700854700855, f1: 29.31828026857266, r: 0.33482736655752854
06/02/2019 11:02:51 *** epoch: 238 ***
06/02/2019 11:02:51 *** training ***
06/02/2019 11:02:52 step: 7826, epoch: 237, batch: 4, loss: 0.06522321701049805, acc: 100.0, f1: 100.0, r: 0.7442931428082171
06/02/2019 11:02:52 step: 7831, epoch: 237, batch: 9, loss: 0.054273538291454315, acc: 96.875, f1: 95.09090185781915, r: 0.7280977253637763
06/02/2019 11:02:52 step: 7836, epoch: 237, batch: 14, loss: 0.07833895087242126, acc: 96.875, f1: 96.6060985797828, r: 0.8081113026861553
06/02/2019 11:02:53 step: 7841, epoch: 237, batch: 19, loss: 0.052862897515296936, acc: 98.4375, f1: 97.90940766550523, r: 0.7459439934107631
06/02/2019 11:02:53 step: 7846, epoch: 237, batch: 24, loss: 0.09750299155712128, acc: 96.875, f1: 94.44444444444444, r: 0.7126089880611545
06/02/2019 11:02:53 step: 7851, epoch: 237, batch: 29, loss: 0.09259939938783646, acc: 96.875, f1: 94.4047619047619, r: 0.7817370037335314
06/02/2019 11:02:54 *** evaluating ***
06/02/2019 11:02:54 step: 238, epoch: 237, acc: 59.401709401709404, f1: 30.520818322820165, r: 0.334386290059412
06/02/2019 11:02:54 *** epoch: 239 ***
06/02/2019 11:02:54 *** training ***
06/02/2019 11:02:54 step: 7859, epoch: 238, batch: 4, loss: 0.12864422798156738, acc: 95.3125, f1: 93.47708894878706, r: 0.6882578191694849
06/02/2019 11:02:55 step: 7864, epoch: 238, batch: 9, loss: 0.09009381383657455, acc: 96.875, f1: 92.74684735554301, r: 0.690819168332974
06/02/2019 11:02:55 step: 7869, epoch: 238, batch: 14, loss: 0.12253884226083755, acc: 93.75, f1: 92.46261999494759, r: 0.7268317113480147
06/02/2019 11:02:55 step: 7874, epoch: 238, batch: 19, loss: 0.1548476368188858, acc: 95.3125, f1: 95.20326416878142, r: 0.7136448042911363
06/02/2019 11:02:56 step: 7879, epoch: 238, batch: 24, loss: 0.13141360878944397, acc: 95.3125, f1: 94.69758416186987, r: 0.7214864817465735
06/02/2019 11:02:56 step: 7884, epoch: 238, batch: 29, loss: 0.12359721958637238, acc: 93.75, f1: 92.97464405695088, r: 0.7731511818154544
06/02/2019 11:02:56 *** evaluating ***
06/02/2019 11:02:57 step: 239, epoch: 238, acc: 59.82905982905983, f1: 30.1764108906966, r: 0.33795536137299886
06/02/2019 11:02:57 *** epoch: 240 ***
06/02/2019 11:02:57 *** training ***
06/02/2019 11:02:57 step: 7892, epoch: 239, batch: 4, loss: 0.03471294790506363, acc: 98.4375, f1: 96.30252100840336, r: 0.7102085785965391
06/02/2019 11:02:58 step: 7897, epoch: 239, batch: 9, loss: 0.1389467418193817, acc: 95.3125, f1: 93.66522366522368, r: 0.7818426735721673
06/02/2019 11:02:58 step: 7902, epoch: 239, batch: 14, loss: 0.10749445855617523, acc: 96.875, f1: 95.98692810457517, r: 0.778064157710329
06/02/2019 11:02:58 step: 7907, epoch: 239, batch: 19, loss: 0.11034637689590454, acc: 93.75, f1: 95.80990596528888, r: 0.7048216633485885
06/02/2019 11:02:59 step: 7912, epoch: 239, batch: 24, loss: 0.04014869034290314, acc: 98.4375, f1: 99.27319712509085, r: 0.6054241392804844
06/02/2019 11:02:59 step: 7917, epoch: 239, batch: 29, loss: 0.03885896876454353, acc: 100.0, f1: 100.0, r: 0.7963740980763863
06/02/2019 11:03:00 *** evaluating ***
06/02/2019 11:03:00 step: 240, epoch: 239, acc: 60.256410256410255, f1: 29.849111414900886, r: 0.3426819260297183
06/02/2019 11:03:00 *** epoch: 241 ***
06/02/2019 11:03:00 *** training ***
06/02/2019 11:03:00 step: 7925, epoch: 240, batch: 4, loss: 0.06492461264133453, acc: 98.4375, f1: 98.80261248185776, r: 0.7936863487731233
06/02/2019 11:03:01 step: 7930, epoch: 240, batch: 9, loss: 0.13281933963298798, acc: 95.3125, f1: 97.52228576234127, r: 0.7791175913056267
06/02/2019 11:03:01 step: 7935, epoch: 240, batch: 14, loss: 0.07190026342868805, acc: 95.3125, f1: 90.22108843537416, r: 0.702978262927794
06/02/2019 11:03:01 step: 7940, epoch: 240, batch: 19, loss: 0.11924970895051956, acc: 96.875, f1: 94.579802259887, r: 0.7617535158546258
06/02/2019 11:03:02 step: 7945, epoch: 240, batch: 24, loss: 0.08193234354257584, acc: 98.4375, f1: 87.24489795918367, r: 0.7179820818342465
06/02/2019 11:03:02 step: 7950, epoch: 240, batch: 29, loss: 0.07110821455717087, acc: 96.875, f1: 88.18474758324382, r: 0.6244584622942967
06/02/2019 11:03:02 *** evaluating ***
06/02/2019 11:03:03 step: 241, epoch: 240, acc: 60.256410256410255, f1: 27.72392869142095, r: 0.3511721182905378
06/02/2019 11:03:03 *** epoch: 242 ***
06/02/2019 11:03:03 *** training ***
06/02/2019 11:03:03 step: 7958, epoch: 241, batch: 4, loss: 0.0760999321937561, acc: 98.4375, f1: 98.60955371159451, r: 0.6612854867600116
06/02/2019 11:03:03 step: 7963, epoch: 241, batch: 9, loss: 0.18604567646980286, acc: 92.1875, f1: 90.03900656333627, r: 0.6882995468569969
06/02/2019 11:03:04 step: 7968, epoch: 241, batch: 14, loss: 0.048254843801259995, acc: 98.4375, f1: 95.28985507246377, r: 0.8036937269086104
06/02/2019 11:03:04 step: 7973, epoch: 241, batch: 19, loss: 0.0746898278594017, acc: 98.4375, f1: 99.26739926739927, r: 0.6848677107759227
06/02/2019 11:03:05 step: 7978, epoch: 241, batch: 24, loss: 0.1440577656030655, acc: 92.1875, f1: 89.49077610842318, r: 0.7688727003922831
06/02/2019 11:03:05 step: 7983, epoch: 241, batch: 29, loss: 0.13109171390533447, acc: 96.875, f1: 96.80104830979438, r: 0.7179330762433274
06/02/2019 11:03:05 *** evaluating ***
06/02/2019 11:03:05 step: 242, epoch: 241, acc: 59.82905982905983, f1: 30.495263052810216, r: 0.3430847848443871
06/02/2019 11:03:05 *** epoch: 243 ***
06/02/2019 11:03:05 *** training ***
06/02/2019 11:03:06 step: 7991, epoch: 242, batch: 4, loss: 0.11571966111660004, acc: 92.1875, f1: 91.11625216888376, r: 0.6999858949610015
06/02/2019 11:03:06 step: 7996, epoch: 242, batch: 9, loss: 0.05308591574430466, acc: 96.875, f1: 95.70428560317336, r: 0.7910941940182337
06/02/2019 11:03:06 step: 8001, epoch: 242, batch: 14, loss: 0.11091834306716919, acc: 93.75, f1: 89.30103291316527, r: 0.7725217343390368
06/02/2019 11:03:07 step: 8006, epoch: 242, batch: 19, loss: 0.1830088198184967, acc: 96.875, f1: 94.53514739229026, r: 0.6599534300668952
06/02/2019 11:03:07 step: 8011, epoch: 242, batch: 24, loss: 0.0903247743844986, acc: 95.3125, f1: 95.5952380952381, r: 0.6654323010055946
06/02/2019 11:03:08 step: 8016, epoch: 242, batch: 29, loss: 0.03903178870677948, acc: 98.4375, f1: 98.59714753331775, r: 0.6251766983837704
06/02/2019 11:03:08 *** evaluating ***
06/02/2019 11:03:08 step: 243, epoch: 242, acc: 58.54700854700855, f1: 30.27403232511592, r: 0.33659166507645316
06/02/2019 11:03:08 *** epoch: 244 ***
06/02/2019 11:03:08 *** training ***
06/02/2019 11:03:09 step: 8024, epoch: 243, batch: 4, loss: 0.03308457136154175, acc: 98.4375, f1: 97.46031746031746, r: 0.7066822552503003
06/02/2019 11:03:09 step: 8029, epoch: 243, batch: 9, loss: 0.09529387205839157, acc: 95.3125, f1: 93.58465608465609, r: 0.7132646668131948
06/02/2019 11:03:09 step: 8034, epoch: 243, batch: 14, loss: 0.08201336115598679, acc: 96.875, f1: 97.21576668945089, r: 0.7257219231760377
06/02/2019 11:03:10 step: 8039, epoch: 243, batch: 19, loss: 0.05217542126774788, acc: 100.0, f1: 100.0, r: 0.7271837433576285
06/02/2019 11:03:10 step: 8044, epoch: 243, batch: 24, loss: 0.05709744989871979, acc: 98.4375, f1: 97.67907162865146, r: 0.6880711991906319
06/02/2019 11:03:11 step: 8049, epoch: 243, batch: 29, loss: 0.10745129734277725, acc: 98.4375, f1: 96.90866510538642, r: 0.6836447104810364
06/02/2019 11:03:11 *** evaluating ***
06/02/2019 11:03:11 step: 244, epoch: 243, acc: 60.256410256410255, f1: 30.050998163183394, r: 0.34331033046811554
06/02/2019 11:03:11 *** epoch: 245 ***
06/02/2019 11:03:11 *** training ***
06/02/2019 11:03:12 step: 8057, epoch: 244, batch: 4, loss: 0.034961216151714325, acc: 98.4375, f1: 98.38056680161944, r: 0.7585648830272471
06/02/2019 11:03:12 step: 8062, epoch: 244, batch: 9, loss: 0.02807874232530594, acc: 100.0, f1: 100.0, r: 0.7573718884176941
06/02/2019 11:03:12 step: 8067, epoch: 244, batch: 14, loss: 0.1467602699995041, acc: 95.3125, f1: 83.13755210306934, r: 0.5676180556552157
06/02/2019 11:03:13 step: 8072, epoch: 244, batch: 19, loss: 0.12930497527122498, acc: 95.3125, f1: 78.36178861788618, r: 0.6928776991376507
06/02/2019 11:03:13 step: 8077, epoch: 244, batch: 24, loss: 0.10104753822088242, acc: 95.3125, f1: 92.31591077776476, r: 0.7135728133762037
06/02/2019 11:03:14 step: 8082, epoch: 244, batch: 29, loss: 0.0785723626613617, acc: 96.875, f1: 98.18840579710145, r: 0.7235815624732699
06/02/2019 11:03:14 *** evaluating ***
06/02/2019 11:03:14 step: 245, epoch: 244, acc: 59.401709401709404, f1: 29.313334830787664, r: 0.34168618984647925
06/02/2019 11:03:14 *** epoch: 246 ***
06/02/2019 11:03:14 *** training ***
06/02/2019 11:03:15 step: 8090, epoch: 245, batch: 4, loss: 0.06606651097536087, acc: 98.4375, f1: 96.8602825745683, r: 0.6897163407391691
06/02/2019 11:03:15 step: 8095, epoch: 245, batch: 9, loss: 0.08742043375968933, acc: 96.875, f1: 80.71428571428572, r: 0.7104069832380007
06/02/2019 11:03:15 step: 8100, epoch: 245, batch: 14, loss: 0.08733247220516205, acc: 98.4375, f1: 98.47619047619048, r: 0.6695515515666645
06/02/2019 11:03:16 step: 8105, epoch: 245, batch: 19, loss: 0.040612973272800446, acc: 100.0, f1: 100.0, r: 0.6645613020375036
06/02/2019 11:03:16 step: 8110, epoch: 245, batch: 24, loss: 0.10453741997480392, acc: 95.3125, f1: 95.42165071770336, r: 0.7566054237825315
06/02/2019 11:03:17 step: 8115, epoch: 245, batch: 29, loss: 0.07604643702507019, acc: 95.3125, f1: 97.57236227824464, r: 0.6487043588518845
06/02/2019 11:03:17 *** evaluating ***
06/02/2019 11:03:17 step: 246, epoch: 245, acc: 59.401709401709404, f1: 29.897646613829597, r: 0.3381227147155179
06/02/2019 11:03:17 *** epoch: 247 ***
06/02/2019 11:03:17 *** training ***
06/02/2019 11:03:18 step: 8123, epoch: 246, batch: 4, loss: 0.23761461675167084, acc: 93.75, f1: 92.51377734107997, r: 0.7398720909202398
06/02/2019 11:03:18 step: 8128, epoch: 246, batch: 9, loss: 0.08412160724401474, acc: 98.4375, f1: 96.39097744360903, r: 0.6405747685447991
06/02/2019 11:03:18 step: 8133, epoch: 246, batch: 14, loss: 0.13753864169120789, acc: 93.75, f1: 90.9672619047619, r: 0.7371491071389342
06/02/2019 11:03:19 step: 8138, epoch: 246, batch: 19, loss: 0.08438491821289062, acc: 98.4375, f1: 97.78325123152709, r: 0.6705059713068781
06/02/2019 11:03:19 step: 8143, epoch: 246, batch: 24, loss: 0.05800609663128853, acc: 96.875, f1: 78.90424987199181, r: 0.6338689026260084
06/02/2019 11:03:20 step: 8148, epoch: 246, batch: 29, loss: 0.1714971959590912, acc: 95.3125, f1: 95.00215665976536, r: 0.7867348240424319
06/02/2019 11:03:20 *** evaluating ***
06/02/2019 11:03:20 step: 247, epoch: 246, acc: 60.68376068376068, f1: 30.58186197860111, r: 0.3408339936830171
06/02/2019 11:03:20 *** epoch: 248 ***
06/02/2019 11:03:20 *** training ***
06/02/2019 11:03:21 step: 8156, epoch: 247, batch: 4, loss: 0.1439172327518463, acc: 95.3125, f1: 95.88696928635953, r: 0.5655305620707608
06/02/2019 11:03:21 step: 8161, epoch: 247, batch: 9, loss: 0.07551628351211548, acc: 95.3125, f1: 96.19922969187675, r: 0.7553144197067454
06/02/2019 11:03:21 step: 8166, epoch: 247, batch: 14, loss: 0.16137583553791046, acc: 95.3125, f1: 90.19023860435122, r: 0.6891917494867832
06/02/2019 11:03:22 step: 8171, epoch: 247, batch: 19, loss: 0.10182560980319977, acc: 96.875, f1: 83.46560846560847, r: 0.7389641751476982
06/02/2019 11:03:22 step: 8176, epoch: 247, batch: 24, loss: 0.032778747379779816, acc: 100.0, f1: 100.0, r: 0.720392795347867
06/02/2019 11:03:23 step: 8181, epoch: 247, batch: 29, loss: 0.07563185691833496, acc: 96.875, f1: 88.1756338899196, r: 0.7329211850960408
06/02/2019 11:03:23 *** evaluating ***
06/02/2019 11:03:23 step: 248, epoch: 247, acc: 59.82905982905983, f1: 29.729966329966327, r: 0.3410209255153405
06/02/2019 11:03:23 *** epoch: 249 ***
06/02/2019 11:03:23 *** training ***
06/02/2019 11:03:24 step: 8189, epoch: 248, batch: 4, loss: 0.1156763806939125, acc: 95.3125, f1: 81.78831694621168, r: 0.6561447751682841
06/02/2019 11:03:24 step: 8194, epoch: 248, batch: 9, loss: 0.021771647036075592, acc: 100.0, f1: 100.0, r: 0.6886301711850245
06/02/2019 11:03:24 step: 8199, epoch: 248, batch: 14, loss: 0.19064563512802124, acc: 92.1875, f1: 93.92857142857143, r: 0.6744009391585325
06/02/2019 11:03:25 step: 8204, epoch: 248, batch: 19, loss: 0.04021546244621277, acc: 100.0, f1: 100.0, r: 0.7511294870273867
06/02/2019 11:03:25 step: 8209, epoch: 248, batch: 24, loss: 0.05266633257269859, acc: 96.875, f1: 94.96918767507003, r: 0.7961185303557756
06/02/2019 11:03:26 step: 8214, epoch: 248, batch: 29, loss: 0.07868985831737518, acc: 98.4375, f1: 98.95652173913044, r: 0.6942186668402971
06/02/2019 11:03:26 *** evaluating ***
06/02/2019 11:03:26 step: 249, epoch: 248, acc: 58.97435897435898, f1: 28.45070072204219, r: 0.3374231142019983
06/02/2019 11:03:26 *** epoch: 250 ***
06/02/2019 11:03:26 *** training ***
06/02/2019 11:03:26 step: 8222, epoch: 249, batch: 4, loss: 0.03758709877729416, acc: 98.4375, f1: 99.1951219512195, r: 0.789106432932049
06/02/2019 11:03:27 step: 8227, epoch: 249, batch: 9, loss: 0.0467192679643631, acc: 98.4375, f1: 99.28193499622071, r: 0.8313273648534389
06/02/2019 11:03:27 step: 8232, epoch: 249, batch: 14, loss: 0.05923940986394882, acc: 98.4375, f1: 99.13718723037101, r: 0.7867577985550931
06/02/2019 11:03:28 step: 8237, epoch: 249, batch: 19, loss: 0.08708292245864868, acc: 96.875, f1: 97.28137631864341, r: 0.7079181240956247
06/02/2019 11:03:28 step: 8242, epoch: 249, batch: 24, loss: 0.1945415884256363, acc: 95.3125, f1: 92.97785547785547, r: 0.7074379165201795
06/02/2019 11:03:29 step: 8247, epoch: 249, batch: 29, loss: 0.06723244488239288, acc: 98.4375, f1: 96.70995670995671, r: 0.6331871263512978
06/02/2019 11:03:29 *** evaluating ***
06/02/2019 11:03:29 step: 250, epoch: 249, acc: 58.119658119658126, f1: 28.412804693577186, r: 0.33510577077926146
06/02/2019 11:03:29 *** epoch: 251 ***
06/02/2019 11:03:29 *** training ***
06/02/2019 11:03:29 step: 8255, epoch: 250, batch: 4, loss: 0.11461024731397629, acc: 98.4375, f1: 99.26415094339622, r: 0.7680531769350324
06/02/2019 11:03:30 step: 8260, epoch: 250, batch: 9, loss: 0.16253449022769928, acc: 93.75, f1: 92.61697722567287, r: 0.6806844383047307
06/02/2019 11:03:30 step: 8265, epoch: 250, batch: 14, loss: 0.1239352747797966, acc: 92.1875, f1: 88.08774810589694, r: 0.6884578812181815
06/02/2019 11:03:31 step: 8270, epoch: 250, batch: 19, loss: 0.09240864217281342, acc: 96.875, f1: 97.89122754639996, r: 0.7420615941340147
06/02/2019 11:03:31 step: 8275, epoch: 250, batch: 24, loss: 0.14150778949260712, acc: 95.3125, f1: 82.57140633883114, r: 0.7452546170159845
06/02/2019 11:03:31 step: 8280, epoch: 250, batch: 29, loss: 0.08607359975576401, acc: 96.875, f1: 92.05698672911787, r: 0.6991270111427996
06/02/2019 11:03:32 *** evaluating ***
06/02/2019 11:03:32 step: 251, epoch: 250, acc: 58.54700854700855, f1: 28.40524904692343, r: 0.33828428111692505
06/02/2019 11:03:32 *** epoch: 252 ***
06/02/2019 11:03:32 *** training ***
06/02/2019 11:03:32 step: 8288, epoch: 251, batch: 4, loss: 0.12874755263328552, acc: 92.1875, f1: 87.99239417989419, r: 0.7833292870961591
06/02/2019 11:03:33 step: 8293, epoch: 251, batch: 9, loss: 0.07732198387384415, acc: 95.3125, f1: 87.62311762311762, r: 0.7629330570091691
06/02/2019 11:03:33 step: 8298, epoch: 251, batch: 14, loss: 0.07991743087768555, acc: 96.875, f1: 95.31849103277675, r: 0.7056127627830916
06/02/2019 11:03:34 step: 8303, epoch: 251, batch: 19, loss: 0.035443015396595, acc: 100.0, f1: 100.0, r: 0.7532109626664085
06/02/2019 11:03:34 step: 8308, epoch: 251, batch: 24, loss: 0.04190826043486595, acc: 100.0, f1: 100.0, r: 0.6061722297791425
06/02/2019 11:03:35 step: 8313, epoch: 251, batch: 29, loss: 0.11850067228078842, acc: 96.875, f1: 98.59375, r: 0.7672096007470703
06/02/2019 11:03:35 *** evaluating ***
06/02/2019 11:03:35 step: 252, epoch: 251, acc: 57.692307692307686, f1: 29.37308018292829, r: 0.33892456376365626
06/02/2019 11:03:35 *** epoch: 253 ***
06/02/2019 11:03:35 *** training ***
06/02/2019 11:03:35 step: 8321, epoch: 252, batch: 4, loss: 0.061471544206142426, acc: 98.4375, f1: 98.10066476733144, r: 0.6463356771904212
06/02/2019 11:03:36 step: 8326, epoch: 252, batch: 9, loss: 0.0896371379494667, acc: 95.3125, f1: 95.18849206349206, r: 0.7467447170996645
06/02/2019 11:03:36 step: 8331, epoch: 252, batch: 14, loss: 0.06141373887658119, acc: 96.875, f1: 98.30043859649122, r: 0.801972686143944
06/02/2019 11:03:37 step: 8336, epoch: 252, batch: 19, loss: 0.10729275643825531, acc: 96.875, f1: 96.34059897217792, r: 0.6471459240933467
06/02/2019 11:03:37 step: 8341, epoch: 252, batch: 24, loss: 0.03513325750827789, acc: 98.4375, f1: 97.22222222222221, r: 0.8104211006304305
06/02/2019 11:03:37 step: 8346, epoch: 252, batch: 29, loss: 0.08820625394582748, acc: 96.875, f1: 95.49194991055457, r: 0.7023541308993249
06/02/2019 11:03:38 *** evaluating ***
06/02/2019 11:03:38 step: 253, epoch: 252, acc: 58.97435897435898, f1: 28.833428045737385, r: 0.3406437660492279
06/02/2019 11:03:38 *** epoch: 254 ***
06/02/2019 11:03:38 *** training ***
06/02/2019 11:03:38 step: 8354, epoch: 253, batch: 4, loss: 0.054686687886714935, acc: 100.0, f1: 100.0, r: 0.7319073713793073
06/02/2019 11:03:39 step: 8359, epoch: 253, batch: 9, loss: 0.08357970416545868, acc: 96.875, f1: 98.22866344605474, r: 0.6906301467285875
06/02/2019 11:03:39 step: 8364, epoch: 253, batch: 14, loss: 0.046648941934108734, acc: 98.4375, f1: 98.86811867604185, r: 0.7117065880821594
06/02/2019 11:03:40 step: 8369, epoch: 253, batch: 19, loss: 0.03078995645046234, acc: 98.4375, f1: 96.75675675675676, r: 0.75230577501888
06/02/2019 11:03:40 step: 8374, epoch: 253, batch: 24, loss: 0.04890003055334091, acc: 98.4375, f1: 98.01587301587303, r: 0.8127470603200252
06/02/2019 11:03:40 step: 8379, epoch: 253, batch: 29, loss: 0.03618228808045387, acc: 100.0, f1: 100.0, r: 0.7085314697282781
06/02/2019 11:03:41 *** evaluating ***
06/02/2019 11:03:41 step: 254, epoch: 253, acc: 60.68376068376068, f1: 29.926020610916986, r: 0.34409618559946353
06/02/2019 11:03:41 *** epoch: 255 ***
06/02/2019 11:03:41 *** training ***
06/02/2019 11:03:41 step: 8387, epoch: 254, batch: 4, loss: 0.03309778496623039, acc: 100.0, f1: 100.0, r: 0.7411748786571632
06/02/2019 11:03:42 step: 8392, epoch: 254, batch: 9, loss: 0.07261165231466293, acc: 96.875, f1: 85.31253080942523, r: 0.8141599047309596
06/02/2019 11:03:42 step: 8397, epoch: 254, batch: 14, loss: 0.1863168478012085, acc: 95.3125, f1: 97.03629703629704, r: 0.6931555357295824
06/02/2019 11:03:42 step: 8402, epoch: 254, batch: 19, loss: 0.047549642622470856, acc: 98.4375, f1: 97.47899159663866, r: 0.7555768252445795
06/02/2019 11:03:43 step: 8407, epoch: 254, batch: 24, loss: 0.17562894523143768, acc: 93.75, f1: 91.83364254792826, r: 0.6988581024378286
06/02/2019 11:03:43 step: 8412, epoch: 254, batch: 29, loss: 0.08874814957380295, acc: 96.875, f1: 96.11658456486043, r: 0.7652071271304851
06/02/2019 11:03:43 *** evaluating ***
06/02/2019 11:03:44 step: 255, epoch: 254, acc: 59.82905982905983, f1: 28.67111348496335, r: 0.33737711098690903
06/02/2019 11:03:44 *** epoch: 256 ***
06/02/2019 11:03:44 *** training ***
06/02/2019 11:03:44 step: 8420, epoch: 255, batch: 4, loss: 0.03622820973396301, acc: 98.4375, f1: 98.98838004101161, r: 0.7000162830482126
06/02/2019 11:03:45 step: 8425, epoch: 255, batch: 9, loss: 0.049167878925800323, acc: 98.4375, f1: 96.42857142857143, r: 0.795600053607781
06/02/2019 11:03:45 step: 8430, epoch: 255, batch: 14, loss: 0.35714033246040344, acc: 92.1875, f1: 87.47596263385738, r: 0.6665352917410803
06/02/2019 11:03:45 step: 8435, epoch: 255, batch: 19, loss: 0.0578845776617527, acc: 96.875, f1: 95.63503028548622, r: 0.7707100585425862
06/02/2019 11:03:46 step: 8440, epoch: 255, batch: 24, loss: 0.11317029595375061, acc: 98.4375, f1: 99.10934020860189, r: 0.688212333689084
06/02/2019 11:03:46 step: 8445, epoch: 255, batch: 29, loss: 0.03718678653240204, acc: 98.4375, f1: 99.1386735572782, r: 0.7487950181867548
06/02/2019 11:03:47 *** evaluating ***
06/02/2019 11:03:47 step: 256, epoch: 255, acc: 59.82905982905983, f1: 28.639895929615555, r: 0.3339601203698854
06/02/2019 11:03:47 *** epoch: 257 ***
06/02/2019 11:03:47 *** training ***
06/02/2019 11:03:47 step: 8453, epoch: 256, batch: 4, loss: 0.2717975080013275, acc: 90.625, f1: 87.54788614163614, r: 0.670370594216869
06/02/2019 11:03:48 step: 8458, epoch: 256, batch: 9, loss: 0.050168219953775406, acc: 98.4375, f1: 86.53846153846155, r: 0.7310966682692758
06/02/2019 11:03:48 step: 8463, epoch: 256, batch: 14, loss: 0.07653734087944031, acc: 95.3125, f1: 93.24032005146556, r: 0.6914648998236252
06/02/2019 11:03:48 step: 8468, epoch: 256, batch: 19, loss: 0.14158672094345093, acc: 93.75, f1: 87.31240981240981, r: 0.8118624001469099
06/02/2019 11:03:49 step: 8473, epoch: 256, batch: 24, loss: 0.2399909794330597, acc: 92.1875, f1: 91.7285390346615, r: 0.6453578242691955
06/02/2019 11:03:49 step: 8478, epoch: 256, batch: 29, loss: 0.09820984303951263, acc: 96.875, f1: 97.23977297065754, r: 0.7615470308373907
06/02/2019 11:03:49 *** evaluating ***
06/02/2019 11:03:50 step: 257, epoch: 256, acc: 59.401709401709404, f1: 28.426118567422908, r: 0.334989347633126
06/02/2019 11:03:50 *** epoch: 258 ***
06/02/2019 11:03:50 *** training ***
06/02/2019 11:03:50 step: 8486, epoch: 257, batch: 4, loss: 0.13109198212623596, acc: 95.3125, f1: 95.3482117484675, r: 0.8013784931782654
06/02/2019 11:03:50 step: 8491, epoch: 257, batch: 9, loss: 0.14040657877922058, acc: 96.875, f1: 94.68864468864469, r: 0.6750676718194883
06/02/2019 11:03:51 step: 8496, epoch: 257, batch: 14, loss: 0.08557787537574768, acc: 96.875, f1: 93.19727891156461, r: 0.6537643729875809
06/02/2019 11:03:51 step: 8501, epoch: 257, batch: 19, loss: 0.08897745609283447, acc: 96.875, f1: 84.8530901722391, r: 0.8222577954730531
06/02/2019 11:03:52 step: 8506, epoch: 257, batch: 24, loss: 0.22438162565231323, acc: 92.1875, f1: 89.38056680161944, r: 0.7415776778014136
06/02/2019 11:03:52 step: 8511, epoch: 257, batch: 29, loss: 0.02427244745194912, acc: 100.0, f1: 100.0, r: 0.7760766566858964
06/02/2019 11:03:52 *** evaluating ***
06/02/2019 11:03:53 step: 258, epoch: 257, acc: 59.82905982905983, f1: 27.81922488666594, r: 0.3364232522703917
06/02/2019 11:03:53 *** epoch: 259 ***
06/02/2019 11:03:53 *** training ***
06/02/2019 11:03:53 step: 8519, epoch: 258, batch: 4, loss: 0.12736192345619202, acc: 95.3125, f1: 89.48412698412697, r: 0.555215189028056
06/02/2019 11:03:53 step: 8524, epoch: 258, batch: 9, loss: 0.10248420387506485, acc: 95.3125, f1: 82.44047619047619, r: 0.7059893770293698
06/02/2019 11:03:54 step: 8529, epoch: 258, batch: 14, loss: 0.054831363260746, acc: 98.4375, f1: 98.01587301587301, r: 0.7325291234638598
06/02/2019 11:03:54 step: 8534, epoch: 258, batch: 19, loss: 0.1418558657169342, acc: 92.1875, f1: 86.46857923497268, r: 0.7328273027762429
06/02/2019 11:03:54 step: 8539, epoch: 258, batch: 24, loss: 0.12079192698001862, acc: 96.875, f1: 95.70428560317336, r: 0.7861695273114473
06/02/2019 11:03:55 step: 8544, epoch: 258, batch: 29, loss: 0.050274889916181564, acc: 98.4375, f1: 98.82882882882883, r: 0.7861178400016573
06/02/2019 11:03:55 *** evaluating ***
06/02/2019 11:03:55 step: 259, epoch: 258, acc: 58.119658119658126, f1: 28.865269746079335, r: 0.33806827161720576
06/02/2019 11:03:55 *** epoch: 260 ***
06/02/2019 11:03:55 *** training ***
06/02/2019 11:03:56 step: 8552, epoch: 259, batch: 4, loss: 0.12922903895378113, acc: 95.3125, f1: 89.14502164502164, r: 0.7456113861475744
06/02/2019 11:03:56 step: 8557, epoch: 259, batch: 9, loss: 0.20233407616615295, acc: 90.625, f1: 88.13171645469782, r: 0.6299879143384342
06/02/2019 11:03:57 step: 8562, epoch: 259, batch: 14, loss: 0.04116414487361908, acc: 98.4375, f1: 96.65024630541872, r: 0.6840570966004125
06/02/2019 11:03:57 step: 8567, epoch: 259, batch: 19, loss: 0.1920951008796692, acc: 92.1875, f1: 92.3506969221255, r: 0.6769230407350015
06/02/2019 11:03:57 step: 8572, epoch: 259, batch: 24, loss: 0.12174269556999207, acc: 96.875, f1: 95.88825694212112, r: 0.6592166315781375
06/02/2019 11:03:58 step: 8577, epoch: 259, batch: 29, loss: 0.07248321175575256, acc: 96.875, f1: 97.97020022583932, r: 0.6546027767145981
06/02/2019 11:03:58 *** evaluating ***
06/02/2019 11:03:58 step: 260, epoch: 259, acc: 59.401709401709404, f1: 29.105930047195113, r: 0.3312343410240418
06/02/2019 11:03:58 *** epoch: 261 ***
06/02/2019 11:03:58 *** training ***
06/02/2019 11:03:59 step: 8585, epoch: 260, batch: 4, loss: 0.08797553181648254, acc: 96.875, f1: 96.95391414141415, r: 0.7747033375448847
06/02/2019 11:03:59 step: 8590, epoch: 260, batch: 9, loss: 0.22459538280963898, acc: 95.3125, f1: 89.35491437954492, r: 0.5741390663863314
06/02/2019 11:03:59 step: 8595, epoch: 260, batch: 14, loss: 0.0550825297832489, acc: 98.4375, f1: 98.33499833499833, r: 0.7097425242322268
06/02/2019 11:04:00 step: 8600, epoch: 260, batch: 19, loss: 0.06318527460098267, acc: 96.875, f1: 96.8806325949183, r: 0.6140398464959983
06/02/2019 11:04:00 step: 8605, epoch: 260, batch: 24, loss: 0.05358929559588432, acc: 98.4375, f1: 95.40229885057472, r: 0.7494579596913108
06/02/2019 11:04:01 step: 8610, epoch: 260, batch: 29, loss: 0.0315413698554039, acc: 100.0, f1: 100.0, r: 0.6731390229135441
06/02/2019 11:04:01 *** evaluating ***
06/02/2019 11:04:01 step: 261, epoch: 260, acc: 58.119658119658126, f1: 28.26883292460542, r: 0.325083171899219
06/02/2019 11:04:01 *** epoch: 262 ***
06/02/2019 11:04:01 *** training ***
06/02/2019 11:04:01 step: 8618, epoch: 261, batch: 4, loss: 0.05355549976229668, acc: 96.875, f1: 97.4020979020979, r: 0.8239670727574693
06/02/2019 11:04:02 step: 8623, epoch: 261, batch: 9, loss: 0.13462573289871216, acc: 92.1875, f1: 74.35939636400465, r: 0.6826354012459879
06/02/2019 11:04:02 step: 8628, epoch: 261, batch: 14, loss: 0.07500702142715454, acc: 98.4375, f1: 97.62695775984812, r: 0.6914148576087976
06/02/2019 11:04:02 step: 8633, epoch: 261, batch: 19, loss: 0.08201081305742264, acc: 96.875, f1: 98.53896103896105, r: 0.7656968767343237
06/02/2019 11:04:03 step: 8638, epoch: 261, batch: 24, loss: 0.10098327696323395, acc: 98.4375, f1: 98.89992360580597, r: 0.6836747062395966
06/02/2019 11:04:03 step: 8643, epoch: 261, batch: 29, loss: 0.10836982727050781, acc: 98.4375, f1: 94.70899470899471, r: 0.6628471641324929
06/02/2019 11:04:04 *** evaluating ***
06/02/2019 11:04:04 step: 262, epoch: 261, acc: 60.256410256410255, f1: 29.079441843462327, r: 0.3211187365308115
06/02/2019 11:04:04 *** epoch: 263 ***
06/02/2019 11:04:04 *** training ***
06/02/2019 11:04:04 step: 8651, epoch: 262, batch: 4, loss: 0.07010085135698318, acc: 96.875, f1: 97.19461697722566, r: 0.7567142485488163
06/02/2019 11:04:05 step: 8656, epoch: 262, batch: 9, loss: 0.08282769471406937, acc: 96.875, f1: 97.46959389816533, r: 0.6261968055870502
06/02/2019 11:04:05 step: 8661, epoch: 262, batch: 14, loss: 0.07791818678379059, acc: 95.3125, f1: 96.90012020133724, r: 0.7846402375651472
06/02/2019 11:04:05 step: 8666, epoch: 262, batch: 19, loss: 0.10479804873466492, acc: 95.3125, f1: 96.64324162679425, r: 0.7327858890114048
06/02/2019 11:04:06 step: 8671, epoch: 262, batch: 24, loss: 0.08014734834432602, acc: 95.3125, f1: 95.80901177675372, r: 0.6804206175146786
06/02/2019 11:04:06 step: 8676, epoch: 262, batch: 29, loss: 0.056479018181562424, acc: 98.4375, f1: 96.83890577507599, r: 0.6487668757820391
06/02/2019 11:04:07 *** evaluating ***
06/02/2019 11:04:07 step: 263, epoch: 262, acc: 58.97435897435898, f1: 29.328874478140165, r: 0.3249950444970051
06/02/2019 11:04:07 *** epoch: 264 ***
06/02/2019 11:04:07 *** training ***
06/02/2019 11:04:07 step: 8684, epoch: 263, batch: 4, loss: 0.03375925123691559, acc: 100.0, f1: 100.0, r: 0.6634209913187822
06/02/2019 11:04:08 step: 8689, epoch: 263, batch: 9, loss: 0.06666399538516998, acc: 96.875, f1: 96.40439094309721, r: 0.7985568149586347
06/02/2019 11:04:08 step: 8694, epoch: 263, batch: 14, loss: 0.06438352167606354, acc: 98.4375, f1: 97.09677419354838, r: 0.8260503089442156
06/02/2019 11:04:08 step: 8699, epoch: 263, batch: 19, loss: 0.08122654259204865, acc: 98.4375, f1: 97.55639097744361, r: 0.7563314166360442
06/02/2019 11:04:09 step: 8704, epoch: 263, batch: 24, loss: 0.2519529163837433, acc: 92.1875, f1: 80.59123649459784, r: 0.6270071415379195
06/02/2019 11:04:09 step: 8709, epoch: 263, batch: 29, loss: 0.03508273512125015, acc: 98.4375, f1: 95.40229885057472, r: 0.7727837800300243
06/02/2019 11:04:09 *** evaluating ***
06/02/2019 11:04:10 step: 264, epoch: 263, acc: 58.97435897435898, f1: 29.41621315192744, r: 0.32918527047870094
06/02/2019 11:04:10 *** epoch: 265 ***
06/02/2019 11:04:10 *** training ***
06/02/2019 11:04:10 step: 8717, epoch: 264, batch: 4, loss: 0.08597226440906525, acc: 98.4375, f1: 98.26839826839827, r: 0.6681123768934168
06/02/2019 11:04:10 step: 8722, epoch: 264, batch: 9, loss: 0.08198094367980957, acc: 96.875, f1: 98.13523573200993, r: 0.7641977837034684
06/02/2019 11:04:11 step: 8727, epoch: 264, batch: 14, loss: 0.052450694143772125, acc: 96.875, f1: 92.84049284049284, r: 0.6694702751538217
06/02/2019 11:04:11 step: 8732, epoch: 264, batch: 19, loss: 0.18865090608596802, acc: 93.75, f1: 94.89407651172357, r: 0.7178809429427455
06/02/2019 11:04:12 step: 8737, epoch: 264, batch: 24, loss: 0.0769597589969635, acc: 96.875, f1: 96.46475611521204, r: 0.7136535172931838
06/02/2019 11:04:12 step: 8742, epoch: 264, batch: 29, loss: 0.05716535449028015, acc: 98.4375, f1: 99.27826784282277, r: 0.7258960800561087
06/02/2019 11:04:12 *** evaluating ***
06/02/2019 11:04:13 step: 265, epoch: 264, acc: 58.119658119658126, f1: 29.580368572307435, r: 0.32414275294185524
06/02/2019 11:04:13 *** epoch: 266 ***
06/02/2019 11:04:13 *** training ***
06/02/2019 11:04:13 step: 8750, epoch: 265, batch: 4, loss: 0.04027184471487999, acc: 100.0, f1: 100.0, r: 0.7699355386281718
06/02/2019 11:04:13 step: 8755, epoch: 265, batch: 9, loss: 0.029540371149778366, acc: 100.0, f1: 100.0, r: 0.7207464405987465
06/02/2019 11:04:14 step: 8760, epoch: 265, batch: 14, loss: 0.144683837890625, acc: 96.875, f1: 96.49197860962568, r: 0.7216520463509122
06/02/2019 11:04:14 step: 8765, epoch: 265, batch: 19, loss: 0.047112151980400085, acc: 98.4375, f1: 86.95652173913044, r: 0.6983677741849419
06/02/2019 11:04:14 step: 8770, epoch: 265, batch: 24, loss: 0.10876148194074631, acc: 96.875, f1: 94.32393693263259, r: 0.6908826253107171
06/02/2019 11:04:15 step: 8775, epoch: 265, batch: 29, loss: 0.17908352613449097, acc: 90.625, f1: 90.68061568061569, r: 0.7820174615695115
06/02/2019 11:04:15 *** evaluating ***
06/02/2019 11:04:15 step: 266, epoch: 265, acc: 58.119658119658126, f1: 25.03042181193358, r: 0.3274858654923883
06/02/2019 11:04:15 *** epoch: 267 ***
06/02/2019 11:04:15 *** training ***
06/02/2019 11:04:15 step: 8783, epoch: 266, batch: 4, loss: 0.14364005625247955, acc: 93.75, f1: 91.17405582922825, r: 0.6751570240840983
06/02/2019 11:04:16 step: 8788, epoch: 266, batch: 9, loss: 0.1177462488412857, acc: 96.875, f1: 95.9469696969697, r: 0.8371064778392062
06/02/2019 11:04:16 step: 8793, epoch: 266, batch: 14, loss: 0.077286496758461, acc: 96.875, f1: 96.93676999101528, r: 0.7962520335525282
06/02/2019 11:04:17 step: 8798, epoch: 266, batch: 19, loss: 0.23928844928741455, acc: 93.75, f1: 95.06953498478921, r: 0.7764087419992749
06/02/2019 11:04:17 step: 8803, epoch: 266, batch: 24, loss: 0.06751170754432678, acc: 96.875, f1: 83.80952380952381, r: 0.7063806114758391
06/02/2019 11:04:17 step: 8808, epoch: 266, batch: 29, loss: 0.14338336884975433, acc: 96.875, f1: 82.52623083131559, r: 0.6209903758389148
06/02/2019 11:04:17 *** evaluating ***
06/02/2019 11:04:18 step: 267, epoch: 266, acc: 58.97435897435898, f1: 29.34266990870764, r: 0.3287892185286464
06/02/2019 11:04:18 *** epoch: 268 ***
06/02/2019 11:04:18 *** training ***
06/02/2019 11:04:18 step: 8816, epoch: 267, batch: 4, loss: 0.0895705372095108, acc: 96.875, f1: 97.58739468416888, r: 0.8213877260538096
06/02/2019 11:04:19 step: 8821, epoch: 267, batch: 9, loss: 0.1455879807472229, acc: 93.75, f1: 92.92399559504823, r: 0.7243615639440748
06/02/2019 11:04:19 step: 8826, epoch: 267, batch: 14, loss: 0.052821505814790726, acc: 98.4375, f1: 85.09316770186335, r: 0.6803850265861298
06/02/2019 11:04:19 step: 8831, epoch: 267, batch: 19, loss: 0.035152893513441086, acc: 100.0, f1: 100.0, r: 0.8182616126823851
06/02/2019 11:04:20 step: 8836, epoch: 267, batch: 24, loss: 0.10575170814990997, acc: 93.75, f1: 96.22727272727273, r: 0.8130981331859883
06/02/2019 11:04:20 step: 8841, epoch: 267, batch: 29, loss: 0.09104759991168976, acc: 95.3125, f1: 97.03970070102793, r: 0.775401470942249
06/02/2019 11:04:21 *** evaluating ***
06/02/2019 11:04:21 step: 268, epoch: 267, acc: 59.401709401709404, f1: 29.93583522513364, r: 0.33204458651627106
06/02/2019 11:04:21 *** epoch: 269 ***
06/02/2019 11:04:21 *** training ***
06/02/2019 11:04:21 step: 8849, epoch: 268, batch: 4, loss: 0.05660956725478172, acc: 98.4375, f1: 99.15910176779742, r: 0.6589941532506809
06/02/2019 11:04:22 step: 8854, epoch: 268, batch: 9, loss: 0.07818685472011566, acc: 96.875, f1: 98.36807928913193, r: 0.7223682998341058
06/02/2019 11:04:22 step: 8859, epoch: 268, batch: 14, loss: 0.10196691006422043, acc: 95.3125, f1: 97.09519787644787, r: 0.8617066362594238
06/02/2019 11:04:22 step: 8864, epoch: 268, batch: 19, loss: 0.10301473736763, acc: 96.875, f1: 95.8852258852259, r: 0.6724039296726366
06/02/2019 11:04:23 step: 8869, epoch: 268, batch: 24, loss: 0.036481793969869614, acc: 100.0, f1: 100.0, r: 0.8336016023931943
06/02/2019 11:04:23 step: 8874, epoch: 268, batch: 29, loss: 0.11519704759120941, acc: 96.875, f1: 96.60654959658281, r: 0.7135132330470516
06/02/2019 11:04:23 *** evaluating ***
06/02/2019 11:04:24 step: 269, epoch: 268, acc: 58.97435897435898, f1: 29.995704440417136, r: 0.33394380261538004
06/02/2019 11:04:24 *** epoch: 270 ***
06/02/2019 11:04:24 *** training ***
06/02/2019 11:04:24 step: 8882, epoch: 269, batch: 4, loss: 0.06497201323509216, acc: 98.4375, f1: 99.34472934472934, r: 0.7586044095661588
06/02/2019 11:04:24 step: 8887, epoch: 269, batch: 9, loss: 0.122553251683712, acc: 95.3125, f1: 83.92789642789643, r: 0.737072453557417
06/02/2019 11:04:25 step: 8892, epoch: 269, batch: 14, loss: 0.08760853111743927, acc: 96.875, f1: 97.07556935817806, r: 0.7737861774917016
06/02/2019 11:04:25 step: 8897, epoch: 269, batch: 19, loss: 0.03440781310200691, acc: 100.0, f1: 100.0, r: 0.7712200769263295
06/02/2019 11:04:26 step: 8902, epoch: 269, batch: 24, loss: 0.10385213047266006, acc: 95.3125, f1: 92.70107552022445, r: 0.7631077372154964
06/02/2019 11:04:26 step: 8907, epoch: 269, batch: 29, loss: 0.048337023705244064, acc: 98.4375, f1: 98.93081761006289, r: 0.750245926010658
06/02/2019 11:04:27 *** evaluating ***
06/02/2019 11:04:27 step: 270, epoch: 269, acc: 59.401709401709404, f1: 30.103875463084723, r: 0.3331666952521344
06/02/2019 11:04:27 *** epoch: 271 ***
06/02/2019 11:04:27 *** training ***
06/02/2019 11:04:27 step: 8915, epoch: 270, batch: 4, loss: 0.08740580826997757, acc: 96.875, f1: 97.15061463181766, r: 0.6443758186673618
06/02/2019 11:04:27 step: 8920, epoch: 270, batch: 9, loss: 0.09157754480838776, acc: 96.875, f1: 85.20811833505331, r: 0.7635560057539417
06/02/2019 11:04:28 step: 8925, epoch: 270, batch: 14, loss: 0.025684108957648277, acc: 100.0, f1: 100.0, r: 0.7958601796915029
06/02/2019 11:04:28 step: 8930, epoch: 270, batch: 19, loss: 0.06647886335849762, acc: 96.875, f1: 95.21711366538953, r: 0.775979593463221
06/02/2019 11:04:29 step: 8935, epoch: 270, batch: 24, loss: 0.056602805852890015, acc: 98.4375, f1: 96.76470588235294, r: 0.7746990560982634
06/02/2019 11:04:29 step: 8940, epoch: 270, batch: 29, loss: 0.13097679615020752, acc: 98.4375, f1: 98.53846153846155, r: 0.8272613788572213
06/02/2019 11:04:29 *** evaluating ***
06/02/2019 11:04:30 step: 271, epoch: 270, acc: 58.97435897435898, f1: 29.248924654545018, r: 0.3353868123842367
06/02/2019 11:04:30 *** epoch: 272 ***
06/02/2019 11:04:30 *** training ***
06/02/2019 11:04:30 step: 8948, epoch: 271, batch: 4, loss: 0.03683547303080559, acc: 100.0, f1: 100.0, r: 0.8308977935562227
06/02/2019 11:04:30 step: 8953, epoch: 271, batch: 9, loss: 0.06810058653354645, acc: 96.875, f1: 97.38311688311688, r: 0.6634249603788204
06/02/2019 11:04:31 step: 8958, epoch: 271, batch: 14, loss: 0.05639929324388504, acc: 96.875, f1: 94.1951219512195, r: 0.7506362347210843
06/02/2019 11:04:31 step: 8963, epoch: 271, batch: 19, loss: 0.1394912600517273, acc: 96.875, f1: 93.58465608465607, r: 0.7640055960103735
06/02/2019 11:04:32 step: 8968, epoch: 271, batch: 24, loss: 0.05908181890845299, acc: 96.875, f1: 98.4375, r: 0.7607522126751552
06/02/2019 11:04:32 step: 8973, epoch: 271, batch: 29, loss: 0.09106844663619995, acc: 96.875, f1: 97.86931818181819, r: 0.7852571505165383
06/02/2019 11:04:32 *** evaluating ***
06/02/2019 11:04:33 step: 272, epoch: 271, acc: 59.82905982905983, f1: 27.903743664613227, r: 0.33510785106201707
06/02/2019 11:04:33 *** epoch: 273 ***
06/02/2019 11:04:33 *** training ***
06/02/2019 11:04:33 step: 8981, epoch: 272, batch: 4, loss: 0.10163016617298126, acc: 98.4375, f1: 99.10735259572469, r: 0.653299066633157
06/02/2019 11:04:33 step: 8986, epoch: 272, batch: 9, loss: 0.05366452783346176, acc: 98.4375, f1: 93.93939393939394, r: 0.688352447534741
06/02/2019 11:04:34 step: 8991, epoch: 272, batch: 14, loss: 0.08423076570034027, acc: 98.4375, f1: 98.796992481203, r: 0.69021988321949
06/02/2019 11:04:34 step: 8996, epoch: 272, batch: 19, loss: 0.2553097605705261, acc: 93.75, f1: 82.56802721088435, r: 0.7022119073804883
06/02/2019 11:04:35 step: 9001, epoch: 272, batch: 24, loss: 0.05591496080160141, acc: 98.4375, f1: 98.93939393939395, r: 0.7906879683581901
06/02/2019 11:04:35 step: 9006, epoch: 272, batch: 29, loss: 0.09563502669334412, acc: 98.4375, f1: 99.35167615433271, r: 0.7713732890970408
06/02/2019 11:04:35 *** evaluating ***
06/02/2019 11:04:36 step: 273, epoch: 272, acc: 59.401709401709404, f1: 29.070348131664524, r: 0.3337839553533001
06/02/2019 11:04:36 *** epoch: 274 ***
06/02/2019 11:04:36 *** training ***
06/02/2019 11:04:36 step: 9014, epoch: 273, batch: 4, loss: 0.04151187092065811, acc: 100.0, f1: 100.0, r: 0.642770815828961
06/02/2019 11:04:36 step: 9019, epoch: 273, batch: 9, loss: 0.0504874587059021, acc: 98.4375, f1: 98.51455733808675, r: 0.5989152358953803
06/02/2019 11:04:37 step: 9024, epoch: 273, batch: 14, loss: 0.059335753321647644, acc: 96.875, f1: 95.82753510052156, r: 0.6744452533778993
06/02/2019 11:04:37 step: 9029, epoch: 273, batch: 19, loss: 0.049085695296525955, acc: 98.4375, f1: 97.11399711399712, r: 0.6329870900764848
06/02/2019 11:04:38 step: 9034, epoch: 273, batch: 24, loss: 0.08849793672561646, acc: 96.875, f1: 98.13852813852814, r: 0.593963207693026
06/02/2019 11:04:38 step: 9039, epoch: 273, batch: 29, loss: 0.10644236207008362, acc: 95.3125, f1: 94.44076038903624, r: 0.75757730501989
06/02/2019 11:04:38 *** evaluating ***
06/02/2019 11:04:39 step: 274, epoch: 273, acc: 58.54700854700855, f1: 27.793341948073568, r: 0.32765873740736357
06/02/2019 11:04:39 *** epoch: 275 ***
06/02/2019 11:04:39 *** training ***
06/02/2019 11:04:39 step: 9047, epoch: 274, batch: 4, loss: 0.10052601993083954, acc: 96.875, f1: 93.73015873015873, r: 0.8361526838253934
06/02/2019 11:04:39 step: 9052, epoch: 274, batch: 9, loss: 0.11455325782299042, acc: 93.75, f1: 80.50887021475258, r: 0.6604738241010752
06/02/2019 11:04:40 step: 9057, epoch: 274, batch: 14, loss: 0.03771381452679634, acc: 100.0, f1: 100.0, r: 0.7256744866737237
06/02/2019 11:04:40 step: 9062, epoch: 274, batch: 19, loss: 0.014435280114412308, acc: 100.0, f1: 100.0, r: 0.6563952301800595
06/02/2019 11:04:40 step: 9067, epoch: 274, batch: 24, loss: 0.10685405135154724, acc: 96.875, f1: 95.12617012617012, r: 0.6167354632006926
06/02/2019 11:04:41 step: 9072, epoch: 274, batch: 29, loss: 0.1732640266418457, acc: 93.75, f1: 90.17432306616381, r: 0.5387129645170788
06/02/2019 11:04:41 *** evaluating ***
06/02/2019 11:04:41 step: 275, epoch: 274, acc: 58.119658119658126, f1: 29.280748790212673, r: 0.33093039175976835
06/02/2019 11:04:41 *** epoch: 276 ***
06/02/2019 11:04:41 *** training ***
06/02/2019 11:04:42 step: 9080, epoch: 275, batch: 4, loss: 0.04870472103357315, acc: 96.875, f1: 95.2249200075287, r: 0.8327876683706241
06/02/2019 11:04:42 step: 9085, epoch: 275, batch: 9, loss: 0.10965509712696075, acc: 96.875, f1: 97.10295040803516, r: 0.7408435670828584
06/02/2019 11:04:42 step: 9090, epoch: 275, batch: 14, loss: 0.05905260518193245, acc: 96.875, f1: 95.70136581764488, r: 0.8218172814876983
06/02/2019 11:04:43 step: 9095, epoch: 275, batch: 19, loss: 0.08045530319213867, acc: 95.3125, f1: 88.41132399901743, r: 0.7736617815102587
06/02/2019 11:04:43 step: 9100, epoch: 275, batch: 24, loss: 0.0869399830698967, acc: 96.875, f1: 97.09578391748934, r: 0.6153200016952337
06/02/2019 11:04:43 step: 9105, epoch: 275, batch: 29, loss: 0.17974798381328583, acc: 93.75, f1: 87.56359381359381, r: 0.6787555781027128
06/02/2019 11:04:44 *** evaluating ***
06/02/2019 11:04:44 step: 276, epoch: 275, acc: 58.119658119658126, f1: 30.309996825012252, r: 0.3325885591962125
06/02/2019 11:04:44 *** epoch: 277 ***
06/02/2019 11:04:44 *** training ***
06/02/2019 11:04:44 step: 9113, epoch: 276, batch: 4, loss: 0.17202721536159515, acc: 93.75, f1: 89.75694444444444, r: 0.7894120897778216
06/02/2019 11:04:44 step: 9118, epoch: 276, batch: 9, loss: 0.21886961162090302, acc: 95.3125, f1: 94.91134952004518, r: 0.6706613423361578
06/02/2019 11:04:45 step: 9123, epoch: 276, batch: 14, loss: 0.1792658567428589, acc: 93.75, f1: 91.83238636363636, r: 0.7537189949874014
06/02/2019 11:04:45 step: 9128, epoch: 276, batch: 19, loss: 0.12239837646484375, acc: 95.3125, f1: 97.06918812181972, r: 0.6373434760478908
06/02/2019 11:04:46 step: 9133, epoch: 276, batch: 24, loss: 0.2165064513683319, acc: 93.75, f1: 95.15215176979883, r: 0.7904120344090244
06/02/2019 11:04:46 step: 9138, epoch: 276, batch: 29, loss: 0.07522885501384735, acc: 98.4375, f1: 98.939883645766, r: 0.6671370789740201
06/02/2019 11:04:46 *** evaluating ***
06/02/2019 11:04:47 step: 277, epoch: 276, acc: 58.54700854700855, f1: 29.9263345872127, r: 0.3343200429616752
06/02/2019 11:04:47 *** epoch: 278 ***
06/02/2019 11:04:47 *** training ***
06/02/2019 11:04:47 step: 9146, epoch: 277, batch: 4, loss: 0.02465503290295601, acc: 100.0, f1: 100.0, r: 0.7143456739524803
06/02/2019 11:04:47 step: 9151, epoch: 277, batch: 9, loss: 0.08382032811641693, acc: 98.4375, f1: 97.57236227824464, r: 0.6694796966972508
06/02/2019 11:04:48 step: 9156, epoch: 277, batch: 14, loss: 0.09995866566896439, acc: 95.3125, f1: 91.456176673568, r: 0.6742347867527947
06/02/2019 11:04:48 step: 9161, epoch: 277, batch: 19, loss: 0.02440890297293663, acc: 100.0, f1: 100.0, r: 0.6811094842961323
06/02/2019 11:04:49 step: 9166, epoch: 277, batch: 24, loss: 0.1265135109424591, acc: 95.3125, f1: 96.12878787878788, r: 0.7716760870762898
06/02/2019 11:04:49 step: 9171, epoch: 277, batch: 29, loss: 0.10538328438997269, acc: 96.875, f1: 95.8943623426382, r: 0.7636665231584342
06/02/2019 11:04:49 *** evaluating ***
06/02/2019 11:04:50 step: 278, epoch: 277, acc: 59.401709401709404, f1: 28.354838548392625, r: 0.33521414475813655
06/02/2019 11:04:50 *** epoch: 279 ***
06/02/2019 11:04:50 *** training ***
06/02/2019 11:04:50 step: 9179, epoch: 278, batch: 4, loss: 0.03141884133219719, acc: 100.0, f1: 100.0, r: 0.694161996593088
06/02/2019 11:04:50 step: 9184, epoch: 278, batch: 9, loss: 0.08011423051357269, acc: 98.4375, f1: 99.27272727272727, r: 0.748874542702802
06/02/2019 11:04:51 step: 9189, epoch: 278, batch: 14, loss: 0.07499290257692337, acc: 96.875, f1: 96.89514426460238, r: 0.6584842509818838
06/02/2019 11:04:51 step: 9194, epoch: 278, batch: 19, loss: 0.03248363360762596, acc: 98.4375, f1: 98.06763285024155, r: 0.8121894201854087
06/02/2019 11:04:51 step: 9199, epoch: 278, batch: 24, loss: 0.022304130718111992, acc: 100.0, f1: 100.0, r: 0.8161619574408239
06/02/2019 11:04:52 step: 9204, epoch: 278, batch: 29, loss: 0.3257865309715271, acc: 90.625, f1: 80.9227280655852, r: 0.6330768652519143
06/02/2019 11:04:52 *** evaluating ***
06/02/2019 11:04:52 step: 279, epoch: 278, acc: 58.119658119658126, f1: 26.803826384922903, r: 0.3333056740781124
06/02/2019 11:04:52 *** epoch: 280 ***
06/02/2019 11:04:52 *** training ***
06/02/2019 11:04:53 step: 9212, epoch: 279, batch: 4, loss: 0.08171221613883972, acc: 96.875, f1: 95.41760383865648, r: 0.7812123486221783
06/02/2019 11:04:53 step: 9217, epoch: 279, batch: 9, loss: 0.02023746632039547, acc: 100.0, f1: 100.0, r: 0.6745221778098462
06/02/2019 11:04:54 step: 9222, epoch: 279, batch: 14, loss: 0.10008810460567474, acc: 96.875, f1: 96.54761904761905, r: 0.8186145651197525
06/02/2019 11:04:54 step: 9227, epoch: 279, batch: 19, loss: 0.06492523103952408, acc: 98.4375, f1: 98.96918767507003, r: 0.6544430758120603
06/02/2019 11:04:54 step: 9232, epoch: 279, batch: 24, loss: 0.12266836315393448, acc: 95.3125, f1: 93.47851335656213, r: 0.627099158498189
06/02/2019 11:04:55 step: 9237, epoch: 279, batch: 29, loss: 0.08530272543430328, acc: 96.875, f1: 90.50877192982456, r: 0.785979240643996
06/02/2019 11:04:55 *** evaluating ***
06/02/2019 11:04:55 step: 280, epoch: 279, acc: 59.401709401709404, f1: 28.029903963242987, r: 0.3377869680045135
06/02/2019 11:04:55 *** epoch: 281 ***
06/02/2019 11:04:55 *** training ***
06/02/2019 11:04:55 step: 9245, epoch: 280, batch: 4, loss: 0.06245771795511246, acc: 95.3125, f1: 91.47608450494823, r: 0.6531880260154747
06/02/2019 11:04:56 step: 9250, epoch: 280, batch: 9, loss: 0.02918432652950287, acc: 98.4375, f1: 99.15966386554622, r: 0.7584809961277628
06/02/2019 11:04:56 step: 9255, epoch: 280, batch: 14, loss: 0.15697838366031647, acc: 96.875, f1: 95.32967032967034, r: 0.7380338864723914
06/02/2019 11:04:57 step: 9260, epoch: 280, batch: 19, loss: 0.08174816519021988, acc: 98.4375, f1: 96.84210526315789, r: 0.7621901577751206
06/02/2019 11:04:57 step: 9265, epoch: 280, batch: 24, loss: 0.19850687682628632, acc: 92.1875, f1: 90.9917216826446, r: 0.8012117575632339
06/02/2019 11:04:57 step: 9270, epoch: 280, batch: 29, loss: 0.042838405817747116, acc: 100.0, f1: 100.0, r: 0.67937721854474
06/02/2019 11:04:57 *** evaluating ***
06/02/2019 11:04:58 step: 281, epoch: 280, acc: 58.119658119658126, f1: 28.104813440527725, r: 0.33074294776410107
06/02/2019 11:04:58 *** epoch: 282 ***
06/02/2019 11:04:58 *** training ***
06/02/2019 11:04:58 step: 9278, epoch: 281, batch: 4, loss: 0.03043215721845627, acc: 100.0, f1: 100.0, r: 0.6125626145236736
06/02/2019 11:04:58 step: 9283, epoch: 281, batch: 9, loss: 0.09635715186595917, acc: 95.3125, f1: 93.37353680210823, r: 0.7071131419259865
06/02/2019 11:04:59 step: 9288, epoch: 281, batch: 14, loss: 0.05054358020424843, acc: 100.0, f1: 100.0, r: 0.7525113174493347
06/02/2019 11:04:59 step: 9293, epoch: 281, batch: 19, loss: 0.0716620683670044, acc: 96.875, f1: 97.31509625126647, r: 0.6974576933607125
06/02/2019 11:05:00 step: 9298, epoch: 281, batch: 24, loss: 0.13764718174934387, acc: 93.75, f1: 93.41322055137844, r: 0.7792306016655701
06/02/2019 11:05:00 step: 9303, epoch: 281, batch: 29, loss: 0.10111050307750702, acc: 96.875, f1: 95.49084249084248, r: 0.7289649652934229
06/02/2019 11:05:00 *** evaluating ***
06/02/2019 11:05:00 step: 282, epoch: 281, acc: 57.692307692307686, f1: 27.324962775161982, r: 0.3291880003636979
06/02/2019 11:05:00 *** epoch: 283 ***
06/02/2019 11:05:00 *** training ***
06/02/2019 11:05:01 step: 9311, epoch: 282, batch: 4, loss: 0.05426344275474548, acc: 98.4375, f1: 95.59748427672956, r: 0.7915029089184965
06/02/2019 11:05:01 step: 9316, epoch: 282, batch: 9, loss: 0.09486447274684906, acc: 96.875, f1: 92.4829931972789, r: 0.7304095622193049
06/02/2019 11:05:02 step: 9321, epoch: 282, batch: 14, loss: 0.09455449134111404, acc: 95.3125, f1: 95.10641284815813, r: 0.7636585024165731
06/02/2019 11:05:02 step: 9326, epoch: 282, batch: 19, loss: 0.1323610246181488, acc: 96.875, f1: 97.79152181529092, r: 0.6828828951015012
06/02/2019 11:05:03 step: 9331, epoch: 282, batch: 24, loss: 0.317810595035553, acc: 92.1875, f1: 79.45465387325852, r: 0.6344948776500265
06/02/2019 11:05:03 step: 9336, epoch: 282, batch: 29, loss: 0.02719501405954361, acc: 100.0, f1: 100.0, r: 0.663011924638249
06/02/2019 11:05:03 *** evaluating ***
06/02/2019 11:05:03 step: 283, epoch: 282, acc: 58.97435897435898, f1: 27.734411946596822, r: 0.33486813236138985
06/02/2019 11:05:03 *** epoch: 284 ***
06/02/2019 11:05:03 *** training ***
06/02/2019 11:05:04 step: 9344, epoch: 283, batch: 4, loss: 0.039324384182691574, acc: 98.4375, f1: 98.86178861788618, r: 0.7772733976614694
06/02/2019 11:05:04 step: 9349, epoch: 283, batch: 9, loss: 0.10631706565618515, acc: 95.3125, f1: 93.73015873015873, r: 0.601795643221172
06/02/2019 11:05:05 step: 9354, epoch: 283, batch: 14, loss: 0.05375586450099945, acc: 98.4375, f1: 95.45454545454545, r: 0.7300660048093475
06/02/2019 11:05:05 step: 9359, epoch: 283, batch: 19, loss: 0.08289282768964767, acc: 96.875, f1: 96.21173330850749, r: 0.7915506207284737
06/02/2019 11:05:05 step: 9364, epoch: 283, batch: 24, loss: 0.2119201123714447, acc: 95.3125, f1: 91.90639589169002, r: 0.6075389490658569
06/02/2019 11:05:06 step: 9369, epoch: 283, batch: 29, loss: 0.14950290322303772, acc: 96.875, f1: 97.81885075561767, r: 0.8160312661579926
06/02/2019 11:05:06 *** evaluating ***
06/02/2019 11:05:06 step: 284, epoch: 283, acc: 59.401709401709404, f1: 28.057692307692307, r: 0.3336661680130806
06/02/2019 11:05:06 *** epoch: 285 ***
06/02/2019 11:05:06 *** training ***
06/02/2019 11:05:07 step: 9377, epoch: 284, batch: 4, loss: 0.06304259598255157, acc: 98.4375, f1: 97.83549783549783, r: 0.7965916209165724
06/02/2019 11:05:07 step: 9382, epoch: 284, batch: 9, loss: 0.06133062392473221, acc: 96.875, f1: 95.9035409035409, r: 0.8137182350921516
06/02/2019 11:05:07 step: 9387, epoch: 284, batch: 14, loss: 0.08416665345430374, acc: 96.875, f1: 98.02102659245516, r: 0.7119889380586159
06/02/2019 11:05:08 step: 9392, epoch: 284, batch: 19, loss: 0.03734986111521721, acc: 98.4375, f1: 99.20255183413079, r: 0.6306194888061555
06/02/2019 11:05:08 step: 9397, epoch: 284, batch: 24, loss: 0.017612820491194725, acc: 100.0, f1: 100.0, r: 0.7075772129438737
06/02/2019 11:05:09 step: 9402, epoch: 284, batch: 29, loss: 0.11610434949398041, acc: 95.3125, f1: 95.94798560315802, r: 0.7077522707929541
06/02/2019 11:05:09 *** evaluating ***
06/02/2019 11:05:09 step: 285, epoch: 284, acc: 59.82905982905983, f1: 30.057488040588286, r: 0.339136137085277
06/02/2019 11:05:09 *** epoch: 286 ***
06/02/2019 11:05:09 *** training ***
06/02/2019 11:05:10 step: 9410, epoch: 285, batch: 4, loss: 0.06176028400659561, acc: 98.4375, f1: 98.90476190476191, r: 0.7554319931616763
06/02/2019 11:05:10 step: 9415, epoch: 285, batch: 9, loss: 0.04542742669582367, acc: 100.0, f1: 100.0, r: 0.7504284901272642
06/02/2019 11:05:10 step: 9420, epoch: 285, batch: 14, loss: 0.08190863579511642, acc: 98.4375, f1: 98.44155844155846, r: 0.7317584214981788
06/02/2019 11:05:11 step: 9425, epoch: 285, batch: 19, loss: 0.039142291992902756, acc: 100.0, f1: 100.0, r: 0.804742386116202
06/02/2019 11:05:11 step: 9430, epoch: 285, batch: 24, loss: 0.10070201754570007, acc: 96.875, f1: 95.9500168861871, r: 0.695626286854679
06/02/2019 11:05:12 step: 9435, epoch: 285, batch: 29, loss: 0.043708886951208115, acc: 98.4375, f1: 86.66666666666667, r: 0.6856104485664075
06/02/2019 11:05:12 *** evaluating ***
06/02/2019 11:05:12 step: 286, epoch: 285, acc: 58.54700854700855, f1: 29.852301207564363, r: 0.3376907974417802
06/02/2019 11:05:12 *** epoch: 287 ***
06/02/2019 11:05:12 *** training ***
06/02/2019 11:05:13 step: 9443, epoch: 286, batch: 4, loss: 0.059393562376499176, acc: 98.4375, f1: 99.38490978676873, r: 0.8231739300634674
06/02/2019 11:05:13 step: 9448, epoch: 286, batch: 9, loss: 0.12911361455917358, acc: 96.875, f1: 85.13888888888889, r: 0.7713305825871064
06/02/2019 11:05:14 step: 9453, epoch: 286, batch: 14, loss: 0.051968853920698166, acc: 98.4375, f1: 93.93939393939394, r: 0.6427175041257032
06/02/2019 11:05:14 step: 9458, epoch: 286, batch: 19, loss: 0.057344336062669754, acc: 98.4375, f1: 94.5578231292517, r: 0.6148416123101486
06/02/2019 11:05:15 step: 9463, epoch: 286, batch: 24, loss: 0.09043627977371216, acc: 95.3125, f1: 95.73820915926179, r: 0.7665535569664852
06/02/2019 11:05:15 step: 9468, epoch: 286, batch: 29, loss: 0.056436438113451004, acc: 98.4375, f1: 99.23404255319149, r: 0.8125282016584159
06/02/2019 11:05:15 *** evaluating ***
06/02/2019 11:05:15 step: 287, epoch: 286, acc: 60.256410256410255, f1: 30.417571558875906, r: 0.33708684800825806
06/02/2019 11:05:15 *** epoch: 288 ***
06/02/2019 11:05:15 *** training ***
06/02/2019 11:05:16 step: 9476, epoch: 287, batch: 4, loss: 0.05373922362923622, acc: 95.3125, f1: 96.60262563523433, r: 0.7520573837475393
06/02/2019 11:05:16 step: 9481, epoch: 287, batch: 9, loss: 0.12734824419021606, acc: 95.3125, f1: 91.5582174898945, r: 0.6761448028238181
06/02/2019 11:05:17 step: 9486, epoch: 287, batch: 14, loss: 0.04622268304228783, acc: 96.875, f1: 96.79374389051809, r: 0.7690282267499816
06/02/2019 11:05:17 step: 9491, epoch: 287, batch: 19, loss: 0.044475577771663666, acc: 98.4375, f1: 94.80519480519482, r: 0.6607700315598501
06/02/2019 11:05:18 step: 9496, epoch: 287, batch: 24, loss: 0.07428301870822906, acc: 96.875, f1: 95.80498866213152, r: 0.6513627861359933
06/02/2019 11:05:18 step: 9501, epoch: 287, batch: 29, loss: 0.0898904949426651, acc: 95.3125, f1: 92.86545077801236, r: 0.7452643403730473
06/02/2019 11:05:18 *** evaluating ***
06/02/2019 11:05:18 step: 288, epoch: 287, acc: 58.97435897435898, f1: 30.411055844395317, r: 0.3418515924432432
06/02/2019 11:05:18 *** epoch: 289 ***
06/02/2019 11:05:18 *** training ***
06/02/2019 11:05:19 step: 9509, epoch: 288, batch: 4, loss: 0.07418924570083618, acc: 98.4375, f1: 99.17748917748919, r: 0.7652206294365148
06/02/2019 11:05:19 step: 9514, epoch: 288, batch: 9, loss: 0.07109490782022476, acc: 95.3125, f1: 95.60858189429618, r: 0.6968166234052642
06/02/2019 11:05:20 step: 9519, epoch: 288, batch: 14, loss: 0.03796583414077759, acc: 100.0, f1: 100.0, r: 0.6806330991551056
06/02/2019 11:05:20 step: 9524, epoch: 288, batch: 19, loss: 0.05903225392103195, acc: 98.4375, f1: 94.94655004859086, r: 0.5840404898676139
06/02/2019 11:05:21 step: 9529, epoch: 288, batch: 24, loss: 0.06488630920648575, acc: 98.4375, f1: 96.53846153846153, r: 0.7904445735968741
06/02/2019 11:05:21 step: 9534, epoch: 288, batch: 29, loss: 0.1595539152622223, acc: 93.75, f1: 93.01724137931035, r: 0.6126852707946313
06/02/2019 11:05:21 *** evaluating ***
06/02/2019 11:05:21 step: 289, epoch: 288, acc: 58.119658119658126, f1: 30.677030764325135, r: 0.33997615861839187
06/02/2019 11:05:21 *** epoch: 290 ***
06/02/2019 11:05:21 *** training ***
06/02/2019 11:05:22 step: 9542, epoch: 289, batch: 4, loss: 0.10019773244857788, acc: 95.3125, f1: 93.09905780494016, r: 0.69710268724486
06/02/2019 11:05:22 step: 9547, epoch: 289, batch: 9, loss: 0.061899296939373016, acc: 98.4375, f1: 97.77777777777779, r: 0.7952537102912586
06/02/2019 11:05:23 step: 9552, epoch: 289, batch: 14, loss: 0.08871360123157501, acc: 96.875, f1: 86.10021430387563, r: 0.6710556275595587
06/02/2019 11:05:23 step: 9557, epoch: 289, batch: 19, loss: 0.03535829856991768, acc: 98.4375, f1: 98.65896815049358, r: 0.6976238328533197
06/02/2019 11:05:23 step: 9562, epoch: 289, batch: 24, loss: 0.11287668347358704, acc: 96.875, f1: 94.85584710074505, r: 0.702226103799541
06/02/2019 11:05:24 step: 9567, epoch: 289, batch: 29, loss: 0.11639967560768127, acc: 96.875, f1: 96.25442803849651, r: 0.6775869723434585
06/02/2019 11:05:24 *** evaluating ***
06/02/2019 11:05:24 step: 290, epoch: 289, acc: 58.54700854700855, f1: 30.206618900107983, r: 0.3429169310025273
06/02/2019 11:05:24 *** epoch: 291 ***
06/02/2019 11:05:24 *** training ***
06/02/2019 11:05:25 step: 9575, epoch: 290, batch: 4, loss: 0.07836586982011795, acc: 98.4375, f1: 98.20868786386028, r: 0.6646411566552715
06/02/2019 11:05:25 step: 9580, epoch: 290, batch: 9, loss: 0.10781043022871017, acc: 93.75, f1: 95.24129927090453, r: 0.7059810227602304
06/02/2019 11:05:25 step: 9585, epoch: 290, batch: 14, loss: 0.07081969082355499, acc: 98.4375, f1: 85.0, r: 0.6932624568322323
06/02/2019 11:05:26 step: 9590, epoch: 290, batch: 19, loss: 0.02382206916809082, acc: 98.4375, f1: 99.15895710681245, r: 0.7286116667049924
06/02/2019 11:05:26 step: 9595, epoch: 290, batch: 24, loss: 0.036884233355522156, acc: 100.0, f1: 100.0, r: 0.7791894343607653
06/02/2019 11:05:27 step: 9600, epoch: 290, batch: 29, loss: 0.01889948360621929, acc: 100.0, f1: 100.0, r: 0.557604127844575
06/02/2019 11:05:27 *** evaluating ***
06/02/2019 11:05:27 step: 291, epoch: 290, acc: 59.401709401709404, f1: 30.48755907700862, r: 0.3382164216914951
06/02/2019 11:05:27 *** epoch: 292 ***
06/02/2019 11:05:27 *** training ***
06/02/2019 11:05:27 step: 9608, epoch: 291, batch: 4, loss: 0.01894419454038143, acc: 100.0, f1: 100.0, r: 0.7124139017246216
06/02/2019 11:05:28 step: 9613, epoch: 291, batch: 9, loss: 0.12705782055854797, acc: 96.875, f1: 95.79068884332044, r: 0.7200929391757881
06/02/2019 11:05:28 step: 9618, epoch: 291, batch: 14, loss: 0.057488925755023956, acc: 96.875, f1: 83.6231884057971, r: 0.7372544209673568
06/02/2019 11:05:28 step: 9623, epoch: 291, batch: 19, loss: 0.11691617220640182, acc: 95.3125, f1: 94.62256493506493, r: 0.8065092443270635
06/02/2019 11:05:29 step: 9628, epoch: 291, batch: 24, loss: 0.10563456267118454, acc: 96.875, f1: 94.55709165386585, r: 0.7708243650884419
06/02/2019 11:05:29 step: 9633, epoch: 291, batch: 29, loss: 0.027882404625415802, acc: 98.4375, f1: 98.99543378995435, r: 0.8610471194072117
06/02/2019 11:05:29 *** evaluating ***
06/02/2019 11:05:30 step: 292, epoch: 291, acc: 59.401709401709404, f1: 30.316202655652198, r: 0.3391364464770041
06/02/2019 11:05:30 *** epoch: 293 ***
06/02/2019 11:05:30 *** training ***
06/02/2019 11:05:30 step: 9641, epoch: 292, batch: 4, loss: 0.04653492569923401, acc: 98.4375, f1: 94.6969696969697, r: 0.7693747835602798
06/02/2019 11:05:30 step: 9646, epoch: 292, batch: 9, loss: 0.0718744695186615, acc: 96.875, f1: 94.93495839681238, r: 0.7757718643933479
06/02/2019 11:05:31 step: 9651, epoch: 292, batch: 14, loss: 0.04073288291692734, acc: 98.4375, f1: 98.27327327327328, r: 0.7899576394158803
06/02/2019 11:05:31 step: 9656, epoch: 292, batch: 19, loss: 0.05756363272666931, acc: 98.4375, f1: 99.1388044579534, r: 0.7567556720602907
06/02/2019 11:05:31 step: 9661, epoch: 292, batch: 24, loss: 0.04451998695731163, acc: 98.4375, f1: 98.13258636788049, r: 0.7010516225500197
06/02/2019 11:05:32 step: 9666, epoch: 292, batch: 29, loss: 0.07283241301774979, acc: 98.4375, f1: 97.84126984126985, r: 0.6330549823601846
06/02/2019 11:05:32 *** evaluating ***
06/02/2019 11:05:32 step: 293, epoch: 292, acc: 59.82905982905983, f1: 30.69847032980674, r: 0.34119916672109973
06/02/2019 11:05:32 *** epoch: 294 ***
06/02/2019 11:05:32 *** training ***
06/02/2019 11:05:33 step: 9674, epoch: 293, batch: 4, loss: 0.1346130073070526, acc: 95.3125, f1: 96.25132275132276, r: 0.7776148240515011
06/02/2019 11:05:33 step: 9679, epoch: 293, batch: 9, loss: 0.0202941931784153, acc: 100.0, f1: 100.0, r: 0.7382293571572861
06/02/2019 11:05:33 step: 9684, epoch: 293, batch: 14, loss: 0.0496964268386364, acc: 96.875, f1: 97.74330244918481, r: 0.7385238098431249
06/02/2019 11:05:34 step: 9689, epoch: 293, batch: 19, loss: 0.14762847125530243, acc: 93.75, f1: 90.78042328042328, r: 0.6996628938202837
06/02/2019 11:05:34 step: 9694, epoch: 293, batch: 24, loss: 0.19660966098308563, acc: 96.875, f1: 96.74498746867168, r: 0.7676353102508033
06/02/2019 11:05:34 step: 9699, epoch: 293, batch: 29, loss: 0.03608278930187225, acc: 98.4375, f1: 97.1188475390156, r: 0.6988417370281297
06/02/2019 11:05:35 *** evaluating ***
06/02/2019 11:05:35 step: 294, epoch: 293, acc: 59.82905982905983, f1: 28.754073354467053, r: 0.34441596499648225
06/02/2019 11:05:35 *** epoch: 295 ***
06/02/2019 11:05:35 *** training ***
06/02/2019 11:05:35 step: 9707, epoch: 294, batch: 4, loss: 0.04706214740872383, acc: 98.4375, f1: 99.03703703703704, r: 0.7513843127506971
06/02/2019 11:05:36 step: 9712, epoch: 294, batch: 9, loss: 0.07533058524131775, acc: 96.875, f1: 96.37865628431666, r: 0.708419417217732
06/02/2019 11:05:36 step: 9717, epoch: 294, batch: 14, loss: 0.06993560492992401, acc: 96.875, f1: 96.08445528749134, r: 0.644610963380842
06/02/2019 11:05:36 step: 9722, epoch: 294, batch: 19, loss: 0.019257964566349983, acc: 100.0, f1: 100.0, r: 0.7024768472733794
06/02/2019 11:05:37 step: 9727, epoch: 294, batch: 24, loss: 0.04602617770433426, acc: 100.0, f1: 100.0, r: 0.7155779953863093
06/02/2019 11:05:37 step: 9732, epoch: 294, batch: 29, loss: 0.06036597862839699, acc: 96.875, f1: 95.99122089010865, r: 0.7952754504498083
06/02/2019 11:05:37 *** evaluating ***
06/02/2019 11:05:37 step: 295, epoch: 294, acc: 60.256410256410255, f1: 31.07041067630677, r: 0.34427703393526854
06/02/2019 11:05:37 *** epoch: 296 ***
06/02/2019 11:05:37 *** training ***
06/02/2019 11:05:38 step: 9740, epoch: 295, batch: 4, loss: 0.04898490756750107, acc: 100.0, f1: 100.0, r: 0.7123918460226392
06/02/2019 11:05:38 step: 9745, epoch: 295, batch: 9, loss: 0.054854050278663635, acc: 98.4375, f1: 96.88311688311687, r: 0.6925500994313893
06/02/2019 11:05:38 step: 9750, epoch: 295, batch: 14, loss: 0.09240447729825974, acc: 96.875, f1: 95.89115646258503, r: 0.695177352166195
06/02/2019 11:05:39 step: 9755, epoch: 295, batch: 19, loss: 0.04344593733549118, acc: 100.0, f1: 100.0, r: 0.8159843290464772
06/02/2019 11:05:39 step: 9760, epoch: 295, batch: 24, loss: 0.07981228828430176, acc: 96.875, f1: 97.29166666666667, r: 0.7871045425405815
06/02/2019 11:05:40 step: 9765, epoch: 295, batch: 29, loss: 0.060048818588256836, acc: 96.875, f1: 98.45049130763417, r: 0.7751600794276097
06/02/2019 11:05:40 *** evaluating ***
06/02/2019 11:05:40 step: 296, epoch: 295, acc: 59.401709401709404, f1: 31.015503354952894, r: 0.34377365002669086
06/02/2019 11:05:40 *** epoch: 297 ***
06/02/2019 11:05:40 *** training ***
06/02/2019 11:05:40 step: 9773, epoch: 296, batch: 4, loss: 0.06580163538455963, acc: 96.875, f1: 93.2682917301457, r: 0.7932290018956076
06/02/2019 11:05:41 step: 9778, epoch: 296, batch: 9, loss: 0.059983085840940475, acc: 98.4375, f1: 97.00680272108843, r: 0.7057173368214633
06/02/2019 11:05:41 step: 9783, epoch: 296, batch: 14, loss: 0.09670639038085938, acc: 98.4375, f1: 99.24340467894474, r: 0.7293698133795812
06/02/2019 11:05:41 step: 9788, epoch: 296, batch: 19, loss: 0.0327535942196846, acc: 100.0, f1: 100.0, r: 0.7019441344630372
06/02/2019 11:05:42 step: 9793, epoch: 296, batch: 24, loss: 0.21978451311588287, acc: 92.1875, f1: 87.73412409287813, r: 0.5747615886328243
06/02/2019 11:05:42 step: 9798, epoch: 296, batch: 29, loss: 0.09577810764312744, acc: 98.4375, f1: 98.39734733351754, r: 0.7473246928261903
06/02/2019 11:05:42 *** evaluating ***
06/02/2019 11:05:42 step: 297, epoch: 296, acc: 59.82905982905983, f1: 27.300611210494928, r: 0.3449202813914853
06/02/2019 11:05:42 *** epoch: 298 ***
06/02/2019 11:05:42 *** training ***
06/02/2019 11:05:43 step: 9806, epoch: 297, batch: 4, loss: 0.13702812790870667, acc: 96.875, f1: 95.64924411109808, r: 0.7636296224919977
06/02/2019 11:05:43 step: 9811, epoch: 297, batch: 9, loss: 0.03784632310271263, acc: 100.0, f1: 100.0, r: 0.7846787666684849
06/02/2019 11:05:44 step: 9816, epoch: 297, batch: 14, loss: 0.08958105742931366, acc: 96.875, f1: 93.63636363636364, r: 0.7370630352141649
06/02/2019 11:05:44 step: 9821, epoch: 297, batch: 19, loss: 0.053281769156455994, acc: 96.875, f1: 98.73563218390805, r: 0.7568593393094909
06/02/2019 11:05:44 step: 9826, epoch: 297, batch: 24, loss: 0.07689005881547928, acc: 96.875, f1: 95.00680272108842, r: 0.6920802844531102
06/02/2019 11:05:45 step: 9831, epoch: 297, batch: 29, loss: 0.11271782219409943, acc: 96.875, f1: 98.13494775900791, r: 0.6764098822852646
06/02/2019 11:05:45 *** evaluating ***
06/02/2019 11:05:45 step: 298, epoch: 297, acc: 59.401709401709404, f1: 28.335365270606232, r: 0.33920488990752395
06/02/2019 11:05:45 *** epoch: 299 ***
06/02/2019 11:05:45 *** training ***
06/02/2019 11:05:46 step: 9839, epoch: 298, batch: 4, loss: 0.048501547425985336, acc: 98.4375, f1: 99.23404255319149, r: 0.8087952865391054
06/02/2019 11:05:46 step: 9844, epoch: 298, batch: 9, loss: 0.06719205528497696, acc: 96.875, f1: 96.3826070968928, r: 0.7345313414775878
06/02/2019 11:05:47 step: 9849, epoch: 298, batch: 14, loss: 0.044364508241415024, acc: 98.4375, f1: 97.16216216216216, r: 0.7507279613142696
06/02/2019 11:05:47 step: 9854, epoch: 298, batch: 19, loss: 0.025529788807034492, acc: 100.0, f1: 100.0, r: 0.665966824919212
06/02/2019 11:05:47 step: 9859, epoch: 298, batch: 24, loss: 0.058557864278554916, acc: 96.875, f1: 94.59102301207565, r: 0.7054110375979852
06/02/2019 11:05:48 step: 9864, epoch: 298, batch: 29, loss: 0.05459854379296303, acc: 98.4375, f1: 98.75607385811468, r: 0.7152437955438601
06/02/2019 11:05:48 *** evaluating ***
06/02/2019 11:05:48 step: 299, epoch: 298, acc: 60.256410256410255, f1: 30.485348730703908, r: 0.3399603654531559
06/02/2019 11:05:48 *** epoch: 300 ***
06/02/2019 11:05:48 *** training ***
06/02/2019 11:05:49 step: 9872, epoch: 299, batch: 4, loss: 0.0410212017595768, acc: 100.0, f1: 100.0, r: 0.7306726505210428
06/02/2019 11:05:49 step: 9877, epoch: 299, batch: 9, loss: 0.03597045689821243, acc: 100.0, f1: 100.0, r: 0.6938280320866128
06/02/2019 11:05:49 step: 9882, epoch: 299, batch: 14, loss: 0.10213218629360199, acc: 95.3125, f1: 86.84386221332034, r: 0.7164808648978191
06/02/2019 11:05:50 step: 9887, epoch: 299, batch: 19, loss: 0.1705511063337326, acc: 95.3125, f1: 85.90476190476191, r: 0.7339527859342672
06/02/2019 11:05:50 step: 9892, epoch: 299, batch: 24, loss: 0.14065900444984436, acc: 95.3125, f1: 93.53174603174604, r: 0.8007223849559615
06/02/2019 11:05:51 step: 9897, epoch: 299, batch: 29, loss: 0.047534454613924026, acc: 100.0, f1: 100.0, r: 0.7243225528554312
06/02/2019 11:05:51 *** evaluating ***
06/02/2019 11:05:51 step: 300, epoch: 299, acc: 59.82905982905983, f1: 30.08317764341861, r: 0.3440137265258809
06/02/2019 11:05:51 
*** Best acc model ***
epoch: 213
acc: 61.111111111111114
f1: 29.97114912339736
corr: 0.34734865130022435
06/02/2019 11:05:51 Loading Test Data
06/02/2019 11:05:51 load data from data/elmo_temp/test_text.npy, data/elmo_temp/test_label.npy, training: False
06/02/2019 11:06:28 loaded. total len: 2228
06/02/2019 11:06:28 Test: length: 2228, total batch: 35, batch size: 64
06/02/2019 11:06:28 
*** Test Result ***
acc: 59.82905982905983
f1: 30.08317764341861
corr: 0.3440137265258809
