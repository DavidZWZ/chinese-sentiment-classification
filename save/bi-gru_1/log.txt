06/02/2019 12:11:13 {'input_path': 'data/word2vec_temp', 'output_path': 'save/bi-gru_1', 'gpu': True, 'seed': 20000125, 'display_per_batch': 5, 'optimizer': 'adagrad', 'lr': 0.01, 'lr_decay': 0, 'weight_decay': 0.0001, 'momentum': 0.985, 'num_epochs': 300, 'batch_size': 64, 'num_labels': 8, 'embedding_size': 300, 'type': 'rnn', 'rnn': {'type': 'gru', 'bidirectional': True, 'rnn_hidden_size': 256, 'mlp_hidden_size': 512, 'dropout': 0.5, 'p_coefficient': 1, 'num_layers': 1, 'param_da': 350, 'param_r': 30, 'loss': 'cross_entropy'}}
06/02/2019 12:11:13 Loading Train Data
06/02/2019 12:11:13 load data from data/word2vec_temp/train_text.npy, data/word2vec_temp/train_label.npy, training: True
06/02/2019 12:11:33 loaded. total len: 2342
06/02/2019 12:11:33 Train: length: 2108, total batch: 33, batch size: 64
06/02/2019 12:11:33 Dev: length: 234, total batch: 4, batch size: 64
06/02/2019 12:11:33 Loading model rnn
06/02/2019 12:11:42 *** epoch: 1 ***
06/02/2019 12:11:42 *** training ***
06/02/2019 12:11:43 step: 5, epoch: 0, batch: 4, loss: 20.107728958129883, acc: 21.875, f1: 4.487179487179487, r: -0.0015349466918674892
06/02/2019 12:11:44 step: 10, epoch: 0, batch: 9, loss: 19.607301712036133, acc: 12.5, f1: 3.2653061224489797, r: -0.04261159539708385
06/02/2019 12:11:45 step: 15, epoch: 0, batch: 14, loss: 6.556689262390137, acc: 40.625, f1: 10.921474358974358, r: -0.069082336524356
06/02/2019 12:11:46 step: 20, epoch: 0, batch: 19, loss: 6.456125259399414, acc: 39.0625, f1: 8.403361344537815, r: -0.006186003080376848
06/02/2019 12:11:47 step: 25, epoch: 0, batch: 24, loss: 6.794713497161865, acc: 25.0, f1: 5.936920222634508, r: -0.003481456942581388
06/02/2019 12:11:48 step: 30, epoch: 0, batch: 29, loss: 6.672266006469727, acc: 32.8125, f1: 8.730158730158731, r: 0.011593836237984594
06/02/2019 12:11:49 *** evaluating ***
06/02/2019 12:11:49 step: 1, epoch: 0, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.2485378798648142
06/02/2019 12:11:49 *** epoch: 2 ***
06/02/2019 12:11:49 *** training ***
06/02/2019 12:11:50 step: 38, epoch: 1, batch: 4, loss: 5.7993574142456055, acc: 34.375, f1: 6.547619047619048, r: 0.09359608147335362
06/02/2019 12:11:51 step: 43, epoch: 1, batch: 9, loss: 5.092176914215088, acc: 20.3125, f1: 10.21594684385382, r: 0.09681151430242792
06/02/2019 12:11:52 step: 48, epoch: 1, batch: 14, loss: 5.326177597045898, acc: 32.8125, f1: 7.142857142857142, r: 0.12187348808109082
06/02/2019 12:11:53 step: 53, epoch: 1, batch: 19, loss: 4.766891956329346, acc: 32.8125, f1: 10.412474849094568, r: 0.05419411627623205
06/02/2019 12:11:54 step: 58, epoch: 1, batch: 24, loss: 4.663054943084717, acc: 46.875, f1: 19.18146265972353, r: 0.12696315220545198
06/02/2019 12:11:55 step: 63, epoch: 1, batch: 29, loss: 4.92464017868042, acc: 35.9375, f1: 17.220149253731346, r: 0.20522216965959197
06/02/2019 12:11:56 *** evaluating ***
06/02/2019 12:11:56 step: 2, epoch: 1, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.2985874448934013
06/02/2019 12:11:56 *** epoch: 3 ***
06/02/2019 12:11:56 *** training ***
06/02/2019 12:11:57 step: 71, epoch: 2, batch: 4, loss: 4.01209020614624, acc: 50.0, f1: 12.149648104704285, r: 0.1495903087011506
06/02/2019 12:11:58 step: 76, epoch: 2, batch: 9, loss: 4.4021525382995605, acc: 28.125, f1: 7.4074074074074066, r: 0.11345958424158421
06/02/2019 12:11:59 step: 81, epoch: 2, batch: 14, loss: 4.159419536590576, acc: 43.75, f1: 16.491102904146384, r: 0.2127065769963989
06/02/2019 12:12:00 step: 86, epoch: 2, batch: 19, loss: 4.342126846313477, acc: 32.8125, f1: 10.091069330199765, r: 0.15294648486508972
06/02/2019 12:12:01 step: 91, epoch: 2, batch: 24, loss: 4.299078464508057, acc: 32.8125, f1: 10.069444444444445, r: 0.19016205997066885
06/02/2019 12:12:02 step: 96, epoch: 2, batch: 29, loss: 3.741220474243164, acc: 48.4375, f1: 18.441558441558442, r: 0.2979522366934489
06/02/2019 12:12:02 *** evaluating ***
06/02/2019 12:12:03 step: 3, epoch: 2, acc: 53.41880341880342, f1: 14.713922685969505, r: 0.2953847669418635
06/02/2019 12:12:03 *** epoch: 4 ***
06/02/2019 12:12:03 *** training ***
06/02/2019 12:12:04 step: 104, epoch: 3, batch: 4, loss: 4.001967430114746, acc: 48.4375, f1: 14.423076923076922, r: 0.10199994439203944
06/02/2019 12:12:05 step: 109, epoch: 3, batch: 9, loss: 4.159667015075684, acc: 45.3125, f1: 12.885154061624648, r: 0.06006737352021319
06/02/2019 12:12:06 step: 114, epoch: 3, batch: 14, loss: 4.043421268463135, acc: 28.125, f1: 8.75, r: 0.1585772571692269
06/02/2019 12:12:07 step: 119, epoch: 3, batch: 19, loss: 3.8638854026794434, acc: 25.0, f1: 10.418547173866324, r: 0.09919584361226352
06/02/2019 12:12:08 step: 124, epoch: 3, batch: 24, loss: 4.067103862762451, acc: 37.5, f1: 9.799651567944249, r: 0.08322734717904146
06/02/2019 12:12:09 step: 129, epoch: 3, batch: 29, loss: 3.941956043243408, acc: 42.1875, f1: 15.949367088607596, r: 0.09713227906855249
06/02/2019 12:12:09 *** evaluating ***
06/02/2019 12:12:10 step: 4, epoch: 3, acc: 56.41025641025641, f1: 16.24214118915035, r: 0.2888117527641433
06/02/2019 12:12:10 *** epoch: 5 ***
06/02/2019 12:12:10 *** training ***
06/02/2019 12:12:11 step: 137, epoch: 4, batch: 4, loss: 3.9041261672973633, acc: 43.75, f1: 21.730305025235904, r: 0.21047220470891687
06/02/2019 12:12:12 step: 142, epoch: 4, batch: 9, loss: 3.883419990539551, acc: 35.9375, f1: 13.632744067526675, r: 0.26687815769397266
06/02/2019 12:12:13 step: 147, epoch: 4, batch: 14, loss: 3.7932662963867188, acc: 50.0, f1: 16.9004601054876, r: 0.23434793608433982
06/02/2019 12:12:14 step: 152, epoch: 4, batch: 19, loss: 4.206287860870361, acc: 32.8125, f1: 11.803429271279853, r: 0.18756921136130603
06/02/2019 12:12:15 step: 157, epoch: 4, batch: 24, loss: 3.595850944519043, acc: 50.0, f1: 17.687074829931973, r: 0.2766840377999606
06/02/2019 12:12:16 step: 162, epoch: 4, batch: 29, loss: 3.522747755050659, acc: 64.0625, f1: 15.903465346534654, r: 0.30233425602070274
06/02/2019 12:12:16 *** evaluating ***
06/02/2019 12:12:17 step: 5, epoch: 4, acc: 56.837606837606835, f1: 16.23745127125953, r: 0.3026265808224623
06/02/2019 12:12:17 *** epoch: 6 ***
06/02/2019 12:12:17 *** training ***
06/02/2019 12:12:18 step: 170, epoch: 5, batch: 4, loss: 3.909498691558838, acc: 39.0625, f1: 13.714285714285715, r: 0.18378484901773542
06/02/2019 12:12:18 step: 175, epoch: 5, batch: 9, loss: 3.536675453186035, acc: 43.75, f1: 12.658514492753623, r: 0.12258329764940995
06/02/2019 12:12:20 step: 180, epoch: 5, batch: 14, loss: 3.466491222381592, acc: 46.875, f1: 18.175029868578253, r: 0.21717021911024337
06/02/2019 12:12:21 step: 185, epoch: 5, batch: 19, loss: 4.472227096557617, acc: 43.75, f1: 12.797619047619047, r: 0.19203319105716984
06/02/2019 12:12:22 step: 190, epoch: 5, batch: 24, loss: 3.220615863800049, acc: 56.25, f1: 15.180265654648956, r: 0.11075273152054296
06/02/2019 12:12:23 step: 195, epoch: 5, batch: 29, loss: 3.6633033752441406, acc: 37.5, f1: 13.836867357994118, r: 0.2583396543317939
06/02/2019 12:12:23 *** evaluating ***
06/02/2019 12:12:24 step: 6, epoch: 5, acc: 55.12820512820513, f1: 15.823358184572427, r: 0.28523761064457887
06/02/2019 12:12:24 *** epoch: 7 ***
06/02/2019 12:12:24 *** training ***
06/02/2019 12:12:24 step: 203, epoch: 6, batch: 4, loss: 3.5616846084594727, acc: 37.5, f1: 20.92519314113435, r: 0.19960494001578138
06/02/2019 12:12:25 step: 208, epoch: 6, batch: 9, loss: 3.5095086097717285, acc: 43.75, f1: 13.825363825363826, r: 0.1714896023005671
06/02/2019 12:12:26 step: 213, epoch: 6, batch: 14, loss: 3.36868953704834, acc: 46.875, f1: 14.637681159420291, r: 0.20849380360221556
06/02/2019 12:12:27 step: 218, epoch: 6, batch: 19, loss: 3.3353190422058105, acc: 51.5625, f1: 15.595238095238095, r: 0.23904968758500392
06/02/2019 12:12:28 step: 223, epoch: 6, batch: 24, loss: 3.1414241790771484, acc: 43.75, f1: 13.210164448554544, r: 0.19202512672114616
06/02/2019 12:12:29 step: 228, epoch: 6, batch: 29, loss: 3.3704285621643066, acc: 42.1875, f1: 14.799910574558464, r: 0.27910557513983025
06/02/2019 12:12:30 *** evaluating ***
06/02/2019 12:12:30 step: 7, epoch: 6, acc: 52.13675213675214, f1: 14.15525114155251, r: 0.32328162069147903
06/02/2019 12:12:30 *** epoch: 8 ***
06/02/2019 12:12:30 *** training ***
06/02/2019 12:12:31 step: 236, epoch: 7, batch: 4, loss: 3.273803234100342, acc: 40.625, f1: 13.700980392156865, r: 0.17859082984297062
06/02/2019 12:12:32 step: 241, epoch: 7, batch: 9, loss: 3.197087287902832, acc: 40.625, f1: 14.769012082444918, r: 0.19589052421968206
06/02/2019 12:12:33 step: 246, epoch: 7, batch: 14, loss: 3.4040253162384033, acc: 40.625, f1: 12.925170068027212, r: 0.17141070529593277
06/02/2019 12:12:34 step: 251, epoch: 7, batch: 19, loss: 3.1220169067382812, acc: 42.1875, f1: 11.16636528028933, r: 0.24706367618321684
06/02/2019 12:12:35 step: 256, epoch: 7, batch: 24, loss: 3.049556255340576, acc: 46.875, f1: 13.450292397660817, r: 0.16923524128624065
06/02/2019 12:12:36 step: 261, epoch: 7, batch: 29, loss: 3.0900073051452637, acc: 42.1875, f1: 12.279411764705882, r: 0.1689953050323061
06/02/2019 12:12:37 *** evaluating ***
06/02/2019 12:12:37 step: 8, epoch: 7, acc: 56.837606837606835, f1: 16.45733173076923, r: 0.31398673574429725
06/02/2019 12:12:37 *** epoch: 9 ***
06/02/2019 12:12:37 *** training ***
06/02/2019 12:12:38 step: 269, epoch: 8, batch: 4, loss: 2.797884464263916, acc: 50.0, f1: 18.222891566265062, r: 0.3689499007138412
06/02/2019 12:12:39 step: 274, epoch: 8, batch: 9, loss: 3.1388134956359863, acc: 43.75, f1: 13.403361344537815, r: 0.24993128260763497
06/02/2019 12:12:40 step: 279, epoch: 8, batch: 14, loss: 3.3609700202941895, acc: 45.3125, f1: 13.547317661241712, r: 0.24830954678405642
06/02/2019 12:12:41 step: 284, epoch: 8, batch: 19, loss: 2.66180157661438, acc: 62.5, f1: 20.158347676419965, r: 0.32777627740227516
06/02/2019 12:12:42 step: 289, epoch: 8, batch: 24, loss: 3.1121373176574707, acc: 40.625, f1: 12.301396244583534, r: 0.18119362067904654
06/02/2019 12:12:43 step: 294, epoch: 8, batch: 29, loss: 3.0463645458221436, acc: 50.0, f1: 18.165413533834585, r: 0.24282741521246673
06/02/2019 12:12:43 *** evaluating ***
06/02/2019 12:12:44 step: 9, epoch: 8, acc: 55.55555555555556, f1: 15.87375812584325, r: 0.3104360359772049
06/02/2019 12:12:44 *** epoch: 10 ***
06/02/2019 12:12:44 *** training ***
06/02/2019 12:12:45 step: 302, epoch: 9, batch: 4, loss: 2.765488624572754, acc: 51.5625, f1: 17.03384056325233, r: 0.2678848744532814
06/02/2019 12:12:46 step: 307, epoch: 9, batch: 9, loss: 3.1850857734680176, acc: 42.1875, f1: 15.421052631578947, r: 0.18337391749332338
06/02/2019 12:12:47 step: 312, epoch: 9, batch: 14, loss: 2.952548027038574, acc: 45.3125, f1: 13.637204826412022, r: 0.18464653515377383
06/02/2019 12:12:48 step: 317, epoch: 9, batch: 19, loss: 3.1589386463165283, acc: 50.0, f1: 18.855548307603105, r: 0.2708276314572678
06/02/2019 12:12:48 step: 322, epoch: 9, batch: 24, loss: 2.782212257385254, acc: 43.75, f1: 14.96476687154653, r: 0.19857834067602517
06/02/2019 12:12:49 step: 327, epoch: 9, batch: 29, loss: 2.769303798675537, acc: 45.3125, f1: 15.692456948823672, r: 0.175617308498187
06/02/2019 12:12:50 *** evaluating ***
06/02/2019 12:12:50 step: 10, epoch: 9, acc: 54.27350427350427, f1: 15.66325461674299, r: 0.30625312837004587
06/02/2019 12:12:50 *** epoch: 11 ***
06/02/2019 12:12:50 *** training ***
06/02/2019 12:12:51 step: 335, epoch: 10, batch: 4, loss: 3.0060694217681885, acc: 43.75, f1: 16.21917808219178, r: 0.21652764986754078
06/02/2019 12:12:52 step: 340, epoch: 10, batch: 9, loss: 2.931046485900879, acc: 40.625, f1: 17.389671361502344, r: 0.137758820568187
06/02/2019 12:12:53 step: 345, epoch: 10, batch: 14, loss: 3.0652425289154053, acc: 45.3125, f1: 17.526789131266742, r: 0.2576257046146915
06/02/2019 12:12:54 step: 350, epoch: 10, batch: 19, loss: 3.029092788696289, acc: 46.875, f1: 15.117540687160941, r: 0.1912001453417408
06/02/2019 12:12:55 step: 355, epoch: 10, batch: 24, loss: 3.577794313430786, acc: 45.3125, f1: 15.81140350877193, r: 0.20096570432649935
06/02/2019 12:12:57 step: 360, epoch: 10, batch: 29, loss: 3.0414955615997314, acc: 43.75, f1: 15.728715728715729, r: 0.1507012514228638
06/02/2019 12:12:57 *** evaluating ***
06/02/2019 12:12:57 step: 11, epoch: 10, acc: 56.837606837606835, f1: 16.431497869550967, r: 0.32898777986814165
06/02/2019 12:12:57 *** epoch: 12 ***
06/02/2019 12:12:57 *** training ***
06/02/2019 12:12:58 step: 368, epoch: 11, batch: 4, loss: 2.7763938903808594, acc: 40.625, f1: 15.936507936507937, r: 0.22353498408218575
06/02/2019 12:12:59 step: 373, epoch: 11, batch: 9, loss: 3.0722837448120117, acc: 42.1875, f1: 18.9157824933687, r: 0.22703553197880205
06/02/2019 12:13:00 step: 378, epoch: 11, batch: 14, loss: 2.7938380241394043, acc: 48.4375, f1: 17.79424098406486, r: 0.16782680709693085
06/02/2019 12:13:01 step: 383, epoch: 11, batch: 19, loss: 2.6452560424804688, acc: 54.6875, f1: 22.712912087912084, r: 0.24343143680177598
06/02/2019 12:13:02 step: 388, epoch: 11, batch: 24, loss: 2.6041808128356934, acc: 56.25, f1: 18.423645320197046, r: 0.2225209696940335
06/02/2019 12:13:03 step: 393, epoch: 11, batch: 29, loss: 2.7904934883117676, acc: 39.0625, f1: 13.38361431529133, r: 0.29042141589540205
06/02/2019 12:13:03 *** evaluating ***
06/02/2019 12:13:04 step: 12, epoch: 11, acc: 56.41025641025641, f1: 16.24181188975405, r: 0.31031888812356756
06/02/2019 12:13:04 *** epoch: 13 ***
06/02/2019 12:13:04 *** training ***
06/02/2019 12:13:05 step: 401, epoch: 12, batch: 4, loss: 2.624152660369873, acc: 45.3125, f1: 13.466947960618846, r: 0.24451082268233165
06/02/2019 12:13:06 step: 406, epoch: 12, batch: 9, loss: 2.9934887886047363, acc: 50.0, f1: 16.165413533834588, r: 0.20435088356641706
06/02/2019 12:13:07 step: 411, epoch: 12, batch: 14, loss: 2.5011067390441895, acc: 50.0, f1: 16.93121693121693, r: 0.2944048780680214
06/02/2019 12:13:08 step: 416, epoch: 12, batch: 19, loss: 2.60551118850708, acc: 51.5625, f1: 16.556966449207827, r: 0.3142929412779134
06/02/2019 12:13:09 step: 421, epoch: 12, batch: 24, loss: 2.728628158569336, acc: 43.75, f1: 14.068908941755536, r: 0.20256602048433756
06/02/2019 12:13:10 step: 426, epoch: 12, batch: 29, loss: 2.8594250679016113, acc: 37.5, f1: 13.480945539135195, r: 0.29228702932897166
06/02/2019 12:13:10 *** evaluating ***
06/02/2019 12:13:11 step: 13, epoch: 12, acc: 56.837606837606835, f1: 17.33731986552562, r: 0.3439279926846634
06/02/2019 12:13:11 *** epoch: 14 ***
06/02/2019 12:13:11 *** training ***
06/02/2019 12:13:12 step: 434, epoch: 13, batch: 4, loss: 2.8012328147888184, acc: 48.4375, f1: 15.177984665936476, r: 0.3341824834915339
06/02/2019 12:13:13 step: 439, epoch: 13, batch: 9, loss: 2.67750883102417, acc: 50.0, f1: 19.129586260733802, r: 0.335995422962244
06/02/2019 12:13:14 step: 444, epoch: 13, batch: 14, loss: 3.166797637939453, acc: 46.875, f1: 15.388257575757578, r: 0.2605861888513671
06/02/2019 12:13:15 step: 449, epoch: 13, batch: 19, loss: 2.794814109802246, acc: 37.5, f1: 14.976958525345621, r: 0.22063343806987712
06/02/2019 12:13:16 step: 454, epoch: 13, batch: 24, loss: 2.703573226928711, acc: 42.1875, f1: 12.931436567164182, r: 0.2275954864531627
06/02/2019 12:13:17 step: 459, epoch: 13, batch: 29, loss: 2.6062278747558594, acc: 57.8125, f1: 22.802562501445454, r: 0.23021254657613716
06/02/2019 12:13:17 *** evaluating ***
06/02/2019 12:13:18 step: 14, epoch: 13, acc: 55.55555555555556, f1: 16.297696749089997, r: 0.32968627719466503
06/02/2019 12:13:18 *** epoch: 15 ***
06/02/2019 12:13:18 *** training ***
06/02/2019 12:13:19 step: 467, epoch: 14, batch: 4, loss: 3.077686309814453, acc: 43.75, f1: 13.981481481481485, r: 0.2833145065201832
06/02/2019 12:13:20 step: 472, epoch: 14, batch: 9, loss: 2.992175579071045, acc: 50.0, f1: 19.46230827429672, r: 0.331934483562383
06/02/2019 12:13:21 step: 477, epoch: 14, batch: 14, loss: 2.6139793395996094, acc: 45.3125, f1: 16.103016662718154, r: 0.29806632701669045
06/02/2019 12:13:22 step: 482, epoch: 14, batch: 19, loss: 2.692995071411133, acc: 45.3125, f1: 17.413769026672252, r: 0.26363957210343547
06/02/2019 12:13:23 step: 487, epoch: 14, batch: 24, loss: 2.57283616065979, acc: 42.1875, f1: 16.398138202649484, r: 0.20440409089577555
06/02/2019 12:13:24 step: 492, epoch: 14, batch: 29, loss: 2.642754554748535, acc: 45.3125, f1: 16.91982534373839, r: 0.27937724126609087
06/02/2019 12:13:25 *** evaluating ***
06/02/2019 12:13:25 step: 15, epoch: 14, acc: 56.41025641025641, f1: 17.335525909169572, r: 0.3389657372232213
06/02/2019 12:13:25 *** epoch: 16 ***
06/02/2019 12:13:25 *** training ***
06/02/2019 12:13:26 step: 500, epoch: 15, batch: 4, loss: 2.375511884689331, acc: 51.5625, f1: 20.634920634920633, r: 0.26232661641471133
06/02/2019 12:13:27 step: 505, epoch: 15, batch: 9, loss: 2.5700087547302246, acc: 51.5625, f1: 17.346938775510207, r: 0.21866407831269866
06/02/2019 12:13:28 step: 510, epoch: 15, batch: 14, loss: 2.4881739616394043, acc: 48.4375, f1: 16.245916245916245, r: 0.24807788556385596
06/02/2019 12:13:29 step: 515, epoch: 15, batch: 19, loss: 2.603391170501709, acc: 42.1875, f1: 15.47008547008547, r: 0.25024483099135486
06/02/2019 12:13:30 step: 520, epoch: 15, batch: 24, loss: 2.571465015411377, acc: 54.6875, f1: 20.620996936786412, r: 0.2108253985450809
06/02/2019 12:13:31 step: 525, epoch: 15, batch: 29, loss: 2.5206332206726074, acc: 37.5, f1: 16.026410564225692, r: 0.3243098158992819
06/02/2019 12:13:31 *** evaluating ***
06/02/2019 12:13:32 step: 16, epoch: 15, acc: 56.837606837606835, f1: 17.693696660586813, r: 0.3348257512119439
06/02/2019 12:13:32 *** epoch: 17 ***
06/02/2019 12:13:32 *** training ***
06/02/2019 12:13:33 step: 533, epoch: 16, batch: 4, loss: 2.410919666290283, acc: 43.75, f1: 18.859649122807017, r: 0.3174983881682291
06/02/2019 12:13:34 step: 538, epoch: 16, batch: 9, loss: 2.2835938930511475, acc: 54.6875, f1: 20.154816264405305, r: 0.33519326078131506
06/02/2019 12:13:34 step: 543, epoch: 16, batch: 14, loss: 3.2049379348754883, acc: 51.5625, f1: 18.68466898954704, r: 0.22546768443358856
06/02/2019 12:13:36 step: 548, epoch: 16, batch: 19, loss: 2.570970058441162, acc: 50.0, f1: 15.203252032520325, r: 0.25225277266730756
06/02/2019 12:13:36 step: 553, epoch: 16, batch: 24, loss: 2.415179491043091, acc: 48.4375, f1: 16.918498168498168, r: 0.3181194641255365
06/02/2019 12:13:37 step: 558, epoch: 16, batch: 29, loss: 2.658228874206543, acc: 43.75, f1: 13.87468030690537, r: 0.22180093638194517
06/02/2019 12:13:38 *** evaluating ***
06/02/2019 12:13:38 step: 17, epoch: 16, acc: 56.837606837606835, f1: 17.98175819038372, r: 0.3399342949520947
06/02/2019 12:13:38 *** epoch: 18 ***
06/02/2019 12:13:38 *** training ***
06/02/2019 12:13:39 step: 566, epoch: 17, batch: 4, loss: 2.3557851314544678, acc: 46.875, f1: 18.340814115462003, r: 0.2527234563703393
06/02/2019 12:13:40 step: 571, epoch: 17, batch: 9, loss: 2.60082745552063, acc: 43.75, f1: 16.098484848484848, r: 0.3325124257017398
06/02/2019 12:13:41 step: 576, epoch: 17, batch: 14, loss: 2.416639804840088, acc: 45.3125, f1: 18.643892339544514, r: 0.26585132204760664
06/02/2019 12:13:42 step: 581, epoch: 17, batch: 19, loss: 2.7784156799316406, acc: 39.0625, f1: 15.170822427331586, r: 0.3306496898187
06/02/2019 12:13:43 step: 586, epoch: 17, batch: 24, loss: 2.9758453369140625, acc: 56.25, f1: 23.361473103297545, r: 0.32960558335107293
06/02/2019 12:13:44 step: 591, epoch: 17, batch: 29, loss: 2.345487594604492, acc: 46.875, f1: 16.330532212885153, r: 0.33988755933133236
06/02/2019 12:13:45 *** evaluating ***
06/02/2019 12:13:45 step: 18, epoch: 17, acc: 54.27350427350427, f1: 15.454509618416907, r: 0.36950800558201513
06/02/2019 12:13:45 *** epoch: 19 ***
06/02/2019 12:13:45 *** training ***
06/02/2019 12:13:46 step: 599, epoch: 18, batch: 4, loss: 2.3079423904418945, acc: 60.9375, f1: 26.639061421670117, r: 0.35742070036844686
06/02/2019 12:13:47 step: 604, epoch: 18, batch: 9, loss: 2.3947935104370117, acc: 53.125, f1: 16.860119047619047, r: 0.32821614927929066
06/02/2019 12:13:48 step: 609, epoch: 18, batch: 14, loss: 2.52238130569458, acc: 45.3125, f1: 19.472880061115355, r: 0.3643337766949178
06/02/2019 12:13:49 step: 614, epoch: 18, batch: 19, loss: 2.258713722229004, acc: 53.125, f1: 21.775510204081634, r: 0.3063777705435928
06/02/2019 12:13:50 step: 619, epoch: 18, batch: 24, loss: 2.6240973472595215, acc: 50.0, f1: 19.466873706004144, r: 0.26730681351209923
06/02/2019 12:13:51 step: 624, epoch: 18, batch: 29, loss: 2.4820899963378906, acc: 35.9375, f1: 12.027252502780867, r: 0.22834665691238198
06/02/2019 12:13:52 *** evaluating ***
06/02/2019 12:13:52 step: 19, epoch: 18, acc: 55.98290598290598, f1: 17.53627155762387, r: 0.36662258062711167
06/02/2019 12:13:52 *** epoch: 20 ***
06/02/2019 12:13:52 *** training ***
06/02/2019 12:13:53 step: 632, epoch: 19, batch: 4, loss: 2.1762237548828125, acc: 54.6875, f1: 21.05743563248084, r: 0.17317514866694378
06/02/2019 12:13:54 step: 637, epoch: 19, batch: 9, loss: 2.469367742538452, acc: 51.5625, f1: 20.85537918871252, r: 0.2049770072371424
06/02/2019 12:13:55 step: 642, epoch: 19, batch: 14, loss: 2.466376304626465, acc: 48.4375, f1: 18.565724815724817, r: 0.31176418427084635
06/02/2019 12:13:56 step: 647, epoch: 19, batch: 19, loss: 2.4096903800964355, acc: 46.875, f1: 31.14269066914864, r: 0.2642853387148945
06/02/2019 12:13:57 step: 652, epoch: 19, batch: 24, loss: 2.322280168533325, acc: 64.0625, f1: 19.093003770423124, r: 0.21042099782076035
06/02/2019 12:13:58 step: 657, epoch: 19, batch: 29, loss: 2.516803741455078, acc: 45.3125, f1: 16.944444444444443, r: 0.3055863815801512
06/02/2019 12:13:59 *** evaluating ***
06/02/2019 12:13:59 step: 20, epoch: 19, acc: 56.837606837606835, f1: 16.796398046398046, r: 0.3572907419395476
06/02/2019 12:13:59 *** epoch: 21 ***
06/02/2019 12:13:59 *** training ***
06/02/2019 12:14:00 step: 665, epoch: 20, batch: 4, loss: 2.508246660232544, acc: 42.1875, f1: 17.47146946210178, r: 0.21837128186133845
06/02/2019 12:14:01 step: 670, epoch: 20, batch: 9, loss: 2.3051629066467285, acc: 45.3125, f1: 17.49885478699038, r: 0.3480497845923292
06/02/2019 12:14:02 step: 675, epoch: 20, batch: 14, loss: 2.5052123069763184, acc: 46.875, f1: 18.259818259818257, r: 0.26610392820919626
06/02/2019 12:14:03 step: 680, epoch: 20, batch: 19, loss: 2.191640853881836, acc: 46.875, f1: 17.575339314469748, r: 0.2705669126160344
06/02/2019 12:14:04 step: 685, epoch: 20, batch: 24, loss: 2.2381505966186523, acc: 53.125, f1: 19.733355189465346, r: 0.33607679312403504
06/02/2019 12:14:05 step: 690, epoch: 20, batch: 29, loss: 2.6028971672058105, acc: 50.0, f1: 22.74436090225564, r: 0.3065119602569218
06/02/2019 12:14:06 *** evaluating ***
06/02/2019 12:14:06 step: 21, epoch: 20, acc: 56.837606837606835, f1: 16.85894863563403, r: 0.34873201396771564
06/02/2019 12:14:06 *** epoch: 22 ***
06/02/2019 12:14:06 *** training ***
06/02/2019 12:14:07 step: 698, epoch: 21, batch: 4, loss: 2.37359619140625, acc: 37.5, f1: 16.36141636141636, r: 0.32741346506323404
06/02/2019 12:14:08 step: 703, epoch: 21, batch: 9, loss: 2.223506450653076, acc: 46.875, f1: 19.242606133362436, r: 0.2967344329674343
06/02/2019 12:14:09 step: 708, epoch: 21, batch: 14, loss: 2.543382167816162, acc: 39.0625, f1: 21.561103073708114, r: 0.30711494187580585
06/02/2019 12:14:10 step: 713, epoch: 21, batch: 19, loss: 2.380997896194458, acc: 45.3125, f1: 15.038145539906106, r: 0.22312737236379993
06/02/2019 12:14:11 step: 718, epoch: 21, batch: 24, loss: 2.4861245155334473, acc: 53.125, f1: 17.81045751633987, r: 0.24127547311991684
06/02/2019 12:14:12 step: 723, epoch: 21, batch: 29, loss: 2.2780826091766357, acc: 43.75, f1: 19.269688644688642, r: 0.41110237541013184
06/02/2019 12:14:13 *** evaluating ***
06/02/2019 12:14:13 step: 22, epoch: 21, acc: 56.41025641025641, f1: 17.605140723417637, r: 0.363810564169408
06/02/2019 12:14:13 *** epoch: 23 ***
06/02/2019 12:14:13 *** training ***
06/02/2019 12:14:14 step: 731, epoch: 22, batch: 4, loss: 2.221374988555908, acc: 56.25, f1: 21.485260770975056, r: 0.270910221837499
06/02/2019 12:14:15 step: 736, epoch: 22, batch: 9, loss: 2.1234169006347656, acc: 56.25, f1: 20.837257980115123, r: 0.35257870062622443
06/02/2019 12:14:16 step: 741, epoch: 22, batch: 14, loss: 2.2958829402923584, acc: 54.6875, f1: 17.80952380952381, r: 0.27540444606929526
06/02/2019 12:14:17 step: 746, epoch: 22, batch: 19, loss: 2.050570011138916, acc: 62.5, f1: 22.88824693735623, r: 0.36382265515239604
06/02/2019 12:14:18 step: 751, epoch: 22, batch: 24, loss: 2.1911628246307373, acc: 48.4375, f1: 24.42156862745098, r: 0.2922128186675975
06/02/2019 12:14:19 step: 756, epoch: 22, batch: 29, loss: 2.318767547607422, acc: 54.6875, f1: 20.583717357910906, r: 0.37285694326719576
06/02/2019 12:14:20 *** evaluating ***
06/02/2019 12:14:20 step: 23, epoch: 22, acc: 56.837606837606835, f1: 21.75626543301125, r: 0.36116818758217967
06/02/2019 12:14:20 *** epoch: 24 ***
06/02/2019 12:14:20 *** training ***
06/02/2019 12:14:21 step: 764, epoch: 23, batch: 4, loss: 1.990143060684204, acc: 53.125, f1: 18.59545542354873, r: 0.38606374812800076
06/02/2019 12:14:22 step: 769, epoch: 23, batch: 9, loss: 1.932395339012146, acc: 54.6875, f1: 17.474204871822145, r: 0.3615426464013206
06/02/2019 12:14:23 step: 774, epoch: 23, batch: 14, loss: 2.240145444869995, acc: 50.0, f1: 25.577956989247312, r: 0.3316759144058279
06/02/2019 12:14:24 step: 779, epoch: 23, batch: 19, loss: 2.2413766384124756, acc: 56.25, f1: 19.276419276419276, r: 0.25594664715993964
06/02/2019 12:14:25 step: 784, epoch: 23, batch: 24, loss: 2.166194438934326, acc: 53.125, f1: 20.952380952380953, r: 0.26161590764227594
06/02/2019 12:14:26 step: 789, epoch: 23, batch: 29, loss: 2.3181700706481934, acc: 46.875, f1: 15.491905354919055, r: 0.2573849500989268
06/02/2019 12:14:27 *** evaluating ***
06/02/2019 12:14:27 step: 24, epoch: 23, acc: 55.55555555555556, f1: 19.484497979547058, r: 0.3446150214153983
06/02/2019 12:14:27 *** epoch: 25 ***
06/02/2019 12:14:27 *** training ***
06/02/2019 12:14:28 step: 797, epoch: 24, batch: 4, loss: 2.3894848823547363, acc: 51.5625, f1: 20.966533466533463, r: 0.3524983507539008
06/02/2019 12:14:29 step: 802, epoch: 24, batch: 9, loss: 2.4916200637817383, acc: 39.0625, f1: 15.783898305084747, r: 0.33172857469922484
06/02/2019 12:14:30 step: 807, epoch: 24, batch: 14, loss: 2.4099583625793457, acc: 42.1875, f1: 19.336677941476925, r: 0.3408127288350706
06/02/2019 12:14:31 step: 812, epoch: 24, batch: 19, loss: 2.1155571937561035, acc: 53.125, f1: 20.09259259259259, r: 0.3875935824986757
06/02/2019 12:14:32 step: 817, epoch: 24, batch: 24, loss: 2.2115862369537354, acc: 51.5625, f1: 19.208242737654505, r: 0.2718074731157648
06/02/2019 12:14:33 step: 822, epoch: 24, batch: 29, loss: 2.5360865592956543, acc: 48.4375, f1: 24.07051282051282, r: 0.3718370380967624
06/02/2019 12:14:34 *** evaluating ***
06/02/2019 12:14:34 step: 25, epoch: 24, acc: 55.98290598290598, f1: 20.486420795554064, r: 0.35058059228974303
06/02/2019 12:14:34 *** epoch: 26 ***
06/02/2019 12:14:34 *** training ***
06/02/2019 12:14:35 step: 830, epoch: 25, batch: 4, loss: 2.2147655487060547, acc: 45.3125, f1: 21.363826232247284, r: 0.3380858171589603
06/02/2019 12:14:36 step: 835, epoch: 25, batch: 9, loss: 2.180939197540283, acc: 56.25, f1: 25.641756495301273, r: 0.4355158488458959
06/02/2019 12:14:38 step: 840, epoch: 25, batch: 14, loss: 2.036754846572876, acc: 62.5, f1: 26.93995460485591, r: 0.3278579637659482
06/02/2019 12:14:39 step: 845, epoch: 25, batch: 19, loss: 2.013847827911377, acc: 50.0, f1: 24.117822318526542, r: 0.36235032551769014
06/02/2019 12:14:40 step: 850, epoch: 25, batch: 24, loss: 2.321133613586426, acc: 42.1875, f1: 20.967711028073204, r: 0.284717732545584
06/02/2019 12:14:41 step: 855, epoch: 25, batch: 29, loss: 2.08036732673645, acc: 54.6875, f1: 27.294110459433043, r: 0.24233690641957945
06/02/2019 12:14:41 *** evaluating ***
06/02/2019 12:14:42 step: 26, epoch: 25, acc: 54.27350427350427, f1: 17.77288872056543, r: 0.3727484356701184
06/02/2019 12:14:42 *** epoch: 27 ***
06/02/2019 12:14:42 *** training ***
06/02/2019 12:14:42 step: 863, epoch: 26, batch: 4, loss: 2.145859479904175, acc: 34.375, f1: 15.575396825396828, r: 0.351427029084402
06/02/2019 12:14:43 step: 868, epoch: 26, batch: 9, loss: 2.1745145320892334, acc: 43.75, f1: 16.76693404634581, r: 0.366319407948942
06/02/2019 12:14:44 step: 873, epoch: 26, batch: 14, loss: 2.0120930671691895, acc: 60.9375, f1: 36.049373162049214, r: 0.3650148335173537
06/02/2019 12:14:45 step: 878, epoch: 26, batch: 19, loss: 1.9067776203155518, acc: 45.3125, f1: 24.77383282079955, r: 0.38441102247522
06/02/2019 12:14:46 step: 883, epoch: 26, batch: 24, loss: 2.2504682540893555, acc: 48.4375, f1: 22.50440917107584, r: 0.27412946216989387
06/02/2019 12:14:47 step: 888, epoch: 26, batch: 29, loss: 2.0887677669525146, acc: 60.9375, f1: 21.03968253968254, r: 0.3974142775336292
06/02/2019 12:14:48 *** evaluating ***
06/02/2019 12:14:48 step: 27, epoch: 26, acc: 55.55555555555556, f1: 19.886950904392762, r: 0.3468567345606778
06/02/2019 12:14:48 *** epoch: 28 ***
06/02/2019 12:14:48 *** training ***
06/02/2019 12:14:49 step: 896, epoch: 27, batch: 4, loss: 2.0349206924438477, acc: 60.9375, f1: 39.91388863237603, r: 0.38755121011972654
06/02/2019 12:14:50 step: 901, epoch: 27, batch: 9, loss: 1.7022874355316162, acc: 65.625, f1: 40.25391974971807, r: 0.4554097809025762
06/02/2019 12:14:51 step: 906, epoch: 27, batch: 14, loss: 1.9059746265411377, acc: 57.8125, f1: 22.631758261917863, r: 0.32061283864780554
06/02/2019 12:14:52 step: 911, epoch: 27, batch: 19, loss: 1.9577293395996094, acc: 59.375, f1: 18.109243697478995, r: 0.28965607277669086
06/02/2019 12:14:53 step: 916, epoch: 27, batch: 24, loss: 2.0565409660339355, acc: 46.875, f1: 20.988427422250954, r: 0.367045268037736
06/02/2019 12:14:54 step: 921, epoch: 27, batch: 29, loss: 2.1367666721343994, acc: 53.125, f1: 19.118773946360154, r: 0.28639380084843086
06/02/2019 12:14:55 *** evaluating ***
06/02/2019 12:14:55 step: 28, epoch: 27, acc: 55.55555555555556, f1: 21.530069673634028, r: 0.3491914020903957
06/02/2019 12:14:55 *** epoch: 29 ***
06/02/2019 12:14:55 *** training ***
06/02/2019 12:14:56 step: 929, epoch: 28, batch: 4, loss: 1.963172435760498, acc: 51.5625, f1: 26.909821816601482, r: 0.3964743087561655
06/02/2019 12:14:57 step: 934, epoch: 28, batch: 9, loss: 2.137035608291626, acc: 46.875, f1: 33.00556586270872, r: 0.3555704590288402
06/02/2019 12:14:58 step: 939, epoch: 28, batch: 14, loss: 1.9629260301589966, acc: 60.9375, f1: 31.613361762615494, r: 0.46537891704338336
06/02/2019 12:14:59 step: 944, epoch: 28, batch: 19, loss: 2.0387909412384033, acc: 46.875, f1: 18.839285714285715, r: 0.3365623418828088
06/02/2019 12:15:00 step: 949, epoch: 28, batch: 24, loss: 1.7046492099761963, acc: 71.875, f1: 27.42450638792102, r: 0.40743759803740287
06/02/2019 12:15:01 step: 954, epoch: 28, batch: 29, loss: 2.0851705074310303, acc: 51.5625, f1: 29.476190476190478, r: 0.4878207057246134
06/02/2019 12:15:02 *** evaluating ***
06/02/2019 12:15:02 step: 29, epoch: 28, acc: 57.692307692307686, f1: 22.09959495541447, r: 0.36894276879072263
06/02/2019 12:15:02 *** epoch: 30 ***
06/02/2019 12:15:02 *** training ***
06/02/2019 12:15:03 step: 962, epoch: 29, batch: 4, loss: 1.9569976329803467, acc: 50.0, f1: 33.77176957739617, r: 0.4171709172306552
06/02/2019 12:15:04 step: 967, epoch: 29, batch: 9, loss: 1.7320177555084229, acc: 65.625, f1: 31.20530877573131, r: 0.44551793610797963
06/02/2019 12:15:05 step: 972, epoch: 29, batch: 14, loss: 2.1263322830200195, acc: 54.6875, f1: 24.702380952380956, r: 0.3068673012910931
06/02/2019 12:15:06 step: 977, epoch: 29, batch: 19, loss: 2.1170265674591064, acc: 45.3125, f1: 22.992199255563474, r: 0.3546013928213075
06/02/2019 12:15:07 step: 982, epoch: 29, batch: 24, loss: 2.0912322998046875, acc: 59.375, f1: 36.36839351125066, r: 0.4223207447072138
06/02/2019 12:15:08 step: 987, epoch: 29, batch: 29, loss: 2.02012300491333, acc: 54.6875, f1: 26.9510582010582, r: 0.39550632534501934
06/02/2019 12:15:08 *** evaluating ***
06/02/2019 12:15:09 step: 30, epoch: 29, acc: 55.98290598290598, f1: 20.93413212742724, r: 0.3408086406039857
06/02/2019 12:15:09 *** epoch: 31 ***
06/02/2019 12:15:09 *** training ***
06/02/2019 12:15:10 step: 995, epoch: 30, batch: 4, loss: 1.8726744651794434, acc: 51.5625, f1: 28.779342723004696, r: 0.42636204498048014
06/02/2019 12:15:11 step: 1000, epoch: 30, batch: 9, loss: 1.81884765625, acc: 56.25, f1: 40.47802480005869, r: 0.35097133920332
06/02/2019 12:15:12 step: 1005, epoch: 30, batch: 14, loss: 2.4313411712646484, acc: 42.1875, f1: 18.26754385964912, r: 0.3096009373203058
06/02/2019 12:15:13 step: 1010, epoch: 30, batch: 19, loss: 2.074758529663086, acc: 53.125, f1: 33.51839660595421, r: 0.40655483343715854
06/02/2019 12:15:14 step: 1015, epoch: 30, batch: 24, loss: 1.7788106203079224, acc: 59.375, f1: 43.558693527136725, r: 0.4679718415288368
06/02/2019 12:15:15 step: 1020, epoch: 30, batch: 29, loss: 2.1166751384735107, acc: 51.5625, f1: 27.83403959874548, r: 0.4604563658966916
06/02/2019 12:15:15 *** evaluating ***
06/02/2019 12:15:16 step: 31, epoch: 30, acc: 52.991452991452995, f1: 21.064177036934993, r: 0.33461423811434937
06/02/2019 12:15:16 *** epoch: 32 ***
06/02/2019 12:15:16 *** training ***
06/02/2019 12:15:17 step: 1028, epoch: 31, batch: 4, loss: 1.8033651113510132, acc: 51.5625, f1: 22.950705208769726, r: 0.40956890438093096
06/02/2019 12:15:18 step: 1033, epoch: 31, batch: 9, loss: 1.772047519683838, acc: 53.125, f1: 27.441696375519907, r: 0.42551330388511366
06/02/2019 12:15:19 step: 1038, epoch: 31, batch: 14, loss: 1.9731431007385254, acc: 48.4375, f1: 17.508197192820983, r: 0.40684031157392614
06/02/2019 12:15:20 step: 1043, epoch: 31, batch: 19, loss: 1.9084787368774414, acc: 51.5625, f1: 28.948336091193237, r: 0.3180510732262215
06/02/2019 12:15:20 step: 1048, epoch: 31, batch: 24, loss: 1.8451039791107178, acc: 53.125, f1: 28.86109282422647, r: 0.3297223289168515
06/02/2019 12:15:21 step: 1053, epoch: 31, batch: 29, loss: 2.021740198135376, acc: 46.875, f1: 17.6271645021645, r: 0.3474290530050967
06/02/2019 12:15:22 *** evaluating ***
06/02/2019 12:15:22 step: 32, epoch: 31, acc: 55.55555555555556, f1: 20.7317546031475, r: 0.3518383790294273
06/02/2019 12:15:22 *** epoch: 33 ***
06/02/2019 12:15:22 *** training ***
06/02/2019 12:15:23 step: 1061, epoch: 32, batch: 4, loss: 1.716658592224121, acc: 51.5625, f1: 28.667182662538703, r: 0.38763023678611885
06/02/2019 12:15:24 step: 1066, epoch: 32, batch: 9, loss: 1.777212381362915, acc: 59.375, f1: 27.967061245496655, r: 0.3787548343036625
06/02/2019 12:15:25 step: 1071, epoch: 32, batch: 14, loss: 1.740162968635559, acc: 53.125, f1: 19.81046365914787, r: 0.38742778156077345
06/02/2019 12:15:26 step: 1076, epoch: 32, batch: 19, loss: 1.7904691696166992, acc: 59.375, f1: 31.359320339830088, r: 0.4494981667746694
06/02/2019 12:15:27 step: 1081, epoch: 32, batch: 24, loss: 2.030682325363159, acc: 65.625, f1: 43.98955362544292, r: 0.34192472773510485
06/02/2019 12:15:28 step: 1086, epoch: 32, batch: 29, loss: 1.9907737970352173, acc: 53.125, f1: 19.173867504582354, r: 0.29676741778107496
06/02/2019 12:15:29 *** evaluating ***
06/02/2019 12:15:29 step: 33, epoch: 32, acc: 55.12820512820513, f1: 22.313067017851917, r: 0.34107267866357205
06/02/2019 12:15:29 *** epoch: 34 ***
06/02/2019 12:15:29 *** training ***
06/02/2019 12:15:30 step: 1094, epoch: 33, batch: 4, loss: 1.9365084171295166, acc: 39.0625, f1: 12.820512820512823, r: 0.3656543874549923
06/02/2019 12:15:31 step: 1099, epoch: 33, batch: 9, loss: 1.7618639469146729, acc: 64.0625, f1: 21.7948717948718, r: 0.3954739093570208
06/02/2019 12:15:32 step: 1104, epoch: 33, batch: 14, loss: 1.8641119003295898, acc: 45.3125, f1: 28.42845612821399, r: 0.4020746963993482
06/02/2019 12:15:33 step: 1109, epoch: 33, batch: 19, loss: 1.9161696434020996, acc: 56.25, f1: 31.67335115864528, r: 0.3817499669060104
06/02/2019 12:15:34 step: 1114, epoch: 33, batch: 24, loss: 1.901633620262146, acc: 53.125, f1: 31.27104377104377, r: 0.3755165998823007
06/02/2019 12:15:35 step: 1119, epoch: 33, batch: 29, loss: 1.920990228652954, acc: 48.4375, f1: 21.367243867243868, r: 0.38595035334028077
06/02/2019 12:15:36 *** evaluating ***
06/02/2019 12:15:36 step: 34, epoch: 33, acc: 55.55555555555556, f1: 23.690537921157574, r: 0.356847025233766
06/02/2019 12:15:36 *** epoch: 35 ***
06/02/2019 12:15:36 *** training ***
06/02/2019 12:15:37 step: 1127, epoch: 34, batch: 4, loss: 1.5660409927368164, acc: 57.8125, f1: 30.033606078316772, r: 0.40661947894551553
06/02/2019 12:15:38 step: 1132, epoch: 34, batch: 9, loss: 1.791212797164917, acc: 62.5, f1: 43.813505335244464, r: 0.35764068326032356
06/02/2019 12:15:39 step: 1137, epoch: 34, batch: 14, loss: 1.8372998237609863, acc: 50.0, f1: 23.333333333333332, r: 0.36548537006414755
06/02/2019 12:15:40 step: 1142, epoch: 34, batch: 19, loss: 1.7229154109954834, acc: 51.5625, f1: 25.780923041153226, r: 0.44658611398612796
06/02/2019 12:15:41 step: 1147, epoch: 34, batch: 24, loss: 1.7641360759735107, acc: 51.5625, f1: 17.893755824790308, r: 0.4036378103789612
06/02/2019 12:15:42 step: 1152, epoch: 34, batch: 29, loss: 1.6310365200042725, acc: 64.0625, f1: 41.122035858877965, r: 0.37888024880200305
06/02/2019 12:15:43 *** evaluating ***
06/02/2019 12:15:43 step: 35, epoch: 34, acc: 56.41025641025641, f1: 23.096527259148615, r: 0.34436138745380623
06/02/2019 12:15:43 *** epoch: 36 ***
06/02/2019 12:15:43 *** training ***
06/02/2019 12:15:44 step: 1160, epoch: 35, batch: 4, loss: 1.6712970733642578, acc: 51.5625, f1: 35.714285714285715, r: 0.5378441140561981
06/02/2019 12:15:45 step: 1165, epoch: 35, batch: 9, loss: 1.570012092590332, acc: 53.125, f1: 30.35590277777778, r: 0.36356180651352416
06/02/2019 12:15:46 step: 1170, epoch: 35, batch: 14, loss: 1.5751872062683105, acc: 59.375, f1: 28.34299291442149, r: 0.37969673563045025
06/02/2019 12:15:47 step: 1175, epoch: 35, batch: 19, loss: 1.6982910633087158, acc: 51.5625, f1: 32.518461930226636, r: 0.4254416435203316
06/02/2019 12:15:48 step: 1180, epoch: 35, batch: 24, loss: 1.5040074586868286, acc: 62.5, f1: 23.621355060034304, r: 0.3749237934540597
06/02/2019 12:15:49 step: 1185, epoch: 35, batch: 29, loss: 1.708694577217102, acc: 48.4375, f1: 20.38018105793042, r: 0.38391141146674845
06/02/2019 12:15:50 *** evaluating ***
06/02/2019 12:15:50 step: 36, epoch: 35, acc: 56.41025641025641, f1: 25.160811108946067, r: 0.3504869919968898
06/02/2019 12:15:50 *** epoch: 37 ***
06/02/2019 12:15:50 *** training ***
06/02/2019 12:15:51 step: 1193, epoch: 36, batch: 4, loss: 1.511812448501587, acc: 64.0625, f1: 36.765005618124334, r: 0.3467819251579462
06/02/2019 12:15:52 step: 1198, epoch: 36, batch: 9, loss: 1.6523525714874268, acc: 48.4375, f1: 24.541979382332645, r: 0.4875074139926402
06/02/2019 12:15:53 step: 1203, epoch: 36, batch: 14, loss: 1.65065336227417, acc: 54.6875, f1: 33.021026592455165, r: 0.4452294080652388
06/02/2019 12:15:54 step: 1208, epoch: 36, batch: 19, loss: 1.6974191665649414, acc: 59.375, f1: 23.00084175084175, r: 0.4280194360010158
06/02/2019 12:15:55 step: 1213, epoch: 36, batch: 24, loss: 1.6073131561279297, acc: 53.125, f1: 33.876553553972904, r: 0.4147955914521704
06/02/2019 12:15:56 step: 1218, epoch: 36, batch: 29, loss: 1.5699150562286377, acc: 50.0, f1: 19.027668674172997, r: 0.38342376428572006
06/02/2019 12:15:57 *** evaluating ***
06/02/2019 12:15:57 step: 37, epoch: 36, acc: 56.837606837606835, f1: 23.398084533742928, r: 0.3435015244409977
06/02/2019 12:15:57 *** epoch: 38 ***
06/02/2019 12:15:57 *** training ***
06/02/2019 12:15:58 step: 1226, epoch: 37, batch: 4, loss: 1.5793116092681885, acc: 57.8125, f1: 41.07302336509836, r: 0.4286947431645224
06/02/2019 12:15:59 step: 1231, epoch: 37, batch: 9, loss: 1.3396676778793335, acc: 59.375, f1: 26.629901960784313, r: 0.521844147278711
06/02/2019 12:16:00 step: 1236, epoch: 37, batch: 14, loss: 1.7572739124298096, acc: 64.0625, f1: 30.618686868686872, r: 0.4088542437128246
06/02/2019 12:16:01 step: 1241, epoch: 37, batch: 19, loss: 1.5174400806427002, acc: 53.125, f1: 19.293614881850175, r: 0.3902592498820036
06/02/2019 12:16:02 step: 1246, epoch: 37, batch: 24, loss: 1.5674140453338623, acc: 57.8125, f1: 45.82390648567119, r: 0.4075519257357903
06/02/2019 12:16:03 step: 1251, epoch: 37, batch: 29, loss: 1.4035403728485107, acc: 64.0625, f1: 43.01864801864802, r: 0.5124584857237692
06/02/2019 12:16:04 *** evaluating ***
06/02/2019 12:16:04 step: 38, epoch: 37, acc: 56.837606837606835, f1: 25.01846898526165, r: 0.34984644622347827
06/02/2019 12:16:04 *** epoch: 39 ***
06/02/2019 12:16:04 *** training ***
06/02/2019 12:16:05 step: 1259, epoch: 38, batch: 4, loss: 1.9702131748199463, acc: 50.0, f1: 29.5021186440678, r: 0.4522149289645081
06/02/2019 12:16:06 step: 1264, epoch: 38, batch: 9, loss: 1.4674277305603027, acc: 51.5625, f1: 23.224930806898016, r: 0.46542578345861235
06/02/2019 12:16:07 step: 1269, epoch: 38, batch: 14, loss: 1.5389277935028076, acc: 54.6875, f1: 32.43650793650794, r: 0.3423782537481351
06/02/2019 12:16:08 step: 1274, epoch: 38, batch: 19, loss: 1.5244312286376953, acc: 60.9375, f1: 41.79108118657299, r: 0.4357722768504016
06/02/2019 12:16:09 step: 1279, epoch: 38, batch: 24, loss: 1.386186122894287, acc: 57.8125, f1: 29.046934865900386, r: 0.46753230216460345
06/02/2019 12:16:10 step: 1284, epoch: 38, batch: 29, loss: 1.3034230470657349, acc: 62.5, f1: 34.6785576293773, r: 0.3899353558379959
06/02/2019 12:16:10 *** evaluating ***
06/02/2019 12:16:11 step: 39, epoch: 38, acc: 56.837606837606835, f1: 22.180250305250304, r: 0.3647058266326899
06/02/2019 12:16:11 *** epoch: 40 ***
06/02/2019 12:16:11 *** training ***
06/02/2019 12:16:11 step: 1292, epoch: 39, batch: 4, loss: 1.3715918064117432, acc: 59.375, f1: 26.430976430976433, r: 0.42177005411434776
06/02/2019 12:16:12 step: 1297, epoch: 39, batch: 9, loss: 1.2782843112945557, acc: 54.6875, f1: 37.06835537344012, r: 0.44986543240617705
06/02/2019 12:16:13 step: 1302, epoch: 39, batch: 14, loss: 1.0875085592269897, acc: 67.1875, f1: 31.126272600926974, r: 0.4989194291290303
06/02/2019 12:16:14 step: 1307, epoch: 39, batch: 19, loss: 1.4006656408309937, acc: 60.9375, f1: 38.22027298039292, r: 0.36997169391617163
06/02/2019 12:16:15 step: 1312, epoch: 39, batch: 24, loss: 1.0065405368804932, acc: 71.875, f1: 35.1542006380716, r: 0.46225371944964244
06/02/2019 12:16:17 step: 1317, epoch: 39, batch: 29, loss: 1.336309552192688, acc: 57.8125, f1: 43.747320790664446, r: 0.39912360445413303
06/02/2019 12:16:17 *** evaluating ***
06/02/2019 12:16:18 step: 40, epoch: 39, acc: 56.837606837606835, f1: 24.636903343078643, r: 0.35626827132374267
06/02/2019 12:16:18 *** epoch: 41 ***
06/02/2019 12:16:18 *** training ***
06/02/2019 12:16:19 step: 1325, epoch: 40, batch: 4, loss: 1.2181084156036377, acc: 53.125, f1: 25.914905014502597, r: 0.4015818106198237
06/02/2019 12:16:20 step: 1330, epoch: 40, batch: 9, loss: 1.3383756875991821, acc: 57.8125, f1: 34.92424242424242, r: 0.4716029497822518
06/02/2019 12:16:20 step: 1335, epoch: 40, batch: 14, loss: 1.1430824995040894, acc: 59.375, f1: 34.8376910445876, r: 0.42995219361988685
06/02/2019 12:16:22 step: 1340, epoch: 40, batch: 19, loss: 1.0236270427703857, acc: 67.1875, f1: 38.109789370293576, r: 0.40337352260652465
06/02/2019 12:16:23 step: 1345, epoch: 40, batch: 24, loss: 1.3580563068389893, acc: 40.625, f1: 21.026999316473, r: 0.5237047617516105
06/02/2019 12:16:24 step: 1350, epoch: 40, batch: 29, loss: 1.2925662994384766, acc: 60.9375, f1: 25.775729646697386, r: 0.3920446735103287
06/02/2019 12:16:24 *** evaluating ***
06/02/2019 12:16:25 step: 41, epoch: 40, acc: 56.41025641025641, f1: 25.38075216203196, r: 0.36507679617079664
06/02/2019 12:16:25 *** epoch: 42 ***
06/02/2019 12:16:25 *** training ***
06/02/2019 12:16:26 step: 1358, epoch: 41, batch: 4, loss: 1.192298412322998, acc: 46.875, f1: 24.044871794871796, r: 0.44080183428561015
06/02/2019 12:16:27 step: 1363, epoch: 41, batch: 9, loss: 1.2523714303970337, acc: 46.875, f1: 22.708719851576994, r: 0.3595877155401934
06/02/2019 12:16:28 step: 1368, epoch: 41, batch: 14, loss: 1.2411019802093506, acc: 54.6875, f1: 34.48930296756384, r: 0.44633681673019804
06/02/2019 12:16:29 step: 1373, epoch: 41, batch: 19, loss: 1.1932663917541504, acc: 57.8125, f1: 28.309424833038026, r: 0.40165758713155847
06/02/2019 12:16:30 step: 1378, epoch: 41, batch: 24, loss: 0.95970618724823, acc: 70.3125, f1: 31.584564860426934, r: 0.4477471641668702
06/02/2019 12:16:31 step: 1383, epoch: 41, batch: 29, loss: 1.030756950378418, acc: 68.75, f1: 41.15412285132633, r: 0.42778776841003513
06/02/2019 12:16:32 *** evaluating ***
06/02/2019 12:16:32 step: 42, epoch: 41, acc: 56.41025641025641, f1: 24.699333485385733, r: 0.37055845836808554
06/02/2019 12:16:32 *** epoch: 43 ***
06/02/2019 12:16:32 *** training ***
06/02/2019 12:16:33 step: 1391, epoch: 42, batch: 4, loss: 1.1936726570129395, acc: 51.5625, f1: 25.860805860805858, r: 0.3836692488360719
06/02/2019 12:16:34 step: 1396, epoch: 42, batch: 9, loss: 1.1253656148910522, acc: 60.9375, f1: 40.1109917651271, r: 0.4424358584197351
06/02/2019 12:16:35 step: 1401, epoch: 42, batch: 14, loss: 1.1693296432495117, acc: 57.8125, f1: 39.2032967032967, r: 0.43316587790790323
06/02/2019 12:16:36 step: 1406, epoch: 42, batch: 19, loss: 1.169438123703003, acc: 60.9375, f1: 32.88165266106443, r: 0.47589750323396784
06/02/2019 12:16:37 step: 1411, epoch: 42, batch: 24, loss: 1.0815010070800781, acc: 57.8125, f1: 37.3078612209047, r: 0.5167000891365454
06/02/2019 12:16:38 step: 1416, epoch: 42, batch: 29, loss: 1.0986850261688232, acc: 57.8125, f1: 37.43066033515071, r: 0.526906162646653
06/02/2019 12:16:38 *** evaluating ***
06/02/2019 12:16:39 step: 43, epoch: 42, acc: 56.837606837606835, f1: 25.540815369162335, r: 0.3646555927743751
06/02/2019 12:16:39 *** epoch: 44 ***
06/02/2019 12:16:39 *** training ***
06/02/2019 12:16:39 step: 1424, epoch: 43, batch: 4, loss: 1.0727473497390747, acc: 64.0625, f1: 55.69817400644468, r: 0.48318368412482465
06/02/2019 12:16:40 step: 1429, epoch: 43, batch: 9, loss: 0.9458842277526855, acc: 71.875, f1: 43.130995988138835, r: 0.45536702622237635
06/02/2019 12:16:41 step: 1434, epoch: 43, batch: 14, loss: 1.0614426136016846, acc: 62.5, f1: 25.95899470899471, r: 0.46062327835218647
06/02/2019 12:16:42 step: 1439, epoch: 43, batch: 19, loss: 1.1097238063812256, acc: 59.375, f1: 27.49536178107607, r: 0.41964000537367935
06/02/2019 12:16:43 step: 1444, epoch: 43, batch: 24, loss: 1.2843427658081055, acc: 56.25, f1: 36.478359459725915, r: 0.4290210261989274
06/02/2019 12:16:44 step: 1449, epoch: 43, batch: 29, loss: 1.1079065799713135, acc: 60.9375, f1: 29.5990120990121, r: 0.45432454188218174
06/02/2019 12:16:45 *** evaluating ***
06/02/2019 12:16:45 step: 44, epoch: 43, acc: 56.41025641025641, f1: 26.912590411658734, r: 0.36332341449926064
06/02/2019 12:16:45 *** epoch: 45 ***
06/02/2019 12:16:45 *** training ***
06/02/2019 12:16:47 step: 1457, epoch: 44, batch: 4, loss: 0.9768087863922119, acc: 64.0625, f1: 63.46155415676235, r: 0.5365388554534447
06/02/2019 12:16:48 step: 1462, epoch: 44, batch: 9, loss: 1.0460349321365356, acc: 57.8125, f1: 31.592653508771928, r: 0.5212175868820083
06/02/2019 12:16:49 step: 1467, epoch: 44, batch: 14, loss: 1.0818380117416382, acc: 59.375, f1: 35.2048504478172, r: 0.3690554209881504
06/02/2019 12:16:50 step: 1472, epoch: 44, batch: 19, loss: 1.087180733680725, acc: 57.8125, f1: 29.252940042413726, r: 0.412311735374437
06/02/2019 12:16:51 step: 1477, epoch: 44, batch: 24, loss: 1.2041913270950317, acc: 59.375, f1: 36.71365022242216, r: 0.44659828460834605
06/02/2019 12:16:52 step: 1482, epoch: 44, batch: 29, loss: 0.9267456531524658, acc: 68.75, f1: 43.41372912801485, r: 0.4280931096294127
06/02/2019 12:16:52 *** evaluating ***
06/02/2019 12:16:53 step: 45, epoch: 44, acc: 57.692307692307686, f1: 27.412620797241736, r: 0.35906147209584277
06/02/2019 12:16:53 *** epoch: 46 ***
06/02/2019 12:16:53 *** training ***
06/02/2019 12:16:54 step: 1490, epoch: 45, batch: 4, loss: 1.1540062427520752, acc: 59.375, f1: 32.9724854582394, r: 0.45667935269833343
06/02/2019 12:16:54 step: 1495, epoch: 45, batch: 9, loss: 0.9504120349884033, acc: 62.5, f1: 42.11682112025348, r: 0.5068274080450885
06/02/2019 12:16:55 step: 1500, epoch: 45, batch: 14, loss: 1.0575863122940063, acc: 62.5, f1: 36.48720682302772, r: 0.5274881861999294
06/02/2019 12:16:56 step: 1505, epoch: 45, batch: 19, loss: 0.9750705361366272, acc: 60.9375, f1: 28.509160179471827, r: 0.4795748403478275
06/02/2019 12:16:58 step: 1510, epoch: 45, batch: 24, loss: 1.0236226320266724, acc: 60.9375, f1: 36.09427214841489, r: 0.463281258408356
06/02/2019 12:16:58 step: 1515, epoch: 45, batch: 29, loss: 1.3565196990966797, acc: 65.625, f1: 43.729270315091206, r: 0.5129432736221152
06/02/2019 12:16:59 *** evaluating ***
06/02/2019 12:16:59 step: 46, epoch: 45, acc: 58.119658119658126, f1: 27.116908422822807, r: 0.36890417663426
06/02/2019 12:16:59 *** epoch: 47 ***
06/02/2019 12:16:59 *** training ***
06/02/2019 12:17:00 step: 1523, epoch: 46, batch: 4, loss: 0.9324369430541992, acc: 67.1875, f1: 43.23067283593599, r: 0.5338442603711875
06/02/2019 12:17:02 step: 1528, epoch: 46, batch: 9, loss: 1.1158266067504883, acc: 53.125, f1: 24.88214418136554, r: 0.40062474825931166
06/02/2019 12:17:03 step: 1533, epoch: 46, batch: 14, loss: 1.1557893753051758, acc: 59.375, f1: 41.378621378621375, r: 0.4459081517935935
06/02/2019 12:17:03 step: 1538, epoch: 46, batch: 19, loss: 0.9120054841041565, acc: 68.75, f1: 47.3702301433394, r: 0.49973521042388813
06/02/2019 12:17:04 step: 1543, epoch: 46, batch: 24, loss: 0.9759907722473145, acc: 67.1875, f1: 35.691699604743086, r: 0.5135164587714435
06/02/2019 12:17:06 step: 1548, epoch: 46, batch: 29, loss: 1.079232096672058, acc: 60.9375, f1: 36.123642439431904, r: 0.4325584091034053
06/02/2019 12:17:06 *** evaluating ***
06/02/2019 12:17:06 step: 47, epoch: 46, acc: 55.12820512820513, f1: 23.857352442299927, r: 0.37075001525994283
06/02/2019 12:17:06 *** epoch: 48 ***
06/02/2019 12:17:06 *** training ***
06/02/2019 12:17:07 step: 1556, epoch: 47, batch: 4, loss: 0.9591016173362732, acc: 71.875, f1: 28.797929544198198, r: 0.4958329634984081
06/02/2019 12:17:08 step: 1561, epoch: 47, batch: 9, loss: 0.9676609635353088, acc: 60.9375, f1: 34.09125188536953, r: 0.49971105100953955
06/02/2019 12:17:10 step: 1566, epoch: 47, batch: 14, loss: 0.9212575554847717, acc: 65.625, f1: 53.690699964196206, r: 0.44668397343420574
06/02/2019 12:17:11 step: 1571, epoch: 47, batch: 19, loss: 1.0991721153259277, acc: 54.6875, f1: 22.25361663652803, r: 0.5397450931045387
06/02/2019 12:17:12 step: 1576, epoch: 47, batch: 24, loss: 1.0868655443191528, acc: 56.25, f1: 40.86270871985157, r: 0.47165479303238234
06/02/2019 12:17:13 step: 1581, epoch: 47, batch: 29, loss: 1.0122376680374146, acc: 59.375, f1: 38.93207773993188, r: 0.466208975589156
06/02/2019 12:17:13 *** evaluating ***
06/02/2019 12:17:13 step: 48, epoch: 47, acc: 58.119658119658126, f1: 28.948521872597432, r: 0.3607233273747535
06/02/2019 12:17:13 *** epoch: 49 ***
06/02/2019 12:17:13 *** training ***
06/02/2019 12:17:14 step: 1589, epoch: 48, batch: 4, loss: 0.9267662763595581, acc: 64.0625, f1: 43.121264700212066, r: 0.4880049228545269
06/02/2019 12:17:15 step: 1594, epoch: 48, batch: 9, loss: 1.001632809638977, acc: 68.75, f1: 45.38690476190476, r: 0.4874017456653994
06/02/2019 12:17:16 step: 1599, epoch: 48, batch: 14, loss: 1.4834734201431274, acc: 60.9375, f1: 39.59018640350877, r: 0.5646148808165508
06/02/2019 12:17:17 step: 1604, epoch: 48, batch: 19, loss: 1.079114317893982, acc: 59.375, f1: 39.682440688320945, r: 0.5027066664448523
06/02/2019 12:17:18 step: 1609, epoch: 48, batch: 24, loss: 0.9772467017173767, acc: 64.0625, f1: 61.86147186147186, r: 0.5801432285973915
06/02/2019 12:17:19 step: 1614, epoch: 48, batch: 29, loss: 1.1160529851913452, acc: 56.25, f1: 35.108522293876995, r: 0.3871909839650561
06/02/2019 12:17:20 *** evaluating ***
06/02/2019 12:17:20 step: 49, epoch: 48, acc: 56.41025641025641, f1: 25.900930758949215, r: 0.36344214942256636
06/02/2019 12:17:20 *** epoch: 50 ***
06/02/2019 12:17:20 *** training ***
06/02/2019 12:17:21 step: 1622, epoch: 49, batch: 4, loss: 1.1137773990631104, acc: 56.25, f1: 40.79521384105425, r: 0.43834764677496507
06/02/2019 12:17:22 step: 1627, epoch: 49, batch: 9, loss: 1.1941851377487183, acc: 51.5625, f1: 30.774157616262876, r: 0.4723980666311087
06/02/2019 12:17:23 step: 1632, epoch: 49, batch: 14, loss: 0.8833001852035522, acc: 64.0625, f1: 47.16503992901509, r: 0.4842752999578505
06/02/2019 12:17:24 step: 1637, epoch: 49, batch: 19, loss: 0.9956538081169128, acc: 65.625, f1: 43.70561127676878, r: 0.376961999777331
06/02/2019 12:17:25 step: 1642, epoch: 49, batch: 24, loss: 1.0145312547683716, acc: 60.9375, f1: 38.99398395721925, r: 0.5538112520287269
06/02/2019 12:17:26 step: 1647, epoch: 49, batch: 29, loss: 0.929232120513916, acc: 70.3125, f1: 40.69789859263543, r: 0.4644566672567106
06/02/2019 12:17:27 *** evaluating ***
06/02/2019 12:17:27 step: 50, epoch: 49, acc: 58.119658119658126, f1: 27.227037430375372, r: 0.3604263319799124
06/02/2019 12:17:27 *** epoch: 51 ***
06/02/2019 12:17:27 *** training ***
06/02/2019 12:17:28 step: 1655, epoch: 50, batch: 4, loss: 1.0012843608856201, acc: 67.1875, f1: 38.02213868003342, r: 0.553932323999053
06/02/2019 12:17:29 step: 1660, epoch: 50, batch: 9, loss: 1.1106715202331543, acc: 59.375, f1: 35.00996358139216, r: 0.38840963392119304
06/02/2019 12:17:30 step: 1665, epoch: 50, batch: 14, loss: 1.0236825942993164, acc: 56.25, f1: 43.983312893724516, r: 0.498083716197252
06/02/2019 12:17:31 step: 1670, epoch: 50, batch: 19, loss: 0.8774292469024658, acc: 67.1875, f1: 32.62691377921031, r: 0.5852053613563399
06/02/2019 12:17:32 step: 1675, epoch: 50, batch: 24, loss: 0.8219815492630005, acc: 65.625, f1: 42.784186138223404, r: 0.5072819093491063
06/02/2019 12:17:33 step: 1680, epoch: 50, batch: 29, loss: 0.9654167890548706, acc: 67.1875, f1: 48.25766210739615, r: 0.5948276107093409
06/02/2019 12:17:34 *** evaluating ***
06/02/2019 12:17:34 step: 51, epoch: 50, acc: 55.98290598290598, f1: 25.798746253326588, r: 0.3467881801401948
06/02/2019 12:17:34 *** epoch: 52 ***
06/02/2019 12:17:34 *** training ***
06/02/2019 12:17:35 step: 1688, epoch: 51, batch: 4, loss: 0.9789480566978455, acc: 65.625, f1: 47.0462962962963, r: 0.47434871720323807
06/02/2019 12:17:36 step: 1693, epoch: 51, batch: 9, loss: 1.0306326150894165, acc: 64.0625, f1: 43.65384615384616, r: 0.5261502274324108
06/02/2019 12:17:37 step: 1698, epoch: 51, batch: 14, loss: 0.9661829471588135, acc: 64.0625, f1: 50.95238095238095, r: 0.5103236907668373
06/02/2019 12:17:38 step: 1703, epoch: 51, batch: 19, loss: 1.0896471738815308, acc: 56.25, f1: 37.815934065934066, r: 0.5593319004543279
06/02/2019 12:17:39 step: 1708, epoch: 51, batch: 24, loss: 1.1239889860153198, acc: 60.9375, f1: 38.52564102564102, r: 0.4762218957444935
06/02/2019 12:17:40 step: 1713, epoch: 51, batch: 29, loss: 1.0588815212249756, acc: 59.375, f1: 35.0773133170257, r: 0.502005932570144
06/02/2019 12:17:40 *** evaluating ***
06/02/2019 12:17:41 step: 52, epoch: 51, acc: 55.55555555555556, f1: 25.83613204785849, r: 0.35483038956615737
06/02/2019 12:17:41 *** epoch: 53 ***
06/02/2019 12:17:41 *** training ***
06/02/2019 12:17:42 step: 1721, epoch: 52, batch: 4, loss: 0.9176296591758728, acc: 75.0, f1: 58.74050415922337, r: 0.5305007503126005
06/02/2019 12:17:43 step: 1726, epoch: 52, batch: 9, loss: 0.9655725359916687, acc: 62.5, f1: 55.49587725338559, r: 0.5136403999194555
06/02/2019 12:17:43 step: 1731, epoch: 52, batch: 14, loss: 1.0046886205673218, acc: 68.75, f1: 46.10995270507781, r: 0.5553939137554136
06/02/2019 12:17:45 step: 1736, epoch: 52, batch: 19, loss: 1.0103238821029663, acc: 59.375, f1: 33.18270735524257, r: 0.5574122088855865
06/02/2019 12:17:46 step: 1741, epoch: 52, batch: 24, loss: 0.9541484713554382, acc: 62.5, f1: 40.95572781445491, r: 0.5733526938037604
06/02/2019 12:17:47 step: 1746, epoch: 52, batch: 29, loss: 0.8068148493766785, acc: 70.3125, f1: 40.00022110686095, r: 0.4986707712279358
06/02/2019 12:17:47 *** evaluating ***
06/02/2019 12:17:48 step: 53, epoch: 52, acc: 56.837606837606835, f1: 23.102968358601682, r: 0.36893930081081705
06/02/2019 12:17:48 *** epoch: 54 ***
06/02/2019 12:17:48 *** training ***
06/02/2019 12:17:49 step: 1754, epoch: 53, batch: 4, loss: 0.7118701338768005, acc: 76.5625, f1: 55.79925115207374, r: 0.6427624753369091
06/02/2019 12:17:50 step: 1759, epoch: 53, batch: 9, loss: 1.0357428789138794, acc: 54.6875, f1: 40.63902243589743, r: 0.547598557784594
06/02/2019 12:17:51 step: 1764, epoch: 53, batch: 14, loss: 0.8508144617080688, acc: 73.4375, f1: 49.39335917433335, r: 0.4070057690159996
06/02/2019 12:17:52 step: 1769, epoch: 53, batch: 19, loss: 0.9603813290596008, acc: 60.9375, f1: 44.40321583178726, r: 0.4647669329051976
06/02/2019 12:17:53 step: 1774, epoch: 53, batch: 24, loss: 1.090853214263916, acc: 65.625, f1: 41.403087693693486, r: 0.5256413462003905
06/02/2019 12:17:54 step: 1779, epoch: 53, batch: 29, loss: 0.9552358388900757, acc: 71.875, f1: 44.96487242590605, r: 0.4818860387178295
06/02/2019 12:17:54 *** evaluating ***
06/02/2019 12:17:54 step: 54, epoch: 53, acc: 54.700854700854705, f1: 26.34405604503985, r: 0.35681939024987896
06/02/2019 12:17:54 *** epoch: 55 ***
06/02/2019 12:17:54 *** training ***
06/02/2019 12:17:55 step: 1787, epoch: 54, batch: 4, loss: 0.8583452105522156, acc: 65.625, f1: 39.962894248608535, r: 0.5556871738851348
06/02/2019 12:17:56 step: 1792, epoch: 54, batch: 9, loss: 0.9505079388618469, acc: 68.75, f1: 51.56776094276094, r: 0.6350968200073247
06/02/2019 12:17:58 step: 1797, epoch: 54, batch: 14, loss: 1.0418250560760498, acc: 56.25, f1: 28.341649159663863, r: 0.5670837917954216
06/02/2019 12:17:59 step: 1802, epoch: 54, batch: 19, loss: 0.9306632280349731, acc: 60.9375, f1: 36.646116780045354, r: 0.5717723804940978
06/02/2019 12:18:00 step: 1807, epoch: 54, batch: 24, loss: 0.8602884411811829, acc: 70.3125, f1: 51.23245614035088, r: 0.5965012070217892
06/02/2019 12:18:01 step: 1812, epoch: 54, batch: 29, loss: 0.8784356117248535, acc: 71.875, f1: 58.295642581356866, r: 0.46844894848406937
06/02/2019 12:18:01 *** evaluating ***
06/02/2019 12:18:02 step: 55, epoch: 54, acc: 58.54700854700855, f1: 29.151945724526367, r: 0.35329139213957134
06/02/2019 12:18:02 *** epoch: 56 ***
06/02/2019 12:18:02 *** training ***
06/02/2019 12:18:03 step: 1820, epoch: 55, batch: 4, loss: 1.0128040313720703, acc: 64.0625, f1: 46.07761437908496, r: 0.40152480030538457
06/02/2019 12:18:04 step: 1825, epoch: 55, batch: 9, loss: 0.8342040777206421, acc: 71.875, f1: 58.36148341869165, r: 0.41567830039236664
06/02/2019 12:18:05 step: 1830, epoch: 55, batch: 14, loss: 0.8368425965309143, acc: 73.4375, f1: 53.0255348516218, r: 0.454214741602972
06/02/2019 12:18:05 step: 1835, epoch: 55, batch: 19, loss: 0.9677579402923584, acc: 67.1875, f1: 43.87779734389904, r: 0.5137628656303425
06/02/2019 12:18:06 step: 1840, epoch: 55, batch: 24, loss: 0.9087583422660828, acc: 62.5, f1: 35.973219327256594, r: 0.51034149085611
06/02/2019 12:18:07 step: 1845, epoch: 55, batch: 29, loss: 1.0238323211669922, acc: 64.0625, f1: 47.263824884792626, r: 0.6223786051905645
06/02/2019 12:18:08 *** evaluating ***
06/02/2019 12:18:08 step: 56, epoch: 55, acc: 55.12820512820513, f1: 26.892574688234273, r: 0.3435064608153931
06/02/2019 12:18:08 *** epoch: 57 ***
06/02/2019 12:18:08 *** training ***
06/02/2019 12:18:09 step: 1853, epoch: 56, batch: 4, loss: 0.9304078817367554, acc: 68.75, f1: 55.6487827514814, r: 0.5911636930021047
06/02/2019 12:18:10 step: 1858, epoch: 56, batch: 9, loss: 0.9387116432189941, acc: 65.625, f1: 53.274434211465696, r: 0.5278461487718953
06/02/2019 12:18:11 step: 1863, epoch: 56, batch: 14, loss: 1.0750066041946411, acc: 59.375, f1: 29.441682776396945, r: 0.4339393483370405
06/02/2019 12:18:12 step: 1868, epoch: 56, batch: 19, loss: 1.0363516807556152, acc: 64.0625, f1: 41.45163064400814, r: 0.515132943079193
06/02/2019 12:18:13 step: 1873, epoch: 56, batch: 24, loss: 0.8335040807723999, acc: 65.625, f1: 51.34113037096303, r: 0.5353230064659542
06/02/2019 12:18:14 step: 1878, epoch: 56, batch: 29, loss: 0.8985702395439148, acc: 70.3125, f1: 55.94982078853047, r: 0.5400734014036642
06/02/2019 12:18:15 *** evaluating ***
06/02/2019 12:18:15 step: 57, epoch: 56, acc: 57.692307692307686, f1: 24.863804115512693, r: 0.3625526151126111
06/02/2019 12:18:15 *** epoch: 58 ***
06/02/2019 12:18:15 *** training ***
06/02/2019 12:18:16 step: 1886, epoch: 57, batch: 4, loss: 0.856745719909668, acc: 65.625, f1: 39.63847746456442, r: 0.44806933129341464
06/02/2019 12:18:17 step: 1891, epoch: 57, batch: 9, loss: 0.9510908722877502, acc: 59.375, f1: 30.759552538964307, r: 0.40329747208805183
06/02/2019 12:18:18 step: 1896, epoch: 57, batch: 14, loss: 0.882847785949707, acc: 70.3125, f1: 50.27797325826883, r: 0.40812859836617094
06/02/2019 12:18:19 step: 1901, epoch: 57, batch: 19, loss: 0.7717559933662415, acc: 78.125, f1: 58.03733226905984, r: 0.5287243732417796
06/02/2019 12:18:20 step: 1906, epoch: 57, batch: 24, loss: 1.3504494428634644, acc: 70.3125, f1: 61.832579185520366, r: 0.4488168991008131
06/02/2019 12:18:21 step: 1911, epoch: 57, batch: 29, loss: 1.034294605255127, acc: 57.8125, f1: 32.870370370370374, r: 0.4659963857463197
06/02/2019 12:18:22 *** evaluating ***
06/02/2019 12:18:22 step: 58, epoch: 57, acc: 56.41025641025641, f1: 30.42210249068028, r: 0.341490279502713
06/02/2019 12:18:22 *** epoch: 59 ***
06/02/2019 12:18:22 *** training ***
06/02/2019 12:18:23 step: 1919, epoch: 58, batch: 4, loss: 0.8443754315376282, acc: 71.875, f1: 57.52380952380952, r: 0.4661878993475848
06/02/2019 12:18:24 step: 1924, epoch: 58, batch: 9, loss: 0.9505727291107178, acc: 56.25, f1: 36.88441188441189, r: 0.49190681674300374
06/02/2019 12:18:25 step: 1929, epoch: 58, batch: 14, loss: 0.8713792562484741, acc: 62.5, f1: 38.327582240625716, r: 0.5929150183578789
06/02/2019 12:18:26 step: 1934, epoch: 58, batch: 19, loss: 0.8202465772628784, acc: 73.4375, f1: 54.79166666666666, r: 0.5214775976745682
06/02/2019 12:18:27 step: 1939, epoch: 58, batch: 24, loss: 0.8939535617828369, acc: 67.1875, f1: 45.05778943278943, r: 0.5730356249651722
06/02/2019 12:18:28 step: 1944, epoch: 58, batch: 29, loss: 0.9692704677581787, acc: 64.0625, f1: 45.8974358974359, r: 0.5687078132786123
06/02/2019 12:18:28 *** evaluating ***
06/02/2019 12:18:29 step: 59, epoch: 58, acc: 53.84615384615385, f1: 27.82069884140972, r: 0.32762648567890945
06/02/2019 12:18:29 *** epoch: 60 ***
06/02/2019 12:18:29 *** training ***
06/02/2019 12:18:29 step: 1952, epoch: 59, batch: 4, loss: 0.9917545318603516, acc: 64.0625, f1: 37.73327549643339, r: 0.44951562518959975
06/02/2019 12:18:31 step: 1957, epoch: 59, batch: 9, loss: 0.8263875246047974, acc: 68.75, f1: 47.308317266300456, r: 0.605569481431171
06/02/2019 12:18:32 step: 1962, epoch: 59, batch: 14, loss: 0.7789678573608398, acc: 67.1875, f1: 55.75139146567718, r: 0.5648519262092148
06/02/2019 12:18:33 step: 1967, epoch: 59, batch: 19, loss: 0.7608601450920105, acc: 75.0, f1: 53.73024361259656, r: 0.47531065505308046
06/02/2019 12:18:34 step: 1972, epoch: 59, batch: 24, loss: 0.9988605380058289, acc: 60.9375, f1: 44.311986863711, r: 0.5328915278874141
06/02/2019 12:18:35 step: 1977, epoch: 59, batch: 29, loss: 0.8948749899864197, acc: 70.3125, f1: 46.56466256580115, r: 0.4749604749420365
06/02/2019 12:18:35 *** evaluating ***
06/02/2019 12:18:36 step: 60, epoch: 59, acc: 56.837606837606835, f1: 25.156094211486725, r: 0.364068301637734
06/02/2019 12:18:36 *** epoch: 61 ***
06/02/2019 12:18:36 *** training ***
06/02/2019 12:18:36 step: 1985, epoch: 60, batch: 4, loss: 0.8166146278381348, acc: 64.0625, f1: 42.50661375661376, r: 0.5820997395513392
06/02/2019 12:18:37 step: 1990, epoch: 60, batch: 9, loss: 0.8429780006408691, acc: 71.875, f1: 56.96775446775446, r: 0.600312665676324
06/02/2019 12:18:38 step: 1995, epoch: 60, batch: 14, loss: 0.9547275900840759, acc: 62.5, f1: 38.33459706813526, r: 0.5289563866953473
06/02/2019 12:18:39 step: 2000, epoch: 60, batch: 19, loss: 0.8019065856933594, acc: 71.875, f1: 45.15625, r: 0.5666020696450339
06/02/2019 12:18:40 step: 2005, epoch: 60, batch: 24, loss: 0.8332081437110901, acc: 67.1875, f1: 43.25772700384502, r: 0.49833589746435203
06/02/2019 12:18:41 step: 2010, epoch: 60, batch: 29, loss: 0.7403168082237244, acc: 75.0, f1: 54.408014571949, r: 0.5792199711762536
06/02/2019 12:18:42 *** evaluating ***
06/02/2019 12:18:42 step: 61, epoch: 60, acc: 55.55555555555556, f1: 27.739591376909615, r: 0.3531660080270325
06/02/2019 12:18:42 *** epoch: 62 ***
06/02/2019 12:18:42 *** training ***
06/02/2019 12:18:43 step: 2018, epoch: 61, batch: 4, loss: 0.9442416429519653, acc: 68.75, f1: 49.85477138515398, r: 0.5417309784335367
06/02/2019 12:18:44 step: 2023, epoch: 61, batch: 9, loss: 0.6512624025344849, acc: 84.375, f1: 69.84121560051031, r: 0.6426037892396393
06/02/2019 12:18:45 step: 2028, epoch: 61, batch: 14, loss: 0.8019471168518066, acc: 67.1875, f1: 43.90970254141609, r: 0.5876465199329607
06/02/2019 12:18:45 step: 2033, epoch: 61, batch: 19, loss: 0.8734915852546692, acc: 68.75, f1: 48.913170163170165, r: 0.5742607060268557
06/02/2019 12:18:46 step: 2038, epoch: 61, batch: 24, loss: 1.299428939819336, acc: 62.5, f1: 45.414614121510674, r: 0.4300278622947937
06/02/2019 12:18:47 step: 2043, epoch: 61, batch: 29, loss: 0.8168015480041504, acc: 68.75, f1: 51.248219658342954, r: 0.5556043386667263
06/02/2019 12:18:48 *** evaluating ***
06/02/2019 12:18:48 step: 62, epoch: 61, acc: 57.26495726495726, f1: 26.76248844640571, r: 0.3582001537579063
06/02/2019 12:18:48 *** epoch: 63 ***
06/02/2019 12:18:48 *** training ***
06/02/2019 12:18:49 step: 2051, epoch: 62, batch: 4, loss: 0.9140879511833191, acc: 64.0625, f1: 41.336580086580085, r: 0.5856340271933336
06/02/2019 12:18:50 step: 2056, epoch: 62, batch: 9, loss: 0.7989466786384583, acc: 71.875, f1: 66.74380757087525, r: 0.5021013196599674
06/02/2019 12:18:51 step: 2061, epoch: 62, batch: 14, loss: 0.6783748269081116, acc: 84.375, f1: 52.647574517825824, r: 0.637017332777608
06/02/2019 12:18:52 step: 2066, epoch: 62, batch: 19, loss: 0.8331024050712585, acc: 70.3125, f1: 50.65980167810831, r: 0.5536276085531302
06/02/2019 12:18:53 step: 2071, epoch: 62, batch: 24, loss: 0.8015010356903076, acc: 75.0, f1: 47.515591050073816, r: 0.5589820959586607
06/02/2019 12:18:54 step: 2076, epoch: 62, batch: 29, loss: 0.7341012358665466, acc: 76.5625, f1: 69.20987654320989, r: 0.4187526194486018
06/02/2019 12:18:54 *** evaluating ***
06/02/2019 12:18:54 step: 63, epoch: 62, acc: 55.98290598290598, f1: 28.00342553372387, r: 0.35298135676781717
06/02/2019 12:18:54 *** epoch: 64 ***
06/02/2019 12:18:54 *** training ***
06/02/2019 12:18:55 step: 2084, epoch: 63, batch: 4, loss: 0.8110935688018799, acc: 70.3125, f1: 45.78603101159492, r: 0.4624023007136489
06/02/2019 12:18:56 step: 2089, epoch: 63, batch: 9, loss: 0.7468891143798828, acc: 71.875, f1: 59.773143523143524, r: 0.5875853951416725
06/02/2019 12:18:57 step: 2094, epoch: 63, batch: 14, loss: 0.8314924240112305, acc: 65.625, f1: 59.57354239962935, r: 0.5127462482224863
06/02/2019 12:18:58 step: 2099, epoch: 63, batch: 19, loss: 0.851058840751648, acc: 71.875, f1: 57.22222222222222, r: 0.4742456402194324
06/02/2019 12:18:59 step: 2104, epoch: 63, batch: 24, loss: 0.9804306030273438, acc: 73.4375, f1: 51.52298850574712, r: 0.5501724513563283
06/02/2019 12:19:00 step: 2109, epoch: 63, batch: 29, loss: 0.7616081237792969, acc: 78.125, f1: 70.90179661608232, r: 0.568927181496635
06/02/2019 12:19:01 *** evaluating ***
06/02/2019 12:19:01 step: 64, epoch: 63, acc: 56.837606837606835, f1: 29.060890800021234, r: 0.34556997048346283
06/02/2019 12:19:01 *** epoch: 65 ***
06/02/2019 12:19:01 *** training ***
06/02/2019 12:19:02 step: 2117, epoch: 64, batch: 4, loss: 0.8680149912834167, acc: 76.5625, f1: 65.62025534851621, r: 0.5741662799238465
06/02/2019 12:19:03 step: 2122, epoch: 64, batch: 9, loss: 0.8181892037391663, acc: 64.0625, f1: 48.15977673120531, r: 0.5364439624478158
06/02/2019 12:19:04 step: 2127, epoch: 64, batch: 14, loss: 0.7391655445098877, acc: 71.875, f1: 58.76903712043031, r: 0.6503699484642381
06/02/2019 12:19:05 step: 2132, epoch: 64, batch: 19, loss: 0.6155837178230286, acc: 79.6875, f1: 51.44411643967932, r: 0.5540974182359891
06/02/2019 12:19:06 step: 2137, epoch: 64, batch: 24, loss: 0.7390170097351074, acc: 68.75, f1: 53.3403271909019, r: 0.5314282194148607
06/02/2019 12:19:07 step: 2142, epoch: 64, batch: 29, loss: 1.0970942974090576, acc: 71.875, f1: 49.662270004109196, r: 0.3947654870847982
06/02/2019 12:19:07 *** evaluating ***
06/02/2019 12:19:07 step: 65, epoch: 64, acc: 55.55555555555556, f1: 25.9899125315792, r: 0.3536058399915439
06/02/2019 12:19:07 *** epoch: 66 ***
06/02/2019 12:19:07 *** training ***
06/02/2019 12:19:08 step: 2150, epoch: 65, batch: 4, loss: 0.8418805599212646, acc: 67.1875, f1: 45.253374076903484, r: 0.4642039125073961
06/02/2019 12:19:09 step: 2155, epoch: 65, batch: 9, loss: 0.8509396910667419, acc: 75.0, f1: 54.949229691876745, r: 0.5565517857340536
06/02/2019 12:19:10 step: 2160, epoch: 65, batch: 14, loss: 0.7971882820129395, acc: 70.3125, f1: 50.38516991403239, r: 0.514300140891046
06/02/2019 12:19:11 step: 2165, epoch: 65, batch: 19, loss: 0.8374761343002319, acc: 64.0625, f1: 45.60176125244618, r: 0.5027062679969507
06/02/2019 12:19:12 step: 2170, epoch: 65, batch: 24, loss: 0.764813244342804, acc: 76.5625, f1: 68.52457516528706, r: 0.5739843358917349
06/02/2019 12:19:13 step: 2175, epoch: 65, batch: 29, loss: 0.9190468788146973, acc: 65.625, f1: 37.87697144075021, r: 0.5287367708624334
06/02/2019 12:19:14 *** evaluating ***
06/02/2019 12:19:14 step: 66, epoch: 65, acc: 56.41025641025641, f1: 25.870975671691056, r: 0.36053595243765235
06/02/2019 12:19:14 *** epoch: 67 ***
06/02/2019 12:19:14 *** training ***
06/02/2019 12:19:15 step: 2183, epoch: 66, batch: 4, loss: 0.8901429176330566, acc: 67.1875, f1: 61.817838246409686, r: 0.5968161807514336
06/02/2019 12:19:16 step: 2188, epoch: 66, batch: 9, loss: 0.7657195329666138, acc: 75.0, f1: 65.28032153032153, r: 0.5873746568722434
06/02/2019 12:19:17 step: 2193, epoch: 66, batch: 14, loss: 0.8635664582252502, acc: 67.1875, f1: 34.49730094466936, r: 0.5337548245460293
06/02/2019 12:19:18 step: 2198, epoch: 66, batch: 19, loss: 0.6933499574661255, acc: 76.5625, f1: 55.546827565270185, r: 0.5675369599639384
06/02/2019 12:19:19 step: 2203, epoch: 66, batch: 24, loss: 1.1696865558624268, acc: 71.875, f1: 54.389643354271215, r: 0.5873041224164952
06/02/2019 12:19:20 step: 2208, epoch: 66, batch: 29, loss: 0.7970364689826965, acc: 70.3125, f1: 53.86680988184748, r: 0.5993028974432528
06/02/2019 12:19:20 *** evaluating ***
06/02/2019 12:19:20 step: 67, epoch: 66, acc: 57.26495726495726, f1: 24.637127578304046, r: 0.3580600456260187
06/02/2019 12:19:20 *** epoch: 68 ***
06/02/2019 12:19:20 *** training ***
06/02/2019 12:19:21 step: 2216, epoch: 67, batch: 4, loss: 0.7460260987281799, acc: 71.875, f1: 58.1931216931217, r: 0.5078180040989644
06/02/2019 12:19:22 step: 2221, epoch: 67, batch: 9, loss: 0.730934202671051, acc: 73.4375, f1: 68.87982246038513, r: 0.4519584626800177
06/02/2019 12:19:23 step: 2226, epoch: 67, batch: 14, loss: 0.8329210877418518, acc: 73.4375, f1: 65.46847710113016, r: 0.6057982242511274
06/02/2019 12:19:24 step: 2231, epoch: 67, batch: 19, loss: 0.816613495349884, acc: 67.1875, f1: 56.12469806763284, r: 0.3973844212109807
06/02/2019 12:19:25 step: 2236, epoch: 67, batch: 24, loss: 0.845184326171875, acc: 71.875, f1: 57.640836175318945, r: 0.591803218818994
06/02/2019 12:19:26 step: 2241, epoch: 67, batch: 29, loss: 0.8948491811752319, acc: 71.875, f1: 64.2099567099567, r: 0.5874788989840732
06/02/2019 12:19:27 *** evaluating ***
06/02/2019 12:19:27 step: 68, epoch: 67, acc: 56.837606837606835, f1: 28.57341800356506, r: 0.3517078497040099
06/02/2019 12:19:27 *** epoch: 69 ***
06/02/2019 12:19:27 *** training ***
06/02/2019 12:19:28 step: 2249, epoch: 68, batch: 4, loss: 0.6785988211631775, acc: 75.0, f1: 48.23715415019763, r: 0.6145559784281344
06/02/2019 12:19:29 step: 2254, epoch: 68, batch: 9, loss: 0.8401262760162354, acc: 67.1875, f1: 58.903927613605035, r: 0.5062622013472523
06/02/2019 12:19:30 step: 2259, epoch: 68, batch: 14, loss: 0.902087926864624, acc: 81.25, f1: 66.9059829059829, r: 0.5736636763475438
06/02/2019 12:19:31 step: 2264, epoch: 68, batch: 19, loss: 0.642699122428894, acc: 82.8125, f1: 75.66926667462349, r: 0.5838917550108779
06/02/2019 12:19:32 step: 2269, epoch: 68, batch: 24, loss: 0.735511064529419, acc: 73.4375, f1: 59.302220665940396, r: 0.6494418683090748
06/02/2019 12:19:33 step: 2274, epoch: 68, batch: 29, loss: 0.859760582447052, acc: 62.5, f1: 50.521816037735846, r: 0.6596780105691605
06/02/2019 12:19:33 *** evaluating ***
06/02/2019 12:19:33 step: 69, epoch: 68, acc: 54.27350427350427, f1: 26.069416068950847, r: 0.3444192060989988
06/02/2019 12:19:33 *** epoch: 70 ***
06/02/2019 12:19:33 *** training ***
06/02/2019 12:19:34 step: 2282, epoch: 69, batch: 4, loss: 0.8368306159973145, acc: 71.875, f1: 54.9541555752736, r: 0.5352106353475781
06/02/2019 12:19:35 step: 2287, epoch: 69, batch: 9, loss: 0.9221855401992798, acc: 67.1875, f1: 46.46746114568633, r: 0.5767492464366667
06/02/2019 12:19:36 step: 2292, epoch: 69, batch: 14, loss: 0.7368325591087341, acc: 73.4375, f1: 58.228905597326644, r: 0.4996845317112393
06/02/2019 12:19:37 step: 2297, epoch: 69, batch: 19, loss: 1.051038146018982, acc: 70.3125, f1: 61.56062424969988, r: 0.5628790188493558
06/02/2019 12:19:38 step: 2302, epoch: 69, batch: 24, loss: 0.6446995735168457, acc: 85.9375, f1: 70.43817527010805, r: 0.5385087142624582
06/02/2019 12:19:39 step: 2307, epoch: 69, batch: 29, loss: 0.6911486983299255, acc: 75.0, f1: 57.47152531089918, r: 0.5928869144863258
06/02/2019 12:19:40 *** evaluating ***
06/02/2019 12:19:40 step: 70, epoch: 69, acc: 57.26495726495726, f1: 25.60390532033, r: 0.35988740210827325
06/02/2019 12:19:40 *** epoch: 71 ***
06/02/2019 12:19:40 *** training ***
06/02/2019 12:19:41 step: 2315, epoch: 70, batch: 4, loss: 0.9871627688407898, acc: 62.5, f1: 45.093487645894484, r: 0.5646133618864563
06/02/2019 12:19:42 step: 2320, epoch: 70, batch: 9, loss: 0.9724467992782593, acc: 54.6875, f1: 34.61562816616008, r: 0.5533545854163965
06/02/2019 12:19:43 step: 2325, epoch: 70, batch: 14, loss: 0.7678390145301819, acc: 67.1875, f1: 42.75601226623161, r: 0.5187386330469672
06/02/2019 12:19:44 step: 2330, epoch: 70, batch: 19, loss: 0.5442639589309692, acc: 82.8125, f1: 57.843763303533414, r: 0.5114897455993628
06/02/2019 12:19:44 step: 2335, epoch: 70, batch: 24, loss: 0.8125759363174438, acc: 76.5625, f1: 36.49100655679604, r: 0.39498706144504747
06/02/2019 12:19:45 step: 2340, epoch: 70, batch: 29, loss: 0.7916852235794067, acc: 67.1875, f1: 48.931694998768165, r: 0.5748446477740092
06/02/2019 12:19:46 *** evaluating ***
06/02/2019 12:19:46 step: 71, epoch: 70, acc: 55.98290598290598, f1: 24.998031188865955, r: 0.3547160114238813
06/02/2019 12:19:46 *** epoch: 72 ***
06/02/2019 12:19:46 *** training ***
06/02/2019 12:19:47 step: 2348, epoch: 71, batch: 4, loss: 0.7024281620979309, acc: 71.875, f1: 57.07673561784645, r: 0.5728215469023813
06/02/2019 12:19:48 step: 2353, epoch: 71, batch: 9, loss: 0.6760126948356628, acc: 75.0, f1: 49.89194602572212, r: 0.4692524952273716
06/02/2019 12:19:49 step: 2358, epoch: 71, batch: 14, loss: 0.7171989679336548, acc: 75.0, f1: 43.508202323991796, r: 0.5580053025368357
06/02/2019 12:19:50 step: 2363, epoch: 71, batch: 19, loss: 0.6813812851905823, acc: 71.875, f1: 75.20348411652759, r: 0.5861817203545541
06/02/2019 12:19:51 step: 2368, epoch: 71, batch: 24, loss: 0.7886583209037781, acc: 70.3125, f1: 44.44935869084476, r: 0.5376050258722858
06/02/2019 12:19:52 step: 2373, epoch: 71, batch: 29, loss: 0.693014919757843, acc: 76.5625, f1: 64.52690166975881, r: 0.516190118213354
06/02/2019 12:19:53 *** evaluating ***
06/02/2019 12:19:53 step: 72, epoch: 71, acc: 55.12820512820513, f1: 28.07213457200443, r: 0.3404828976252528
06/02/2019 12:19:53 *** epoch: 73 ***
06/02/2019 12:19:53 *** training ***
06/02/2019 12:19:54 step: 2381, epoch: 72, batch: 4, loss: 0.760932207107544, acc: 70.3125, f1: 47.43659420289855, r: 0.510323403604884
06/02/2019 12:19:55 step: 2386, epoch: 72, batch: 9, loss: 0.752021849155426, acc: 70.3125, f1: 59.306239737274225, r: 0.5820990530449371
06/02/2019 12:19:56 step: 2391, epoch: 72, batch: 14, loss: 0.8186091184616089, acc: 71.875, f1: 57.48368955605798, r: 0.5187721937556655
06/02/2019 12:19:57 step: 2396, epoch: 72, batch: 19, loss: 0.7754999995231628, acc: 75.0, f1: 63.5221396250808, r: 0.5301930704593013
06/02/2019 12:19:58 step: 2401, epoch: 72, batch: 24, loss: 0.8060721755027771, acc: 71.875, f1: 52.029154518950435, r: 0.5555234905018576
06/02/2019 12:19:59 step: 2406, epoch: 72, batch: 29, loss: 0.7944985628128052, acc: 68.75, f1: 45.30689468769964, r: 0.5549257493889946
06/02/2019 12:20:00 *** evaluating ***
06/02/2019 12:20:00 step: 73, epoch: 72, acc: 55.12820512820513, f1: 25.053645771964966, r: 0.3500210698569432
06/02/2019 12:20:00 *** epoch: 74 ***
06/02/2019 12:20:00 *** training ***
06/02/2019 12:20:01 step: 2414, epoch: 73, batch: 4, loss: 0.8005586862564087, acc: 71.875, f1: 50.05241469057259, r: 0.5519433650559892
06/02/2019 12:20:02 step: 2419, epoch: 73, batch: 9, loss: 0.7821803092956543, acc: 73.4375, f1: 59.85389610389611, r: 0.6379254865824273
06/02/2019 12:20:03 step: 2424, epoch: 73, batch: 14, loss: 0.6915181279182434, acc: 78.125, f1: 59.93603411513858, r: 0.5129551236298537
06/02/2019 12:20:04 step: 2429, epoch: 73, batch: 19, loss: 0.7442545294761658, acc: 76.5625, f1: 53.22260533846171, r: 0.5948619192371494
06/02/2019 12:20:05 step: 2434, epoch: 73, batch: 24, loss: 0.7055850028991699, acc: 78.125, f1: 48.54692760942761, r: 0.5508403915892089
06/02/2019 12:20:06 step: 2439, epoch: 73, batch: 29, loss: 0.6400066614151001, acc: 78.125, f1: 67.65805622948479, r: 0.5341523249501302
06/02/2019 12:20:07 *** evaluating ***
06/02/2019 12:20:07 step: 74, epoch: 73, acc: 57.26495726495726, f1: 27.68587932499118, r: 0.34561333139732586
06/02/2019 12:20:07 *** epoch: 75 ***
06/02/2019 12:20:07 *** training ***
06/02/2019 12:20:08 step: 2447, epoch: 74, batch: 4, loss: 0.7997013330459595, acc: 65.625, f1: 51.78593113019343, r: 0.4882044824585607
06/02/2019 12:20:09 step: 2452, epoch: 74, batch: 9, loss: 0.7535127997398376, acc: 71.875, f1: 50.35125588697017, r: 0.620518124953241
06/02/2019 12:20:10 step: 2457, epoch: 74, batch: 14, loss: 0.7308012247085571, acc: 70.3125, f1: 56.6042069632495, r: 0.648195489565012
06/02/2019 12:20:11 step: 2462, epoch: 74, batch: 19, loss: 0.5815814137458801, acc: 82.8125, f1: 72.39300382157525, r: 0.53061600870138
06/02/2019 12:20:12 step: 2467, epoch: 74, batch: 24, loss: 0.5776735544204712, acc: 78.125, f1: 64.5554720436796, r: 0.5558806840827359
06/02/2019 12:20:13 step: 2472, epoch: 74, batch: 29, loss: 0.7253236174583435, acc: 70.3125, f1: 52.53521280306995, r: 0.47073933174872173
06/02/2019 12:20:14 *** evaluating ***
06/02/2019 12:20:14 step: 75, epoch: 74, acc: 56.41025641025641, f1: 27.241271090375953, r: 0.34349722889102857
06/02/2019 12:20:14 *** epoch: 76 ***
06/02/2019 12:20:14 *** training ***
06/02/2019 12:20:15 step: 2480, epoch: 75, batch: 4, loss: 0.6920413374900818, acc: 76.5625, f1: 49.76943076081006, r: 0.5678057095616054
06/02/2019 12:20:16 step: 2485, epoch: 75, batch: 9, loss: 1.0128591060638428, acc: 81.25, f1: 66.22187884815762, r: 0.61714151512065
06/02/2019 12:20:17 step: 2490, epoch: 75, batch: 14, loss: 0.614629864692688, acc: 73.4375, f1: 58.299168152109324, r: 0.7128083825799576
06/02/2019 12:20:18 step: 2495, epoch: 75, batch: 19, loss: 0.6999437808990479, acc: 79.6875, f1: 57.625168690958176, r: 0.5528956014644383
06/02/2019 12:20:19 step: 2500, epoch: 75, batch: 24, loss: 0.6435748934745789, acc: 82.8125, f1: 72.45668078552175, r: 0.6797421736935991
06/02/2019 12:20:20 step: 2505, epoch: 75, batch: 29, loss: 0.8049856424331665, acc: 67.1875, f1: 58.77551020408164, r: 0.5519698099243532
06/02/2019 12:20:20 *** evaluating ***
06/02/2019 12:20:21 step: 76, epoch: 75, acc: 55.55555555555556, f1: 27.11227540948903, r: 0.34293239705958073
06/02/2019 12:20:21 *** epoch: 77 ***
06/02/2019 12:20:21 *** training ***
06/02/2019 12:20:22 step: 2513, epoch: 76, batch: 4, loss: 1.0442689657211304, acc: 76.5625, f1: 73.19430778077394, r: 0.6237989297084078
06/02/2019 12:20:23 step: 2518, epoch: 76, batch: 9, loss: 0.9598064422607422, acc: 76.5625, f1: 48.098544973544975, r: 0.596178689476108
06/02/2019 12:20:24 step: 2523, epoch: 76, batch: 14, loss: 0.6811902523040771, acc: 76.5625, f1: 54.1421568627451, r: 0.5607997039526489
06/02/2019 12:20:25 step: 2528, epoch: 76, batch: 19, loss: 0.755139946937561, acc: 75.0, f1: 56.091644204851754, r: 0.58571449322947
06/02/2019 12:20:26 step: 2533, epoch: 76, batch: 24, loss: 0.6947242021560669, acc: 78.125, f1: 56.045548654244314, r: 0.5579240147867608
06/02/2019 12:20:27 step: 2538, epoch: 76, batch: 29, loss: 0.7027283310890198, acc: 65.625, f1: 57.87540046160736, r: 0.5685971578292806
06/02/2019 12:20:27 *** evaluating ***
06/02/2019 12:20:28 step: 77, epoch: 76, acc: 58.97435897435898, f1: 27.61989993365767, r: 0.3506335347251688
06/02/2019 12:20:28 *** epoch: 78 ***
06/02/2019 12:20:28 *** training ***
06/02/2019 12:20:29 step: 2546, epoch: 77, batch: 4, loss: 0.7279369831085205, acc: 76.5625, f1: 64.20541613774697, r: 0.5445025925189445
06/02/2019 12:20:30 step: 2551, epoch: 77, batch: 9, loss: 0.8718101978302002, acc: 64.0625, f1: 52.15686274509803, r: 0.6901160893771014
06/02/2019 12:20:31 step: 2556, epoch: 77, batch: 14, loss: 1.1119377613067627, acc: 54.6875, f1: 43.01307488807489, r: 0.4843505989987353
06/02/2019 12:20:32 step: 2561, epoch: 77, batch: 19, loss: 0.5862587094306946, acc: 76.5625, f1: 51.326007326007314, r: 0.46009842983248095
06/02/2019 12:20:33 step: 2566, epoch: 77, batch: 24, loss: 0.6654636263847351, acc: 84.375, f1: 63.4447496947497, r: 0.42320017830738116
06/02/2019 12:20:34 step: 2571, epoch: 77, batch: 29, loss: 0.734981119632721, acc: 73.4375, f1: 61.019134374397524, r: 0.583084467938624
06/02/2019 12:20:34 *** evaluating ***
06/02/2019 12:20:35 step: 78, epoch: 77, acc: 57.692307692307686, f1: 27.186836206748634, r: 0.3506985552690902
06/02/2019 12:20:35 *** epoch: 79 ***
06/02/2019 12:20:35 *** training ***
06/02/2019 12:20:35 step: 2579, epoch: 78, batch: 4, loss: 0.6081761121749878, acc: 84.375, f1: 68.38624338624338, r: 0.5836810974139095
06/02/2019 12:20:36 step: 2584, epoch: 78, batch: 9, loss: 0.7278856039047241, acc: 73.4375, f1: 65.5534941249227, r: 0.5929171741242689
06/02/2019 12:20:37 step: 2589, epoch: 78, batch: 14, loss: 0.619027316570282, acc: 73.4375, f1: 63.609216357883305, r: 0.6522518432261547
06/02/2019 12:20:38 step: 2594, epoch: 78, batch: 19, loss: 0.6443944573402405, acc: 85.9375, f1: 73.14074553266282, r: 0.5487425467186972
06/02/2019 12:20:39 step: 2599, epoch: 78, batch: 24, loss: 0.4944620132446289, acc: 84.375, f1: 77.91658787534972, r: 0.6151060420394778
06/02/2019 12:20:41 step: 2604, epoch: 78, batch: 29, loss: 0.49178045988082886, acc: 82.8125, f1: 81.6559934318555, r: 0.6328892694247884
06/02/2019 12:20:41 *** evaluating ***
06/02/2019 12:20:42 step: 79, epoch: 78, acc: 57.26495726495726, f1: 28.738255653425103, r: 0.3472508471887472
06/02/2019 12:20:42 *** epoch: 80 ***
06/02/2019 12:20:42 *** training ***
06/02/2019 12:20:43 step: 2612, epoch: 79, batch: 4, loss: 0.6978586912155151, acc: 78.125, f1: 58.09095419277654, r: 0.5786242602089485
06/02/2019 12:20:43 step: 2617, epoch: 79, batch: 9, loss: 0.678903341293335, acc: 75.0, f1: 58.08505220269927, r: 0.5512080857854732
06/02/2019 12:20:45 step: 2622, epoch: 79, batch: 14, loss: 0.6657137274742126, acc: 70.3125, f1: 66.69209097780525, r: 0.5315487249601392
06/02/2019 12:20:45 step: 2627, epoch: 79, batch: 19, loss: 0.6582217216491699, acc: 70.3125, f1: 36.401168014375564, r: 0.6256833392138392
06/02/2019 12:20:47 step: 2632, epoch: 79, batch: 24, loss: 0.43187397718429565, acc: 87.5, f1: 76.39425185366501, r: 0.5975913756448334
06/02/2019 12:20:47 step: 2637, epoch: 79, batch: 29, loss: 0.6932700276374817, acc: 73.4375, f1: 65.66949152542372, r: 0.7204217640850801
06/02/2019 12:20:48 *** evaluating ***
06/02/2019 12:20:48 step: 80, epoch: 79, acc: 57.692307692307686, f1: 28.965812284904075, r: 0.3498963997343969
06/02/2019 12:20:48 *** epoch: 81 ***
06/02/2019 12:20:48 *** training ***
06/02/2019 12:20:49 step: 2645, epoch: 80, batch: 4, loss: 0.791688084602356, acc: 73.4375, f1: 57.87719298245614, r: 0.5897664021326986
06/02/2019 12:20:50 step: 2650, epoch: 80, batch: 9, loss: 0.6104353666305542, acc: 76.5625, f1: 68.05891848264729, r: 0.5931635289926802
06/02/2019 12:20:51 step: 2655, epoch: 80, batch: 14, loss: 0.563454806804657, acc: 85.9375, f1: 66.9291060684347, r: 0.563282302885787
06/02/2019 12:20:52 step: 2660, epoch: 80, batch: 19, loss: 0.5888049602508545, acc: 81.25, f1: 57.3205329153605, r: 0.5330397603179924
06/02/2019 12:20:53 step: 2665, epoch: 80, batch: 24, loss: 0.5934345126152039, acc: 82.8125, f1: 57.72222222222223, r: 0.5201249489295645
06/02/2019 12:20:54 step: 2670, epoch: 80, batch: 29, loss: 0.7007328271865845, acc: 78.125, f1: 69.45027486256873, r: 0.5605910423955628
06/02/2019 12:20:55 *** evaluating ***
06/02/2019 12:20:55 step: 81, epoch: 80, acc: 53.41880341880342, f1: 26.63030917696232, r: 0.33817668993666233
06/02/2019 12:20:55 *** epoch: 82 ***
06/02/2019 12:20:55 *** training ***
06/02/2019 12:20:56 step: 2678, epoch: 81, batch: 4, loss: 0.7650241255760193, acc: 79.6875, f1: 70.31310288992229, r: 0.5548027539226427
06/02/2019 12:20:57 step: 2683, epoch: 81, batch: 9, loss: 0.8264918327331543, acc: 71.875, f1: 57.344212804739115, r: 0.6067256066848847
06/02/2019 12:20:58 step: 2688, epoch: 81, batch: 14, loss: 0.6039186120033264, acc: 81.25, f1: 60.20429488409784, r: 0.5032679295864291
06/02/2019 12:20:59 step: 2693, epoch: 81, batch: 19, loss: 0.4971984326839447, acc: 79.6875, f1: 63.27729044834308, r: 0.6093982365799803
06/02/2019 12:21:00 step: 2698, epoch: 81, batch: 24, loss: 0.615702211856842, acc: 81.25, f1: 68.54341736694678, r: 0.5845040284866407
06/02/2019 12:21:01 step: 2703, epoch: 81, batch: 29, loss: 0.5378527045249939, acc: 79.6875, f1: 60.64087174562527, r: 0.7057999808614129
06/02/2019 12:21:02 *** evaluating ***
06/02/2019 12:21:02 step: 82, epoch: 81, acc: 57.26495726495726, f1: 27.155112044817926, r: 0.3490888492270849
06/02/2019 12:21:02 *** epoch: 83 ***
06/02/2019 12:21:02 *** training ***
06/02/2019 12:21:03 step: 2711, epoch: 82, batch: 4, loss: 0.7140488028526306, acc: 68.75, f1: 52.42383910085773, r: 0.6398268480211884
06/02/2019 12:21:04 step: 2716, epoch: 82, batch: 9, loss: 0.7141972184181213, acc: 68.75, f1: 55.926274468731776, r: 0.5568789142992805
06/02/2019 12:21:05 step: 2721, epoch: 82, batch: 14, loss: 0.579110860824585, acc: 82.8125, f1: 60.79669540229884, r: 0.518989874339273
06/02/2019 12:21:06 step: 2726, epoch: 82, batch: 19, loss: 0.7509209513664246, acc: 70.3125, f1: 52.364302034239586, r: 0.4111071970652975
06/02/2019 12:21:07 step: 2731, epoch: 82, batch: 24, loss: 0.6467490792274475, acc: 75.0, f1: 57.65078970277329, r: 0.6178437706925098
06/02/2019 12:21:08 step: 2736, epoch: 82, batch: 29, loss: 0.5807065367698669, acc: 82.8125, f1: 73.96284829721361, r: 0.5386427258036591
06/02/2019 12:21:09 *** evaluating ***
06/02/2019 12:21:09 step: 83, epoch: 82, acc: 56.837606837606835, f1: 28.227755541871925, r: 0.3383114028013494
06/02/2019 12:21:09 *** epoch: 84 ***
06/02/2019 12:21:09 *** training ***
06/02/2019 12:21:10 step: 2744, epoch: 83, batch: 4, loss: 0.72157222032547, acc: 68.75, f1: 51.811642689862836, r: 0.49028348375526565
06/02/2019 12:21:11 step: 2749, epoch: 83, batch: 9, loss: 0.6036573648452759, acc: 79.6875, f1: 72.07564091555277, r: 0.5861183344753571
06/02/2019 12:21:12 step: 2754, epoch: 83, batch: 14, loss: 0.6159786581993103, acc: 79.6875, f1: 58.43678722710981, r: 0.6798786947885582
06/02/2019 12:21:13 step: 2759, epoch: 83, batch: 19, loss: 0.6558125019073486, acc: 75.0, f1: 56.854005167958654, r: 0.6278790796867743
06/02/2019 12:21:14 step: 2764, epoch: 83, batch: 24, loss: 0.691063404083252, acc: 75.0, f1: 48.31152248810943, r: 0.5666587278761468
06/02/2019 12:21:15 step: 2769, epoch: 83, batch: 29, loss: 0.6362725496292114, acc: 79.6875, f1: 67.08874458874458, r: 0.5479578754195548
06/02/2019 12:21:16 *** evaluating ***
06/02/2019 12:21:16 step: 84, epoch: 83, acc: 56.837606837606835, f1: 28.407146592946802, r: 0.3445788578534433
06/02/2019 12:21:16 *** epoch: 85 ***
06/02/2019 12:21:16 *** training ***
06/02/2019 12:21:17 step: 2777, epoch: 84, batch: 4, loss: 0.5264350771903992, acc: 82.8125, f1: 70.4296294921295, r: 0.6843783095179715
06/02/2019 12:21:18 step: 2782, epoch: 84, batch: 9, loss: 0.5546717643737793, acc: 81.25, f1: 60.071548821548824, r: 0.6655390021789716
06/02/2019 12:21:19 step: 2787, epoch: 84, batch: 14, loss: 1.0988073348999023, acc: 73.4375, f1: 62.83616133240193, r: 0.5148472243507589
06/02/2019 12:21:19 step: 2792, epoch: 84, batch: 19, loss: 0.631635308265686, acc: 75.0, f1: 70.86385836385837, r: 0.6018039463636021
06/02/2019 12:21:20 step: 2797, epoch: 84, batch: 24, loss: 0.4520499110221863, acc: 82.8125, f1: 71.4983535433931, r: 0.6805744069820235
06/02/2019 12:21:21 step: 2802, epoch: 84, batch: 29, loss: 0.6495976448059082, acc: 73.4375, f1: 48.3796992481203, r: 0.6735358798325382
06/02/2019 12:21:22 *** evaluating ***
06/02/2019 12:21:22 step: 85, epoch: 84, acc: 57.692307692307686, f1: 27.554853380746238, r: 0.3503368596660551
06/02/2019 12:21:22 *** epoch: 86 ***
06/02/2019 12:21:22 *** training ***
06/02/2019 12:21:23 step: 2810, epoch: 85, batch: 4, loss: 0.6896669864654541, acc: 75.0, f1: 65.37414965986395, r: 0.5620629982696703
06/02/2019 12:21:24 step: 2815, epoch: 85, batch: 9, loss: 0.6229627132415771, acc: 79.6875, f1: 73.63246121010096, r: 0.582264417279963
06/02/2019 12:21:25 step: 2820, epoch: 85, batch: 14, loss: 0.7471269965171814, acc: 68.75, f1: 56.49173955296405, r: 0.5319336391941655
06/02/2019 12:21:26 step: 2825, epoch: 85, batch: 19, loss: 0.5618700981140137, acc: 82.8125, f1: 67.3989898989899, r: 0.725296152984225
06/02/2019 12:21:27 step: 2830, epoch: 85, batch: 24, loss: 0.5751034617424011, acc: 78.125, f1: 58.99624707764243, r: 0.6789756128567499
06/02/2019 12:21:28 step: 2835, epoch: 85, batch: 29, loss: 0.5941691994667053, acc: 82.8125, f1: 82.5392742634122, r: 0.5938216203508321
06/02/2019 12:21:28 *** evaluating ***
06/02/2019 12:21:28 step: 86, epoch: 85, acc: 56.41025641025641, f1: 28.615154780440367, r: 0.3359262216274671
06/02/2019 12:21:28 *** epoch: 87 ***
06/02/2019 12:21:28 *** training ***
06/02/2019 12:21:29 step: 2843, epoch: 86, batch: 4, loss: 0.5229180455207825, acc: 76.5625, f1: 63.094866405060046, r: 0.5595056904016106
06/02/2019 12:21:30 step: 2848, epoch: 86, batch: 9, loss: 0.6603136658668518, acc: 79.6875, f1: 75.31486575821552, r: 0.5656966044363543
06/02/2019 12:21:31 step: 2853, epoch: 86, batch: 14, loss: 0.5237652063369751, acc: 76.5625, f1: 47.13152985074627, r: 0.6842939414495562
06/02/2019 12:21:32 step: 2858, epoch: 86, batch: 19, loss: 0.5446117520332336, acc: 79.6875, f1: 58.44395825629648, r: 0.669866548993324
06/02/2019 12:21:33 step: 2863, epoch: 86, batch: 24, loss: 0.6468785405158997, acc: 81.25, f1: 57.227020314440345, r: 0.6205399380135083
06/02/2019 12:21:34 step: 2868, epoch: 86, batch: 29, loss: 0.6527425050735474, acc: 75.0, f1: 58.08497536945813, r: 0.6109169879832513
06/02/2019 12:21:34 *** evaluating ***
06/02/2019 12:21:35 step: 87, epoch: 86, acc: 57.26495726495726, f1: 27.563298640351665, r: 0.34936169373006265
06/02/2019 12:21:35 *** epoch: 88 ***
06/02/2019 12:21:35 *** training ***
06/02/2019 12:21:35 step: 2876, epoch: 87, batch: 4, loss: 0.3984357416629791, acc: 89.0625, f1: 84.92964075262212, r: 0.6174808010495945
06/02/2019 12:21:36 step: 2881, epoch: 87, batch: 9, loss: 0.9711996912956238, acc: 71.875, f1: 52.8025066331518, r: 0.6470372189911768
06/02/2019 12:21:37 step: 2886, epoch: 87, batch: 14, loss: 0.6502195000648499, acc: 73.4375, f1: 49.01069282431955, r: 0.582817228642938
06/02/2019 12:21:38 step: 2891, epoch: 87, batch: 19, loss: 0.5924268364906311, acc: 81.25, f1: 62.70804729214341, r: 0.6269242204246005
06/02/2019 12:21:39 step: 2896, epoch: 87, batch: 24, loss: 0.493105947971344, acc: 81.25, f1: 56.062271062271066, r: 0.6242299276959508
06/02/2019 12:21:40 step: 2901, epoch: 87, batch: 29, loss: 0.5686440467834473, acc: 82.8125, f1: 69.43591539179775, r: 0.6239068934589705
06/02/2019 12:21:41 *** evaluating ***
06/02/2019 12:21:41 step: 88, epoch: 87, acc: 55.55555555555556, f1: 26.955313462677623, r: 0.34500753514434984
06/02/2019 12:21:41 *** epoch: 89 ***
06/02/2019 12:21:41 *** training ***
06/02/2019 12:21:42 step: 2909, epoch: 88, batch: 4, loss: 0.5787164568901062, acc: 81.25, f1: 81.6020321171947, r: 0.6041219733420883
06/02/2019 12:21:43 step: 2914, epoch: 88, batch: 9, loss: 0.5177750587463379, acc: 87.5, f1: 68.58288530465948, r: 0.6655148990203048
06/02/2019 12:21:44 step: 2919, epoch: 88, batch: 14, loss: 0.46022725105285645, acc: 89.0625, f1: 79.70521541950112, r: 0.5707389634185878
06/02/2019 12:21:45 step: 2924, epoch: 88, batch: 19, loss: 0.6073835492134094, acc: 78.125, f1: 61.53518123667377, r: 0.5831882891826714
06/02/2019 12:21:46 step: 2929, epoch: 88, batch: 24, loss: 0.5535433888435364, acc: 89.0625, f1: 72.47990105132963, r: 0.5719345297863195
06/02/2019 12:21:47 step: 2934, epoch: 88, batch: 29, loss: 0.5879462361335754, acc: 78.125, f1: 58.904914529914535, r: 0.5736526125223669
06/02/2019 12:21:48 *** evaluating ***
06/02/2019 12:21:48 step: 89, epoch: 88, acc: 54.700854700854705, f1: 26.499067146380543, r: 0.3395346624771093
06/02/2019 12:21:48 *** epoch: 90 ***
06/02/2019 12:21:48 *** training ***
06/02/2019 12:21:49 step: 2942, epoch: 89, batch: 4, loss: 0.58722984790802, acc: 81.25, f1: 70.71028411364546, r: 0.5486889979203419
06/02/2019 12:21:50 step: 2947, epoch: 89, batch: 9, loss: 0.4764630198478699, acc: 85.9375, f1: 73.47749472749473, r: 0.6631670196011608
06/02/2019 12:21:51 step: 2952, epoch: 89, batch: 14, loss: 0.5849800109863281, acc: 78.125, f1: 49.678427406283625, r: 0.6085131557782077
06/02/2019 12:21:52 step: 2957, epoch: 89, batch: 19, loss: 0.7628273963928223, acc: 70.3125, f1: 55.05604727640051, r: 0.5161818009435534
06/02/2019 12:21:53 step: 2962, epoch: 89, batch: 24, loss: 0.5329498052597046, acc: 78.125, f1: 50.50046289702321, r: 0.5628216029288241
06/02/2019 12:21:54 step: 2967, epoch: 89, batch: 29, loss: 0.5769438743591309, acc: 84.375, f1: 78.61942405420666, r: 0.6368862828938158
06/02/2019 12:21:55 *** evaluating ***
06/02/2019 12:21:55 step: 90, epoch: 89, acc: 57.692307692307686, f1: 25.640243077241266, r: 0.35142995343424843
06/02/2019 12:21:55 *** epoch: 91 ***
06/02/2019 12:21:55 *** training ***
06/02/2019 12:21:56 step: 2975, epoch: 90, batch: 4, loss: 0.4781702756881714, acc: 85.9375, f1: 79.35889507318078, r: 0.6196527486417498
06/02/2019 12:21:57 step: 2980, epoch: 90, batch: 9, loss: 0.5652749538421631, acc: 85.9375, f1: 81.7298404930733, r: 0.672823923244448
06/02/2019 12:21:58 step: 2985, epoch: 90, batch: 14, loss: 0.910666823387146, acc: 84.375, f1: 74.42765567765568, r: 0.6978581251853956
06/02/2019 12:21:59 step: 2990, epoch: 90, batch: 19, loss: 0.5359269380569458, acc: 81.25, f1: 68.6302766983855, r: 0.5834376705284668
06/02/2019 12:22:00 step: 2995, epoch: 90, batch: 24, loss: 0.5519331693649292, acc: 81.25, f1: 75.36630036630036, r: 0.5964996429762517
06/02/2019 12:22:01 step: 3000, epoch: 90, batch: 29, loss: 0.5333552360534668, acc: 79.6875, f1: 66.03702277556724, r: 0.5810578967298632
06/02/2019 12:22:01 *** evaluating ***
06/02/2019 12:22:01 step: 91, epoch: 90, acc: 55.12820512820513, f1: 27.439719859952096, r: 0.342790408029548
06/02/2019 12:22:01 *** epoch: 92 ***
06/02/2019 12:22:01 *** training ***
06/02/2019 12:22:02 step: 3008, epoch: 91, batch: 4, loss: 0.5292204022407532, acc: 87.5, f1: 80.96444319354538, r: 0.6032406699002457
06/02/2019 12:22:03 step: 3013, epoch: 91, batch: 9, loss: 0.6080654859542847, acc: 75.0, f1: 63.17340067340067, r: 0.685663896225408
06/02/2019 12:22:04 step: 3018, epoch: 91, batch: 14, loss: 0.530112624168396, acc: 79.6875, f1: 74.61645497359783, r: 0.6031349886169015
06/02/2019 12:22:05 step: 3023, epoch: 91, batch: 19, loss: 0.5551815032958984, acc: 79.6875, f1: 67.58125472411187, r: 0.6800787172647829
06/02/2019 12:22:06 step: 3028, epoch: 91, batch: 24, loss: 0.5508959889411926, acc: 81.25, f1: 71.06821352924717, r: 0.6684188818185655
06/02/2019 12:22:07 step: 3033, epoch: 91, batch: 29, loss: 0.5169557929039001, acc: 82.8125, f1: 57.788499457754114, r: 0.6824395887450384
06/02/2019 12:22:08 *** evaluating ***
06/02/2019 12:22:08 step: 92, epoch: 91, acc: 56.837606837606835, f1: 27.681316096487983, r: 0.34833568964169526
06/02/2019 12:22:08 *** epoch: 93 ***
06/02/2019 12:22:08 *** training ***
06/02/2019 12:22:09 step: 3041, epoch: 92, batch: 4, loss: 0.41522225737571716, acc: 89.0625, f1: 84.52073806111073, r: 0.6382590405433103
06/02/2019 12:22:10 step: 3046, epoch: 92, batch: 9, loss: 0.5626218914985657, acc: 81.25, f1: 68.51346673421142, r: 0.7098565043422485
06/02/2019 12:22:11 step: 3051, epoch: 92, batch: 14, loss: 0.5835935473442078, acc: 78.125, f1: 65.72448979591837, r: 0.5083050477771427
06/02/2019 12:22:12 step: 3056, epoch: 92, batch: 19, loss: 0.4882877469062805, acc: 87.5, f1: 61.18675066043486, r: 0.6501290131690194
06/02/2019 12:22:13 step: 3061, epoch: 92, batch: 24, loss: 0.5866180062294006, acc: 79.6875, f1: 75.46044210496036, r: 0.5935365641677366
06/02/2019 12:22:14 step: 3066, epoch: 92, batch: 29, loss: 0.6425442695617676, acc: 78.125, f1: 70.17607042817127, r: 0.5028036144207377
06/02/2019 12:22:14 *** evaluating ***
06/02/2019 12:22:15 step: 93, epoch: 92, acc: 55.12820512820513, f1: 28.408195332568177, r: 0.3333838129280797
06/02/2019 12:22:15 *** epoch: 94 ***
06/02/2019 12:22:15 *** training ***
06/02/2019 12:22:16 step: 3074, epoch: 93, batch: 4, loss: 0.5216814279556274, acc: 84.375, f1: 64.13710910720877, r: 0.549869741540857
06/02/2019 12:22:17 step: 3079, epoch: 93, batch: 9, loss: 0.6376452445983887, acc: 73.4375, f1: 68.94819466248038, r: 0.6018341719460233
06/02/2019 12:22:17 step: 3084, epoch: 93, batch: 14, loss: 0.4983643889427185, acc: 79.6875, f1: 71.72771672771673, r: 0.5607612080683897
06/02/2019 12:22:19 step: 3089, epoch: 93, batch: 19, loss: 0.5679051280021667, acc: 78.125, f1: 63.995052566481135, r: 0.6002646566182128
06/02/2019 12:22:20 step: 3094, epoch: 93, batch: 24, loss: 0.47016847133636475, acc: 90.625, f1: 88.18743161600304, r: 0.6663801927100323
06/02/2019 12:22:21 step: 3099, epoch: 93, batch: 29, loss: 0.3792176842689514, acc: 90.625, f1: 76.61437217227804, r: 0.5874851685077519
06/02/2019 12:22:21 *** evaluating ***
06/02/2019 12:22:21 step: 94, epoch: 93, acc: 56.41025641025641, f1: 27.475497849174012, r: 0.3365112786034452
06/02/2019 12:22:21 *** epoch: 95 ***
06/02/2019 12:22:21 *** training ***
06/02/2019 12:22:22 step: 3107, epoch: 94, batch: 4, loss: 0.6097916960716248, acc: 73.4375, f1: 75.23674242424242, r: 0.6346237084529806
06/02/2019 12:22:23 step: 3112, epoch: 94, batch: 9, loss: 0.5433727502822876, acc: 85.9375, f1: 82.799741765259, r: 0.5482161678173494
06/02/2019 12:22:24 step: 3117, epoch: 94, batch: 14, loss: 0.4733582139015198, acc: 84.375, f1: 73.20875122117359, r: 0.6494158871203597
06/02/2019 12:22:25 step: 3122, epoch: 94, batch: 19, loss: 0.5785167217254639, acc: 78.125, f1: 56.983805668016196, r: 0.5785977810595101
06/02/2019 12:22:26 step: 3127, epoch: 94, batch: 24, loss: 0.547309160232544, acc: 81.25, f1: 65.17054374197231, r: 0.6242587147287588
06/02/2019 12:22:27 step: 3132, epoch: 94, batch: 29, loss: 0.5677233934402466, acc: 82.8125, f1: 72.78532870638134, r: 0.6400083008234606
06/02/2019 12:22:28 *** evaluating ***
06/02/2019 12:22:28 step: 95, epoch: 94, acc: 55.55555555555556, f1: 26.83215391419726, r: 0.34107047505516647
06/02/2019 12:22:28 *** epoch: 96 ***
06/02/2019 12:22:28 *** training ***
06/02/2019 12:22:29 step: 3140, epoch: 95, batch: 4, loss: 0.5789189338684082, acc: 78.125, f1: 59.05251911830859, r: 0.6061714529491194
06/02/2019 12:22:30 step: 3145, epoch: 95, batch: 9, loss: 0.6193341016769409, acc: 79.6875, f1: 72.20666721973342, r: 0.5989484610924449
06/02/2019 12:22:31 step: 3150, epoch: 95, batch: 14, loss: 0.5356807708740234, acc: 82.8125, f1: 62.013827919227396, r: 0.6807674930248537
06/02/2019 12:22:32 step: 3155, epoch: 95, batch: 19, loss: 0.5058179497718811, acc: 81.25, f1: 51.9944924356689, r: 0.5847915367310942
06/02/2019 12:22:33 step: 3160, epoch: 95, batch: 24, loss: 0.46127021312713623, acc: 85.9375, f1: 78.23183499875229, r: 0.6518540270975356
06/02/2019 12:22:34 step: 3165, epoch: 95, batch: 29, loss: 0.5348439812660217, acc: 84.375, f1: 76.967759685151, r: 0.5421208285883853
06/02/2019 12:22:35 *** evaluating ***
06/02/2019 12:22:35 step: 96, epoch: 95, acc: 56.41025641025641, f1: 26.10726279254666, r: 0.3446530446532403
06/02/2019 12:22:35 *** epoch: 97 ***
06/02/2019 12:22:35 *** training ***
06/02/2019 12:22:36 step: 3173, epoch: 96, batch: 4, loss: 0.703458845615387, acc: 68.75, f1: 62.0805439770957, r: 0.5493675749633309
06/02/2019 12:22:37 step: 3178, epoch: 96, batch: 9, loss: 0.5377225279808044, acc: 81.25, f1: 70.84656084656085, r: 0.4967094866297969
06/02/2019 12:22:38 step: 3183, epoch: 96, batch: 14, loss: 0.525012195110321, acc: 84.375, f1: 66.01635690091462, r: 0.5519198694849495
06/02/2019 12:22:39 step: 3188, epoch: 96, batch: 19, loss: 0.5123175382614136, acc: 81.25, f1: 73.75117650583937, r: 0.6255480177117466
06/02/2019 12:22:40 step: 3193, epoch: 96, batch: 24, loss: 0.43233829736709595, acc: 85.9375, f1: 67.87314605329311, r: 0.7597304270345484
06/02/2019 12:22:41 step: 3198, epoch: 96, batch: 29, loss: 0.5253656506538391, acc: 84.375, f1: 81.58687943262412, r: 0.7399207752394216
06/02/2019 12:22:42 *** evaluating ***
06/02/2019 12:22:42 step: 97, epoch: 96, acc: 55.98290598290598, f1: 26.32630430078692, r: 0.3432188917212706
06/02/2019 12:22:42 *** epoch: 98 ***
06/02/2019 12:22:42 *** training ***
06/02/2019 12:22:43 step: 3206, epoch: 97, batch: 4, loss: 0.8630322217941284, acc: 85.9375, f1: 81.23463401005773, r: 0.6388216768874871
06/02/2019 12:22:44 step: 3211, epoch: 97, batch: 9, loss: 0.5613932609558105, acc: 79.6875, f1: 49.0987460815047, r: 0.4758631479222507
06/02/2019 12:22:45 step: 3216, epoch: 97, batch: 14, loss: 0.5278983116149902, acc: 78.125, f1: 52.549887923022254, r: 0.47685733485272763
06/02/2019 12:22:46 step: 3221, epoch: 97, batch: 19, loss: 0.48675301671028137, acc: 79.6875, f1: 63.76642036124795, r: 0.5988233454215681
06/02/2019 12:22:47 step: 3226, epoch: 97, batch: 24, loss: 0.5765218138694763, acc: 82.8125, f1: 62.943722943722946, r: 0.595670606201061
06/02/2019 12:22:48 step: 3231, epoch: 97, batch: 29, loss: 0.5831559896469116, acc: 73.4375, f1: 59.386308136308145, r: 0.6668075135927547
06/02/2019 12:22:49 *** evaluating ***
06/02/2019 12:22:49 step: 98, epoch: 97, acc: 57.692307692307686, f1: 25.888144229321764, r: 0.35029156443653675
06/02/2019 12:22:49 *** epoch: 99 ***
06/02/2019 12:22:49 *** training ***
06/02/2019 12:22:50 step: 3239, epoch: 98, batch: 4, loss: 0.5998045206069946, acc: 78.125, f1: 55.705289362121654, r: 0.6074529250926237
06/02/2019 12:22:51 step: 3244, epoch: 98, batch: 9, loss: 0.40668222308158875, acc: 82.8125, f1: 64.31275660595853, r: 0.6462222257941909
06/02/2019 12:22:52 step: 3249, epoch: 98, batch: 14, loss: 0.42353782057762146, acc: 84.375, f1: 74.48077715934859, r: 0.5904409113315054
06/02/2019 12:22:53 step: 3254, epoch: 98, batch: 19, loss: 0.4462359547615051, acc: 85.9375, f1: 71.23938223938224, r: 0.6704923497131894
06/02/2019 12:22:54 step: 3259, epoch: 98, batch: 24, loss: 0.4061579406261444, acc: 87.5, f1: 74.2934154793993, r: 0.6634687047753156
06/02/2019 12:22:55 step: 3264, epoch: 98, batch: 29, loss: 0.8417582511901855, acc: 84.375, f1: 63.25798011512297, r: 0.5266531060593991
06/02/2019 12:22:55 *** evaluating ***
06/02/2019 12:22:56 step: 99, epoch: 98, acc: 55.98290598290598, f1: 26.68549237836907, r: 0.3426664406029952
06/02/2019 12:22:56 *** epoch: 100 ***
06/02/2019 12:22:56 *** training ***
06/02/2019 12:22:57 step: 3272, epoch: 99, batch: 4, loss: 0.4771304130554199, acc: 87.5, f1: 78.33139083139083, r: 0.7604773910710363
06/02/2019 12:22:58 step: 3277, epoch: 99, batch: 9, loss: 0.3502942621707916, acc: 93.75, f1: 89.1807157645667, r: 0.5862636741435407
06/02/2019 12:22:59 step: 3282, epoch: 99, batch: 14, loss: 0.49551600217819214, acc: 90.625, f1: 77.86780631608218, r: 0.6396328617143886
06/02/2019 12:23:00 step: 3287, epoch: 99, batch: 19, loss: 0.6403742432594299, acc: 78.125, f1: 69.94415141943303, r: 0.6725719314386095
06/02/2019 12:23:01 step: 3292, epoch: 99, batch: 24, loss: 0.46565982699394226, acc: 85.9375, f1: 65.37885134959602, r: 0.6371414028528304
06/02/2019 12:23:02 step: 3297, epoch: 99, batch: 29, loss: 0.7962755560874939, acc: 68.75, f1: 71.28709454796412, r: 0.5567855370068915
06/02/2019 12:23:03 *** evaluating ***
06/02/2019 12:23:03 step: 100, epoch: 99, acc: 56.41025641025641, f1: 25.471513439078514, r: 0.34217813375336403
06/02/2019 12:23:03 *** epoch: 101 ***
06/02/2019 12:23:03 *** training ***
06/02/2019 12:23:04 step: 3305, epoch: 100, batch: 4, loss: 0.4891912639141083, acc: 81.25, f1: 75.40352504638219, r: 0.5869743396487287
06/02/2019 12:23:05 step: 3310, epoch: 100, batch: 9, loss: 0.4863739609718323, acc: 82.8125, f1: 69.90335751205316, r: 0.5980385287892961
06/02/2019 12:23:06 step: 3315, epoch: 100, batch: 14, loss: 0.5336441993713379, acc: 79.6875, f1: 61.431799194957094, r: 0.6514016696129036
06/02/2019 12:23:07 step: 3320, epoch: 100, batch: 19, loss: 0.5781480669975281, acc: 78.125, f1: 56.24280481423338, r: 0.510211066337869
06/02/2019 12:23:08 step: 3325, epoch: 100, batch: 24, loss: 0.7144453525543213, acc: 92.1875, f1: 91.26111983254842, r: 0.6375115300670938
06/02/2019 12:23:09 step: 3330, epoch: 100, batch: 29, loss: 0.5182445645332336, acc: 82.8125, f1: 73.29489815142612, r: 0.7704732600761247
06/02/2019 12:23:10 *** evaluating ***
06/02/2019 12:23:10 step: 101, epoch: 100, acc: 55.98290598290598, f1: 26.527128891496186, r: 0.35003856972891656
06/02/2019 12:23:10 *** epoch: 102 ***
06/02/2019 12:23:10 *** training ***
06/02/2019 12:23:11 step: 3338, epoch: 101, batch: 4, loss: 0.3715801239013672, acc: 89.0625, f1: 77.64132553606238, r: 0.6919919309769967
06/02/2019 12:23:12 step: 3343, epoch: 101, batch: 9, loss: 0.4690934121608734, acc: 84.375, f1: 73.2401987240697, r: 0.6954644714922754
06/02/2019 12:23:13 step: 3348, epoch: 101, batch: 14, loss: 0.5730640888214111, acc: 79.6875, f1: 76.91699099325308, r: 0.5970953448525426
06/02/2019 12:23:14 step: 3353, epoch: 101, batch: 19, loss: 0.5462527871131897, acc: 82.8125, f1: 66.89246064246063, r: 0.6246886696973424
06/02/2019 12:23:15 step: 3358, epoch: 101, batch: 24, loss: 0.4373270869255066, acc: 89.0625, f1: 68.550400124613, r: 0.573784498864249
06/02/2019 12:23:16 step: 3363, epoch: 101, batch: 29, loss: 0.4935958981513977, acc: 81.25, f1: 75.66626650660264, r: 0.5654231581229209
06/02/2019 12:23:17 *** evaluating ***
06/02/2019 12:23:17 step: 102, epoch: 101, acc: 55.55555555555556, f1: 27.719924018573238, r: 0.33214750779669583
06/02/2019 12:23:17 *** epoch: 103 ***
06/02/2019 12:23:17 *** training ***
06/02/2019 12:23:18 step: 3371, epoch: 102, batch: 4, loss: 0.53846275806427, acc: 84.375, f1: 67.9341736694678, r: 0.6705677832913218
06/02/2019 12:23:19 step: 3376, epoch: 102, batch: 9, loss: 0.34089046716690063, acc: 90.625, f1: 84.44179478547113, r: 0.6039925306191506
06/02/2019 12:23:20 step: 3381, epoch: 102, batch: 14, loss: 0.3762167990207672, acc: 85.9375, f1: 81.58163265306123, r: 0.5907438055888392
06/02/2019 12:23:21 step: 3386, epoch: 102, batch: 19, loss: 0.6079934239387512, acc: 73.4375, f1: 59.927335091808764, r: 0.6957546632009994
06/02/2019 12:23:22 step: 3391, epoch: 102, batch: 24, loss: 0.6021755337715149, acc: 79.6875, f1: 72.25887258649355, r: 0.6914272063129673
06/02/2019 12:23:23 step: 3396, epoch: 102, batch: 29, loss: 0.5508202314376831, acc: 84.375, f1: 76.96012129191853, r: 0.6417516584507831
06/02/2019 12:23:23 *** evaluating ***
06/02/2019 12:23:24 step: 103, epoch: 102, acc: 56.41025641025641, f1: 25.83492539069442, r: 0.3468690479740599
06/02/2019 12:23:24 *** epoch: 104 ***
06/02/2019 12:23:24 *** training ***
06/02/2019 12:23:25 step: 3404, epoch: 103, batch: 4, loss: 0.4988740086555481, acc: 85.9375, f1: 83.17671299288946, r: 0.6928097291426444
06/02/2019 12:23:26 step: 3409, epoch: 103, batch: 9, loss: 0.4311966300010681, acc: 87.5, f1: 83.98046398046398, r: 0.6404820502599724
06/02/2019 12:23:27 step: 3414, epoch: 103, batch: 14, loss: 0.44611263275146484, acc: 89.0625, f1: 78.66156587359815, r: 0.5855921587799194
06/02/2019 12:23:27 step: 3419, epoch: 103, batch: 19, loss: 0.42876774072647095, acc: 84.375, f1: 72.95856410746117, r: 0.7115224355626206
06/02/2019 12:23:28 step: 3424, epoch: 103, batch: 24, loss: 0.5425307750701904, acc: 79.6875, f1: 67.52896613190731, r: 0.6109271267800518
06/02/2019 12:23:29 step: 3429, epoch: 103, batch: 29, loss: 0.5011602640151978, acc: 81.25, f1: 63.06006493506493, r: 0.6623722010164035
06/02/2019 12:23:30 *** evaluating ***
06/02/2019 12:23:30 step: 104, epoch: 103, acc: 55.55555555555556, f1: 27.34864039955605, r: 0.33312780883224385
06/02/2019 12:23:30 *** epoch: 105 ***
06/02/2019 12:23:30 *** training ***
06/02/2019 12:23:31 step: 3437, epoch: 104, batch: 4, loss: 0.5397258400917053, acc: 79.6875, f1: 74.16676665066923, r: 0.6221130155573052
06/02/2019 12:23:32 step: 3442, epoch: 104, batch: 9, loss: 0.49854323267936707, acc: 78.125, f1: 58.56009070294785, r: 0.5579923409885297
06/02/2019 12:23:33 step: 3447, epoch: 104, batch: 14, loss: 0.9311246275901794, acc: 76.5625, f1: 52.42788461538461, r: 0.6587931481757721
06/02/2019 12:23:34 step: 3452, epoch: 104, batch: 19, loss: 0.45172950625419617, acc: 82.8125, f1: 80.82886904761905, r: 0.73011836593396
06/02/2019 12:23:35 step: 3457, epoch: 104, batch: 24, loss: 0.5684173107147217, acc: 81.25, f1: 67.93117613698277, r: 0.6576601203903594
06/02/2019 12:23:36 step: 3462, epoch: 104, batch: 29, loss: 0.5010547041893005, acc: 87.5, f1: 65.94705012094212, r: 0.6395896398655737
06/02/2019 12:23:36 *** evaluating ***
06/02/2019 12:23:36 step: 105, epoch: 104, acc: 56.41025641025641, f1: 26.777426160337548, r: 0.34061231284665755
06/02/2019 12:23:36 *** epoch: 106 ***
06/02/2019 12:23:36 *** training ***
06/02/2019 12:23:37 step: 3470, epoch: 105, batch: 4, loss: 0.6163023114204407, acc: 81.25, f1: 77.08931419457737, r: 0.7351407705371414
06/02/2019 12:23:38 step: 3475, epoch: 105, batch: 9, loss: 0.37980732321739197, acc: 92.1875, f1: 91.70040109066882, r: 0.6445083985847051
06/02/2019 12:23:39 step: 3480, epoch: 105, batch: 14, loss: 0.5039790272712708, acc: 81.25, f1: 67.21024103332948, r: 0.6021644392673955
06/02/2019 12:23:40 step: 3485, epoch: 105, batch: 19, loss: 0.5851197838783264, acc: 76.5625, f1: 57.337819185645266, r: 0.666898356991649
06/02/2019 12:23:41 step: 3490, epoch: 105, batch: 24, loss: 0.6423689723014832, acc: 81.25, f1: 56.42761752136752, r: 0.5707940775857879
06/02/2019 12:23:42 step: 3495, epoch: 105, batch: 29, loss: 0.571781575679779, acc: 76.5625, f1: 55.73154659741881, r: 0.6432366081473673
06/02/2019 12:23:42 *** evaluating ***
06/02/2019 12:23:43 step: 106, epoch: 105, acc: 54.27350427350427, f1: 27.126706755576723, r: 0.3267479133926148
06/02/2019 12:23:43 *** epoch: 107 ***
06/02/2019 12:23:43 *** training ***
06/02/2019 12:23:43 step: 3503, epoch: 106, batch: 4, loss: 0.45230376720428467, acc: 81.25, f1: 70.13685548168307, r: 0.7180742898959508
06/02/2019 12:23:44 step: 3508, epoch: 106, batch: 9, loss: 0.44539350271224976, acc: 84.375, f1: 79.53433754454163, r: 0.6275525938593806
06/02/2019 12:23:45 step: 3513, epoch: 106, batch: 14, loss: 0.5184515714645386, acc: 84.375, f1: 66.55692873434809, r: 0.6054029492386483
06/02/2019 12:23:46 step: 3518, epoch: 106, batch: 19, loss: 0.5349934101104736, acc: 82.8125, f1: 79.0454830257462, r: 0.7010624908286867
06/02/2019 12:23:47 step: 3523, epoch: 106, batch: 24, loss: 0.5882608890533447, acc: 73.4375, f1: 53.07007219083491, r: 0.6150496440142137
06/02/2019 12:23:48 step: 3528, epoch: 106, batch: 29, loss: 0.6197660565376282, acc: 82.8125, f1: 74.66725820763088, r: 0.5767261127688915
06/02/2019 12:23:49 *** evaluating ***
06/02/2019 12:23:49 step: 107, epoch: 106, acc: 56.41025641025641, f1: 27.92837411481479, r: 0.33991691013391634
06/02/2019 12:23:49 *** epoch: 108 ***
06/02/2019 12:23:49 *** training ***
06/02/2019 12:23:50 step: 3536, epoch: 107, batch: 4, loss: 0.33823302388191223, acc: 92.1875, f1: 80.38726820443239, r: 0.6469380413865666
06/02/2019 12:23:51 step: 3541, epoch: 107, batch: 9, loss: 0.5227444171905518, acc: 82.8125, f1: 67.18591691995948, r: 0.6500483900570376
06/02/2019 12:23:52 step: 3546, epoch: 107, batch: 14, loss: 0.4411695599555969, acc: 85.9375, f1: 61.397907647907644, r: 0.611754968705251
06/02/2019 12:23:53 step: 3551, epoch: 107, batch: 19, loss: 0.4935144782066345, acc: 82.8125, f1: 68.88978979897806, r: 0.4886524291476957
06/02/2019 12:23:54 step: 3556, epoch: 107, batch: 24, loss: 0.45430123805999756, acc: 85.9375, f1: 73.55163804652295, r: 0.7122629368921509
06/02/2019 12:23:55 step: 3561, epoch: 107, batch: 29, loss: 0.4760233759880066, acc: 87.5, f1: 84.79909451046971, r: 0.6768985828751711
06/02/2019 12:23:56 *** evaluating ***
06/02/2019 12:23:56 step: 108, epoch: 107, acc: 55.55555555555556, f1: 27.165289889047617, r: 0.33391684312605685
06/02/2019 12:23:56 *** epoch: 109 ***
06/02/2019 12:23:56 *** training ***
06/02/2019 12:23:57 step: 3569, epoch: 108, batch: 4, loss: 0.5899878144264221, acc: 78.125, f1: 65.89514652014653, r: 0.6783336386095438
06/02/2019 12:23:58 step: 3574, epoch: 108, batch: 9, loss: 0.4004848301410675, acc: 95.3125, f1: 83.67898441427852, r: 0.6875528989566793
06/02/2019 12:23:59 step: 3579, epoch: 108, batch: 14, loss: 0.5056232213973999, acc: 79.6875, f1: 59.25796425796426, r: 0.598966384494921
06/02/2019 12:24:00 step: 3584, epoch: 108, batch: 19, loss: 0.5107810497283936, acc: 82.8125, f1: 74.79544265258552, r: 0.632553374932088
06/02/2019 12:24:01 step: 3589, epoch: 108, batch: 24, loss: 0.5469849109649658, acc: 82.8125, f1: 78.5374149659864, r: 0.6312224094120223
06/02/2019 12:24:03 step: 3594, epoch: 108, batch: 29, loss: 0.5160444378852844, acc: 81.25, f1: 59.03359380217725, r: 0.5952878350583216
06/02/2019 12:24:03 *** evaluating ***
06/02/2019 12:24:03 step: 109, epoch: 108, acc: 55.98290598290598, f1: 26.938891995154222, r: 0.33840817573627174
06/02/2019 12:24:03 *** epoch: 110 ***
06/02/2019 12:24:03 *** training ***
06/02/2019 12:24:04 step: 3602, epoch: 109, batch: 4, loss: 0.4341821074485779, acc: 89.0625, f1: 76.21568950373299, r: 0.6991648654145193
06/02/2019 12:24:05 step: 3607, epoch: 109, batch: 9, loss: 0.5091882944107056, acc: 81.25, f1: 70.64255472418738, r: 0.5533799203691091
06/02/2019 12:24:06 step: 3612, epoch: 109, batch: 14, loss: 0.39082998037338257, acc: 87.5, f1: 77.82986111111111, r: 0.6079402516431761
06/02/2019 12:24:07 step: 3617, epoch: 109, batch: 19, loss: 0.4836626648902893, acc: 89.0625, f1: 72.99669312169313, r: 0.6138757060760915
06/02/2019 12:24:08 step: 3622, epoch: 109, batch: 24, loss: 0.5093846917152405, acc: 82.8125, f1: 79.66236529848022, r: 0.5736723215685956
06/02/2019 12:24:09 step: 3627, epoch: 109, batch: 29, loss: 0.4782744348049164, acc: 85.9375, f1: 74.95986652236653, r: 0.6809973326171619
06/02/2019 12:24:10 *** evaluating ***
06/02/2019 12:24:10 step: 110, epoch: 109, acc: 56.41025641025641, f1: 26.920974153097667, r: 0.326137955792663
06/02/2019 12:24:10 *** epoch: 111 ***
06/02/2019 12:24:10 *** training ***
06/02/2019 12:24:11 step: 3635, epoch: 110, batch: 4, loss: 0.5372411012649536, acc: 82.8125, f1: 65.65361721611721, r: 0.6873830562477633
06/02/2019 12:24:12 step: 3640, epoch: 110, batch: 9, loss: 0.5570142269134521, acc: 81.25, f1: 67.22974822112752, r: 0.6312512030932964
06/02/2019 12:24:13 step: 3645, epoch: 110, batch: 14, loss: 0.46589261293411255, acc: 85.9375, f1: 85.02510683760683, r: 0.7469787793028404
06/02/2019 12:24:14 step: 3650, epoch: 110, batch: 19, loss: 0.3340250253677368, acc: 92.1875, f1: 89.56876456876456, r: 0.7288373495867008
06/02/2019 12:24:15 step: 3655, epoch: 110, batch: 24, loss: 0.47641706466674805, acc: 87.5, f1: 85.85544889892716, r: 0.6763502349361474
06/02/2019 12:24:16 step: 3660, epoch: 110, batch: 29, loss: 0.7597318291664124, acc: 92.1875, f1: 82.75974025974025, r: 0.6099267168846838
06/02/2019 12:24:17 *** evaluating ***
06/02/2019 12:24:17 step: 111, epoch: 110, acc: 53.84615384615385, f1: 26.241384227774688, r: 0.32415976500284793
06/02/2019 12:24:17 *** epoch: 112 ***
06/02/2019 12:24:17 *** training ***
06/02/2019 12:24:18 step: 3668, epoch: 111, batch: 4, loss: 0.399086058139801, acc: 89.0625, f1: 82.24315196297954, r: 0.7231834885992368
06/02/2019 12:24:19 step: 3673, epoch: 111, batch: 9, loss: 0.4932976961135864, acc: 82.8125, f1: 81.34814323607426, r: 0.6468614468569432
06/02/2019 12:24:20 step: 3678, epoch: 111, batch: 14, loss: 0.5266677737236023, acc: 78.125, f1: 75.63985594237695, r: 0.5532905786845664
06/02/2019 12:24:22 step: 3683, epoch: 111, batch: 19, loss: 0.46310457587242126, acc: 85.9375, f1: 69.12859809411535, r: 0.5951105945886573
06/02/2019 12:24:23 step: 3688, epoch: 111, batch: 24, loss: 0.5197209715843201, acc: 79.6875, f1: 64.7778272319276, r: 0.6326023327805977
06/02/2019 12:24:24 step: 3693, epoch: 111, batch: 29, loss: 0.4376082420349121, acc: 82.8125, f1: 77.83491441173382, r: 0.634369391162521
06/02/2019 12:24:24 *** evaluating ***
06/02/2019 12:24:25 step: 112, epoch: 111, acc: 54.27350427350427, f1: 26.474296315701828, r: 0.32135600194405367
06/02/2019 12:24:25 *** epoch: 113 ***
06/02/2019 12:24:25 *** training ***
06/02/2019 12:24:26 step: 3701, epoch: 112, batch: 4, loss: 0.45245662331581116, acc: 82.8125, f1: 53.35646249202443, r: 0.6436473289407426
06/02/2019 12:24:27 step: 3706, epoch: 112, batch: 9, loss: 0.4554273188114166, acc: 82.8125, f1: 84.05973669646393, r: 0.5688936300688271
06/02/2019 12:24:28 step: 3711, epoch: 112, batch: 14, loss: 0.4898121654987335, acc: 81.25, f1: 83.55769230769229, r: 0.7144777755630587
06/02/2019 12:24:29 step: 3716, epoch: 112, batch: 19, loss: 0.48339834809303284, acc: 79.6875, f1: 65.41906980183377, r: 0.5690388395884667
06/02/2019 12:24:29 step: 3721, epoch: 112, batch: 24, loss: 0.4193257689476013, acc: 89.0625, f1: 78.76157407407408, r: 0.6183992731930943
06/02/2019 12:24:31 step: 3726, epoch: 112, batch: 29, loss: 0.5284521579742432, acc: 81.25, f1: 70.6458477110651, r: 0.6215941246730942
06/02/2019 12:24:31 *** evaluating ***
06/02/2019 12:24:32 step: 113, epoch: 112, acc: 56.837606837606835, f1: 27.12960636888494, r: 0.33790518099625444
06/02/2019 12:24:32 *** epoch: 114 ***
06/02/2019 12:24:32 *** training ***
06/02/2019 12:24:33 step: 3734, epoch: 113, batch: 4, loss: 0.4471225440502167, acc: 85.9375, f1: 74.78666905358635, r: 0.6307881459258475
06/02/2019 12:24:34 step: 3739, epoch: 113, batch: 9, loss: 0.43407967686653137, acc: 82.8125, f1: 77.13895786827062, r: 0.6288440629361212
06/02/2019 12:24:35 step: 3744, epoch: 113, batch: 14, loss: 0.5034759044647217, acc: 85.9375, f1: 65.12566137566138, r: 0.5933183536835357
06/02/2019 12:24:35 step: 3749, epoch: 113, batch: 19, loss: 0.38970693945884705, acc: 84.375, f1: 81.11486699943136, r: 0.6144223041899559
06/02/2019 12:24:37 step: 3754, epoch: 113, batch: 24, loss: 0.4624785780906677, acc: 85.9375, f1: 84.02145473574045, r: 0.6384420713919258
06/02/2019 12:24:38 step: 3759, epoch: 113, batch: 29, loss: 0.8076987862586975, acc: 84.375, f1: 87.99689440993788, r: 0.665517386636178
06/02/2019 12:24:38 *** evaluating ***
06/02/2019 12:24:38 step: 114, epoch: 113, acc: 57.26495726495726, f1: 26.09295800898897, r: 0.338138547806369
06/02/2019 12:24:38 *** epoch: 115 ***
06/02/2019 12:24:38 *** training ***
06/02/2019 12:24:39 step: 3767, epoch: 114, batch: 4, loss: 0.43347257375717163, acc: 84.375, f1: 75.57540498716968, r: 0.556624381067198
06/02/2019 12:24:40 step: 3772, epoch: 114, batch: 9, loss: 0.5947498083114624, acc: 79.6875, f1: 72.35516184495776, r: 0.5775070291992142
06/02/2019 12:24:41 step: 3777, epoch: 114, batch: 14, loss: 0.4048592150211334, acc: 87.5, f1: 73.15359286103967, r: 0.6987518172991338
06/02/2019 12:24:42 step: 3782, epoch: 114, batch: 19, loss: 0.49301061034202576, acc: 82.8125, f1: 66.90701567983515, r: 0.5927390817522867
06/02/2019 12:24:43 step: 3787, epoch: 114, batch: 24, loss: 0.4867742657661438, acc: 81.25, f1: 62.3008249474377, r: 0.6065204116665317
06/02/2019 12:24:45 step: 3792, epoch: 114, batch: 29, loss: 0.4224405288696289, acc: 82.8125, f1: 75.71371717713181, r: 0.6274008794137647
06/02/2019 12:24:45 *** evaluating ***
06/02/2019 12:24:45 step: 115, epoch: 114, acc: 55.12820512820513, f1: 27.238279678906395, r: 0.3314701361651093
06/02/2019 12:24:45 *** epoch: 116 ***
06/02/2019 12:24:45 *** training ***
06/02/2019 12:24:47 step: 3800, epoch: 115, batch: 4, loss: 0.5083169937133789, acc: 90.625, f1: 63.99103317989516, r: 0.5327564737437309
06/02/2019 12:24:47 step: 3805, epoch: 115, batch: 9, loss: 0.36898571252822876, acc: 89.0625, f1: 84.13725562820409, r: 0.6087049388761333
06/02/2019 12:24:48 step: 3810, epoch: 115, batch: 14, loss: 0.35413336753845215, acc: 85.9375, f1: 83.06296877725448, r: 0.7017431039511877
06/02/2019 12:24:49 step: 3815, epoch: 115, batch: 19, loss: 0.4849381148815155, acc: 79.6875, f1: 66.05602240896357, r: 0.7204818250244543
06/02/2019 12:24:50 step: 3820, epoch: 115, batch: 24, loss: 0.4141456186771393, acc: 90.625, f1: 93.9534108914535, r: 0.7348734644955149
06/02/2019 12:24:51 step: 3825, epoch: 115, batch: 29, loss: 0.40353256464004517, acc: 85.9375, f1: 71.30321067821069, r: 0.7783319529513635
06/02/2019 12:24:52 *** evaluating ***
06/02/2019 12:24:52 step: 116, epoch: 115, acc: 55.98290598290598, f1: 27.569546794399415, r: 0.33705111742287425
06/02/2019 12:24:52 *** epoch: 117 ***
06/02/2019 12:24:52 *** training ***
06/02/2019 12:24:53 step: 3833, epoch: 116, batch: 4, loss: 0.30251601338386536, acc: 95.3125, f1: 92.15966386554622, r: 0.6934796170553034
06/02/2019 12:24:54 step: 3838, epoch: 116, batch: 9, loss: 0.3597177565097809, acc: 85.9375, f1: 75.8002949431521, r: 0.6086956897546627
06/02/2019 12:24:55 step: 3843, epoch: 116, batch: 14, loss: 0.3684510886669159, acc: 90.625, f1: 84.81800766283524, r: 0.7954568861817806
06/02/2019 12:24:56 step: 3848, epoch: 116, batch: 19, loss: 0.4663032293319702, acc: 81.25, f1: 79.02409681821446, r: 0.5653576745121468
06/02/2019 12:24:57 step: 3853, epoch: 116, batch: 24, loss: 0.3550702631473541, acc: 90.625, f1: 84.8051948051948, r: 0.6879250573131651
06/02/2019 12:24:58 step: 3858, epoch: 116, batch: 29, loss: 0.4519222676753998, acc: 82.8125, f1: 68.46833721833721, r: 0.6950412218131893
06/02/2019 12:24:59 *** evaluating ***
06/02/2019 12:24:59 step: 117, epoch: 116, acc: 54.27350427350427, f1: 27.192778372306737, r: 0.32096091195459303
06/02/2019 12:24:59 *** epoch: 118 ***
06/02/2019 12:24:59 *** training ***
06/02/2019 12:25:00 step: 3866, epoch: 117, batch: 4, loss: 0.5854507088661194, acc: 81.25, f1: 72.71509740259741, r: 0.699144201718509
06/02/2019 12:25:01 step: 3871, epoch: 117, batch: 9, loss: 0.42258793115615845, acc: 90.625, f1: 88.19624819624819, r: 0.575021923279756
06/02/2019 12:25:02 step: 3876, epoch: 117, batch: 14, loss: 0.4191112220287323, acc: 90.625, f1: 82.6274058010301, r: 0.6594633411020867
06/02/2019 12:25:03 step: 3881, epoch: 117, batch: 19, loss: 0.3854389488697052, acc: 89.0625, f1: 91.09746752639941, r: 0.6262331892259158
06/02/2019 12:25:04 step: 3886, epoch: 117, batch: 24, loss: 0.3389531970024109, acc: 93.75, f1: 90.20054375622493, r: 0.638193233840955
06/02/2019 12:25:05 step: 3891, epoch: 117, batch: 29, loss: 0.5272243618965149, acc: 82.8125, f1: 78.66332497911445, r: 0.6450802091647339
06/02/2019 12:25:06 *** evaluating ***
06/02/2019 12:25:06 step: 118, epoch: 117, acc: 54.27350427350427, f1: 25.521667467435407, r: 0.31837295843828173
06/02/2019 12:25:06 *** epoch: 119 ***
06/02/2019 12:25:06 *** training ***
06/02/2019 12:25:07 step: 3899, epoch: 118, batch: 4, loss: 0.48444801568984985, acc: 84.375, f1: 69.99499714122356, r: 0.5836569326686116
06/02/2019 12:25:08 step: 3904, epoch: 118, batch: 9, loss: 0.467207670211792, acc: 84.375, f1: 68.54640371668236, r: 0.5625758132176742
06/02/2019 12:25:09 step: 3909, epoch: 118, batch: 14, loss: 0.3858194053173065, acc: 89.0625, f1: 89.95617285939866, r: 0.6670912593151878
06/02/2019 12:25:10 step: 3914, epoch: 118, batch: 19, loss: 0.42974743247032166, acc: 89.0625, f1: 88.48971118836185, r: 0.6622728164810046
06/02/2019 12:25:11 step: 3919, epoch: 118, batch: 24, loss: 0.3838261067867279, acc: 84.375, f1: 75.21781086297216, r: 0.49053828786909187
06/02/2019 12:25:12 step: 3924, epoch: 118, batch: 29, loss: 0.8233593702316284, acc: 84.375, f1: 60.650635913793806, r: 0.6440886110965475
06/02/2019 12:25:13 *** evaluating ***
06/02/2019 12:25:13 step: 119, epoch: 118, acc: 54.27350427350427, f1: 26.984937214290728, r: 0.3215278583291828
06/02/2019 12:25:13 *** epoch: 120 ***
06/02/2019 12:25:13 *** training ***
06/02/2019 12:25:14 step: 3932, epoch: 119, batch: 4, loss: 0.39678430557250977, acc: 84.375, f1: 63.94314724102863, r: 0.591967702829561
06/02/2019 12:25:15 step: 3937, epoch: 119, batch: 9, loss: 0.45889419317245483, acc: 85.9375, f1: 74.23983505022304, r: 0.6975551386450635
06/02/2019 12:25:16 step: 3942, epoch: 119, batch: 14, loss: 0.407990962266922, acc: 84.375, f1: 59.66641337386018, r: 0.6717617873806453
06/02/2019 12:25:17 step: 3947, epoch: 119, batch: 19, loss: 0.4757578372955322, acc: 81.25, f1: 65.43350168350169, r: 0.74787267713863
06/02/2019 12:25:18 step: 3952, epoch: 119, batch: 24, loss: 0.3365504741668701, acc: 87.5, f1: 73.28070175438596, r: 0.5485821242465228
06/02/2019 12:25:19 step: 3957, epoch: 119, batch: 29, loss: 0.31579113006591797, acc: 96.875, f1: 92.0479302832244, r: 0.6476211748434307
06/02/2019 12:25:20 *** evaluating ***
06/02/2019 12:25:20 step: 120, epoch: 119, acc: 55.98290598290598, f1: 26.92115085747816, r: 0.3294696255321379
06/02/2019 12:25:20 *** epoch: 121 ***
06/02/2019 12:25:20 *** training ***
06/02/2019 12:25:21 step: 3965, epoch: 120, batch: 4, loss: 0.4804169237613678, acc: 85.9375, f1: 81.9356833642548, r: 0.5807574570301912
06/02/2019 12:25:22 step: 3970, epoch: 120, batch: 9, loss: 0.39558708667755127, acc: 84.375, f1: 67.36487256264132, r: 0.6247173242298352
06/02/2019 12:25:23 step: 3975, epoch: 120, batch: 14, loss: 0.37016192078590393, acc: 84.375, f1: 55.35851472471192, r: 0.6074139048683171
06/02/2019 12:25:24 step: 3980, epoch: 120, batch: 19, loss: 0.5787875056266785, acc: 81.25, f1: 68.01947266569908, r: 0.6404024595301305
06/02/2019 12:25:25 step: 3985, epoch: 120, batch: 24, loss: 0.44234591722488403, acc: 87.5, f1: 78.10641497978501, r: 0.6413357272050073
06/02/2019 12:25:26 step: 3990, epoch: 120, batch: 29, loss: 0.5218650102615356, acc: 84.375, f1: 79.22510822510822, r: 0.6359368544093174
06/02/2019 12:25:27 *** evaluating ***
06/02/2019 12:25:27 step: 121, epoch: 120, acc: 55.98290598290598, f1: 26.4334758378876, r: 0.32926982509528663
06/02/2019 12:25:27 *** epoch: 122 ***
06/02/2019 12:25:27 *** training ***
06/02/2019 12:25:28 step: 3998, epoch: 121, batch: 4, loss: 0.32789430022239685, acc: 93.75, f1: 94.56177503052503, r: 0.653266157678211
06/02/2019 12:25:29 step: 4003, epoch: 121, batch: 9, loss: 0.3218167722225189, acc: 89.0625, f1: 66.09835416130817, r: 0.5518291660529245
06/02/2019 12:25:30 step: 4008, epoch: 121, batch: 14, loss: 0.38535013794898987, acc: 85.9375, f1: 75.67241968557758, r: 0.6857435821568363
06/02/2019 12:25:31 step: 4013, epoch: 121, batch: 19, loss: 0.375259131193161, acc: 84.375, f1: 72.50355366027009, r: 0.7501525432659504
06/02/2019 12:25:32 step: 4018, epoch: 121, batch: 24, loss: 0.5464935898780823, acc: 85.9375, f1: 64.26917989417989, r: 0.7395699030664459
06/02/2019 12:25:33 step: 4023, epoch: 121, batch: 29, loss: 0.4480311870574951, acc: 85.9375, f1: 82.19813519813519, r: 0.7408155718956452
06/02/2019 12:25:34 *** evaluating ***
06/02/2019 12:25:34 step: 122, epoch: 121, acc: 57.26495726495726, f1: 26.476570003969414, r: 0.33494418033031953
06/02/2019 12:25:34 *** epoch: 123 ***
06/02/2019 12:25:34 *** training ***
06/02/2019 12:25:35 step: 4031, epoch: 122, batch: 4, loss: 0.7500742077827454, acc: 85.9375, f1: 84.6174082232313, r: 0.60053665270864
06/02/2019 12:25:36 step: 4036, epoch: 122, batch: 9, loss: 0.6457376480102539, acc: 90.625, f1: 73.34700342053283, r: 0.6721116233142028
06/02/2019 12:25:37 step: 4041, epoch: 122, batch: 14, loss: 0.44137030839920044, acc: 87.5, f1: 82.14583333333333, r: 0.6812861287338491
06/02/2019 12:25:38 step: 4046, epoch: 122, batch: 19, loss: 0.3349511921405792, acc: 84.375, f1: 75.31084204016535, r: 0.7171436861766283
06/02/2019 12:25:39 step: 4051, epoch: 122, batch: 24, loss: 0.2521553337574005, acc: 93.75, f1: 87.49440214957457, r: 0.6153715269034004
06/02/2019 12:25:40 step: 4056, epoch: 122, batch: 29, loss: 0.4330737292766571, acc: 84.375, f1: 73.78721980595519, r: 0.605824654691242
06/02/2019 12:25:41 *** evaluating ***
06/02/2019 12:25:41 step: 123, epoch: 122, acc: 56.837606837606835, f1: 26.25772066148635, r: 0.33389208589244385
06/02/2019 12:25:41 *** epoch: 124 ***
06/02/2019 12:25:41 *** training ***
06/02/2019 12:25:42 step: 4064, epoch: 123, batch: 4, loss: 0.4063771963119507, acc: 87.5, f1: 77.75465838509317, r: 0.7368512478774363
06/02/2019 12:25:43 step: 4069, epoch: 123, batch: 9, loss: 0.5016406774520874, acc: 85.9375, f1: 83.91766566766566, r: 0.7034429083894462
06/02/2019 12:25:44 step: 4074, epoch: 123, batch: 14, loss: 0.43694183230400085, acc: 78.125, f1: 67.8671811280507, r: 0.572565634504556
06/02/2019 12:25:45 step: 4079, epoch: 123, batch: 19, loss: 0.3741283714771271, acc: 89.0625, f1: 81.71203600068169, r: 0.5755676850287615
06/02/2019 12:25:46 step: 4084, epoch: 123, batch: 24, loss: 0.4224851429462433, acc: 87.5, f1: 84.65612648221344, r: 0.6818492014823178
06/02/2019 12:25:47 step: 4089, epoch: 123, batch: 29, loss: 0.47403329610824585, acc: 84.375, f1: 66.5714701964702, r: 0.6040879166470838
06/02/2019 12:25:47 *** evaluating ***
06/02/2019 12:25:48 step: 124, epoch: 123, acc: 56.837606837606835, f1: 27.76261838761839, r: 0.330820966133629
06/02/2019 12:25:48 *** epoch: 125 ***
06/02/2019 12:25:48 *** training ***
06/02/2019 12:25:49 step: 4097, epoch: 124, batch: 4, loss: 0.39880627393722534, acc: 89.0625, f1: 89.8702112423917, r: 0.648980948184156
06/02/2019 12:25:50 step: 4102, epoch: 124, batch: 9, loss: 0.39382830262184143, acc: 82.8125, f1: 72.17194519317161, r: 0.5873640780793133
06/02/2019 12:25:50 step: 4107, epoch: 124, batch: 14, loss: 0.5489464998245239, acc: 82.8125, f1: 69.13338943288454, r: 0.5928041822854094
06/02/2019 12:25:51 step: 4112, epoch: 124, batch: 19, loss: 0.3401380777359009, acc: 87.5, f1: 83.72853903466148, r: 0.6774747506890764
06/02/2019 12:25:52 step: 4117, epoch: 124, batch: 24, loss: 0.28019270300865173, acc: 92.1875, f1: 84.18406593406593, r: 0.7329726049590497
06/02/2019 12:25:53 step: 4122, epoch: 124, batch: 29, loss: 0.31574904918670654, acc: 90.625, f1: 91.07069143446851, r: 0.6081668204975962
06/02/2019 12:25:54 *** evaluating ***
06/02/2019 12:25:54 step: 125, epoch: 124, acc: 57.26495726495726, f1: 28.013577331759155, r: 0.33614215685752646
06/02/2019 12:25:54 *** epoch: 126 ***
06/02/2019 12:25:54 *** training ***
06/02/2019 12:25:55 step: 4130, epoch: 125, batch: 4, loss: 0.4676498472690582, acc: 84.375, f1: 71.98401311744212, r: 0.6987998588373634
06/02/2019 12:25:56 step: 4135, epoch: 125, batch: 9, loss: 0.5940901041030884, acc: 73.4375, f1: 50.64039408866995, r: 0.4850602514678562
06/02/2019 12:25:57 step: 4140, epoch: 125, batch: 14, loss: 0.3899460434913635, acc: 90.625, f1: 76.10868226600985, r: 0.713270259690343
06/02/2019 12:25:58 step: 4145, epoch: 125, batch: 19, loss: 0.5338497161865234, acc: 84.375, f1: 79.27130622363889, r: 0.6097543558744979
06/02/2019 12:25:59 step: 4150, epoch: 125, batch: 24, loss: 0.414899080991745, acc: 85.9375, f1: 70.90305352910394, r: 0.6235006605107157
06/02/2019 12:26:00 step: 4155, epoch: 125, batch: 29, loss: 0.40202292799949646, acc: 85.9375, f1: 79.69845882462792, r: 0.6360859254189786
06/02/2019 12:26:01 *** evaluating ***
06/02/2019 12:26:01 step: 126, epoch: 125, acc: 56.41025641025641, f1: 28.067306338605714, r: 0.3225235597896243
06/02/2019 12:26:01 *** epoch: 127 ***
06/02/2019 12:26:01 *** training ***
06/02/2019 12:26:03 step: 4163, epoch: 126, batch: 4, loss: 0.26063334941864014, acc: 90.625, f1: 85.6423396800459, r: 0.6242143332249348
06/02/2019 12:26:04 step: 4168, epoch: 126, batch: 9, loss: 0.35562676191329956, acc: 87.5, f1: 82.71395271395272, r: 0.5802598668930317
06/02/2019 12:26:05 step: 4173, epoch: 126, batch: 14, loss: 0.3529636263847351, acc: 89.0625, f1: 77.21069902319903, r: 0.6894461477940779
06/02/2019 12:26:06 step: 4178, epoch: 126, batch: 19, loss: 0.5657722353935242, acc: 79.6875, f1: 72.20057171804892, r: 0.5803290628204002
06/02/2019 12:26:07 step: 4183, epoch: 126, batch: 24, loss: 0.428783118724823, acc: 87.5, f1: 83.45022962162922, r: 0.6018150681674767
06/02/2019 12:26:08 step: 4188, epoch: 126, batch: 29, loss: 0.528495728969574, acc: 87.5, f1: 70.07936507936508, r: 0.5781095095417713
06/02/2019 12:26:08 *** evaluating ***
06/02/2019 12:26:09 step: 127, epoch: 126, acc: 55.98290598290598, f1: 24.756902578483018, r: 0.3248340755551614
06/02/2019 12:26:09 *** epoch: 128 ***
06/02/2019 12:26:09 *** training ***
06/02/2019 12:26:10 step: 4196, epoch: 127, batch: 4, loss: 0.34581461548805237, acc: 92.1875, f1: 71.55896575291165, r: 0.6562143221685288
06/02/2019 12:26:11 step: 4201, epoch: 127, batch: 9, loss: 0.44936028122901917, acc: 81.25, f1: 51.32471942319141, r: 0.5448145251227303
06/02/2019 12:26:12 step: 4206, epoch: 127, batch: 14, loss: 0.3031783998012543, acc: 89.0625, f1: 77.14965986394559, r: 0.6326826934673547
06/02/2019 12:26:13 step: 4211, epoch: 127, batch: 19, loss: 0.33836662769317627, acc: 93.75, f1: 87.5964429659011, r: 0.5578667086449202
06/02/2019 12:26:14 step: 4216, epoch: 127, batch: 24, loss: 0.3618679344654083, acc: 89.0625, f1: 73.87972680326502, r: 0.4922304417417555
06/02/2019 12:26:15 step: 4221, epoch: 127, batch: 29, loss: 0.3696051836013794, acc: 87.5, f1: 59.62021221532091, r: 0.5907828574220556
06/02/2019 12:26:16 *** evaluating ***
06/02/2019 12:26:16 step: 128, epoch: 127, acc: 56.837606837606835, f1: 28.37903775635018, r: 0.3243924505035616
06/02/2019 12:26:16 *** epoch: 129 ***
06/02/2019 12:26:16 *** training ***
06/02/2019 12:26:17 step: 4229, epoch: 128, batch: 4, loss: 0.44782695174217224, acc: 85.9375, f1: 72.30978105527925, r: 0.6006893222151115
06/02/2019 12:26:18 step: 4234, epoch: 128, batch: 9, loss: 0.38198450207710266, acc: 87.5, f1: 70.26771677122794, r: 0.525901942003646
06/02/2019 12:26:19 step: 4239, epoch: 128, batch: 14, loss: 0.23441724479198456, acc: 92.1875, f1: 89.81188949938951, r: 0.5943469106555157
06/02/2019 12:26:20 step: 4244, epoch: 128, batch: 19, loss: 0.4653246998786926, acc: 78.125, f1: 66.34806166056167, r: 0.7156068402338382
06/02/2019 12:26:21 step: 4249, epoch: 128, batch: 24, loss: 0.32027468085289, acc: 92.1875, f1: 84.52243511012855, r: 0.6807639025040901
06/02/2019 12:26:22 step: 4254, epoch: 128, batch: 29, loss: 0.4518270790576935, acc: 82.8125, f1: 77.46446255702682, r: 0.7081779409357672
06/02/2019 12:26:23 *** evaluating ***
06/02/2019 12:26:23 step: 129, epoch: 128, acc: 55.98290598290598, f1: 27.42496392496393, r: 0.32533167385617295
06/02/2019 12:26:23 *** epoch: 130 ***
06/02/2019 12:26:23 *** training ***
06/02/2019 12:26:24 step: 4262, epoch: 129, batch: 4, loss: 0.5025413036346436, acc: 90.625, f1: 80.27447089947091, r: 0.7536876098485595
06/02/2019 12:26:25 step: 4267, epoch: 129, batch: 9, loss: 0.3897962272167206, acc: 85.9375, f1: 72.12720015703724, r: 0.6372316047216602
06/02/2019 12:26:26 step: 4272, epoch: 129, batch: 14, loss: 0.28134289383888245, acc: 92.1875, f1: 70.65910603371783, r: 0.5600291828236975
06/02/2019 12:26:27 step: 4277, epoch: 129, batch: 19, loss: 0.4805088937282562, acc: 84.375, f1: 66.52793965293965, r: 0.6201525030352126
06/02/2019 12:26:28 step: 4282, epoch: 129, batch: 24, loss: 0.3939688503742218, acc: 85.9375, f1: 78.92156862745098, r: 0.6801717510164305
06/02/2019 12:26:29 step: 4287, epoch: 129, batch: 29, loss: 0.3828451335430145, acc: 87.5, f1: 79.81856586653731, r: 0.6597185221234478
06/02/2019 12:26:30 *** evaluating ***
06/02/2019 12:26:30 step: 130, epoch: 129, acc: 57.692307692307686, f1: 26.409695531485365, r: 0.32952893814237144
06/02/2019 12:26:30 *** epoch: 131 ***
06/02/2019 12:26:30 *** training ***
06/02/2019 12:26:31 step: 4295, epoch: 130, batch: 4, loss: 0.4085395932197571, acc: 89.0625, f1: 89.71412971412971, r: 0.7002898115556737
06/02/2019 12:26:32 step: 4300, epoch: 130, batch: 9, loss: 0.3351099193096161, acc: 87.5, f1: 71.11019821946074, r: 0.5783141105927859
06/02/2019 12:26:33 step: 4305, epoch: 130, batch: 14, loss: 0.3342425227165222, acc: 89.0625, f1: 86.40859021246953, r: 0.7299532540529283
06/02/2019 12:26:34 step: 4310, epoch: 130, batch: 19, loss: 0.3664511442184448, acc: 89.0625, f1: 86.31776953205524, r: 0.6502288830969414
06/02/2019 12:26:35 step: 4315, epoch: 130, batch: 24, loss: 0.7213231325149536, acc: 89.0625, f1: 85.05190677966101, r: 0.7005647759369145
06/02/2019 12:26:36 step: 4320, epoch: 130, batch: 29, loss: 0.44935184717178345, acc: 84.375, f1: 61.83990937223696, r: 0.6469664667119615
06/02/2019 12:26:37 *** evaluating ***
06/02/2019 12:26:37 step: 131, epoch: 130, acc: 56.837606837606835, f1: 26.18067946172743, r: 0.3311233704715967
06/02/2019 12:26:37 *** epoch: 132 ***
06/02/2019 12:26:37 *** training ***
06/02/2019 12:26:38 step: 4328, epoch: 131, batch: 4, loss: 0.2792549729347229, acc: 90.625, f1: 77.81898656898657, r: 0.7567080762678602
06/02/2019 12:26:39 step: 4333, epoch: 131, batch: 9, loss: 0.268801212310791, acc: 90.625, f1: 79.13592410144135, r: 0.633742128507704
06/02/2019 12:26:40 step: 4338, epoch: 131, batch: 14, loss: 0.39458900690078735, acc: 89.0625, f1: 80.14131072361755, r: 0.6895714358811034
06/02/2019 12:26:41 step: 4343, epoch: 131, batch: 19, loss: 0.326083779335022, acc: 87.5, f1: 85.3279493615628, r: 0.628149434424826
06/02/2019 12:26:42 step: 4348, epoch: 131, batch: 24, loss: 0.43230265378952026, acc: 85.9375, f1: 68.94511650814171, r: 0.5422768005164086
06/02/2019 12:26:43 step: 4353, epoch: 131, batch: 29, loss: 0.7339727878570557, acc: 84.375, f1: 76.80646421247926, r: 0.653348942263168
06/02/2019 12:26:44 *** evaluating ***
06/02/2019 12:26:44 step: 132, epoch: 131, acc: 56.41025641025641, f1: 27.693258324063276, r: 0.32238313872710556
06/02/2019 12:26:44 *** epoch: 133 ***
06/02/2019 12:26:44 *** training ***
06/02/2019 12:26:45 step: 4361, epoch: 132, batch: 4, loss: 0.29012537002563477, acc: 85.9375, f1: 63.01816239316239, r: 0.6859122644164654
06/02/2019 12:26:46 step: 4366, epoch: 132, batch: 9, loss: 0.4428732693195343, acc: 84.375, f1: 70.89173618778881, r: 0.6294453096690606
06/02/2019 12:26:47 step: 4371, epoch: 132, batch: 14, loss: 0.4204835593700409, acc: 85.9375, f1: 78.48912720973192, r: 0.7374213504825254
06/02/2019 12:26:48 step: 4376, epoch: 132, batch: 19, loss: 0.6329333782196045, acc: 92.1875, f1: 87.52019075049982, r: 0.6129298737927251
06/02/2019 12:26:49 step: 4381, epoch: 132, batch: 24, loss: 0.31997525691986084, acc: 90.625, f1: 91.27986211319545, r: 0.5964070919767231
06/02/2019 12:26:50 step: 4386, epoch: 132, batch: 29, loss: 0.42588624358177185, acc: 87.5, f1: 86.31676319176319, r: 0.7121950530863335
06/02/2019 12:26:51 *** evaluating ***
06/02/2019 12:26:51 step: 133, epoch: 132, acc: 57.692307692307686, f1: 27.57947118241236, r: 0.3264215390083239
06/02/2019 12:26:51 *** epoch: 134 ***
06/02/2019 12:26:51 *** training ***
06/02/2019 12:26:52 step: 4394, epoch: 133, batch: 4, loss: 0.3953354060649872, acc: 85.9375, f1: 74.88726240113589, r: 0.6281884529378069
06/02/2019 12:26:53 step: 4399, epoch: 133, batch: 9, loss: 0.3392740488052368, acc: 87.5, f1: 84.49061355311355, r: 0.7213959445216147
06/02/2019 12:26:54 step: 4404, epoch: 133, batch: 14, loss: 0.3379405438899994, acc: 90.625, f1: 88.62852833441069, r: 0.6268428370109747
06/02/2019 12:26:55 step: 4409, epoch: 133, batch: 19, loss: 0.4120432436466217, acc: 85.9375, f1: 78.6489071510708, r: 0.7540477276398742
06/02/2019 12:26:56 step: 4414, epoch: 133, batch: 24, loss: 0.3528604805469513, acc: 89.0625, f1: 78.66883116883116, r: 0.688471343938235
06/02/2019 12:26:57 step: 4419, epoch: 133, batch: 29, loss: 0.35650259256362915, acc: 87.5, f1: 78.04228004699702, r: 0.7446447784829519
06/02/2019 12:26:58 *** evaluating ***
06/02/2019 12:26:58 step: 134, epoch: 133, acc: 57.692307692307686, f1: 27.131994074452383, r: 0.3249013053940049
06/02/2019 12:26:58 *** epoch: 135 ***
06/02/2019 12:26:58 *** training ***
06/02/2019 12:26:59 step: 4427, epoch: 134, batch: 4, loss: 0.38784259557724, acc: 84.375, f1: 71.62507631257631, r: 0.6297239233133104
06/02/2019 12:27:00 step: 4432, epoch: 134, batch: 9, loss: 0.5765082240104675, acc: 78.125, f1: 62.10678210678211, r: 0.6298824146090283
06/02/2019 12:27:01 step: 4437, epoch: 134, batch: 14, loss: 0.4705192744731903, acc: 84.375, f1: 76.09961219336219, r: 0.644507206092224
06/02/2019 12:27:02 step: 4442, epoch: 134, batch: 19, loss: 0.3162805140018463, acc: 90.625, f1: 88.42490842490844, r: 0.6101291975501677
06/02/2019 12:27:03 step: 4447, epoch: 134, batch: 24, loss: 0.46709099411964417, acc: 87.5, f1: 78.82019145917809, r: 0.5778519941223664
06/02/2019 12:27:04 step: 4452, epoch: 134, batch: 29, loss: 0.34291794896125793, acc: 93.75, f1: 89.91777356103732, r: 0.72181462958764
06/02/2019 12:27:05 *** evaluating ***
06/02/2019 12:27:05 step: 135, epoch: 134, acc: 57.692307692307686, f1: 28.63516997708174, r: 0.32405835388242754
06/02/2019 12:27:05 *** epoch: 136 ***
06/02/2019 12:27:05 *** training ***
06/02/2019 12:27:06 step: 4460, epoch: 135, batch: 4, loss: 0.27166637778282166, acc: 95.3125, f1: 90.98624270594698, r: 0.6343770545193786
06/02/2019 12:27:07 step: 4465, epoch: 135, batch: 9, loss: 0.40971672534942627, acc: 90.625, f1: 86.658239001989, r: 0.6629989998922153
06/02/2019 12:27:08 step: 4470, epoch: 135, batch: 14, loss: 0.36223477125167847, acc: 92.1875, f1: 90.75107277632853, r: 0.7541603397437198
06/02/2019 12:27:09 step: 4475, epoch: 135, batch: 19, loss: 0.326896995306015, acc: 90.625, f1: 73.07023128287116, r: 0.5606188736844362
06/02/2019 12:27:11 step: 4480, epoch: 135, batch: 24, loss: 0.20571498572826385, acc: 96.875, f1: 96.95061004987174, r: 0.6766405358422419
06/02/2019 12:27:12 step: 4485, epoch: 135, batch: 29, loss: 0.5119581818580627, acc: 84.375, f1: 67.99740632038768, r: 0.6318794439857941
06/02/2019 12:27:12 *** evaluating ***
06/02/2019 12:27:13 step: 136, epoch: 135, acc: 56.837606837606835, f1: 25.655345877446127, r: 0.323419518530511
06/02/2019 12:27:13 *** epoch: 137 ***
06/02/2019 12:27:13 *** training ***
06/02/2019 12:27:14 step: 4493, epoch: 136, batch: 4, loss: 0.4801434278488159, acc: 82.8125, f1: 76.1372135060921, r: 0.6499209515327508
06/02/2019 12:27:15 step: 4498, epoch: 136, batch: 9, loss: 0.4784042239189148, acc: 82.8125, f1: 72.93166422820678, r: 0.6052220566112817
06/02/2019 12:27:16 step: 4503, epoch: 136, batch: 14, loss: 0.3108050525188446, acc: 92.1875, f1: 78.76615721443308, r: 0.6733414816609186
06/02/2019 12:27:17 step: 4508, epoch: 136, batch: 19, loss: 0.3608001470565796, acc: 87.5, f1: 79.50125504747353, r: 0.6764595644152224
06/02/2019 12:27:18 step: 4513, epoch: 136, batch: 24, loss: 0.7822195291519165, acc: 85.9375, f1: 82.92517006802721, r: 0.5409502607576278
06/02/2019 12:27:19 step: 4518, epoch: 136, batch: 29, loss: 0.277936726808548, acc: 95.3125, f1: 91.50252525252525, r: 0.7541687118382917
06/02/2019 12:27:19 *** evaluating ***
06/02/2019 12:27:20 step: 137, epoch: 136, acc: 56.41025641025641, f1: 26.576257162602268, r: 0.32268881752059747
06/02/2019 12:27:20 *** epoch: 138 ***
06/02/2019 12:27:20 *** training ***
06/02/2019 12:27:21 step: 4526, epoch: 137, batch: 4, loss: 0.26195141673088074, acc: 96.875, f1: 95.83152173913044, r: 0.7681926040295884
06/02/2019 12:27:22 step: 4531, epoch: 137, batch: 9, loss: 0.33249714970588684, acc: 89.0625, f1: 74.33237547892719, r: 0.6747905974352241
06/02/2019 12:27:23 step: 4536, epoch: 137, batch: 14, loss: 0.3650721609592438, acc: 85.9375, f1: 66.04086040208144, r: 0.6071089903180092
06/02/2019 12:27:24 step: 4541, epoch: 137, batch: 19, loss: 0.3972713351249695, acc: 79.6875, f1: 52.65878003946693, r: 0.5541744641972501
06/02/2019 12:27:25 step: 4546, epoch: 137, batch: 24, loss: 0.378179669380188, acc: 84.375, f1: 65.89433699388157, r: 0.6043892738112231
06/02/2019 12:27:26 step: 4551, epoch: 137, batch: 29, loss: 0.3812723755836487, acc: 84.375, f1: 78.45164152617568, r: 0.6028838928634018
06/02/2019 12:27:26 *** evaluating ***
06/02/2019 12:27:27 step: 138, epoch: 137, acc: 57.692307692307686, f1: 27.179454801286173, r: 0.329647047597987
06/02/2019 12:27:27 *** epoch: 139 ***
06/02/2019 12:27:27 *** training ***
06/02/2019 12:27:28 step: 4559, epoch: 138, batch: 4, loss: 0.7479788661003113, acc: 84.375, f1: 48.443770589997, r: 0.5077911042552603
06/02/2019 12:27:29 step: 4564, epoch: 138, batch: 9, loss: 0.42611163854599, acc: 82.8125, f1: 70.97423589341693, r: 0.6322679816997995
06/02/2019 12:27:30 step: 4569, epoch: 138, batch: 14, loss: 0.3328578770160675, acc: 89.0625, f1: 86.39047694469795, r: 0.6878578949517845
06/02/2019 12:27:31 step: 4574, epoch: 138, batch: 19, loss: 0.3810817301273346, acc: 89.0625, f1: 83.06528037571617, r: 0.6291285909968973
06/02/2019 12:27:32 step: 4579, epoch: 138, batch: 24, loss: 0.34196364879608154, acc: 84.375, f1: 86.13053613053613, r: 0.7633100698618386
06/02/2019 12:27:33 step: 4584, epoch: 138, batch: 29, loss: 0.26976653933525085, acc: 96.875, f1: 86.22109597719354, r: 0.6433715050947652
06/02/2019 12:27:34 *** evaluating ***
06/02/2019 12:27:34 step: 139, epoch: 138, acc: 57.692307692307686, f1: 26.900427182474253, r: 0.3258743664447141
06/02/2019 12:27:34 *** epoch: 140 ***
06/02/2019 12:27:34 *** training ***
06/02/2019 12:27:35 step: 4592, epoch: 139, batch: 4, loss: 0.3986147940158844, acc: 90.625, f1: 89.12829129934393, r: 0.7115826193682002
06/02/2019 12:27:36 step: 4597, epoch: 139, batch: 9, loss: 0.28377631306648254, acc: 90.625, f1: 89.7974247974248, r: 0.6844113816345698
06/02/2019 12:27:37 step: 4602, epoch: 139, batch: 14, loss: 0.5113146305084229, acc: 84.375, f1: 83.06970296860003, r: 0.7577075394161017
06/02/2019 12:27:38 step: 4607, epoch: 139, batch: 19, loss: 0.47190842032432556, acc: 84.375, f1: 73.73205741626793, r: 0.6520731757491678
06/02/2019 12:27:39 step: 4612, epoch: 139, batch: 24, loss: 0.28327205777168274, acc: 93.75, f1: 83.81802721088437, r: 0.6749992406602497
06/02/2019 12:27:40 step: 4617, epoch: 139, batch: 29, loss: 0.4666275084018707, acc: 87.5, f1: 85.05535547552353, r: 0.6418857617942146
06/02/2019 12:27:41 *** evaluating ***
06/02/2019 12:27:41 step: 140, epoch: 139, acc: 56.837606837606835, f1: 27.25597540757115, r: 0.32566846212989425
06/02/2019 12:27:41 *** epoch: 141 ***
06/02/2019 12:27:41 *** training ***
06/02/2019 12:27:42 step: 4625, epoch: 140, batch: 4, loss: 0.33770760893821716, acc: 85.9375, f1: 82.80068701775141, r: 0.6333545651819366
06/02/2019 12:27:43 step: 4630, epoch: 140, batch: 9, loss: 0.39317983388900757, acc: 89.0625, f1: 88.59632034632034, r: 0.637942974455105
06/02/2019 12:27:44 step: 4635, epoch: 140, batch: 14, loss: 0.3988996148109436, acc: 89.0625, f1: 85.6422569027611, r: 0.6499948904814181
06/02/2019 12:27:45 step: 4640, epoch: 140, batch: 19, loss: 0.2702597379684448, acc: 93.75, f1: 82.2141928120189, r: 0.7247378857209148
06/02/2019 12:27:46 step: 4645, epoch: 140, batch: 24, loss: 0.34360164403915405, acc: 90.625, f1: 77.69918699186992, r: 0.5775790440939005
06/02/2019 12:27:47 step: 4650, epoch: 140, batch: 29, loss: 0.49000710248947144, acc: 81.25, f1: 78.00123685837971, r: 0.6391459215846905
06/02/2019 12:27:48 *** evaluating ***
06/02/2019 12:27:48 step: 141, epoch: 140, acc: 56.837606837606835, f1: 27.357355985872612, r: 0.3291151313917973
06/02/2019 12:27:48 *** epoch: 142 ***
06/02/2019 12:27:48 *** training ***
06/02/2019 12:27:49 step: 4658, epoch: 141, batch: 4, loss: 0.6862883567810059, acc: 89.0625, f1: 91.65983420261357, r: 0.6640900398153047
06/02/2019 12:27:50 step: 4663, epoch: 141, batch: 9, loss: 0.36621078848838806, acc: 87.5, f1: 88.56827183656452, r: 0.6684753087826018
06/02/2019 12:27:51 step: 4668, epoch: 141, batch: 14, loss: 0.3189829885959625, acc: 90.625, f1: 77.96775081846832, r: 0.5667099278028007
06/02/2019 12:27:52 step: 4673, epoch: 141, batch: 19, loss: 0.28876230120658875, acc: 87.5, f1: 80.29223036732424, r: 0.6509398179608217
06/02/2019 12:27:53 step: 4678, epoch: 141, batch: 24, loss: 0.2554548680782318, acc: 92.1875, f1: 73.4927035330261, r: 0.6259815825814599
06/02/2019 12:27:54 step: 4683, epoch: 141, batch: 29, loss: 0.4261343479156494, acc: 79.6875, f1: 63.931494237195665, r: 0.5025978535552299
06/02/2019 12:27:55 *** evaluating ***
06/02/2019 12:27:55 step: 142, epoch: 141, acc: 57.26495726495726, f1: 25.704181305227298, r: 0.32791092545622225
06/02/2019 12:27:55 *** epoch: 143 ***
06/02/2019 12:27:55 *** training ***
06/02/2019 12:27:56 step: 4691, epoch: 142, batch: 4, loss: 0.346467524766922, acc: 87.5, f1: 86.8921308576481, r: 0.5735599543326151
06/02/2019 12:27:57 step: 4696, epoch: 142, batch: 9, loss: 0.3811799883842468, acc: 85.9375, f1: 69.15252119056467, r: 0.6559001067242094
06/02/2019 12:27:58 step: 4701, epoch: 142, batch: 14, loss: 0.37409672141075134, acc: 89.0625, f1: 79.57539682539682, r: 0.5713432514325761
06/02/2019 12:27:59 step: 4706, epoch: 142, batch: 19, loss: 0.5364927053451538, acc: 96.875, f1: 97.90249433106577, r: 0.6755438432247015
06/02/2019 12:28:00 step: 4711, epoch: 142, batch: 24, loss: 0.1888580024242401, acc: 95.3125, f1: 87.975592813615, r: 0.5779709998155247
06/02/2019 12:28:01 step: 4716, epoch: 142, batch: 29, loss: 0.40809303522109985, acc: 89.0625, f1: 80.52921841031598, r: 0.7554528583397322
06/02/2019 12:28:02 *** evaluating ***
06/02/2019 12:28:02 step: 143, epoch: 142, acc: 56.41025641025641, f1: 25.95383161316056, r: 0.3258004558269824
06/02/2019 12:28:02 *** epoch: 144 ***
06/02/2019 12:28:02 *** training ***
06/02/2019 12:28:03 step: 4724, epoch: 143, batch: 4, loss: 0.6354036927223206, acc: 93.75, f1: 88.93638768638769, r: 0.7291006406358274
06/02/2019 12:28:04 step: 4729, epoch: 143, batch: 9, loss: 0.32819893956184387, acc: 87.5, f1: 71.76587301587301, r: 0.6735354766995394
06/02/2019 12:28:05 step: 4734, epoch: 143, batch: 14, loss: 0.35990989208221436, acc: 89.0625, f1: 76.77680141495931, r: 0.5954199768796525
06/02/2019 12:28:06 step: 4739, epoch: 143, batch: 19, loss: 0.31998759508132935, acc: 89.0625, f1: 90.85598611914402, r: 0.6350225728603747
06/02/2019 12:28:07 step: 4744, epoch: 143, batch: 24, loss: 0.2200230360031128, acc: 95.3125, f1: 87.02380952380952, r: 0.6071372518219247
06/02/2019 12:28:08 step: 4749, epoch: 143, batch: 29, loss: 0.32763782143592834, acc: 90.625, f1: 92.2584541062802, r: 0.6728070420887594
06/02/2019 12:28:09 *** evaluating ***
06/02/2019 12:28:09 step: 144, epoch: 143, acc: 55.55555555555556, f1: 26.635625073125073, r: 0.32429573723792204
06/02/2019 12:28:09 *** epoch: 145 ***
06/02/2019 12:28:09 *** training ***
06/02/2019 12:28:10 step: 4757, epoch: 144, batch: 4, loss: 0.3900855481624603, acc: 85.9375, f1: 82.81539888682747, r: 0.6221845174918125
06/02/2019 12:28:11 step: 4762, epoch: 144, batch: 9, loss: 0.4175799489021301, acc: 79.6875, f1: 68.05734596914536, r: 0.5986621872280422
06/02/2019 12:28:12 step: 4767, epoch: 144, batch: 14, loss: 0.2959436774253845, acc: 90.625, f1: 91.14442454097626, r: 0.7103549480990798
06/02/2019 12:28:13 step: 4772, epoch: 144, batch: 19, loss: 0.31649860739707947, acc: 85.9375, f1: 59.0495981001096, r: 0.6283782235798018
06/02/2019 12:28:14 step: 4777, epoch: 144, batch: 24, loss: 0.3298553228378296, acc: 84.375, f1: 73.07569492443442, r: 0.600559639293232
06/02/2019 12:28:15 step: 4782, epoch: 144, batch: 29, loss: 0.28785455226898193, acc: 92.1875, f1: 94.87781954887218, r: 0.8008005077910231
06/02/2019 12:28:16 *** evaluating ***
06/02/2019 12:28:16 step: 145, epoch: 144, acc: 57.26495726495726, f1: 27.90146123895455, r: 0.31892843599796605
06/02/2019 12:28:16 *** epoch: 146 ***
06/02/2019 12:28:16 *** training ***
06/02/2019 12:28:17 step: 4790, epoch: 145, batch: 4, loss: 0.35569241642951965, acc: 90.625, f1: 86.56856553408278, r: 0.5814572797461222
06/02/2019 12:28:18 step: 4795, epoch: 145, batch: 9, loss: 0.3569367229938507, acc: 90.625, f1: 89.68688661235245, r: 0.6631942506876989
06/02/2019 12:28:19 step: 4800, epoch: 145, batch: 14, loss: 0.29266685247421265, acc: 87.5, f1: 78.77797495809918, r: 0.6299664344608678
06/02/2019 12:28:20 step: 4805, epoch: 145, batch: 19, loss: 0.3338123559951782, acc: 95.3125, f1: 95.82152166829587, r: 0.7557098727492229
06/02/2019 12:28:21 step: 4810, epoch: 145, batch: 24, loss: 0.38702502846717834, acc: 85.9375, f1: 69.90968056226698, r: 0.5682535144067274
06/02/2019 12:28:22 step: 4815, epoch: 145, batch: 29, loss: 0.3833332061767578, acc: 87.5, f1: 81.33354218880535, r: 0.7561815830363936
06/02/2019 12:28:23 *** evaluating ***
06/02/2019 12:28:23 step: 146, epoch: 145, acc: 56.41025641025641, f1: 25.945207400519, r: 0.32510889629271894
06/02/2019 12:28:23 *** epoch: 147 ***
06/02/2019 12:28:23 *** training ***
06/02/2019 12:28:24 step: 4823, epoch: 146, batch: 4, loss: 0.37470728158950806, acc: 84.375, f1: 76.04612455638326, r: 0.6187436442278043
06/02/2019 12:28:25 step: 4828, epoch: 146, batch: 9, loss: 0.24698756635189056, acc: 93.75, f1: 90.4286282103707, r: 0.5405713412122162
06/02/2019 12:28:26 step: 4833, epoch: 146, batch: 14, loss: 0.42954838275909424, acc: 81.25, f1: 67.63232407127757, r: 0.642176045406474
06/02/2019 12:28:27 step: 4838, epoch: 146, batch: 19, loss: 0.3204297721385956, acc: 90.625, f1: 89.10035590886656, r: 0.6735129226336554
06/02/2019 12:28:28 step: 4843, epoch: 146, batch: 24, loss: 0.28647124767303467, acc: 95.3125, f1: 95.41176470588235, r: 0.6318267446500916
06/02/2019 12:28:29 step: 4848, epoch: 146, batch: 29, loss: 0.3409188687801361, acc: 89.0625, f1: 82.508658008658, r: 0.644128219286563
06/02/2019 12:28:30 *** evaluating ***
06/02/2019 12:28:30 step: 147, epoch: 146, acc: 57.26495726495726, f1: 27.120633713766352, r: 0.32096217752823053
06/02/2019 12:28:30 *** epoch: 148 ***
06/02/2019 12:28:30 *** training ***
06/02/2019 12:28:31 step: 4856, epoch: 147, batch: 4, loss: 0.30259066820144653, acc: 92.1875, f1: 89.07122269197535, r: 0.6327448839960584
06/02/2019 12:28:33 step: 4861, epoch: 147, batch: 9, loss: 0.38757041096687317, acc: 90.625, f1: 79.81829573934837, r: 0.7398240314490625
06/02/2019 12:28:33 step: 4866, epoch: 147, batch: 14, loss: 0.30394548177719116, acc: 93.75, f1: 93.25638379508348, r: 0.6783159380266727
06/02/2019 12:28:35 step: 4871, epoch: 147, batch: 19, loss: 0.34151363372802734, acc: 87.5, f1: 72.3735119047619, r: 0.704650894668756
06/02/2019 12:28:36 step: 4876, epoch: 147, batch: 24, loss: 0.37606173753738403, acc: 87.5, f1: 81.41975818667547, r: 0.6141260240244186
06/02/2019 12:28:37 step: 4881, epoch: 147, batch: 29, loss: 0.44911548495292664, acc: 84.375, f1: 80.60515873015872, r: 0.7197168343368266
06/02/2019 12:28:37 *** evaluating ***
06/02/2019 12:28:38 step: 148, epoch: 147, acc: 55.12820512820513, f1: 25.4968876291356, r: 0.31952105735005154
06/02/2019 12:28:38 *** epoch: 149 ***
06/02/2019 12:28:38 *** training ***
06/02/2019 12:28:39 step: 4889, epoch: 148, batch: 4, loss: 0.3177592158317566, acc: 85.9375, f1: 69.69099832915622, r: 0.5669733309556584
06/02/2019 12:28:40 step: 4894, epoch: 148, batch: 9, loss: 0.6891125440597534, acc: 85.9375, f1: 82.11515324214793, r: 0.6926069045124803
06/02/2019 12:28:41 step: 4899, epoch: 148, batch: 14, loss: 0.3240674138069153, acc: 89.0625, f1: 85.69427449885441, r: 0.7036338094273704
06/02/2019 12:28:42 step: 4904, epoch: 148, batch: 19, loss: 0.32017531991004944, acc: 92.1875, f1: 88.34791059280855, r: 0.6843890059088055
06/02/2019 12:28:43 step: 4909, epoch: 148, batch: 24, loss: 0.21564903855323792, acc: 95.3125, f1: 93.14619883040936, r: 0.7110833843205321
06/02/2019 12:28:44 step: 4914, epoch: 148, batch: 29, loss: 0.37324994802474976, acc: 87.5, f1: 71.00484006734007, r: 0.6246584648615571
06/02/2019 12:28:44 *** evaluating ***
06/02/2019 12:28:45 step: 149, epoch: 148, acc: 58.119658119658126, f1: 26.86107221552856, r: 0.32173356905177625
06/02/2019 12:28:45 *** epoch: 150 ***
06/02/2019 12:28:45 *** training ***
06/02/2019 12:28:46 step: 4922, epoch: 149, batch: 4, loss: 0.35411861538887024, acc: 85.9375, f1: 68.11355311355311, r: 0.6597890389623775
06/02/2019 12:28:46 step: 4927, epoch: 149, batch: 9, loss: 0.318094402551651, acc: 89.0625, f1: 77.08536585365854, r: 0.6927941520342711
06/02/2019 12:28:47 step: 4932, epoch: 149, batch: 14, loss: 0.32412904500961304, acc: 87.5, f1: 77.9398762157383, r: 0.5770626631441221
06/02/2019 12:28:49 step: 4937, epoch: 149, batch: 19, loss: 0.35921600461006165, acc: 89.0625, f1: 66.60548941798942, r: 0.5992416616800622
06/02/2019 12:28:50 step: 4942, epoch: 149, batch: 24, loss: 0.3777281641960144, acc: 84.375, f1: 64.59951722624534, r: 0.5694171041787928
06/02/2019 12:28:51 step: 4947, epoch: 149, batch: 29, loss: 0.2827852666378021, acc: 93.75, f1: 89.2517006802721, r: 0.5958190296538488
06/02/2019 12:28:51 *** evaluating ***
06/02/2019 12:28:51 step: 150, epoch: 149, acc: 56.837606837606835, f1: 28.177682824421957, r: 0.32058066989086387
06/02/2019 12:28:51 *** epoch: 151 ***
06/02/2019 12:28:51 *** training ***
06/02/2019 12:28:53 step: 4955, epoch: 150, batch: 4, loss: 0.3175719082355499, acc: 92.1875, f1: 82.90815644621922, r: 0.6186315508761809
06/02/2019 12:28:54 step: 4960, epoch: 150, batch: 9, loss: 0.287369042634964, acc: 90.625, f1: 77.09808861448207, r: 0.6688148424009454
06/02/2019 12:28:55 step: 4965, epoch: 150, batch: 14, loss: 0.6202638745307922, acc: 92.1875, f1: 94.66718077244393, r: 0.6465492298788191
06/02/2019 12:28:56 step: 4970, epoch: 150, batch: 19, loss: 0.37522974610328674, acc: 84.375, f1: 56.41820121022158, r: 0.6313089264455298
06/02/2019 12:28:57 step: 4975, epoch: 150, batch: 24, loss: 0.30491873621940613, acc: 89.0625, f1: 87.2093482554312, r: 0.691480367902981
06/02/2019 12:28:58 step: 4980, epoch: 150, batch: 29, loss: 0.21965442597866058, acc: 93.75, f1: 90.69295900178254, r: 0.7101117176765588
06/02/2019 12:28:58 *** evaluating ***
06/02/2019 12:28:59 step: 151, epoch: 150, acc: 56.837606837606835, f1: 27.990424430641824, r: 0.324210805030197
06/02/2019 12:28:59 *** epoch: 152 ***
06/02/2019 12:28:59 *** training ***
06/02/2019 12:29:00 step: 4988, epoch: 151, batch: 4, loss: 0.2548621594905853, acc: 87.5, f1: 75.3061224489796, r: 0.6311232498046949
06/02/2019 12:29:01 step: 4993, epoch: 151, batch: 9, loss: 0.3665037751197815, acc: 90.625, f1: 81.29176053684388, r: 0.6825369311773707
06/02/2019 12:29:02 step: 4998, epoch: 151, batch: 14, loss: 0.38941192626953125, acc: 84.375, f1: 82.19025139560152, r: 0.6578787231323278
06/02/2019 12:29:03 step: 5003, epoch: 151, batch: 19, loss: 0.2836664319038391, acc: 90.625, f1: 86.04755234982674, r: 0.7223987034703252
06/02/2019 12:29:03 step: 5008, epoch: 151, batch: 24, loss: 0.4072117805480957, acc: 84.375, f1: 57.60416666666668, r: 0.5412879051202153
06/02/2019 12:29:05 step: 5013, epoch: 151, batch: 29, loss: 0.41158345341682434, acc: 90.625, f1: 83.32442067736186, r: 0.7733042688781054
06/02/2019 12:29:05 *** evaluating ***
06/02/2019 12:29:05 step: 152, epoch: 151, acc: 56.837606837606835, f1: 26.362474163703542, r: 0.32186684525987086
06/02/2019 12:29:05 *** epoch: 153 ***
06/02/2019 12:29:05 *** training ***
06/02/2019 12:29:07 step: 5021, epoch: 152, batch: 4, loss: 0.3554609417915344, acc: 87.5, f1: 82.70192307692308, r: 0.7193699824754638
06/02/2019 12:29:08 step: 5026, epoch: 152, batch: 9, loss: 0.43778255581855774, acc: 81.25, f1: 80.49856425823918, r: 0.7117410309063111
06/02/2019 12:29:09 step: 5031, epoch: 152, batch: 14, loss: 0.2928713262081146, acc: 93.75, f1: 92.64381562652957, r: 0.7410386044442784
06/02/2019 12:29:10 step: 5036, epoch: 152, batch: 19, loss: 0.35039862990379333, acc: 90.625, f1: 85.95182595182594, r: 0.5287535723997009
06/02/2019 12:29:11 step: 5041, epoch: 152, batch: 24, loss: 0.2241445928812027, acc: 93.75, f1: 78.08547342381928, r: 0.5341908201424647
06/02/2019 12:29:12 step: 5046, epoch: 152, batch: 29, loss: 0.38986721634864807, acc: 90.625, f1: 88.04855275443512, r: 0.6529253023786689
06/02/2019 12:29:12 *** evaluating ***
06/02/2019 12:29:13 step: 153, epoch: 152, acc: 57.692307692307686, f1: 28.754989468409764, r: 0.3164180440182928
06/02/2019 12:29:13 *** epoch: 154 ***
06/02/2019 12:29:13 *** training ***
06/02/2019 12:29:14 step: 5054, epoch: 153, batch: 4, loss: 0.3319704532623291, acc: 85.9375, f1: 68.3720689435453, r: 0.7162956910809115
06/02/2019 12:29:15 step: 5059, epoch: 153, batch: 9, loss: 0.19149377942085266, acc: 98.4375, f1: 95.45454545454545, r: 0.7353839563891879
06/02/2019 12:29:16 step: 5064, epoch: 153, batch: 14, loss: 0.3431745767593384, acc: 87.5, f1: 78.86551663439536, r: 0.7139995204153682
06/02/2019 12:29:17 step: 5069, epoch: 153, batch: 19, loss: 0.2895504832267761, acc: 93.75, f1: 94.43920572181442, r: 0.8063854555811228
06/02/2019 12:29:18 step: 5074, epoch: 153, batch: 24, loss: 0.3854399025440216, acc: 84.375, f1: 82.97524113313587, r: 0.5999408651243756
06/02/2019 12:29:19 step: 5079, epoch: 153, batch: 29, loss: 0.3166298568248749, acc: 92.1875, f1: 90.27994227994228, r: 0.6042909507407631
06/02/2019 12:29:20 *** evaluating ***
06/02/2019 12:29:20 step: 154, epoch: 153, acc: 58.119658119658126, f1: 27.072577793493636, r: 0.31939863747329683
06/02/2019 12:29:20 *** epoch: 155 ***
06/02/2019 12:29:20 *** training ***
06/02/2019 12:29:21 step: 5087, epoch: 154, batch: 4, loss: 0.33814847469329834, acc: 85.9375, f1: 80.72601794340926, r: 0.6345396751300134
06/02/2019 12:29:22 step: 5092, epoch: 154, batch: 9, loss: 0.36814889311790466, acc: 87.5, f1: 87.02024635204359, r: 0.6254855971894262
06/02/2019 12:29:23 step: 5097, epoch: 154, batch: 14, loss: 0.307002454996109, acc: 92.1875, f1: 87.82922031047968, r: 0.616081707740217
06/02/2019 12:29:24 step: 5102, epoch: 154, batch: 19, loss: 0.32520702481269836, acc: 90.625, f1: 85.80409086711607, r: 0.6866996802543162
06/02/2019 12:29:25 step: 5107, epoch: 154, batch: 24, loss: 0.2927083671092987, acc: 90.625, f1: 82.73001508295626, r: 0.7414923861956125
06/02/2019 12:29:26 step: 5112, epoch: 154, batch: 29, loss: 0.3009331226348877, acc: 90.625, f1: 88.92105431514298, r: 0.6757429396156874
06/02/2019 12:29:26 *** evaluating ***
06/02/2019 12:29:26 step: 155, epoch: 154, acc: 57.26495726495726, f1: 27.770535474370018, r: 0.31927217335739183
06/02/2019 12:29:26 *** epoch: 156 ***
06/02/2019 12:29:26 *** training ***
06/02/2019 12:29:27 step: 5120, epoch: 155, batch: 4, loss: 0.2502552568912506, acc: 92.1875, f1: 92.09054251071058, r: 0.5983526964769379
06/02/2019 12:29:28 step: 5125, epoch: 155, batch: 9, loss: 0.2817228138446808, acc: 87.5, f1: 80.83654325993388, r: 0.5955170984685748
06/02/2019 12:29:29 step: 5130, epoch: 155, batch: 14, loss: 0.3541960120201111, acc: 85.9375, f1: 69.58728877615076, r: 0.6371999358886982
06/02/2019 12:29:30 step: 5135, epoch: 155, batch: 19, loss: 0.37154263257980347, acc: 82.8125, f1: 62.31094371467042, r: 0.6293545294636609
06/02/2019 12:29:31 step: 5140, epoch: 155, batch: 24, loss: 0.31265687942504883, acc: 89.0625, f1: 84.75170068027211, r: 0.6428461265957123
06/02/2019 12:29:32 step: 5145, epoch: 155, batch: 29, loss: 0.3893393576145172, acc: 84.375, f1: 78.03419635697102, r: 0.7053294562004389
06/02/2019 12:29:33 *** evaluating ***
06/02/2019 12:29:33 step: 156, epoch: 155, acc: 57.26495726495726, f1: 26.507039122703024, r: 0.32353551432283123
06/02/2019 12:29:33 *** epoch: 157 ***
06/02/2019 12:29:33 *** training ***
06/02/2019 12:29:34 step: 5153, epoch: 156, batch: 4, loss: 0.340516060590744, acc: 92.1875, f1: 87.8046473578345, r: 0.6506914480139232
06/02/2019 12:29:35 step: 5158, epoch: 156, batch: 9, loss: 0.4240933656692505, acc: 92.1875, f1: 92.05648926237161, r: 0.6066935413258878
06/02/2019 12:29:36 step: 5163, epoch: 156, batch: 14, loss: 0.308771550655365, acc: 89.0625, f1: 80.12674362674362, r: 0.7160479673553302
06/02/2019 12:29:37 step: 5168, epoch: 156, batch: 19, loss: 0.6007556319236755, acc: 89.0625, f1: 81.73476702508961, r: 0.7399665689619983
06/02/2019 12:29:38 step: 5173, epoch: 156, batch: 24, loss: 0.31849080324172974, acc: 92.1875, f1: 77.83075654504226, r: 0.6613811469687344
06/02/2019 12:29:39 step: 5178, epoch: 156, batch: 29, loss: 0.32829660177230835, acc: 87.5, f1: 86.33838383838383, r: 0.717342772647042
06/02/2019 12:29:40 *** evaluating ***
06/02/2019 12:29:40 step: 157, epoch: 156, acc: 57.26495726495726, f1: 27.120633713766352, r: 0.31797508652372347
06/02/2019 12:29:40 *** epoch: 158 ***
06/02/2019 12:29:40 *** training ***
06/02/2019 12:29:41 step: 5186, epoch: 157, batch: 4, loss: 0.2896770238876343, acc: 93.75, f1: 92.74489502444636, r: 0.7101117274037555
06/02/2019 12:29:42 step: 5191, epoch: 157, batch: 9, loss: 0.3548673093318939, acc: 87.5, f1: 75.70330543206909, r: 0.6149217554700905
06/02/2019 12:29:43 step: 5196, epoch: 157, batch: 14, loss: 0.45830202102661133, acc: 90.625, f1: 80.95663786696396, r: 0.6976259298390449
06/02/2019 12:29:44 step: 5201, epoch: 157, batch: 19, loss: 0.3665786385536194, acc: 85.9375, f1: 81.06832570426545, r: 0.7433400660646238
06/02/2019 12:29:46 step: 5206, epoch: 157, batch: 24, loss: 0.6886182427406311, acc: 85.9375, f1: 82.35449735449735, r: 0.7086654707601958
06/02/2019 12:29:47 step: 5211, epoch: 157, batch: 29, loss: 0.3222915232181549, acc: 87.5, f1: 87.02752448497128, r: 0.6733790605817921
06/02/2019 12:29:47 *** evaluating ***
06/02/2019 12:29:47 step: 158, epoch: 157, acc: 58.119658119658126, f1: 27.48245380241649, r: 0.3202838249531894
06/02/2019 12:29:47 *** epoch: 159 ***
06/02/2019 12:29:47 *** training ***
06/02/2019 12:29:49 step: 5219, epoch: 158, batch: 4, loss: 0.32057085633277893, acc: 92.1875, f1: 86.76470588235294, r: 0.6773862493805806
06/02/2019 12:29:50 step: 5224, epoch: 158, batch: 9, loss: 0.42931097745895386, acc: 89.0625, f1: 85.8049886621315, r: 0.6141118905302831
06/02/2019 12:29:51 step: 5229, epoch: 158, batch: 14, loss: 0.26819491386413574, acc: 93.75, f1: 91.94749563993513, r: 0.7215097283070263
06/02/2019 12:29:51 step: 5234, epoch: 158, batch: 19, loss: 0.33526650071144104, acc: 92.1875, f1: 75.42798937756922, r: 0.6347326835525932
06/02/2019 12:29:53 step: 5239, epoch: 158, batch: 24, loss: 0.272522896528244, acc: 89.0625, f1: 85.65082785987958, r: 0.6810560122747314
06/02/2019 12:29:53 step: 5244, epoch: 158, batch: 29, loss: 0.22826729714870453, acc: 95.3125, f1: 83.20136581764488, r: 0.6959887264425962
06/02/2019 12:29:54 *** evaluating ***
06/02/2019 12:29:54 step: 159, epoch: 158, acc: 57.692307692307686, f1: 28.864706649572362, r: 0.3174046195458813
06/02/2019 12:29:54 *** epoch: 160 ***
06/02/2019 12:29:54 *** training ***
06/02/2019 12:29:55 step: 5252, epoch: 159, batch: 4, loss: 0.3951041102409363, acc: 84.375, f1: 72.23428837244626, r: 0.6302190890086886
06/02/2019 12:29:56 step: 5257, epoch: 159, batch: 9, loss: 0.37077999114990234, acc: 87.5, f1: 82.46791898577612, r: 0.7135905785484358
06/02/2019 12:29:57 step: 5262, epoch: 159, batch: 14, loss: 0.2869725823402405, acc: 87.5, f1: 85.96542346542347, r: 0.5906735344853739
06/02/2019 12:29:58 step: 5267, epoch: 159, batch: 19, loss: 0.20688177645206451, acc: 96.875, f1: 96.65969907631789, r: 0.7271009276645042
06/02/2019 12:29:59 step: 5272, epoch: 159, batch: 24, loss: 0.2432752549648285, acc: 92.1875, f1: 87.89285714285714, r: 0.7116456931317219
06/02/2019 12:30:00 step: 5277, epoch: 159, batch: 29, loss: 0.348254919052124, acc: 87.5, f1: 76.63917399804497, r: 0.6849147483270737
06/02/2019 12:30:01 *** evaluating ***
06/02/2019 12:30:01 step: 160, epoch: 159, acc: 56.837606837606835, f1: 26.4175434270371, r: 0.3174171216054327
06/02/2019 12:30:01 *** epoch: 161 ***
06/02/2019 12:30:01 *** training ***
06/02/2019 12:30:02 step: 5285, epoch: 160, batch: 4, loss: 0.2342761754989624, acc: 93.75, f1: 89.75093984962406, r: 0.7264547054790054
06/02/2019 12:30:03 step: 5290, epoch: 160, batch: 9, loss: 0.37517666816711426, acc: 85.9375, f1: 81.06720430107526, r: 0.7014459666495316
06/02/2019 12:30:04 step: 5295, epoch: 160, batch: 14, loss: 0.2893725633621216, acc: 89.0625, f1: 88.41711650922177, r: 0.7198726921474086
06/02/2019 12:30:05 step: 5300, epoch: 160, batch: 19, loss: 0.3098101019859314, acc: 90.625, f1: 76.57077795334139, r: 0.5792479351736752
06/02/2019 12:30:06 step: 5305, epoch: 160, batch: 24, loss: 0.30101311206817627, acc: 92.1875, f1: 79.6154483298696, r: 0.7165821654621166
06/02/2019 12:30:07 step: 5310, epoch: 160, batch: 29, loss: 0.27780985832214355, acc: 90.625, f1: 75.19969796285586, r: 0.6601133528767558
06/02/2019 12:30:07 *** evaluating ***
06/02/2019 12:30:08 step: 161, epoch: 160, acc: 56.837606837606835, f1: 27.162872219053728, r: 0.316957629271216
06/02/2019 12:30:08 *** epoch: 162 ***
06/02/2019 12:30:08 *** training ***
06/02/2019 12:30:09 step: 5318, epoch: 161, batch: 4, loss: 0.38441452383995056, acc: 85.9375, f1: 80.89155097775789, r: 0.5801074313394453
06/02/2019 12:30:10 step: 5323, epoch: 161, batch: 9, loss: 0.21654009819030762, acc: 96.875, f1: 93.99092970521542, r: 0.7228812799056683
06/02/2019 12:30:10 step: 5328, epoch: 161, batch: 14, loss: 0.30797871947288513, acc: 93.75, f1: 89.31908972669842, r: 0.7400029343232776
06/02/2019 12:30:11 step: 5333, epoch: 161, batch: 19, loss: 0.40310540795326233, acc: 84.375, f1: 83.14572814572814, r: 0.6019031926502733
06/02/2019 12:30:13 step: 5338, epoch: 161, batch: 24, loss: 0.329651802778244, acc: 90.625, f1: 86.51939324116744, r: 0.7076539160043347
06/02/2019 12:30:14 step: 5343, epoch: 161, batch: 29, loss: 0.40936577320098877, acc: 85.9375, f1: 71.98660223905023, r: 0.6083780693063732
06/02/2019 12:30:14 *** evaluating ***
06/02/2019 12:30:15 step: 162, epoch: 161, acc: 56.837606837606835, f1: 26.299339683117196, r: 0.32235638712152836
06/02/2019 12:30:15 *** epoch: 163 ***
06/02/2019 12:30:15 *** training ***
06/02/2019 12:30:16 step: 5351, epoch: 162, batch: 4, loss: 0.24817414581775665, acc: 93.75, f1: 78.67800807043285, r: 0.6742841706104509
06/02/2019 12:30:16 step: 5356, epoch: 162, batch: 9, loss: 0.22270159423351288, acc: 93.75, f1: 89.76675498414629, r: 0.6830784533409971
06/02/2019 12:30:17 step: 5361, epoch: 162, batch: 14, loss: 0.28272587060928345, acc: 87.5, f1: 75.21507187780773, r: 0.6840885134635494
06/02/2019 12:30:18 step: 5366, epoch: 162, batch: 19, loss: 0.35819655656814575, acc: 89.0625, f1: 80.25676937441644, r: 0.7315757315588218
06/02/2019 12:30:20 step: 5371, epoch: 162, batch: 24, loss: 0.38870716094970703, acc: 92.1875, f1: 78.19727891156462, r: 0.598564767205686
06/02/2019 12:30:20 step: 5376, epoch: 162, batch: 29, loss: 0.29300805926322937, acc: 92.1875, f1: 79.44594843085976, r: 0.73993377800921
06/02/2019 12:30:21 *** evaluating ***
06/02/2019 12:30:21 step: 163, epoch: 162, acc: 57.692307692307686, f1: 26.60649525953509, r: 0.3264686460592351
06/02/2019 12:30:21 *** epoch: 164 ***
06/02/2019 12:30:21 *** training ***
06/02/2019 12:30:22 step: 5384, epoch: 163, batch: 4, loss: 0.2624726891517639, acc: 93.75, f1: 94.73905723905725, r: 0.7629597539121482
06/02/2019 12:30:23 step: 5389, epoch: 163, batch: 9, loss: 0.32520538568496704, acc: 89.0625, f1: 87.73299497437428, r: 0.7520498486302765
06/02/2019 12:30:24 step: 5394, epoch: 163, batch: 14, loss: 0.2765703797340393, acc: 93.75, f1: 88.50388382303277, r: 0.785696218237476
06/02/2019 12:30:25 step: 5399, epoch: 163, batch: 19, loss: 0.34996917843818665, acc: 90.625, f1: 85.66502463054188, r: 0.5803392066763855
06/02/2019 12:30:26 step: 5404, epoch: 163, batch: 24, loss: 0.21189314126968384, acc: 92.1875, f1: 92.95902268953557, r: 0.6652407910672987
06/02/2019 12:30:27 step: 5409, epoch: 163, batch: 29, loss: 0.30869731307029724, acc: 93.75, f1: 92.18044559314271, r: 0.7182681399890598
06/02/2019 12:30:28 *** evaluating ***
06/02/2019 12:30:28 step: 164, epoch: 163, acc: 57.692307692307686, f1: 28.099440503521368, r: 0.32206500722297315
06/02/2019 12:30:28 *** epoch: 165 ***
06/02/2019 12:30:28 *** training ***
06/02/2019 12:30:29 step: 5417, epoch: 164, batch: 4, loss: 0.24692147970199585, acc: 92.1875, f1: 89.72277013037882, r: 0.7699774959210631
06/02/2019 12:30:30 step: 5422, epoch: 164, batch: 9, loss: 0.351094126701355, acc: 87.5, f1: 86.32105692732871, r: 0.6630043958425005
06/02/2019 12:30:31 step: 5427, epoch: 164, batch: 14, loss: 0.2203470766544342, acc: 93.75, f1: 79.30871212121211, r: 0.7416619619817408
06/02/2019 12:30:32 step: 5432, epoch: 164, batch: 19, loss: 0.24486610293388367, acc: 90.625, f1: 87.8525641025641, r: 0.7703904403144733
06/02/2019 12:30:33 step: 5437, epoch: 164, batch: 24, loss: 0.23148690164089203, acc: 93.75, f1: 87.6589462764578, r: 0.6798658869211945
06/02/2019 12:30:34 step: 5442, epoch: 164, batch: 29, loss: 0.35546737909317017, acc: 92.1875, f1: 86.68137254901961, r: 0.7388940617407209
06/02/2019 12:30:35 *** evaluating ***
06/02/2019 12:30:35 step: 165, epoch: 164, acc: 56.837606837606835, f1: 25.461382872373584, r: 0.31695248442695156
06/02/2019 12:30:35 *** epoch: 166 ***
06/02/2019 12:30:35 *** training ***
06/02/2019 12:30:36 step: 5450, epoch: 165, batch: 4, loss: 0.41113701462745667, acc: 90.625, f1: 85.10730574280994, r: 0.7219552742110064
06/02/2019 12:30:37 step: 5455, epoch: 165, batch: 9, loss: 0.3186587691307068, acc: 87.5, f1: 82.71803593232164, r: 0.6209257989786128
06/02/2019 12:30:38 step: 5460, epoch: 165, batch: 14, loss: 0.5523836016654968, acc: 93.75, f1: 94.238843862904, r: 0.6304986526234234
06/02/2019 12:30:39 step: 5465, epoch: 165, batch: 19, loss: 0.3143301010131836, acc: 89.0625, f1: 74.82458513708514, r: 0.7332630900751427
06/02/2019 12:30:40 step: 5470, epoch: 165, batch: 24, loss: 0.30947431921958923, acc: 89.0625, f1: 88.03910181958963, r: 0.7715623146349092
06/02/2019 12:30:41 step: 5475, epoch: 165, batch: 29, loss: 0.29153376817703247, acc: 92.1875, f1: 88.81817256817257, r: 0.6366473150873507
06/02/2019 12:30:42 *** evaluating ***
06/02/2019 12:30:42 step: 166, epoch: 165, acc: 55.12820512820513, f1: 28.061806793424438, r: 0.3137754178107655
06/02/2019 12:30:42 *** epoch: 167 ***
06/02/2019 12:30:42 *** training ***
06/02/2019 12:30:43 step: 5483, epoch: 166, batch: 4, loss: 0.31219255924224854, acc: 90.625, f1: 81.84162995222904, r: 0.5930737975830647
06/02/2019 12:30:44 step: 5488, epoch: 166, batch: 9, loss: 0.5563293099403381, acc: 95.3125, f1: 83.14814814814815, r: 0.6774283464032944
06/02/2019 12:30:45 step: 5493, epoch: 166, batch: 14, loss: 0.2894528806209564, acc: 90.625, f1: 88.6640159604361, r: 0.7239979962599805
06/02/2019 12:30:46 step: 5498, epoch: 166, batch: 19, loss: 0.3495064377784729, acc: 87.5, f1: 72.76738612264928, r: 0.5944901349273138
06/02/2019 12:30:47 step: 5503, epoch: 166, batch: 24, loss: 0.22622810304164886, acc: 96.875, f1: 85.14957264957266, r: 0.6486976234478965
06/02/2019 12:30:48 step: 5508, epoch: 166, batch: 29, loss: 0.305193156003952, acc: 87.5, f1: 84.41609977324262, r: 0.674061688450407
06/02/2019 12:30:49 *** evaluating ***
06/02/2019 12:30:49 step: 167, epoch: 166, acc: 57.692307692307686, f1: 28.27950744342269, r: 0.32254933916384243
06/02/2019 12:30:49 *** epoch: 168 ***
06/02/2019 12:30:49 *** training ***
06/02/2019 12:30:50 step: 5516, epoch: 167, batch: 4, loss: 0.38183292746543884, acc: 84.375, f1: 82.2229609929078, r: 0.712851508491846
06/02/2019 12:30:51 step: 5521, epoch: 167, batch: 9, loss: 0.31191688776016235, acc: 90.625, f1: 71.83160508741902, r: 0.602628331917182
06/02/2019 12:30:52 step: 5526, epoch: 167, batch: 14, loss: 0.5937809944152832, acc: 92.1875, f1: 73.03030303030303, r: 0.5890604676064867
06/02/2019 12:30:53 step: 5531, epoch: 167, batch: 19, loss: 0.28420352935791016, acc: 93.75, f1: 93.87560429577235, r: 0.6635023868677812
06/02/2019 12:30:55 step: 5536, epoch: 167, batch: 24, loss: 0.21791915595531464, acc: 95.3125, f1: 92.74645342007773, r: 0.5910271679601281
06/02/2019 12:30:55 step: 5541, epoch: 167, batch: 29, loss: 0.3227408826351166, acc: 85.9375, f1: 74.20787545787546, r: 0.615895038169594
06/02/2019 12:30:56 *** evaluating ***
06/02/2019 12:30:56 step: 168, epoch: 167, acc: 55.98290598290598, f1: 25.933100531819086, r: 0.3197705642504501
06/02/2019 12:30:56 *** epoch: 169 ***
06/02/2019 12:30:56 *** training ***
06/02/2019 12:30:57 step: 5549, epoch: 168, batch: 4, loss: 0.5504918098449707, acc: 92.1875, f1: 65.61887254901961, r: 0.7197232688871944
06/02/2019 12:30:59 step: 5554, epoch: 168, batch: 9, loss: 0.32627272605895996, acc: 87.5, f1: 66.55762783582333, r: 0.6134578646799824
06/02/2019 12:30:59 step: 5559, epoch: 168, batch: 14, loss: 0.6529051065444946, acc: 87.5, f1: 83.14532272239039, r: 0.7504082826760026
06/02/2019 12:31:00 step: 5564, epoch: 168, batch: 19, loss: 0.24620464444160461, acc: 90.625, f1: 87.94286680189317, r: 0.7063056908748021
06/02/2019 12:31:02 step: 5569, epoch: 168, batch: 24, loss: 0.39787641167640686, acc: 85.9375, f1: 84.43142606186083, r: 0.5084512699362089
06/02/2019 12:31:03 step: 5574, epoch: 168, batch: 29, loss: 0.23395496606826782, acc: 98.4375, f1: 84.76190476190476, r: 0.637906598038872
06/02/2019 12:31:03 *** evaluating ***
06/02/2019 12:31:03 step: 169, epoch: 168, acc: 57.692307692307686, f1: 27.242788584813137, r: 0.31929574764017393
06/02/2019 12:31:03 *** epoch: 170 ***
06/02/2019 12:31:03 *** training ***
06/02/2019 12:31:04 step: 5582, epoch: 169, batch: 4, loss: 0.19584251940250397, acc: 98.4375, f1: 99.13702623906705, r: 0.6219014555635636
06/02/2019 12:31:05 step: 5587, epoch: 169, batch: 9, loss: 0.2027847021818161, acc: 95.3125, f1: 80.61351578674949, r: 0.7098104440494942
06/02/2019 12:31:06 step: 5592, epoch: 169, batch: 14, loss: 0.22452101111412048, acc: 93.75, f1: 93.7401609571421, r: 0.6212946310674398
06/02/2019 12:31:08 step: 5597, epoch: 169, batch: 19, loss: 0.25866279006004333, acc: 90.625, f1: 74.32228718993426, r: 0.6553257772145074
06/02/2019 12:31:09 step: 5602, epoch: 169, batch: 24, loss: 0.29801762104034424, acc: 90.625, f1: 77.37330464017806, r: 0.6686418931745407
06/02/2019 12:31:10 step: 5607, epoch: 169, batch: 29, loss: 0.2875814139842987, acc: 87.5, f1: 81.92290249433105, r: 0.599712275982131
06/02/2019 12:31:10 *** evaluating ***
06/02/2019 12:31:11 step: 170, epoch: 169, acc: 57.26495726495726, f1: 26.480648168206017, r: 0.3205090345838721
06/02/2019 12:31:11 *** epoch: 171 ***
06/02/2019 12:31:11 *** training ***
06/02/2019 12:31:12 step: 5615, epoch: 170, batch: 4, loss: 0.34154704213142395, acc: 89.0625, f1: 63.75120136030953, r: 0.5674519532913114
06/02/2019 12:31:13 step: 5620, epoch: 170, batch: 9, loss: 0.6012175679206848, acc: 90.625, f1: 88.78068475452197, r: 0.7554074614187399
06/02/2019 12:31:14 step: 5625, epoch: 170, batch: 14, loss: 0.22403258085250854, acc: 95.3125, f1: 92.28166160081052, r: 0.7440758179046945
06/02/2019 12:31:15 step: 5630, epoch: 170, batch: 19, loss: 0.4649382531642914, acc: 89.0625, f1: 84.32792207792208, r: 0.7314220545287827
06/02/2019 12:31:16 step: 5635, epoch: 170, batch: 24, loss: 0.2949524521827698, acc: 85.9375, f1: 77.6072261072261, r: 0.700793589640776
06/02/2019 12:31:17 step: 5640, epoch: 170, batch: 29, loss: 0.32109346985816956, acc: 92.1875, f1: 87.91702578467284, r: 0.7249788802275821
06/02/2019 12:31:17 *** evaluating ***
06/02/2019 12:31:18 step: 171, epoch: 170, acc: 57.692307692307686, f1: 27.159491117401867, r: 0.3206138994110157
06/02/2019 12:31:18 *** epoch: 172 ***
06/02/2019 12:31:18 *** training ***
06/02/2019 12:31:19 step: 5648, epoch: 171, batch: 4, loss: 0.2447415590286255, acc: 90.625, f1: 90.26459068475874, r: 0.7252964275968392
06/02/2019 12:31:20 step: 5653, epoch: 171, batch: 9, loss: 0.2834082245826721, acc: 93.75, f1: 79.80808702791462, r: 0.6494207423372049
06/02/2019 12:31:21 step: 5658, epoch: 171, batch: 14, loss: 0.4542529284954071, acc: 96.875, f1: 86.5079365079365, r: 0.5613212086690821
06/02/2019 12:31:22 step: 5663, epoch: 171, batch: 19, loss: 0.2563830614089966, acc: 87.5, f1: 81.97677203065135, r: 0.709066099586837
06/02/2019 12:31:23 step: 5668, epoch: 171, batch: 24, loss: 0.31661033630371094, acc: 89.0625, f1: 86.94715007215008, r: 0.7355932005184678
06/02/2019 12:31:24 step: 5673, epoch: 171, batch: 29, loss: 0.24371200799942017, acc: 95.3125, f1: 90.3164009393153, r: 0.7088865312608527
06/02/2019 12:31:24 *** evaluating ***
06/02/2019 12:31:25 step: 172, epoch: 171, acc: 56.837606837606835, f1: 25.456837783014063, r: 0.3195790927032054
06/02/2019 12:31:25 *** epoch: 173 ***
06/02/2019 12:31:25 *** training ***
06/02/2019 12:31:26 step: 5681, epoch: 172, batch: 4, loss: 0.3391914963722229, acc: 87.5, f1: 78.10648191389447, r: 0.7823741932788635
06/02/2019 12:31:27 step: 5686, epoch: 172, batch: 9, loss: 0.22598379850387573, acc: 96.875, f1: 95.55555555555556, r: 0.6569535358204217
06/02/2019 12:31:28 step: 5691, epoch: 172, batch: 14, loss: 0.1932341307401657, acc: 95.3125, f1: 93.88888888888889, r: 0.8432266251009763
06/02/2019 12:31:29 step: 5696, epoch: 172, batch: 19, loss: 0.25963398814201355, acc: 90.625, f1: 82.1082907577109, r: 0.5525888679845796
06/02/2019 12:31:30 step: 5701, epoch: 172, batch: 24, loss: 0.19131489098072052, acc: 95.3125, f1: 95.56935817805383, r: 0.6556243369363283
06/02/2019 12:31:31 step: 5706, epoch: 172, batch: 29, loss: 0.290470689535141, acc: 92.1875, f1: 90.8611111111111, r: 0.7272892343030526
06/02/2019 12:31:32 *** evaluating ***
06/02/2019 12:31:32 step: 173, epoch: 172, acc: 57.26495726495726, f1: 25.83261340203535, r: 0.3173190707876898
06/02/2019 12:31:32 *** epoch: 174 ***
06/02/2019 12:31:32 *** training ***
06/02/2019 12:31:33 step: 5714, epoch: 173, batch: 4, loss: 0.2738572955131531, acc: 92.1875, f1: 82.36547004388217, r: 0.7260593146113252
06/02/2019 12:31:34 step: 5719, epoch: 173, batch: 9, loss: 0.3102298676967621, acc: 95.3125, f1: 94.88272144522145, r: 0.7420515463522457
06/02/2019 12:31:35 step: 5724, epoch: 173, batch: 14, loss: 0.3015415370464325, acc: 89.0625, f1: 83.19018516497508, r: 0.677509215173818
06/02/2019 12:31:36 step: 5729, epoch: 173, batch: 19, loss: 0.2528560161590576, acc: 92.1875, f1: 87.97080498866214, r: 0.7456709418779339
06/02/2019 12:31:37 step: 5734, epoch: 173, batch: 24, loss: 0.2823217511177063, acc: 87.5, f1: 74.55437701396347, r: 0.7058293334664296
06/02/2019 12:31:38 step: 5739, epoch: 173, batch: 29, loss: 0.3231615126132965, acc: 89.0625, f1: 83.97817460317461, r: 0.6571265027608202
06/02/2019 12:31:39 *** evaluating ***
06/02/2019 12:31:39 step: 174, epoch: 173, acc: 57.692307692307686, f1: 26.86792374292374, r: 0.32036165063869815
06/02/2019 12:31:39 *** epoch: 175 ***
06/02/2019 12:31:39 *** training ***
06/02/2019 12:31:40 step: 5747, epoch: 174, batch: 4, loss: 0.3517504036426544, acc: 85.9375, f1: 71.45254255548373, r: 0.7075227955457305
06/02/2019 12:31:41 step: 5752, epoch: 174, batch: 9, loss: 0.2604143023490906, acc: 90.625, f1: 86.84012066365008, r: 0.6041989439700207
06/02/2019 12:31:42 step: 5757, epoch: 174, batch: 14, loss: 0.262005090713501, acc: 93.75, f1: 89.15323447238342, r: 0.7566828535536592
06/02/2019 12:31:43 step: 5762, epoch: 174, batch: 19, loss: 0.29918646812438965, acc: 87.5, f1: 84.70406405189014, r: 0.7087613325400871
06/02/2019 12:31:44 step: 5767, epoch: 174, batch: 24, loss: 0.28574925661087036, acc: 93.75, f1: 91.25776397515529, r: 0.6998128275275043
06/02/2019 12:31:45 step: 5772, epoch: 174, batch: 29, loss: 0.3584029972553253, acc: 87.5, f1: 81.14140517867226, r: 0.6552757604620261
06/02/2019 12:31:46 *** evaluating ***
06/02/2019 12:31:46 step: 175, epoch: 174, acc: 55.98290598290598, f1: 25.914251790610486, r: 0.3180110148103127
06/02/2019 12:31:46 *** epoch: 176 ***
06/02/2019 12:31:46 *** training ***
06/02/2019 12:31:47 step: 5780, epoch: 175, batch: 4, loss: 0.2886463701725006, acc: 90.625, f1: 80.75083542188806, r: 0.7194694476326389
06/02/2019 12:31:48 step: 5785, epoch: 175, batch: 9, loss: 0.30109772086143494, acc: 93.75, f1: 88.99301825993557, r: 0.6342501342960956
06/02/2019 12:31:49 step: 5790, epoch: 175, batch: 14, loss: 0.289139062166214, acc: 90.625, f1: 84.97231424621131, r: 0.6755802007586602
06/02/2019 12:31:50 step: 5795, epoch: 175, batch: 19, loss: 0.2761504352092743, acc: 85.9375, f1: 66.96386809687303, r: 0.5618992681163228
06/02/2019 12:31:51 step: 5800, epoch: 175, batch: 24, loss: 0.22524511814117432, acc: 95.3125, f1: 91.26068376068376, r: 0.8091599359056293
06/02/2019 12:31:52 step: 5805, epoch: 175, batch: 29, loss: 0.3269037902355194, acc: 87.5, f1: 83.3221470889444, r: 0.7453957747368659
06/02/2019 12:31:53 *** evaluating ***
06/02/2019 12:31:53 step: 176, epoch: 175, acc: 56.837606837606835, f1: 27.16584069525246, r: 0.3184640176366219
06/02/2019 12:31:53 *** epoch: 177 ***
06/02/2019 12:31:53 *** training ***
06/02/2019 12:31:54 step: 5813, epoch: 176, batch: 4, loss: 0.2381110042333603, acc: 95.3125, f1: 83.6140642303433, r: 0.7677539211013672
06/02/2019 12:31:55 step: 5818, epoch: 176, batch: 9, loss: 0.3332984149456024, acc: 85.9375, f1: 73.73866213151928, r: 0.6741061253637803
06/02/2019 12:31:56 step: 5823, epoch: 176, batch: 14, loss: 0.33059245347976685, acc: 85.9375, f1: 68.56829573934837, r: 0.6518865880043218
06/02/2019 12:31:57 step: 5828, epoch: 176, batch: 19, loss: 0.7096747756004333, acc: 82.8125, f1: 69.61538461538461, r: 0.7178013762657189
06/02/2019 12:31:58 step: 5833, epoch: 176, batch: 24, loss: 0.2094993144273758, acc: 93.75, f1: 77.62541116578385, r: 0.5941696370361471
06/02/2019 12:31:59 step: 5838, epoch: 176, batch: 29, loss: 0.30180099606513977, acc: 89.0625, f1: 85.44351664417454, r: 0.6945813436562389
06/02/2019 12:32:00 *** evaluating ***
06/02/2019 12:32:00 step: 177, epoch: 176, acc: 58.119658119658126, f1: 27.89419934640523, r: 0.31800193974443697
06/02/2019 12:32:00 *** epoch: 178 ***
06/02/2019 12:32:00 *** training ***
06/02/2019 12:32:01 step: 5846, epoch: 177, batch: 4, loss: 0.24683484435081482, acc: 95.3125, f1: 92.41220997400596, r: 0.7328334015439786
06/02/2019 12:32:02 step: 5851, epoch: 177, batch: 9, loss: 0.2428063154220581, acc: 96.875, f1: 91.75983436853002, r: 0.7220306134281104
06/02/2019 12:32:03 step: 5856, epoch: 177, batch: 14, loss: 0.34407979249954224, acc: 87.5, f1: 80.07034632034632, r: 0.7389050537052859
06/02/2019 12:32:04 step: 5861, epoch: 177, batch: 19, loss: 0.20349717140197754, acc: 100.0, f1: 100.0, r: 0.6401299002375335
06/02/2019 12:32:05 step: 5866, epoch: 177, batch: 24, loss: 0.23881854116916656, acc: 90.625, f1: 89.89766081871345, r: 0.7035237388741777
06/02/2019 12:32:06 step: 5871, epoch: 177, batch: 29, loss: 0.6897339224815369, acc: 85.9375, f1: 84.46579497599906, r: 0.6511942935740873
06/02/2019 12:32:06 *** evaluating ***
06/02/2019 12:32:07 step: 178, epoch: 177, acc: 57.692307692307686, f1: 27.042702268995374, r: 0.32414085747830257
06/02/2019 12:32:07 *** epoch: 179 ***
06/02/2019 12:32:07 *** training ***
06/02/2019 12:32:08 step: 5879, epoch: 178, batch: 4, loss: 0.5894187092781067, acc: 93.75, f1: 90.08441798674873, r: 0.7148932286065686
06/02/2019 12:32:09 step: 5884, epoch: 178, batch: 9, loss: 0.1964615136384964, acc: 93.75, f1: 89.92793727660556, r: 0.6283171023238623
06/02/2019 12:32:10 step: 5889, epoch: 178, batch: 14, loss: 0.2513570785522461, acc: 90.625, f1: 67.16178816024016, r: 0.6394290989898436
06/02/2019 12:32:11 step: 5894, epoch: 178, batch: 19, loss: 0.28396302461624146, acc: 92.1875, f1: 84.61569043201695, r: 0.5804166524434622
06/02/2019 12:32:12 step: 5899, epoch: 178, batch: 24, loss: 0.24148455262184143, acc: 89.0625, f1: 70.56725486230492, r: 0.6263599150457835
06/02/2019 12:32:13 step: 5904, epoch: 178, batch: 29, loss: 0.3824906349182129, acc: 89.0625, f1: 87.32658572048106, r: 0.749890842603374
06/02/2019 12:32:13 *** evaluating ***
06/02/2019 12:32:14 step: 179, epoch: 178, acc: 56.41025641025641, f1: 26.22825332085621, r: 0.3212831094888681
06/02/2019 12:32:14 *** epoch: 180 ***
06/02/2019 12:32:14 *** training ***
06/02/2019 12:32:15 step: 5912, epoch: 179, batch: 4, loss: 0.2515319287776947, acc: 90.625, f1: 89.77771132376395, r: 0.6973497139066658
06/02/2019 12:32:16 step: 5917, epoch: 179, batch: 9, loss: 0.3645908534526825, acc: 89.0625, f1: 86.2360446570973, r: 0.7275372271937272
06/02/2019 12:32:17 step: 5922, epoch: 179, batch: 14, loss: 0.19839246571063995, acc: 95.3125, f1: 90.17572723772184, r: 0.6298258032198903
06/02/2019 12:32:18 step: 5927, epoch: 179, batch: 19, loss: 0.23608873784542084, acc: 93.75, f1: 89.66638631903561, r: 0.7022422305695216
06/02/2019 12:32:19 step: 5932, epoch: 179, batch: 24, loss: 0.3395760953426361, acc: 93.75, f1: 93.18840579710145, r: 0.6706893653053383
06/02/2019 12:32:20 step: 5937, epoch: 179, batch: 29, loss: 0.2715839445590973, acc: 93.75, f1: 95.24233935998642, r: 0.6772154172926317
06/02/2019 12:32:20 *** evaluating ***
06/02/2019 12:32:20 step: 180, epoch: 179, acc: 56.837606837606835, f1: 26.835980154773775, r: 0.32205195491860555
06/02/2019 12:32:20 *** epoch: 181 ***
06/02/2019 12:32:20 *** training ***
06/02/2019 12:32:21 step: 5945, epoch: 180, batch: 4, loss: 0.48482224345207214, acc: 96.875, f1: 95.33070371005546, r: 0.6029680863686323
06/02/2019 12:32:22 step: 5950, epoch: 180, batch: 9, loss: 0.29543638229370117, acc: 90.625, f1: 87.35147205735441, r: 0.7305741851843345
06/02/2019 12:32:23 step: 5955, epoch: 180, batch: 14, loss: 0.1807461380958557, acc: 93.75, f1: 83.17460317460318, r: 0.636888403914895
06/02/2019 12:32:24 step: 5960, epoch: 180, batch: 19, loss: 0.3554058074951172, acc: 84.375, f1: 73.1934731934732, r: 0.7369896241390302
06/02/2019 12:32:25 step: 5965, epoch: 180, batch: 24, loss: 0.3002511262893677, acc: 92.1875, f1: 76.43123425038318, r: 0.6326258439815139
06/02/2019 12:32:26 step: 5970, epoch: 180, batch: 29, loss: 0.20584538578987122, acc: 95.3125, f1: 95.03538257408226, r: 0.7393291199369496
06/02/2019 12:32:26 *** evaluating ***
06/02/2019 12:32:27 step: 181, epoch: 180, acc: 56.837606837606835, f1: 26.31474432797663, r: 0.3246299227729453
06/02/2019 12:32:27 *** epoch: 182 ***
06/02/2019 12:32:27 *** training ***
06/02/2019 12:32:28 step: 5978, epoch: 181, batch: 4, loss: 0.21152667701244354, acc: 93.75, f1: 66.36363636363636, r: 0.6689682654275816
06/02/2019 12:32:29 step: 5983, epoch: 181, batch: 9, loss: 0.2666623592376709, acc: 92.1875, f1: 80.4749430136427, r: 0.6428257363316222
06/02/2019 12:32:30 step: 5988, epoch: 181, batch: 14, loss: 0.39533182978630066, acc: 85.9375, f1: 79.04761904761905, r: 0.614038719665045
06/02/2019 12:32:31 step: 5993, epoch: 181, batch: 19, loss: 0.2682565152645111, acc: 87.5, f1: 86.81544509130717, r: 0.6572833105953679
06/02/2019 12:32:32 step: 5998, epoch: 181, batch: 24, loss: 0.25045812129974365, acc: 93.75, f1: 92.3663836163836, r: 0.7522418532465545
06/02/2019 12:32:33 step: 6003, epoch: 181, batch: 29, loss: 0.21254189312458038, acc: 96.875, f1: 98.23871409028729, r: 0.7823572048981191
06/02/2019 12:32:33 *** evaluating ***
06/02/2019 12:32:34 step: 182, epoch: 181, acc: 56.837606837606835, f1: 27.000195841048285, r: 0.32169610894691136
06/02/2019 12:32:34 *** epoch: 183 ***
06/02/2019 12:32:34 *** training ***
06/02/2019 12:32:35 step: 6011, epoch: 182, batch: 4, loss: 0.27757734060287476, acc: 87.5, f1: 76.18009714783909, r: 0.6304120599593539
06/02/2019 12:32:36 step: 6016, epoch: 182, batch: 9, loss: 0.18815620243549347, acc: 93.75, f1: 93.42464184574379, r: 0.7556512587147987
06/02/2019 12:32:37 step: 6021, epoch: 182, batch: 14, loss: 0.22889523208141327, acc: 92.1875, f1: 87.79927110140244, r: 0.6629418068580528
06/02/2019 12:32:38 step: 6026, epoch: 182, batch: 19, loss: 0.22635479271411896, acc: 93.75, f1: 94.17687513432193, r: 0.7783705151081638
06/02/2019 12:32:38 step: 6031, epoch: 182, batch: 24, loss: 0.34217357635498047, acc: 87.5, f1: 83.31168831168833, r: 0.6479615664987213
06/02/2019 12:32:39 step: 6036, epoch: 182, batch: 29, loss: 0.11443591862916946, acc: 98.4375, f1: 99.14280148061563, r: 0.5270588790108465
06/02/2019 12:32:40 *** evaluating ***
06/02/2019 12:32:40 step: 183, epoch: 182, acc: 58.119658119658126, f1: 29.104098177374038, r: 0.3161141430871939
06/02/2019 12:32:40 *** epoch: 184 ***
06/02/2019 12:32:40 *** training ***
06/02/2019 12:32:41 step: 6044, epoch: 183, batch: 4, loss: 0.23512929677963257, acc: 95.3125, f1: 83.15026697177727, r: 0.6911944926989757
06/02/2019 12:32:42 step: 6049, epoch: 183, batch: 9, loss: 0.4732356667518616, acc: 79.6875, f1: 69.85847485847485, r: 0.6101808131509554
06/02/2019 12:32:43 step: 6054, epoch: 183, batch: 14, loss: 0.32695093750953674, acc: 89.0625, f1: 85.87027914614121, r: 0.6267063599096325
06/02/2019 12:32:44 step: 6059, epoch: 183, batch: 19, loss: 0.2938220500946045, acc: 90.625, f1: 78.18429189857761, r: 0.6984087901922429
06/02/2019 12:32:45 step: 6064, epoch: 183, batch: 24, loss: 0.3405580222606659, acc: 90.625, f1: 80.50178598571212, r: 0.6748764841237032
06/02/2019 12:32:46 step: 6069, epoch: 183, batch: 29, loss: 0.2996971905231476, acc: 84.375, f1: 75.85603054353054, r: 0.7340713186052107
06/02/2019 12:32:47 *** evaluating ***
06/02/2019 12:32:47 step: 184, epoch: 183, acc: 56.41025641025641, f1: 26.562648838602044, r: 0.31799483059338945
06/02/2019 12:32:47 *** epoch: 185 ***
06/02/2019 12:32:47 *** training ***
06/02/2019 12:32:48 step: 6077, epoch: 184, batch: 4, loss: 0.2937450408935547, acc: 89.0625, f1: 81.21385518042372, r: 0.6579830373905763
06/02/2019 12:32:49 step: 6082, epoch: 184, batch: 9, loss: 0.2080608755350113, acc: 92.1875, f1: 87.37324079429342, r: 0.7315262655051565
06/02/2019 12:32:50 step: 6087, epoch: 184, batch: 14, loss: 0.269185334444046, acc: 90.625, f1: 84.42572180387306, r: 0.7032689142882518
06/02/2019 12:32:51 step: 6092, epoch: 184, batch: 19, loss: 0.2805476486682892, acc: 93.75, f1: 91.53598785951728, r: 0.6102800280669637
06/02/2019 12:32:52 step: 6097, epoch: 184, batch: 24, loss: 0.2625832259654999, acc: 87.5, f1: 81.35416666666666, r: 0.6778394530786851
06/02/2019 12:32:53 step: 6102, epoch: 184, batch: 29, loss: 0.19250504672527313, acc: 98.4375, f1: 98.14471243042672, r: 0.5425010130481436
06/02/2019 12:32:53 *** evaluating ***
06/02/2019 12:32:54 step: 185, epoch: 184, acc: 57.26495726495726, f1: 27.22944172010562, r: 0.31555818037297534
06/02/2019 12:32:54 *** epoch: 186 ***
06/02/2019 12:32:54 *** training ***
06/02/2019 12:32:55 step: 6110, epoch: 185, batch: 4, loss: 0.30599069595336914, acc: 90.625, f1: 78.5453216374269, r: 0.6751288459695928
06/02/2019 12:32:56 step: 6115, epoch: 185, batch: 9, loss: 0.20597873628139496, acc: 95.3125, f1: 95.95298569355172, r: 0.6139452399592985
06/02/2019 12:32:57 step: 6120, epoch: 185, batch: 14, loss: 0.23317690193653107, acc: 93.75, f1: 90.3294260030503, r: 0.6216118541499248
06/02/2019 12:32:58 step: 6125, epoch: 185, batch: 19, loss: 0.23307986557483673, acc: 90.625, f1: 91.16005022698734, r: 0.6245725489042945
06/02/2019 12:32:59 step: 6130, epoch: 185, batch: 24, loss: 0.2739173173904419, acc: 92.1875, f1: 82.01786201786202, r: 0.7684326738682037
06/02/2019 12:33:00 step: 6135, epoch: 185, batch: 29, loss: 0.22456151247024536, acc: 93.75, f1: 89.37985644185106, r: 0.7035186637806679
06/02/2019 12:33:00 *** evaluating ***
06/02/2019 12:33:01 step: 186, epoch: 185, acc: 57.692307692307686, f1: 26.515749228838004, r: 0.30865658012764097
06/02/2019 12:33:01 *** epoch: 187 ***
06/02/2019 12:33:01 *** training ***
06/02/2019 12:33:02 step: 6143, epoch: 186, batch: 4, loss: 0.3336156904697418, acc: 92.1875, f1: 90.48340548340548, r: 0.7452000217606224
06/02/2019 12:33:03 step: 6148, epoch: 186, batch: 9, loss: 0.2441941648721695, acc: 90.625, f1: 85.28822055137846, r: 0.7292816668428439
06/02/2019 12:33:04 step: 6153, epoch: 186, batch: 14, loss: 0.24918091297149658, acc: 95.3125, f1: 83.43024916113373, r: 0.6666293880880015
06/02/2019 12:33:05 step: 6158, epoch: 186, batch: 19, loss: 0.2381870299577713, acc: 93.75, f1: 90.75104789390504, r: 0.6247816015392492
06/02/2019 12:33:06 step: 6163, epoch: 186, batch: 24, loss: 0.23302055895328522, acc: 93.75, f1: 96.62581699346406, r: 0.6519097326855549
06/02/2019 12:33:07 step: 6168, epoch: 186, batch: 29, loss: 0.14139744639396667, acc: 96.875, f1: 87.56613756613757, r: 0.6147812566516921
06/02/2019 12:33:07 *** evaluating ***
06/02/2019 12:33:08 step: 187, epoch: 186, acc: 58.119658119658126, f1: 27.397219503098725, r: 0.3171322424604927
06/02/2019 12:33:08 *** epoch: 188 ***
06/02/2019 12:33:08 *** training ***
06/02/2019 12:33:09 step: 6176, epoch: 187, batch: 4, loss: 0.2948545813560486, acc: 92.1875, f1: 78.44594426628882, r: 0.6253504071825683
06/02/2019 12:33:10 step: 6181, epoch: 187, batch: 9, loss: 0.2611333727836609, acc: 89.0625, f1: 84.01833420834124, r: 0.5621020086984151
06/02/2019 12:33:11 step: 6186, epoch: 187, batch: 14, loss: 0.20650064945220947, acc: 96.875, f1: 94.72474873171738, r: 0.69848335900361
06/02/2019 12:33:12 step: 6191, epoch: 187, batch: 19, loss: 0.23533006012439728, acc: 95.3125, f1: 90.76964118980926, r: 0.6830465281243667
06/02/2019 12:33:13 step: 6196, epoch: 187, batch: 24, loss: 0.17864929139614105, acc: 95.3125, f1: 94.33823529411765, r: 0.8000968025030996
06/02/2019 12:33:14 step: 6201, epoch: 187, batch: 29, loss: 0.32060644030570984, acc: 89.0625, f1: 88.83057985098802, r: 0.6718383900139292
06/02/2019 12:33:15 *** evaluating ***
06/02/2019 12:33:15 step: 188, epoch: 187, acc: 57.692307692307686, f1: 26.932138812843682, r: 0.3192027292388745
06/02/2019 12:33:15 *** epoch: 189 ***
06/02/2019 12:33:15 *** training ***
06/02/2019 12:33:16 step: 6209, epoch: 188, batch: 4, loss: 0.2228759080171585, acc: 93.75, f1: 91.48904006046862, r: 0.6095418402291751
06/02/2019 12:33:17 step: 6214, epoch: 188, batch: 9, loss: 0.20722438395023346, acc: 95.3125, f1: 95.7209145021645, r: 0.7535152413434945
06/02/2019 12:33:18 step: 6219, epoch: 188, batch: 14, loss: 0.3306451141834259, acc: 85.9375, f1: 76.68122672409925, r: 0.6589967768574831
06/02/2019 12:33:19 step: 6224, epoch: 188, batch: 19, loss: 0.3036250174045563, acc: 90.625, f1: 82.77517825311944, r: 0.6934628271550312
06/02/2019 12:33:20 step: 6229, epoch: 188, batch: 24, loss: 0.24571144580841064, acc: 93.75, f1: 87.90586932447397, r: 0.6522824974217731
06/02/2019 12:33:21 step: 6234, epoch: 188, batch: 29, loss: 0.25057119131088257, acc: 95.3125, f1: 93.92290249433107, r: 0.7220046925918732
06/02/2019 12:33:22 *** evaluating ***
06/02/2019 12:33:22 step: 189, epoch: 188, acc: 57.692307692307686, f1: 26.357227880268923, r: 0.3188193291152284
06/02/2019 12:33:22 *** epoch: 190 ***
06/02/2019 12:33:22 *** training ***
06/02/2019 12:33:23 step: 6242, epoch: 189, batch: 4, loss: 0.23072665929794312, acc: 89.0625, f1: 88.76079734219269, r: 0.7668181661566518
06/02/2019 12:33:24 step: 6247, epoch: 189, batch: 9, loss: 0.19096435606479645, acc: 92.1875, f1: 85.51190476190477, r: 0.766645826187242
06/02/2019 12:33:25 step: 6252, epoch: 189, batch: 14, loss: 0.5617020130157471, acc: 92.1875, f1: 76.22474747474747, r: 0.6460268280844331
06/02/2019 12:33:26 step: 6257, epoch: 189, batch: 19, loss: 0.22515493631362915, acc: 93.75, f1: 79.65665743291001, r: 0.6550513121718594
06/02/2019 12:33:27 step: 6262, epoch: 189, batch: 24, loss: 0.2382768839597702, acc: 98.4375, f1: 98.38383838383838, r: 0.6050618393583544
06/02/2019 12:33:28 step: 6267, epoch: 189, batch: 29, loss: 0.28157922625541687, acc: 90.625, f1: 86.28031412514171, r: 0.752628959193865
06/02/2019 12:33:28 *** evaluating ***
06/02/2019 12:33:29 step: 190, epoch: 189, acc: 56.41025641025641, f1: 27.3671086170444, r: 0.3161277510205868
06/02/2019 12:33:29 *** epoch: 191 ***
06/02/2019 12:33:29 *** training ***
06/02/2019 12:33:30 step: 6275, epoch: 190, batch: 4, loss: 0.30983129143714905, acc: 92.1875, f1: 82.36306984981428, r: 0.605782836692293
06/02/2019 12:33:31 step: 6280, epoch: 190, batch: 9, loss: 0.24085913598537445, acc: 95.3125, f1: 87.86951144094, r: 0.6522166385894731
06/02/2019 12:33:32 step: 6285, epoch: 190, batch: 14, loss: 0.5994164347648621, acc: 90.625, f1: 84.66407496012759, r: 0.711630997534385
06/02/2019 12:33:33 step: 6290, epoch: 190, batch: 19, loss: 0.34419959783554077, acc: 89.0625, f1: 86.03174603174602, r: 0.627509400573421
06/02/2019 12:33:34 step: 6295, epoch: 190, batch: 24, loss: 0.1747131645679474, acc: 96.875, f1: 91.28205128205128, r: 0.6460926786957488
06/02/2019 12:33:35 step: 6300, epoch: 190, batch: 29, loss: 0.191054105758667, acc: 93.75, f1: 88.33649939699781, r: 0.6305587841652059
06/02/2019 12:33:35 *** evaluating ***
06/02/2019 12:33:36 step: 191, epoch: 190, acc: 57.692307692307686, f1: 25.94712353507663, r: 0.31049915662971067
06/02/2019 12:33:36 *** epoch: 192 ***
06/02/2019 12:33:36 *** training ***
06/02/2019 12:33:36 step: 6308, epoch: 191, batch: 4, loss: 0.23235633969306946, acc: 95.3125, f1: 95.8695652173913, r: 0.72936588188276
06/02/2019 12:33:37 step: 6313, epoch: 191, batch: 9, loss: 0.21354158222675323, acc: 92.1875, f1: 76.61495911495912, r: 0.6732744621938132
06/02/2019 12:33:38 step: 6318, epoch: 191, batch: 14, loss: 0.3192104995250702, acc: 90.625, f1: 88.27131673269194, r: 0.7280641029319448
06/02/2019 12:33:39 step: 6323, epoch: 191, batch: 19, loss: 0.21825386583805084, acc: 96.875, f1: 97.51900760304123, r: 0.6571129322705803
06/02/2019 12:33:40 step: 6328, epoch: 191, batch: 24, loss: 0.2022106796503067, acc: 95.3125, f1: 89.36063218390804, r: 0.7006940970999133
06/02/2019 12:33:41 step: 6333, epoch: 191, batch: 29, loss: 0.3562851548194885, acc: 87.5, f1: 74.51418067226892, r: 0.6426920747120226
06/02/2019 12:33:42 *** evaluating ***
06/02/2019 12:33:42 step: 192, epoch: 191, acc: 55.98290598290598, f1: 24.396469137133035, r: 0.3150940349888976
06/02/2019 12:33:42 *** epoch: 193 ***
06/02/2019 12:33:42 *** training ***
06/02/2019 12:33:43 step: 6341, epoch: 192, batch: 4, loss: 0.20816092193126678, acc: 90.625, f1: 86.01682778374509, r: 0.6406278769755948
06/02/2019 12:33:44 step: 6346, epoch: 192, batch: 9, loss: 0.31034359335899353, acc: 87.5, f1: 76.58317834625868, r: 0.7225062154688586
06/02/2019 12:33:45 step: 6351, epoch: 192, batch: 14, loss: 0.2774934768676758, acc: 89.0625, f1: 75.13667285095858, r: 0.6433060212609576
06/02/2019 12:33:47 step: 6356, epoch: 192, batch: 19, loss: 0.4196237325668335, acc: 87.5, f1: 84.75198271116639, r: 0.6399446838427753
06/02/2019 12:33:47 step: 6361, epoch: 192, batch: 24, loss: 0.2497766762971878, acc: 90.625, f1: 78.82928475033738, r: 0.7638380842020395
06/02/2019 12:33:48 step: 6366, epoch: 192, batch: 29, loss: 0.23595136404037476, acc: 93.75, f1: 93.17795691150954, r: 0.711593408026244
06/02/2019 12:33:49 *** evaluating ***
06/02/2019 12:33:49 step: 193, epoch: 192, acc: 58.54700854700855, f1: 27.58662046908315, r: 0.31429051214979165
06/02/2019 12:33:49 *** epoch: 194 ***
06/02/2019 12:33:49 *** training ***
06/02/2019 12:33:50 step: 6374, epoch: 193, batch: 4, loss: 0.27969691157341003, acc: 92.1875, f1: 76.20229652144546, r: 0.6402311144402991
06/02/2019 12:33:51 step: 6379, epoch: 193, batch: 9, loss: 0.20767346024513245, acc: 93.75, f1: 84.87318840579711, r: 0.7244149054569141
06/02/2019 12:33:52 step: 6384, epoch: 193, batch: 14, loss: 0.5196390748023987, acc: 93.75, f1: 89.37334656084657, r: 0.7396458432841384
06/02/2019 12:33:53 step: 6389, epoch: 193, batch: 19, loss: 0.23372741043567657, acc: 95.3125, f1: 92.70165598290599, r: 0.7938296042407142
06/02/2019 12:33:54 step: 6394, epoch: 193, batch: 24, loss: 0.28585660457611084, acc: 90.625, f1: 68.20238095238096, r: 0.6774531743028869
06/02/2019 12:33:55 step: 6399, epoch: 193, batch: 29, loss: 0.22669439017772675, acc: 92.1875, f1: 89.39782611609238, r: 0.5666993675725538
06/02/2019 12:33:56 *** evaluating ***
06/02/2019 12:33:56 step: 194, epoch: 193, acc: 57.26495726495726, f1: 25.034677225647606, r: 0.31384830140557196
06/02/2019 12:33:56 *** epoch: 195 ***
06/02/2019 12:33:56 *** training ***
06/02/2019 12:33:57 step: 6407, epoch: 194, batch: 4, loss: 0.5867253541946411, acc: 89.0625, f1: 80.75549450549451, r: 0.694041306121897
06/02/2019 12:33:58 step: 6412, epoch: 194, batch: 9, loss: 0.2510463297367096, acc: 95.3125, f1: 91.36419136419136, r: 0.7542025118540068
06/02/2019 12:33:59 step: 6417, epoch: 194, batch: 14, loss: 0.18963021039962769, acc: 93.75, f1: 86.09446109446111, r: 0.6449756306978168
06/02/2019 12:34:00 step: 6422, epoch: 194, batch: 19, loss: 0.3269699811935425, acc: 93.75, f1: 82.81684131142335, r: 0.6776191717004196
06/02/2019 12:34:01 step: 6427, epoch: 194, batch: 24, loss: 0.25370657444000244, acc: 92.1875, f1: 77.47468218056453, r: 0.73350459986623
06/02/2019 12:34:02 step: 6432, epoch: 194, batch: 29, loss: 0.25855889916419983, acc: 96.875, f1: 84.80128893662729, r: 0.7463850903586697
06/02/2019 12:34:02 *** evaluating ***
06/02/2019 12:34:03 step: 195, epoch: 194, acc: 56.837606837606835, f1: 26.994642354549477, r: 0.3146657856476813
06/02/2019 12:34:03 *** epoch: 196 ***
06/02/2019 12:34:03 *** training ***
06/02/2019 12:34:04 step: 6440, epoch: 195, batch: 4, loss: 0.22513717412948608, acc: 95.3125, f1: 83.61733582321817, r: 0.6116535982800544
06/02/2019 12:34:05 step: 6445, epoch: 195, batch: 9, loss: 0.3784983158111572, acc: 85.9375, f1: 73.59620463394047, r: 0.6585442933523746
06/02/2019 12:34:06 step: 6450, epoch: 195, batch: 14, loss: 0.19502459466457367, acc: 93.75, f1: 85.84357923497268, r: 0.688450115461517
06/02/2019 12:34:07 step: 6455, epoch: 195, batch: 19, loss: 0.2785073518753052, acc: 90.625, f1: 87.07351539649676, r: 0.6502369545993822
06/02/2019 12:34:08 step: 6460, epoch: 195, batch: 24, loss: 0.24298521876335144, acc: 93.75, f1: 90.10227584439467, r: 0.6315998041976969
06/02/2019 12:34:09 step: 6465, epoch: 195, batch: 29, loss: 0.260711133480072, acc: 89.0625, f1: 74.2469545957918, r: 0.580892407653977
06/02/2019 12:34:10 *** evaluating ***
06/02/2019 12:34:10 step: 196, epoch: 195, acc: 57.692307692307686, f1: 27.104379795396422, r: 0.3129269286534323
06/02/2019 12:34:10 *** epoch: 197 ***
06/02/2019 12:34:10 *** training ***
06/02/2019 12:34:11 step: 6473, epoch: 196, batch: 4, loss: 0.35421767830848694, acc: 90.625, f1: 79.61128157556729, r: 0.7763566541195561
06/02/2019 12:34:12 step: 6478, epoch: 196, batch: 9, loss: 0.33435264229774475, acc: 90.625, f1: 83.37053571428572, r: 0.7344703143122228
06/02/2019 12:34:13 step: 6483, epoch: 196, batch: 14, loss: 0.1891854703426361, acc: 93.75, f1: 94.02872260015116, r: 0.6194176273628954
06/02/2019 12:34:14 step: 6488, epoch: 196, batch: 19, loss: 0.21979370713233948, acc: 93.75, f1: 69.38375350140056, r: 0.5838252992627768
06/02/2019 12:34:15 step: 6493, epoch: 196, batch: 24, loss: 0.34463006258010864, acc: 84.375, f1: 62.657976983897015, r: 0.6672226840002197
06/02/2019 12:34:16 step: 6498, epoch: 196, batch: 29, loss: 0.23971432447433472, acc: 92.1875, f1: 92.27513227513228, r: 0.6417874394456488
06/02/2019 12:34:17 *** evaluating ***
06/02/2019 12:34:17 step: 197, epoch: 196, acc: 58.97435897435898, f1: 28.492947340748596, r: 0.314260982869654
06/02/2019 12:34:17 *** epoch: 198 ***
06/02/2019 12:34:17 *** training ***
06/02/2019 12:34:18 step: 6506, epoch: 197, batch: 4, loss: 0.18534080684185028, acc: 95.3125, f1: 92.77919278996866, r: 0.7438566772555244
06/02/2019 12:34:19 step: 6511, epoch: 197, batch: 9, loss: 0.17500552535057068, acc: 93.75, f1: 92.9849832067574, r: 0.7546174381726309
06/02/2019 12:34:20 step: 6516, epoch: 197, batch: 14, loss: 0.24887555837631226, acc: 92.1875, f1: 88.76235553064822, r: 0.7357276474580898
06/02/2019 12:34:21 step: 6521, epoch: 197, batch: 19, loss: 0.21971149742603302, acc: 96.875, f1: 93.20915032679738, r: 0.8129543648141433
06/02/2019 12:34:22 step: 6526, epoch: 197, batch: 24, loss: 0.28977450728416443, acc: 87.5, f1: 71.17889971663556, r: 0.7206094668282759
06/02/2019 12:34:23 step: 6531, epoch: 197, batch: 29, loss: 0.25987809896469116, acc: 92.1875, f1: 77.96262254901961, r: 0.6841836347953962
06/02/2019 12:34:24 *** evaluating ***
06/02/2019 12:34:24 step: 198, epoch: 197, acc: 57.692307692307686, f1: 24.853751951600312, r: 0.3135092445491483
06/02/2019 12:34:24 *** epoch: 199 ***
06/02/2019 12:34:24 *** training ***
06/02/2019 12:34:25 step: 6539, epoch: 198, batch: 4, loss: 0.6245752573013306, acc: 85.9375, f1: 68.6019536019536, r: 0.572843488628243
06/02/2019 12:34:26 step: 6544, epoch: 198, batch: 9, loss: 0.3377050757408142, acc: 85.9375, f1: 71.29754668665885, r: 0.6438401313774363
06/02/2019 12:34:27 step: 6549, epoch: 198, batch: 14, loss: 0.331527978181839, acc: 89.0625, f1: 75.06087036614326, r: 0.7319046721117093
06/02/2019 12:34:28 step: 6554, epoch: 198, batch: 19, loss: 0.230503648519516, acc: 92.1875, f1: 86.99306645735217, r: 0.6084630851785575
06/02/2019 12:34:29 step: 6559, epoch: 198, batch: 24, loss: 0.2655858099460602, acc: 90.625, f1: 81.07611199716462, r: 0.7299244976312105
06/02/2019 12:34:30 step: 6564, epoch: 198, batch: 29, loss: 0.251510351896286, acc: 93.75, f1: 89.9464061284338, r: 0.7418376660615436
06/02/2019 12:34:31 *** evaluating ***
06/02/2019 12:34:31 step: 199, epoch: 198, acc: 58.119658119658126, f1: 26.469898053849228, r: 0.31363152295103625
06/02/2019 12:34:31 *** epoch: 200 ***
06/02/2019 12:34:31 *** training ***
06/02/2019 12:34:32 step: 6572, epoch: 199, batch: 4, loss: 0.33810704946517944, acc: 89.0625, f1: 86.96653243237094, r: 0.6201520388903007
06/02/2019 12:34:33 step: 6577, epoch: 199, batch: 9, loss: 0.22802020609378815, acc: 92.1875, f1: 90.22204647204647, r: 0.7407999199625174
06/02/2019 12:34:34 step: 6582, epoch: 199, batch: 14, loss: 0.31890687346458435, acc: 90.625, f1: 89.16125541125541, r: 0.7866136330184385
06/02/2019 12:34:35 step: 6587, epoch: 199, batch: 19, loss: 0.1697918027639389, acc: 95.3125, f1: 94.360045412677, r: 0.6436398103882363
06/02/2019 12:34:36 step: 6592, epoch: 199, batch: 24, loss: 0.1811372935771942, acc: 98.4375, f1: 99.09922589725547, r: 0.6166969805046344
06/02/2019 12:34:37 step: 6597, epoch: 199, batch: 29, loss: 0.27945685386657715, acc: 92.1875, f1: 83.94763711056214, r: 0.6024039105158187
06/02/2019 12:34:38 *** evaluating ***
06/02/2019 12:34:38 step: 200, epoch: 199, acc: 58.119658119658126, f1: 25.825633771602895, r: 0.3161398356509272
06/02/2019 12:34:38 *** epoch: 201 ***
06/02/2019 12:34:38 *** training ***
06/02/2019 12:34:39 step: 6605, epoch: 200, batch: 4, loss: 0.4424191117286682, acc: 84.375, f1: 68.14984627484628, r: 0.6158853679253646
06/02/2019 12:34:40 step: 6610, epoch: 200, batch: 9, loss: 0.3319816291332245, acc: 87.5, f1: 66.07369614512471, r: 0.6690719262773418
06/02/2019 12:34:41 step: 6615, epoch: 200, batch: 14, loss: 0.6287427544593811, acc: 85.9375, f1: 79.83846618357488, r: 0.6818335095088996
06/02/2019 12:34:42 step: 6620, epoch: 200, batch: 19, loss: 0.24930515885353088, acc: 90.625, f1: 86.5873015873016, r: 0.6014684604913862
06/02/2019 12:34:43 step: 6625, epoch: 200, batch: 24, loss: 0.24742507934570312, acc: 93.75, f1: 81.75151292866312, r: 0.6254293658723179
06/02/2019 12:34:44 step: 6630, epoch: 200, batch: 29, loss: 0.16483646631240845, acc: 96.875, f1: 95.10212950890917, r: 0.611788075170226
06/02/2019 12:34:45 *** evaluating ***
06/02/2019 12:34:45 step: 201, epoch: 200, acc: 58.119658119658126, f1: 26.872077235384275, r: 0.30995130216529176
06/02/2019 12:34:45 *** epoch: 202 ***
06/02/2019 12:34:45 *** training ***
06/02/2019 12:34:46 step: 6638, epoch: 201, batch: 4, loss: 0.2416536957025528, acc: 93.75, f1: 91.70068027210885, r: 0.6431556443248914
06/02/2019 12:34:47 step: 6643, epoch: 201, batch: 9, loss: 0.271062433719635, acc: 90.625, f1: 78.77859477124183, r: 0.697685147908102
06/02/2019 12:34:48 step: 6648, epoch: 201, batch: 14, loss: 0.24130669236183167, acc: 90.625, f1: 67.45376371964394, r: 0.7017290512347789
06/02/2019 12:34:49 step: 6653, epoch: 201, batch: 19, loss: 0.34041738510131836, acc: 96.875, f1: 93.48848311254326, r: 0.6151993052433478
06/02/2019 12:34:50 step: 6658, epoch: 201, batch: 24, loss: 0.23473909497261047, acc: 93.75, f1: 92.29075582802292, r: 0.6806541457921953
06/02/2019 12:34:51 step: 6663, epoch: 201, batch: 29, loss: 0.2899223268032074, acc: 92.1875, f1: 84.44475489229286, r: 0.6630734784917995
06/02/2019 12:34:52 *** evaluating ***
06/02/2019 12:34:52 step: 202, epoch: 201, acc: 56.837606837606835, f1: 26.466602980484588, r: 0.3160805203062764
06/02/2019 12:34:52 *** epoch: 203 ***
06/02/2019 12:34:52 *** training ***
06/02/2019 12:34:53 step: 6671, epoch: 202, batch: 4, loss: 0.27058279514312744, acc: 90.625, f1: 83.0203526632098, r: 0.6228406384409587
06/02/2019 12:34:54 step: 6676, epoch: 202, batch: 9, loss: 0.24418453872203827, acc: 93.75, f1: 93.91497391497393, r: 0.7109917448948814
06/02/2019 12:34:55 step: 6681, epoch: 202, batch: 14, loss: 0.2148137092590332, acc: 93.75, f1: 93.13852813852814, r: 0.7658835310112917
06/02/2019 12:34:56 step: 6686, epoch: 202, batch: 19, loss: 0.24321095645427704, acc: 93.75, f1: 93.23227041941216, r: 0.6677975741463601
06/02/2019 12:34:57 step: 6691, epoch: 202, batch: 24, loss: 0.30181029438972473, acc: 89.0625, f1: 78.38806305349351, r: 0.6413358733490431
06/02/2019 12:34:58 step: 6696, epoch: 202, batch: 29, loss: 0.2416328489780426, acc: 90.625, f1: 70.04029521870734, r: 0.6764388304612015
06/02/2019 12:34:59 *** evaluating ***
06/02/2019 12:34:59 step: 203, epoch: 202, acc: 56.837606837606835, f1: 28.003761826072072, r: 0.3168837147201527
06/02/2019 12:34:59 *** epoch: 204 ***
06/02/2019 12:34:59 *** training ***
06/02/2019 12:35:00 step: 6704, epoch: 203, batch: 4, loss: 0.16766034066677094, acc: 95.3125, f1: 84.28465490756926, r: 0.5903845502538826
06/02/2019 12:35:01 step: 6709, epoch: 203, batch: 9, loss: 0.17978741228580475, acc: 95.3125, f1: 94.156462585034, r: 0.6796955099788264
06/02/2019 12:35:02 step: 6714, epoch: 203, batch: 14, loss: 0.22241511940956116, acc: 95.3125, f1: 83.0982493880276, r: 0.6729590942866491
06/02/2019 12:35:03 step: 6719, epoch: 203, batch: 19, loss: 0.2057674527168274, acc: 93.75, f1: 82.40948114968272, r: 0.7835147840419132
06/02/2019 12:35:04 step: 6724, epoch: 203, batch: 24, loss: 0.3289616107940674, acc: 90.625, f1: 81.14169698887306, r: 0.5974882388361561
06/02/2019 12:35:05 step: 6729, epoch: 203, batch: 29, loss: 0.28546053171157837, acc: 93.75, f1: 91.02436513383665, r: 0.650160996771958
06/02/2019 12:35:06 *** evaluating ***
06/02/2019 12:35:06 step: 204, epoch: 203, acc: 57.692307692307686, f1: 26.714766603592167, r: 0.31820259761103603
06/02/2019 12:35:06 *** epoch: 205 ***
06/02/2019 12:35:06 *** training ***
06/02/2019 12:35:07 step: 6737, epoch: 204, batch: 4, loss: 0.250993013381958, acc: 92.1875, f1: 77.25451559934318, r: 0.6234841315296752
06/02/2019 12:35:08 step: 6742, epoch: 204, batch: 9, loss: 0.16944968700408936, acc: 98.4375, f1: 96.33699633699634, r: 0.5832045420101449
06/02/2019 12:35:09 step: 6747, epoch: 204, batch: 14, loss: 0.2131337672472, acc: 95.3125, f1: 90.81232492997198, r: 0.7480968593155034
06/02/2019 12:35:10 step: 6752, epoch: 204, batch: 19, loss: 0.27079033851623535, acc: 90.625, f1: 90.26327121154706, r: 0.7906675349240009
06/02/2019 12:35:11 step: 6757, epoch: 204, batch: 24, loss: 0.17335012555122375, acc: 95.3125, f1: 96.06604506604506, r: 0.6231608304363566
06/02/2019 12:35:12 step: 6762, epoch: 204, batch: 29, loss: 0.2599112093448639, acc: 93.75, f1: 94.9279711884754, r: 0.7745190054823989
06/02/2019 12:35:13 *** evaluating ***
06/02/2019 12:35:13 step: 205, epoch: 204, acc: 58.119658119658126, f1: 28.83277178748933, r: 0.3180644764151479
06/02/2019 12:35:13 *** epoch: 206 ***
06/02/2019 12:35:13 *** training ***
06/02/2019 12:35:14 step: 6770, epoch: 205, batch: 4, loss: 0.32724833488464355, acc: 89.0625, f1: 80.06282335550628, r: 0.5899212199902928
06/02/2019 12:35:15 step: 6775, epoch: 205, batch: 9, loss: 0.22009927034378052, acc: 92.1875, f1: 87.21891534391534, r: 0.7689884940234979
06/02/2019 12:35:16 step: 6780, epoch: 205, batch: 14, loss: 0.46555599570274353, acc: 96.875, f1: 94.23029623748812, r: 0.6820314859536662
06/02/2019 12:35:17 step: 6785, epoch: 205, batch: 19, loss: 0.20747296512126923, acc: 95.3125, f1: 89.91666666666667, r: 0.6753928894513805
06/02/2019 12:35:19 step: 6790, epoch: 205, batch: 24, loss: 0.2789801359176636, acc: 93.75, f1: 90.07936507936509, r: 0.6713390800351775
06/02/2019 12:35:20 step: 6795, epoch: 205, batch: 29, loss: 0.27385255694389343, acc: 93.75, f1: 89.88049834731757, r: 0.7420059886975923
06/02/2019 12:35:20 *** evaluating ***
06/02/2019 12:35:20 step: 206, epoch: 205, acc: 56.41025641025641, f1: 25.41905824582328, r: 0.32311054477061313
06/02/2019 12:35:20 *** epoch: 207 ***
06/02/2019 12:35:20 *** training ***
06/02/2019 12:35:21 step: 6803, epoch: 206, batch: 4, loss: 0.29447996616363525, acc: 87.5, f1: 77.51569858712716, r: 0.6855783260042447
06/02/2019 12:35:23 step: 6808, epoch: 206, batch: 9, loss: 0.25193965435028076, acc: 90.625, f1: 83.18542568542568, r: 0.64488468053819
06/02/2019 12:35:23 step: 6813, epoch: 206, batch: 14, loss: 0.3380909264087677, acc: 93.75, f1: 93.58359538784067, r: 0.8133663799612645
06/02/2019 12:35:24 step: 6818, epoch: 206, batch: 19, loss: 0.2097456306219101, acc: 95.3125, f1: 93.3358388987929, r: 0.6203136888338712
06/02/2019 12:35:25 step: 6823, epoch: 206, batch: 24, loss: 0.22926436364650726, acc: 93.75, f1: 92.63038548752836, r: 0.7092021861609185
06/02/2019 12:35:26 step: 6828, epoch: 206, batch: 29, loss: 0.2942265272140503, acc: 90.625, f1: 73.71955233706386, r: 0.7233206256825221
06/02/2019 12:35:27 *** evaluating ***
06/02/2019 12:35:27 step: 207, epoch: 206, acc: 58.119658119658126, f1: 27.61187952587901, r: 0.31869776459752797
06/02/2019 12:35:27 *** epoch: 208 ***
06/02/2019 12:35:27 *** training ***
06/02/2019 12:35:28 step: 6836, epoch: 207, batch: 4, loss: 0.17481578886508942, acc: 96.875, f1: 96.09523809523809, r: 0.6211702414754754
06/02/2019 12:35:29 step: 6841, epoch: 207, batch: 9, loss: 0.3083566427230835, acc: 87.5, f1: 72.95970077220076, r: 0.6548021369979143
06/02/2019 12:35:30 step: 6846, epoch: 207, batch: 14, loss: 0.18756605684757233, acc: 96.875, f1: 93.40472015808076, r: 0.6699293706306447
06/02/2019 12:35:31 step: 6851, epoch: 207, batch: 19, loss: 0.156415194272995, acc: 96.875, f1: 96.55633120678714, r: 0.7088609443054744
06/02/2019 12:35:32 step: 6856, epoch: 207, batch: 24, loss: 0.2525038421154022, acc: 95.3125, f1: 81.76884518989782, r: 0.6042384480517502
06/02/2019 12:35:33 step: 6861, epoch: 207, batch: 29, loss: 0.1497349590063095, acc: 96.875, f1: 96.09627302086061, r: 0.6026190988625303
06/02/2019 12:35:34 *** evaluating ***
06/02/2019 12:35:34 step: 208, epoch: 207, acc: 58.54700854700855, f1: 27.481302002193036, r: 0.3137040471404856
06/02/2019 12:35:34 *** epoch: 209 ***
06/02/2019 12:35:34 *** training ***
06/02/2019 12:35:35 step: 6869, epoch: 208, batch: 4, loss: 0.2996097803115845, acc: 85.9375, f1: 84.80283022299828, r: 0.5775011413502229
06/02/2019 12:35:36 step: 6874, epoch: 208, batch: 9, loss: 0.19424951076507568, acc: 95.3125, f1: 94.57658925639223, r: 0.6761389410487844
06/02/2019 12:35:37 step: 6879, epoch: 208, batch: 14, loss: 0.22212770581245422, acc: 93.75, f1: 91.80204625985026, r: 0.6820554086029208
06/02/2019 12:35:38 step: 6884, epoch: 208, batch: 19, loss: 0.262180894613266, acc: 89.0625, f1: 89.93248814677386, r: 0.6490489772226712
06/02/2019 12:35:39 step: 6889, epoch: 208, batch: 24, loss: 0.21163682639598846, acc: 92.1875, f1: 87.31743591087071, r: 0.7665505494266621
06/02/2019 12:35:40 step: 6894, epoch: 208, batch: 29, loss: 0.24506764113903046, acc: 92.1875, f1: 86.66106881624123, r: 0.6503585541323522
06/02/2019 12:35:41 *** evaluating ***
06/02/2019 12:35:41 step: 209, epoch: 208, acc: 58.54700854700855, f1: 27.418496813649796, r: 0.3140045093034567
06/02/2019 12:35:41 *** epoch: 210 ***
06/02/2019 12:35:41 *** training ***
06/02/2019 12:35:42 step: 6902, epoch: 209, batch: 4, loss: 0.17766457796096802, acc: 95.3125, f1: 94.11380315673067, r: 0.6239465360906666
06/02/2019 12:35:44 step: 6907, epoch: 209, batch: 9, loss: 0.24910767376422882, acc: 90.625, f1: 69.97474747474747, r: 0.610073039819524
06/02/2019 12:35:45 step: 6912, epoch: 209, batch: 14, loss: 0.26784709095954895, acc: 89.0625, f1: 89.7202380952381, r: 0.7477968532518153
06/02/2019 12:35:46 step: 6917, epoch: 209, batch: 19, loss: 0.3252413272857666, acc: 95.3125, f1: 88.92594436148443, r: 0.7258767468767002
06/02/2019 12:35:47 step: 6922, epoch: 209, batch: 24, loss: 0.24229830503463745, acc: 93.75, f1: 80.06566515495088, r: 0.6671175744779767
06/02/2019 12:35:48 step: 6927, epoch: 209, batch: 29, loss: 0.25536689162254333, acc: 89.0625, f1: 78.15777972027972, r: 0.7400021470825423
06/02/2019 12:35:48 *** evaluating ***
06/02/2019 12:35:49 step: 210, epoch: 209, acc: 57.26495726495726, f1: 26.35198913810777, r: 0.31068040249032647
06/02/2019 12:35:49 *** epoch: 211 ***
06/02/2019 12:35:49 *** training ***
06/02/2019 12:35:50 step: 6935, epoch: 210, batch: 4, loss: 0.12710990011692047, acc: 98.4375, f1: 94.93414387031407, r: 0.6768156931012328
06/02/2019 12:35:51 step: 6940, epoch: 210, batch: 9, loss: 0.3137354254722595, acc: 92.1875, f1: 76.5371324581851, r: 0.6743271006971175
06/02/2019 12:35:52 step: 6945, epoch: 210, batch: 14, loss: 0.28936567902565, acc: 87.5, f1: 76.25007550588946, r: 0.5124282188200417
06/02/2019 12:35:53 step: 6950, epoch: 210, batch: 19, loss: 0.16478964686393738, acc: 95.3125, f1: 92.46714456391875, r: 0.7601457567348324
06/02/2019 12:35:54 step: 6955, epoch: 210, batch: 24, loss: 0.34945306181907654, acc: 87.5, f1: 80.64323064323065, r: 0.6088883042382778
06/02/2019 12:35:55 step: 6960, epoch: 210, batch: 29, loss: 0.30511438846588135, acc: 92.1875, f1: 77.31871303299874, r: 0.7024672745747301
06/02/2019 12:35:55 *** evaluating ***
06/02/2019 12:35:56 step: 211, epoch: 210, acc: 58.119658119658126, f1: 27.57376301345126, r: 0.3168731991604467
06/02/2019 12:35:56 *** epoch: 212 ***
06/02/2019 12:35:56 *** training ***
06/02/2019 12:35:57 step: 6968, epoch: 211, batch: 4, loss: 0.215183824300766, acc: 93.75, f1: 88.30835978115942, r: 0.6601029543783409
06/02/2019 12:35:58 step: 6973, epoch: 211, batch: 9, loss: 0.20206300914287567, acc: 93.75, f1: 86.73290598290598, r: 0.8065432899990854
06/02/2019 12:35:59 step: 6978, epoch: 211, batch: 14, loss: 0.29911673069000244, acc: 89.0625, f1: 86.00146384629144, r: 0.6877648856155466
06/02/2019 12:36:00 step: 6983, epoch: 211, batch: 19, loss: 0.294013112783432, acc: 89.0625, f1: 72.30235042735043, r: 0.6643778541069051
06/02/2019 12:36:01 step: 6988, epoch: 211, batch: 24, loss: 0.2871851623058319, acc: 89.0625, f1: 79.85569985569985, r: 0.5543148929183487
06/02/2019 12:36:02 step: 6993, epoch: 211, batch: 29, loss: 0.2610454261302948, acc: 90.625, f1: 80.06827731092436, r: 0.6264352811021809
06/02/2019 12:36:03 *** evaluating ***
06/02/2019 12:36:03 step: 212, epoch: 211, acc: 57.26495726495726, f1: 26.363811653693737, r: 0.31559693029687946
06/02/2019 12:36:03 *** epoch: 213 ***
06/02/2019 12:36:03 *** training ***
06/02/2019 12:36:04 step: 7001, epoch: 212, batch: 4, loss: 0.1760447770357132, acc: 98.4375, f1: 99.27272727272727, r: 0.768841947927026
06/02/2019 12:36:05 step: 7006, epoch: 212, batch: 9, loss: 0.20077437162399292, acc: 95.3125, f1: 92.94832826747721, r: 0.6563216623430158
06/02/2019 12:36:06 step: 7011, epoch: 212, batch: 14, loss: 0.25310713052749634, acc: 90.625, f1: 77.06535850776169, r: 0.6461772944293016
06/02/2019 12:36:07 step: 7016, epoch: 212, batch: 19, loss: 0.18736757338047028, acc: 95.3125, f1: 89.98310632456973, r: 0.5798461130027736
06/02/2019 12:36:08 step: 7021, epoch: 212, batch: 24, loss: 0.30658096075057983, acc: 85.9375, f1: 73.68661274911274, r: 0.7160722592590101
06/02/2019 12:36:09 step: 7026, epoch: 212, batch: 29, loss: 0.219702810049057, acc: 92.1875, f1: 89.84583800075214, r: 0.6446820031362984
06/02/2019 12:36:10 *** evaluating ***
06/02/2019 12:36:10 step: 213, epoch: 212, acc: 58.54700854700855, f1: 26.7130070731441, r: 0.30798860978596654
06/02/2019 12:36:10 *** epoch: 214 ***
06/02/2019 12:36:10 *** training ***
06/02/2019 12:36:11 step: 7034, epoch: 213, batch: 4, loss: 0.17020435631275177, acc: 96.875, f1: 86.45982783357246, r: 0.6377874599216816
06/02/2019 12:36:12 step: 7039, epoch: 213, batch: 9, loss: 0.2421688288450241, acc: 89.0625, f1: 87.40599746209845, r: 0.6023105015029697
06/02/2019 12:36:14 step: 7044, epoch: 213, batch: 14, loss: 0.2161690592765808, acc: 93.75, f1: 93.60606060606061, r: 0.7320660974015358
06/02/2019 12:36:15 step: 7049, epoch: 213, batch: 19, loss: 0.145649254322052, acc: 96.875, f1: 88.92543859649122, r: 0.8273355336470057
06/02/2019 12:36:16 step: 7054, epoch: 213, batch: 24, loss: 0.1648254096508026, acc: 95.3125, f1: 96.71893147502904, r: 0.8253084138901828
06/02/2019 12:36:17 step: 7059, epoch: 213, batch: 29, loss: 0.23384879529476166, acc: 93.75, f1: 75.37632074106237, r: 0.6207408113116724
06/02/2019 12:36:17 *** evaluating ***
06/02/2019 12:36:18 step: 214, epoch: 213, acc: 58.54700854700855, f1: 27.300062403075387, r: 0.31490421992349266
06/02/2019 12:36:18 *** epoch: 215 ***
06/02/2019 12:36:18 *** training ***
06/02/2019 12:36:19 step: 7067, epoch: 214, batch: 4, loss: 0.16264857351779938, acc: 96.875, f1: 97.11397395928847, r: 0.6908199664094586
06/02/2019 12:36:20 step: 7072, epoch: 214, batch: 9, loss: 0.1714928150177002, acc: 95.3125, f1: 89.53601953601954, r: 0.6775338764894351
06/02/2019 12:36:21 step: 7077, epoch: 214, batch: 14, loss: 0.20970742404460907, acc: 93.75, f1: 78.81899350649351, r: 0.7301020363622814
06/02/2019 12:36:22 step: 7082, epoch: 214, batch: 19, loss: 0.27153515815734863, acc: 92.1875, f1: 88.91269841269842, r: 0.6413660414469236
06/02/2019 12:36:23 step: 7087, epoch: 214, batch: 24, loss: 0.18744099140167236, acc: 96.875, f1: 95.69264069264068, r: 0.6038663043670123
06/02/2019 12:36:24 step: 7092, epoch: 214, batch: 29, loss: 0.6040008664131165, acc: 96.875, f1: 95.07292456444999, r: 0.6458905395094172
06/02/2019 12:36:25 *** evaluating ***
06/02/2019 12:36:25 step: 215, epoch: 214, acc: 57.692307692307686, f1: 26.91966811505646, r: 0.31515679208972686
06/02/2019 12:36:25 *** epoch: 216 ***
06/02/2019 12:36:25 *** training ***
06/02/2019 12:36:27 step: 7100, epoch: 215, batch: 4, loss: 0.20578508079051971, acc: 93.75, f1: 92.49679487179488, r: 0.7115601366985161
06/02/2019 12:36:27 step: 7105, epoch: 215, batch: 9, loss: 0.2520400583744049, acc: 92.1875, f1: 91.7791304224231, r: 0.7770108619722799
06/02/2019 12:36:28 step: 7110, epoch: 215, batch: 14, loss: 0.36477339267730713, acc: 90.625, f1: 77.2311769005848, r: 0.655730210100442
06/02/2019 12:36:29 step: 7115, epoch: 215, batch: 19, loss: 0.2957046329975128, acc: 85.9375, f1: 70.06764069264068, r: 0.6783758336316411
06/02/2019 12:36:31 step: 7120, epoch: 215, batch: 24, loss: 0.2410804182291031, acc: 89.0625, f1: 81.37368443754313, r: 0.7407443169881168
06/02/2019 12:36:32 step: 7125, epoch: 215, batch: 29, loss: 0.24172848463058472, acc: 90.625, f1: 74.59548201073625, r: 0.5758494546441563
06/02/2019 12:36:32 *** evaluating ***
06/02/2019 12:36:33 step: 216, epoch: 215, acc: 58.119658119658126, f1: 26.468822750163152, r: 0.3136480246015046
06/02/2019 12:36:33 *** epoch: 217 ***
06/02/2019 12:36:33 *** training ***
06/02/2019 12:36:34 step: 7133, epoch: 216, batch: 4, loss: 0.2711198031902313, acc: 92.1875, f1: 85.54549315418882, r: 0.7053230012392849
06/02/2019 12:36:35 step: 7138, epoch: 216, batch: 9, loss: 0.16472163796424866, acc: 95.3125, f1: 83.59700722394221, r: 0.7916220896809105
06/02/2019 12:36:36 step: 7143, epoch: 216, batch: 14, loss: 0.21754249930381775, acc: 96.875, f1: 96.90485000660763, r: 0.7626081271812729
06/02/2019 12:36:37 step: 7148, epoch: 216, batch: 19, loss: 0.25739559531211853, acc: 90.625, f1: 90.54164836773532, r: 0.6500678639584642
06/02/2019 12:36:38 step: 7153, epoch: 216, batch: 24, loss: 0.17106923460960388, acc: 95.3125, f1: 93.24738986503692, r: 0.7436923771952897
06/02/2019 12:36:39 step: 7158, epoch: 216, batch: 29, loss: 0.3571447432041168, acc: 90.625, f1: 90.0826997465653, r: 0.7279285844409552
06/02/2019 12:36:39 *** evaluating ***
06/02/2019 12:36:40 step: 217, epoch: 216, acc: 56.41025641025641, f1: 26.559145024935233, r: 0.31505253811907824
06/02/2019 12:36:40 *** epoch: 218 ***
06/02/2019 12:36:40 *** training ***
06/02/2019 12:36:41 step: 7166, epoch: 217, batch: 4, loss: 0.1990572065114975, acc: 95.3125, f1: 84.11292989417989, r: 0.7572318490708898
06/02/2019 12:36:42 step: 7171, epoch: 217, batch: 9, loss: 0.30866488814353943, acc: 87.5, f1: 85.07837192047718, r: 0.7158478592212485
06/02/2019 12:36:43 step: 7176, epoch: 217, batch: 14, loss: 0.29309412837028503, acc: 85.9375, f1: 65.77365306215167, r: 0.5185996998795639
06/02/2019 12:36:44 step: 7181, epoch: 217, batch: 19, loss: 0.1844678521156311, acc: 95.3125, f1: 91.47303921568628, r: 0.7791735437871186
06/02/2019 12:36:45 step: 7186, epoch: 217, batch: 24, loss: 0.1162063255906105, acc: 100.0, f1: 100.0, r: 0.5506076702364457
06/02/2019 12:36:46 step: 7191, epoch: 217, batch: 29, loss: 0.14531266689300537, acc: 98.4375, f1: 95.71428571428571, r: 0.7261644733492764
06/02/2019 12:36:47 *** evaluating ***
06/02/2019 12:36:47 step: 218, epoch: 217, acc: 58.119658119658126, f1: 27.988467823026646, r: 0.31078409351814523
06/02/2019 12:36:47 *** epoch: 219 ***
06/02/2019 12:36:47 *** training ***
06/02/2019 12:36:48 step: 7199, epoch: 218, batch: 4, loss: 0.293781578540802, acc: 90.625, f1: 82.89808018068886, r: 0.7151829989650058
06/02/2019 12:36:49 step: 7204, epoch: 218, batch: 9, loss: 0.2966887950897217, acc: 89.0625, f1: 82.94464162476585, r: 0.6236403686853852
06/02/2019 12:36:50 step: 7209, epoch: 218, batch: 14, loss: 0.27931341528892517, acc: 90.625, f1: 86.41945516945516, r: 0.6284358042777729
06/02/2019 12:36:51 step: 7214, epoch: 218, batch: 19, loss: 0.21996918320655823, acc: 93.75, f1: 95.07049196490189, r: 0.6208669251871748
06/02/2019 12:36:52 step: 7219, epoch: 218, batch: 24, loss: 0.2393704205751419, acc: 93.75, f1: 89.59920634920636, r: 0.7461030466311755
06/02/2019 12:36:53 step: 7224, epoch: 218, batch: 29, loss: 0.28092142939567566, acc: 89.0625, f1: 85.58726851670544, r: 0.7338112923321323
06/02/2019 12:36:54 *** evaluating ***
06/02/2019 12:36:54 step: 219, epoch: 218, acc: 58.119658119658126, f1: 28.00485592024089, r: 0.31131784696169557
06/02/2019 12:36:54 *** epoch: 220 ***
06/02/2019 12:36:54 *** training ***
06/02/2019 12:36:55 step: 7232, epoch: 219, batch: 4, loss: 0.1837044060230255, acc: 95.3125, f1: 92.92200854700855, r: 0.8310543344316553
06/02/2019 12:36:56 step: 7237, epoch: 219, batch: 9, loss: 0.22949093580245972, acc: 96.875, f1: 96.04787369493253, r: 0.6459482149867827
06/02/2019 12:36:57 step: 7242, epoch: 219, batch: 14, loss: 0.17655232548713684, acc: 92.1875, f1: 91.77655677655677, r: 0.712188937804766
06/02/2019 12:36:58 step: 7247, epoch: 219, batch: 19, loss: 0.5693281888961792, acc: 93.75, f1: 92.44379382635725, r: 0.7277773952849902
06/02/2019 12:36:59 step: 7252, epoch: 219, batch: 24, loss: 0.2699659764766693, acc: 90.625, f1: 87.96216507784831, r: 0.6833767026061114
06/02/2019 12:37:00 step: 7257, epoch: 219, batch: 29, loss: 0.34980300068855286, acc: 85.9375, f1: 84.92879927090453, r: 0.747458560799286
06/02/2019 12:37:01 *** evaluating ***
06/02/2019 12:37:01 step: 220, epoch: 219, acc: 57.692307692307686, f1: 27.10931947405828, r: 0.3146540640325658
06/02/2019 12:37:01 *** epoch: 221 ***
06/02/2019 12:37:01 *** training ***
06/02/2019 12:37:02 step: 7265, epoch: 220, batch: 4, loss: 0.20529377460479736, acc: 95.3125, f1: 89.4829931972789, r: 0.7645065768302214
06/02/2019 12:37:04 step: 7270, epoch: 220, batch: 9, loss: 0.21930192410945892, acc: 92.1875, f1: 84.48602668017561, r: 0.721108063479167
06/02/2019 12:37:05 step: 7275, epoch: 220, batch: 14, loss: 0.3229154050350189, acc: 89.0625, f1: 84.3791747468218, r: 0.6508027462559607
06/02/2019 12:37:06 step: 7280, epoch: 220, batch: 19, loss: 0.2405071258544922, acc: 96.875, f1: 96.8671679197995, r: 0.6609324033406233
06/02/2019 12:37:07 step: 7285, epoch: 220, batch: 24, loss: 0.2901267409324646, acc: 92.1875, f1: 87.96041921041922, r: 0.671229749997787
06/02/2019 12:37:08 step: 7290, epoch: 220, batch: 29, loss: 0.19128954410552979, acc: 93.75, f1: 77.47292662297566, r: 0.589696547735022
06/02/2019 12:37:09 *** evaluating ***
06/02/2019 12:37:09 step: 221, epoch: 220, acc: 58.97435897435898, f1: 28.71586969180552, r: 0.31381486269719655
06/02/2019 12:37:09 *** epoch: 222 ***
06/02/2019 12:37:09 *** training ***
06/02/2019 12:37:10 step: 7298, epoch: 221, batch: 4, loss: 0.2294909656047821, acc: 90.625, f1: 74.96660482374769, r: 0.5976450117628872
06/02/2019 12:37:11 step: 7303, epoch: 221, batch: 9, loss: 0.18075697124004364, acc: 96.875, f1: 96.31416202844775, r: 0.6626315471735187
06/02/2019 12:37:12 step: 7308, epoch: 221, batch: 14, loss: 0.21054159104824066, acc: 96.875, f1: 92.05128205128206, r: 0.7950126089831694
06/02/2019 12:37:14 step: 7313, epoch: 221, batch: 19, loss: 0.2152339071035385, acc: 92.1875, f1: 88.52411583869386, r: 0.6549204181724159
06/02/2019 12:37:14 step: 7318, epoch: 221, batch: 24, loss: 0.2827720642089844, acc: 90.625, f1: 80.27953369650872, r: 0.6988176283326559
06/02/2019 12:37:15 step: 7323, epoch: 221, batch: 29, loss: 0.18042391538619995, acc: 95.3125, f1: 81.01587301587303, r: 0.5835362609816261
06/02/2019 12:37:16 *** evaluating ***
06/02/2019 12:37:16 step: 222, epoch: 221, acc: 57.692307692307686, f1: 27.527467948099538, r: 0.30922648644721706
06/02/2019 12:37:16 *** epoch: 223 ***
06/02/2019 12:37:16 *** training ***
06/02/2019 12:37:17 step: 7331, epoch: 222, batch: 4, loss: 0.2077934741973877, acc: 95.3125, f1: 95.29724933451641, r: 0.6995842463699734
06/02/2019 12:37:18 step: 7336, epoch: 222, batch: 9, loss: 0.22670258581638336, acc: 92.1875, f1: 74.15547415547417, r: 0.6336440292598449
06/02/2019 12:37:19 step: 7341, epoch: 222, batch: 14, loss: 0.25132936239242554, acc: 92.1875, f1: 77.3520171957672, r: 0.6510043828522915
06/02/2019 12:37:20 step: 7346, epoch: 222, batch: 19, loss: 0.26278239488601685, acc: 93.75, f1: 77.2275641025641, r: 0.7485722549390164
06/02/2019 12:37:22 step: 7351, epoch: 222, batch: 24, loss: 0.22560206055641174, acc: 95.3125, f1: 91.66627698542592, r: 0.725853860775072
06/02/2019 12:37:23 step: 7356, epoch: 222, batch: 29, loss: 0.18406900763511658, acc: 98.4375, f1: 96.9047619047619, r: 0.8018489932554791
06/02/2019 12:37:23 *** evaluating ***
06/02/2019 12:37:23 step: 223, epoch: 222, acc: 56.41025641025641, f1: 25.5842769206448, r: 0.30650719806288784
06/02/2019 12:37:23 *** epoch: 224 ***
06/02/2019 12:37:23 *** training ***
06/02/2019 12:37:24 step: 7364, epoch: 223, batch: 4, loss: 0.1990024745464325, acc: 96.875, f1: 94.44014447884416, r: 0.7443879618755758
06/02/2019 12:37:25 step: 7369, epoch: 223, batch: 9, loss: 0.16939079761505127, acc: 95.3125, f1: 97.86956521739131, r: 0.6702050570914921
06/02/2019 12:37:26 step: 7374, epoch: 223, batch: 14, loss: 0.6301224827766418, acc: 89.0625, f1: 84.72568448110952, r: 0.7445195772818953
06/02/2019 12:37:27 step: 7379, epoch: 223, batch: 19, loss: 0.27949464321136475, acc: 90.625, f1: 69.3201581027668, r: 0.6045302011424094
06/02/2019 12:37:28 step: 7384, epoch: 223, batch: 24, loss: 0.16623899340629578, acc: 96.875, f1: 91.16666666666666, r: 0.6527316438268503
06/02/2019 12:37:29 step: 7389, epoch: 223, batch: 29, loss: 0.2608272135257721, acc: 87.5, f1: 84.89161632018775, r: 0.6611230099216935
06/02/2019 12:37:30 *** evaluating ***
06/02/2019 12:37:30 step: 224, epoch: 223, acc: 56.837606837606835, f1: 27.33407624416474, r: 0.31940702376073693
06/02/2019 12:37:30 *** epoch: 225 ***
06/02/2019 12:37:30 *** training ***
06/02/2019 12:37:31 step: 7397, epoch: 224, batch: 4, loss: 0.1575818955898285, acc: 96.875, f1: 80.12121212121214, r: 0.5593748919552602
06/02/2019 12:37:32 step: 7402, epoch: 224, batch: 9, loss: 0.169733926653862, acc: 96.875, f1: 91.58549783549783, r: 0.6809147714846342
06/02/2019 12:37:33 step: 7407, epoch: 224, batch: 14, loss: 0.4904305934906006, acc: 93.75, f1: 90.87738348607914, r: 0.6060970130328268
06/02/2019 12:37:34 step: 7412, epoch: 224, batch: 19, loss: 0.21031931042671204, acc: 92.1875, f1: 91.25368095956333, r: 0.7083066229327856
06/02/2019 12:37:35 step: 7417, epoch: 224, batch: 24, loss: 0.2714138329029083, acc: 92.1875, f1: 89.3895436962664, r: 0.5807764000947502
06/02/2019 12:37:36 step: 7422, epoch: 224, batch: 29, loss: 0.11872194707393646, acc: 98.4375, f1: 96.6137566137566, r: 0.7471850711309884
06/02/2019 12:37:37 *** evaluating ***
06/02/2019 12:37:37 step: 225, epoch: 224, acc: 57.26495726495726, f1: 26.04003948933712, r: 0.3093809754088358
06/02/2019 12:37:37 *** epoch: 226 ***
06/02/2019 12:37:37 *** training ***
06/02/2019 12:37:38 step: 7430, epoch: 225, batch: 4, loss: 0.18387877941131592, acc: 93.75, f1: 82.12620712620712, r: 0.6270935267846809
06/02/2019 12:37:39 step: 7435, epoch: 225, batch: 9, loss: 0.22888673841953278, acc: 93.75, f1: 89.05008247113511, r: 0.6838333314565118
06/02/2019 12:37:40 step: 7440, epoch: 225, batch: 14, loss: 0.5136451721191406, acc: 93.75, f1: 72.46212121212122, r: 0.6032432186024063
06/02/2019 12:37:40 step: 7445, epoch: 225, batch: 19, loss: 0.27156519889831543, acc: 90.625, f1: 85.96500721500722, r: 0.7171005959804829
06/02/2019 12:37:42 step: 7450, epoch: 225, batch: 24, loss: 0.18858589231967926, acc: 93.75, f1: 80.1352048334807, r: 0.7673412934029404
06/02/2019 12:37:43 step: 7455, epoch: 225, batch: 29, loss: 0.20810164511203766, acc: 92.1875, f1: 82.77777777777779, r: 0.588169159057023
06/02/2019 12:37:43 *** evaluating ***
06/02/2019 12:37:43 step: 226, epoch: 225, acc: 57.692307692307686, f1: 27.058260234418064, r: 0.31252354535087057
06/02/2019 12:37:43 *** epoch: 227 ***
06/02/2019 12:37:43 *** training ***
06/02/2019 12:37:45 step: 7463, epoch: 226, batch: 4, loss: 0.22262510657310486, acc: 93.75, f1: 87.51472571144703, r: 0.6383957245629908
06/02/2019 12:37:46 step: 7468, epoch: 226, batch: 9, loss: 0.2758568227291107, acc: 89.0625, f1: 88.6389652014652, r: 0.759832429812848
06/02/2019 12:37:47 step: 7473, epoch: 226, batch: 14, loss: 0.3080243766307831, acc: 84.375, f1: 75.39923374497843, r: 0.7072629084393642
06/02/2019 12:37:47 step: 7478, epoch: 226, batch: 19, loss: 0.2852276861667633, acc: 89.0625, f1: 80.25869963369964, r: 0.6644593073755558
06/02/2019 12:37:48 step: 7483, epoch: 226, batch: 24, loss: 0.30669698119163513, acc: 92.1875, f1: 89.48511904761904, r: 0.7896335214955216
06/02/2019 12:37:50 step: 7488, epoch: 226, batch: 29, loss: 0.19727270305156708, acc: 90.625, f1: 88.02219947540144, r: 0.7010792319240939
06/02/2019 12:37:50 *** evaluating ***
06/02/2019 12:37:50 step: 227, epoch: 226, acc: 57.26495726495726, f1: 26.11526760038352, r: 0.30863292251435237
06/02/2019 12:37:50 *** epoch: 228 ***
06/02/2019 12:37:50 *** training ***
06/02/2019 12:37:51 step: 7496, epoch: 227, batch: 4, loss: 0.2311386913061142, acc: 92.1875, f1: 82.24405249405248, r: 0.6414883944252555
06/02/2019 12:37:52 step: 7501, epoch: 227, batch: 9, loss: 0.22639736533164978, acc: 92.1875, f1: 84.16313559322035, r: 0.735633086660598
06/02/2019 12:37:53 step: 7506, epoch: 227, batch: 14, loss: 0.23022408783435822, acc: 95.3125, f1: 90.44056429232192, r: 0.6760153389294448
06/02/2019 12:37:54 step: 7511, epoch: 227, batch: 19, loss: 0.23903252184391022, acc: 93.75, f1: 80.25058275058275, r: 0.7551225843846808
06/02/2019 12:37:55 step: 7516, epoch: 227, batch: 24, loss: 0.1903674304485321, acc: 96.875, f1: 95.53482763512555, r: 0.715546724980766
06/02/2019 12:37:56 step: 7521, epoch: 227, batch: 29, loss: 0.1582394540309906, acc: 93.75, f1: 82.43913493913495, r: 0.7613054045461122
06/02/2019 12:37:57 *** evaluating ***
06/02/2019 12:37:57 step: 228, epoch: 227, acc: 58.119658119658126, f1: 27.17614115593549, r: 0.31214855089209803
06/02/2019 12:37:57 *** epoch: 229 ***
06/02/2019 12:37:57 *** training ***
06/02/2019 12:37:58 step: 7529, epoch: 228, batch: 4, loss: 0.2352873533964157, acc: 90.625, f1: 90.47402515382811, r: 0.6671429850226115
06/02/2019 12:37:59 step: 7534, epoch: 228, batch: 9, loss: 0.24832476675510406, acc: 93.75, f1: 90.8619226302153, r: 0.6949500348079951
06/02/2019 12:38:00 step: 7539, epoch: 228, batch: 14, loss: 0.4649239778518677, acc: 95.3125, f1: 94.5227501456645, r: 0.6486130356034898
06/02/2019 12:38:01 step: 7544, epoch: 228, batch: 19, loss: 0.20137184858322144, acc: 95.3125, f1: 84.31737588652483, r: 0.6534053920837695
06/02/2019 12:38:02 step: 7549, epoch: 228, batch: 24, loss: 0.23087090253829956, acc: 90.625, f1: 82.2223283252695, r: 0.7198514235299543
06/02/2019 12:38:03 step: 7554, epoch: 228, batch: 29, loss: 0.15110111236572266, acc: 95.3125, f1: 94.52329749103941, r: 0.7035770370425651
06/02/2019 12:38:03 *** evaluating ***
06/02/2019 12:38:04 step: 229, epoch: 228, acc: 58.119658119658126, f1: 27.686559472273753, r: 0.30962690531206305
06/02/2019 12:38:04 *** epoch: 230 ***
06/02/2019 12:38:04 *** training ***
06/02/2019 12:38:05 step: 7562, epoch: 229, batch: 4, loss: 0.18644152581691742, acc: 93.75, f1: 76.30021141649048, r: 0.7240949775933082
06/02/2019 12:38:06 step: 7567, epoch: 229, batch: 9, loss: 0.2272498458623886, acc: 93.75, f1: 93.72675546068159, r: 0.8105135238746084
06/02/2019 12:38:07 step: 7572, epoch: 229, batch: 14, loss: 0.20527929067611694, acc: 92.1875, f1: 91.36892013583743, r: 0.6997555949952438
06/02/2019 12:38:08 step: 7577, epoch: 229, batch: 19, loss: 0.5891835689544678, acc: 90.625, f1: 83.44234025348577, r: 0.6088530283085677
06/02/2019 12:38:09 step: 7582, epoch: 229, batch: 24, loss: 0.5649813413619995, acc: 90.625, f1: 69.52067669172932, r: 0.6227927018264707
06/02/2019 12:38:10 step: 7587, epoch: 229, batch: 29, loss: 0.2060360461473465, acc: 95.3125, f1: 90.51470588235294, r: 0.7516520375899431
06/02/2019 12:38:11 *** evaluating ***
06/02/2019 12:38:11 step: 230, epoch: 229, acc: 57.692307692307686, f1: 27.612109554205084, r: 0.30677702640389115
06/02/2019 12:38:11 *** epoch: 231 ***
06/02/2019 12:38:11 *** training ***
06/02/2019 12:38:12 step: 7595, epoch: 230, batch: 4, loss: 0.21519891917705536, acc: 89.0625, f1: 78.9074074074074, r: 0.6762797815045701
06/02/2019 12:38:13 step: 7600, epoch: 230, batch: 9, loss: 0.1169632226228714, acc: 96.875, f1: 95.91569332135369, r: 0.6112264894303632
06/02/2019 12:38:14 step: 7605, epoch: 230, batch: 14, loss: 0.25066742300987244, acc: 92.1875, f1: 85.10952533129952, r: 0.7284549241803854
06/02/2019 12:38:15 step: 7610, epoch: 230, batch: 19, loss: 0.215226948261261, acc: 95.3125, f1: 88.3898668381427, r: 0.7497109919145665
06/02/2019 12:38:16 step: 7615, epoch: 230, batch: 24, loss: 0.1632155328989029, acc: 96.875, f1: 95.69240196078431, r: 0.7850222186174306
06/02/2019 12:38:17 step: 7620, epoch: 230, batch: 29, loss: 0.22071819007396698, acc: 90.625, f1: 67.68429487179488, r: 0.6876240586075006
06/02/2019 12:38:18 *** evaluating ***
06/02/2019 12:38:18 step: 231, epoch: 230, acc: 57.692307692307686, f1: 26.847319347319342, r: 0.3119146251379214
06/02/2019 12:38:18 *** epoch: 232 ***
06/02/2019 12:38:18 *** training ***
06/02/2019 12:38:19 step: 7628, epoch: 231, batch: 4, loss: 0.22365908324718475, acc: 96.875, f1: 81.08465608465607, r: 0.7281737681926019
06/02/2019 12:38:20 step: 7633, epoch: 231, batch: 9, loss: 0.1648922562599182, acc: 96.875, f1: 97.2990847990848, r: 0.7168910896241496
06/02/2019 12:38:21 step: 7638, epoch: 231, batch: 14, loss: 0.231558695435524, acc: 93.75, f1: 85.09316770186336, r: 0.6135579266539732
06/02/2019 12:38:22 step: 7643, epoch: 231, batch: 19, loss: 0.2749328315258026, acc: 93.75, f1: 81.89718189151039, r: 0.7196226267532592
06/02/2019 12:38:23 step: 7648, epoch: 231, batch: 24, loss: 0.18583248555660248, acc: 93.75, f1: 66.1030901722391, r: 0.6717990214927639
06/02/2019 12:38:24 step: 7653, epoch: 231, batch: 29, loss: 0.25338587164878845, acc: 92.1875, f1: 90.03647835945972, r: 0.627251329058061
06/02/2019 12:38:25 *** evaluating ***
06/02/2019 12:38:25 step: 232, epoch: 231, acc: 57.26495726495726, f1: 27.042635373639868, r: 0.31504488666283154
06/02/2019 12:38:25 *** epoch: 233 ***
06/02/2019 12:38:25 *** training ***
06/02/2019 12:38:26 step: 7661, epoch: 232, batch: 4, loss: 0.20794007182121277, acc: 93.75, f1: 84.08662900188324, r: 0.7371773819484001
06/02/2019 12:38:27 step: 7666, epoch: 232, batch: 9, loss: 0.2313656359910965, acc: 92.1875, f1: 78.72116370471633, r: 0.7874301141264267
06/02/2019 12:38:28 step: 7671, epoch: 232, batch: 14, loss: 0.15959301590919495, acc: 95.3125, f1: 90.07936507936508, r: 0.6334905970182837
06/02/2019 12:38:30 step: 7676, epoch: 232, batch: 19, loss: 0.535750150680542, acc: 92.1875, f1: 74.61954429023454, r: 0.5835822104754914
06/02/2019 12:38:31 step: 7681, epoch: 232, batch: 24, loss: 0.31136375665664673, acc: 95.3125, f1: 90.68002309594623, r: 0.7126656053119107
06/02/2019 12:38:32 step: 7686, epoch: 232, batch: 29, loss: 0.1812758594751358, acc: 93.75, f1: 94.25320275930031, r: 0.7810638336439159
06/02/2019 12:38:32 *** evaluating ***
06/02/2019 12:38:32 step: 233, epoch: 232, acc: 56.41025641025641, f1: 26.43418655681371, r: 0.3135706600412889
06/02/2019 12:38:32 *** epoch: 234 ***
06/02/2019 12:38:32 *** training ***
06/02/2019 12:38:33 step: 7694, epoch: 233, batch: 4, loss: 0.32332053780555725, acc: 90.625, f1: 88.29580745341615, r: 0.7552127577562779
06/02/2019 12:38:34 step: 7699, epoch: 233, batch: 9, loss: 0.23894117772579193, acc: 93.75, f1: 93.2689393939394, r: 0.7898978963293606
06/02/2019 12:38:35 step: 7704, epoch: 233, batch: 14, loss: 0.17202284932136536, acc: 95.3125, f1: 86.675843357871, r: 0.6260674579051939
06/02/2019 12:38:36 step: 7709, epoch: 233, batch: 19, loss: 0.2191615253686905, acc: 92.1875, f1: 92.76056507951098, r: 0.7174520292188237
06/02/2019 12:38:38 step: 7714, epoch: 233, batch: 24, loss: 0.24728384613990784, acc: 95.3125, f1: 80.9036341809451, r: 0.6281632632009435
06/02/2019 12:38:39 step: 7719, epoch: 233, batch: 29, loss: 0.2113671898841858, acc: 93.75, f1: 91.11401366769539, r: 0.6883875096789782
06/02/2019 12:38:39 *** evaluating ***
06/02/2019 12:38:40 step: 234, epoch: 233, acc: 57.692307692307686, f1: 27.27005457355356, r: 0.3167358122537678
06/02/2019 12:38:40 *** epoch: 235 ***
06/02/2019 12:38:40 *** training ***
06/02/2019 12:38:41 step: 7727, epoch: 234, batch: 4, loss: 0.2830665111541748, acc: 89.0625, f1: 86.09003899326481, r: 0.5871292610832327
06/02/2019 12:38:41 step: 7732, epoch: 234, batch: 9, loss: 0.31160610914230347, acc: 87.5, f1: 83.7030976686149, r: 0.6524828336036542
06/02/2019 12:38:43 step: 7737, epoch: 234, batch: 14, loss: 0.20278838276863098, acc: 93.75, f1: 94.63487830627791, r: 0.704169054855518
06/02/2019 12:38:44 step: 7742, epoch: 234, batch: 19, loss: 0.14079265296459198, acc: 98.4375, f1: 97.60239760239759, r: 0.6974501382897172
06/02/2019 12:38:45 step: 7747, epoch: 234, batch: 24, loss: 0.263649046421051, acc: 92.1875, f1: 89.0923772609819, r: 0.6690881892864105
06/02/2019 12:38:46 step: 7752, epoch: 234, batch: 29, loss: 0.22940567135810852, acc: 93.75, f1: 76.66050817490117, r: 0.668971417914483
06/02/2019 12:38:46 *** evaluating ***
06/02/2019 12:38:47 step: 235, epoch: 234, acc: 56.837606837606835, f1: 29.733819966720908, r: 0.317360653260834
06/02/2019 12:38:47 *** epoch: 236 ***
06/02/2019 12:38:47 *** training ***
06/02/2019 12:38:48 step: 7760, epoch: 235, batch: 4, loss: 0.2442050576210022, acc: 90.625, f1: 89.83008495752125, r: 0.7378519253686786
06/02/2019 12:38:49 step: 7765, epoch: 235, batch: 9, loss: 0.13741013407707214, acc: 96.875, f1: 89.33747412008282, r: 0.7385771348291017
06/02/2019 12:38:50 step: 7770, epoch: 235, batch: 14, loss: 0.263422429561615, acc: 93.75, f1: 80.05555555555554, r: 0.6580989185929977
06/02/2019 12:38:51 step: 7775, epoch: 235, batch: 19, loss: 0.19269496202468872, acc: 95.3125, f1: 80.4020979020979, r: 0.6202348661327663
06/02/2019 12:38:52 step: 7780, epoch: 235, batch: 24, loss: 0.3467000722885132, acc: 87.5, f1: 75.24543908472478, r: 0.6871051167638116
06/02/2019 12:38:53 step: 7785, epoch: 235, batch: 29, loss: 0.27285298705101013, acc: 90.625, f1: 90.43960008245723, r: 0.6927642824098271
06/02/2019 12:38:54 *** evaluating ***
06/02/2019 12:38:54 step: 236, epoch: 235, acc: 57.692307692307686, f1: 26.922010281385276, r: 0.3135849089729776
06/02/2019 12:38:54 *** epoch: 237 ***
06/02/2019 12:38:54 *** training ***
06/02/2019 12:38:55 step: 7793, epoch: 236, batch: 4, loss: 0.1443360298871994, acc: 96.875, f1: 84.72388955582232, r: 0.7593683522570455
06/02/2019 12:38:56 step: 7798, epoch: 236, batch: 9, loss: 0.261047899723053, acc: 89.0625, f1: 86.02271434569572, r: 0.6169720685715457
06/02/2019 12:38:57 step: 7803, epoch: 236, batch: 14, loss: 0.17218486964702606, acc: 93.75, f1: 87.23337285628722, r: 0.6593530565121939
06/02/2019 12:38:58 step: 7808, epoch: 236, batch: 19, loss: 0.15276911854743958, acc: 93.75, f1: 85.31590413943356, r: 0.5652252157323077
06/02/2019 12:38:59 step: 7813, epoch: 236, batch: 24, loss: 0.1676863133907318, acc: 96.875, f1: 97.74114774114774, r: 0.6757334158598279
06/02/2019 12:39:00 step: 7818, epoch: 236, batch: 29, loss: 0.24397867918014526, acc: 93.75, f1: 89.37314661935154, r: 0.6784488306692001
06/02/2019 12:39:01 *** evaluating ***
06/02/2019 12:39:01 step: 237, epoch: 236, acc: 57.26495726495726, f1: 28.698851330103913, r: 0.31695405965359613
06/02/2019 12:39:01 *** epoch: 238 ***
06/02/2019 12:39:01 *** training ***
06/02/2019 12:39:02 step: 7826, epoch: 237, batch: 4, loss: 0.18540805578231812, acc: 93.75, f1: 80.74306043056043, r: 0.7078915911050977
06/02/2019 12:39:03 step: 7831, epoch: 237, batch: 9, loss: 0.1825506091117859, acc: 95.3125, f1: 93.69272237196766, r: 0.6844230227204124
06/02/2019 12:39:04 step: 7836, epoch: 237, batch: 14, loss: 0.27703404426574707, acc: 93.75, f1: 90.55413832199545, r: 0.7184567384993843
06/02/2019 12:39:05 step: 7841, epoch: 237, batch: 19, loss: 0.23179543018341064, acc: 92.1875, f1: 83.54889781999756, r: 0.7627176129567926
06/02/2019 12:39:06 step: 7846, epoch: 237, batch: 24, loss: 0.18485291302204132, acc: 95.3125, f1: 97.29219014933301, r: 0.650944006421937
06/02/2019 12:39:07 step: 7851, epoch: 237, batch: 29, loss: 0.2183873951435089, acc: 92.1875, f1: 80.68783068783068, r: 0.6606858231616207
06/02/2019 12:39:08 *** evaluating ***
06/02/2019 12:39:08 step: 238, epoch: 237, acc: 57.692307692307686, f1: 28.422470056389948, r: 0.31511753108536067
06/02/2019 12:39:08 *** epoch: 239 ***
06/02/2019 12:39:08 *** training ***
06/02/2019 12:39:09 step: 7859, epoch: 238, batch: 4, loss: 0.1929628998041153, acc: 95.3125, f1: 93.16802478567185, r: 0.7963581150382668
06/02/2019 12:39:11 step: 7864, epoch: 238, batch: 9, loss: 0.24112918972969055, acc: 89.0625, f1: 87.48653803953343, r: 0.6804412930289697
06/02/2019 12:39:11 step: 7869, epoch: 238, batch: 14, loss: 0.29200541973114014, acc: 90.625, f1: 88.80036630036629, r: 0.7547189683493302
06/02/2019 12:39:12 step: 7874, epoch: 238, batch: 19, loss: 0.2137308418750763, acc: 96.875, f1: 84.23945335710042, r: 0.749796348300906
06/02/2019 12:39:14 step: 7879, epoch: 238, batch: 24, loss: 0.1734790802001953, acc: 95.3125, f1: 84.36011904761904, r: 0.7548317628848129
06/02/2019 12:39:14 step: 7884, epoch: 238, batch: 29, loss: 0.1886940449476242, acc: 92.1875, f1: 77.80466524216524, r: 0.6268993134911902
06/02/2019 12:39:15 *** evaluating ***
06/02/2019 12:39:15 step: 239, epoch: 238, acc: 57.26495726495726, f1: 27.07282913165266, r: 0.31605544311066475
06/02/2019 12:39:15 *** epoch: 240 ***
06/02/2019 12:39:15 *** training ***
06/02/2019 12:39:16 step: 7892, epoch: 239, batch: 4, loss: 0.192330464720726, acc: 93.75, f1: 92.99879973749654, r: 0.7984880554289413
06/02/2019 12:39:17 step: 7897, epoch: 239, batch: 9, loss: 0.15657779574394226, acc: 95.3125, f1: 89.09645909645909, r: 0.6957889503446311
06/02/2019 12:39:18 step: 7902, epoch: 239, batch: 14, loss: 0.24413728713989258, acc: 90.625, f1: 75.93039341167953, r: 0.6080366280430524
06/02/2019 12:39:19 step: 7907, epoch: 239, batch: 19, loss: 0.23047003149986267, acc: 93.75, f1: 91.22990265847409, r: 0.572123582664458
06/02/2019 12:39:21 step: 7912, epoch: 239, batch: 24, loss: 0.20559062063694, acc: 93.75, f1: 87.7732683982684, r: 0.6965456108575867
06/02/2019 12:39:22 step: 7917, epoch: 239, batch: 29, loss: 0.21519654989242554, acc: 92.1875, f1: 92.70843603141739, r: 0.6498784514857482
06/02/2019 12:39:22 *** evaluating ***
06/02/2019 12:39:23 step: 240, epoch: 239, acc: 57.26495726495726, f1: 25.56033418515364, r: 0.30876011245310964
06/02/2019 12:39:23 *** epoch: 241 ***
06/02/2019 12:39:23 *** training ***
06/02/2019 12:39:24 step: 7925, epoch: 240, batch: 4, loss: 0.11578056961297989, acc: 98.4375, f1: 98.85571249776505, r: 0.7018373542477778
06/02/2019 12:39:25 step: 7930, epoch: 240, batch: 9, loss: 0.2370452731847763, acc: 95.3125, f1: 91.76593521421108, r: 0.6871084558544376
06/02/2019 12:39:26 step: 7935, epoch: 240, batch: 14, loss: 0.2124129980802536, acc: 92.1875, f1: 66.79919279539772, r: 0.6241896194712068
06/02/2019 12:39:27 step: 7940, epoch: 240, batch: 19, loss: 0.2548699676990509, acc: 93.75, f1: 79.91869918699186, r: 0.7309541000131468
06/02/2019 12:39:28 step: 7945, epoch: 240, batch: 24, loss: 0.20193959772586823, acc: 93.75, f1: 88.10707970190728, r: 0.696573451107503
06/02/2019 12:39:29 step: 7950, epoch: 240, batch: 29, loss: 0.15923011302947998, acc: 96.875, f1: 92.21120717484578, r: 0.6898455111676806
06/02/2019 12:39:30 *** evaluating ***
06/02/2019 12:39:30 step: 241, epoch: 240, acc: 58.54700854700855, f1: 27.407676251392544, r: 0.3166396398298516
06/02/2019 12:39:30 *** epoch: 242 ***
06/02/2019 12:39:30 *** training ***
06/02/2019 12:39:31 step: 7958, epoch: 241, batch: 4, loss: 0.17722448706626892, acc: 95.3125, f1: 91.27832951362363, r: 0.615712141097577
06/02/2019 12:39:32 step: 7963, epoch: 241, batch: 9, loss: 0.1745273768901825, acc: 93.75, f1: 92.04006046863189, r: 0.7075082306596462
06/02/2019 12:39:33 step: 7968, epoch: 241, batch: 14, loss: 0.2876408100128174, acc: 89.0625, f1: 73.34671907040328, r: 0.6104545562142433
06/02/2019 12:39:34 step: 7973, epoch: 241, batch: 19, loss: 0.2050752490758896, acc: 92.1875, f1: 89.84533744557329, r: 0.7606343401823695
06/02/2019 12:39:36 step: 7978, epoch: 241, batch: 24, loss: 0.23163817822933197, acc: 87.5, f1: 73.74615163531142, r: 0.6713702782468828
06/02/2019 12:39:37 step: 7983, epoch: 241, batch: 29, loss: 0.19108058512210846, acc: 95.3125, f1: 88.59410430839002, r: 0.6663090832896442
06/02/2019 12:39:37 *** evaluating ***
06/02/2019 12:39:37 step: 242, epoch: 241, acc: 57.692307692307686, f1: 27.393207282913163, r: 0.32354832485917906
06/02/2019 12:39:37 *** epoch: 243 ***
06/02/2019 12:39:37 *** training ***
06/02/2019 12:39:38 step: 7991, epoch: 242, batch: 4, loss: 0.18963919579982758, acc: 92.1875, f1: 85.28834812041643, r: 0.7619783249666323
06/02/2019 12:39:39 step: 7996, epoch: 242, batch: 9, loss: 0.22930938005447388, acc: 92.1875, f1: 86.74891774891775, r: 0.6640986964664836
06/02/2019 12:39:41 step: 8001, epoch: 242, batch: 14, loss: 0.2297857403755188, acc: 95.3125, f1: 90.84656084656085, r: 0.5562280839165455
06/02/2019 12:39:42 step: 8006, epoch: 242, batch: 19, loss: 0.2030411958694458, acc: 93.75, f1: 93.06691493624344, r: 0.7165308634844135
06/02/2019 12:39:43 step: 8011, epoch: 242, batch: 24, loss: 0.199952632188797, acc: 95.3125, f1: 92.86663762385086, r: 0.652681869862843
06/02/2019 12:39:44 step: 8016, epoch: 242, batch: 29, loss: 0.23841440677642822, acc: 92.1875, f1: 82.15525793650794, r: 0.7690590515647426
06/02/2019 12:39:44 *** evaluating ***
06/02/2019 12:39:45 step: 243, epoch: 242, acc: 56.837606837606835, f1: 27.0344533567636, r: 0.31883083189202105
06/02/2019 12:39:45 *** epoch: 244 ***
06/02/2019 12:39:45 *** training ***
06/02/2019 12:39:46 step: 8024, epoch: 243, batch: 4, loss: 0.239297017455101, acc: 87.5, f1: 70.30045351473923, r: 0.589385546081678
06/02/2019 12:39:47 step: 8029, epoch: 243, batch: 9, loss: 0.22341413795948029, acc: 93.75, f1: 93.83283211321502, r: 0.7496596255262071
06/02/2019 12:39:48 step: 8034, epoch: 243, batch: 14, loss: 0.14624536037445068, acc: 96.875, f1: 84.07614781634938, r: 0.756395050300759
06/02/2019 12:39:49 step: 8039, epoch: 243, batch: 19, loss: 0.20417702198028564, acc: 92.1875, f1: 73.97588408176864, r: 0.595582414767939
06/02/2019 12:39:50 step: 8044, epoch: 243, batch: 24, loss: 0.1388719379901886, acc: 98.4375, f1: 98.50649350649351, r: 0.7575067445042906
06/02/2019 12:39:51 step: 8049, epoch: 243, batch: 29, loss: 0.22745612263679504, acc: 93.75, f1: 95.95487491232171, r: 0.7483607484539664
06/02/2019 12:39:52 *** evaluating ***
06/02/2019 12:39:52 step: 244, epoch: 243, acc: 57.692307692307686, f1: 29.69475717997571, r: 0.3176410592789939
06/02/2019 12:39:52 *** epoch: 245 ***
06/02/2019 12:39:52 *** training ***
06/02/2019 12:39:53 step: 8057, epoch: 244, batch: 4, loss: 0.250931054353714, acc: 92.1875, f1: 80.10749316974051, r: 0.6844399096415565
06/02/2019 12:39:54 step: 8062, epoch: 244, batch: 9, loss: 0.1482386589050293, acc: 96.875, f1: 96.75936768149882, r: 0.7919491030114891
06/02/2019 12:39:55 step: 8067, epoch: 244, batch: 14, loss: 0.11982125788927078, acc: 98.4375, f1: 97.94941900205059, r: 0.6306339992533875
06/02/2019 12:39:56 step: 8072, epoch: 244, batch: 19, loss: 0.23742805421352386, acc: 93.75, f1: 82.8125, r: 0.7228861772501711
06/02/2019 12:39:57 step: 8077, epoch: 244, batch: 24, loss: 0.2846889793872833, acc: 90.625, f1: 83.45785440613027, r: 0.7065053501767947
06/02/2019 12:39:58 step: 8082, epoch: 244, batch: 29, loss: 0.24606451392173767, acc: 87.5, f1: 75.5483989694516, r: 0.696517728144056
06/02/2019 12:39:59 *** evaluating ***
06/02/2019 12:39:59 step: 245, epoch: 244, acc: 58.119658119658126, f1: 27.13956932225523, r: 0.3156014260866214
06/02/2019 12:39:59 *** epoch: 246 ***
06/02/2019 12:39:59 *** training ***
06/02/2019 12:40:00 step: 8090, epoch: 245, batch: 4, loss: 0.17192494869232178, acc: 98.4375, f1: 99.29118773946361, r: 0.7719989550987183
06/02/2019 12:40:01 step: 8095, epoch: 245, batch: 9, loss: 0.22081322968006134, acc: 90.625, f1: 79.9948459431218, r: 0.7141807693794406
06/02/2019 12:40:02 step: 8100, epoch: 245, batch: 14, loss: 0.19338002800941467, acc: 95.3125, f1: 94.08022533022533, r: 0.49557571450359966
06/02/2019 12:40:03 step: 8105, epoch: 245, batch: 19, loss: 0.11064954847097397, acc: 95.3125, f1: 82.36658456486042, r: 0.7053366073697023
06/02/2019 12:40:04 step: 8110, epoch: 245, batch: 24, loss: 0.23593547940254211, acc: 95.3125, f1: 93.1431141957458, r: 0.69943822192972
06/02/2019 12:40:05 step: 8115, epoch: 245, batch: 29, loss: 0.19640259444713593, acc: 95.3125, f1: 92.26551226551226, r: 0.6459302872484458
06/02/2019 12:40:06 *** evaluating ***
06/02/2019 12:40:06 step: 246, epoch: 245, acc: 57.26495726495726, f1: 28.324916019364316, r: 0.3169752630509573
06/02/2019 12:40:06 *** epoch: 247 ***
06/02/2019 12:40:06 *** training ***
06/02/2019 12:40:07 step: 8123, epoch: 246, batch: 4, loss: 0.30230674147605896, acc: 84.375, f1: 79.07467532467533, r: 0.7171038032614373
06/02/2019 12:40:08 step: 8128, epoch: 246, batch: 9, loss: 0.25361964106559753, acc: 90.625, f1: 74.05664771055986, r: 0.5336438796348211
06/02/2019 12:40:09 step: 8133, epoch: 246, batch: 14, loss: 0.21331509947776794, acc: 90.625, f1: 77.42063492063492, r: 0.5776006715806974
06/02/2019 12:40:10 step: 8138, epoch: 246, batch: 19, loss: 0.2438272088766098, acc: 92.1875, f1: 83.29730195365526, r: 0.6927966724475464
06/02/2019 12:40:11 step: 8143, epoch: 246, batch: 24, loss: 0.38012999296188354, acc: 89.0625, f1: 87.27777777777777, r: 0.7133832311581124
06/02/2019 12:40:12 step: 8148, epoch: 246, batch: 29, loss: 0.09449819475412369, acc: 98.4375, f1: 86.8421052631579, r: 0.7087665005923942
06/02/2019 12:40:13 *** evaluating ***
06/02/2019 12:40:13 step: 247, epoch: 246, acc: 57.692307692307686, f1: 27.813672452876894, r: 0.3196259090465083
06/02/2019 12:40:13 *** epoch: 248 ***
06/02/2019 12:40:13 *** training ***
06/02/2019 12:40:14 step: 8156, epoch: 247, batch: 4, loss: 0.2146410048007965, acc: 93.75, f1: 92.2584541062802, r: 0.7376049991730369
06/02/2019 12:40:15 step: 8161, epoch: 247, batch: 9, loss: 0.17798784375190735, acc: 95.3125, f1: 93.53765429616824, r: 0.6894729236486887
06/02/2019 12:40:16 step: 8166, epoch: 247, batch: 14, loss: 0.16612470149993896, acc: 98.4375, f1: 95.71428571428571, r: 0.7594773678220118
06/02/2019 12:40:17 step: 8171, epoch: 247, batch: 19, loss: 0.33123332262039185, acc: 84.375, f1: 84.18261562998404, r: 0.762089155452914
06/02/2019 12:40:18 step: 8176, epoch: 247, batch: 24, loss: 0.3124106228351593, acc: 90.625, f1: 84.02325708711578, r: 0.7090615471863201
06/02/2019 12:40:19 step: 8181, epoch: 247, batch: 29, loss: 0.5392680168151855, acc: 92.1875, f1: 71.25546353807223, r: 0.5825141067532402
06/02/2019 12:40:20 *** evaluating ***
06/02/2019 12:40:20 step: 248, epoch: 247, acc: 57.692307692307686, f1: 26.066335460182998, r: 0.3123749188763345
06/02/2019 12:40:20 *** epoch: 249 ***
06/02/2019 12:40:20 *** training ***
06/02/2019 12:40:21 step: 8189, epoch: 248, batch: 4, loss: 0.1850558966398239, acc: 95.3125, f1: 90.05915409642118, r: 0.6608348819369015
06/02/2019 12:40:22 step: 8194, epoch: 248, batch: 9, loss: 0.2709760069847107, acc: 92.1875, f1: 84.59498480243161, r: 0.6956329780889349
06/02/2019 12:40:23 step: 8199, epoch: 248, batch: 14, loss: 0.2634594738483429, acc: 92.1875, f1: 89.41798941798942, r: 0.8003086725393859
06/02/2019 12:40:24 step: 8204, epoch: 248, batch: 19, loss: 0.1672217845916748, acc: 95.3125, f1: 94.02145473574045, r: 0.6583211742315818
06/02/2019 12:40:25 step: 8209, epoch: 248, batch: 24, loss: 0.2721880078315735, acc: 90.625, f1: 65.78714545927662, r: 0.674148231180709
06/02/2019 12:40:26 step: 8214, epoch: 248, batch: 29, loss: 0.19340410828590393, acc: 95.3125, f1: 90.21188403676884, r: 0.7640458343370151
06/02/2019 12:40:27 *** evaluating ***
06/02/2019 12:40:27 step: 249, epoch: 248, acc: 56.837606837606835, f1: 24.97626166664212, r: 0.31578024685968914
06/02/2019 12:40:27 *** epoch: 250 ***
06/02/2019 12:40:27 *** training ***
06/02/2019 12:40:28 step: 8222, epoch: 249, batch: 4, loss: 0.1796364039182663, acc: 96.875, f1: 93.1279178338002, r: 0.6974762653999463
06/02/2019 12:40:29 step: 8227, epoch: 249, batch: 9, loss: 0.23517607152462006, acc: 92.1875, f1: 77.16680378445085, r: 0.6364414599901901
06/02/2019 12:40:30 step: 8232, epoch: 249, batch: 14, loss: 0.19064843654632568, acc: 95.3125, f1: 80.4981884057971, r: 0.693982458503368
06/02/2019 12:40:32 step: 8237, epoch: 249, batch: 19, loss: 0.20130443572998047, acc: 93.75, f1: 91.41821622265927, r: 0.7515063961264566
06/02/2019 12:40:32 step: 8242, epoch: 249, batch: 24, loss: 0.2389860600233078, acc: 89.0625, f1: 73.68993051766161, r: 0.6252799256124232
06/02/2019 12:40:33 step: 8247, epoch: 249, batch: 29, loss: 0.5301357507705688, acc: 92.1875, f1: 77.45483682983682, r: 0.6808465762580611
06/02/2019 12:40:34 *** evaluating ***
06/02/2019 12:40:35 step: 250, epoch: 249, acc: 58.54700854700855, f1: 25.761081339379288, r: 0.31586503535299837
06/02/2019 12:40:35 *** epoch: 251 ***
06/02/2019 12:40:35 *** training ***
06/02/2019 12:40:36 step: 8255, epoch: 250, batch: 4, loss: 0.12363424152135849, acc: 96.875, f1: 95.82414526094125, r: 0.7796695278294472
06/02/2019 12:40:37 step: 8260, epoch: 250, batch: 9, loss: 0.2690412402153015, acc: 92.1875, f1: 86.90239149539575, r: 0.6690111383495517
06/02/2019 12:40:38 step: 8265, epoch: 250, batch: 14, loss: 0.5527359843254089, acc: 92.1875, f1: 78.53553921568627, r: 0.6443207558826043
06/02/2019 12:40:39 step: 8270, epoch: 250, batch: 19, loss: 0.0982765480875969, acc: 98.4375, f1: 98.78335949764521, r: 0.6740067939674094
06/02/2019 12:40:40 step: 8275, epoch: 250, batch: 24, loss: 0.21059715747833252, acc: 92.1875, f1: 89.36319843997373, r: 0.7797742864495685
06/02/2019 12:40:41 step: 8280, epoch: 250, batch: 29, loss: 0.14351190626621246, acc: 98.4375, f1: 85.25345622119815, r: 0.7066141197580086
06/02/2019 12:40:41 *** evaluating ***
06/02/2019 12:40:42 step: 251, epoch: 250, acc: 58.97435897435898, f1: 28.792016806722685, r: 0.30996992526310524
06/02/2019 12:40:42 *** epoch: 252 ***
06/02/2019 12:40:42 *** training ***
06/02/2019 12:40:43 step: 8288, epoch: 251, batch: 4, loss: 0.2358415424823761, acc: 92.1875, f1: 82.91327820938015, r: 0.7491551041410175
06/02/2019 12:40:44 step: 8293, epoch: 251, batch: 9, loss: 0.2094351053237915, acc: 93.75, f1: 86.10819969954233, r: 0.6489738025564453
06/02/2019 12:40:45 step: 8298, epoch: 251, batch: 14, loss: 0.41920745372772217, acc: 93.75, f1: 93.86264127889217, r: 0.7531826124775515
06/02/2019 12:40:46 step: 8303, epoch: 251, batch: 19, loss: 0.1884322315454483, acc: 89.0625, f1: 69.65148378191857, r: 0.5948583486632922
06/02/2019 12:40:47 step: 8308, epoch: 251, batch: 24, loss: 0.20365312695503235, acc: 92.1875, f1: 68.97727272727273, r: 0.5954402379599132
06/02/2019 12:40:48 step: 8313, epoch: 251, batch: 29, loss: 0.5505968928337097, acc: 93.75, f1: 81.74596237096236, r: 0.8260181081627075
06/02/2019 12:40:49 *** evaluating ***
06/02/2019 12:40:49 step: 252, epoch: 251, acc: 58.54700854700855, f1: 25.912826524825828, r: 0.3100836200497612
06/02/2019 12:40:49 *** epoch: 253 ***
06/02/2019 12:40:49 *** training ***
06/02/2019 12:40:50 step: 8321, epoch: 252, batch: 4, loss: 0.2502308785915375, acc: 90.625, f1: 76.47607022607022, r: 0.7147889476768647
06/02/2019 12:40:51 step: 8326, epoch: 252, batch: 9, loss: 0.12934912741184235, acc: 98.4375, f1: 97.67080745341615, r: 0.816497128158725
06/02/2019 12:40:52 step: 8331, epoch: 252, batch: 14, loss: 0.21336176991462708, acc: 90.625, f1: 71.96536796536796, r: 0.5908050633995822
06/02/2019 12:40:53 step: 8336, epoch: 252, batch: 19, loss: 0.19503317773342133, acc: 92.1875, f1: 90.14588859416446, r: 0.6911619414937468
06/02/2019 12:40:54 step: 8341, epoch: 252, batch: 24, loss: 0.09620948135852814, acc: 98.4375, f1: 98.43175692232296, r: 0.6720030285188636
06/02/2019 12:40:55 step: 8346, epoch: 252, batch: 29, loss: 0.2535954415798187, acc: 89.0625, f1: 76.34415584415585, r: 0.5517736671109377
06/02/2019 12:40:56 *** evaluating ***
06/02/2019 12:40:56 step: 253, epoch: 252, acc: 58.54700854700855, f1: 27.311205721002736, r: 0.3107737506319587
06/02/2019 12:40:56 *** epoch: 254 ***
06/02/2019 12:40:56 *** training ***
06/02/2019 12:40:57 step: 8354, epoch: 253, batch: 4, loss: 0.5363248586654663, acc: 92.1875, f1: 77.8083028083028, r: 0.7148604865417063
06/02/2019 12:40:58 step: 8359, epoch: 253, batch: 9, loss: 0.17758090794086456, acc: 90.625, f1: 73.69390771829796, r: 0.4738457211248934
06/02/2019 12:40:59 step: 8364, epoch: 253, batch: 14, loss: 0.1546413004398346, acc: 95.3125, f1: 81.48216862502578, r: 0.6317459960966432
06/02/2019 12:41:00 step: 8369, epoch: 253, batch: 19, loss: 0.24519874155521393, acc: 90.625, f1: 82.09855681402996, r: 0.6781963687760535
06/02/2019 12:41:01 step: 8374, epoch: 253, batch: 24, loss: 0.17156466841697693, acc: 98.4375, f1: 97.94871794871796, r: 0.6908463863029961
06/02/2019 12:41:02 step: 8379, epoch: 253, batch: 29, loss: 0.20142023265361786, acc: 93.75, f1: 82.40626710454296, r: 0.6756964742100682
06/02/2019 12:41:03 *** evaluating ***
06/02/2019 12:41:03 step: 254, epoch: 253, acc: 57.692307692307686, f1: 26.5965019809955, r: 0.31729453694514737
06/02/2019 12:41:03 *** epoch: 255 ***
06/02/2019 12:41:03 *** training ***
06/02/2019 12:41:04 step: 8387, epoch: 254, batch: 4, loss: 0.20146356523036957, acc: 96.875, f1: 95.71428571428571, r: 0.7347477747318734
06/02/2019 12:41:05 step: 8392, epoch: 254, batch: 9, loss: 0.28457117080688477, acc: 90.625, f1: 80.5335307966887, r: 0.7637769770505474
06/02/2019 12:41:06 step: 8397, epoch: 254, batch: 14, loss: 0.2674909830093384, acc: 89.0625, f1: 85.96710526315789, r: 0.7701871783549944
06/02/2019 12:41:08 step: 8402, epoch: 254, batch: 19, loss: 0.15333017706871033, acc: 95.3125, f1: 84.84564535913862, r: 0.6698633866117667
06/02/2019 12:41:09 step: 8407, epoch: 254, batch: 24, loss: 0.17196911573410034, acc: 93.75, f1: 79.20497493366555, r: 0.6104338424349066
06/02/2019 12:41:10 step: 8412, epoch: 254, batch: 29, loss: 0.3011725842952728, acc: 87.5, f1: 83.14039408866995, r: 0.6695983911735452
06/02/2019 12:41:10 *** evaluating ***
06/02/2019 12:41:10 step: 255, epoch: 254, acc: 58.119658119658126, f1: 27.091527250169474, r: 0.31212749997731837
06/02/2019 12:41:10 *** epoch: 256 ***
06/02/2019 12:41:10 *** training ***
06/02/2019 12:41:11 step: 8420, epoch: 255, batch: 4, loss: 0.22176888585090637, acc: 95.3125, f1: 84.00360430049665, r: 0.7114006991710459
06/02/2019 12:41:12 step: 8425, epoch: 255, batch: 9, loss: 0.25256308913230896, acc: 92.1875, f1: 76.7745925349612, r: 0.5731518011506488
06/02/2019 12:41:13 step: 8430, epoch: 255, batch: 14, loss: 0.16915060579776764, acc: 93.75, f1: 93.01787101787102, r: 0.7381813235943856
06/02/2019 12:41:15 step: 8435, epoch: 255, batch: 19, loss: 0.15120148658752441, acc: 95.3125, f1: 86.98736637512148, r: 0.6240096478816645
06/02/2019 12:41:15 step: 8440, epoch: 255, batch: 24, loss: 0.1976785808801651, acc: 98.4375, f1: 94.92063492063492, r: 0.6817544910394385
06/02/2019 12:41:16 step: 8445, epoch: 255, batch: 29, loss: 0.1533551961183548, acc: 98.4375, f1: 98.91589438713062, r: 0.7068157549937483
06/02/2019 12:41:17 *** evaluating ***
06/02/2019 12:41:17 step: 256, epoch: 255, acc: 56.837606837606835, f1: 27.43731519537056, r: 0.31687570133481324
06/02/2019 12:41:17 *** epoch: 257 ***
06/02/2019 12:41:17 *** training ***
06/02/2019 12:41:18 step: 8453, epoch: 256, batch: 4, loss: 0.2802799344062805, acc: 87.5, f1: 68.72619047619048, r: 0.75277843051876
06/02/2019 12:41:20 step: 8458, epoch: 256, batch: 9, loss: 0.1814049780368805, acc: 96.875, f1: 97.99382716049382, r: 0.651699344859054
06/02/2019 12:41:21 step: 8463, epoch: 256, batch: 14, loss: 0.24067474901676178, acc: 93.75, f1: 87.37674203191445, r: 0.6652068957233028
06/02/2019 12:41:22 step: 8468, epoch: 256, batch: 19, loss: 0.2483111023902893, acc: 93.75, f1: 94.41269841269842, r: 0.6627554349166671
06/02/2019 12:41:23 step: 8473, epoch: 256, batch: 24, loss: 0.22682565450668335, acc: 93.75, f1: 87.49776386404294, r: 0.7156348184296767
06/02/2019 12:41:24 step: 8478, epoch: 256, batch: 29, loss: 0.237665057182312, acc: 93.75, f1: 91.24206033520308, r: 0.7030693434559253
06/02/2019 12:41:24 *** evaluating ***
06/02/2019 12:41:25 step: 257, epoch: 256, acc: 58.54700854700855, f1: 28.543540352486435, r: 0.30985878246214593
06/02/2019 12:41:25 *** epoch: 258 ***
06/02/2019 12:41:25 *** training ***
06/02/2019 12:41:26 step: 8486, epoch: 257, batch: 4, loss: 0.13284103572368622, acc: 96.875, f1: 97.15367965367965, r: 0.7515401293417793
06/02/2019 12:41:27 step: 8491, epoch: 257, batch: 9, loss: 0.14541387557983398, acc: 96.875, f1: 85.38056680161944, r: 0.7323878592143366
06/02/2019 12:41:28 step: 8496, epoch: 257, batch: 14, loss: 0.1919131875038147, acc: 93.75, f1: 89.35946406534642, r: 0.6902143388646402
06/02/2019 12:41:29 step: 8501, epoch: 257, batch: 19, loss: 0.15122951567173004, acc: 96.875, f1: 96.9157587847169, r: 0.7180907977130525
06/02/2019 12:41:30 step: 8506, epoch: 257, batch: 24, loss: 0.2045738697052002, acc: 93.75, f1: 93.37068160597572, r: 0.6122279410791656
06/02/2019 12:41:31 step: 8511, epoch: 257, batch: 29, loss: 0.19834907352924347, acc: 95.3125, f1: 93.24665350010972, r: 0.6506141549278015
06/02/2019 12:41:32 *** evaluating ***
06/02/2019 12:41:32 step: 258, epoch: 257, acc: 57.26495726495726, f1: 26.34163764286715, r: 0.31232388653368537
06/02/2019 12:41:32 *** epoch: 259 ***
06/02/2019 12:41:32 *** training ***
06/02/2019 12:41:33 step: 8519, epoch: 258, batch: 4, loss: 0.2518746554851532, acc: 95.3125, f1: 81.42361111111111, r: 0.5967274790970636
06/02/2019 12:41:34 step: 8524, epoch: 258, batch: 9, loss: 0.17722265422344208, acc: 95.3125, f1: 88.15695925622096, r: 0.677456682651711
06/02/2019 12:41:35 step: 8529, epoch: 258, batch: 14, loss: 0.23602724075317383, acc: 90.625, f1: 85.79304214574529, r: 0.6539561813917035
06/02/2019 12:41:36 step: 8534, epoch: 258, batch: 19, loss: 0.12967979907989502, acc: 98.4375, f1: 98.45614035087719, r: 0.6108955576589203
06/02/2019 12:41:37 step: 8539, epoch: 258, batch: 24, loss: 0.1438991278409958, acc: 98.4375, f1: 97.27891156462584, r: 0.6668260348480937
06/02/2019 12:41:38 step: 8544, epoch: 258, batch: 29, loss: 0.20217585563659668, acc: 93.75, f1: 95.74377299154237, r: 0.7159390059677919
06/02/2019 12:41:39 *** evaluating ***
06/02/2019 12:41:39 step: 259, epoch: 258, acc: 55.98290598290598, f1: 26.552348823805165, r: 0.30875335298785384
06/02/2019 12:41:39 *** epoch: 260 ***
06/02/2019 12:41:39 *** training ***
06/02/2019 12:41:40 step: 8552, epoch: 259, batch: 4, loss: 0.20212261378765106, acc: 92.1875, f1: 88.46825396825398, r: 0.79251194790799
06/02/2019 12:41:41 step: 8557, epoch: 259, batch: 9, loss: 0.11915113776922226, acc: 96.875, f1: 96.30681818181819, r: 0.731024858122851
06/02/2019 12:41:42 step: 8562, epoch: 259, batch: 14, loss: 0.22407834231853485, acc: 92.1875, f1: 84.52335858585859, r: 0.7524024674009165
06/02/2019 12:41:43 step: 8567, epoch: 259, batch: 19, loss: 0.19365058839321136, acc: 95.3125, f1: 94.58333333333333, r: 0.7542553615040696
06/02/2019 12:41:44 step: 8572, epoch: 259, batch: 24, loss: 0.1560957431793213, acc: 96.875, f1: 96.75150519978106, r: 0.7490835521339139
06/02/2019 12:41:45 step: 8577, epoch: 259, batch: 29, loss: 0.16947491466999054, acc: 96.875, f1: 93.74297813499265, r: 0.654225389048413
06/02/2019 12:41:46 *** evaluating ***
06/02/2019 12:41:46 step: 260, epoch: 259, acc: 57.692307692307686, f1: 26.142516186462018, r: 0.3057135608969882
06/02/2019 12:41:46 *** epoch: 261 ***
06/02/2019 12:41:46 *** training ***
06/02/2019 12:41:47 step: 8585, epoch: 260, batch: 4, loss: 0.1808430403470993, acc: 93.75, f1: 90.04542255773782, r: 0.6889957636953266
06/02/2019 12:41:48 step: 8590, epoch: 260, batch: 9, loss: 0.3014023005962372, acc: 87.5, f1: 86.45999468367889, r: 0.7142028357504266
06/02/2019 12:41:49 step: 8595, epoch: 260, batch: 14, loss: 0.17562463879585266, acc: 92.1875, f1: 72.1246680490378, r: 0.5945320133779232
06/02/2019 12:41:50 step: 8600, epoch: 260, batch: 19, loss: 0.16830885410308838, acc: 95.3125, f1: 94.1896645021645, r: 0.7865876031499708
06/02/2019 12:41:51 step: 8605, epoch: 260, batch: 24, loss: 0.21927036345005035, acc: 92.1875, f1: 91.0837839409268, r: 0.5839584430660265
06/02/2019 12:41:52 step: 8610, epoch: 260, batch: 29, loss: 0.15513363480567932, acc: 96.875, f1: 98.30982298087561, r: 0.7695484723407812
06/02/2019 12:41:53 *** evaluating ***
06/02/2019 12:41:53 step: 261, epoch: 260, acc: 58.119658119658126, f1: 27.881690927737214, r: 0.3093898067091695
06/02/2019 12:41:53 *** epoch: 262 ***
06/02/2019 12:41:53 *** training ***
06/02/2019 12:41:54 step: 8618, epoch: 261, batch: 4, loss: 0.1432490050792694, acc: 96.875, f1: 96.22122762148338, r: 0.8049549324069889
06/02/2019 12:41:55 step: 8623, epoch: 261, batch: 9, loss: 0.2169913798570633, acc: 93.75, f1: 95.16106086472219, r: 0.762746992155713
06/02/2019 12:41:56 step: 8628, epoch: 261, batch: 14, loss: 0.2461472749710083, acc: 89.0625, f1: 77.15789166543811, r: 0.7727183897731131
06/02/2019 12:41:57 step: 8633, epoch: 261, batch: 19, loss: 0.24090084433555603, acc: 93.75, f1: 91.27309888179454, r: 0.677198917871268
06/02/2019 12:41:58 step: 8638, epoch: 261, batch: 24, loss: 0.2689720690250397, acc: 89.0625, f1: 88.993494631054, r: 0.6640206137039939
06/02/2019 12:41:59 step: 8643, epoch: 261, batch: 29, loss: 0.2022285908460617, acc: 95.3125, f1: 80.77421444768385, r: 0.5287993531773281
06/02/2019 12:41:59 *** evaluating ***
06/02/2019 12:42:00 step: 262, epoch: 261, acc: 58.54700854700855, f1: 26.048840487960067, r: 0.3137746724392794
06/02/2019 12:42:00 *** epoch: 263 ***
06/02/2019 12:42:00 *** training ***
06/02/2019 12:42:01 step: 8651, epoch: 262, batch: 4, loss: 0.28162989020347595, acc: 90.625, f1: 87.92887667887666, r: 0.6801018476999456
06/02/2019 12:42:02 step: 8656, epoch: 262, batch: 9, loss: 0.21064318716526031, acc: 90.625, f1: 90.34151034151033, r: 0.588705031099047
06/02/2019 12:42:03 step: 8661, epoch: 262, batch: 14, loss: 0.168786883354187, acc: 95.3125, f1: 92.92183026736802, r: 0.6159901695886351
06/02/2019 12:42:04 step: 8666, epoch: 262, batch: 19, loss: 0.18368194997310638, acc: 93.75, f1: 88.65054793847364, r: 0.7053063018035844
06/02/2019 12:42:05 step: 8671, epoch: 262, batch: 24, loss: 0.2739175260066986, acc: 87.5, f1: 80.79129157289321, r: 0.6160168480868768
06/02/2019 12:42:06 step: 8676, epoch: 262, batch: 29, loss: 0.1871226578950882, acc: 93.75, f1: 89.87521370558306, r: 0.7178448941226432
06/02/2019 12:42:07 *** evaluating ***
06/02/2019 12:42:07 step: 263, epoch: 262, acc: 57.26495726495726, f1: 26.93787004219354, r: 0.3126186633920962
06/02/2019 12:42:07 *** epoch: 264 ***
06/02/2019 12:42:07 *** training ***
06/02/2019 12:42:08 step: 8684, epoch: 263, batch: 4, loss: 0.31219789385795593, acc: 87.5, f1: 75.89182328511596, r: 0.6442057916109883
06/02/2019 12:42:09 step: 8689, epoch: 263, batch: 9, loss: 0.19841638207435608, acc: 93.75, f1: 84.08139083139083, r: 0.7019301346599063
06/02/2019 12:42:10 step: 8694, epoch: 263, batch: 14, loss: 0.5330777764320374, acc: 92.1875, f1: 83.83928571428572, r: 0.7403864577706601
06/02/2019 12:42:11 step: 8699, epoch: 263, batch: 19, loss: 0.17657844722270966, acc: 95.3125, f1: 95.70574162679426, r: 0.7495172597250032
06/02/2019 12:42:12 step: 8704, epoch: 263, batch: 24, loss: 0.149950310587883, acc: 96.875, f1: 97.79158040027606, r: 0.6729434823009277
06/02/2019 12:42:13 step: 8709, epoch: 263, batch: 29, loss: 0.3090725839138031, acc: 93.75, f1: 85.27477851605758, r: 0.7327054002725637
06/02/2019 12:42:14 *** evaluating ***
06/02/2019 12:42:14 step: 264, epoch: 263, acc: 58.119658119658126, f1: 26.503038143263936, r: 0.3067362701297066
06/02/2019 12:42:14 *** epoch: 265 ***
06/02/2019 12:42:14 *** training ***
06/02/2019 12:42:15 step: 8717, epoch: 264, batch: 4, loss: 0.27742886543273926, acc: 89.0625, f1: 70.16833166833167, r: 0.6464244165623952
06/02/2019 12:42:16 step: 8722, epoch: 264, batch: 9, loss: 0.19744561612606049, acc: 93.75, f1: 81.5550462609286, r: 0.7821539293793621
06/02/2019 12:42:17 step: 8727, epoch: 264, batch: 14, loss: 0.6792654395103455, acc: 89.0625, f1: 81.87488596971355, r: 0.733933084651724
06/02/2019 12:42:18 step: 8732, epoch: 264, batch: 19, loss: 0.24826680123806, acc: 90.625, f1: 88.3345996407221, r: 0.6806763548926763
06/02/2019 12:42:19 step: 8737, epoch: 264, batch: 24, loss: 0.14577144384384155, acc: 96.875, f1: 97.7570869196485, r: 0.6938931773748458
06/02/2019 12:42:20 step: 8742, epoch: 264, batch: 29, loss: 0.17776450514793396, acc: 95.3125, f1: 94.22305764411027, r: 0.7922236971608576
06/02/2019 12:42:20 *** evaluating ***
06/02/2019 12:42:21 step: 265, epoch: 264, acc: 57.26495726495726, f1: 26.992742737540336, r: 0.3103166735876084
06/02/2019 12:42:21 *** epoch: 266 ***
06/02/2019 12:42:21 *** training ***
06/02/2019 12:42:22 step: 8750, epoch: 265, batch: 4, loss: 0.2166767418384552, acc: 93.75, f1: 87.01870718241345, r: 0.6962543100233807
06/02/2019 12:42:23 step: 8755, epoch: 265, batch: 9, loss: 0.23307162523269653, acc: 92.1875, f1: 90.87584653718375, r: 0.756771491081885
06/02/2019 12:42:24 step: 8760, epoch: 265, batch: 14, loss: 0.25061267614364624, acc: 95.3125, f1: 93.11315223763924, r: 0.6759605710082511
06/02/2019 12:42:25 step: 8765, epoch: 265, batch: 19, loss: 0.2035016566514969, acc: 95.3125, f1: 82.18660968660969, r: 0.7850172350184194
06/02/2019 12:42:26 step: 8770, epoch: 265, batch: 24, loss: 0.1762533187866211, acc: 92.1875, f1: 65.361484616143, r: 0.5388915589949551
06/02/2019 12:42:27 step: 8775, epoch: 265, batch: 29, loss: 0.21356917917728424, acc: 92.1875, f1: 85.97634195378556, r: 0.7247571638240372
06/02/2019 12:42:28 *** evaluating ***
06/02/2019 12:42:28 step: 266, epoch: 265, acc: 58.119658119658126, f1: 26.767833822538485, r: 0.30345142592818597
06/02/2019 12:42:28 *** epoch: 267 ***
06/02/2019 12:42:28 *** training ***
06/02/2019 12:42:29 step: 8783, epoch: 266, batch: 4, loss: 0.15919490158557892, acc: 95.3125, f1: 95.84776334776335, r: 0.6534073353831402
06/02/2019 12:42:30 step: 8788, epoch: 266, batch: 9, loss: 0.46736496686935425, acc: 96.875, f1: 95.01282051282051, r: 0.7230781659984009
06/02/2019 12:42:31 step: 8793, epoch: 266, batch: 14, loss: 0.1691504716873169, acc: 96.875, f1: 95.99206349206348, r: 0.762683643493492
06/02/2019 12:42:32 step: 8798, epoch: 266, batch: 19, loss: 0.2382960170507431, acc: 93.75, f1: 80.05593505593505, r: 0.734452949188542
06/02/2019 12:42:33 step: 8803, epoch: 266, batch: 24, loss: 0.304953396320343, acc: 90.625, f1: 90.016016353491, r: 0.7472598501635642
06/02/2019 12:42:35 step: 8808, epoch: 266, batch: 29, loss: 0.21196284890174866, acc: 90.625, f1: 85.98939575830332, r: 0.6587602689525047
06/02/2019 12:42:35 *** evaluating ***
06/02/2019 12:42:35 step: 267, epoch: 266, acc: 58.119658119658126, f1: 26.294383939473224, r: 0.3041167111731087
06/02/2019 12:42:35 *** epoch: 268 ***
06/02/2019 12:42:35 *** training ***
06/02/2019 12:42:36 step: 8816, epoch: 267, batch: 4, loss: 0.39921534061431885, acc: 81.25, f1: 74.96981089086351, r: 0.6728181213916657
06/02/2019 12:42:37 step: 8821, epoch: 267, batch: 9, loss: 0.1624029129743576, acc: 90.625, f1: 89.34942608411995, r: 0.6818960208898285
06/02/2019 12:42:38 step: 8826, epoch: 267, batch: 14, loss: 0.20545834302902222, acc: 93.75, f1: 88.49556445741844, r: 0.7470197639331017
06/02/2019 12:42:40 step: 8831, epoch: 267, batch: 19, loss: 0.3069310188293457, acc: 90.625, f1: 82.7605981416957, r: 0.6426744184868666
06/02/2019 12:42:41 step: 8836, epoch: 267, batch: 24, loss: 0.12188415974378586, acc: 96.875, f1: 92.65504610332196, r: 0.8016998323830834
06/02/2019 12:42:42 step: 8841, epoch: 267, batch: 29, loss: 0.17041169106960297, acc: 96.875, f1: 96.504884004884, r: 0.6735858770506085
06/02/2019 12:42:42 *** evaluating ***
06/02/2019 12:42:43 step: 268, epoch: 267, acc: 58.119658119658126, f1: 27.19936767541908, r: 0.3097298841537842
06/02/2019 12:42:43 *** epoch: 269 ***
06/02/2019 12:42:43 *** training ***
06/02/2019 12:42:44 step: 8849, epoch: 268, batch: 4, loss: 0.1311427503824234, acc: 98.4375, f1: 97.38775510204081, r: 0.6550056198383758
06/02/2019 12:42:45 step: 8854, epoch: 268, batch: 9, loss: 0.2403176873922348, acc: 89.0625, f1: 82.26025894228658, r: 0.6319435530447236
06/02/2019 12:42:46 step: 8859, epoch: 268, batch: 14, loss: 0.2524617612361908, acc: 89.0625, f1: 75.16400266400267, r: 0.6667417164449353
06/02/2019 12:42:47 step: 8864, epoch: 268, batch: 19, loss: 0.25877684354782104, acc: 90.625, f1: 85.70278352887047, r: 0.6555634416428541
06/02/2019 12:42:49 step: 8869, epoch: 268, batch: 24, loss: 0.21575289964675903, acc: 89.0625, f1: 63.89101994670112, r: 0.6128960469804988
06/02/2019 12:42:50 step: 8874, epoch: 268, batch: 29, loss: 0.17220979928970337, acc: 93.75, f1: 89.99337504600663, r: 0.6296208507913774
06/02/2019 12:42:50 *** evaluating ***
06/02/2019 12:42:50 step: 269, epoch: 268, acc: 58.97435897435898, f1: 28.449482037960816, r: 0.30902607804104776
06/02/2019 12:42:50 *** epoch: 270 ***
06/02/2019 12:42:50 *** training ***
06/02/2019 12:42:52 step: 8882, epoch: 269, batch: 4, loss: 0.15117715299129486, acc: 95.3125, f1: 80.65528286833613, r: 0.7331892594939609
06/02/2019 12:42:53 step: 8887, epoch: 269, batch: 9, loss: 0.17799383401870728, acc: 95.3125, f1: 95.48011709831495, r: 0.6416141939715191
06/02/2019 12:42:54 step: 8892, epoch: 269, batch: 14, loss: 0.15084224939346313, acc: 98.4375, f1: 98.08018068887634, r: 0.6061754382547777
06/02/2019 12:42:55 step: 8897, epoch: 269, batch: 19, loss: 0.19265751540660858, acc: 93.75, f1: 95.99329242186386, r: 0.6475676332114789
06/02/2019 12:42:56 step: 8902, epoch: 269, batch: 24, loss: 0.1586371809244156, acc: 95.3125, f1: 91.06287138202032, r: 0.7688111171864186
06/02/2019 12:42:56 step: 8907, epoch: 269, batch: 29, loss: 0.14103643596172333, acc: 96.875, f1: 94.9047619047619, r: 0.7661439078308891
06/02/2019 12:42:57 *** evaluating ***
06/02/2019 12:42:57 step: 270, epoch: 269, acc: 59.401709401709404, f1: 28.855596673986163, r: 0.3106536802460977
06/02/2019 12:42:57 *** epoch: 271 ***
06/02/2019 12:42:57 *** training ***
06/02/2019 12:42:58 step: 8915, epoch: 270, batch: 4, loss: 0.2296106517314911, acc: 90.625, f1: 76.11988304093566, r: 0.7291608289010114
06/02/2019 12:42:59 step: 8920, epoch: 270, batch: 9, loss: 0.18373316526412964, acc: 93.75, f1: 90.1979456691819, r: 0.6638480835942578
06/02/2019 12:43:00 step: 8925, epoch: 270, batch: 14, loss: 0.177243173122406, acc: 95.3125, f1: 93.59369079196665, r: 0.7655857994979178
06/02/2019 12:43:01 step: 8930, epoch: 270, batch: 19, loss: 0.2601782977581024, acc: 93.75, f1: 89.32539682539682, r: 0.6604390631007795
06/02/2019 12:43:03 step: 8935, epoch: 270, batch: 24, loss: 0.14908213913440704, acc: 95.3125, f1: 81.83466193670277, r: 0.569934031771735
06/02/2019 12:43:04 step: 8940, epoch: 270, batch: 29, loss: 0.09483873844146729, acc: 100.0, f1: 100.0, r: 0.8027440012723159
06/02/2019 12:43:04 *** evaluating ***
06/02/2019 12:43:05 step: 271, epoch: 270, acc: 58.119658119658126, f1: 27.277477412940076, r: 0.3083159565339382
06/02/2019 12:43:05 *** epoch: 272 ***
06/02/2019 12:43:05 *** training ***
06/02/2019 12:43:06 step: 8948, epoch: 271, batch: 4, loss: 0.17711429297924042, acc: 98.4375, f1: 95.10204081632652, r: 0.6614372928831963
06/02/2019 12:43:07 step: 8953, epoch: 271, batch: 9, loss: 0.23440754413604736, acc: 95.3125, f1: 95.66257816257816, r: 0.7830165855706295
06/02/2019 12:43:08 step: 8958, epoch: 271, batch: 14, loss: 0.24220792949199677, acc: 93.75, f1: 88.28042328042328, r: 0.5777909348999111
06/02/2019 12:43:09 step: 8963, epoch: 271, batch: 19, loss: 0.19985872507095337, acc: 87.5, f1: 76.18107769423558, r: 0.7126428640197775
06/02/2019 12:43:10 step: 8968, epoch: 271, batch: 24, loss: 0.20991559326648712, acc: 90.625, f1: 72.5099729175816, r: 0.6298906242613466
06/02/2019 12:43:11 step: 8973, epoch: 271, batch: 29, loss: 0.1506863534450531, acc: 95.3125, f1: 90.95428064842959, r: 0.684860941501892
06/02/2019 12:43:12 *** evaluating ***
06/02/2019 12:43:12 step: 272, epoch: 271, acc: 57.26495726495726, f1: 26.410257133084507, r: 0.30696967695534133
06/02/2019 12:43:12 *** epoch: 273 ***
06/02/2019 12:43:12 *** training ***
06/02/2019 12:43:13 step: 8981, epoch: 272, batch: 4, loss: 0.1873646080493927, acc: 92.1875, f1: 89.13163556020697, r: 0.6788867966171996
06/02/2019 12:43:14 step: 8986, epoch: 272, batch: 9, loss: 0.1358822137117386, acc: 95.3125, f1: 93.9121830550402, r: 0.6915562902353638
06/02/2019 12:43:15 step: 8991, epoch: 272, batch: 14, loss: 0.14240103960037231, acc: 95.3125, f1: 83.24489795918367, r: 0.735852123346429
06/02/2019 12:43:16 step: 8996, epoch: 272, batch: 19, loss: 0.1964316964149475, acc: 95.3125, f1: 95.69659935792659, r: 0.6914042328728178
06/02/2019 12:43:18 step: 9001, epoch: 272, batch: 24, loss: 0.1358279436826706, acc: 96.875, f1: 96.03682052531157, r: 0.7908015831341602
06/02/2019 12:43:19 step: 9006, epoch: 272, batch: 29, loss: 0.10621848702430725, acc: 93.75, f1: 90.00107105370265, r: 0.6091344888965123
06/02/2019 12:43:19 *** evaluating ***
06/02/2019 12:43:19 step: 273, epoch: 272, acc: 58.97435897435898, f1: 28.449482037960816, r: 0.30732386304605924
06/02/2019 12:43:19 *** epoch: 274 ***
06/02/2019 12:43:19 *** training ***
06/02/2019 12:43:20 step: 9014, epoch: 273, batch: 4, loss: 0.1870485246181488, acc: 95.3125, f1: 84.63636363636364, r: 0.7188591576835233
06/02/2019 12:43:21 step: 9019, epoch: 273, batch: 9, loss: 0.17814666032791138, acc: 95.3125, f1: 88.18825910931174, r: 0.7575127758369467
06/02/2019 12:43:23 step: 9024, epoch: 273, batch: 14, loss: 0.2395639568567276, acc: 92.1875, f1: 86.50918047469771, r: 0.7374073949456432
06/02/2019 12:43:23 step: 9029, epoch: 273, batch: 19, loss: 0.19277191162109375, acc: 93.75, f1: 91.28809099397334, r: 0.6572801047010093
06/02/2019 12:43:24 step: 9034, epoch: 273, batch: 24, loss: 0.17515133321285248, acc: 98.4375, f1: 97.25490196078431, r: 0.7291507247503896
06/02/2019 12:43:26 step: 9039, epoch: 273, batch: 29, loss: 0.15371324121952057, acc: 98.4375, f1: 85.14285714285714, r: 0.6012598313091503
06/02/2019 12:43:26 *** evaluating ***
06/02/2019 12:43:27 step: 274, epoch: 273, acc: 57.692307692307686, f1: 27.061688311688307, r: 0.30959185553572277
06/02/2019 12:43:27 *** epoch: 275 ***
06/02/2019 12:43:27 *** training ***
06/02/2019 12:43:28 step: 9047, epoch: 274, batch: 4, loss: 0.08629938960075378, acc: 100.0, f1: 100.0, r: 0.7182909900833159
06/02/2019 12:43:29 step: 9052, epoch: 274, batch: 9, loss: 0.23094786703586578, acc: 90.625, f1: 87.41491948795922, r: 0.6828873821558425
06/02/2019 12:43:30 step: 9057, epoch: 274, batch: 14, loss: 0.17009368538856506, acc: 98.4375, f1: 97.70855710705335, r: 0.6800347543511138
06/02/2019 12:43:31 step: 9062, epoch: 274, batch: 19, loss: 0.22880561649799347, acc: 93.75, f1: 93.04015139890544, r: 0.5603279851292159
06/02/2019 12:43:32 step: 9067, epoch: 274, batch: 24, loss: 0.18489189445972443, acc: 96.875, f1: 98.41008067394874, r: 0.6067155640174854
06/02/2019 12:43:33 step: 9072, epoch: 274, batch: 29, loss: 0.16306890547275543, acc: 93.75, f1: 76.66666666666666, r: 0.6647269292250025
06/02/2019 12:43:33 *** evaluating ***
06/02/2019 12:43:34 step: 275, epoch: 274, acc: 58.54700854700855, f1: 28.685933425094177, r: 0.30536272575652235
06/02/2019 12:43:34 *** epoch: 276 ***
06/02/2019 12:43:34 *** training ***
06/02/2019 12:43:35 step: 9080, epoch: 275, batch: 4, loss: 0.26325488090515137, acc: 93.75, f1: 93.40057665639063, r: 0.6614787711102028
06/02/2019 12:43:36 step: 9085, epoch: 275, batch: 9, loss: 0.17246104776859283, acc: 95.3125, f1: 90.42676134781398, r: 0.7002075926340389
06/02/2019 12:43:37 step: 9090, epoch: 275, batch: 14, loss: 0.31486624479293823, acc: 93.75, f1: 88.33099906629319, r: 0.6660724796001594
06/02/2019 12:43:38 step: 9095, epoch: 275, batch: 19, loss: 0.18245448172092438, acc: 90.625, f1: 88.38012589327455, r: 0.7251959378469638
06/02/2019 12:43:39 step: 9100, epoch: 275, batch: 24, loss: 0.18540073931217194, acc: 93.75, f1: 94.67559523809524, r: 0.7761032760173993
06/02/2019 12:43:40 step: 9105, epoch: 275, batch: 29, loss: 0.151274174451828, acc: 96.875, f1: 95.38126361655773, r: 0.677465901737488
06/02/2019 12:43:41 *** evaluating ***
06/02/2019 12:43:41 step: 276, epoch: 275, acc: 58.54700854700855, f1: 28.709728868585326, r: 0.30862795284383365
06/02/2019 12:43:41 *** epoch: 277 ***
06/02/2019 12:43:41 *** training ***
06/02/2019 12:43:42 step: 9113, epoch: 276, batch: 4, loss: 0.21266677975654602, acc: 95.3125, f1: 96.16959064327486, r: 0.6472589158086822
06/02/2019 12:43:44 step: 9118, epoch: 276, batch: 9, loss: 0.2700536251068115, acc: 89.0625, f1: 89.05960969293378, r: 0.7122750635077304
06/02/2019 12:43:45 step: 9123, epoch: 276, batch: 14, loss: 0.22691988945007324, acc: 90.625, f1: 68.10897435897436, r: 0.6476397941851463
06/02/2019 12:43:46 step: 9128, epoch: 276, batch: 19, loss: 0.210087388753891, acc: 92.1875, f1: 90.99773242630386, r: 0.6416123024113659
06/02/2019 12:43:47 step: 9133, epoch: 276, batch: 24, loss: 0.20924456417560577, acc: 93.75, f1: 93.11226408000603, r: 0.6101597323105974
06/02/2019 12:43:48 step: 9138, epoch: 276, batch: 29, loss: 0.24476557970046997, acc: 90.625, f1: 86.45064477403864, r: 0.6688767468698511
06/02/2019 12:43:48 *** evaluating ***
06/02/2019 12:43:49 step: 277, epoch: 276, acc: 58.54700854700855, f1: 27.088034733124022, r: 0.3096279721369231
06/02/2019 12:43:49 *** epoch: 278 ***
06/02/2019 12:43:49 *** training ***
06/02/2019 12:43:50 step: 9146, epoch: 277, batch: 4, loss: 0.17763657867908478, acc: 96.875, f1: 96.28663003663004, r: 0.8046910623046536
06/02/2019 12:43:51 step: 9151, epoch: 277, batch: 9, loss: 0.19068901240825653, acc: 92.1875, f1: 76.76751402315311, r: 0.5679237746132748
06/02/2019 12:43:52 step: 9156, epoch: 277, batch: 14, loss: 0.10258162766695023, acc: 100.0, f1: 100.0, r: 0.7192789160832523
06/02/2019 12:43:53 step: 9161, epoch: 277, batch: 19, loss: 0.18063496053218842, acc: 95.3125, f1: 92.15536664593269, r: 0.6218113032568898
06/02/2019 12:43:54 step: 9166, epoch: 277, batch: 24, loss: 0.4836588203907013, acc: 93.75, f1: 93.44188029821858, r: 0.6864544513586188
06/02/2019 12:43:55 step: 9171, epoch: 277, batch: 29, loss: 0.27992936968803406, acc: 87.5, f1: 76.78800366300365, r: 0.6550760763432979
06/02/2019 12:43:56 *** evaluating ***
06/02/2019 12:43:56 step: 278, epoch: 277, acc: 58.54700854700855, f1: 29.07392610906752, r: 0.3046802948641973
06/02/2019 12:43:56 *** epoch: 279 ***
06/02/2019 12:43:56 *** training ***
06/02/2019 12:43:57 step: 9179, epoch: 278, batch: 4, loss: 0.16013887524604797, acc: 96.875, f1: 95.71025020177562, r: 0.7073513987170166
06/02/2019 12:43:58 step: 9184, epoch: 278, batch: 9, loss: 0.1629907488822937, acc: 95.3125, f1: 94.68160377358491, r: 0.6239437881151615
06/02/2019 12:43:59 step: 9189, epoch: 278, batch: 14, loss: 0.25998222827911377, acc: 95.3125, f1: 90.80586080586082, r: 0.6293865802678892
06/02/2019 12:44:00 step: 9194, epoch: 278, batch: 19, loss: 0.12360940128564835, acc: 95.3125, f1: 96.75274725274726, r: 0.8024096962509811
06/02/2019 12:44:01 step: 9199, epoch: 278, batch: 24, loss: 0.2338271290063858, acc: 90.625, f1: 84.72660511142278, r: 0.6985141321251132
06/02/2019 12:44:02 step: 9204, epoch: 278, batch: 29, loss: 0.16071391105651855, acc: 95.3125, f1: 96.12596899224806, r: 0.7897511535658541
06/02/2019 12:44:03 *** evaluating ***
06/02/2019 12:44:03 step: 279, epoch: 278, acc: 58.54700854700855, f1: 27.407676251392544, r: 0.3112503813046556
06/02/2019 12:44:03 *** epoch: 280 ***
06/02/2019 12:44:03 *** training ***
06/02/2019 12:44:04 step: 9212, epoch: 279, batch: 4, loss: 0.2069084197282791, acc: 93.75, f1: 82.74525474525475, r: 0.6213673480583005
06/02/2019 12:44:05 step: 9217, epoch: 279, batch: 9, loss: 0.20874576270580292, acc: 92.1875, f1: 87.21743622440486, r: 0.6417245831627217
06/02/2019 12:44:06 step: 9222, epoch: 279, batch: 14, loss: 0.22460363805294037, acc: 92.1875, f1: 91.75908606759671, r: 0.6927830758361906
06/02/2019 12:44:07 step: 9227, epoch: 279, batch: 19, loss: 0.1399979442358017, acc: 96.875, f1: 98.41269841269842, r: 0.6964538802557015
06/02/2019 12:44:08 step: 9232, epoch: 279, batch: 24, loss: 0.15726865828037262, acc: 95.3125, f1: 89.08234958151178, r: 0.663994035065958
06/02/2019 12:44:10 step: 9237, epoch: 279, batch: 29, loss: 0.14363840222358704, acc: 96.875, f1: 95.76982258336999, r: 0.6259301210396522
06/02/2019 12:44:10 *** evaluating ***
06/02/2019 12:44:10 step: 280, epoch: 279, acc: 58.119658119658126, f1: 26.50181671698065, r: 0.30519635087464897
06/02/2019 12:44:10 *** epoch: 281 ***
06/02/2019 12:44:10 *** training ***
06/02/2019 12:44:11 step: 9245, epoch: 280, batch: 4, loss: 0.15938834846019745, acc: 96.875, f1: 93.35924846128928, r: 0.6927060447333369
06/02/2019 12:44:13 step: 9250, epoch: 280, batch: 9, loss: 0.21077270805835724, acc: 95.3125, f1: 96.5323216029456, r: 0.7085454551915369
06/02/2019 12:44:14 step: 9255, epoch: 280, batch: 14, loss: 0.12161044031381607, acc: 96.875, f1: 97.28137631864341, r: 0.7046496751410165
06/02/2019 12:44:15 step: 9260, epoch: 280, batch: 19, loss: 0.2937982380390167, acc: 92.1875, f1: 89.6399433899434, r: 0.6011677826821938
06/02/2019 12:44:16 step: 9265, epoch: 280, batch: 24, loss: 0.16766409575939178, acc: 95.3125, f1: 93.73015873015873, r: 0.8096814736978029
06/02/2019 12:44:17 step: 9270, epoch: 280, batch: 29, loss: 0.14645321667194366, acc: 95.3125, f1: 82.71458520515125, r: 0.6208861810730694
06/02/2019 12:44:17 *** evaluating ***
06/02/2019 12:44:18 step: 281, epoch: 280, acc: 58.54700854700855, f1: 28.666693061846043, r: 0.30876295309702506
06/02/2019 12:44:18 *** epoch: 282 ***
06/02/2019 12:44:18 *** training ***
06/02/2019 12:44:19 step: 9278, epoch: 281, batch: 4, loss: 0.1721254140138626, acc: 93.75, f1: 91.3039583499632, r: 0.637474290286741
06/02/2019 12:44:20 step: 9283, epoch: 281, batch: 9, loss: 0.23875576257705688, acc: 89.0625, f1: 70.3518798101878, r: 0.5898650424054076
06/02/2019 12:44:21 step: 9288, epoch: 281, batch: 14, loss: 0.23576395213603973, acc: 90.625, f1: 82.62925799690505, r: 0.7584756940884854
06/02/2019 12:44:22 step: 9293, epoch: 281, batch: 19, loss: 0.19794659316539764, acc: 92.1875, f1: 84.28796688972453, r: 0.7455438396284355
06/02/2019 12:44:23 step: 9298, epoch: 281, batch: 24, loss: 0.17766503989696503, acc: 92.1875, f1: 73.13492063492063, r: 0.6557534047879711
06/02/2019 12:44:24 step: 9303, epoch: 281, batch: 29, loss: 0.18318873643875122, acc: 90.625, f1: 77.65745766155686, r: 0.6191813158212864
06/02/2019 12:44:25 *** evaluating ***
06/02/2019 12:44:25 step: 282, epoch: 281, acc: 58.54700854700855, f1: 26.963051349572087, r: 0.30960911397538343
06/02/2019 12:44:25 *** epoch: 283 ***
06/02/2019 12:44:25 *** training ***
06/02/2019 12:44:26 step: 9311, epoch: 282, batch: 4, loss: 0.5224261283874512, acc: 90.625, f1: 82.72594364699628, r: 0.7994604702017186
06/02/2019 12:44:27 step: 9316, epoch: 282, batch: 9, loss: 0.21869397163391113, acc: 90.625, f1: 77.11480095990966, r: 0.6901548213846518
06/02/2019 12:44:28 step: 9321, epoch: 282, batch: 14, loss: 0.2892058491706848, acc: 95.3125, f1: 92.13882163034705, r: 0.6622961380592305
06/02/2019 12:44:29 step: 9326, epoch: 282, batch: 19, loss: 0.2868702709674835, acc: 87.5, f1: 79.13458746538015, r: 0.7510946894048713
06/02/2019 12:44:30 step: 9331, epoch: 282, batch: 24, loss: 0.23457744717597961, acc: 93.75, f1: 77.38095238095238, r: 0.6608097886997789
06/02/2019 12:44:31 step: 9336, epoch: 282, batch: 29, loss: 0.24331076443195343, acc: 93.75, f1: 91.98432589410032, r: 0.6206977736670269
06/02/2019 12:44:32 *** evaluating ***
06/02/2019 12:44:32 step: 283, epoch: 282, acc: 58.54700854700855, f1: 27.90334426453508, r: 0.3102799104485073
06/02/2019 12:44:32 *** epoch: 284 ***
06/02/2019 12:44:32 *** training ***
06/02/2019 12:44:34 step: 9344, epoch: 283, batch: 4, loss: 0.22002695500850677, acc: 92.1875, f1: 92.99771815310106, r: 0.759024387962561
06/02/2019 12:44:35 step: 9349, epoch: 283, batch: 9, loss: 0.27220800518989563, acc: 93.75, f1: 91.56634546660123, r: 0.7101343934289504
06/02/2019 12:44:36 step: 9354, epoch: 283, batch: 14, loss: 0.20009613037109375, acc: 95.3125, f1: 84.07594086021506, r: 0.6127022534814808
06/02/2019 12:44:37 step: 9359, epoch: 283, batch: 19, loss: 0.1647043079137802, acc: 93.75, f1: 90.31235431235432, r: 0.771158299310968
06/02/2019 12:44:38 step: 9364, epoch: 283, batch: 24, loss: 0.13573364913463593, acc: 93.75, f1: 91.9088319088319, r: 0.7482803510174492
06/02/2019 12:44:39 step: 9369, epoch: 283, batch: 29, loss: 0.16030870378017426, acc: 95.3125, f1: 96.02065826330532, r: 0.7730902522629214
06/02/2019 12:44:39 *** evaluating ***
06/02/2019 12:44:40 step: 284, epoch: 283, acc: 58.119658119658126, f1: 28.362467118356616, r: 0.31190599985920625
06/02/2019 12:44:40 *** epoch: 285 ***
06/02/2019 12:44:40 *** training ***
06/02/2019 12:44:41 step: 9377, epoch: 284, batch: 4, loss: 0.20898906886577606, acc: 92.1875, f1: 77.57521921812204, r: 0.7255658622123511
06/02/2019 12:44:42 step: 9382, epoch: 284, batch: 9, loss: 0.5023914575576782, acc: 95.3125, f1: 93.61892736892736, r: 0.7503746697242035
06/02/2019 12:44:43 step: 9387, epoch: 284, batch: 14, loss: 0.1800055354833603, acc: 93.75, f1: 91.11531792204062, r: 0.6326297738466322
06/02/2019 12:44:44 step: 9392, epoch: 284, batch: 19, loss: 0.11030987650156021, acc: 98.4375, f1: 94.13919413919413, r: 0.6358261212120125
06/02/2019 12:44:45 step: 9397, epoch: 284, batch: 24, loss: 0.11877517402172089, acc: 98.4375, f1: 96.83890577507597, r: 0.6398685402587705
06/02/2019 12:44:46 step: 9402, epoch: 284, batch: 29, loss: 0.1749274879693985, acc: 95.3125, f1: 81.18200639939771, r: 0.5884445314781964
06/02/2019 12:44:47 *** evaluating ***
06/02/2019 12:44:47 step: 285, epoch: 284, acc: 55.98290598290598, f1: 23.996046038208586, r: 0.31104060433757935
06/02/2019 12:44:47 *** epoch: 286 ***
06/02/2019 12:44:47 *** training ***
06/02/2019 12:44:48 step: 9410, epoch: 285, batch: 4, loss: 0.20502646267414093, acc: 90.625, f1: 83.07329244829245, r: 0.7113013286632122
06/02/2019 12:44:49 step: 9415, epoch: 285, batch: 9, loss: 0.24860532581806183, acc: 90.625, f1: 87.52420687203296, r: 0.765857554394686
06/02/2019 12:44:50 step: 9420, epoch: 285, batch: 14, loss: 0.4808158874511719, acc: 93.75, f1: 92.81494675517457, r: 0.6445983915569122
06/02/2019 12:44:51 step: 9425, epoch: 285, batch: 19, loss: 0.20294761657714844, acc: 96.875, f1: 91.73701298701299, r: 0.7443878494444736
06/02/2019 12:44:52 step: 9430, epoch: 285, batch: 24, loss: 0.17490839958190918, acc: 93.75, f1: 89.12565026010404, r: 0.6567814595581666
06/02/2019 12:44:53 step: 9435, epoch: 285, batch: 29, loss: 0.24683097004890442, acc: 92.1875, f1: 78.30195335710042, r: 0.7136781228529361
06/02/2019 12:44:54 *** evaluating ***
06/02/2019 12:44:54 step: 286, epoch: 285, acc: 58.119658119658126, f1: 27.77664200993929, r: 0.302710200865047
06/02/2019 12:44:54 *** epoch: 287 ***
06/02/2019 12:44:54 *** training ***
06/02/2019 12:44:56 step: 9443, epoch: 286, batch: 4, loss: 0.18934966623783112, acc: 95.3125, f1: 94.21390013495277, r: 0.7336174375399848
06/02/2019 12:44:57 step: 9448, epoch: 286, batch: 9, loss: 0.24298612773418427, acc: 92.1875, f1: 88.75721500721501, r: 0.7017677778233719
06/02/2019 12:44:58 step: 9453, epoch: 286, batch: 14, loss: 0.16852328181266785, acc: 92.1875, f1: 88.36118844701001, r: 0.6731414782580762
06/02/2019 12:44:59 step: 9458, epoch: 286, batch: 19, loss: 0.39454734325408936, acc: 98.4375, f1: 99.33209647495362, r: 0.5958851872274935
06/02/2019 12:45:00 step: 9463, epoch: 286, batch: 24, loss: 0.17599695920944214, acc: 95.3125, f1: 93.32722832722831, r: 0.6013687759486146
06/02/2019 12:45:01 step: 9468, epoch: 286, batch: 29, loss: 0.23747102916240692, acc: 89.0625, f1: 81.54449716949716, r: 0.6668077187589505
06/02/2019 12:45:01 *** evaluating ***
06/02/2019 12:45:02 step: 287, epoch: 286, acc: 58.119658119658126, f1: 27.387673819527812, r: 0.3066175109755699
06/02/2019 12:45:02 *** epoch: 288 ***
06/02/2019 12:45:02 *** training ***
06/02/2019 12:45:03 step: 9476, epoch: 287, batch: 4, loss: 0.12602102756500244, acc: 96.875, f1: 95.48896168636946, r: 0.6364493871977517
06/02/2019 12:45:04 step: 9481, epoch: 287, batch: 9, loss: 0.1697108894586563, acc: 98.4375, f1: 96.86028257456829, r: 0.7375516294733486
06/02/2019 12:45:05 step: 9486, epoch: 287, batch: 14, loss: 0.24981963634490967, acc: 89.0625, f1: 77.04969958202717, r: 0.7166066538900105
06/02/2019 12:45:06 step: 9491, epoch: 287, batch: 19, loss: 0.13245688378810883, acc: 96.875, f1: 95.04510246102559, r: 0.7949208693044771
06/02/2019 12:45:07 step: 9496, epoch: 287, batch: 24, loss: 0.20536936819553375, acc: 93.75, f1: 90.05153576582148, r: 0.6477432866977766
06/02/2019 12:45:08 step: 9501, epoch: 287, batch: 29, loss: 0.11688232421875, acc: 95.3125, f1: 92.9920634920635, r: 0.720630742717978
06/02/2019 12:45:09 *** evaluating ***
06/02/2019 12:45:09 step: 288, epoch: 287, acc: 57.692307692307686, f1: 26.818031106824208, r: 0.3097186230313682
06/02/2019 12:45:09 *** epoch: 289 ***
06/02/2019 12:45:09 *** training ***
06/02/2019 12:45:10 step: 9509, epoch: 288, batch: 4, loss: 0.19266767799854279, acc: 93.75, f1: 93.87780135906073, r: 0.6720488338516775
06/02/2019 12:45:11 step: 9514, epoch: 288, batch: 9, loss: 0.21753650903701782, acc: 92.1875, f1: 90.07816060523197, r: 0.7449463008588211
06/02/2019 12:45:13 step: 9519, epoch: 288, batch: 14, loss: 0.15989428758621216, acc: 93.75, f1: 91.56108597285068, r: 0.5872434197827356
06/02/2019 12:45:14 step: 9524, epoch: 288, batch: 19, loss: 0.1772490292787552, acc: 93.75, f1: 91.75330886234761, r: 0.7891934723874227
06/02/2019 12:45:15 step: 9529, epoch: 288, batch: 24, loss: 0.16216467320919037, acc: 93.75, f1: 80.8630873956961, r: 0.8014121098236096
06/02/2019 12:45:16 step: 9534, epoch: 288, batch: 29, loss: 0.4870494604110718, acc: 92.1875, f1: 76.98155019583591, r: 0.6831362504687536
06/02/2019 12:45:16 *** evaluating ***
06/02/2019 12:45:17 step: 289, epoch: 288, acc: 58.54700854700855, f1: 28.207950152313703, r: 0.30377929797310815
06/02/2019 12:45:17 *** epoch: 290 ***
06/02/2019 12:45:17 *** training ***
06/02/2019 12:45:18 step: 9542, epoch: 289, batch: 4, loss: 0.23474037647247314, acc: 92.1875, f1: 78.06552847313716, r: 0.655351051555664
06/02/2019 12:45:19 step: 9547, epoch: 289, batch: 9, loss: 0.20349249243736267, acc: 92.1875, f1: 82.26831383746277, r: 0.718957569018872
06/02/2019 12:45:20 step: 9552, epoch: 289, batch: 14, loss: 0.16960623860359192, acc: 98.4375, f1: 94.88966318234611, r: 0.6630770892901265
06/02/2019 12:45:21 step: 9557, epoch: 289, batch: 19, loss: 0.2043612003326416, acc: 93.75, f1: 93.7807519057519, r: 0.6992704364218093
06/02/2019 12:45:22 step: 9562, epoch: 289, batch: 24, loss: 0.22877106070518494, acc: 90.625, f1: 71.14322875192441, r: 0.5453262955238583
06/02/2019 12:45:23 step: 9567, epoch: 289, batch: 29, loss: 0.16634470224380493, acc: 95.3125, f1: 90.40277777777777, r: 0.7459340402912342
06/02/2019 12:45:23 *** evaluating ***
06/02/2019 12:45:24 step: 290, epoch: 289, acc: 58.119658119658126, f1: 26.503038143263936, r: 0.30493672833663393
06/02/2019 12:45:24 *** epoch: 291 ***
06/02/2019 12:45:24 *** training ***
06/02/2019 12:45:25 step: 9575, epoch: 290, batch: 4, loss: 0.22005559504032135, acc: 92.1875, f1: 81.83890013495278, r: 0.7209301652060749
06/02/2019 12:45:26 step: 9580, epoch: 290, batch: 9, loss: 0.5259863138198853, acc: 93.75, f1: 79.5940170940171, r: 0.6204820746932224
06/02/2019 12:45:27 step: 9585, epoch: 290, batch: 14, loss: 0.1686706691980362, acc: 96.875, f1: 84.55433455433456, r: 0.6954591481674406
06/02/2019 12:45:28 step: 9590, epoch: 290, batch: 19, loss: 0.18962882459163666, acc: 95.3125, f1: 95.73646760579611, r: 0.7938272470014082
06/02/2019 12:45:29 step: 9595, epoch: 290, batch: 24, loss: 0.12862548232078552, acc: 98.4375, f1: 95.40229885057472, r: 0.7532828280469535
06/02/2019 12:45:30 step: 9600, epoch: 290, batch: 29, loss: 0.2590501308441162, acc: 89.0625, f1: 84.04481538992408, r: 0.6687616092206836
06/02/2019 12:45:31 *** evaluating ***
06/02/2019 12:45:31 step: 291, epoch: 290, acc: 58.119658119658126, f1: 28.427564580228516, r: 0.3028941420086472
06/02/2019 12:45:31 *** epoch: 292 ***
06/02/2019 12:45:31 *** training ***
06/02/2019 12:45:32 step: 9608, epoch: 291, batch: 4, loss: 0.19638025760650635, acc: 95.3125, f1: 83.57384952212539, r: 0.6865730604713691
06/02/2019 12:45:33 step: 9613, epoch: 291, batch: 9, loss: 0.25632166862487793, acc: 90.625, f1: 84.57417582417581, r: 0.7233630255408195
06/02/2019 12:45:34 step: 9618, epoch: 291, batch: 14, loss: 0.10429039597511292, acc: 100.0, f1: 100.0, r: 0.727684583593618
06/02/2019 12:45:35 step: 9623, epoch: 291, batch: 19, loss: 0.0949908122420311, acc: 98.4375, f1: 99.27272727272727, r: 0.7473280982879821
06/02/2019 12:45:36 step: 9628, epoch: 291, batch: 24, loss: 0.23953582346439362, acc: 90.625, f1: 76.60386851520573, r: 0.5979234319692833
06/02/2019 12:45:37 step: 9633, epoch: 291, batch: 29, loss: 0.2728448808193207, acc: 89.0625, f1: 83.39918837201445, r: 0.7505226559880704
06/02/2019 12:45:38 *** evaluating ***
06/02/2019 12:45:38 step: 292, epoch: 291, acc: 57.692307692307686, f1: 24.999278305926847, r: 0.3052244464705714
06/02/2019 12:45:38 *** epoch: 293 ***
06/02/2019 12:45:38 *** training ***
06/02/2019 12:45:39 step: 9641, epoch: 292, batch: 4, loss: 0.13877207040786743, acc: 96.875, f1: 96.875, r: 0.6678825969093483
06/02/2019 12:45:40 step: 9646, epoch: 292, batch: 9, loss: 0.1612718105316162, acc: 93.75, f1: 89.85119047619048, r: 0.8145156412141373
06/02/2019 12:45:41 step: 9651, epoch: 292, batch: 14, loss: 0.11775034666061401, acc: 96.875, f1: 94.6555032269318, r: 0.6544029761169534
06/02/2019 12:45:43 step: 9656, epoch: 292, batch: 19, loss: 0.14540401101112366, acc: 95.3125, f1: 90.23809523809524, r: 0.6774799265285556
06/02/2019 12:45:44 step: 9661, epoch: 292, batch: 24, loss: 0.23722541332244873, acc: 95.3125, f1: 95.888209013209, r: 0.7199480673019019
06/02/2019 12:45:45 step: 9666, epoch: 292, batch: 29, loss: 0.12945005297660828, acc: 96.875, f1: 94.81105990783409, r: 0.7561957506122714
06/02/2019 12:45:46 *** evaluating ***
06/02/2019 12:45:46 step: 293, epoch: 292, acc: 57.26495726495726, f1: 24.662772804419085, r: 0.30658225464417577
06/02/2019 12:45:46 *** epoch: 294 ***
06/02/2019 12:45:46 *** training ***
06/02/2019 12:45:47 step: 9674, epoch: 293, batch: 4, loss: 0.18734601140022278, acc: 93.75, f1: 79.68759726112667, r: 0.735112327468593
06/02/2019 12:45:48 step: 9679, epoch: 293, batch: 9, loss: 0.14093856513500214, acc: 96.875, f1: 94.67451938040173, r: 0.6693481367894011
06/02/2019 12:45:49 step: 9684, epoch: 293, batch: 14, loss: 0.1899614930152893, acc: 93.75, f1: 91.40246796392975, r: 0.6090161481498403
06/02/2019 12:45:50 step: 9689, epoch: 293, batch: 19, loss: 0.22732122242450714, acc: 93.75, f1: 91.94238400488402, r: 0.7487408800911964
06/02/2019 12:45:51 step: 9694, epoch: 293, batch: 24, loss: 0.23160876333713531, acc: 95.3125, f1: 96.58415214067865, r: 0.6974221143230204
06/02/2019 12:45:52 step: 9699, epoch: 293, batch: 29, loss: 0.2622629702091217, acc: 90.625, f1: 86.20039682539684, r: 0.755423364022801
06/02/2019 12:45:53 *** evaluating ***
06/02/2019 12:45:53 step: 294, epoch: 293, acc: 58.119658119658126, f1: 28.11978616583245, r: 0.3056626896790765
06/02/2019 12:45:53 *** epoch: 295 ***
06/02/2019 12:45:53 *** training ***
06/02/2019 12:45:54 step: 9707, epoch: 294, batch: 4, loss: 0.14203356206417084, acc: 93.75, f1: 88.8904824602652, r: 0.6225133301179913
06/02/2019 12:45:55 step: 9712, epoch: 294, batch: 9, loss: 0.2218930572271347, acc: 92.1875, f1: 80.94148414514548, r: 0.7240052517496888
06/02/2019 12:45:56 step: 9717, epoch: 294, batch: 14, loss: 0.1580246090888977, acc: 95.3125, f1: 92.49299719887956, r: 0.669319452727939
06/02/2019 12:45:57 step: 9722, epoch: 294, batch: 19, loss: 0.2427782416343689, acc: 92.1875, f1: 78.19384972917582, r: 0.6439979701960418
06/02/2019 12:45:58 step: 9727, epoch: 294, batch: 24, loss: 0.20422030985355377, acc: 95.3125, f1: 95.76138147566719, r: 0.6817269016306549
06/02/2019 12:45:59 step: 9732, epoch: 294, batch: 29, loss: 0.24680650234222412, acc: 92.1875, f1: 85.63595135023706, r: 0.6024624537243297
06/02/2019 12:46:00 *** evaluating ***
06/02/2019 12:46:00 step: 295, epoch: 294, acc: 58.119658119658126, f1: 26.31537738803633, r: 0.3062471728247616
06/02/2019 12:46:00 *** epoch: 296 ***
06/02/2019 12:46:00 *** training ***
06/02/2019 12:46:01 step: 9740, epoch: 295, batch: 4, loss: 0.21792401373386383, acc: 90.625, f1: 88.1032677000419, r: 0.7081951978730309
06/02/2019 12:46:02 step: 9745, epoch: 295, batch: 9, loss: 0.17444218695163727, acc: 95.3125, f1: 91.22248427672956, r: 0.725986507491364
06/02/2019 12:46:03 step: 9750, epoch: 295, batch: 14, loss: 0.11763060092926025, acc: 98.4375, f1: 97.95321637426902, r: 0.800339653299587
06/02/2019 12:46:04 step: 9755, epoch: 295, batch: 19, loss: 0.12566842138767242, acc: 96.875, f1: 96.87222715173024, r: 0.7438619455396664
06/02/2019 12:46:06 step: 9760, epoch: 295, batch: 24, loss: 0.17668062448501587, acc: 92.1875, f1: 78.62778730703259, r: 0.7354271467189379
06/02/2019 12:46:07 step: 9765, epoch: 295, batch: 29, loss: 0.20545294880867004, acc: 93.75, f1: 92.45957003491051, r: 0.6587954733433494
06/02/2019 12:46:07 *** evaluating ***
06/02/2019 12:46:08 step: 296, epoch: 295, acc: 57.26495726495726, f1: 26.56105895915678, r: 0.3064464084576787
06/02/2019 12:46:08 *** epoch: 297 ***
06/02/2019 12:46:08 *** training ***
06/02/2019 12:46:09 step: 9773, epoch: 296, batch: 4, loss: 0.18876735866069794, acc: 92.1875, f1: 90.9252652109795, r: 0.6561374325365888
06/02/2019 12:46:10 step: 9778, epoch: 296, batch: 9, loss: 0.19766606390476227, acc: 95.3125, f1: 94.50852004969904, r: 0.6810184649392135
06/02/2019 12:46:11 step: 9783, epoch: 296, batch: 14, loss: 0.17273417115211487, acc: 95.3125, f1: 94.57554777199617, r: 0.6717831473479048
06/02/2019 12:46:12 step: 9788, epoch: 296, batch: 19, loss: 0.22542297840118408, acc: 90.625, f1: 76.12318840579711, r: 0.7886292845598873
06/02/2019 12:46:13 step: 9793, epoch: 296, batch: 24, loss: 0.2935827076435089, acc: 87.5, f1: 82.54827921027079, r: 0.6960752041554232
06/02/2019 12:46:14 step: 9798, epoch: 296, batch: 29, loss: 0.41654103994369507, acc: 93.75, f1: 91.93692881192881, r: 0.75067013072556
06/02/2019 12:46:14 *** evaluating ***
06/02/2019 12:46:15 step: 297, epoch: 296, acc: 57.692307692307686, f1: 27.358331177332985, r: 0.3023001411069285
06/02/2019 12:46:15 *** epoch: 298 ***
06/02/2019 12:46:15 *** training ***
06/02/2019 12:46:15 step: 9806, epoch: 297, batch: 4, loss: 0.22656655311584473, acc: 93.75, f1: 89.69729827414837, r: 0.5510696756563676
06/02/2019 12:46:17 step: 9811, epoch: 297, batch: 9, loss: 0.26811516284942627, acc: 87.5, f1: 65.35815083609202, r: 0.6052694259195973
06/02/2019 12:46:18 step: 9816, epoch: 297, batch: 14, loss: 0.14460885524749756, acc: 95.3125, f1: 76.73701298701299, r: 0.6621156638063157
06/02/2019 12:46:19 step: 9821, epoch: 297, batch: 19, loss: 0.5376717448234558, acc: 95.3125, f1: 89.83508554937127, r: 0.6319949741964592
06/02/2019 12:46:20 step: 9826, epoch: 297, batch: 24, loss: 0.20111532509326935, acc: 89.0625, f1: 72.78571428571428, r: 0.6405693403677329
06/02/2019 12:46:21 step: 9831, epoch: 297, batch: 29, loss: 0.22119028866291046, acc: 95.3125, f1: 97.56426006426007, r: 0.7467297175983323
06/02/2019 12:46:22 *** evaluating ***
06/02/2019 12:46:22 step: 298, epoch: 297, acc: 58.54700854700855, f1: 28.071128206590867, r: 0.3065983836469285
06/02/2019 12:46:22 *** epoch: 299 ***
06/02/2019 12:46:22 *** training ***
06/02/2019 12:46:23 step: 9839, epoch: 298, batch: 4, loss: 0.20754943788051605, acc: 95.3125, f1: 76.23073383942949, r: 0.6249403545432987
06/02/2019 12:46:24 step: 9844, epoch: 298, batch: 9, loss: 0.1617930829524994, acc: 93.75, f1: 95.8838920183458, r: 0.6451435972419823
06/02/2019 12:46:25 step: 9849, epoch: 298, batch: 14, loss: 0.4824431240558624, acc: 93.75, f1: 78.7972334682861, r: 0.7782422955475347
06/02/2019 12:46:26 step: 9854, epoch: 298, batch: 19, loss: 0.19914628565311432, acc: 93.75, f1: 90.95257058970515, r: 0.7281283716574012
06/02/2019 12:46:27 step: 9859, epoch: 298, batch: 24, loss: 0.11883477866649628, acc: 96.875, f1: 91.32034632034632, r: 0.7719262353479063
06/02/2019 12:46:28 step: 9864, epoch: 298, batch: 29, loss: 0.14594131708145142, acc: 92.1875, f1: 75.70320713177857, r: 0.5628870253679333
06/02/2019 12:46:29 *** evaluating ***
06/02/2019 12:46:29 step: 299, epoch: 298, acc: 56.41025641025641, f1: 24.335901433001684, r: 0.3036256532280819
06/02/2019 12:46:29 *** epoch: 300 ***
06/02/2019 12:46:29 *** training ***
06/02/2019 12:46:30 step: 9872, epoch: 299, batch: 4, loss: 0.23293954133987427, acc: 93.75, f1: 90.17111964122833, r: 0.7576978056078869
06/02/2019 12:46:31 step: 9877, epoch: 299, batch: 9, loss: 0.19182652235031128, acc: 92.1875, f1: 79.22608004240656, r: 0.6456313932888718
06/02/2019 12:46:32 step: 9882, epoch: 299, batch: 14, loss: 0.1514672338962555, acc: 95.3125, f1: 79.95587027914614, r: 0.792819296198461
06/02/2019 12:46:33 step: 9887, epoch: 299, batch: 19, loss: 0.20112855732440948, acc: 90.625, f1: 76.22599039615847, r: 0.5794768054439066
06/02/2019 12:46:34 step: 9892, epoch: 299, batch: 24, loss: 0.10125048458576202, acc: 98.4375, f1: 80.0, r: 0.5838416864643303
06/02/2019 12:46:35 step: 9897, epoch: 299, batch: 29, loss: 0.17485032975673676, acc: 95.3125, f1: 91.69322344322343, r: 0.7643179309375326
06/02/2019 12:46:36 *** evaluating ***
06/02/2019 12:46:36 step: 300, epoch: 299, acc: 57.26495726495726, f1: 26.349828182639172, r: 0.3073711054580966
06/02/2019 12:46:36 
*** Best acc model ***
epoch: 270
acc: 59.401709401709404
f1: 28.855596673986163
corr: 0.3106536802460977
06/02/2019 12:46:36 Loading Test Data
06/02/2019 12:46:36 load data from data/word2vec_temp/test_text.npy, data/word2vec_temp/test_label.npy, training: False
06/02/2019 12:46:57 loaded. total len: 2228
06/02/2019 12:46:57 Test: length: 2228, total batch: 35, batch size: 64
06/02/2019 12:46:57 
*** Test Result ***
acc: 57.26495726495726
f1: 26.349828182639172
corr: 0.3073711054580966
