06/02/2019 09:27:01 {'input_path': 'data/elmo_temp', 'output_path': 'save/mlp_3', 'gpu': True, 'seed': 20000125, 'display_per_batch': 5, 'optimizer': 'adagrad', 'lr': 0.01, 'lr_decay': 0, 'weight_decay': 0.0001, 'momentum': 0.985, 'num_epochs': 300, 'batch_size': 64, 'num_labels': 8, 'embedding_size': 1024, 'type': 'mlp', 'mlp': {'max_length': 512, 'dropout': 0.5, 'hidden_size': 512, 'loss': 'cross_entropy'}}
06/02/2019 09:27:01 Loading Train Data
06/02/2019 09:27:01 load data from data/elmo_temp/train_text.npy, data/elmo_temp/train_label.npy, training: True
06/02/2019 09:27:44 loaded. total len: 2342
06/02/2019 09:27:44 Train: length: 2108, total batch: 33, batch size: 64
06/02/2019 09:27:44 Dev: length: 234, total batch: 4, batch size: 64
06/02/2019 09:27:44 Loading model mlp
06/02/2019 09:28:01 *** epoch: 1 ***
06/02/2019 09:28:01 *** training ***
06/02/2019 09:28:02 step: 5, epoch: 0, batch: 4, loss: 74.61909484863281, acc: 34.375, f1: 9.981684981684982, r: -0.016375793034665517
06/02/2019 09:28:03 step: 10, epoch: 0, batch: 9, loss: 4.302999019622803, acc: 26.5625, f1: 14.523329493087559, r: 0.06483857653810199
06/02/2019 09:28:04 step: 15, epoch: 0, batch: 14, loss: 1.8304497003555298, acc: 25.0, f1: 20.17482517482517, r: 0.09590174373829562
06/02/2019 09:28:05 step: 20, epoch: 0, batch: 19, loss: 1.9543101787567139, acc: 43.75, f1: 22.915269394142634, r: 0.21909827139508434
06/02/2019 09:28:05 step: 25, epoch: 0, batch: 24, loss: 2.0739896297454834, acc: 31.25, f1: 8.776371308016877, r: 0.02073087145610046
06/02/2019 09:28:06 step: 30, epoch: 0, batch: 29, loss: 2.301867723464966, acc: 28.125, f1: 12.713675213675216, r: 0.0991844074819297
06/02/2019 09:28:07 *** evaluating ***
06/02/2019 09:28:07 step: 1, epoch: 0, acc: 44.871794871794876, f1: 16.010966667758513, r: 0.178079828027843
06/02/2019 09:28:07 *** epoch: 2 ***
06/02/2019 09:28:07 *** training ***
06/02/2019 09:28:08 step: 38, epoch: 1, batch: 4, loss: 1.6413244009017944, acc: 40.625, f1: 19.37612358376186, r: 0.21025227838366262
06/02/2019 09:28:09 step: 43, epoch: 1, batch: 9, loss: 1.724919080734253, acc: 40.625, f1: 29.40191387559809, r: 0.31935036537663186
06/02/2019 09:28:10 step: 48, epoch: 1, batch: 14, loss: 1.3310543298721313, acc: 59.375, f1: 42.537766280221014, r: 0.383408255136536
06/02/2019 09:28:11 step: 53, epoch: 1, batch: 19, loss: 1.8821375370025635, acc: 32.8125, f1: 18.706018518518515, r: 0.1850838422379713
06/02/2019 09:28:11 step: 58, epoch: 1, batch: 24, loss: 1.5954540967941284, acc: 43.75, f1: 20.628787878787875, r: 0.23942365964343515
06/02/2019 09:28:12 step: 63, epoch: 1, batch: 29, loss: 1.7171579599380493, acc: 32.8125, f1: 23.890128251030504, r: 0.3732745461747145
06/02/2019 09:28:13 *** evaluating ***
06/02/2019 09:28:13 step: 2, epoch: 1, acc: 46.15384615384615, f1: 14.877462913643082, r: 0.22511165388379736
06/02/2019 09:28:13 *** epoch: 3 ***
06/02/2019 09:28:13 *** training ***
06/02/2019 09:28:14 step: 71, epoch: 2, batch: 4, loss: 1.4731190204620361, acc: 48.4375, f1: 30.272341813437702, r: 0.3707454282883269
06/02/2019 09:28:15 step: 76, epoch: 2, batch: 9, loss: 1.7820560932159424, acc: 53.125, f1: 35.05028805780685, r: 0.3306392705602678
06/02/2019 09:28:16 step: 81, epoch: 2, batch: 14, loss: 1.5493314266204834, acc: 46.875, f1: 27.80210371819961, r: 0.3922637913900612
06/02/2019 09:28:17 step: 86, epoch: 2, batch: 19, loss: 1.3332589864730835, acc: 54.6875, f1: 32.469474969474966, r: 0.39742611318658644
06/02/2019 09:28:17 step: 91, epoch: 2, batch: 24, loss: 1.3402099609375, acc: 59.375, f1: 33.45743360635393, r: 0.29244530044408235
06/02/2019 09:28:18 step: 96, epoch: 2, batch: 29, loss: 1.3799941539764404, acc: 53.125, f1: 26.438598321509716, r: 0.3002229261614634
06/02/2019 09:28:19 *** evaluating ***
06/02/2019 09:28:19 step: 3, epoch: 2, acc: 54.27350427350427, f1: 19.760754945875327, r: 0.23483994881123432
06/02/2019 09:28:19 *** epoch: 4 ***
06/02/2019 09:28:19 *** training ***
06/02/2019 09:28:20 step: 104, epoch: 3, batch: 4, loss: 1.2510048151016235, acc: 51.5625, f1: 47.594537815126046, r: 0.38348175454887057
06/02/2019 09:28:21 step: 109, epoch: 3, batch: 9, loss: 1.4402166604995728, acc: 48.4375, f1: 36.69973918421124, r: 0.3188784413958802
06/02/2019 09:28:21 step: 114, epoch: 3, batch: 14, loss: 0.9315643310546875, acc: 67.1875, f1: 38.395931142410014, r: 0.4080436852466537
06/02/2019 09:28:22 step: 119, epoch: 3, batch: 19, loss: 1.2328155040740967, acc: 46.875, f1: 21.936274509803923, r: 0.39724211849736146
06/02/2019 09:28:23 step: 124, epoch: 3, batch: 24, loss: 1.469435691833496, acc: 46.875, f1: 31.25, r: 0.3572096543901809
06/02/2019 09:28:24 step: 129, epoch: 3, batch: 29, loss: 1.1499090194702148, acc: 53.125, f1: 30.604395604395613, r: 0.337313308692406
06/02/2019 09:28:24 *** evaluating ***
06/02/2019 09:28:25 step: 4, epoch: 3, acc: 53.41880341880342, f1: 17.68851099967648, r: 0.2685401810402085
06/02/2019 09:28:25 *** epoch: 5 ***
06/02/2019 09:28:25 *** training ***
06/02/2019 09:28:25 step: 137, epoch: 4, batch: 4, loss: 1.1712604761123657, acc: 59.375, f1: 30.854776693286013, r: 0.32827223207094075
06/02/2019 09:28:26 step: 142, epoch: 4, batch: 9, loss: 1.2752571105957031, acc: 53.125, f1: 31.301492111087313, r: 0.37792059038464426
06/02/2019 09:28:27 step: 147, epoch: 4, batch: 14, loss: 1.1671332120895386, acc: 54.6875, f1: 36.68478260869565, r: 0.42945685474731027
06/02/2019 09:28:28 step: 152, epoch: 4, batch: 19, loss: 1.1273691654205322, acc: 62.5, f1: 34.25038402457757, r: 0.3929258086131342
06/02/2019 09:28:29 step: 157, epoch: 4, batch: 24, loss: 0.9440720081329346, acc: 62.5, f1: 44.20933757318197, r: 0.47914990488248005
06/02/2019 09:28:29 step: 162, epoch: 4, batch: 29, loss: 1.2145969867706299, acc: 53.125, f1: 30.577270381836946, r: 0.37777799112178473
06/02/2019 09:28:30 *** evaluating ***
06/02/2019 09:28:30 step: 5, epoch: 4, acc: 55.55555555555556, f1: 19.651133180544946, r: 0.25478589065117146
06/02/2019 09:28:30 *** epoch: 6 ***
06/02/2019 09:28:30 *** training ***
06/02/2019 09:28:31 step: 170, epoch: 5, batch: 4, loss: 1.3169611692428589, acc: 57.8125, f1: 43.56532356532357, r: 0.2920083313405906
06/02/2019 09:28:32 step: 175, epoch: 5, batch: 9, loss: 0.8379948139190674, acc: 64.0625, f1: 50.528499278499275, r: 0.6016352556584322
06/02/2019 09:28:32 step: 180, epoch: 5, batch: 14, loss: 1.1945263147354126, acc: 56.25, f1: 31.987661461345674, r: 0.41035677245053753
06/02/2019 09:28:33 step: 185, epoch: 5, batch: 19, loss: 1.089735746383667, acc: 64.0625, f1: 35.208442351299496, r: 0.459315527657066
06/02/2019 09:28:34 step: 190, epoch: 5, batch: 24, loss: 1.2343947887420654, acc: 51.5625, f1: 27.310510225595614, r: 0.44007553845901076
06/02/2019 09:28:35 step: 195, epoch: 5, batch: 29, loss: 0.8475874662399292, acc: 62.5, f1: 51.104946539238604, r: 0.5002792636551343
06/02/2019 09:28:35 *** evaluating ***
06/02/2019 09:28:35 step: 6, epoch: 5, acc: 55.12820512820513, f1: 21.111174199409493, r: 0.258670587231301
06/02/2019 09:28:35 *** epoch: 7 ***
06/02/2019 09:28:35 *** training ***
06/02/2019 09:28:36 step: 203, epoch: 6, batch: 4, loss: 0.8381931185722351, acc: 64.0625, f1: 32.23739495798319, r: 0.5179196385060517
06/02/2019 09:28:37 step: 208, epoch: 6, batch: 9, loss: 1.3025082349777222, acc: 54.6875, f1: 42.845169965913, r: 0.38731880405640057
06/02/2019 09:28:38 step: 213, epoch: 6, batch: 14, loss: 0.9322023391723633, acc: 62.5, f1: 57.95358992403327, r: 0.49159642446846463
06/02/2019 09:28:38 step: 218, epoch: 6, batch: 19, loss: 0.7713670134544373, acc: 65.625, f1: 49.08812755871579, r: 0.45290400252420365
06/02/2019 09:28:39 step: 223, epoch: 6, batch: 24, loss: 1.2482037544250488, acc: 42.1875, f1: 20.583333333333336, r: 0.45641851257990823
06/02/2019 09:28:40 step: 228, epoch: 6, batch: 29, loss: 0.7898284196853638, acc: 70.3125, f1: 46.36904761904762, r: 0.515087380655869
06/02/2019 09:28:40 *** evaluating ***
06/02/2019 09:28:41 step: 7, epoch: 6, acc: 54.27350427350427, f1: 20.480051863030585, r: 0.260313471083161
06/02/2019 09:28:41 *** epoch: 8 ***
06/02/2019 09:28:41 *** training ***
06/02/2019 09:28:41 step: 236, epoch: 7, batch: 4, loss: 1.1202223300933838, acc: 46.875, f1: 27.619731800766285, r: 0.47102862695827186
06/02/2019 09:28:42 step: 241, epoch: 7, batch: 9, loss: 0.8417279124259949, acc: 59.375, f1: 31.980761482675256, r: 0.39931168865967304
06/02/2019 09:28:43 step: 246, epoch: 7, batch: 14, loss: 0.9044668078422546, acc: 68.75, f1: 46.44757094757095, r: 0.4975514822660427
06/02/2019 09:28:44 step: 251, epoch: 7, batch: 19, loss: 0.9107949733734131, acc: 65.625, f1: 31.580828445747798, r: 0.41035110678373626
06/02/2019 09:28:44 step: 256, epoch: 7, batch: 24, loss: 0.9722126722335815, acc: 62.5, f1: 53.333333333333336, r: 0.488699411168653
06/02/2019 09:28:45 step: 261, epoch: 7, batch: 29, loss: 0.9344900846481323, acc: 57.8125, f1: 43.4126804567981, r: 0.4898996908653642
06/02/2019 09:28:45 *** evaluating ***
06/02/2019 09:28:45 step: 8, epoch: 7, acc: 57.692307692307686, f1: 20.702320207952248, r: 0.2750646854131767
06/02/2019 09:28:45 *** epoch: 9 ***
06/02/2019 09:28:45 *** training ***
06/02/2019 09:28:46 step: 269, epoch: 8, batch: 4, loss: 0.6500445008277893, acc: 70.3125, f1: 51.56046689628779, r: 0.5911172698520957
06/02/2019 09:28:46 step: 274, epoch: 8, batch: 9, loss: 0.8253273367881775, acc: 67.1875, f1: 40.703476664297554, r: 0.5589839215983401
06/02/2019 09:28:47 step: 279, epoch: 8, batch: 14, loss: 0.7725535035133362, acc: 65.625, f1: 45.53351544876969, r: 0.45584359078919595
06/02/2019 09:28:47 step: 284, epoch: 8, batch: 19, loss: 0.7887141108512878, acc: 68.75, f1: 37.21980595516895, r: 0.46096646216283177
06/02/2019 09:28:48 step: 289, epoch: 8, batch: 24, loss: 0.8882710337638855, acc: 57.8125, f1: 55.28599275967697, r: 0.5086435402339116
06/02/2019 09:28:48 step: 294, epoch: 8, batch: 29, loss: 0.787352979183197, acc: 64.0625, f1: 58.83242517570876, r: 0.5254308530672053
06/02/2019 09:28:49 *** evaluating ***
06/02/2019 09:28:49 step: 9, epoch: 8, acc: 55.98290598290598, f1: 19.91195867240029, r: 0.28347625119509123
06/02/2019 09:28:49 *** epoch: 10 ***
06/02/2019 09:28:49 *** training ***
06/02/2019 09:28:49 step: 302, epoch: 9, batch: 4, loss: 0.7870379686355591, acc: 65.625, f1: 42.22990777338603, r: 0.5084152110838456
06/02/2019 09:28:50 step: 307, epoch: 9, batch: 9, loss: 0.8013720512390137, acc: 73.4375, f1: 50.990107310529844, r: 0.5780198383975973
06/02/2019 09:28:50 step: 312, epoch: 9, batch: 14, loss: 0.934473991394043, acc: 54.6875, f1: 34.33704974271012, r: 0.5501366131760772
06/02/2019 09:28:51 step: 317, epoch: 9, batch: 19, loss: 0.8395674228668213, acc: 62.5, f1: 58.639948733116434, r: 0.5084527272211984
06/02/2019 09:28:51 step: 322, epoch: 9, batch: 24, loss: 0.8484892249107361, acc: 62.5, f1: 45.18829754123872, r: 0.45713400183687525
06/02/2019 09:28:52 step: 327, epoch: 9, batch: 29, loss: 0.7405005097389221, acc: 65.625, f1: 37.12175133864438, r: 0.5098679882832475
06/02/2019 09:28:52 *** evaluating ***
06/02/2019 09:28:52 step: 10, epoch: 9, acc: 55.12820512820513, f1: 17.923939730266035, r: 0.27941213605467113
06/02/2019 09:28:52 *** epoch: 11 ***
06/02/2019 09:28:52 *** training ***
06/02/2019 09:28:53 step: 335, epoch: 10, batch: 4, loss: 0.6695073246955872, acc: 68.75, f1: 57.34755440637793, r: 0.575449348848088
06/02/2019 09:28:53 step: 340, epoch: 10, batch: 9, loss: 0.8221909403800964, acc: 64.0625, f1: 39.58198457589387, r: 0.5325440432398343
06/02/2019 09:28:54 step: 345, epoch: 10, batch: 14, loss: 0.7588530778884888, acc: 65.625, f1: 55.45808966861598, r: 0.6046476485732966
06/02/2019 09:28:54 step: 350, epoch: 10, batch: 19, loss: 0.6044914126396179, acc: 75.0, f1: 59.26655719759169, r: 0.44927681163012434
06/02/2019 09:28:55 step: 355, epoch: 10, batch: 24, loss: 0.7413715720176697, acc: 68.75, f1: 63.61961451247166, r: 0.4680473371866483
06/02/2019 09:28:55 step: 360, epoch: 10, batch: 29, loss: 0.7681535482406616, acc: 65.625, f1: 38.566335625159155, r: 0.4322308663563483
06/02/2019 09:28:56 *** evaluating ***
06/02/2019 09:28:56 step: 11, epoch: 10, acc: 57.26495726495726, f1: 22.829398958422264, r: 0.2743805538076795
06/02/2019 09:28:56 *** epoch: 12 ***
06/02/2019 09:28:56 *** training ***
06/02/2019 09:28:56 step: 368, epoch: 11, batch: 4, loss: 0.6194444894790649, acc: 75.0, f1: 45.76710451710452, r: 0.5005307337601487
06/02/2019 09:28:57 step: 373, epoch: 11, batch: 9, loss: 0.48213309049606323, acc: 76.5625, f1: 61.927203065134094, r: 0.5913310356508291
06/02/2019 09:28:57 step: 378, epoch: 11, batch: 14, loss: 0.8408433794975281, acc: 64.0625, f1: 43.72825091575092, r: 0.4599119673558887
06/02/2019 09:28:58 step: 383, epoch: 11, batch: 19, loss: 0.850020706653595, acc: 62.5, f1: 41.45522388059702, r: 0.49014528708369476
06/02/2019 09:28:58 step: 388, epoch: 11, batch: 24, loss: 0.5975382924079895, acc: 68.75, f1: 40.59814323607427, r: 0.5290351003534816
06/02/2019 09:28:59 step: 393, epoch: 11, batch: 29, loss: 0.5580301880836487, acc: 71.875, f1: 45.07167004732927, r: 0.5736259505151087
06/02/2019 09:28:59 *** evaluating ***
06/02/2019 09:29:00 step: 12, epoch: 11, acc: 56.837606837606835, f1: 22.432853027718025, r: 0.2741578834480972
06/02/2019 09:29:00 *** epoch: 13 ***
06/02/2019 09:29:00 *** training ***
06/02/2019 09:29:00 step: 401, epoch: 12, batch: 4, loss: 0.7838071584701538, acc: 67.1875, f1: 45.22076206369073, r: 0.5575427532409329
06/02/2019 09:29:00 step: 406, epoch: 12, batch: 9, loss: 0.6968266367912292, acc: 73.4375, f1: 56.81476418318523, r: 0.4982658350091224
06/02/2019 09:29:01 step: 411, epoch: 12, batch: 14, loss: 0.5774634480476379, acc: 70.3125, f1: 34.10702503607138, r: 0.5195943672048279
06/02/2019 09:29:01 step: 416, epoch: 12, batch: 19, loss: 0.8005721569061279, acc: 64.0625, f1: 42.75076029108288, r: 0.5781396250108668
06/02/2019 09:29:02 step: 421, epoch: 12, batch: 24, loss: 0.6927381157875061, acc: 65.625, f1: 54.661918726434855, r: 0.5625205885136271
06/02/2019 09:29:02 step: 426, epoch: 12, batch: 29, loss: 0.5998790860176086, acc: 75.0, f1: 38.58276643990929, r: 0.5323188804503519
06/02/2019 09:29:03 *** evaluating ***
06/02/2019 09:29:03 step: 13, epoch: 12, acc: 57.26495726495726, f1: 20.332252555178286, r: 0.29378297667218994
06/02/2019 09:29:03 *** epoch: 14 ***
06/02/2019 09:29:03 *** training ***
06/02/2019 09:29:03 step: 434, epoch: 13, batch: 4, loss: 0.782019317150116, acc: 67.1875, f1: 43.82733828298345, r: 0.5713366906744555
06/02/2019 09:29:04 step: 439, epoch: 13, batch: 9, loss: 0.746275007724762, acc: 65.625, f1: 32.82083254909342, r: 0.567221692137923
06/02/2019 09:29:04 step: 444, epoch: 13, batch: 14, loss: 0.6446911692619324, acc: 67.1875, f1: 50.375740160222925, r: 0.6510905107100388
06/02/2019 09:29:05 step: 449, epoch: 13, batch: 19, loss: 0.770603597164154, acc: 67.1875, f1: 58.21581196581197, r: 0.6269115466314785
06/02/2019 09:29:05 step: 454, epoch: 13, batch: 24, loss: 0.7185779213905334, acc: 68.75, f1: 63.4920634920635, r: 0.5435757722724122
06/02/2019 09:29:06 step: 459, epoch: 13, batch: 29, loss: 0.4644003212451935, acc: 87.5, f1: 71.34929852699887, r: 0.5287623612805045
06/02/2019 09:29:06 *** evaluating ***
06/02/2019 09:29:06 step: 14, epoch: 13, acc: 55.98290598290598, f1: 20.389761233346128, r: 0.28987516629477106
06/02/2019 09:29:06 *** epoch: 15 ***
06/02/2019 09:29:06 *** training ***
06/02/2019 09:29:06 step: 467, epoch: 14, batch: 4, loss: 0.593978762626648, acc: 71.875, f1: 60.5562224889956, r: 0.5387551086616339
06/02/2019 09:29:07 step: 472, epoch: 14, batch: 9, loss: 0.5884684324264526, acc: 73.4375, f1: 52.94393106893107, r: 0.5464258756378395
06/02/2019 09:29:07 step: 477, epoch: 14, batch: 14, loss: 0.4691210687160492, acc: 75.0, f1: 65.01424292438413, r: 0.6603782363790421
06/02/2019 09:29:08 step: 482, epoch: 14, batch: 19, loss: 0.7175335884094238, acc: 60.9375, f1: 31.68145402815214, r: 0.5005590611452595
06/02/2019 09:29:08 step: 487, epoch: 14, batch: 24, loss: 0.5969154238700867, acc: 71.875, f1: 48.02304964539007, r: 0.5658185884420145
06/02/2019 09:29:09 step: 492, epoch: 14, batch: 29, loss: 0.45942407846450806, acc: 79.6875, f1: 54.79068550497123, r: 0.5627648073280256
06/02/2019 09:29:09 *** evaluating ***
06/02/2019 09:29:09 step: 15, epoch: 14, acc: 58.54700854700855, f1: 20.7760989010989, r: 0.2859560074291252
06/02/2019 09:29:09 *** epoch: 16 ***
06/02/2019 09:29:09 *** training ***
06/02/2019 09:29:10 step: 500, epoch: 15, batch: 4, loss: 0.5540705919265747, acc: 75.0, f1: 56.296992481203, r: 0.4255715358456669
06/02/2019 09:29:10 step: 505, epoch: 15, batch: 9, loss: 0.6969099640846252, acc: 67.1875, f1: 47.4121938006931, r: 0.5450469838185846
06/02/2019 09:29:11 step: 510, epoch: 15, batch: 14, loss: 0.5805447101593018, acc: 68.75, f1: 45.42700501253133, r: 0.5557769741787957
06/02/2019 09:29:11 step: 515, epoch: 15, batch: 19, loss: 0.6194927096366882, acc: 71.875, f1: 66.08083583083582, r: 0.6793287922598561
06/02/2019 09:29:12 step: 520, epoch: 15, batch: 24, loss: 0.6100731492042542, acc: 70.3125, f1: 53.93668831168831, r: 0.5855414681507795
06/02/2019 09:29:12 step: 525, epoch: 15, batch: 29, loss: 0.6341330409049988, acc: 73.4375, f1: 66.90995707389149, r: 0.6316899433107893
06/02/2019 09:29:13 *** evaluating ***
06/02/2019 09:29:13 step: 16, epoch: 15, acc: 57.692307692307686, f1: 23.275833771008404, r: 0.2753004279829321
06/02/2019 09:29:13 *** epoch: 17 ***
06/02/2019 09:29:13 *** training ***
06/02/2019 09:29:13 step: 533, epoch: 16, batch: 4, loss: 0.5542251467704773, acc: 78.125, f1: 56.72949735449735, r: 0.5905984472585087
06/02/2019 09:29:14 step: 538, epoch: 16, batch: 9, loss: 0.5339601039886475, acc: 76.5625, f1: 45.327494982667396, r: 0.48533960712783164
06/02/2019 09:29:14 step: 543, epoch: 16, batch: 14, loss: 0.7089641690254211, acc: 65.625, f1: 55.978809837505494, r: 0.5039058700452497
06/02/2019 09:29:15 step: 548, epoch: 16, batch: 19, loss: 0.5989586710929871, acc: 71.875, f1: 55.7204700061843, r: 0.5846606342178123
06/02/2019 09:29:15 step: 553, epoch: 16, batch: 24, loss: 0.4786776304244995, acc: 79.6875, f1: 54.40076030412164, r: 0.531209393332169
06/02/2019 09:29:16 step: 558, epoch: 16, batch: 29, loss: 0.716790497303009, acc: 64.0625, f1: 52.1069566687814, r: 0.4718303301460643
06/02/2019 09:29:16 *** evaluating ***
06/02/2019 09:29:17 step: 17, epoch: 16, acc: 57.26495726495726, f1: 20.655369412123466, r: 0.2798461759436893
06/02/2019 09:29:17 *** epoch: 18 ***
06/02/2019 09:29:17 *** training ***
06/02/2019 09:29:17 step: 566, epoch: 17, batch: 4, loss: 0.5770062804222107, acc: 70.3125, f1: 59.07239976401968, r: 0.6517998718913048
06/02/2019 09:29:18 step: 571, epoch: 17, batch: 9, loss: 0.49383094906806946, acc: 70.3125, f1: 55.85434173669468, r: 0.5904043885888504
06/02/2019 09:29:18 step: 576, epoch: 17, batch: 14, loss: 0.5587669014930725, acc: 67.1875, f1: 36.90573770491803, r: 0.6394796215660494
06/02/2019 09:29:19 step: 581, epoch: 17, batch: 19, loss: 0.7084330916404724, acc: 68.75, f1: 55.209021204783916, r: 0.5191925692563387
06/02/2019 09:29:19 step: 586, epoch: 17, batch: 24, loss: 0.6973433494567871, acc: 68.75, f1: 46.768837803320565, r: 0.520872285522391
06/02/2019 09:29:20 step: 591, epoch: 17, batch: 29, loss: 0.48830878734588623, acc: 79.6875, f1: 59.12762416794675, r: 0.5730934641683909
06/02/2019 09:29:20 *** evaluating ***
06/02/2019 09:29:20 step: 18, epoch: 17, acc: 56.41025641025641, f1: 20.21289410524019, r: 0.27522657272737283
06/02/2019 09:29:20 *** epoch: 19 ***
06/02/2019 09:29:20 *** training ***
06/02/2019 09:29:21 step: 599, epoch: 18, batch: 4, loss: 0.6135651469230652, acc: 71.875, f1: 49.24204564384475, r: 0.5455777392773564
06/02/2019 09:29:21 step: 604, epoch: 18, batch: 9, loss: 0.5918121337890625, acc: 75.0, f1: 73.73409744377487, r: 0.601723133320806
06/02/2019 09:29:22 step: 609, epoch: 18, batch: 14, loss: 0.5210063457489014, acc: 78.125, f1: 52.48238903196485, r: 0.5919649966375532
06/02/2019 09:29:22 step: 614, epoch: 18, batch: 19, loss: 0.7630048990249634, acc: 62.5, f1: 46.55751010402173, r: 0.5896661622976732
06/02/2019 09:29:23 step: 619, epoch: 18, batch: 24, loss: 0.4435880184173584, acc: 76.5625, f1: 63.28256302521008, r: 0.6148025104878785
06/02/2019 09:29:23 step: 624, epoch: 18, batch: 29, loss: 0.5721042156219482, acc: 76.5625, f1: 62.98400714800971, r: 0.6982829515427904
06/02/2019 09:29:24 *** evaluating ***
06/02/2019 09:29:24 step: 19, epoch: 18, acc: 55.12820512820513, f1: 20.167517977787746, r: 0.28327063311194023
06/02/2019 09:29:24 *** epoch: 20 ***
06/02/2019 09:29:24 *** training ***
06/02/2019 09:29:24 step: 632, epoch: 19, batch: 4, loss: 0.851111888885498, acc: 57.8125, f1: 45.537655279503106, r: 0.534440430744272
06/02/2019 09:29:25 step: 637, epoch: 19, batch: 9, loss: 0.528805136680603, acc: 73.4375, f1: 52.759372979961206, r: 0.5100534911186463
06/02/2019 09:29:25 step: 642, epoch: 19, batch: 14, loss: 0.5245628952980042, acc: 79.6875, f1: 64.45657130395132, r: 0.6149415104157411
06/02/2019 09:29:26 step: 647, epoch: 19, batch: 19, loss: 0.4625287652015686, acc: 81.25, f1: 81.15000941087898, r: 0.6497798588892248
06/02/2019 09:29:26 step: 652, epoch: 19, batch: 24, loss: 0.6566304564476013, acc: 68.75, f1: 51.670843776106935, r: 0.6243706953359165
06/02/2019 09:29:27 step: 657, epoch: 19, batch: 29, loss: 0.5892773866653442, acc: 68.75, f1: 39.788245758073344, r: 0.5189283032467031
06/02/2019 09:29:27 *** evaluating ***
06/02/2019 09:29:27 step: 20, epoch: 19, acc: 58.119658119658126, f1: 20.591265924051104, r: 0.2876476719894495
06/02/2019 09:29:27 *** epoch: 21 ***
06/02/2019 09:29:27 *** training ***
06/02/2019 09:29:28 step: 665, epoch: 20, batch: 4, loss: 0.47462695837020874, acc: 79.6875, f1: 50.06058920743438, r: 0.5345009750136245
06/02/2019 09:29:28 step: 670, epoch: 20, batch: 9, loss: 0.51221764087677, acc: 78.125, f1: 53.86385836385836, r: 0.6363984468517426
06/02/2019 09:29:29 step: 675, epoch: 20, batch: 14, loss: 0.49520328640937805, acc: 81.25, f1: 67.4664565012997, r: 0.6416748953254632
06/02/2019 09:29:29 step: 680, epoch: 20, batch: 19, loss: 0.5005543828010559, acc: 78.125, f1: 58.31946584488957, r: 0.6226194778313778
06/02/2019 09:29:30 step: 685, epoch: 20, batch: 24, loss: 0.49283623695373535, acc: 76.5625, f1: 49.31318681318681, r: 0.4871717893601629
06/02/2019 09:29:30 step: 690, epoch: 20, batch: 29, loss: 0.41943180561065674, acc: 85.9375, f1: 84.39047125202168, r: 0.6394671853034877
06/02/2019 09:29:31 *** evaluating ***
06/02/2019 09:29:31 step: 21, epoch: 20, acc: 54.700854700854705, f1: 20.53324953169674, r: 0.27664401684526047
06/02/2019 09:29:31 *** epoch: 22 ***
06/02/2019 09:29:31 *** training ***
06/02/2019 09:29:32 step: 698, epoch: 21, batch: 4, loss: 0.5223690867424011, acc: 73.4375, f1: 51.4328231292517, r: 0.604993735712206
06/02/2019 09:29:32 step: 703, epoch: 21, batch: 9, loss: 0.5897029638290405, acc: 71.875, f1: 64.4446497495278, r: 0.5959409968014505
06/02/2019 09:29:33 step: 708, epoch: 21, batch: 14, loss: 0.37367621064186096, acc: 82.8125, f1: 73.06949806949807, r: 0.6882168763429921
06/02/2019 09:29:33 step: 713, epoch: 21, batch: 19, loss: 0.661273717880249, acc: 70.3125, f1: 66.38873405177752, r: 0.6044099939660696
06/02/2019 09:29:34 step: 718, epoch: 21, batch: 24, loss: 0.3531562089920044, acc: 89.0625, f1: 73.54882391828205, r: 0.5217632570670802
06/02/2019 09:29:34 step: 723, epoch: 21, batch: 29, loss: 0.46986356377601624, acc: 75.0, f1: 51.77406832298137, r: 0.5977046109014874
06/02/2019 09:29:35 *** evaluating ***
06/02/2019 09:29:35 step: 22, epoch: 21, acc: 56.41025641025641, f1: 20.093491047532602, r: 0.2722592565914187
06/02/2019 09:29:35 *** epoch: 23 ***
06/02/2019 09:29:35 *** training ***
06/02/2019 09:29:35 step: 731, epoch: 22, batch: 4, loss: 0.5152638554573059, acc: 76.5625, f1: 64.23201012823654, r: 0.6350619187491255
06/02/2019 09:29:36 step: 736, epoch: 22, batch: 9, loss: 0.5554338693618774, acc: 81.25, f1: 66.66125541125541, r: 0.6069543637513963
06/02/2019 09:29:36 step: 741, epoch: 22, batch: 14, loss: 0.5206891894340515, acc: 78.125, f1: 55.287996088063, r: 0.65045362913372
06/02/2019 09:29:37 step: 746, epoch: 22, batch: 19, loss: 0.48981204628944397, acc: 81.25, f1: 82.50087077673285, r: 0.6769549684248035
06/02/2019 09:29:37 step: 751, epoch: 22, batch: 24, loss: 0.5062093734741211, acc: 76.5625, f1: 54.830022918258216, r: 0.5202634550549579
06/02/2019 09:29:38 step: 756, epoch: 22, batch: 29, loss: 0.6669442057609558, acc: 68.75, f1: 56.658008658008654, r: 0.5226070998405234
06/02/2019 09:29:38 *** evaluating ***
06/02/2019 09:29:39 step: 23, epoch: 22, acc: 56.41025641025641, f1: 20.38255297721702, r: 0.26982478078506694
06/02/2019 09:29:39 *** epoch: 24 ***
06/02/2019 09:29:39 *** training ***
06/02/2019 09:29:39 step: 764, epoch: 23, batch: 4, loss: 0.41334041953086853, acc: 79.6875, f1: 68.18506493506494, r: 0.5326447954100783
06/02/2019 09:29:39 step: 769, epoch: 23, batch: 9, loss: 0.45586511492729187, acc: 76.5625, f1: 65.75778388278388, r: 0.6682131532532352
06/02/2019 09:29:40 step: 774, epoch: 23, batch: 14, loss: 0.4097946286201477, acc: 81.25, f1: 67.17687074829932, r: 0.5914604203789825
06/02/2019 09:29:40 step: 779, epoch: 23, batch: 19, loss: 0.4175429940223694, acc: 76.5625, f1: 58.75429168532618, r: 0.6105719410899526
06/02/2019 09:29:41 step: 784, epoch: 23, batch: 24, loss: 0.5887623429298401, acc: 67.1875, f1: 53.023809523809526, r: 0.585636138518846
06/02/2019 09:29:42 step: 789, epoch: 23, batch: 29, loss: 0.449850469827652, acc: 82.8125, f1: 73.25863678804856, r: 0.662624256679932
06/02/2019 09:29:42 *** evaluating ***
06/02/2019 09:29:42 step: 24, epoch: 23, acc: 56.41025641025641, f1: 20.73307594598676, r: 0.2656005214924641
06/02/2019 09:29:42 *** epoch: 25 ***
06/02/2019 09:29:42 *** training ***
06/02/2019 09:29:43 step: 797, epoch: 24, batch: 4, loss: 0.4228508472442627, acc: 81.25, f1: 61.04010551552739, r: 0.5887031399538868
06/02/2019 09:29:43 step: 802, epoch: 24, batch: 9, loss: 0.4313884377479553, acc: 81.25, f1: 78.982683982684, r: 0.5809144450941867
06/02/2019 09:29:44 step: 807, epoch: 24, batch: 14, loss: 0.4035117030143738, acc: 81.25, f1: 65.9735987696514, r: 0.6539243090096313
06/02/2019 09:29:44 step: 812, epoch: 24, batch: 19, loss: 0.4210120439529419, acc: 79.6875, f1: 57.486325611325604, r: 0.603660030331869
06/02/2019 09:29:45 step: 817, epoch: 24, batch: 24, loss: 0.3277864456176758, acc: 85.9375, f1: 66.68318391186261, r: 0.5687326413506089
06/02/2019 09:29:45 step: 822, epoch: 24, batch: 29, loss: 0.48853808641433716, acc: 76.5625, f1: 61.63625150674179, r: 0.6331464015572962
06/02/2019 09:29:45 *** evaluating ***
06/02/2019 09:29:46 step: 25, epoch: 24, acc: 55.55555555555556, f1: 20.48315893933424, r: 0.27040802360022137
06/02/2019 09:29:46 *** epoch: 26 ***
06/02/2019 09:29:46 *** training ***
06/02/2019 09:29:46 step: 830, epoch: 25, batch: 4, loss: 0.6014127731323242, acc: 71.875, f1: 52.32829393333023, r: 0.5720663315717244
06/02/2019 09:29:47 step: 835, epoch: 25, batch: 9, loss: 0.36767521500587463, acc: 78.125, f1: 62.43613529327814, r: 0.6047800925395087
06/02/2019 09:29:47 step: 840, epoch: 25, batch: 14, loss: 0.45159533619880676, acc: 84.375, f1: 70.21030418856506, r: 0.6000828139045346
06/02/2019 09:29:48 step: 845, epoch: 25, batch: 19, loss: 0.47894707322120667, acc: 78.125, f1: 68.22050371872888, r: 0.7105329888515124
06/02/2019 09:29:48 step: 850, epoch: 25, batch: 24, loss: 0.6228023767471313, acc: 70.3125, f1: 53.70670995670995, r: 0.557419661285174
06/02/2019 09:29:49 step: 855, epoch: 25, batch: 29, loss: 0.3987768292427063, acc: 78.125, f1: 58.81685875931401, r: 0.6276005846045025
06/02/2019 09:29:49 *** evaluating ***
06/02/2019 09:29:50 step: 26, epoch: 25, acc: 56.41025641025641, f1: 20.361939859751143, r: 0.26497889816524683
06/02/2019 09:29:50 *** epoch: 27 ***
06/02/2019 09:29:50 *** training ***
06/02/2019 09:29:50 step: 863, epoch: 26, batch: 4, loss: 0.40298622846603394, acc: 82.8125, f1: 63.41954022988505, r: 0.5688041764884031
06/02/2019 09:29:51 step: 868, epoch: 26, batch: 9, loss: 0.32302427291870117, acc: 85.9375, f1: 67.64815541601257, r: 0.6097542312468647
06/02/2019 09:29:51 step: 873, epoch: 26, batch: 14, loss: 0.3362921178340912, acc: 85.9375, f1: 78.41269841269842, r: 0.6586428653558967
06/02/2019 09:29:52 step: 878, epoch: 26, batch: 19, loss: 0.42126503586769104, acc: 75.0, f1: 53.12205910031997, r: 0.6855647522670274
06/02/2019 09:29:52 step: 883, epoch: 26, batch: 24, loss: 0.38918250799179077, acc: 79.6875, f1: 58.66459627329191, r: 0.5298145926724038
06/02/2019 09:29:53 step: 888, epoch: 26, batch: 29, loss: 0.5099261403083801, acc: 78.125, f1: 56.90841293782469, r: 0.49601283849599803
06/02/2019 09:29:53 *** evaluating ***
06/02/2019 09:29:53 step: 27, epoch: 26, acc: 55.98290598290598, f1: 20.714638824848052, r: 0.27823658086249997
06/02/2019 09:29:53 *** epoch: 28 ***
06/02/2019 09:29:53 *** training ***
06/02/2019 09:29:54 step: 896, epoch: 27, batch: 4, loss: 0.39670711755752563, acc: 79.6875, f1: 63.05182599355531, r: 0.6193413717871992
06/02/2019 09:29:54 step: 901, epoch: 27, batch: 9, loss: 0.4278701841831207, acc: 84.375, f1: 53.7212968191229, r: 0.6027405646863525
06/02/2019 09:29:55 step: 906, epoch: 27, batch: 14, loss: 0.4253285825252533, acc: 84.375, f1: 85.82585996379099, r: 0.6598400497840592
06/02/2019 09:29:55 step: 911, epoch: 27, batch: 19, loss: 0.41370710730552673, acc: 76.5625, f1: 58.79361866203971, r: 0.581406972238079
06/02/2019 09:29:56 step: 916, epoch: 27, batch: 24, loss: 0.3841180205345154, acc: 79.6875, f1: 56.6597841813359, r: 0.5964447836914251
06/02/2019 09:29:56 step: 921, epoch: 27, batch: 29, loss: 0.4295046031475067, acc: 76.5625, f1: 71.5254586683158, r: 0.6005694865808548
06/02/2019 09:29:57 *** evaluating ***
06/02/2019 09:29:57 step: 28, epoch: 27, acc: 57.692307692307686, f1: 22.001657196969695, r: 0.2690003280291499
06/02/2019 09:29:57 *** epoch: 29 ***
06/02/2019 09:29:57 *** training ***
06/02/2019 09:29:57 step: 929, epoch: 28, batch: 4, loss: 0.33009642362594604, acc: 82.8125, f1: 75.57507155867812, r: 0.6087272856003125
06/02/2019 09:29:58 step: 934, epoch: 28, batch: 9, loss: 0.3576251268386841, acc: 84.375, f1: 72.06845238095238, r: 0.6638867171946903
06/02/2019 09:29:58 step: 939, epoch: 28, batch: 14, loss: 0.34376341104507446, acc: 84.375, f1: 65.5529968066814, r: 0.6127149147750792
06/02/2019 09:29:59 step: 944, epoch: 28, batch: 19, loss: 0.49193239212036133, acc: 76.5625, f1: 60.251697633822886, r: 0.5204175022739187
06/02/2019 09:29:59 step: 949, epoch: 28, batch: 24, loss: 0.49466177821159363, acc: 78.125, f1: 69.90676477518583, r: 0.625698841933241
06/02/2019 09:30:00 step: 954, epoch: 28, batch: 29, loss: 0.41926658153533936, acc: 79.6875, f1: 64.61574933003504, r: 0.5982044543542286
06/02/2019 09:30:00 *** evaluating ***
06/02/2019 09:30:00 step: 29, epoch: 28, acc: 57.692307692307686, f1: 20.937812187812185, r: 0.2721140263784573
06/02/2019 09:30:00 *** epoch: 30 ***
06/02/2019 09:30:00 *** training ***
06/02/2019 09:30:01 step: 962, epoch: 29, batch: 4, loss: 0.4269031584262848, acc: 79.6875, f1: 64.51120763620763, r: 0.6518642907424215
06/02/2019 09:30:01 step: 967, epoch: 29, batch: 9, loss: 0.523919939994812, acc: 71.875, f1: 54.732441861821925, r: 0.5535842506742753
06/02/2019 09:30:02 step: 972, epoch: 29, batch: 14, loss: 0.35706865787506104, acc: 81.25, f1: 67.04731946147847, r: 0.5974025352431604
06/02/2019 09:30:03 step: 977, epoch: 29, batch: 19, loss: 0.483631432056427, acc: 76.5625, f1: 63.21180555555556, r: 0.6213853000749454
06/02/2019 09:30:03 step: 982, epoch: 29, batch: 24, loss: 0.41044509410858154, acc: 81.25, f1: 73.97490309255015, r: 0.7113246435670371
06/02/2019 09:30:04 step: 987, epoch: 29, batch: 29, loss: 0.4238632321357727, acc: 79.6875, f1: 73.35119047619048, r: 0.7239276566261486
06/02/2019 09:30:04 *** evaluating ***
06/02/2019 09:30:04 step: 30, epoch: 29, acc: 56.41025641025641, f1: 20.823277377525905, r: 0.2734088374552941
06/02/2019 09:30:04 *** epoch: 31 ***
06/02/2019 09:30:04 *** training ***
06/02/2019 09:30:05 step: 995, epoch: 30, batch: 4, loss: 0.3294590413570404, acc: 89.0625, f1: 71.81685273790536, r: 0.6525782707560358
06/02/2019 09:30:05 step: 1000, epoch: 30, batch: 9, loss: 0.4083952009677887, acc: 81.25, f1: 61.44793730859305, r: 0.544882045598105
06/02/2019 09:30:06 step: 1005, epoch: 30, batch: 14, loss: 0.337999165058136, acc: 87.5, f1: 68.18452380952381, r: 0.662828218746381
06/02/2019 09:30:06 step: 1010, epoch: 30, batch: 19, loss: 0.4070223271846771, acc: 78.125, f1: 60.73072955738934, r: 0.5902512082801119
06/02/2019 09:30:07 step: 1015, epoch: 30, batch: 24, loss: 0.40333878993988037, acc: 79.6875, f1: 70.64007421150276, r: 0.5835811919073416
06/02/2019 09:30:07 step: 1020, epoch: 30, batch: 29, loss: 0.29937970638275146, acc: 89.0625, f1: 85.43083900226758, r: 0.6647578449777547
06/02/2019 09:30:07 *** evaluating ***
06/02/2019 09:30:08 step: 31, epoch: 30, acc: 56.837606837606835, f1: 20.435447874591535, r: 0.2674861462686439
06/02/2019 09:30:08 *** epoch: 32 ***
06/02/2019 09:30:08 *** training ***
06/02/2019 09:30:08 step: 1028, epoch: 31, batch: 4, loss: 0.4319625496864319, acc: 78.125, f1: 60.387091356918944, r: 0.6121412219100458
06/02/2019 09:30:09 step: 1033, epoch: 31, batch: 9, loss: 0.39007848501205444, acc: 81.25, f1: 74.84577922077922, r: 0.6761728378149416
06/02/2019 09:30:09 step: 1038, epoch: 31, batch: 14, loss: 0.4565088748931885, acc: 78.125, f1: 65.59580946783832, r: 0.6568294877458676
06/02/2019 09:30:10 step: 1043, epoch: 31, batch: 19, loss: 0.28099095821380615, acc: 90.625, f1: 56.00704038868932, r: 0.658462760423332
06/02/2019 09:30:10 step: 1048, epoch: 31, batch: 24, loss: 0.3476952910423279, acc: 84.375, f1: 66.53856306430941, r: 0.62308554748751
06/02/2019 09:30:11 step: 1053, epoch: 31, batch: 29, loss: 0.2278396040201187, acc: 89.0625, f1: 85.13851917578624, r: 0.6759748401957646
06/02/2019 09:30:11 *** evaluating ***
06/02/2019 09:30:11 step: 32, epoch: 31, acc: 55.98290598290598, f1: 21.24126984126984, r: 0.2772265163847802
06/02/2019 09:30:11 *** epoch: 33 ***
06/02/2019 09:30:11 *** training ***
06/02/2019 09:30:12 step: 1061, epoch: 32, batch: 4, loss: 0.38490527868270874, acc: 84.375, f1: 64.20634920634919, r: 0.5576204132633906
06/02/2019 09:30:12 step: 1066, epoch: 32, batch: 9, loss: 0.39497315883636475, acc: 82.8125, f1: 57.37280835106922, r: 0.5483896644073625
06/02/2019 09:30:13 step: 1071, epoch: 32, batch: 14, loss: 0.3302978277206421, acc: 82.8125, f1: 73.72812372812373, r: 0.617856028330081
06/02/2019 09:30:13 step: 1076, epoch: 32, batch: 19, loss: 0.39139410853385925, acc: 79.6875, f1: 68.89550264550263, r: 0.7032856544973034
06/02/2019 09:30:14 step: 1081, epoch: 32, batch: 24, loss: 0.5795366764068604, acc: 75.0, f1: 60.871917937135336, r: 0.651959989227232
06/02/2019 09:30:15 step: 1086, epoch: 32, batch: 29, loss: 0.1846131980419159, acc: 95.3125, f1: 83.20574162679426, r: 0.7743786520440421
06/02/2019 09:30:15 *** evaluating ***
06/02/2019 09:30:15 step: 33, epoch: 32, acc: 56.837606837606835, f1: 21.576823817298564, r: 0.2799112731243584
06/02/2019 09:30:15 *** epoch: 34 ***
06/02/2019 09:30:15 *** training ***
06/02/2019 09:30:16 step: 1094, epoch: 33, batch: 4, loss: 0.4243963956832886, acc: 81.25, f1: 63.681681222003796, r: 0.6806893468578133
06/02/2019 09:30:16 step: 1099, epoch: 33, batch: 9, loss: 0.384349524974823, acc: 79.6875, f1: 60.88017174082747, r: 0.5743719926602524
06/02/2019 09:30:17 step: 1104, epoch: 33, batch: 14, loss: 0.27726271748542786, acc: 89.0625, f1: 75.00450937950937, r: 0.6789451266975564
06/02/2019 09:30:17 step: 1109, epoch: 33, batch: 19, loss: 0.2936174273490906, acc: 90.625, f1: 89.99819041835848, r: 0.6772441130952503
06/02/2019 09:30:18 step: 1114, epoch: 33, batch: 24, loss: 0.27254045009613037, acc: 85.9375, f1: 82.39964887852949, r: 0.6457743592269217
06/02/2019 09:30:18 step: 1119, epoch: 33, batch: 29, loss: 0.3863336443901062, acc: 82.8125, f1: 69.67258794845002, r: 0.5755736997606751
06/02/2019 09:30:18 *** evaluating ***
06/02/2019 09:30:19 step: 34, epoch: 33, acc: 56.837606837606835, f1: 22.71089064433132, r: 0.2823329465504071
06/02/2019 09:30:19 *** epoch: 35 ***
06/02/2019 09:30:19 *** training ***
06/02/2019 09:30:19 step: 1127, epoch: 34, batch: 4, loss: 0.3401791751384735, acc: 85.9375, f1: 74.24560682968371, r: 0.6137646640098137
06/02/2019 09:30:20 step: 1132, epoch: 34, batch: 9, loss: 0.4927060008049011, acc: 76.5625, f1: 68.39464464464464, r: 0.6220808923123895
06/02/2019 09:30:20 step: 1137, epoch: 34, batch: 14, loss: 0.3560110330581665, acc: 85.9375, f1: 74.59577922077922, r: 0.7373881802658004
06/02/2019 09:30:20 step: 1142, epoch: 34, batch: 19, loss: 0.21519817411899567, acc: 90.625, f1: 89.872878444307, r: 0.6685266853418789
06/02/2019 09:30:21 step: 1147, epoch: 34, batch: 24, loss: 0.2670453190803528, acc: 89.0625, f1: 73.64746968007839, r: 0.7160320862150512
06/02/2019 09:30:22 step: 1152, epoch: 34, batch: 29, loss: 0.4850985109806061, acc: 79.6875, f1: 49.3959991158267, r: 0.5915022434519484
06/02/2019 09:30:22 *** evaluating ***
06/02/2019 09:30:22 step: 35, epoch: 34, acc: 56.41025641025641, f1: 20.531207074175825, r: 0.2629392089737234
06/02/2019 09:30:22 *** epoch: 36 ***
06/02/2019 09:30:22 *** training ***
06/02/2019 09:30:23 step: 1160, epoch: 35, batch: 4, loss: 0.3212270736694336, acc: 87.5, f1: 81.69667053813394, r: 0.5453845635720941
06/02/2019 09:30:23 step: 1165, epoch: 35, batch: 9, loss: 0.45276927947998047, acc: 79.6875, f1: 65.98214285714286, r: 0.6867034902590138
06/02/2019 09:30:24 step: 1170, epoch: 35, batch: 14, loss: 0.43156522512435913, acc: 78.125, f1: 48.07375222816399, r: 0.5382973296743266
06/02/2019 09:30:24 step: 1175, epoch: 35, batch: 19, loss: 0.44384467601776123, acc: 79.6875, f1: 65.02652374080945, r: 0.6035173707391783
06/02/2019 09:30:25 step: 1180, epoch: 35, batch: 24, loss: 0.43377941846847534, acc: 78.125, f1: 57.91666666666666, r: 0.5771401936615147
06/02/2019 09:30:25 step: 1185, epoch: 35, batch: 29, loss: 0.19428986310958862, acc: 92.1875, f1: 91.01909362778929, r: 0.6714980017976936
06/02/2019 09:30:25 *** evaluating ***
06/02/2019 09:30:26 step: 36, epoch: 35, acc: 56.41025641025641, f1: 20.867197160218495, r: 0.2671942538688322
06/02/2019 09:30:26 *** epoch: 37 ***
06/02/2019 09:30:26 *** training ***
06/02/2019 09:30:26 step: 1193, epoch: 36, batch: 4, loss: 0.3988904058933258, acc: 78.125, f1: 74.53770173958625, r: 0.6598212883512269
06/02/2019 09:30:27 step: 1198, epoch: 36, batch: 9, loss: 0.22706735134124756, acc: 89.0625, f1: 73.21875, r: 0.6094920583344201
06/02/2019 09:30:27 step: 1203, epoch: 36, batch: 14, loss: 0.5557467341423035, acc: 75.0, f1: 57.56230336148369, r: 0.5393917970382269
06/02/2019 09:30:28 step: 1208, epoch: 36, batch: 19, loss: 0.4189712703227997, acc: 81.25, f1: 66.86771561771562, r: 0.6053352638846782
06/02/2019 09:30:28 step: 1213, epoch: 36, batch: 24, loss: 0.22966225445270538, acc: 92.1875, f1: 86.77759740259741, r: 0.728701695100218
06/02/2019 09:30:29 step: 1218, epoch: 36, batch: 29, loss: 0.3115878105163574, acc: 84.375, f1: 75.72514789339813, r: 0.568424782430289
06/02/2019 09:30:29 *** evaluating ***
06/02/2019 09:30:29 step: 37, epoch: 36, acc: 55.98290598290598, f1: 20.07503237794921, r: 0.27375615809862086
06/02/2019 09:30:29 *** epoch: 38 ***
06/02/2019 09:30:29 *** training ***
06/02/2019 09:30:30 step: 1226, epoch: 37, batch: 4, loss: 0.3130190372467041, acc: 85.9375, f1: 77.19336219336219, r: 0.649091369118714
06/02/2019 09:30:30 step: 1231, epoch: 37, batch: 9, loss: 0.31250765919685364, acc: 84.375, f1: 80.9920634920635, r: 0.7260062952488764
06/02/2019 09:30:31 step: 1236, epoch: 37, batch: 14, loss: 0.332058310508728, acc: 85.9375, f1: 57.17773275318033, r: 0.575690652089999
06/02/2019 09:30:31 step: 1241, epoch: 37, batch: 19, loss: 0.4437048137187958, acc: 78.125, f1: 62.606086789554524, r: 0.6166892998583109
06/02/2019 09:30:32 step: 1246, epoch: 37, batch: 24, loss: 0.4008522629737854, acc: 81.25, f1: 67.21484957619411, r: 0.6078818642376034
06/02/2019 09:30:32 step: 1251, epoch: 37, batch: 29, loss: 0.3491543233394623, acc: 85.9375, f1: 81.91678691678692, r: 0.7533876627102447
06/02/2019 09:30:33 *** evaluating ***
06/02/2019 09:30:33 step: 38, epoch: 37, acc: 56.41025641025641, f1: 20.80530991821314, r: 0.2718245205714016
06/02/2019 09:30:33 *** epoch: 39 ***
06/02/2019 09:30:33 *** training ***
06/02/2019 09:30:33 step: 1259, epoch: 38, batch: 4, loss: 0.25599759817123413, acc: 87.5, f1: 82.45816561606036, r: 0.60889564232332
06/02/2019 09:30:34 step: 1264, epoch: 38, batch: 9, loss: 0.3420718312263489, acc: 82.8125, f1: 80.5238095238095, r: 0.6356729112143523
06/02/2019 09:30:34 step: 1269, epoch: 38, batch: 14, loss: 0.2988704741001129, acc: 85.9375, f1: 65.0925925925926, r: 0.6317367837931199
06/02/2019 09:30:35 step: 1274, epoch: 38, batch: 19, loss: 0.39151301980018616, acc: 82.8125, f1: 60.78331390831391, r: 0.580837522665854
06/02/2019 09:30:36 step: 1279, epoch: 38, batch: 24, loss: 0.3395504653453827, acc: 84.375, f1: 57.54726890756303, r: 0.7083541438899196
06/02/2019 09:30:36 step: 1284, epoch: 38, batch: 29, loss: 0.3791414499282837, acc: 84.375, f1: 59.774904214559385, r: 0.6259809504694069
06/02/2019 09:30:36 *** evaluating ***
06/02/2019 09:30:37 step: 39, epoch: 38, acc: 56.837606837606835, f1: 21.437731105699857, r: 0.2842265533158791
06/02/2019 09:30:37 *** epoch: 40 ***
06/02/2019 09:30:37 *** training ***
06/02/2019 09:30:37 step: 1292, epoch: 39, batch: 4, loss: 0.30120787024497986, acc: 82.8125, f1: 62.68394969182075, r: 0.6163936386912353
06/02/2019 09:30:38 step: 1297, epoch: 39, batch: 9, loss: 0.3928854167461395, acc: 82.8125, f1: 65.75357010139619, r: 0.5748867493491497
06/02/2019 09:30:38 step: 1302, epoch: 39, batch: 14, loss: 0.3101939558982849, acc: 84.375, f1: 71.2706043956044, r: 0.5970117857247098
06/02/2019 09:30:39 step: 1307, epoch: 39, batch: 19, loss: 0.35125744342803955, acc: 85.9375, f1: 77.35691391941391, r: 0.6656816971512693
06/02/2019 09:30:39 step: 1312, epoch: 39, batch: 24, loss: 0.27789950370788574, acc: 87.5, f1: 86.64010366051181, r: 0.634681670960873
06/02/2019 09:30:40 step: 1317, epoch: 39, batch: 29, loss: 0.39339926838874817, acc: 78.125, f1: 62.869273583559306, r: 0.5419440475871573
06/02/2019 09:30:40 *** evaluating ***
06/02/2019 09:30:40 step: 40, epoch: 39, acc: 58.119658119658126, f1: 20.99875415282392, r: 0.2721701039194522
06/02/2019 09:30:40 *** epoch: 41 ***
06/02/2019 09:30:40 *** training ***
06/02/2019 09:30:41 step: 1325, epoch: 40, batch: 4, loss: 0.4179862141609192, acc: 79.6875, f1: 74.40594270372777, r: 0.661079818214771
06/02/2019 09:30:41 step: 1330, epoch: 40, batch: 9, loss: 0.37177178263664246, acc: 85.9375, f1: 69.87234477124183, r: 0.6313301128562293
06/02/2019 09:30:42 step: 1335, epoch: 40, batch: 14, loss: 0.23002460598945618, acc: 89.0625, f1: 84.57846053190308, r: 0.6239016216117914
06/02/2019 09:30:42 step: 1340, epoch: 40, batch: 19, loss: 0.14877408742904663, acc: 95.3125, f1: 82.65070921985816, r: 0.7854330805249616
06/02/2019 09:30:43 step: 1345, epoch: 40, batch: 24, loss: 0.22340574860572815, acc: 90.625, f1: 75.69688823721081, r: 0.7571525739057909
06/02/2019 09:30:44 step: 1350, epoch: 40, batch: 29, loss: 0.3970005512237549, acc: 84.375, f1: 73.79918101992571, r: 0.605223200016668
06/02/2019 09:30:44 *** evaluating ***
06/02/2019 09:30:44 step: 41, epoch: 40, acc: 56.41025641025641, f1: 21.26040730757712, r: 0.27230820463808664
06/02/2019 09:30:44 *** epoch: 42 ***
06/02/2019 09:30:44 *** training ***
06/02/2019 09:30:44 step: 1358, epoch: 41, batch: 4, loss: 0.2517167627811432, acc: 92.1875, f1: 89.50643300798579, r: 0.5901274822232906
06/02/2019 09:30:45 step: 1363, epoch: 41, batch: 9, loss: 0.358704149723053, acc: 84.375, f1: 78.71904172924582, r: 0.5739587411104579
06/02/2019 09:30:45 step: 1368, epoch: 41, batch: 14, loss: 0.20750702917575836, acc: 95.3125, f1: 93.56309162760775, r: 0.5887881524538604
06/02/2019 09:30:46 step: 1373, epoch: 41, batch: 19, loss: 0.4550462067127228, acc: 78.125, f1: 68.80731561582625, r: 0.5885390202696548
06/02/2019 09:30:46 step: 1378, epoch: 41, batch: 24, loss: 0.41649332642555237, acc: 81.25, f1: 55.69444444444444, r: 0.49534982216624573
06/02/2019 09:30:47 step: 1383, epoch: 41, batch: 29, loss: 0.4398094713687897, acc: 78.125, f1: 56.72186735366084, r: 0.6482663069332036
06/02/2019 09:30:47 *** evaluating ***
06/02/2019 09:30:48 step: 42, epoch: 41, acc: 58.119658119658126, f1: 20.484574737797228, r: 0.27277367039328215
06/02/2019 09:30:48 *** epoch: 43 ***
06/02/2019 09:30:48 *** training ***
06/02/2019 09:30:48 step: 1391, epoch: 42, batch: 4, loss: 0.19437479972839355, acc: 93.75, f1: 78.88020833333333, r: 0.7328167687999564
06/02/2019 09:30:49 step: 1396, epoch: 42, batch: 9, loss: 0.3514561057090759, acc: 84.375, f1: 67.66022198353026, r: 0.48387188942820925
06/02/2019 09:30:49 step: 1401, epoch: 42, batch: 14, loss: 0.29479706287384033, acc: 87.5, f1: 71.03702582728006, r: 0.7431923982531845
06/02/2019 09:30:50 step: 1406, epoch: 42, batch: 19, loss: 0.38192200660705566, acc: 81.25, f1: 75.8831585081585, r: 0.7038158267548927
06/02/2019 09:30:50 step: 1411, epoch: 42, batch: 24, loss: 0.4088306725025177, acc: 79.6875, f1: 64.80643539467069, r: 0.6317342892722883
06/02/2019 09:30:51 step: 1416, epoch: 42, batch: 29, loss: 0.2985284626483917, acc: 82.8125, f1: 61.876665001664996, r: 0.6511729051007697
06/02/2019 09:30:51 *** evaluating ***
06/02/2019 09:30:51 step: 43, epoch: 42, acc: 56.837606837606835, f1: 20.477183810628937, r: 0.27517262740347004
06/02/2019 09:30:51 *** epoch: 44 ***
06/02/2019 09:30:51 *** training ***
06/02/2019 09:30:52 step: 1424, epoch: 43, batch: 4, loss: 0.2821759581565857, acc: 89.0625, f1: 75.32329988851728, r: 0.7082230456395838
06/02/2019 09:30:52 step: 1429, epoch: 43, batch: 9, loss: 0.2071612924337387, acc: 90.625, f1: 89.0448897507721, r: 0.670897895488663
06/02/2019 09:30:53 step: 1434, epoch: 43, batch: 14, loss: 0.30443131923675537, acc: 85.9375, f1: 72.62578616352202, r: 0.7234355120061863
06/02/2019 09:30:53 step: 1439, epoch: 43, batch: 19, loss: 0.40737101435661316, acc: 76.5625, f1: 60.352323838080956, r: 0.6280532015813916
06/02/2019 09:30:54 step: 1444, epoch: 43, batch: 24, loss: 0.24914968013763428, acc: 90.625, f1: 54.39102564102565, r: 0.5897956739245557
06/02/2019 09:30:54 step: 1449, epoch: 43, batch: 29, loss: 0.4066649377346039, acc: 78.125, f1: 69.3625813062461, r: 0.682342282231481
06/02/2019 09:30:55 *** evaluating ***
06/02/2019 09:30:55 step: 44, epoch: 43, acc: 56.837606837606835, f1: 20.94942726395073, r: 0.2705964875560168
06/02/2019 09:30:55 *** epoch: 45 ***
06/02/2019 09:30:55 *** training ***
06/02/2019 09:30:55 step: 1457, epoch: 44, batch: 4, loss: 0.25426578521728516, acc: 84.375, f1: 71.90722665877945, r: 0.602122943556638
06/02/2019 09:30:56 step: 1462, epoch: 44, batch: 9, loss: 0.21105113625526428, acc: 89.0625, f1: 83.53409875149006, r: 0.6240042712026707
06/02/2019 09:30:56 step: 1467, epoch: 44, batch: 14, loss: 0.239023819565773, acc: 89.0625, f1: 74.14820433821137, r: 0.5955130095272381
06/02/2019 09:30:57 step: 1472, epoch: 44, batch: 19, loss: 0.495001882314682, acc: 71.875, f1: 48.57698099002447, r: 0.6407881435292717
06/02/2019 09:30:57 step: 1477, epoch: 44, batch: 24, loss: 0.2052033692598343, acc: 90.625, f1: 74.90974711562947, r: 0.7757836660115096
06/02/2019 09:30:58 step: 1482, epoch: 44, batch: 29, loss: 0.241530179977417, acc: 87.5, f1: 85.25406113862549, r: 0.6883681478120319
06/02/2019 09:30:58 *** evaluating ***
06/02/2019 09:30:59 step: 45, epoch: 44, acc: 55.98290598290598, f1: 20.933149249798934, r: 0.25751051464433156
06/02/2019 09:30:59 *** epoch: 46 ***
06/02/2019 09:30:59 *** training ***
06/02/2019 09:30:59 step: 1490, epoch: 45, batch: 4, loss: 0.2521970868110657, acc: 89.0625, f1: 69.93354598617756, r: 0.5856781590508608
06/02/2019 09:31:00 step: 1495, epoch: 45, batch: 9, loss: 0.29523229598999023, acc: 85.9375, f1: 81.62857087100399, r: 0.643095895821522
06/02/2019 09:31:00 step: 1500, epoch: 45, batch: 14, loss: 0.4221041798591614, acc: 84.375, f1: 82.52058718224131, r: 0.7021909463426482
06/02/2019 09:31:01 step: 1505, epoch: 45, batch: 19, loss: 0.5550847053527832, acc: 76.5625, f1: 68.61051261051261, r: 0.5488275247370185
06/02/2019 09:31:01 step: 1510, epoch: 45, batch: 24, loss: 0.23656678199768066, acc: 85.9375, f1: 73.50575894693542, r: 0.7470049732661365
06/02/2019 09:31:02 step: 1515, epoch: 45, batch: 29, loss: 0.24159470200538635, acc: 89.0625, f1: 84.92424242424244, r: 0.6250883697801544
06/02/2019 09:31:02 *** evaluating ***
06/02/2019 09:31:02 step: 46, epoch: 45, acc: 56.837606837606835, f1: 22.35954469507101, r: 0.26358132904677894
06/02/2019 09:31:02 *** epoch: 47 ***
06/02/2019 09:31:02 *** training ***
06/02/2019 09:31:03 step: 1523, epoch: 46, batch: 4, loss: 0.34323617815971375, acc: 84.375, f1: 82.5716374269006, r: 0.6997240381181361
06/02/2019 09:31:04 step: 1528, epoch: 46, batch: 9, loss: 0.2541171908378601, acc: 85.9375, f1: 69.02732683982684, r: 0.6842624472738357
06/02/2019 09:31:04 step: 1533, epoch: 46, batch: 14, loss: 0.3906940817832947, acc: 76.5625, f1: 60.068094765214916, r: 0.5411033119737727
06/02/2019 09:31:05 step: 1538, epoch: 46, batch: 19, loss: 0.38282904028892517, acc: 82.8125, f1: 67.71935806510274, r: 0.6266030397276697
06/02/2019 09:31:05 step: 1543, epoch: 46, batch: 24, loss: 0.15565529465675354, acc: 93.75, f1: 76.74093792106214, r: 0.589768144475269
06/02/2019 09:31:06 step: 1548, epoch: 46, batch: 29, loss: 0.2696063220500946, acc: 87.5, f1: 81.01562128347844, r: 0.719988373868864
06/02/2019 09:31:06 *** evaluating ***
06/02/2019 09:31:06 step: 47, epoch: 46, acc: 58.119658119658126, f1: 21.656162464985997, r: 0.2624121182642585
06/02/2019 09:31:06 *** epoch: 48 ***
06/02/2019 09:31:06 *** training ***
06/02/2019 09:31:07 step: 1556, epoch: 47, batch: 4, loss: 0.31154167652130127, acc: 82.8125, f1: 64.77411477411478, r: 0.5172337055470869
06/02/2019 09:31:07 step: 1561, epoch: 47, batch: 9, loss: 0.46719634532928467, acc: 81.25, f1: 58.77450980392157, r: 0.6143830043215621
06/02/2019 09:31:08 step: 1566, epoch: 47, batch: 14, loss: 0.43856295943260193, acc: 82.8125, f1: 72.67543859649123, r: 0.6835924102001044
06/02/2019 09:31:09 step: 1571, epoch: 47, batch: 19, loss: 0.40136876702308655, acc: 82.8125, f1: 78.32944832944833, r: 0.7017032743175725
06/02/2019 09:31:09 step: 1576, epoch: 47, batch: 24, loss: 0.2565016746520996, acc: 85.9375, f1: 71.91058065738373, r: 0.5812220261673224
06/02/2019 09:31:10 step: 1581, epoch: 47, batch: 29, loss: 0.23693177103996277, acc: 90.625, f1: 89.60477822228975, r: 0.6116786739644486
06/02/2019 09:31:10 *** evaluating ***
06/02/2019 09:31:10 step: 48, epoch: 47, acc: 54.700854700854705, f1: 20.086885377191926, r: 0.25657701659098675
06/02/2019 09:31:10 *** epoch: 49 ***
06/02/2019 09:31:10 *** training ***
06/02/2019 09:31:11 step: 1589, epoch: 48, batch: 4, loss: 0.31364715099334717, acc: 81.25, f1: 58.91762649115591, r: 0.6829977124362357
06/02/2019 09:31:11 step: 1594, epoch: 48, batch: 9, loss: 0.24985209107398987, acc: 87.5, f1: 72.95918367346938, r: 0.657269347940626
06/02/2019 09:31:12 step: 1599, epoch: 48, batch: 14, loss: 0.24106955528259277, acc: 87.5, f1: 72.79251700680271, r: 0.5855652946916329
06/02/2019 09:31:12 step: 1604, epoch: 48, batch: 19, loss: 0.28229081630706787, acc: 85.9375, f1: 56.824945887445885, r: 0.6149142875760432
06/02/2019 09:31:13 step: 1609, epoch: 48, batch: 24, loss: 0.13445734977722168, acc: 93.75, f1: 93.29328310083834, r: 0.6575005761294415
06/02/2019 09:31:13 step: 1614, epoch: 48, batch: 29, loss: 0.428203821182251, acc: 82.8125, f1: 62.7203569599883, r: 0.6277997827571852
06/02/2019 09:31:14 *** evaluating ***
06/02/2019 09:31:14 step: 49, epoch: 48, acc: 58.54700854700855, f1: 20.74727762010231, r: 0.2481584677407764
06/02/2019 09:31:14 *** epoch: 50 ***
06/02/2019 09:31:14 *** training ***
06/02/2019 09:31:14 step: 1622, epoch: 49, batch: 4, loss: 0.23464038968086243, acc: 92.1875, f1: 86.3466239230615, r: 0.6837214336079532
06/02/2019 09:31:15 step: 1627, epoch: 49, batch: 9, loss: 0.40194958448410034, acc: 81.25, f1: 54.277743964292966, r: 0.6148828169490136
06/02/2019 09:31:15 step: 1632, epoch: 49, batch: 14, loss: 0.30476999282836914, acc: 84.375, f1: 56.22503840245776, r: 0.5391523318775161
06/02/2019 09:31:16 step: 1637, epoch: 49, batch: 19, loss: 0.11565777659416199, acc: 95.3125, f1: 93.95299473172729, r: 0.7479079206520788
06/02/2019 09:31:17 step: 1642, epoch: 49, batch: 24, loss: 0.2687613368034363, acc: 87.5, f1: 77.39169965184982, r: 0.6951718674777179
06/02/2019 09:31:17 step: 1647, epoch: 49, batch: 29, loss: 0.42730534076690674, acc: 82.8125, f1: 70.55403979773727, r: 0.569117122334534
06/02/2019 09:31:17 *** evaluating ***
06/02/2019 09:31:18 step: 50, epoch: 49, acc: 55.12820512820513, f1: 20.3504995754743, r: 0.25861949094981956
06/02/2019 09:31:18 *** epoch: 51 ***
06/02/2019 09:31:18 *** training ***
06/02/2019 09:31:18 step: 1655, epoch: 50, batch: 4, loss: 0.3569527864456177, acc: 82.8125, f1: 58.85283849423193, r: 0.57621118113082
06/02/2019 09:31:19 step: 1660, epoch: 50, batch: 9, loss: 0.27398353815078735, acc: 85.9375, f1: 71.77302888368462, r: 0.7094890761615374
06/02/2019 09:31:19 step: 1665, epoch: 50, batch: 14, loss: 0.2831898033618927, acc: 85.9375, f1: 52.67437304075236, r: 0.5540012410693489
06/02/2019 09:31:20 step: 1670, epoch: 50, batch: 19, loss: 0.22585883736610413, acc: 90.625, f1: 75.56390977443608, r: 0.5941869504486907
06/02/2019 09:31:20 step: 1675, epoch: 50, batch: 24, loss: 0.2863785922527313, acc: 82.8125, f1: 60.558565531475736, r: 0.6675411275174797
06/02/2019 09:31:21 step: 1680, epoch: 50, batch: 29, loss: 0.2397715151309967, acc: 87.5, f1: 75.39835164835165, r: 0.6487286699428442
06/02/2019 09:31:21 *** evaluating ***
06/02/2019 09:31:21 step: 51, epoch: 50, acc: 56.41025641025641, f1: 19.954031117397452, r: 0.2642270000685518
06/02/2019 09:31:21 *** epoch: 52 ***
06/02/2019 09:31:21 *** training ***
06/02/2019 09:31:22 step: 1688, epoch: 51, batch: 4, loss: 0.4853840470314026, acc: 76.5625, f1: 70.26190476190477, r: 0.6936237460889749
06/02/2019 09:31:22 step: 1693, epoch: 51, batch: 9, loss: 0.3329852223396301, acc: 87.5, f1: 77.04095549840231, r: 0.689064167809286
06/02/2019 09:31:23 step: 1698, epoch: 51, batch: 14, loss: 0.29291900992393494, acc: 87.5, f1: 69.52418121575124, r: 0.5939866804702396
06/02/2019 09:31:23 step: 1703, epoch: 51, batch: 19, loss: 0.27765440940856934, acc: 85.9375, f1: 81.65966386554622, r: 0.6214512763177021
06/02/2019 09:31:24 step: 1708, epoch: 51, batch: 24, loss: 0.3235846757888794, acc: 85.9375, f1: 75.7492852213349, r: 0.708189453248856
06/02/2019 09:31:25 step: 1713, epoch: 51, batch: 29, loss: 0.34519654512405396, acc: 84.375, f1: 77.10936824820672, r: 0.6474137070541602
06/02/2019 09:31:25 *** evaluating ***
06/02/2019 09:31:25 step: 52, epoch: 51, acc: 55.98290598290598, f1: 20.0609080841639, r: 0.25569174132139316
06/02/2019 09:31:25 *** epoch: 53 ***
06/02/2019 09:31:25 *** training ***
06/02/2019 09:31:25 step: 1721, epoch: 52, batch: 4, loss: 0.24849772453308105, acc: 90.625, f1: 74.10069097569098, r: 0.6948686158150821
06/02/2019 09:31:26 step: 1726, epoch: 52, batch: 9, loss: 0.25363889336586, acc: 89.0625, f1: 72.86448670377241, r: 0.6874726730693721
06/02/2019 09:31:26 step: 1731, epoch: 52, batch: 14, loss: 0.504131555557251, acc: 75.0, f1: 60.55674312165185, r: 0.6008912819786845
06/02/2019 09:31:27 step: 1736, epoch: 52, batch: 19, loss: 0.2702796459197998, acc: 87.5, f1: 68.55637254901961, r: 0.7226476771394571
06/02/2019 09:31:27 step: 1741, epoch: 52, batch: 24, loss: 0.11449598520994186, acc: 95.3125, f1: 82.79883381924199, r: 0.6921300597111132
06/02/2019 09:31:28 step: 1746, epoch: 52, batch: 29, loss: 0.29911091923713684, acc: 84.375, f1: 68.42261904761904, r: 0.706479982163008
06/02/2019 09:31:28 *** evaluating ***
06/02/2019 09:31:29 step: 53, epoch: 52, acc: 55.98290598290598, f1: 20.50146101454569, r: 0.2556909234386819
06/02/2019 09:31:29 *** epoch: 54 ***
06/02/2019 09:31:29 *** training ***
06/02/2019 09:31:29 step: 1754, epoch: 53, batch: 4, loss: 0.36287766695022583, acc: 79.6875, f1: 63.89194139194139, r: 0.6099369060156622
06/02/2019 09:31:30 step: 1759, epoch: 53, batch: 9, loss: 0.3129289150238037, acc: 84.375, f1: 70.24806262187624, r: 0.6360078843794877
06/02/2019 09:31:30 step: 1764, epoch: 53, batch: 14, loss: 0.3606799244880676, acc: 82.8125, f1: 69.932208994709, r: 0.651079087105991
06/02/2019 09:31:31 step: 1769, epoch: 53, batch: 19, loss: 0.34722262620925903, acc: 79.6875, f1: 72.09906035992992, r: 0.5861320906808646
06/02/2019 09:31:31 step: 1774, epoch: 53, batch: 24, loss: 0.22178666293621063, acc: 92.1875, f1: 77.97921441303795, r: 0.7194241152034115
06/02/2019 09:31:32 step: 1779, epoch: 53, batch: 29, loss: 0.25703343749046326, acc: 90.625, f1: 70.12574290116662, r: 0.7259173056578644
06/02/2019 09:31:32 *** evaluating ***
06/02/2019 09:31:32 step: 54, epoch: 53, acc: 58.119658119658126, f1: 20.962837837837835, r: 0.2635060081725426
06/02/2019 09:31:32 *** epoch: 55 ***
06/02/2019 09:31:32 *** training ***
06/02/2019 09:31:33 step: 1787, epoch: 54, batch: 4, loss: 0.24085669219493866, acc: 87.5, f1: 77.62009018166653, r: 0.6044486311786814
06/02/2019 09:31:33 step: 1792, epoch: 54, batch: 9, loss: 0.23657536506652832, acc: 92.1875, f1: 67.80329593267882, r: 0.6162602462033373
06/02/2019 09:31:34 step: 1797, epoch: 54, batch: 14, loss: 0.4206947386264801, acc: 79.6875, f1: 66.86916433239962, r: 0.7084010283985215
06/02/2019 09:31:34 step: 1802, epoch: 54, batch: 19, loss: 0.430511474609375, acc: 85.9375, f1: 69.98745109618366, r: 0.6648210450201552
06/02/2019 09:31:35 step: 1807, epoch: 54, batch: 24, loss: 0.31171342730522156, acc: 87.5, f1: 84.73748360797389, r: 0.6776337648859775
06/02/2019 09:31:35 step: 1812, epoch: 54, batch: 29, loss: 0.192661851644516, acc: 92.1875, f1: 91.65001665001665, r: 0.7149162658824564
06/02/2019 09:31:36 *** evaluating ***
06/02/2019 09:31:36 step: 55, epoch: 54, acc: 55.55555555555556, f1: 21.048727183180965, r: 0.26516959788859623
06/02/2019 09:31:36 *** epoch: 56 ***
06/02/2019 09:31:36 *** training ***
06/02/2019 09:31:36 step: 1820, epoch: 55, batch: 4, loss: 0.2347162961959839, acc: 92.1875, f1: 93.76486087012404, r: 0.737517531554839
06/02/2019 09:31:37 step: 1825, epoch: 55, batch: 9, loss: 0.16145646572113037, acc: 92.1875, f1: 89.66733523119392, r: 0.7888034981984196
06/02/2019 09:31:37 step: 1830, epoch: 55, batch: 14, loss: 0.24509727954864502, acc: 89.0625, f1: 61.956521739130444, r: 0.6328333886823835
06/02/2019 09:31:38 step: 1835, epoch: 55, batch: 19, loss: 0.2773023843765259, acc: 87.5, f1: 84.6188379521713, r: 0.5515068360570043
06/02/2019 09:31:38 step: 1840, epoch: 55, batch: 24, loss: 0.342746764421463, acc: 82.8125, f1: 71.10877496159755, r: 0.6875576106993587
06/02/2019 09:31:39 step: 1845, epoch: 55, batch: 29, loss: 0.14678294956684113, acc: 95.3125, f1: 94.75490196078431, r: 0.6659629374674678
06/02/2019 09:31:39 *** evaluating ***
06/02/2019 09:31:39 step: 56, epoch: 55, acc: 55.98290598290598, f1: 20.673314306165754, r: 0.26414559480196775
06/02/2019 09:31:39 *** epoch: 57 ***
06/02/2019 09:31:39 *** training ***
06/02/2019 09:31:40 step: 1853, epoch: 56, batch: 4, loss: 0.2652231454849243, acc: 87.5, f1: 68.42207792207793, r: 0.586910501887643
06/02/2019 09:31:40 step: 1858, epoch: 56, batch: 9, loss: 0.3814143240451813, acc: 87.5, f1: 73.56684052931517, r: 0.6889211530255351
06/02/2019 09:31:41 step: 1863, epoch: 56, batch: 14, loss: 0.26760995388031006, acc: 89.0625, f1: 79.68993152816682, r: 0.6695733306062109
06/02/2019 09:31:41 step: 1868, epoch: 56, batch: 19, loss: 0.40362846851348877, acc: 79.6875, f1: 68.26248313090419, r: 0.6069271954339598
06/02/2019 09:31:42 step: 1873, epoch: 56, batch: 24, loss: 0.3343660831451416, acc: 87.5, f1: 84.87211767824013, r: 0.6727220300967045
06/02/2019 09:31:42 step: 1878, epoch: 56, batch: 29, loss: 0.28828737139701843, acc: 85.9375, f1: 72.65151515151516, r: 0.6465174799187441
06/02/2019 09:31:42 *** evaluating ***
06/02/2019 09:31:43 step: 57, epoch: 56, acc: 58.97435897435898, f1: 19.987824675324678, r: 0.25381206870978823
06/02/2019 09:31:43 *** epoch: 58 ***
06/02/2019 09:31:43 *** training ***
06/02/2019 09:31:43 step: 1886, epoch: 57, batch: 4, loss: 0.2754768431186676, acc: 90.625, f1: 81.64273190281372, r: 0.7064337127631375
06/02/2019 09:31:44 step: 1891, epoch: 57, batch: 9, loss: 0.2822308838367462, acc: 85.9375, f1: 79.86866516941706, r: 0.6472100997021512
06/02/2019 09:31:44 step: 1896, epoch: 57, batch: 14, loss: 0.23872092366218567, acc: 89.0625, f1: 89.59768907563024, r: 0.7587053424935236
06/02/2019 09:31:45 step: 1901, epoch: 57, batch: 19, loss: 0.17424382269382477, acc: 89.0625, f1: 81.74860138319897, r: 0.6294164954700765
06/02/2019 09:31:45 step: 1906, epoch: 57, batch: 24, loss: 0.3264886438846588, acc: 84.375, f1: 67.12320246803006, r: 0.6440732559920913
06/02/2019 09:31:46 step: 1911, epoch: 57, batch: 29, loss: 0.350226491689682, acc: 84.375, f1: 68.99937929717342, r: 0.6356962981601326
06/02/2019 09:31:46 *** evaluating ***
06/02/2019 09:31:46 step: 58, epoch: 57, acc: 57.26495726495726, f1: 21.019128654596635, r: 0.26874974468136625
06/02/2019 09:31:46 *** epoch: 59 ***
06/02/2019 09:31:46 *** training ***
06/02/2019 09:31:47 step: 1919, epoch: 58, batch: 4, loss: 0.2576866149902344, acc: 85.9375, f1: 73.04566401340595, r: 0.6029526656364143
06/02/2019 09:31:47 step: 1924, epoch: 58, batch: 9, loss: 0.3191554546356201, acc: 82.8125, f1: 67.83931314461269, r: 0.6469028761465067
06/02/2019 09:31:48 step: 1929, epoch: 58, batch: 14, loss: 0.25269314646720886, acc: 92.1875, f1: 94.79617604617604, r: 0.70650959096813
06/02/2019 09:31:49 step: 1934, epoch: 58, batch: 19, loss: 0.09230617433786392, acc: 95.3125, f1: 86.35870111591434, r: 0.6691828387560855
06/02/2019 09:31:49 step: 1939, epoch: 58, batch: 24, loss: 0.30956265330314636, acc: 87.5, f1: 79.52339327339327, r: 0.6794857806041197
06/02/2019 09:31:50 step: 1944, epoch: 58, batch: 29, loss: 0.16775433719158173, acc: 95.3125, f1: 78.7936507936508, r: 0.6759022862638834
06/02/2019 09:31:50 *** evaluating ***
06/02/2019 09:31:50 step: 59, epoch: 58, acc: 58.119658119658126, f1: 21.175759165355245, r: 0.26756939183013584
06/02/2019 09:31:50 *** epoch: 60 ***
06/02/2019 09:31:50 *** training ***
06/02/2019 09:31:51 step: 1952, epoch: 59, batch: 4, loss: 0.21025776863098145, acc: 93.75, f1: 83.73020657503416, r: 0.7758447053241408
06/02/2019 09:31:51 step: 1957, epoch: 59, batch: 9, loss: 0.21185144782066345, acc: 89.0625, f1: 71.42881241565452, r: 0.6447992064944164
06/02/2019 09:31:52 step: 1962, epoch: 59, batch: 14, loss: 0.15584014356136322, acc: 92.1875, f1: 91.01117370024934, r: 0.6736880967556148
06/02/2019 09:31:52 step: 1967, epoch: 59, batch: 19, loss: 0.36410871148109436, acc: 85.9375, f1: 72.8083028083028, r: 0.6403181327574143
06/02/2019 09:31:53 step: 1972, epoch: 59, batch: 24, loss: 0.3333057761192322, acc: 84.375, f1: 70.96015096015095, r: 0.5795333333848391
06/02/2019 09:31:53 step: 1977, epoch: 59, batch: 29, loss: 0.22240972518920898, acc: 90.625, f1: 84.0168165168165, r: 0.6632728330825783
06/02/2019 09:31:54 *** evaluating ***
06/02/2019 09:31:54 step: 60, epoch: 59, acc: 57.26495726495726, f1: 20.761809102909943, r: 0.271404304156003
06/02/2019 09:31:54 *** epoch: 61 ***
06/02/2019 09:31:54 *** training ***
06/02/2019 09:31:54 step: 1985, epoch: 60, batch: 4, loss: 0.2914324998855591, acc: 87.5, f1: 84.71983555062823, r: 0.7053500351066564
06/02/2019 09:31:55 step: 1990, epoch: 60, batch: 9, loss: 0.2358393520116806, acc: 93.75, f1: 89.06996644811773, r: 0.6433270363393577
06/02/2019 09:31:55 step: 1995, epoch: 60, batch: 14, loss: 0.2631112337112427, acc: 85.9375, f1: 73.25017321939346, r: 0.6485772292987572
06/02/2019 09:31:56 step: 2000, epoch: 60, batch: 19, loss: 0.3798667788505554, acc: 82.8125, f1: 83.01553471630199, r: 0.6755346249574306
06/02/2019 09:31:57 step: 2005, epoch: 60, batch: 24, loss: 0.2693334221839905, acc: 85.9375, f1: 67.16772993088783, r: 0.64626924186184
06/02/2019 09:31:57 step: 2010, epoch: 60, batch: 29, loss: 0.2732229232788086, acc: 87.5, f1: 80.57919586452196, r: 0.7516026807793125
06/02/2019 09:31:57 *** evaluating ***
06/02/2019 09:31:58 step: 61, epoch: 60, acc: 57.692307692307686, f1: 20.804870804870802, r: 0.27576225432619517
06/02/2019 09:31:58 *** epoch: 62 ***
06/02/2019 09:31:58 *** training ***
06/02/2019 09:31:58 step: 2018, epoch: 61, batch: 4, loss: 0.12079805880784988, acc: 92.1875, f1: 89.1544310031705, r: 0.7132948933556282
06/02/2019 09:31:59 step: 2023, epoch: 61, batch: 9, loss: 0.13596200942993164, acc: 96.875, f1: 90.9373235460192, r: 0.5970932907712903
06/02/2019 09:31:59 step: 2028, epoch: 61, batch: 14, loss: 0.2971947193145752, acc: 85.9375, f1: 67.57575757575758, r: 0.7071755010288578
06/02/2019 09:32:00 step: 2033, epoch: 61, batch: 19, loss: 0.36674049496650696, acc: 82.8125, f1: 59.47972076232946, r: 0.5951829736511884
06/02/2019 09:32:00 step: 2038, epoch: 61, batch: 24, loss: 0.14170916378498077, acc: 95.3125, f1: 83.33523302938197, r: 0.7750616086604295
06/02/2019 09:32:01 step: 2043, epoch: 61, batch: 29, loss: 0.29581746459007263, acc: 84.375, f1: 69.38124375624375, r: 0.684701155981599
06/02/2019 09:32:01 *** evaluating ***
06/02/2019 09:32:02 step: 62, epoch: 61, acc: 55.98290598290598, f1: 21.074948796722992, r: 0.26576204218004085
06/02/2019 09:32:02 *** epoch: 63 ***
06/02/2019 09:32:02 *** training ***
06/02/2019 09:32:02 step: 2051, epoch: 62, batch: 4, loss: 0.16939979791641235, acc: 90.625, f1: 85.73759573759574, r: 0.725850784054684
06/02/2019 09:32:03 step: 2056, epoch: 62, batch: 9, loss: 0.18831729888916016, acc: 90.625, f1: 85.40145155051988, r: 0.6515543617307176
06/02/2019 09:32:03 step: 2061, epoch: 62, batch: 14, loss: 0.3569735288619995, acc: 87.5, f1: 65.6753663003663, r: 0.6389415326540839
06/02/2019 09:32:04 step: 2066, epoch: 62, batch: 19, loss: 0.24666312336921692, acc: 92.1875, f1: 78.08052066980639, r: 0.5702361355340021
06/02/2019 09:32:04 step: 2071, epoch: 62, batch: 24, loss: 0.3815247714519501, acc: 87.5, f1: 85.0256435697862, r: 0.7582299681253502
06/02/2019 09:32:05 step: 2076, epoch: 62, batch: 29, loss: 0.30972421169281006, acc: 84.375, f1: 69.34181718664478, r: 0.6425900067413959
06/02/2019 09:32:05 *** evaluating ***
06/02/2019 09:32:05 step: 63, epoch: 62, acc: 58.97435897435898, f1: 21.9455460929745, r: 0.27385243164099976
06/02/2019 09:32:05 *** epoch: 64 ***
06/02/2019 09:32:05 *** training ***
06/02/2019 09:32:06 step: 2084, epoch: 63, batch: 4, loss: 0.16284622251987457, acc: 93.75, f1: 85.74618736383444, r: 0.6887384800825729
06/02/2019 09:32:06 step: 2089, epoch: 63, batch: 9, loss: 0.1920129358768463, acc: 90.625, f1: 89.62076784736882, r: 0.5985652567153606
06/02/2019 09:32:07 step: 2094, epoch: 63, batch: 14, loss: 0.2640018165111542, acc: 87.5, f1: 88.1210407239819, r: 0.7562909847404805
06/02/2019 09:32:07 step: 2099, epoch: 63, batch: 19, loss: 0.17083534598350525, acc: 90.625, f1: 92.76307324694422, r: 0.680831943871994
06/02/2019 09:32:08 step: 2104, epoch: 63, batch: 24, loss: 0.2429247796535492, acc: 90.625, f1: 88.01779984504236, r: 0.6800080841563824
06/02/2019 09:32:08 step: 2109, epoch: 63, batch: 29, loss: 0.25802281498908997, acc: 87.5, f1: 66.41534391534391, r: 0.729581805202472
06/02/2019 09:32:09 *** evaluating ***
06/02/2019 09:32:09 step: 64, epoch: 63, acc: 58.119658119658126, f1: 22.284793646852986, r: 0.2808206647218336
06/02/2019 09:32:09 *** epoch: 65 ***
06/02/2019 09:32:09 *** training ***
06/02/2019 09:32:09 step: 2117, epoch: 64, batch: 4, loss: 0.21604454517364502, acc: 93.75, f1: 81.50623885918003, r: 0.7659126823557217
06/02/2019 09:32:10 step: 2122, epoch: 64, batch: 9, loss: 0.34957456588745117, acc: 87.5, f1: 70.93657902032288, r: 0.6312013808237714
06/02/2019 09:32:10 step: 2127, epoch: 64, batch: 14, loss: 0.22616587579250336, acc: 89.0625, f1: 88.32334459336748, r: 0.6348486103148904
06/02/2019 09:32:11 step: 2132, epoch: 64, batch: 19, loss: 0.12199453264474869, acc: 92.1875, f1: 78.11294329052376, r: 0.6018737816827057
06/02/2019 09:32:12 step: 2137, epoch: 64, batch: 24, loss: 0.24868999421596527, acc: 89.0625, f1: 75.92582101896377, r: 0.6875063187905395
06/02/2019 09:32:12 step: 2142, epoch: 64, batch: 29, loss: 0.2661803066730499, acc: 89.0625, f1: 82.00814536340853, r: 0.678602679772489
06/02/2019 09:32:12 *** evaluating ***
06/02/2019 09:32:13 step: 65, epoch: 64, acc: 58.54700854700855, f1: 21.702441596984617, r: 0.26904597022701526
06/02/2019 09:32:13 *** epoch: 66 ***
06/02/2019 09:32:13 *** training ***
06/02/2019 09:32:13 step: 2150, epoch: 65, batch: 4, loss: 0.16286201775074005, acc: 93.75, f1: 92.98269020251779, r: 0.7910185425729925
06/02/2019 09:32:14 step: 2155, epoch: 65, batch: 9, loss: 0.15652546286582947, acc: 98.4375, f1: 98.67669172932331, r: 0.7104708885270539
06/02/2019 09:32:14 step: 2160, epoch: 65, batch: 14, loss: 0.16169385612010956, acc: 90.625, f1: 88.85354141656663, r: 0.6699789005409695
06/02/2019 09:32:15 step: 2165, epoch: 65, batch: 19, loss: 0.25634342432022095, acc: 85.9375, f1: 81.6247418507633, r: 0.651122474956305
06/02/2019 09:32:15 step: 2170, epoch: 65, batch: 24, loss: 0.324391633272171, acc: 92.1875, f1: 85.67018398268398, r: 0.7584439745608081
06/02/2019 09:32:16 step: 2175, epoch: 65, batch: 29, loss: 0.12958501279354095, acc: 95.3125, f1: 91.66298261125847, r: 0.7228489761870861
06/02/2019 09:32:16 *** evaluating ***
06/02/2019 09:32:16 step: 66, epoch: 65, acc: 56.41025641025641, f1: 21.174288713009954, r: 0.25774624607356056
06/02/2019 09:32:16 *** epoch: 67 ***
06/02/2019 09:32:16 *** training ***
06/02/2019 09:32:17 step: 2183, epoch: 66, batch: 4, loss: 0.19353672862052917, acc: 90.625, f1: 80.03538257408226, r: 0.7144953214406757
06/02/2019 09:32:17 step: 2188, epoch: 66, batch: 9, loss: 0.2045082002878189, acc: 90.625, f1: 78.76146409980997, r: 0.5967184142953906
06/02/2019 09:32:18 step: 2193, epoch: 66, batch: 14, loss: 0.30332836508750916, acc: 85.9375, f1: 67.72086652176027, r: 0.690599964405702
06/02/2019 09:32:18 step: 2198, epoch: 66, batch: 19, loss: 0.1869250237941742, acc: 93.75, f1: 88.68511186525893, r: 0.6694155376810348
06/02/2019 09:32:19 step: 2203, epoch: 66, batch: 24, loss: 0.15706376731395721, acc: 92.1875, f1: 83.19498556998556, r: 0.810763582323183
06/02/2019 09:32:19 step: 2208, epoch: 66, batch: 29, loss: 0.24082697927951813, acc: 90.625, f1: 66.20896464646464, r: 0.6366902849832575
06/02/2019 09:32:20 *** evaluating ***
06/02/2019 09:32:20 step: 67, epoch: 66, acc: 56.837606837606835, f1: 20.998417621368436, r: 0.26169127989945423
06/02/2019 09:32:20 *** epoch: 68 ***
06/02/2019 09:32:20 *** training ***
06/02/2019 09:32:20 step: 2216, epoch: 67, batch: 4, loss: 0.17563647031784058, acc: 95.3125, f1: 93.8059434300036, r: 0.6767823473831337
06/02/2019 09:32:21 step: 2221, epoch: 67, batch: 9, loss: 0.12286939471960068, acc: 93.75, f1: 89.88893459481694, r: 0.6535417791401134
06/02/2019 09:32:21 step: 2226, epoch: 67, batch: 14, loss: 0.22886796295642853, acc: 90.625, f1: 87.22325293753866, r: 0.6117966221452421
06/02/2019 09:32:22 step: 2231, epoch: 67, batch: 19, loss: 0.15340366959571838, acc: 95.3125, f1: 95.2190170940171, r: 0.7345135228921487
06/02/2019 09:32:22 step: 2236, epoch: 67, batch: 24, loss: 0.3040604293346405, acc: 87.5, f1: 89.6449718863512, r: 0.7193868218240309
06/02/2019 09:32:23 step: 2241, epoch: 67, batch: 29, loss: 0.24592140316963196, acc: 90.625, f1: 85.65081181213905, r: 0.6414312932389078
06/02/2019 09:32:23 *** evaluating ***
06/02/2019 09:32:24 step: 68, epoch: 67, acc: 56.41025641025641, f1: 20.55239351485379, r: 0.2647935169304483
06/02/2019 09:32:24 *** epoch: 69 ***
06/02/2019 09:32:24 *** training ***
06/02/2019 09:32:24 step: 2249, epoch: 68, batch: 4, loss: 0.1620757281780243, acc: 89.0625, f1: 76.2533422459893, r: 0.6316803499127065
06/02/2019 09:32:25 step: 2254, epoch: 68, batch: 9, loss: 0.14167623221874237, acc: 93.75, f1: 93.36057620540379, r: 0.7783409099453545
06/02/2019 09:32:25 step: 2259, epoch: 68, batch: 14, loss: 0.339693546295166, acc: 84.375, f1: 72.00979483376591, r: 0.6770837779012435
06/02/2019 09:32:26 step: 2264, epoch: 68, batch: 19, loss: 0.23514336347579956, acc: 90.625, f1: 86.98684210526315, r: 0.6633237093489017
06/02/2019 09:32:26 step: 2269, epoch: 68, batch: 24, loss: 0.18810129165649414, acc: 90.625, f1: 87.3961038961039, r: 0.7779124871311639
06/02/2019 09:32:27 step: 2274, epoch: 68, batch: 29, loss: 0.21907299757003784, acc: 92.1875, f1: 90.94822448083318, r: 0.7681567338496748
06/02/2019 09:32:27 *** evaluating ***
06/02/2019 09:32:27 step: 69, epoch: 68, acc: 57.692307692307686, f1: 20.827418621235687, r: 0.2597586542107345
06/02/2019 09:32:27 *** epoch: 70 ***
06/02/2019 09:32:27 *** training ***
06/02/2019 09:32:28 step: 2282, epoch: 69, batch: 4, loss: 0.22751446068286896, acc: 92.1875, f1: 82.92953196604313, r: 0.6545738273397811
06/02/2019 09:32:28 step: 2287, epoch: 69, batch: 9, loss: 0.2564745247364044, acc: 85.9375, f1: 69.75311147186147, r: 0.6488311776243818
06/02/2019 09:32:29 step: 2292, epoch: 69, batch: 14, loss: 0.17860886454582214, acc: 93.75, f1: 79.26358249446706, r: 0.6706559856852478
06/02/2019 09:32:29 step: 2297, epoch: 69, batch: 19, loss: 0.21695807576179504, acc: 89.0625, f1: 80.3061224489796, r: 0.6758816773441081
06/02/2019 09:32:30 step: 2302, epoch: 69, batch: 24, loss: 0.18055590987205505, acc: 93.75, f1: 79.40481418742287, r: 0.7066321804357308
06/02/2019 09:32:30 step: 2307, epoch: 69, batch: 29, loss: 0.2749485373497009, acc: 87.5, f1: 72.0809019509853, r: 0.6638120454812076
06/02/2019 09:32:31 *** evaluating ***
06/02/2019 09:32:31 step: 70, epoch: 69, acc: 57.692307692307686, f1: 21.5940194975628, r: 0.27661610121001173
06/02/2019 09:32:31 *** epoch: 71 ***
06/02/2019 09:32:31 *** training ***
06/02/2019 09:32:31 step: 2315, epoch: 70, batch: 4, loss: 0.309317409992218, acc: 84.375, f1: 80.96842580252192, r: 0.6939450672289802
06/02/2019 09:32:32 step: 2320, epoch: 70, batch: 9, loss: 0.20929186046123505, acc: 93.75, f1: 93.98565223190725, r: 0.6714326843292164
06/02/2019 09:32:32 step: 2325, epoch: 70, batch: 14, loss: 0.1665460616350174, acc: 92.1875, f1: 77.47061287858253, r: 0.6159662394797158
06/02/2019 09:32:33 step: 2330, epoch: 70, batch: 19, loss: 0.27862149477005005, acc: 85.9375, f1: 81.93971818915104, r: 0.7274776754852057
06/02/2019 09:32:33 step: 2335, epoch: 70, batch: 24, loss: 0.1728747934103012, acc: 93.75, f1: 85.13849984438221, r: 0.6552354635977363
06/02/2019 09:32:34 step: 2340, epoch: 70, batch: 29, loss: 0.1564399003982544, acc: 95.3125, f1: 94.90685045948204, r: 0.7086662981415612
06/02/2019 09:32:34 *** evaluating ***
06/02/2019 09:32:34 step: 71, epoch: 70, acc: 57.692307692307686, f1: 22.067877357882384, r: 0.2673169004360549
06/02/2019 09:32:34 *** epoch: 72 ***
06/02/2019 09:32:34 *** training ***
06/02/2019 09:32:35 step: 2348, epoch: 71, batch: 4, loss: 0.20937176048755646, acc: 90.625, f1: 88.49718130569195, r: 0.6885536292842915
06/02/2019 09:32:35 step: 2353, epoch: 71, batch: 9, loss: 0.2284649908542633, acc: 90.625, f1: 80.32397959183673, r: 0.7058309274137102
06/02/2019 09:32:36 step: 2358, epoch: 71, batch: 14, loss: 0.14765655994415283, acc: 93.75, f1: 68.06006493506493, r: 0.6996495702647091
06/02/2019 09:32:36 step: 2363, epoch: 71, batch: 19, loss: 0.21939115226268768, acc: 92.1875, f1: 77.35343665768194, r: 0.6451592136647788
06/02/2019 09:32:37 step: 2368, epoch: 71, batch: 24, loss: 0.22788123786449432, acc: 92.1875, f1: 79.41850255427842, r: 0.6425725627431668
06/02/2019 09:32:37 step: 2373, epoch: 71, batch: 29, loss: 0.1464167982339859, acc: 93.75, f1: 86.94154148473912, r: 0.6610863732229625
06/02/2019 09:32:38 *** evaluating ***
06/02/2019 09:32:38 step: 72, epoch: 71, acc: 58.119658119658126, f1: 21.854205702131967, r: 0.2665233298436303
06/02/2019 09:32:38 *** epoch: 73 ***
06/02/2019 09:32:38 *** training ***
06/02/2019 09:32:39 step: 2381, epoch: 72, batch: 4, loss: 0.21817244589328766, acc: 90.625, f1: 86.667817185644, r: 0.7159004469481556
06/02/2019 09:32:39 step: 2386, epoch: 72, batch: 9, loss: 0.22073832154273987, acc: 90.625, f1: 78.79862700228833, r: 0.7557589149978362
06/02/2019 09:32:40 step: 2391, epoch: 72, batch: 14, loss: 0.3225058317184448, acc: 85.9375, f1: 71.83269687301946, r: 0.6918941441458639
06/02/2019 09:32:40 step: 2396, epoch: 72, batch: 19, loss: 0.1870400756597519, acc: 93.75, f1: 87.0945521065569, r: 0.6446193114850685
06/02/2019 09:32:41 step: 2401, epoch: 72, batch: 24, loss: 0.23078250885009766, acc: 89.0625, f1: 75.45597340649488, r: 0.6581933589192702
06/02/2019 09:32:41 step: 2406, epoch: 72, batch: 29, loss: 0.17384783923625946, acc: 92.1875, f1: 87.13718820861678, r: 0.7154696706736422
06/02/2019 09:32:41 *** evaluating ***
06/02/2019 09:32:42 step: 73, epoch: 72, acc: 56.837606837606835, f1: 20.722672790046254, r: 0.2634358082358589
06/02/2019 09:32:42 *** epoch: 74 ***
06/02/2019 09:32:42 *** training ***
06/02/2019 09:32:42 step: 2414, epoch: 73, batch: 4, loss: 0.2690731883049011, acc: 87.5, f1: 72.37839679700146, r: 0.5673134980242946
06/02/2019 09:32:43 step: 2419, epoch: 73, batch: 9, loss: 0.16233189404010773, acc: 92.1875, f1: 86.01732468324782, r: 0.8033339891377094
06/02/2019 09:32:43 step: 2424, epoch: 73, batch: 14, loss: 0.18127234280109406, acc: 93.75, f1: 84.0322640430399, r: 0.6693677704004084
06/02/2019 09:32:44 step: 2429, epoch: 73, batch: 19, loss: 0.243965744972229, acc: 89.0625, f1: 84.10402097902097, r: 0.705794214765457
06/02/2019 09:32:44 step: 2434, epoch: 73, batch: 24, loss: 0.17133599519729614, acc: 93.75, f1: 91.0176282051282, r: 0.6789123084932229
06/02/2019 09:32:45 step: 2439, epoch: 73, batch: 29, loss: 0.09603039920330048, acc: 98.4375, f1: 96.65024630541872, r: 0.6808157052042221
06/02/2019 09:32:45 *** evaluating ***
06/02/2019 09:32:45 step: 74, epoch: 73, acc: 57.26495726495726, f1: 21.099891679077327, r: 0.2550710360487713
06/02/2019 09:32:45 *** epoch: 75 ***
06/02/2019 09:32:45 *** training ***
06/02/2019 09:32:46 step: 2447, epoch: 74, batch: 4, loss: 0.23699907958507538, acc: 87.5, f1: 82.09837781266353, r: 0.68620313010218
06/02/2019 09:32:46 step: 2452, epoch: 74, batch: 9, loss: 0.2339678853750229, acc: 87.5, f1: 76.00740068754774, r: 0.6765726255001859
06/02/2019 09:32:47 step: 2457, epoch: 74, batch: 14, loss: 0.23139767348766327, acc: 89.0625, f1: 84.47377501725329, r: 0.559751958490523
06/02/2019 09:32:47 step: 2462, epoch: 74, batch: 19, loss: 0.19682590663433075, acc: 89.0625, f1: 61.8081174059435, r: 0.564631222459543
06/02/2019 09:32:48 step: 2467, epoch: 74, batch: 24, loss: 0.20411188900470734, acc: 89.0625, f1: 86.49659863945578, r: 0.6590625053030437
06/02/2019 09:32:48 step: 2472, epoch: 74, batch: 29, loss: 0.23992101848125458, acc: 85.9375, f1: 71.39601878258865, r: 0.5893744058505377
06/02/2019 09:32:49 *** evaluating ***
06/02/2019 09:32:49 step: 75, epoch: 74, acc: 58.119658119658126, f1: 21.26601511246376, r: 0.25841218984962633
06/02/2019 09:32:49 *** epoch: 76 ***
06/02/2019 09:32:49 *** training ***
06/02/2019 09:32:49 step: 2480, epoch: 75, batch: 4, loss: 0.23603780567646027, acc: 87.5, f1: 71.6055154792601, r: 0.7336878841656823
06/02/2019 09:32:50 step: 2485, epoch: 75, batch: 9, loss: 0.1494600474834442, acc: 93.75, f1: 91.87376686396843, r: 0.7520832102926881
06/02/2019 09:32:51 step: 2490, epoch: 75, batch: 14, loss: 0.12499665468931198, acc: 93.75, f1: 85.57940478244085, r: 0.6460337892672863
06/02/2019 09:32:51 step: 2495, epoch: 75, batch: 19, loss: 0.18931631743907928, acc: 90.625, f1: 73.9263789371548, r: 0.7026569019733765
06/02/2019 09:32:52 step: 2500, epoch: 75, batch: 24, loss: 0.11261885613203049, acc: 93.75, f1: 79.59262959262959, r: 0.6008924993866102
06/02/2019 09:32:52 step: 2505, epoch: 75, batch: 29, loss: 0.2103765308856964, acc: 90.625, f1: 75.70198141626713, r: 0.5750083541712983
06/02/2019 09:32:52 *** evaluating ***
06/02/2019 09:32:53 step: 76, epoch: 75, acc: 57.692307692307686, f1: 21.589521589521592, r: 0.2834198470164925
06/02/2019 09:32:53 *** epoch: 77 ***
06/02/2019 09:32:53 *** training ***
06/02/2019 09:32:53 step: 2513, epoch: 76, batch: 4, loss: 0.1210029125213623, acc: 95.3125, f1: 96.0774764451235, r: 0.7469308891332892
06/02/2019 09:32:54 step: 2518, epoch: 76, batch: 9, loss: 0.19569110870361328, acc: 93.75, f1: 87.12945393211048, r: 0.726085794065298
06/02/2019 09:32:54 step: 2523, epoch: 76, batch: 14, loss: 0.12399108707904816, acc: 96.875, f1: 93.66459627329192, r: 0.7276412476542399
06/02/2019 09:32:55 step: 2528, epoch: 76, batch: 19, loss: 0.23015812039375305, acc: 87.5, f1: 69.05063686313686, r: 0.6989852533602458
06/02/2019 09:32:55 step: 2533, epoch: 76, batch: 24, loss: 0.22700819373130798, acc: 92.1875, f1: 89.58789312826579, r: 0.6731880531506967
06/02/2019 09:32:56 step: 2538, epoch: 76, batch: 29, loss: 0.15721821784973145, acc: 93.75, f1: 81.384484228474, r: 0.6754004044199888
06/02/2019 09:32:56 *** evaluating ***
06/02/2019 09:32:56 step: 77, epoch: 76, acc: 58.119658119658126, f1: 21.535176651305683, r: 0.2555139813339225
06/02/2019 09:32:56 *** epoch: 78 ***
06/02/2019 09:32:56 *** training ***
06/02/2019 09:32:57 step: 2546, epoch: 77, batch: 4, loss: 0.1558527946472168, acc: 93.75, f1: 92.83808385014085, r: 0.6942262971553762
06/02/2019 09:32:57 step: 2551, epoch: 77, batch: 9, loss: 0.21557016670703888, acc: 90.625, f1: 83.49931318681318, r: 0.7162220446845289
06/02/2019 09:32:58 step: 2556, epoch: 77, batch: 14, loss: 0.1370171755552292, acc: 93.75, f1: 90.81849761181068, r: 0.6541434932150471
06/02/2019 09:32:58 step: 2561, epoch: 77, batch: 19, loss: 0.2138555496931076, acc: 92.1875, f1: 87.26939655172414, r: 0.7050170715253486
06/02/2019 09:32:59 step: 2566, epoch: 77, batch: 24, loss: 0.27191174030303955, acc: 89.0625, f1: 76.79744489227247, r: 0.6625523583633439
06/02/2019 09:32:59 step: 2571, epoch: 77, batch: 29, loss: 0.2771080434322357, acc: 87.5, f1: 73.27492789623726, r: 0.6390607394080261
06/02/2019 09:33:00 *** evaluating ***
06/02/2019 09:33:00 step: 78, epoch: 77, acc: 58.54700854700855, f1: 21.746093944369807, r: 0.26917265740832563
06/02/2019 09:33:00 *** epoch: 79 ***
06/02/2019 09:33:00 *** training ***
06/02/2019 09:33:01 step: 2579, epoch: 78, batch: 4, loss: 0.24153298139572144, acc: 90.625, f1: 79.0514122315593, r: 0.6115835440411167
06/02/2019 09:33:01 step: 2584, epoch: 78, batch: 9, loss: 0.32563936710357666, acc: 92.1875, f1: 87.16160297594828, r: 0.6595100723246743
06/02/2019 09:33:02 step: 2589, epoch: 78, batch: 14, loss: 0.2639980614185333, acc: 92.1875, f1: 76.93877551020408, r: 0.5918120526304131
06/02/2019 09:33:02 step: 2594, epoch: 78, batch: 19, loss: 0.28080514073371887, acc: 89.0625, f1: 73.60724896019015, r: 0.6667240798310712
06/02/2019 09:33:03 step: 2599, epoch: 78, batch: 24, loss: 0.18871718645095825, acc: 95.3125, f1: 93.36796536796538, r: 0.6745978266766045
06/02/2019 09:33:03 step: 2604, epoch: 78, batch: 29, loss: 0.16021832823753357, acc: 95.3125, f1: 93.07290535551405, r: 0.7069959200206051
06/02/2019 09:33:04 *** evaluating ***
06/02/2019 09:33:04 step: 79, epoch: 78, acc: 59.401709401709404, f1: 21.692077415761627, r: 0.2600042221801431
06/02/2019 09:33:04 *** epoch: 80 ***
06/02/2019 09:33:04 *** training ***
06/02/2019 09:33:04 step: 2612, epoch: 79, batch: 4, loss: 0.20892421901226044, acc: 89.0625, f1: 85.44419761525025, r: 0.7757675444519134
06/02/2019 09:33:05 step: 2617, epoch: 79, batch: 9, loss: 0.3519842028617859, acc: 85.9375, f1: 81.03276353276354, r: 0.5350094973384215
06/02/2019 09:33:05 step: 2622, epoch: 79, batch: 14, loss: 0.26634958386421204, acc: 89.0625, f1: 73.9484126984127, r: 0.7023684562548492
06/02/2019 09:33:06 step: 2627, epoch: 79, batch: 19, loss: 0.19341279566287994, acc: 92.1875, f1: 90.57056668821375, r: 0.6705539841280053
06/02/2019 09:33:06 step: 2632, epoch: 79, batch: 24, loss: 0.15809345245361328, acc: 93.75, f1: 91.83770098948669, r: 0.7350118395058828
06/02/2019 09:33:07 step: 2637, epoch: 79, batch: 29, loss: 0.23988273739814758, acc: 87.5, f1: 79.62236176521891, r: 0.6138860344668693
06/02/2019 09:33:07 *** evaluating ***
06/02/2019 09:33:07 step: 80, epoch: 79, acc: 57.26495726495726, f1: 21.885674809061907, r: 0.26698349657367004
06/02/2019 09:33:07 *** epoch: 81 ***
06/02/2019 09:33:07 *** training ***
06/02/2019 09:33:08 step: 2645, epoch: 80, batch: 4, loss: 0.13638710975646973, acc: 93.75, f1: 80.19782003710574, r: 0.7533981133983957
06/02/2019 09:33:08 step: 2650, epoch: 80, batch: 9, loss: 0.21897311508655548, acc: 90.625, f1: 85.75755576956058, r: 0.6371838600224347
06/02/2019 09:33:09 step: 2655, epoch: 80, batch: 14, loss: 0.14916068315505981, acc: 93.75, f1: 87.98046398046398, r: 0.7000332891058754
06/02/2019 09:33:09 step: 2660, epoch: 80, batch: 19, loss: 0.3050483167171478, acc: 89.0625, f1: 67.07511473488917, r: 0.5637708279783692
06/02/2019 09:33:10 step: 2665, epoch: 80, batch: 24, loss: 0.18198950588703156, acc: 90.625, f1: 74.60768398268398, r: 0.715774612187836
06/02/2019 09:33:10 step: 2670, epoch: 80, batch: 29, loss: 0.10658007115125656, acc: 96.875, f1: 94.92229992229993, r: 0.724275999362199
06/02/2019 09:33:11 *** evaluating ***
06/02/2019 09:33:11 step: 81, epoch: 80, acc: 59.401709401709404, f1: 22.11739837832071, r: 0.26230464814598764
06/02/2019 09:33:11 *** epoch: 82 ***
06/02/2019 09:33:11 *** training ***
06/02/2019 09:33:11 step: 2678, epoch: 81, batch: 4, loss: 0.10164004564285278, acc: 95.3125, f1: 95.01170960187353, r: 0.6813496393884306
06/02/2019 09:33:12 step: 2683, epoch: 81, batch: 9, loss: 0.22229349613189697, acc: 92.1875, f1: 85.55197517603534, r: 0.6601467331626074
06/02/2019 09:33:12 step: 2688, epoch: 81, batch: 14, loss: 0.1462826430797577, acc: 92.1875, f1: 91.28461641066681, r: 0.6594093436110927
06/02/2019 09:33:13 step: 2693, epoch: 81, batch: 19, loss: 0.1866511106491089, acc: 93.75, f1: 88.14393939393939, r: 0.6768302683505639
06/02/2019 09:33:14 step: 2698, epoch: 81, batch: 24, loss: 0.1720428764820099, acc: 92.1875, f1: 89.81481481481481, r: 0.7621037952733511
06/02/2019 09:33:14 step: 2703, epoch: 81, batch: 29, loss: 0.3198585510253906, acc: 84.375, f1: 70.56391010910463, r: 0.6872139188721017
06/02/2019 09:33:14 *** evaluating ***
06/02/2019 09:33:15 step: 82, epoch: 81, acc: 56.837606837606835, f1: 21.011297080119462, r: 0.26096422560833055
06/02/2019 09:33:15 *** epoch: 83 ***
06/02/2019 09:33:15 *** training ***
06/02/2019 09:33:15 step: 2711, epoch: 82, batch: 4, loss: 0.2003132402896881, acc: 89.0625, f1: 52.330128205128204, r: 0.502486777455208
06/02/2019 09:33:16 step: 2716, epoch: 82, batch: 9, loss: 0.17949844896793365, acc: 93.75, f1: 81.53400363077782, r: 0.760347884790317
06/02/2019 09:33:16 step: 2721, epoch: 82, batch: 14, loss: 0.1729002147912979, acc: 93.75, f1: 89.26489166377942, r: 0.7494134552476254
06/02/2019 09:33:17 step: 2726, epoch: 82, batch: 19, loss: 0.2557147741317749, acc: 87.5, f1: 83.5664589475565, r: 0.7199134831083635
06/02/2019 09:33:17 step: 2731, epoch: 82, batch: 24, loss: 0.14336028695106506, acc: 93.75, f1: 82.31227106227107, r: 0.7122541779506175
06/02/2019 09:33:18 step: 2736, epoch: 82, batch: 29, loss: 0.2299296259880066, acc: 87.5, f1: 68.36607142857143, r: 0.678719935389066
06/02/2019 09:33:18 *** evaluating ***
06/02/2019 09:33:18 step: 83, epoch: 82, acc: 56.837606837606835, f1: 21.576823817298564, r: 0.26887668228613815
06/02/2019 09:33:18 *** epoch: 84 ***
06/02/2019 09:33:18 *** training ***
06/02/2019 09:33:19 step: 2744, epoch: 83, batch: 4, loss: 0.13175681233406067, acc: 93.75, f1: 81.19270833333333, r: 0.7045461184525527
06/02/2019 09:33:19 step: 2749, epoch: 83, batch: 9, loss: 0.1858176589012146, acc: 93.75, f1: 91.72486772486774, r: 0.7148112583587672
06/02/2019 09:33:20 step: 2754, epoch: 83, batch: 14, loss: 0.26081937551498413, acc: 85.9375, f1: 82.9211368341803, r: 0.7235488602566731
06/02/2019 09:33:20 step: 2759, epoch: 83, batch: 19, loss: 0.1499432623386383, acc: 93.75, f1: 84.9874686716792, r: 0.6872346016808601
06/02/2019 09:33:21 step: 2764, epoch: 83, batch: 24, loss: 0.2671178877353668, acc: 87.5, f1: 77.64113448997169, r: 0.7193149814197849
06/02/2019 09:33:21 step: 2769, epoch: 83, batch: 29, loss: 0.20581123232841492, acc: 89.0625, f1: 81.85196995464852, r: 0.708010402934803
06/02/2019 09:33:22 *** evaluating ***
06/02/2019 09:33:22 step: 84, epoch: 83, acc: 58.119658119658126, f1: 21.813651464814257, r: 0.25436655913410217
06/02/2019 09:33:22 *** epoch: 85 ***
06/02/2019 09:33:22 *** training ***
06/02/2019 09:33:22 step: 2777, epoch: 84, batch: 4, loss: 0.16752412915229797, acc: 93.75, f1: 92.80633520840658, r: 0.7462047739416828
06/02/2019 09:33:23 step: 2782, epoch: 84, batch: 9, loss: 0.10424380749464035, acc: 96.875, f1: 94.9023199023199, r: 0.7190964800850215
06/02/2019 09:33:23 step: 2787, epoch: 84, batch: 14, loss: 0.14517110586166382, acc: 95.3125, f1: 70.78373015873017, r: 0.6882557152761795
06/02/2019 09:33:24 step: 2792, epoch: 84, batch: 19, loss: 0.13419124484062195, acc: 95.3125, f1: 88.15606769926532, r: 0.659965449314488
06/02/2019 09:33:24 step: 2797, epoch: 84, batch: 24, loss: 0.10167425125837326, acc: 95.3125, f1: 83.91239986067573, r: 0.6605698919225348
06/02/2019 09:33:25 step: 2802, epoch: 84, batch: 29, loss: 0.19820767641067505, acc: 90.625, f1: 75.35299082468893, r: 0.6298212247261613
06/02/2019 09:33:25 *** evaluating ***
06/02/2019 09:33:25 step: 85, epoch: 84, acc: 57.692307692307686, f1: 21.22014357013223, r: 0.25805528589225996
06/02/2019 09:33:25 *** epoch: 86 ***
06/02/2019 09:33:25 *** training ***
06/02/2019 09:33:26 step: 2810, epoch: 85, batch: 4, loss: 0.32894372940063477, acc: 85.9375, f1: 62.72321428571429, r: 0.5859746068113126
06/02/2019 09:33:26 step: 2815, epoch: 85, batch: 9, loss: 0.02686898410320282, acc: 100.0, f1: 100.0, r: 0.6535264394458286
06/02/2019 09:33:27 step: 2820, epoch: 85, batch: 14, loss: 0.1399611383676529, acc: 93.75, f1: 91.82436611008039, r: 0.6683608639131754
06/02/2019 09:33:27 step: 2825, epoch: 85, batch: 19, loss: 0.11396259069442749, acc: 95.3125, f1: 92.24376921387791, r: 0.8056689608493256
06/02/2019 09:33:28 step: 2830, epoch: 85, batch: 24, loss: 0.20827965438365936, acc: 93.75, f1: 90.75448361162647, r: 0.5569452485393771
06/02/2019 09:33:28 step: 2835, epoch: 85, batch: 29, loss: 0.180900439620018, acc: 92.1875, f1: 85.61605767114241, r: 0.7584556771274793
06/02/2019 09:33:29 *** evaluating ***
06/02/2019 09:33:29 step: 86, epoch: 85, acc: 56.837606837606835, f1: 21.524415482215993, r: 0.2571784198399787
06/02/2019 09:33:29 *** epoch: 87 ***
06/02/2019 09:33:29 *** training ***
06/02/2019 09:33:29 step: 2843, epoch: 86, batch: 4, loss: 0.13252727687358856, acc: 96.875, f1: 97.75752314814814, r: 0.7324187418603794
06/02/2019 09:33:30 step: 2848, epoch: 86, batch: 9, loss: 0.08909225463867188, acc: 98.4375, f1: 97.00680272108845, r: 0.6811096150474998
06/02/2019 09:33:31 step: 2853, epoch: 86, batch: 14, loss: 0.1771419644355774, acc: 90.625, f1: 73.89827327327328, r: 0.6423228408746928
06/02/2019 09:33:31 step: 2858, epoch: 86, batch: 19, loss: 0.2879437804222107, acc: 87.5, f1: 72.18987595038016, r: 0.6760907074441151
06/02/2019 09:33:32 step: 2863, epoch: 86, batch: 24, loss: 0.11375416070222855, acc: 96.875, f1: 81.87179487179486, r: 0.6271421955312859
06/02/2019 09:33:32 step: 2868, epoch: 86, batch: 29, loss: 0.14796210825443268, acc: 92.1875, f1: 78.57577220077219, r: 0.6731845139111561
06/02/2019 09:33:32 *** evaluating ***
06/02/2019 09:33:33 step: 87, epoch: 86, acc: 57.692307692307686, f1: 21.221186087778456, r: 0.23901172388909944
06/02/2019 09:33:33 *** epoch: 88 ***
06/02/2019 09:33:33 *** training ***
06/02/2019 09:33:33 step: 2876, epoch: 87, batch: 4, loss: 0.10252217203378677, acc: 95.3125, f1: 93.05973615756224, r: 0.5285186800230887
06/02/2019 09:33:34 step: 2881, epoch: 87, batch: 9, loss: 0.14472171664237976, acc: 92.1875, f1: 84.72283775170148, r: 0.6251160972629158
06/02/2019 09:33:34 step: 2886, epoch: 87, batch: 14, loss: 0.201310396194458, acc: 93.75, f1: 93.10545776063017, r: 0.6661458349345264
06/02/2019 09:33:35 step: 2891, epoch: 87, batch: 19, loss: 0.20124979317188263, acc: 93.75, f1: 88.10921717171718, r: 0.7373436067723288
06/02/2019 09:33:35 step: 2896, epoch: 87, batch: 24, loss: 0.14526142179965973, acc: 90.625, f1: 77.69043357947453, r: 0.6328739034015407
06/02/2019 09:33:36 step: 2901, epoch: 87, batch: 29, loss: 0.1393105685710907, acc: 92.1875, f1: 82.60265445149166, r: 0.6735459637507843
06/02/2019 09:33:36 *** evaluating ***
06/02/2019 09:33:36 step: 88, epoch: 87, acc: 57.26495726495726, f1: 21.508348610566546, r: 0.24499518969478473
06/02/2019 09:33:36 *** epoch: 89 ***
06/02/2019 09:33:36 *** training ***
06/02/2019 09:33:37 step: 2909, epoch: 88, batch: 4, loss: 0.0830211192369461, acc: 93.75, f1: 93.86973180076629, r: 0.5861291362739627
06/02/2019 09:33:37 step: 2914, epoch: 88, batch: 9, loss: 0.19693607091903687, acc: 90.625, f1: 84.63991563991566, r: 0.6100719382498335
06/02/2019 09:33:38 step: 2919, epoch: 88, batch: 14, loss: 0.20062977075576782, acc: 90.625, f1: 85.0915750915751, r: 0.6350671980759983
06/02/2019 09:33:38 step: 2924, epoch: 88, batch: 19, loss: 0.09201415628194809, acc: 95.3125, f1: 96.04708279708281, r: 0.7885146997688732
06/02/2019 09:33:39 step: 2929, epoch: 88, batch: 24, loss: 0.22240641713142395, acc: 93.75, f1: 79.7490326341045, r: 0.712608931247755
06/02/2019 09:33:39 step: 2934, epoch: 88, batch: 29, loss: 0.19168752431869507, acc: 92.1875, f1: 76.48393848854678, r: 0.6724021617694065
06/02/2019 09:33:40 *** evaluating ***
06/02/2019 09:33:40 step: 89, epoch: 88, acc: 58.119658119658126, f1: 21.377729775295702, r: 0.25881815891439974
06/02/2019 09:33:40 *** epoch: 90 ***
06/02/2019 09:33:40 *** training ***
06/02/2019 09:33:40 step: 2942, epoch: 89, batch: 4, loss: 0.10125090926885605, acc: 95.3125, f1: 82.6096862710135, r: 0.694814142884028
06/02/2019 09:33:41 step: 2947, epoch: 89, batch: 9, loss: 0.16338835656642914, acc: 93.75, f1: 83.01587301587301, r: 0.710485907275704
06/02/2019 09:33:41 step: 2952, epoch: 89, batch: 14, loss: 0.2339576631784439, acc: 89.0625, f1: 82.94201455767886, r: 0.6534301784524894
06/02/2019 09:33:42 step: 2957, epoch: 89, batch: 19, loss: 0.07950003445148468, acc: 96.875, f1: 98.08211900425015, r: 0.7460053072657742
06/02/2019 09:33:42 step: 2962, epoch: 89, batch: 24, loss: 0.14588335156440735, acc: 93.75, f1: 80.13125763125764, r: 0.770744757380433
06/02/2019 09:33:43 step: 2967, epoch: 89, batch: 29, loss: 0.12420272082090378, acc: 93.75, f1: 93.59800750901454, r: 0.6924049514988575
06/02/2019 09:33:43 *** evaluating ***
06/02/2019 09:33:43 step: 90, epoch: 89, acc: 56.837606837606835, f1: 20.94942726395073, r: 0.24990162665338528
06/02/2019 09:33:43 *** epoch: 91 ***
06/02/2019 09:33:43 *** training ***
06/02/2019 09:33:44 step: 2975, epoch: 90, batch: 4, loss: 0.09110941737890244, acc: 96.875, f1: 95.98484848484848, r: 0.789952515616722
06/02/2019 09:33:44 step: 2980, epoch: 90, batch: 9, loss: 0.07359077036380768, acc: 95.3125, f1: 87.08220542897962, r: 0.7463397147397727
06/02/2019 09:33:45 step: 2985, epoch: 90, batch: 14, loss: 0.19695325195789337, acc: 93.75, f1: 88.639424730756, r: 0.7275110245360209
06/02/2019 09:33:45 step: 2990, epoch: 90, batch: 19, loss: 0.2821313440799713, acc: 85.9375, f1: 75.26909494014757, r: 0.6827519481320256
06/02/2019 09:33:46 step: 2995, epoch: 90, batch: 24, loss: 0.23151452839374542, acc: 89.0625, f1: 86.59632034632034, r: 0.7316271508806904
06/02/2019 09:33:46 step: 3000, epoch: 90, batch: 29, loss: 0.09850071370601654, acc: 95.3125, f1: 89.91763565891473, r: 0.739618700228609
06/02/2019 09:33:47 *** evaluating ***
06/02/2019 09:33:47 step: 91, epoch: 90, acc: 56.837606837606835, f1: 21.143372756275983, r: 0.2634386758521286
06/02/2019 09:33:47 *** epoch: 92 ***
06/02/2019 09:33:47 *** training ***
06/02/2019 09:33:47 step: 3008, epoch: 91, batch: 4, loss: 0.23168164491653442, acc: 89.0625, f1: 75.27625152625153, r: 0.6386709020165712
06/02/2019 09:33:48 step: 3013, epoch: 91, batch: 9, loss: 0.14273150265216827, acc: 92.1875, f1: 77.30821495264773, r: 0.724561387554495
06/02/2019 09:33:48 step: 3018, epoch: 91, batch: 14, loss: 0.1727166771888733, acc: 90.625, f1: 79.389338731444, r: 0.7234766465340489
06/02/2019 09:33:49 step: 3023, epoch: 91, batch: 19, loss: 0.11358163505792618, acc: 95.3125, f1: 83.93994082158976, r: 0.6912015973763005
06/02/2019 09:33:49 step: 3028, epoch: 91, batch: 24, loss: 0.13924051821231842, acc: 93.75, f1: 78.07115307553596, r: 0.6977907768848861
06/02/2019 09:33:50 step: 3033, epoch: 91, batch: 29, loss: 0.3285267651081085, acc: 85.9375, f1: 74.72070100967754, r: 0.5233740290527539
06/02/2019 09:33:50 *** evaluating ***
06/02/2019 09:33:50 step: 92, epoch: 91, acc: 57.692307692307686, f1: 21.188771719635685, r: 0.25788313485968817
06/02/2019 09:33:50 *** epoch: 93 ***
06/02/2019 09:33:50 *** training ***
06/02/2019 09:33:51 step: 3041, epoch: 92, batch: 4, loss: 0.1707010716199875, acc: 92.1875, f1: 77.2904995331466, r: 0.6866657487629189
06/02/2019 09:33:51 step: 3046, epoch: 92, batch: 9, loss: 0.09760244190692902, acc: 93.75, f1: 96.02216079035621, r: 0.7558573389242097
06/02/2019 09:33:52 step: 3051, epoch: 92, batch: 14, loss: 0.26993098855018616, acc: 89.0625, f1: 71.8924715635242, r: 0.7131264154832828
06/02/2019 09:33:52 step: 3056, epoch: 92, batch: 19, loss: 0.1266956925392151, acc: 98.4375, f1: 98.7153931339978, r: 0.6957747284473738
06/02/2019 09:33:53 step: 3061, epoch: 92, batch: 24, loss: 0.14891362190246582, acc: 93.75, f1: 92.76716439759919, r: 0.5354137039958512
06/02/2019 09:33:54 step: 3066, epoch: 92, batch: 29, loss: 0.14311008155345917, acc: 92.1875, f1: 81.76470588235294, r: 0.7329803571796684
06/02/2019 09:33:54 *** evaluating ***
06/02/2019 09:33:54 step: 93, epoch: 92, acc: 56.41025641025641, f1: 20.77300670624623, r: 0.253220420782874
06/02/2019 09:33:54 *** epoch: 94 ***
06/02/2019 09:33:54 *** training ***
06/02/2019 09:33:54 step: 3074, epoch: 93, batch: 4, loss: 0.18991592526435852, acc: 92.1875, f1: 76.39750409165302, r: 0.7209446723220708
06/02/2019 09:33:55 step: 3079, epoch: 93, batch: 9, loss: 0.10752512514591217, acc: 96.875, f1: 83.83152173913044, r: 0.7349584052363176
06/02/2019 09:33:55 step: 3084, epoch: 93, batch: 14, loss: 0.1131022572517395, acc: 95.3125, f1: 93.3103992781412, r: 0.6898899218537158
06/02/2019 09:33:56 step: 3089, epoch: 93, batch: 19, loss: 0.11808933317661285, acc: 95.3125, f1: 93.24010305876736, r: 0.7381412948319965
06/02/2019 09:33:56 step: 3094, epoch: 93, batch: 24, loss: 0.1960276961326599, acc: 90.625, f1: 79.96926759834369, r: 0.6808624884805109
06/02/2019 09:33:57 step: 3099, epoch: 93, batch: 29, loss: 0.09342078864574432, acc: 95.3125, f1: 91.64285714285715, r: 0.7927354396909556
06/02/2019 09:33:57 *** evaluating ***
06/02/2019 09:33:58 step: 94, epoch: 93, acc: 56.837606837606835, f1: 21.366658132787165, r: 0.2496555169417015
06/02/2019 09:33:58 *** epoch: 95 ***
06/02/2019 09:33:58 *** training ***
06/02/2019 09:33:58 step: 3107, epoch: 94, batch: 4, loss: 0.15869075059890747, acc: 90.625, f1: 87.32495765104461, r: 0.6639052686031833
06/02/2019 09:33:59 step: 3112, epoch: 94, batch: 9, loss: 0.13123449683189392, acc: 95.3125, f1: 91.26523386034255, r: 0.7104594869096872
06/02/2019 09:33:59 step: 3117, epoch: 94, batch: 14, loss: 0.30129358172416687, acc: 84.375, f1: 75.39115646258503, r: 0.6311481578563672
06/02/2019 09:34:00 step: 3122, epoch: 94, batch: 19, loss: 0.24258343875408173, acc: 92.1875, f1: 79.70427059712775, r: 0.6831492794037551
06/02/2019 09:34:00 step: 3127, epoch: 94, batch: 24, loss: 0.14643485844135284, acc: 95.3125, f1: 94.05284370801613, r: 0.6530519464416868
06/02/2019 09:34:01 step: 3132, epoch: 94, batch: 29, loss: 0.12638847529888153, acc: 93.75, f1: 78.34114959114959, r: 0.676234915705648
06/02/2019 09:34:01 *** evaluating ***
06/02/2019 09:34:01 step: 95, epoch: 94, acc: 57.26495726495726, f1: 21.937866869409355, r: 0.24262350544137154
06/02/2019 09:34:01 *** epoch: 96 ***
06/02/2019 09:34:01 *** training ***
06/02/2019 09:34:02 step: 3140, epoch: 95, batch: 4, loss: 0.14736220240592957, acc: 93.75, f1: 94.03822055137844, r: 0.7913990263940769
06/02/2019 09:34:02 step: 3145, epoch: 95, batch: 9, loss: 0.20030993223190308, acc: 92.1875, f1: 89.83465608465609, r: 0.796499664579791
06/02/2019 09:34:03 step: 3150, epoch: 95, batch: 14, loss: 0.193834125995636, acc: 92.1875, f1: 84.63347416472415, r: 0.7439369671016997
06/02/2019 09:34:03 step: 3155, epoch: 95, batch: 19, loss: 0.2091204822063446, acc: 90.625, f1: 82.6244417744908, r: 0.6324932491896023
06/02/2019 09:34:04 step: 3160, epoch: 95, batch: 24, loss: 0.15437428653240204, acc: 95.3125, f1: 91.40070921985814, r: 0.7501371206748688
06/02/2019 09:34:04 step: 3165, epoch: 95, batch: 29, loss: 0.16866883635520935, acc: 93.75, f1: 83.99764521193092, r: 0.6919807165674055
06/02/2019 09:34:05 *** evaluating ***
06/02/2019 09:34:05 step: 96, epoch: 95, acc: 57.26495726495726, f1: 21.39341633498898, r: 0.24801042440061352
06/02/2019 09:34:05 *** epoch: 97 ***
06/02/2019 09:34:05 *** training ***
06/02/2019 09:34:05 step: 3173, epoch: 96, batch: 4, loss: 0.14974741637706757, acc: 93.75, f1: 89.67510501468347, r: 0.6264866168854097
06/02/2019 09:34:06 step: 3178, epoch: 96, batch: 9, loss: 0.24831733107566833, acc: 87.5, f1: 80.48381242740999, r: 0.75455020305934
06/02/2019 09:34:06 step: 3183, epoch: 96, batch: 14, loss: 0.1225319355726242, acc: 96.875, f1: 95.64007421150278, r: 0.6860434179541206
06/02/2019 09:34:07 step: 3188, epoch: 96, batch: 19, loss: 0.12394730746746063, acc: 96.875, f1: 81.54761904761905, r: 0.7225805233470782
06/02/2019 09:34:07 step: 3193, epoch: 96, batch: 24, loss: 0.13270115852355957, acc: 90.625, f1: 77.33777198822793, r: 0.5877727921581947
06/02/2019 09:34:08 step: 3198, epoch: 96, batch: 29, loss: 0.1718953251838684, acc: 93.75, f1: 86.52014652014651, r: 0.6570225388889465
06/02/2019 09:34:08 *** evaluating ***
06/02/2019 09:34:08 step: 97, epoch: 96, acc: 57.692307692307686, f1: 21.0315969152176, r: 0.24248868662954456
06/02/2019 09:34:08 *** epoch: 98 ***
06/02/2019 09:34:08 *** training ***
06/02/2019 09:34:09 step: 3206, epoch: 97, batch: 4, loss: 0.1211799755692482, acc: 95.3125, f1: 90.84680467168945, r: 0.6898863410839098
06/02/2019 09:34:09 step: 3211, epoch: 97, batch: 9, loss: 0.09861375391483307, acc: 93.75, f1: 93.92597472307617, r: 0.5533623653509185
06/02/2019 09:34:10 step: 3216, epoch: 97, batch: 14, loss: 0.08589667081832886, acc: 96.875, f1: 97.61697722567287, r: 0.6716606384805284
06/02/2019 09:34:10 step: 3221, epoch: 97, batch: 19, loss: 0.09752881526947021, acc: 96.875, f1: 97.45115995115997, r: 0.7452606402322888
06/02/2019 09:34:11 step: 3226, epoch: 97, batch: 24, loss: 0.08493455499410629, acc: 96.875, f1: 96.37362637362638, r: 0.6525044723181429
06/02/2019 09:34:11 step: 3231, epoch: 97, batch: 29, loss: 0.21305713057518005, acc: 89.0625, f1: 74.33835261960262, r: 0.7481778240983268
06/02/2019 09:34:12 *** evaluating ***
06/02/2019 09:34:12 step: 98, epoch: 97, acc: 58.119658119658126, f1: 22.25959729092396, r: 0.25258135722894653
06/02/2019 09:34:12 *** epoch: 99 ***
06/02/2019 09:34:12 *** training ***
06/02/2019 09:34:13 step: 3239, epoch: 98, batch: 4, loss: 0.07246412336826324, acc: 98.4375, f1: 86.36363636363636, r: 0.7777267440569715
06/02/2019 09:34:13 step: 3244, epoch: 98, batch: 9, loss: 0.18911480903625488, acc: 93.75, f1: 81.29431935817806, r: 0.7263946644139709
06/02/2019 09:34:14 step: 3249, epoch: 98, batch: 14, loss: 0.14920924603939056, acc: 93.75, f1: 83.64794625664193, r: 0.5619497876742109
06/02/2019 09:34:14 step: 3254, epoch: 98, batch: 19, loss: 0.14501605927944183, acc: 90.625, f1: 74.22077922077924, r: 0.6549230625800753
06/02/2019 09:34:15 step: 3259, epoch: 98, batch: 24, loss: 0.13685272634029388, acc: 93.75, f1: 88.27045969903112, r: 0.6793028680297134
06/02/2019 09:34:15 step: 3264, epoch: 98, batch: 29, loss: 0.11253628134727478, acc: 95.3125, f1: 94.37179487179488, r: 0.8265084282649218
06/02/2019 09:34:15 *** evaluating ***
06/02/2019 09:34:16 step: 99, epoch: 98, acc: 57.26495726495726, f1: 21.742676737881382, r: 0.248724632626107
06/02/2019 09:34:16 *** epoch: 100 ***
06/02/2019 09:34:16 *** training ***
06/02/2019 09:34:16 step: 3272, epoch: 99, batch: 4, loss: 0.1397891640663147, acc: 93.75, f1: 81.5390749601276, r: 0.6786790519536271
06/02/2019 09:34:17 step: 3277, epoch: 99, batch: 9, loss: 0.10195713490247726, acc: 95.3125, f1: 80.08928571428571, r: 0.7409359921618967
06/02/2019 09:34:17 step: 3282, epoch: 99, batch: 14, loss: 0.18928000330924988, acc: 87.5, f1: 84.57590029018601, r: 0.6212661777001383
06/02/2019 09:34:18 step: 3287, epoch: 99, batch: 19, loss: 0.1355339139699936, acc: 93.75, f1: 86.99633699633699, r: 0.8164869692942854
06/02/2019 09:34:18 step: 3292, epoch: 99, batch: 24, loss: 0.15953034162521362, acc: 92.1875, f1: 85.64392646690784, r: 0.6350696474512622
06/02/2019 09:34:19 step: 3297, epoch: 99, batch: 29, loss: 0.2650561034679413, acc: 90.625, f1: 72.5895400895401, r: 0.7130099778639111
06/02/2019 09:34:19 *** evaluating ***
06/02/2019 09:34:19 step: 100, epoch: 99, acc: 57.26495726495726, f1: 21.131373621993365, r: 0.25045844910217663
06/02/2019 09:34:19 *** epoch: 101 ***
06/02/2019 09:34:19 *** training ***
06/02/2019 09:34:20 step: 3305, epoch: 100, batch: 4, loss: 0.10548463463783264, acc: 93.75, f1: 89.87918664733654, r: 0.6641786112413413
06/02/2019 09:34:20 step: 3310, epoch: 100, batch: 9, loss: 0.10718458890914917, acc: 95.3125, f1: 90.06734006734007, r: 0.7348277845277263
06/02/2019 09:34:21 step: 3315, epoch: 100, batch: 14, loss: 0.20712894201278687, acc: 89.0625, f1: 73.31694185731452, r: 0.6024671759119471
06/02/2019 09:34:21 step: 3320, epoch: 100, batch: 19, loss: 0.12077193707227707, acc: 96.875, f1: 83.7037037037037, r: 0.7112238618077696
06/02/2019 09:34:22 step: 3325, epoch: 100, batch: 24, loss: 0.1431778520345688, acc: 93.75, f1: 86.21933621933621, r: 0.6981482152136573
06/02/2019 09:34:22 step: 3330, epoch: 100, batch: 29, loss: 0.13436126708984375, acc: 92.1875, f1: 84.44444444444444, r: 0.6553883239510274
06/02/2019 09:34:23 *** evaluating ***
06/02/2019 09:34:23 step: 101, epoch: 100, acc: 57.692307692307686, f1: 21.132386502608636, r: 0.25002126114211404
06/02/2019 09:34:23 *** epoch: 102 ***
06/02/2019 09:34:23 *** training ***
06/02/2019 09:34:23 step: 3338, epoch: 101, batch: 4, loss: 0.10596281290054321, acc: 96.875, f1: 95.52845528455285, r: 0.8024098088703009
06/02/2019 09:34:24 step: 3343, epoch: 101, batch: 9, loss: 0.20705094933509827, acc: 90.625, f1: 90.07401800178485, r: 0.7035151367884239
06/02/2019 09:34:24 step: 3348, epoch: 101, batch: 14, loss: 0.050344981253147125, acc: 98.4375, f1: 97.96918767507002, r: 0.723799517809995
06/02/2019 09:34:25 step: 3353, epoch: 101, batch: 19, loss: 0.10396593809127808, acc: 93.75, f1: 95.34637659637659, r: 0.7964780615735948
06/02/2019 09:34:25 step: 3358, epoch: 101, batch: 24, loss: 0.1746511608362198, acc: 93.75, f1: 93.73526580830554, r: 0.6454507809583822
06/02/2019 09:34:26 step: 3363, epoch: 101, batch: 29, loss: 0.10287979245185852, acc: 95.3125, f1: 92.28633347185541, r: 0.6742553466218469
06/02/2019 09:34:26 *** evaluating ***
06/02/2019 09:34:27 step: 102, epoch: 101, acc: 56.41025641025641, f1: 21.97811340012147, r: 0.25422932406742443
06/02/2019 09:34:27 *** epoch: 103 ***
06/02/2019 09:34:27 *** training ***
06/02/2019 09:34:27 step: 3371, epoch: 102, batch: 4, loss: 0.08623696118593216, acc: 98.4375, f1: 99.15824915824916, r: 0.814735566297488
06/02/2019 09:34:28 step: 3376, epoch: 102, batch: 9, loss: 0.046181369572877884, acc: 100.0, f1: 100.0, r: 0.7818852260934372
06/02/2019 09:34:28 step: 3381, epoch: 102, batch: 14, loss: 0.12218356132507324, acc: 95.3125, f1: 93.09723889555822, r: 0.6809974301237148
06/02/2019 09:34:29 step: 3386, epoch: 102, batch: 19, loss: 0.22230328619480133, acc: 89.0625, f1: 81.58207826959047, r: 0.6198569994504632
06/02/2019 09:34:29 step: 3391, epoch: 102, batch: 24, loss: 0.16007106006145477, acc: 92.1875, f1: 77.09750566893425, r: 0.6083210580705695
06/02/2019 09:34:30 step: 3396, epoch: 102, batch: 29, loss: 0.1617666780948639, acc: 93.75, f1: 79.5563909774436, r: 0.7522411353870709
06/02/2019 09:34:30 *** evaluating ***
06/02/2019 09:34:30 step: 103, epoch: 102, acc: 58.119658119658126, f1: 21.319052820067018, r: 0.2408359366267288
06/02/2019 09:34:30 *** epoch: 104 ***
06/02/2019 09:34:30 *** training ***
06/02/2019 09:34:31 step: 3404, epoch: 103, batch: 4, loss: 0.13705891370773315, acc: 96.875, f1: 96.77041548326648, r: 0.6945842837234418
06/02/2019 09:34:31 step: 3409, epoch: 103, batch: 9, loss: 0.16807803511619568, acc: 93.75, f1: 93.79454926624737, r: 0.6582389432431788
06/02/2019 09:34:32 step: 3414, epoch: 103, batch: 14, loss: 0.211821511387825, acc: 90.625, f1: 74.03292075831969, r: 0.6450340868615003
06/02/2019 09:34:32 step: 3419, epoch: 103, batch: 19, loss: 0.19443097710609436, acc: 92.1875, f1: 91.33202671312426, r: 0.720035251556004
06/02/2019 09:34:33 step: 3424, epoch: 103, batch: 24, loss: 0.04002051055431366, acc: 100.0, f1: 100.0, r: 0.7342349813281303
06/02/2019 09:34:33 step: 3429, epoch: 103, batch: 29, loss: 0.04898766428232193, acc: 96.875, f1: 96.51093580165912, r: 0.7067569470389364
06/02/2019 09:34:33 *** evaluating ***
06/02/2019 09:34:34 step: 104, epoch: 103, acc: 57.26495726495726, f1: 21.810997767173067, r: 0.22919865027063338
06/02/2019 09:34:34 *** epoch: 105 ***
06/02/2019 09:34:34 *** training ***
06/02/2019 09:34:34 step: 3437, epoch: 104, batch: 4, loss: 0.15897861123085022, acc: 92.1875, f1: 86.28879746892169, r: 0.7111340004657806
06/02/2019 09:34:35 step: 3442, epoch: 104, batch: 9, loss: 0.22011283040046692, acc: 89.0625, f1: 81.67812579577286, r: 0.6159127484742273
06/02/2019 09:34:35 step: 3447, epoch: 104, batch: 14, loss: 0.11235100775957108, acc: 95.3125, f1: 92.3268921095008, r: 0.7296375078827754
06/02/2019 09:34:36 step: 3452, epoch: 104, batch: 19, loss: 0.08883018791675568, acc: 96.875, f1: 80.33333333333333, r: 0.7019041936949828
06/02/2019 09:34:36 step: 3457, epoch: 104, batch: 24, loss: 0.13471364974975586, acc: 92.1875, f1: 76.35766264798522, r: 0.6688203127843605
06/02/2019 09:34:37 step: 3462, epoch: 104, batch: 29, loss: 0.17821091413497925, acc: 95.3125, f1: 90.32563481716024, r: 0.6283685827262112
06/02/2019 09:34:37 *** evaluating ***
06/02/2019 09:34:37 step: 105, epoch: 104, acc: 56.837606837606835, f1: 21.3673277626766, r: 0.22603305001170454
06/02/2019 09:34:37 *** epoch: 106 ***
06/02/2019 09:34:37 *** training ***
06/02/2019 09:34:38 step: 3470, epoch: 105, batch: 4, loss: 0.15620802342891693, acc: 93.75, f1: 88.80790736053895, r: 0.6521995482053562
06/02/2019 09:34:38 step: 3475, epoch: 105, batch: 9, loss: 0.14382779598236084, acc: 95.3125, f1: 92.10648148148148, r: 0.7812375570630432
06/02/2019 09:34:39 step: 3480, epoch: 105, batch: 14, loss: 0.19456639885902405, acc: 89.0625, f1: 83.47150690326946, r: 0.6889549480192647
06/02/2019 09:34:39 step: 3485, epoch: 105, batch: 19, loss: 0.15689824521541595, acc: 93.75, f1: 88.91817838246409, r: 0.6393524019755418
06/02/2019 09:34:40 step: 3490, epoch: 105, batch: 24, loss: 0.14976871013641357, acc: 95.3125, f1: 80.19959318586321, r: 0.6915312101283843
06/02/2019 09:34:40 step: 3495, epoch: 105, batch: 29, loss: 0.33431920409202576, acc: 84.375, f1: 69.71560846560847, r: 0.6140451536847433
06/02/2019 09:34:41 *** evaluating ***
06/02/2019 09:34:41 step: 106, epoch: 105, acc: 57.26495726495726, f1: 21.364540407509157, r: 0.24485160563033057
06/02/2019 09:34:41 *** epoch: 107 ***
06/02/2019 09:34:41 *** training ***
06/02/2019 09:34:41 step: 3503, epoch: 106, batch: 4, loss: 0.15152443945407867, acc: 95.3125, f1: 92.54032258064517, r: 0.7521572498850613
06/02/2019 09:34:42 step: 3508, epoch: 106, batch: 9, loss: 0.08221887052059174, acc: 95.3125, f1: 79.40607012035584, r: 0.5921250189479194
06/02/2019 09:34:42 step: 3513, epoch: 106, batch: 14, loss: 0.17521953582763672, acc: 92.1875, f1: 89.31579514658783, r: 0.8058929973391173
06/02/2019 09:34:43 step: 3518, epoch: 106, batch: 19, loss: 0.18052154779434204, acc: 92.1875, f1: 73.73015873015872, r: 0.6067188790979312
06/02/2019 09:34:43 step: 3523, epoch: 106, batch: 24, loss: 0.1494031399488449, acc: 93.75, f1: 77.0408163265306, r: 0.6874581655263831
06/02/2019 09:34:44 step: 3528, epoch: 106, batch: 29, loss: 0.07914985716342926, acc: 96.875, f1: 96.36633362381694, r: 0.6444553778370488
06/02/2019 09:34:44 *** evaluating ***
06/02/2019 09:34:44 step: 107, epoch: 106, acc: 58.119658119658126, f1: 21.654571122318927, r: 0.2316220779766
06/02/2019 09:34:44 *** epoch: 108 ***
06/02/2019 09:34:44 *** training ***
06/02/2019 09:34:45 step: 3536, epoch: 107, batch: 4, loss: 0.14641515910625458, acc: 93.75, f1: 95.97752144335995, r: 0.6198684685273609
06/02/2019 09:34:45 step: 3541, epoch: 107, batch: 9, loss: 0.14326676726341248, acc: 90.625, f1: 86.08361221650259, r: 0.6772030402958584
06/02/2019 09:34:46 step: 3546, epoch: 107, batch: 14, loss: 0.13523848354816437, acc: 93.75, f1: 84.61420932009167, r: 0.6521184023350495
06/02/2019 09:34:46 step: 3551, epoch: 107, batch: 19, loss: 0.10204072296619415, acc: 96.875, f1: 95.01344086021504, r: 0.7416301600451362
06/02/2019 09:34:47 step: 3556, epoch: 107, batch: 24, loss: 0.22094319760799408, acc: 92.1875, f1: 75.82853654282225, r: 0.5466746570991737
06/02/2019 09:34:47 step: 3561, epoch: 107, batch: 29, loss: 0.2039816528558731, acc: 92.1875, f1: 83.56899902818269, r: 0.6836921139076908
06/02/2019 09:34:48 *** evaluating ***
06/02/2019 09:34:48 step: 108, epoch: 107, acc: 58.119658119658126, f1: 21.297343433265763, r: 0.2356372462945254
06/02/2019 09:34:48 *** epoch: 109 ***
06/02/2019 09:34:48 *** training ***
06/02/2019 09:34:49 step: 3569, epoch: 108, batch: 4, loss: 0.1837213784456253, acc: 87.5, f1: 81.9765784297804, r: 0.6127991372361203
06/02/2019 09:34:49 step: 3574, epoch: 108, batch: 9, loss: 0.1430457979440689, acc: 92.1875, f1: 89.48913786442391, r: 0.7352375973295148
06/02/2019 09:34:50 step: 3579, epoch: 108, batch: 14, loss: 0.1905035823583603, acc: 92.1875, f1: 88.06816420261798, r: 0.6264963486211068
06/02/2019 09:34:50 step: 3584, epoch: 108, batch: 19, loss: 0.05955204740166664, acc: 98.4375, f1: 99.00226757369614, r: 0.6718532507256982
06/02/2019 09:34:51 step: 3589, epoch: 108, batch: 24, loss: 0.06962192058563232, acc: 98.4375, f1: 99.23112767940354, r: 0.7486204330194932
06/02/2019 09:34:51 step: 3594, epoch: 108, batch: 29, loss: 0.19341978430747986, acc: 89.0625, f1: 85.80717250952445, r: 0.7197378805581892
06/02/2019 09:34:52 *** evaluating ***
06/02/2019 09:34:52 step: 109, epoch: 108, acc: 58.54700854700855, f1: 21.40285979583676, r: 0.2357246889600016
06/02/2019 09:34:52 *** epoch: 110 ***
06/02/2019 09:34:52 *** training ***
06/02/2019 09:34:52 step: 3602, epoch: 109, batch: 4, loss: 0.09287956357002258, acc: 95.3125, f1: 93.96331738437001, r: 0.678900953380087
06/02/2019 09:34:53 step: 3607, epoch: 109, batch: 9, loss: 0.15377125144004822, acc: 92.1875, f1: 84.47028130671507, r: 0.7441504348057362
06/02/2019 09:34:53 step: 3612, epoch: 109, batch: 14, loss: 0.15348351001739502, acc: 96.875, f1: 93.88011808496984, r: 0.6492879976518909
06/02/2019 09:34:54 step: 3617, epoch: 109, batch: 19, loss: 0.09518714994192123, acc: 96.875, f1: 82.9483282674772, r: 0.6781167958439518
06/02/2019 09:34:55 step: 3622, epoch: 109, batch: 24, loss: 0.1178969293832779, acc: 93.75, f1: 79.08730158730158, r: 0.6908017644602984
06/02/2019 09:34:55 step: 3627, epoch: 109, batch: 29, loss: 0.19300103187561035, acc: 92.1875, f1: 90.54839896945161, r: 0.7948993379771702
06/02/2019 09:34:55 *** evaluating ***
06/02/2019 09:34:56 step: 110, epoch: 109, acc: 58.119658119658126, f1: 21.30690660102425, r: 0.2438662221955329
06/02/2019 09:34:56 *** epoch: 111 ***
06/02/2019 09:34:56 *** training ***
06/02/2019 09:34:56 step: 3635, epoch: 110, batch: 4, loss: 0.12404480576515198, acc: 92.1875, f1: 94.85526627542465, r: 0.7483648614444598
06/02/2019 09:34:57 step: 3640, epoch: 110, batch: 9, loss: 0.13082154095172882, acc: 96.875, f1: 97.00774068698597, r: 0.8006029608163772
06/02/2019 09:34:57 step: 3645, epoch: 110, batch: 14, loss: 0.16060684621334076, acc: 92.1875, f1: 80.47305764411028, r: 0.7709416252663883
06/02/2019 09:34:58 step: 3650, epoch: 110, batch: 19, loss: 0.062409430742263794, acc: 98.4375, f1: 98.88627115013921, r: 0.6719689695093397
06/02/2019 09:34:58 step: 3655, epoch: 110, batch: 24, loss: 0.30042314529418945, acc: 90.625, f1: 82.49845392702537, r: 0.5484126749431436
06/02/2019 09:34:59 step: 3660, epoch: 110, batch: 29, loss: 0.07843336462974548, acc: 95.3125, f1: 89.8005698005698, r: 0.7000237540569375
06/02/2019 09:34:59 *** evaluating ***
06/02/2019 09:34:59 step: 111, epoch: 110, acc: 18.803418803418804, f1: 8.20102676511306, r: 0.12998629736686884
06/02/2019 09:34:59 *** epoch: 112 ***
06/02/2019 09:34:59 *** training ***
06/02/2019 09:35:00 step: 3668, epoch: 111, batch: 4, loss: 0.12296202033758163, acc: 93.75, f1: 95.52310119450081, r: 0.6277026770503814
06/02/2019 09:35:00 step: 3673, epoch: 111, batch: 9, loss: 0.07144780457019806, acc: 96.875, f1: 97.34335839598998, r: 0.6965927077038456
06/02/2019 09:35:01 step: 3678, epoch: 111, batch: 14, loss: 0.18425855040550232, acc: 92.1875, f1: 66.78030303030302, r: 0.6594549535214072
06/02/2019 09:35:02 step: 3683, epoch: 111, batch: 19, loss: 0.18414565920829773, acc: 95.3125, f1: 92.8373015873016, r: 0.7792397366451983
06/02/2019 09:35:02 step: 3688, epoch: 111, batch: 24, loss: 0.10119529813528061, acc: 95.3125, f1: 82.00292397660817, r: 0.7338788919882384
06/02/2019 09:35:03 step: 3693, epoch: 111, batch: 29, loss: 0.16698361933231354, acc: 93.75, f1: 85.98281417830289, r: 0.6105466516128663
06/02/2019 09:35:03 *** evaluating ***
06/02/2019 09:35:03 step: 112, epoch: 111, acc: 56.41025641025641, f1: 20.971055088702144, r: 0.2505437528121042
06/02/2019 09:35:03 *** epoch: 113 ***
06/02/2019 09:35:03 *** training ***
06/02/2019 09:35:04 step: 3701, epoch: 112, batch: 4, loss: 0.026840679347515106, acc: 100.0, f1: 100.0, r: 0.7714807039004042
06/02/2019 09:35:04 step: 3706, epoch: 112, batch: 9, loss: 0.1143750473856926, acc: 95.3125, f1: 87.85182644316907, r: 0.7320148834818284
06/02/2019 09:35:05 step: 3711, epoch: 112, batch: 14, loss: 0.19551792740821838, acc: 92.1875, f1: 86.66099642486449, r: 0.6249849611963371
06/02/2019 09:35:05 step: 3716, epoch: 112, batch: 19, loss: 0.1258547306060791, acc: 93.75, f1: 90.33126293995858, r: 0.6492039478296264
06/02/2019 09:35:06 step: 3721, epoch: 112, batch: 24, loss: 0.0412173867225647, acc: 98.4375, f1: 97.97979797979798, r: 0.729094525029525
06/02/2019 09:35:06 step: 3726, epoch: 112, batch: 29, loss: 0.13262160122394562, acc: 95.3125, f1: 91.82539682539682, r: 0.7466901513963172
06/02/2019 09:35:07 *** evaluating ***
06/02/2019 09:35:07 step: 113, epoch: 112, acc: 56.41025641025641, f1: 20.98775111070193, r: 0.2395294372228358
06/02/2019 09:35:07 *** epoch: 114 ***
06/02/2019 09:35:07 *** training ***
06/02/2019 09:35:08 step: 3734, epoch: 113, batch: 4, loss: 0.22362151741981506, acc: 87.5, f1: 69.04148695381865, r: 0.6629730319352719
06/02/2019 09:35:08 step: 3739, epoch: 113, batch: 9, loss: 0.03747852146625519, acc: 98.4375, f1: 86.90476190476191, r: 0.7489637059126117
06/02/2019 09:35:09 step: 3744, epoch: 113, batch: 14, loss: 0.06636608392000198, acc: 96.875, f1: 88.86178861788618, r: 0.7280150359574377
06/02/2019 09:35:09 step: 3749, epoch: 113, batch: 19, loss: 0.1170533075928688, acc: 96.875, f1: 94.67106967106966, r: 0.7417439381326677
06/02/2019 09:35:10 step: 3754, epoch: 113, batch: 24, loss: 0.14290103316307068, acc: 92.1875, f1: 88.34571127383907, r: 0.7237541719022575
06/02/2019 09:35:10 step: 3759, epoch: 113, batch: 29, loss: 0.11943239718675613, acc: 95.3125, f1: 71.47177419354838, r: 0.7098479566844875
06/02/2019 09:35:10 *** evaluating ***
06/02/2019 09:35:11 step: 114, epoch: 113, acc: 57.26495726495726, f1: 20.923472614132137, r: 0.24293406080348512
06/02/2019 09:35:11 *** epoch: 115 ***
06/02/2019 09:35:11 *** training ***
06/02/2019 09:35:11 step: 3767, epoch: 114, batch: 4, loss: 0.15733186900615692, acc: 95.3125, f1: 91.47071872227153, r: 0.692925125739241
06/02/2019 09:35:12 step: 3772, epoch: 114, batch: 9, loss: 0.10979034006595612, acc: 95.3125, f1: 96.54228124816359, r: 0.747062573006876
06/02/2019 09:35:12 step: 3777, epoch: 114, batch: 14, loss: 0.03636593371629715, acc: 98.4375, f1: 98.46710666382798, r: 0.7162329263190873
06/02/2019 09:35:13 step: 3782, epoch: 114, batch: 19, loss: 0.08609781414270401, acc: 96.875, f1: 95.08110936682364, r: 0.7501323481179065
06/02/2019 09:35:13 step: 3787, epoch: 114, batch: 24, loss: 0.027802154421806335, acc: 98.4375, f1: 85.71428571428572, r: 0.6863216740059909
06/02/2019 09:35:14 step: 3792, epoch: 114, batch: 29, loss: 0.226183220744133, acc: 92.1875, f1: 91.55708874458874, r: 0.7658975156338618
06/02/2019 09:35:14 *** evaluating ***
06/02/2019 09:35:14 step: 115, epoch: 114, acc: 56.41025641025641, f1: 20.85569076731206, r: 0.24034528599465835
06/02/2019 09:35:14 *** epoch: 116 ***
06/02/2019 09:35:14 *** training ***
06/02/2019 09:35:15 step: 3800, epoch: 115, batch: 4, loss: 0.08321693539619446, acc: 96.875, f1: 95.41062801932367, r: 0.7069950832770235
06/02/2019 09:35:15 step: 3805, epoch: 115, batch: 9, loss: 0.15642336010932922, acc: 90.625, f1: 74.13050435227855, r: 0.6366849270522951
06/02/2019 09:35:16 step: 3810, epoch: 115, batch: 14, loss: 0.34858012199401855, acc: 89.0625, f1: 89.04200638071606, r: 0.5592312326582106
06/02/2019 09:35:16 step: 3815, epoch: 115, batch: 19, loss: 0.1413632333278656, acc: 95.3125, f1: 93.57202395837055, r: 0.6234005346988202
06/02/2019 09:35:17 step: 3820, epoch: 115, batch: 24, loss: 0.13018186390399933, acc: 95.3125, f1: 90.94516594516594, r: 0.6872421659733057
06/02/2019 09:35:17 step: 3825, epoch: 115, batch: 29, loss: 0.1510011851787567, acc: 93.75, f1: 88.08333333333334, r: 0.7434820758604698
06/02/2019 09:35:18 *** evaluating ***
06/02/2019 09:35:18 step: 116, epoch: 115, acc: 56.837606837606835, f1: 20.795691271890405, r: 0.2545140417954364
06/02/2019 09:35:18 *** epoch: 117 ***
06/02/2019 09:35:18 *** training ***
06/02/2019 09:35:18 step: 3833, epoch: 116, batch: 4, loss: 0.13978725671768188, acc: 95.3125, f1: 81.78193499622071, r: 0.7897297518060329
06/02/2019 09:35:19 step: 3838, epoch: 116, batch: 9, loss: 0.05156928300857544, acc: 98.4375, f1: 98.01587301587301, r: 0.7646130223528516
06/02/2019 09:35:19 step: 3843, epoch: 116, batch: 14, loss: 0.12970325350761414, acc: 93.75, f1: 92.28973407544837, r: 0.7720998238378047
06/02/2019 09:35:20 step: 3848, epoch: 116, batch: 19, loss: 0.07572856545448303, acc: 96.875, f1: 96.148167941481, r: 0.6838748631190226
06/02/2019 09:35:20 step: 3853, epoch: 116, batch: 24, loss: 0.11755184084177017, acc: 95.3125, f1: 96.33255633255632, r: 0.6294490532814282
06/02/2019 09:35:21 step: 3858, epoch: 116, batch: 29, loss: 0.13730178773403168, acc: 95.3125, f1: 90.58481890780027, r: 0.6771116483368003
06/02/2019 09:35:21 *** evaluating ***
06/02/2019 09:35:21 step: 117, epoch: 116, acc: 58.119658119658126, f1: 21.36377193077168, r: 0.2530846435156773
06/02/2019 09:35:21 *** epoch: 118 ***
06/02/2019 09:35:21 *** training ***
06/02/2019 09:35:22 step: 3866, epoch: 117, batch: 4, loss: 0.2169535905122757, acc: 90.625, f1: 75.65759637188208, r: 0.5642179809745527
06/02/2019 09:35:22 step: 3871, epoch: 117, batch: 9, loss: 0.10244192183017731, acc: 95.3125, f1: 93.64127693447456, r: 0.6659739642692315
06/02/2019 09:35:23 step: 3876, epoch: 117, batch: 14, loss: 0.15505856275558472, acc: 93.75, f1: 78.78030303030303, r: 0.6760268591647364
06/02/2019 09:35:23 step: 3881, epoch: 117, batch: 19, loss: 0.09887698292732239, acc: 95.3125, f1: 94.47480267152399, r: 0.6806591067128369
06/02/2019 09:35:24 step: 3886, epoch: 117, batch: 24, loss: 0.0766686350107193, acc: 95.3125, f1: 95.61208010335918, r: 0.807692454546797
06/02/2019 09:35:24 step: 3891, epoch: 117, batch: 29, loss: 0.261811226606369, acc: 89.0625, f1: 68.07173627026569, r: 0.42912401191015853
06/02/2019 09:35:25 *** evaluating ***
06/02/2019 09:35:25 step: 118, epoch: 117, acc: 58.97435897435898, f1: 21.455055252225065, r: 0.2415148761363384
06/02/2019 09:35:25 *** epoch: 119 ***
06/02/2019 09:35:25 *** training ***
06/02/2019 09:35:25 step: 3899, epoch: 118, batch: 4, loss: 0.14113306999206543, acc: 92.1875, f1: 90.46387839491288, r: 0.5594016804431434
06/02/2019 09:35:26 step: 3904, epoch: 118, batch: 9, loss: 0.09474946558475494, acc: 95.3125, f1: 82.21117506831793, r: 0.6418036441387662
06/02/2019 09:35:26 step: 3909, epoch: 118, batch: 14, loss: 0.09504380822181702, acc: 96.875, f1: 95.73830541043655, r: 0.7741385760688821
06/02/2019 09:35:27 step: 3914, epoch: 118, batch: 19, loss: 0.13226033747196198, acc: 95.3125, f1: 96.24867091972355, r: 0.8075289530189669
06/02/2019 09:35:27 step: 3919, epoch: 118, batch: 24, loss: 0.09725835919380188, acc: 93.75, f1: 89.95073891625616, r: 0.6569314443297529
06/02/2019 09:35:28 step: 3924, epoch: 118, batch: 29, loss: 0.0680883601307869, acc: 98.4375, f1: 97.38775510204081, r: 0.7272764901064119
06/02/2019 09:35:28 *** evaluating ***
06/02/2019 09:35:29 step: 119, epoch: 118, acc: 57.26495726495726, f1: 21.074737712931974, r: 0.23682446863731002
06/02/2019 09:35:29 *** epoch: 120 ***
06/02/2019 09:35:29 *** training ***
06/02/2019 09:35:29 step: 3932, epoch: 119, batch: 4, loss: 0.184988871216774, acc: 92.1875, f1: 81.27136752136752, r: 0.6189172741779131
06/02/2019 09:35:30 step: 3937, epoch: 119, batch: 9, loss: 0.09063038975000381, acc: 95.3125, f1: 88.68624059793409, r: 0.6781525269413043
06/02/2019 09:35:30 step: 3942, epoch: 119, batch: 14, loss: 0.10397840291261673, acc: 96.875, f1: 84.375, r: 0.7873585012509423
06/02/2019 09:35:31 step: 3947, epoch: 119, batch: 19, loss: 0.1613427847623825, acc: 92.1875, f1: 74.3718671679198, r: 0.6891244170425117
06/02/2019 09:35:31 step: 3952, epoch: 119, batch: 24, loss: 0.12563830614089966, acc: 95.3125, f1: 82.49054708732129, r: 0.7367059047268405
06/02/2019 09:35:32 step: 3957, epoch: 119, batch: 29, loss: 0.06304483115673065, acc: 95.3125, f1: 84.51921172509408, r: 0.7104063645630392
06/02/2019 09:35:32 *** evaluating ***
06/02/2019 09:35:32 step: 120, epoch: 119, acc: 58.119658119658126, f1: 21.419528201948168, r: 0.235882772715139
06/02/2019 09:35:32 *** epoch: 121 ***
06/02/2019 09:35:32 *** training ***
06/02/2019 09:35:33 step: 3965, epoch: 120, batch: 4, loss: 0.07586073875427246, acc: 96.875, f1: 94.80848943350925, r: 0.700860080101636
06/02/2019 09:35:33 step: 3970, epoch: 120, batch: 9, loss: 0.1975860446691513, acc: 90.625, f1: 72.43423017012198, r: 0.6127461957363921
06/02/2019 09:35:34 step: 3975, epoch: 120, batch: 14, loss: 0.15355184674263, acc: 92.1875, f1: 73.45395747433099, r: 0.5767748443624311
06/02/2019 09:35:34 step: 3980, epoch: 120, batch: 19, loss: 0.11408518254756927, acc: 98.4375, f1: 98.14921920185078, r: 0.6656461088286777
06/02/2019 09:35:35 step: 3985, epoch: 120, batch: 24, loss: 0.15780353546142578, acc: 93.75, f1: 89.84966280884649, r: 0.6715430870213966
06/02/2019 09:35:35 step: 3990, epoch: 120, batch: 29, loss: 0.13948218524456024, acc: 93.75, f1: 77.19322344322345, r: 0.7076311144140366
06/02/2019 09:35:36 *** evaluating ***
06/02/2019 09:35:36 step: 121, epoch: 120, acc: 58.119658119658126, f1: 21.559774594558075, r: 0.260853522032916
06/02/2019 09:35:36 *** epoch: 122 ***
06/02/2019 09:35:36 *** training ***
06/02/2019 09:35:36 step: 3998, epoch: 121, batch: 4, loss: 0.33250266313552856, acc: 87.5, f1: 60.50242130750605, r: 0.6199760593588772
06/02/2019 09:35:37 step: 4003, epoch: 121, batch: 9, loss: 0.21786430478096008, acc: 90.625, f1: 85.66666395934689, r: 0.7107472432368516
06/02/2019 09:35:37 step: 4008, epoch: 121, batch: 14, loss: 0.12781073153018951, acc: 95.3125, f1: 93.46853816681403, r: 0.7386167991107766
06/02/2019 09:35:38 step: 4013, epoch: 121, batch: 19, loss: 0.06522995978593826, acc: 98.4375, f1: 96.53846153846153, r: 0.7756933518434085
06/02/2019 09:35:38 step: 4018, epoch: 121, batch: 24, loss: 0.2356719821691513, acc: 85.9375, f1: 68.53895253069781, r: 0.6780477048113223
06/02/2019 09:35:39 step: 4023, epoch: 121, batch: 29, loss: 0.06281446665525436, acc: 98.4375, f1: 98.14315663372267, r: 0.7269391841061145
06/02/2019 09:35:39 *** evaluating ***
06/02/2019 09:35:40 step: 122, epoch: 121, acc: 57.692307692307686, f1: 21.304850733303223, r: 0.2387148443419078
06/02/2019 09:35:40 *** epoch: 123 ***
06/02/2019 09:35:40 *** training ***
06/02/2019 09:35:40 step: 4031, epoch: 122, batch: 4, loss: 0.036138761788606644, acc: 100.0, f1: 100.0, r: 0.7104758344164587
06/02/2019 09:35:41 step: 4036, epoch: 122, batch: 9, loss: 0.11347874253988266, acc: 93.75, f1: 92.89740050609615, r: 0.6452411617596924
06/02/2019 09:35:41 step: 4041, epoch: 122, batch: 14, loss: 0.15320312976837158, acc: 93.75, f1: 91.66434821506503, r: 0.7825267782386515
06/02/2019 09:35:42 step: 4046, epoch: 122, batch: 19, loss: 0.07599172741174698, acc: 100.0, f1: 100.0, r: 0.6425721819612459
06/02/2019 09:35:42 step: 4051, epoch: 122, batch: 24, loss: 0.08736606687307358, acc: 95.3125, f1: 83.63188955294218, r: 0.7325249984871645
06/02/2019 09:35:43 step: 4056, epoch: 122, batch: 29, loss: 0.13259372115135193, acc: 95.3125, f1: 93.36230129333578, r: 0.6064266836080475
06/02/2019 09:35:43 *** evaluating ***
06/02/2019 09:35:43 step: 123, epoch: 122, acc: 57.26495726495726, f1: 21.636316948816948, r: 0.24170045962971548
06/02/2019 09:35:43 *** epoch: 124 ***
06/02/2019 09:35:43 *** training ***
06/02/2019 09:35:44 step: 4064, epoch: 123, batch: 4, loss: 0.12136560678482056, acc: 95.3125, f1: 95.88056680161942, r: 0.7236095477883973
06/02/2019 09:35:44 step: 4069, epoch: 123, batch: 9, loss: 0.0975627601146698, acc: 95.3125, f1: 90.43650793650794, r: 0.7596278519714493
06/02/2019 09:35:45 step: 4074, epoch: 123, batch: 14, loss: 0.19351337850093842, acc: 93.75, f1: 87.08702848728423, r: 0.724012159582023
06/02/2019 09:35:45 step: 4079, epoch: 123, batch: 19, loss: 0.13536334037780762, acc: 93.75, f1: 85.57041339368926, r: 0.7348275475091041
06/02/2019 09:35:46 step: 4084, epoch: 123, batch: 24, loss: 0.20553293824195862, acc: 90.625, f1: 84.80725623582767, r: 0.6693842467331398
06/02/2019 09:35:46 step: 4089, epoch: 123, batch: 29, loss: 0.1598847210407257, acc: 92.1875, f1: 90.55315055315056, r: 0.6547545156421932
06/02/2019 09:35:47 *** evaluating ***
06/02/2019 09:35:47 step: 124, epoch: 123, acc: 57.692307692307686, f1: 21.074646074646076, r: 0.24359969150503596
06/02/2019 09:35:47 *** epoch: 125 ***
06/02/2019 09:35:47 *** training ***
06/02/2019 09:35:47 step: 4097, epoch: 124, batch: 4, loss: 0.03308670222759247, acc: 100.0, f1: 100.0, r: 0.5805246845680563
06/02/2019 09:35:48 step: 4102, epoch: 124, batch: 9, loss: 0.14212848246097565, acc: 92.1875, f1: 77.16485507246377, r: 0.7158343509362906
06/02/2019 09:35:48 step: 4107, epoch: 124, batch: 14, loss: 0.1685905158519745, acc: 93.75, f1: 72.67080745341615, r: 0.6508308185682417
06/02/2019 09:35:49 step: 4112, epoch: 124, batch: 19, loss: 0.0750652477145195, acc: 96.875, f1: 97.56033726621962, r: 0.7235458974145095
06/02/2019 09:35:49 step: 4117, epoch: 124, batch: 24, loss: 0.12606003880500793, acc: 93.75, f1: 88.10215946843853, r: 0.7338014625778895
06/02/2019 09:35:50 step: 4122, epoch: 124, batch: 29, loss: 0.08499158173799515, acc: 95.3125, f1: 79.8639455782313, r: 0.7098583743833279
06/02/2019 09:35:50 *** evaluating ***
06/02/2019 09:35:50 step: 125, epoch: 124, acc: 57.692307692307686, f1: 21.54032097017228, r: 0.24216552693266832
06/02/2019 09:35:50 *** epoch: 126 ***
06/02/2019 09:35:50 *** training ***
06/02/2019 09:35:51 step: 4130, epoch: 125, batch: 4, loss: 0.1195717379450798, acc: 96.875, f1: 97.70729270729271, r: 0.7569040559352239
06/02/2019 09:35:51 step: 4135, epoch: 125, batch: 9, loss: 0.06718660891056061, acc: 98.4375, f1: 98.46041055718476, r: 0.6825265458445691
06/02/2019 09:35:52 step: 4140, epoch: 125, batch: 14, loss: 0.024573922157287598, acc: 100.0, f1: 100.0, r: 0.7809784348387017
06/02/2019 09:35:53 step: 4145, epoch: 125, batch: 19, loss: 0.07821201533079147, acc: 95.3125, f1: 92.87506400409626, r: 0.5911134141080702
06/02/2019 09:35:53 step: 4150, epoch: 125, batch: 24, loss: 0.1944814920425415, acc: 89.0625, f1: 76.2544280384965, r: 0.6952561698071389
06/02/2019 09:35:54 step: 4155, epoch: 125, batch: 29, loss: 0.15368957817554474, acc: 93.75, f1: 78.93123425038317, r: 0.7181135761387679
06/02/2019 09:35:54 *** evaluating ***
06/02/2019 09:35:54 step: 126, epoch: 125, acc: 57.26495726495726, f1: 21.761947203123675, r: 0.2521585238399374
06/02/2019 09:35:54 *** epoch: 127 ***
06/02/2019 09:35:54 *** training ***
06/02/2019 09:35:55 step: 4163, epoch: 126, batch: 4, loss: 0.11912015080451965, acc: 95.3125, f1: 92.1799628942486, r: 0.6295163501093232
06/02/2019 09:35:55 step: 4168, epoch: 126, batch: 9, loss: 0.10224676877260208, acc: 96.875, f1: 94.99888437449732, r: 0.7165210333846269
06/02/2019 09:35:56 step: 4173, epoch: 126, batch: 14, loss: 0.048626989126205444, acc: 100.0, f1: 100.0, r: 0.7804266616648948
06/02/2019 09:35:56 step: 4178, epoch: 126, batch: 19, loss: 0.1270543336868286, acc: 93.75, f1: 74.45824706694273, r: 0.5594420249925571
06/02/2019 09:35:57 step: 4183, epoch: 126, batch: 24, loss: 0.02583417296409607, acc: 100.0, f1: 100.0, r: 0.6855943915944775
06/02/2019 09:35:58 step: 4188, epoch: 126, batch: 29, loss: 0.10415774583816528, acc: 93.75, f1: 79.5704134366925, r: 0.6469260387743878
06/02/2019 09:35:58 *** evaluating ***
06/02/2019 09:35:58 step: 127, epoch: 126, acc: 57.692307692307686, f1: 21.596944991886627, r: 0.23522120115059242
06/02/2019 09:35:58 *** epoch: 128 ***
06/02/2019 09:35:58 *** training ***
06/02/2019 09:35:59 step: 4196, epoch: 127, batch: 4, loss: 0.06902071833610535, acc: 95.3125, f1: 95.15934950717559, r: 0.7055569416567948
06/02/2019 09:35:59 step: 4201, epoch: 127, batch: 9, loss: 0.10846171528100967, acc: 96.875, f1: 97.95029122760216, r: 0.6859275028398822
06/02/2019 09:36:00 step: 4206, epoch: 127, batch: 14, loss: 0.08610033988952637, acc: 95.3125, f1: 87.54744002864302, r: 0.6104877193430872
06/02/2019 09:36:00 step: 4211, epoch: 127, batch: 19, loss: 0.08244801312685013, acc: 95.3125, f1: 81.86507936507937, r: 0.644321823686866
06/02/2019 09:36:01 step: 4216, epoch: 127, batch: 24, loss: 0.20231369137763977, acc: 90.625, f1: 88.80952380952381, r: 0.7489009992381858
06/02/2019 09:36:01 step: 4221, epoch: 127, batch: 29, loss: 0.09744643419981003, acc: 98.4375, f1: 97.40259740259741, r: 0.7456805790060258
06/02/2019 09:36:01 *** evaluating ***
06/02/2019 09:36:02 step: 128, epoch: 127, acc: 57.692307692307686, f1: 21.408835417456107, r: 0.24454500918220698
06/02/2019 09:36:02 *** epoch: 129 ***
06/02/2019 09:36:02 *** training ***
06/02/2019 09:36:02 step: 4229, epoch: 128, batch: 4, loss: 0.11879338324069977, acc: 95.3125, f1: 90.11359011359012, r: 0.6803189348697771
06/02/2019 09:36:03 step: 4234, epoch: 128, batch: 9, loss: 0.06846821308135986, acc: 96.875, f1: 94.11462243760381, r: 0.7067439518677658
06/02/2019 09:36:03 step: 4239, epoch: 128, batch: 14, loss: 0.08062849938869476, acc: 95.3125, f1: 79.30242272347536, r: 0.6687999961851679
06/02/2019 09:36:04 step: 4244, epoch: 128, batch: 19, loss: 0.0817226693034172, acc: 96.875, f1: 97.43027440395862, r: 0.7477459196367412
06/02/2019 09:36:04 step: 4249, epoch: 128, batch: 24, loss: 0.09888964891433716, acc: 93.75, f1: 90.29591323708969, r: 0.5665401594827193
06/02/2019 09:36:05 step: 4254, epoch: 128, batch: 29, loss: 0.07863758504390717, acc: 98.4375, f1: 85.0, r: 0.855541132123006
06/02/2019 09:36:05 *** evaluating ***
06/02/2019 09:36:05 step: 129, epoch: 128, acc: 58.119658119658126, f1: 22.01662414757352, r: 0.24069189149411185
06/02/2019 09:36:05 *** epoch: 130 ***
06/02/2019 09:36:05 *** training ***
06/02/2019 09:36:06 step: 4262, epoch: 129, batch: 4, loss: 0.010335952043533325, acc: 100.0, f1: 100.0, r: 0.8156471284590165
06/02/2019 09:36:06 step: 4267, epoch: 129, batch: 9, loss: 0.06518978625535965, acc: 98.4375, f1: 98.62318840579711, r: 0.7320661228967509
06/02/2019 09:36:07 step: 4272, epoch: 129, batch: 14, loss: 0.10393981635570526, acc: 95.3125, f1: 83.24802074802075, r: 0.7968644307288526
06/02/2019 09:36:07 step: 4277, epoch: 129, batch: 19, loss: 0.0896892249584198, acc: 95.3125, f1: 76.4500537056928, r: 0.6130630804210218
06/02/2019 09:36:08 step: 4282, epoch: 129, batch: 24, loss: 0.09079418331384659, acc: 95.3125, f1: 80.76958988723695, r: 0.748386675422757
06/02/2019 09:36:08 step: 4287, epoch: 129, batch: 29, loss: 0.06231578066945076, acc: 98.4375, f1: 95.71428571428571, r: 0.7564809300178538
06/02/2019 09:36:09 *** evaluating ***
06/02/2019 09:36:09 step: 130, epoch: 129, acc: 58.54700854700855, f1: 22.016483516483515, r: 0.25303545417367745
06/02/2019 09:36:09 *** epoch: 131 ***
06/02/2019 09:36:09 *** training ***
06/02/2019 09:36:09 step: 4295, epoch: 130, batch: 4, loss: 0.10891261696815491, acc: 95.3125, f1: 93.88332800097506, r: 0.819516982814089
06/02/2019 09:36:10 step: 4300, epoch: 130, batch: 9, loss: 0.11343467235565186, acc: 95.3125, f1: 94.4207702020202, r: 0.7414773040534525
06/02/2019 09:36:10 step: 4305, epoch: 130, batch: 14, loss: 0.0569680854678154, acc: 98.4375, f1: 86.53846153846155, r: 0.841975337979582
06/02/2019 09:36:11 step: 4310, epoch: 130, batch: 19, loss: 0.07712101191282272, acc: 96.875, f1: 95.91224747474747, r: 0.7329506264537231
06/02/2019 09:36:11 step: 4315, epoch: 130, batch: 24, loss: 0.05830636993050575, acc: 98.4375, f1: 97.79158040027606, r: 0.6512828811384768
06/02/2019 09:36:12 step: 4320, epoch: 130, batch: 29, loss: 0.018409207463264465, acc: 100.0, f1: 100.0, r: 0.8044538451896904
06/02/2019 09:36:12 *** evaluating ***
06/02/2019 09:36:12 step: 131, epoch: 130, acc: 58.119658119658126, f1: 21.311564897091213, r: 0.2431660452057932
06/02/2019 09:36:12 *** epoch: 132 ***
06/02/2019 09:36:12 *** training ***
06/02/2019 09:36:13 step: 4328, epoch: 131, batch: 4, loss: 0.10300946980714798, acc: 96.875, f1: 82.49549549549549, r: 0.6662287445579893
06/02/2019 09:36:13 step: 4333, epoch: 131, batch: 9, loss: 0.06182806193828583, acc: 98.4375, f1: 95.71428571428572, r: 0.7549353070281924
06/02/2019 09:36:14 step: 4338, epoch: 131, batch: 14, loss: 0.14046847820281982, acc: 95.3125, f1: 92.09195566338424, r: 0.7326967344340173
06/02/2019 09:36:14 step: 4343, epoch: 131, batch: 19, loss: 0.0619870088994503, acc: 98.4375, f1: 96.9047619047619, r: 0.7548222059882645
06/02/2019 09:36:15 step: 4348, epoch: 131, batch: 24, loss: 0.07723131775856018, acc: 96.875, f1: 92.65873015873017, r: 0.7828281759105912
06/02/2019 09:36:15 step: 4353, epoch: 131, batch: 29, loss: 0.15883049368858337, acc: 92.1875, f1: 88.78052503052503, r: 0.7352474267813085
06/02/2019 09:36:15 *** evaluating ***
06/02/2019 09:36:16 step: 132, epoch: 131, acc: 57.692307692307686, f1: 21.913291670267622, r: 0.241789458051282
06/02/2019 09:36:16 *** epoch: 133 ***
06/02/2019 09:36:16 *** training ***
06/02/2019 09:36:16 step: 4361, epoch: 132, batch: 4, loss: 0.14358468353748322, acc: 95.3125, f1: 87.19696969696969, r: 0.7561749838514419
06/02/2019 09:36:17 step: 4366, epoch: 132, batch: 9, loss: 0.10867592692375183, acc: 93.75, f1: 89.5202979644469, r: 0.7933537061186539
06/02/2019 09:36:17 step: 4371, epoch: 132, batch: 14, loss: 0.08900842070579529, acc: 96.875, f1: 82.79261111559248, r: 0.7066315398197212
06/02/2019 09:36:18 step: 4376, epoch: 132, batch: 19, loss: 0.118327297270298, acc: 93.75, f1: 81.50883838383838, r: 0.7084742464021232
06/02/2019 09:36:18 step: 4381, epoch: 132, batch: 24, loss: 0.17436465620994568, acc: 92.1875, f1: 79.52895058158217, r: 0.5716066260846474
06/02/2019 09:36:19 step: 4386, epoch: 132, batch: 29, loss: 0.1631179004907608, acc: 93.75, f1: 89.40476190476191, r: 0.708012483237485
06/02/2019 09:36:19 *** evaluating ***
06/02/2019 09:36:19 step: 133, epoch: 132, acc: 59.401709401709404, f1: 24.062726470727547, r: 0.25646464829153076
06/02/2019 09:36:19 *** epoch: 134 ***
06/02/2019 09:36:19 *** training ***
06/02/2019 09:36:20 step: 4394, epoch: 133, batch: 4, loss: 0.077567458152771, acc: 96.875, f1: 94.33874824975527, r: 0.6418498453153627
06/02/2019 09:36:20 step: 4399, epoch: 133, batch: 9, loss: 0.09709904342889786, acc: 93.75, f1: 85.47619047619047, r: 0.6840770142640463
06/02/2019 09:36:21 step: 4404, epoch: 133, batch: 14, loss: 0.04971485957503319, acc: 98.4375, f1: 99.14108879626122, r: 0.6799937556938505
06/02/2019 09:36:21 step: 4409, epoch: 133, batch: 19, loss: 0.09371551126241684, acc: 93.75, f1: 89.40849083906586, r: 0.6161227239994765
06/02/2019 09:36:22 step: 4414, epoch: 133, batch: 24, loss: 0.10749447345733643, acc: 96.875, f1: 79.5495284856987, r: 0.668211999378722
06/02/2019 09:36:22 step: 4419, epoch: 133, batch: 29, loss: 0.012118428945541382, acc: 100.0, f1: 100.0, r: 0.8127961414093519
06/02/2019 09:36:23 *** evaluating ***
06/02/2019 09:36:23 step: 134, epoch: 133, acc: 57.26495726495726, f1: 21.020827330968185, r: 0.24106025864437453
06/02/2019 09:36:23 *** epoch: 135 ***
06/02/2019 09:36:23 *** training ***
06/02/2019 09:36:23 step: 4427, epoch: 134, batch: 4, loss: 0.09039726853370667, acc: 96.875, f1: 79.13832199546486, r: 0.659789621966051
06/02/2019 09:36:24 step: 4432, epoch: 134, batch: 9, loss: 0.06324733793735504, acc: 98.4375, f1: 97.20730397422128, r: 0.7254993481851939
06/02/2019 09:36:24 step: 4437, epoch: 134, batch: 14, loss: 0.05867835879325867, acc: 98.4375, f1: 96.3718820861678, r: 0.5822335770202658
06/02/2019 09:36:25 step: 4442, epoch: 134, batch: 19, loss: 0.13039430975914001, acc: 95.3125, f1: 94.33669400774664, r: 0.8154079566326844
06/02/2019 09:36:25 step: 4447, epoch: 134, batch: 24, loss: 0.08842609822750092, acc: 96.875, f1: 81.81818181818181, r: 0.5855300567584589
06/02/2019 09:36:26 step: 4452, epoch: 134, batch: 29, loss: 0.09779873490333557, acc: 96.875, f1: 80.11363636363636, r: 0.7330757490653217
06/02/2019 09:36:26 *** evaluating ***
06/02/2019 09:36:26 step: 135, epoch: 134, acc: 58.119658119658126, f1: 23.042682323046368, r: 0.24393462363233817
06/02/2019 09:36:26 *** epoch: 136 ***
06/02/2019 09:36:26 *** training ***
06/02/2019 09:36:27 step: 4460, epoch: 135, batch: 4, loss: 0.2711043953895569, acc: 85.9375, f1: 74.98553368715908, r: 0.6753134116314604
06/02/2019 09:36:28 step: 4465, epoch: 135, batch: 9, loss: 0.14368398487567902, acc: 93.75, f1: 86.18326118326118, r: 0.6918514547435604
06/02/2019 09:36:28 step: 4470, epoch: 135, batch: 14, loss: 0.0763365775346756, acc: 96.875, f1: 97.2997918384981, r: 0.7795460522116396
06/02/2019 09:36:29 step: 4475, epoch: 135, batch: 19, loss: 0.027992069721221924, acc: 98.4375, f1: 95.0, r: 0.7732548467324827
06/02/2019 09:36:29 step: 4480, epoch: 135, batch: 24, loss: 0.11190679669380188, acc: 95.3125, f1: 91.64777021919879, r: 0.7592125693993331
06/02/2019 09:36:30 step: 4485, epoch: 135, batch: 29, loss: 0.0800965204834938, acc: 95.3125, f1: 93.22751322751323, r: 0.5851821756286093
06/02/2019 09:36:30 *** evaluating ***
06/02/2019 09:36:30 step: 136, epoch: 135, acc: 57.692307692307686, f1: 21.558993568691847, r: 0.23633980074911765
06/02/2019 09:36:30 *** epoch: 137 ***
06/02/2019 09:36:30 *** training ***
06/02/2019 09:36:31 step: 4493, epoch: 136, batch: 4, loss: 0.17988157272338867, acc: 93.75, f1: 89.32118647635889, r: 0.7263599790309637
06/02/2019 09:36:31 step: 4498, epoch: 136, batch: 9, loss: 0.02060854434967041, acc: 98.4375, f1: 96.39097744360903, r: 0.713470314806415
06/02/2019 09:36:32 step: 4503, epoch: 136, batch: 14, loss: 0.14113977551460266, acc: 93.75, f1: 90.70354457572503, r: 0.7023156693499059
06/02/2019 09:36:32 step: 4508, epoch: 136, batch: 19, loss: 0.10131769627332687, acc: 96.875, f1: 85.05639097744361, r: 0.8025891809026992
06/02/2019 09:36:33 step: 4513, epoch: 136, batch: 24, loss: 0.08719868212938309, acc: 93.75, f1: 92.75012104280398, r: 0.6680656945778612
06/02/2019 09:36:33 step: 4518, epoch: 136, batch: 29, loss: 0.12243510782718658, acc: 93.75, f1: 89.36564835724499, r: 0.6698361066473215
06/02/2019 09:36:34 *** evaluating ***
06/02/2019 09:36:34 step: 137, epoch: 136, acc: 58.119658119658126, f1: 22.582121821281465, r: 0.2534055981107097
06/02/2019 09:36:34 *** epoch: 138 ***
06/02/2019 09:36:34 *** training ***
06/02/2019 09:36:34 step: 4526, epoch: 137, batch: 4, loss: 0.19787830114364624, acc: 89.0625, f1: 75.67360277444311, r: 0.6098883593998041
06/02/2019 09:36:35 step: 4531, epoch: 137, batch: 9, loss: 0.03172361105680466, acc: 98.4375, f1: 97.84126984126985, r: 0.6563284289448151
06/02/2019 09:36:35 step: 4536, epoch: 137, batch: 14, loss: 0.050913453102111816, acc: 96.875, f1: 95.23809523809524, r: 0.7335866968660669
06/02/2019 09:36:36 step: 4541, epoch: 137, batch: 19, loss: 0.11517487466335297, acc: 95.3125, f1: 93.37542087542087, r: 0.7365921602333626
06/02/2019 09:36:36 step: 4546, epoch: 137, batch: 24, loss: 0.0736817717552185, acc: 98.4375, f1: 95.71428571428571, r: 0.7673475268616765
06/02/2019 09:36:37 step: 4551, epoch: 137, batch: 29, loss: 0.14903992414474487, acc: 95.3125, f1: 91.27083333333333, r: 0.747516892178153
06/02/2019 09:36:37 *** evaluating ***
06/02/2019 09:36:38 step: 138, epoch: 137, acc: 56.837606837606835, f1: 21.30951490161336, r: 0.23715935276871428
06/02/2019 09:36:38 *** epoch: 139 ***
06/02/2019 09:36:38 *** training ***
06/02/2019 09:36:38 step: 4559, epoch: 138, batch: 4, loss: 0.09246919304132462, acc: 96.875, f1: 95.18408089188489, r: 0.71066648761796
06/02/2019 09:36:39 step: 4564, epoch: 138, batch: 9, loss: 0.028331033885478973, acc: 100.0, f1: 100.0, r: 0.5966202277204314
06/02/2019 09:36:39 step: 4569, epoch: 138, batch: 14, loss: 0.09423087537288666, acc: 96.875, f1: 95.16537928302634, r: 0.8067237771869307
06/02/2019 09:36:40 step: 4574, epoch: 138, batch: 19, loss: 0.13628588616847992, acc: 93.75, f1: 94.4466011042098, r: 0.7290413618884717
06/02/2019 09:36:40 step: 4579, epoch: 138, batch: 24, loss: 0.06449519097805023, acc: 96.875, f1: 80.41374708041374, r: 0.468240561905826
06/02/2019 09:36:41 step: 4584, epoch: 138, batch: 29, loss: 0.08441749215126038, acc: 95.3125, f1: 94.48062913108507, r: 0.7105306369128808
06/02/2019 09:36:41 *** evaluating ***
06/02/2019 09:36:41 step: 139, epoch: 138, acc: 58.54700854700855, f1: 21.80669876466191, r: 0.24732266821064125
06/02/2019 09:36:41 *** epoch: 140 ***
06/02/2019 09:36:41 *** training ***
06/02/2019 09:36:42 step: 4592, epoch: 139, batch: 4, loss: 0.05319462716579437, acc: 96.875, f1: 97.7752009894867, r: 0.8016855618328728
06/02/2019 09:36:42 step: 4597, epoch: 139, batch: 9, loss: 0.09388203918933868, acc: 95.3125, f1: 96.30818463656374, r: 0.6937589620513868
06/02/2019 09:36:43 step: 4602, epoch: 139, batch: 14, loss: 0.11097864806652069, acc: 95.3125, f1: 92.21247563352826, r: 0.7451735723304208
06/02/2019 09:36:43 step: 4607, epoch: 139, batch: 19, loss: 0.18451865017414093, acc: 90.625, f1: 73.46407335963126, r: 0.692568101919819
06/02/2019 09:36:44 step: 4612, epoch: 139, batch: 24, loss: 0.11577475070953369, acc: 93.75, f1: 88.93047112462006, r: 0.6841322934015159
06/02/2019 09:36:45 step: 4617, epoch: 139, batch: 29, loss: 0.17365716397762299, acc: 95.3125, f1: 89.23974995403567, r: 0.6577212657687267
06/02/2019 09:36:45 *** evaluating ***
06/02/2019 09:36:45 step: 140, epoch: 139, acc: 58.97435897435898, f1: 21.93811256311256, r: 0.2467799329739825
06/02/2019 09:36:45 *** epoch: 141 ***
06/02/2019 09:36:45 *** training ***
06/02/2019 09:36:46 step: 4625, epoch: 140, batch: 4, loss: 0.13220256567001343, acc: 93.75, f1: 76.14974937343358, r: 0.7390039617439137
06/02/2019 09:36:46 step: 4630, epoch: 140, batch: 9, loss: 0.16881266236305237, acc: 95.3125, f1: 76.97540554683411, r: 0.5841151580812282
06/02/2019 09:36:47 step: 4635, epoch: 140, batch: 14, loss: 0.09975212812423706, acc: 95.3125, f1: 86.8671679197995, r: 0.6750040326487906
06/02/2019 09:36:47 step: 4640, epoch: 140, batch: 19, loss: 0.16236995160579681, acc: 93.75, f1: 84.7754872563718, r: 0.6738251003172679
06/02/2019 09:36:48 step: 4645, epoch: 140, batch: 24, loss: 0.0647319108247757, acc: 98.4375, f1: 86.90476190476191, r: 0.7173357028574031
06/02/2019 09:36:48 step: 4650, epoch: 140, batch: 29, loss: 0.03406228870153427, acc: 98.4375, f1: 95.0, r: 0.799019422100488
06/02/2019 09:36:48 *** evaluating ***
06/02/2019 09:36:49 step: 141, epoch: 140, acc: 58.119658119658126, f1: 21.68920224194058, r: 0.26228767161502675
06/02/2019 09:36:49 *** epoch: 142 ***
06/02/2019 09:36:49 *** training ***
06/02/2019 09:36:49 step: 4658, epoch: 141, batch: 4, loss: 0.08210042119026184, acc: 96.875, f1: 91.7989417989418, r: 0.7485348434864338
06/02/2019 09:36:50 step: 4663, epoch: 141, batch: 9, loss: 0.007902197539806366, acc: 100.0, f1: 100.0, r: 0.6922820110466885
06/02/2019 09:36:50 step: 4668, epoch: 141, batch: 14, loss: 0.13639430701732635, acc: 92.1875, f1: 65.28285227134332, r: 0.7218059146539153
06/02/2019 09:36:51 step: 4673, epoch: 141, batch: 19, loss: 0.1466829478740692, acc: 95.3125, f1: 94.36208010335918, r: 0.7102293702321435
06/02/2019 09:36:51 step: 4678, epoch: 141, batch: 24, loss: 0.12122447788715363, acc: 96.875, f1: 95.65644500175951, r: 0.6858180874597923
06/02/2019 09:36:52 step: 4683, epoch: 141, batch: 29, loss: 0.11380382627248764, acc: 95.3125, f1: 77.21560846560847, r: 0.6525004907341881
06/02/2019 09:36:52 *** evaluating ***
06/02/2019 09:36:52 step: 142, epoch: 141, acc: 58.119658119658126, f1: 21.86029275523439, r: 0.2418989887451682
06/02/2019 09:36:52 *** epoch: 143 ***
06/02/2019 09:36:52 *** training ***
06/02/2019 09:36:53 step: 4691, epoch: 142, batch: 4, loss: 0.041177693754434586, acc: 98.4375, f1: 99.33554817275747, r: 0.6666933115763543
06/02/2019 09:36:53 step: 4696, epoch: 142, batch: 9, loss: 0.028478816151618958, acc: 98.4375, f1: 97.90209790209789, r: 0.8219782009557555
06/02/2019 09:36:54 step: 4701, epoch: 142, batch: 14, loss: 0.15319502353668213, acc: 93.75, f1: 92.68306161923184, r: 0.6844607556265132
06/02/2019 09:36:54 step: 4706, epoch: 142, batch: 19, loss: 0.10154399275779724, acc: 96.875, f1: 95.27135298563871, r: 0.7169054871595062
06/02/2019 09:36:55 step: 4711, epoch: 142, batch: 24, loss: 0.04382558912038803, acc: 98.4375, f1: 95.55555555555556, r: 0.7301635561206842
06/02/2019 09:36:55 step: 4716, epoch: 142, batch: 29, loss: 0.0637463927268982, acc: 100.0, f1: 100.0, r: 0.8016206844440275
06/02/2019 09:36:56 *** evaluating ***
06/02/2019 09:36:56 step: 143, epoch: 142, acc: 56.837606837606835, f1: 20.929415137840916, r: 0.24764705577618423
06/02/2019 09:36:56 *** epoch: 144 ***
06/02/2019 09:36:56 *** training ***
06/02/2019 09:36:56 step: 4724, epoch: 143, batch: 4, loss: 0.20082136988639832, acc: 92.1875, f1: 88.99426660296224, r: 0.7145226572313287
06/02/2019 09:36:57 step: 4729, epoch: 143, batch: 9, loss: 0.08327892422676086, acc: 98.4375, f1: 99.19056429232192, r: 0.7500540636286357
06/02/2019 09:36:57 step: 4734, epoch: 143, batch: 14, loss: 0.0665988177061081, acc: 98.4375, f1: 98.52941176470588, r: 0.7940471424216884
06/02/2019 09:36:58 step: 4739, epoch: 143, batch: 19, loss: 0.06879539042711258, acc: 98.4375, f1: 94.28571428571428, r: 0.6817893237409425
06/02/2019 09:36:58 step: 4744, epoch: 143, batch: 24, loss: 0.058467548340559006, acc: 98.4375, f1: 98.38056680161944, r: 0.7704072251882367
06/02/2019 09:36:59 step: 4749, epoch: 143, batch: 29, loss: 0.0781940370798111, acc: 95.3125, f1: 92.22404970760235, r: 0.8119769578929068
06/02/2019 09:36:59 *** evaluating ***
06/02/2019 09:37:00 step: 144, epoch: 143, acc: 57.26495726495726, f1: 21.693561145388387, r: 0.24481132047101248
06/02/2019 09:37:00 *** epoch: 145 ***
06/02/2019 09:37:00 *** training ***
06/02/2019 09:37:00 step: 4757, epoch: 144, batch: 4, loss: 0.14413228631019592, acc: 92.1875, f1: 75.0, r: 0.7254192166714263
06/02/2019 09:37:01 step: 4762, epoch: 144, batch: 9, loss: 0.05978310853242874, acc: 96.875, f1: 97.27096811495788, r: 0.6780953612777026
06/02/2019 09:37:01 step: 4767, epoch: 144, batch: 14, loss: 0.14455559849739075, acc: 92.1875, f1: 78.98809523809524, r: 0.7673412847754499
06/02/2019 09:37:02 step: 4772, epoch: 144, batch: 19, loss: 0.11200644075870514, acc: 93.75, f1: 87.481780972347, r: 0.6366608238541113
06/02/2019 09:37:02 step: 4777, epoch: 144, batch: 24, loss: 0.1463342308998108, acc: 93.75, f1: 92.14150432900433, r: 0.7492463590674473
06/02/2019 09:37:03 step: 4782, epoch: 144, batch: 29, loss: 0.15624627470970154, acc: 96.875, f1: 94.96957403651116, r: 0.5778608456809761
06/02/2019 09:37:03 *** evaluating ***
06/02/2019 09:37:03 step: 145, epoch: 144, acc: 57.692307692307686, f1: 21.36319805385758, r: 0.24725085871472172
06/02/2019 09:37:03 *** epoch: 146 ***
06/02/2019 09:37:03 *** training ***
06/02/2019 09:37:04 step: 4790, epoch: 145, batch: 4, loss: 0.0912846177816391, acc: 95.3125, f1: 94.77832512315271, r: 0.7657051348610094
06/02/2019 09:37:04 step: 4795, epoch: 145, batch: 9, loss: 0.08987291157245636, acc: 95.3125, f1: 95.97601702864861, r: 0.7119120344085035
06/02/2019 09:37:05 step: 4800, epoch: 145, batch: 14, loss: 0.0879642516374588, acc: 95.3125, f1: 78.08018068887634, r: 0.6573586851895111
06/02/2019 09:37:05 step: 4805, epoch: 145, batch: 19, loss: 0.06912706792354584, acc: 96.875, f1: 83.9737274220033, r: 0.7310370109332716
06/02/2019 09:37:06 step: 4810, epoch: 145, batch: 24, loss: 0.10762530565261841, acc: 95.3125, f1: 90.84284996049703, r: 0.7318761344710113
06/02/2019 09:37:06 step: 4815, epoch: 145, batch: 29, loss: 0.13572435081005096, acc: 95.3125, f1: 93.10519942185242, r: 0.6056616075809816
06/02/2019 09:37:07 *** evaluating ***
06/02/2019 09:37:07 step: 146, epoch: 145, acc: 57.26495726495726, f1: 21.18056873316955, r: 0.2311013931900119
06/02/2019 09:37:07 *** epoch: 147 ***
06/02/2019 09:37:07 *** training ***
06/02/2019 09:37:07 step: 4823, epoch: 146, batch: 4, loss: 0.1553182303905487, acc: 95.3125, f1: 84.0909090909091, r: 0.7623481536435163
06/02/2019 09:37:08 step: 4828, epoch: 146, batch: 9, loss: 0.08108404278755188, acc: 95.3125, f1: 93.88594398798482, r: 0.7284936922865562
06/02/2019 09:37:09 step: 4833, epoch: 146, batch: 14, loss: 0.10939884930849075, acc: 95.3125, f1: 92.14785214785215, r: 0.6770931390171921
06/02/2019 09:37:09 step: 4838, epoch: 146, batch: 19, loss: 0.02138097584247589, acc: 100.0, f1: 100.0, r: 0.6958309168802423
06/02/2019 09:37:10 step: 4843, epoch: 146, batch: 24, loss: 0.09261789917945862, acc: 95.3125, f1: 90.48304047736897, r: 0.7497770629276435
06/02/2019 09:37:10 step: 4848, epoch: 146, batch: 29, loss: 0.011412043124437332, acc: 100.0, f1: 100.0, r: 0.7093062603051791
06/02/2019 09:37:10 *** evaluating ***
06/02/2019 09:37:11 step: 147, epoch: 146, acc: 57.692307692307686, f1: 21.251434121656253, r: 0.24366062309109848
06/02/2019 09:37:11 *** epoch: 148 ***
06/02/2019 09:37:11 *** training ***
06/02/2019 09:37:11 step: 4856, epoch: 147, batch: 4, loss: 0.1373797208070755, acc: 93.75, f1: 78.17627050820329, r: 0.6917948052530843
06/02/2019 09:37:12 step: 4861, epoch: 147, batch: 9, loss: 0.14894059300422668, acc: 93.75, f1: 90.85982877999686, r: 0.6902468755345298
06/02/2019 09:37:12 step: 4866, epoch: 147, batch: 14, loss: 0.1274588406085968, acc: 95.3125, f1: 85.8263305322129, r: 0.5846596489421026
06/02/2019 09:37:13 step: 4871, epoch: 147, batch: 19, loss: 0.0870383232831955, acc: 93.75, f1: 79.15464165464165, r: 0.771659722535302
06/02/2019 09:37:13 step: 4876, epoch: 147, batch: 24, loss: 0.056998006999492645, acc: 96.875, f1: 93.73015873015873, r: 0.7366060242174676
06/02/2019 09:37:14 step: 4881, epoch: 147, batch: 29, loss: 0.06851600855588913, acc: 98.4375, f1: 94.9579831932773, r: 0.6740660214280926
06/02/2019 09:37:14 *** evaluating ***
06/02/2019 09:37:14 step: 148, epoch: 147, acc: 58.119658119658126, f1: 21.692444694413197, r: 0.23388165807300407
06/02/2019 09:37:14 *** epoch: 149 ***
06/02/2019 09:37:14 *** training ***
06/02/2019 09:37:15 step: 4889, epoch: 148, batch: 4, loss: 0.12030645459890366, acc: 95.3125, f1: 88.47939175670268, r: 0.6518100214838614
06/02/2019 09:37:15 step: 4894, epoch: 148, batch: 9, loss: 0.10177800059318542, acc: 96.875, f1: 96.2474248188534, r: 0.654496763101275
06/02/2019 09:37:16 step: 4899, epoch: 148, batch: 14, loss: 0.043574247509241104, acc: 98.4375, f1: 96.52173913043478, r: 0.6801193042160876
06/02/2019 09:37:16 step: 4904, epoch: 148, batch: 19, loss: 0.11425048112869263, acc: 93.75, f1: 90.17667496473774, r: 0.6613205275196989
06/02/2019 09:37:17 step: 4909, epoch: 148, batch: 24, loss: 0.09104844927787781, acc: 96.875, f1: 91.16883116883116, r: 0.7647435531583345
06/02/2019 09:37:17 step: 4914, epoch: 148, batch: 29, loss: 0.08871100097894669, acc: 95.3125, f1: 96.4442842354333, r: 0.562017920714469
06/02/2019 09:37:18 *** evaluating ***
06/02/2019 09:37:18 step: 149, epoch: 148, acc: 57.26495726495726, f1: 21.213739855454442, r: 0.2355410584313513
06/02/2019 09:37:18 *** epoch: 150 ***
06/02/2019 09:37:18 *** training ***
06/02/2019 09:37:19 step: 4922, epoch: 149, batch: 4, loss: 0.03615982457995415, acc: 98.4375, f1: 99.30029154518951, r: 0.6791838232533312
06/02/2019 09:37:19 step: 4927, epoch: 149, batch: 9, loss: 0.23722654581069946, acc: 92.1875, f1: 89.12745879851143, r: 0.6579088880438201
06/02/2019 09:37:20 step: 4932, epoch: 149, batch: 14, loss: 0.0784263014793396, acc: 96.875, f1: 93.95982783357245, r: 0.7374924812075065
06/02/2019 09:37:20 step: 4937, epoch: 149, batch: 19, loss: 0.125590518116951, acc: 92.1875, f1: 78.64802570311045, r: 0.7385206580198568
06/02/2019 09:37:21 step: 4942, epoch: 149, batch: 24, loss: 0.09841691702604294, acc: 95.3125, f1: 93.44481340923465, r: 0.7567885172252121
06/02/2019 09:37:21 step: 4947, epoch: 149, batch: 29, loss: 0.04283641278743744, acc: 98.4375, f1: 97.57236227824464, r: 0.7154477043717639
06/02/2019 09:37:22 *** evaluating ***
06/02/2019 09:37:22 step: 150, epoch: 149, acc: 56.837606837606835, f1: 20.90385183491047, r: 0.22277383053808517
06/02/2019 09:37:22 *** epoch: 151 ***
06/02/2019 09:37:22 *** training ***
06/02/2019 09:37:22 step: 4955, epoch: 150, batch: 4, loss: 0.04192719608545303, acc: 98.4375, f1: 98.45916795069337, r: 0.6552986967008015
06/02/2019 09:37:23 step: 4960, epoch: 150, batch: 9, loss: 0.16695530712604523, acc: 93.75, f1: 74.6137566137566, r: 0.6323326318529345
06/02/2019 09:37:23 step: 4965, epoch: 150, batch: 14, loss: 0.18753091990947723, acc: 92.1875, f1: 77.42832989275115, r: 0.7485152915430864
06/02/2019 09:37:24 step: 4970, epoch: 150, batch: 19, loss: 0.24925993382930756, acc: 95.3125, f1: 89.47772657450076, r: 0.7253848278501165
06/02/2019 09:37:24 step: 4975, epoch: 150, batch: 24, loss: 0.13459232449531555, acc: 95.3125, f1: 79.94418280132567, r: 0.7016572945226964
06/02/2019 09:37:25 step: 4980, epoch: 150, batch: 29, loss: 0.07729335129261017, acc: 98.4375, f1: 98.2051282051282, r: 0.8233743930834874
06/02/2019 09:37:25 *** evaluating ***
06/02/2019 09:37:25 step: 151, epoch: 150, acc: 58.119658119658126, f1: 21.44765528363005, r: 0.21745754504094805
06/02/2019 09:37:25 *** epoch: 152 ***
06/02/2019 09:37:25 *** training ***
06/02/2019 09:37:26 step: 4988, epoch: 151, batch: 4, loss: 0.1228770911693573, acc: 96.875, f1: 85.03401360544218, r: 0.6660311769670814
06/02/2019 09:37:26 step: 4993, epoch: 151, batch: 9, loss: 0.13693107664585114, acc: 96.875, f1: 82.16687683900798, r: 0.8011550391569242
06/02/2019 09:37:27 step: 4998, epoch: 151, batch: 14, loss: 0.03917689993977547, acc: 98.4375, f1: 86.11111111111111, r: 0.741660333598301
06/02/2019 09:37:27 step: 5003, epoch: 151, batch: 19, loss: 0.09014696627855301, acc: 95.3125, f1: 96.46722086922959, r: 0.6331321978042194
06/02/2019 09:37:28 step: 5008, epoch: 151, batch: 24, loss: 0.049776121973991394, acc: 96.875, f1: 95.58760107816711, r: 0.6773706112470276
06/02/2019 09:37:29 step: 5013, epoch: 151, batch: 29, loss: 0.08181889355182648, acc: 95.3125, f1: 95.11989795918367, r: 0.7623888076001841
06/02/2019 09:37:29 *** evaluating ***
06/02/2019 09:37:29 step: 152, epoch: 151, acc: 57.692307692307686, f1: 21.161796243185282, r: 0.2267196934291943
06/02/2019 09:37:29 *** epoch: 153 ***
06/02/2019 09:37:29 *** training ***
06/02/2019 09:37:30 step: 5021, epoch: 152, batch: 4, loss: 0.0793883353471756, acc: 95.3125, f1: 92.20872317646511, r: 0.6773615937146664
06/02/2019 09:37:30 step: 5026, epoch: 152, batch: 9, loss: 0.24034830927848816, acc: 89.0625, f1: 76.6426282051282, r: 0.6690302219486222
06/02/2019 09:37:31 step: 5031, epoch: 152, batch: 14, loss: 0.07143433392047882, acc: 96.875, f1: 92.00680272108842, r: 0.6500631582089761
06/02/2019 09:37:31 step: 5036, epoch: 152, batch: 19, loss: 0.06395742297172546, acc: 98.4375, f1: 96.86274509803921, r: 0.681533414654931
06/02/2019 09:37:32 step: 5041, epoch: 152, batch: 24, loss: 0.02376803755760193, acc: 100.0, f1: 100.0, r: 0.7164475440788876
06/02/2019 09:37:32 step: 5046, epoch: 152, batch: 29, loss: 0.046316154301166534, acc: 98.4375, f1: 98.44155844155846, r: 0.7547161564685464
06/02/2019 09:37:33 *** evaluating ***
06/02/2019 09:37:33 step: 153, epoch: 152, acc: 57.692307692307686, f1: 21.147575499098437, r: 0.23424037771150466
06/02/2019 09:37:33 *** epoch: 154 ***
06/02/2019 09:37:33 *** training ***
06/02/2019 09:37:33 step: 5054, epoch: 153, batch: 4, loss: 0.05497866868972778, acc: 98.4375, f1: 97.68964189449365, r: 0.658631356324479
06/02/2019 09:37:34 step: 5059, epoch: 153, batch: 9, loss: 0.1609746217727661, acc: 92.1875, f1: 81.5963765963766, r: 0.7524233634501626
06/02/2019 09:37:34 step: 5064, epoch: 153, batch: 14, loss: 0.05030471086502075, acc: 100.0, f1: 100.0, r: 0.7223086470446964
06/02/2019 09:37:35 step: 5069, epoch: 153, batch: 19, loss: 0.02619001641869545, acc: 100.0, f1: 100.0, r: 0.7613337292537719
06/02/2019 09:37:36 step: 5074, epoch: 153, batch: 24, loss: 0.06519502401351929, acc: 96.875, f1: 93.60535726668449, r: 0.6438220246067665
06/02/2019 09:37:36 step: 5079, epoch: 153, batch: 29, loss: 0.10478590428829193, acc: 96.875, f1: 82.91666666666666, r: 0.7071264147968389
06/02/2019 09:37:36 *** evaluating ***
06/02/2019 09:37:37 step: 154, epoch: 153, acc: 58.119658119658126, f1: 22.147054620798055, r: 0.2375711025416543
06/02/2019 09:37:37 *** epoch: 155 ***
06/02/2019 09:37:37 *** training ***
06/02/2019 09:37:37 step: 5087, epoch: 154, batch: 4, loss: 0.07937745749950409, acc: 96.875, f1: 84.28571428571429, r: 0.6590156115561869
06/02/2019 09:37:38 step: 5092, epoch: 154, batch: 9, loss: 0.09297657757997513, acc: 96.875, f1: 85.36271481745466, r: 0.7389028990589198
06/02/2019 09:37:38 step: 5097, epoch: 154, batch: 14, loss: 0.09611926227807999, acc: 95.3125, f1: 81.82171276998864, r: 0.7256569021968208
06/02/2019 09:37:39 step: 5102, epoch: 154, batch: 19, loss: 0.01590752601623535, acc: 100.0, f1: 100.0, r: 0.7124548766544196
06/02/2019 09:37:39 step: 5107, epoch: 154, batch: 24, loss: 0.05310036614537239, acc: 96.875, f1: 95.04989219274933, r: 0.6898977893263956
06/02/2019 09:37:40 step: 5112, epoch: 154, batch: 29, loss: 0.10796895623207092, acc: 95.3125, f1: 91.43826786835263, r: 0.7148222524514545
06/02/2019 09:37:40 *** evaluating ***
06/02/2019 09:37:40 step: 155, epoch: 154, acc: 57.692307692307686, f1: 21.013584117032394, r: 0.25211673920070915
06/02/2019 09:37:40 *** epoch: 156 ***
06/02/2019 09:37:40 *** training ***
06/02/2019 09:37:41 step: 5120, epoch: 155, batch: 4, loss: 0.14868301153182983, acc: 92.1875, f1: 87.12318840579711, r: 0.7634916804823093
06/02/2019 09:37:41 step: 5125, epoch: 155, batch: 9, loss: 0.04286812245845795, acc: 98.4375, f1: 85.71428571428572, r: 0.7237884720833144
06/02/2019 09:37:42 step: 5130, epoch: 155, batch: 14, loss: 0.02845725789666176, acc: 100.0, f1: 100.0, r: 0.7840218485322425
06/02/2019 09:37:42 step: 5135, epoch: 155, batch: 19, loss: 0.07968807220458984, acc: 98.4375, f1: 99.1864406779661, r: 0.597896387021417
06/02/2019 09:37:43 step: 5140, epoch: 155, batch: 24, loss: 0.06255632638931274, acc: 98.4375, f1: 97.25490196078431, r: 0.7200975764669231
06/02/2019 09:37:43 step: 5145, epoch: 155, batch: 29, loss: 0.10344478487968445, acc: 96.875, f1: 96.70954045954046, r: 0.7050635248371675
06/02/2019 09:37:44 *** evaluating ***
06/02/2019 09:37:44 step: 156, epoch: 155, acc: 58.119658119658126, f1: 21.258771857631174, r: 0.24439533983355013
06/02/2019 09:37:44 *** epoch: 157 ***
06/02/2019 09:37:44 *** training ***
06/02/2019 09:37:44 step: 5153, epoch: 156, batch: 4, loss: 0.0678064227104187, acc: 96.875, f1: 95.90067340067341, r: 0.8053595977055539
06/02/2019 09:37:45 step: 5158, epoch: 156, batch: 9, loss: 0.03471478074789047, acc: 100.0, f1: 100.0, r: 0.8123621947663745
06/02/2019 09:37:46 step: 5163, epoch: 156, batch: 14, loss: 0.060210444033145905, acc: 98.4375, f1: 92.59259259259261, r: 0.5946758916246005
06/02/2019 09:37:46 step: 5168, epoch: 156, batch: 19, loss: 0.042232394218444824, acc: 100.0, f1: 100.0, r: 0.7339221424363632
06/02/2019 09:37:47 step: 5173, epoch: 156, batch: 24, loss: 0.14930613338947296, acc: 92.1875, f1: 84.43035742725183, r: 0.6962791912890128
06/02/2019 09:37:47 step: 5178, epoch: 156, batch: 29, loss: 0.06943696737289429, acc: 98.4375, f1: 95.17543859649122, r: 0.839952672706602
06/02/2019 09:37:47 *** evaluating ***
06/02/2019 09:37:48 step: 157, epoch: 156, acc: 58.54700854700855, f1: 21.785115307455264, r: 0.25112045120542886
06/02/2019 09:37:48 *** epoch: 158 ***
06/02/2019 09:37:48 *** training ***
06/02/2019 09:37:48 step: 5186, epoch: 157, batch: 4, loss: 0.018810972571372986, acc: 100.0, f1: 100.0, r: 0.6936340371832848
06/02/2019 09:37:49 step: 5191, epoch: 157, batch: 9, loss: 0.07155749946832657, acc: 96.875, f1: 96.54441486739623, r: 0.65095649240783
06/02/2019 09:37:49 step: 5196, epoch: 157, batch: 14, loss: 0.035271406173706055, acc: 98.4375, f1: 99.29118773946361, r: 0.7280725100572842
06/02/2019 09:37:50 step: 5201, epoch: 157, batch: 19, loss: 0.04539695382118225, acc: 98.4375, f1: 94.87179487179486, r: 0.8041129950101409
06/02/2019 09:37:50 step: 5206, epoch: 157, batch: 24, loss: 0.03538366034626961, acc: 100.0, f1: 100.0, r: 0.8116387782944225
06/02/2019 09:37:51 step: 5211, epoch: 157, batch: 29, loss: 0.08972533047199249, acc: 96.875, f1: 95.81529581529583, r: 0.7076690091495497
06/02/2019 09:37:51 *** evaluating ***
06/02/2019 09:37:51 step: 158, epoch: 157, acc: 58.119658119658126, f1: 21.635127584280124, r: 0.2445174665233648
06/02/2019 09:37:51 *** epoch: 159 ***
06/02/2019 09:37:51 *** training ***
06/02/2019 09:37:52 step: 5219, epoch: 158, batch: 4, loss: 0.059407491236925125, acc: 98.4375, f1: 97.55102040816327, r: 0.7539964936750507
06/02/2019 09:37:52 step: 5224, epoch: 158, batch: 9, loss: 0.02963140606880188, acc: 100.0, f1: 100.0, r: 0.8164500384691739
06/02/2019 09:37:53 step: 5229, epoch: 158, batch: 14, loss: 0.18032413721084595, acc: 89.0625, f1: 79.42088293650794, r: 0.7133498645291286
06/02/2019 09:37:53 step: 5234, epoch: 158, batch: 19, loss: 0.0866277813911438, acc: 95.3125, f1: 95.30630654650811, r: 0.7365964717598421
06/02/2019 09:37:54 step: 5239, epoch: 158, batch: 24, loss: 0.1254151612520218, acc: 95.3125, f1: 89.18832490261062, r: 0.705466273464196
06/02/2019 09:37:54 step: 5244, epoch: 158, batch: 29, loss: 0.10359339416027069, acc: 95.3125, f1: 77.1969696969697, r: 0.7526870703704832
06/02/2019 09:37:55 *** evaluating ***
06/02/2019 09:37:55 step: 159, epoch: 158, acc: 55.55555555555556, f1: 20.90563023161603, r: 0.21984920217161427
06/02/2019 09:37:55 *** epoch: 160 ***
06/02/2019 09:37:55 *** training ***
06/02/2019 09:37:56 step: 5252, epoch: 159, batch: 4, loss: 0.13288788497447968, acc: 93.75, f1: 91.62549230218404, r: 0.6463770250980626
06/02/2019 09:37:56 step: 5257, epoch: 159, batch: 9, loss: 0.10273409634828568, acc: 95.3125, f1: 89.81962481962482, r: 0.6804445309543264
06/02/2019 09:37:57 step: 5262, epoch: 159, batch: 14, loss: 0.06136362627148628, acc: 98.4375, f1: 85.0, r: 0.7410224059574468
06/02/2019 09:37:57 step: 5267, epoch: 159, batch: 19, loss: 0.12127722054719925, acc: 93.75, f1: 86.34920634920636, r: 0.7975601146886551
06/02/2019 09:37:58 step: 5272, epoch: 159, batch: 24, loss: 0.05858374014496803, acc: 96.875, f1: 84.21182266009852, r: 0.6799119694514058
06/02/2019 09:37:58 step: 5277, epoch: 159, batch: 29, loss: 0.04101424291729927, acc: 98.4375, f1: 97.20279720279721, r: 0.5914733339325106
06/02/2019 09:37:58 *** evaluating ***
06/02/2019 09:37:59 step: 160, epoch: 159, acc: 57.692307692307686, f1: 21.453667862378317, r: 0.2556760789184503
06/02/2019 09:37:59 *** epoch: 161 ***
06/02/2019 09:37:59 *** training ***
06/02/2019 09:37:59 step: 5285, epoch: 160, batch: 4, loss: 0.06523633748292923, acc: 98.4375, f1: 98.14814814814815, r: 0.7850053549598834
06/02/2019 09:38:00 step: 5290, epoch: 160, batch: 9, loss: 0.05781152471899986, acc: 98.4375, f1: 97.86096256684492, r: 0.6884147387206984
06/02/2019 09:38:00 step: 5295, epoch: 160, batch: 14, loss: 0.14568811655044556, acc: 92.1875, f1: 87.12603722807805, r: 0.6389648670392047
06/02/2019 09:38:01 step: 5300, epoch: 160, batch: 19, loss: 0.05145121365785599, acc: 98.4375, f1: 97.57236227824464, r: 0.7140593914818453
06/02/2019 09:38:01 step: 5305, epoch: 160, batch: 24, loss: 0.044226355850696564, acc: 98.4375, f1: 85.71428571428572, r: 0.7280656217152766
06/02/2019 09:38:02 step: 5310, epoch: 160, batch: 29, loss: 0.09695044159889221, acc: 96.875, f1: 94.66914038342608, r: 0.6758995557827459
06/02/2019 09:38:02 *** evaluating ***
06/02/2019 09:38:03 step: 161, epoch: 160, acc: 57.26495726495726, f1: 21.402242307897403, r: 0.23695476731306542
06/02/2019 09:38:03 *** epoch: 162 ***
06/02/2019 09:38:03 *** training ***
06/02/2019 09:38:03 step: 5318, epoch: 161, batch: 4, loss: 0.09081454575061798, acc: 96.875, f1: 85.01587301587301, r: 0.8017582920788116
06/02/2019 09:38:04 step: 5323, epoch: 161, batch: 9, loss: 0.0436457023024559, acc: 96.875, f1: 91.35924101441343, r: 0.6368238125730379
06/02/2019 09:38:04 step: 5328, epoch: 161, batch: 14, loss: 0.13998347520828247, acc: 95.3125, f1: 79.32539682539684, r: 0.7323827697059523
06/02/2019 09:38:05 step: 5333, epoch: 161, batch: 19, loss: 0.01804417371749878, acc: 100.0, f1: 100.0, r: 0.6342206335447461
06/02/2019 09:38:05 step: 5338, epoch: 161, batch: 24, loss: 0.027650661766529083, acc: 98.4375, f1: 93.33333333333333, r: 0.7889987249818385
06/02/2019 09:38:06 step: 5343, epoch: 161, batch: 29, loss: 0.08824869990348816, acc: 96.875, f1: 95.37787163631435, r: 0.6646782596425165
06/02/2019 09:38:06 *** evaluating ***
06/02/2019 09:38:06 step: 162, epoch: 161, acc: 57.692307692307686, f1: 21.762861321684852, r: 0.22775283746582833
06/02/2019 09:38:06 *** epoch: 163 ***
06/02/2019 09:38:06 *** training ***
06/02/2019 09:38:07 step: 5351, epoch: 162, batch: 4, loss: 0.07525242120027542, acc: 98.4375, f1: 97.94941900205059, r: 0.7171550021914572
06/02/2019 09:38:07 step: 5356, epoch: 162, batch: 9, loss: 0.11156250536441803, acc: 95.3125, f1: 82.41666666666667, r: 0.7693900642525789
06/02/2019 09:38:08 step: 5361, epoch: 162, batch: 14, loss: 0.09493192285299301, acc: 95.3125, f1: 93.78335949764521, r: 0.8085686073245625
06/02/2019 09:38:08 step: 5366, epoch: 162, batch: 19, loss: 0.08081801235675812, acc: 96.875, f1: 94.52685139096289, r: 0.7028001765083182
06/02/2019 09:38:09 step: 5371, epoch: 162, batch: 24, loss: 0.02619228884577751, acc: 100.0, f1: 100.0, r: 0.7422361287112318
06/02/2019 09:38:09 step: 5376, epoch: 162, batch: 29, loss: 0.10614605247974396, acc: 95.3125, f1: 89.08237207926648, r: 0.7739347780725077
06/02/2019 09:38:10 *** evaluating ***
06/02/2019 09:38:10 step: 163, epoch: 162, acc: 57.26495726495726, f1: 22.334476463747116, r: 0.2501201556452382
06/02/2019 09:38:10 *** epoch: 164 ***
06/02/2019 09:38:10 *** training ***
06/02/2019 09:38:10 step: 5384, epoch: 163, batch: 4, loss: 0.0665750652551651, acc: 96.875, f1: 92.23689475790316, r: 0.6461527386409764
06/02/2019 09:38:11 step: 5389, epoch: 163, batch: 9, loss: 0.07742586731910706, acc: 98.4375, f1: 97.6608187134503, r: 0.6589691571956845
06/02/2019 09:38:11 step: 5394, epoch: 163, batch: 14, loss: 0.015678852796554565, acc: 100.0, f1: 100.0, r: 0.7018134071066486
06/02/2019 09:38:12 step: 5399, epoch: 163, batch: 19, loss: 0.07456236332654953, acc: 95.3125, f1: 95.36193470511849, r: 0.7797300574792393
06/02/2019 09:38:12 step: 5404, epoch: 163, batch: 24, loss: 0.12830597162246704, acc: 93.75, f1: 94.32973189275711, r: 0.7243887906200535
06/02/2019 09:38:13 step: 5409, epoch: 163, batch: 29, loss: 0.04179768264293671, acc: 100.0, f1: 100.0, r: 0.8203392499569109
06/02/2019 09:38:13 *** evaluating ***
06/02/2019 09:38:13 step: 164, epoch: 163, acc: 59.82905982905983, f1: 22.8974517556843, r: 0.2439447944483094
06/02/2019 09:38:13 *** epoch: 165 ***
06/02/2019 09:38:13 *** training ***
06/02/2019 09:38:14 step: 5417, epoch: 164, batch: 4, loss: 0.12057790905237198, acc: 95.3125, f1: 79.72769567597153, r: 0.6631718771156293
06/02/2019 09:38:14 step: 5422, epoch: 164, batch: 9, loss: 0.049570996314287186, acc: 100.0, f1: 100.0, r: 0.7862781567820372
06/02/2019 09:38:15 step: 5427, epoch: 164, batch: 14, loss: 0.12607797980308533, acc: 93.75, f1: 80.94306855934764, r: 0.6963831289793676
06/02/2019 09:38:15 step: 5432, epoch: 164, batch: 19, loss: 0.040449973195791245, acc: 100.0, f1: 100.0, r: 0.7497002945085133
06/02/2019 09:38:16 step: 5437, epoch: 164, batch: 24, loss: 0.07000739872455597, acc: 96.875, f1: 94.01320901320902, r: 0.7612135516913638
06/02/2019 09:38:16 step: 5442, epoch: 164, batch: 29, loss: 0.06841996312141418, acc: 96.875, f1: 95.02645502645503, r: 0.6461302168269845
06/02/2019 09:38:17 *** evaluating ***
06/02/2019 09:38:17 step: 165, epoch: 164, acc: 56.41025641025641, f1: 21.43846749877878, r: 0.23717653593689947
06/02/2019 09:38:17 *** epoch: 166 ***
06/02/2019 09:38:17 *** training ***
06/02/2019 09:38:17 step: 5450, epoch: 165, batch: 4, loss: 0.02439142018556595, acc: 100.0, f1: 100.0, r: 0.71424002350734
06/02/2019 09:38:18 step: 5455, epoch: 165, batch: 9, loss: 0.0485229529440403, acc: 98.4375, f1: 98.80745341614906, r: 0.7321585356886492
06/02/2019 09:38:18 step: 5460, epoch: 165, batch: 14, loss: 0.06468778103590012, acc: 98.4375, f1: 98.89012208657047, r: 0.6917958915471774
06/02/2019 09:38:19 step: 5465, epoch: 165, batch: 19, loss: 0.05455400422215462, acc: 98.4375, f1: 97.06896551724138, r: 0.8073335339030148
06/02/2019 09:38:19 step: 5470, epoch: 165, batch: 24, loss: 0.07955263555049896, acc: 96.875, f1: 80.07518796992481, r: 0.5157253334696016
06/02/2019 09:38:20 step: 5475, epoch: 165, batch: 29, loss: 0.0828704684972763, acc: 95.3125, f1: 89.33313779048807, r: 0.6477820602872689
06/02/2019 09:38:20 *** evaluating ***
06/02/2019 09:38:21 step: 166, epoch: 165, acc: 56.837606837606835, f1: 21.016202430077403, r: 0.23042389992009937
06/02/2019 09:38:21 *** epoch: 167 ***
06/02/2019 09:38:21 *** training ***
06/02/2019 09:38:21 step: 5483, epoch: 166, batch: 4, loss: 0.08860860764980316, acc: 95.3125, f1: 88.31668331668332, r: 0.7400082817897475
06/02/2019 09:38:22 step: 5488, epoch: 166, batch: 9, loss: 0.04751705005764961, acc: 98.4375, f1: 97.64957264957265, r: 0.8049020006856364
06/02/2019 09:38:22 step: 5493, epoch: 166, batch: 14, loss: 0.050653159618377686, acc: 98.4375, f1: 98.76344086021506, r: 0.7717056562272645
06/02/2019 09:38:23 step: 5498, epoch: 166, batch: 19, loss: 0.08314228057861328, acc: 95.3125, f1: 93.30357142857142, r: 0.7915978759217164
06/02/2019 09:38:23 step: 5503, epoch: 166, batch: 24, loss: 0.09200248122215271, acc: 96.875, f1: 97.62690616423785, r: 0.8022334005859361
06/02/2019 09:38:24 step: 5508, epoch: 166, batch: 29, loss: 0.14665254950523376, acc: 92.1875, f1: 80.24943310657596, r: 0.6264937530642314
06/02/2019 09:38:24 *** evaluating ***
06/02/2019 09:38:24 step: 167, epoch: 166, acc: 57.26495726495726, f1: 21.071610629508932, r: 0.22233913581691023
06/02/2019 09:38:24 *** epoch: 168 ***
06/02/2019 09:38:24 *** training ***
06/02/2019 09:38:25 step: 5516, epoch: 167, batch: 4, loss: 0.08902090787887573, acc: 98.4375, f1: 87.06896551724138, r: 0.7898187415239254
06/02/2019 09:38:25 step: 5521, epoch: 167, batch: 9, loss: 0.0812297835946083, acc: 95.3125, f1: 92.85970992170452, r: 0.6768312990799094
06/02/2019 09:38:26 step: 5526, epoch: 167, batch: 14, loss: 0.10211101919412613, acc: 93.75, f1: 81.47649572649573, r: 0.8539473668249569
06/02/2019 09:38:26 step: 5531, epoch: 167, batch: 19, loss: 0.037314023822546005, acc: 98.4375, f1: 96.1111111111111, r: 0.7960002608467682
06/02/2019 09:38:27 step: 5536, epoch: 167, batch: 24, loss: 0.0730527937412262, acc: 96.875, f1: 80.84415584415584, r: 0.6861574464646568
06/02/2019 09:38:27 step: 5541, epoch: 167, batch: 29, loss: 0.08896923810243607, acc: 96.875, f1: 93.65079365079364, r: 0.6741998238332643
06/02/2019 09:38:28 *** evaluating ***
06/02/2019 09:38:28 step: 168, epoch: 167, acc: 57.26495726495726, f1: 21.215322789732955, r: 0.2378178487271566
06/02/2019 09:38:28 *** epoch: 169 ***
06/02/2019 09:38:28 *** training ***
06/02/2019 09:38:28 step: 5549, epoch: 168, batch: 4, loss: 0.16463004052639008, acc: 95.3125, f1: 79.39147506509936, r: 0.6468622298097803
06/02/2019 09:38:29 step: 5554, epoch: 168, batch: 9, loss: 0.028604283928871155, acc: 98.4375, f1: 97.6608187134503, r: 0.6808289173794967
06/02/2019 09:38:29 step: 5559, epoch: 168, batch: 14, loss: 0.056079715490341187, acc: 98.4375, f1: 98.97071872227151, r: 0.7021276042357062
06/02/2019 09:38:30 step: 5564, epoch: 168, batch: 19, loss: 0.12111058831214905, acc: 95.3125, f1: 93.51613298981721, r: 0.7499275111933448
06/02/2019 09:38:30 step: 5569, epoch: 168, batch: 24, loss: 0.10615482926368713, acc: 93.75, f1: 80.1663253346212, r: 0.5222943797077616
06/02/2019 09:38:31 step: 5574, epoch: 168, batch: 29, loss: 0.08143134415149689, acc: 96.875, f1: 94.49205166418282, r: 0.7208992181994175
06/02/2019 09:38:31 *** evaluating ***
06/02/2019 09:38:31 step: 169, epoch: 168, acc: 56.837606837606835, f1: 21.348354982375596, r: 0.23315876715335737
06/02/2019 09:38:31 *** epoch: 170 ***
06/02/2019 09:38:31 *** training ***
06/02/2019 09:38:32 step: 5582, epoch: 169, batch: 4, loss: 0.08421654254198074, acc: 96.875, f1: 89.58333333333333, r: 0.7418807139633169
06/02/2019 09:38:32 step: 5587, epoch: 169, batch: 9, loss: 0.030948854982852936, acc: 98.4375, f1: 97.43008314436887, r: 0.6807529091086953
06/02/2019 09:38:33 step: 5592, epoch: 169, batch: 14, loss: 0.06154303252696991, acc: 98.4375, f1: 99.1436925647452, r: 0.8072653531010827
06/02/2019 09:38:34 step: 5597, epoch: 169, batch: 19, loss: 0.1765855997800827, acc: 92.1875, f1: 81.74264762565527, r: 0.7009348708510558
06/02/2019 09:38:34 step: 5602, epoch: 169, batch: 24, loss: 0.04163273423910141, acc: 98.4375, f1: 98.93939393939395, r: 0.7940811957413034
06/02/2019 09:38:35 step: 5607, epoch: 169, batch: 29, loss: 0.05309547483921051, acc: 98.4375, f1: 97.27891156462584, r: 0.6790994241827898
06/02/2019 09:38:35 *** evaluating ***
06/02/2019 09:38:35 step: 170, epoch: 169, acc: 58.54700854700855, f1: 21.442424447739146, r: 0.2503767199615273
06/02/2019 09:38:35 *** epoch: 171 ***
06/02/2019 09:38:35 *** training ***
06/02/2019 09:38:36 step: 5615, epoch: 170, batch: 4, loss: 0.03443082422018051, acc: 98.4375, f1: 99.24633936261843, r: 0.7296347447179911
06/02/2019 09:38:36 step: 5620, epoch: 170, batch: 9, loss: 0.10989576578140259, acc: 95.3125, f1: 95.93487993487993, r: 0.5390272490665912
06/02/2019 09:38:37 step: 5625, epoch: 170, batch: 14, loss: 0.13433682918548584, acc: 93.75, f1: 78.74054708732127, r: 0.6653106734457946
06/02/2019 09:38:37 step: 5630, epoch: 170, batch: 19, loss: 0.05000089854001999, acc: 98.4375, f1: 98.3201581027668, r: 0.7633934770343007
06/02/2019 09:38:38 step: 5635, epoch: 170, batch: 24, loss: 0.07499060034751892, acc: 96.875, f1: 95.56102900930487, r: 0.7911617296450818
06/02/2019 09:38:38 step: 5640, epoch: 170, batch: 29, loss: 0.05450282618403435, acc: 98.4375, f1: 98.85714285714286, r: 0.7480179179404381
06/02/2019 09:38:39 *** evaluating ***
06/02/2019 09:38:39 step: 171, epoch: 170, acc: 56.837606837606835, f1: 21.097198421353273, r: 0.23068848759085078
06/02/2019 09:38:39 *** epoch: 172 ***
06/02/2019 09:38:39 *** training ***
06/02/2019 09:38:39 step: 5648, epoch: 171, batch: 4, loss: 0.038368530571460724, acc: 100.0, f1: 100.0, r: 0.7133642378410779
06/02/2019 09:38:40 step: 5653, epoch: 171, batch: 9, loss: 0.08295881748199463, acc: 95.3125, f1: 92.25808745453585, r: 0.7549072279760325
06/02/2019 09:38:41 step: 5658, epoch: 171, batch: 14, loss: 0.12417390197515488, acc: 95.3125, f1: 90.23394100996049, r: 0.7835321442524975
06/02/2019 09:38:41 step: 5663, epoch: 171, batch: 19, loss: 0.02646421268582344, acc: 100.0, f1: 100.0, r: 0.790579832308106
06/02/2019 09:38:42 step: 5668, epoch: 171, batch: 24, loss: 0.07889388501644135, acc: 95.3125, f1: 85.03654970760233, r: 0.794495038553959
06/02/2019 09:38:42 step: 5673, epoch: 171, batch: 29, loss: 0.09561740607023239, acc: 95.3125, f1: 83.90883190883191, r: 0.7537051881987388
06/02/2019 09:38:42 *** evaluating ***
06/02/2019 09:38:43 step: 172, epoch: 171, acc: 57.26495726495726, f1: 21.241945584631093, r: 0.2228263344201439
06/02/2019 09:38:43 *** epoch: 173 ***
06/02/2019 09:38:43 *** training ***
06/02/2019 09:38:43 step: 5681, epoch: 172, batch: 4, loss: 0.05385289341211319, acc: 96.875, f1: 98.19109461966605, r: 0.6278754933032165
06/02/2019 09:38:44 step: 5686, epoch: 172, batch: 9, loss: 0.041373927146196365, acc: 100.0, f1: 100.0, r: 0.7976428739518808
06/02/2019 09:38:44 step: 5691, epoch: 172, batch: 14, loss: 0.05002891272306442, acc: 98.4375, f1: 94.87179487179486, r: 0.7993101936481753
06/02/2019 09:38:45 step: 5696, epoch: 172, batch: 19, loss: 0.05953468382358551, acc: 98.4375, f1: 99.37611408199643, r: 0.7514369450534228
06/02/2019 09:38:45 step: 5701, epoch: 172, batch: 24, loss: 0.07227887213230133, acc: 98.4375, f1: 98.79336349924586, r: 0.767067063554567
06/02/2019 09:38:46 step: 5706, epoch: 172, batch: 29, loss: 0.0912427082657814, acc: 95.3125, f1: 94.33066933066934, r: 0.8166958769573235
06/02/2019 09:38:46 *** evaluating ***
06/02/2019 09:38:46 step: 173, epoch: 172, acc: 57.26495726495726, f1: 20.964342674868988, r: 0.22921351226890477
06/02/2019 09:38:46 *** epoch: 174 ***
06/02/2019 09:38:46 *** training ***
06/02/2019 09:38:47 step: 5714, epoch: 173, batch: 4, loss: 0.03944878652691841, acc: 100.0, f1: 100.0, r: 0.6814572518445726
06/02/2019 09:38:47 step: 5719, epoch: 173, batch: 9, loss: 0.06504624336957932, acc: 98.4375, f1: 98.06076276664511, r: 0.6987098061403505
06/02/2019 09:38:48 step: 5724, epoch: 173, batch: 14, loss: 0.10229401290416718, acc: 93.75, f1: 79.53514739229026, r: 0.6736099677161315
06/02/2019 09:38:49 step: 5729, epoch: 173, batch: 19, loss: 0.09405422955751419, acc: 93.75, f1: 57.03703703703704, r: 0.6364561325696411
06/02/2019 09:38:49 step: 5734, epoch: 173, batch: 24, loss: 0.06414012610912323, acc: 96.875, f1: 95.92837134853941, r: 0.6997300218637508
06/02/2019 09:38:50 step: 5739, epoch: 173, batch: 29, loss: 0.060756754130125046, acc: 98.4375, f1: 94.6969696969697, r: 0.7698654045847402
06/02/2019 09:38:50 *** evaluating ***
06/02/2019 09:38:50 step: 174, epoch: 173, acc: 57.692307692307686, f1: 21.36221189456606, r: 0.23028244084102253
06/02/2019 09:38:50 *** epoch: 175 ***
06/02/2019 09:38:50 *** training ***
06/02/2019 09:38:51 step: 5747, epoch: 174, batch: 4, loss: 0.10808286815881729, acc: 93.75, f1: 82.41812354312354, r: 0.747438800085324
06/02/2019 09:38:51 step: 5752, epoch: 174, batch: 9, loss: 0.0699186623096466, acc: 98.4375, f1: 98.69883437245866, r: 0.637532798242449
06/02/2019 09:38:52 step: 5757, epoch: 174, batch: 14, loss: 0.057605523616075516, acc: 98.4375, f1: 95.55555555555556, r: 0.6520792451611744
06/02/2019 09:38:52 step: 5762, epoch: 174, batch: 19, loss: 0.14941486716270447, acc: 95.3125, f1: 91.39957264957265, r: 0.7262681117441768
06/02/2019 09:38:53 step: 5767, epoch: 174, batch: 24, loss: 0.03662201017141342, acc: 98.4375, f1: 86.11111111111111, r: 0.7745193218687634
06/02/2019 09:38:53 step: 5772, epoch: 174, batch: 29, loss: 0.05104551464319229, acc: 96.875, f1: 84.45652173913044, r: 0.6961598320030983
06/02/2019 09:38:54 *** evaluating ***
06/02/2019 09:38:54 step: 175, epoch: 174, acc: 56.837606837606835, f1: 20.675765174104832, r: 0.23189825731665767
06/02/2019 09:38:54 *** epoch: 176 ***
06/02/2019 09:38:54 *** training ***
06/02/2019 09:38:54 step: 5780, epoch: 175, batch: 4, loss: 0.12080302834510803, acc: 96.875, f1: 89.29263565891472, r: 0.6852076939032594
06/02/2019 09:38:55 step: 5785, epoch: 175, batch: 9, loss: 0.028837107121944427, acc: 100.0, f1: 100.0, r: 0.7185279690573227
06/02/2019 09:38:55 step: 5790, epoch: 175, batch: 14, loss: 0.07587666809558868, acc: 95.3125, f1: 82.74343583587282, r: 0.6088975058636503
06/02/2019 09:38:56 step: 5795, epoch: 175, batch: 19, loss: 0.08863585442304611, acc: 96.875, f1: 97.06959706959707, r: 0.6847077035125044
06/02/2019 09:38:57 step: 5800, epoch: 175, batch: 24, loss: 0.08985738456249237, acc: 95.3125, f1: 94.80913823019085, r: 0.8200348946223048
06/02/2019 09:38:57 step: 5805, epoch: 175, batch: 29, loss: 0.0858486145734787, acc: 96.875, f1: 83.21428571428571, r: 0.7942570570110495
06/02/2019 09:38:57 *** evaluating ***
06/02/2019 09:38:58 step: 176, epoch: 175, acc: 57.692307692307686, f1: 22.053577928696228, r: 0.23342090108811397
06/02/2019 09:38:58 *** epoch: 177 ***
06/02/2019 09:38:58 *** training ***
06/02/2019 09:38:58 step: 5813, epoch: 176, batch: 4, loss: 0.07250210642814636, acc: 98.4375, f1: 94.44444444444444, r: 0.7997363986308075
06/02/2019 09:38:59 step: 5818, epoch: 176, batch: 9, loss: 0.03792262822389603, acc: 96.875, f1: 96.08033689666343, r: 0.6582732908802996
06/02/2019 09:38:59 step: 5823, epoch: 176, batch: 14, loss: 0.057048119604587555, acc: 96.875, f1: 94.53703703703704, r: 0.7919689598814752
06/02/2019 09:39:00 step: 5828, epoch: 176, batch: 19, loss: 0.04328174144029617, acc: 98.4375, f1: 97.11399711399712, r: 0.7430851291995211
06/02/2019 09:39:00 step: 5833, epoch: 176, batch: 24, loss: 0.14081618189811707, acc: 95.3125, f1: 94.20518207282913, r: 0.6834777718974733
06/02/2019 09:39:01 step: 5838, epoch: 176, batch: 29, loss: 0.09560766816139221, acc: 95.3125, f1: 93.87277002744243, r: 0.7296361221664069
06/02/2019 09:39:01 *** evaluating ***
06/02/2019 09:39:01 step: 177, epoch: 176, acc: 58.54700854700855, f1: 22.622367485878875, r: 0.24177150390272661
06/02/2019 09:39:01 *** epoch: 178 ***
06/02/2019 09:39:01 *** training ***
06/02/2019 09:39:02 step: 5846, epoch: 177, batch: 4, loss: 0.05240942910313606, acc: 96.875, f1: 84.16666666666669, r: 0.816117915946708
06/02/2019 09:39:02 step: 5851, epoch: 177, batch: 9, loss: 0.035243429243564606, acc: 98.4375, f1: 98.04639804639805, r: 0.7346037095468986
06/02/2019 09:39:03 step: 5856, epoch: 177, batch: 14, loss: 0.055973175913095474, acc: 96.875, f1: 92.542602325211, r: 0.7637125633776776
06/02/2019 09:39:03 step: 5861, epoch: 177, batch: 19, loss: 0.037388045340776443, acc: 98.4375, f1: 98.96800825593395, r: 0.6570239177372645
06/02/2019 09:39:04 step: 5866, epoch: 177, batch: 24, loss: 0.07353045791387558, acc: 98.4375, f1: 97.11399711399712, r: 0.6797830813580127
06/02/2019 09:39:04 step: 5871, epoch: 177, batch: 29, loss: 0.10843362659215927, acc: 96.875, f1: 96.58705587498157, r: 0.7037405208395301
06/02/2019 09:39:05 *** evaluating ***
06/02/2019 09:39:05 step: 178, epoch: 177, acc: 58.54700854700855, f1: 24.169212796484874, r: 0.24207714683023057
06/02/2019 09:39:05 *** epoch: 179 ***
06/02/2019 09:39:05 *** training ***
06/02/2019 09:39:05 step: 5879, epoch: 178, batch: 4, loss: 0.07189644873142242, acc: 98.4375, f1: 85.71428571428572, r: 0.7874126622895578
06/02/2019 09:39:06 step: 5884, epoch: 178, batch: 9, loss: 0.15602272748947144, acc: 93.75, f1: 81.63789428815005, r: 0.6922207422426052
06/02/2019 09:39:06 step: 5889, epoch: 178, batch: 14, loss: 0.033534467220306396, acc: 100.0, f1: 100.0, r: 0.6964019417095688
06/02/2019 09:39:07 step: 5894, epoch: 178, batch: 19, loss: 0.09598451107740402, acc: 93.75, f1: 75.46875, r: 0.6373086815059431
06/02/2019 09:39:08 step: 5899, epoch: 178, batch: 24, loss: 0.05441021919250488, acc: 96.875, f1: 92.48426643384626, r: 0.6927728918425106
06/02/2019 09:39:08 step: 5904, epoch: 178, batch: 29, loss: 0.020786438137292862, acc: 100.0, f1: 100.0, r: 0.7540536450570596
06/02/2019 09:39:08 *** evaluating ***
06/02/2019 09:39:09 step: 179, epoch: 178, acc: 57.692307692307686, f1: 24.008847303257898, r: 0.239774628094238
06/02/2019 09:39:09 *** epoch: 180 ***
06/02/2019 09:39:09 *** training ***
06/02/2019 09:39:09 step: 5912, epoch: 179, batch: 4, loss: 0.0900360643863678, acc: 98.4375, f1: 99.27943024717217, r: 0.6774616505146629
06/02/2019 09:39:10 step: 5917, epoch: 179, batch: 9, loss: 0.12118127197027206, acc: 93.75, f1: 82.3373015873016, r: 0.7531967770712694
06/02/2019 09:39:10 step: 5922, epoch: 179, batch: 14, loss: 0.0964701771736145, acc: 95.3125, f1: 90.07227891156462, r: 0.788785372673166
06/02/2019 09:39:11 step: 5927, epoch: 179, batch: 19, loss: 0.038072939962148666, acc: 100.0, f1: 100.0, r: 0.7060995323218264
06/02/2019 09:39:11 step: 5932, epoch: 179, batch: 24, loss: 0.03745907545089722, acc: 98.4375, f1: 96.36363636363636, r: 0.783964311675819
06/02/2019 09:39:12 step: 5937, epoch: 179, batch: 29, loss: 0.12870126962661743, acc: 95.3125, f1: 86.53985507246377, r: 0.6784711065694058
06/02/2019 09:39:12 *** evaluating ***
06/02/2019 09:39:12 step: 180, epoch: 179, acc: 57.26495726495726, f1: 21.406614815385154, r: 0.22262408791698776
06/02/2019 09:39:12 *** epoch: 181 ***
06/02/2019 09:39:12 *** training ***
06/02/2019 09:39:13 step: 5945, epoch: 180, batch: 4, loss: 0.016652919352054596, acc: 100.0, f1: 100.0, r: 0.7420150331916522
06/02/2019 09:39:13 step: 5950, epoch: 180, batch: 9, loss: 0.09791835397481918, acc: 96.875, f1: 92.89115646258503, r: 0.7026264394140793
06/02/2019 09:39:14 step: 5955, epoch: 180, batch: 14, loss: 0.012933671474456787, acc: 100.0, f1: 100.0, r: 0.7047095837978866
06/02/2019 09:39:14 step: 5960, epoch: 180, batch: 19, loss: 0.1467055380344391, acc: 93.75, f1: 82.00266282181175, r: 0.7209778614913739
06/02/2019 09:39:15 step: 5965, epoch: 180, batch: 24, loss: 0.13534453511238098, acc: 93.75, f1: 94.44936595327785, r: 0.6235460605591867
06/02/2019 09:39:15 step: 5970, epoch: 180, batch: 29, loss: 0.1208399161696434, acc: 93.75, f1: 93.23062558356676, r: 0.7596216796220754
06/02/2019 09:39:16 *** evaluating ***
06/02/2019 09:39:16 step: 181, epoch: 180, acc: 57.692307692307686, f1: 21.74794818237441, r: 0.23688943396786183
06/02/2019 09:39:16 *** epoch: 182 ***
06/02/2019 09:39:16 *** training ***
06/02/2019 09:39:17 step: 5978, epoch: 181, batch: 4, loss: 0.13813109695911407, acc: 92.1875, f1: 90.9721648443453, r: 0.671277552103521
06/02/2019 09:39:17 step: 5983, epoch: 181, batch: 9, loss: 0.08575424551963806, acc: 96.875, f1: 91.63650075414782, r: 0.7500530890115455
06/02/2019 09:39:18 step: 5988, epoch: 181, batch: 14, loss: 0.06530681997537613, acc: 98.4375, f1: 97.46031746031747, r: 0.7044371718540646
06/02/2019 09:39:18 step: 5993, epoch: 181, batch: 19, loss: 0.01624920964241028, acc: 100.0, f1: 100.0, r: 0.7659783145402517
06/02/2019 09:39:19 step: 5998, epoch: 181, batch: 24, loss: 0.039169684052467346, acc: 98.4375, f1: 96.37188208616782, r: 0.7095502738860606
06/02/2019 09:39:19 step: 6003, epoch: 181, batch: 29, loss: 0.09841388463973999, acc: 95.3125, f1: 88.09523809523809, r: 0.7791619756322523
06/02/2019 09:39:19 *** evaluating ***
06/02/2019 09:39:20 step: 182, epoch: 181, acc: 59.401709401709404, f1: 23.584435285824433, r: 0.23768737114837377
06/02/2019 09:39:20 *** epoch: 183 ***
06/02/2019 09:39:20 *** training ***
06/02/2019 09:39:20 step: 6011, epoch: 182, batch: 4, loss: 0.06632432341575623, acc: 96.875, f1: 95.29059134322291, r: 0.6991895458884424
06/02/2019 09:39:21 step: 6016, epoch: 182, batch: 9, loss: 0.12087003886699677, acc: 93.75, f1: 82.39087301587303, r: 0.6658847116288766
06/02/2019 09:39:21 step: 6021, epoch: 182, batch: 14, loss: 0.07220041751861572, acc: 96.875, f1: 93.19727891156464, r: 0.6830057549694873
06/02/2019 09:39:22 step: 6026, epoch: 182, batch: 19, loss: 0.0541209913790226, acc: 96.875, f1: 93.56776556776558, r: 0.7155994606781348
06/02/2019 09:39:22 step: 6031, epoch: 182, batch: 24, loss: 0.023373566567897797, acc: 100.0, f1: 100.0, r: 0.7787956074868119
06/02/2019 09:39:23 step: 6036, epoch: 182, batch: 29, loss: 0.11285258084535599, acc: 92.1875, f1: 88.8263515932689, r: 0.7029241809440597
06/02/2019 09:39:23 *** evaluating ***
06/02/2019 09:39:23 step: 183, epoch: 182, acc: 57.26495726495726, f1: 21.394705069336883, r: 0.23136635138348816
06/02/2019 09:39:23 *** epoch: 184 ***
06/02/2019 09:39:23 *** training ***
06/02/2019 09:39:24 step: 6044, epoch: 183, batch: 4, loss: 0.10940764844417572, acc: 95.3125, f1: 80.66798941798943, r: 0.7268649606335132
06/02/2019 09:39:24 step: 6049, epoch: 183, batch: 9, loss: 0.004881270229816437, acc: 100.0, f1: 100.0, r: 0.7539784041410847
06/02/2019 09:39:25 step: 6054, epoch: 183, batch: 14, loss: 0.13061368465423584, acc: 90.625, f1: 86.99157049600745, r: 0.7762535877620965
06/02/2019 09:39:25 step: 6059, epoch: 183, batch: 19, loss: 0.04532138258218765, acc: 100.0, f1: 100.0, r: 0.7697295276332358
06/02/2019 09:39:26 step: 6064, epoch: 183, batch: 24, loss: 0.04338570684194565, acc: 100.0, f1: 100.0, r: 0.7200684819407807
06/02/2019 09:39:26 step: 6069, epoch: 183, batch: 29, loss: 0.1024957001209259, acc: 95.3125, f1: 96.0344438170525, r: 0.7250668794097063
06/02/2019 09:39:26 *** evaluating ***
06/02/2019 09:39:27 step: 184, epoch: 183, acc: 57.26495726495726, f1: 21.253975211748443, r: 0.23425795834972524
06/02/2019 09:39:27 *** epoch: 185 ***
06/02/2019 09:39:27 *** training ***
06/02/2019 09:39:27 step: 6077, epoch: 184, batch: 4, loss: 0.03301772475242615, acc: 98.4375, f1: 97.77777777777779, r: 0.8121959544576761
06/02/2019 09:39:28 step: 6082, epoch: 184, batch: 9, loss: 0.04273422062397003, acc: 98.4375, f1: 97.22222222222221, r: 0.7901810915959963
06/02/2019 09:39:28 step: 6087, epoch: 184, batch: 14, loss: 0.009942114353179932, acc: 100.0, f1: 100.0, r: 0.764261657239422
06/02/2019 09:39:29 step: 6092, epoch: 184, batch: 19, loss: 0.07136834412813187, acc: 98.4375, f1: 98.3201581027668, r: 0.8431346522660917
06/02/2019 09:39:29 step: 6097, epoch: 184, batch: 24, loss: 0.23187384009361267, acc: 92.1875, f1: 89.77006911217438, r: 0.6790788541723192
06/02/2019 09:39:30 step: 6102, epoch: 184, batch: 29, loss: 0.09358563274145126, acc: 95.3125, f1: 90.64007421150279, r: 0.7636080623852499
06/02/2019 09:39:30 *** evaluating ***
06/02/2019 09:39:30 step: 185, epoch: 184, acc: 58.97435897435898, f1: 22.31822344322344, r: 0.23588492367269512
06/02/2019 09:39:30 *** epoch: 186 ***
06/02/2019 09:39:30 *** training ***
06/02/2019 09:39:31 step: 6110, epoch: 185, batch: 4, loss: 0.07782652974128723, acc: 95.3125, f1: 83.84210526315789, r: 0.7449745361437479
06/02/2019 09:39:31 step: 6115, epoch: 185, batch: 9, loss: 0.0733010545372963, acc: 96.875, f1: 79.46428571428572, r: 0.6691882605535988
06/02/2019 09:39:32 step: 6120, epoch: 185, batch: 14, loss: 0.1035337820649147, acc: 93.75, f1: 87.44010647737356, r: 0.6403587241373525
06/02/2019 09:39:32 step: 6125, epoch: 185, batch: 19, loss: 0.20961661636829376, acc: 92.1875, f1: 78.74902382768516, r: 0.6781076037193502
06/02/2019 09:39:33 step: 6130, epoch: 185, batch: 24, loss: 0.07226799428462982, acc: 98.4375, f1: 93.33333333333333, r: 0.6997775591813644
06/02/2019 09:39:33 step: 6135, epoch: 185, batch: 29, loss: 0.015420593321323395, acc: 100.0, f1: 100.0, r: 0.7427212180826577
06/02/2019 09:39:33 *** evaluating ***
06/02/2019 09:39:34 step: 186, epoch: 185, acc: 58.119658119658126, f1: 22.542352078765788, r: 0.2223979267786655
06/02/2019 09:39:34 *** epoch: 187 ***
06/02/2019 09:39:34 *** training ***
06/02/2019 09:39:34 step: 6143, epoch: 186, batch: 4, loss: 0.05983956158161163, acc: 96.875, f1: 96.25977886847451, r: 0.7194615503109286
06/02/2019 09:39:35 step: 6148, epoch: 186, batch: 9, loss: 0.011753112077713013, acc: 100.0, f1: 100.0, r: 0.8422251868813593
06/02/2019 09:39:35 step: 6153, epoch: 186, batch: 14, loss: 0.08263514935970306, acc: 96.875, f1: 82.8420854507811, r: 0.6649983865602236
06/02/2019 09:39:36 step: 6158, epoch: 186, batch: 19, loss: 0.026672184467315674, acc: 98.4375, f1: 98.8191632928475, r: 0.825757713570296
06/02/2019 09:39:36 step: 6163, epoch: 186, batch: 24, loss: 0.18103104829788208, acc: 92.1875, f1: 75.6722334682861, r: 0.7234677917360934
06/02/2019 09:39:37 step: 6168, epoch: 186, batch: 29, loss: 0.08048390597105026, acc: 95.3125, f1: 93.41203703703704, r: 0.7978833600620503
06/02/2019 09:39:37 *** evaluating ***
06/02/2019 09:39:37 step: 187, epoch: 186, acc: 58.119658119658126, f1: 22.516633655750628, r: 0.23822837585961978
06/02/2019 09:39:37 *** epoch: 188 ***
06/02/2019 09:39:37 *** training ***
06/02/2019 09:39:38 step: 6176, epoch: 187, batch: 4, loss: 0.06714378297328949, acc: 96.875, f1: 82.87581699346404, r: 0.7906714250131793
06/02/2019 09:39:38 step: 6181, epoch: 187, batch: 9, loss: 0.061438556760549545, acc: 96.875, f1: 95.1890331890332, r: 0.786342604635802
06/02/2019 09:39:39 step: 6186, epoch: 187, batch: 14, loss: 0.07208787649869919, acc: 96.875, f1: 94.38536306460834, r: 0.7627703574801283
06/02/2019 09:39:39 step: 6191, epoch: 187, batch: 19, loss: 0.094151571393013, acc: 95.3125, f1: 94.7288343840068, r: 0.667795709666758
06/02/2019 09:39:40 step: 6196, epoch: 187, batch: 24, loss: 0.04862837493419647, acc: 98.4375, f1: 96.42857142857143, r: 0.7852749056123645
06/02/2019 09:39:40 step: 6201, epoch: 187, batch: 29, loss: 0.07220476865768433, acc: 95.3125, f1: 79.68260188087774, r: 0.7563539326951385
06/02/2019 09:39:41 *** evaluating ***
06/02/2019 09:39:41 step: 188, epoch: 187, acc: 58.54700854700855, f1: 23.54088816759485, r: 0.23569267688245057
06/02/2019 09:39:41 *** epoch: 189 ***
06/02/2019 09:39:41 *** training ***
06/02/2019 09:39:41 step: 6209, epoch: 188, batch: 4, loss: 0.06797940284013748, acc: 96.875, f1: 91.43247462919595, r: 0.6840754992924374
06/02/2019 09:39:42 step: 6214, epoch: 188, batch: 9, loss: 0.021117575466632843, acc: 100.0, f1: 100.0, r: 0.7530621904247423
06/02/2019 09:39:42 step: 6219, epoch: 188, batch: 14, loss: 0.035895831882953644, acc: 96.875, f1: 82.2701407607068, r: 0.5905048126728122
06/02/2019 09:39:43 step: 6224, epoch: 188, batch: 19, loss: 0.025535181164741516, acc: 100.0, f1: 100.0, r: 0.7967530951155521
06/02/2019 09:39:43 step: 6229, epoch: 188, batch: 24, loss: 0.058629266917705536, acc: 98.4375, f1: 97.1188475390156, r: 0.7026935556747219
06/02/2019 09:39:44 step: 6234, epoch: 188, batch: 29, loss: 0.04352673143148422, acc: 98.4375, f1: 97.74891774891775, r: 0.6829015454542623
06/02/2019 09:39:44 *** evaluating ***
06/02/2019 09:39:45 step: 189, epoch: 188, acc: 57.26495726495726, f1: 22.335236404936865, r: 0.24097607443952937
06/02/2019 09:39:45 *** epoch: 190 ***
06/02/2019 09:39:45 *** training ***
06/02/2019 09:39:45 step: 6242, epoch: 189, batch: 4, loss: 0.09051330387592316, acc: 95.3125, f1: 92.11669283097854, r: 0.7185064825726056
06/02/2019 09:39:46 step: 6247, epoch: 189, batch: 9, loss: 0.13191291689872742, acc: 93.75, f1: 85.01984126984128, r: 0.6304230699323465
06/02/2019 09:39:46 step: 6252, epoch: 189, batch: 14, loss: 0.011577799916267395, acc: 100.0, f1: 100.0, r: 0.7566427063118285
06/02/2019 09:39:47 step: 6257, epoch: 189, batch: 19, loss: 0.0800183117389679, acc: 98.4375, f1: 95.71428571428571, r: 0.7464696403195995
06/02/2019 09:39:47 step: 6262, epoch: 189, batch: 24, loss: 0.04811311513185501, acc: 98.4375, f1: 97.27891156462584, r: 0.6457813937878597
06/02/2019 09:39:48 step: 6267, epoch: 189, batch: 29, loss: 0.026929326355457306, acc: 98.4375, f1: 98.8421052631579, r: 0.7561192156538742
06/02/2019 09:39:48 *** evaluating ***
06/02/2019 09:39:48 step: 190, epoch: 189, acc: 57.26495726495726, f1: 20.735063550665437, r: 0.23292825074195356
06/02/2019 09:39:48 *** epoch: 191 ***
06/02/2019 09:39:48 *** training ***
06/02/2019 09:39:49 step: 6275, epoch: 190, batch: 4, loss: 0.054012417793273926, acc: 96.875, f1: 91.26344086021506, r: 0.7176381545140424
06/02/2019 09:39:49 step: 6280, epoch: 190, batch: 9, loss: 0.0461433082818985, acc: 100.0, f1: 100.0, r: 0.6509359936424686
06/02/2019 09:39:50 step: 6285, epoch: 190, batch: 14, loss: 0.057340897619724274, acc: 98.4375, f1: 97.61904761904762, r: 0.7870422603576787
06/02/2019 09:39:50 step: 6290, epoch: 190, batch: 19, loss: 0.5878320336341858, acc: 95.3125, f1: 90.4854890768317, r: 0.5599413225776513
06/02/2019 09:39:51 step: 6295, epoch: 190, batch: 24, loss: 0.10202309489250183, acc: 96.875, f1: 86.42805232558139, r: 0.6798278716331365
06/02/2019 09:39:51 step: 6300, epoch: 190, batch: 29, loss: 0.10656283050775528, acc: 93.75, f1: 88.70184558349452, r: 0.7711034899311059
06/02/2019 09:39:52 *** evaluating ***
06/02/2019 09:39:52 step: 191, epoch: 190, acc: 58.54700854700855, f1: 23.627166491596636, r: 0.2639003657295701
06/02/2019 09:39:52 *** epoch: 192 ***
06/02/2019 09:39:52 *** training ***
06/02/2019 09:39:52 step: 6308, epoch: 191, batch: 4, loss: 0.1225428432226181, acc: 96.875, f1: 84.68137254901961, r: 0.6719918421814224
06/02/2019 09:39:53 step: 6313, epoch: 191, batch: 9, loss: 0.08143478631973267, acc: 96.875, f1: 91.69934640522877, r: 0.7310474443574235
06/02/2019 09:39:53 step: 6318, epoch: 191, batch: 14, loss: 0.042441561818122864, acc: 96.875, f1: 96.12891751682753, r: 0.7039366664104062
06/02/2019 09:39:54 step: 6323, epoch: 191, batch: 19, loss: 0.07708407193422318, acc: 96.875, f1: 94.21182266009852, r: 0.7527018347238208
06/02/2019 09:39:54 step: 6328, epoch: 191, batch: 24, loss: 0.032597124576568604, acc: 98.4375, f1: 97.83549783549783, r: 0.7483805520068292
06/02/2019 09:39:55 step: 6333, epoch: 191, batch: 29, loss: 0.059966761618852615, acc: 96.875, f1: 93.4640522875817, r: 0.5326552159628132
06/02/2019 09:39:55 *** evaluating ***
06/02/2019 09:39:56 step: 192, epoch: 191, acc: 58.97435897435898, f1: 23.677277466762124, r: 0.25112631461181745
06/02/2019 09:39:56 *** epoch: 193 ***
06/02/2019 09:39:56 *** training ***
06/02/2019 09:39:56 step: 6341, epoch: 192, batch: 4, loss: 0.02973843365907669, acc: 98.4375, f1: 96.37188208616782, r: 0.6363519702532802
06/02/2019 09:39:57 step: 6346, epoch: 192, batch: 9, loss: 0.12985466420650482, acc: 95.3125, f1: 81.5020115020115, r: 0.6473591843660518
06/02/2019 09:39:57 step: 6351, epoch: 192, batch: 14, loss: 0.036754488945007324, acc: 98.4375, f1: 83.67346938775509, r: 0.5884221804436922
06/02/2019 09:39:58 step: 6356, epoch: 192, batch: 19, loss: 0.04070456326007843, acc: 98.4375, f1: 98.20868786386028, r: 0.7148106123990817
06/02/2019 09:39:58 step: 6361, epoch: 192, batch: 24, loss: 0.06311681866645813, acc: 98.4375, f1: 98.67669172932331, r: 0.726288072095189
06/02/2019 09:39:59 step: 6366, epoch: 192, batch: 29, loss: 0.10605708509683609, acc: 95.3125, f1: 92.59006810136742, r: 0.6492572958942496
06/02/2019 09:39:59 *** evaluating ***
06/02/2019 09:39:59 step: 193, epoch: 192, acc: 57.692307692307686, f1: 22.250693899979552, r: 0.22883030174464797
06/02/2019 09:39:59 *** epoch: 194 ***
06/02/2019 09:39:59 *** training ***
06/02/2019 09:40:00 step: 6374, epoch: 193, batch: 4, loss: 0.04913496971130371, acc: 96.875, f1: 93.42592592592594, r: 0.5677696858956631
06/02/2019 09:40:00 step: 6379, epoch: 193, batch: 9, loss: 0.06784485280513763, acc: 96.875, f1: 93.9094942324756, r: 0.6433035047644252
06/02/2019 09:40:01 step: 6384, epoch: 193, batch: 14, loss: 0.04068318009376526, acc: 98.4375, f1: 93.19727891156461, r: 0.6758030731882853
06/02/2019 09:40:01 step: 6389, epoch: 193, batch: 19, loss: 0.1298048496246338, acc: 96.875, f1: 97.66164994425863, r: 0.7707709689320754
06/02/2019 09:40:02 step: 6394, epoch: 193, batch: 24, loss: 0.06523506343364716, acc: 98.4375, f1: 85.09316770186335, r: 0.6093417961110208
06/02/2019 09:40:02 step: 6399, epoch: 193, batch: 29, loss: 0.07162778079509735, acc: 96.875, f1: 87.08333333333333, r: 0.6493206888127174
06/02/2019 09:40:03 *** evaluating ***
06/02/2019 09:40:03 step: 194, epoch: 193, acc: 56.41025641025641, f1: 21.783793722333343, r: 0.23146495252188323
06/02/2019 09:40:03 *** epoch: 195 ***
06/02/2019 09:40:03 *** training ***
06/02/2019 09:40:03 step: 6407, epoch: 194, batch: 4, loss: 0.020857512950897217, acc: 100.0, f1: 100.0, r: 0.7014128366529387
06/02/2019 09:40:04 step: 6412, epoch: 194, batch: 9, loss: 0.05560871213674545, acc: 98.4375, f1: 85.14285714285714, r: 0.5811525862428971
06/02/2019 09:40:04 step: 6417, epoch: 194, batch: 14, loss: 0.0995762050151825, acc: 96.875, f1: 96.73274715291521, r: 0.6811894861222756
06/02/2019 09:40:05 step: 6422, epoch: 194, batch: 19, loss: 0.03350833058357239, acc: 98.4375, f1: 97.11399711399712, r: 0.67696571082812
06/02/2019 09:40:05 step: 6427, epoch: 194, batch: 24, loss: 0.05641733482480049, acc: 98.4375, f1: 98.3201581027668, r: 0.750331866459896
06/02/2019 09:40:06 step: 6432, epoch: 194, batch: 29, loss: 0.03696635738015175, acc: 98.4375, f1: 98.72122762148338, r: 0.7687977901003942
06/02/2019 09:40:06 *** evaluating ***
06/02/2019 09:40:06 step: 195, epoch: 194, acc: 57.26495726495726, f1: 22.36624147289908, r: 0.24926616789508935
06/02/2019 09:40:06 *** epoch: 196 ***
06/02/2019 09:40:06 *** training ***
06/02/2019 09:40:07 step: 6440, epoch: 195, batch: 4, loss: 0.027132198214530945, acc: 100.0, f1: 100.0, r: 0.6878263723976655
06/02/2019 09:40:07 step: 6445, epoch: 195, batch: 9, loss: 0.07504428923130035, acc: 96.875, f1: 93.90491861080096, r: 0.7887270155506212
06/02/2019 09:40:08 step: 6450, epoch: 195, batch: 14, loss: 0.02064421772956848, acc: 100.0, f1: 100.0, r: 0.7951926152567546
06/02/2019 09:40:08 step: 6455, epoch: 195, batch: 19, loss: 0.050416722893714905, acc: 96.875, f1: 91.28205128205128, r: 0.6738826240083824
06/02/2019 09:40:09 step: 6460, epoch: 195, batch: 24, loss: 0.057619739323854446, acc: 98.4375, f1: 93.93939393939394, r: 0.6920661964461088
06/02/2019 09:40:09 step: 6465, epoch: 195, batch: 29, loss: 0.07036309689283371, acc: 96.875, f1: 94.30363543266768, r: 0.5755450742740412
06/02/2019 09:40:10 *** evaluating ***
06/02/2019 09:40:10 step: 196, epoch: 195, acc: 56.837606837606835, f1: 21.55846654734926, r: 0.22386479306642235
06/02/2019 09:40:10 *** epoch: 197 ***
06/02/2019 09:40:10 *** training ***
06/02/2019 09:40:11 step: 6473, epoch: 196, batch: 4, loss: 0.08338862657546997, acc: 95.3125, f1: 69.80706075533662, r: 0.6628008902420176
06/02/2019 09:40:11 step: 6478, epoch: 196, batch: 9, loss: 0.13766811788082123, acc: 95.3125, f1: 72.91504395237564, r: 0.6827455398847375
06/02/2019 09:40:12 step: 6483, epoch: 196, batch: 14, loss: 0.018720053136348724, acc: 100.0, f1: 100.0, r: 0.7597908285618298
06/02/2019 09:40:12 step: 6488, epoch: 196, batch: 19, loss: 0.10308478772640228, acc: 95.3125, f1: 83.0294604283482, r: 0.8213305137036926
06/02/2019 09:40:13 step: 6493, epoch: 196, batch: 24, loss: 0.05401022732257843, acc: 96.875, f1: 94.03716216216216, r: 0.7597069719504177
06/02/2019 09:40:13 step: 6498, epoch: 196, batch: 29, loss: 0.058313339948654175, acc: 96.875, f1: 96.62377994676132, r: 0.7194120161275811
06/02/2019 09:40:13 *** evaluating ***
06/02/2019 09:40:14 step: 197, epoch: 196, acc: 56.837606837606835, f1: 21.445962096699915, r: 0.20583336672740826
06/02/2019 09:40:14 *** epoch: 198 ***
06/02/2019 09:40:14 *** training ***
06/02/2019 09:40:14 step: 6506, epoch: 197, batch: 4, loss: 0.06989631056785583, acc: 96.875, f1: 93.26923076923077, r: 0.807684228369749
06/02/2019 09:40:15 step: 6511, epoch: 197, batch: 9, loss: 0.020000524818897247, acc: 100.0, f1: 100.0, r: 0.6093570960816087
06/02/2019 09:40:15 step: 6516, epoch: 197, batch: 14, loss: 0.008001364767551422, acc: 100.0, f1: 100.0, r: 0.7041320270605801
06/02/2019 09:40:16 step: 6521, epoch: 197, batch: 19, loss: 0.06193692982196808, acc: 98.4375, f1: 86.36363636363636, r: 0.7732552973448049
06/02/2019 09:40:16 step: 6526, epoch: 197, batch: 24, loss: 0.04027021676301956, acc: 98.4375, f1: 97.72727272727273, r: 0.8173927785928891
06/02/2019 09:40:17 step: 6531, epoch: 197, batch: 29, loss: 0.032204560935497284, acc: 100.0, f1: 100.0, r: 0.8466447380048934
06/02/2019 09:40:17 *** evaluating ***
06/02/2019 09:40:18 step: 198, epoch: 197, acc: 57.26495726495726, f1: 22.80835003430848, r: 0.24347111303677804
06/02/2019 09:40:18 *** epoch: 199 ***
06/02/2019 09:40:18 *** training ***
06/02/2019 09:40:18 step: 6539, epoch: 198, batch: 4, loss: 0.04846003279089928, acc: 98.4375, f1: 94.44444444444444, r: 0.7417200988802344
06/02/2019 09:40:18 step: 6544, epoch: 198, batch: 9, loss: 0.03823975846171379, acc: 98.4375, f1: 93.19727891156464, r: 0.7091564988819111
06/02/2019 09:40:19 step: 6549, epoch: 198, batch: 14, loss: 0.05201626569032669, acc: 96.875, f1: 95.86385836385836, r: 0.7104040567588644
06/02/2019 09:40:20 step: 6554, epoch: 198, batch: 19, loss: 0.0397869348526001, acc: 98.4375, f1: 99.09937888198758, r: 0.8113931819731062
06/02/2019 09:40:20 step: 6559, epoch: 198, batch: 24, loss: 0.01642507314682007, acc: 98.4375, f1: 98.14921920185078, r: 0.6953931495209367
06/02/2019 09:40:21 step: 6564, epoch: 198, batch: 29, loss: 0.06482025980949402, acc: 98.4375, f1: 99.01577652337531, r: 0.692108149729171
06/02/2019 09:40:21 *** evaluating ***
06/02/2019 09:40:21 step: 199, epoch: 198, acc: 58.119658119658126, f1: 22.29038632498454, r: 0.23113006902365257
06/02/2019 09:40:21 *** epoch: 200 ***
06/02/2019 09:40:21 *** training ***
06/02/2019 09:40:22 step: 6572, epoch: 199, batch: 4, loss: 0.018812574446201324, acc: 100.0, f1: 100.0, r: 0.7090890676257641
06/02/2019 09:40:22 step: 6577, epoch: 199, batch: 9, loss: 0.04797136038541794, acc: 96.875, f1: 89.77777777777777, r: 0.7485880675052524
06/02/2019 09:40:23 step: 6582, epoch: 199, batch: 14, loss: 0.022585786879062653, acc: 100.0, f1: 100.0, r: 0.7063639148181645
06/02/2019 09:40:23 step: 6587, epoch: 199, batch: 19, loss: 0.01749381422996521, acc: 100.0, f1: 100.0, r: 0.8406349915510001
06/02/2019 09:40:24 step: 6592, epoch: 199, batch: 24, loss: 0.026212461292743683, acc: 100.0, f1: 100.0, r: 0.5776582306615851
06/02/2019 09:40:24 step: 6597, epoch: 199, batch: 29, loss: 0.10825134068727493, acc: 95.3125, f1: 84.15229885057471, r: 0.8272479447295915
06/02/2019 09:40:25 *** evaluating ***
06/02/2019 09:40:25 step: 200, epoch: 199, acc: 58.54700854700855, f1: 23.13869955075224, r: 0.24725260012664613
06/02/2019 09:40:25 *** epoch: 201 ***
06/02/2019 09:40:25 *** training ***
06/02/2019 09:40:25 step: 6605, epoch: 200, batch: 4, loss: 0.022861160337924957, acc: 100.0, f1: 100.0, r: 0.7812054289950906
06/02/2019 09:40:26 step: 6610, epoch: 200, batch: 9, loss: 0.0406096875667572, acc: 98.4375, f1: 95.91836734693878, r: 0.7115437323759374
06/02/2019 09:40:27 step: 6615, epoch: 200, batch: 14, loss: 0.03128726780414581, acc: 98.4375, f1: 95.10204081632654, r: 0.7076988773148963
06/02/2019 09:40:27 step: 6620, epoch: 200, batch: 19, loss: 0.11732742935419083, acc: 96.875, f1: 94.34865900383143, r: 0.6448301051290559
06/02/2019 09:40:28 step: 6625, epoch: 200, batch: 24, loss: 0.03631956875324249, acc: 98.4375, f1: 95.43010752688173, r: 0.8307398482581219
06/02/2019 09:40:28 step: 6630, epoch: 200, batch: 29, loss: 0.045983705669641495, acc: 98.4375, f1: 97.90209790209789, r: 0.8410912505676523
06/02/2019 09:40:28 *** evaluating ***
06/02/2019 09:40:29 step: 201, epoch: 200, acc: 59.82905982905983, f1: 23.75925544671752, r: 0.2527502868359039
06/02/2019 09:40:29 *** epoch: 202 ***
06/02/2019 09:40:29 *** training ***
06/02/2019 09:40:29 step: 6638, epoch: 201, batch: 4, loss: 0.0578894205391407, acc: 96.875, f1: 91.93077453947019, r: 0.7801609942755086
06/02/2019 09:40:29 step: 6643, epoch: 201, batch: 9, loss: 0.06211631000041962, acc: 95.3125, f1: 95.37575910931174, r: 0.7327597035708765
06/02/2019 09:40:30 step: 6648, epoch: 201, batch: 14, loss: 0.04736796021461487, acc: 98.4375, f1: 84.61538461538461, r: 0.6687663390141514
06/02/2019 09:40:31 step: 6653, epoch: 201, batch: 19, loss: 0.054946694523096085, acc: 96.875, f1: 94.17469627147047, r: 0.7556361074465706
06/02/2019 09:40:31 step: 6658, epoch: 201, batch: 24, loss: 0.007295042276382446, acc: 100.0, f1: 100.0, r: 0.7145255923086031
06/02/2019 09:40:32 step: 6663, epoch: 201, batch: 29, loss: 0.010897524654865265, acc: 100.0, f1: 100.0, r: 0.7664213805814164
06/02/2019 09:40:32 *** evaluating ***
06/02/2019 09:40:32 step: 202, epoch: 201, acc: 57.692307692307686, f1: 21.63101938336877, r: 0.23708020683208608
06/02/2019 09:40:32 *** epoch: 203 ***
06/02/2019 09:40:32 *** training ***
06/02/2019 09:40:33 step: 6671, epoch: 202, batch: 4, loss: 0.08287087082862854, acc: 96.875, f1: 92.48497702368329, r: 0.7205961738435385
06/02/2019 09:40:33 step: 6676, epoch: 202, batch: 9, loss: 0.052583299577236176, acc: 98.4375, f1: 96.82539682539684, r: 0.7220898803505871
06/02/2019 09:40:34 step: 6681, epoch: 202, batch: 14, loss: 0.05812925100326538, acc: 98.4375, f1: 97.78325123152709, r: 0.7276153176506718
06/02/2019 09:40:34 step: 6686, epoch: 202, batch: 19, loss: 0.12834815680980682, acc: 93.75, f1: 94.19921814879798, r: 0.7141313451440412
06/02/2019 09:40:35 step: 6691, epoch: 202, batch: 24, loss: 0.058738213032484055, acc: 96.875, f1: 83.32015810276681, r: 0.7183294449030787
06/02/2019 09:40:35 step: 6696, epoch: 202, batch: 29, loss: 0.007509291172027588, acc: 100.0, f1: 100.0, r: 0.7445154090789415
06/02/2019 09:40:35 *** evaluating ***
06/02/2019 09:40:36 step: 203, epoch: 202, acc: 55.98290598290598, f1: 20.95607839215279, r: 0.23714722052133785
06/02/2019 09:40:36 *** epoch: 204 ***
06/02/2019 09:40:36 *** training ***
06/02/2019 09:40:36 step: 6704, epoch: 203, batch: 4, loss: 0.03502017632126808, acc: 98.4375, f1: 98.53846153846153, r: 0.7733299670691349
06/02/2019 09:40:37 step: 6709, epoch: 203, batch: 9, loss: 0.036297403275966644, acc: 98.4375, f1: 99.24489795918367, r: 0.7477077605005981
06/02/2019 09:40:37 step: 6714, epoch: 203, batch: 14, loss: 0.015219278633594513, acc: 100.0, f1: 100.0, r: 0.7388936829522544
06/02/2019 09:40:38 step: 6719, epoch: 203, batch: 19, loss: 0.12680217623710632, acc: 93.75, f1: 93.90905542544887, r: 0.7656978959768587
06/02/2019 09:40:38 step: 6724, epoch: 203, batch: 24, loss: 0.015787526965141296, acc: 100.0, f1: 100.0, r: 0.7886727278586718
06/02/2019 09:40:39 step: 6729, epoch: 203, batch: 29, loss: 0.11216812580823898, acc: 96.875, f1: 96.58678955453149, r: 0.6870452285748131
06/02/2019 09:40:39 *** evaluating ***
06/02/2019 09:40:39 step: 204, epoch: 203, acc: 57.26495726495726, f1: 21.716697031400706, r: 0.2235137684454592
06/02/2019 09:40:39 *** epoch: 205 ***
06/02/2019 09:40:39 *** training ***
06/02/2019 09:40:40 step: 6737, epoch: 204, batch: 4, loss: 0.025309555232524872, acc: 98.4375, f1: 97.74891774891775, r: 0.6910940497265264
06/02/2019 09:40:40 step: 6742, epoch: 204, batch: 9, loss: 0.11087075620889664, acc: 95.3125, f1: 79.71049783549785, r: 0.7384234016837713
06/02/2019 09:40:41 step: 6747, epoch: 204, batch: 14, loss: 0.04476571083068848, acc: 98.4375, f1: 97.00680272108843, r: 0.6149001820927265
06/02/2019 09:40:42 step: 6752, epoch: 204, batch: 19, loss: 0.06946291774511337, acc: 96.875, f1: 92.11899211899211, r: 0.6634733856906091
06/02/2019 09:40:42 step: 6757, epoch: 204, batch: 24, loss: 0.023567790165543556, acc: 98.4375, f1: 99.13100724160631, r: 0.6570507620415555
06/02/2019 09:40:43 step: 6762, epoch: 204, batch: 29, loss: 0.019956260919570923, acc: 100.0, f1: 100.0, r: 0.7846221892778162
06/02/2019 09:40:43 *** evaluating ***
06/02/2019 09:40:43 step: 205, epoch: 204, acc: 58.97435897435898, f1: 23.94935510781778, r: 0.2421875435829373
06/02/2019 09:40:43 *** epoch: 206 ***
06/02/2019 09:40:43 *** training ***
06/02/2019 09:40:44 step: 6770, epoch: 205, batch: 4, loss: 0.058181747794151306, acc: 96.875, f1: 95.2108843537415, r: 0.6759464568218725
06/02/2019 09:40:44 step: 6775, epoch: 205, batch: 9, loss: 0.050587400794029236, acc: 98.4375, f1: 82.85714285714285, r: 0.7189053428060654
06/02/2019 09:40:45 step: 6780, epoch: 205, batch: 14, loss: 0.05909626558423042, acc: 96.875, f1: 89.39102564102564, r: 0.7179672081740753
06/02/2019 09:40:45 step: 6785, epoch: 205, batch: 19, loss: 0.12362314015626907, acc: 96.875, f1: 97.62546476080311, r: 0.7014365451966764
06/02/2019 09:40:46 step: 6790, epoch: 205, batch: 24, loss: 0.09476049989461899, acc: 96.875, f1: 95.041928721174, r: 0.7353377957371929
06/02/2019 09:40:46 step: 6795, epoch: 205, batch: 29, loss: 0.042136773467063904, acc: 98.4375, f1: 98.83367139959432, r: 0.7899993220852409
06/02/2019 09:40:46 *** evaluating ***
06/02/2019 09:40:47 step: 206, epoch: 205, acc: 57.26495726495726, f1: 21.658393086964516, r: 0.21228241879978044
06/02/2019 09:40:47 *** epoch: 207 ***
06/02/2019 09:40:47 *** training ***
06/02/2019 09:40:47 step: 6803, epoch: 206, batch: 4, loss: 0.21651330590248108, acc: 89.0625, f1: 78.87438021185486, r: 0.7755898720208675
06/02/2019 09:40:48 step: 6808, epoch: 206, batch: 9, loss: 0.04918915405869484, acc: 98.4375, f1: 98.0952380952381, r: 0.7124707054752588
06/02/2019 09:40:48 step: 6813, epoch: 206, batch: 14, loss: 0.010946713387966156, acc: 100.0, f1: 100.0, r: 0.802544520554958
06/02/2019 09:40:49 step: 6818, epoch: 206, batch: 19, loss: 0.08581621944904327, acc: 93.75, f1: 73.62037037037037, r: 0.728503958450834
06/02/2019 09:40:49 step: 6823, epoch: 206, batch: 24, loss: 0.0740162581205368, acc: 96.875, f1: 96.67820344512073, r: 0.6621332104515546
06/02/2019 09:40:50 step: 6828, epoch: 206, batch: 29, loss: 0.005825646221637726, acc: 100.0, f1: 100.0, r: 0.6683923927186388
06/02/2019 09:40:50 *** evaluating ***
06/02/2019 09:40:50 step: 207, epoch: 206, acc: 58.119658119658126, f1: 22.308889650125728, r: 0.20588598528795904
06/02/2019 09:40:50 *** epoch: 208 ***
06/02/2019 09:40:50 *** training ***
06/02/2019 09:40:51 step: 6836, epoch: 207, batch: 4, loss: 0.02496233582496643, acc: 100.0, f1: 100.0, r: 0.6806102374457812
06/02/2019 09:40:51 step: 6841, epoch: 207, batch: 9, loss: 0.017749261111021042, acc: 100.0, f1: 100.0, r: 0.6955497733718686
06/02/2019 09:40:52 step: 6846, epoch: 207, batch: 14, loss: 0.09013232588768005, acc: 95.3125, f1: 88.14506814506815, r: 0.7800862974452911
06/02/2019 09:40:52 step: 6851, epoch: 207, batch: 19, loss: 0.029822152107954025, acc: 98.4375, f1: 98.8994708994709, r: 0.7124933695497898
06/02/2019 09:40:53 step: 6856, epoch: 207, batch: 24, loss: 0.0458168126642704, acc: 98.4375, f1: 96.04395604395604, r: 0.6602471969748013
06/02/2019 09:40:54 step: 6861, epoch: 207, batch: 29, loss: 0.048037488013505936, acc: 98.4375, f1: 83.6734693877551, r: 0.7068837333207311
06/02/2019 09:40:54 *** evaluating ***
06/02/2019 09:40:54 step: 208, epoch: 207, acc: 56.837606837606835, f1: 21.158189771135415, r: 0.2324749144164285
06/02/2019 09:40:54 *** epoch: 209 ***
06/02/2019 09:40:54 *** training ***
06/02/2019 09:40:54 step: 6869, epoch: 208, batch: 4, loss: 0.059405237436294556, acc: 96.875, f1: 98.00683110214561, r: 0.6836311406209411
06/02/2019 09:40:55 step: 6874, epoch: 208, batch: 9, loss: 0.06187454238533974, acc: 96.875, f1: 91.81199752628324, r: 0.681361179897646
06/02/2019 09:40:55 step: 6879, epoch: 208, batch: 14, loss: 0.0817706435918808, acc: 95.3125, f1: 82.39898989898991, r: 0.7142144386706841
06/02/2019 09:40:56 step: 6884, epoch: 208, batch: 19, loss: 0.034811608493328094, acc: 98.4375, f1: 98.6670530281078, r: 0.7390035906472168
06/02/2019 09:40:56 step: 6889, epoch: 208, batch: 24, loss: 0.14429618418216705, acc: 92.1875, f1: 75.89728631924639, r: 0.7292661347934787
06/02/2019 09:40:57 step: 6894, epoch: 208, batch: 29, loss: 0.0571473054587841, acc: 96.875, f1: 90.70512820512819, r: 0.8048670894083478
06/02/2019 09:40:57 *** evaluating ***
06/02/2019 09:40:58 step: 209, epoch: 208, acc: 58.119658119658126, f1: 22.102251753414542, r: 0.2195497329309544
06/02/2019 09:40:58 *** epoch: 210 ***
06/02/2019 09:40:58 *** training ***
06/02/2019 09:40:58 step: 6902, epoch: 209, batch: 4, loss: 0.09036783874034882, acc: 93.75, f1: 92.74056234960109, r: 0.7815155302072253
06/02/2019 09:40:58 step: 6907, epoch: 209, batch: 9, loss: 0.03547368571162224, acc: 98.4375, f1: 98.20868786386028, r: 0.6163972949120865
06/02/2019 09:40:59 step: 6912, epoch: 209, batch: 14, loss: 0.11774399876594543, acc: 98.4375, f1: 95.28985507246377, r: 0.8092387218202932
06/02/2019 09:41:00 step: 6917, epoch: 209, batch: 19, loss: 0.018700480461120605, acc: 100.0, f1: 100.0, r: 0.7938103459762724
06/02/2019 09:41:00 step: 6922, epoch: 209, batch: 24, loss: 0.03997644782066345, acc: 98.4375, f1: 94.13919413919415, r: 0.734587317898248
06/02/2019 09:41:01 step: 6927, epoch: 209, batch: 29, loss: 0.021768003702163696, acc: 98.4375, f1: 98.06763285024155, r: 0.7892061535845195
06/02/2019 09:41:01 *** evaluating ***
06/02/2019 09:41:01 step: 210, epoch: 209, acc: 57.26495726495726, f1: 21.91643323996265, r: 0.23790600491300115
06/02/2019 09:41:01 *** epoch: 211 ***
06/02/2019 09:41:01 *** training ***
06/02/2019 09:41:02 step: 6935, epoch: 210, batch: 4, loss: 0.24362586438655853, acc: 92.1875, f1: 74.2558662809386, r: 0.6172704373091393
06/02/2019 09:41:02 step: 6940, epoch: 210, batch: 9, loss: 0.07580970227718353, acc: 96.875, f1: 95.73412698412697, r: 0.7803550077891424
06/02/2019 09:41:03 step: 6945, epoch: 210, batch: 14, loss: 0.08677026629447937, acc: 96.875, f1: 96.74603174603175, r: 0.6444381980910179
06/02/2019 09:41:03 step: 6950, epoch: 210, batch: 19, loss: 0.02789776772260666, acc: 100.0, f1: 100.0, r: 0.6172524797731752
06/02/2019 09:41:04 step: 6955, epoch: 210, batch: 24, loss: 0.1024872362613678, acc: 96.875, f1: 80.83333333333333, r: 0.7835990809996518
06/02/2019 09:41:05 step: 6960, epoch: 210, batch: 29, loss: 0.03897581994533539, acc: 98.4375, f1: 86.36363636363636, r: 0.7709565607928148
06/02/2019 09:41:05 *** evaluating ***
06/02/2019 09:41:05 step: 211, epoch: 210, acc: 58.97435897435898, f1: 21.81105541631857, r: 0.22865458624812957
06/02/2019 09:41:05 *** epoch: 212 ***
06/02/2019 09:41:05 *** training ***
06/02/2019 09:41:05 step: 6968, epoch: 211, batch: 4, loss: 0.05775280296802521, acc: 98.4375, f1: 97.90209790209789, r: 0.8225943346007843
06/02/2019 09:41:06 step: 6973, epoch: 211, batch: 9, loss: 0.029247023165225983, acc: 100.0, f1: 100.0, r: 0.7102978397084182
06/02/2019 09:41:06 step: 6978, epoch: 211, batch: 14, loss: 0.0863000676035881, acc: 96.875, f1: 90.35549703752469, r: 0.6920698872422835
06/02/2019 09:41:07 step: 6983, epoch: 211, batch: 19, loss: 0.08612778037786484, acc: 93.75, f1: 87.85434995112415, r: 0.704979895895904
06/02/2019 09:41:07 step: 6988, epoch: 211, batch: 24, loss: 0.018749050796031952, acc: 100.0, f1: 100.0, r: 0.667981842863772
06/02/2019 09:41:08 step: 6993, epoch: 211, batch: 29, loss: 0.04478183388710022, acc: 98.4375, f1: 98.9254718280755, r: 0.7136086225080978
06/02/2019 09:41:08 *** evaluating ***
06/02/2019 09:41:08 step: 212, epoch: 211, acc: 59.82905982905983, f1: 21.60328803085625, r: 0.22472630156330534
06/02/2019 09:41:08 *** epoch: 213 ***
06/02/2019 09:41:08 *** training ***
06/02/2019 09:41:09 step: 7001, epoch: 212, batch: 4, loss: 0.008830606937408447, acc: 100.0, f1: 100.0, r: 0.6654922236102638
06/02/2019 09:41:09 step: 7006, epoch: 212, batch: 9, loss: 0.02875928208231926, acc: 98.4375, f1: 96.3718820861678, r: 0.7128941910591096
06/02/2019 09:41:10 step: 7011, epoch: 212, batch: 14, loss: 0.17361213266849518, acc: 93.75, f1: 78.29022988505747, r: 0.7178431015313754
06/02/2019 09:41:10 step: 7016, epoch: 212, batch: 19, loss: 0.10976635664701462, acc: 95.3125, f1: 82.15078182291296, r: 0.7476887607508439
06/02/2019 09:41:11 step: 7021, epoch: 212, batch: 24, loss: 0.0793149322271347, acc: 96.875, f1: 85.9375, r: 0.8177132262034265
06/02/2019 09:41:11 step: 7026, epoch: 212, batch: 29, loss: 0.01617760956287384, acc: 100.0, f1: 100.0, r: 0.7271361972847507
06/02/2019 09:41:12 *** evaluating ***
06/02/2019 09:41:12 step: 213, epoch: 212, acc: 58.97435897435898, f1: 21.763090836366697, r: 0.2301323643549933
06/02/2019 09:41:12 *** epoch: 214 ***
06/02/2019 09:41:12 *** training ***
06/02/2019 09:41:13 step: 7034, epoch: 213, batch: 4, loss: 0.0869208425283432, acc: 96.875, f1: 79.31972789115648, r: 0.645039991948688
06/02/2019 09:41:13 step: 7039, epoch: 213, batch: 9, loss: 0.03307846933603287, acc: 98.4375, f1: 98.77250409165302, r: 0.8023806375428035
06/02/2019 09:41:14 step: 7044, epoch: 213, batch: 14, loss: 0.038245655596256256, acc: 98.4375, f1: 97.40259740259741, r: 0.6934795619880816
06/02/2019 09:41:14 step: 7049, epoch: 213, batch: 19, loss: 0.05458855628967285, acc: 96.875, f1: 84.7093023255814, r: 0.8248026297749164
06/02/2019 09:41:15 step: 7054, epoch: 213, batch: 24, loss: 0.016047239303588867, acc: 100.0, f1: 100.0, r: 0.8326836605999859
06/02/2019 09:41:15 step: 7059, epoch: 213, batch: 29, loss: 0.12498098611831665, acc: 98.4375, f1: 97.73242630385488, r: 0.6966570041724605
06/02/2019 09:41:15 *** evaluating ***
06/02/2019 09:41:16 step: 214, epoch: 213, acc: 58.97435897435898, f1: 21.21837421837422, r: 0.2260561884880759
06/02/2019 09:41:16 *** epoch: 215 ***
06/02/2019 09:41:16 *** training ***
06/02/2019 09:41:16 step: 7067, epoch: 214, batch: 4, loss: 0.013353068381547928, acc: 100.0, f1: 100.0, r: 0.640421125061553
06/02/2019 09:41:17 step: 7072, epoch: 214, batch: 9, loss: 0.06098261475563049, acc: 96.875, f1: 96.1717144643974, r: 0.6493839707555992
06/02/2019 09:41:17 step: 7077, epoch: 214, batch: 14, loss: 0.04398251324892044, acc: 98.4375, f1: 98.93939393939394, r: 0.8303257499298893
06/02/2019 09:41:18 step: 7082, epoch: 214, batch: 19, loss: 0.013890769332647324, acc: 100.0, f1: 100.0, r: 0.8009727281272507
06/02/2019 09:41:18 step: 7087, epoch: 214, batch: 24, loss: 0.018246926367282867, acc: 100.0, f1: 100.0, r: 0.7061441651732209
06/02/2019 09:41:19 step: 7092, epoch: 214, batch: 29, loss: 0.021020717918872833, acc: 98.4375, f1: 98.42118665648077, r: 0.6558165011319009
06/02/2019 09:41:19 *** evaluating ***
06/02/2019 09:41:19 step: 215, epoch: 214, acc: 58.97435897435898, f1: 21.794253044253047, r: 0.22861745188399885
06/02/2019 09:41:19 *** epoch: 216 ***
06/02/2019 09:41:19 *** training ***
06/02/2019 09:41:20 step: 7100, epoch: 215, batch: 4, loss: 0.018316231667995453, acc: 100.0, f1: 100.0, r: 0.7234450143465248
06/02/2019 09:41:20 step: 7105, epoch: 215, batch: 9, loss: 0.038267333060503006, acc: 98.4375, f1: 99.23404255319149, r: 0.7647694980598774
06/02/2019 09:41:21 step: 7110, epoch: 215, batch: 14, loss: 0.06164829432964325, acc: 96.875, f1: 97.48519199346406, r: 0.7526871132986146
06/02/2019 09:41:21 step: 7115, epoch: 215, batch: 19, loss: 0.05383485555648804, acc: 98.4375, f1: 86.36363636363636, r: 0.7792478050668091
06/02/2019 09:41:22 step: 7120, epoch: 215, batch: 24, loss: 0.054837003350257874, acc: 98.4375, f1: 98.1111111111111, r: 0.7899947154684893
06/02/2019 09:41:22 step: 7125, epoch: 215, batch: 29, loss: 0.043230269104242325, acc: 98.4375, f1: 98.51851851851852, r: 0.7073010578526651
06/02/2019 09:41:22 *** evaluating ***
06/02/2019 09:41:23 step: 216, epoch: 215, acc: 58.119658119658126, f1: 22.25698386903015, r: 0.22662613891349018
06/02/2019 09:41:23 *** epoch: 217 ***
06/02/2019 09:41:23 *** training ***
06/02/2019 09:41:23 step: 7133, epoch: 216, batch: 4, loss: 0.08389214426279068, acc: 95.3125, f1: 81.64688759516345, r: 0.7210369499249861
06/02/2019 09:41:24 step: 7138, epoch: 216, batch: 9, loss: 0.046641454100608826, acc: 98.4375, f1: 95.0, r: 0.748603569579464
06/02/2019 09:41:24 step: 7143, epoch: 216, batch: 14, loss: 0.027082404121756554, acc: 100.0, f1: 100.0, r: 0.7389234762981797
06/02/2019 09:41:25 step: 7148, epoch: 216, batch: 19, loss: 0.060288816690444946, acc: 96.875, f1: 95.35201640464798, r: 0.7406685667965053
06/02/2019 09:41:25 step: 7153, epoch: 216, batch: 24, loss: 0.021032966673374176, acc: 100.0, f1: 100.0, r: 0.7694655682697902
06/02/2019 09:41:26 step: 7158, epoch: 216, batch: 29, loss: 0.039068952202796936, acc: 96.875, f1: 95.16250944822373, r: 0.6580249797070394
06/02/2019 09:41:26 *** evaluating ***
06/02/2019 09:41:27 step: 217, epoch: 216, acc: 57.692307692307686, f1: 21.77259092613423, r: 0.23235170353431245
06/02/2019 09:41:27 *** epoch: 218 ***
06/02/2019 09:41:27 *** training ***
06/02/2019 09:41:27 step: 7166, epoch: 217, batch: 4, loss: 0.04970068857073784, acc: 98.4375, f1: 94.28571428571429, r: 0.5412204371500137
06/02/2019 09:41:28 step: 7171, epoch: 217, batch: 9, loss: 0.049950242042541504, acc: 98.4375, f1: 98.86128364389234, r: 0.7931370220009399
06/02/2019 09:41:28 step: 7176, epoch: 217, batch: 14, loss: 0.0378243550658226, acc: 98.4375, f1: 98.12987012987013, r: 0.7357127597191415
06/02/2019 09:41:29 step: 7181, epoch: 217, batch: 19, loss: 0.007946476340293884, acc: 100.0, f1: 100.0, r: 0.7373318341210113
06/02/2019 09:41:29 step: 7186, epoch: 217, batch: 24, loss: 0.029477551579475403, acc: 98.4375, f1: 97.26415094339622, r: 0.824147944090943
06/02/2019 09:41:30 step: 7191, epoch: 217, batch: 29, loss: 0.04895676672458649, acc: 98.4375, f1: 97.25274725274726, r: 0.8166496977441499
06/02/2019 09:41:30 *** evaluating ***
06/02/2019 09:41:31 step: 218, epoch: 217, acc: 58.119658119658126, f1: 21.974789915966387, r: 0.23392297160701875
06/02/2019 09:41:31 *** epoch: 219 ***
06/02/2019 09:41:31 *** training ***
06/02/2019 09:41:31 step: 7199, epoch: 218, batch: 4, loss: 0.10573117434978485, acc: 96.875, f1: 92.1441303794245, r: 0.6437683899483051
06/02/2019 09:41:32 step: 7204, epoch: 218, batch: 9, loss: 0.0030259937047958374, acc: 100.0, f1: 100.0, r: 0.8142532104315238
06/02/2019 09:41:32 step: 7209, epoch: 218, batch: 14, loss: 0.025441855192184448, acc: 100.0, f1: 100.0, r: 0.5743108433490759
06/02/2019 09:41:33 step: 7214, epoch: 218, batch: 19, loss: 0.06575477123260498, acc: 96.875, f1: 95.14957264957266, r: 0.7561002161780016
06/02/2019 09:41:33 step: 7219, epoch: 218, batch: 24, loss: 0.060794904828071594, acc: 98.4375, f1: 96.4625850340136, r: 0.6866501353661499
06/02/2019 09:41:34 step: 7224, epoch: 218, batch: 29, loss: 0.014470726251602173, acc: 100.0, f1: 100.0, r: 0.6748857730964951
06/02/2019 09:41:34 *** evaluating ***
06/02/2019 09:41:34 step: 219, epoch: 218, acc: 58.97435897435898, f1: 23.06055912219842, r: 0.24842033154405416
06/02/2019 09:41:34 *** epoch: 220 ***
06/02/2019 09:41:34 *** training ***
06/02/2019 09:41:35 step: 7232, epoch: 219, batch: 4, loss: 0.04263506457209587, acc: 100.0, f1: 100.0, r: 0.7588407435405949
06/02/2019 09:41:35 step: 7237, epoch: 219, batch: 9, loss: 0.012243948876857758, acc: 100.0, f1: 100.0, r: 0.6510118518897197
06/02/2019 09:41:36 step: 7242, epoch: 219, batch: 14, loss: 0.03129487857222557, acc: 98.4375, f1: 85.09316770186335, r: 0.6782882448488317
06/02/2019 09:41:36 step: 7247, epoch: 219, batch: 19, loss: 0.003506496548652649, acc: 100.0, f1: 100.0, r: 0.782947421514577
06/02/2019 09:41:37 step: 7252, epoch: 219, batch: 24, loss: 0.030957676470279694, acc: 98.4375, f1: 99.16883116883116, r: 0.6185993219844984
06/02/2019 09:41:37 step: 7257, epoch: 219, batch: 29, loss: 0.10271234810352325, acc: 95.3125, f1: 89.6969696969697, r: 0.7154119499411786
06/02/2019 09:41:38 *** evaluating ***
06/02/2019 09:41:38 step: 220, epoch: 219, acc: 58.97435897435898, f1: 23.361663453795888, r: 0.2465953888026824
06/02/2019 09:41:38 *** epoch: 221 ***
06/02/2019 09:41:38 *** training ***
06/02/2019 09:41:39 step: 7265, epoch: 220, batch: 4, loss: 0.04079767316579819, acc: 98.4375, f1: 85.0, r: 0.7767771023737446
06/02/2019 09:41:39 step: 7270, epoch: 220, batch: 9, loss: 0.07220721244812012, acc: 96.875, f1: 91.2183055040198, r: 0.6597131282952613
06/02/2019 09:41:40 step: 7275, epoch: 220, batch: 14, loss: 0.08719116449356079, acc: 96.875, f1: 96.70129870129871, r: 0.682289447106993
06/02/2019 09:41:40 step: 7280, epoch: 220, batch: 19, loss: 0.06692703068256378, acc: 96.875, f1: 96.97959183673468, r: 0.6946661166425013
06/02/2019 09:41:41 step: 7285, epoch: 220, batch: 24, loss: 0.019093431532382965, acc: 100.0, f1: 100.0, r: 0.681778500279901
06/02/2019 09:41:41 step: 7290, epoch: 220, batch: 29, loss: 0.03211654722690582, acc: 100.0, f1: 100.0, r: 0.8169337079801134
06/02/2019 09:41:42 *** evaluating ***
06/02/2019 09:41:42 step: 221, epoch: 220, acc: 58.97435897435898, f1: 22.429567981038566, r: 0.23829054398769878
06/02/2019 09:41:42 *** epoch: 222 ***
06/02/2019 09:41:42 *** training ***
06/02/2019 09:41:42 step: 7298, epoch: 221, batch: 4, loss: 0.0952635407447815, acc: 96.875, f1: 83.33333333333333, r: 0.6414243268261335
06/02/2019 09:41:43 step: 7303, epoch: 221, batch: 9, loss: 0.08707796037197113, acc: 95.3125, f1: 90.7936507936508, r: 0.6808797816175737
06/02/2019 09:41:43 step: 7308, epoch: 221, batch: 14, loss: 0.013270311057567596, acc: 100.0, f1: 100.0, r: 0.6785537954406123
06/02/2019 09:41:44 step: 7313, epoch: 221, batch: 19, loss: 0.006385989487171173, acc: 100.0, f1: 100.0, r: 0.7049073697606085
06/02/2019 09:41:44 step: 7318, epoch: 221, batch: 24, loss: 0.04363696277141571, acc: 98.4375, f1: 99.02548725637182, r: 0.8623244137223387
06/02/2019 09:41:45 step: 7323, epoch: 221, batch: 29, loss: 0.013923630118370056, acc: 100.0, f1: 100.0, r: 0.7448114525277445
06/02/2019 09:41:45 *** evaluating ***
06/02/2019 09:41:46 step: 222, epoch: 221, acc: 58.119658119658126, f1: 22.241425392972726, r: 0.22052272189700703
06/02/2019 09:41:46 *** epoch: 223 ***
06/02/2019 09:41:46 *** training ***
06/02/2019 09:41:46 step: 7331, epoch: 222, batch: 4, loss: 0.0682903528213501, acc: 96.875, f1: 96.56045751633987, r: 0.8022865459383295
06/02/2019 09:41:47 step: 7336, epoch: 222, batch: 9, loss: 0.066028892993927, acc: 98.4375, f1: 85.0, r: 0.749232403604105
06/02/2019 09:41:47 step: 7341, epoch: 222, batch: 14, loss: 0.042993634939193726, acc: 98.4375, f1: 97.86096256684492, r: 0.6955304639435864
06/02/2019 09:41:48 step: 7346, epoch: 222, batch: 19, loss: 0.09257689118385315, acc: 95.3125, f1: 87.84159596853095, r: 0.7132933947127065
06/02/2019 09:41:48 step: 7351, epoch: 222, batch: 24, loss: 0.03926391899585724, acc: 98.4375, f1: 98.4006734006734, r: 0.7349750681319629
06/02/2019 09:41:49 step: 7356, epoch: 222, batch: 29, loss: 0.012923579663038254, acc: 100.0, f1: 100.0, r: 0.714069080834547
06/02/2019 09:41:49 *** evaluating ***
06/02/2019 09:41:49 step: 223, epoch: 222, acc: 58.97435897435898, f1: 22.790656155026763, r: 0.2419028791234907
06/02/2019 09:41:49 *** epoch: 224 ***
06/02/2019 09:41:49 *** training ***
06/02/2019 09:41:50 step: 7364, epoch: 223, batch: 4, loss: 0.07866676896810532, acc: 96.875, f1: 93.3068783068783, r: 0.8113567088233293
06/02/2019 09:41:50 step: 7369, epoch: 223, batch: 9, loss: 0.04110369086265564, acc: 98.4375, f1: 98.22082679225537, r: 0.6819647063513181
06/02/2019 09:41:51 step: 7374, epoch: 223, batch: 14, loss: 0.09916555136442184, acc: 96.875, f1: 96.4737274220033, r: 0.7467048058176339
06/02/2019 09:41:51 step: 7379, epoch: 223, batch: 19, loss: 0.02451077476143837, acc: 100.0, f1: 100.0, r: 0.7606328174897872
06/02/2019 09:41:52 step: 7384, epoch: 223, batch: 24, loss: 0.05233857408165932, acc: 98.4375, f1: 86.66666666666667, r: 0.6500132110884335
06/02/2019 09:41:52 step: 7389, epoch: 223, batch: 29, loss: 0.08852001279592514, acc: 96.875, f1: 95.65476190476191, r: 0.7997846808222243
06/02/2019 09:41:53 *** evaluating ***
06/02/2019 09:41:53 step: 224, epoch: 223, acc: 58.54700854700855, f1: 22.212228962516946, r: 0.22895790883069023
06/02/2019 09:41:53 *** epoch: 225 ***
06/02/2019 09:41:53 *** training ***
06/02/2019 09:41:53 step: 7397, epoch: 224, batch: 4, loss: 0.12120170146226883, acc: 95.3125, f1: 93.79805512875545, r: 0.6968296272266118
06/02/2019 09:41:54 step: 7402, epoch: 224, batch: 9, loss: 0.06694367527961731, acc: 96.875, f1: 86.13692089301846, r: 0.676911896524033
06/02/2019 09:41:54 step: 7407, epoch: 224, batch: 14, loss: 0.012826729565858841, acc: 100.0, f1: 100.0, r: 0.7587590308992214
06/02/2019 09:41:55 step: 7412, epoch: 224, batch: 19, loss: 0.04886516183614731, acc: 98.4375, f1: 94.74548440065682, r: 0.647250708808906
06/02/2019 09:41:56 step: 7417, epoch: 224, batch: 24, loss: 0.04082835465669632, acc: 98.4375, f1: 96.52173913043478, r: 0.7532747155721622
06/02/2019 09:41:56 step: 7422, epoch: 224, batch: 29, loss: 0.043180741369724274, acc: 96.875, f1: 95.46703296703296, r: 0.7705384403470906
06/02/2019 09:41:56 *** evaluating ***
06/02/2019 09:41:57 step: 225, epoch: 224, acc: 58.54700854700855, f1: 21.982793841718525, r: 0.24922382032726184
06/02/2019 09:41:57 *** epoch: 226 ***
06/02/2019 09:41:57 *** training ***
06/02/2019 09:41:57 step: 7430, epoch: 225, batch: 4, loss: 0.009557090699672699, acc: 100.0, f1: 100.0, r: 0.7144372665404007
06/02/2019 09:41:58 step: 7435, epoch: 225, batch: 9, loss: 0.07635641098022461, acc: 96.875, f1: 96.17292200989777, r: 0.8135809743904837
06/02/2019 09:41:58 step: 7440, epoch: 225, batch: 14, loss: 0.047285519540309906, acc: 96.875, f1: 96.45259112472226, r: 0.7075035187269567
06/02/2019 09:41:59 step: 7445, epoch: 225, batch: 19, loss: 0.017628610134124756, acc: 100.0, f1: 100.0, r: 0.7118385780334735
06/02/2019 09:42:00 step: 7450, epoch: 225, batch: 24, loss: 0.05576103925704956, acc: 96.875, f1: 96.9441577800711, r: 0.6142347883959108
06/02/2019 09:42:00 step: 7455, epoch: 225, batch: 29, loss: 0.008420057594776154, acc: 100.0, f1: 100.0, r: 0.6807714351048525
06/02/2019 09:42:00 *** evaluating ***
06/02/2019 09:42:01 step: 226, epoch: 225, acc: 56.837606837606835, f1: 21.51162790697674, r: 0.20235179482880056
06/02/2019 09:42:01 *** epoch: 227 ***
06/02/2019 09:42:01 *** training ***
06/02/2019 09:42:01 step: 7463, epoch: 226, batch: 4, loss: 0.036594390869140625, acc: 98.4375, f1: 96.8944099378882, r: 0.6071539013556464
06/02/2019 09:42:01 step: 7468, epoch: 226, batch: 9, loss: 0.015479102730751038, acc: 100.0, f1: 100.0, r: 0.7192881129475468
06/02/2019 09:42:02 step: 7473, epoch: 226, batch: 14, loss: 0.03875379264354706, acc: 96.875, f1: 98.31691297208539, r: 0.6967544409020171
06/02/2019 09:42:03 step: 7478, epoch: 226, batch: 19, loss: 0.021939557045698166, acc: 100.0, f1: 100.0, r: 0.5684020738439104
06/02/2019 09:42:03 step: 7483, epoch: 226, batch: 24, loss: 0.02590189129114151, acc: 100.0, f1: 100.0, r: 0.6534252300451249
06/02/2019 09:42:04 step: 7488, epoch: 226, batch: 29, loss: 0.030696382746100426, acc: 98.4375, f1: 96.82539682539682, r: 0.8064332744589104
06/02/2019 09:42:04 *** evaluating ***
06/02/2019 09:42:04 step: 227, epoch: 226, acc: 57.692307692307686, f1: 21.826205807801852, r: 0.22746987694505746
06/02/2019 09:42:04 *** epoch: 228 ***
06/02/2019 09:42:04 *** training ***
06/02/2019 09:42:05 step: 7496, epoch: 227, batch: 4, loss: 0.032924771308898926, acc: 98.4375, f1: 98.24046920821115, r: 0.6959056344803117
06/02/2019 09:42:05 step: 7501, epoch: 227, batch: 9, loss: 0.027169346809387207, acc: 98.4375, f1: 98.49498327759196, r: 0.7521088219998016
06/02/2019 09:42:06 step: 7506, epoch: 227, batch: 14, loss: 0.0670204684138298, acc: 96.875, f1: 96.25992063492063, r: 0.8069506769731155
06/02/2019 09:42:06 step: 7511, epoch: 227, batch: 19, loss: 0.042261168360710144, acc: 100.0, f1: 100.0, r: 0.7566961532563539
06/02/2019 09:42:07 step: 7516, epoch: 227, batch: 24, loss: 0.020343735814094543, acc: 100.0, f1: 100.0, r: 0.7879582729834796
06/02/2019 09:42:08 step: 7521, epoch: 227, batch: 29, loss: 0.02064008265733719, acc: 100.0, f1: 100.0, r: 0.7421824321946974
06/02/2019 09:42:08 *** evaluating ***
06/02/2019 09:42:08 step: 228, epoch: 227, acc: 57.26495726495726, f1: 21.568089752515984, r: 0.212620097627728
06/02/2019 09:42:08 *** epoch: 229 ***
06/02/2019 09:42:08 *** training ***
06/02/2019 09:42:09 step: 7529, epoch: 228, batch: 4, loss: 0.02137932926416397, acc: 100.0, f1: 100.0, r: 0.6959788964997469
06/02/2019 09:42:09 step: 7534, epoch: 228, batch: 9, loss: 0.007848847657442093, acc: 100.0, f1: 100.0, r: 0.767004285400123
06/02/2019 09:42:10 step: 7539, epoch: 228, batch: 14, loss: 0.016087878495454788, acc: 100.0, f1: 100.0, r: 0.6685357510839777
06/02/2019 09:42:10 step: 7544, epoch: 228, batch: 19, loss: 0.1415451467037201, acc: 95.3125, f1: 94.09532044042915, r: 0.7956251211968804
06/02/2019 09:42:11 step: 7549, epoch: 228, batch: 24, loss: 0.05771223083138466, acc: 96.875, f1: 94.66666666666667, r: 0.6723813283888438
06/02/2019 09:42:12 step: 7554, epoch: 228, batch: 29, loss: 0.03310675173997879, acc: 100.0, f1: 100.0, r: 0.8082554136067819
06/02/2019 09:42:12 *** evaluating ***
06/02/2019 09:42:12 step: 229, epoch: 228, acc: 57.692307692307686, f1: 22.02365951420891, r: 0.2142463695256287
06/02/2019 09:42:12 *** epoch: 230 ***
06/02/2019 09:42:12 *** training ***
06/02/2019 09:42:13 step: 7562, epoch: 229, batch: 4, loss: 0.10419730097055435, acc: 96.875, f1: 98.48901098901099, r: 0.744721191415254
06/02/2019 09:42:13 step: 7567, epoch: 229, batch: 9, loss: 0.018786095082759857, acc: 100.0, f1: 100.0, r: 0.6489038813801058
06/02/2019 09:42:14 step: 7572, epoch: 229, batch: 14, loss: 0.03734619542956352, acc: 98.4375, f1: 97.11399711399712, r: 0.6905383475259343
06/02/2019 09:42:14 step: 7577, epoch: 229, batch: 19, loss: 0.016441691666841507, acc: 100.0, f1: 100.0, r: 0.8035797980670356
06/02/2019 09:42:15 step: 7582, epoch: 229, batch: 24, loss: 0.05482972040772438, acc: 98.4375, f1: 98.18007662835248, r: 0.7767893659709844
06/02/2019 09:42:15 step: 7587, epoch: 229, batch: 29, loss: 0.02250342071056366, acc: 98.4375, f1: 97.95186891961086, r: 0.7145125136080801
06/02/2019 09:42:16 *** evaluating ***
06/02/2019 09:42:16 step: 230, epoch: 229, acc: 57.692307692307686, f1: 21.283558571149065, r: 0.21463682167284978
06/02/2019 09:42:16 *** epoch: 231 ***
06/02/2019 09:42:16 *** training ***
06/02/2019 09:42:16 step: 7595, epoch: 230, batch: 4, loss: 0.00464649498462677, acc: 100.0, f1: 100.0, r: 0.7432879026499192
06/02/2019 09:42:17 step: 7600, epoch: 230, batch: 9, loss: 0.03298313543200493, acc: 100.0, f1: 100.0, r: 0.8517860742163167
06/02/2019 09:42:17 step: 7605, epoch: 230, batch: 14, loss: 0.00536298006772995, acc: 100.0, f1: 100.0, r: 0.8142746574702312
06/02/2019 09:42:18 step: 7610, epoch: 230, batch: 19, loss: 0.05361473187804222, acc: 98.4375, f1: 96.3718820861678, r: 0.6847816148300241
06/02/2019 09:42:18 step: 7615, epoch: 230, batch: 24, loss: 0.016803227365016937, acc: 100.0, f1: 100.0, r: 0.7005898403670883
06/02/2019 09:42:19 step: 7620, epoch: 230, batch: 29, loss: 0.10957756638526917, acc: 95.3125, f1: 92.63440860215056, r: 0.672466566849464
06/02/2019 09:42:19 *** evaluating ***
06/02/2019 09:42:20 step: 231, epoch: 230, acc: 58.97435897435898, f1: 24.170494411356774, r: 0.23436065763006458
06/02/2019 09:42:20 *** epoch: 232 ***
06/02/2019 09:42:20 *** training ***
06/02/2019 09:42:20 step: 7628, epoch: 231, batch: 4, loss: 0.043295592069625854, acc: 98.4375, f1: 83.33333333333333, r: 0.7570098249697941
06/02/2019 09:42:21 step: 7633, epoch: 231, batch: 9, loss: 0.06637290865182877, acc: 96.875, f1: 96.71303414983015, r: 0.7698143728058326
06/02/2019 09:42:21 step: 7638, epoch: 231, batch: 14, loss: 0.060593243688344955, acc: 96.875, f1: 95.4020979020979, r: 0.7469062669655708
06/02/2019 09:42:22 step: 7643, epoch: 231, batch: 19, loss: 0.029133331030607224, acc: 98.4375, f1: 96.19047619047619, r: 0.7142870311061577
06/02/2019 09:42:22 step: 7648, epoch: 231, batch: 24, loss: 0.03906171768903732, acc: 98.4375, f1: 95.71428571428572, r: 0.8231737602936925
06/02/2019 09:42:23 step: 7653, epoch: 231, batch: 29, loss: 0.039496053010225296, acc: 98.4375, f1: 86.66666666666667, r: 0.6532947997598236
06/02/2019 09:42:23 *** evaluating ***
06/02/2019 09:42:23 step: 232, epoch: 231, acc: 57.692307692307686, f1: 21.94553012851982, r: 0.22960310758184246
06/02/2019 09:42:23 *** epoch: 233 ***
06/02/2019 09:42:23 *** training ***
06/02/2019 09:42:24 step: 7661, epoch: 232, batch: 4, loss: 0.008691541850566864, acc: 100.0, f1: 100.0, r: 0.789676917211604
06/02/2019 09:42:24 step: 7666, epoch: 232, batch: 9, loss: 0.04823783412575722, acc: 96.875, f1: 97.89740050609616, r: 0.7111869002738819
06/02/2019 09:42:25 step: 7671, epoch: 232, batch: 14, loss: 0.00759616494178772, acc: 100.0, f1: 100.0, r: 0.7184264560548616
06/02/2019 09:42:25 step: 7676, epoch: 232, batch: 19, loss: 0.028479158878326416, acc: 98.4375, f1: 98.01587301587301, r: 0.829474705037348
06/02/2019 09:42:26 step: 7681, epoch: 232, batch: 24, loss: 0.10443751513957977, acc: 95.3125, f1: 91.29669411624299, r: 0.6658617325406382
06/02/2019 09:42:27 step: 7686, epoch: 232, batch: 29, loss: 0.012478373944759369, acc: 100.0, f1: 100.0, r: 0.7736018533728174
06/02/2019 09:42:27 *** evaluating ***
06/02/2019 09:42:27 step: 233, epoch: 232, acc: 59.401709401709404, f1: 25.53105124255886, r: 0.2445581317219427
06/02/2019 09:42:27 *** epoch: 234 ***
06/02/2019 09:42:27 *** training ***
06/02/2019 09:42:28 step: 7694, epoch: 233, batch: 4, loss: 0.018383711576461792, acc: 100.0, f1: 100.0, r: 0.7062026103810775
06/02/2019 09:42:28 step: 7699, epoch: 233, batch: 9, loss: 0.11448869109153748, acc: 96.875, f1: 82.58928571428572, r: 0.7846922827496476
06/02/2019 09:42:29 step: 7704, epoch: 233, batch: 14, loss: 0.040563128888607025, acc: 98.4375, f1: 97.85714285714286, r: 0.7842854593278423
06/02/2019 09:42:29 step: 7709, epoch: 233, batch: 19, loss: 0.02849065512418747, acc: 96.875, f1: 95.57823129251702, r: 0.7716412408518551
06/02/2019 09:42:30 step: 7714, epoch: 233, batch: 24, loss: 0.02792222797870636, acc: 100.0, f1: 100.0, r: 0.7258519721767345
06/02/2019 09:42:31 step: 7719, epoch: 233, batch: 29, loss: 0.08325269818305969, acc: 96.875, f1: 93.23818333994097, r: 0.7968182160176472
06/02/2019 09:42:31 *** evaluating ***
06/02/2019 09:42:31 step: 234, epoch: 233, acc: 58.97435897435898, f1: 22.274679836702738, r: 0.24112476112477815
06/02/2019 09:42:31 *** epoch: 235 ***
06/02/2019 09:42:31 *** training ***
06/02/2019 09:42:31 step: 7727, epoch: 234, batch: 4, loss: 0.03900320827960968, acc: 98.4375, f1: 97.55639097744361, r: 0.809987038467903
06/02/2019 09:42:32 step: 7732, epoch: 234, batch: 9, loss: 0.05057671666145325, acc: 98.4375, f1: 95.55555555555556, r: 0.7117494908835906
06/02/2019 09:42:32 step: 7737, epoch: 234, batch: 14, loss: 0.09668959677219391, acc: 95.3125, f1: 89.85347985347985, r: 0.8481912673072921
06/02/2019 09:42:33 step: 7742, epoch: 234, batch: 19, loss: 0.10749092698097229, acc: 96.875, f1: 95.85766479383501, r: 0.6439982661225137
06/02/2019 09:42:34 step: 7747, epoch: 234, batch: 24, loss: 0.03704889118671417, acc: 98.4375, f1: 85.0, r: 0.7198501984600921
06/02/2019 09:42:34 step: 7752, epoch: 234, batch: 29, loss: 0.09363934397697449, acc: 95.3125, f1: 90.76984126984127, r: 0.8107900424826952
06/02/2019 09:42:34 *** evaluating ***
06/02/2019 09:42:35 step: 235, epoch: 234, acc: 57.692307692307686, f1: 22.223141723794782, r: 0.21463456544115864
06/02/2019 09:42:35 *** epoch: 236 ***
06/02/2019 09:42:35 *** training ***
06/02/2019 09:42:35 step: 7760, epoch: 235, batch: 4, loss: 0.1000097319483757, acc: 95.3125, f1: 90.53030303030303, r: 0.7868312487495699
06/02/2019 09:42:36 step: 7765, epoch: 235, batch: 9, loss: 0.02702806144952774, acc: 100.0, f1: 100.0, r: 0.7114902391970541
06/02/2019 09:42:36 step: 7770, epoch: 235, batch: 14, loss: 0.014752775430679321, acc: 100.0, f1: 100.0, r: 0.7141267374364537
06/02/2019 09:42:37 step: 7775, epoch: 235, batch: 19, loss: 0.020002149045467377, acc: 98.4375, f1: 97.97843665768194, r: 0.8110041353421976
06/02/2019 09:42:37 step: 7780, epoch: 235, batch: 24, loss: 0.05246560275554657, acc: 98.4375, f1: 97.03703703703704, r: 0.7192765103918228
06/02/2019 09:42:38 step: 7785, epoch: 235, batch: 29, loss: 0.05341220647096634, acc: 98.4375, f1: 97.25274725274726, r: 0.7433260138206628
06/02/2019 09:42:38 *** evaluating ***
06/02/2019 09:42:38 step: 236, epoch: 235, acc: 58.97435897435898, f1: 24.331646756199184, r: 0.23546333928701907
06/02/2019 09:42:38 *** epoch: 237 ***
06/02/2019 09:42:38 *** training ***
06/02/2019 09:42:39 step: 7793, epoch: 236, batch: 4, loss: 0.013566479086875916, acc: 100.0, f1: 100.0, r: 0.8050079675548526
06/02/2019 09:42:39 step: 7798, epoch: 236, batch: 9, loss: 0.043009400367736816, acc: 98.4375, f1: 98.80745341614906, r: 0.7171460898550682
06/02/2019 09:42:40 step: 7803, epoch: 236, batch: 14, loss: 0.006581634283065796, acc: 100.0, f1: 100.0, r: 0.7521561536492306
06/02/2019 09:42:41 step: 7808, epoch: 236, batch: 19, loss: 0.04988390579819679, acc: 98.4375, f1: 95.23809523809523, r: 0.7514472312968659
06/02/2019 09:42:41 step: 7813, epoch: 236, batch: 24, loss: 0.020794466137886047, acc: 100.0, f1: 100.0, r: 0.7666569045718717
06/02/2019 09:42:42 step: 7818, epoch: 236, batch: 29, loss: 0.034089863300323486, acc: 98.4375, f1: 97.95321637426902, r: 0.7631922668387808
06/02/2019 09:42:42 *** evaluating ***
06/02/2019 09:42:42 step: 237, epoch: 236, acc: 59.401709401709404, f1: 21.9474335535741, r: 0.24925594171486065
06/02/2019 09:42:42 *** epoch: 238 ***
06/02/2019 09:42:42 *** training ***
06/02/2019 09:42:43 step: 7826, epoch: 237, batch: 4, loss: 0.07602915167808533, acc: 98.4375, f1: 98.1111111111111, r: 0.7793576229797803
06/02/2019 09:42:43 step: 7831, epoch: 237, batch: 9, loss: 0.07169324904680252, acc: 96.875, f1: 95.27272727272728, r: 0.635451072283091
06/02/2019 09:42:44 step: 7836, epoch: 237, batch: 14, loss: 0.00776132196187973, acc: 100.0, f1: 100.0, r: 0.8541768264220163
06/02/2019 09:42:44 step: 7841, epoch: 237, batch: 19, loss: 0.03711998835206032, acc: 98.4375, f1: 96.82539682539682, r: 0.808701088339194
06/02/2019 09:42:45 step: 7846, epoch: 237, batch: 24, loss: 0.07358261942863464, acc: 98.4375, f1: 97.94832826747721, r: 0.7540562954429484
06/02/2019 09:42:45 step: 7851, epoch: 237, batch: 29, loss: 0.02869550511240959, acc: 100.0, f1: 100.0, r: 0.7014100952398375
06/02/2019 09:42:46 *** evaluating ***
06/02/2019 09:42:46 step: 238, epoch: 237, acc: 58.97435897435898, f1: 22.55714789219944, r: 0.2271655334677086
06/02/2019 09:42:46 *** epoch: 239 ***
06/02/2019 09:42:46 *** training ***
06/02/2019 09:42:46 step: 7859, epoch: 238, batch: 4, loss: 0.06146017462015152, acc: 96.875, f1: 97.33946608946609, r: 0.8314949177255831
06/02/2019 09:42:47 step: 7864, epoch: 238, batch: 9, loss: 0.011650502681732178, acc: 100.0, f1: 100.0, r: 0.7660329843786712
06/02/2019 09:42:47 step: 7869, epoch: 238, batch: 14, loss: 0.002723485231399536, acc: 100.0, f1: 100.0, r: 0.6141282434694924
06/02/2019 09:42:48 step: 7874, epoch: 238, batch: 19, loss: 0.11232879757881165, acc: 95.3125, f1: 89.22619047619047, r: 0.6939348253866428
06/02/2019 09:42:48 step: 7879, epoch: 238, batch: 24, loss: 0.04234635457396507, acc: 96.875, f1: 96.31685273790538, r: 0.7555349807760526
06/02/2019 09:42:49 step: 7884, epoch: 238, batch: 29, loss: 0.08196007460355759, acc: 96.875, f1: 81.94444444444444, r: 0.6808490270324429
06/02/2019 09:42:49 *** evaluating ***
06/02/2019 09:42:50 step: 239, epoch: 238, acc: 57.26495726495726, f1: 22.483360170572148, r: 0.23518768503250304
06/02/2019 09:42:50 *** epoch: 240 ***
06/02/2019 09:42:50 *** training ***
06/02/2019 09:42:50 step: 7892, epoch: 239, batch: 4, loss: 0.07801840454339981, acc: 96.875, f1: 79.86111111111111, r: 0.6702524935389359
06/02/2019 09:42:51 step: 7897, epoch: 239, batch: 9, loss: 0.02685936540365219, acc: 100.0, f1: 100.0, r: 0.7849794799031704
06/02/2019 09:42:51 step: 7902, epoch: 239, batch: 14, loss: 0.0717485100030899, acc: 98.4375, f1: 84.12698412698413, r: 0.5935987197624054
06/02/2019 09:42:52 step: 7907, epoch: 239, batch: 19, loss: 0.04096478223800659, acc: 98.4375, f1: 97.78325123152709, r: 0.7647343660264699
06/02/2019 09:42:52 step: 7912, epoch: 239, batch: 24, loss: 0.06222478672862053, acc: 96.875, f1: 92.71428571428572, r: 0.7981103985910146
06/02/2019 09:42:53 step: 7917, epoch: 239, batch: 29, loss: 0.08302333205938339, acc: 96.875, f1: 90.89591567852437, r: 0.7732644141637427
06/02/2019 09:42:53 *** evaluating ***
06/02/2019 09:42:53 step: 240, epoch: 239, acc: 57.26495726495726, f1: 21.449540043290042, r: 0.2255774360176285
06/02/2019 09:42:53 *** epoch: 241 ***
06/02/2019 09:42:53 *** training ***
06/02/2019 09:42:54 step: 7925, epoch: 240, batch: 4, loss: 0.02668934315443039, acc: 100.0, f1: 100.0, r: 0.8333417832328699
06/02/2019 09:42:54 step: 7930, epoch: 240, batch: 9, loss: 0.028397485613822937, acc: 100.0, f1: 100.0, r: 0.727016198569244
06/02/2019 09:42:55 step: 7935, epoch: 240, batch: 14, loss: 0.09980875253677368, acc: 96.875, f1: 97.32549857549857, r: 0.7248334049941054
06/02/2019 09:42:55 step: 7940, epoch: 240, batch: 19, loss: 0.0964784100651741, acc: 96.875, f1: 86.08244467377595, r: 0.7606500719211601
06/02/2019 09:42:56 step: 7945, epoch: 240, batch: 24, loss: 0.012707382440567017, acc: 100.0, f1: 100.0, r: 0.7252159707974777
06/02/2019 09:42:57 step: 7950, epoch: 240, batch: 29, loss: 0.07391698658466339, acc: 96.875, f1: 94.75081699346406, r: 0.7360548598044794
06/02/2019 09:42:57 *** evaluating ***
06/02/2019 09:42:57 step: 241, epoch: 240, acc: 58.119658119658126, f1: 21.85682279925413, r: 0.22941145944570052
06/02/2019 09:42:57 *** epoch: 242 ***
06/02/2019 09:42:57 *** training ***
06/02/2019 09:42:58 step: 7958, epoch: 241, batch: 4, loss: 0.014813914895057678, acc: 98.4375, f1: 99.0204081632653, r: 0.6727313434678331
06/02/2019 09:42:58 step: 7963, epoch: 241, batch: 9, loss: 0.028180835768580437, acc: 98.4375, f1: 94.61697722567288, r: 0.7276296765590635
06/02/2019 09:42:58 step: 7968, epoch: 241, batch: 14, loss: 0.008082397282123566, acc: 100.0, f1: 100.0, r: 0.7722324600595569
06/02/2019 09:42:59 step: 7973, epoch: 241, batch: 19, loss: 0.13334150612354279, acc: 98.4375, f1: 99.11111111111111, r: 0.7100421564104592
06/02/2019 09:42:59 step: 7978, epoch: 241, batch: 24, loss: 0.054185234010219574, acc: 96.875, f1: 96.19047619047619, r: 0.6776055831868874
06/02/2019 09:43:00 step: 7983, epoch: 241, batch: 29, loss: 0.018680516630411148, acc: 100.0, f1: 100.0, r: 0.7905545587478068
06/02/2019 09:43:00 *** evaluating ***
06/02/2019 09:43:01 step: 242, epoch: 241, acc: 57.692307692307686, f1: 22.122071576404693, r: 0.22267538827947989
06/02/2019 09:43:01 *** epoch: 243 ***
06/02/2019 09:43:01 *** training ***
06/02/2019 09:43:01 step: 7991, epoch: 242, batch: 4, loss: 0.09178844839334488, acc: 95.3125, f1: 78.49025974025975, r: 0.6808393681752305
06/02/2019 09:43:02 step: 7996, epoch: 242, batch: 9, loss: 0.03269175440073013, acc: 98.4375, f1: 93.33333333333333, r: 0.7554522917013856
06/02/2019 09:43:02 step: 8001, epoch: 242, batch: 14, loss: 0.07114063203334808, acc: 98.4375, f1: 99.05998763141622, r: 0.6724675575722112
06/02/2019 09:43:03 step: 8006, epoch: 242, batch: 19, loss: 0.042678266763687134, acc: 98.4375, f1: 97.6608187134503, r: 0.6992965886176283
06/02/2019 09:43:03 step: 8011, epoch: 242, batch: 24, loss: 0.038972388952970505, acc: 98.4375, f1: 98.40619307832424, r: 0.8173073352055984
06/02/2019 09:43:04 step: 8016, epoch: 242, batch: 29, loss: 0.058813150972127914, acc: 96.875, f1: 92.61160117615611, r: 0.7772001477603305
06/02/2019 09:43:04 *** evaluating ***
06/02/2019 09:43:04 step: 243, epoch: 242, acc: 58.97435897435898, f1: 24.103649786639476, r: 0.23896933141334425
06/02/2019 09:43:04 *** epoch: 244 ***
06/02/2019 09:43:04 *** training ***
06/02/2019 09:43:05 step: 8024, epoch: 243, batch: 4, loss: 0.018970727920532227, acc: 98.4375, f1: 94.48621553884712, r: 0.6591947175681893
06/02/2019 09:43:05 step: 8029, epoch: 243, batch: 9, loss: 0.010840266942977905, acc: 100.0, f1: 100.0, r: 0.7919370810189532
06/02/2019 09:43:06 step: 8034, epoch: 243, batch: 14, loss: 0.021694988012313843, acc: 100.0, f1: 100.0, r: 0.7743803882322826
06/02/2019 09:43:06 step: 8039, epoch: 243, batch: 19, loss: 0.003657698631286621, acc: 100.0, f1: 100.0, r: 0.6994880709759931
06/02/2019 09:43:07 step: 8044, epoch: 243, batch: 24, loss: 0.07650815695524216, acc: 96.875, f1: 93.982683982684, r: 0.7626226679395488
06/02/2019 09:43:07 step: 8049, epoch: 243, batch: 29, loss: 0.05894291028380394, acc: 96.875, f1: 95.578231292517, r: 0.7816295673796442
06/02/2019 09:43:08 *** evaluating ***
06/02/2019 09:43:08 step: 244, epoch: 243, acc: 58.119658119658126, f1: 22.04812296105763, r: 0.2265302995142616
06/02/2019 09:43:08 *** epoch: 245 ***
06/02/2019 09:43:08 *** training ***
06/02/2019 09:43:08 step: 8057, epoch: 244, batch: 4, loss: 0.0399601012468338, acc: 98.4375, f1: 85.0, r: 0.745681349947
06/02/2019 09:43:09 step: 8062, epoch: 244, batch: 9, loss: 0.04570099338889122, acc: 98.4375, f1: 97.25274725274726, r: 0.713967315153663
06/02/2019 09:43:09 step: 8067, epoch: 244, batch: 14, loss: 0.04271706938743591, acc: 98.4375, f1: 93.33333333333333, r: 0.7957327067668769
06/02/2019 09:43:10 step: 8072, epoch: 244, batch: 19, loss: 0.04742204397916794, acc: 98.4375, f1: 82.85714285714285, r: 0.6537614271046722
06/02/2019 09:43:10 step: 8077, epoch: 244, batch: 24, loss: 0.04489268362522125, acc: 98.4375, f1: 96.39097744360903, r: 0.7634868401770795
06/02/2019 09:43:11 step: 8082, epoch: 244, batch: 29, loss: 0.06420775502920151, acc: 95.3125, f1: 90.05618901075778, r: 0.6461084203687908
06/02/2019 09:43:11 *** evaluating ***
06/02/2019 09:43:12 step: 245, epoch: 244, acc: 58.119658119658126, f1: 22.000473434899664, r: 0.22910779418852326
06/02/2019 09:43:12 *** epoch: 246 ***
06/02/2019 09:43:12 *** training ***
06/02/2019 09:43:12 step: 8090, epoch: 245, batch: 4, loss: 0.012623831629753113, acc: 100.0, f1: 100.0, r: 0.7929102238479497
06/02/2019 09:43:13 step: 8095, epoch: 245, batch: 9, loss: 0.018329545855522156, acc: 100.0, f1: 100.0, r: 0.6648829407611714
06/02/2019 09:43:13 step: 8100, epoch: 245, batch: 14, loss: 0.0029775500297546387, acc: 100.0, f1: 100.0, r: 0.8395566680781348
06/02/2019 09:43:14 step: 8105, epoch: 245, batch: 19, loss: 0.06956418603658676, acc: 96.875, f1: 95.10204081632654, r: 0.6893366117041833
06/02/2019 09:43:14 step: 8110, epoch: 245, batch: 24, loss: 0.061154741793870926, acc: 95.3125, f1: 92.62775842044134, r: 0.6549877839803006
06/02/2019 09:43:15 step: 8115, epoch: 245, batch: 29, loss: 0.05764186009764671, acc: 96.875, f1: 84.32539682539682, r: 0.7300481842541191
06/02/2019 09:43:15 *** evaluating ***
06/02/2019 09:43:15 step: 246, epoch: 245, acc: 58.119658119658126, f1: 21.89948393150364, r: 0.2283691418186694
06/02/2019 09:43:15 *** epoch: 247 ***
06/02/2019 09:43:15 *** training ***
06/02/2019 09:43:16 step: 8123, epoch: 246, batch: 4, loss: 0.01429450511932373, acc: 100.0, f1: 100.0, r: 0.7984824596130006
06/02/2019 09:43:16 step: 8128, epoch: 246, batch: 9, loss: 0.02391311526298523, acc: 98.4375, f1: 98.22082679225535, r: 0.6527689881138253
06/02/2019 09:43:17 step: 8133, epoch: 246, batch: 14, loss: 0.005966790020465851, acc: 100.0, f1: 100.0, r: 0.7717773078627794
06/02/2019 09:43:17 step: 8138, epoch: 246, batch: 19, loss: 0.022202342748641968, acc: 98.4375, f1: 98.29573934837093, r: 0.730100720037815
06/02/2019 09:43:18 step: 8143, epoch: 246, batch: 24, loss: 0.034119561314582825, acc: 98.4375, f1: 99.21182266009852, r: 0.793944407829682
06/02/2019 09:43:19 step: 8148, epoch: 246, batch: 29, loss: 0.04854733496904373, acc: 96.875, f1: 95.70574162679426, r: 0.8079140934330313
06/02/2019 09:43:19 *** evaluating ***
06/02/2019 09:43:19 step: 247, epoch: 246, acc: 59.401709401709404, f1: 23.247254880778847, r: 0.2469378063997731
06/02/2019 09:43:19 *** epoch: 248 ***
06/02/2019 09:43:19 *** training ***
06/02/2019 09:43:20 step: 8156, epoch: 247, batch: 4, loss: 0.08524800091981888, acc: 95.3125, f1: 95.83260844130409, r: 0.6681587495429483
06/02/2019 09:43:20 step: 8161, epoch: 247, batch: 9, loss: 0.0761910006403923, acc: 96.875, f1: 94.1991341991342, r: 0.7237675020385679
06/02/2019 09:43:21 step: 8166, epoch: 247, batch: 14, loss: 0.025474179536104202, acc: 100.0, f1: 100.0, r: 0.8151540109507734
06/02/2019 09:43:21 step: 8171, epoch: 247, batch: 19, loss: 0.027649443596601486, acc: 98.4375, f1: 98.00453514739228, r: 0.6525389150314275
06/02/2019 09:43:22 step: 8176, epoch: 247, batch: 24, loss: 0.026338636875152588, acc: 98.4375, f1: 99.08553585368574, r: 0.6989303470556097
06/02/2019 09:43:22 step: 8181, epoch: 247, batch: 29, loss: 0.027470503002405167, acc: 98.4375, f1: 98.61471861471863, r: 0.6923061529427885
06/02/2019 09:43:23 *** evaluating ***
06/02/2019 09:43:23 step: 248, epoch: 247, acc: 58.119658119658126, f1: 23.6349969519989, r: 0.23812786045076448
06/02/2019 09:43:23 *** epoch: 249 ***
06/02/2019 09:43:23 *** training ***
06/02/2019 09:43:23 step: 8189, epoch: 248, batch: 4, loss: 0.014082886278629303, acc: 100.0, f1: 100.0, r: 0.7839206297481164
06/02/2019 09:43:24 step: 8194, epoch: 248, batch: 9, loss: 0.010138057172298431, acc: 100.0, f1: 100.0, r: 0.6579375721960471
06/02/2019 09:43:24 step: 8199, epoch: 248, batch: 14, loss: 0.013873295858502388, acc: 100.0, f1: 100.0, r: 0.7284569403459271
06/02/2019 09:43:25 step: 8204, epoch: 248, batch: 19, loss: 0.03515872359275818, acc: 98.4375, f1: 99.1045991045991, r: 0.6860976455097352
06/02/2019 09:43:25 step: 8209, epoch: 248, batch: 24, loss: 0.025511860847473145, acc: 98.4375, f1: 95.17543859649122, r: 0.742081167700757
06/02/2019 09:43:26 step: 8214, epoch: 248, batch: 29, loss: 0.043835826218128204, acc: 98.4375, f1: 98.01587301587303, r: 0.7925881150210503
06/02/2019 09:43:26 *** evaluating ***
06/02/2019 09:43:27 step: 249, epoch: 248, acc: 57.26495726495726, f1: 21.84134307145323, r: 0.2449369550719411
06/02/2019 09:43:27 *** epoch: 250 ***
06/02/2019 09:43:27 *** training ***
06/02/2019 09:43:27 step: 8222, epoch: 249, batch: 4, loss: 0.013831567019224167, acc: 100.0, f1: 100.0, r: 0.6122124058838868
06/02/2019 09:43:28 step: 8227, epoch: 249, batch: 9, loss: 0.1697710007429123, acc: 93.75, f1: 90.29780982905984, r: 0.7202062188838417
06/02/2019 09:43:28 step: 8232, epoch: 249, batch: 14, loss: 0.07420162856578827, acc: 96.875, f1: 97.49679487179486, r: 0.7797691380334626
06/02/2019 09:43:29 step: 8237, epoch: 249, batch: 19, loss: 0.02823065035045147, acc: 98.4375, f1: 97.31379731379732, r: 0.6470865456586276
06/02/2019 09:43:29 step: 8242, epoch: 249, batch: 24, loss: 0.12044888734817505, acc: 98.4375, f1: 97.95918367346938, r: 0.801104858702672
06/02/2019 09:43:30 step: 8247, epoch: 249, batch: 29, loss: 0.028570394963026047, acc: 98.4375, f1: 99.22067268252665, r: 0.8151289019645723
06/02/2019 09:43:30 *** evaluating ***
06/02/2019 09:43:30 step: 250, epoch: 249, acc: 58.119658119658126, f1: 22.045562411010913, r: 0.24849221578530212
06/02/2019 09:43:30 *** epoch: 251 ***
06/02/2019 09:43:30 *** training ***
06/02/2019 09:43:31 step: 8255, epoch: 250, batch: 4, loss: 0.0040930286049842834, acc: 100.0, f1: 100.0, r: 0.7761386663848131
06/02/2019 09:43:31 step: 8260, epoch: 250, batch: 9, loss: 0.043978821486234665, acc: 98.4375, f1: 95.84415584415584, r: 0.6799885732272147
06/02/2019 09:43:32 step: 8265, epoch: 250, batch: 14, loss: 0.028549395501613617, acc: 98.4375, f1: 85.18518518518519, r: 0.665650157797734
06/02/2019 09:43:32 step: 8270, epoch: 250, batch: 19, loss: 0.061290889978408813, acc: 96.875, f1: 85.17080745341615, r: 0.7097319330748779
06/02/2019 09:43:33 step: 8275, epoch: 250, batch: 24, loss: 0.004079744219779968, acc: 100.0, f1: 100.0, r: 0.7503871955412813
06/02/2019 09:43:33 step: 8280, epoch: 250, batch: 29, loss: 0.07152998447418213, acc: 96.875, f1: 96.05352896307264, r: 0.7910710830051808
06/02/2019 09:43:34 *** evaluating ***
06/02/2019 09:43:34 step: 251, epoch: 250, acc: 58.119658119658126, f1: 21.403853923895507, r: 0.24367461985743705
06/02/2019 09:43:34 *** epoch: 252 ***
06/02/2019 09:43:34 *** training ***
06/02/2019 09:43:35 step: 8288, epoch: 251, batch: 4, loss: 0.06828278303146362, acc: 96.875, f1: 93.06737588652483, r: 0.7959323120282517
06/02/2019 09:43:35 step: 8293, epoch: 251, batch: 9, loss: 0.022765450179576874, acc: 98.4375, f1: 99.2755543775952, r: 0.7163246931594861
06/02/2019 09:43:36 step: 8298, epoch: 251, batch: 14, loss: 0.043517597019672394, acc: 98.4375, f1: 96.82539682539682, r: 0.7382292110452341
06/02/2019 09:43:36 step: 8303, epoch: 251, batch: 19, loss: 0.0026327520608901978, acc: 100.0, f1: 100.0, r: 0.7795683186917705
06/02/2019 09:43:37 step: 8308, epoch: 251, batch: 24, loss: 0.013492099940776825, acc: 100.0, f1: 100.0, r: 0.7156482083253236
06/02/2019 09:43:37 step: 8313, epoch: 251, batch: 29, loss: 0.026137854903936386, acc: 98.4375, f1: 97.6608187134503, r: 0.6413730932381524
06/02/2019 09:43:38 *** evaluating ***
06/02/2019 09:43:38 step: 252, epoch: 251, acc: 57.692307692307686, f1: 22.127145488792564, r: 0.21860351930515817
06/02/2019 09:43:38 *** epoch: 253 ***
06/02/2019 09:43:38 *** training ***
06/02/2019 09:43:38 step: 8321, epoch: 252, batch: 4, loss: 0.05049365758895874, acc: 98.4375, f1: 97.67080745341615, r: 0.7148973293065839
06/02/2019 09:43:39 step: 8326, epoch: 252, batch: 9, loss: 0.04355641454458237, acc: 98.4375, f1: 95.59748427672956, r: 0.7734112361400406
06/02/2019 09:43:39 step: 8331, epoch: 252, batch: 14, loss: 0.006009213626384735, acc: 100.0, f1: 100.0, r: 0.703956250945745
06/02/2019 09:43:40 step: 8336, epoch: 252, batch: 19, loss: 0.010880380868911743, acc: 100.0, f1: 100.0, r: 0.8022705306059124
06/02/2019 09:43:40 step: 8341, epoch: 252, batch: 24, loss: 0.05729726701974869, acc: 96.875, f1: 96.13138184566756, r: 0.6876071378673025
06/02/2019 09:43:41 step: 8346, epoch: 252, batch: 29, loss: 0.08819485455751419, acc: 98.4375, f1: 98.39924670433145, r: 0.7720760358623724
06/02/2019 09:43:41 *** evaluating ***
06/02/2019 09:43:42 step: 253, epoch: 252, acc: 58.54700854700855, f1: 21.746093944369807, r: 0.22689038533176126
06/02/2019 09:43:42 *** epoch: 254 ***
06/02/2019 09:43:42 *** training ***
06/02/2019 09:43:42 step: 8354, epoch: 253, batch: 4, loss: 0.02546742558479309, acc: 100.0, f1: 100.0, r: 0.7387303335554183
06/02/2019 09:43:42 step: 8359, epoch: 253, batch: 9, loss: 0.050436679273843765, acc: 96.875, f1: 93.89196310935442, r: 0.5352443279906608
06/02/2019 09:43:43 step: 8364, epoch: 253, batch: 14, loss: 0.02441702038049698, acc: 100.0, f1: 100.0, r: 0.7944172287827792
06/02/2019 09:43:44 step: 8369, epoch: 253, batch: 19, loss: 0.06968643516302109, acc: 98.4375, f1: 97.1139971139971, r: 0.5686013588317499
06/02/2019 09:43:44 step: 8374, epoch: 253, batch: 24, loss: 0.012054406106472015, acc: 100.0, f1: 100.0, r: 0.7807427656970497
06/02/2019 09:43:45 step: 8379, epoch: 253, batch: 29, loss: 0.11226097494363785, acc: 95.3125, f1: 93.43915343915344, r: 0.763699673996794
06/02/2019 09:43:45 *** evaluating ***
06/02/2019 09:43:45 step: 254, epoch: 253, acc: 59.82905982905983, f1: 23.40387726248942, r: 0.23410205719819377
06/02/2019 09:43:45 *** epoch: 255 ***
06/02/2019 09:43:45 *** training ***
06/02/2019 09:43:46 step: 8387, epoch: 254, batch: 4, loss: 0.010353997349739075, acc: 100.0, f1: 100.0, r: 0.6842646128346388
06/02/2019 09:43:46 step: 8392, epoch: 254, batch: 9, loss: 0.044105321168899536, acc: 96.875, f1: 95.71820175438597, r: 0.8503659382200149
06/02/2019 09:43:47 step: 8397, epoch: 254, batch: 14, loss: 0.0018185079097747803, acc: 100.0, f1: 100.0, r: 0.7161146172257491
06/02/2019 09:43:47 step: 8402, epoch: 254, batch: 19, loss: 0.022875497117638588, acc: 100.0, f1: 100.0, r: 0.7982565096428169
06/02/2019 09:43:48 step: 8407, epoch: 254, batch: 24, loss: 0.017684772610664368, acc: 100.0, f1: 100.0, r: 0.6781297173161599
06/02/2019 09:43:48 step: 8412, epoch: 254, batch: 29, loss: 0.02007044106721878, acc: 98.4375, f1: 87.2340425531915, r: 0.6815777624968603
06/02/2019 09:43:49 *** evaluating ***
06/02/2019 09:43:49 step: 255, epoch: 254, acc: 58.97435897435898, f1: 23.142243214804846, r: 0.2423884576452982
06/02/2019 09:43:49 *** epoch: 256 ***
06/02/2019 09:43:49 *** training ***
06/02/2019 09:43:49 step: 8420, epoch: 255, batch: 4, loss: 0.02202417328953743, acc: 100.0, f1: 100.0, r: 0.6486862325472025
06/02/2019 09:43:50 step: 8425, epoch: 255, batch: 9, loss: 0.01537305861711502, acc: 100.0, f1: 100.0, r: 0.8028913245599406
06/02/2019 09:43:50 step: 8430, epoch: 255, batch: 14, loss: 0.06489237397909164, acc: 96.875, f1: 95.06294471811714, r: 0.7012776862813016
06/02/2019 09:43:51 step: 8435, epoch: 255, batch: 19, loss: 0.004501566290855408, acc: 100.0, f1: 100.0, r: 0.7730764228382124
06/02/2019 09:43:51 step: 8440, epoch: 255, batch: 24, loss: 0.028846293687820435, acc: 98.4375, f1: 99.29416954635688, r: 0.7042657078193788
06/02/2019 09:43:52 step: 8445, epoch: 255, batch: 29, loss: 0.036513764411211014, acc: 98.4375, f1: 98.53479853479854, r: 0.6619815997809441
06/02/2019 09:43:52 *** evaluating ***
06/02/2019 09:43:52 step: 256, epoch: 255, acc: 58.119658119658126, f1: 21.831235038322287, r: 0.22666870041561366
06/02/2019 09:43:52 *** epoch: 257 ***
06/02/2019 09:43:52 *** training ***
06/02/2019 09:43:53 step: 8453, epoch: 256, batch: 4, loss: 0.10789619386196136, acc: 96.875, f1: 97.10317460317461, r: 0.7734557072628131
06/02/2019 09:43:53 step: 8458, epoch: 256, batch: 9, loss: 0.07563752681016922, acc: 95.3125, f1: 94.61415396198005, r: 0.7944876631463118
06/02/2019 09:43:54 step: 8463, epoch: 256, batch: 14, loss: 0.09302329272031784, acc: 95.3125, f1: 86.65940766550523, r: 0.7158185267444136
06/02/2019 09:43:54 step: 8468, epoch: 256, batch: 19, loss: 0.06506012380123138, acc: 98.4375, f1: 98.44322344322345, r: 0.7786515949118171
06/02/2019 09:43:55 step: 8473, epoch: 256, batch: 24, loss: 0.0732293576002121, acc: 96.875, f1: 95.80991017096494, r: 0.7134186616583916
06/02/2019 09:43:56 step: 8478, epoch: 256, batch: 29, loss: 0.07868850976228714, acc: 95.3125, f1: 71.97496947496947, r: 0.7349700540937385
06/02/2019 09:43:56 *** evaluating ***
06/02/2019 09:43:56 step: 257, epoch: 256, acc: 58.97435897435898, f1: 21.709223360908755, r: 0.23842234689133857
06/02/2019 09:43:56 *** epoch: 258 ***
06/02/2019 09:43:56 *** training ***
06/02/2019 09:43:57 step: 8486, epoch: 257, batch: 4, loss: 0.015068262815475464, acc: 100.0, f1: 100.0, r: 0.6603627314759213
06/02/2019 09:43:57 step: 8491, epoch: 257, batch: 9, loss: 0.03275351598858833, acc: 98.4375, f1: 98.99793692897141, r: 0.6529035489502493
06/02/2019 09:43:58 step: 8496, epoch: 257, batch: 14, loss: 0.059790365397930145, acc: 96.875, f1: 84.31685273790536, r: 0.7572126331217252
06/02/2019 09:43:58 step: 8501, epoch: 257, batch: 19, loss: 0.008384466171264648, acc: 100.0, f1: 100.0, r: 0.7196309733773103
06/02/2019 09:43:59 step: 8506, epoch: 257, batch: 24, loss: 0.028880715370178223, acc: 98.4375, f1: 98.75776397515527, r: 0.7150171693489098
06/02/2019 09:43:59 step: 8511, epoch: 257, batch: 29, loss: 0.06482000648975372, acc: 96.875, f1: 97.21853816681403, r: 0.8540188450122892
06/02/2019 09:44:00 *** evaluating ***
06/02/2019 09:44:00 step: 258, epoch: 257, acc: 60.256410256410255, f1: 24.245545152521895, r: 0.256512171579967
06/02/2019 09:44:00 *** epoch: 259 ***
06/02/2019 09:44:00 *** training ***
06/02/2019 09:44:01 step: 8519, epoch: 258, batch: 4, loss: 0.06645628809928894, acc: 98.4375, f1: 99.23404255319149, r: 0.8290540778415708
06/02/2019 09:44:01 step: 8524, epoch: 258, batch: 9, loss: 0.04165232181549072, acc: 98.4375, f1: 97.62695775984812, r: 0.7281954431470901
06/02/2019 09:44:02 step: 8529, epoch: 258, batch: 14, loss: 0.00944405049085617, acc: 100.0, f1: 100.0, r: 0.7610044552268015
06/02/2019 09:44:02 step: 8534, epoch: 258, batch: 19, loss: 0.03582977131009102, acc: 98.4375, f1: 83.33333333333333, r: 0.727819817853552
06/02/2019 09:44:03 step: 8539, epoch: 258, batch: 24, loss: 0.053460270166397095, acc: 95.3125, f1: 94.31216931216932, r: 0.6177618667256037
06/02/2019 09:44:03 step: 8544, epoch: 258, batch: 29, loss: 0.014566898345947266, acc: 98.4375, f1: 98.49498327759196, r: 0.8301383645065842
06/02/2019 09:44:03 *** evaluating ***
06/02/2019 09:44:04 step: 259, epoch: 258, acc: 57.692307692307686, f1: 22.77363026981235, r: 0.24744164142407785
06/02/2019 09:44:04 *** epoch: 260 ***
06/02/2019 09:44:04 *** training ***
06/02/2019 09:44:04 step: 8552, epoch: 259, batch: 4, loss: 0.060877636075019836, acc: 96.875, f1: 97.27664239292146, r: 0.7731161029790755
06/02/2019 09:44:05 step: 8557, epoch: 259, batch: 9, loss: 0.04061024636030197, acc: 98.4375, f1: 94.44444444444444, r: 0.822663799725636
06/02/2019 09:44:05 step: 8562, epoch: 259, batch: 14, loss: 0.0040953755378723145, acc: 100.0, f1: 100.0, r: 0.5549867133345951
06/02/2019 09:44:06 step: 8567, epoch: 259, batch: 19, loss: 0.0012099146842956543, acc: 100.0, f1: 100.0, r: 0.7336212732188783
06/02/2019 09:44:06 step: 8572, epoch: 259, batch: 24, loss: 0.026346057653427124, acc: 98.4375, f1: 99.15895710681245, r: 0.7141866002797647
06/02/2019 09:44:07 step: 8577, epoch: 259, batch: 29, loss: 0.07020701467990875, acc: 96.875, f1: 86.41156462585035, r: 0.7065184462268409
06/02/2019 09:44:07 *** evaluating ***
06/02/2019 09:44:07 step: 260, epoch: 259, acc: 58.54700854700855, f1: 22.898489465920793, r: 0.2390889046885946
06/02/2019 09:44:07 *** epoch: 261 ***
06/02/2019 09:44:07 *** training ***
06/02/2019 09:44:08 step: 8585, epoch: 260, batch: 4, loss: 0.027768783271312714, acc: 98.4375, f1: 97.77777777777779, r: 0.7560506497555168
06/02/2019 09:44:08 step: 8590, epoch: 260, batch: 9, loss: 0.03902898728847504, acc: 98.4375, f1: 99.30300807043287, r: 0.8010294956609754
06/02/2019 09:44:09 step: 8595, epoch: 260, batch: 14, loss: 0.020967349410057068, acc: 100.0, f1: 100.0, r: 0.7915795193617277
06/02/2019 09:44:09 step: 8600, epoch: 260, batch: 19, loss: 0.06793111562728882, acc: 96.875, f1: 96.4360726945154, r: 0.6741295996234228
06/02/2019 09:44:10 step: 8605, epoch: 260, batch: 24, loss: 0.058560632169246674, acc: 96.875, f1: 92.8936212166026, r: 0.6318454333542911
06/02/2019 09:44:10 step: 8610, epoch: 260, batch: 29, loss: 0.06991250067949295, acc: 98.4375, f1: 94.44444444444444, r: 0.7514957184128604
06/02/2019 09:44:11 *** evaluating ***
06/02/2019 09:44:11 step: 261, epoch: 260, acc: 58.54700854700855, f1: 23.403459869714087, r: 0.25099808220170083
06/02/2019 09:44:11 *** epoch: 262 ***
06/02/2019 09:44:11 *** training ***
06/02/2019 09:44:11 step: 8618, epoch: 261, batch: 4, loss: 0.04340319335460663, acc: 98.4375, f1: 93.19727891156461, r: 0.6996446650971063
06/02/2019 09:44:12 step: 8623, epoch: 261, batch: 9, loss: 0.004757396876811981, acc: 100.0, f1: 100.0, r: 0.663234031432627
06/02/2019 09:44:12 step: 8628, epoch: 261, batch: 14, loss: 0.058555204421281815, acc: 96.875, f1: 95.56763285024155, r: 0.7806328575691794
06/02/2019 09:44:13 step: 8633, epoch: 261, batch: 19, loss: 0.006141498684883118, acc: 100.0, f1: 100.0, r: 0.8292070625287876
06/02/2019 09:44:13 step: 8638, epoch: 261, batch: 24, loss: 0.060090288519859314, acc: 96.875, f1: 93.54741896758703, r: 0.7081718003148633
06/02/2019 09:44:14 step: 8643, epoch: 261, batch: 29, loss: 0.045198503881692886, acc: 98.4375, f1: 84.12698412698413, r: 0.605712540919792
06/02/2019 09:44:14 *** evaluating ***
06/02/2019 09:44:14 step: 262, epoch: 261, acc: 59.401709401709404, f1: 24.478206587433935, r: 0.2506754822268421
06/02/2019 09:44:14 *** epoch: 263 ***
06/02/2019 09:44:14 *** training ***
06/02/2019 09:44:15 step: 8651, epoch: 262, batch: 4, loss: 0.03324225917458534, acc: 98.4375, f1: 97.85714285714286, r: 0.754732036888316
06/02/2019 09:44:15 step: 8656, epoch: 262, batch: 9, loss: 0.04956798627972603, acc: 98.4375, f1: 94.6969696969697, r: 0.7291234030645539
06/02/2019 09:44:16 step: 8661, epoch: 262, batch: 14, loss: 0.043858062475919724, acc: 98.4375, f1: 96.42857142857143, r: 0.7299203581126048
06/02/2019 09:44:17 step: 8666, epoch: 262, batch: 19, loss: 0.03697093576192856, acc: 96.875, f1: 94.84126984126985, r: 0.7534069405269465
06/02/2019 09:44:17 step: 8671, epoch: 262, batch: 24, loss: 0.0015062615275382996, acc: 100.0, f1: 100.0, r: 0.6194168753053443
06/02/2019 09:44:18 step: 8676, epoch: 262, batch: 29, loss: 0.027107641100883484, acc: 98.4375, f1: 96.86028257456829, r: 0.7260820635306339
06/02/2019 09:44:18 *** evaluating ***
06/02/2019 09:44:18 step: 263, epoch: 262, acc: 59.401709401709404, f1: 24.062726470727547, r: 0.24998312889815777
06/02/2019 09:44:18 *** epoch: 264 ***
06/02/2019 09:44:18 *** training ***
06/02/2019 09:44:19 step: 8684, epoch: 263, batch: 4, loss: 0.0010359585285186768, acc: 100.0, f1: 100.0, r: 0.721337579781501
06/02/2019 09:44:19 step: 8689, epoch: 263, batch: 9, loss: 0.0072097256779670715, acc: 100.0, f1: 100.0, r: 0.744389649899195
06/02/2019 09:44:20 step: 8694, epoch: 263, batch: 14, loss: 0.0019395053386688232, acc: 100.0, f1: 100.0, r: 0.8132775616003013
06/02/2019 09:44:20 step: 8699, epoch: 263, batch: 19, loss: 0.02895878627896309, acc: 98.4375, f1: 94.92063492063492, r: 0.7114672286600956
06/02/2019 09:44:21 step: 8704, epoch: 263, batch: 24, loss: 0.06078185513615608, acc: 95.3125, f1: 84.50605553866424, r: 0.8100854548024079
06/02/2019 09:44:21 step: 8709, epoch: 263, batch: 29, loss: 0.021258123219013214, acc: 100.0, f1: 100.0, r: 0.7956888412968398
06/02/2019 09:44:21 *** evaluating ***
06/02/2019 09:44:22 step: 264, epoch: 263, acc: 59.401709401709404, f1: 24.184609333321276, r: 0.2445047418836418
06/02/2019 09:44:22 *** epoch: 265 ***
06/02/2019 09:44:22 *** training ***
06/02/2019 09:44:22 step: 8717, epoch: 264, batch: 4, loss: 0.015495702624320984, acc: 100.0, f1: 100.0, r: 0.7093210184399863
06/02/2019 09:44:23 step: 8722, epoch: 264, batch: 9, loss: 0.07559987902641296, acc: 96.875, f1: 82.79220779220779, r: 0.6757691963769019
06/02/2019 09:44:23 step: 8727, epoch: 264, batch: 14, loss: 0.006272017955780029, acc: 100.0, f1: 100.0, r: 0.8279619121698507
06/02/2019 09:44:24 step: 8732, epoch: 264, batch: 19, loss: 0.0011440962553024292, acc: 100.0, f1: 100.0, r: 0.7337409463254856
06/02/2019 09:44:24 step: 8737, epoch: 264, batch: 24, loss: 0.029536083340644836, acc: 98.4375, f1: 97.43589743589743, r: 0.5625655129739683
06/02/2019 09:44:25 step: 8742, epoch: 264, batch: 29, loss: 0.024697527289390564, acc: 100.0, f1: 100.0, r: 0.7193366144496119
06/02/2019 09:44:25 *** evaluating ***
06/02/2019 09:44:25 step: 265, epoch: 264, acc: 59.401709401709404, f1: 23.603773584905664, r: 0.24546668466171695
06/02/2019 09:44:25 *** epoch: 266 ***
06/02/2019 09:44:25 *** training ***
06/02/2019 09:44:26 step: 8750, epoch: 265, batch: 4, loss: 0.029633015394210815, acc: 98.4375, f1: 97.6608187134503, r: 0.7855042455572246
06/02/2019 09:44:26 step: 8755, epoch: 265, batch: 9, loss: 0.011112265288829803, acc: 100.0, f1: 100.0, r: 0.6776808337781202
06/02/2019 09:44:27 step: 8760, epoch: 265, batch: 14, loss: 0.028683245182037354, acc: 100.0, f1: 100.0, r: 0.7605943846976286
06/02/2019 09:44:27 step: 8765, epoch: 265, batch: 19, loss: 0.012030109763145447, acc: 100.0, f1: 100.0, r: 0.8028588380060673
06/02/2019 09:44:28 step: 8770, epoch: 265, batch: 24, loss: 0.056141506880521774, acc: 98.4375, f1: 83.33333333333333, r: 0.6392028878539929
06/02/2019 09:44:28 step: 8775, epoch: 265, batch: 29, loss: 0.010886818170547485, acc: 100.0, f1: 100.0, r: 0.7794073867218742
06/02/2019 09:44:29 *** evaluating ***
06/02/2019 09:44:29 step: 266, epoch: 265, acc: 58.97435897435898, f1: 23.77315104673371, r: 0.24454173692913111
06/02/2019 09:44:29 *** epoch: 267 ***
06/02/2019 09:44:29 *** training ***
06/02/2019 09:44:29 step: 8783, epoch: 266, batch: 4, loss: 0.014512773603200912, acc: 100.0, f1: 100.0, r: 0.6815932713916061
06/02/2019 09:44:30 step: 8788, epoch: 266, batch: 9, loss: 0.03883317857980728, acc: 98.4375, f1: 97.74891774891775, r: 0.6688900261023091
06/02/2019 09:44:30 step: 8793, epoch: 266, batch: 14, loss: 0.03715898096561432, acc: 98.4375, f1: 87.17948717948718, r: 0.751759839156945
06/02/2019 09:44:31 step: 8798, epoch: 266, batch: 19, loss: 0.0015413016080856323, acc: 100.0, f1: 100.0, r: 0.7314065945351768
06/02/2019 09:44:31 step: 8803, epoch: 266, batch: 24, loss: 0.0027605295181274414, acc: 100.0, f1: 100.0, r: 0.7130089850363688
06/02/2019 09:44:32 step: 8808, epoch: 266, batch: 29, loss: 0.04991579055786133, acc: 98.4375, f1: 84.87394957983193, r: 0.6121647245941373
06/02/2019 09:44:32 *** evaluating ***
06/02/2019 09:44:33 step: 267, epoch: 266, acc: 59.82905982905983, f1: 23.04460592127074, r: 0.23187294432649574
06/02/2019 09:44:33 *** epoch: 268 ***
06/02/2019 09:44:33 *** training ***
06/02/2019 09:44:33 step: 8816, epoch: 267, batch: 4, loss: 0.027353398501873016, acc: 98.4375, f1: 98.01587301587303, r: 0.821977569685214
06/02/2019 09:44:33 step: 8821, epoch: 267, batch: 9, loss: 0.012033067643642426, acc: 100.0, f1: 100.0, r: 0.761923678444466
06/02/2019 09:44:34 step: 8826, epoch: 267, batch: 14, loss: 0.03458661586046219, acc: 98.4375, f1: 98.32967032967034, r: 0.6395908964126908
06/02/2019 09:44:35 step: 8831, epoch: 267, batch: 19, loss: 0.11919829994440079, acc: 95.3125, f1: 91.48911859115941, r: 0.7066190136610189
06/02/2019 09:44:35 step: 8836, epoch: 267, batch: 24, loss: 0.08852863311767578, acc: 96.875, f1: 93.66666666666667, r: 0.756664800366461
06/02/2019 09:44:36 step: 8841, epoch: 267, batch: 29, loss: 0.016376957297325134, acc: 100.0, f1: 100.0, r: 0.8058818986195713
06/02/2019 09:44:36 *** evaluating ***
06/02/2019 09:44:36 step: 268, epoch: 267, acc: 58.54700854700855, f1: 23.626758597655247, r: 0.2393203601906679
06/02/2019 09:44:36 *** epoch: 269 ***
06/02/2019 09:44:36 *** training ***
06/02/2019 09:44:37 step: 8849, epoch: 268, batch: 4, loss: 0.011681981384754181, acc: 100.0, f1: 100.0, r: 0.6933655861730711
06/02/2019 09:44:37 step: 8854, epoch: 268, batch: 9, loss: 0.020210638642311096, acc: 98.4375, f1: 99.12280701754386, r: 0.8557884822775026
06/02/2019 09:44:38 step: 8859, epoch: 268, batch: 14, loss: 0.014134712517261505, acc: 100.0, f1: 100.0, r: 0.8285379047515483
06/02/2019 09:44:38 step: 8864, epoch: 268, batch: 19, loss: 0.06785063445568085, acc: 98.4375, f1: 86.95652173913044, r: 0.6621224350826078
06/02/2019 09:44:39 step: 8869, epoch: 268, batch: 24, loss: 0.030524950474500656, acc: 100.0, f1: 100.0, r: 0.8437009472090053
06/02/2019 09:44:39 step: 8874, epoch: 268, batch: 29, loss: 0.002317868173122406, acc: 100.0, f1: 100.0, r: 0.8428046713180819
06/02/2019 09:44:39 *** evaluating ***
06/02/2019 09:44:40 step: 269, epoch: 268, acc: 58.119658119658126, f1: 22.682607754130895, r: 0.22876822747495976
06/02/2019 09:44:40 *** epoch: 270 ***
06/02/2019 09:44:40 *** training ***
06/02/2019 09:44:40 step: 8882, epoch: 269, batch: 4, loss: 0.053767167031764984, acc: 98.4375, f1: 97.71428571428571, r: 0.8039421795366348
06/02/2019 09:44:41 step: 8887, epoch: 269, batch: 9, loss: 0.009989157319068909, acc: 100.0, f1: 100.0, r: 0.6692708641244175
06/02/2019 09:44:41 step: 8892, epoch: 269, batch: 14, loss: 0.0297122560441494, acc: 100.0, f1: 100.0, r: 0.7197342047898397
06/02/2019 09:44:42 step: 8897, epoch: 269, batch: 19, loss: 0.0110912024974823, acc: 100.0, f1: 100.0, r: 0.8324418464868288
06/02/2019 09:44:42 step: 8902, epoch: 269, batch: 24, loss: 0.0061438605189323425, acc: 100.0, f1: 100.0, r: 0.7831189292269269
06/02/2019 09:44:43 step: 8907, epoch: 269, batch: 29, loss: 0.03682872653007507, acc: 96.875, f1: 96.34118541033433, r: 0.7799993384784457
06/02/2019 09:44:43 *** evaluating ***
06/02/2019 09:44:43 step: 270, epoch: 269, acc: 58.54700854700855, f1: 22.410636866933846, r: 0.23353181941622983
06/02/2019 09:44:43 *** epoch: 271 ***
06/02/2019 09:44:43 *** training ***
06/02/2019 09:44:44 step: 8915, epoch: 270, batch: 4, loss: 0.011445552110671997, acc: 100.0, f1: 100.0, r: 0.6740142272771069
06/02/2019 09:44:44 step: 8920, epoch: 270, batch: 9, loss: 0.02515678107738495, acc: 100.0, f1: 100.0, r: 0.6991612588407512
06/02/2019 09:44:45 step: 8925, epoch: 270, batch: 14, loss: 0.0042467862367630005, acc: 100.0, f1: 100.0, r: 0.6914536353706335
06/02/2019 09:44:45 step: 8930, epoch: 270, batch: 19, loss: 0.03800128400325775, acc: 98.4375, f1: 99.02818270165209, r: 0.6836305692742828
06/02/2019 09:44:46 step: 8935, epoch: 270, batch: 24, loss: 0.03265456110239029, acc: 98.4375, f1: 96.8831168831169, r: 0.6992703270582022
06/02/2019 09:44:46 step: 8940, epoch: 270, batch: 29, loss: 0.04724990576505661, acc: 98.4375, f1: 86.66666666666667, r: 0.7924439683379968
06/02/2019 09:44:47 *** evaluating ***
06/02/2019 09:44:47 step: 271, epoch: 270, acc: 58.54700854700855, f1: 22.733410165685704, r: 0.23976649870030242
06/02/2019 09:44:47 *** epoch: 272 ***
06/02/2019 09:44:47 *** training ***
06/02/2019 09:44:47 step: 8948, epoch: 271, batch: 4, loss: 0.006443746387958527, acc: 100.0, f1: 100.0, r: 0.7780263915873615
06/02/2019 09:44:48 step: 8953, epoch: 271, batch: 9, loss: 0.04228445887565613, acc: 98.4375, f1: 99.07493061979649, r: 0.6841366313347077
06/02/2019 09:44:48 step: 8958, epoch: 271, batch: 14, loss: 0.01476268470287323, acc: 98.4375, f1: 98.82711705371804, r: 0.6721155956812053
06/02/2019 09:44:49 step: 8963, epoch: 271, batch: 19, loss: 0.00911739468574524, acc: 100.0, f1: 100.0, r: 0.7176825134724686
06/02/2019 09:44:50 step: 8968, epoch: 271, batch: 24, loss: 0.018324218690395355, acc: 100.0, f1: 100.0, r: 0.7088376282854977
06/02/2019 09:44:50 step: 8973, epoch: 271, batch: 29, loss: 0.015331022441387177, acc: 100.0, f1: 100.0, r: 0.7046281478823122
06/02/2019 09:44:50 *** evaluating ***
06/02/2019 09:44:51 step: 272, epoch: 271, acc: 58.54700854700855, f1: 23.676382827276615, r: 0.23915156789010503
06/02/2019 09:44:51 *** epoch: 273 ***
06/02/2019 09:44:51 *** training ***
06/02/2019 09:44:51 step: 8981, epoch: 272, batch: 4, loss: 0.02545274794101715, acc: 98.4375, f1: 97.22222222222221, r: 0.8314442142524181
06/02/2019 09:44:52 step: 8986, epoch: 272, batch: 9, loss: 0.006183288991451263, acc: 100.0, f1: 100.0, r: 0.7410979749188232
06/02/2019 09:44:52 step: 8991, epoch: 272, batch: 14, loss: 0.008053958415985107, acc: 100.0, f1: 100.0, r: 0.715121940542548
06/02/2019 09:44:53 step: 8996, epoch: 272, batch: 19, loss: 0.060406461358070374, acc: 98.4375, f1: 99.00226757369614, r: 0.7195741975582038
06/02/2019 09:44:53 step: 9001, epoch: 272, batch: 24, loss: 0.005921594798564911, acc: 100.0, f1: 100.0, r: 0.665787436384605
06/02/2019 09:44:54 step: 9006, epoch: 272, batch: 29, loss: 0.004637211561203003, acc: 100.0, f1: 100.0, r: 0.8394110848251729
06/02/2019 09:44:54 *** evaluating ***
06/02/2019 09:44:54 step: 273, epoch: 272, acc: 58.97435897435898, f1: 23.965070220727547, r: 0.24550260631997667
06/02/2019 09:44:54 *** epoch: 274 ***
06/02/2019 09:44:54 *** training ***
06/02/2019 09:44:55 step: 9014, epoch: 273, batch: 4, loss: 0.007987648248672485, acc: 100.0, f1: 100.0, r: 0.7454640838993765
06/02/2019 09:44:55 step: 9019, epoch: 273, batch: 9, loss: 0.023800961673259735, acc: 98.4375, f1: 96.8922305764411, r: 0.6721236017152351
06/02/2019 09:44:56 step: 9024, epoch: 273, batch: 14, loss: 0.011396661400794983, acc: 100.0, f1: 100.0, r: 0.7865503655049981
06/02/2019 09:44:56 step: 9029, epoch: 273, batch: 19, loss: 0.01329077035188675, acc: 100.0, f1: 100.0, r: 0.7519547218319889
06/02/2019 09:44:57 step: 9034, epoch: 273, batch: 24, loss: 0.013844102621078491, acc: 100.0, f1: 100.0, r: 0.6834330631918056
06/02/2019 09:44:57 step: 9039, epoch: 273, batch: 29, loss: 0.011611118912696838, acc: 100.0, f1: 100.0, r: 0.8241135825251934
06/02/2019 09:44:58 *** evaluating ***
06/02/2019 09:44:58 step: 274, epoch: 273, acc: 59.401709401709404, f1: 24.448684391789506, r: 0.23639278490495744
06/02/2019 09:44:58 *** epoch: 275 ***
06/02/2019 09:44:58 *** training ***
06/02/2019 09:44:58 step: 9047, epoch: 274, batch: 4, loss: 0.03992941230535507, acc: 98.4375, f1: 98.1111111111111, r: 0.7944086317716027
06/02/2019 09:44:59 step: 9052, epoch: 274, batch: 9, loss: 0.0129217728972435, acc: 100.0, f1: 100.0, r: 0.708681572775266
06/02/2019 09:44:59 step: 9057, epoch: 274, batch: 14, loss: 0.03227663040161133, acc: 98.4375, f1: 96.66666666666667, r: 0.8176570923966079
06/02/2019 09:45:00 step: 9062, epoch: 274, batch: 19, loss: 0.02736254781484604, acc: 98.4375, f1: 98.68131868131869, r: 0.718808461356555
06/02/2019 09:45:00 step: 9067, epoch: 274, batch: 24, loss: 0.0205623060464859, acc: 98.4375, f1: 96.52173913043478, r: 0.7076613301615851
06/02/2019 09:45:01 step: 9072, epoch: 274, batch: 29, loss: 0.005200780928134918, acc: 100.0, f1: 100.0, r: 0.8426824847417423
06/02/2019 09:45:01 *** evaluating ***
06/02/2019 09:45:01 step: 275, epoch: 274, acc: 58.54700854700855, f1: 23.88869955075224, r: 0.24357531285932502
06/02/2019 09:45:01 *** epoch: 276 ***
06/02/2019 09:45:01 *** training ***
06/02/2019 09:45:02 step: 9080, epoch: 275, batch: 4, loss: 0.037755098193883896, acc: 98.4375, f1: 99.28698752228165, r: 0.7291346813825913
06/02/2019 09:45:02 step: 9085, epoch: 275, batch: 9, loss: 0.039790134876966476, acc: 98.4375, f1: 99.08700322234158, r: 0.7071649477108699
06/02/2019 09:45:03 step: 9090, epoch: 275, batch: 14, loss: 0.03273627907037735, acc: 98.4375, f1: 97.94871794871796, r: 0.7436566521249875
06/02/2019 09:45:03 step: 9095, epoch: 275, batch: 19, loss: 0.06237998977303505, acc: 96.875, f1: 83.35600907029479, r: 0.7013873635411578
06/02/2019 09:45:04 step: 9100, epoch: 275, batch: 24, loss: 0.0071288421750068665, acc: 100.0, f1: 100.0, r: 0.7960614406870146
06/02/2019 09:45:04 step: 9105, epoch: 275, batch: 29, loss: 0.017568454146385193, acc: 100.0, f1: 100.0, r: 0.7833925554302179
06/02/2019 09:45:05 *** evaluating ***
06/02/2019 09:45:05 step: 276, epoch: 275, acc: 58.119658119658126, f1: 23.082460935087393, r: 0.2346132144061551
06/02/2019 09:45:05 *** epoch: 277 ***
06/02/2019 09:45:05 *** training ***
06/02/2019 09:45:06 step: 9113, epoch: 276, batch: 4, loss: 0.01086561381816864, acc: 100.0, f1: 100.0, r: 0.6909387765161685
06/02/2019 09:45:06 step: 9118, epoch: 276, batch: 9, loss: 0.06558330357074738, acc: 96.875, f1: 88.75, r: 0.756383878531457
06/02/2019 09:45:07 step: 9123, epoch: 276, batch: 14, loss: 0.048004135489463806, acc: 96.875, f1: 92.43493348756509, r: 0.6539944402271575
06/02/2019 09:45:07 step: 9128, epoch: 276, batch: 19, loss: 0.08884726464748383, acc: 96.875, f1: 92.17543859649123, r: 0.7919989931532323
06/02/2019 09:45:08 step: 9133, epoch: 276, batch: 24, loss: 0.012961812317371368, acc: 100.0, f1: 100.0, r: 0.7658498585012009
06/02/2019 09:45:08 step: 9138, epoch: 276, batch: 29, loss: 0.15595461428165436, acc: 93.75, f1: 82.35836267751162, r: 0.7671933428676079
06/02/2019 09:45:08 *** evaluating ***
06/02/2019 09:45:09 step: 277, epoch: 276, acc: 59.401709401709404, f1: 22.882016132880135, r: 0.2317910994886268
06/02/2019 09:45:09 *** epoch: 278 ***
06/02/2019 09:45:09 *** training ***
06/02/2019 09:45:09 step: 9146, epoch: 277, batch: 4, loss: 0.054264284670352936, acc: 98.4375, f1: 98.02102659245516, r: 0.6873047904628029
06/02/2019 09:45:10 step: 9151, epoch: 277, batch: 9, loss: 0.02215651422739029, acc: 100.0, f1: 100.0, r: 0.7989884378092147
06/02/2019 09:45:10 step: 9156, epoch: 277, batch: 14, loss: 0.08787363022565842, acc: 96.875, f1: 88.91941391941391, r: 0.7204578728459957
06/02/2019 09:45:11 step: 9161, epoch: 277, batch: 19, loss: 0.007643520832061768, acc: 100.0, f1: 100.0, r: 0.7227463487122533
06/02/2019 09:45:11 step: 9166, epoch: 277, batch: 24, loss: 0.06565780937671661, acc: 96.875, f1: 91.66666666666666, r: 0.7084475352297736
06/02/2019 09:45:12 step: 9171, epoch: 277, batch: 29, loss: 0.011157356202602386, acc: 100.0, f1: 100.0, r: 0.7612594058494357
06/02/2019 09:45:12 *** evaluating ***
06/02/2019 09:45:12 step: 278, epoch: 277, acc: 58.54700854700855, f1: 21.595812119397024, r: 0.21862588830134977
06/02/2019 09:45:12 *** epoch: 279 ***
06/02/2019 09:45:12 *** training ***
06/02/2019 09:45:13 step: 9179, epoch: 278, batch: 4, loss: 0.01667541265487671, acc: 100.0, f1: 100.0, r: 0.7128189792168445
06/02/2019 09:45:13 step: 9184, epoch: 278, batch: 9, loss: 0.13993072509765625, acc: 93.75, f1: 77.91666666666666, r: 0.7069971295980579
06/02/2019 09:45:14 step: 9189, epoch: 278, batch: 14, loss: 0.01816808432340622, acc: 98.4375, f1: 99.05329593267882, r: 0.7557006751479703
06/02/2019 09:45:14 step: 9194, epoch: 278, batch: 19, loss: 0.00819530338048935, acc: 100.0, f1: 100.0, r: 0.6900552265917227
06/02/2019 09:45:15 step: 9199, epoch: 278, batch: 24, loss: 0.02066279947757721, acc: 100.0, f1: 100.0, r: 0.7731912651055777
06/02/2019 09:45:15 step: 9204, epoch: 278, batch: 29, loss: 0.01998145878314972, acc: 100.0, f1: 100.0, r: 0.7494923068562032
06/02/2019 09:45:15 *** evaluating ***
06/02/2019 09:45:16 step: 279, epoch: 278, acc: 59.401709401709404, f1: 27.400150099193954, r: 0.26218560693989124
06/02/2019 09:45:16 *** epoch: 280 ***
06/02/2019 09:45:16 *** training ***
06/02/2019 09:45:16 step: 9212, epoch: 279, batch: 4, loss: 0.009029999375343323, acc: 100.0, f1: 100.0, r: 0.835060702255161
06/02/2019 09:45:17 step: 9217, epoch: 279, batch: 9, loss: 0.08147925138473511, acc: 96.875, f1: 84.375, r: 0.8088320540488136
06/02/2019 09:45:17 step: 9222, epoch: 279, batch: 14, loss: 0.027061790227890015, acc: 98.4375, f1: 97.86096256684492, r: 0.70527816348848
06/02/2019 09:45:18 step: 9227, epoch: 279, batch: 19, loss: 0.06511230021715164, acc: 98.4375, f1: 94.28571428571428, r: 0.6762407469595183
06/02/2019 09:45:18 step: 9232, epoch: 279, batch: 24, loss: 0.029229938983917236, acc: 98.4375, f1: 96.8602825745683, r: 0.6798578248885981
06/02/2019 09:45:19 step: 9237, epoch: 279, batch: 29, loss: 0.04801919311285019, acc: 96.875, f1: 95.14581026208934, r: 0.7435586516388697
06/02/2019 09:45:19 *** evaluating ***
06/02/2019 09:45:19 step: 280, epoch: 279, acc: 57.692307692307686, f1: 22.095721681843212, r: 0.24052217561334588
06/02/2019 09:45:19 *** epoch: 281 ***
06/02/2019 09:45:19 *** training ***
06/02/2019 09:45:20 step: 9245, epoch: 280, batch: 4, loss: 0.028014764189720154, acc: 100.0, f1: 100.0, r: 0.6899562295681041
06/02/2019 09:45:20 step: 9250, epoch: 280, batch: 9, loss: 0.08612466603517532, acc: 96.875, f1: 91.68934240362812, r: 0.817703191332414
06/02/2019 09:45:21 step: 9255, epoch: 280, batch: 14, loss: 0.014480575919151306, acc: 100.0, f1: 100.0, r: 0.751147639521116
06/02/2019 09:45:21 step: 9260, epoch: 280, batch: 19, loss: 0.005063489079475403, acc: 100.0, f1: 100.0, r: 0.8102431140412387
06/02/2019 09:45:22 step: 9265, epoch: 280, batch: 24, loss: 0.05365917086601257, acc: 98.4375, f1: 84.12698412698413, r: 0.6686599539183766
06/02/2019 09:45:22 step: 9270, epoch: 280, batch: 29, loss: 0.004845224320888519, acc: 100.0, f1: 100.0, r: 0.7133246487001004
06/02/2019 09:45:23 *** evaluating ***
06/02/2019 09:45:23 step: 281, epoch: 280, acc: 58.97435897435898, f1: 22.785438671361, r: 0.2504665119442183
06/02/2019 09:45:23 *** epoch: 282 ***
06/02/2019 09:45:23 *** training ***
06/02/2019 09:45:23 step: 9278, epoch: 281, batch: 4, loss: 0.026782408356666565, acc: 98.4375, f1: 99.02597402597402, r: 0.7493201192306931
06/02/2019 09:45:24 step: 9283, epoch: 281, batch: 9, loss: 0.024126581847667694, acc: 100.0, f1: 100.0, r: 0.6963943018557317
06/02/2019 09:45:24 step: 9288, epoch: 281, batch: 14, loss: 0.007968246936798096, acc: 100.0, f1: 100.0, r: 0.769835475399465
06/02/2019 09:45:25 step: 9293, epoch: 281, batch: 19, loss: 0.03778032213449478, acc: 98.4375, f1: 97.3076923076923, r: 0.8004032375257542
06/02/2019 09:45:25 step: 9298, epoch: 281, batch: 24, loss: 0.0162833109498024, acc: 100.0, f1: 100.0, r: 0.7645538315699371
06/02/2019 09:45:26 step: 9303, epoch: 281, batch: 29, loss: 0.017410945147275925, acc: 100.0, f1: 100.0, r: 0.7130772503305614
06/02/2019 09:45:26 *** evaluating ***
06/02/2019 09:45:27 step: 282, epoch: 281, acc: 58.119658119658126, f1: 22.524697710627787, r: 0.24610906716704062
06/02/2019 09:45:27 *** epoch: 283 ***
06/02/2019 09:45:27 *** training ***
06/02/2019 09:45:27 step: 9311, epoch: 282, batch: 4, loss: 0.010426506400108337, acc: 100.0, f1: 100.0, r: 0.6708579749667866
06/02/2019 09:45:28 step: 9316, epoch: 282, batch: 9, loss: 0.0010590925812721252, acc: 100.0, f1: 100.0, r: 0.7861004469283986
06/02/2019 09:45:28 step: 9321, epoch: 282, batch: 14, loss: 0.006697893142700195, acc: 100.0, f1: 100.0, r: 0.6913802420476448
06/02/2019 09:45:29 step: 9326, epoch: 282, batch: 19, loss: 0.01523241400718689, acc: 100.0, f1: 100.0, r: 0.8238547090861336
06/02/2019 09:45:29 step: 9331, epoch: 282, batch: 24, loss: 0.09287888556718826, acc: 96.875, f1: 92.75132275132275, r: 0.8288080500109505
06/02/2019 09:45:30 step: 9336, epoch: 282, batch: 29, loss: 0.0572393536567688, acc: 98.4375, f1: 98.38056680161944, r: 0.7634976179363954
06/02/2019 09:45:30 *** evaluating ***
06/02/2019 09:45:30 step: 283, epoch: 282, acc: 57.26495726495726, f1: 22.149597167693372, r: 0.22872715634166724
06/02/2019 09:45:30 *** epoch: 284 ***
06/02/2019 09:45:30 *** training ***
06/02/2019 09:45:31 step: 9344, epoch: 283, batch: 4, loss: 0.01284695416688919, acc: 100.0, f1: 100.0, r: 0.8022434926003155
06/02/2019 09:45:31 step: 9349, epoch: 283, batch: 9, loss: 0.006483793258666992, acc: 100.0, f1: 100.0, r: 0.7415493132106924
06/02/2019 09:45:32 step: 9354, epoch: 283, batch: 14, loss: 0.04205326735973358, acc: 98.4375, f1: 97.72727272727273, r: 0.7740365444925793
06/02/2019 09:45:33 step: 9359, epoch: 283, batch: 19, loss: 0.03773421794176102, acc: 98.4375, f1: 99.01392826328116, r: 0.7491350446604353
06/02/2019 09:45:33 step: 9364, epoch: 283, batch: 24, loss: 0.1122952476143837, acc: 95.3125, f1: 95.87966499731205, r: 0.6725231853227237
06/02/2019 09:45:34 step: 9369, epoch: 283, batch: 29, loss: 0.022737104445695877, acc: 98.4375, f1: 96.30252100840336, r: 0.6658495049631905
06/02/2019 09:45:34 *** evaluating ***
06/02/2019 09:45:34 step: 284, epoch: 283, acc: 59.401709401709404, f1: 22.901549468878287, r: 0.24801505551696176
06/02/2019 09:45:34 *** epoch: 285 ***
06/02/2019 09:45:34 *** training ***
06/02/2019 09:45:35 step: 9377, epoch: 284, batch: 4, loss: 0.06741490215063095, acc: 98.4375, f1: 97.90940766550523, r: 0.7770894408218046
06/02/2019 09:45:35 step: 9382, epoch: 284, batch: 9, loss: 0.015622079372406006, acc: 100.0, f1: 100.0, r: 0.8343373764300334
06/02/2019 09:45:36 step: 9387, epoch: 284, batch: 14, loss: 0.024531591683626175, acc: 100.0, f1: 100.0, r: 0.7050958025007721
06/02/2019 09:45:36 step: 9392, epoch: 284, batch: 19, loss: 0.013433840125799179, acc: 100.0, f1: 100.0, r: 0.7443644570562442
06/02/2019 09:45:37 step: 9397, epoch: 284, batch: 24, loss: 0.09168460965156555, acc: 98.4375, f1: 99.17935428139509, r: 0.6638474272196941
06/02/2019 09:45:37 step: 9402, epoch: 284, batch: 29, loss: 0.013203680515289307, acc: 100.0, f1: 100.0, r: 0.801495253553605
06/02/2019 09:45:38 *** evaluating ***
06/02/2019 09:45:38 step: 285, epoch: 284, acc: 58.54700854700855, f1: 21.647500082614588, r: 0.23176453828099175
06/02/2019 09:45:38 *** epoch: 286 ***
06/02/2019 09:45:38 *** training ***
06/02/2019 09:45:38 step: 9410, epoch: 285, batch: 4, loss: 0.09107043594121933, acc: 98.4375, f1: 99.18546365914787, r: 0.778656756930328
06/02/2019 09:45:39 step: 9415, epoch: 285, batch: 9, loss: 0.021518245339393616, acc: 100.0, f1: 100.0, r: 0.707264717657178
06/02/2019 09:45:39 step: 9420, epoch: 285, batch: 14, loss: 0.014261990785598755, acc: 100.0, f1: 100.0, r: 0.7969614564115279
06/02/2019 09:45:40 step: 9425, epoch: 285, batch: 19, loss: 0.010780014097690582, acc: 100.0, f1: 100.0, r: 0.7114106613743724
06/02/2019 09:45:41 step: 9430, epoch: 285, batch: 24, loss: 0.039657339453697205, acc: 98.4375, f1: 86.11111111111111, r: 0.6892031958237549
06/02/2019 09:45:41 step: 9435, epoch: 285, batch: 29, loss: 0.0385698601603508, acc: 98.4375, f1: 99.08733679807327, r: 0.6900462702369837
06/02/2019 09:45:41 *** evaluating ***
06/02/2019 09:45:42 step: 286, epoch: 285, acc: 56.41025641025641, f1: 22.353849107881366, r: 0.21822605724114474
06/02/2019 09:45:42 *** epoch: 287 ***
06/02/2019 09:45:42 *** training ***
06/02/2019 09:45:42 step: 9443, epoch: 286, batch: 4, loss: 0.007854476571083069, acc: 100.0, f1: 100.0, r: 0.8361055981211636
06/02/2019 09:45:43 step: 9448, epoch: 286, batch: 9, loss: 0.07089537382125854, acc: 98.4375, f1: 86.11111111111111, r: 0.7878669976463091
06/02/2019 09:45:43 step: 9453, epoch: 286, batch: 14, loss: 0.0030930638313293457, acc: 100.0, f1: 100.0, r: 0.7268002378382991
06/02/2019 09:45:44 step: 9458, epoch: 286, batch: 19, loss: 0.008544191718101501, acc: 100.0, f1: 100.0, r: 0.7169894374601302
06/02/2019 09:45:44 step: 9463, epoch: 286, batch: 24, loss: 0.007059279829263687, acc: 100.0, f1: 100.0, r: 0.7592240648738802
06/02/2019 09:45:45 step: 9468, epoch: 286, batch: 29, loss: 0.016448330134153366, acc: 100.0, f1: 100.0, r: 0.7662559537405993
06/02/2019 09:45:45 *** evaluating ***
06/02/2019 09:45:45 step: 287, epoch: 286, acc: 56.41025641025641, f1: 21.29606829188213, r: 0.21901581372013343
06/02/2019 09:45:45 *** epoch: 288 ***
06/02/2019 09:45:45 *** training ***
06/02/2019 09:45:46 step: 9476, epoch: 287, batch: 4, loss: 0.0027731508016586304, acc: 100.0, f1: 100.0, r: 0.618120177325511
06/02/2019 09:45:46 step: 9481, epoch: 287, batch: 9, loss: 0.00769834965467453, acc: 100.0, f1: 100.0, r: 0.6107391729475969
06/02/2019 09:45:47 step: 9486, epoch: 287, batch: 14, loss: 0.03176714479923248, acc: 96.875, f1: 79.43722943722943, r: 0.49579840930037056
06/02/2019 09:45:47 step: 9491, epoch: 287, batch: 19, loss: 0.006943225860595703, acc: 100.0, f1: 100.0, r: 0.7359762589521914
06/02/2019 09:45:48 step: 9496, epoch: 287, batch: 24, loss: 0.05844532698392868, acc: 96.875, f1: 94.85029463290333, r: 0.7284329362625438
06/02/2019 09:45:48 step: 9501, epoch: 287, batch: 29, loss: 0.04046160727739334, acc: 98.4375, f1: 97.78325123152709, r: 0.8041513476836356
06/02/2019 09:45:48 *** evaluating ***
06/02/2019 09:45:49 step: 288, epoch: 287, acc: 56.837606837606835, f1: 22.170299203331524, r: 0.21718665682361096
06/02/2019 09:45:49 *** epoch: 289 ***
06/02/2019 09:45:49 *** training ***
06/02/2019 09:45:49 step: 9509, epoch: 288, batch: 4, loss: 0.1422409862279892, acc: 96.875, f1: 93.63544257161278, r: 0.6665282089654082
06/02/2019 09:45:50 step: 9514, epoch: 288, batch: 9, loss: 0.009517468512058258, acc: 100.0, f1: 100.0, r: 0.6348143328891138
06/02/2019 09:45:50 step: 9519, epoch: 288, batch: 14, loss: 0.01259804517030716, acc: 100.0, f1: 100.0, r: 0.6561986521023035
06/02/2019 09:45:51 step: 9524, epoch: 288, batch: 19, loss: 0.00783514603972435, acc: 100.0, f1: 100.0, r: 0.6783995597314906
06/02/2019 09:45:51 step: 9529, epoch: 288, batch: 24, loss: 0.0042255595326423645, acc: 100.0, f1: 100.0, r: 0.7378887292296669
06/02/2019 09:45:52 step: 9534, epoch: 288, batch: 29, loss: 0.01034456491470337, acc: 100.0, f1: 100.0, r: 0.8069343484890072
06/02/2019 09:45:52 *** evaluating ***
06/02/2019 09:45:52 step: 289, epoch: 288, acc: 58.97435897435898, f1: 23.47709079295163, r: 0.2312260546457561
06/02/2019 09:45:52 *** epoch: 290 ***
06/02/2019 09:45:52 *** training ***
06/02/2019 09:45:53 step: 9542, epoch: 289, batch: 4, loss: 0.004080496728420258, acc: 100.0, f1: 100.0, r: 0.738910936179456
06/02/2019 09:45:53 step: 9547, epoch: 289, batch: 9, loss: 0.010849330574274063, acc: 100.0, f1: 100.0, r: 0.7657832972383456
06/02/2019 09:45:54 step: 9552, epoch: 289, batch: 14, loss: 0.050983183085918427, acc: 98.4375, f1: 99.22222222222223, r: 0.761721427697085
06/02/2019 09:45:54 step: 9557, epoch: 289, batch: 19, loss: 0.040978431701660156, acc: 96.875, f1: 91.65615713560919, r: 0.6290800876719976
06/02/2019 09:45:55 step: 9562, epoch: 289, batch: 24, loss: 0.006179191172122955, acc: 100.0, f1: 100.0, r: 0.6662904092276903
06/02/2019 09:45:55 step: 9567, epoch: 289, batch: 29, loss: 0.025880873203277588, acc: 98.4375, f1: 98.20868786386028, r: 0.6895045057044986
06/02/2019 09:45:56 *** evaluating ***
06/02/2019 09:45:56 step: 290, epoch: 289, acc: 57.26495726495726, f1: 22.20890718453756, r: 0.22532641852291488
06/02/2019 09:45:56 *** epoch: 291 ***
06/02/2019 09:45:56 *** training ***
06/02/2019 09:45:56 step: 9575, epoch: 290, batch: 4, loss: 0.006113715469837189, acc: 100.0, f1: 100.0, r: 0.724847088082144
06/02/2019 09:45:57 step: 9580, epoch: 290, batch: 9, loss: 0.05082404986023903, acc: 98.4375, f1: 98.12987012987013, r: 0.6830706199460806
06/02/2019 09:45:57 step: 9585, epoch: 290, batch: 14, loss: 0.018023371696472168, acc: 100.0, f1: 100.0, r: 0.6516082882665217
06/02/2019 09:45:58 step: 9590, epoch: 290, batch: 19, loss: 0.007319986820220947, acc: 100.0, f1: 100.0, r: 0.7081132548353875
06/02/2019 09:45:58 step: 9595, epoch: 290, batch: 24, loss: 0.053007081151008606, acc: 96.875, f1: 95.00610500610502, r: 0.6990135572922813
06/02/2019 09:45:59 step: 9600, epoch: 290, batch: 29, loss: 0.03539768606424332, acc: 98.4375, f1: 97.27891156462584, r: 0.6810805821146415
06/02/2019 09:45:59 *** evaluating ***
06/02/2019 09:46:00 step: 291, epoch: 290, acc: 59.401709401709404, f1: 22.855058247719924, r: 0.23094693215474169
06/02/2019 09:46:00 *** epoch: 292 ***
06/02/2019 09:46:00 *** training ***
06/02/2019 09:46:00 step: 9608, epoch: 291, batch: 4, loss: 0.012155421078205109, acc: 100.0, f1: 100.0, r: 0.7806664303049955
06/02/2019 09:46:01 step: 9613, epoch: 291, batch: 9, loss: 0.011216789484024048, acc: 100.0, f1: 100.0, r: 0.6215003212952716
06/02/2019 09:46:01 step: 9618, epoch: 291, batch: 14, loss: 0.01856779307126999, acc: 100.0, f1: 100.0, r: 0.7128251904367755
06/02/2019 09:46:02 step: 9623, epoch: 291, batch: 19, loss: 0.0053511857986450195, acc: 100.0, f1: 100.0, r: 0.7518953215204331
06/02/2019 09:46:02 step: 9628, epoch: 291, batch: 24, loss: 0.047142744064331055, acc: 98.4375, f1: 99.08733679807327, r: 0.7062346286752673
06/02/2019 09:46:03 step: 9633, epoch: 291, batch: 29, loss: 0.012852877378463745, acc: 100.0, f1: 100.0, r: 0.787248367582427
06/02/2019 09:46:03 *** evaluating ***
06/02/2019 09:46:03 step: 292, epoch: 291, acc: 58.54700854700855, f1: 22.648185148185146, r: 0.23975769669222405
06/02/2019 09:46:03 *** epoch: 293 ***
06/02/2019 09:46:03 *** training ***
06/02/2019 09:46:04 step: 9641, epoch: 292, batch: 4, loss: 0.003758341073989868, acc: 100.0, f1: 100.0, r: 0.766810481927026
06/02/2019 09:46:04 step: 9646, epoch: 292, batch: 9, loss: 0.008797235786914825, acc: 100.0, f1: 100.0, r: 0.724725298608865
06/02/2019 09:46:05 step: 9651, epoch: 292, batch: 14, loss: 0.010125286877155304, acc: 100.0, f1: 100.0, r: 0.7284952328698195
06/02/2019 09:46:06 step: 9656, epoch: 292, batch: 19, loss: 0.01272139698266983, acc: 100.0, f1: 100.0, r: 0.7034164156917653
06/02/2019 09:46:06 step: 9661, epoch: 292, batch: 24, loss: 0.025262415409088135, acc: 98.4375, f1: 99.11483253588517, r: 0.759329479787794
06/02/2019 09:46:07 step: 9666, epoch: 292, batch: 29, loss: 0.04131974279880524, acc: 98.4375, f1: 86.36363636363636, r: 0.8385403660191352
06/02/2019 09:46:07 *** evaluating ***
06/02/2019 09:46:07 step: 293, epoch: 292, acc: 58.97435897435898, f1: 22.73939461439461, r: 0.22915264806027316
06/02/2019 09:46:07 *** epoch: 294 ***
06/02/2019 09:46:07 *** training ***
06/02/2019 09:46:08 step: 9674, epoch: 293, batch: 4, loss: 0.0022014155983924866, acc: 100.0, f1: 100.0, r: 0.7117525323777774
06/02/2019 09:46:08 step: 9679, epoch: 293, batch: 9, loss: 0.05931754782795906, acc: 96.875, f1: 96.8920901694011, r: 0.6973050943633962
06/02/2019 09:46:09 step: 9684, epoch: 293, batch: 14, loss: 0.031245242804288864, acc: 98.4375, f1: 95.0, r: 0.7299036842895331
06/02/2019 09:46:09 step: 9689, epoch: 293, batch: 19, loss: 0.01062978059053421, acc: 100.0, f1: 100.0, r: 0.6493579450865219
06/02/2019 09:46:10 step: 9694, epoch: 293, batch: 24, loss: 0.020003870129585266, acc: 100.0, f1: 100.0, r: 0.7607186898932754
06/02/2019 09:46:11 step: 9699, epoch: 293, batch: 29, loss: 0.0006912872195243835, acc: 100.0, f1: 100.0, r: 0.7518002006052554
06/02/2019 09:46:11 *** evaluating ***
06/02/2019 09:46:11 step: 294, epoch: 293, acc: 58.54700854700855, f1: 23.008868336400177, r: 0.2301322615530652
06/02/2019 09:46:11 *** epoch: 295 ***
06/02/2019 09:46:11 *** training ***
06/02/2019 09:46:12 step: 9707, epoch: 294, batch: 4, loss: 0.08158349245786667, acc: 96.875, f1: 96.23903508771929, r: 0.6882313579295781
06/02/2019 09:46:12 step: 9712, epoch: 294, batch: 9, loss: 0.018955305218696594, acc: 98.4375, f1: 99.09700722394221, r: 0.8397506741068631
06/02/2019 09:46:13 step: 9717, epoch: 294, batch: 14, loss: 0.016046598553657532, acc: 98.4375, f1: 99.2755543775952, r: 0.7379083141050253
06/02/2019 09:46:13 step: 9722, epoch: 294, batch: 19, loss: 0.06394970417022705, acc: 96.875, f1: 96.70285135330728, r: 0.7128926321782537
06/02/2019 09:46:14 step: 9727, epoch: 294, batch: 24, loss: 0.026032190769910812, acc: 100.0, f1: 100.0, r: 0.7216876906869139
06/02/2019 09:46:14 step: 9732, epoch: 294, batch: 29, loss: 0.017801202833652496, acc: 100.0, f1: 100.0, r: 0.8017391859157654
06/02/2019 09:46:15 *** evaluating ***
06/02/2019 09:46:15 step: 295, epoch: 294, acc: 58.97435897435898, f1: 21.483856806802034, r: 0.22020188328759113
06/02/2019 09:46:15 *** epoch: 296 ***
06/02/2019 09:46:15 *** training ***
06/02/2019 09:46:15 step: 9740, epoch: 295, batch: 4, loss: 0.00831981748342514, acc: 100.0, f1: 100.0, r: 0.8011506520652358
06/02/2019 09:46:16 step: 9745, epoch: 295, batch: 9, loss: 0.009231366217136383, acc: 100.0, f1: 100.0, r: 0.5185820985204738
06/02/2019 09:46:16 step: 9750, epoch: 295, batch: 14, loss: 0.03775731846690178, acc: 98.4375, f1: 99.17516324894031, r: 0.7050436848149049
06/02/2019 09:46:17 step: 9755, epoch: 295, batch: 19, loss: 0.15070220828056335, acc: 93.75, f1: 89.90794143744453, r: 0.7217581581739003
06/02/2019 09:46:18 step: 9760, epoch: 295, batch: 24, loss: 0.007954992353916168, acc: 100.0, f1: 100.0, r: 0.7732890264254194
06/02/2019 09:46:18 step: 9765, epoch: 295, batch: 29, loss: 0.04904210567474365, acc: 98.4375, f1: 99.26415094339622, r: 0.8062063976216635
06/02/2019 09:46:18 *** evaluating ***
06/02/2019 09:46:19 step: 296, epoch: 295, acc: 58.119658119658126, f1: 21.95559845559846, r: 0.2278962854425578
06/02/2019 09:46:19 *** epoch: 297 ***
06/02/2019 09:46:19 *** training ***
06/02/2019 09:46:19 step: 9773, epoch: 296, batch: 4, loss: 0.016632743179798126, acc: 100.0, f1: 100.0, r: 0.7123076027614226
06/02/2019 09:46:20 step: 9778, epoch: 296, batch: 9, loss: 0.03180166333913803, acc: 98.4375, f1: 98.35600907029477, r: 0.8285431727660972
06/02/2019 09:46:20 step: 9783, epoch: 296, batch: 14, loss: 0.08215907216072083, acc: 96.875, f1: 92.09677419354838, r: 0.7631266102569118
06/02/2019 09:46:21 step: 9788, epoch: 296, batch: 19, loss: 0.016161009669303894, acc: 100.0, f1: 100.0, r: 0.7054959476073859
06/02/2019 09:46:21 step: 9793, epoch: 296, batch: 24, loss: 0.05354778468608856, acc: 96.875, f1: 90.88733934919333, r: 0.6726500545401881
06/02/2019 09:46:22 step: 9798, epoch: 296, batch: 29, loss: 0.012048929929733276, acc: 100.0, f1: 100.0, r: 0.7818216988531709
06/02/2019 09:46:22 *** evaluating ***
06/02/2019 09:46:22 step: 297, epoch: 296, acc: 58.119658119658126, f1: 21.92182563835029, r: 0.24230200548216763
06/02/2019 09:46:22 *** epoch: 298 ***
06/02/2019 09:46:22 *** training ***
06/02/2019 09:46:23 step: 9806, epoch: 297, batch: 4, loss: 0.03575252369046211, acc: 98.4375, f1: 97.67080745341615, r: 0.7984375222270872
06/02/2019 09:46:23 step: 9811, epoch: 297, batch: 9, loss: 0.047944679856300354, acc: 98.4375, f1: 97.25274725274726, r: 0.7995764297225357
06/02/2019 09:46:24 step: 9816, epoch: 297, batch: 14, loss: 0.015791915357112885, acc: 100.0, f1: 100.0, r: 0.8111250179078274
06/02/2019 09:46:24 step: 9821, epoch: 297, batch: 19, loss: 0.03791534900665283, acc: 98.4375, f1: 95.59748427672956, r: 0.7161448596024671
06/02/2019 09:46:25 step: 9826, epoch: 297, batch: 24, loss: 0.0037511438131332397, acc: 100.0, f1: 100.0, r: 0.7807522996371388
06/02/2019 09:46:26 step: 9831, epoch: 297, batch: 29, loss: 0.012813039124011993, acc: 100.0, f1: 100.0, r: 0.6955924109274421
06/02/2019 09:46:26 *** evaluating ***
06/02/2019 09:46:26 step: 298, epoch: 297, acc: 58.97435897435898, f1: 21.714165464165465, r: 0.22344291164105723
06/02/2019 09:46:26 *** epoch: 299 ***
06/02/2019 09:46:26 *** training ***
06/02/2019 09:46:27 step: 9839, epoch: 298, batch: 4, loss: 0.026099851354956627, acc: 100.0, f1: 100.0, r: 0.7622161860360139
06/02/2019 09:46:27 step: 9844, epoch: 298, batch: 9, loss: 0.008228302001953125, acc: 100.0, f1: 100.0, r: 0.5990176407693761
06/02/2019 09:46:28 step: 9849, epoch: 298, batch: 14, loss: 0.0017543435096740723, acc: 100.0, f1: 100.0, r: 0.7133430173221588
06/02/2019 09:46:28 step: 9854, epoch: 298, batch: 19, loss: 0.05144454538822174, acc: 98.4375, f1: 96.95652173913044, r: 0.8148260671377333
06/02/2019 09:46:29 step: 9859, epoch: 298, batch: 24, loss: 0.011658802628517151, acc: 100.0, f1: 100.0, r: 0.7330180354479308
06/02/2019 09:46:29 step: 9864, epoch: 298, batch: 29, loss: 0.003736138343811035, acc: 100.0, f1: 100.0, r: 0.6757499921758751
06/02/2019 09:46:30 *** evaluating ***
06/02/2019 09:46:30 step: 299, epoch: 298, acc: 56.41025641025641, f1: 21.3214787634481, r: 0.19894483990916118
06/02/2019 09:46:30 *** epoch: 300 ***
06/02/2019 09:46:30 *** training ***
06/02/2019 09:46:30 step: 9872, epoch: 299, batch: 4, loss: 0.023603640496730804, acc: 100.0, f1: 100.0, r: 0.7924703388721572
06/02/2019 09:46:31 step: 9877, epoch: 299, batch: 9, loss: 0.04785212129354477, acc: 98.4375, f1: 83.33333333333333, r: 0.6159799190754097
06/02/2019 09:46:31 step: 9882, epoch: 299, batch: 14, loss: 0.04394166171550751, acc: 96.875, f1: 94.54580745341615, r: 0.7323230531963149
06/02/2019 09:46:32 step: 9887, epoch: 299, batch: 19, loss: 0.00924035906791687, acc: 100.0, f1: 100.0, r: 0.6094761591707599
06/02/2019 09:46:33 step: 9892, epoch: 299, batch: 24, loss: 0.02813606709241867, acc: 98.4375, f1: 99.33081674673987, r: 0.7889019784628983
06/02/2019 09:46:33 step: 9897, epoch: 299, batch: 29, loss: 0.05007169768214226, acc: 96.875, f1: 92.63917369180528, r: 0.6374361472293822
06/02/2019 09:46:33 *** evaluating ***
06/02/2019 09:46:34 step: 300, epoch: 299, acc: 58.119658119658126, f1: 21.86555985911079, r: 0.2107032481442666
06/02/2019 09:46:34 
*** Best acc model ***
epoch: 258
acc: 60.256410256410255
f1: 24.245545152521895
corr: 0.256512171579967
06/02/2019 09:46:34 Loading Test Data
06/02/2019 09:46:34 load data from data/elmo_temp/test_text.npy, data/elmo_temp/test_label.npy, training: False
06/02/2019 09:47:31 loaded. total len: 2228
06/02/2019 09:47:31 Test: length: 2228, total batch: 35, batch size: 64
06/02/2019 09:47:31 
*** Test Result ***
acc: 58.119658119658126
f1: 21.86555985911079
corr: 0.2107032481442666
