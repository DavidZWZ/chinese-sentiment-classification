06/02/2019 09:12:08 {'input_path': 'data/word2vec_temp', 'output_path': 'save/cnn_4', 'gpu': True, 'seed': 20000125, 'display_per_batch': 5, 'optimizer': 'adagrad', 'lr': 0.01, 'lr_decay': 0, 'weight_decay': 0.0001, 'momentum': 0.985, 'num_epochs': 300, 'batch_size': 64, 'num_labels': 8, 'embedding_size': 300, 'type': 'cnn', 'cnn': {'max_length': 512, 'conv_1': {'size': 512, 'kernel_size': 3, 'dropout': 0.9}, 'max_pool_1': {'kernel_size': 2, 'stride': 2}, 'fc': {'hidden_size': 2000, 'dropout': 0.9}, 'loss': 'cross_entropy'}}
06/02/2019 09:12:08 Loading Train Data
06/02/2019 09:12:08 load data from data/word2vec_temp/train_text.npy, data/word2vec_temp/train_label.npy, training: True
06/02/2019 09:12:33 loaded. total len: 2342
06/02/2019 09:12:33 Train: length: 2108, total batch: 33, batch size: 64
06/02/2019 09:12:33 Dev: length: 234, total batch: 4, batch size: 64
06/02/2019 09:12:33 Loading model cnn
06/02/2019 09:13:01 *** epoch: 1 ***
06/02/2019 09:13:01 *** training ***
06/02/2019 09:13:02 step: 5, epoch: 0, batch: 4, loss: 2.624959707260132, acc: 21.875, f1: 13.11974900825075, r: 0.004764067030264066
06/02/2019 09:13:03 step: 10, epoch: 0, batch: 9, loss: 2.082308053970337, acc: 23.4375, f1: 10.468988638480162, r: -0.03807259003809179
06/02/2019 09:13:03 step: 15, epoch: 0, batch: 14, loss: 1.842531442642212, acc: 29.6875, f1: 13.26890756302521, r: 0.06161967872191994
06/02/2019 09:13:04 step: 20, epoch: 0, batch: 19, loss: 1.9120221138000488, acc: 28.125, f1: 10.509446693657221, r: -0.020210307469050327
06/02/2019 09:13:04 step: 25, epoch: 0, batch: 24, loss: 1.7985514402389526, acc: 29.6875, f1: 9.441262962389725, r: -0.009551886334553542
06/02/2019 09:13:05 step: 30, epoch: 0, batch: 29, loss: 1.611778974533081, acc: 42.1875, f1: 15.041339612768184, r: 0.05646563174447891
06/02/2019 09:13:05 *** evaluating ***
06/02/2019 09:13:05 step: 1, epoch: 0, acc: 45.72649572649573, f1: 9.291249596383597, r: 0.18185764057589404
06/02/2019 09:13:05 *** epoch: 2 ***
06/02/2019 09:13:05 *** training ***
06/02/2019 09:13:06 step: 38, epoch: 1, batch: 4, loss: 1.6968659162521362, acc: 40.625, f1: 16.757662835249043, r: 0.09731208752197128
06/02/2019 09:13:06 step: 43, epoch: 1, batch: 9, loss: 1.7359397411346436, acc: 34.375, f1: 15.370953720754152, r: 0.10964892707936093
06/02/2019 09:13:07 step: 48, epoch: 1, batch: 14, loss: 1.675607681274414, acc: 35.9375, f1: 13.823953823953824, r: 0.11878081705402141
06/02/2019 09:13:07 step: 53, epoch: 1, batch: 19, loss: 1.439700722694397, acc: 48.4375, f1: 17.190704032809297, r: 0.13562531141261597
06/02/2019 09:13:08 step: 58, epoch: 1, batch: 24, loss: 1.715087652206421, acc: 42.1875, f1: 20.186781609195403, r: 0.04926619185094466
06/02/2019 09:13:08 step: 63, epoch: 1, batch: 29, loss: 1.747899055480957, acc: 35.9375, f1: 12.830290010741138, r: 0.12698506840857487
06/02/2019 09:13:08 *** evaluating ***
06/02/2019 09:13:08 step: 2, epoch: 1, acc: 52.13675213675214, f1: 15.087657582041409, r: 0.2131087735700801
06/02/2019 09:13:08 *** epoch: 3 ***
06/02/2019 09:13:08 *** training ***
06/02/2019 09:13:09 step: 71, epoch: 2, batch: 4, loss: 1.5342178344726562, acc: 42.1875, f1: 16.530220315463435, r: 0.19341679439157206
06/02/2019 09:13:09 step: 76, epoch: 2, batch: 9, loss: 1.5802276134490967, acc: 42.1875, f1: 20.465072424481715, r: 0.1484340526439467
06/02/2019 09:13:10 step: 81, epoch: 2, batch: 14, loss: 1.6865850687026978, acc: 40.625, f1: 13.858695652173914, r: 0.21659540993341295
06/02/2019 09:13:10 step: 86, epoch: 2, batch: 19, loss: 1.642268180847168, acc: 40.625, f1: 15.871418990131266, r: 0.14828228256287748
06/02/2019 09:13:11 step: 91, epoch: 2, batch: 24, loss: 1.4942817687988281, acc: 45.3125, f1: 18.204481792717083, r: 0.12796445762230407
06/02/2019 09:13:11 step: 96, epoch: 2, batch: 29, loss: 1.613419771194458, acc: 45.3125, f1: 19.550092764378476, r: 0.16870045387040603
06/02/2019 09:13:12 *** evaluating ***
06/02/2019 09:13:12 step: 3, epoch: 2, acc: 55.55555555555556, f1: 16.53610079147149, r: 0.24467403171327773
06/02/2019 09:13:12 *** epoch: 4 ***
06/02/2019 09:13:12 *** training ***
06/02/2019 09:13:12 step: 104, epoch: 3, batch: 4, loss: 1.6052619218826294, acc: 46.875, f1: 18.42918985776129, r: 0.2475654930606051
06/02/2019 09:13:13 step: 109, epoch: 3, batch: 9, loss: 1.4742296934127808, acc: 51.5625, f1: 20.227038183694532, r: 0.19847035101232474
06/02/2019 09:13:13 step: 114, epoch: 3, batch: 14, loss: 1.4904108047485352, acc: 46.875, f1: 23.585017133404232, r: 0.21103298776841434
06/02/2019 09:13:14 step: 119, epoch: 3, batch: 19, loss: 1.8862483501434326, acc: 35.9375, f1: 13.194052324487107, r: 0.12513402786970784
06/02/2019 09:13:14 step: 124, epoch: 3, batch: 24, loss: 1.5592509508132935, acc: 45.3125, f1: 17.499167499167502, r: 0.28149249155740824
06/02/2019 09:13:15 step: 129, epoch: 3, batch: 29, loss: 1.6902685165405273, acc: 43.75, f1: 16.84453295206972, r: 0.21591651313125929
06/02/2019 09:13:15 *** evaluating ***
06/02/2019 09:13:15 step: 4, epoch: 3, acc: 55.55555555555556, f1: 17.661389705185325, r: 0.2539943584065803
06/02/2019 09:13:15 *** epoch: 5 ***
06/02/2019 09:13:15 *** training ***
06/02/2019 09:13:15 step: 137, epoch: 4, batch: 4, loss: 1.5661323070526123, acc: 45.3125, f1: 19.093627789279964, r: 0.1642306120343109
06/02/2019 09:13:16 step: 142, epoch: 4, batch: 9, loss: 1.6032819747924805, acc: 42.1875, f1: 20.343137254901958, r: 0.20885087656489337
06/02/2019 09:13:16 step: 147, epoch: 4, batch: 14, loss: 1.6353373527526855, acc: 37.5, f1: 15.27987263281381, r: 0.09835288175041856
06/02/2019 09:13:17 step: 152, epoch: 4, batch: 19, loss: 1.487126111984253, acc: 42.1875, f1: 14.791666666666664, r: 0.18072802720180606
06/02/2019 09:13:17 step: 157, epoch: 4, batch: 24, loss: 1.5276777744293213, acc: 51.5625, f1: 19.03846153846154, r: 0.3059951793296467
06/02/2019 09:13:18 step: 162, epoch: 4, batch: 29, loss: 1.4421343803405762, acc: 51.5625, f1: 15.52653399668325, r: 0.21201860666344707
06/02/2019 09:13:18 *** evaluating ***
06/02/2019 09:13:18 step: 5, epoch: 4, acc: 55.98290598290598, f1: 16.45183852917666, r: 0.26520456061853986
06/02/2019 09:13:18 *** epoch: 6 ***
06/02/2019 09:13:18 *** training ***
06/02/2019 09:13:19 step: 170, epoch: 5, batch: 4, loss: 1.6731613874435425, acc: 42.1875, f1: 14.691742081447964, r: 0.2383021706878736
06/02/2019 09:13:19 step: 175, epoch: 5, batch: 9, loss: 1.4310674667358398, acc: 51.5625, f1: 20.19902491150738, r: 0.2692376009813523
06/02/2019 09:13:19 step: 180, epoch: 5, batch: 14, loss: 1.4510496854782104, acc: 54.6875, f1: 22.50516368163427, r: 0.16157874505138678
06/02/2019 09:13:20 step: 185, epoch: 5, batch: 19, loss: 1.3569269180297852, acc: 43.75, f1: 16.296296296296298, r: 0.2031686472930088
06/02/2019 09:13:20 step: 190, epoch: 5, batch: 24, loss: 1.622968077659607, acc: 37.5, f1: 16.975142258161128, r: 0.16297814321345847
06/02/2019 09:13:21 step: 195, epoch: 5, batch: 29, loss: 1.4675511121749878, acc: 37.5, f1: 13.010410641989589, r: 0.21405693675398688
06/02/2019 09:13:21 *** evaluating ***
06/02/2019 09:13:21 step: 6, epoch: 5, acc: 55.98290598290598, f1: 17.1702694235589, r: 0.27579641369656294
06/02/2019 09:13:21 *** epoch: 7 ***
06/02/2019 09:13:21 *** training ***
06/02/2019 09:13:22 step: 203, epoch: 6, batch: 4, loss: 1.4671401977539062, acc: 48.4375, f1: 22.242043670615093, r: 0.2152422111795099
06/02/2019 09:13:22 step: 208, epoch: 6, batch: 9, loss: 1.3327990770339966, acc: 62.5, f1: 26.35836385836386, r: 0.26305612386849664
06/02/2019 09:13:23 step: 213, epoch: 6, batch: 14, loss: 1.7015937566757202, acc: 39.0625, f1: 19.420557002604685, r: 0.23924107030040598
06/02/2019 09:13:23 step: 218, epoch: 6, batch: 19, loss: 1.4769829511642456, acc: 46.875, f1: 20.478124704207154, r: 0.2782516908705827
06/02/2019 09:13:23 step: 223, epoch: 6, batch: 24, loss: 1.6915642023086548, acc: 46.875, f1: 21.526151444184233, r: 0.22936151654489234
06/02/2019 09:13:24 step: 228, epoch: 6, batch: 29, loss: 1.5720759630203247, acc: 48.4375, f1: 19.962209921226314, r: 0.1865614024742
06/02/2019 09:13:24 *** evaluating ***
06/02/2019 09:13:24 step: 7, epoch: 6, acc: 58.97435897435898, f1: 18.70849357292348, r: 0.2951553489775824
06/02/2019 09:13:24 *** epoch: 8 ***
06/02/2019 09:13:24 *** training ***
06/02/2019 09:13:25 step: 236, epoch: 7, batch: 4, loss: 1.4372539520263672, acc: 46.875, f1: 18.948412698412696, r: 0.3074703202343144
06/02/2019 09:13:25 step: 241, epoch: 7, batch: 9, loss: 1.334295392036438, acc: 48.4375, f1: 23.435498094810235, r: 0.3018660746554942
06/02/2019 09:13:26 step: 246, epoch: 7, batch: 14, loss: 1.5620770454406738, acc: 42.1875, f1: 20.463515109613112, r: 0.19290378045250015
06/02/2019 09:13:26 step: 251, epoch: 7, batch: 19, loss: 1.4520375728607178, acc: 48.4375, f1: 23.06667489499802, r: 0.20756999580096405
06/02/2019 09:13:27 step: 256, epoch: 7, batch: 24, loss: 1.600266456604004, acc: 45.3125, f1: 17.963386727688786, r: 0.19496648955929421
06/02/2019 09:13:27 step: 261, epoch: 7, batch: 29, loss: 1.638480305671692, acc: 43.75, f1: 19.70520421607378, r: 0.29721797059691324
06/02/2019 09:13:27 *** evaluating ***
06/02/2019 09:13:27 step: 8, epoch: 7, acc: 56.837606837606835, f1: 17.430622009569376, r: 0.28303411204391554
06/02/2019 09:13:27 *** epoch: 9 ***
06/02/2019 09:13:27 *** training ***
06/02/2019 09:13:28 step: 269, epoch: 8, batch: 4, loss: 1.315039873123169, acc: 56.25, f1: 25.498866213151928, r: 0.2559135439039788
06/02/2019 09:13:28 step: 274, epoch: 8, batch: 9, loss: 1.6686517000198364, acc: 46.875, f1: 18.23118279569892, r: 0.16662916862530097
06/02/2019 09:13:29 step: 279, epoch: 8, batch: 14, loss: 1.2457058429718018, acc: 60.9375, f1: 26.741467975689577, r: 0.28074725632559966
06/02/2019 09:13:29 step: 284, epoch: 8, batch: 19, loss: 1.302778720855713, acc: 64.0625, f1: 24.563860435339315, r: 0.2228414298863014
06/02/2019 09:13:30 step: 289, epoch: 8, batch: 24, loss: 1.3129653930664062, acc: 46.875, f1: 15.5, r: 0.26135741412580193
06/02/2019 09:13:30 step: 294, epoch: 8, batch: 29, loss: 1.5360019207000732, acc: 42.1875, f1: 17.80884135220681, r: 0.2731705378914087
06/02/2019 09:13:30 *** evaluating ***
06/02/2019 09:13:30 step: 9, epoch: 8, acc: 58.119658119658126, f1: 18.480130486358245, r: 0.3164420046837534
06/02/2019 09:13:30 *** epoch: 10 ***
06/02/2019 09:13:30 *** training ***
06/02/2019 09:13:31 step: 302, epoch: 9, batch: 4, loss: 1.3228542804718018, acc: 57.8125, f1: 24.116694899303596, r: 0.2348621640485058
06/02/2019 09:13:31 step: 307, epoch: 9, batch: 9, loss: 1.3718761205673218, acc: 48.4375, f1: 27.29095465937571, r: 0.404963601096759
06/02/2019 09:13:32 step: 312, epoch: 9, batch: 14, loss: 1.3453521728515625, acc: 54.6875, f1: 24.306824178946943, r: 0.3412754804406838
06/02/2019 09:13:32 step: 317, epoch: 9, batch: 19, loss: 1.2964363098144531, acc: 51.5625, f1: 21.73573975044563, r: 0.3336723633607561
06/02/2019 09:13:33 step: 322, epoch: 9, batch: 24, loss: 1.4487380981445312, acc: 40.625, f1: 15.86289726533629, r: 0.32300815690162255
06/02/2019 09:13:33 step: 327, epoch: 9, batch: 29, loss: 1.2447434663772583, acc: 56.25, f1: 21.578268783496814, r: 0.3625281254500422
06/02/2019 09:13:33 *** evaluating ***
06/02/2019 09:13:33 step: 10, epoch: 9, acc: 57.692307692307686, f1: 18.599032546400966, r: 0.3030614622210129
06/02/2019 09:13:33 *** epoch: 11 ***
06/02/2019 09:13:33 *** training ***
06/02/2019 09:13:34 step: 335, epoch: 10, batch: 4, loss: 1.2141236066818237, acc: 62.5, f1: 25.58983666061706, r: 0.2684561440650584
06/02/2019 09:13:34 step: 340, epoch: 10, batch: 9, loss: 1.4138977527618408, acc: 51.5625, f1: 20.052083333333332, r: 0.29993384576534
06/02/2019 09:13:35 step: 345, epoch: 10, batch: 14, loss: 1.1643282175064087, acc: 60.9375, f1: 30.25442557543284, r: 0.38880448799216993
06/02/2019 09:13:35 step: 350, epoch: 10, batch: 19, loss: 1.4334867000579834, acc: 56.25, f1: 27.01525054466231, r: 0.3146039568623803
06/02/2019 09:13:36 step: 355, epoch: 10, batch: 24, loss: 1.3437291383743286, acc: 56.25, f1: 20.962732919254655, r: 0.3317456673051746
06/02/2019 09:13:36 step: 360, epoch: 10, batch: 29, loss: 1.352380633354187, acc: 57.8125, f1: 30.819454218468994, r: 0.3329524861597517
06/02/2019 09:13:36 *** evaluating ***
06/02/2019 09:13:37 step: 11, epoch: 10, acc: 58.119658119658126, f1: 17.907307153630683, r: 0.303240229412314
06/02/2019 09:13:37 *** epoch: 12 ***
06/02/2019 09:13:37 *** training ***
06/02/2019 09:13:37 step: 368, epoch: 11, batch: 4, loss: 1.450868010520935, acc: 40.625, f1: 16.638321995464853, r: 0.2921832975616819
06/02/2019 09:13:37 step: 373, epoch: 11, batch: 9, loss: 1.414691686630249, acc: 54.6875, f1: 25.688092721788987, r: 0.23561666060700162
06/02/2019 09:13:38 step: 378, epoch: 11, batch: 14, loss: 1.3374176025390625, acc: 54.6875, f1: 21.023809523809526, r: 0.25358156601502807
06/02/2019 09:13:38 step: 383, epoch: 11, batch: 19, loss: 1.4394479990005493, acc: 48.4375, f1: 18.93562490577416, r: 0.29204813003801827
06/02/2019 09:13:39 step: 388, epoch: 11, batch: 24, loss: 1.5747432708740234, acc: 37.5, f1: 17.224257224257222, r: 0.23344119048203107
06/02/2019 09:13:39 step: 393, epoch: 11, batch: 29, loss: 1.2931861877441406, acc: 57.8125, f1: 24.223542727787212, r: 0.30674722938880195
06/02/2019 09:13:39 *** evaluating ***
06/02/2019 09:13:40 step: 12, epoch: 11, acc: 59.82905982905983, f1: 19.533365185539097, r: 0.3119009628961446
06/02/2019 09:13:40 *** epoch: 13 ***
06/02/2019 09:13:40 *** training ***
06/02/2019 09:13:40 step: 401, epoch: 12, batch: 4, loss: 1.236080288887024, acc: 59.375, f1: 29.86991661868509, r: 0.39731354857199774
06/02/2019 09:13:41 step: 406, epoch: 12, batch: 9, loss: 1.332615613937378, acc: 56.25, f1: 27.797766749379647, r: 0.34996608491547104
06/02/2019 09:13:41 step: 411, epoch: 12, batch: 14, loss: 1.2621705532073975, acc: 54.6875, f1: 24.24349049964814, r: 0.3143697533348397
06/02/2019 09:13:41 step: 416, epoch: 12, batch: 19, loss: 1.414074182510376, acc: 54.6875, f1: 24.024216524216524, r: 0.2825686076596525
06/02/2019 09:13:42 step: 421, epoch: 12, batch: 24, loss: 1.4293980598449707, acc: 45.3125, f1: 22.921214997626958, r: 0.3245707638713186
06/02/2019 09:13:42 step: 426, epoch: 12, batch: 29, loss: 1.2732195854187012, acc: 54.6875, f1: 34.781573106696264, r: 0.26099913164428856
06/02/2019 09:13:43 *** evaluating ***
06/02/2019 09:13:43 step: 13, epoch: 12, acc: 58.119658119658126, f1: 17.87318519467669, r: 0.3127331974998328
06/02/2019 09:13:43 *** epoch: 14 ***
06/02/2019 09:13:43 *** training ***
06/02/2019 09:13:43 step: 434, epoch: 13, batch: 4, loss: 1.1387052536010742, acc: 64.0625, f1: 25.774635200864708, r: 0.4614827638440649
06/02/2019 09:13:44 step: 439, epoch: 13, batch: 9, loss: 1.1357375383377075, acc: 57.8125, f1: 28.85919165580183, r: 0.3996388937311263
06/02/2019 09:13:44 step: 444, epoch: 13, batch: 14, loss: 1.3304095268249512, acc: 51.5625, f1: 19.761904761904763, r: 0.2672117418231555
06/02/2019 09:13:45 step: 449, epoch: 13, batch: 19, loss: 1.1685523986816406, acc: 51.5625, f1: 22.325293753865182, r: 0.29392625813573703
06/02/2019 09:13:45 step: 454, epoch: 13, batch: 24, loss: 1.2060680389404297, acc: 57.8125, f1: 31.866868763420484, r: 0.4177398517314711
06/02/2019 09:13:45 step: 459, epoch: 13, batch: 29, loss: 1.2305985689163208, acc: 53.125, f1: 31.02947845804988, r: 0.3602972895759488
06/02/2019 09:13:46 *** evaluating ***
06/02/2019 09:13:46 step: 14, epoch: 13, acc: 58.119658119658126, f1: 18.959195223633134, r: 0.30484956282527254
06/02/2019 09:13:46 *** epoch: 15 ***
06/02/2019 09:13:46 *** training ***
06/02/2019 09:13:46 step: 467, epoch: 14, batch: 4, loss: 1.2186514139175415, acc: 54.6875, f1: 34.895833333333336, r: 0.4779282607488683
06/02/2019 09:13:47 step: 472, epoch: 14, batch: 9, loss: 1.2023041248321533, acc: 54.6875, f1: 26.87074829931973, r: 0.39269369604173704
06/02/2019 09:13:47 step: 477, epoch: 14, batch: 14, loss: 1.2385003566741943, acc: 57.8125, f1: 34.58064516129033, r: 0.3712366104822282
06/02/2019 09:13:48 step: 482, epoch: 14, batch: 19, loss: 1.2783534526824951, acc: 57.8125, f1: 32.588691390534706, r: 0.271997394612962
06/02/2019 09:13:48 step: 487, epoch: 14, batch: 24, loss: 1.3341792821884155, acc: 54.6875, f1: 30.681354904957388, r: 0.3264870911654072
06/02/2019 09:13:49 step: 492, epoch: 14, batch: 29, loss: 1.2969691753387451, acc: 57.8125, f1: 32.4895178197065, r: 0.3224665933166932
06/02/2019 09:13:49 *** evaluating ***
06/02/2019 09:13:49 step: 15, epoch: 14, acc: 58.119658119658126, f1: 18.463471297446024, r: 0.31942946947889855
06/02/2019 09:13:49 *** epoch: 16 ***
06/02/2019 09:13:49 *** training ***
06/02/2019 09:13:49 step: 500, epoch: 15, batch: 4, loss: 1.067189335823059, acc: 62.5, f1: 25.677419354838708, r: 0.36998324109854114
06/02/2019 09:13:50 step: 505, epoch: 15, batch: 9, loss: 1.300803303718567, acc: 51.5625, f1: 24.19642857142857, r: 0.4510963495181007
06/02/2019 09:13:50 step: 510, epoch: 15, batch: 14, loss: 1.226759433746338, acc: 60.9375, f1: 24.958949096880133, r: 0.3139723969868624
06/02/2019 09:13:51 step: 515, epoch: 15, batch: 19, loss: 1.1788368225097656, acc: 48.4375, f1: 23.038106017732503, r: 0.44695120937641053
06/02/2019 09:13:51 step: 520, epoch: 15, batch: 24, loss: 1.1887845993041992, acc: 62.5, f1: 31.15641711229947, r: 0.3823673305942232
06/02/2019 09:13:52 step: 525, epoch: 15, batch: 29, loss: 0.9166867136955261, acc: 65.625, f1: 34.74160514408192, r: 0.36280731984986386
06/02/2019 09:13:52 *** evaluating ***
06/02/2019 09:13:52 step: 16, epoch: 15, acc: 58.119658119658126, f1: 19.025840818531776, r: 0.3187585330834745
06/02/2019 09:13:52 *** epoch: 17 ***
06/02/2019 09:13:52 *** training ***
06/02/2019 09:13:52 step: 533, epoch: 16, batch: 4, loss: 1.3628696203231812, acc: 53.125, f1: 30.642332262471843, r: 0.4590027224419314
06/02/2019 09:13:53 step: 538, epoch: 16, batch: 9, loss: 1.0566767454147339, acc: 60.9375, f1: 33.59608133801682, r: 0.30867924009041997
06/02/2019 09:13:53 step: 543, epoch: 16, batch: 14, loss: 1.2561113834381104, acc: 56.25, f1: 31.14373112982679, r: 0.3597540965850137
06/02/2019 09:13:54 step: 548, epoch: 16, batch: 19, loss: 1.1997411251068115, acc: 56.25, f1: 27.7015455304929, r: 0.2995863613547432
06/02/2019 09:13:54 step: 553, epoch: 16, batch: 24, loss: 1.202645182609558, acc: 56.25, f1: 26.96759259259259, r: 0.34341715876652207
06/02/2019 09:13:55 step: 558, epoch: 16, batch: 29, loss: 1.2662007808685303, acc: 51.5625, f1: 29.461816984858462, r: 0.3246790208956354
06/02/2019 09:13:55 *** evaluating ***
06/02/2019 09:13:55 step: 17, epoch: 16, acc: 58.54700854700855, f1: 19.512108262108267, r: 0.3111531263279021
06/02/2019 09:13:55 *** epoch: 18 ***
06/02/2019 09:13:55 *** training ***
06/02/2019 09:13:55 step: 566, epoch: 17, batch: 4, loss: 1.0740256309509277, acc: 62.5, f1: 41.71600171600171, r: 0.3241952324931124
06/02/2019 09:13:56 step: 571, epoch: 17, batch: 9, loss: 1.066604495048523, acc: 62.5, f1: 37.06758832565284, r: 0.4446945200777967
06/02/2019 09:13:56 step: 576, epoch: 17, batch: 14, loss: 0.9175412654876709, acc: 65.625, f1: 29.969807722866676, r: 0.31942848600555707
06/02/2019 09:13:57 step: 581, epoch: 17, batch: 19, loss: 0.879065215587616, acc: 71.875, f1: 38.36734693877551, r: 0.3599973209610317
06/02/2019 09:13:57 step: 586, epoch: 17, batch: 24, loss: 0.9468077421188354, acc: 67.1875, f1: 41.11065498996533, r: 0.3595262781172361
06/02/2019 09:13:58 step: 591, epoch: 17, batch: 29, loss: 1.1131614446640015, acc: 67.1875, f1: 41.00196010607633, r: 0.3777513370091598
06/02/2019 09:13:58 *** evaluating ***
06/02/2019 09:13:58 step: 18, epoch: 17, acc: 58.54700854700855, f1: 19.510796221322536, r: 0.32685802865435754
06/02/2019 09:13:58 *** epoch: 19 ***
06/02/2019 09:13:58 *** training ***
06/02/2019 09:13:59 step: 599, epoch: 18, batch: 4, loss: 1.0149316787719727, acc: 51.5625, f1: 27.261303511303513, r: 0.46240809198290433
06/02/2019 09:13:59 step: 604, epoch: 18, batch: 9, loss: 1.2503581047058105, acc: 56.25, f1: 40.431387608806965, r: 0.31551484734664265
06/02/2019 09:13:59 step: 609, epoch: 18, batch: 14, loss: 1.0792903900146484, acc: 64.0625, f1: 37.46527777777778, r: 0.4875881783182374
06/02/2019 09:14:00 step: 614, epoch: 18, batch: 19, loss: 0.9051998853683472, acc: 59.375, f1: 32.33543417366947, r: 0.44879937236559003
06/02/2019 09:14:00 step: 619, epoch: 18, batch: 24, loss: 0.9681573510169983, acc: 65.625, f1: 49.641187470780736, r: 0.4852869420045805
06/02/2019 09:14:01 step: 624, epoch: 18, batch: 29, loss: 1.2461940050125122, acc: 54.6875, f1: 28.63677536231884, r: 0.35941804535416866
06/02/2019 09:14:01 *** evaluating ***
06/02/2019 09:14:01 step: 19, epoch: 18, acc: 60.256410256410255, f1: 22.499418942604915, r: 0.31223943342371274
06/02/2019 09:14:01 *** epoch: 20 ***
06/02/2019 09:14:01 *** training ***
06/02/2019 09:14:02 step: 632, epoch: 19, batch: 4, loss: 0.9198868870735168, acc: 67.1875, f1: 46.01659451659452, r: 0.4792020584500456
06/02/2019 09:14:02 step: 637, epoch: 19, batch: 9, loss: 0.8078877925872803, acc: 68.75, f1: 47.343846629560915, r: 0.47910962573622085
06/02/2019 09:14:03 step: 642, epoch: 19, batch: 14, loss: 0.8784046173095703, acc: 60.9375, f1: 33.467236999845696, r: 0.4004764659442408
06/02/2019 09:14:03 step: 647, epoch: 19, batch: 19, loss: 1.0184022188186646, acc: 67.1875, f1: 46.786585365853654, r: 0.41423824616547583
06/02/2019 09:14:03 step: 652, epoch: 19, batch: 24, loss: 0.8355101346969604, acc: 67.1875, f1: 59.91344605475041, r: 0.40659518208990936
06/02/2019 09:14:04 step: 657, epoch: 19, batch: 29, loss: 0.7825098633766174, acc: 73.4375, f1: 54.03692163135022, r: 0.42948640029097496
06/02/2019 09:14:04 *** evaluating ***
06/02/2019 09:14:04 step: 20, epoch: 19, acc: 59.82905982905983, f1: 21.37493330822584, r: 0.3362927679014754
06/02/2019 09:14:04 *** epoch: 21 ***
06/02/2019 09:14:04 *** training ***
06/02/2019 09:14:05 step: 665, epoch: 20, batch: 4, loss: 0.912494957447052, acc: 68.75, f1: 53.40277777777778, r: 0.4880796801131911
06/02/2019 09:14:05 step: 670, epoch: 20, batch: 9, loss: 0.9268738031387329, acc: 70.3125, f1: 48.597027972027966, r: 0.45082156127348
06/02/2019 09:14:06 step: 675, epoch: 20, batch: 14, loss: 0.6484379768371582, acc: 78.125, f1: 56.03755933612834, r: 0.530861972430246
06/02/2019 09:14:06 step: 680, epoch: 20, batch: 19, loss: 0.8748022317886353, acc: 60.9375, f1: 35.08373205741627, r: 0.47174612390199683
06/02/2019 09:14:07 step: 685, epoch: 20, batch: 24, loss: 0.694911003112793, acc: 75.0, f1: 31.79106044984153, r: 0.49567675935238864
06/02/2019 09:14:07 step: 690, epoch: 20, batch: 29, loss: 0.9673130512237549, acc: 60.9375, f1: 35.716575091575095, r: 0.4736175934302615
06/02/2019 09:14:07 *** evaluating ***
06/02/2019 09:14:07 step: 21, epoch: 20, acc: 60.68376068376068, f1: 24.283166832227906, r: 0.342701479206841
06/02/2019 09:14:07 *** epoch: 22 ***
06/02/2019 09:14:07 *** training ***
06/02/2019 09:14:08 step: 698, epoch: 21, batch: 4, loss: 0.6230129599571228, acc: 78.125, f1: 76.21374346840184, r: 0.504136180674764
06/02/2019 09:14:08 step: 703, epoch: 21, batch: 9, loss: 0.8350328803062439, acc: 65.625, f1: 47.45833333333333, r: 0.4994724039502261
06/02/2019 09:14:09 step: 708, epoch: 21, batch: 14, loss: 0.7611336708068848, acc: 73.4375, f1: 40.74074074074075, r: 0.5436926059841237
06/02/2019 09:14:09 step: 713, epoch: 21, batch: 19, loss: 1.0016424655914307, acc: 64.0625, f1: 37.6521299660143, r: 0.5755878765065432
06/02/2019 09:14:10 step: 718, epoch: 21, batch: 24, loss: 0.8904467821121216, acc: 68.75, f1: 46.497668997669, r: 0.5041820145320016
06/02/2019 09:14:10 step: 723, epoch: 21, batch: 29, loss: 0.7927500009536743, acc: 73.4375, f1: 67.53272069061543, r: 0.45268542494867725
06/02/2019 09:14:10 *** evaluating ***
06/02/2019 09:14:10 step: 22, epoch: 21, acc: 61.965811965811966, f1: 26.9076635924462, r: 0.3303641560013271
06/02/2019 09:14:10 *** epoch: 23 ***
06/02/2019 09:14:10 *** training ***
06/02/2019 09:14:11 step: 731, epoch: 22, batch: 4, loss: 1.0465357303619385, acc: 62.5, f1: 40.17424242424242, r: 0.4378711180667121
06/02/2019 09:14:11 step: 736, epoch: 22, batch: 9, loss: 0.7817798256874084, acc: 73.4375, f1: 53.17857142857143, r: 0.47402261814050733
06/02/2019 09:14:12 step: 741, epoch: 22, batch: 14, loss: 0.6695541143417358, acc: 78.125, f1: 63.01203318856147, r: 0.4627930760001926
06/02/2019 09:14:12 step: 746, epoch: 22, batch: 19, loss: 0.8715410828590393, acc: 67.1875, f1: 61.421061178094426, r: 0.5051844064928904
06/02/2019 09:14:13 step: 751, epoch: 22, batch: 24, loss: 0.8647748827934265, acc: 65.625, f1: 47.04459561602419, r: 0.37634049749317855
06/02/2019 09:14:13 step: 756, epoch: 22, batch: 29, loss: 0.8526798486709595, acc: 64.0625, f1: 49.96386222473179, r: 0.4901514063120064
06/02/2019 09:14:13 *** evaluating ***
06/02/2019 09:14:14 step: 23, epoch: 22, acc: 61.53846153846154, f1: 25.691344246031743, r: 0.33399161982494263
06/02/2019 09:14:14 *** epoch: 24 ***
06/02/2019 09:14:14 *** training ***
06/02/2019 09:14:14 step: 764, epoch: 23, batch: 4, loss: 0.8702971339225769, acc: 70.3125, f1: 48.92616262760547, r: 0.5026022972600955
06/02/2019 09:14:14 step: 769, epoch: 23, batch: 9, loss: 0.7129831314086914, acc: 79.6875, f1: 71.7006686732664, r: 0.5228877373676091
06/02/2019 09:14:15 step: 774, epoch: 23, batch: 14, loss: 1.0729113817214966, acc: 64.0625, f1: 42.316017316017316, r: 0.34369722644044254
06/02/2019 09:14:15 step: 779, epoch: 23, batch: 19, loss: 0.75041663646698, acc: 75.0, f1: 45.88246199480905, r: 0.44568502665700743
06/02/2019 09:14:16 step: 784, epoch: 23, batch: 24, loss: 0.760956883430481, acc: 76.5625, f1: 59.492048567678815, r: 0.5503399655626535
06/02/2019 09:14:16 step: 789, epoch: 23, batch: 29, loss: 1.0177322626113892, acc: 64.0625, f1: 46.967304163044524, r: 0.5214185047240325
06/02/2019 09:14:17 *** evaluating ***
06/02/2019 09:14:17 step: 24, epoch: 23, acc: 60.256410256410255, f1: 25.92313025585972, r: 0.33054006619451054
06/02/2019 09:14:17 *** epoch: 25 ***
06/02/2019 09:14:17 *** training ***
06/02/2019 09:14:17 step: 797, epoch: 24, batch: 4, loss: 0.973764181137085, acc: 65.625, f1: 59.707635359809274, r: 0.519302433163423
06/02/2019 09:14:18 step: 802, epoch: 24, batch: 9, loss: 0.8646829128265381, acc: 71.875, f1: 52.94201693216471, r: 0.42701613418793105
06/02/2019 09:14:18 step: 807, epoch: 24, batch: 14, loss: 0.6990020871162415, acc: 73.4375, f1: 57.1396215315227, r: 0.5433711859817001
06/02/2019 09:14:19 step: 812, epoch: 24, batch: 19, loss: 0.9867120981216431, acc: 67.1875, f1: 39.449953544882796, r: 0.46303768499720677
06/02/2019 09:14:19 step: 817, epoch: 24, batch: 24, loss: 0.6907342076301575, acc: 76.5625, f1: 61.659241788958774, r: 0.5786105576619753
06/02/2019 09:14:19 step: 822, epoch: 24, batch: 29, loss: 0.7614611387252808, acc: 75.0, f1: 48.89520202020202, r: 0.49691884724211777
06/02/2019 09:14:20 *** evaluating ***
06/02/2019 09:14:20 step: 25, epoch: 24, acc: 61.111111111111114, f1: 23.551573426573423, r: 0.3689635245895851
06/02/2019 09:14:20 *** epoch: 26 ***
06/02/2019 09:14:20 *** training ***
06/02/2019 09:14:20 step: 830, epoch: 25, batch: 4, loss: 0.5604754686355591, acc: 81.25, f1: 73.9075993373756, r: 0.5688904950155274
06/02/2019 09:14:21 step: 835, epoch: 25, batch: 9, loss: 0.6920169591903687, acc: 78.125, f1: 63.50108225108224, r: 0.5883111064062759
06/02/2019 09:14:21 step: 840, epoch: 25, batch: 14, loss: 0.7330796122550964, acc: 73.4375, f1: 55.701318834024875, r: 0.41975942849643283
06/02/2019 09:14:22 step: 845, epoch: 25, batch: 19, loss: 0.7300794720649719, acc: 75.0, f1: 52.074905968348595, r: 0.6122779071969069
06/02/2019 09:14:22 step: 850, epoch: 25, batch: 24, loss: 0.6781567931175232, acc: 79.6875, f1: 68.4551282051282, r: 0.5481819904881728
06/02/2019 09:14:23 step: 855, epoch: 25, batch: 29, loss: 0.58583003282547, acc: 79.6875, f1: 73.47267111972994, r: 0.4743749396825869
06/02/2019 09:14:23 *** evaluating ***
06/02/2019 09:14:23 step: 26, epoch: 25, acc: 61.965811965811966, f1: 26.035903874386797, r: 0.36554869847501814
06/02/2019 09:14:23 *** epoch: 27 ***
06/02/2019 09:14:23 *** training ***
06/02/2019 09:14:23 step: 863, epoch: 26, batch: 4, loss: 0.6443779468536377, acc: 71.875, f1: 55.72446072446072, r: 0.569166563981287
06/02/2019 09:14:24 step: 868, epoch: 26, batch: 9, loss: 0.7838845252990723, acc: 71.875, f1: 54.16477702191989, r: 0.5039946857577811
06/02/2019 09:14:24 step: 873, epoch: 26, batch: 14, loss: 0.7085781693458557, acc: 71.875, f1: 53.68688318315185, r: 0.5838323558218246
06/02/2019 09:14:25 step: 878, epoch: 26, batch: 19, loss: 0.6987311840057373, acc: 76.5625, f1: 59.067460317460316, r: 0.6070571881867927
06/02/2019 09:14:25 step: 883, epoch: 26, batch: 24, loss: 0.6332652568817139, acc: 76.5625, f1: 58.70940653549349, r: 0.4519979883819803
06/02/2019 09:14:26 step: 888, epoch: 26, batch: 29, loss: 0.6349811553955078, acc: 76.5625, f1: 65.19607843137256, r: 0.631129880001121
06/02/2019 09:14:26 *** evaluating ***
06/02/2019 09:14:26 step: 27, epoch: 26, acc: 61.53846153846154, f1: 26.76046588553389, r: 0.35696709872622434
06/02/2019 09:14:26 *** epoch: 28 ***
06/02/2019 09:14:26 *** training ***
06/02/2019 09:14:27 step: 896, epoch: 27, batch: 4, loss: 0.4974876344203949, acc: 81.25, f1: 66.15263649319377, r: 0.6439028080999211
06/02/2019 09:14:27 step: 901, epoch: 27, batch: 9, loss: 0.7322341203689575, acc: 70.3125, f1: 58.678240526980034, r: 0.5206940720465816
06/02/2019 09:14:27 step: 906, epoch: 27, batch: 14, loss: 0.7728568315505981, acc: 71.875, f1: 50.672628025569196, r: 0.396677395723916
06/02/2019 09:14:28 step: 911, epoch: 27, batch: 19, loss: 0.49960148334503174, acc: 84.375, f1: 72.81807893103573, r: 0.5786296896859568
06/02/2019 09:14:28 step: 916, epoch: 27, batch: 24, loss: 0.7876131534576416, acc: 68.75, f1: 66.52208902208902, r: 0.5676083458341047
06/02/2019 09:14:29 step: 921, epoch: 27, batch: 29, loss: 0.6870892643928528, acc: 78.125, f1: 64.78625541125541, r: 0.6127919352222816
06/02/2019 09:14:29 *** evaluating ***
06/02/2019 09:14:29 step: 28, epoch: 27, acc: 60.68376068376068, f1: 27.247393878500013, r: 0.3627542688922034
06/02/2019 09:14:29 *** epoch: 29 ***
06/02/2019 09:14:29 *** training ***
06/02/2019 09:14:30 step: 929, epoch: 28, batch: 4, loss: 0.670072615146637, acc: 71.875, f1: 52.70658263305322, r: 0.5942831673137721
06/02/2019 09:14:30 step: 934, epoch: 28, batch: 9, loss: 0.47188320755958557, acc: 82.8125, f1: 75.51431796714816, r: 0.6414953355521908
06/02/2019 09:14:31 step: 939, epoch: 28, batch: 14, loss: 0.6851370930671692, acc: 76.5625, f1: 62.10144022644022, r: 0.5085538236419053
06/02/2019 09:14:31 step: 944, epoch: 28, batch: 19, loss: 0.7689090371131897, acc: 73.4375, f1: 60.70745965624671, r: 0.47641208942090824
06/02/2019 09:14:32 step: 949, epoch: 28, batch: 24, loss: 0.41510534286499023, acc: 90.625, f1: 75.20574162679425, r: 0.674662092163302
06/02/2019 09:14:32 step: 954, epoch: 28, batch: 29, loss: 0.563593327999115, acc: 81.25, f1: 64.87688781806429, r: 0.5992813780988002
06/02/2019 09:14:32 *** evaluating ***
06/02/2019 09:14:32 step: 29, epoch: 28, acc: 62.39316239316239, f1: 27.188465307071446, r: 0.37921726468816686
06/02/2019 09:14:32 *** epoch: 30 ***
06/02/2019 09:14:32 *** training ***
06/02/2019 09:14:33 step: 962, epoch: 29, batch: 4, loss: 0.42153194546699524, acc: 87.5, f1: 84.13038089729818, r: 0.6167145593664434
06/02/2019 09:14:33 step: 967, epoch: 29, batch: 9, loss: 0.45734068751335144, acc: 82.8125, f1: 60.538833147528806, r: 0.4995152731401004
06/02/2019 09:14:34 step: 972, epoch: 29, batch: 14, loss: 0.6904374361038208, acc: 76.5625, f1: 59.55244996549344, r: 0.5072944493541216
06/02/2019 09:14:34 step: 977, epoch: 29, batch: 19, loss: 0.7412847876548767, acc: 75.0, f1: 59.038600288600286, r: 0.5167670109970695
06/02/2019 09:14:35 step: 982, epoch: 29, batch: 24, loss: 0.5228240489959717, acc: 79.6875, f1: 67.8308768933769, r: 0.6429344152980943
06/02/2019 09:14:35 step: 987, epoch: 29, batch: 29, loss: 0.578423798084259, acc: 79.6875, f1: 70.43822996225742, r: 0.5866789404989788
06/02/2019 09:14:35 *** evaluating ***
06/02/2019 09:14:35 step: 30, epoch: 29, acc: 62.82051282051282, f1: 28.344121191694498, r: 0.376679518441498
06/02/2019 09:14:35 *** epoch: 31 ***
06/02/2019 09:14:35 *** training ***
06/02/2019 09:14:36 step: 995, epoch: 30, batch: 4, loss: 0.3941315710544586, acc: 82.8125, f1: 66.25000000000001, r: 0.6332591547745933
06/02/2019 09:14:36 step: 1000, epoch: 30, batch: 9, loss: 0.5761839747428894, acc: 79.6875, f1: 79.2253316689407, r: 0.542431407052618
06/02/2019 09:14:37 step: 1005, epoch: 30, batch: 14, loss: 0.47458416223526, acc: 84.375, f1: 81.61052715400541, r: 0.6337081245103878
06/02/2019 09:14:37 step: 1010, epoch: 30, batch: 19, loss: 0.4722040295600891, acc: 84.375, f1: 77.47680890538034, r: 0.626646739255949
06/02/2019 09:14:38 step: 1015, epoch: 30, batch: 24, loss: 0.5909775495529175, acc: 79.6875, f1: 58.38068181818181, r: 0.593099310236699
06/02/2019 09:14:38 step: 1020, epoch: 30, batch: 29, loss: 0.6489187479019165, acc: 79.6875, f1: 63.04761904761905, r: 0.5667682298525654
06/02/2019 09:14:38 *** evaluating ***
06/02/2019 09:14:39 step: 31, epoch: 30, acc: 63.67521367521367, f1: 29.111004356375936, r: 0.3782877198347774
06/02/2019 09:14:39 *** epoch: 32 ***
06/02/2019 09:14:39 *** training ***
06/02/2019 09:14:39 step: 1028, epoch: 31, batch: 4, loss: 0.3760899603366852, acc: 87.5, f1: 87.74151886613893, r: 0.6317489526864637
06/02/2019 09:14:39 step: 1033, epoch: 31, batch: 9, loss: 0.5133314728736877, acc: 79.6875, f1: 75.44994013079119, r: 0.6098616859351345
06/02/2019 09:14:40 step: 1038, epoch: 31, batch: 14, loss: 0.4728511869907379, acc: 82.8125, f1: 77.50979179550608, r: 0.583074286928968
06/02/2019 09:14:40 step: 1043, epoch: 31, batch: 19, loss: 0.6136155128479004, acc: 76.5625, f1: 58.406750254576345, r: 0.7270423275168153
06/02/2019 09:14:41 step: 1048, epoch: 31, batch: 24, loss: 0.5624319911003113, acc: 85.9375, f1: 56.41456582633053, r: 0.3963291066654361
06/02/2019 09:14:41 step: 1053, epoch: 31, batch: 29, loss: 0.3639726936817169, acc: 90.625, f1: 90.54474936827877, r: 0.6833252907114388
06/02/2019 09:14:41 *** evaluating ***
06/02/2019 09:14:42 step: 32, epoch: 31, acc: 64.1025641025641, f1: 29.688799579504625, r: 0.37966680553415677
06/02/2019 09:14:42 *** epoch: 33 ***
06/02/2019 09:14:42 *** training ***
06/02/2019 09:14:42 step: 1061, epoch: 32, batch: 4, loss: 0.3557491898536682, acc: 92.1875, f1: 89.98049569478141, r: 0.6453368946471012
06/02/2019 09:14:42 step: 1066, epoch: 32, batch: 9, loss: 0.5124247074127197, acc: 84.375, f1: 84.2212051514377, r: 0.6671332128811791
06/02/2019 09:14:43 step: 1071, epoch: 32, batch: 14, loss: 0.4000283479690552, acc: 85.9375, f1: 74.375, r: 0.6361452264478208
06/02/2019 09:14:43 step: 1076, epoch: 32, batch: 19, loss: 0.4776937961578369, acc: 82.8125, f1: 76.60477820052289, r: 0.7041612380873474
06/02/2019 09:14:44 step: 1081, epoch: 32, batch: 24, loss: 0.6223711371421814, acc: 75.0, f1: 59.673482545822964, r: 0.6202834208421792
06/02/2019 09:14:44 step: 1086, epoch: 32, batch: 29, loss: 0.4105823040008545, acc: 82.8125, f1: 70.66891441891443, r: 0.5355680067300135
06/02/2019 09:14:44 *** evaluating ***
06/02/2019 09:14:45 step: 33, epoch: 32, acc: 61.53846153846154, f1: 27.986723591295576, r: 0.38517143979764856
06/02/2019 09:14:45 *** epoch: 34 ***
06/02/2019 09:14:45 *** training ***
06/02/2019 09:14:45 step: 1094, epoch: 33, batch: 4, loss: 0.373449444770813, acc: 85.9375, f1: 82.29335242869078, r: 0.6191225723634648
06/02/2019 09:14:46 step: 1099, epoch: 33, batch: 9, loss: 0.38922739028930664, acc: 87.5, f1: 75.37277537277535, r: 0.6306009372611481
06/02/2019 09:14:46 step: 1104, epoch: 33, batch: 14, loss: 0.32815125584602356, acc: 84.375, f1: 78.15935569630152, r: 0.5875407395679245
06/02/2019 09:14:46 step: 1109, epoch: 33, batch: 19, loss: 0.45017340779304504, acc: 84.375, f1: 74.94848901098902, r: 0.6443089492761322
06/02/2019 09:14:47 step: 1114, epoch: 33, batch: 24, loss: 0.4166289269924164, acc: 89.0625, f1: 83.01935394040657, r: 0.7289726606606277
06/02/2019 09:14:47 step: 1119, epoch: 33, batch: 29, loss: 0.47444164752960205, acc: 85.9375, f1: 82.36896081840511, r: 0.6154960425841205
06/02/2019 09:14:48 *** evaluating ***
06/02/2019 09:14:48 step: 34, epoch: 33, acc: 61.53846153846154, f1: 27.369679090889115, r: 0.3824476286538328
06/02/2019 09:14:48 *** epoch: 35 ***
06/02/2019 09:14:48 *** training ***
06/02/2019 09:14:48 step: 1127, epoch: 34, batch: 4, loss: 0.46432796120643616, acc: 81.25, f1: 67.96449046449047, r: 0.6601130724139873
06/02/2019 09:14:49 step: 1132, epoch: 34, batch: 9, loss: 0.6189418435096741, acc: 76.5625, f1: 72.2394168769274, r: 0.6404728054955191
06/02/2019 09:14:49 step: 1137, epoch: 34, batch: 14, loss: 0.44204723834991455, acc: 81.25, f1: 76.64250261233019, r: 0.688510718069868
06/02/2019 09:14:50 step: 1142, epoch: 34, batch: 19, loss: 0.3893642723560333, acc: 85.9375, f1: 84.22424797424797, r: 0.6523388814629089
06/02/2019 09:14:50 step: 1147, epoch: 34, batch: 24, loss: 0.3776799142360687, acc: 89.0625, f1: 84.10432777099443, r: 0.5537669764551224
06/02/2019 09:14:50 step: 1152, epoch: 34, batch: 29, loss: 0.4802534580230713, acc: 82.8125, f1: 75.73143945426554, r: 0.6730965771127442
06/02/2019 09:14:51 *** evaluating ***
06/02/2019 09:14:51 step: 35, epoch: 34, acc: 62.39316239316239, f1: 26.789445178820593, r: 0.3891219168851334
06/02/2019 09:14:51 *** epoch: 36 ***
06/02/2019 09:14:51 *** training ***
06/02/2019 09:14:51 step: 1160, epoch: 35, batch: 4, loss: 0.29217132925987244, acc: 93.75, f1: 94.66588966588967, r: 0.7627233344649823
06/02/2019 09:14:52 step: 1165, epoch: 35, batch: 9, loss: 0.4249296486377716, acc: 89.0625, f1: 74.47035168582484, r: 0.5908683745259548
06/02/2019 09:14:52 step: 1170, epoch: 35, batch: 14, loss: 0.36304110288619995, acc: 85.9375, f1: 86.33928571428571, r: 0.7508385790176167
06/02/2019 09:14:53 step: 1175, epoch: 35, batch: 19, loss: 0.30103352665901184, acc: 85.9375, f1: 66.85368655037773, r: 0.6114362308161128
06/02/2019 09:14:53 step: 1180, epoch: 35, batch: 24, loss: 0.42373180389404297, acc: 87.5, f1: 78.78082062864671, r: 0.6766608932097851
06/02/2019 09:14:53 step: 1185, epoch: 35, batch: 29, loss: 0.41892677545547485, acc: 85.9375, f1: 73.82757867132868, r: 0.6481483047046896
06/02/2019 09:14:54 *** evaluating ***
06/02/2019 09:14:54 step: 36, epoch: 35, acc: 62.82051282051282, f1: 28.548694499290363, r: 0.38500087170010466
06/02/2019 09:14:54 *** epoch: 37 ***
06/02/2019 09:14:54 *** training ***
06/02/2019 09:14:54 step: 1193, epoch: 36, batch: 4, loss: 0.3686154782772064, acc: 90.625, f1: 84.96325032439255, r: 0.5752250734518615
06/02/2019 09:14:55 step: 1198, epoch: 36, batch: 9, loss: 0.5567539930343628, acc: 82.8125, f1: 83.33823409295108, r: 0.6666746734899031
06/02/2019 09:14:55 step: 1203, epoch: 36, batch: 14, loss: 0.43524566292762756, acc: 82.8125, f1: 80.93219159008632, r: 0.5754868578489393
06/02/2019 09:14:56 step: 1208, epoch: 36, batch: 19, loss: 0.382451593875885, acc: 84.375, f1: 67.4854615759347, r: 0.6722382821483179
06/02/2019 09:14:56 step: 1213, epoch: 36, batch: 24, loss: 0.29990220069885254, acc: 85.9375, f1: 73.29640683677951, r: 0.6000986044402984
06/02/2019 09:14:57 step: 1218, epoch: 36, batch: 29, loss: 0.39466458559036255, acc: 87.5, f1: 87.59503801520607, r: 0.6740954475863495
06/02/2019 09:14:57 *** evaluating ***
06/02/2019 09:14:57 step: 37, epoch: 36, acc: 61.53846153846154, f1: 26.066530853952834, r: 0.3926007467311671
06/02/2019 09:14:57 *** epoch: 38 ***
06/02/2019 09:14:57 *** training ***
06/02/2019 09:14:57 step: 1226, epoch: 37, batch: 4, loss: 0.35192447900772095, acc: 85.9375, f1: 79.41848067898488, r: 0.637024352292541
06/02/2019 09:14:58 step: 1231, epoch: 37, batch: 9, loss: 0.37692445516586304, acc: 87.5, f1: 81.56799616848042, r: 0.6055804096111136
06/02/2019 09:14:58 step: 1236, epoch: 37, batch: 14, loss: 0.24588146805763245, acc: 95.3125, f1: 83.84210526315789, r: 0.6650159311637045
06/02/2019 09:14:59 step: 1241, epoch: 37, batch: 19, loss: 0.39485853910446167, acc: 89.0625, f1: 67.17057176539936, r: 0.6002602651339869
06/02/2019 09:14:59 step: 1246, epoch: 37, batch: 24, loss: 0.2711934447288513, acc: 85.9375, f1: 81.61740558292283, r: 0.648898701147823
06/02/2019 09:15:00 step: 1251, epoch: 37, batch: 29, loss: 0.39032095670700073, acc: 90.625, f1: 78.11460202560906, r: 0.5225357163262531
06/02/2019 09:15:00 *** evaluating ***
06/02/2019 09:15:00 step: 38, epoch: 37, acc: 61.965811965811966, f1: 26.95573288603979, r: 0.38243978111506005
06/02/2019 09:15:00 *** epoch: 39 ***
06/02/2019 09:15:00 *** training ***
06/02/2019 09:15:01 step: 1259, epoch: 38, batch: 4, loss: 0.4121086895465851, acc: 82.8125, f1: 80.92583127466848, r: 0.6643292635821576
06/02/2019 09:15:01 step: 1264, epoch: 38, batch: 9, loss: 0.3481370508670807, acc: 87.5, f1: 74.9980574980575, r: 0.683893441724664
06/02/2019 09:15:01 step: 1269, epoch: 38, batch: 14, loss: 0.3599281311035156, acc: 90.625, f1: 88.91939021985607, r: 0.6741597289222988
06/02/2019 09:15:02 step: 1274, epoch: 38, batch: 19, loss: 0.43352392315864563, acc: 82.8125, f1: 81.9501133786848, r: 0.6121889775968785
06/02/2019 09:15:02 step: 1279, epoch: 38, batch: 24, loss: 0.18895402550697327, acc: 96.875, f1: 94.21615421615421, r: 0.7066498061993665
06/02/2019 09:15:03 step: 1284, epoch: 38, batch: 29, loss: 0.44693684577941895, acc: 85.9375, f1: 75.04960317460316, r: 0.625745761402582
06/02/2019 09:15:03 *** evaluating ***
06/02/2019 09:15:03 step: 39, epoch: 38, acc: 62.82051282051282, f1: 28.184489111611, r: 0.3855775239581811
06/02/2019 09:15:03 *** epoch: 40 ***
06/02/2019 09:15:03 *** training ***
06/02/2019 09:15:04 step: 1292, epoch: 39, batch: 4, loss: 0.2252495139837265, acc: 89.0625, f1: 83.8405493524708, r: 0.700244842396231
06/02/2019 09:15:04 step: 1297, epoch: 39, batch: 9, loss: 0.47984981536865234, acc: 78.125, f1: 78.34147869674186, r: 0.6474323211926372
06/02/2019 09:15:05 step: 1302, epoch: 39, batch: 14, loss: 0.25419682264328003, acc: 95.3125, f1: 95.862296433725, r: 0.7016284457333657
06/02/2019 09:15:05 step: 1307, epoch: 39, batch: 19, loss: 0.2184574007987976, acc: 93.75, f1: 95.25103672569111, r: 0.6248556645118303
06/02/2019 09:15:05 step: 1312, epoch: 39, batch: 24, loss: 0.4250357449054718, acc: 81.25, f1: 77.84580498866215, r: 0.6016295145008593
06/02/2019 09:15:06 step: 1317, epoch: 39, batch: 29, loss: 0.26745373010635376, acc: 92.1875, f1: 81.48775894538606, r: 0.6451820477737796
06/02/2019 09:15:06 *** evaluating ***
06/02/2019 09:15:06 step: 40, epoch: 39, acc: 61.965811965811966, f1: 27.686132423984088, r: 0.3841078803186091
06/02/2019 09:15:06 *** epoch: 41 ***
06/02/2019 09:15:06 *** training ***
06/02/2019 09:15:07 step: 1325, epoch: 40, batch: 4, loss: 0.295396089553833, acc: 90.625, f1: 89.26908063413613, r: 0.7157933160985346
06/02/2019 09:15:07 step: 1330, epoch: 40, batch: 9, loss: 0.34243854880332947, acc: 89.0625, f1: 84.8922902494331, r: 0.6288727247157456
06/02/2019 09:15:08 step: 1335, epoch: 40, batch: 14, loss: 0.32324767112731934, acc: 89.0625, f1: 86.6750700280112, r: 0.5903224318622414
06/02/2019 09:15:08 step: 1340, epoch: 40, batch: 19, loss: 0.37084028124809265, acc: 90.625, f1: 93.21428571428572, r: 0.6961762181575113
06/02/2019 09:15:09 step: 1345, epoch: 40, batch: 24, loss: 0.7031450867652893, acc: 76.5625, f1: 61.44704433497537, r: 0.5593850645066967
06/02/2019 09:15:09 step: 1350, epoch: 40, batch: 29, loss: 0.25468990206718445, acc: 93.75, f1: 90.40322580645162, r: 0.622971082551752
06/02/2019 09:15:09 *** evaluating ***
06/02/2019 09:15:09 step: 41, epoch: 40, acc: 61.53846153846154, f1: 25.59406117916756, r: 0.3853645421450694
06/02/2019 09:15:09 *** epoch: 42 ***
06/02/2019 09:15:09 *** training ***
06/02/2019 09:15:10 step: 1358, epoch: 41, batch: 4, loss: 0.31980761885643005, acc: 87.5, f1: 87.76672561155318, r: 0.6432784591767463
06/02/2019 09:15:10 step: 1363, epoch: 41, batch: 9, loss: 0.48097121715545654, acc: 85.9375, f1: 76.0893341076268, r: 0.6528141562839355
06/02/2019 09:15:11 step: 1368, epoch: 41, batch: 14, loss: 0.303994357585907, acc: 89.0625, f1: 73.75667506835269, r: 0.6785319938250124
06/02/2019 09:15:11 step: 1373, epoch: 41, batch: 19, loss: 0.29807740449905396, acc: 89.0625, f1: 85.55555555555556, r: 0.5979582976368715
06/02/2019 09:15:12 step: 1378, epoch: 41, batch: 24, loss: 0.29668501019477844, acc: 90.625, f1: 87.27014652014653, r: 0.7037848153597659
06/02/2019 09:15:12 step: 1383, epoch: 41, batch: 29, loss: 0.2612951695919037, acc: 92.1875, f1: 92.17313218390805, r: 0.7335327844543533
06/02/2019 09:15:12 *** evaluating ***
06/02/2019 09:15:12 step: 42, epoch: 41, acc: 62.39316239316239, f1: 27.72890959789265, r: 0.40251058668406065
06/02/2019 09:15:12 *** epoch: 43 ***
06/02/2019 09:15:12 *** training ***
06/02/2019 09:15:13 step: 1391, epoch: 42, batch: 4, loss: 0.23512277007102966, acc: 92.1875, f1: 88.60681114551083, r: 0.8223034182311677
06/02/2019 09:15:13 step: 1396, epoch: 42, batch: 9, loss: 0.3212341368198395, acc: 85.9375, f1: 74.65249036677608, r: 0.6256675858555941
06/02/2019 09:15:14 step: 1401, epoch: 42, batch: 14, loss: 0.4684025049209595, acc: 82.8125, f1: 75.3937728937729, r: 0.6468127068224787
06/02/2019 09:15:14 step: 1406, epoch: 42, batch: 19, loss: 0.2597588896751404, acc: 89.0625, f1: 76.81998556998558, r: 0.7604291276827204
06/02/2019 09:15:15 step: 1411, epoch: 42, batch: 24, loss: 0.16091743111610413, acc: 93.75, f1: 82.88250930356193, r: 0.7768686553560613
06/02/2019 09:15:15 step: 1416, epoch: 42, batch: 29, loss: 0.23742029070854187, acc: 92.1875, f1: 89.02645502645503, r: 0.6417849729164987
06/02/2019 09:15:15 *** evaluating ***
06/02/2019 09:15:16 step: 43, epoch: 42, acc: 61.53846153846154, f1: 26.790355382770127, r: 0.39364388158725994
06/02/2019 09:15:16 *** epoch: 44 ***
06/02/2019 09:15:16 *** training ***
06/02/2019 09:15:16 step: 1424, epoch: 43, batch: 4, loss: 0.30939531326293945, acc: 90.625, f1: 89.85714285714286, r: 0.7369829548064395
06/02/2019 09:15:16 step: 1429, epoch: 43, batch: 9, loss: 0.34495341777801514, acc: 85.9375, f1: 72.18007662835248, r: 0.5817190835737547
06/02/2019 09:15:17 step: 1434, epoch: 43, batch: 14, loss: 0.3552781939506531, acc: 90.625, f1: 82.1875, r: 0.660481262260152
06/02/2019 09:15:17 step: 1439, epoch: 43, batch: 19, loss: 0.2099049687385559, acc: 93.75, f1: 95.83405483405483, r: 0.6540534102542954
06/02/2019 09:15:18 step: 1444, epoch: 43, batch: 24, loss: 0.33949682116508484, acc: 87.5, f1: 61.483683585076776, r: 0.6524579778759113
06/02/2019 09:15:18 step: 1449, epoch: 43, batch: 29, loss: 0.5536695718765259, acc: 79.6875, f1: 70.8212839791787, r: 0.5811738578554642
06/02/2019 09:15:18 *** evaluating ***
06/02/2019 09:15:19 step: 44, epoch: 43, acc: 61.965811965811966, f1: 26.881027582281046, r: 0.3994303950497019
06/02/2019 09:15:19 *** epoch: 45 ***
06/02/2019 09:15:19 *** training ***
06/02/2019 09:15:19 step: 1457, epoch: 44, batch: 4, loss: 0.1813831925392151, acc: 92.1875, f1: 78.44905273937532, r: 0.7199712335295942
06/02/2019 09:15:20 step: 1462, epoch: 44, batch: 9, loss: 0.2778204083442688, acc: 92.1875, f1: 90.68473322343291, r: 0.7211172543222437
06/02/2019 09:15:20 step: 1467, epoch: 44, batch: 14, loss: 0.34039896726608276, acc: 92.1875, f1: 94.58333333333333, r: 0.7147074910142714
06/02/2019 09:15:20 step: 1472, epoch: 44, batch: 19, loss: 0.15335281193256378, acc: 95.3125, f1: 91.00274725274726, r: 0.7927102650523833
06/02/2019 09:15:21 step: 1477, epoch: 44, batch: 24, loss: 0.16709817945957184, acc: 96.875, f1: 96.90125747268604, r: 0.6725417033432238
06/02/2019 09:15:21 step: 1482, epoch: 44, batch: 29, loss: 0.4397883415222168, acc: 79.6875, f1: 64.81303465914102, r: 0.642622866376255
06/02/2019 09:15:22 *** evaluating ***
06/02/2019 09:15:22 step: 45, epoch: 44, acc: 61.111111111111114, f1: 25.41610879117584, r: 0.39801149266407215
06/02/2019 09:15:22 *** epoch: 46 ***
06/02/2019 09:15:22 *** training ***
06/02/2019 09:15:22 step: 1490, epoch: 45, batch: 4, loss: 0.18627949059009552, acc: 95.3125, f1: 96.01012498022465, r: 0.6859890198048311
06/02/2019 09:15:23 step: 1495, epoch: 45, batch: 9, loss: 0.24191097915172577, acc: 93.75, f1: 93.1212507299464, r: 0.6146714005901996
06/02/2019 09:15:23 step: 1500, epoch: 45, batch: 14, loss: 0.261263906955719, acc: 95.3125, f1: 95.5662393162393, r: 0.720624415942601
06/02/2019 09:15:24 step: 1505, epoch: 45, batch: 19, loss: 0.3066456913948059, acc: 87.5, f1: 87.14439655172413, r: 0.6992093787355097
06/02/2019 09:15:24 step: 1510, epoch: 45, batch: 24, loss: 0.3041941225528717, acc: 93.75, f1: 95.46430580913339, r: 0.7280268399920354
06/02/2019 09:15:24 step: 1515, epoch: 45, batch: 29, loss: 0.20596669614315033, acc: 95.3125, f1: 93.04597790490403, r: 0.7065706087277439
06/02/2019 09:15:25 *** evaluating ***
06/02/2019 09:15:25 step: 46, epoch: 45, acc: 61.111111111111114, f1: 25.364498139060753, r: 0.3939494143338191
06/02/2019 09:15:25 *** epoch: 47 ***
06/02/2019 09:15:25 *** training ***
06/02/2019 09:15:25 step: 1523, epoch: 46, batch: 4, loss: 0.19023148715496063, acc: 95.3125, f1: 93.10278902384167, r: 0.799067321608146
06/02/2019 09:15:26 step: 1528, epoch: 46, batch: 9, loss: 0.2911328375339508, acc: 95.3125, f1: 93.48450491307635, r: 0.6076607004995039
06/02/2019 09:15:26 step: 1533, epoch: 46, batch: 14, loss: 0.3234010338783264, acc: 89.0625, f1: 87.48563218390805, r: 0.739440304322434
06/02/2019 09:15:27 step: 1538, epoch: 46, batch: 19, loss: 0.3994636535644531, acc: 87.5, f1: 73.5916952544311, r: 0.6337203227745448
06/02/2019 09:15:27 step: 1543, epoch: 46, batch: 24, loss: 0.16252800822257996, acc: 93.75, f1: 89.15549169859514, r: 0.5579528205216516
06/02/2019 09:15:28 step: 1548, epoch: 46, batch: 29, loss: 0.27481961250305176, acc: 90.625, f1: 74.04672370189613, r: 0.5730687887046387
06/02/2019 09:15:28 *** evaluating ***
06/02/2019 09:15:28 step: 47, epoch: 46, acc: 60.256410256410255, f1: 24.011609102937886, r: 0.39598444457072735
06/02/2019 09:15:28 *** epoch: 48 ***
06/02/2019 09:15:28 *** training ***
06/02/2019 09:15:28 step: 1556, epoch: 47, batch: 4, loss: 0.2870887517929077, acc: 90.625, f1: 93.23809523809523, r: 0.7019479585728425
06/02/2019 09:15:29 step: 1561, epoch: 47, batch: 9, loss: 0.1490372121334076, acc: 96.875, f1: 93.24014336917563, r: 0.6025132435476129
06/02/2019 09:15:29 step: 1566, epoch: 47, batch: 14, loss: 0.17705290019512177, acc: 95.3125, f1: 82.87990196078432, r: 0.6664099718364788
06/02/2019 09:15:30 step: 1571, epoch: 47, batch: 19, loss: 0.1617365926504135, acc: 95.3125, f1: 96.32275132275132, r: 0.7141138274139592
06/02/2019 09:15:30 step: 1576, epoch: 47, batch: 24, loss: 0.2764711081981659, acc: 90.625, f1: 78.10522092849679, r: 0.7377422800443939
06/02/2019 09:15:31 step: 1581, epoch: 47, batch: 29, loss: 0.2180894911289215, acc: 95.3125, f1: 80.03202450570872, r: 0.6488560023358273
06/02/2019 09:15:31 *** evaluating ***
06/02/2019 09:15:31 step: 48, epoch: 47, acc: 61.965811965811966, f1: 25.814910126681724, r: 0.3999080841299777
06/02/2019 09:15:31 *** epoch: 49 ***
06/02/2019 09:15:31 *** training ***
06/02/2019 09:15:31 step: 1589, epoch: 48, batch: 4, loss: 0.20465677976608276, acc: 93.75, f1: 90.55889780154487, r: 0.734110493133761
06/02/2019 09:15:32 step: 1594, epoch: 48, batch: 9, loss: 0.1815321445465088, acc: 95.3125, f1: 92.01402462272029, r: 0.6418165329857293
06/02/2019 09:15:32 step: 1599, epoch: 48, batch: 14, loss: 0.2752985656261444, acc: 87.5, f1: 81.50953984287318, r: 0.5284784110795813
06/02/2019 09:15:33 step: 1604, epoch: 48, batch: 19, loss: 0.3892267942428589, acc: 92.1875, f1: 91.15474384408915, r: 0.7288681140629244
06/02/2019 09:15:33 step: 1609, epoch: 48, batch: 24, loss: 0.21978488564491272, acc: 93.75, f1: 82.6114002276793, r: 0.7385867423264236
06/02/2019 09:15:34 step: 1614, epoch: 48, batch: 29, loss: 0.2713366150856018, acc: 89.0625, f1: 74.83403693081112, r: 0.7948541910706223
06/02/2019 09:15:34 *** evaluating ***
06/02/2019 09:15:34 step: 49, epoch: 48, acc: 61.53846153846154, f1: 26.26733359900337, r: 0.3948239128535523
06/02/2019 09:15:34 *** epoch: 50 ***
06/02/2019 09:15:34 *** training ***
06/02/2019 09:15:35 step: 1622, epoch: 49, batch: 4, loss: 0.2096659541130066, acc: 93.75, f1: 95.21221532091097, r: 0.6427530352223267
06/02/2019 09:15:35 step: 1627, epoch: 49, batch: 9, loss: 0.17975930869579315, acc: 92.1875, f1: 86.22570163331032, r: 0.7266844728638125
06/02/2019 09:15:35 step: 1632, epoch: 49, batch: 14, loss: 0.1528465747833252, acc: 95.3125, f1: 93.9529028890731, r: 0.7056487680189151
06/02/2019 09:15:36 step: 1637, epoch: 49, batch: 19, loss: 0.18031556904315948, acc: 93.75, f1: 90.60609438870308, r: 0.7921032844568601
06/02/2019 09:15:36 step: 1642, epoch: 49, batch: 24, loss: 0.394849956035614, acc: 85.9375, f1: 72.34917297448837, r: 0.723874864192214
06/02/2019 09:15:37 step: 1647, epoch: 49, batch: 29, loss: 0.17931510508060455, acc: 93.75, f1: 96.18681318681318, r: 0.6078368779452412
06/02/2019 09:15:37 *** evaluating ***
06/02/2019 09:15:37 step: 50, epoch: 49, acc: 61.965811965811966, f1: 26.65651512732562, r: 0.39851982751780285
06/02/2019 09:15:37 *** epoch: 51 ***
06/02/2019 09:15:37 *** training ***
06/02/2019 09:15:38 step: 1655, epoch: 50, batch: 4, loss: 0.19323644042015076, acc: 96.875, f1: 97.54944964967073, r: 0.6345047685358128
06/02/2019 09:15:38 step: 1660, epoch: 50, batch: 9, loss: 0.12550611793994904, acc: 95.3125, f1: 96.09401874785001, r: 0.7090385066960396
06/02/2019 09:15:39 step: 1665, epoch: 50, batch: 14, loss: 0.09945264458656311, acc: 100.0, f1: 100.0, r: 0.6772329019692002
06/02/2019 09:15:39 step: 1670, epoch: 50, batch: 19, loss: 0.23792055249214172, acc: 93.75, f1: 91.10494208084731, r: 0.6205832569994942
06/02/2019 09:15:40 step: 1675, epoch: 50, batch: 24, loss: 0.27547839283943176, acc: 89.0625, f1: 91.05643226282761, r: 0.7124822399567095
06/02/2019 09:15:40 step: 1680, epoch: 50, batch: 29, loss: 0.2994399964809418, acc: 89.0625, f1: 79.06512006512007, r: 0.4891624823948892
06/02/2019 09:15:40 *** evaluating ***
06/02/2019 09:15:40 step: 51, epoch: 50, acc: 62.39316239316239, f1: 27.27656295677804, r: 0.4003700460409292
06/02/2019 09:15:40 *** epoch: 52 ***
06/02/2019 09:15:40 *** training ***
06/02/2019 09:15:41 step: 1688, epoch: 51, batch: 4, loss: 0.288074254989624, acc: 89.0625, f1: 88.04659498207886, r: 0.6920483512466876
06/02/2019 09:15:41 step: 1693, epoch: 51, batch: 9, loss: 0.26162752509117126, acc: 92.1875, f1: 94.74012775842044, r: 0.7570396385489837
06/02/2019 09:15:42 step: 1698, epoch: 51, batch: 14, loss: 0.23716986179351807, acc: 90.625, f1: 90.11514602547211, r: 0.7320858168924569
06/02/2019 09:15:42 step: 1703, epoch: 51, batch: 19, loss: 0.12161566317081451, acc: 98.4375, f1: 99.02885682574917, r: 0.7639704849157938
06/02/2019 09:15:43 step: 1708, epoch: 51, batch: 24, loss: 0.17416949570178986, acc: 96.875, f1: 85.38324420677363, r: 0.7741565195378459
06/02/2019 09:15:43 step: 1713, epoch: 51, batch: 29, loss: 0.17239910364151, acc: 95.3125, f1: 95.1669661575322, r: 0.6308939319708339
06/02/2019 09:15:43 *** evaluating ***
06/02/2019 09:15:43 step: 52, epoch: 51, acc: 61.965811965811966, f1: 25.77424681299209, r: 0.39510809130330493
06/02/2019 09:15:43 *** epoch: 53 ***
06/02/2019 09:15:43 *** training ***
06/02/2019 09:15:44 step: 1721, epoch: 52, batch: 4, loss: 0.30021926760673523, acc: 90.625, f1: 78.99087339659607, r: 0.6008105554164366
06/02/2019 09:15:44 step: 1726, epoch: 52, batch: 9, loss: 0.1927274763584137, acc: 92.1875, f1: 81.45632057396763, r: 0.7667634752041457
06/02/2019 09:15:45 step: 1731, epoch: 52, batch: 14, loss: 0.31190988421440125, acc: 89.0625, f1: 86.80986887508627, r: 0.745609903076752
06/02/2019 09:15:45 step: 1736, epoch: 52, batch: 19, loss: 0.14312651753425598, acc: 95.3125, f1: 93.67845117845117, r: 0.7857975568335348
06/02/2019 09:15:46 step: 1741, epoch: 52, batch: 24, loss: 0.12230119109153748, acc: 93.75, f1: 94.44146079484426, r: 0.6829692329990067
06/02/2019 09:15:46 step: 1746, epoch: 52, batch: 29, loss: 0.15690283477306366, acc: 96.875, f1: 96.83388506917919, r: 0.7339238177393174
06/02/2019 09:15:46 *** evaluating ***
06/02/2019 09:15:47 step: 53, epoch: 52, acc: 62.82051282051282, f1: 28.309293212300485, r: 0.3951790858858361
06/02/2019 09:15:47 *** epoch: 54 ***
06/02/2019 09:15:47 *** training ***
06/02/2019 09:15:47 step: 1754, epoch: 53, batch: 4, loss: 0.11077914386987686, acc: 98.4375, f1: 99.35358758888171, r: 0.6262448813693329
06/02/2019 09:15:47 step: 1759, epoch: 53, batch: 9, loss: 0.23791663348674774, acc: 92.1875, f1: 92.76130237041052, r: 0.660206512200125
06/02/2019 09:15:48 step: 1764, epoch: 53, batch: 14, loss: 0.149411141872406, acc: 93.75, f1: 91.90530303030303, r: 0.8043040236562996
06/02/2019 09:15:48 step: 1769, epoch: 53, batch: 19, loss: 0.14314626157283783, acc: 96.875, f1: 96.42002734107997, r: 0.7264799972104204
06/02/2019 09:15:49 step: 1774, epoch: 53, batch: 24, loss: 0.14990808069705963, acc: 96.875, f1: 96.09375, r: 0.7451718571005668
06/02/2019 09:15:49 step: 1779, epoch: 53, batch: 29, loss: 0.18291378021240234, acc: 93.75, f1: 91.5138888888889, r: 0.7776948642965459
06/02/2019 09:15:49 *** evaluating ***
06/02/2019 09:15:50 step: 54, epoch: 53, acc: 61.53846153846154, f1: 25.73146273804169, r: 0.39248373353277205
06/02/2019 09:15:50 *** epoch: 55 ***
06/02/2019 09:15:50 *** training ***
06/02/2019 09:15:50 step: 1787, epoch: 54, batch: 4, loss: 0.1424965262413025, acc: 96.875, f1: 93.19047619047619, r: 0.646707887330993
06/02/2019 09:15:51 step: 1792, epoch: 54, batch: 9, loss: 0.169371098279953, acc: 93.75, f1: 95.52226931938239, r: 0.7195340068921392
06/02/2019 09:15:51 step: 1797, epoch: 54, batch: 14, loss: 0.21486859023571014, acc: 89.0625, f1: 78.47320746480409, r: 0.6151771322406656
06/02/2019 09:15:51 step: 1802, epoch: 54, batch: 19, loss: 0.15825262665748596, acc: 95.3125, f1: 92.49034147557327, r: 0.7221890480543529
06/02/2019 09:15:52 step: 1807, epoch: 54, batch: 24, loss: 0.06949041783809662, acc: 98.4375, f1: 99.29703975630711, r: 0.6572259633817805
06/02/2019 09:15:52 step: 1812, epoch: 54, batch: 29, loss: 0.09855875372886658, acc: 95.3125, f1: 71.38582826747721, r: 0.73126611949969
06/02/2019 09:15:53 *** evaluating ***
06/02/2019 09:15:53 step: 55, epoch: 54, acc: 61.965811965811966, f1: 26.07186688770658, r: 0.392578676292459
06/02/2019 09:15:53 *** epoch: 56 ***
06/02/2019 09:15:53 *** training ***
06/02/2019 09:15:53 step: 1820, epoch: 55, batch: 4, loss: 0.10085950791835785, acc: 96.875, f1: 96.89163708961844, r: 0.7270283089640799
06/02/2019 09:15:54 step: 1825, epoch: 55, batch: 9, loss: 0.25278258323669434, acc: 92.1875, f1: 81.38941102756893, r: 0.7088409004976768
06/02/2019 09:15:54 step: 1830, epoch: 55, batch: 14, loss: 0.13099703192710876, acc: 96.875, f1: 96.00274725274724, r: 0.8045869115022845
06/02/2019 09:15:55 step: 1835, epoch: 55, batch: 19, loss: 0.19172319769859314, acc: 95.3125, f1: 91.7965367965368, r: 0.5891340281495824
06/02/2019 09:15:55 step: 1840, epoch: 55, batch: 24, loss: 0.19850364327430725, acc: 90.625, f1: 74.54735740450026, r: 0.6009484958037073
06/02/2019 09:15:55 step: 1845, epoch: 55, batch: 29, loss: 0.188838928937912, acc: 95.3125, f1: 81.48629148629148, r: 0.6367702175069981
06/02/2019 09:15:56 *** evaluating ***
06/02/2019 09:15:56 step: 56, epoch: 55, acc: 61.53846153846154, f1: 25.96695560402563, r: 0.39607645275089415
06/02/2019 09:15:56 *** epoch: 57 ***
06/02/2019 09:15:56 *** training ***
06/02/2019 09:15:56 step: 1853, epoch: 56, batch: 4, loss: 0.2273576408624649, acc: 95.3125, f1: 92.80969634230503, r: 0.7968752545809503
06/02/2019 09:15:57 step: 1858, epoch: 56, batch: 9, loss: 0.25048306584358215, acc: 90.625, f1: 87.53757816257817, r: 0.7673218882810678
06/02/2019 09:15:57 step: 1863, epoch: 56, batch: 14, loss: 0.08840525150299072, acc: 95.3125, f1: 97.6497990500548, r: 0.800505385365643
06/02/2019 09:15:58 step: 1868, epoch: 56, batch: 19, loss: 0.23331058025360107, acc: 93.75, f1: 90.66521470248176, r: 0.649667674002124
06/02/2019 09:15:58 step: 1873, epoch: 56, batch: 24, loss: 0.15366987884044647, acc: 95.3125, f1: 92.29665071770334, r: 0.744831584805747
06/02/2019 09:15:59 step: 1878, epoch: 56, batch: 29, loss: 0.12275685369968414, acc: 96.875, f1: 86.33354350567465, r: 0.7124271391898489
06/02/2019 09:15:59 *** evaluating ***
06/02/2019 09:15:59 step: 57, epoch: 56, acc: 61.965811965811966, f1: 25.734246954035143, r: 0.3933207874594448
06/02/2019 09:15:59 *** epoch: 58 ***
06/02/2019 09:15:59 *** training ***
06/02/2019 09:15:59 step: 1886, epoch: 57, batch: 4, loss: 0.1193477064371109, acc: 96.875, f1: 96.8956706545359, r: 0.5662569317063385
06/02/2019 09:16:00 step: 1891, epoch: 57, batch: 9, loss: 0.2504879832267761, acc: 90.625, f1: 89.04110142118863, r: 0.6808421579972262
06/02/2019 09:16:00 step: 1896, epoch: 57, batch: 14, loss: 0.06323296576738358, acc: 100.0, f1: 100.0, r: 0.7269292200730534
06/02/2019 09:16:01 step: 1901, epoch: 57, batch: 19, loss: 0.1749998927116394, acc: 95.3125, f1: 95.83363691007449, r: 0.6962173424513061
06/02/2019 09:16:01 step: 1906, epoch: 57, batch: 24, loss: 0.2122831493616104, acc: 93.75, f1: 95.06222943722945, r: 0.6038234682412891
06/02/2019 09:16:02 step: 1911, epoch: 57, batch: 29, loss: 0.0726410299539566, acc: 100.0, f1: 100.0, r: 0.8093831215728897
06/02/2019 09:16:02 *** evaluating ***
06/02/2019 09:16:02 step: 58, epoch: 57, acc: 61.53846153846154, f1: 25.88918026418027, r: 0.39709993532736176
06/02/2019 09:16:02 *** epoch: 59 ***
06/02/2019 09:16:02 *** training ***
06/02/2019 09:16:02 step: 1919, epoch: 58, batch: 4, loss: 0.05982821434736252, acc: 98.4375, f1: 99.1436925647452, r: 0.799526445989129
06/02/2019 09:16:03 step: 1924, epoch: 58, batch: 9, loss: 0.22320887446403503, acc: 90.625, f1: 93.43805704099822, r: 0.7572674779101769
06/02/2019 09:16:03 step: 1929, epoch: 58, batch: 14, loss: 0.21231450140476227, acc: 92.1875, f1: 94.57402008422416, r: 0.7186802575848735
06/02/2019 09:16:04 step: 1934, epoch: 58, batch: 19, loss: 0.15648657083511353, acc: 95.3125, f1: 96.31423503121616, r: 0.6540860020411798
06/02/2019 09:16:04 step: 1939, epoch: 58, batch: 24, loss: 0.10458475351333618, acc: 96.875, f1: 94.65608465608464, r: 0.580386494971804
06/02/2019 09:16:05 step: 1944, epoch: 58, batch: 29, loss: 0.23560604453086853, acc: 89.0625, f1: 90.99458874458874, r: 0.7937463533844686
06/02/2019 09:16:05 *** evaluating ***
06/02/2019 09:16:05 step: 59, epoch: 58, acc: 61.965811965811966, f1: 26.029890687057016, r: 0.38910197635587784
06/02/2019 09:16:05 *** epoch: 60 ***
06/02/2019 09:16:05 *** training ***
06/02/2019 09:16:06 step: 1952, epoch: 59, batch: 4, loss: 0.2928093373775482, acc: 92.1875, f1: 92.89285714285714, r: 0.6987773443272474
06/02/2019 09:16:06 step: 1957, epoch: 59, batch: 9, loss: 0.11202624440193176, acc: 96.875, f1: 95.81632653061224, r: 0.781749632521308
06/02/2019 09:16:06 step: 1962, epoch: 59, batch: 14, loss: 0.13445793092250824, acc: 95.3125, f1: 90.64713064713065, r: 0.6421374749869125
06/02/2019 09:16:07 step: 1967, epoch: 59, batch: 19, loss: 0.10786186158657074, acc: 98.4375, f1: 98.80745341614906, r: 0.6862768206576962
06/02/2019 09:16:07 step: 1972, epoch: 59, batch: 24, loss: 0.23938366770744324, acc: 93.75, f1: 94.03671725100297, r: 0.6428821712851795
06/02/2019 09:16:08 step: 1977, epoch: 59, batch: 29, loss: 0.23267003893852234, acc: 92.1875, f1: 89.85104669887278, r: 0.7549445553246376
06/02/2019 09:16:08 *** evaluating ***
06/02/2019 09:16:08 step: 60, epoch: 59, acc: 61.53846153846154, f1: 25.6128073419596, r: 0.39444082138011954
06/02/2019 09:16:08 *** epoch: 61 ***
06/02/2019 09:16:08 *** training ***
06/02/2019 09:16:09 step: 1985, epoch: 60, batch: 4, loss: 0.10396642237901688, acc: 98.4375, f1: 97.86096256684492, r: 0.6146236380486357
06/02/2019 09:16:09 step: 1990, epoch: 60, batch: 9, loss: 0.11062631011009216, acc: 95.3125, f1: 93.85874980702566, r: 0.7963255830867316
06/02/2019 09:16:10 step: 1995, epoch: 60, batch: 14, loss: 0.17039842903614044, acc: 95.3125, f1: 95.66466054270933, r: 0.6771809701718019
06/02/2019 09:16:10 step: 2000, epoch: 60, batch: 19, loss: 0.23897480964660645, acc: 90.625, f1: 80.15209592608974, r: 0.6854091487415412
06/02/2019 09:16:10 step: 2005, epoch: 60, batch: 24, loss: 0.11628048866987228, acc: 93.75, f1: 95.21989419741625, r: 0.7040941693333997
06/02/2019 09:16:11 step: 2010, epoch: 60, batch: 29, loss: 0.18194317817687988, acc: 96.875, f1: 83.49498327759197, r: 0.6911875099073543
06/02/2019 09:16:11 *** evaluating ***
06/02/2019 09:16:11 step: 61, epoch: 60, acc: 61.53846153846154, f1: 25.87913517694993, r: 0.3996187157483199
06/02/2019 09:16:11 *** epoch: 62 ***
06/02/2019 09:16:11 *** training ***
06/02/2019 09:16:12 step: 2018, epoch: 61, batch: 4, loss: 0.09794636070728302, acc: 96.875, f1: 85.50936768149883, r: 0.7285893347849892
06/02/2019 09:16:12 step: 2023, epoch: 61, batch: 9, loss: 0.12334991991519928, acc: 96.875, f1: 97.98311184939092, r: 0.7557042530475389
06/02/2019 09:16:13 step: 2028, epoch: 61, batch: 14, loss: 0.14140364527702332, acc: 95.3125, f1: 92.82501124606388, r: 0.7909336991719111
06/02/2019 09:16:13 step: 2033, epoch: 61, batch: 19, loss: 0.09969539940357208, acc: 96.875, f1: 96.65316762090956, r: 0.6979324979040674
06/02/2019 09:16:14 step: 2038, epoch: 61, batch: 24, loss: 0.1332588791847229, acc: 95.3125, f1: 90.63088074715981, r: 0.7409042489222214
06/02/2019 09:16:14 step: 2043, epoch: 61, batch: 29, loss: 0.11815403401851654, acc: 95.3125, f1: 96.76720030493615, r: 0.7725364869811587
06/02/2019 09:16:14 *** evaluating ***
06/02/2019 09:16:14 step: 62, epoch: 61, acc: 61.111111111111114, f1: 26.503106410920864, r: 0.3970045524385968
06/02/2019 09:16:14 *** epoch: 63 ***
06/02/2019 09:16:14 *** training ***
06/02/2019 09:16:15 step: 2051, epoch: 62, batch: 4, loss: 0.20703375339508057, acc: 93.75, f1: 84.44322344322343, r: 0.7496916534095875
06/02/2019 09:16:15 step: 2056, epoch: 62, batch: 9, loss: 0.10222045332193375, acc: 96.875, f1: 96.90402476780186, r: 0.6907702267115053
06/02/2019 09:16:16 step: 2061, epoch: 62, batch: 14, loss: 0.07578625530004501, acc: 98.4375, f1: 99.20634920634922, r: 0.7968283196867578
06/02/2019 09:16:16 step: 2066, epoch: 62, batch: 19, loss: 0.07828553020954132, acc: 98.4375, f1: 99.23521913913127, r: 0.7076233087501292
06/02/2019 09:16:17 step: 2071, epoch: 62, batch: 24, loss: 0.10168498754501343, acc: 95.3125, f1: 96.67405423707945, r: 0.6695892996213136
06/02/2019 09:16:17 step: 2076, epoch: 62, batch: 29, loss: 0.11466000974178314, acc: 95.3125, f1: 96.94991789819376, r: 0.7243221307097134
06/02/2019 09:16:17 *** evaluating ***
06/02/2019 09:16:17 step: 63, epoch: 62, acc: 61.111111111111114, f1: 26.325385939678608, r: 0.39800161079305096
06/02/2019 09:16:17 *** epoch: 64 ***
06/02/2019 09:16:17 *** training ***
06/02/2019 09:16:18 step: 2084, epoch: 63, batch: 4, loss: 0.09488420933485031, acc: 96.875, f1: 96.8586545729403, r: 0.6769786170304336
06/02/2019 09:16:18 step: 2089, epoch: 63, batch: 9, loss: 0.247924342751503, acc: 92.1875, f1: 94.52380952380952, r: 0.7419026208065508
06/02/2019 09:16:19 step: 2094, epoch: 63, batch: 14, loss: 0.11882607638835907, acc: 98.4375, f1: 96.52173913043478, r: 0.6497662644967778
06/02/2019 09:16:19 step: 2099, epoch: 63, batch: 19, loss: 0.1355363428592682, acc: 96.875, f1: 97.99107142857143, r: 0.7443289280526997
06/02/2019 09:16:20 step: 2104, epoch: 63, batch: 24, loss: 0.08310827612876892, acc: 96.875, f1: 94.03145873734108, r: 0.6488653591315761
06/02/2019 09:16:20 step: 2109, epoch: 63, batch: 29, loss: 0.13912025094032288, acc: 95.3125, f1: 97.84150502306186, r: 0.735162605363986
06/02/2019 09:16:20 *** evaluating ***
06/02/2019 09:16:21 step: 64, epoch: 63, acc: 61.53846153846154, f1: 26.628874672098974, r: 0.4003556413570101
06/02/2019 09:16:21 *** epoch: 65 ***
06/02/2019 09:16:21 *** training ***
06/02/2019 09:16:21 step: 2117, epoch: 64, batch: 4, loss: 0.27390527725219727, acc: 93.75, f1: 80.42277669664108, r: 0.6160874554110158
06/02/2019 09:16:21 step: 2122, epoch: 64, batch: 9, loss: 0.17348888516426086, acc: 95.3125, f1: 96.17845117845118, r: 0.7743433881167638
06/02/2019 09:16:22 step: 2127, epoch: 64, batch: 14, loss: 0.06955142319202423, acc: 96.875, f1: 96.29140418614102, r: 0.6226623598061501
06/02/2019 09:16:22 step: 2132, epoch: 64, batch: 19, loss: 0.13276708126068115, acc: 96.875, f1: 98.57142857142858, r: 0.7716293486117108
06/02/2019 09:16:23 step: 2137, epoch: 64, batch: 24, loss: 0.21863898634910583, acc: 89.0625, f1: 86.83608058608058, r: 0.7276949322096928
06/02/2019 09:16:23 step: 2142, epoch: 64, batch: 29, loss: 0.09386961907148361, acc: 96.875, f1: 90.98290598290598, r: 0.7332911407615813
06/02/2019 09:16:23 *** evaluating ***
06/02/2019 09:16:24 step: 65, epoch: 64, acc: 61.111111111111114, f1: 25.726320019544314, r: 0.40580687438738927
06/02/2019 09:16:24 *** epoch: 66 ***
06/02/2019 09:16:24 *** training ***
06/02/2019 09:16:24 step: 2150, epoch: 65, batch: 4, loss: 0.21698243916034698, acc: 93.75, f1: 96.17826449983708, r: 0.7572447933346803
06/02/2019 09:16:25 step: 2155, epoch: 65, batch: 9, loss: 0.3165459930896759, acc: 93.75, f1: 96.6323077030881, r: 0.741472367158861
06/02/2019 09:16:25 step: 2160, epoch: 65, batch: 14, loss: 0.10494019836187363, acc: 96.875, f1: 93.63446653707021, r: 0.6781243772940093
06/02/2019 09:16:25 step: 2165, epoch: 65, batch: 19, loss: 0.08584955334663391, acc: 98.4375, f1: 96.39097744360903, r: 0.6660099846905071
06/02/2019 09:16:26 step: 2170, epoch: 65, batch: 24, loss: 0.16664372384548187, acc: 93.75, f1: 92.69556530426097, r: 0.6577575728509458
06/02/2019 09:16:26 step: 2175, epoch: 65, batch: 29, loss: 0.12042590230703354, acc: 95.3125, f1: 95.74020084224166, r: 0.6651524415807344
06/02/2019 09:16:27 *** evaluating ***
06/02/2019 09:16:27 step: 66, epoch: 65, acc: 61.53846153846154, f1: 26.673676166600696, r: 0.4005468073245487
06/02/2019 09:16:27 *** epoch: 67 ***
06/02/2019 09:16:27 *** training ***
06/02/2019 09:16:27 step: 2183, epoch: 66, batch: 4, loss: 0.1527974009513855, acc: 93.75, f1: 82.86811521664683, r: 0.6398339670843333
06/02/2019 09:16:28 step: 2188, epoch: 66, batch: 9, loss: 0.31031984090805054, acc: 92.1875, f1: 92.92812579577286, r: 0.7263418303596949
06/02/2019 09:16:28 step: 2193, epoch: 66, batch: 14, loss: 0.11349974572658539, acc: 96.875, f1: 84.60111317254174, r: 0.6264008486550812
06/02/2019 09:16:28 step: 2198, epoch: 66, batch: 19, loss: 0.34477701783180237, acc: 90.625, f1: 93.55156213295747, r: 0.7192667238248853
06/02/2019 09:16:29 step: 2203, epoch: 66, batch: 24, loss: 0.2056310921907425, acc: 90.625, f1: 85.63309795968077, r: 0.6432377757247317
06/02/2019 09:16:29 step: 2208, epoch: 66, batch: 29, loss: 0.12288299202919006, acc: 96.875, f1: 96.92752100840336, r: 0.7884190937169002
06/02/2019 09:16:30 *** evaluating ***
06/02/2019 09:16:30 step: 67, epoch: 66, acc: 61.111111111111114, f1: 25.107833479091322, r: 0.3911575667669711
06/02/2019 09:16:30 *** epoch: 68 ***
06/02/2019 09:16:30 *** training ***
06/02/2019 09:16:30 step: 2216, epoch: 67, batch: 4, loss: 0.12744027376174927, acc: 95.3125, f1: 85.66836349924584, r: 0.650584033677912
06/02/2019 09:16:31 step: 2221, epoch: 67, batch: 9, loss: 0.038496579974889755, acc: 100.0, f1: 100.0, r: 0.7340943617251035
06/02/2019 09:16:31 step: 2226, epoch: 67, batch: 14, loss: 0.16877120733261108, acc: 92.1875, f1: 92.12845472307578, r: 0.6338027812188671
06/02/2019 09:16:32 step: 2231, epoch: 67, batch: 19, loss: 0.06530249863862991, acc: 98.4375, f1: 99.24759924759925, r: 0.7200353285573006
06/02/2019 09:16:32 step: 2236, epoch: 67, batch: 24, loss: 0.10303322970867157, acc: 98.4375, f1: 96.95652173913044, r: 0.7172386614356411
06/02/2019 09:16:32 step: 2241, epoch: 67, batch: 29, loss: 0.12458910048007965, acc: 96.875, f1: 95.87914988611853, r: 0.7293943477025355
06/02/2019 09:16:33 *** evaluating ***
06/02/2019 09:16:33 step: 68, epoch: 67, acc: 61.111111111111114, f1: 25.212135073105745, r: 0.38696524317169484
06/02/2019 09:16:33 *** epoch: 69 ***
06/02/2019 09:16:33 *** training ***
06/02/2019 09:16:33 step: 2249, epoch: 68, batch: 4, loss: 0.06316249817609787, acc: 98.4375, f1: 99.20343779478041, r: 0.6728341839027366
06/02/2019 09:16:34 step: 2254, epoch: 68, batch: 9, loss: 0.09579557180404663, acc: 96.875, f1: 96.56391084962513, r: 0.5872228668543752
06/02/2019 09:16:34 step: 2259, epoch: 68, batch: 14, loss: 0.09604337811470032, acc: 98.4375, f1: 99.35710111046173, r: 0.7772617043270506
06/02/2019 09:16:35 step: 2264, epoch: 68, batch: 19, loss: 0.1170167624950409, acc: 93.75, f1: 92.63628936423054, r: 0.7553397747471045
06/02/2019 09:16:35 step: 2269, epoch: 68, batch: 24, loss: 0.06981653720140457, acc: 98.4375, f1: 98.88627115013921, r: 0.7231837189413409
06/02/2019 09:16:36 step: 2274, epoch: 68, batch: 29, loss: 0.055240172892808914, acc: 98.4375, f1: 95.10204081632652, r: 0.6563012937872743
06/02/2019 09:16:36 *** evaluating ***
06/02/2019 09:16:36 step: 69, epoch: 68, acc: 61.965811965811966, f1: 26.128822739054957, r: 0.3896580301893969
06/02/2019 09:16:36 *** epoch: 70 ***
06/02/2019 09:16:36 *** training ***
06/02/2019 09:16:36 step: 2282, epoch: 69, batch: 4, loss: 0.18141265213489532, acc: 93.75, f1: 85.16751244325995, r: 0.6185131810483216
06/02/2019 09:16:37 step: 2287, epoch: 69, batch: 9, loss: 0.11163542419672012, acc: 96.875, f1: 96.45652173913042, r: 0.8076605161616353
06/02/2019 09:16:37 step: 2292, epoch: 69, batch: 14, loss: 0.0919274315237999, acc: 96.875, f1: 97.46300548539355, r: 0.6918708213407042
06/02/2019 09:16:38 step: 2297, epoch: 69, batch: 19, loss: 0.08998849987983704, acc: 95.3125, f1: 96.5764047800661, r: 0.7829850124266299
06/02/2019 09:16:38 step: 2302, epoch: 69, batch: 24, loss: 0.1343829184770584, acc: 95.3125, f1: 94.58912037037037, r: 0.7463149591764843
06/02/2019 09:16:39 step: 2307, epoch: 69, batch: 29, loss: 0.14992545545101166, acc: 96.875, f1: 94.65852130325814, r: 0.5695209471881725
06/02/2019 09:16:39 *** evaluating ***
06/02/2019 09:16:39 step: 70, epoch: 69, acc: 61.965811965811966, f1: 27.23039199178488, r: 0.3918846724221178
06/02/2019 09:16:39 *** epoch: 71 ***
06/02/2019 09:16:39 *** training ***
06/02/2019 09:16:39 step: 2315, epoch: 70, batch: 4, loss: 0.1492416262626648, acc: 90.625, f1: 80.86640211640211, r: 0.7542177099781909
06/02/2019 09:16:40 step: 2320, epoch: 70, batch: 9, loss: 0.07666558027267456, acc: 95.3125, f1: 93.75, r: 0.7725308706359454
06/02/2019 09:16:40 step: 2325, epoch: 70, batch: 14, loss: 0.09904108196496964, acc: 98.4375, f1: 99.20694459329118, r: 0.6361305340940266
06/02/2019 09:16:41 step: 2330, epoch: 70, batch: 19, loss: 0.07826389372348785, acc: 96.875, f1: 95.06294471811714, r: 0.6701881484328599
06/02/2019 09:16:41 step: 2335, epoch: 70, batch: 24, loss: 0.09247587621212006, acc: 96.875, f1: 93.96310596310596, r: 0.7776175548254398
06/02/2019 09:16:42 step: 2340, epoch: 70, batch: 29, loss: 0.10431969165802002, acc: 96.875, f1: 98.16890782408024, r: 0.6832983339213169
06/02/2019 09:16:42 *** evaluating ***
06/02/2019 09:16:42 step: 71, epoch: 70, acc: 62.39316239316239, f1: 28.118935815111314, r: 0.3872644672988854
06/02/2019 09:16:42 *** epoch: 72 ***
06/02/2019 09:16:42 *** training ***
06/02/2019 09:16:43 step: 2348, epoch: 71, batch: 4, loss: 0.07793039083480835, acc: 96.875, f1: 98.25, r: 0.7886876697843356
06/02/2019 09:16:43 step: 2353, epoch: 71, batch: 9, loss: 0.10232949256896973, acc: 95.3125, f1: 92.42152961980548, r: 0.7791590447166856
06/02/2019 09:16:43 step: 2358, epoch: 71, batch: 14, loss: 0.1911177933216095, acc: 93.75, f1: 86.55633120678714, r: 0.5795012572556554
06/02/2019 09:16:44 step: 2363, epoch: 71, batch: 19, loss: 0.12884260714054108, acc: 93.75, f1: 81.5023815023815, r: 0.6703454329976604
06/02/2019 09:16:44 step: 2368, epoch: 71, batch: 24, loss: 0.12332141399383545, acc: 93.75, f1: 91.20518207282913, r: 0.7871892430110214
06/02/2019 09:16:45 step: 2373, epoch: 71, batch: 29, loss: 0.20534195005893707, acc: 93.75, f1: 94.10422910422909, r: 0.6448878705544168
06/02/2019 09:16:45 *** evaluating ***
06/02/2019 09:16:45 step: 72, epoch: 71, acc: 61.111111111111114, f1: 26.56501316802008, r: 0.39099750936183386
06/02/2019 09:16:45 *** epoch: 73 ***
06/02/2019 09:16:45 *** training ***
06/02/2019 09:16:46 step: 2381, epoch: 72, batch: 4, loss: 0.06634904444217682, acc: 98.4375, f1: 99.06896551724138, r: 0.7481632906620989
06/02/2019 09:16:46 step: 2386, epoch: 72, batch: 9, loss: 0.05453789234161377, acc: 98.4375, f1: 99.06629318394025, r: 0.7763768459911664
06/02/2019 09:16:47 step: 2391, epoch: 72, batch: 14, loss: 0.09343347698450089, acc: 98.4375, f1: 99.17516324894031, r: 0.65967391316404
06/02/2019 09:16:47 step: 2396, epoch: 72, batch: 19, loss: 0.08055345714092255, acc: 98.4375, f1: 96.3718820861678, r: 0.6467082374879992
06/02/2019 09:16:47 step: 2401, epoch: 72, batch: 24, loss: 0.14385560154914856, acc: 93.75, f1: 91.90132584918119, r: 0.701080218211736
06/02/2019 09:16:48 step: 2406, epoch: 72, batch: 29, loss: 0.19965189695358276, acc: 95.3125, f1: 92.50797448165868, r: 0.7803782886121262
06/02/2019 09:16:48 *** evaluating ***
06/02/2019 09:16:48 step: 73, epoch: 72, acc: 60.68376068376068, f1: 24.998251133756348, r: 0.386706045331726
06/02/2019 09:16:48 *** epoch: 74 ***
06/02/2019 09:16:48 *** training ***
06/02/2019 09:16:49 step: 2414, epoch: 73, batch: 4, loss: 0.08928340673446655, acc: 98.4375, f1: 98.39734733351754, r: 0.6351333405006127
06/02/2019 09:16:49 step: 2419, epoch: 73, batch: 9, loss: 0.07243155688047409, acc: 100.0, f1: 100.0, r: 0.7956502481640904
06/02/2019 09:16:50 step: 2424, epoch: 73, batch: 14, loss: 0.15514428913593292, acc: 96.875, f1: 84.69286423606185, r: 0.6565098211253143
06/02/2019 09:16:50 step: 2429, epoch: 73, batch: 19, loss: 0.11720628291368484, acc: 98.4375, f1: 96.33699633699635, r: 0.6456148163226798
06/02/2019 09:16:51 step: 2434, epoch: 73, batch: 24, loss: 0.10860393941402435, acc: 95.3125, f1: 96.90594498669034, r: 0.4538097886340934
06/02/2019 09:16:51 step: 2439, epoch: 73, batch: 29, loss: 0.09092919528484344, acc: 95.3125, f1: 94.80231451848475, r: 0.7266966398533852
06/02/2019 09:16:51 *** evaluating ***
06/02/2019 09:16:51 step: 74, epoch: 73, acc: 60.68376068376068, f1: 24.952012605478714, r: 0.3896239559425064
06/02/2019 09:16:51 *** epoch: 75 ***
06/02/2019 09:16:51 *** training ***
06/02/2019 09:16:52 step: 2447, epoch: 74, batch: 4, loss: 0.06638968735933304, acc: 98.4375, f1: 97.90849673202615, r: 0.6253846594184654
06/02/2019 09:16:52 step: 2452, epoch: 74, batch: 9, loss: 0.18462467193603516, acc: 95.3125, f1: 92.18191229060794, r: 0.6413175060332816
06/02/2019 09:16:53 step: 2457, epoch: 74, batch: 14, loss: 0.0541515126824379, acc: 98.4375, f1: 98.31932773109244, r: 0.7132939706100014
06/02/2019 09:16:53 step: 2462, epoch: 74, batch: 19, loss: 0.18186762928962708, acc: 90.625, f1: 92.25572047000618, r: 0.6094108531082293
06/02/2019 09:16:54 step: 2467, epoch: 74, batch: 24, loss: 0.12781846523284912, acc: 93.75, f1: 95.91154970760233, r: 0.7092377090323164
06/02/2019 09:16:54 step: 2472, epoch: 74, batch: 29, loss: 0.099181167781353, acc: 95.3125, f1: 93.0515873015873, r: 0.7600010634892814
06/02/2019 09:16:54 *** evaluating ***
06/02/2019 09:16:54 step: 75, epoch: 74, acc: 60.68376068376068, f1: 24.912835474101918, r: 0.3891314411704641
06/02/2019 09:16:54 *** epoch: 76 ***
06/02/2019 09:16:54 *** training ***
06/02/2019 09:16:55 step: 2480, epoch: 75, batch: 4, loss: 0.05740031972527504, acc: 98.4375, f1: 99.24963924963926, r: 0.6577014613764771
06/02/2019 09:16:55 step: 2485, epoch: 75, batch: 9, loss: 0.06716399639844894, acc: 98.4375, f1: 98.38056680161944, r: 0.7663945640528299
06/02/2019 09:16:56 step: 2490, epoch: 75, batch: 14, loss: 0.10348249971866608, acc: 96.875, f1: 96.84829663090532, r: 0.7161727747560507
06/02/2019 09:16:56 step: 2495, epoch: 75, batch: 19, loss: 0.21197426319122314, acc: 93.75, f1: 90.44934640522875, r: 0.7437948050449033
06/02/2019 09:16:57 step: 2500, epoch: 75, batch: 24, loss: 0.06479001045227051, acc: 96.875, f1: 95.0203622334155, r: 0.7745686144923812
06/02/2019 09:16:57 step: 2505, epoch: 75, batch: 29, loss: 0.03742799162864685, acc: 98.4375, f1: 99.17874396135265, r: 0.7704026120738856
06/02/2019 09:16:57 *** evaluating ***
06/02/2019 09:16:58 step: 76, epoch: 75, acc: 61.111111111111114, f1: 24.75996009199134, r: 0.3884884061123662
06/02/2019 09:16:58 *** epoch: 77 ***
06/02/2019 09:16:58 *** training ***
06/02/2019 09:16:58 step: 2513, epoch: 76, batch: 4, loss: 0.061105068773031235, acc: 98.4375, f1: 98.22082679225537, r: 0.6988959220048108
06/02/2019 09:16:58 step: 2518, epoch: 76, batch: 9, loss: 0.13847790658473969, acc: 93.75, f1: 92.62856424621131, r: 0.7763961802008743
06/02/2019 09:16:59 step: 2523, epoch: 76, batch: 14, loss: 0.06607045978307724, acc: 98.4375, f1: 94.28571428571428, r: 0.63920808049496
06/02/2019 09:16:59 step: 2528, epoch: 76, batch: 19, loss: 0.12205122411251068, acc: 93.75, f1: 96.67049568365358, r: 0.7512780963658602
06/02/2019 09:17:00 step: 2533, epoch: 76, batch: 24, loss: 0.04557851329445839, acc: 98.4375, f1: 96.6604823747681, r: 0.6911927471824837
06/02/2019 09:17:00 step: 2538, epoch: 76, batch: 29, loss: 0.08573504537343979, acc: 95.3125, f1: 84.44163687908296, r: 0.7367101547695404
06/02/2019 09:17:00 *** evaluating ***
06/02/2019 09:17:01 step: 77, epoch: 76, acc: 61.111111111111114, f1: 24.774643829876393, r: 0.37860467382032625
06/02/2019 09:17:01 *** epoch: 78 ***
06/02/2019 09:17:01 *** training ***
06/02/2019 09:17:01 step: 2546, epoch: 77, batch: 4, loss: 0.08209805935621262, acc: 96.875, f1: 96.1012072813315, r: 0.6933010977710313
06/02/2019 09:17:02 step: 2551, epoch: 77, batch: 9, loss: 0.0679706409573555, acc: 96.875, f1: 97.83705949223192, r: 0.6930723882412986
06/02/2019 09:17:02 step: 2556, epoch: 77, batch: 14, loss: 0.12405078113079071, acc: 93.75, f1: 91.00188526608773, r: 0.7375418448935023
06/02/2019 09:17:02 step: 2561, epoch: 77, batch: 19, loss: 0.22257746756076813, acc: 96.875, f1: 91.20915032679738, r: 0.6940532540947805
06/02/2019 09:17:03 step: 2566, epoch: 77, batch: 24, loss: 0.18909698724746704, acc: 95.3125, f1: 93.11730097444384, r: 0.6369992762716834
06/02/2019 09:17:03 step: 2571, epoch: 77, batch: 29, loss: 0.25914624333381653, acc: 90.625, f1: 88.8475975975976, r: 0.7357203962630042
06/02/2019 09:17:04 *** evaluating ***
06/02/2019 09:17:04 step: 78, epoch: 77, acc: 60.256410256410255, f1: 24.963525924203676, r: 0.3880759051089307
06/02/2019 09:17:04 *** epoch: 79 ***
06/02/2019 09:17:04 *** training ***
06/02/2019 09:17:04 step: 2579, epoch: 78, batch: 4, loss: 0.07187382131814957, acc: 98.4375, f1: 96.82539682539682, r: 0.7733045274558591
06/02/2019 09:17:05 step: 2584, epoch: 78, batch: 9, loss: 0.14872199296951294, acc: 96.875, f1: 97.3216942010771, r: 0.7701147803653664
06/02/2019 09:17:05 step: 2589, epoch: 78, batch: 14, loss: 0.07841015607118607, acc: 96.875, f1: 97.52331002331003, r: 0.7749344627737602
06/02/2019 09:17:06 step: 2594, epoch: 78, batch: 19, loss: 0.09069579094648361, acc: 96.875, f1: 96.73350041771094, r: 0.6542743531456543
06/02/2019 09:17:06 step: 2599, epoch: 78, batch: 24, loss: 0.18214742839336395, acc: 95.3125, f1: 92.99633699633699, r: 0.660534248650729
06/02/2019 09:17:06 step: 2604, epoch: 78, batch: 29, loss: 0.07222981750965118, acc: 96.875, f1: 96.71705222835155, r: 0.6677048087036361
06/02/2019 09:17:07 *** evaluating ***
06/02/2019 09:17:07 step: 79, epoch: 78, acc: 60.68376068376068, f1: 25.269876526611057, r: 0.38479481428117646
06/02/2019 09:17:07 *** epoch: 80 ***
06/02/2019 09:17:07 *** training ***
06/02/2019 09:17:07 step: 2612, epoch: 79, batch: 4, loss: 0.10713228583335876, acc: 96.875, f1: 98.1532940743467, r: 0.7587495746411943
06/02/2019 09:17:08 step: 2617, epoch: 79, batch: 9, loss: 0.07640459388494492, acc: 96.875, f1: 97.11188540456834, r: 0.605321272491639
06/02/2019 09:17:08 step: 2622, epoch: 79, batch: 14, loss: 0.040850572288036346, acc: 98.4375, f1: 99.27107959022851, r: 0.7343609570294078
06/02/2019 09:17:09 step: 2627, epoch: 79, batch: 19, loss: 0.06874358654022217, acc: 98.4375, f1: 99.06910132474042, r: 0.6251023099082442
06/02/2019 09:17:09 step: 2632, epoch: 79, batch: 24, loss: 0.16221478581428528, acc: 93.75, f1: 95.13446833549848, r: 0.6697785551522383
06/02/2019 09:17:10 step: 2637, epoch: 79, batch: 29, loss: 0.03867669776082039, acc: 98.4375, f1: 98.99749373433583, r: 0.7318178313525229
06/02/2019 09:17:10 *** evaluating ***
06/02/2019 09:17:10 step: 80, epoch: 79, acc: 60.68376068376068, f1: 24.328066136857807, r: 0.3773101861960339
06/02/2019 09:17:10 *** epoch: 81 ***
06/02/2019 09:17:10 *** training ***
06/02/2019 09:17:10 step: 2645, epoch: 80, batch: 4, loss: 0.15234434604644775, acc: 95.3125, f1: 91.83808578048895, r: 0.7467774764564741
06/02/2019 09:17:11 step: 2650, epoch: 80, batch: 9, loss: 0.20039375126361847, acc: 92.1875, f1: 92.93946874985673, r: 0.7420650393925013
06/02/2019 09:17:11 step: 2655, epoch: 80, batch: 14, loss: 0.1623115986585617, acc: 95.3125, f1: 91.42754206945936, r: 0.6695526827666466
06/02/2019 09:17:12 step: 2660, epoch: 80, batch: 19, loss: 0.13630908727645874, acc: 95.3125, f1: 95.81413210445469, r: 0.7517884043585472
06/02/2019 09:17:12 step: 2665, epoch: 80, batch: 24, loss: 0.2414582073688507, acc: 93.75, f1: 91.22448979591837, r: 0.6208281399581165
06/02/2019 09:17:13 step: 2670, epoch: 80, batch: 29, loss: 0.10531597584486008, acc: 96.875, f1: 85.93137254901961, r: 0.7592317566507134
06/02/2019 09:17:13 *** evaluating ***
06/02/2019 09:17:13 step: 81, epoch: 80, acc: 60.68376068376068, f1: 24.66230384199134, r: 0.3701789185108415
06/02/2019 09:17:13 *** epoch: 82 ***
06/02/2019 09:17:13 *** training ***
06/02/2019 09:17:14 step: 2678, epoch: 81, batch: 4, loss: 0.11305198818445206, acc: 96.875, f1: 93.70859538784067, r: 0.7494966312185278
06/02/2019 09:17:14 step: 2683, epoch: 81, batch: 9, loss: 0.06190698221325874, acc: 100.0, f1: 100.0, r: 0.808032982859101
06/02/2019 09:17:14 step: 2688, epoch: 81, batch: 14, loss: 0.07264899462461472, acc: 98.4375, f1: 98.88888888888889, r: 0.8162994382371257
06/02/2019 09:17:15 step: 2693, epoch: 81, batch: 19, loss: 0.1370696723461151, acc: 95.3125, f1: 96.4935064935065, r: 0.6981063288124806
06/02/2019 09:17:15 step: 2698, epoch: 81, batch: 24, loss: 0.12892383337020874, acc: 93.75, f1: 90.91836734693877, r: 0.6949996827202801
06/02/2019 09:17:16 step: 2703, epoch: 81, batch: 29, loss: 0.03170452639460564, acc: 100.0, f1: 100.0, r: 0.8055159474126496
06/02/2019 09:17:16 *** evaluating ***
06/02/2019 09:17:16 step: 82, epoch: 81, acc: 60.68376068376068, f1: 25.142519838320045, r: 0.375129102684432
06/02/2019 09:17:16 *** epoch: 83 ***
06/02/2019 09:17:16 *** training ***
06/02/2019 09:17:17 step: 2711, epoch: 82, batch: 4, loss: 0.02279624529182911, acc: 100.0, f1: 100.0, r: 0.6573422111070946
06/02/2019 09:17:17 step: 2716, epoch: 82, batch: 9, loss: 0.13273456692695618, acc: 93.75, f1: 90.33736378563965, r: 0.7552692685458843
06/02/2019 09:17:18 step: 2721, epoch: 82, batch: 14, loss: 0.1741992086172104, acc: 93.75, f1: 94.47363489223955, r: 0.6784761648769646
06/02/2019 09:17:18 step: 2726, epoch: 82, batch: 19, loss: 0.08514291048049927, acc: 95.3125, f1: 97.1107703665843, r: 0.6841585107092163
06/02/2019 09:17:18 step: 2731, epoch: 82, batch: 24, loss: 0.14899584650993347, acc: 93.75, f1: 89.91130820399114, r: 0.6242725244597307
06/02/2019 09:17:19 step: 2736, epoch: 82, batch: 29, loss: 0.12427690625190735, acc: 96.875, f1: 93.9670605084139, r: 0.7057253843290441
06/02/2019 09:17:19 *** evaluating ***
06/02/2019 09:17:19 step: 83, epoch: 82, acc: 60.68376068376068, f1: 24.387080942313503, r: 0.37996458310555625
06/02/2019 09:17:19 *** epoch: 84 ***
06/02/2019 09:17:19 *** training ***
06/02/2019 09:17:20 step: 2744, epoch: 83, batch: 4, loss: 0.07618463784456253, acc: 95.3125, f1: 93.74020084224165, r: 0.5817749678545838
06/02/2019 09:17:20 step: 2749, epoch: 83, batch: 9, loss: 0.07325822114944458, acc: 96.875, f1: 96.93747230693043, r: 0.6589479331863248
06/02/2019 09:17:21 step: 2754, epoch: 83, batch: 14, loss: 0.06657151132822037, acc: 98.4375, f1: 99.06878306878308, r: 0.6003594496855031
06/02/2019 09:17:21 step: 2759, epoch: 83, batch: 19, loss: 0.030056197196245193, acc: 100.0, f1: 100.0, r: 0.7807532248291129
06/02/2019 09:17:21 step: 2764, epoch: 83, batch: 24, loss: 0.07650193572044373, acc: 98.4375, f1: 99.23404255319149, r: 0.8287866240831406
06/02/2019 09:17:22 step: 2769, epoch: 83, batch: 29, loss: 0.02773332968354225, acc: 100.0, f1: 100.0, r: 0.7852191341924519
06/02/2019 09:17:22 *** evaluating ***
06/02/2019 09:17:22 step: 84, epoch: 83, acc: 59.401709401709404, f1: 23.8630504993694, r: 0.37193132904123033
06/02/2019 09:17:22 *** epoch: 85 ***
06/02/2019 09:17:22 *** training ***
06/02/2019 09:17:23 step: 2777, epoch: 84, batch: 4, loss: 0.054453931748867035, acc: 96.875, f1: 98.19444444444446, r: 0.7425636922778042
06/02/2019 09:17:23 step: 2782, epoch: 84, batch: 9, loss: 0.17214684188365936, acc: 96.875, f1: 98.33227395518831, r: 0.6934500644017494
06/02/2019 09:17:24 step: 2787, epoch: 84, batch: 14, loss: 0.027956631034612656, acc: 100.0, f1: 100.0, r: 0.5608587215552813
06/02/2019 09:17:24 step: 2792, epoch: 84, batch: 19, loss: 0.041474681347608566, acc: 98.4375, f1: 98.04639804639805, r: 0.7335395980352775
06/02/2019 09:17:25 step: 2797, epoch: 84, batch: 24, loss: 0.1632770597934723, acc: 96.875, f1: 82.6923076923077, r: 0.529240791812753
06/02/2019 09:17:25 step: 2802, epoch: 84, batch: 29, loss: 0.03515607863664627, acc: 100.0, f1: 100.0, r: 0.7476154025689747
06/02/2019 09:17:25 *** evaluating ***
06/02/2019 09:17:25 step: 85, epoch: 84, acc: 59.82905982905983, f1: 24.332750478359067, r: 0.37271258075190344
06/02/2019 09:17:25 *** epoch: 86 ***
06/02/2019 09:17:25 *** training ***
06/02/2019 09:17:26 step: 2810, epoch: 85, batch: 4, loss: 0.07187126576900482, acc: 96.875, f1: 96.8265148894689, r: 0.7053734776740197
06/02/2019 09:17:26 step: 2815, epoch: 85, batch: 9, loss: 0.17805570363998413, acc: 90.625, f1: 86.51449909988808, r: 0.6695422102402159
06/02/2019 09:17:27 step: 2820, epoch: 85, batch: 14, loss: 0.0483681783080101, acc: 98.4375, f1: 98.79336349924586, r: 0.6953790676607532
06/02/2019 09:17:27 step: 2825, epoch: 85, batch: 19, loss: 0.13366904854774475, acc: 92.1875, f1: 92.87684537684538, r: 0.79880109950568
06/02/2019 09:17:28 step: 2830, epoch: 85, batch: 24, loss: 0.04981708154082298, acc: 98.4375, f1: 95.33333333333334, r: 0.817170498438067
06/02/2019 09:17:28 step: 2835, epoch: 85, batch: 29, loss: 0.17454208433628082, acc: 90.625, f1: 88.59401709401709, r: 0.8043350332922519
06/02/2019 09:17:28 *** evaluating ***
06/02/2019 09:17:28 step: 86, epoch: 85, acc: 61.111111111111114, f1: 24.705781711146567, r: 0.3753417317376978
06/02/2019 09:17:28 *** epoch: 87 ***
06/02/2019 09:17:28 *** training ***
06/02/2019 09:17:29 step: 2843, epoch: 86, batch: 4, loss: 0.08974231779575348, acc: 96.875, f1: 97.69420234937476, r: 0.6809841375811352
06/02/2019 09:17:29 step: 2848, epoch: 86, batch: 9, loss: 0.04849288985133171, acc: 98.4375, f1: 87.06896551724138, r: 0.7520817213021247
06/02/2019 09:17:30 step: 2853, epoch: 86, batch: 14, loss: 0.07577124238014221, acc: 96.875, f1: 95.25132275132276, r: 0.7164528815547497
06/02/2019 09:17:30 step: 2858, epoch: 86, batch: 19, loss: 0.08566281199455261, acc: 95.3125, f1: 95.5466244468802, r: 0.7685597821121526
06/02/2019 09:17:31 step: 2863, epoch: 86, batch: 24, loss: 0.09127762913703918, acc: 95.3125, f1: 95.20415445871836, r: 0.6060441560835308
06/02/2019 09:17:31 step: 2868, epoch: 86, batch: 29, loss: 0.2383960634469986, acc: 93.75, f1: 96.52014652014653, r: 0.695371769500174
06/02/2019 09:17:31 *** evaluating ***
06/02/2019 09:17:32 step: 87, epoch: 86, acc: 61.53846153846154, f1: 27.251486436269047, r: 0.37976660408355034
06/02/2019 09:17:32 *** epoch: 88 ***
06/02/2019 09:17:32 *** training ***
06/02/2019 09:17:32 step: 2876, epoch: 87, batch: 4, loss: 0.07259715348482132, acc: 96.875, f1: 95.8846038415366, r: 0.8177314399782104
06/02/2019 09:17:32 step: 2881, epoch: 87, batch: 9, loss: 0.027659572660923004, acc: 100.0, f1: 100.0, r: 0.7435122901628618
06/02/2019 09:17:33 step: 2886, epoch: 87, batch: 14, loss: 0.060416705906391144, acc: 96.875, f1: 98.13925570228092, r: 0.6931932837766199
06/02/2019 09:17:33 step: 2891, epoch: 87, batch: 19, loss: 0.14123116433620453, acc: 95.3125, f1: 94.54139143580137, r: 0.6563146941271716
06/02/2019 09:17:34 step: 2896, epoch: 87, batch: 24, loss: 0.11800828576087952, acc: 95.3125, f1: 96.10949141561387, r: 0.6940311939570677
06/02/2019 09:17:34 step: 2901, epoch: 87, batch: 29, loss: 0.021697035059332848, acc: 100.0, f1: 100.0, r: 0.6385323877840317
06/02/2019 09:17:34 *** evaluating ***
06/02/2019 09:17:35 step: 88, epoch: 87, acc: 61.111111111111114, f1: 25.82689251167512, r: 0.37871629567858245
06/02/2019 09:17:35 *** epoch: 89 ***
06/02/2019 09:17:35 *** training ***
06/02/2019 09:17:35 step: 2909, epoch: 88, batch: 4, loss: 0.09216947853565216, acc: 95.3125, f1: 97.44121561668146, r: 0.8145134251151028
06/02/2019 09:17:35 step: 2914, epoch: 88, batch: 9, loss: 0.11371428519487381, acc: 96.875, f1: 97.36811081638668, r: 0.7946813480182798
06/02/2019 09:17:36 step: 2919, epoch: 88, batch: 14, loss: 0.03586192429065704, acc: 98.4375, f1: 99.28193499622071, r: 0.7791235330798344
06/02/2019 09:17:36 step: 2924, epoch: 88, batch: 19, loss: 0.2905125021934509, acc: 92.1875, f1: 87.2685185185185, r: 0.7138030895663764
06/02/2019 09:17:37 step: 2929, epoch: 88, batch: 24, loss: 0.15094055235385895, acc: 95.3125, f1: 92.63755980861244, r: 0.7462107383967584
06/02/2019 09:17:37 step: 2934, epoch: 88, batch: 29, loss: 0.08314631879329681, acc: 98.4375, f1: 97.38095238095238, r: 0.7936304488149454
06/02/2019 09:17:37 *** evaluating ***
06/02/2019 09:17:38 step: 89, epoch: 88, acc: 61.965811965811966, f1: 26.709481879428466, r: 0.3806074760855832
06/02/2019 09:17:38 *** epoch: 90 ***
06/02/2019 09:17:38 *** training ***
06/02/2019 09:17:38 step: 2942, epoch: 89, batch: 4, loss: 0.07881277799606323, acc: 95.3125, f1: 93.76984126984127, r: 0.6987081708554856
06/02/2019 09:17:39 step: 2947, epoch: 89, batch: 9, loss: 0.06344005465507507, acc: 98.4375, f1: 97.98701298701299, r: 0.8137019170129485
06/02/2019 09:17:39 step: 2952, epoch: 89, batch: 14, loss: 0.06580810248851776, acc: 100.0, f1: 100.0, r: 0.7872696180177599
06/02/2019 09:17:39 step: 2957, epoch: 89, batch: 19, loss: 0.1399998962879181, acc: 93.75, f1: 94.50326632225216, r: 0.7482268628074585
06/02/2019 09:17:40 step: 2962, epoch: 89, batch: 24, loss: 0.0805428996682167, acc: 96.875, f1: 98.43014128728414, r: 0.6764065137311392
06/02/2019 09:17:40 step: 2967, epoch: 89, batch: 29, loss: 0.2032845914363861, acc: 93.75, f1: 92.3015873015873, r: 0.7120449726028604
06/02/2019 09:17:41 *** evaluating ***
06/02/2019 09:17:41 step: 90, epoch: 89, acc: 61.965811965811966, f1: 27.45755665715095, r: 0.3849956861744409
06/02/2019 09:17:41 *** epoch: 91 ***
06/02/2019 09:17:41 *** training ***
06/02/2019 09:17:41 step: 2975, epoch: 90, batch: 4, loss: 0.10261496156454086, acc: 93.75, f1: 94.20327857260706, r: 0.7359396762561402
06/02/2019 09:17:42 step: 2980, epoch: 90, batch: 9, loss: 0.03961116075515747, acc: 98.4375, f1: 96.52173913043478, r: 0.6279696283009598
06/02/2019 09:17:42 step: 2985, epoch: 90, batch: 14, loss: 0.1107855886220932, acc: 96.875, f1: 92.83097854526426, r: 0.7162446995550582
06/02/2019 09:17:43 step: 2990, epoch: 90, batch: 19, loss: 0.06171208992600441, acc: 98.4375, f1: 95.37037037037037, r: 0.7538821073436437
06/02/2019 09:17:43 step: 2995, epoch: 90, batch: 24, loss: 0.07852382212877274, acc: 96.875, f1: 96.86318972033258, r: 0.6930338702102403
06/02/2019 09:17:43 step: 3000, epoch: 90, batch: 29, loss: 0.08069532364606857, acc: 98.4375, f1: 99.07448872966114, r: 0.7412107561579968
06/02/2019 09:17:44 *** evaluating ***
06/02/2019 09:17:44 step: 91, epoch: 90, acc: 61.53846153846154, f1: 25.75369782641147, r: 0.38104285233385293
06/02/2019 09:17:44 *** epoch: 92 ***
06/02/2019 09:17:44 *** training ***
06/02/2019 09:17:44 step: 3008, epoch: 91, batch: 4, loss: 0.06632226705551147, acc: 98.4375, f1: 99.10625620655412, r: 0.7690830415714853
06/02/2019 09:17:45 step: 3013, epoch: 91, batch: 9, loss: 0.05773377791047096, acc: 98.4375, f1: 98.14921920185078, r: 0.6303102688796897
06/02/2019 09:17:45 step: 3018, epoch: 91, batch: 14, loss: 0.04418986663222313, acc: 100.0, f1: 100.0, r: 0.5064942530484746
06/02/2019 09:17:46 step: 3023, epoch: 91, batch: 19, loss: 0.09939971566200256, acc: 96.875, f1: 93.7766410912191, r: 0.6942025961038183
06/02/2019 09:17:46 step: 3028, epoch: 91, batch: 24, loss: 0.02201196551322937, acc: 100.0, f1: 100.0, r: 0.7575450007444768
06/02/2019 09:17:47 step: 3033, epoch: 91, batch: 29, loss: 0.18790075182914734, acc: 92.1875, f1: 81.85685685685687, r: 0.6615492393491099
06/02/2019 09:17:47 *** evaluating ***
06/02/2019 09:17:47 step: 92, epoch: 91, acc: 61.53846153846154, f1: 25.67091393735569, r: 0.3780805796042858
06/02/2019 09:17:47 *** epoch: 93 ***
06/02/2019 09:17:47 *** training ***
06/02/2019 09:17:47 step: 3041, epoch: 92, batch: 4, loss: 0.13459044694900513, acc: 96.875, f1: 96.39094059579236, r: 0.7050650896068713
06/02/2019 09:17:48 step: 3046, epoch: 92, batch: 9, loss: 0.2318444848060608, acc: 95.3125, f1: 94.87293144208037, r: 0.7313407900302106
06/02/2019 09:17:48 step: 3051, epoch: 92, batch: 14, loss: 0.12832766771316528, acc: 95.3125, f1: 96.45691609977324, r: 0.785588615075761
06/02/2019 09:17:49 step: 3056, epoch: 92, batch: 19, loss: 0.21198871731758118, acc: 92.1875, f1: 92.57806583387979, r: 0.6847849777604562
06/02/2019 09:17:49 step: 3061, epoch: 92, batch: 24, loss: 0.070768341422081, acc: 98.4375, f1: 98.47939175670268, r: 0.67311094170507
06/02/2019 09:17:50 step: 3066, epoch: 92, batch: 29, loss: 0.07075804471969604, acc: 98.4375, f1: 99.02818270165209, r: 0.6192274151023841
06/02/2019 09:17:50 *** evaluating ***
06/02/2019 09:17:50 step: 93, epoch: 92, acc: 61.53846153846154, f1: 26.322734474654425, r: 0.3814798098811644
06/02/2019 09:17:50 *** epoch: 94 ***
06/02/2019 09:17:50 *** training ***
06/02/2019 09:17:50 step: 3074, epoch: 93, batch: 4, loss: 0.15186312794685364, acc: 93.75, f1: 91.99434922710503, r: 0.6955981439513098
06/02/2019 09:17:51 step: 3079, epoch: 93, batch: 9, loss: 0.050873130559921265, acc: 98.4375, f1: 99.25490196078431, r: 0.8087364356464417
06/02/2019 09:17:51 step: 3084, epoch: 93, batch: 14, loss: 0.053862445056438446, acc: 100.0, f1: 100.0, r: 0.6698117167081753
06/02/2019 09:17:52 step: 3089, epoch: 93, batch: 19, loss: 0.05568063259124756, acc: 100.0, f1: 100.0, r: 0.6751967823585215
06/02/2019 09:17:52 step: 3094, epoch: 93, batch: 24, loss: 0.18098033964633942, acc: 95.3125, f1: 96.02447816733532, r: 0.7475884087901854
06/02/2019 09:17:53 step: 3099, epoch: 93, batch: 29, loss: 0.046537138521671295, acc: 98.4375, f1: 99.38775510204081, r: 0.7599518480148938
06/02/2019 09:17:53 *** evaluating ***
06/02/2019 09:17:53 step: 94, epoch: 93, acc: 61.111111111111114, f1: 25.019184913694897, r: 0.37419066872549717
06/02/2019 09:17:53 *** epoch: 95 ***
06/02/2019 09:17:53 *** training ***
06/02/2019 09:17:54 step: 3107, epoch: 94, batch: 4, loss: 0.09468412399291992, acc: 96.875, f1: 93.68802959780405, r: 0.6840056600196898
06/02/2019 09:17:54 step: 3112, epoch: 94, batch: 9, loss: 0.053085364401340485, acc: 98.4375, f1: 93.65079365079366, r: 0.709276056751102
06/02/2019 09:17:54 step: 3117, epoch: 94, batch: 14, loss: 0.08756539970636368, acc: 96.875, f1: 97.62780112044818, r: 0.8108704532065851
06/02/2019 09:17:55 step: 3122, epoch: 94, batch: 19, loss: 0.13107505440711975, acc: 95.3125, f1: 95.57178932178932, r: 0.6834137477224579
06/02/2019 09:17:55 step: 3127, epoch: 94, batch: 24, loss: 0.11010348051786423, acc: 95.3125, f1: 96.7320099255583, r: 0.7703865321110258
06/02/2019 09:17:56 step: 3132, epoch: 94, batch: 29, loss: 0.09990184009075165, acc: 93.75, f1: 93.26077097505669, r: 0.7051304018568524
06/02/2019 09:17:56 *** evaluating ***
06/02/2019 09:17:56 step: 95, epoch: 94, acc: 60.68376068376068, f1: 25.367532803198966, r: 0.3744984315310598
06/02/2019 09:17:56 *** epoch: 96 ***
06/02/2019 09:17:56 *** training ***
06/02/2019 09:17:57 step: 3140, epoch: 95, batch: 4, loss: 0.051242396235466, acc: 98.4375, f1: 99.10934020860189, r: 0.6846523668138561
06/02/2019 09:17:57 step: 3145, epoch: 95, batch: 9, loss: 0.08540768176317215, acc: 95.3125, f1: 91.49775061539768, r: 0.819448112465002
06/02/2019 09:17:58 step: 3150, epoch: 95, batch: 14, loss: 0.09598179161548615, acc: 95.3125, f1: 82.44336695717249, r: 0.6957311176894073
06/02/2019 09:17:58 step: 3155, epoch: 95, batch: 19, loss: 0.120796337723732, acc: 96.875, f1: 97.38361520276413, r: 0.7440923456563652
06/02/2019 09:17:58 step: 3160, epoch: 95, batch: 24, loss: 0.04559699445962906, acc: 98.4375, f1: 97.98701298701299, r: 0.8276146649186084
06/02/2019 09:17:59 step: 3165, epoch: 95, batch: 29, loss: 0.03959948942065239, acc: 100.0, f1: 100.0, r: 0.7837826389823872
06/02/2019 09:17:59 *** evaluating ***
06/02/2019 09:17:59 step: 96, epoch: 95, acc: 61.111111111111114, f1: 25.75049875761952, r: 0.37715966058944034
06/02/2019 09:17:59 *** epoch: 97 ***
06/02/2019 09:17:59 *** training ***
06/02/2019 09:18:00 step: 3173, epoch: 96, batch: 4, loss: 0.08090115338563919, acc: 96.875, f1: 92.9787234042553, r: 0.553891424375447
06/02/2019 09:18:00 step: 3178, epoch: 96, batch: 9, loss: 0.03649979084730148, acc: 100.0, f1: 100.0, r: 0.700302518183566
06/02/2019 09:18:01 step: 3183, epoch: 96, batch: 14, loss: 0.10892268270254135, acc: 96.875, f1: 96.30385487528346, r: 0.6889476938727456
06/02/2019 09:18:01 step: 3188, epoch: 96, batch: 19, loss: 0.045104704797267914, acc: 98.4375, f1: 99.22027290448344, r: 0.7346562210363091
06/02/2019 09:18:02 step: 3193, epoch: 96, batch: 24, loss: 0.07110225409269333, acc: 98.4375, f1: 87.0, r: 0.7827303342895539
06/02/2019 09:18:02 step: 3198, epoch: 96, batch: 29, loss: 0.2282189428806305, acc: 90.625, f1: 93.82646276595746, r: 0.813352625078884
06/02/2019 09:18:02 *** evaluating ***
06/02/2019 09:18:02 step: 97, epoch: 96, acc: 61.53846153846154, f1: 25.86314611314611, r: 0.36983636384365415
06/02/2019 09:18:02 *** epoch: 98 ***
06/02/2019 09:18:02 *** training ***
06/02/2019 09:18:03 step: 3206, epoch: 97, batch: 4, loss: 0.13816235959529877, acc: 95.3125, f1: 97.15838645443274, r: 0.6597242717025421
06/02/2019 09:18:03 step: 3211, epoch: 97, batch: 9, loss: 0.08976871520280838, acc: 96.875, f1: 97.32830098962822, r: 0.703622398165701
06/02/2019 09:18:04 step: 3216, epoch: 97, batch: 14, loss: 0.042962826788425446, acc: 100.0, f1: 100.0, r: 0.711750988016535
06/02/2019 09:18:04 step: 3221, epoch: 97, batch: 19, loss: 0.07991746068000793, acc: 96.875, f1: 96.40151515151516, r: 0.749083241452993
06/02/2019 09:18:05 step: 3226, epoch: 97, batch: 24, loss: 0.028831128031015396, acc: 100.0, f1: 100.0, r: 0.6998268445978201
06/02/2019 09:18:05 step: 3231, epoch: 97, batch: 29, loss: 0.14185155928134918, acc: 93.75, f1: 80.57753649858913, r: 0.6892586328306729
06/02/2019 09:18:05 *** evaluating ***
06/02/2019 09:18:06 step: 98, epoch: 97, acc: 61.111111111111114, f1: 25.27566656750414, r: 0.37467939135197753
06/02/2019 09:18:06 *** epoch: 99 ***
06/02/2019 09:18:06 *** training ***
06/02/2019 09:18:06 step: 3239, epoch: 98, batch: 4, loss: 0.03617053106427193, acc: 100.0, f1: 100.0, r: 0.8054172409623532
06/02/2019 09:18:06 step: 3244, epoch: 98, batch: 9, loss: 0.11299668252468109, acc: 95.3125, f1: 95.53457398284985, r: 0.7962973072923641
06/02/2019 09:18:07 step: 3249, epoch: 98, batch: 14, loss: 0.06163209676742554, acc: 98.4375, f1: 96.92307692307692, r: 0.6554864593909374
06/02/2019 09:18:07 step: 3254, epoch: 98, batch: 19, loss: 0.043773625046014786, acc: 100.0, f1: 100.0, r: 0.6647519224386418
06/02/2019 09:18:08 step: 3259, epoch: 98, batch: 24, loss: 0.025856636464595795, acc: 100.0, f1: 100.0, r: 0.7561227492548318
06/02/2019 09:18:08 step: 3264, epoch: 98, batch: 29, loss: 0.17244133353233337, acc: 95.3125, f1: 93.37999944455437, r: 0.8066742678180936
06/02/2019 09:18:08 *** evaluating ***
06/02/2019 09:18:09 step: 99, epoch: 98, acc: 61.53846153846154, f1: 26.08774913097343, r: 0.3767720818011097
06/02/2019 09:18:09 *** epoch: 100 ***
06/02/2019 09:18:09 *** training ***
06/02/2019 09:18:09 step: 3272, epoch: 99, batch: 4, loss: 0.1660119593143463, acc: 95.3125, f1: 95.30489855482665, r: 0.5780885755095974
06/02/2019 09:18:09 step: 3277, epoch: 99, batch: 9, loss: 0.02408282458782196, acc: 100.0, f1: 100.0, r: 0.691838974930395
06/02/2019 09:18:10 step: 3282, epoch: 99, batch: 14, loss: 0.211173415184021, acc: 92.1875, f1: 90.21790534395576, r: 0.6690297040077906
06/02/2019 09:18:10 step: 3287, epoch: 99, batch: 19, loss: 0.2120472937822342, acc: 96.875, f1: 94.67543859649123, r: 0.7696018601545374
06/02/2019 09:18:11 step: 3292, epoch: 99, batch: 24, loss: 0.08493268489837646, acc: 95.3125, f1: 95.31382364266462, r: 0.6023855045462847
06/02/2019 09:18:11 step: 3297, epoch: 99, batch: 29, loss: 0.13059094548225403, acc: 95.3125, f1: 94.277950310559, r: 0.7880609255605956
06/02/2019 09:18:11 *** evaluating ***
06/02/2019 09:18:12 step: 100, epoch: 99, acc: 61.53846153846154, f1: 25.749118683901294, r: 0.37352844283372083
06/02/2019 09:18:12 *** epoch: 101 ***
06/02/2019 09:18:12 *** training ***
06/02/2019 09:18:12 step: 3305, epoch: 100, batch: 4, loss: 0.0773276537656784, acc: 98.4375, f1: 98.64135864135865, r: 0.6799386846761172
06/02/2019 09:18:13 step: 3310, epoch: 100, batch: 9, loss: 0.16866138577461243, acc: 93.75, f1: 91.29495900924474, r: 0.6605435189007882
06/02/2019 09:18:13 step: 3315, epoch: 100, batch: 14, loss: 0.01449962891638279, acc: 100.0, f1: 100.0, r: 0.7581830606395648
06/02/2019 09:18:13 step: 3320, epoch: 100, batch: 19, loss: 0.13910509645938873, acc: 96.875, f1: 98.32882882882883, r: 0.7889696168380947
06/02/2019 09:18:14 step: 3325, epoch: 100, batch: 24, loss: 0.11307521164417267, acc: 96.875, f1: 98.13438438438439, r: 0.7724344080214474
06/02/2019 09:18:14 step: 3330, epoch: 100, batch: 29, loss: 0.07309162616729736, acc: 98.4375, f1: 98.91156462585035, r: 0.7959876572765884
06/02/2019 09:18:15 *** evaluating ***
06/02/2019 09:18:15 step: 101, epoch: 100, acc: 61.53846153846154, f1: 25.749118683901294, r: 0.3733367505112258
06/02/2019 09:18:15 *** epoch: 102 ***
06/02/2019 09:18:15 *** training ***
06/02/2019 09:18:15 step: 3338, epoch: 101, batch: 4, loss: 0.09657582640647888, acc: 96.875, f1: 94.67329545454545, r: 0.7523238144793595
06/02/2019 09:18:16 step: 3343, epoch: 101, batch: 9, loss: 0.05228317528963089, acc: 98.4375, f1: 98.96396914930355, r: 0.5912221476795926
06/02/2019 09:18:16 step: 3348, epoch: 101, batch: 14, loss: 0.01861323043704033, acc: 100.0, f1: 100.0, r: 0.8287863907146262
06/02/2019 09:18:17 step: 3353, epoch: 101, batch: 19, loss: 0.036223433911800385, acc: 98.4375, f1: 95.33333333333334, r: 0.7146139223153721
06/02/2019 09:18:17 step: 3358, epoch: 101, batch: 24, loss: 0.05180773884057999, acc: 96.875, f1: 96.78750368405541, r: 0.7836556745562037
06/02/2019 09:18:17 step: 3363, epoch: 101, batch: 29, loss: 0.061046287417411804, acc: 96.875, f1: 96.1576354679803, r: 0.7514015840396514
06/02/2019 09:18:18 *** evaluating ***
06/02/2019 09:18:18 step: 102, epoch: 101, acc: 61.111111111111114, f1: 25.548676883022107, r: 0.3759755578978607
06/02/2019 09:18:18 *** epoch: 103 ***
06/02/2019 09:18:18 *** training ***
06/02/2019 09:18:18 step: 3371, epoch: 102, batch: 4, loss: 0.022211264818906784, acc: 100.0, f1: 100.0, r: 0.687924396870016
06/02/2019 09:18:19 step: 3376, epoch: 102, batch: 9, loss: 0.03068266063928604, acc: 98.4375, f1: 99.22067268252665, r: 0.8320506671687123
06/02/2019 09:18:19 step: 3381, epoch: 102, batch: 14, loss: 0.03355248272418976, acc: 100.0, f1: 100.0, r: 0.7446298835386165
06/02/2019 09:18:20 step: 3386, epoch: 102, batch: 19, loss: 0.12912654876708984, acc: 95.3125, f1: 95.5163418750959, r: 0.684532149615875
06/02/2019 09:18:20 step: 3391, epoch: 102, batch: 24, loss: 0.05850043520331383, acc: 95.3125, f1: 94.43551333576909, r: 0.8209523319461597
06/02/2019 09:18:20 step: 3396, epoch: 102, batch: 29, loss: 0.06358754634857178, acc: 96.875, f1: 94.98708010335916, r: 0.8266340507107701
06/02/2019 09:18:21 *** evaluating ***
06/02/2019 09:18:21 step: 103, epoch: 102, acc: 61.53846153846154, f1: 25.813081015305485, r: 0.3738552548402525
06/02/2019 09:18:21 *** epoch: 104 ***
06/02/2019 09:18:21 *** training ***
06/02/2019 09:18:21 step: 3404, epoch: 103, batch: 4, loss: 0.09262505918741226, acc: 93.75, f1: 89.74270353302612, r: 0.7152059922195407
06/02/2019 09:18:22 step: 3409, epoch: 103, batch: 9, loss: 0.04709724336862564, acc: 100.0, f1: 100.0, r: 0.6731115256724536
06/02/2019 09:18:22 step: 3414, epoch: 103, batch: 14, loss: 0.06899003684520721, acc: 96.875, f1: 98.40686006203248, r: 0.6629730219297085
06/02/2019 09:18:23 step: 3419, epoch: 103, batch: 19, loss: 0.038298770785331726, acc: 98.4375, f1: 98.62318840579711, r: 0.7718369977276988
06/02/2019 09:18:23 step: 3424, epoch: 103, batch: 24, loss: 0.06324877589941025, acc: 96.875, f1: 98.51851851851852, r: 0.6625705062275982
06/02/2019 09:18:24 step: 3429, epoch: 103, batch: 29, loss: 0.07532081007957458, acc: 96.875, f1: 95.44366744366745, r: 0.7073333590453366
06/02/2019 09:18:24 *** evaluating ***
06/02/2019 09:18:24 step: 104, epoch: 103, acc: 61.53846153846154, f1: 25.899787739943243, r: 0.38140102474966736
06/02/2019 09:18:24 *** epoch: 105 ***
06/02/2019 09:18:24 *** training ***
06/02/2019 09:18:24 step: 3437, epoch: 104, batch: 4, loss: 0.11157786101102829, acc: 95.3125, f1: 94.43101685118492, r: 0.6574299861460117
06/02/2019 09:18:25 step: 3442, epoch: 104, batch: 9, loss: 0.18562521040439606, acc: 96.875, f1: 97.45208098941269, r: 0.7860225321165129
06/02/2019 09:18:25 step: 3447, epoch: 104, batch: 14, loss: 0.040330104529857635, acc: 98.4375, f1: 99.05329593267882, r: 0.7947282894662113
06/02/2019 09:18:26 step: 3452, epoch: 104, batch: 19, loss: 0.11782458424568176, acc: 96.875, f1: 94.87179487179488, r: 0.6092430682716538
06/02/2019 09:18:26 step: 3457, epoch: 104, batch: 24, loss: 0.136931374669075, acc: 96.875, f1: 98.37848932676519, r: 0.7935754121935218
06/02/2019 09:18:27 step: 3462, epoch: 104, batch: 29, loss: 0.06237661838531494, acc: 98.4375, f1: 99.25925925925925, r: 0.7715588380479416
06/02/2019 09:18:27 *** evaluating ***
06/02/2019 09:18:27 step: 105, epoch: 104, acc: 62.39316239316239, f1: 28.113526521872096, r: 0.3823832295895543
06/02/2019 09:18:27 *** epoch: 106 ***
06/02/2019 09:18:27 *** training ***
06/02/2019 09:18:27 step: 3470, epoch: 105, batch: 4, loss: 0.05471733585000038, acc: 98.4375, f1: 99.25961082107261, r: 0.722689650062272
06/02/2019 09:18:28 step: 3475, epoch: 105, batch: 9, loss: 0.045396238565444946, acc: 96.875, f1: 95.52772868058133, r: 0.6861085221053496
06/02/2019 09:18:28 step: 3480, epoch: 105, batch: 14, loss: 0.04839882627129555, acc: 98.4375, f1: 99.04761904761905, r: 0.8180031588350501
06/02/2019 09:18:29 step: 3485, epoch: 105, batch: 19, loss: 0.04088988155126572, acc: 98.4375, f1: 97.72499163599866, r: 0.6868003588360995
06/02/2019 09:18:29 step: 3490, epoch: 105, batch: 24, loss: 0.12736746668815613, acc: 96.875, f1: 97.10996240601504, r: 0.7821559887070835
06/02/2019 09:18:30 step: 3495, epoch: 105, batch: 29, loss: 0.1503080576658249, acc: 93.75, f1: 94.6814161591704, r: 0.8105598048846943
06/02/2019 09:18:30 *** evaluating ***
06/02/2019 09:18:30 step: 106, epoch: 105, acc: 61.965811965811966, f1: 27.327197017250903, r: 0.3810717876356712
06/02/2019 09:18:30 *** epoch: 107 ***
06/02/2019 09:18:30 *** training ***
06/02/2019 09:18:31 step: 3503, epoch: 106, batch: 4, loss: 0.08395551890134811, acc: 96.875, f1: 98.44924812030075, r: 0.788853324896494
06/02/2019 09:18:31 step: 3508, epoch: 106, batch: 9, loss: 0.05177835747599602, acc: 100.0, f1: 100.0, r: 0.7141889472722684
06/02/2019 09:18:31 step: 3513, epoch: 106, batch: 14, loss: 0.097877137362957, acc: 95.3125, f1: 95.70436507936509, r: 0.6993144950503013
06/02/2019 09:18:32 step: 3518, epoch: 106, batch: 19, loss: 0.05566519498825073, acc: 98.4375, f1: 99.0599876314162, r: 0.6948416751227473
06/02/2019 09:18:32 step: 3523, epoch: 106, batch: 24, loss: 0.061696819961071014, acc: 96.875, f1: 98.6670530281078, r: 0.6754039246841229
06/02/2019 09:18:33 step: 3528, epoch: 106, batch: 29, loss: 0.008565237745642662, acc: 100.0, f1: 100.0, r: 0.6818141850680492
06/02/2019 09:18:33 *** evaluating ***
06/02/2019 09:18:33 step: 107, epoch: 106, acc: 62.39316239316239, f1: 27.240575114560055, r: 0.37392727985684626
06/02/2019 09:18:33 *** epoch: 108 ***
06/02/2019 09:18:33 *** training ***
06/02/2019 09:18:34 step: 3536, epoch: 107, batch: 4, loss: 0.05476992577314377, acc: 98.4375, f1: 94.70899470899471, r: 0.6186601439044721
06/02/2019 09:18:34 step: 3541, epoch: 107, batch: 9, loss: 0.10351721942424774, acc: 96.875, f1: 97.99647417571946, r: 0.7670848103086225
06/02/2019 09:18:35 step: 3546, epoch: 107, batch: 14, loss: 0.08498843014240265, acc: 96.875, f1: 96.78073249501821, r: 0.6641173442278074
06/02/2019 09:18:35 step: 3551, epoch: 107, batch: 19, loss: 0.06032443791627884, acc: 98.4375, f1: 96.4625850340136, r: 0.6837110799546053
06/02/2019 09:18:35 step: 3556, epoch: 107, batch: 24, loss: 0.014116857200860977, acc: 100.0, f1: 100.0, r: 0.7148896772628389
06/02/2019 09:18:36 step: 3561, epoch: 107, batch: 29, loss: 0.1394895315170288, acc: 96.875, f1: 98.71875, r: 0.761277360230678
06/02/2019 09:18:36 *** evaluating ***
06/02/2019 09:18:36 step: 108, epoch: 107, acc: 62.39316239316239, f1: 27.39450776612476, r: 0.3784886546748586
06/02/2019 09:18:36 *** epoch: 109 ***
06/02/2019 09:18:36 *** training ***
06/02/2019 09:18:37 step: 3569, epoch: 108, batch: 4, loss: 0.06636624783277512, acc: 98.4375, f1: 99.15966386554622, r: 0.7524119454941007
06/02/2019 09:18:37 step: 3574, epoch: 108, batch: 9, loss: 0.04652681201696396, acc: 98.4375, f1: 98.89968824500276, r: 0.685209711273081
06/02/2019 09:18:38 step: 3579, epoch: 108, batch: 14, loss: 0.08328460156917572, acc: 98.4375, f1: 99.16582406471183, r: 0.8311446538504684
06/02/2019 09:18:38 step: 3584, epoch: 108, batch: 19, loss: 0.03289688378572464, acc: 100.0, f1: 100.0, r: 0.6548756845528876
06/02/2019 09:18:39 step: 3589, epoch: 108, batch: 24, loss: 0.03767794743180275, acc: 98.4375, f1: 98.8422035480859, r: 0.6994144601183918
06/02/2019 09:18:39 step: 3594, epoch: 108, batch: 29, loss: 0.10424678027629852, acc: 96.875, f1: 91.21621621621621, r: 0.5883011493297554
06/02/2019 09:18:39 *** evaluating ***
06/02/2019 09:18:39 step: 109, epoch: 108, acc: 62.39316239316239, f1: 28.066716942779234, r: 0.37877417076070036
06/02/2019 09:18:39 *** epoch: 110 ***
06/02/2019 09:18:39 *** training ***
06/02/2019 09:18:40 step: 3602, epoch: 109, batch: 4, loss: 0.11611544340848923, acc: 96.875, f1: 94.95552066980639, r: 0.7432522964272356
06/02/2019 09:18:40 step: 3607, epoch: 109, batch: 9, loss: 0.05549940839409828, acc: 96.875, f1: 92.40158397391102, r: 0.6035567305492858
06/02/2019 09:18:41 step: 3612, epoch: 109, batch: 14, loss: 0.05257898569107056, acc: 98.4375, f1: 96.57142857142857, r: 0.6708915686683254
06/02/2019 09:18:41 step: 3617, epoch: 109, batch: 19, loss: 0.019737370312213898, acc: 100.0, f1: 100.0, r: 0.7472249852440761
06/02/2019 09:18:42 step: 3622, epoch: 109, batch: 24, loss: 0.021437957882881165, acc: 98.4375, f1: 95.33333333333334, r: 0.8003511800450153
06/02/2019 09:18:42 step: 3627, epoch: 109, batch: 29, loss: 0.05297885462641716, acc: 98.4375, f1: 99.35764469722315, r: 0.6446186821567319
06/02/2019 09:18:42 *** evaluating ***
06/02/2019 09:18:42 step: 110, epoch: 109, acc: 61.965811965811966, f1: 27.232713836862843, r: 0.3807263632081682
06/02/2019 09:18:42 *** epoch: 111 ***
06/02/2019 09:18:42 *** training ***
06/02/2019 09:18:43 step: 3635, epoch: 110, batch: 4, loss: 0.030335785821080208, acc: 98.4375, f1: 97.43008314436885, r: 0.7306329171801281
06/02/2019 09:18:43 step: 3640, epoch: 110, batch: 9, loss: 0.027102798223495483, acc: 100.0, f1: 100.0, r: 0.6579112133025531
06/02/2019 09:18:44 step: 3645, epoch: 110, batch: 14, loss: 0.04345894604921341, acc: 98.4375, f1: 99.28698752228165, r: 0.641517313569463
06/02/2019 09:18:44 step: 3650, epoch: 110, batch: 19, loss: 0.11055034399032593, acc: 96.875, f1: 98.44348659003832, r: 0.7910067328702196
06/02/2019 09:18:45 step: 3655, epoch: 110, batch: 24, loss: 0.09215322136878967, acc: 95.3125, f1: 94.98242705570293, r: 0.8108046438744847
06/02/2019 09:18:45 step: 3660, epoch: 110, batch: 29, loss: 0.06403612345457077, acc: 98.4375, f1: 99.22027290448344, r: 0.7431611249636076
06/02/2019 09:18:45 *** evaluating ***
06/02/2019 09:18:46 step: 111, epoch: 110, acc: 62.39316239316239, f1: 27.39450776612476, r: 0.3748103672570537
06/02/2019 09:18:46 *** epoch: 112 ***
06/02/2019 09:18:46 *** training ***
06/02/2019 09:18:46 step: 3668, epoch: 111, batch: 4, loss: 0.05229615420103073, acc: 98.4375, f1: 97.98701298701299, r: 0.7719651802068839
06/02/2019 09:18:47 step: 3673, epoch: 111, batch: 9, loss: 0.049536000937223434, acc: 100.0, f1: 100.0, r: 0.8066827755150603
06/02/2019 09:18:47 step: 3678, epoch: 111, batch: 14, loss: 0.02405496872961521, acc: 100.0, f1: 100.0, r: 0.7063990847200311
06/02/2019 09:18:47 step: 3683, epoch: 111, batch: 19, loss: 0.0344211682677269, acc: 98.4375, f1: 99.17516324894031, r: 0.8396480087510121
06/02/2019 09:18:48 step: 3688, epoch: 111, batch: 24, loss: 0.07813122123479843, acc: 98.4375, f1: 98.98296465802187, r: 0.6236226249589026
06/02/2019 09:18:48 step: 3693, epoch: 111, batch: 29, loss: 0.08966459333896637, acc: 95.3125, f1: 96.05374396135267, r: 0.800566484824808
06/02/2019 09:18:49 *** evaluating ***
06/02/2019 09:18:49 step: 112, epoch: 111, acc: 61.53846153846154, f1: 27.332196472222048, r: 0.37364275263581626
06/02/2019 09:18:49 *** epoch: 113 ***
06/02/2019 09:18:49 *** training ***
06/02/2019 09:18:49 step: 3701, epoch: 112, batch: 4, loss: 0.060040391981601715, acc: 100.0, f1: 100.0, r: 0.7144807350786837
06/02/2019 09:18:50 step: 3706, epoch: 112, batch: 9, loss: 0.07161863893270493, acc: 96.875, f1: 98.27998088867655, r: 0.686542620693649
06/02/2019 09:18:50 step: 3711, epoch: 112, batch: 14, loss: 0.011727714911103249, acc: 100.0, f1: 100.0, r: 0.7007545366312916
06/02/2019 09:18:51 step: 3716, epoch: 112, batch: 19, loss: 0.08900315314531326, acc: 98.4375, f1: 99.31899641577061, r: 0.7704609574216437
06/02/2019 09:18:51 step: 3721, epoch: 112, batch: 24, loss: 0.0298939049243927, acc: 100.0, f1: 100.0, r: 0.7297596942415514
06/02/2019 09:18:51 step: 3726, epoch: 112, batch: 29, loss: 0.055887870490550995, acc: 96.875, f1: 95.87182241317579, r: 0.6909893774843557
06/02/2019 09:18:52 *** evaluating ***
06/02/2019 09:18:52 step: 113, epoch: 112, acc: 61.53846153846154, f1: 25.772314637329806, r: 0.3768144005890317
06/02/2019 09:18:52 *** epoch: 114 ***
06/02/2019 09:18:52 *** training ***
06/02/2019 09:18:52 step: 3734, epoch: 113, batch: 4, loss: 0.011960665695369244, acc: 100.0, f1: 100.0, r: 0.6892482849995274
06/02/2019 09:18:53 step: 3739, epoch: 113, batch: 9, loss: 0.09090033918619156, acc: 95.3125, f1: 94.25155950904285, r: 0.6974504558484846
06/02/2019 09:18:53 step: 3744, epoch: 113, batch: 14, loss: 0.022494452074170113, acc: 100.0, f1: 100.0, r: 0.8454333530486802
06/02/2019 09:18:54 step: 3749, epoch: 113, batch: 19, loss: 0.016889894381165504, acc: 100.0, f1: 100.0, r: 0.7142851254357915
06/02/2019 09:18:54 step: 3754, epoch: 113, batch: 24, loss: 0.08406095206737518, acc: 95.3125, f1: 95.69712324288983, r: 0.773308405143191
06/02/2019 09:18:55 step: 3759, epoch: 113, batch: 29, loss: 0.04129978269338608, acc: 98.4375, f1: 99.28193499622071, r: 0.7842432730849369
06/02/2019 09:18:55 *** evaluating ***
06/02/2019 09:18:55 step: 114, epoch: 113, acc: 61.965811965811966, f1: 25.951713768617136, r: 0.38095891326295567
06/02/2019 09:18:55 *** epoch: 115 ***
06/02/2019 09:18:55 *** training ***
06/02/2019 09:18:55 step: 3767, epoch: 114, batch: 4, loss: 0.049985677003860474, acc: 96.875, f1: 97.86324786324786, r: 0.6667267666535422
06/02/2019 09:18:56 step: 3772, epoch: 114, batch: 9, loss: 0.01657683029770851, acc: 100.0, f1: 100.0, r: 0.8086210868592255
06/02/2019 09:18:56 step: 3777, epoch: 114, batch: 14, loss: 0.13478058576583862, acc: 95.3125, f1: 93.84658861045668, r: 0.6806169467505199
06/02/2019 09:18:57 step: 3782, epoch: 114, batch: 19, loss: 0.02352764829993248, acc: 100.0, f1: 100.0, r: 0.6455739751618985
06/02/2019 09:18:57 step: 3787, epoch: 114, batch: 24, loss: 0.12118004262447357, acc: 98.4375, f1: 98.796992481203, r: 0.6763857764666324
06/02/2019 09:18:58 step: 3792, epoch: 114, batch: 29, loss: 0.027993226423859596, acc: 100.0, f1: 100.0, r: 0.6838469421565029
06/02/2019 09:18:58 *** evaluating ***
06/02/2019 09:18:58 step: 115, epoch: 114, acc: 61.53846153846154, f1: 25.439178911752215, r: 0.38050947836192833
06/02/2019 09:18:58 *** epoch: 116 ***
06/02/2019 09:18:58 *** training ***
06/02/2019 09:18:58 step: 3800, epoch: 115, batch: 4, loss: 0.08741772174835205, acc: 96.875, f1: 94.90927419354838, r: 0.7194244115556415
06/02/2019 09:18:59 step: 3805, epoch: 115, batch: 9, loss: 0.01979019306600094, acc: 100.0, f1: 100.0, r: 0.7272420837715678
06/02/2019 09:18:59 step: 3810, epoch: 115, batch: 14, loss: 0.022951042279601097, acc: 100.0, f1: 100.0, r: 0.6251234990161704
06/02/2019 09:19:00 step: 3815, epoch: 115, batch: 19, loss: 0.044190920889377594, acc: 98.4375, f1: 98.58678955453149, r: 0.7053265532949158
06/02/2019 09:19:00 step: 3820, epoch: 115, batch: 24, loss: 0.01947380229830742, acc: 100.0, f1: 100.0, r: 0.7766935586534137
06/02/2019 09:19:01 step: 3825, epoch: 115, batch: 29, loss: 0.03657778725028038, acc: 98.4375, f1: 97.38775510204081, r: 0.6041800599715819
06/02/2019 09:19:01 *** evaluating ***
06/02/2019 09:19:01 step: 116, epoch: 115, acc: 61.53846153846154, f1: 25.9241047653178, r: 0.37816972214625594
06/02/2019 09:19:01 *** epoch: 117 ***
06/02/2019 09:19:01 *** training ***
06/02/2019 09:19:01 step: 3833, epoch: 116, batch: 4, loss: 0.1208522766828537, acc: 93.75, f1: 96.21428571428571, r: 0.637008626234769
06/02/2019 09:19:02 step: 3838, epoch: 116, batch: 9, loss: 0.044815827161073685, acc: 98.4375, f1: 93.93939393939394, r: 0.6074049560525479
06/02/2019 09:19:02 step: 3843, epoch: 116, batch: 14, loss: 0.08862970024347305, acc: 96.875, f1: 96.74769674769675, r: 0.6543188784658971
06/02/2019 09:19:03 step: 3848, epoch: 116, batch: 19, loss: 0.056205082684755325, acc: 98.4375, f1: 99.15307012081206, r: 0.7047363587612505
06/02/2019 09:19:03 step: 3853, epoch: 116, batch: 24, loss: 0.039643432945013046, acc: 98.4375, f1: 97.07792207792207, r: 0.8244270467665176
06/02/2019 09:19:04 step: 3858, epoch: 116, batch: 29, loss: 0.02712368592619896, acc: 98.4375, f1: 99.22027290448344, r: 0.6855955078899282
06/02/2019 09:19:04 *** evaluating ***
06/02/2019 09:19:04 step: 117, epoch: 116, acc: 61.965811965811966, f1: 26.577357004364217, r: 0.38208028199529354
06/02/2019 09:19:04 *** epoch: 118 ***
06/02/2019 09:19:04 *** training ***
06/02/2019 09:19:05 step: 3866, epoch: 117, batch: 4, loss: 0.09677096456289291, acc: 96.875, f1: 98.20599904129568, r: 0.6835986837893164
06/02/2019 09:19:05 step: 3871, epoch: 117, batch: 9, loss: 0.16608008742332458, acc: 96.875, f1: 98.14471774501567, r: 0.7331711844169084
06/02/2019 09:19:05 step: 3876, epoch: 117, batch: 14, loss: 0.16448353230953217, acc: 95.3125, f1: 93.09048041231087, r: 0.6295804996504778
06/02/2019 09:19:06 step: 3881, epoch: 117, batch: 19, loss: 0.013140648603439331, acc: 100.0, f1: 100.0, r: 0.7550417439817144
06/02/2019 09:19:06 step: 3886, epoch: 117, batch: 24, loss: 0.022586388513445854, acc: 100.0, f1: 100.0, r: 0.6461577700646324
06/02/2019 09:19:07 step: 3891, epoch: 117, batch: 29, loss: 0.05668232589960098, acc: 96.875, f1: 98.30299908424908, r: 0.7758222012531493
06/02/2019 09:19:07 *** evaluating ***
06/02/2019 09:19:07 step: 118, epoch: 117, acc: 61.111111111111114, f1: 25.69915178082155, r: 0.3754903999253744
06/02/2019 09:19:07 *** epoch: 119 ***
06/02/2019 09:19:07 *** training ***
06/02/2019 09:19:08 step: 3899, epoch: 118, batch: 4, loss: 0.08047659695148468, acc: 96.875, f1: 95.88056680161942, r: 0.7371660085883582
06/02/2019 09:19:08 step: 3904, epoch: 118, batch: 9, loss: 0.015859317034482956, acc: 100.0, f1: 100.0, r: 0.8213150447930979
06/02/2019 09:19:08 step: 3909, epoch: 118, batch: 14, loss: 0.10313587635755539, acc: 95.3125, f1: 92.86767286767287, r: 0.6656530873764657
06/02/2019 09:19:09 step: 3914, epoch: 118, batch: 19, loss: 0.08542157709598541, acc: 96.875, f1: 97.70291112396376, r: 0.7309423307012451
06/02/2019 09:19:09 step: 3919, epoch: 118, batch: 24, loss: 0.031679604202508926, acc: 98.4375, f1: 98.72122762148338, r: 0.8158280087687907
06/02/2019 09:19:10 step: 3924, epoch: 118, batch: 29, loss: 0.03059181198477745, acc: 98.4375, f1: 95.03105590062113, r: 0.6913900817732946
06/02/2019 09:19:10 *** evaluating ***
06/02/2019 09:19:10 step: 119, epoch: 118, acc: 61.111111111111114, f1: 25.731667003554158, r: 0.37772944514391865
06/02/2019 09:19:10 *** epoch: 120 ***
06/02/2019 09:19:10 *** training ***
06/02/2019 09:19:11 step: 3932, epoch: 119, batch: 4, loss: 0.1011880561709404, acc: 98.4375, f1: 98.54875283446712, r: 0.6720132919049453
06/02/2019 09:19:11 step: 3937, epoch: 119, batch: 9, loss: 0.01092564594000578, acc: 100.0, f1: 100.0, r: 0.8066031040356332
06/02/2019 09:19:12 step: 3942, epoch: 119, batch: 14, loss: 0.12293464690446854, acc: 95.3125, f1: 94.49198717948718, r: 0.756481485588178
06/02/2019 09:19:12 step: 3947, epoch: 119, batch: 19, loss: 0.08514027297496796, acc: 96.875, f1: 96.20286110174887, r: 0.794693718109545
06/02/2019 09:19:12 step: 3952, epoch: 119, batch: 24, loss: 0.09678828716278076, acc: 96.875, f1: 96.59074079559255, r: 0.6350006215614189
06/02/2019 09:19:13 step: 3957, epoch: 119, batch: 29, loss: 0.03524146229028702, acc: 98.4375, f1: 96.83890577507599, r: 0.6996559542252829
06/02/2019 09:19:13 *** evaluating ***
06/02/2019 09:19:13 step: 120, epoch: 119, acc: 61.965811965811966, f1: 25.943572056376933, r: 0.37724003982169374
06/02/2019 09:19:13 *** epoch: 121 ***
06/02/2019 09:19:13 *** training ***
06/02/2019 09:19:14 step: 3965, epoch: 120, batch: 4, loss: 0.25290346145629883, acc: 95.3125, f1: 95.74672597754066, r: 0.66401875787531
06/02/2019 09:19:14 step: 3970, epoch: 120, batch: 9, loss: 0.014270463958382607, acc: 100.0, f1: 100.0, r: 0.7323413205223065
06/02/2019 09:19:15 step: 3975, epoch: 120, batch: 14, loss: 0.024370558559894562, acc: 100.0, f1: 100.0, r: 0.6394360875057234
06/02/2019 09:19:15 step: 3980, epoch: 120, batch: 19, loss: 0.029223047196865082, acc: 100.0, f1: 100.0, r: 0.7122824539835667
06/02/2019 09:19:16 step: 3985, epoch: 120, batch: 24, loss: 0.0329095683991909, acc: 98.4375, f1: 85.03401360544218, r: 0.600572956877966
06/02/2019 09:19:16 step: 3990, epoch: 120, batch: 29, loss: 0.10198420286178589, acc: 96.875, f1: 93.58275404202139, r: 0.5839342629163534
06/02/2019 09:19:16 *** evaluating ***
06/02/2019 09:19:16 step: 121, epoch: 120, acc: 61.965811965811966, f1: 25.964126071884692, r: 0.3844670092258859
06/02/2019 09:19:16 *** epoch: 122 ***
06/02/2019 09:19:16 *** training ***
06/02/2019 09:19:17 step: 3998, epoch: 121, batch: 4, loss: 0.06907255947589874, acc: 98.4375, f1: 97.96918767507003, r: 0.7790194481218752
06/02/2019 09:19:17 step: 4003, epoch: 121, batch: 9, loss: 0.09265951067209244, acc: 98.4375, f1: 98.8795518207283, r: 0.710358270216089
06/02/2019 09:19:18 step: 4008, epoch: 121, batch: 14, loss: 0.09186246246099472, acc: 95.3125, f1: 95.75171886936593, r: 0.7728219299968031
06/02/2019 09:19:18 step: 4013, epoch: 121, batch: 19, loss: 0.15516462922096252, acc: 93.75, f1: 90.742784992785, r: 0.5847849465249974
06/02/2019 09:19:19 step: 4018, epoch: 121, batch: 24, loss: 0.05768037959933281, acc: 96.875, f1: 93.30357142857143, r: 0.797079138250146
06/02/2019 09:19:19 step: 4023, epoch: 121, batch: 29, loss: 0.03985941410064697, acc: 100.0, f1: 100.0, r: 0.6207102373723997
06/02/2019 09:19:19 *** evaluating ***
06/02/2019 09:19:19 step: 122, epoch: 121, acc: 61.111111111111114, f1: 25.800995758549078, r: 0.3812364095400078
06/02/2019 09:19:19 *** epoch: 123 ***
06/02/2019 09:19:19 *** training ***
06/02/2019 09:19:20 step: 4031, epoch: 122, batch: 4, loss: 0.09721343219280243, acc: 96.875, f1: 93.58768794874273, r: 0.6718274278443522
06/02/2019 09:19:20 step: 4036, epoch: 122, batch: 9, loss: 0.06818078458309174, acc: 96.875, f1: 95.85137085137086, r: 0.7203032278942745
06/02/2019 09:19:21 step: 4041, epoch: 122, batch: 14, loss: 0.03812476247549057, acc: 98.4375, f1: 99.19437939110071, r: 0.6928972263864642
06/02/2019 09:19:21 step: 4046, epoch: 122, batch: 19, loss: 0.06899934262037277, acc: 96.875, f1: 94.31633407243163, r: 0.75066368028941
06/02/2019 09:19:22 step: 4051, epoch: 122, batch: 24, loss: 0.027717556804418564, acc: 100.0, f1: 100.0, r: 0.8447682022133882
06/02/2019 09:19:22 step: 4056, epoch: 122, batch: 29, loss: 0.029076911509037018, acc: 100.0, f1: 100.0, r: 0.7190333956533542
06/02/2019 09:19:22 *** evaluating ***
06/02/2019 09:19:23 step: 123, epoch: 122, acc: 61.53846153846154, f1: 25.602443871613644, r: 0.37742612818064875
06/02/2019 09:19:23 *** epoch: 124 ***
06/02/2019 09:19:23 *** training ***
06/02/2019 09:19:23 step: 4064, epoch: 123, batch: 4, loss: 0.1334601193666458, acc: 95.3125, f1: 82.0054945054945, r: 0.7569199941371938
06/02/2019 09:19:24 step: 4069, epoch: 123, batch: 9, loss: 0.13416153192520142, acc: 96.875, f1: 94.52144494161301, r: 0.7019975266186606
06/02/2019 09:19:24 step: 4074, epoch: 123, batch: 14, loss: 0.017393456771969795, acc: 100.0, f1: 100.0, r: 0.6996091987145336
06/02/2019 09:19:25 step: 4079, epoch: 123, batch: 19, loss: 0.028019681572914124, acc: 100.0, f1: 100.0, r: 0.6306087741393904
06/02/2019 09:19:25 step: 4084, epoch: 123, batch: 24, loss: 0.04858287051320076, acc: 98.4375, f1: 97.20730397422128, r: 0.6305227707103248
06/02/2019 09:19:25 step: 4089, epoch: 123, batch: 29, loss: 0.029997363686561584, acc: 98.4375, f1: 98.95657511124752, r: 0.7091231818016195
06/02/2019 09:19:26 *** evaluating ***
06/02/2019 09:19:26 step: 124, epoch: 123, acc: 61.965811965811966, f1: 26.80404657148843, r: 0.37979214616145657
06/02/2019 09:19:26 *** epoch: 125 ***
06/02/2019 09:19:26 *** training ***
06/02/2019 09:19:26 step: 4097, epoch: 124, batch: 4, loss: 0.026825860142707825, acc: 100.0, f1: 100.0, r: 0.7786570873221942
06/02/2019 09:19:27 step: 4102, epoch: 124, batch: 9, loss: 0.0619938001036644, acc: 95.3125, f1: 92.1366385552432, r: 0.7188110698355411
06/02/2019 09:19:27 step: 4107, epoch: 124, batch: 14, loss: 0.03698287531733513, acc: 100.0, f1: 100.0, r: 0.7248332044067871
06/02/2019 09:19:28 step: 4112, epoch: 124, batch: 19, loss: 0.028843874111771584, acc: 100.0, f1: 100.0, r: 0.714012716373779
06/02/2019 09:19:28 step: 4117, epoch: 124, batch: 24, loss: 0.054951462894678116, acc: 98.4375, f1: 93.93939393939394, r: 0.69306238864788
06/02/2019 09:19:29 step: 4122, epoch: 124, batch: 29, loss: 0.04746514558792114, acc: 98.4375, f1: 96.4625850340136, r: 0.6729998173876961
06/02/2019 09:19:29 *** evaluating ***
06/02/2019 09:19:29 step: 125, epoch: 124, acc: 61.111111111111114, f1: 26.47739651416122, r: 0.3796327632909708
06/02/2019 09:19:29 *** epoch: 126 ***
06/02/2019 09:19:29 *** training ***
06/02/2019 09:19:29 step: 4130, epoch: 125, batch: 4, loss: 0.05075163394212723, acc: 98.4375, f1: 99.23404255319149, r: 0.7581202901963389
06/02/2019 09:19:30 step: 4135, epoch: 125, batch: 9, loss: 0.13597339391708374, acc: 95.3125, f1: 97.7960102960103, r: 0.7341699725111227
06/02/2019 09:19:30 step: 4140, epoch: 125, batch: 14, loss: 0.027595877647399902, acc: 100.0, f1: 100.0, r: 0.8054012201497048
06/02/2019 09:19:31 step: 4145, epoch: 125, batch: 19, loss: 0.014603283256292343, acc: 100.0, f1: 100.0, r: 0.6584481619068729
06/02/2019 09:19:31 step: 4150, epoch: 125, batch: 24, loss: 0.19421428442001343, acc: 96.875, f1: 94.41919191919193, r: 0.767330175793786
06/02/2019 09:19:32 step: 4155, epoch: 125, batch: 29, loss: 0.31826528906822205, acc: 92.1875, f1: 89.60317460317462, r: 0.6587770992620923
06/02/2019 09:19:32 *** evaluating ***
06/02/2019 09:19:32 step: 126, epoch: 125, acc: 61.111111111111114, f1: 25.69915178082155, r: 0.38484647189527005
06/02/2019 09:19:32 *** epoch: 127 ***
06/02/2019 09:19:32 *** training ***
06/02/2019 09:19:32 step: 4163, epoch: 126, batch: 4, loss: 0.05268705263733864, acc: 98.4375, f1: 95.23809523809523, r: 0.7765706469784269
06/02/2019 09:19:33 step: 4168, epoch: 126, batch: 9, loss: 0.05782487988471985, acc: 95.3125, f1: 94.97412008281574, r: 0.6879229480120383
06/02/2019 09:19:33 step: 4173, epoch: 126, batch: 14, loss: 0.08239715546369553, acc: 96.875, f1: 97.57882882882882, r: 0.8299050970428031
06/02/2019 09:19:34 step: 4178, epoch: 126, batch: 19, loss: 0.03421274945139885, acc: 98.4375, f1: 98.03030303030302, r: 0.7484507645414676
06/02/2019 09:19:34 step: 4183, epoch: 126, batch: 24, loss: 0.03955428674817085, acc: 98.4375, f1: 99.1140642303433, r: 0.7924365353335181
06/02/2019 09:19:35 step: 4188, epoch: 126, batch: 29, loss: 0.062423981726169586, acc: 98.4375, f1: 99.17935428139509, r: 0.6633704329930337
06/02/2019 09:19:35 *** evaluating ***
06/02/2019 09:19:35 step: 127, epoch: 126, acc: 61.53846153846154, f1: 26.181325893429708, r: 0.3876070604598703
06/02/2019 09:19:35 *** epoch: 128 ***
06/02/2019 09:19:35 *** training ***
06/02/2019 09:19:35 step: 4196, epoch: 127, batch: 4, loss: 0.03490874916315079, acc: 100.0, f1: 100.0, r: 0.7620143579324005
06/02/2019 09:19:36 step: 4201, epoch: 127, batch: 9, loss: 0.07884149253368378, acc: 96.875, f1: 95.24727192929959, r: 0.6871670582086304
06/02/2019 09:19:36 step: 4206, epoch: 127, batch: 14, loss: 0.06399492919445038, acc: 96.875, f1: 96.91712228658041, r: 0.6549369801596824
06/02/2019 09:19:37 step: 4211, epoch: 127, batch: 19, loss: 0.02689998969435692, acc: 100.0, f1: 100.0, r: 0.736842113006389
06/02/2019 09:19:37 step: 4216, epoch: 127, batch: 24, loss: 0.04145387187600136, acc: 98.4375, f1: 99.1282554211616, r: 0.7743932758735927
06/02/2019 09:19:38 step: 4221, epoch: 127, batch: 29, loss: 0.054976750165224075, acc: 98.4375, f1: 97.67080745341615, r: 0.7840980011410805
06/02/2019 09:19:38 *** evaluating ***
06/02/2019 09:19:38 step: 128, epoch: 127, acc: 61.965811965811966, f1: 26.852995863061796, r: 0.38961987675069687
06/02/2019 09:19:38 *** epoch: 129 ***
06/02/2019 09:19:38 *** training ***
06/02/2019 09:19:39 step: 4229, epoch: 128, batch: 4, loss: 0.07762613147497177, acc: 98.4375, f1: 99.12246741515034, r: 0.7282054198157616
06/02/2019 09:19:39 step: 4234, epoch: 128, batch: 9, loss: 0.06767585128545761, acc: 98.4375, f1: 95.58823529411764, r: 0.8241803487919049
06/02/2019 09:19:39 step: 4239, epoch: 128, batch: 14, loss: 0.18486273288726807, acc: 96.875, f1: 97.16576597744361, r: 0.7225924869030529
06/02/2019 09:19:40 step: 4244, epoch: 128, batch: 19, loss: 0.035568151623010635, acc: 98.4375, f1: 99.23404255319149, r: 0.8195166630396781
06/02/2019 09:19:40 step: 4249, epoch: 128, batch: 24, loss: 0.05722477659583092, acc: 98.4375, f1: 97.92008757526, r: 0.6625108373025905
06/02/2019 09:19:41 step: 4254, epoch: 128, batch: 29, loss: 0.07635270804166794, acc: 96.875, f1: 97.20238095238096, r: 0.6936930380382343
06/02/2019 09:19:41 *** evaluating ***
06/02/2019 09:19:41 step: 129, epoch: 128, acc: 61.53846153846154, f1: 25.91930441222894, r: 0.3869927801913005
06/02/2019 09:19:41 *** epoch: 130 ***
06/02/2019 09:19:41 *** training ***
06/02/2019 09:19:42 step: 4262, epoch: 129, batch: 4, loss: 0.025823377072811127, acc: 98.4375, f1: 97.86096256684492, r: 0.7208316843012877
06/02/2019 09:19:42 step: 4267, epoch: 129, batch: 9, loss: 0.027754580602049828, acc: 98.4375, f1: 98.95657511124752, r: 0.6743220831438979
06/02/2019 09:19:43 step: 4272, epoch: 129, batch: 14, loss: 0.020646892488002777, acc: 100.0, f1: 100.0, r: 0.8283286393578035
06/02/2019 09:19:43 step: 4277, epoch: 129, batch: 19, loss: 0.04362644627690315, acc: 98.4375, f1: 99.36415785472389, r: 0.6474087808327094
06/02/2019 09:19:43 step: 4282, epoch: 129, batch: 24, loss: 0.13014763593673706, acc: 96.875, f1: 98.07692307692308, r: 0.6115928432573594
06/02/2019 09:19:44 step: 4287, epoch: 129, batch: 29, loss: 0.011519575491547585, acc: 100.0, f1: 100.0, r: 0.7642628898055591
06/02/2019 09:19:44 *** evaluating ***
06/02/2019 09:19:44 step: 130, epoch: 129, acc: 61.965811965811966, f1: 26.4523425958627, r: 0.38297040804322047
06/02/2019 09:19:44 *** epoch: 131 ***
06/02/2019 09:19:44 *** training ***
06/02/2019 09:19:45 step: 4295, epoch: 130, batch: 4, loss: 0.033308904618024826, acc: 98.4375, f1: 86.95652173913044, r: 0.7197408451008835
06/02/2019 09:19:45 step: 4300, epoch: 130, batch: 9, loss: 0.01980249397456646, acc: 100.0, f1: 100.0, r: 0.6193207017845942
06/02/2019 09:19:46 step: 4305, epoch: 130, batch: 14, loss: 0.04928484931588173, acc: 98.4375, f1: 99.23404255319149, r: 0.8134694080204361
06/02/2019 09:19:46 step: 4310, epoch: 130, batch: 19, loss: 0.04522594064474106, acc: 98.4375, f1: 99.1864406779661, r: 0.6725261629006699
06/02/2019 09:19:46 step: 4315, epoch: 130, batch: 24, loss: 0.1245579794049263, acc: 95.3125, f1: 91.59013605442176, r: 0.659116567354231
06/02/2019 09:19:47 step: 4320, epoch: 130, batch: 29, loss: 0.00683477520942688, acc: 100.0, f1: 100.0, r: 0.6642125458367893
06/02/2019 09:19:47 *** evaluating ***
06/02/2019 09:19:47 step: 131, epoch: 130, acc: 61.111111111111114, f1: 24.367715617715618, r: 0.37720165554255297
06/02/2019 09:19:47 *** epoch: 132 ***
06/02/2019 09:19:47 *** training ***
06/02/2019 09:19:48 step: 4328, epoch: 131, batch: 4, loss: 0.06049078702926636, acc: 96.875, f1: 97.38775510204081, r: 0.6571900814799095
06/02/2019 09:19:48 step: 4333, epoch: 131, batch: 9, loss: 0.07087992876768112, acc: 96.875, f1: 97.32228555757968, r: 0.6908462917562486
06/02/2019 09:19:49 step: 4338, epoch: 131, batch: 14, loss: 0.03876470401883125, acc: 100.0, f1: 100.0, r: 0.7587777265170994
06/02/2019 09:19:49 step: 4343, epoch: 131, batch: 19, loss: 0.011017028242349625, acc: 100.0, f1: 100.0, r: 0.8662184173169465
06/02/2019 09:19:50 step: 4348, epoch: 131, batch: 24, loss: 0.02031901851296425, acc: 100.0, f1: 100.0, r: 0.7655875322026208
06/02/2019 09:19:50 step: 4353, epoch: 131, batch: 29, loss: 0.06910457462072372, acc: 98.4375, f1: 97.38775510204081, r: 0.6703883344171648
06/02/2019 09:19:50 *** evaluating ***
06/02/2019 09:19:50 step: 132, epoch: 131, acc: 61.965811965811966, f1: 26.204852790140365, r: 0.37579219144959514
06/02/2019 09:19:50 *** epoch: 133 ***
06/02/2019 09:19:50 *** training ***
06/02/2019 09:19:51 step: 4361, epoch: 132, batch: 4, loss: 0.1279125213623047, acc: 96.875, f1: 97.08617023902289, r: 0.6200876099418495
06/02/2019 09:19:51 step: 4366, epoch: 132, batch: 9, loss: 0.1034671887755394, acc: 96.875, f1: 97.48133858060027, r: 0.6620900044858483
06/02/2019 09:19:52 step: 4371, epoch: 132, batch: 14, loss: 0.02458994835615158, acc: 100.0, f1: 100.0, r: 0.7568752652931957
06/02/2019 09:19:52 step: 4376, epoch: 132, batch: 19, loss: 0.02703498676419258, acc: 100.0, f1: 100.0, r: 0.7648912766424497
06/02/2019 09:19:53 step: 4381, epoch: 132, batch: 24, loss: 0.03881958872079849, acc: 100.0, f1: 100.0, r: 0.769865933085472
06/02/2019 09:19:53 step: 4386, epoch: 132, batch: 29, loss: 0.07142335921525955, acc: 98.4375, f1: 99.30607651912979, r: 0.8040895630666325
06/02/2019 09:19:53 *** evaluating ***
06/02/2019 09:19:53 step: 133, epoch: 132, acc: 61.53846153846154, f1: 24.84426197260167, r: 0.37537755552771646
06/02/2019 09:19:53 *** epoch: 134 ***
06/02/2019 09:19:53 *** training ***
06/02/2019 09:19:54 step: 4394, epoch: 133, batch: 4, loss: 0.11478167772293091, acc: 95.3125, f1: 85.31323877068557, r: 0.7303171454837907
06/02/2019 09:19:54 step: 4399, epoch: 133, batch: 9, loss: 0.05803669989109039, acc: 96.875, f1: 95.99354893472541, r: 0.694753665564039
06/02/2019 09:19:55 step: 4404, epoch: 133, batch: 14, loss: 0.025306079536676407, acc: 98.4375, f1: 97.16216216216216, r: 0.8094351140352077
06/02/2019 09:19:55 step: 4409, epoch: 133, batch: 19, loss: 0.015569571405649185, acc: 100.0, f1: 100.0, r: 0.7510763690969724
06/02/2019 09:19:56 step: 4414, epoch: 133, batch: 24, loss: 0.03718661516904831, acc: 98.4375, f1: 99.21142369991475, r: 0.7834038984218865
06/02/2019 09:19:56 step: 4419, epoch: 133, batch: 29, loss: 0.03101821057498455, acc: 100.0, f1: 100.0, r: 0.7219631287268597
06/02/2019 09:19:56 *** evaluating ***
06/02/2019 09:19:57 step: 134, epoch: 133, acc: 61.53846153846154, f1: 25.74854458014523, r: 0.373129273879149
06/02/2019 09:19:57 *** epoch: 135 ***
06/02/2019 09:19:57 *** training ***
06/02/2019 09:19:57 step: 4427, epoch: 134, batch: 4, loss: 0.019372163340449333, acc: 100.0, f1: 100.0, r: 0.8193819538555195
06/02/2019 09:19:57 step: 4432, epoch: 134, batch: 9, loss: 0.11124289035797119, acc: 96.875, f1: 95.93073593073593, r: 0.7283330786586941
06/02/2019 09:19:58 step: 4437, epoch: 134, batch: 14, loss: 0.05937262251973152, acc: 98.4375, f1: 98.6670530281078, r: 0.701953150356081
06/02/2019 09:19:58 step: 4442, epoch: 134, batch: 19, loss: 0.03249117732048035, acc: 100.0, f1: 100.0, r: 0.8413780835657306
06/02/2019 09:19:59 step: 4447, epoch: 134, batch: 24, loss: 0.04172912985086441, acc: 98.4375, f1: 99.29118773946361, r: 0.7423522660215136
06/02/2019 09:19:59 step: 4452, epoch: 134, batch: 29, loss: 0.048401039093732834, acc: 98.4375, f1: 99.30300807043287, r: 0.8242276656773313
06/02/2019 09:19:59 *** evaluating ***
06/02/2019 09:20:00 step: 135, epoch: 134, acc: 61.111111111111114, f1: 24.743704309544, r: 0.37421300838337473
06/02/2019 09:20:00 *** epoch: 136 ***
06/02/2019 09:20:00 *** training ***
06/02/2019 09:20:00 step: 4460, epoch: 135, batch: 4, loss: 0.06578976660966873, acc: 98.4375, f1: 98.30316742081449, r: 0.7951858541653944
06/02/2019 09:20:00 step: 4465, epoch: 135, batch: 9, loss: 0.13690415024757385, acc: 96.875, f1: 97.98701298701299, r: 0.7401920095766068
06/02/2019 09:20:01 step: 4470, epoch: 135, batch: 14, loss: 0.025136034935712814, acc: 100.0, f1: 100.0, r: 0.647350714885188
06/02/2019 09:20:01 step: 4475, epoch: 135, batch: 19, loss: 0.011647363193333149, acc: 100.0, f1: 100.0, r: 0.7641216631298725
06/02/2019 09:20:02 step: 4480, epoch: 135, batch: 24, loss: 0.10024790465831757, acc: 96.875, f1: 98.10564542022342, r: 0.6577167659699903
06/02/2019 09:20:02 step: 4485, epoch: 135, batch: 29, loss: 0.07803503423929214, acc: 96.875, f1: 92.67040149393092, r: 0.5593239345964901
06/02/2019 09:20:02 *** evaluating ***
06/02/2019 09:20:03 step: 136, epoch: 135, acc: 61.965811965811966, f1: 25.355442609807554, r: 0.3661335814227277
06/02/2019 09:20:03 *** epoch: 137 ***
06/02/2019 09:20:03 *** training ***
06/02/2019 09:20:03 step: 4493, epoch: 136, batch: 4, loss: 0.01334289275109768, acc: 100.0, f1: 100.0, r: 0.7000316068968442
06/02/2019 09:20:04 step: 4498, epoch: 136, batch: 9, loss: 0.03474089130759239, acc: 100.0, f1: 100.0, r: 0.6815717320259305
06/02/2019 09:20:04 step: 4503, epoch: 136, batch: 14, loss: 0.03015904314815998, acc: 98.4375, f1: 96.77655677655677, r: 0.6314519379439169
06/02/2019 09:20:04 step: 4508, epoch: 136, batch: 19, loss: 0.011903876438736916, acc: 100.0, f1: 100.0, r: 0.6711276744097714
06/02/2019 09:20:05 step: 4513, epoch: 136, batch: 24, loss: 0.19187799096107483, acc: 93.75, f1: 94.47916666666667, r: 0.7534886449296329
06/02/2019 09:20:05 step: 4518, epoch: 136, batch: 29, loss: 0.08521389216184616, acc: 96.875, f1: 96.52292152292152, r: 0.7762518589377243
06/02/2019 09:20:06 *** evaluating ***
06/02/2019 09:20:06 step: 137, epoch: 136, acc: 61.53846153846154, f1: 25.369259617350416, r: 0.36057933404409037
06/02/2019 09:20:06 *** epoch: 138 ***
06/02/2019 09:20:06 *** training ***
06/02/2019 09:20:06 step: 4526, epoch: 137, batch: 4, loss: 0.06577248871326447, acc: 96.875, f1: 98.5576923076923, r: 0.6830654183370803
06/02/2019 09:20:07 step: 4531, epoch: 137, batch: 9, loss: 0.027680642902851105, acc: 100.0, f1: 100.0, r: 0.7247486336683743
06/02/2019 09:20:07 step: 4536, epoch: 137, batch: 14, loss: 0.04142318665981293, acc: 100.0, f1: 100.0, r: 0.7106856813948772
06/02/2019 09:20:08 step: 4541, epoch: 137, batch: 19, loss: 0.05184660106897354, acc: 96.875, f1: 94.11462243760378, r: 0.6858552181160745
06/02/2019 09:20:08 step: 4546, epoch: 137, batch: 24, loss: 0.014419956132769585, acc: 100.0, f1: 100.0, r: 0.7757476452411366
06/02/2019 09:20:08 step: 4551, epoch: 137, batch: 29, loss: 0.024441998451948166, acc: 98.4375, f1: 97.0, r: 0.8464196620881976
06/02/2019 09:20:09 *** evaluating ***
06/02/2019 09:20:09 step: 138, epoch: 137, acc: 62.39316239316239, f1: 26.417605351091588, r: 0.3629523116984066
06/02/2019 09:20:09 *** epoch: 139 ***
06/02/2019 09:20:09 *** training ***
06/02/2019 09:20:09 step: 4559, epoch: 138, batch: 4, loss: 0.031956180930137634, acc: 98.4375, f1: 98.86811867604185, r: 0.7138722513243172
06/02/2019 09:20:10 step: 4564, epoch: 138, batch: 9, loss: 0.037465810775756836, acc: 100.0, f1: 100.0, r: 0.5931955665725348
06/02/2019 09:20:10 step: 4569, epoch: 138, batch: 14, loss: 0.09610309451818466, acc: 96.875, f1: 97.70658719811263, r: 0.6252481375888698
06/02/2019 09:20:11 step: 4574, epoch: 138, batch: 19, loss: 0.08018669486045837, acc: 95.3125, f1: 95.91359311186898, r: 0.7437743773388186
06/02/2019 09:20:11 step: 4579, epoch: 138, batch: 24, loss: 0.0576021745800972, acc: 98.4375, f1: 94.44444444444444, r: 0.7251214611689987
06/02/2019 09:20:12 step: 4584, epoch: 138, batch: 29, loss: 0.004018719308078289, acc: 100.0, f1: 100.0, r: 0.6843600617016133
06/02/2019 09:20:12 *** evaluating ***
06/02/2019 09:20:12 step: 139, epoch: 138, acc: 61.965811965811966, f1: 25.98353740799892, r: 0.3629972298761435
06/02/2019 09:20:12 *** epoch: 140 ***
06/02/2019 09:20:12 *** training ***
06/02/2019 09:20:12 step: 4592, epoch: 139, batch: 4, loss: 0.06940791010856628, acc: 95.3125, f1: 95.89965986394556, r: 0.7066982822312323
06/02/2019 09:20:13 step: 4597, epoch: 139, batch: 9, loss: 0.03210468217730522, acc: 98.4375, f1: 96.4625850340136, r: 0.7013225487831872
06/02/2019 09:20:13 step: 4602, epoch: 139, batch: 14, loss: 0.024123501032590866, acc: 100.0, f1: 100.0, r: 0.8178072456365378
06/02/2019 09:20:14 step: 4607, epoch: 139, batch: 19, loss: 0.0330667681992054, acc: 98.4375, f1: 97.96918767507002, r: 0.7865185325232609
06/02/2019 09:20:14 step: 4612, epoch: 139, batch: 24, loss: 0.009943986311554909, acc: 100.0, f1: 100.0, r: 0.7117520916302315
06/02/2019 09:20:15 step: 4617, epoch: 139, batch: 29, loss: 0.031981583684682846, acc: 98.4375, f1: 98.47619047619048, r: 0.7169609014251358
06/02/2019 09:20:15 *** evaluating ***
06/02/2019 09:20:15 step: 140, epoch: 139, acc: 61.53846153846154, f1: 24.39806363781637, r: 0.35988903345548084
06/02/2019 09:20:15 *** epoch: 141 ***
06/02/2019 09:20:15 *** training ***
06/02/2019 09:20:16 step: 4625, epoch: 140, batch: 4, loss: 0.022435124963521957, acc: 100.0, f1: 100.0, r: 0.7155338932413856
06/02/2019 09:20:16 step: 4630, epoch: 140, batch: 9, loss: 0.03015952929854393, acc: 98.4375, f1: 99.00653594771242, r: 0.5872947840597226
06/02/2019 09:20:16 step: 4635, epoch: 140, batch: 14, loss: 0.03096291609108448, acc: 98.4375, f1: 96.82539682539682, r: 0.7537720307992357
06/02/2019 09:20:17 step: 4640, epoch: 140, batch: 19, loss: 0.07682129740715027, acc: 98.4375, f1: 98.4265010351967, r: 0.7336515123719042
06/02/2019 09:20:17 step: 4645, epoch: 140, batch: 24, loss: 0.048615191131830215, acc: 98.4375, f1: 98.51851851851852, r: 0.6992407280862302
06/02/2019 09:20:18 step: 4650, epoch: 140, batch: 29, loss: 0.05850386992096901, acc: 98.4375, f1: 98.7780772686433, r: 0.67961296778467
06/02/2019 09:20:18 *** evaluating ***
06/02/2019 09:20:18 step: 141, epoch: 140, acc: 61.111111111111114, f1: 24.577739638258162, r: 0.35992542086449303
06/02/2019 09:20:18 *** epoch: 142 ***
06/02/2019 09:20:18 *** training ***
06/02/2019 09:20:19 step: 4658, epoch: 141, batch: 4, loss: 0.024549342691898346, acc: 100.0, f1: 100.0, r: 0.6456898032932962
06/02/2019 09:20:19 step: 4663, epoch: 141, batch: 9, loss: 0.021587273105978966, acc: 98.4375, f1: 98.75607385811468, r: 0.7132231346265193
06/02/2019 09:20:20 step: 4668, epoch: 141, batch: 14, loss: 0.07918491214513779, acc: 95.3125, f1: 95.78162578162579, r: 0.7538449401841144
06/02/2019 09:20:20 step: 4673, epoch: 141, batch: 19, loss: 0.04172135517001152, acc: 98.4375, f1: 98.1283422459893, r: 0.7632602940377236
06/02/2019 09:20:21 step: 4678, epoch: 141, batch: 24, loss: 0.006644172593951225, acc: 100.0, f1: 100.0, r: 0.7991756907376905
06/02/2019 09:20:21 step: 4683, epoch: 141, batch: 29, loss: 0.10050279647111893, acc: 96.875, f1: 97.22729671578776, r: 0.7382757786109265
06/02/2019 09:20:21 *** evaluating ***
06/02/2019 09:20:21 step: 142, epoch: 141, acc: 61.111111111111114, f1: 24.64245432984687, r: 0.36233614244669954
06/02/2019 09:20:21 *** epoch: 143 ***
06/02/2019 09:20:21 *** training ***
06/02/2019 09:20:22 step: 4691, epoch: 142, batch: 4, loss: 0.15134255588054657, acc: 96.875, f1: 98.35290216882508, r: 0.8320782358070046
06/02/2019 09:20:22 step: 4696, epoch: 142, batch: 9, loss: 0.06584612280130386, acc: 95.3125, f1: 93.73978100011935, r: 0.7767296542002127
06/02/2019 09:20:23 step: 4701, epoch: 142, batch: 14, loss: 0.010234581306576729, acc: 100.0, f1: 100.0, r: 0.8597511889045496
06/02/2019 09:20:23 step: 4706, epoch: 142, batch: 19, loss: 0.0773657038807869, acc: 96.875, f1: 96.86111111111111, r: 0.7707612865318891
06/02/2019 09:20:24 step: 4711, epoch: 142, batch: 24, loss: 0.028919531032443047, acc: 100.0, f1: 100.0, r: 0.6787837158738782
06/02/2019 09:20:24 step: 4716, epoch: 142, batch: 29, loss: 0.0076465904712677, acc: 100.0, f1: 100.0, r: 0.8239840847569248
06/02/2019 09:20:24 *** evaluating ***
06/02/2019 09:20:25 step: 143, epoch: 142, acc: 61.111111111111114, f1: 24.93425541460992, r: 0.3642824072453474
06/02/2019 09:20:25 *** epoch: 144 ***
06/02/2019 09:20:25 *** training ***
06/02/2019 09:20:25 step: 4724, epoch: 143, batch: 4, loss: 0.09528438746929169, acc: 95.3125, f1: 89.46006960580807, r: 0.6859785729950436
06/02/2019 09:20:25 step: 4729, epoch: 143, batch: 9, loss: 0.13423390686511993, acc: 96.875, f1: 97.31625354544335, r: 0.6114294433053519
06/02/2019 09:20:26 step: 4734, epoch: 143, batch: 14, loss: 0.05103675276041031, acc: 98.4375, f1: 97.33806566104703, r: 0.7181964034128359
06/02/2019 09:20:26 step: 4739, epoch: 143, batch: 19, loss: 0.03601982071995735, acc: 100.0, f1: 100.0, r: 0.6153600893701525
06/02/2019 09:20:27 step: 4744, epoch: 143, batch: 24, loss: 0.1280336230993271, acc: 96.875, f1: 97.86319073083779, r: 0.8228377529185508
06/02/2019 09:20:27 step: 4749, epoch: 143, batch: 29, loss: 0.027697792276740074, acc: 98.4375, f1: 99.29193899782135, r: 0.8123568348962554
06/02/2019 09:20:28 *** evaluating ***
06/02/2019 09:20:28 step: 144, epoch: 143, acc: 61.111111111111114, f1: 24.522616893599015, r: 0.36724079355222294
06/02/2019 09:20:28 *** epoch: 145 ***
06/02/2019 09:20:28 *** training ***
06/02/2019 09:20:28 step: 4757, epoch: 144, batch: 4, loss: 0.04001344367861748, acc: 98.4375, f1: 96.75675675675676, r: 0.6610110842775153
06/02/2019 09:20:29 step: 4762, epoch: 144, batch: 9, loss: 0.10592088103294373, acc: 96.875, f1: 97.3248494800219, r: 0.6584987968652214
06/02/2019 09:20:29 step: 4767, epoch: 144, batch: 14, loss: 0.07054165005683899, acc: 98.4375, f1: 99.08479908479907, r: 0.7390244183258988
06/02/2019 09:20:30 step: 4772, epoch: 144, batch: 19, loss: 0.07135140150785446, acc: 98.4375, f1: 97.86096256684492, r: 0.7021876491163911
06/02/2019 09:20:30 step: 4777, epoch: 144, batch: 24, loss: 0.03632263466715813, acc: 98.4375, f1: 97.95186891961085, r: 0.7527621546932758
06/02/2019 09:20:30 step: 4782, epoch: 144, batch: 29, loss: 0.059298209846019745, acc: 98.4375, f1: 98.53854585312386, r: 0.7283504323477185
06/02/2019 09:20:31 *** evaluating ***
06/02/2019 09:20:31 step: 145, epoch: 144, acc: 60.68376068376068, f1: 24.585398294002676, r: 0.36742813508803795
06/02/2019 09:20:31 *** epoch: 146 ***
06/02/2019 09:20:31 *** training ***
06/02/2019 09:20:31 step: 4790, epoch: 145, batch: 4, loss: 0.12004692107439041, acc: 93.75, f1: 93.7737642078775, r: 0.7252319917828599
06/02/2019 09:20:32 step: 4795, epoch: 145, batch: 9, loss: 0.02736923284828663, acc: 98.4375, f1: 97.74891774891773, r: 0.7001705498642492
06/02/2019 09:20:32 step: 4800, epoch: 145, batch: 14, loss: 0.02836894430220127, acc: 100.0, f1: 100.0, r: 0.7128163382974926
06/02/2019 09:20:33 step: 4805, epoch: 145, batch: 19, loss: 0.09629052877426147, acc: 96.875, f1: 95.51052395879982, r: 0.7869578921433077
06/02/2019 09:20:33 step: 4810, epoch: 145, batch: 24, loss: 0.014126062393188477, acc: 100.0, f1: 100.0, r: 0.7487250253090717
06/02/2019 09:20:33 step: 4815, epoch: 145, batch: 29, loss: 0.030934857204556465, acc: 100.0, f1: 100.0, r: 0.7965764502871983
06/02/2019 09:20:34 *** evaluating ***
06/02/2019 09:20:34 step: 146, epoch: 145, acc: 60.256410256410255, f1: 23.394709858175233, r: 0.36748372317122163
06/02/2019 09:20:34 *** epoch: 147 ***
06/02/2019 09:20:34 *** training ***
06/02/2019 09:20:34 step: 4823, epoch: 146, batch: 4, loss: 0.011628199368715286, acc: 100.0, f1: 100.0, r: 0.7994441425963839
06/02/2019 09:20:35 step: 4828, epoch: 146, batch: 9, loss: 0.02851930260658264, acc: 98.4375, f1: 97.79158040027606, r: 0.7359546096188789
06/02/2019 09:20:35 step: 4833, epoch: 146, batch: 14, loss: 0.06319797039031982, acc: 96.875, f1: 94.20926899187769, r: 0.7730119459021677
06/02/2019 09:20:36 step: 4838, epoch: 146, batch: 19, loss: 0.07866609841585159, acc: 95.3125, f1: 96.5079365079365, r: 0.6839017355312328
06/02/2019 09:20:36 step: 4843, epoch: 146, batch: 24, loss: 0.020918678492307663, acc: 100.0, f1: 100.0, r: 0.7086740769976558
06/02/2019 09:20:37 step: 4848, epoch: 146, batch: 29, loss: 0.01718340441584587, acc: 100.0, f1: 100.0, r: 0.7889709548978843
06/02/2019 09:20:37 *** evaluating ***
06/02/2019 09:20:37 step: 147, epoch: 146, acc: 61.111111111111114, f1: 24.759983944766557, r: 0.3634996701739898
06/02/2019 09:20:37 *** epoch: 148 ***
06/02/2019 09:20:37 *** training ***
06/02/2019 09:20:37 step: 4856, epoch: 147, batch: 4, loss: 0.08462649583816528, acc: 96.875, f1: 95.76839826839827, r: 0.7404507723880732
06/02/2019 09:20:38 step: 4861, epoch: 147, batch: 9, loss: 0.02537287399172783, acc: 100.0, f1: 100.0, r: 0.7041706862294178
06/02/2019 09:20:38 step: 4866, epoch: 147, batch: 14, loss: 0.016067467629909515, acc: 100.0, f1: 100.0, r: 0.7208676940499026
06/02/2019 09:20:39 step: 4871, epoch: 147, batch: 19, loss: 0.022894689813256264, acc: 100.0, f1: 100.0, r: 0.7801977113732359
06/02/2019 09:20:39 step: 4876, epoch: 147, batch: 24, loss: 0.03254825994372368, acc: 98.4375, f1: 99.02136293113738, r: 0.7470974988962544
06/02/2019 09:20:40 step: 4881, epoch: 147, batch: 29, loss: 0.013369236141443253, acc: 100.0, f1: 100.0, r: 0.7892766040340911
06/02/2019 09:20:40 *** evaluating ***
06/02/2019 09:20:40 step: 148, epoch: 147, acc: 61.53846153846154, f1: 25.643067715781353, r: 0.36847336925562524
06/02/2019 09:20:40 *** epoch: 149 ***
06/02/2019 09:20:40 *** training ***
06/02/2019 09:20:40 step: 4889, epoch: 148, batch: 4, loss: 0.06380671262741089, acc: 96.875, f1: 93.4891520804947, r: 0.7032401297874191
06/02/2019 09:20:41 step: 4894, epoch: 148, batch: 9, loss: 0.04183584451675415, acc: 98.4375, f1: 99.06227106227107, r: 0.6828732750993624
06/02/2019 09:20:41 step: 4899, epoch: 148, batch: 14, loss: 0.027530934661626816, acc: 100.0, f1: 100.0, r: 0.6971838730683568
06/02/2019 09:20:42 step: 4904, epoch: 148, batch: 19, loss: 0.02255096286535263, acc: 100.0, f1: 100.0, r: 0.8390384790368799
06/02/2019 09:20:42 step: 4909, epoch: 148, batch: 24, loss: 0.03685427084565163, acc: 98.4375, f1: 97.71428571428571, r: 0.7841427197301146
06/02/2019 09:20:43 step: 4914, epoch: 148, batch: 29, loss: 0.1018446683883667, acc: 96.875, f1: 94.15614773258532, r: 0.6355905620544067
06/02/2019 09:20:43 *** evaluating ***
06/02/2019 09:20:43 step: 149, epoch: 148, acc: 60.68376068376068, f1: 24.44547341958505, r: 0.3654790859749128
06/02/2019 09:20:43 *** epoch: 150 ***
06/02/2019 09:20:43 *** training ***
06/02/2019 09:20:44 step: 4922, epoch: 149, batch: 4, loss: 0.021747872233390808, acc: 98.4375, f1: 98.61853832442068, r: 0.7746381184671838
06/02/2019 09:20:44 step: 4927, epoch: 149, batch: 9, loss: 0.020284604281187057, acc: 100.0, f1: 100.0, r: 0.6800511402394496
06/02/2019 09:20:44 step: 4932, epoch: 149, batch: 14, loss: 0.009501662105321884, acc: 100.0, f1: 100.0, r: 0.6557597335477563
06/02/2019 09:20:45 step: 4937, epoch: 149, batch: 19, loss: 0.014889221638441086, acc: 100.0, f1: 100.0, r: 0.8282327395252697
06/02/2019 09:20:45 step: 4942, epoch: 149, batch: 24, loss: 0.013617290183901787, acc: 100.0, f1: 100.0, r: 0.7808741558458329
06/02/2019 09:20:46 step: 4947, epoch: 149, batch: 29, loss: 0.04579734057188034, acc: 98.4375, f1: 99.1388044579534, r: 0.7887800183645163
06/02/2019 09:20:46 *** evaluating ***
06/02/2019 09:20:46 step: 150, epoch: 149, acc: 61.111111111111114, f1: 24.605491892913875, r: 0.3610029122472311
06/02/2019 09:20:46 *** epoch: 151 ***
06/02/2019 09:20:46 *** training ***
06/02/2019 09:20:47 step: 4955, epoch: 150, batch: 4, loss: 0.04651179909706116, acc: 98.4375, f1: 99.33797909407666, r: 0.7513356339895356
06/02/2019 09:20:47 step: 4960, epoch: 150, batch: 9, loss: 0.06977859139442444, acc: 98.4375, f1: 99.33081674673987, r: 0.7650009865781615
06/02/2019 09:20:47 step: 4965, epoch: 150, batch: 14, loss: 0.08612518757581711, acc: 96.875, f1: 97.19328564156152, r: 0.7954713531413339
06/02/2019 09:20:48 step: 4970, epoch: 150, batch: 19, loss: 0.059566251933574677, acc: 96.875, f1: 94.58874458874459, r: 0.7040700645359403
06/02/2019 09:20:48 step: 4975, epoch: 150, batch: 24, loss: 0.0643143504858017, acc: 98.4375, f1: 99.05018611218071, r: 0.6837974480627834
06/02/2019 09:20:49 step: 4980, epoch: 150, batch: 29, loss: 0.027041206136345863, acc: 100.0, f1: 100.0, r: 0.7601714721738494
06/02/2019 09:20:49 *** evaluating ***
06/02/2019 09:20:49 step: 151, epoch: 150, acc: 61.53846153846154, f1: 25.21126463990921, r: 0.371387036292838
06/02/2019 09:20:49 *** epoch: 152 ***
06/02/2019 09:20:49 *** training ***
06/02/2019 09:20:50 step: 4988, epoch: 151, batch: 4, loss: 0.02242160402238369, acc: 100.0, f1: 100.0, r: 0.6549656750557982
06/02/2019 09:20:50 step: 4993, epoch: 151, batch: 9, loss: 0.02395024709403515, acc: 98.4375, f1: 98.08018068887634, r: 0.7484822081244633
06/02/2019 09:20:50 step: 4998, epoch: 151, batch: 14, loss: 0.1005592942237854, acc: 93.75, f1: 79.4973544973545, r: 0.59629241467495
06/02/2019 09:20:51 step: 5003, epoch: 151, batch: 19, loss: 0.06359440088272095, acc: 93.75, f1: 94.05647840531562, r: 0.7744701736623667
06/02/2019 09:20:51 step: 5008, epoch: 151, batch: 24, loss: 0.08531499654054642, acc: 98.4375, f1: 85.3061224489796, r: 0.5883210383722334
06/02/2019 09:20:52 step: 5013, epoch: 151, batch: 29, loss: 0.03130186349153519, acc: 98.4375, f1: 98.3204134366925, r: 0.7629804742363216
06/02/2019 09:20:52 *** evaluating ***
06/02/2019 09:20:52 step: 152, epoch: 151, acc: 61.53846153846154, f1: 25.860803201614985, r: 0.3715631610105419
06/02/2019 09:20:52 *** epoch: 153 ***
06/02/2019 09:20:52 *** training ***
06/02/2019 09:20:53 step: 5021, epoch: 152, batch: 4, loss: 0.03405294194817543, acc: 100.0, f1: 100.0, r: 0.7899112845448384
06/02/2019 09:20:53 step: 5026, epoch: 152, batch: 9, loss: 0.033706799149513245, acc: 98.4375, f1: 99.25490196078431, r: 0.7196846154368213
06/02/2019 09:20:54 step: 5031, epoch: 152, batch: 14, loss: 0.01751398667693138, acc: 100.0, f1: 100.0, r: 0.6882500315320599
06/02/2019 09:20:54 step: 5036, epoch: 152, batch: 19, loss: 0.01777665689587593, acc: 100.0, f1: 100.0, r: 0.6742906650976319
06/02/2019 09:20:54 step: 5041, epoch: 152, batch: 24, loss: 0.03215206041932106, acc: 100.0, f1: 100.0, r: 0.7849552190176456
06/02/2019 09:20:55 step: 5046, epoch: 152, batch: 29, loss: 0.03799574077129364, acc: 98.4375, f1: 98.00242130750605, r: 0.7772068881796625
06/02/2019 09:20:55 *** evaluating ***
06/02/2019 09:20:55 step: 153, epoch: 152, acc: 61.53846153846154, f1: 25.340381100059673, r: 0.3702014886817669
06/02/2019 09:20:55 *** epoch: 154 ***
06/02/2019 09:20:55 *** training ***
06/02/2019 09:20:56 step: 5054, epoch: 153, batch: 4, loss: 0.0898546576499939, acc: 96.875, f1: 97.23429951690822, r: 0.7373042767728972
06/02/2019 09:20:56 step: 5059, epoch: 153, batch: 9, loss: 0.06259594112634659, acc: 98.4375, f1: 87.03703703703704, r: 0.7900109222297746
06/02/2019 09:20:57 step: 5064, epoch: 153, batch: 14, loss: 0.026839548721909523, acc: 98.4375, f1: 99.22222222222223, r: 0.8411738454281894
06/02/2019 09:20:57 step: 5069, epoch: 153, batch: 19, loss: 0.046994078904390335, acc: 98.4375, f1: 98.41269841269842, r: 0.8140545324762049
06/02/2019 09:20:57 step: 5074, epoch: 153, batch: 24, loss: 0.012341746129095554, acc: 100.0, f1: 100.0, r: 0.5905605837825219
06/02/2019 09:20:58 step: 5079, epoch: 153, batch: 29, loss: 0.020080266520380974, acc: 100.0, f1: 100.0, r: 0.7231370669374956
06/02/2019 09:20:58 *** evaluating ***
06/02/2019 09:20:58 step: 154, epoch: 153, acc: 61.111111111111114, f1: 24.791295949523708, r: 0.3706029511999338
06/02/2019 09:20:58 *** epoch: 155 ***
06/02/2019 09:20:58 *** training ***
06/02/2019 09:20:59 step: 5087, epoch: 154, batch: 4, loss: 0.014971736818552017, acc: 100.0, f1: 100.0, r: 0.6996144759068219
06/02/2019 09:20:59 step: 5092, epoch: 154, batch: 9, loss: 0.018433719873428345, acc: 98.4375, f1: 99.09634551495017, r: 0.662177777671817
06/02/2019 09:21:00 step: 5097, epoch: 154, batch: 14, loss: 0.11209560930728912, acc: 98.4375, f1: 99.00960384153662, r: 0.7323998018125555
06/02/2019 09:21:00 step: 5102, epoch: 154, batch: 19, loss: 0.04199675843119621, acc: 98.4375, f1: 99.27943024717217, r: 0.6623741978970483
06/02/2019 09:21:01 step: 5107, epoch: 154, batch: 24, loss: 0.009596402756869793, acc: 100.0, f1: 100.0, r: 0.7980442463563114
06/02/2019 09:21:01 step: 5112, epoch: 154, batch: 29, loss: 0.05281529575586319, acc: 100.0, f1: 100.0, r: 0.7376131500885554
06/02/2019 09:21:01 *** evaluating ***
06/02/2019 09:21:01 step: 155, epoch: 154, acc: 61.111111111111114, f1: 24.73076798654244, r: 0.36857669523608577
06/02/2019 09:21:01 *** epoch: 156 ***
06/02/2019 09:21:01 *** training ***
06/02/2019 09:21:02 step: 5120, epoch: 155, batch: 4, loss: 0.08650580048561096, acc: 96.875, f1: 97.79187683900798, r: 0.7507960973026748
06/02/2019 09:21:02 step: 5125, epoch: 155, batch: 9, loss: 0.14615502953529358, acc: 98.4375, f1: 99.17287014061208, r: 0.6249122016848613
06/02/2019 09:21:03 step: 5130, epoch: 155, batch: 14, loss: 0.027821913361549377, acc: 98.4375, f1: 99.09937888198758, r: 0.7420014444067644
06/02/2019 09:21:03 step: 5135, epoch: 155, batch: 19, loss: 0.025618132203817368, acc: 98.4375, f1: 98.82882882882883, r: 0.8313306798916942
06/02/2019 09:21:04 step: 5140, epoch: 155, batch: 24, loss: 0.019496314227581024, acc: 98.4375, f1: 98.8795518207283, r: 0.653088522868541
06/02/2019 09:21:04 step: 5145, epoch: 155, batch: 29, loss: 0.0881618782877922, acc: 98.4375, f1: 97.67907162865147, r: 0.6927298876385065
06/02/2019 09:21:04 *** evaluating ***
06/02/2019 09:21:04 step: 156, epoch: 155, acc: 61.111111111111114, f1: 24.78449066283036, r: 0.36957104077115505
06/02/2019 09:21:04 *** epoch: 157 ***
06/02/2019 09:21:04 *** training ***
06/02/2019 09:21:05 step: 5153, epoch: 156, batch: 4, loss: 0.028512030839920044, acc: 100.0, f1: 100.0, r: 0.6790157238125664
06/02/2019 09:21:05 step: 5158, epoch: 156, batch: 9, loss: 0.09321268647909164, acc: 96.875, f1: 95.84262796027502, r: 0.8122077706483759
06/02/2019 09:21:06 step: 5163, epoch: 156, batch: 14, loss: 0.06042454391717911, acc: 98.4375, f1: 99.24633936261843, r: 0.8222342434307387
06/02/2019 09:21:06 step: 5168, epoch: 156, batch: 19, loss: 0.04089530557394028, acc: 98.4375, f1: 99.18099918099918, r: 0.73528558037563
06/02/2019 09:21:07 step: 5173, epoch: 156, batch: 24, loss: 0.008123094215989113, acc: 100.0, f1: 100.0, r: 0.6380613394276168
06/02/2019 09:21:07 step: 5178, epoch: 156, batch: 29, loss: 0.013874365016818047, acc: 100.0, f1: 100.0, r: 0.8117830409525262
06/02/2019 09:21:07 *** evaluating ***
06/02/2019 09:21:08 step: 157, epoch: 156, acc: 61.111111111111114, f1: 24.869833044441236, r: 0.37294796879820785
06/02/2019 09:21:08 *** epoch: 158 ***
06/02/2019 09:21:08 *** training ***
06/02/2019 09:21:08 step: 5186, epoch: 157, batch: 4, loss: 0.047904763370752335, acc: 98.4375, f1: 97.95186891961085, r: 0.6784242057133412
06/02/2019 09:21:08 step: 5191, epoch: 157, batch: 9, loss: 0.009203381836414337, acc: 100.0, f1: 100.0, r: 0.7878123985393548
06/02/2019 09:21:09 step: 5196, epoch: 157, batch: 14, loss: 0.07462989538908005, acc: 96.875, f1: 96.78401090165795, r: 0.7323504661016971
06/02/2019 09:21:09 step: 5201, epoch: 157, batch: 19, loss: 0.008684090338647366, acc: 100.0, f1: 100.0, r: 0.7915174951880171
06/02/2019 09:21:10 step: 5206, epoch: 157, batch: 24, loss: 0.011552799493074417, acc: 100.0, f1: 100.0, r: 0.7798622722680105
06/02/2019 09:21:10 step: 5211, epoch: 157, batch: 29, loss: 0.04517242684960365, acc: 98.4375, f1: 99.16891284815813, r: 0.7556427086774958
06/02/2019 09:21:10 *** evaluating ***
06/02/2019 09:21:11 step: 158, epoch: 157, acc: 61.53846153846154, f1: 25.859163895775527, r: 0.3747256097822163
06/02/2019 09:21:11 *** epoch: 159 ***
06/02/2019 09:21:11 *** training ***
06/02/2019 09:21:11 step: 5219, epoch: 158, batch: 4, loss: 0.01751425303518772, acc: 100.0, f1: 100.0, r: 0.6486020171096938
06/02/2019 09:21:12 step: 5224, epoch: 158, batch: 9, loss: 0.032252050936222076, acc: 100.0, f1: 100.0, r: 0.7900711117798915
06/02/2019 09:21:12 step: 5229, epoch: 158, batch: 14, loss: 0.03403355926275253, acc: 98.4375, f1: 98.80261248185775, r: 0.7527184828400756
06/02/2019 09:21:12 step: 5234, epoch: 158, batch: 19, loss: 0.04828055202960968, acc: 98.4375, f1: 98.08018068887634, r: 0.6992423116996118
06/02/2019 09:21:13 step: 5239, epoch: 158, batch: 24, loss: 0.0018531009554862976, acc: 100.0, f1: 100.0, r: 0.71567134020612
06/02/2019 09:21:13 step: 5244, epoch: 158, batch: 29, loss: 0.02239849604666233, acc: 100.0, f1: 100.0, r: 0.7146956514569425
06/02/2019 09:21:14 *** evaluating ***
06/02/2019 09:21:14 step: 159, epoch: 158, acc: 61.53846153846154, f1: 25.768215828734352, r: 0.3756914975765237
06/02/2019 09:21:14 *** epoch: 160 ***
06/02/2019 09:21:14 *** training ***
06/02/2019 09:21:14 step: 5252, epoch: 159, batch: 4, loss: 0.05606282502412796, acc: 96.875, f1: 94.93933588761175, r: 0.7426901430353189
06/02/2019 09:21:15 step: 5257, epoch: 159, batch: 9, loss: 0.045236822217702866, acc: 98.4375, f1: 99.19056429232192, r: 0.8243051042242131
06/02/2019 09:21:15 step: 5262, epoch: 159, batch: 14, loss: 0.058334462344646454, acc: 98.4375, f1: 99.2158439730572, r: 0.6865021087149573
06/02/2019 09:21:16 step: 5267, epoch: 159, batch: 19, loss: 0.011587090790271759, acc: 100.0, f1: 100.0, r: 0.8070537358442259
06/02/2019 09:21:16 step: 5272, epoch: 159, batch: 24, loss: 0.06197287142276764, acc: 98.4375, f1: 97.33806566104703, r: 0.69524279504956
06/02/2019 09:21:16 step: 5277, epoch: 159, batch: 29, loss: 0.012993848882615566, acc: 100.0, f1: 100.0, r: 0.7301899521532181
06/02/2019 09:21:17 *** evaluating ***
06/02/2019 09:21:17 step: 160, epoch: 159, acc: 61.53846153846154, f1: 25.389982373678027, r: 0.3739031236948954
06/02/2019 09:21:17 *** epoch: 161 ***
06/02/2019 09:21:17 *** training ***
06/02/2019 09:21:17 step: 5285, epoch: 160, batch: 4, loss: 0.028075458481907845, acc: 98.4375, f1: 99.29193899782135, r: 0.8218271460214575
06/02/2019 09:21:18 step: 5290, epoch: 160, batch: 9, loss: 0.014835285022854805, acc: 100.0, f1: 100.0, r: 0.7810656516158524
06/02/2019 09:21:18 step: 5295, epoch: 160, batch: 14, loss: 0.08472330123186111, acc: 96.875, f1: 94.94670742791043, r: 0.6832715582119568
06/02/2019 09:21:19 step: 5300, epoch: 160, batch: 19, loss: 0.06539517641067505, acc: 98.4375, f1: 98.77356347944584, r: 0.7248406896184235
06/02/2019 09:21:19 step: 5305, epoch: 160, batch: 24, loss: 0.07186160981655121, acc: 96.875, f1: 93.4968734968735, r: 0.6632988429316676
06/02/2019 09:21:20 step: 5310, epoch: 160, batch: 29, loss: 0.008484882302582264, acc: 100.0, f1: 100.0, r: 0.6931057567772002
06/02/2019 09:21:20 *** evaluating ***
06/02/2019 09:21:20 step: 161, epoch: 160, acc: 60.68376068376068, f1: 24.790027083131573, r: 0.378459298756115
06/02/2019 09:21:20 *** epoch: 162 ***
06/02/2019 09:21:20 *** training ***
06/02/2019 09:21:20 step: 5318, epoch: 161, batch: 4, loss: 0.022412624210119247, acc: 100.0, f1: 100.0, r: 0.7405892397523479
06/02/2019 09:21:21 step: 5323, epoch: 161, batch: 9, loss: 0.05798337608575821, acc: 98.4375, f1: 97.74891774891775, r: 0.6415504269987721
06/02/2019 09:21:21 step: 5328, epoch: 161, batch: 14, loss: 0.039088718593120575, acc: 96.875, f1: 85.78335949764521, r: 0.826143804745724
06/02/2019 09:21:22 step: 5333, epoch: 161, batch: 19, loss: 0.07554994523525238, acc: 96.875, f1: 95.72582249273978, r: 0.6834596991673475
06/02/2019 09:21:22 step: 5338, epoch: 161, batch: 24, loss: 0.030240386724472046, acc: 98.4375, f1: 98.53479853479854, r: 0.6984290064163703
06/02/2019 09:21:23 step: 5343, epoch: 161, batch: 29, loss: 0.08607973158359528, acc: 96.875, f1: 97.50797448165869, r: 0.7213604076412677
06/02/2019 09:21:23 *** evaluating ***
06/02/2019 09:21:23 step: 162, epoch: 161, acc: 60.68376068376068, f1: 24.409534500770086, r: 0.3825256958372263
06/02/2019 09:21:23 *** epoch: 163 ***
06/02/2019 09:21:23 *** training ***
06/02/2019 09:21:23 step: 5351, epoch: 162, batch: 4, loss: 0.03221295773983002, acc: 98.4375, f1: 99.09876994275972, r: 0.7005792393028328
06/02/2019 09:21:24 step: 5356, epoch: 162, batch: 9, loss: 0.07228483259677887, acc: 98.4375, f1: 98.81118881118881, r: 0.7725041949912591
06/02/2019 09:21:24 step: 5361, epoch: 162, batch: 14, loss: 0.01965542882680893, acc: 100.0, f1: 100.0, r: 0.6902009591748623
06/02/2019 09:21:25 step: 5366, epoch: 162, batch: 19, loss: 0.08062908053398132, acc: 96.875, f1: 98.31746031746033, r: 0.6598370376873824
06/02/2019 09:21:25 step: 5371, epoch: 162, batch: 24, loss: 0.0498344823718071, acc: 98.4375, f1: 97.71428571428571, r: 0.6894429198641333
06/02/2019 09:21:26 step: 5376, epoch: 162, batch: 29, loss: 0.042205698788166046, acc: 98.4375, f1: 87.06896551724138, r: 0.6756395067689731
06/02/2019 09:21:26 *** evaluating ***
06/02/2019 09:21:26 step: 163, epoch: 162, acc: 61.111111111111114, f1: 24.613252966539985, r: 0.3760658054847499
06/02/2019 09:21:26 *** epoch: 164 ***
06/02/2019 09:21:26 *** training ***
06/02/2019 09:21:26 step: 5384, epoch: 163, batch: 4, loss: 0.014120824635028839, acc: 100.0, f1: 100.0, r: 0.795994620908148
06/02/2019 09:21:27 step: 5389, epoch: 163, batch: 9, loss: 0.0766114741563797, acc: 95.3125, f1: 95.62224889955982, r: 0.7128005562864194
06/02/2019 09:21:27 step: 5394, epoch: 163, batch: 14, loss: 0.062386296689510345, acc: 96.875, f1: 97.34126984126983, r: 0.7765532124238571
06/02/2019 09:21:28 step: 5399, epoch: 163, batch: 19, loss: 0.053841978311538696, acc: 98.4375, f1: 98.58585858585859, r: 0.5878026941791723
06/02/2019 09:21:28 step: 5404, epoch: 163, batch: 24, loss: 0.07230274379253387, acc: 96.875, f1: 95.63492063492063, r: 0.6534888414366267
06/02/2019 09:21:29 step: 5409, epoch: 163, batch: 29, loss: 0.02577803283929825, acc: 98.4375, f1: 99.13675123697232, r: 0.7082575163635839
06/02/2019 09:21:29 *** evaluating ***
06/02/2019 09:21:29 step: 164, epoch: 163, acc: 61.53846153846154, f1: 25.345094081399665, r: 0.38358616140909296
06/02/2019 09:21:29 *** epoch: 165 ***
06/02/2019 09:21:29 *** training ***
06/02/2019 09:21:30 step: 5417, epoch: 164, batch: 4, loss: 0.012828534469008446, acc: 100.0, f1: 100.0, r: 0.7023367358772872
06/02/2019 09:21:30 step: 5422, epoch: 164, batch: 9, loss: 0.025252163410186768, acc: 98.4375, f1: 97.46657283603096, r: 0.6904141592922544
06/02/2019 09:21:30 step: 5427, epoch: 164, batch: 14, loss: 0.016839738935232162, acc: 100.0, f1: 100.0, r: 0.6819084021091968
06/02/2019 09:21:31 step: 5432, epoch: 164, batch: 19, loss: 0.025520600378513336, acc: 100.0, f1: 100.0, r: 0.654194440726361
06/02/2019 09:21:31 step: 5437, epoch: 164, batch: 24, loss: 0.06206026300787926, acc: 98.4375, f1: 98.7468671679198, r: 0.7525806542280963
06/02/2019 09:21:32 step: 5442, epoch: 164, batch: 29, loss: 0.09854444861412048, acc: 95.3125, f1: 82.9010989010989, r: 0.6012133878879145
06/02/2019 09:21:32 *** evaluating ***
06/02/2019 09:21:32 step: 165, epoch: 164, acc: 61.53846153846154, f1: 25.598839898301406, r: 0.3836275638092631
06/02/2019 09:21:32 *** epoch: 166 ***
06/02/2019 09:21:32 *** training ***
06/02/2019 09:21:33 step: 5450, epoch: 165, batch: 4, loss: 0.013137098401784897, acc: 100.0, f1: 100.0, r: 0.7723453768341088
06/02/2019 09:21:33 step: 5455, epoch: 165, batch: 9, loss: 0.0672646164894104, acc: 95.3125, f1: 90.43083900226759, r: 0.7019372990432061
06/02/2019 09:21:34 step: 5460, epoch: 165, batch: 14, loss: 0.012479878962039948, acc: 100.0, f1: 100.0, r: 0.6931377554347571
06/02/2019 09:21:34 step: 5465, epoch: 165, batch: 19, loss: 0.013268530368804932, acc: 100.0, f1: 100.0, r: 0.8061005823960183
06/02/2019 09:21:34 step: 5470, epoch: 165, batch: 24, loss: 0.018618322908878326, acc: 100.0, f1: 100.0, r: 0.685795998041881
06/02/2019 09:21:35 step: 5475, epoch: 165, batch: 29, loss: 0.047609761357307434, acc: 98.4375, f1: 99.03722721437741, r: 0.8059335913893277
06/02/2019 09:21:35 *** evaluating ***
06/02/2019 09:21:35 step: 166, epoch: 165, acc: 61.111111111111114, f1: 25.20102280971846, r: 0.38223127981744265
06/02/2019 09:21:35 *** epoch: 167 ***
06/02/2019 09:21:35 *** training ***
06/02/2019 09:21:36 step: 5483, epoch: 166, batch: 4, loss: 0.07290549576282501, acc: 98.4375, f1: 99.20343779478041, r: 0.6882573896409683
06/02/2019 09:21:36 step: 5488, epoch: 166, batch: 9, loss: 0.0288995448499918, acc: 98.4375, f1: 97.03703703703704, r: 0.7735511389612395
06/02/2019 09:21:37 step: 5493, epoch: 166, batch: 14, loss: 0.037157364189624786, acc: 98.4375, f1: 98.60681114551085, r: 0.7396071691953637
06/02/2019 09:21:37 step: 5498, epoch: 166, batch: 19, loss: 0.05251684784889221, acc: 98.4375, f1: 97.667638483965, r: 0.6587577273526806
06/02/2019 09:21:38 step: 5503, epoch: 166, batch: 24, loss: 0.0035668164491653442, acc: 100.0, f1: 100.0, r: 0.6649957954626188
06/02/2019 09:21:38 step: 5508, epoch: 166, batch: 29, loss: 0.03260205686092377, acc: 98.4375, f1: 99.12280701754386, r: 0.7387881381485606
06/02/2019 09:21:38 *** evaluating ***
06/02/2019 09:21:38 step: 167, epoch: 166, acc: 61.111111111111114, f1: 24.599693784476393, r: 0.3768331949030539
06/02/2019 09:21:38 *** epoch: 168 ***
06/02/2019 09:21:38 *** training ***
06/02/2019 09:21:39 step: 5516, epoch: 167, batch: 4, loss: 0.11220856010913849, acc: 96.875, f1: 98.04328346579399, r: 0.6240025135087378
06/02/2019 09:21:39 step: 5521, epoch: 167, batch: 9, loss: 0.03306817263364792, acc: 98.4375, f1: 99.19078742608154, r: 0.6927986425314345
06/02/2019 09:21:40 step: 5526, epoch: 167, batch: 14, loss: 0.048388831317424774, acc: 100.0, f1: 100.0, r: 0.7181122931118629
06/02/2019 09:21:40 step: 5531, epoch: 167, batch: 19, loss: 0.01731877774000168, acc: 100.0, f1: 100.0, r: 0.691667716505839
06/02/2019 09:21:41 step: 5536, epoch: 167, batch: 24, loss: 0.002688191831111908, acc: 100.0, f1: 100.0, r: 0.7738742888291595
06/02/2019 09:21:41 step: 5541, epoch: 167, batch: 29, loss: 0.07729539275169373, acc: 96.875, f1: 98.59203296703296, r: 0.7655648546146803
06/02/2019 09:21:41 *** evaluating ***
06/02/2019 09:21:41 step: 168, epoch: 167, acc: 61.53846153846154, f1: 26.030758203345687, r: 0.3777126717955003
06/02/2019 09:21:41 *** epoch: 169 ***
06/02/2019 09:21:41 *** training ***
06/02/2019 09:21:42 step: 5549, epoch: 168, batch: 4, loss: 0.06332983821630478, acc: 98.4375, f1: 99.16582406471183, r: 0.8044088341945598
06/02/2019 09:21:42 step: 5554, epoch: 168, batch: 9, loss: 0.028228243812918663, acc: 98.4375, f1: 97.20930232558139, r: 0.7921128150392974
06/02/2019 09:21:43 step: 5559, epoch: 168, batch: 14, loss: 0.020432276651263237, acc: 100.0, f1: 100.0, r: 0.6622525072969108
06/02/2019 09:21:43 step: 5564, epoch: 168, batch: 19, loss: 0.052798304706811905, acc: 96.875, f1: 97.59970877983301, r: 0.719117915478936
06/02/2019 09:21:44 step: 5569, epoch: 168, batch: 24, loss: 0.07184869050979614, acc: 98.4375, f1: 87.25490196078431, r: 0.7402909524112535
06/02/2019 09:21:44 step: 5574, epoch: 168, batch: 29, loss: 0.011009259149432182, acc: 100.0, f1: 100.0, r: 0.688295008487242
06/02/2019 09:21:44 *** evaluating ***
06/02/2019 09:21:44 step: 169, epoch: 168, acc: 60.68376068376068, f1: 25.049435750508266, r: 0.375540245018941
06/02/2019 09:21:44 *** epoch: 170 ***
06/02/2019 09:21:44 *** training ***
06/02/2019 09:21:45 step: 5582, epoch: 169, batch: 4, loss: 0.07323414087295532, acc: 96.875, f1: 94.15993046501521, r: 0.8283815436725824
06/02/2019 09:21:45 step: 5587, epoch: 169, batch: 9, loss: 0.03481012210249901, acc: 98.4375, f1: 97.1951219512195, r: 0.7567136905690689
06/02/2019 09:21:46 step: 5592, epoch: 169, batch: 14, loss: 0.012815222144126892, acc: 100.0, f1: 100.0, r: 0.6157706081330456
06/02/2019 09:21:46 step: 5597, epoch: 169, batch: 19, loss: 0.006094466894865036, acc: 100.0, f1: 100.0, r: 0.774743503363904
06/02/2019 09:21:47 step: 5602, epoch: 169, batch: 24, loss: 0.029183343052864075, acc: 100.0, f1: 100.0, r: 0.6894660575123577
06/02/2019 09:21:47 step: 5607, epoch: 169, batch: 29, loss: 0.035968754440546036, acc: 98.4375, f1: 99.24633936261843, r: 0.7975990418711697
06/02/2019 09:21:47 *** evaluating ***
06/02/2019 09:21:48 step: 170, epoch: 169, acc: 61.111111111111114, f1: 24.767900125885685, r: 0.3679235956252591
06/02/2019 09:21:48 *** epoch: 171 ***
06/02/2019 09:21:48 *** training ***
06/02/2019 09:21:48 step: 5615, epoch: 170, batch: 4, loss: 0.02294922061264515, acc: 100.0, f1: 100.0, r: 0.8054851386442979
06/02/2019 09:21:49 step: 5620, epoch: 170, batch: 9, loss: 0.05075148865580559, acc: 98.4375, f1: 98.53854585312386, r: 0.7316134527090219
06/02/2019 09:21:49 step: 5625, epoch: 170, batch: 14, loss: 0.07933352142572403, acc: 96.875, f1: 98.66946778711485, r: 0.7371221647341241
06/02/2019 09:21:49 step: 5630, epoch: 170, batch: 19, loss: 0.027619438245892525, acc: 100.0, f1: 100.0, r: 0.6516664960225483
06/02/2019 09:21:50 step: 5635, epoch: 170, batch: 24, loss: 0.014680972322821617, acc: 100.0, f1: 100.0, r: 0.7533451319272658
06/02/2019 09:21:50 step: 5640, epoch: 170, batch: 29, loss: 0.00870181992650032, acc: 100.0, f1: 100.0, r: 0.7062438380633183
06/02/2019 09:21:51 *** evaluating ***
06/02/2019 09:21:51 step: 171, epoch: 170, acc: 60.256410256410255, f1: 24.10738505844947, r: 0.371287760803717
06/02/2019 09:21:51 *** epoch: 172 ***
06/02/2019 09:21:51 *** training ***
06/02/2019 09:21:51 step: 5648, epoch: 171, batch: 4, loss: 0.005232623778283596, acc: 100.0, f1: 100.0, r: 0.787860438747922
06/02/2019 09:21:52 step: 5653, epoch: 171, batch: 9, loss: 0.008557094261050224, acc: 100.0, f1: 100.0, r: 0.8261415468409635
06/02/2019 09:21:52 step: 5658, epoch: 171, batch: 14, loss: 0.059013575315475464, acc: 96.875, f1: 94.5689119848351, r: 0.6980677509510478
06/02/2019 09:21:53 step: 5663, epoch: 171, batch: 19, loss: 0.016541704535484314, acc: 100.0, f1: 100.0, r: 0.666491050382519
06/02/2019 09:21:53 step: 5668, epoch: 171, batch: 24, loss: 0.014484206214547157, acc: 100.0, f1: 100.0, r: 0.7788753921358581
06/02/2019 09:21:54 step: 5673, epoch: 171, batch: 29, loss: 0.05599106848239899, acc: 98.4375, f1: 99.24633936261843, r: 0.7602402687317421
06/02/2019 09:21:54 *** evaluating ***
06/02/2019 09:21:54 step: 172, epoch: 171, acc: 61.111111111111114, f1: 24.886180929285416, r: 0.37110773623990745
06/02/2019 09:21:54 *** epoch: 173 ***
06/02/2019 09:21:54 *** training ***
06/02/2019 09:21:54 step: 5681, epoch: 172, batch: 4, loss: 0.08048950135707855, acc: 96.875, f1: 97.3611197770429, r: 0.8401400450857852
06/02/2019 09:21:55 step: 5686, epoch: 172, batch: 9, loss: 0.07662161439657211, acc: 96.875, f1: 97.72264772264772, r: 0.6683651532793691
06/02/2019 09:21:55 step: 5691, epoch: 172, batch: 14, loss: 0.02815784141421318, acc: 98.4375, f1: 97.38775510204081, r: 0.6748261067547586
06/02/2019 09:21:56 step: 5696, epoch: 172, batch: 19, loss: 0.04822862893342972, acc: 96.875, f1: 98.64583333333334, r: 0.7033093899138186
06/02/2019 09:21:56 step: 5701, epoch: 172, batch: 24, loss: 0.0524778738617897, acc: 98.4375, f1: 98.97400820793433, r: 0.7026294130627995
06/02/2019 09:21:57 step: 5706, epoch: 172, batch: 29, loss: 0.025927629321813583, acc: 100.0, f1: 100.0, r: 0.6637703212030946
06/02/2019 09:21:57 *** evaluating ***
06/02/2019 09:21:57 step: 173, epoch: 172, acc: 60.68376068376068, f1: 24.57454128309321, r: 0.37137046051219724
06/02/2019 09:21:57 *** epoch: 174 ***
06/02/2019 09:21:57 *** training ***
06/02/2019 09:21:57 step: 5714, epoch: 173, batch: 4, loss: 0.020669231191277504, acc: 100.0, f1: 100.0, r: 0.6967008213559167
06/02/2019 09:21:58 step: 5719, epoch: 173, batch: 9, loss: 0.0850718542933464, acc: 95.3125, f1: 96.51534526854219, r: 0.7940593625852007
06/02/2019 09:21:58 step: 5724, epoch: 173, batch: 14, loss: 0.023570764809846878, acc: 98.4375, f1: 95.52845528455285, r: 0.6985088093460075
06/02/2019 09:21:59 step: 5729, epoch: 173, batch: 19, loss: 0.007638520561158657, acc: 100.0, f1: 100.0, r: 0.6845855718954059
06/02/2019 09:21:59 step: 5734, epoch: 173, batch: 24, loss: 0.036623165011405945, acc: 98.4375, f1: 87.0, r: 0.6602780330338043
06/02/2019 09:22:00 step: 5739, epoch: 173, batch: 29, loss: 0.13033728301525116, acc: 95.3125, f1: 94.32387891404287, r: 0.7004244549957483
06/02/2019 09:22:00 *** evaluating ***
06/02/2019 09:22:00 step: 174, epoch: 173, acc: 61.111111111111114, f1: 24.745468229255223, r: 0.36994530007941356
06/02/2019 09:22:00 *** epoch: 175 ***
06/02/2019 09:22:00 *** training ***
06/02/2019 09:22:00 step: 5747, epoch: 174, batch: 4, loss: 0.02570105530321598, acc: 98.4375, f1: 99.22727711774365, r: 0.6905610234100006
06/02/2019 09:22:01 step: 5752, epoch: 174, batch: 9, loss: 0.017762737348675728, acc: 100.0, f1: 100.0, r: 0.7743997593104548
06/02/2019 09:22:01 step: 5757, epoch: 174, batch: 14, loss: 0.014322932809591293, acc: 100.0, f1: 100.0, r: 0.5637995449453935
06/02/2019 09:22:02 step: 5762, epoch: 174, batch: 19, loss: 0.03231745958328247, acc: 100.0, f1: 100.0, r: 0.7533317291681261
06/02/2019 09:22:02 step: 5767, epoch: 174, batch: 24, loss: 0.07074196636676788, acc: 98.4375, f1: 97.6023976023976, r: 0.6961612458100075
06/02/2019 09:22:03 step: 5772, epoch: 174, batch: 29, loss: 0.0176599882543087, acc: 100.0, f1: 100.0, r: 0.7138231289280842
06/02/2019 09:22:03 *** evaluating ***
06/02/2019 09:22:03 step: 175, epoch: 174, acc: 60.68376068376068, f1: 24.502702202633436, r: 0.3701251206479585
06/02/2019 09:22:03 *** epoch: 176 ***
06/02/2019 09:22:03 *** training ***
06/02/2019 09:22:04 step: 5780, epoch: 175, batch: 4, loss: 0.0218379907310009, acc: 98.4375, f1: 99.26415094339622, r: 0.8059022975122523
06/02/2019 09:22:04 step: 5785, epoch: 175, batch: 9, loss: 0.004961623810231686, acc: 100.0, f1: 100.0, r: 0.7233264408548691
06/02/2019 09:22:04 step: 5790, epoch: 175, batch: 14, loss: 0.01718895323574543, acc: 100.0, f1: 100.0, r: 0.7176851898244794
06/02/2019 09:22:05 step: 5795, epoch: 175, batch: 19, loss: 0.08369423449039459, acc: 96.875, f1: 95.47135217723454, r: 0.7178380693398126
06/02/2019 09:22:05 step: 5800, epoch: 175, batch: 24, loss: 0.042464517056941986, acc: 98.4375, f1: 99.24762531740808, r: 0.6841510458742099
06/02/2019 09:22:06 step: 5805, epoch: 175, batch: 29, loss: 0.07289794832468033, acc: 95.3125, f1: 93.34552229379815, r: 0.688277194061993
06/02/2019 09:22:06 *** evaluating ***
06/02/2019 09:22:06 step: 176, epoch: 175, acc: 58.97435897435898, f1: 24.68662486938349, r: 0.37275302473832495
06/02/2019 09:22:06 *** epoch: 177 ***
06/02/2019 09:22:06 *** training ***
06/02/2019 09:22:07 step: 5813, epoch: 176, batch: 4, loss: 0.04840879142284393, acc: 98.4375, f1: 99.27943024717217, r: 0.6708998641209813
06/02/2019 09:22:07 step: 5818, epoch: 176, batch: 9, loss: 0.05989663302898407, acc: 96.875, f1: 96.86011904761904, r: 0.8128387826479246
06/02/2019 09:22:08 step: 5823, epoch: 176, batch: 14, loss: 0.011289152316749096, acc: 100.0, f1: 100.0, r: 0.6479274401926152
06/02/2019 09:22:08 step: 5828, epoch: 176, batch: 19, loss: 0.07951302826404572, acc: 96.875, f1: 98.44179894179894, r: 0.6901937004372499
06/02/2019 09:22:08 step: 5833, epoch: 176, batch: 24, loss: 0.00708998367190361, acc: 100.0, f1: 100.0, r: 0.6254203353556491
06/02/2019 09:22:09 step: 5838, epoch: 176, batch: 29, loss: 0.04334661364555359, acc: 98.4375, f1: 99.2292490118577, r: 0.7651514828804532
06/02/2019 09:22:09 *** evaluating ***
06/02/2019 09:22:09 step: 177, epoch: 176, acc: 60.68376068376068, f1: 24.61077115423497, r: 0.3660136220544919
06/02/2019 09:22:09 *** epoch: 178 ***
06/02/2019 09:22:09 *** training ***
06/02/2019 09:22:10 step: 5846, epoch: 177, batch: 4, loss: 0.1188095211982727, acc: 96.875, f1: 97.44078144078145, r: 0.6997740339336375
06/02/2019 09:22:10 step: 5851, epoch: 177, batch: 9, loss: 0.017582949250936508, acc: 100.0, f1: 100.0, r: 0.6882060964192427
06/02/2019 09:22:11 step: 5856, epoch: 177, batch: 14, loss: 0.01123669184744358, acc: 100.0, f1: 100.0, r: 0.7796603023993406
06/02/2019 09:22:11 step: 5861, epoch: 177, batch: 19, loss: 0.07856591790914536, acc: 98.4375, f1: 97.97979797979798, r: 0.667274201320018
06/02/2019 09:22:11 step: 5866, epoch: 177, batch: 24, loss: 0.005210665985941887, acc: 100.0, f1: 100.0, r: 0.7022062909855714
06/02/2019 09:22:12 step: 5871, epoch: 177, batch: 29, loss: 0.03812316432595253, acc: 98.4375, f1: 97.57236227824464, r: 0.6704926315014076
06/02/2019 09:22:12 *** evaluating ***
06/02/2019 09:22:12 step: 178, epoch: 177, acc: 60.68376068376068, f1: 24.502702202633436, r: 0.3693138889584909
06/02/2019 09:22:12 *** epoch: 179 ***
06/02/2019 09:22:12 *** training ***
06/02/2019 09:22:13 step: 5879, epoch: 178, batch: 4, loss: 0.016847409307956696, acc: 100.0, f1: 100.0, r: 0.7108121902233161
06/02/2019 09:22:13 step: 5884, epoch: 178, batch: 9, loss: 0.07068166136741638, acc: 95.3125, f1: 96.43544384677108, r: 0.7152982789013552
06/02/2019 09:22:14 step: 5889, epoch: 178, batch: 14, loss: 0.040114738047122955, acc: 96.875, f1: 83.52272727272727, r: 0.6758823359059184
06/02/2019 09:22:14 step: 5894, epoch: 178, batch: 19, loss: 0.00234946608543396, acc: 100.0, f1: 100.0, r: 0.8344144780104731
06/02/2019 09:22:15 step: 5899, epoch: 178, batch: 24, loss: 0.018567252904176712, acc: 100.0, f1: 100.0, r: 0.6676179933350774
06/02/2019 09:22:15 step: 5904, epoch: 178, batch: 29, loss: 0.014865335077047348, acc: 100.0, f1: 100.0, r: 0.801629256629124
06/02/2019 09:22:15 *** evaluating ***
06/02/2019 09:22:15 step: 179, epoch: 178, acc: 60.68376068376068, f1: 24.548733722454386, r: 0.36774698690383306
06/02/2019 09:22:15 *** epoch: 180 ***
06/02/2019 09:22:15 *** training ***
06/02/2019 09:22:16 step: 5912, epoch: 179, batch: 4, loss: 0.02287963218986988, acc: 100.0, f1: 100.0, r: 0.6567470388288373
06/02/2019 09:22:16 step: 5917, epoch: 179, batch: 9, loss: 0.005719510838389397, acc: 100.0, f1: 100.0, r: 0.7319252981533588
06/02/2019 09:22:17 step: 5922, epoch: 179, batch: 14, loss: 0.019962776452302933, acc: 98.4375, f1: 96.41025641025641, r: 0.5743886986000032
06/02/2019 09:22:17 step: 5927, epoch: 179, batch: 19, loss: 0.07733476907014847, acc: 98.4375, f1: 96.36363636363635, r: 0.5630071053855071
06/02/2019 09:22:18 step: 5932, epoch: 179, batch: 24, loss: 0.017741937190294266, acc: 98.4375, f1: 99.14108879626122, r: 0.6821066828781931
06/02/2019 09:22:18 step: 5937, epoch: 179, batch: 29, loss: 0.08208397775888443, acc: 98.4375, f1: 96.4625850340136, r: 0.675070222346728
06/02/2019 09:22:18 *** evaluating ***
06/02/2019 09:22:18 step: 180, epoch: 179, acc: 59.401709401709404, f1: 23.871286162686424, r: 0.3706193492263095
06/02/2019 09:22:18 *** epoch: 181 ***
06/02/2019 09:22:18 *** training ***
06/02/2019 09:22:19 step: 5945, epoch: 180, batch: 4, loss: 0.08162474632263184, acc: 96.875, f1: 98.26839826839827, r: 0.6645630437937369
06/02/2019 09:22:19 step: 5950, epoch: 180, batch: 9, loss: 0.035192009061574936, acc: 98.4375, f1: 97.17948717948718, r: 0.7715753548715171
06/02/2019 09:22:20 step: 5955, epoch: 180, batch: 14, loss: 0.16020768880844116, acc: 93.75, f1: 82.48842352615937, r: 0.7676153001227841
06/02/2019 09:22:20 step: 5960, epoch: 180, batch: 19, loss: 0.022119686007499695, acc: 100.0, f1: 100.0, r: 0.6813102258520561
06/02/2019 09:22:21 step: 5965, epoch: 180, batch: 24, loss: 0.010296372696757317, acc: 100.0, f1: 100.0, r: 0.6702621934626777
06/02/2019 09:22:21 step: 5970, epoch: 180, batch: 29, loss: 0.007754972204566002, acc: 100.0, f1: 100.0, r: 0.6926611488136704
06/02/2019 09:22:21 *** evaluating ***
06/02/2019 09:22:22 step: 181, epoch: 180, acc: 59.82905982905983, f1: 24.045481459836054, r: 0.37223619203295116
06/02/2019 09:22:22 *** epoch: 182 ***
06/02/2019 09:22:22 *** training ***
06/02/2019 09:22:22 step: 5978, epoch: 181, batch: 4, loss: 0.042234644293785095, acc: 98.4375, f1: 98.26839826839826, r: 0.7609694072740595
06/02/2019 09:22:22 step: 5983, epoch: 181, batch: 9, loss: 0.06988551467657089, acc: 98.4375, f1: 98.95262452807209, r: 0.6493802023446017
06/02/2019 09:22:23 step: 5988, epoch: 181, batch: 14, loss: 0.02001056633889675, acc: 100.0, f1: 100.0, r: 0.7093195529379293
06/02/2019 09:22:23 step: 5993, epoch: 181, batch: 19, loss: 0.11625918745994568, acc: 96.875, f1: 98.28042328042328, r: 0.6864274774769351
06/02/2019 09:22:24 step: 5998, epoch: 181, batch: 24, loss: 0.01206476055085659, acc: 100.0, f1: 100.0, r: 0.6379757057662915
06/02/2019 09:22:24 step: 6003, epoch: 181, batch: 29, loss: 0.04999233037233353, acc: 98.4375, f1: 98.64135864135865, r: 0.7008211792722258
06/02/2019 09:22:24 *** evaluating ***
06/02/2019 09:22:25 step: 182, epoch: 181, acc: 60.68376068376068, f1: 25.26705361998085, r: 0.37164726342855864
06/02/2019 09:22:25 *** epoch: 183 ***
06/02/2019 09:22:25 *** training ***
06/02/2019 09:22:25 step: 6011, epoch: 182, batch: 4, loss: 0.007187597453594208, acc: 100.0, f1: 100.0, r: 0.8084717272391879
06/02/2019 09:22:26 step: 6016, epoch: 182, batch: 9, loss: 0.029319019988179207, acc: 98.4375, f1: 99.3552546744036, r: 0.7438363695069362
06/02/2019 09:22:26 step: 6021, epoch: 182, batch: 14, loss: 0.014428170397877693, acc: 100.0, f1: 100.0, r: 0.75826900151692
06/02/2019 09:22:26 step: 6026, epoch: 182, batch: 19, loss: 0.021197527647018433, acc: 100.0, f1: 100.0, r: 0.6783622892855163
06/02/2019 09:22:27 step: 6031, epoch: 182, batch: 24, loss: 0.05574481934309006, acc: 98.4375, f1: 97.38775510204081, r: 0.7096453225162093
06/02/2019 09:22:27 step: 6036, epoch: 182, batch: 29, loss: 0.02472778409719467, acc: 100.0, f1: 100.0, r: 0.6881363271764117
06/02/2019 09:22:27 *** evaluating ***
06/02/2019 09:22:28 step: 183, epoch: 182, acc: 60.68376068376068, f1: 24.484939222047412, r: 0.3635041858757764
06/02/2019 09:22:28 *** epoch: 184 ***
06/02/2019 09:22:28 *** training ***
06/02/2019 09:22:28 step: 6044, epoch: 183, batch: 4, loss: 0.035018809139728546, acc: 100.0, f1: 100.0, r: 0.696829718860078
06/02/2019 09:22:29 step: 6049, epoch: 183, batch: 9, loss: 0.015534946694970131, acc: 100.0, f1: 100.0, r: 0.7442876950611099
06/02/2019 09:22:29 step: 6054, epoch: 183, batch: 14, loss: 0.0062567088752985, acc: 100.0, f1: 100.0, r: 0.7861698256396459
06/02/2019 09:22:29 step: 6059, epoch: 183, batch: 19, loss: 0.04220988601446152, acc: 98.4375, f1: 96.53846153846153, r: 0.7941607577357168
06/02/2019 09:22:30 step: 6064, epoch: 183, batch: 24, loss: 0.015803908929228783, acc: 100.0, f1: 100.0, r: 0.6809562895970149
06/02/2019 09:22:30 step: 6069, epoch: 183, batch: 29, loss: 0.029829014092683792, acc: 98.4375, f1: 98.99355877616746, r: 0.7858853433781179
06/02/2019 09:22:31 *** evaluating ***
06/02/2019 09:22:31 step: 184, epoch: 183, acc: 60.68376068376068, f1: 24.315457580428866, r: 0.36186863434926514
06/02/2019 09:22:31 *** epoch: 185 ***
06/02/2019 09:22:31 *** training ***
06/02/2019 09:22:31 step: 6077, epoch: 184, batch: 4, loss: 0.00420251302421093, acc: 100.0, f1: 100.0, r: 0.6289559540856372
06/02/2019 09:22:32 step: 6082, epoch: 184, batch: 9, loss: 0.034173473715782166, acc: 98.4375, f1: 98.91107078039929, r: 0.8162429538967375
06/02/2019 09:22:32 step: 6087, epoch: 184, batch: 14, loss: 0.01783132180571556, acc: 100.0, f1: 100.0, r: 0.8298746173853035
06/02/2019 09:22:33 step: 6092, epoch: 184, batch: 19, loss: 0.13441972434520721, acc: 96.875, f1: 97.19065656565657, r: 0.7336995622085756
06/02/2019 09:22:33 step: 6097, epoch: 184, batch: 24, loss: 0.01539631001651287, acc: 100.0, f1: 100.0, r: 0.7246859369880657
06/02/2019 09:22:33 step: 6102, epoch: 184, batch: 29, loss: 0.04591257497668266, acc: 98.4375, f1: 99.25490196078431, r: 0.7242794236402741
06/02/2019 09:22:34 *** evaluating ***
06/02/2019 09:22:34 step: 185, epoch: 184, acc: 59.82905982905983, f1: 24.07996894409938, r: 0.36156702367365917
06/02/2019 09:22:34 *** epoch: 186 ***
06/02/2019 09:22:34 *** training ***
06/02/2019 09:22:34 step: 6110, epoch: 185, batch: 4, loss: 0.026022562757134438, acc: 98.4375, f1: 97.99498746867168, r: 0.7734123787705717
06/02/2019 09:22:35 step: 6115, epoch: 185, batch: 9, loss: 0.016611352562904358, acc: 100.0, f1: 100.0, r: 0.657319007186623
06/02/2019 09:22:35 step: 6120, epoch: 185, batch: 14, loss: 0.034154362976551056, acc: 98.4375, f1: 98.64135864135865, r: 0.7154653199580122
06/02/2019 09:22:36 step: 6125, epoch: 185, batch: 19, loss: 0.0480818897485733, acc: 98.4375, f1: 98.59714753331774, r: 0.6653254336184703
06/02/2019 09:22:36 step: 6130, epoch: 185, batch: 24, loss: 0.021709658205509186, acc: 100.0, f1: 100.0, r: 0.7735443652932985
06/02/2019 09:22:37 step: 6135, epoch: 185, batch: 29, loss: 0.1265222430229187, acc: 98.4375, f1: 97.49835418038182, r: 0.7305731709095648
06/02/2019 09:22:37 *** evaluating ***
06/02/2019 09:22:37 step: 186, epoch: 185, acc: 60.256410256410255, f1: 25.235995447654325, r: 0.36558035397788385
06/02/2019 09:22:37 *** epoch: 187 ***
06/02/2019 09:22:37 *** training ***
06/02/2019 09:22:37 step: 6143, epoch: 186, batch: 4, loss: 0.020258218050003052, acc: 100.0, f1: 100.0, r: 0.7485575509742861
06/02/2019 09:22:38 step: 6148, epoch: 186, batch: 9, loss: 0.013696728274226189, acc: 100.0, f1: 100.0, r: 0.8247508671052745
06/02/2019 09:22:38 step: 6153, epoch: 186, batch: 14, loss: 0.0873323380947113, acc: 96.875, f1: 94.10430839002268, r: 0.7046040771471903
06/02/2019 09:22:39 step: 6158, epoch: 186, batch: 19, loss: 0.007821236737072468, acc: 100.0, f1: 100.0, r: 0.7110941335786733
06/02/2019 09:22:39 step: 6163, epoch: 186, batch: 24, loss: 0.007038518786430359, acc: 100.0, f1: 100.0, r: 0.7000269043578359
06/02/2019 09:22:40 step: 6168, epoch: 186, batch: 29, loss: 0.02865600772202015, acc: 98.4375, f1: 98.95652173913044, r: 0.7507082918476089
06/02/2019 09:22:40 *** evaluating ***
06/02/2019 09:22:40 step: 187, epoch: 186, acc: 60.256410256410255, f1: 24.159504407600437, r: 0.3677466511596126
06/02/2019 09:22:40 *** epoch: 188 ***
06/02/2019 09:22:40 *** training ***
06/02/2019 09:22:40 step: 6176, epoch: 187, batch: 4, loss: 0.041495174169540405, acc: 100.0, f1: 100.0, r: 0.6810779724011385
06/02/2019 09:22:41 step: 6181, epoch: 187, batch: 9, loss: 0.008320489898324013, acc: 100.0, f1: 100.0, r: 0.8190336220151566
06/02/2019 09:22:41 step: 6186, epoch: 187, batch: 14, loss: 0.02542906254529953, acc: 100.0, f1: 100.0, r: 0.7554322804116173
06/02/2019 09:22:42 step: 6191, epoch: 187, batch: 19, loss: 0.07797252386808395, acc: 96.875, f1: 98.4375, r: 0.7551468228137168
06/02/2019 09:22:42 step: 6196, epoch: 187, batch: 24, loss: 0.029226046055555344, acc: 98.4375, f1: 98.57293868921776, r: 0.7933419959538464
06/02/2019 09:22:43 step: 6201, epoch: 187, batch: 29, loss: 0.03253180533647537, acc: 98.4375, f1: 98.84978145847711, r: 0.7229795828935153
06/02/2019 09:22:43 *** evaluating ***
06/02/2019 09:22:43 step: 188, epoch: 187, acc: 60.68376068376068, f1: 24.37041020845989, r: 0.36734889855835146
06/02/2019 09:22:43 *** epoch: 189 ***
06/02/2019 09:22:43 *** training ***
06/02/2019 09:22:44 step: 6209, epoch: 188, batch: 4, loss: 0.09413043409585953, acc: 95.3125, f1: 81.1336032388664, r: 0.6213918880811126
06/02/2019 09:22:44 step: 6214, epoch: 188, batch: 9, loss: 0.035330042243003845, acc: 98.4375, f1: 97.12121212121212, r: 0.790169316194379
06/02/2019 09:22:45 step: 6219, epoch: 188, batch: 14, loss: 0.012474261224269867, acc: 100.0, f1: 100.0, r: 0.6776337986173174
06/02/2019 09:22:45 step: 6224, epoch: 188, batch: 19, loss: 0.009464945644140244, acc: 100.0, f1: 100.0, r: 0.7004103837916623
06/02/2019 09:22:46 step: 6229, epoch: 188, batch: 24, loss: 0.01474703662097454, acc: 100.0, f1: 100.0, r: 0.7587086825176668
06/02/2019 09:22:46 step: 6234, epoch: 188, batch: 29, loss: 0.019717196002602577, acc: 100.0, f1: 100.0, r: 0.7437457790147045
06/02/2019 09:22:46 *** evaluating ***
06/02/2019 09:22:46 step: 189, epoch: 188, acc: 60.68376068376068, f1: 24.372675916139734, r: 0.3686194989729911
06/02/2019 09:22:46 *** epoch: 190 ***
06/02/2019 09:22:46 *** training ***
06/02/2019 09:22:47 step: 6242, epoch: 189, batch: 4, loss: 0.006029097363352776, acc: 100.0, f1: 100.0, r: 0.6399457195064473
06/02/2019 09:22:47 step: 6247, epoch: 189, batch: 9, loss: 0.018076078966259956, acc: 100.0, f1: 100.0, r: 0.6895218736865846
06/02/2019 09:22:48 step: 6252, epoch: 189, batch: 14, loss: 0.010091847740113735, acc: 100.0, f1: 100.0, r: 0.8033929525609786
06/02/2019 09:22:48 step: 6257, epoch: 189, batch: 19, loss: 0.010588794946670532, acc: 100.0, f1: 100.0, r: 0.6972904512119573
06/02/2019 09:22:49 step: 6262, epoch: 189, batch: 24, loss: 0.0038485415279865265, acc: 100.0, f1: 100.0, r: 0.6211638947979178
06/02/2019 09:22:49 step: 6267, epoch: 189, batch: 29, loss: 0.011459598317742348, acc: 100.0, f1: 100.0, r: 0.6467567680182756
06/02/2019 09:22:49 *** evaluating ***
06/02/2019 09:22:49 step: 190, epoch: 189, acc: 61.111111111111114, f1: 24.251837286811973, r: 0.36670064839409355
06/02/2019 09:22:49 *** epoch: 191 ***
06/02/2019 09:22:49 *** training ***
06/02/2019 09:22:50 step: 6275, epoch: 190, batch: 4, loss: 0.07832568883895874, acc: 98.4375, f1: 99.12280701754386, r: 0.7902027047018831
06/02/2019 09:22:50 step: 6280, epoch: 190, batch: 9, loss: 0.01012348011136055, acc: 100.0, f1: 100.0, r: 0.7212407730952479
06/02/2019 09:22:51 step: 6285, epoch: 190, batch: 14, loss: 0.006242722272872925, acc: 100.0, f1: 100.0, r: 0.6085106786557811
06/02/2019 09:22:51 step: 6290, epoch: 190, batch: 19, loss: 0.013389033265411854, acc: 100.0, f1: 100.0, r: 0.6190119604046213
06/02/2019 09:22:52 step: 6295, epoch: 190, batch: 24, loss: 0.003180723637342453, acc: 100.0, f1: 100.0, r: 0.6505525713749869
06/02/2019 09:22:52 step: 6300, epoch: 190, batch: 29, loss: 0.0357695072889328, acc: 98.4375, f1: 95.60606060606061, r: 0.7611765462269425
06/02/2019 09:22:52 *** evaluating ***
06/02/2019 09:22:53 step: 191, epoch: 190, acc: 60.68376068376068, f1: 24.23677672186421, r: 0.3665046362455679
06/02/2019 09:22:53 *** epoch: 192 ***
06/02/2019 09:22:53 *** training ***
06/02/2019 09:22:53 step: 6308, epoch: 191, batch: 4, loss: 0.00776102626696229, acc: 100.0, f1: 100.0, r: 0.7596802673026399
06/02/2019 09:22:54 step: 6313, epoch: 191, batch: 9, loss: 0.06686030328273773, acc: 96.875, f1: 98.51190476190477, r: 0.7213489324395214
06/02/2019 09:22:54 step: 6318, epoch: 191, batch: 14, loss: 0.019289812073111534, acc: 100.0, f1: 100.0, r: 0.7399948659798289
06/02/2019 09:22:54 step: 6323, epoch: 191, batch: 19, loss: 0.05384227633476257, acc: 98.4375, f1: 98.50649350649351, r: 0.736938304774518
06/02/2019 09:22:55 step: 6328, epoch: 191, batch: 24, loss: 0.005097134038805962, acc: 100.0, f1: 100.0, r: 0.7760768694216779
06/02/2019 09:22:55 step: 6333, epoch: 191, batch: 29, loss: 0.012121038511395454, acc: 100.0, f1: 100.0, r: 0.7406133138550495
06/02/2019 09:22:56 *** evaluating ***
06/02/2019 09:22:56 step: 192, epoch: 191, acc: 60.68376068376068, f1: 24.548733722454386, r: 0.3625730827527978
06/02/2019 09:22:56 *** epoch: 193 ***
06/02/2019 09:22:56 *** training ***
06/02/2019 09:22:56 step: 6341, epoch: 192, batch: 4, loss: 0.023235084488987923, acc: 98.4375, f1: 99.28193499622071, r: 0.8246579467228785
06/02/2019 09:22:57 step: 6346, epoch: 192, batch: 9, loss: 0.0082782544195652, acc: 100.0, f1: 100.0, r: 0.6711149985036486
06/02/2019 09:22:57 step: 6351, epoch: 192, batch: 14, loss: 0.07572796940803528, acc: 98.4375, f1: 97.20730397422128, r: 0.663136458156632
06/02/2019 09:22:58 step: 6356, epoch: 192, batch: 19, loss: 0.012790534645318985, acc: 100.0, f1: 100.0, r: 0.7614667040123629
06/02/2019 09:22:58 step: 6361, epoch: 192, batch: 24, loss: 0.015175978653132915, acc: 100.0, f1: 100.0, r: 0.8504532448183175
06/02/2019 09:22:58 step: 6366, epoch: 192, batch: 29, loss: 0.04412130266427994, acc: 98.4375, f1: 97.00680272108843, r: 0.7371465245714878
06/02/2019 09:22:59 *** evaluating ***
06/02/2019 09:22:59 step: 193, epoch: 192, acc: 61.111111111111114, f1: 25.389300627742774, r: 0.3658842894299437
06/02/2019 09:22:59 *** epoch: 194 ***
06/02/2019 09:22:59 *** training ***
06/02/2019 09:22:59 step: 6374, epoch: 193, batch: 4, loss: 0.031085586175322533, acc: 98.4375, f1: 99.03703703703704, r: 0.8445221605444673
06/02/2019 09:23:00 step: 6379, epoch: 193, batch: 9, loss: 0.007801258936524391, acc: 100.0, f1: 100.0, r: 0.7702146998874958
06/02/2019 09:23:00 step: 6384, epoch: 193, batch: 14, loss: 0.027820486575365067, acc: 98.4375, f1: 99.10934020860189, r: 0.7015470668805244
06/02/2019 09:23:01 step: 6389, epoch: 193, batch: 19, loss: 0.09109855443239212, acc: 96.875, f1: 95.95807701070859, r: 0.6775290619111842
06/02/2019 09:23:01 step: 6394, epoch: 193, batch: 24, loss: 0.0496496707201004, acc: 96.875, f1: 96.08948429385772, r: 0.7923304775264142
06/02/2019 09:23:02 step: 6399, epoch: 193, batch: 29, loss: 0.07851777970790863, acc: 96.875, f1: 98.05258467023174, r: 0.78218367203011
06/02/2019 09:23:02 *** evaluating ***
06/02/2019 09:23:02 step: 194, epoch: 193, acc: 59.82905982905983, f1: 24.596106168906694, r: 0.36068570023398144
06/02/2019 09:23:02 *** epoch: 195 ***
06/02/2019 09:23:02 *** training ***
06/02/2019 09:23:02 step: 6407, epoch: 194, batch: 4, loss: 0.029012303799390793, acc: 98.4375, f1: 98.59714753331774, r: 0.7373605554492545
06/02/2019 09:23:03 step: 6412, epoch: 194, batch: 9, loss: 0.0016330070793628693, acc: 100.0, f1: 100.0, r: 0.8108492323653201
06/02/2019 09:23:03 step: 6417, epoch: 194, batch: 14, loss: 0.027463918551802635, acc: 98.4375, f1: 99.00426742532005, r: 0.7203030197105327
06/02/2019 09:23:04 step: 6422, epoch: 194, batch: 19, loss: 0.02207346260547638, acc: 100.0, f1: 100.0, r: 0.6197216526321525
06/02/2019 09:23:04 step: 6427, epoch: 194, batch: 24, loss: 0.03977369889616966, acc: 98.4375, f1: 98.60742705570291, r: 0.7793508494996908
06/02/2019 09:23:05 step: 6432, epoch: 194, batch: 29, loss: 0.039945416152477264, acc: 98.4375, f1: 97.31379731379732, r: 0.7028885466600054
06/02/2019 09:23:05 *** evaluating ***
06/02/2019 09:23:05 step: 195, epoch: 194, acc: 60.68376068376068, f1: 24.633111736542446, r: 0.36331100820166307
06/02/2019 09:23:05 *** epoch: 196 ***
06/02/2019 09:23:05 *** training ***
06/02/2019 09:23:06 step: 6440, epoch: 195, batch: 4, loss: 0.03739223629236221, acc: 98.4375, f1: 94.97835497835497, r: 0.6338655038881955
06/02/2019 09:23:06 step: 6445, epoch: 195, batch: 9, loss: 0.012509463354945183, acc: 100.0, f1: 100.0, r: 0.726343005106546
06/02/2019 09:23:06 step: 6450, epoch: 195, batch: 14, loss: 0.011612150818109512, acc: 100.0, f1: 100.0, r: 0.5760531129654605
06/02/2019 09:23:07 step: 6455, epoch: 195, batch: 19, loss: 0.009652161970734596, acc: 100.0, f1: 100.0, r: 0.6915208986944221
06/02/2019 09:23:07 step: 6460, epoch: 195, batch: 24, loss: 0.00862734206020832, acc: 100.0, f1: 100.0, r: 0.6264016165794801
06/02/2019 09:23:08 step: 6465, epoch: 195, batch: 29, loss: 0.1018407866358757, acc: 96.875, f1: 93.4526623888326, r: 0.6826162192573535
06/02/2019 09:23:08 *** evaluating ***
06/02/2019 09:23:08 step: 196, epoch: 195, acc: 60.68376068376068, f1: 24.633111736542446, r: 0.36577141416369346
06/02/2019 09:23:08 *** epoch: 197 ***
06/02/2019 09:23:08 *** training ***
06/02/2019 09:23:09 step: 6473, epoch: 196, batch: 4, loss: 0.025526683777570724, acc: 100.0, f1: 100.0, r: 0.6786858120125685
06/02/2019 09:23:09 step: 6478, epoch: 196, batch: 9, loss: 0.014399146661162376, acc: 100.0, f1: 100.0, r: 0.6844987392926715
06/02/2019 09:23:10 step: 6483, epoch: 196, batch: 14, loss: 0.12336256355047226, acc: 96.875, f1: 94.13718723037101, r: 0.7220610298077096
06/02/2019 09:23:10 step: 6488, epoch: 196, batch: 19, loss: 0.06961317360401154, acc: 95.3125, f1: 92.7659661127403, r: 0.781308909813677
06/02/2019 09:23:10 step: 6493, epoch: 196, batch: 24, loss: 0.018758254125714302, acc: 100.0, f1: 100.0, r: 0.8429416576278301
06/02/2019 09:23:11 step: 6498, epoch: 196, batch: 29, loss: 0.023148000240325928, acc: 100.0, f1: 100.0, r: 0.6454960910355032
06/02/2019 09:23:11 *** evaluating ***
06/02/2019 09:23:11 step: 197, epoch: 196, acc: 61.53846153846154, f1: 24.92130053138802, r: 0.3664012364533028
06/02/2019 09:23:11 *** epoch: 198 ***
06/02/2019 09:23:11 *** training ***
06/02/2019 09:23:12 step: 6506, epoch: 197, batch: 4, loss: 0.011897878721356392, acc: 100.0, f1: 100.0, r: 0.758804642308458
06/02/2019 09:23:12 step: 6511, epoch: 197, batch: 9, loss: 0.02082640305161476, acc: 100.0, f1: 100.0, r: 0.6916975921108787
06/02/2019 09:23:13 step: 6516, epoch: 197, batch: 14, loss: 0.02413146011531353, acc: 98.4375, f1: 97.87644787644788, r: 0.8192268679945058
06/02/2019 09:23:13 step: 6521, epoch: 197, batch: 19, loss: 0.029416009783744812, acc: 98.4375, f1: 98.9329064959317, r: 0.686901194455388
06/02/2019 09:23:14 step: 6526, epoch: 197, batch: 24, loss: 0.013439079746603966, acc: 100.0, f1: 100.0, r: 0.7902165776377654
06/02/2019 09:23:14 step: 6531, epoch: 197, batch: 29, loss: 0.0022775940597057343, acc: 100.0, f1: 100.0, r: 0.8304466945683006
06/02/2019 09:23:14 *** evaluating ***
06/02/2019 09:23:14 step: 198, epoch: 197, acc: 61.53846153846154, f1: 25.73479908058952, r: 0.36316908215384036
06/02/2019 09:23:14 *** epoch: 199 ***
06/02/2019 09:23:14 *** training ***
06/02/2019 09:23:15 step: 6539, epoch: 198, batch: 4, loss: 0.03211137652397156, acc: 98.4375, f1: 98.10874704491727, r: 0.6748968276686406
06/02/2019 09:23:15 step: 6544, epoch: 198, batch: 9, loss: 0.06829570233821869, acc: 96.875, f1: 94.19504643962848, r: 0.7694959809806338
06/02/2019 09:23:16 step: 6549, epoch: 198, batch: 14, loss: 0.017558176070451736, acc: 100.0, f1: 100.0, r: 0.7143376612587697
06/02/2019 09:23:16 step: 6554, epoch: 198, batch: 19, loss: 0.030078496783971786, acc: 98.4375, f1: 99.00226757369614, r: 0.7020179062654279
06/02/2019 09:23:17 step: 6559, epoch: 198, batch: 24, loss: 0.04758353903889656, acc: 98.4375, f1: 97.96918767507003, r: 0.807896417496115
06/02/2019 09:23:17 step: 6564, epoch: 198, batch: 29, loss: 0.020961198955774307, acc: 100.0, f1: 100.0, r: 0.6825250487024426
06/02/2019 09:23:17 *** evaluating ***
06/02/2019 09:23:17 step: 199, epoch: 198, acc: 61.965811965811966, f1: 25.87043541156581, r: 0.36309500651827326
06/02/2019 09:23:17 *** epoch: 200 ***
06/02/2019 09:23:17 *** training ***
06/02/2019 09:23:18 step: 6572, epoch: 199, batch: 4, loss: 0.009081274271011353, acc: 100.0, f1: 100.0, r: 0.6761089464570014
06/02/2019 09:23:18 step: 6577, epoch: 199, batch: 9, loss: 0.012725241482257843, acc: 100.0, f1: 100.0, r: 0.7792510040451464
06/02/2019 09:23:19 step: 6582, epoch: 199, batch: 14, loss: 0.0032632723450660706, acc: 100.0, f1: 100.0, r: 0.6821929494759125
06/02/2019 09:23:19 step: 6587, epoch: 199, batch: 19, loss: 0.031119339168071747, acc: 98.4375, f1: 98.63523573200992, r: 0.7609998908937999
06/02/2019 09:23:20 step: 6592, epoch: 199, batch: 24, loss: 0.14644302427768707, acc: 95.3125, f1: 96.56876456876458, r: 0.7853477081655237
06/02/2019 09:23:20 step: 6597, epoch: 199, batch: 29, loss: 0.01744496077299118, acc: 100.0, f1: 100.0, r: 0.7373222781009757
06/02/2019 09:23:20 *** evaluating ***
06/02/2019 09:23:21 step: 200, epoch: 199, acc: 61.111111111111114, f1: 24.550898926521228, r: 0.3660861923611665
06/02/2019 09:23:21 *** epoch: 201 ***
06/02/2019 09:23:21 *** training ***
06/02/2019 09:23:21 step: 6605, epoch: 200, batch: 4, loss: 0.0840478390455246, acc: 98.4375, f1: 97.80219780219781, r: 0.6960476178717434
06/02/2019 09:23:21 step: 6610, epoch: 200, batch: 9, loss: 0.01677543669939041, acc: 100.0, f1: 100.0, r: 0.6882070057313036
06/02/2019 09:23:22 step: 6615, epoch: 200, batch: 14, loss: 0.11555624008178711, acc: 96.875, f1: 95.69445311037623, r: 0.728425548716027
06/02/2019 09:23:22 step: 6620, epoch: 200, batch: 19, loss: 0.020140836015343666, acc: 100.0, f1: 100.0, r: 0.6562970085458588
06/02/2019 09:23:23 step: 6625, epoch: 200, batch: 24, loss: 0.010652020573616028, acc: 100.0, f1: 100.0, r: 0.7370417684397793
06/02/2019 09:23:23 step: 6630, epoch: 200, batch: 29, loss: 0.0547684021294117, acc: 96.875, f1: 98.14457601222307, r: 0.7809699105114295
06/02/2019 09:23:23 *** evaluating ***
06/02/2019 09:23:24 step: 201, epoch: 200, acc: 61.965811965811966, f1: 26.927840480547616, r: 0.37268980003894964
06/02/2019 09:23:24 *** epoch: 202 ***
06/02/2019 09:23:24 *** training ***
06/02/2019 09:23:24 step: 6638, epoch: 201, batch: 4, loss: 0.05341270565986633, acc: 96.875, f1: 97.50061968107082, r: 0.6082220495844883
06/02/2019 09:23:24 step: 6643, epoch: 201, batch: 9, loss: 0.026689346879720688, acc: 98.4375, f1: 98.39924670433145, r: 0.7682566528451222
06/02/2019 09:23:25 step: 6648, epoch: 201, batch: 14, loss: 0.04211971163749695, acc: 98.4375, f1: 96.37188208616782, r: 0.6744800831739682
06/02/2019 09:23:25 step: 6653, epoch: 201, batch: 19, loss: 0.01151762343943119, acc: 100.0, f1: 100.0, r: 0.7821965275071661
06/02/2019 09:23:26 step: 6658, epoch: 201, batch: 24, loss: 0.022198496386408806, acc: 98.4375, f1: 98.96774193548387, r: 0.6719114325081572
06/02/2019 09:23:26 step: 6663, epoch: 201, batch: 29, loss: 0.011345608159899712, acc: 100.0, f1: 100.0, r: 0.6951745733950749
06/02/2019 09:23:26 *** evaluating ***
06/02/2019 09:23:27 step: 202, epoch: 201, acc: 61.53846153846154, f1: 26.10163218330195, r: 0.3714974822068829
06/02/2019 09:23:27 *** epoch: 203 ***
06/02/2019 09:23:27 *** training ***
06/02/2019 09:23:27 step: 6671, epoch: 202, batch: 4, loss: 0.04889335110783577, acc: 98.4375, f1: 99.06273620559335, r: 0.7309257691397385
06/02/2019 09:23:27 step: 6676, epoch: 202, batch: 9, loss: 0.024511903524398804, acc: 98.4375, f1: 99.22171018945212, r: 0.6668979449219319
06/02/2019 09:23:28 step: 6681, epoch: 202, batch: 14, loss: 0.03651007264852524, acc: 100.0, f1: 100.0, r: 0.6715098997750873
06/02/2019 09:23:28 step: 6686, epoch: 202, batch: 19, loss: 0.008633963763713837, acc: 100.0, f1: 100.0, r: 0.6405912076977394
06/02/2019 09:23:29 step: 6691, epoch: 202, batch: 24, loss: 0.029214123263955116, acc: 98.4375, f1: 98.20574162679425, r: 0.8018607987659316
06/02/2019 09:23:29 step: 6696, epoch: 202, batch: 29, loss: 0.011998634785413742, acc: 100.0, f1: 100.0, r: 0.7541212374659279
06/02/2019 09:23:30 *** evaluating ***
06/02/2019 09:23:30 step: 203, epoch: 202, acc: 61.53846153846154, f1: 24.76163193421942, r: 0.3686921796283814
06/02/2019 09:23:30 *** epoch: 204 ***
06/02/2019 09:23:30 *** training ***
06/02/2019 09:23:30 step: 6704, epoch: 203, batch: 4, loss: 0.0029116757214069366, acc: 100.0, f1: 100.0, r: 0.7832439013859278
06/02/2019 09:23:31 step: 6709, epoch: 203, batch: 9, loss: 0.13399508595466614, acc: 96.875, f1: 93.90902208617227, r: 0.7070245033248618
06/02/2019 09:23:31 step: 6714, epoch: 203, batch: 14, loss: 0.01111580803990364, acc: 100.0, f1: 100.0, r: 0.8059486355569554
06/02/2019 09:23:31 step: 6719, epoch: 203, batch: 19, loss: 0.0014820992946624756, acc: 100.0, f1: 100.0, r: 0.6695964637719167
06/02/2019 09:23:32 step: 6724, epoch: 203, batch: 24, loss: 0.011480779387056828, acc: 100.0, f1: 100.0, r: 0.7074027314030908
06/02/2019 09:23:32 step: 6729, epoch: 203, batch: 29, loss: 0.029497237876057625, acc: 98.4375, f1: 98.06076276664513, r: 0.7314758130391232
06/02/2019 09:23:33 *** evaluating ***
06/02/2019 09:23:33 step: 204, epoch: 203, acc: 60.68376068376068, f1: 24.503539938322547, r: 0.36548084506834705
06/02/2019 09:23:33 *** epoch: 205 ***
06/02/2019 09:23:33 *** training ***
06/02/2019 09:23:33 step: 6737, epoch: 204, batch: 4, loss: 0.01706717163324356, acc: 100.0, f1: 100.0, r: 0.8186846863738988
06/02/2019 09:23:34 step: 6742, epoch: 204, batch: 9, loss: 0.09158872067928314, acc: 95.3125, f1: 93.31629735942404, r: 0.7080567096900398
06/02/2019 09:23:34 step: 6747, epoch: 204, batch: 14, loss: 0.06206235662102699, acc: 98.4375, f1: 99.26314819931841, r: 0.6942066262624125
06/02/2019 09:23:35 step: 6752, epoch: 204, batch: 19, loss: 0.012138663791120052, acc: 100.0, f1: 100.0, r: 0.6930012833371234
06/02/2019 09:23:35 step: 6757, epoch: 204, batch: 24, loss: 0.08477519452571869, acc: 95.3125, f1: 96.54121863799283, r: 0.7061683889450129
06/02/2019 09:23:35 step: 6762, epoch: 204, batch: 29, loss: 0.013319244608283043, acc: 100.0, f1: 100.0, r: 0.715254318290342
06/02/2019 09:23:36 *** evaluating ***
06/02/2019 09:23:36 step: 205, epoch: 204, acc: 59.82905982905983, f1: 23.993385678168288, r: 0.3730796815988714
06/02/2019 09:23:36 *** epoch: 206 ***
06/02/2019 09:23:36 *** training ***
06/02/2019 09:23:36 step: 6770, epoch: 205, batch: 4, loss: 0.038364265114068985, acc: 98.4375, f1: 98.40975351179434, r: 0.7011925399729146
06/02/2019 09:23:37 step: 6775, epoch: 205, batch: 9, loss: 0.046903096139431, acc: 98.4375, f1: 97.83549783549783, r: 0.7865242117115787
06/02/2019 09:23:37 step: 6780, epoch: 205, batch: 14, loss: 0.012027906253933907, acc: 100.0, f1: 100.0, r: 0.6938022913875459
06/02/2019 09:23:38 step: 6785, epoch: 205, batch: 19, loss: 0.011255241930484772, acc: 100.0, f1: 100.0, r: 0.6888327615486736
06/02/2019 09:23:38 step: 6790, epoch: 205, batch: 24, loss: 0.09236463904380798, acc: 95.3125, f1: 92.2924297924298, r: 0.7997371441077717
06/02/2019 09:23:39 step: 6795, epoch: 205, batch: 29, loss: 0.035866037011146545, acc: 100.0, f1: 100.0, r: 0.7157286622845166
06/02/2019 09:23:39 *** evaluating ***
06/02/2019 09:23:39 step: 206, epoch: 205, acc: 60.256410256410255, f1: 24.064028505858374, r: 0.36926609190707593
06/02/2019 09:23:39 *** epoch: 207 ***
06/02/2019 09:23:39 *** training ***
06/02/2019 09:23:39 step: 6803, epoch: 206, batch: 4, loss: 0.021655069664120674, acc: 98.4375, f1: 95.28985507246377, r: 0.8370750677956738
06/02/2019 09:23:40 step: 6808, epoch: 206, batch: 9, loss: 0.016581229865550995, acc: 100.0, f1: 100.0, r: 0.7929564383183182
06/02/2019 09:23:40 step: 6813, epoch: 206, batch: 14, loss: 0.07370929419994354, acc: 96.875, f1: 95.83090379008746, r: 0.6277125541867359
06/02/2019 09:23:41 step: 6818, epoch: 206, batch: 19, loss: 0.011519908905029297, acc: 100.0, f1: 100.0, r: 0.8040131008270188
06/02/2019 09:23:41 step: 6823, epoch: 206, batch: 24, loss: 0.04118098318576813, acc: 98.4375, f1: 98.2905982905983, r: 0.8137457056455115
06/02/2019 09:23:42 step: 6828, epoch: 206, batch: 29, loss: 0.008956434205174446, acc: 100.0, f1: 100.0, r: 0.7768618411123925
06/02/2019 09:23:42 *** evaluating ***
06/02/2019 09:23:42 step: 207, epoch: 206, acc: 61.53846153846154, f1: 25.660428371207367, r: 0.36645731446511914
06/02/2019 09:23:42 *** epoch: 208 ***
06/02/2019 09:23:42 *** training ***
06/02/2019 09:23:42 step: 6836, epoch: 207, batch: 4, loss: 0.014447351917624474, acc: 100.0, f1: 100.0, r: 0.8158082629086727
06/02/2019 09:23:43 step: 6841, epoch: 207, batch: 9, loss: 0.003608301281929016, acc: 100.0, f1: 100.0, r: 0.8311903321489076
06/02/2019 09:23:43 step: 6846, epoch: 207, batch: 14, loss: 0.007080478593707085, acc: 100.0, f1: 100.0, r: 0.8205781366303895
06/02/2019 09:23:44 step: 6851, epoch: 207, batch: 19, loss: 0.023791613057255745, acc: 98.4375, f1: 85.45454545454547, r: 0.6543279164266267
06/02/2019 09:23:44 step: 6856, epoch: 207, batch: 24, loss: 0.01946870982646942, acc: 100.0, f1: 100.0, r: 0.6956494965266986
06/02/2019 09:23:45 step: 6861, epoch: 207, batch: 29, loss: 0.015160324051976204, acc: 100.0, f1: 100.0, r: 0.7480313336017975
06/02/2019 09:23:45 *** evaluating ***
06/02/2019 09:23:45 step: 208, epoch: 207, acc: 61.53846153846154, f1: 25.071871669444977, r: 0.36340811573443926
06/02/2019 09:23:45 *** epoch: 209 ***
06/02/2019 09:23:45 *** training ***
06/02/2019 09:23:45 step: 6869, epoch: 208, batch: 4, loss: 0.02463674731552601, acc: 98.4375, f1: 85.09316770186335, r: 0.6582114349609139
06/02/2019 09:23:46 step: 6874, epoch: 208, batch: 9, loss: 0.012555161491036415, acc: 100.0, f1: 100.0, r: 0.723604000402432
06/02/2019 09:23:46 step: 6879, epoch: 208, batch: 14, loss: 0.03883462771773338, acc: 98.4375, f1: 96.82539682539682, r: 0.7230853737777789
06/02/2019 09:23:47 step: 6884, epoch: 208, batch: 19, loss: 0.015268583782017231, acc: 100.0, f1: 100.0, r: 0.7816236126850167
06/02/2019 09:23:47 step: 6889, epoch: 208, batch: 24, loss: 0.03004520758986473, acc: 100.0, f1: 100.0, r: 0.716414652975248
06/02/2019 09:23:48 step: 6894, epoch: 208, batch: 29, loss: 0.012648331932723522, acc: 100.0, f1: 100.0, r: 0.6681456769677241
06/02/2019 09:23:48 *** evaluating ***
06/02/2019 09:23:48 step: 209, epoch: 208, acc: 61.111111111111114, f1: 25.4264482993114, r: 0.3667294082897899
06/02/2019 09:23:48 *** epoch: 210 ***
06/02/2019 09:23:48 *** training ***
06/02/2019 09:23:49 step: 6902, epoch: 209, batch: 4, loss: 0.010589424520730972, acc: 100.0, f1: 100.0, r: 0.756301018430729
06/02/2019 09:23:49 step: 6907, epoch: 209, batch: 9, loss: 0.013996783643960953, acc: 100.0, f1: 100.0, r: 0.6653058265779136
06/02/2019 09:23:49 step: 6912, epoch: 209, batch: 14, loss: 0.035189345479011536, acc: 98.4375, f1: 97.55102040816327, r: 0.6277284248560789
06/02/2019 09:23:50 step: 6917, epoch: 209, batch: 19, loss: 0.01542419008910656, acc: 100.0, f1: 100.0, r: 0.7302479538365924
06/02/2019 09:23:50 step: 6922, epoch: 209, batch: 24, loss: 0.017418937757611275, acc: 100.0, f1: 100.0, r: 0.6451605878545246
06/02/2019 09:23:51 step: 6927, epoch: 209, batch: 29, loss: 0.017458025366067886, acc: 100.0, f1: 100.0, r: 0.7109588992242404
06/02/2019 09:23:51 *** evaluating ***
06/02/2019 09:23:51 step: 210, epoch: 209, acc: 60.68376068376068, f1: 25.27912136079113, r: 0.3683142054387133
06/02/2019 09:23:51 *** epoch: 211 ***
06/02/2019 09:23:51 *** training ***
06/02/2019 09:23:52 step: 6935, epoch: 210, batch: 4, loss: 0.004319369792938232, acc: 100.0, f1: 100.0, r: 0.6933345658361784
06/02/2019 09:23:52 step: 6940, epoch: 210, batch: 9, loss: 0.011482637375593185, acc: 100.0, f1: 100.0, r: 0.6788269427449627
06/02/2019 09:23:53 step: 6945, epoch: 210, batch: 14, loss: 0.08006548136472702, acc: 96.875, f1: 94.43945293001896, r: 0.7272523053350484
06/02/2019 09:23:53 step: 6950, epoch: 210, batch: 19, loss: 0.029507745057344437, acc: 98.4375, f1: 97.4814814814815, r: 0.6566754186388407
06/02/2019 09:23:53 step: 6955, epoch: 210, batch: 24, loss: 0.02604036033153534, acc: 98.4375, f1: 99.17989417989418, r: 0.8169944147503692
06/02/2019 09:23:54 step: 6960, epoch: 210, batch: 29, loss: 0.01273912563920021, acc: 100.0, f1: 100.0, r: 0.6761553474150402
06/02/2019 09:23:54 *** evaluating ***
06/02/2019 09:23:54 step: 211, epoch: 210, acc: 59.401709401709404, f1: 24.13259144237405, r: 0.3644859852404305
06/02/2019 09:23:54 *** epoch: 212 ***
06/02/2019 09:23:54 *** training ***
06/02/2019 09:23:55 step: 6968, epoch: 211, batch: 4, loss: 0.008731842041015625, acc: 100.0, f1: 100.0, r: 0.7744277124977953
06/02/2019 09:23:55 step: 6973, epoch: 211, batch: 9, loss: 0.0011153332889080048, acc: 100.0, f1: 100.0, r: 0.7767099333284754
06/02/2019 09:23:56 step: 6978, epoch: 211, batch: 14, loss: 0.01788254827260971, acc: 98.4375, f1: 99.04761904761905, r: 0.7610439695152104
06/02/2019 09:23:56 step: 6983, epoch: 211, batch: 19, loss: 0.011134818196296692, acc: 100.0, f1: 100.0, r: 0.7031354732632222
06/02/2019 09:23:57 step: 6988, epoch: 211, batch: 24, loss: 0.03881696239113808, acc: 98.4375, f1: 99.00426742532005, r: 0.8388426073249252
06/02/2019 09:23:57 step: 6993, epoch: 211, batch: 29, loss: 0.004614550620317459, acc: 100.0, f1: 100.0, r: 0.8033026929198691
06/02/2019 09:23:57 *** evaluating ***
06/02/2019 09:23:57 step: 212, epoch: 211, acc: 59.82905982905983, f1: 24.034884761723447, r: 0.3689019842553816
06/02/2019 09:23:57 *** epoch: 213 ***
06/02/2019 09:23:57 *** training ***
06/02/2019 09:23:58 step: 7001, epoch: 212, batch: 4, loss: 0.010450251400470734, acc: 100.0, f1: 100.0, r: 0.7211928087160309
06/02/2019 09:23:58 step: 7006, epoch: 212, batch: 9, loss: 0.0068758465349674225, acc: 100.0, f1: 100.0, r: 0.816504719136488
06/02/2019 09:23:59 step: 7011, epoch: 212, batch: 14, loss: 0.010649547912180424, acc: 100.0, f1: 100.0, r: 0.5881856845866917
06/02/2019 09:23:59 step: 7016, epoch: 212, batch: 19, loss: 0.0016882233321666718, acc: 100.0, f1: 100.0, r: 0.8485810056612907
06/02/2019 09:24:00 step: 7021, epoch: 212, batch: 24, loss: 0.04893467575311661, acc: 98.4375, f1: 97.22222222222221, r: 0.7642203710854414
06/02/2019 09:24:00 step: 7026, epoch: 212, batch: 29, loss: 0.003930173814296722, acc: 100.0, f1: 100.0, r: 0.7439490661640457
06/02/2019 09:24:00 *** evaluating ***
06/02/2019 09:24:00 step: 213, epoch: 212, acc: 60.256410256410255, f1: 25.75042188402339, r: 0.3696897705173337
06/02/2019 09:24:00 *** epoch: 214 ***
06/02/2019 09:24:00 *** training ***
06/02/2019 09:24:01 step: 7034, epoch: 213, batch: 4, loss: 0.025688905268907547, acc: 100.0, f1: 100.0, r: 0.6676328685229562
06/02/2019 09:24:01 step: 7039, epoch: 213, batch: 9, loss: 0.011817239224910736, acc: 100.0, f1: 100.0, r: 0.797059921570797
06/02/2019 09:24:02 step: 7044, epoch: 213, batch: 14, loss: 0.050809212028980255, acc: 96.875, f1: 96.59937888198758, r: 0.71190572996892
06/02/2019 09:24:02 step: 7049, epoch: 213, batch: 19, loss: 0.004657629877328873, acc: 100.0, f1: 100.0, r: 0.726027079997202
06/02/2019 09:24:03 step: 7054, epoch: 213, batch: 24, loss: 0.00806792639195919, acc: 100.0, f1: 100.0, r: 0.7709881756656797
06/02/2019 09:24:03 step: 7059, epoch: 213, batch: 29, loss: 0.042500149458646774, acc: 96.875, f1: 96.85592185592185, r: 0.6919222919811071
06/02/2019 09:24:03 *** evaluating ***
06/02/2019 09:24:04 step: 214, epoch: 213, acc: 61.111111111111114, f1: 25.07164880397455, r: 0.3637468567018104
06/02/2019 09:24:04 *** epoch: 215 ***
06/02/2019 09:24:04 *** training ***
06/02/2019 09:24:04 step: 7067, epoch: 214, batch: 4, loss: 0.04799193888902664, acc: 96.875, f1: 98.18007662835248, r: 0.7954823048211342
06/02/2019 09:24:04 step: 7072, epoch: 214, batch: 9, loss: 0.020622724667191505, acc: 98.4375, f1: 98.95657511124752, r: 0.7648373455056356
06/02/2019 09:24:05 step: 7077, epoch: 214, batch: 14, loss: 0.06821144372224808, acc: 96.875, f1: 96.28885400313972, r: 0.620796530484612
06/02/2019 09:24:05 step: 7082, epoch: 214, batch: 19, loss: 0.016066832467913628, acc: 100.0, f1: 100.0, r: 0.8223683000517454
06/02/2019 09:24:06 step: 7087, epoch: 214, batch: 24, loss: 0.06612817943096161, acc: 95.3125, f1: 94.34217171717172, r: 0.8024933705752171
06/02/2019 09:24:06 step: 7092, epoch: 214, batch: 29, loss: 0.03588801249861717, acc: 98.4375, f1: 99.37145589943295, r: 0.6771437428133258
06/02/2019 09:24:06 *** evaluating ***
06/02/2019 09:24:07 step: 215, epoch: 214, acc: 61.111111111111114, f1: 25.78475871954133, r: 0.3631558077890622
06/02/2019 09:24:07 *** epoch: 216 ***
06/02/2019 09:24:07 *** training ***
06/02/2019 09:24:07 step: 7100, epoch: 215, batch: 4, loss: 0.038388434797525406, acc: 98.4375, f1: 97.55639097744361, r: 0.7517288404886298
06/02/2019 09:24:08 step: 7105, epoch: 215, batch: 9, loss: 0.014700271189212799, acc: 100.0, f1: 100.0, r: 0.8168376537241111
06/02/2019 09:24:08 step: 7110, epoch: 215, batch: 14, loss: 0.0068619754165410995, acc: 100.0, f1: 100.0, r: 0.6840115724059866
06/02/2019 09:24:08 step: 7115, epoch: 215, batch: 19, loss: 0.010885436087846756, acc: 100.0, f1: 100.0, r: 0.6667153926871255
06/02/2019 09:24:09 step: 7120, epoch: 215, batch: 24, loss: 0.01618305593729019, acc: 98.4375, f1: 99.28698752228165, r: 0.619517531424303
06/02/2019 09:24:09 step: 7125, epoch: 215, batch: 29, loss: 0.006944924592971802, acc: 100.0, f1: 100.0, r: 0.7982282118795128
06/02/2019 09:24:10 *** evaluating ***
06/02/2019 09:24:10 step: 216, epoch: 215, acc: 59.82905982905983, f1: 25.600845410628022, r: 0.36000879200770514
06/02/2019 09:24:10 *** epoch: 217 ***
06/02/2019 09:24:10 *** training ***
06/02/2019 09:24:10 step: 7133, epoch: 216, batch: 4, loss: 0.010075639933347702, acc: 100.0, f1: 100.0, r: 0.7983400531501681
06/02/2019 09:24:11 step: 7138, epoch: 216, batch: 9, loss: 0.021915288642048836, acc: 98.4375, f1: 96.9047619047619, r: 0.8029580089886721
06/02/2019 09:24:11 step: 7143, epoch: 216, batch: 14, loss: 0.024865616112947464, acc: 100.0, f1: 100.0, r: 0.8480581242698892
06/02/2019 09:24:11 step: 7148, epoch: 216, batch: 19, loss: 0.021006572991609573, acc: 98.4375, f1: 99.05284147557329, r: 0.7744628331107504
06/02/2019 09:24:12 step: 7153, epoch: 216, batch: 24, loss: 0.05245579779148102, acc: 96.875, f1: 92.95977011494253, r: 0.6753636162307144
06/02/2019 09:24:12 step: 7158, epoch: 216, batch: 29, loss: 0.05406506359577179, acc: 96.875, f1: 84.04395604395604, r: 0.5884722689030698
06/02/2019 09:24:13 *** evaluating ***
06/02/2019 09:24:13 step: 217, epoch: 216, acc: 59.82905982905983, f1: 25.613197189284143, r: 0.36054391551189124
06/02/2019 09:24:13 *** epoch: 218 ***
06/02/2019 09:24:13 *** training ***
06/02/2019 09:24:13 step: 7166, epoch: 217, batch: 4, loss: 0.022072769701480865, acc: 100.0, f1: 100.0, r: 0.779534303247088
06/02/2019 09:24:14 step: 7171, epoch: 217, batch: 9, loss: 0.04622324928641319, acc: 96.875, f1: 98.40816326530613, r: 0.6956493103630154
06/02/2019 09:24:14 step: 7176, epoch: 217, batch: 14, loss: 0.02316959574818611, acc: 100.0, f1: 100.0, r: 0.7960126485159164
06/02/2019 09:24:15 step: 7181, epoch: 217, batch: 19, loss: 0.09251785278320312, acc: 96.875, f1: 92.8502315294768, r: 0.7787306595232516
06/02/2019 09:24:15 step: 7186, epoch: 217, batch: 24, loss: 0.010396447032690048, acc: 100.0, f1: 100.0, r: 0.8096260539164507
06/02/2019 09:24:15 step: 7191, epoch: 217, batch: 29, loss: 0.03923824056982994, acc: 98.4375, f1: 99.1282554211616, r: 0.7007317038596936
06/02/2019 09:24:16 *** evaluating ***
06/02/2019 09:24:16 step: 218, epoch: 217, acc: 60.256410256410255, f1: 25.674269338623933, r: 0.35833005452876304
06/02/2019 09:24:16 *** epoch: 219 ***
06/02/2019 09:24:16 *** training ***
06/02/2019 09:24:16 step: 7199, epoch: 218, batch: 4, loss: 0.00797196663916111, acc: 100.0, f1: 100.0, r: 0.8648264216925141
06/02/2019 09:24:17 step: 7204, epoch: 218, batch: 9, loss: 0.007734663784503937, acc: 100.0, f1: 100.0, r: 0.6578272271972276
06/02/2019 09:24:17 step: 7209, epoch: 218, batch: 14, loss: 0.029350385069847107, acc: 98.4375, f1: 97.67907162865146, r: 0.7257641584530359
06/02/2019 09:24:18 step: 7214, epoch: 218, batch: 19, loss: 0.00806429237127304, acc: 100.0, f1: 100.0, r: 0.6733320440961194
06/02/2019 09:24:18 step: 7219, epoch: 218, batch: 24, loss: 0.0040337685495615005, acc: 100.0, f1: 100.0, r: 0.7152798300698598
06/02/2019 09:24:19 step: 7224, epoch: 218, batch: 29, loss: 0.04495719447731972, acc: 98.4375, f1: 97.75132275132275, r: 0.8157703867430082
06/02/2019 09:24:19 *** evaluating ***
06/02/2019 09:24:19 step: 219, epoch: 218, acc: 61.111111111111114, f1: 26.011244542527685, r: 0.35850649076340585
06/02/2019 09:24:19 *** epoch: 220 ***
06/02/2019 09:24:19 *** training ***
06/02/2019 09:24:19 step: 7232, epoch: 219, batch: 4, loss: 0.029026074334979057, acc: 98.4375, f1: 96.81063122923588, r: 0.6794229607709452
06/02/2019 09:24:20 step: 7237, epoch: 219, batch: 9, loss: 0.015017274767160416, acc: 100.0, f1: 100.0, r: 0.7475027598490888
06/02/2019 09:24:20 step: 7242, epoch: 219, batch: 14, loss: 0.06182941049337387, acc: 98.4375, f1: 95.62146892655367, r: 0.769726682596896
06/02/2019 09:24:21 step: 7247, epoch: 219, batch: 19, loss: 0.03550737723708153, acc: 98.4375, f1: 98.81123180979709, r: 0.6328867533352769
06/02/2019 09:24:21 step: 7252, epoch: 219, batch: 24, loss: 0.036787934601306915, acc: 98.4375, f1: 99.31172468987596, r: 0.6781598949803821
06/02/2019 09:24:22 step: 7257, epoch: 219, batch: 29, loss: 0.0511230006814003, acc: 96.875, f1: 98.05555555555556, r: 0.7841655150669363
06/02/2019 09:24:22 *** evaluating ***
06/02/2019 09:24:22 step: 220, epoch: 219, acc: 60.256410256410255, f1: 25.61288580898732, r: 0.3615388730295815
06/02/2019 09:24:22 *** epoch: 221 ***
06/02/2019 09:24:22 *** training ***
06/02/2019 09:24:22 step: 7265, epoch: 220, batch: 4, loss: 0.05795465037226677, acc: 96.875, f1: 97.52415458937197, r: 0.7767814810772781
06/02/2019 09:24:23 step: 7270, epoch: 220, batch: 9, loss: 0.023370083421468735, acc: 100.0, f1: 100.0, r: 0.6787605825703568
06/02/2019 09:24:23 step: 7275, epoch: 220, batch: 14, loss: 0.02350398525595665, acc: 98.4375, f1: 98.02197802197803, r: 0.7748617033410088
06/02/2019 09:24:24 step: 7280, epoch: 220, batch: 19, loss: 0.01339009404182434, acc: 100.0, f1: 100.0, r: 0.6862646788738593
06/02/2019 09:24:24 step: 7285, epoch: 220, batch: 24, loss: 0.031156273558735847, acc: 98.4375, f1: 97.55639097744361, r: 0.7722283661845104
06/02/2019 09:24:25 step: 7290, epoch: 220, batch: 29, loss: 0.061403535306453705, acc: 96.875, f1: 96.58613445378151, r: 0.675585087254172
06/02/2019 09:24:25 *** evaluating ***
06/02/2019 09:24:25 step: 221, epoch: 220, acc: 60.68376068376068, f1: 26.099649348816484, r: 0.3604667487174042
06/02/2019 09:24:25 *** epoch: 222 ***
06/02/2019 09:24:25 *** training ***
06/02/2019 09:24:25 step: 7298, epoch: 221, batch: 4, loss: 0.026147274300456047, acc: 98.4375, f1: 87.03703703703704, r: 0.7840401498838081
06/02/2019 09:24:26 step: 7303, epoch: 221, batch: 9, loss: 0.014625519514083862, acc: 100.0, f1: 100.0, r: 0.7104847228110895
06/02/2019 09:24:26 step: 7308, epoch: 221, batch: 14, loss: 0.028678661212325096, acc: 98.4375, f1: 98.7878787878788, r: 0.6722262899273257
06/02/2019 09:24:27 step: 7313, epoch: 221, batch: 19, loss: 0.07680065184831619, acc: 95.3125, f1: 95.41950113378685, r: 0.6528716511564925
06/02/2019 09:24:27 step: 7318, epoch: 221, batch: 24, loss: 0.021561311557888985, acc: 98.4375, f1: 99.31623931623932, r: 0.6597527554236984
06/02/2019 09:24:28 step: 7323, epoch: 221, batch: 29, loss: 0.044821597635746, acc: 98.4375, f1: 99.15164369034994, r: 0.8058218461640663
06/02/2019 09:24:28 *** evaluating ***
06/02/2019 09:24:28 step: 222, epoch: 221, acc: 60.68376068376068, f1: 25.77708319143779, r: 0.3607384815143605
06/02/2019 09:24:28 *** epoch: 223 ***
06/02/2019 09:24:28 *** training ***
06/02/2019 09:24:29 step: 7331, epoch: 222, batch: 4, loss: 0.020816802978515625, acc: 98.4375, f1: 98.85571249776507, r: 0.7798452584276184
06/02/2019 09:24:29 step: 7336, epoch: 222, batch: 9, loss: 0.04920584708452225, acc: 98.4375, f1: 98.65047233468286, r: 0.7193028351357683
06/02/2019 09:24:30 step: 7341, epoch: 222, batch: 14, loss: 0.01402306742966175, acc: 100.0, f1: 100.0, r: 0.8195857747521057
06/02/2019 09:24:30 step: 7346, epoch: 222, batch: 19, loss: 0.010225232690572739, acc: 100.0, f1: 100.0, r: 0.6387338536930622
06/02/2019 09:24:30 step: 7351, epoch: 222, batch: 24, loss: 0.036295078694820404, acc: 98.4375, f1: 97.85575048732943, r: 0.645758514246157
06/02/2019 09:24:31 step: 7356, epoch: 222, batch: 29, loss: 0.017948685213923454, acc: 100.0, f1: 100.0, r: 0.7602929517650767
06/02/2019 09:24:31 *** evaluating ***
06/02/2019 09:24:31 step: 223, epoch: 222, acc: 60.256410256410255, f1: 24.98316660253944, r: 0.3617476779418262
06/02/2019 09:24:31 *** epoch: 224 ***
06/02/2019 09:24:31 *** training ***
06/02/2019 09:24:32 step: 7364, epoch: 223, batch: 4, loss: 0.008310988545417786, acc: 100.0, f1: 100.0, r: 0.8087097023582454
06/02/2019 09:24:32 step: 7369, epoch: 223, batch: 9, loss: 0.020731257274746895, acc: 100.0, f1: 100.0, r: 0.6846649484497649
06/02/2019 09:24:33 step: 7374, epoch: 223, batch: 14, loss: 0.006867647171020508, acc: 100.0, f1: 100.0, r: 0.6549600131664319
06/02/2019 09:24:33 step: 7379, epoch: 223, batch: 19, loss: 0.05470959097146988, acc: 98.4375, f1: 98.45067213488267, r: 0.6672014674034076
06/02/2019 09:24:33 step: 7384, epoch: 223, batch: 24, loss: 0.01488213986158371, acc: 100.0, f1: 100.0, r: 0.788637962969561
06/02/2019 09:24:34 step: 7389, epoch: 223, batch: 29, loss: 0.028567055240273476, acc: 98.4375, f1: 98.62318840579711, r: 0.8134677135843176
06/02/2019 09:24:34 *** evaluating ***
06/02/2019 09:24:34 step: 224, epoch: 223, acc: 59.82905982905983, f1: 25.032077728179235, r: 0.36833363097929206
06/02/2019 09:24:34 *** epoch: 225 ***
06/02/2019 09:24:34 *** training ***
06/02/2019 09:24:35 step: 7397, epoch: 224, batch: 4, loss: 0.013606719672679901, acc: 100.0, f1: 100.0, r: 0.5808097698680955
06/02/2019 09:24:35 step: 7402, epoch: 224, batch: 9, loss: 0.010977596044540405, acc: 100.0, f1: 100.0, r: 0.7927572423768285
06/02/2019 09:24:36 step: 7407, epoch: 224, batch: 14, loss: 0.03419220820069313, acc: 100.0, f1: 100.0, r: 0.7064595279341437
06/02/2019 09:24:36 step: 7412, epoch: 224, batch: 19, loss: 0.03624968230724335, acc: 98.4375, f1: 95.01133786848072, r: 0.6876230841977984
06/02/2019 09:24:37 step: 7417, epoch: 224, batch: 24, loss: 0.041285280138254166, acc: 98.4375, f1: 95.37037037037037, r: 0.7313033550010875
06/02/2019 09:24:37 step: 7422, epoch: 224, batch: 29, loss: 0.016147734597325325, acc: 98.4375, f1: 94.80519480519482, r: 0.7207634383035317
06/02/2019 09:24:37 *** evaluating ***
06/02/2019 09:24:37 step: 225, epoch: 224, acc: 59.82905982905983, f1: 24.79766841768297, r: 0.367099438065311
06/02/2019 09:24:37 *** epoch: 226 ***
06/02/2019 09:24:37 *** training ***
06/02/2019 09:24:38 step: 7430, epoch: 225, batch: 4, loss: 0.0031883493065834045, acc: 100.0, f1: 100.0, r: 0.6669951188350919
06/02/2019 09:24:38 step: 7435, epoch: 225, batch: 9, loss: 0.1050054207444191, acc: 98.4375, f1: 98.29313543599258, r: 0.6426344561051676
06/02/2019 09:24:39 step: 7440, epoch: 225, batch: 14, loss: 0.005220755934715271, acc: 100.0, f1: 100.0, r: 0.780326706780923
06/02/2019 09:24:39 step: 7445, epoch: 225, batch: 19, loss: 0.04816814139485359, acc: 96.875, f1: 96.70285135330727, r: 0.7364898498002523
06/02/2019 09:24:40 step: 7450, epoch: 225, batch: 24, loss: 0.034181542694568634, acc: 98.4375, f1: 99.22067268252665, r: 0.7856149834160675
06/02/2019 09:24:40 step: 7455, epoch: 225, batch: 29, loss: 0.05049946904182434, acc: 98.4375, f1: 99.15212461796312, r: 0.7279382437732775
06/02/2019 09:24:40 *** evaluating ***
06/02/2019 09:24:40 step: 226, epoch: 225, acc: 59.82905982905983, f1: 25.04528985507246, r: 0.3661924314486121
06/02/2019 09:24:40 *** epoch: 227 ***
06/02/2019 09:24:40 *** training ***
06/02/2019 09:24:41 step: 7463, epoch: 226, batch: 4, loss: 0.009468100033700466, acc: 100.0, f1: 100.0, r: 0.7692262337051293
06/02/2019 09:24:41 step: 7468, epoch: 226, batch: 9, loss: 0.013278279453516006, acc: 100.0, f1: 100.0, r: 0.6146278079804998
06/02/2019 09:24:42 step: 7473, epoch: 226, batch: 14, loss: 0.008242650888860226, acc: 100.0, f1: 100.0, r: 0.7754178513660244
06/02/2019 09:24:42 step: 7478, epoch: 226, batch: 19, loss: 0.04005246236920357, acc: 98.4375, f1: 99.13702623906705, r: 0.6170783335916393
06/02/2019 09:24:43 step: 7483, epoch: 226, batch: 24, loss: 0.03110792487859726, acc: 100.0, f1: 100.0, r: 0.7820745669024951
06/02/2019 09:24:43 step: 7488, epoch: 226, batch: 29, loss: 0.04599030688405037, acc: 98.4375, f1: 98.00936768149884, r: 0.8021820604953186
06/02/2019 09:24:43 *** evaluating ***
06/02/2019 09:24:43 step: 227, epoch: 226, acc: 59.401709401709404, f1: 24.050971673254285, r: 0.36885315670491214
06/02/2019 09:24:43 *** epoch: 228 ***
06/02/2019 09:24:43 *** training ***
06/02/2019 09:24:44 step: 7496, epoch: 227, batch: 4, loss: 0.01389976404607296, acc: 100.0, f1: 100.0, r: 0.8028202250319677
06/02/2019 09:24:44 step: 7501, epoch: 227, batch: 9, loss: 0.009093981236219406, acc: 100.0, f1: 100.0, r: 0.8585002986785191
06/02/2019 09:24:45 step: 7506, epoch: 227, batch: 14, loss: 0.010309243574738503, acc: 100.0, f1: 100.0, r: 0.8051729996749426
06/02/2019 09:24:45 step: 7511, epoch: 227, batch: 19, loss: 0.027704868465662003, acc: 98.4375, f1: 96.42857142857143, r: 0.7943013573245792
06/02/2019 09:24:46 step: 7516, epoch: 227, batch: 24, loss: 0.14130957424640656, acc: 95.3125, f1: 95.65482826352391, r: 0.6664885289934264
06/02/2019 09:24:46 step: 7521, epoch: 227, batch: 29, loss: 0.02226300723850727, acc: 100.0, f1: 100.0, r: 0.7182978713007487
06/02/2019 09:24:46 *** evaluating ***
06/02/2019 09:24:47 step: 228, epoch: 227, acc: 59.401709401709404, f1: 24.10008701357983, r: 0.3690017127722228
06/02/2019 09:24:47 *** epoch: 229 ***
06/02/2019 09:24:47 *** training ***
06/02/2019 09:24:47 step: 7529, epoch: 228, batch: 4, loss: 0.006713654845952988, acc: 100.0, f1: 100.0, r: 0.6902964063777034
06/02/2019 09:24:47 step: 7534, epoch: 228, batch: 9, loss: 0.04559755325317383, acc: 96.875, f1: 97.7356572258533, r: 0.6183035071837408
06/02/2019 09:24:48 step: 7539, epoch: 228, batch: 14, loss: 0.03535899892449379, acc: 98.4375, f1: 87.14285714285714, r: 0.7232233217210216
06/02/2019 09:24:48 step: 7544, epoch: 228, batch: 19, loss: 0.04123874008655548, acc: 96.875, f1: 91.47392290249432, r: 0.7079077839366724
06/02/2019 09:24:49 step: 7549, epoch: 228, batch: 24, loss: 0.04211119934916496, acc: 96.875, f1: 96.21323529411765, r: 0.7550929614618145
06/02/2019 09:24:49 step: 7554, epoch: 228, batch: 29, loss: 0.015477567911148071, acc: 100.0, f1: 100.0, r: 0.7911213234255388
06/02/2019 09:24:49 *** evaluating ***
06/02/2019 09:24:50 step: 229, epoch: 228, acc: 61.53846153846154, f1: 26.437343113228483, r: 0.3629255453418815
06/02/2019 09:24:50 *** epoch: 230 ***
06/02/2019 09:24:50 *** training ***
06/02/2019 09:24:50 step: 7562, epoch: 229, batch: 4, loss: 0.015518375672399998, acc: 100.0, f1: 100.0, r: 0.768159881919318
06/02/2019 09:24:50 step: 7567, epoch: 229, batch: 9, loss: 0.00912473350763321, acc: 100.0, f1: 100.0, r: 0.8189184938013483
06/02/2019 09:24:51 step: 7572, epoch: 229, batch: 14, loss: 0.0031159333884716034, acc: 100.0, f1: 100.0, r: 0.708773813339378
06/02/2019 09:24:51 step: 7577, epoch: 229, batch: 19, loss: 0.008096814155578613, acc: 100.0, f1: 100.0, r: 0.7306201219330722
06/02/2019 09:24:52 step: 7582, epoch: 229, batch: 24, loss: 0.010447055101394653, acc: 100.0, f1: 100.0, r: 0.7317232014386922
06/02/2019 09:24:52 step: 7587, epoch: 229, batch: 29, loss: 0.012714352458715439, acc: 100.0, f1: 100.0, r: 0.6731702768782059
06/02/2019 09:24:52 *** evaluating ***
06/02/2019 09:24:53 step: 230, epoch: 229, acc: 60.256410256410255, f1: 25.807258759932566, r: 0.3663644929828909
06/02/2019 09:24:53 *** epoch: 231 ***
06/02/2019 09:24:53 *** training ***
06/02/2019 09:24:53 step: 7595, epoch: 230, batch: 4, loss: 0.13066457211971283, acc: 95.3125, f1: 91.49206349206348, r: 0.6535471332605755
06/02/2019 09:24:54 step: 7600, epoch: 230, batch: 9, loss: 0.010511653497815132, acc: 100.0, f1: 100.0, r: 0.8347710098108683
06/02/2019 09:24:54 step: 7605, epoch: 230, batch: 14, loss: 0.04770266264677048, acc: 95.3125, f1: 92.65649920255183, r: 0.7844225341990372
06/02/2019 09:24:54 step: 7610, epoch: 230, batch: 19, loss: 0.02686230279505253, acc: 98.4375, f1: 99.1951219512195, r: 0.7207810154237839
06/02/2019 09:24:55 step: 7615, epoch: 230, batch: 24, loss: 0.026093441992998123, acc: 100.0, f1: 100.0, r: 0.8097887332244198
06/02/2019 09:24:55 step: 7620, epoch: 230, batch: 29, loss: 0.016075244173407555, acc: 100.0, f1: 100.0, r: 0.7260357375068488
06/02/2019 09:24:56 *** evaluating ***
06/02/2019 09:24:56 step: 231, epoch: 230, acc: 60.68376068376068, f1: 25.05410814890491, r: 0.3713125076222903
06/02/2019 09:24:56 *** epoch: 232 ***
06/02/2019 09:24:56 *** training ***
06/02/2019 09:24:56 step: 7628, epoch: 231, batch: 4, loss: 0.06208180636167526, acc: 96.875, f1: 97.24747474747475, r: 0.8054838460370565
06/02/2019 09:24:57 step: 7633, epoch: 231, batch: 9, loss: 0.006765551865100861, acc: 100.0, f1: 100.0, r: 0.7500487936042288
06/02/2019 09:24:57 step: 7638, epoch: 231, batch: 14, loss: 0.04307261109352112, acc: 98.4375, f1: 99.08733679807327, r: 0.6678814809172079
06/02/2019 09:24:58 step: 7643, epoch: 231, batch: 19, loss: 0.018825765699148178, acc: 100.0, f1: 100.0, r: 0.6766743703098655
06/02/2019 09:24:58 step: 7648, epoch: 231, batch: 24, loss: 0.010039729997515678, acc: 100.0, f1: 100.0, r: 0.8161872973685257
06/02/2019 09:24:58 step: 7653, epoch: 231, batch: 29, loss: 0.02395772933959961, acc: 100.0, f1: 100.0, r: 0.6776076441448268
06/02/2019 09:24:59 *** evaluating ***
06/02/2019 09:24:59 step: 232, epoch: 231, acc: 61.111111111111114, f1: 25.956468891251504, r: 0.3721287419528307
06/02/2019 09:24:59 *** epoch: 233 ***
06/02/2019 09:24:59 *** training ***
06/02/2019 09:24:59 step: 7661, epoch: 232, batch: 4, loss: 0.026791583746671677, acc: 98.4375, f1: 97.667638483965, r: 0.6953172555146983
06/02/2019 09:25:00 step: 7666, epoch: 232, batch: 9, loss: 0.02573593147099018, acc: 98.4375, f1: 98.32967032967034, r: 0.6822980029235125
06/02/2019 09:25:00 step: 7671, epoch: 232, batch: 14, loss: 0.010646909475326538, acc: 100.0, f1: 100.0, r: 0.6255696062538275
06/02/2019 09:25:01 step: 7676, epoch: 232, batch: 19, loss: 0.07150530070066452, acc: 98.4375, f1: 97.6023976023976, r: 0.7049734086008256
06/02/2019 09:25:01 step: 7681, epoch: 232, batch: 24, loss: 0.002751260995864868, acc: 100.0, f1: 100.0, r: 0.763233727390509
06/02/2019 09:25:02 step: 7686, epoch: 232, batch: 29, loss: 0.04462099447846413, acc: 98.4375, f1: 96.37188208616782, r: 0.680247073552717
06/02/2019 09:25:02 *** evaluating ***
06/02/2019 09:25:02 step: 233, epoch: 232, acc: 60.256410256410255, f1: 24.93305820351856, r: 0.3718388765198267
06/02/2019 09:25:02 *** epoch: 234 ***
06/02/2019 09:25:02 *** training ***
06/02/2019 09:25:02 step: 7694, epoch: 233, batch: 4, loss: 0.00541532039642334, acc: 100.0, f1: 100.0, r: 0.6484709954293345
06/02/2019 09:25:03 step: 7699, epoch: 233, batch: 9, loss: 0.030293434858322144, acc: 100.0, f1: 100.0, r: 0.8409118308283898
06/02/2019 09:25:03 step: 7704, epoch: 233, batch: 14, loss: 0.008883781731128693, acc: 100.0, f1: 100.0, r: 0.8121066907575346
06/02/2019 09:25:04 step: 7709, epoch: 233, batch: 19, loss: 0.016514981165528297, acc: 100.0, f1: 100.0, r: 0.7551915510265186
06/02/2019 09:25:04 step: 7714, epoch: 233, batch: 24, loss: 0.020314542576670647, acc: 100.0, f1: 100.0, r: 0.7625632665239304
06/02/2019 09:25:05 step: 7719, epoch: 233, batch: 29, loss: 0.00845874659717083, acc: 100.0, f1: 100.0, r: 0.7853608698340873
06/02/2019 09:25:05 *** evaluating ***
06/02/2019 09:25:05 step: 234, epoch: 233, acc: 59.82905982905983, f1: 24.79766841768297, r: 0.36867351509562535
06/02/2019 09:25:05 *** epoch: 235 ***
06/02/2019 09:25:05 *** training ***
06/02/2019 09:25:06 step: 7727, epoch: 234, batch: 4, loss: 0.02374918758869171, acc: 98.4375, f1: 98.67947178871549, r: 0.5640948730678851
06/02/2019 09:25:06 step: 7732, epoch: 234, batch: 9, loss: 0.009176034480333328, acc: 100.0, f1: 100.0, r: 0.7015233547503582
06/02/2019 09:25:06 step: 7737, epoch: 234, batch: 14, loss: 0.02009034901857376, acc: 98.4375, f1: 97.99498746867168, r: 0.7642649819038956
06/02/2019 09:25:07 step: 7742, epoch: 234, batch: 19, loss: 0.003015307243913412, acc: 100.0, f1: 100.0, r: 0.7550179562212518
06/02/2019 09:25:07 step: 7747, epoch: 234, batch: 24, loss: 0.007620006799697876, acc: 100.0, f1: 100.0, r: 0.7818296138325174
06/02/2019 09:25:08 step: 7752, epoch: 234, batch: 29, loss: 0.053940653800964355, acc: 96.875, f1: 97.11130829143252, r: 0.6816113176112736
06/02/2019 09:25:08 *** evaluating ***
06/02/2019 09:25:08 step: 235, epoch: 234, acc: 60.68376068376068, f1: 24.923497729385424, r: 0.36819073363674026
06/02/2019 09:25:08 *** epoch: 236 ***
06/02/2019 09:25:08 *** training ***
06/02/2019 09:25:09 step: 7760, epoch: 235, batch: 4, loss: 0.041789401322603226, acc: 98.4375, f1: 97.68964189449365, r: 0.6652442741316761
06/02/2019 09:25:09 step: 7765, epoch: 235, batch: 9, loss: 0.005830269306898117, acc: 100.0, f1: 100.0, r: 0.7210863613286204
06/02/2019 09:25:09 step: 7770, epoch: 235, batch: 14, loss: 0.017115622758865356, acc: 100.0, f1: 100.0, r: 0.7697051112249553
06/02/2019 09:25:10 step: 7775, epoch: 235, batch: 19, loss: 0.02169708162546158, acc: 100.0, f1: 100.0, r: 0.7172019664894057
06/02/2019 09:25:10 step: 7780, epoch: 235, batch: 24, loss: 0.008328262716531754, acc: 100.0, f1: 100.0, r: 0.8075438384727099
06/02/2019 09:25:11 step: 7785, epoch: 235, batch: 29, loss: 0.0019902102649211884, acc: 100.0, f1: 100.0, r: 0.7110808918327546
06/02/2019 09:25:11 *** evaluating ***
06/02/2019 09:25:11 step: 236, epoch: 235, acc: 61.111111111111114, f1: 25.11909139057774, r: 0.3686972499767397
06/02/2019 09:25:11 *** epoch: 237 ***
06/02/2019 09:25:11 *** training ***
06/02/2019 09:25:12 step: 7793, epoch: 236, batch: 4, loss: 0.02336922660470009, acc: 98.4375, f1: 98.75607385811468, r: 0.6927804084663675
06/02/2019 09:25:12 step: 7798, epoch: 236, batch: 9, loss: 0.010949034243822098, acc: 100.0, f1: 100.0, r: 0.7895070995053497
06/02/2019 09:25:13 step: 7803, epoch: 236, batch: 14, loss: 0.006532927975058556, acc: 100.0, f1: 100.0, r: 0.5821882837338637
06/02/2019 09:25:13 step: 7808, epoch: 236, batch: 19, loss: 0.01764427125453949, acc: 100.0, f1: 100.0, r: 0.8097963653344866
06/02/2019 09:25:13 step: 7813, epoch: 236, batch: 24, loss: 0.033448874950408936, acc: 98.4375, f1: 98.44155844155846, r: 0.7613289383392334
06/02/2019 09:25:14 step: 7818, epoch: 236, batch: 29, loss: 0.1598917841911316, acc: 96.875, f1: 97.23981546641646, r: 0.6408295809237312
06/02/2019 09:25:14 *** evaluating ***
06/02/2019 09:25:14 step: 237, epoch: 236, acc: 60.256410256410255, f1: 24.84856204710145, r: 0.36783110948816494
06/02/2019 09:25:14 *** epoch: 238 ***
06/02/2019 09:25:14 *** training ***
06/02/2019 09:25:15 step: 7826, epoch: 237, batch: 4, loss: 0.062058936804533005, acc: 95.3125, f1: 84.37778730703259, r: 0.8012275466660987
06/02/2019 09:25:15 step: 7831, epoch: 237, batch: 9, loss: 0.013417611829936504, acc: 100.0, f1: 100.0, r: 0.689473123070676
06/02/2019 09:25:16 step: 7836, epoch: 237, batch: 14, loss: 0.03509029000997543, acc: 98.4375, f1: 99.42438513867086, r: 0.7282954335563115
06/02/2019 09:25:16 step: 7841, epoch: 237, batch: 19, loss: 0.066281259059906, acc: 98.4375, f1: 98.16207184628237, r: 0.7178535186205416
06/02/2019 09:25:16 step: 7846, epoch: 237, batch: 24, loss: 0.058997273445129395, acc: 96.875, f1: 94.77139952861276, r: 0.6555328848387052
06/02/2019 09:25:17 step: 7851, epoch: 237, batch: 29, loss: 0.011739371344447136, acc: 100.0, f1: 100.0, r: 0.7143368527705252
06/02/2019 09:25:17 *** evaluating ***
06/02/2019 09:25:17 step: 238, epoch: 237, acc: 60.256410256410255, f1: 24.93291006766827, r: 0.36436120960761775
06/02/2019 09:25:17 *** epoch: 239 ***
06/02/2019 09:25:17 *** training ***
06/02/2019 09:25:18 step: 7859, epoch: 238, batch: 4, loss: 0.05343364179134369, acc: 98.4375, f1: 99.17935428139509, r: 0.6866524538836476
06/02/2019 09:25:18 step: 7864, epoch: 238, batch: 9, loss: 0.011634405702352524, acc: 100.0, f1: 100.0, r: 0.7267179249739205
06/02/2019 09:25:19 step: 7869, epoch: 238, batch: 14, loss: 0.09252560138702393, acc: 95.3125, f1: 96.62581699346406, r: 0.7956354976992046
06/02/2019 09:25:19 step: 7874, epoch: 238, batch: 19, loss: 0.0017958879470825195, acc: 100.0, f1: 100.0, r: 0.7234260991568124
06/02/2019 09:25:19 step: 7879, epoch: 238, batch: 24, loss: 0.04073004797101021, acc: 98.4375, f1: 99.12457912457913, r: 0.574724857797388
06/02/2019 09:25:20 step: 7884, epoch: 238, batch: 29, loss: 0.024074075743556023, acc: 100.0, f1: 100.0, r: 0.8559475707496631
06/02/2019 09:25:20 *** evaluating ***
06/02/2019 09:25:20 step: 239, epoch: 238, acc: 60.68376068376068, f1: 25.85273529841905, r: 0.36478712300826094
06/02/2019 09:25:20 *** epoch: 240 ***
06/02/2019 09:25:20 *** training ***
06/02/2019 09:25:21 step: 7892, epoch: 239, batch: 4, loss: 0.004594985395669937, acc: 100.0, f1: 100.0, r: 0.7664608107541423
06/02/2019 09:25:21 step: 7897, epoch: 239, batch: 9, loss: 0.00756565947085619, acc: 100.0, f1: 100.0, r: 0.8088657324387146
06/02/2019 09:25:22 step: 7902, epoch: 239, batch: 14, loss: 0.024319525808095932, acc: 98.4375, f1: 98.92156862745098, r: 0.7968445289258285
06/02/2019 09:25:22 step: 7907, epoch: 239, batch: 19, loss: 0.024043286219239235, acc: 98.4375, f1: 98.1111111111111, r: 0.7411631854767237
06/02/2019 09:25:23 step: 7912, epoch: 239, batch: 24, loss: 0.014483558014035225, acc: 100.0, f1: 100.0, r: 0.8477499703682996
06/02/2019 09:25:23 step: 7917, epoch: 239, batch: 29, loss: 0.012697255238890648, acc: 100.0, f1: 100.0, r: 0.7911239984132606
06/02/2019 09:25:23 *** evaluating ***
06/02/2019 09:25:23 step: 240, epoch: 239, acc: 59.82905982905983, f1: 24.79766841768297, r: 0.3688050907673408
06/02/2019 09:25:23 *** epoch: 241 ***
06/02/2019 09:25:23 *** training ***
06/02/2019 09:25:24 step: 7925, epoch: 240, batch: 4, loss: 0.02910025045275688, acc: 98.4375, f1: 99.35215946843854, r: 0.7697569942824125
06/02/2019 09:25:24 step: 7930, epoch: 240, batch: 9, loss: 0.014759736135601997, acc: 100.0, f1: 100.0, r: 0.783965783915885
06/02/2019 09:25:25 step: 7935, epoch: 240, batch: 14, loss: 0.11032374203205109, acc: 95.3125, f1: 94.00100071926697, r: 0.5919650861090124
06/02/2019 09:25:25 step: 7940, epoch: 240, batch: 19, loss: 0.012484695762395859, acc: 100.0, f1: 100.0, r: 0.7124932879412798
06/02/2019 09:25:26 step: 7945, epoch: 240, batch: 24, loss: 0.049897219985723495, acc: 96.875, f1: 93.20829546393456, r: 0.7124636514653563
06/02/2019 09:25:26 step: 7950, epoch: 240, batch: 29, loss: 0.011376742273569107, acc: 100.0, f1: 100.0, r: 0.7322125857215207
06/02/2019 09:25:26 *** evaluating ***
06/02/2019 09:25:26 step: 241, epoch: 240, acc: 59.82905982905983, f1: 24.566434319911597, r: 0.36589661959536274
06/02/2019 09:25:26 *** epoch: 242 ***
06/02/2019 09:25:26 *** training ***
06/02/2019 09:25:27 step: 7958, epoch: 241, batch: 4, loss: 0.0044172052294015884, acc: 100.0, f1: 100.0, r: 0.7778911526563803
06/02/2019 09:25:27 step: 7963, epoch: 241, batch: 9, loss: 0.027537131682038307, acc: 100.0, f1: 100.0, r: 0.8026997146557298
06/02/2019 09:25:28 step: 7968, epoch: 241, batch: 14, loss: 0.003467787057161331, acc: 100.0, f1: 100.0, r: 0.6061134165598251
06/02/2019 09:25:28 step: 7973, epoch: 241, batch: 19, loss: 0.05213798210024834, acc: 96.875, f1: 96.05640559511185, r: 0.7490888508894145
06/02/2019 09:25:29 step: 7978, epoch: 241, batch: 24, loss: 0.008778072893619537, acc: 100.0, f1: 100.0, r: 0.7193725009111862
06/02/2019 09:25:29 step: 7983, epoch: 241, batch: 29, loss: 0.0822630226612091, acc: 98.4375, f1: 99.15307012081206, r: 0.6841286190519178
06/02/2019 09:25:29 *** evaluating ***
06/02/2019 09:25:29 step: 242, epoch: 241, acc: 60.256410256410255, f1: 25.70487899065988, r: 0.36690071675126934
06/02/2019 09:25:29 *** epoch: 243 ***
06/02/2019 09:25:29 *** training ***
06/02/2019 09:25:30 step: 7991, epoch: 242, batch: 4, loss: 0.0030994676053524017, acc: 100.0, f1: 100.0, r: 0.7016822882556037
06/02/2019 09:25:30 step: 7996, epoch: 242, batch: 9, loss: 0.00519086979329586, acc: 100.0, f1: 100.0, r: 0.5938500232131171
06/02/2019 09:25:31 step: 8001, epoch: 242, batch: 14, loss: 0.003964148461818695, acc: 100.0, f1: 100.0, r: 0.6810528432566493
06/02/2019 09:25:31 step: 8006, epoch: 242, batch: 19, loss: 0.16877615451812744, acc: 93.75, f1: 88.82554945054946, r: 0.7613228756315299
06/02/2019 09:25:32 step: 8011, epoch: 242, batch: 24, loss: 0.018234018236398697, acc: 100.0, f1: 100.0, r: 0.7204755102028357
06/02/2019 09:25:32 step: 8016, epoch: 242, batch: 29, loss: 0.00802769884467125, acc: 100.0, f1: 100.0, r: 0.7203544476034398
06/02/2019 09:25:32 *** evaluating ***
06/02/2019 09:25:33 step: 243, epoch: 242, acc: 60.68376068376068, f1: 25.89550626823821, r: 0.3683361383232291
06/02/2019 09:25:33 *** epoch: 244 ***
06/02/2019 09:25:33 *** training ***
06/02/2019 09:25:33 step: 8024, epoch: 243, batch: 4, loss: 0.017900995910167694, acc: 98.4375, f1: 98.7878787878788, r: 0.7549979486741402
06/02/2019 09:25:33 step: 8029, epoch: 243, batch: 9, loss: 0.04357669875025749, acc: 98.4375, f1: 95.40229885057472, r: 0.7983675234775113
06/02/2019 09:25:34 step: 8034, epoch: 243, batch: 14, loss: 0.0070558227598667145, acc: 100.0, f1: 100.0, r: 0.8001858469099398
06/02/2019 09:25:34 step: 8039, epoch: 243, batch: 19, loss: 0.0049939267337322235, acc: 100.0, f1: 100.0, r: 0.7345929201976613
06/02/2019 09:25:35 step: 8044, epoch: 243, batch: 24, loss: 0.01645647920668125, acc: 100.0, f1: 100.0, r: 0.7928295282483859
06/02/2019 09:25:35 step: 8049, epoch: 243, batch: 29, loss: 0.056823670864105225, acc: 96.875, f1: 97.3138646756654, r: 0.5541563983986463
06/02/2019 09:25:35 *** evaluating ***
06/02/2019 09:25:36 step: 244, epoch: 243, acc: 61.111111111111114, f1: 26.04475560717629, r: 0.36661826157773136
06/02/2019 09:25:36 *** epoch: 245 ***
06/02/2019 09:25:36 *** training ***
06/02/2019 09:25:36 step: 8057, epoch: 244, batch: 4, loss: 0.004304917529225349, acc: 100.0, f1: 100.0, r: 0.7029574049909777
06/02/2019 09:25:37 step: 8062, epoch: 244, batch: 9, loss: 0.0063090138137340546, acc: 100.0, f1: 100.0, r: 0.7721161413292389
06/02/2019 09:25:37 step: 8067, epoch: 244, batch: 14, loss: 0.018763229250907898, acc: 100.0, f1: 100.0, r: 0.6028427749825748
06/02/2019 09:25:37 step: 8072, epoch: 244, batch: 19, loss: 0.012129842303693295, acc: 100.0, f1: 100.0, r: 0.7365297360794942
06/02/2019 09:25:38 step: 8077, epoch: 244, batch: 24, loss: 0.02386634610593319, acc: 100.0, f1: 100.0, r: 0.7699008959865847
06/02/2019 09:25:38 step: 8082, epoch: 244, batch: 29, loss: 0.0749344676733017, acc: 98.4375, f1: 97.60239760239759, r: 0.6418261977738096
06/02/2019 09:25:39 *** evaluating ***
06/02/2019 09:25:39 step: 245, epoch: 244, acc: 60.256410256410255, f1: 24.909124062605322, r: 0.37220483388184084
06/02/2019 09:25:39 *** epoch: 246 ***
06/02/2019 09:25:39 *** training ***
06/02/2019 09:25:39 step: 8090, epoch: 245, batch: 4, loss: 0.007288377732038498, acc: 100.0, f1: 100.0, r: 0.7156925288793443
06/02/2019 09:25:40 step: 8095, epoch: 245, batch: 9, loss: 0.01711481437087059, acc: 98.4375, f1: 95.17543859649122, r: 0.7347515044394671
06/02/2019 09:25:40 step: 8100, epoch: 245, batch: 14, loss: 0.002244282513856888, acc: 100.0, f1: 100.0, r: 0.8162632527589394
06/02/2019 09:25:41 step: 8105, epoch: 245, batch: 19, loss: 0.005224084481596947, acc: 100.0, f1: 100.0, r: 0.7028303164715767
06/02/2019 09:25:41 step: 8110, epoch: 245, batch: 24, loss: 0.05581489950418472, acc: 98.4375, f1: 98.95657511124752, r: 0.6900067984747591
06/02/2019 09:25:41 step: 8115, epoch: 245, batch: 29, loss: 0.02798185683786869, acc: 98.4375, f1: 98.38383838383838, r: 0.7171367864857965
06/02/2019 09:25:42 *** evaluating ***
06/02/2019 09:25:42 step: 246, epoch: 245, acc: 60.68376068376068, f1: 24.939531923227577, r: 0.3683174070379559
06/02/2019 09:25:42 *** epoch: 247 ***
06/02/2019 09:25:42 *** training ***
06/02/2019 09:25:42 step: 8123, epoch: 246, batch: 4, loss: 0.07715412974357605, acc: 98.4375, f1: 98.93939393939395, r: 0.7456381775472832
06/02/2019 09:25:43 step: 8128, epoch: 246, batch: 9, loss: 0.0061461590230464935, acc: 100.0, f1: 100.0, r: 0.7568855024144194
06/02/2019 09:25:43 step: 8133, epoch: 246, batch: 14, loss: 0.12005528807640076, acc: 98.4375, f1: 97.79158040027606, r: 0.6727606272239397
06/02/2019 09:25:44 step: 8138, epoch: 246, batch: 19, loss: 0.10291323065757751, acc: 96.875, f1: 95.3912753912754, r: 0.677836712812723
06/02/2019 09:25:44 step: 8143, epoch: 246, batch: 24, loss: 0.0284639373421669, acc: 98.4375, f1: 98.36363636363636, r: 0.7027336062294525
06/02/2019 09:25:44 step: 8148, epoch: 246, batch: 29, loss: 0.041987109929323196, acc: 98.4375, f1: 98.44322344322345, r: 0.8056926742603707
06/02/2019 09:25:45 *** evaluating ***
06/02/2019 09:25:45 step: 247, epoch: 246, acc: 60.68376068376068, f1: 24.886378485069393, r: 0.3663691700591641
06/02/2019 09:25:45 *** epoch: 248 ***
06/02/2019 09:25:45 *** training ***
06/02/2019 09:25:45 step: 8156, epoch: 247, batch: 4, loss: 0.008017566055059433, acc: 100.0, f1: 100.0, r: 0.6863600122792055
06/02/2019 09:25:46 step: 8161, epoch: 247, batch: 9, loss: 0.010078977793455124, acc: 100.0, f1: 100.0, r: 0.7227590787698283
06/02/2019 09:25:46 step: 8166, epoch: 247, batch: 14, loss: 0.04782889783382416, acc: 98.4375, f1: 98.88591800356507, r: 0.7604316869816473
06/02/2019 09:25:47 step: 8171, epoch: 247, batch: 19, loss: 0.011909952387213707, acc: 100.0, f1: 100.0, r: 0.7615818411538893
06/02/2019 09:25:47 step: 8176, epoch: 247, batch: 24, loss: 0.01876262202858925, acc: 100.0, f1: 100.0, r: 0.7953111340155875
06/02/2019 09:25:48 step: 8181, epoch: 247, batch: 29, loss: 0.029062187299132347, acc: 98.4375, f1: 98.97400820793433, r: 0.7773839573398091
06/02/2019 09:25:48 *** evaluating ***
06/02/2019 09:25:48 step: 248, epoch: 247, acc: 61.111111111111114, f1: 25.086892706413572, r: 0.3645065409316431
06/02/2019 09:25:48 *** epoch: 249 ***
06/02/2019 09:25:48 *** training ***
06/02/2019 09:25:48 step: 8189, epoch: 248, batch: 4, loss: 0.009365338832139969, acc: 100.0, f1: 100.0, r: 0.7758267354717959
06/02/2019 09:25:49 step: 8194, epoch: 248, batch: 9, loss: 0.01057828962802887, acc: 100.0, f1: 100.0, r: 0.6774896655376221
06/02/2019 09:25:49 step: 8199, epoch: 248, batch: 14, loss: 0.052007485181093216, acc: 98.4375, f1: 97.80801209372639, r: 0.6544177744938771
06/02/2019 09:25:50 step: 8204, epoch: 248, batch: 19, loss: 0.004544448107481003, acc: 100.0, f1: 100.0, r: 0.7086394235039652
06/02/2019 09:25:50 step: 8209, epoch: 248, batch: 24, loss: 0.002020474523305893, acc: 100.0, f1: 100.0, r: 0.66438248194383
06/02/2019 09:25:51 step: 8214, epoch: 248, batch: 29, loss: 0.016873642802238464, acc: 98.4375, f1: 98.13258636788049, r: 0.729328481099419
06/02/2019 09:25:51 *** evaluating ***
06/02/2019 09:25:51 step: 249, epoch: 248, acc: 60.68376068376068, f1: 24.17048884307633, r: 0.368458992745817
06/02/2019 09:25:51 *** epoch: 250 ***
06/02/2019 09:25:51 *** training ***
06/02/2019 09:25:51 step: 8222, epoch: 249, batch: 4, loss: 0.012743931263685226, acc: 100.0, f1: 100.0, r: 0.704923921450454
06/02/2019 09:25:52 step: 8227, epoch: 249, batch: 9, loss: 0.021594058722257614, acc: 100.0, f1: 100.0, r: 0.6673360362741875
06/02/2019 09:25:52 step: 8232, epoch: 249, batch: 14, loss: 0.05318395793437958, acc: 96.875, f1: 97.94708994708995, r: 0.6918701556083284
06/02/2019 09:25:53 step: 8237, epoch: 249, batch: 19, loss: 0.00845138169825077, acc: 100.0, f1: 100.0, r: 0.7715191963165122
06/02/2019 09:25:53 step: 8242, epoch: 249, batch: 24, loss: 0.1253150999546051, acc: 98.4375, f1: 98.26839826839827, r: 0.6404597051432155
06/02/2019 09:25:54 step: 8247, epoch: 249, batch: 29, loss: 0.004951769486069679, acc: 100.0, f1: 100.0, r: 0.7506872762932102
06/02/2019 09:25:54 *** evaluating ***
06/02/2019 09:25:54 step: 250, epoch: 249, acc: 60.256410256410255, f1: 25.83139233954451, r: 0.3675134938188485
06/02/2019 09:25:54 *** epoch: 251 ***
06/02/2019 09:25:54 *** training ***
06/02/2019 09:25:54 step: 8255, epoch: 250, batch: 4, loss: 0.011054675094783306, acc: 100.0, f1: 100.0, r: 0.8235028489206638
06/02/2019 09:25:55 step: 8260, epoch: 250, batch: 9, loss: 0.01514742523431778, acc: 100.0, f1: 100.0, r: 0.7624874617698706
06/02/2019 09:25:55 step: 8265, epoch: 250, batch: 14, loss: 0.02050112560391426, acc: 100.0, f1: 100.0, r: 0.700965266866911
06/02/2019 09:25:56 step: 8270, epoch: 250, batch: 19, loss: 0.013745453208684921, acc: 100.0, f1: 100.0, r: 0.7167065346614613
06/02/2019 09:25:56 step: 8275, epoch: 250, batch: 24, loss: 0.00325886532664299, acc: 100.0, f1: 100.0, r: 0.6896532074908402
06/02/2019 09:25:57 step: 8280, epoch: 250, batch: 29, loss: 0.023805467411875725, acc: 98.4375, f1: 98.80952380952381, r: 0.8001083012143848
06/02/2019 09:25:57 *** evaluating ***
06/02/2019 09:25:57 step: 251, epoch: 250, acc: 61.111111111111114, f1: 26.01699987060041, r: 0.36908072344392245
06/02/2019 09:25:57 *** epoch: 252 ***
06/02/2019 09:25:57 *** training ***
06/02/2019 09:25:58 step: 8288, epoch: 251, batch: 4, loss: 0.020990505814552307, acc: 98.4375, f1: 96.8733153638814, r: 0.6842493052100098
06/02/2019 09:25:58 step: 8293, epoch: 251, batch: 9, loss: 0.049472417682409286, acc: 96.875, f1: 82.29423024336887, r: 0.6080971646386082
06/02/2019 09:25:58 step: 8298, epoch: 251, batch: 14, loss: 0.006515424698591232, acc: 100.0, f1: 100.0, r: 0.7699971562352048
06/02/2019 09:25:59 step: 8303, epoch: 251, batch: 19, loss: 0.019430484622716904, acc: 100.0, f1: 100.0, r: 0.6121004292585918
06/02/2019 09:25:59 step: 8308, epoch: 251, batch: 24, loss: 0.03112523816525936, acc: 98.4375, f1: 98.30623306233062, r: 0.760014184925122
06/02/2019 09:26:00 step: 8313, epoch: 251, batch: 29, loss: 0.008602420799434185, acc: 100.0, f1: 100.0, r: 0.7193496404849
06/02/2019 09:26:00 *** evaluating ***
06/02/2019 09:26:00 step: 252, epoch: 251, acc: 61.53846153846154, f1: 26.120508088046684, r: 0.36623976094820687
06/02/2019 09:26:00 *** epoch: 253 ***
06/02/2019 09:26:00 *** training ***
06/02/2019 09:26:01 step: 8321, epoch: 252, batch: 4, loss: 0.022150173783302307, acc: 98.4375, f1: 96.83890577507597, r: 0.6605046221122649
06/02/2019 09:26:01 step: 8326, epoch: 252, batch: 9, loss: 0.010837037116289139, acc: 100.0, f1: 100.0, r: 0.6963947578037382
06/02/2019 09:26:01 step: 8331, epoch: 252, batch: 14, loss: 0.012560777366161346, acc: 100.0, f1: 100.0, r: 0.7277259836238489
06/02/2019 09:26:02 step: 8336, epoch: 252, batch: 19, loss: 0.008172210305929184, acc: 100.0, f1: 100.0, r: 0.7088922717794668
06/02/2019 09:26:02 step: 8341, epoch: 252, batch: 24, loss: 0.008115336298942566, acc: 100.0, f1: 100.0, r: 0.7667756031682229
06/02/2019 09:26:03 step: 8346, epoch: 252, batch: 29, loss: 0.006878800690174103, acc: 100.0, f1: 100.0, r: 0.7511791807198026
06/02/2019 09:26:03 *** evaluating ***
06/02/2019 09:26:03 step: 253, epoch: 252, acc: 61.53846153846154, f1: 26.069472531176274, r: 0.3673177659061339
06/02/2019 09:26:03 *** epoch: 254 ***
06/02/2019 09:26:03 *** training ***
06/02/2019 09:26:04 step: 8354, epoch: 253, batch: 4, loss: 0.007677119225263596, acc: 100.0, f1: 100.0, r: 0.8036897730479965
06/02/2019 09:26:04 step: 8359, epoch: 253, batch: 9, loss: 0.007796816527843475, acc: 100.0, f1: 100.0, r: 0.7306773607568313
06/02/2019 09:26:05 step: 8364, epoch: 253, batch: 14, loss: 0.028429238125681877, acc: 98.4375, f1: 99.30300807043287, r: 0.7874108602563616
06/02/2019 09:26:05 step: 8369, epoch: 253, batch: 19, loss: 0.045844461768865585, acc: 98.4375, f1: 98.65807836822329, r: 0.6008996382664068
06/02/2019 09:26:05 step: 8374, epoch: 253, batch: 24, loss: 0.01850832626223564, acc: 100.0, f1: 100.0, r: 0.8125210466585266
06/02/2019 09:26:06 step: 8379, epoch: 253, batch: 29, loss: 0.014318656176328659, acc: 100.0, f1: 100.0, r: 0.8272464382843487
06/02/2019 09:26:06 *** evaluating ***
06/02/2019 09:26:06 step: 254, epoch: 253, acc: 61.111111111111114, f1: 25.921555884809464, r: 0.3602407222324679
06/02/2019 09:26:06 *** epoch: 255 ***
06/02/2019 09:26:06 *** training ***
06/02/2019 09:26:07 step: 8387, epoch: 254, batch: 4, loss: 0.08556628972291946, acc: 98.4375, f1: 99.15966386554622, r: 0.7541029042113321
06/02/2019 09:26:07 step: 8392, epoch: 254, batch: 9, loss: 0.04971325024962425, acc: 96.875, f1: 97.45208098941269, r: 0.8247138181783177
06/02/2019 09:26:08 step: 8397, epoch: 254, batch: 14, loss: 0.012821033596992493, acc: 100.0, f1: 100.0, r: 0.7127011657853616
06/02/2019 09:26:08 step: 8402, epoch: 254, batch: 19, loss: 0.009532835334539413, acc: 100.0, f1: 100.0, r: 0.7906460641798615
06/02/2019 09:26:09 step: 8407, epoch: 254, batch: 24, loss: 0.05318712443113327, acc: 98.4375, f1: 99.33081674673987, r: 0.7905638253244196
06/02/2019 09:26:09 step: 8412, epoch: 254, batch: 29, loss: 0.00328979454934597, acc: 100.0, f1: 100.0, r: 0.666435712466441
06/02/2019 09:26:09 *** evaluating ***
06/02/2019 09:26:09 step: 255, epoch: 254, acc: 61.111111111111114, f1: 24.47523491001752, r: 0.36131525184979235
06/02/2019 09:26:09 *** epoch: 256 ***
06/02/2019 09:26:09 *** training ***
06/02/2019 09:26:10 step: 8420, epoch: 255, batch: 4, loss: 0.031197167932987213, acc: 98.4375, f1: 99.12462006079028, r: 0.6873620385605116
06/02/2019 09:26:10 step: 8425, epoch: 255, batch: 9, loss: 0.03274686262011528, acc: 98.4375, f1: 99.16216216216216, r: 0.8156526156229547
06/02/2019 09:26:11 step: 8430, epoch: 255, batch: 14, loss: 0.09414896368980408, acc: 95.3125, f1: 96.97596153846153, r: 0.7470434359886768
06/02/2019 09:26:11 step: 8435, epoch: 255, batch: 19, loss: 0.016417179256677628, acc: 100.0, f1: 100.0, r: 0.6838469123389115
06/02/2019 09:26:12 step: 8440, epoch: 255, batch: 24, loss: 0.034881334751844406, acc: 98.4375, f1: 99.27272727272727, r: 0.8122400170023405
06/02/2019 09:26:12 step: 8445, epoch: 255, batch: 29, loss: 0.02196349762380123, acc: 100.0, f1: 100.0, r: 0.807227590019571
06/02/2019 09:26:12 *** evaluating ***
06/02/2019 09:26:12 step: 256, epoch: 255, acc: 61.111111111111114, f1: 24.377507116977227, r: 0.36119539884954244
06/02/2019 09:26:12 *** epoch: 257 ***
06/02/2019 09:26:12 *** training ***
06/02/2019 09:26:13 step: 8453, epoch: 256, batch: 4, loss: 0.0018063168972730637, acc: 100.0, f1: 100.0, r: 0.724313738799158
06/02/2019 09:26:13 step: 8458, epoch: 256, batch: 9, loss: 0.01362956315279007, acc: 100.0, f1: 100.0, r: 0.8382594632424238
06/02/2019 09:26:14 step: 8463, epoch: 256, batch: 14, loss: 0.024493463337421417, acc: 98.4375, f1: 98.33333333333334, r: 0.7229740084662848
06/02/2019 09:26:14 step: 8468, epoch: 256, batch: 19, loss: 0.05136565864086151, acc: 98.4375, f1: 98.86811867604185, r: 0.6734988444664862
06/02/2019 09:26:15 step: 8473, epoch: 256, batch: 24, loss: 0.03332824259996414, acc: 98.4375, f1: 87.1951219512195, r: 0.7035257992199735
06/02/2019 09:26:15 step: 8478, epoch: 256, batch: 29, loss: 0.02818436175584793, acc: 100.0, f1: 100.0, r: 0.7135600428483984
06/02/2019 09:26:15 *** evaluating ***
06/02/2019 09:26:15 step: 257, epoch: 256, acc: 60.68376068376068, f1: 24.9833844889673, r: 0.3688402763639211
06/02/2019 09:26:15 *** epoch: 258 ***
06/02/2019 09:26:15 *** training ***
06/02/2019 09:26:16 step: 8486, epoch: 257, batch: 4, loss: 0.011166846379637718, acc: 100.0, f1: 100.0, r: 0.8014571921761068
06/02/2019 09:26:16 step: 8491, epoch: 257, batch: 9, loss: 0.05104835331439972, acc: 96.875, f1: 93.937575030012, r: 0.7031513564266463
06/02/2019 09:26:17 step: 8496, epoch: 257, batch: 14, loss: 0.06430166214704514, acc: 98.4375, f1: 98.8422035480859, r: 0.7036377544747876
06/02/2019 09:26:17 step: 8501, epoch: 257, batch: 19, loss: 0.020543532446026802, acc: 98.4375, f1: 98.85714285714286, r: 0.6698055348229155
06/02/2019 09:26:18 step: 8506, epoch: 257, batch: 24, loss: 0.011113645508885384, acc: 100.0, f1: 100.0, r: 0.7564164228384265
06/02/2019 09:26:18 step: 8511, epoch: 257, batch: 29, loss: 0.02124090865254402, acc: 100.0, f1: 100.0, r: 0.6904438387871475
06/02/2019 09:26:18 *** evaluating ***
06/02/2019 09:26:19 step: 258, epoch: 257, acc: 60.68376068376068, f1: 25.981258449957107, r: 0.3734138888504768
06/02/2019 09:26:19 *** epoch: 259 ***
06/02/2019 09:26:19 *** training ***
06/02/2019 09:26:19 step: 8519, epoch: 258, batch: 4, loss: 0.07075135409832001, acc: 98.4375, f1: 98.06426635694928, r: 0.7278402068515102
06/02/2019 09:26:19 step: 8524, epoch: 258, batch: 9, loss: 0.007348311133682728, acc: 100.0, f1: 100.0, r: 0.7234088565819453
06/02/2019 09:26:20 step: 8529, epoch: 258, batch: 14, loss: 0.014549286104738712, acc: 100.0, f1: 100.0, r: 0.6863171223406708
06/02/2019 09:26:20 step: 8534, epoch: 258, batch: 19, loss: 0.010741882026195526, acc: 100.0, f1: 100.0, r: 0.7009511336575986
06/02/2019 09:26:21 step: 8539, epoch: 258, batch: 24, loss: 0.007859334349632263, acc: 100.0, f1: 100.0, r: 0.6927355308684404
06/02/2019 09:26:21 step: 8544, epoch: 258, batch: 29, loss: 0.06332913786172867, acc: 95.3125, f1: 82.79656862745098, r: 0.7761187749464903
06/02/2019 09:26:21 *** evaluating ***
06/02/2019 09:26:22 step: 259, epoch: 258, acc: 61.53846153846154, f1: 25.936471633873815, r: 0.3710546817451851
06/02/2019 09:26:22 *** epoch: 260 ***
06/02/2019 09:26:22 *** training ***
06/02/2019 09:26:22 step: 8552, epoch: 259, batch: 4, loss: 0.01575750671327114, acc: 100.0, f1: 100.0, r: 0.6993214095024473
06/02/2019 09:26:23 step: 8557, epoch: 259, batch: 9, loss: 0.009213495999574661, acc: 100.0, f1: 100.0, r: 0.7794804325985032
06/02/2019 09:26:23 step: 8562, epoch: 259, batch: 14, loss: 0.020862920209765434, acc: 100.0, f1: 100.0, r: 0.7437832112883203
06/02/2019 09:26:23 step: 8567, epoch: 259, batch: 19, loss: 0.004618465900421143, acc: 100.0, f1: 100.0, r: 0.7019993702176635
06/02/2019 09:26:24 step: 8572, epoch: 259, batch: 24, loss: 0.006191409192979336, acc: 100.0, f1: 100.0, r: 0.756483631529866
06/02/2019 09:26:24 step: 8577, epoch: 259, batch: 29, loss: 0.007836561650037766, acc: 100.0, f1: 100.0, r: 0.7091718871378319
06/02/2019 09:26:24 *** evaluating ***
06/02/2019 09:26:25 step: 260, epoch: 259, acc: 60.256410256410255, f1: 23.030010530010532, r: 0.3671673661727154
06/02/2019 09:26:25 *** epoch: 261 ***
06/02/2019 09:26:25 *** training ***
06/02/2019 09:26:25 step: 8585, epoch: 260, batch: 4, loss: 0.027938414365053177, acc: 98.4375, f1: 98.36363636363636, r: 0.7041775365268919
06/02/2019 09:26:26 step: 8590, epoch: 260, batch: 9, loss: 0.033516138792037964, acc: 98.4375, f1: 98.63945578231294, r: 0.6791463500939027
06/02/2019 09:26:26 step: 8595, epoch: 260, batch: 14, loss: 0.029278025031089783, acc: 98.4375, f1: 98.88888888888889, r: 0.7588915550904279
06/02/2019 09:26:26 step: 8600, epoch: 260, batch: 19, loss: 0.11288191378116608, acc: 96.875, f1: 94.30107526881721, r: 0.6912271342395021
06/02/2019 09:26:27 step: 8605, epoch: 260, batch: 24, loss: 0.050207335501909256, acc: 98.4375, f1: 99.02818270165209, r: 0.7049951125528606
06/02/2019 09:26:27 step: 8610, epoch: 260, batch: 29, loss: 0.018508227542042732, acc: 100.0, f1: 100.0, r: 0.6405432697545239
06/02/2019 09:26:28 *** evaluating ***
06/02/2019 09:26:28 step: 261, epoch: 260, acc: 60.256410256410255, f1: 23.030010530010532, r: 0.3668196841505983
06/02/2019 09:26:28 *** epoch: 262 ***
06/02/2019 09:26:28 *** training ***
06/02/2019 09:26:28 step: 8618, epoch: 261, batch: 4, loss: 0.026028502732515335, acc: 98.4375, f1: 99.0599876314162, r: 0.7258630456835056
06/02/2019 09:26:29 step: 8623, epoch: 261, batch: 9, loss: 0.03208140283823013, acc: 98.4375, f1: 98.89992360580597, r: 0.6710007497187771
06/02/2019 09:26:29 step: 8628, epoch: 261, batch: 14, loss: 0.03926386311650276, acc: 98.4375, f1: 99.25111925111925, r: 0.6895096365961031
06/02/2019 09:26:30 step: 8633, epoch: 261, batch: 19, loss: 0.006351090967655182, acc: 100.0, f1: 100.0, r: 0.6385754779748568
06/02/2019 09:26:30 step: 8638, epoch: 261, batch: 24, loss: 0.020595917478203773, acc: 98.4375, f1: 98.9459815546772, r: 0.6968327934017772
06/02/2019 09:26:30 step: 8643, epoch: 261, batch: 29, loss: 0.011958098970353603, acc: 100.0, f1: 100.0, r: 0.7013572831366511
06/02/2019 09:26:31 *** evaluating ***
06/02/2019 09:26:31 step: 262, epoch: 261, acc: 60.256410256410255, f1: 25.01617281095842, r: 0.372368533080296
06/02/2019 09:26:31 *** epoch: 263 ***
06/02/2019 09:26:31 *** training ***
06/02/2019 09:26:31 step: 8651, epoch: 262, batch: 4, loss: 0.0242875125259161, acc: 98.4375, f1: 94.66666666666667, r: 0.7031638356072087
06/02/2019 09:26:32 step: 8656, epoch: 262, batch: 9, loss: 0.002123948186635971, acc: 100.0, f1: 100.0, r: 0.7213521960597405
06/02/2019 09:26:32 step: 8661, epoch: 262, batch: 14, loss: 0.02425084076821804, acc: 100.0, f1: 100.0, r: 0.6499057655303198
06/02/2019 09:26:33 step: 8666, epoch: 262, batch: 19, loss: 0.04471294581890106, acc: 96.875, f1: 90.625, r: 0.7041706804019274
06/02/2019 09:26:33 step: 8671, epoch: 262, batch: 24, loss: 0.02275587059557438, acc: 100.0, f1: 100.0, r: 0.6759764709312609
06/02/2019 09:26:33 step: 8676, epoch: 262, batch: 29, loss: 0.013111531734466553, acc: 100.0, f1: 100.0, r: 0.6704595303053242
06/02/2019 09:26:34 *** evaluating ***
06/02/2019 09:26:34 step: 263, epoch: 262, acc: 61.111111111111114, f1: 25.44001948207556, r: 0.3724163796804331
06/02/2019 09:26:34 *** epoch: 264 ***
06/02/2019 09:26:34 *** training ***
06/02/2019 09:26:34 step: 8684, epoch: 263, batch: 4, loss: 0.01230250671505928, acc: 100.0, f1: 100.0, r: 0.8228755229934357
06/02/2019 09:26:35 step: 8689, epoch: 263, batch: 9, loss: 0.016608763486146927, acc: 100.0, f1: 100.0, r: 0.67005367781114
06/02/2019 09:26:35 step: 8694, epoch: 263, batch: 14, loss: 0.03720230609178543, acc: 96.875, f1: 95.5952380952381, r: 0.5860889735383783
06/02/2019 09:26:36 step: 8699, epoch: 263, batch: 19, loss: 0.02466808632016182, acc: 98.4375, f1: 99.30994824611847, r: 0.7020337324738308
06/02/2019 09:26:36 step: 8704, epoch: 263, batch: 24, loss: 0.022898096591234207, acc: 98.4375, f1: 99.01258162127728, r: 0.7222831789948237
06/02/2019 09:26:37 step: 8709, epoch: 263, batch: 29, loss: 0.0033427849411964417, acc: 100.0, f1: 100.0, r: 0.8312885499552588
06/02/2019 09:26:37 *** evaluating ***
06/02/2019 09:26:37 step: 264, epoch: 263, acc: 60.68376068376068, f1: 24.608505446555128, r: 0.36522024369013184
06/02/2019 09:26:37 *** epoch: 265 ***
06/02/2019 09:26:37 *** training ***
06/02/2019 09:26:37 step: 8717, epoch: 264, batch: 4, loss: 0.07420305907726288, acc: 96.875, f1: 92.43867243867244, r: 0.65098544763793
06/02/2019 09:26:38 step: 8722, epoch: 264, batch: 9, loss: 0.021577564999461174, acc: 100.0, f1: 100.0, r: 0.675396710433161
06/02/2019 09:26:38 step: 8727, epoch: 264, batch: 14, loss: 0.008589683100581169, acc: 100.0, f1: 100.0, r: 0.6766648591401195
06/02/2019 09:26:39 step: 8732, epoch: 264, batch: 19, loss: 0.01663830690085888, acc: 100.0, f1: 100.0, r: 0.6421948386570976
06/02/2019 09:26:39 step: 8737, epoch: 264, batch: 24, loss: 0.03634380176663399, acc: 98.4375, f1: 99.29118773946361, r: 0.7701661382040798
06/02/2019 09:26:40 step: 8742, epoch: 264, batch: 29, loss: 0.009348031133413315, acc: 100.0, f1: 100.0, r: 0.6739887644398351
06/02/2019 09:26:40 *** evaluating ***
06/02/2019 09:26:40 step: 265, epoch: 264, acc: 60.68376068376068, f1: 25.307210871896828, r: 0.3630644921255749
06/02/2019 09:26:40 *** epoch: 266 ***
06/02/2019 09:26:40 *** training ***
06/02/2019 09:26:40 step: 8750, epoch: 265, batch: 4, loss: 0.043981727212667465, acc: 98.4375, f1: 99.15966386554622, r: 0.7605925072004874
06/02/2019 09:26:41 step: 8755, epoch: 265, batch: 9, loss: 0.06218507140874863, acc: 98.4375, f1: 99.05284147557329, r: 0.79057260142256
06/02/2019 09:26:41 step: 8760, epoch: 265, batch: 14, loss: 0.008007306605577469, acc: 100.0, f1: 100.0, r: 0.6098118899055012
06/02/2019 09:26:42 step: 8765, epoch: 265, batch: 19, loss: 0.007675750181078911, acc: 100.0, f1: 100.0, r: 0.6551908242161997
06/02/2019 09:26:42 step: 8770, epoch: 265, batch: 24, loss: 0.016251185908913612, acc: 100.0, f1: 100.0, r: 0.7827252330569835
06/02/2019 09:26:43 step: 8775, epoch: 265, batch: 29, loss: 0.023336730897426605, acc: 98.4375, f1: 99.09700722394221, r: 0.8406962825165835
06/02/2019 09:26:43 *** evaluating ***
06/02/2019 09:26:43 step: 266, epoch: 265, acc: 60.256410256410255, f1: 24.32225804192437, r: 0.36551410474401086
06/02/2019 09:26:43 *** epoch: 267 ***
06/02/2019 09:26:43 *** training ***
06/02/2019 09:26:43 step: 8783, epoch: 266, batch: 4, loss: 0.023785637691617012, acc: 98.4375, f1: 99.16883116883118, r: 0.6376558065246313
06/02/2019 09:26:44 step: 8788, epoch: 266, batch: 9, loss: 0.01112996693700552, acc: 100.0, f1: 100.0, r: 0.8414133822868626
06/02/2019 09:26:44 step: 8793, epoch: 266, batch: 14, loss: 0.007417075335979462, acc: 100.0, f1: 100.0, r: 0.8124302651492752
06/02/2019 09:26:45 step: 8798, epoch: 266, batch: 19, loss: 0.010845355689525604, acc: 100.0, f1: 100.0, r: 0.7919862092929066
06/02/2019 09:26:45 step: 8803, epoch: 266, batch: 24, loss: 0.0310216024518013, acc: 98.4375, f1: 98.41269841269842, r: 0.7978465989348742
06/02/2019 09:26:46 step: 8808, epoch: 266, batch: 29, loss: 0.02931746281683445, acc: 100.0, f1: 100.0, r: 0.7413075072341785
06/02/2019 09:26:46 *** evaluating ***
06/02/2019 09:26:46 step: 267, epoch: 266, acc: 59.82905982905983, f1: 24.133383295230125, r: 0.36517724631018506
06/02/2019 09:26:46 *** epoch: 268 ***
06/02/2019 09:26:46 *** training ***
06/02/2019 09:26:47 step: 8816, epoch: 267, batch: 4, loss: 0.042700231075286865, acc: 96.875, f1: 98.57142857142858, r: 0.82088477391203
06/02/2019 09:26:47 step: 8821, epoch: 267, batch: 9, loss: 0.006744472309947014, acc: 100.0, f1: 100.0, r: 0.7924959506339115
06/02/2019 09:26:48 step: 8826, epoch: 267, batch: 14, loss: 0.009975442662835121, acc: 100.0, f1: 100.0, r: 0.8150539537343886
06/02/2019 09:26:48 step: 8831, epoch: 267, batch: 19, loss: 0.09354644268751144, acc: 98.4375, f1: 99.24489795918367, r: 0.7506009646989451
06/02/2019 09:26:48 step: 8836, epoch: 267, batch: 24, loss: 0.005353616550564766, acc: 100.0, f1: 100.0, r: 0.7348208926438986
06/02/2019 09:26:49 step: 8841, epoch: 267, batch: 29, loss: 0.008460462093353271, acc: 100.0, f1: 100.0, r: 0.835081426714392
06/02/2019 09:26:49 *** evaluating ***
06/02/2019 09:26:49 step: 268, epoch: 267, acc: 59.82905982905983, f1: 23.930146609435297, r: 0.36929508894989754
06/02/2019 09:26:49 *** epoch: 269 ***
06/02/2019 09:26:49 *** training ***
06/02/2019 09:26:50 step: 8849, epoch: 268, batch: 4, loss: 0.032429710030555725, acc: 98.4375, f1: 99.00598955014655, r: 0.6759460271458997
06/02/2019 09:26:50 step: 8854, epoch: 268, batch: 9, loss: 0.020772889256477356, acc: 100.0, f1: 100.0, r: 0.7313021546376617
06/02/2019 09:26:51 step: 8859, epoch: 268, batch: 14, loss: 0.0056613292545080185, acc: 100.0, f1: 100.0, r: 0.749736367164131
06/02/2019 09:26:51 step: 8864, epoch: 268, batch: 19, loss: 0.056404002010822296, acc: 98.4375, f1: 99.26314819931841, r: 0.7768108876634499
06/02/2019 09:26:52 step: 8869, epoch: 268, batch: 24, loss: 0.038197681307792664, acc: 98.4375, f1: 97.64957264957265, r: 0.7925921756937166
06/02/2019 09:26:52 step: 8874, epoch: 268, batch: 29, loss: 0.0037718191742897034, acc: 100.0, f1: 100.0, r: 0.7049955027941198
06/02/2019 09:26:52 *** evaluating ***
06/02/2019 09:26:52 step: 269, epoch: 268, acc: 59.82905982905983, f1: 23.87230337399549, r: 0.36011141673475466
06/02/2019 09:26:52 *** epoch: 270 ***
06/02/2019 09:26:52 *** training ***
06/02/2019 09:26:53 step: 8882, epoch: 269, batch: 4, loss: 0.1064535528421402, acc: 96.875, f1: 97.1280915466962, r: 0.6894954560135341
06/02/2019 09:26:53 step: 8887, epoch: 269, batch: 9, loss: 0.0423685722053051, acc: 98.4375, f1: 96.1111111111111, r: 0.8355778119023469
06/02/2019 09:26:54 step: 8892, epoch: 269, batch: 14, loss: 0.007395796477794647, acc: 100.0, f1: 100.0, r: 0.752413108726322
06/02/2019 09:26:54 step: 8897, epoch: 269, batch: 19, loss: 0.032014600932598114, acc: 98.4375, f1: 98.67669172932331, r: 0.6859387845683488
06/02/2019 09:26:55 step: 8902, epoch: 269, batch: 24, loss: 0.004393764305859804, acc: 100.0, f1: 100.0, r: 0.7797502695750541
06/02/2019 09:26:55 step: 8907, epoch: 269, batch: 29, loss: 0.011881187558174133, acc: 100.0, f1: 100.0, r: 0.7064903781998547
06/02/2019 09:26:55 *** evaluating ***
06/02/2019 09:26:55 step: 270, epoch: 269, acc: 60.256410256410255, f1: 24.183738733516595, r: 0.36201737770768305
06/02/2019 09:26:55 *** epoch: 271 ***
06/02/2019 09:26:55 *** training ***
06/02/2019 09:26:56 step: 8915, epoch: 270, batch: 4, loss: 0.002123180776834488, acc: 100.0, f1: 100.0, r: 0.7936535981629348
06/02/2019 09:26:56 step: 8920, epoch: 270, batch: 9, loss: 0.004001053050160408, acc: 100.0, f1: 100.0, r: 0.7350879785951057
06/02/2019 09:26:57 step: 8925, epoch: 270, batch: 14, loss: 0.018438296392560005, acc: 100.0, f1: 100.0, r: 0.8074277130457829
06/02/2019 09:26:57 step: 8930, epoch: 270, batch: 19, loss: 0.00928284227848053, acc: 100.0, f1: 100.0, r: 0.8254950302181349
06/02/2019 09:26:58 step: 8935, epoch: 270, batch: 24, loss: 0.04099026322364807, acc: 98.4375, f1: 99.07759714055115, r: 0.6800410174830139
06/02/2019 09:26:58 step: 8940, epoch: 270, batch: 29, loss: 0.0039000585675239563, acc: 100.0, f1: 100.0, r: 0.6752990169423503
06/02/2019 09:26:59 *** evaluating ***
06/02/2019 09:26:59 step: 271, epoch: 270, acc: 59.82905982905983, f1: 23.982937414733865, r: 0.3602925312334868
06/02/2019 09:26:59 *** epoch: 272 ***
06/02/2019 09:26:59 *** training ***
06/02/2019 09:26:59 step: 8948, epoch: 271, batch: 4, loss: 0.009529037401080132, acc: 100.0, f1: 100.0, r: 0.7653400078781506
06/02/2019 09:27:00 step: 8953, epoch: 271, batch: 9, loss: 0.0012778043746948242, acc: 100.0, f1: 100.0, r: 0.7806990737408843
06/02/2019 09:27:00 step: 8958, epoch: 271, batch: 14, loss: 0.010073816403746605, acc: 100.0, f1: 100.0, r: 0.8330168394810628
06/02/2019 09:27:01 step: 8963, epoch: 271, batch: 19, loss: 0.010243232361972332, acc: 100.0, f1: 100.0, r: 0.7465925541182119
06/02/2019 09:27:01 step: 8968, epoch: 271, batch: 24, loss: 0.010729996487498283, acc: 100.0, f1: 100.0, r: 0.7062673230463268
06/02/2019 09:27:02 step: 8973, epoch: 271, batch: 29, loss: 0.010881073772907257, acc: 100.0, f1: 100.0, r: 0.8398585596644969
06/02/2019 09:27:02 *** evaluating ***
06/02/2019 09:27:02 step: 272, epoch: 271, acc: 59.82905982905983, f1: 24.764257481648784, r: 0.35787488758784697
06/02/2019 09:27:02 *** epoch: 273 ***
06/02/2019 09:27:02 *** training ***
06/02/2019 09:27:03 step: 8981, epoch: 272, batch: 4, loss: 0.006191415712237358, acc: 100.0, f1: 100.0, r: 0.7429012581572261
06/02/2019 09:27:03 step: 8986, epoch: 272, batch: 9, loss: 0.005556900054216385, acc: 100.0, f1: 100.0, r: 0.7211425914367865
06/02/2019 09:27:04 step: 8991, epoch: 272, batch: 14, loss: 0.024955477565526962, acc: 98.4375, f1: 98.62700228832952, r: 0.6839610260434263
06/02/2019 09:27:04 step: 8996, epoch: 272, batch: 19, loss: 0.010698527097702026, acc: 100.0, f1: 100.0, r: 0.7381447841495286
06/02/2019 09:27:05 step: 9001, epoch: 272, batch: 24, loss: 0.0480419397354126, acc: 96.875, f1: 98.4539270253556, r: 0.6797369378289889
06/02/2019 09:27:05 step: 9006, epoch: 272, batch: 29, loss: 0.010670416057109833, acc: 100.0, f1: 100.0, r: 0.8073210487787537
06/02/2019 09:27:05 *** evaluating ***
06/02/2019 09:27:05 step: 273, epoch: 272, acc: 59.401709401709404, f1: 23.730237154150196, r: 0.3563079152087784
06/02/2019 09:27:05 *** epoch: 274 ***
06/02/2019 09:27:05 *** training ***
06/02/2019 09:27:06 step: 9014, epoch: 273, batch: 4, loss: 0.013041255995631218, acc: 100.0, f1: 100.0, r: 0.7228661385852828
06/02/2019 09:27:06 step: 9019, epoch: 273, batch: 9, loss: 0.007894255220890045, acc: 100.0, f1: 100.0, r: 0.7162203521869087
06/02/2019 09:27:07 step: 9024, epoch: 273, batch: 14, loss: 0.007639728486537933, acc: 100.0, f1: 100.0, r: 0.813277516729273
06/02/2019 09:27:07 step: 9029, epoch: 273, batch: 19, loss: 0.03489029034972191, acc: 98.4375, f1: 98.92156862745098, r: 0.8039603714196787
06/02/2019 09:27:08 step: 9034, epoch: 273, batch: 24, loss: 0.015348841436207294, acc: 100.0, f1: 100.0, r: 0.8349654241704475
06/02/2019 09:27:08 step: 9039, epoch: 273, batch: 29, loss: 0.0020916517823934555, acc: 100.0, f1: 100.0, r: 0.6870287956593872
06/02/2019 09:27:09 *** evaluating ***
06/02/2019 09:27:09 step: 274, epoch: 273, acc: 61.111111111111114, f1: 25.526263530132354, r: 0.3565592473311751
06/02/2019 09:27:09 *** epoch: 275 ***
06/02/2019 09:27:09 *** training ***
06/02/2019 09:27:09 step: 9047, epoch: 274, batch: 4, loss: 0.005961954593658447, acc: 100.0, f1: 100.0, r: 0.7286034656718992
06/02/2019 09:27:10 step: 9052, epoch: 274, batch: 9, loss: 0.0043907612562179565, acc: 100.0, f1: 100.0, r: 0.8245597210943166
06/02/2019 09:27:10 step: 9057, epoch: 274, batch: 14, loss: 0.015253571793437004, acc: 100.0, f1: 100.0, r: 0.7404802764311822
06/02/2019 09:27:11 step: 9062, epoch: 274, batch: 19, loss: 0.024868007749319077, acc: 100.0, f1: 100.0, r: 0.7753824135564958
06/02/2019 09:27:11 step: 9067, epoch: 274, batch: 24, loss: 0.02067449688911438, acc: 98.4375, f1: 96.19047619047619, r: 0.6932806030697595
06/02/2019 09:27:11 step: 9072, epoch: 274, batch: 29, loss: 0.0021331198513507843, acc: 100.0, f1: 100.0, r: 0.7010922129050169
06/02/2019 09:27:12 *** evaluating ***
06/02/2019 09:27:12 step: 275, epoch: 274, acc: 61.111111111111114, f1: 26.131778266710796, r: 0.3588079192566017
06/02/2019 09:27:12 *** epoch: 276 ***
06/02/2019 09:27:12 *** training ***
06/02/2019 09:27:12 step: 9080, epoch: 275, batch: 4, loss: 0.06919358670711517, acc: 98.4375, f1: 99.0514075887393, r: 0.8198170558119917
06/02/2019 09:27:13 step: 9085, epoch: 275, batch: 9, loss: 0.0045628901571035385, acc: 100.0, f1: 100.0, r: 0.6639087504439931
06/02/2019 09:27:13 step: 9090, epoch: 275, batch: 14, loss: 0.05981747806072235, acc: 98.4375, f1: 98.86128364389234, r: 0.8330239851294388
06/02/2019 09:27:14 step: 9095, epoch: 275, batch: 19, loss: 0.018221614882349968, acc: 100.0, f1: 100.0, r: 0.6802088324171575
06/02/2019 09:27:14 step: 9100, epoch: 275, batch: 24, loss: 0.006707571446895599, acc: 100.0, f1: 100.0, r: 0.7229600999079441
06/02/2019 09:27:15 step: 9105, epoch: 275, batch: 29, loss: 0.029901068657636642, acc: 98.4375, f1: 98.46819846819848, r: 0.7146831570907535
06/02/2019 09:27:15 *** evaluating ***
06/02/2019 09:27:15 step: 276, epoch: 275, acc: 60.256410256410255, f1: 24.986179223984102, r: 0.3634224644066789
06/02/2019 09:27:15 *** epoch: 277 ***
06/02/2019 09:27:15 *** training ***
06/02/2019 09:27:15 step: 9113, epoch: 276, batch: 4, loss: 0.031620535999536514, acc: 98.4375, f1: 99.33081674673987, r: 0.7995580336486419
06/02/2019 09:27:16 step: 9118, epoch: 276, batch: 9, loss: 0.051982998847961426, acc: 96.875, f1: 95.89569160997733, r: 0.7066908524071023
06/02/2019 09:27:16 step: 9123, epoch: 276, batch: 14, loss: 0.06876850128173828, acc: 96.875, f1: 96.21195846759755, r: 0.6812527415949776
06/02/2019 09:27:17 step: 9128, epoch: 276, batch: 19, loss: 0.09253977239131927, acc: 96.875, f1: 98.10066476733142, r: 0.657836873751038
06/02/2019 09:27:17 step: 9133, epoch: 276, batch: 24, loss: 0.005909966304898262, acc: 100.0, f1: 100.0, r: 0.6810116165297632
06/02/2019 09:27:18 step: 9138, epoch: 276, batch: 29, loss: 0.04816310480237007, acc: 98.4375, f1: 97.79158040027606, r: 0.7102685805768081
06/02/2019 09:27:18 *** evaluating ***
06/02/2019 09:27:18 step: 277, epoch: 276, acc: 59.401709401709404, f1: 24.644448688566335, r: 0.36599553242427324
06/02/2019 09:27:18 *** epoch: 278 ***
06/02/2019 09:27:18 *** training ***
06/02/2019 09:27:19 step: 9146, epoch: 277, batch: 4, loss: 0.008888132870197296, acc: 100.0, f1: 100.0, r: 0.7104093516017063
06/02/2019 09:27:19 step: 9151, epoch: 277, batch: 9, loss: 0.1094457134604454, acc: 98.4375, f1: 99.18099918099918, r: 0.7190222516149899
06/02/2019 09:27:20 step: 9156, epoch: 277, batch: 14, loss: 0.018818354234099388, acc: 100.0, f1: 100.0, r: 0.6868556855078087
06/02/2019 09:27:20 step: 9161, epoch: 277, batch: 19, loss: 0.031219156458973885, acc: 98.4375, f1: 97.47474747474747, r: 0.7444298044479924
06/02/2019 09:27:21 step: 9166, epoch: 277, batch: 24, loss: 0.004622414708137512, acc: 100.0, f1: 100.0, r: 0.6822885390675842
06/02/2019 09:27:21 step: 9171, epoch: 277, batch: 29, loss: 0.02149367518723011, acc: 100.0, f1: 100.0, r: 0.82756453168912
06/02/2019 09:27:21 *** evaluating ***
06/02/2019 09:27:21 step: 278, epoch: 277, acc: 59.82905982905983, f1: 24.790947585733196, r: 0.3655934479913221
06/02/2019 09:27:21 *** epoch: 279 ***
06/02/2019 09:27:21 *** training ***
06/02/2019 09:27:22 step: 9179, epoch: 278, batch: 4, loss: 0.022702176123857498, acc: 98.4375, f1: 99.01960784313727, r: 0.7931092107139659
06/02/2019 09:27:22 step: 9184, epoch: 278, batch: 9, loss: 0.016710026189684868, acc: 98.4375, f1: 98.1111111111111, r: 0.7434362863502352
06/02/2019 09:27:23 step: 9189, epoch: 278, batch: 14, loss: 0.028672881424427032, acc: 98.4375, f1: 98.14315663372267, r: 0.7641695795527318
06/02/2019 09:27:23 step: 9194, epoch: 278, batch: 19, loss: 0.08233186602592468, acc: 98.4375, f1: 95.58823529411764, r: 0.7898393498058557
06/02/2019 09:27:24 step: 9199, epoch: 278, batch: 24, loss: 0.021259840577840805, acc: 98.4375, f1: 99.20930232558139, r: 0.8393747439786372
06/02/2019 09:27:24 step: 9204, epoch: 278, batch: 29, loss: 0.0653281956911087, acc: 98.4375, f1: 98.1283422459893, r: 0.817301641338662
06/02/2019 09:27:24 *** evaluating ***
06/02/2019 09:27:24 step: 279, epoch: 278, acc: 60.256410256410255, f1: 24.99375837291438, r: 0.3630685442583744
06/02/2019 09:27:24 *** epoch: 280 ***
06/02/2019 09:27:24 *** training ***
06/02/2019 09:27:25 step: 9212, epoch: 279, batch: 4, loss: 0.006108429282903671, acc: 100.0, f1: 100.0, r: 0.7657881004258875
06/02/2019 09:27:25 step: 9217, epoch: 279, batch: 9, loss: 0.033755071461200714, acc: 98.4375, f1: 98.94179894179894, r: 0.6996421917670413
06/02/2019 09:27:26 step: 9222, epoch: 279, batch: 14, loss: 0.012757264077663422, acc: 100.0, f1: 100.0, r: 0.7485922078793636
06/02/2019 09:27:26 step: 9227, epoch: 279, batch: 19, loss: 0.002056580036878586, acc: 100.0, f1: 100.0, r: 0.6898468300032571
06/02/2019 09:27:27 step: 9232, epoch: 279, batch: 24, loss: 0.0018716491758823395, acc: 100.0, f1: 100.0, r: 0.8280452632757354
06/02/2019 09:27:27 step: 9237, epoch: 279, batch: 29, loss: 0.004789233207702637, acc: 100.0, f1: 100.0, r: 0.7420549171184087
06/02/2019 09:27:27 *** evaluating ***
06/02/2019 09:27:28 step: 280, epoch: 279, acc: 61.53846153846154, f1: 25.550730621113026, r: 0.3633039977071553
06/02/2019 09:27:28 *** epoch: 281 ***
06/02/2019 09:27:28 *** training ***
06/02/2019 09:27:28 step: 9245, epoch: 280, batch: 4, loss: 0.0030289869755506516, acc: 100.0, f1: 100.0, r: 0.8026356528297985
06/02/2019 09:27:28 step: 9250, epoch: 280, batch: 9, loss: 0.05208292976021767, acc: 98.4375, f1: 98.9459815546772, r: 0.6629950892158566
06/02/2019 09:27:29 step: 9255, epoch: 280, batch: 14, loss: 0.00870206393301487, acc: 100.0, f1: 100.0, r: 0.7065560915802949
06/02/2019 09:27:29 step: 9260, epoch: 280, batch: 19, loss: 0.03643086552619934, acc: 98.4375, f1: 99.3661100803958, r: 0.7761778817502142
06/02/2019 09:27:30 step: 9265, epoch: 280, batch: 24, loss: 0.006678549572825432, acc: 100.0, f1: 100.0, r: 0.7612228267259159
06/02/2019 09:27:30 step: 9270, epoch: 280, batch: 29, loss: 0.02340460941195488, acc: 98.4375, f1: 99.22027290448344, r: 0.6608707488343114
06/02/2019 09:27:31 *** evaluating ***
06/02/2019 09:27:31 step: 281, epoch: 280, acc: 60.256410256410255, f1: 25.242237506872904, r: 0.36690998051335594
06/02/2019 09:27:31 *** epoch: 282 ***
06/02/2019 09:27:31 *** training ***
06/02/2019 09:27:31 step: 9278, epoch: 281, batch: 4, loss: 0.04992208629846573, acc: 98.4375, f1: 98.94736842105263, r: 0.8034219715965274
06/02/2019 09:27:32 step: 9283, epoch: 281, batch: 9, loss: 0.008972657844424248, acc: 100.0, f1: 100.0, r: 0.6631168621238931
06/02/2019 09:27:32 step: 9288, epoch: 281, batch: 14, loss: 0.010923026129603386, acc: 100.0, f1: 100.0, r: 0.8028685213349642
06/02/2019 09:27:33 step: 9293, epoch: 281, batch: 19, loss: 0.0033655837178230286, acc: 100.0, f1: 100.0, r: 0.7580232867693826
06/02/2019 09:27:33 step: 9298, epoch: 281, batch: 24, loss: 0.08114384859800339, acc: 96.875, f1: 94.62585034013607, r: 0.6730654235991277
06/02/2019 09:27:33 step: 9303, epoch: 281, batch: 29, loss: 0.004406286403536797, acc: 100.0, f1: 100.0, r: 0.8160914070769456
06/02/2019 09:27:34 *** evaluating ***
06/02/2019 09:27:34 step: 282, epoch: 281, acc: 60.68376068376068, f1: 25.11382906095842, r: 0.3705664446328246
06/02/2019 09:27:34 *** epoch: 283 ***
06/02/2019 09:27:34 *** training ***
06/02/2019 09:27:34 step: 9311, epoch: 282, batch: 4, loss: 0.025077831000089645, acc: 98.4375, f1: 97.47474747474747, r: 0.8226412407126811
06/02/2019 09:27:35 step: 9316, epoch: 282, batch: 9, loss: 0.022826597094535828, acc: 98.4375, f1: 97.78325123152709, r: 0.7510579107158801
06/02/2019 09:27:35 step: 9321, epoch: 282, batch: 14, loss: 0.007003359496593475, acc: 100.0, f1: 100.0, r: 0.751715245954416
06/02/2019 09:27:36 step: 9326, epoch: 282, batch: 19, loss: 0.00966713298112154, acc: 100.0, f1: 100.0, r: 0.8048265036545033
06/02/2019 09:27:36 step: 9331, epoch: 282, batch: 24, loss: 0.015706103295087814, acc: 100.0, f1: 100.0, r: 0.7096348198072582
06/02/2019 09:27:37 step: 9336, epoch: 282, batch: 29, loss: 0.010569185018539429, acc: 100.0, f1: 100.0, r: 0.7231634634721038
06/02/2019 09:27:37 *** evaluating ***
06/02/2019 09:27:37 step: 283, epoch: 282, acc: 60.68376068376068, f1: 25.07659049091951, r: 0.36981623197552993
06/02/2019 09:27:37 *** epoch: 284 ***
06/02/2019 09:27:37 *** training ***
06/02/2019 09:27:37 step: 9344, epoch: 283, batch: 4, loss: 0.057130903005599976, acc: 96.875, f1: 98.02197802197803, r: 0.6779053186715546
06/02/2019 09:27:38 step: 9349, epoch: 283, batch: 9, loss: 0.05041037127375603, acc: 95.3125, f1: 90.938013136289, r: 0.7734255654133728
06/02/2019 09:27:38 step: 9354, epoch: 283, batch: 14, loss: 0.015678366646170616, acc: 100.0, f1: 100.0, r: 0.6473722919684156
06/02/2019 09:27:39 step: 9359, epoch: 283, batch: 19, loss: 0.04654757305979729, acc: 98.4375, f1: 97.79158040027606, r: 0.6378476581441631
06/02/2019 09:27:39 step: 9364, epoch: 283, batch: 24, loss: 0.057397641241550446, acc: 98.4375, f1: 95.10204081632652, r: 0.6558006898551723
06/02/2019 09:27:40 step: 9369, epoch: 283, batch: 29, loss: 0.03573068231344223, acc: 98.4375, f1: 98.49498327759196, r: 0.7607574277639736
06/02/2019 09:27:40 *** evaluating ***
06/02/2019 09:27:40 step: 284, epoch: 283, acc: 60.68376068376068, f1: 25.15234806667709, r: 0.37197995574188425
06/02/2019 09:27:40 *** epoch: 285 ***
06/02/2019 09:27:40 *** training ***
06/02/2019 09:27:41 step: 9377, epoch: 284, batch: 4, loss: 0.03561529889702797, acc: 98.4375, f1: 97.74891774891773, r: 0.7048420518640206
06/02/2019 09:27:41 step: 9382, epoch: 284, batch: 9, loss: 0.03494952619075775, acc: 98.4375, f1: 98.76750700280112, r: 0.7086512074837357
06/02/2019 09:27:41 step: 9387, epoch: 284, batch: 14, loss: 0.010274875909090042, acc: 100.0, f1: 100.0, r: 0.7738345256547078
06/02/2019 09:27:42 step: 9392, epoch: 284, batch: 19, loss: 0.02025923691689968, acc: 100.0, f1: 100.0, r: 0.7524504443737713
06/02/2019 09:27:42 step: 9397, epoch: 284, batch: 24, loss: 0.010551072657108307, acc: 100.0, f1: 100.0, r: 0.768733938468373
06/02/2019 09:27:43 step: 9402, epoch: 284, batch: 29, loss: 0.004061194136738777, acc: 100.0, f1: 100.0, r: 0.5436662910377662
06/02/2019 09:27:43 *** evaluating ***
06/02/2019 09:27:43 step: 285, epoch: 284, acc: 59.82905982905983, f1: 23.930146609435297, r: 0.3702721093207299
06/02/2019 09:27:43 *** epoch: 286 ***
06/02/2019 09:27:43 *** training ***
06/02/2019 09:27:44 step: 9410, epoch: 285, batch: 4, loss: 0.012255126610398293, acc: 100.0, f1: 100.0, r: 0.6801641635325735
06/02/2019 09:27:44 step: 9415, epoch: 285, batch: 9, loss: 0.01699177920818329, acc: 100.0, f1: 100.0, r: 0.785360223897407
06/02/2019 09:27:45 step: 9420, epoch: 285, batch: 14, loss: 0.08382643759250641, acc: 96.875, f1: 95.95238095238095, r: 0.6896691074312056
06/02/2019 09:27:45 step: 9425, epoch: 285, batch: 19, loss: 0.011545373126864433, acc: 100.0, f1: 100.0, r: 0.774660819302569
06/02/2019 09:27:45 step: 9430, epoch: 285, batch: 24, loss: 0.03514716029167175, acc: 98.4375, f1: 98.3451536643026, r: 0.7961023798484075
06/02/2019 09:27:46 step: 9435, epoch: 285, batch: 29, loss: 0.03218219429254532, acc: 98.4375, f1: 99.17516324894031, r: 0.6897644871878331
06/02/2019 09:27:46 *** evaluating ***
06/02/2019 09:27:46 step: 286, epoch: 285, acc: 60.256410256410255, f1: 24.94036616743039, r: 0.3714930091425403
06/02/2019 09:27:46 *** epoch: 287 ***
06/02/2019 09:27:46 *** training ***
06/02/2019 09:27:47 step: 9443, epoch: 286, batch: 4, loss: 0.006798360496759415, acc: 100.0, f1: 100.0, r: 0.7757191125535998
06/02/2019 09:27:47 step: 9448, epoch: 286, batch: 9, loss: 0.012236086651682854, acc: 100.0, f1: 100.0, r: 0.7615997938766188
06/02/2019 09:27:48 step: 9453, epoch: 286, batch: 14, loss: 0.03826967254281044, acc: 98.4375, f1: 99.1140642303433, r: 0.7920955445138369
06/02/2019 09:27:48 step: 9458, epoch: 286, batch: 19, loss: 0.01982051506638527, acc: 100.0, f1: 100.0, r: 0.6989530870813074
06/02/2019 09:27:48 step: 9463, epoch: 286, batch: 24, loss: 0.014919187873601913, acc: 100.0, f1: 100.0, r: 0.6315517220556933
06/02/2019 09:27:49 step: 9468, epoch: 286, batch: 29, loss: 0.10541191697120667, acc: 95.3125, f1: 97.08454810495627, r: 0.7121948087203988
06/02/2019 09:27:49 *** evaluating ***
06/02/2019 09:27:49 step: 287, epoch: 286, acc: 60.256410256410255, f1: 25.078716856060606, r: 0.37453829023847895
06/02/2019 09:27:49 *** epoch: 288 ***
06/02/2019 09:27:49 *** training ***
06/02/2019 09:27:50 step: 9476, epoch: 287, batch: 4, loss: 0.008859025314450264, acc: 100.0, f1: 100.0, r: 0.8000333695329581
06/02/2019 09:27:50 step: 9481, epoch: 287, batch: 9, loss: 0.08428776264190674, acc: 98.4375, f1: 99.17516324894031, r: 0.7078490869979983
06/02/2019 09:27:51 step: 9486, epoch: 287, batch: 14, loss: 0.06573736667633057, acc: 96.875, f1: 96.57599882058085, r: 0.7003748242022388
06/02/2019 09:27:51 step: 9491, epoch: 287, batch: 19, loss: 0.005528056062757969, acc: 100.0, f1: 100.0, r: 0.7526726047906817
06/02/2019 09:27:52 step: 9496, epoch: 287, batch: 24, loss: 0.06445161998271942, acc: 96.875, f1: 98.45833333333334, r: 0.7524246011835535
06/02/2019 09:27:52 step: 9501, epoch: 287, batch: 29, loss: 0.008862491697072983, acc: 100.0, f1: 100.0, r: 0.7888428146657209
06/02/2019 09:27:52 *** evaluating ***
06/02/2019 09:27:52 step: 288, epoch: 287, acc: 60.68376068376068, f1: 26.822463768115938, r: 0.379310075405898
06/02/2019 09:27:52 *** epoch: 289 ***
06/02/2019 09:27:52 *** training ***
06/02/2019 09:27:53 step: 9509, epoch: 288, batch: 4, loss: 0.025435009971261024, acc: 98.4375, f1: 99.1484593837535, r: 0.6907766627491846
06/02/2019 09:27:53 step: 9514, epoch: 288, batch: 9, loss: 0.023988302797079086, acc: 98.4375, f1: 99.15895710681245, r: 0.6783041901197812
06/02/2019 09:27:54 step: 9519, epoch: 288, batch: 14, loss: 0.001109834760427475, acc: 100.0, f1: 100.0, r: 0.7247810667049955
06/02/2019 09:27:54 step: 9524, epoch: 288, batch: 19, loss: 0.020152434706687927, acc: 98.4375, f1: 96.8831168831169, r: 0.7240662353919636
06/02/2019 09:27:55 step: 9529, epoch: 288, batch: 24, loss: 0.013400457799434662, acc: 100.0, f1: 100.0, r: 0.7354889816761566
06/02/2019 09:27:55 step: 9534, epoch: 288, batch: 29, loss: 0.004363302141427994, acc: 100.0, f1: 100.0, r: 0.70949475545347
06/02/2019 09:27:55 *** evaluating ***
06/02/2019 09:27:56 step: 289, epoch: 288, acc: 61.111111111111114, f1: 25.44001948207556, r: 0.37731589014651734
06/02/2019 09:27:56 *** epoch: 290 ***
06/02/2019 09:27:56 *** training ***
06/02/2019 09:27:56 step: 9542, epoch: 289, batch: 4, loss: 0.010322954505681992, acc: 100.0, f1: 100.0, r: 0.770632520149268
06/02/2019 09:27:56 step: 9547, epoch: 289, batch: 9, loss: 0.06958803534507751, acc: 96.875, f1: 92.1139054532884, r: 0.6291826719054436
06/02/2019 09:27:57 step: 9552, epoch: 289, batch: 14, loss: 0.05097370222210884, acc: 96.875, f1: 98.58630952380953, r: 0.7550388439225099
06/02/2019 09:27:57 step: 9557, epoch: 289, batch: 19, loss: 0.04784373939037323, acc: 98.4375, f1: 95.28985507246377, r: 0.7176283981636554
06/02/2019 09:27:58 step: 9562, epoch: 289, batch: 24, loss: 0.05927892029285431, acc: 98.4375, f1: 98.31932773109244, r: 0.7381466219072422
06/02/2019 09:27:58 step: 9567, epoch: 289, batch: 29, loss: 0.010156277567148209, acc: 100.0, f1: 100.0, r: 0.6902338495804246
06/02/2019 09:27:59 *** evaluating ***
06/02/2019 09:27:59 step: 290, epoch: 289, acc: 60.256410256410255, f1: 24.965702189966894, r: 0.37158237638920844
06/02/2019 09:27:59 *** epoch: 291 ***
06/02/2019 09:27:59 *** training ***
06/02/2019 09:27:59 step: 9575, epoch: 290, batch: 4, loss: 0.06058604642748833, acc: 95.3125, f1: 96.88523573200992, r: 0.8153278309614133
06/02/2019 09:28:00 step: 9580, epoch: 290, batch: 9, loss: 0.037462104111909866, acc: 96.875, f1: 96.3327859879584, r: 0.6341938544702099
06/02/2019 09:28:00 step: 9585, epoch: 290, batch: 14, loss: 0.05243034288287163, acc: 98.4375, f1: 96.68202764976958, r: 0.7103690433585997
06/02/2019 09:28:01 step: 9590, epoch: 290, batch: 19, loss: 0.031074851751327515, acc: 98.4375, f1: 96.9047619047619, r: 0.7251936080632787
06/02/2019 09:28:01 step: 9595, epoch: 290, batch: 24, loss: 0.015422157943248749, acc: 100.0, f1: 100.0, r: 0.7818229939849983
06/02/2019 09:28:02 step: 9600, epoch: 290, batch: 29, loss: 0.0271503534168005, acc: 98.4375, f1: 98.70370370370371, r: 0.8206465595975265
06/02/2019 09:28:02 *** evaluating ***
06/02/2019 09:28:02 step: 291, epoch: 290, acc: 61.111111111111114, f1: 25.292428017718716, r: 0.36690776238268563
06/02/2019 09:28:02 *** epoch: 292 ***
06/02/2019 09:28:02 *** training ***
06/02/2019 09:28:03 step: 9608, epoch: 291, batch: 4, loss: 0.011552641168236732, acc: 100.0, f1: 100.0, r: 0.5662996626010299
06/02/2019 09:28:03 step: 9613, epoch: 291, batch: 9, loss: 0.05662883445620537, acc: 96.875, f1: 98.3718487394958, r: 0.8011605088423712
06/02/2019 09:28:04 step: 9618, epoch: 291, batch: 14, loss: 0.003544604405760765, acc: 100.0, f1: 100.0, r: 0.7069890249938193
06/02/2019 09:28:05 step: 9623, epoch: 291, batch: 19, loss: 0.01671454682946205, acc: 100.0, f1: 100.0, r: 0.6590554014590656
06/02/2019 09:28:05 step: 9628, epoch: 291, batch: 24, loss: 0.002901265397667885, acc: 100.0, f1: 100.0, r: 0.7998752627238933
06/02/2019 09:28:06 step: 9633, epoch: 291, batch: 29, loss: 0.05472411587834358, acc: 98.4375, f1: 98.29573934837093, r: 0.6910557527046609
06/02/2019 09:28:06 *** evaluating ***
06/02/2019 09:28:07 step: 292, epoch: 291, acc: 60.68376068376068, f1: 25.26977069897726, r: 0.3661336364709006
06/02/2019 09:28:07 *** epoch: 293 ***
06/02/2019 09:28:07 *** training ***
06/02/2019 09:28:07 step: 9641, epoch: 292, batch: 4, loss: 0.018808217719197273, acc: 98.4375, f1: 98.86178861788618, r: 0.8328939079467271
06/02/2019 09:28:08 step: 9646, epoch: 292, batch: 9, loss: 0.11347287893295288, acc: 98.4375, f1: 97.78325123152709, r: 0.7770972070910035
06/02/2019 09:28:09 step: 9651, epoch: 292, batch: 14, loss: 0.06311076134443283, acc: 98.4375, f1: 99.18546365914787, r: 0.7688407898746205
06/02/2019 09:28:09 step: 9656, epoch: 292, batch: 19, loss: 0.004518318921327591, acc: 100.0, f1: 100.0, r: 0.5794356178333359
06/02/2019 09:28:10 step: 9661, epoch: 292, batch: 24, loss: 0.004943709820508957, acc: 100.0, f1: 100.0, r: 0.7496120047655022
06/02/2019 09:28:11 step: 9666, epoch: 292, batch: 29, loss: 0.00377640500664711, acc: 100.0, f1: 100.0, r: 0.6332139676714829
06/02/2019 09:28:11 *** evaluating ***
06/02/2019 09:28:11 step: 293, epoch: 292, acc: 60.68376068376068, f1: 26.070078073244794, r: 0.3694792722116765
06/02/2019 09:28:11 *** epoch: 294 ***
06/02/2019 09:28:11 *** training ***
06/02/2019 09:28:12 step: 9674, epoch: 293, batch: 4, loss: 0.012189051136374474, acc: 100.0, f1: 100.0, r: 0.7122460322920914
06/02/2019 09:28:12 step: 9679, epoch: 293, batch: 9, loss: 0.034187935292720795, acc: 98.4375, f1: 97.979797979798, r: 0.6706975477067886
06/02/2019 09:28:13 step: 9684, epoch: 293, batch: 14, loss: 0.03178270533680916, acc: 98.4375, f1: 99.01234567901234, r: 0.6151081094461645
06/02/2019 09:28:14 step: 9689, epoch: 293, batch: 19, loss: 0.002468898892402649, acc: 100.0, f1: 100.0, r: 0.7085406408765884
06/02/2019 09:28:14 step: 9694, epoch: 293, batch: 24, loss: 0.003806101158261299, acc: 100.0, f1: 100.0, r: 0.7205916378521786
06/02/2019 09:28:15 step: 9699, epoch: 293, batch: 29, loss: 0.00894845835864544, acc: 100.0, f1: 100.0, r: 0.7626757435587721
06/02/2019 09:28:15 *** evaluating ***
06/02/2019 09:28:16 step: 294, epoch: 293, acc: 60.256410256410255, f1: 25.102454624394237, r: 0.3687959369843911
06/02/2019 09:28:16 *** epoch: 295 ***
06/02/2019 09:28:16 *** training ***
06/02/2019 09:28:16 step: 9707, epoch: 294, batch: 4, loss: 0.02544117346405983, acc: 98.4375, f1: 98.97071872227151, r: 0.7958130052247779
06/02/2019 09:28:17 step: 9712, epoch: 294, batch: 9, loss: 0.009808307513594627, acc: 100.0, f1: 100.0, r: 0.5568697975020547
06/02/2019 09:28:18 step: 9717, epoch: 294, batch: 14, loss: 0.021069953218102455, acc: 100.0, f1: 100.0, r: 0.7182445150022956
06/02/2019 09:28:19 step: 9722, epoch: 294, batch: 19, loss: 0.007306646555662155, acc: 100.0, f1: 100.0, r: 0.8304358074525984
06/02/2019 09:28:19 step: 9727, epoch: 294, batch: 24, loss: 0.014858759939670563, acc: 100.0, f1: 100.0, r: 0.7976414024939248
06/02/2019 09:28:20 step: 9732, epoch: 294, batch: 29, loss: 0.020884033292531967, acc: 100.0, f1: 100.0, r: 0.7918081794900965
06/02/2019 09:28:20 *** evaluating ***
06/02/2019 09:28:20 step: 295, epoch: 294, acc: 60.68376068376068, f1: 25.26977069897726, r: 0.37026853655246217
06/02/2019 09:28:20 *** epoch: 296 ***
06/02/2019 09:28:20 *** training ***
06/02/2019 09:28:21 step: 9740, epoch: 295, batch: 4, loss: 0.0017897672951221466, acc: 100.0, f1: 100.0, r: 0.7496536372317979
06/02/2019 09:28:22 step: 9745, epoch: 295, batch: 9, loss: 0.004413817077875137, acc: 100.0, f1: 100.0, r: 0.6887899193777017
06/02/2019 09:28:23 step: 9750, epoch: 295, batch: 14, loss: 0.03767148405313492, acc: 98.4375, f1: 98.3201581027668, r: 0.7859007134083339
06/02/2019 09:28:23 step: 9755, epoch: 295, batch: 19, loss: 0.007401186972856522, acc: 100.0, f1: 100.0, r: 0.7732313474000106
06/02/2019 09:28:24 step: 9760, epoch: 295, batch: 24, loss: 0.045412275940179825, acc: 96.875, f1: 93.64586414275855, r: 0.7098469500383625
06/02/2019 09:28:25 step: 9765, epoch: 295, batch: 29, loss: 0.1011221706867218, acc: 96.875, f1: 97.77204502814259, r: 0.7689150244182746
06/02/2019 09:28:25 *** evaluating ***
06/02/2019 09:28:25 step: 296, epoch: 295, acc: 61.111111111111114, f1: 25.30115796750778, r: 0.3799084086619936
06/02/2019 09:28:25 *** epoch: 297 ***
06/02/2019 09:28:25 *** training ***
06/02/2019 09:28:26 step: 9773, epoch: 296, batch: 4, loss: 0.006756499409675598, acc: 100.0, f1: 100.0, r: 0.6903022377563647
06/02/2019 09:28:27 step: 9778, epoch: 296, batch: 9, loss: 0.02844616398215294, acc: 98.4375, f1: 98.72721018231854, r: 0.6573466504265281
06/02/2019 09:28:27 step: 9783, epoch: 296, batch: 14, loss: 0.025400662794709206, acc: 98.4375, f1: 99.28337428337429, r: 0.7615461780303432
06/02/2019 09:28:28 step: 9788, epoch: 296, batch: 19, loss: 0.03462817519903183, acc: 100.0, f1: 100.0, r: 0.7960415316257654
06/02/2019 09:28:29 step: 9793, epoch: 296, batch: 24, loss: 0.0071164146065711975, acc: 100.0, f1: 100.0, r: 0.7277081486832806
06/02/2019 09:28:29 step: 9798, epoch: 296, batch: 29, loss: 0.006066121160984039, acc: 100.0, f1: 100.0, r: 0.6819500511133865
06/02/2019 09:28:30 *** evaluating ***
06/02/2019 09:28:30 step: 297, epoch: 296, acc: 61.111111111111114, f1: 25.27929332666362, r: 0.3773036531760455
06/02/2019 09:28:30 *** epoch: 298 ***
06/02/2019 09:28:30 *** training ***
06/02/2019 09:28:31 step: 9806, epoch: 297, batch: 4, loss: 0.005175376310944557, acc: 100.0, f1: 100.0, r: 0.7514390880472019
06/02/2019 09:28:31 step: 9811, epoch: 297, batch: 9, loss: 0.008542457595467567, acc: 100.0, f1: 100.0, r: 0.8142734210295742
06/02/2019 09:28:32 step: 9816, epoch: 297, batch: 14, loss: 0.01869276538491249, acc: 100.0, f1: 100.0, r: 0.6674817114913402
06/02/2019 09:28:33 step: 9821, epoch: 297, batch: 19, loss: 0.004713218659162521, acc: 100.0, f1: 100.0, r: 0.689668962130455
06/02/2019 09:28:33 step: 9826, epoch: 297, batch: 24, loss: 0.0032445546239614487, acc: 100.0, f1: 100.0, r: 0.7225785981568611
06/02/2019 09:28:34 step: 9831, epoch: 297, batch: 29, loss: 0.04355566203594208, acc: 96.875, f1: 97.25819714256347, r: 0.7541719934811316
06/02/2019 09:28:34 *** evaluating ***
06/02/2019 09:28:35 step: 298, epoch: 297, acc: 60.68376068376068, f1: 25.648458337350178, r: 0.3709703958840319
06/02/2019 09:28:35 *** epoch: 299 ***
06/02/2019 09:28:35 *** training ***
06/02/2019 09:28:35 step: 9839, epoch: 298, batch: 4, loss: 0.0039030276238918304, acc: 100.0, f1: 100.0, r: 0.8004449855314266
06/02/2019 09:28:36 step: 9844, epoch: 298, batch: 9, loss: 0.03422557935118675, acc: 100.0, f1: 100.0, r: 0.7163613702361309
06/02/2019 09:28:37 step: 9849, epoch: 298, batch: 14, loss: 0.0074767302721738815, acc: 100.0, f1: 100.0, r: 0.790574235343937
06/02/2019 09:28:38 step: 9854, epoch: 298, batch: 19, loss: 0.00959625095129013, acc: 100.0, f1: 100.0, r: 0.7567329367160858
06/02/2019 09:28:38 step: 9859, epoch: 298, batch: 24, loss: 0.00937461107969284, acc: 100.0, f1: 100.0, r: 0.7471950368717781
06/02/2019 09:28:39 step: 9864, epoch: 298, batch: 29, loss: 0.01648644171655178, acc: 100.0, f1: 100.0, r: 0.7930925187687374
06/02/2019 09:28:39 *** evaluating ***
06/02/2019 09:28:39 step: 299, epoch: 298, acc: 61.53846153846154, f1: 26.205004033071766, r: 0.3807657711728293
06/02/2019 09:28:39 *** epoch: 300 ***
06/02/2019 09:28:39 *** training ***
06/02/2019 09:28:40 step: 9872, epoch: 299, batch: 4, loss: 0.007977666333317757, acc: 100.0, f1: 100.0, r: 0.640010634664399
06/02/2019 09:28:41 step: 9877, epoch: 299, batch: 9, loss: 0.01044636219739914, acc: 100.0, f1: 100.0, r: 0.7909578088057394
06/02/2019 09:28:41 step: 9882, epoch: 299, batch: 14, loss: 0.035078320652246475, acc: 98.4375, f1: 98.97828863346105, r: 0.6939693056527083
06/02/2019 09:28:42 step: 9887, epoch: 299, batch: 19, loss: 0.003613259643316269, acc: 100.0, f1: 100.0, r: 0.8207941160837854
06/02/2019 09:28:43 step: 9892, epoch: 299, batch: 24, loss: 0.009834205731749535, acc: 100.0, f1: 100.0, r: 0.7474237431420149
06/02/2019 09:28:43 step: 9897, epoch: 299, batch: 29, loss: 0.014094803482294083, acc: 100.0, f1: 100.0, r: 0.8054290182936128
06/02/2019 09:28:44 *** evaluating ***
06/02/2019 09:28:44 step: 300, epoch: 299, acc: 61.111111111111114, f1: 26.054957562152158, r: 0.3789432599851398
06/02/2019 09:28:44 
*** Best acc model ***
epoch: 32
acc: 64.1025641025641
f1: 29.688799579504625
corr: 0.37966680553415677
06/02/2019 09:28:44 Loading Test Data
06/02/2019 09:28:44 load data from data/word2vec_temp/test_text.npy, data/word2vec_temp/test_label.npy, training: False
06/02/2019 09:29:08 loaded. total len: 2228
06/02/2019 09:29:08 Test: length: 2228, total batch: 35, batch size: 64
06/02/2019 09:29:09 
*** Test Result ***
acc: 61.111111111111114
f1: 26.054957562152158
corr: 0.3789432599851398
