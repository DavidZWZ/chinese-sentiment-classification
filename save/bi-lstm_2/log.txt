06/02/2019 09:14:08 {'input_path': 'data/word2vec_temp', 'output_path': 'save/bi-lstm_2', 'gpu': True, 'seed': 20000125, 'display_per_batch': 5, 'optimizer': 'adagrad', 'lr': 0.01, 'lr_decay': 0, 'weight_decay': 0.0001, 'momentum': 0.985, 'num_epochs': 300, 'batch_size': 64, 'num_labels': 8, 'embedding_size': 300, 'type': 'rnn', 'rnn': {'type': 'lstm', 'bidirectional': True, 'rnn_hidden_size': 256, 'mlp_hidden_size': 512, 'dropout': 0.9, 'p_coefficient': 0.3, 'num_layers': 1, 'param_da': 350, 'param_r': 30, 'loss': 'cross_entropy'}}
06/02/2019 09:14:08 Loading Train Data
06/02/2019 09:14:08 load data from data/word2vec_temp/train_text.npy, data/word2vec_temp/train_label.npy, training: True
06/02/2019 09:14:30 loaded. total len: 2342
06/02/2019 09:14:30 Train: length: 2108, total batch: 33, batch size: 64
06/02/2019 09:14:30 Dev: length: 234, total batch: 4, batch size: 64
06/02/2019 09:14:30 Loading model rnn
06/02/2019 09:14:37 *** epoch: 1 ***
06/02/2019 09:14:37 *** training ***
06/02/2019 09:14:39 step: 5, epoch: 0, batch: 4, loss: 7.096730709075928, acc: 21.875, f1: 15.284062342885873, r: 0.058338036884096674
06/02/2019 09:14:40 step: 10, epoch: 0, batch: 9, loss: 4.344412803649902, acc: 23.4375, f1: 14.013424518743667, r: 0.02143811779725434
06/02/2019 09:14:41 step: 15, epoch: 0, batch: 14, loss: 3.232423782348633, acc: 42.1875, f1: 14.948901935203306, r: 0.07478686225787576
06/02/2019 09:14:42 step: 20, epoch: 0, batch: 19, loss: 2.760165214538574, acc: 35.9375, f1: 14.523809523809522, r: 0.06492111989322828
06/02/2019 09:14:43 step: 25, epoch: 0, batch: 24, loss: 3.0536506175994873, acc: 28.125, f1: 10.212550607287449, r: 0.0073970942687177
06/02/2019 09:14:44 step: 30, epoch: 0, batch: 29, loss: 3.097132682800293, acc: 17.1875, f1: 6.048780487804878, r: 0.024358288960771596
06/02/2019 09:14:45 *** evaluating ***
06/02/2019 09:14:45 step: 1, epoch: 0, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.2564344165531536
06/02/2019 09:14:45 *** epoch: 2 ***
06/02/2019 09:14:45 *** training ***
06/02/2019 09:14:47 step: 38, epoch: 1, batch: 4, loss: 2.698295831680298, acc: 32.8125, f1: 10.370370370370372, r: -0.02078507009753442
06/02/2019 09:14:48 step: 43, epoch: 1, batch: 9, loss: 2.802912473678589, acc: 32.8125, f1: 11.292517006802722, r: -0.0587449851070957
06/02/2019 09:14:49 step: 48, epoch: 1, batch: 14, loss: 2.4969091415405273, acc: 29.6875, f1: 8.63970588235294, r: 0.09953574656760955
06/02/2019 09:14:50 step: 53, epoch: 1, batch: 19, loss: 2.4508728981018066, acc: 31.25, f1: 8.957816377171216, r: 0.01207629946973104
06/02/2019 09:14:51 step: 58, epoch: 1, batch: 24, loss: 2.212679862976074, acc: 46.875, f1: 13.605442176870747, r: 0.08486156729239518
06/02/2019 09:14:52 step: 63, epoch: 1, batch: 29, loss: 2.4175498485565186, acc: 28.125, f1: 7.562785388127854, r: 0.07952915426184835
06/02/2019 09:14:53 *** evaluating ***
06/02/2019 09:14:53 step: 2, epoch: 1, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.2613367388213634
06/02/2019 09:14:53 *** epoch: 3 ***
06/02/2019 09:14:53 *** training ***
06/02/2019 09:14:55 step: 71, epoch: 2, batch: 4, loss: 2.327849864959717, acc: 26.5625, f1: 8.461538461538462, r: 0.04346048059079202
06/02/2019 09:14:56 step: 76, epoch: 2, batch: 9, loss: 2.431696891784668, acc: 40.625, f1: 13.431824342640281, r: 0.07893321327924459
06/02/2019 09:14:57 step: 81, epoch: 2, batch: 14, loss: 2.104339122772217, acc: 37.5, f1: 11.832611832611832, r: 0.09012730269379934
06/02/2019 09:14:58 step: 86, epoch: 2, batch: 19, loss: 2.167792320251465, acc: 35.9375, f1: 9.926470588235295, r: 0.11189820548095963
06/02/2019 09:14:59 step: 91, epoch: 2, batch: 24, loss: 2.1070456504821777, acc: 34.375, f1: 8.561776061776062, r: 0.04692322350901558
06/02/2019 09:15:00 step: 96, epoch: 2, batch: 29, loss: 2.199039936065674, acc: 42.1875, f1: 10.701956271576526, r: 0.10520931777385156
06/02/2019 09:15:01 *** evaluating ***
06/02/2019 09:15:01 step: 3, epoch: 2, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.20810443003380993
06/02/2019 09:15:01 *** epoch: 4 ***
06/02/2019 09:15:01 *** training ***
06/02/2019 09:15:02 step: 104, epoch: 3, batch: 4, loss: 2.046248197555542, acc: 45.3125, f1: 11.58700980392157, r: 0.1293518757481783
06/02/2019 09:15:03 step: 109, epoch: 3, batch: 9, loss: 2.21671724319458, acc: 42.1875, f1: 11.317254174397032, r: 0.08248947024129827
06/02/2019 09:15:04 step: 114, epoch: 3, batch: 14, loss: 2.2524125576019287, acc: 29.6875, f1: 8.583433373349338, r: 0.0825946577167753
06/02/2019 09:15:05 step: 119, epoch: 3, batch: 19, loss: 2.0782508850097656, acc: 39.0625, f1: 7.621951219512195, r: 0.04482879234256583
06/02/2019 09:15:07 step: 124, epoch: 3, batch: 24, loss: 2.2137513160705566, acc: 42.1875, f1: 14.683775128980608, r: 0.1097902413653746
06/02/2019 09:15:08 step: 129, epoch: 3, batch: 29, loss: 2.1170668601989746, acc: 34.375, f1: 13.206349206349206, r: 0.17702042561843584
06/02/2019 09:15:09 *** evaluating ***
06/02/2019 09:15:09 step: 4, epoch: 3, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.27238723497873724
06/02/2019 09:15:09 *** epoch: 5 ***
06/02/2019 09:15:09 *** training ***
06/02/2019 09:15:10 step: 137, epoch: 4, batch: 4, loss: 2.005096912384033, acc: 45.3125, f1: 12.557773109243698, r: 0.1258290642814873
06/02/2019 09:15:11 step: 142, epoch: 4, batch: 9, loss: 2.0972466468811035, acc: 40.625, f1: 9.690402476780186, r: 0.13980140188154644
06/02/2019 09:15:12 step: 147, epoch: 4, batch: 14, loss: 2.2542641162872314, acc: 39.0625, f1: 10.997732426303854, r: 0.06568018343480578
06/02/2019 09:15:13 step: 152, epoch: 4, batch: 19, loss: 2.022855043411255, acc: 37.5, f1: 7.973421926910299, r: 0.03994040700467912
06/02/2019 09:15:14 step: 157, epoch: 4, batch: 24, loss: 2.013852834701538, acc: 53.125, f1: 10.014727540500736, r: 0.11035615668168447
06/02/2019 09:15:16 step: 162, epoch: 4, batch: 29, loss: 1.980499029159546, acc: 46.875, f1: 8.333333333333332, r: 0.1566749380357775
06/02/2019 09:15:16 *** evaluating ***
06/02/2019 09:15:17 step: 5, epoch: 4, acc: 46.15384615384615, f1: 9.250915235145028, r: 0.27510290932373455
06/02/2019 09:15:17 *** epoch: 6 ***
06/02/2019 09:15:17 *** training ***
06/02/2019 09:15:18 step: 170, epoch: 5, batch: 4, loss: 2.0077691078186035, acc: 43.75, f1: 9.681697612732096, r: 0.10788259471003142
06/02/2019 09:15:19 step: 175, epoch: 5, batch: 9, loss: 2.3203272819519043, acc: 32.8125, f1: 9.265633964429146, r: 0.09650736618459727
06/02/2019 09:15:20 step: 180, epoch: 5, batch: 14, loss: 1.985226035118103, acc: 37.5, f1: 9.727891156462585, r: 0.12993000141817215
06/02/2019 09:15:21 step: 185, epoch: 5, batch: 19, loss: 2.159318447113037, acc: 35.9375, f1: 8.918296892980438, r: 0.08067504954599017
06/02/2019 09:15:22 step: 190, epoch: 5, batch: 24, loss: 1.9630136489868164, acc: 46.875, f1: 11.904761904761905, r: 0.11765305482051513
06/02/2019 09:15:24 step: 195, epoch: 5, batch: 29, loss: 1.9754376411437988, acc: 42.1875, f1: 11.199095022624434, r: 0.12472670826975588
06/02/2019 09:15:24 *** evaluating ***
06/02/2019 09:15:25 step: 6, epoch: 5, acc: 45.72649572649573, f1: 8.797359357060849, r: 0.27558264898474594
06/02/2019 09:15:25 *** epoch: 7 ***
06/02/2019 09:15:25 *** training ***
06/02/2019 09:15:26 step: 203, epoch: 6, batch: 4, loss: 1.9680222272872925, acc: 32.8125, f1: 7.743506493506493, r: 0.13743621211145168
06/02/2019 09:15:27 step: 208, epoch: 6, batch: 9, loss: 1.8580466508865356, acc: 45.3125, f1: 13.831488708335998, r: 0.15448145758123183
06/02/2019 09:15:28 step: 213, epoch: 6, batch: 14, loss: 2.1015491485595703, acc: 42.1875, f1: 12.873647533467816, r: 0.06164149440734177
06/02/2019 09:15:29 step: 218, epoch: 6, batch: 19, loss: 2.1940793991088867, acc: 28.125, f1: 10.465661871573912, r: 0.1265383413646605
06/02/2019 09:15:30 step: 223, epoch: 6, batch: 24, loss: 2.065368890762329, acc: 28.125, f1: 10.47822113920195, r: 0.1001471880388814
06/02/2019 09:15:31 step: 228, epoch: 6, batch: 29, loss: 1.8797411918640137, acc: 50.0, f1: 15.020505809979493, r: 0.12182225531208851
06/02/2019 09:15:32 *** evaluating ***
06/02/2019 09:15:32 step: 7, epoch: 6, acc: 53.41880341880342, f1: 15.040048277375467, r: 0.2756798312185207
06/02/2019 09:15:32 *** epoch: 8 ***
06/02/2019 09:15:32 *** training ***
06/02/2019 09:15:33 step: 236, epoch: 7, batch: 4, loss: 2.0166802406311035, acc: 43.75, f1: 11.724137931034484, r: -0.0362253037583226
06/02/2019 09:15:34 step: 241, epoch: 7, batch: 9, loss: 1.8408684730529785, acc: 42.1875, f1: 12.751322751322752, r: 0.16811266737114097
06/02/2019 09:15:36 step: 246, epoch: 7, batch: 14, loss: 1.7736413478851318, acc: 42.1875, f1: 10.719936708860759, r: 0.12492175119092166
06/02/2019 09:15:37 step: 251, epoch: 7, batch: 19, loss: 1.9715794324874878, acc: 34.375, f1: 7.952069716775599, r: 0.1500054891899413
06/02/2019 09:15:38 step: 256, epoch: 7, batch: 24, loss: 1.9728055000305176, acc: 32.8125, f1: 10.104166666666666, r: 0.16589905252149478
06/02/2019 09:15:39 step: 261, epoch: 7, batch: 29, loss: 1.9875539541244507, acc: 37.5, f1: 12.809568271753143, r: 0.16096338386704412
06/02/2019 09:15:40 *** evaluating ***
06/02/2019 09:15:40 step: 8, epoch: 7, acc: 52.56410256410257, f1: 14.531302031302031, r: 0.2964028063804244
06/02/2019 09:15:40 *** epoch: 9 ***
06/02/2019 09:15:40 *** training ***
06/02/2019 09:15:41 step: 269, epoch: 8, batch: 4, loss: 1.796495795249939, acc: 35.9375, f1: 9.545855379188714, r: 0.1760333583500263
06/02/2019 09:15:43 step: 274, epoch: 8, batch: 9, loss: 1.934346318244934, acc: 35.9375, f1: 11.083743842364534, r: 0.05268721602408841
06/02/2019 09:15:44 step: 279, epoch: 8, batch: 14, loss: 1.8845410346984863, acc: 43.75, f1: 14.411976911976915, r: 0.15143303227772176
06/02/2019 09:15:45 step: 284, epoch: 8, batch: 19, loss: 1.8811285495758057, acc: 39.0625, f1: 10.3125, r: 0.12276233144343464
06/02/2019 09:15:46 step: 289, epoch: 8, batch: 24, loss: 1.8547155857086182, acc: 39.0625, f1: 12.276080084299263, r: 0.202447601562545
06/02/2019 09:15:47 step: 294, epoch: 8, batch: 29, loss: 1.856915831565857, acc: 48.4375, f1: 14.285714285714285, r: 0.21206005517148457
06/02/2019 09:15:48 *** evaluating ***
06/02/2019 09:15:48 step: 9, epoch: 8, acc: 55.12820512820513, f1: 15.854320451584888, r: 0.2893289330905897
06/02/2019 09:15:48 *** epoch: 10 ***
06/02/2019 09:15:48 *** training ***
06/02/2019 09:15:49 step: 302, epoch: 9, batch: 4, loss: 1.6918803453445435, acc: 51.5625, f1: 13.761369716425897, r: 0.16321238447495925
06/02/2019 09:15:50 step: 307, epoch: 9, batch: 9, loss: 1.9384348392486572, acc: 32.8125, f1: 9.652076318742985, r: 0.16092506170231485
06/02/2019 09:15:51 step: 312, epoch: 9, batch: 14, loss: 1.7277107238769531, acc: 40.625, f1: 13.221935360489576, r: 0.24899998837149398
06/02/2019 09:15:53 step: 317, epoch: 9, batch: 19, loss: 1.9264501333236694, acc: 34.375, f1: 14.930359495576887, r: 0.17478909529170827
06/02/2019 09:15:54 step: 322, epoch: 9, batch: 24, loss: 1.8967070579528809, acc: 39.0625, f1: 13.142857142857142, r: 0.11244554931926763
06/02/2019 09:15:55 step: 327, epoch: 9, batch: 29, loss: 1.9020495414733887, acc: 43.75, f1: 11.348684210526315, r: 0.1938806342085316
06/02/2019 09:15:56 *** evaluating ***
06/02/2019 09:15:56 step: 10, epoch: 9, acc: 55.12820512820513, f1: 15.641534391534389, r: 0.2881819627995396
06/02/2019 09:15:56 *** epoch: 11 ***
06/02/2019 09:15:56 *** training ***
06/02/2019 09:15:57 step: 335, epoch: 10, batch: 4, loss: 1.8357470035552979, acc: 39.0625, f1: 9.151992585727525, r: 0.2111780739204988
06/02/2019 09:15:59 step: 340, epoch: 10, batch: 9, loss: 1.8528547286987305, acc: 29.6875, f1: 8.243243243243244, r: 0.1362162144173061
06/02/2019 09:16:00 step: 345, epoch: 10, batch: 14, loss: 1.8095182180404663, acc: 43.75, f1: 20.827664399092967, r: 0.21863108508915122
06/02/2019 09:16:01 step: 350, epoch: 10, batch: 19, loss: 1.589450716972351, acc: 57.8125, f1: 16.971136280599193, r: 0.1606601421870251
06/02/2019 09:16:02 step: 355, epoch: 10, batch: 24, loss: 2.034041166305542, acc: 34.375, f1: 9.125085440874914, r: 0.13210476491336057
06/02/2019 09:16:03 step: 360, epoch: 10, batch: 29, loss: 1.732311725616455, acc: 34.375, f1: 12.747707778763678, r: 0.1921967706725571
06/02/2019 09:16:04 *** evaluating ***
06/02/2019 09:16:04 step: 11, epoch: 10, acc: 54.700854700854705, f1: 15.72799332498957, r: 0.30501486245226134
06/02/2019 09:16:04 *** epoch: 12 ***
06/02/2019 09:16:04 *** training ***
06/02/2019 09:16:05 step: 368, epoch: 11, batch: 4, loss: 1.8761093616485596, acc: 35.9375, f1: 12.40162037037037, r: 0.2010424299333479
06/02/2019 09:16:06 step: 373, epoch: 11, batch: 9, loss: 2.0465149879455566, acc: 34.375, f1: 15.50703614779245, r: 0.18888779294235367
06/02/2019 09:16:07 step: 378, epoch: 11, batch: 14, loss: 1.8130475282669067, acc: 42.1875, f1: 14.657251789604734, r: 0.24993813228716844
06/02/2019 09:16:09 step: 383, epoch: 11, batch: 19, loss: 1.843679666519165, acc: 35.9375, f1: 14.70188075230092, r: 0.23547897032804468
06/02/2019 09:16:10 step: 388, epoch: 11, batch: 24, loss: 1.7825459241867065, acc: 37.5, f1: 18.53676336434957, r: 0.1732701098987381
06/02/2019 09:16:11 step: 393, epoch: 11, batch: 29, loss: 1.7287213802337646, acc: 46.875, f1: 13.565621370499418, r: 0.15622857437790905
06/02/2019 09:16:11 *** evaluating ***
06/02/2019 09:16:12 step: 12, epoch: 11, acc: 56.837606837606835, f1: 16.47164026571283, r: 0.30716564513808636
06/02/2019 09:16:12 *** epoch: 13 ***
06/02/2019 09:16:12 *** training ***
06/02/2019 09:16:13 step: 401, epoch: 12, batch: 4, loss: 1.8021183013916016, acc: 34.375, f1: 8.223684210526315, r: 0.10182356167699962
06/02/2019 09:16:14 step: 406, epoch: 12, batch: 9, loss: 1.7721757888793945, acc: 37.5, f1: 18.57434640522876, r: 0.15123718177818618
06/02/2019 09:16:15 step: 411, epoch: 12, batch: 14, loss: 1.9867877960205078, acc: 35.9375, f1: 12.095238095238095, r: 0.1810231992062283
06/02/2019 09:16:16 step: 416, epoch: 12, batch: 19, loss: 1.7096234560012817, acc: 40.625, f1: 15.683621933621936, r: 0.20439225473804368
06/02/2019 09:16:17 step: 421, epoch: 12, batch: 24, loss: 1.7497016191482544, acc: 42.1875, f1: 13.044507575757578, r: 0.2357274141188878
06/02/2019 09:16:19 step: 426, epoch: 12, batch: 29, loss: 1.5983940362930298, acc: 51.5625, f1: 18.233740272720784, r: 0.26825439548232416
06/02/2019 09:16:19 *** evaluating ***
06/02/2019 09:16:20 step: 13, epoch: 12, acc: 53.41880341880342, f1: 15.152996090321553, r: 0.3107482131517018
06/02/2019 09:16:20 *** epoch: 14 ***
06/02/2019 09:16:20 *** training ***
06/02/2019 09:16:21 step: 434, epoch: 13, batch: 4, loss: 1.5082367658615112, acc: 54.6875, f1: 18.905021173623716, r: 0.24903036733442974
06/02/2019 09:16:22 step: 439, epoch: 13, batch: 9, loss: 1.8876659870147705, acc: 34.375, f1: 14.641574851989839, r: 0.1610568742653208
06/02/2019 09:16:23 step: 444, epoch: 13, batch: 14, loss: 1.633856177330017, acc: 59.375, f1: 23.077230892356944, r: 0.262830265312013
06/02/2019 09:16:24 step: 449, epoch: 13, batch: 19, loss: 1.5826330184936523, acc: 53.125, f1: 23.477564102564102, r: 0.1842611945820786
06/02/2019 09:16:25 step: 454, epoch: 13, batch: 24, loss: 1.9022661447525024, acc: 32.8125, f1: 9.156785243741766, r: 0.10626104597869754
06/02/2019 09:16:26 step: 459, epoch: 13, batch: 29, loss: 1.735687255859375, acc: 35.9375, f1: 13.965997770345595, r: 0.28565167275756
06/02/2019 09:16:27 *** evaluating ***
06/02/2019 09:16:27 step: 14, epoch: 13, acc: 55.55555555555556, f1: 16.18288144603934, r: 0.31330818101924063
06/02/2019 09:16:27 *** epoch: 15 ***
06/02/2019 09:16:27 *** training ***
06/02/2019 09:16:28 step: 467, epoch: 14, batch: 4, loss: 1.6948148012161255, acc: 43.75, f1: 21.89822652882341, r: 0.2155251430068667
06/02/2019 09:16:29 step: 472, epoch: 14, batch: 9, loss: 1.610960841178894, acc: 50.0, f1: 16.498756218905474, r: 0.28096163010927205
06/02/2019 09:16:31 step: 477, epoch: 14, batch: 14, loss: 1.6055785417556763, acc: 53.125, f1: 21.42943301722603, r: 0.17901014757378075
06/02/2019 09:16:32 step: 482, epoch: 14, batch: 19, loss: 1.7796169519424438, acc: 40.625, f1: 13.713683576697274, r: 0.2015116272783472
06/02/2019 09:16:33 step: 487, epoch: 14, batch: 24, loss: 1.705940842628479, acc: 43.75, f1: 13.88888888888889, r: 0.2453929915789051
06/02/2019 09:16:34 step: 492, epoch: 14, batch: 29, loss: 1.8105542659759521, acc: 46.875, f1: 14.912280701754385, r: 0.23837508687826697
06/02/2019 09:16:34 *** evaluating ***
06/02/2019 09:16:35 step: 15, epoch: 14, acc: 56.41025641025641, f1: 16.226105137395464, r: 0.3161055400957465
06/02/2019 09:16:35 *** epoch: 16 ***
06/02/2019 09:16:35 *** training ***
06/02/2019 09:16:36 step: 500, epoch: 15, batch: 4, loss: 1.569676160812378, acc: 46.875, f1: 16.534322820037104, r: 0.20826835460474274
06/02/2019 09:16:37 step: 505, epoch: 15, batch: 9, loss: 1.843908429145813, acc: 42.1875, f1: 18.8203214695752, r: 0.17636504854162033
06/02/2019 09:16:38 step: 510, epoch: 15, batch: 14, loss: 1.7886775732040405, acc: 35.9375, f1: 11.898839137645108, r: 0.18505468145826426
06/02/2019 09:16:39 step: 515, epoch: 15, batch: 19, loss: 1.4616169929504395, acc: 56.25, f1: 24.38095238095238, r: 0.22411466085237833
06/02/2019 09:16:40 step: 520, epoch: 15, batch: 24, loss: 1.7401584386825562, acc: 43.75, f1: 13.860358958398175, r: 0.19234378018473297
06/02/2019 09:16:41 step: 525, epoch: 15, batch: 29, loss: 1.5401923656463623, acc: 51.5625, f1: 17.032967032967033, r: 0.24951136573179292
06/02/2019 09:16:42 *** evaluating ***
06/02/2019 09:16:42 step: 16, epoch: 15, acc: 55.55555555555556, f1: 16.087616087616084, r: 0.31699789833478254
06/02/2019 09:16:42 *** epoch: 17 ***
06/02/2019 09:16:42 *** training ***
06/02/2019 09:16:43 step: 533, epoch: 16, batch: 4, loss: 1.526663899421692, acc: 48.4375, f1: 16.666666666666664, r: 0.30324673138343156
06/02/2019 09:16:44 step: 538, epoch: 16, batch: 9, loss: 1.7471588850021362, acc: 45.3125, f1: 20.82940868655154, r: 0.2017725915528224
06/02/2019 09:16:45 step: 543, epoch: 16, batch: 14, loss: 1.5081700086593628, acc: 46.875, f1: 14.754550468836184, r: 0.2565447003036219
06/02/2019 09:16:46 step: 548, epoch: 16, batch: 19, loss: 1.5259979963302612, acc: 45.3125, f1: 16.287021801727686, r: 0.22636630685840592
06/02/2019 09:16:47 step: 553, epoch: 16, batch: 24, loss: 1.8206326961517334, acc: 43.75, f1: 14.257327372081468, r: 0.23521819457808069
06/02/2019 09:16:49 step: 558, epoch: 16, batch: 29, loss: 1.8584781885147095, acc: 45.3125, f1: 14.919354838709678, r: 0.2452989366738413
06/02/2019 09:16:49 *** evaluating ***
06/02/2019 09:16:50 step: 17, epoch: 16, acc: 57.692307692307686, f1: 17.45830107110595, r: 0.31714585540453516
06/02/2019 09:16:50 *** epoch: 18 ***
06/02/2019 09:16:50 *** training ***
06/02/2019 09:16:51 step: 566, epoch: 17, batch: 4, loss: 1.6518304347991943, acc: 46.875, f1: 14.285714285714285, r: 0.2792126269256863
06/02/2019 09:16:52 step: 571, epoch: 17, batch: 9, loss: 1.5857648849487305, acc: 45.3125, f1: 12.561643835616437, r: 0.2781161493650818
06/02/2019 09:16:53 step: 576, epoch: 17, batch: 14, loss: 1.5206965208053589, acc: 51.5625, f1: 33.2558312840003, r: 0.259823206712254
06/02/2019 09:16:54 step: 581, epoch: 17, batch: 19, loss: 1.4902926683425903, acc: 48.4375, f1: 15.929336703984593, r: 0.2895998965828094
06/02/2019 09:16:55 step: 586, epoch: 17, batch: 24, loss: 1.5199320316314697, acc: 45.3125, f1: 15.346746575342465, r: 0.27886604523927055
06/02/2019 09:16:57 step: 591, epoch: 17, batch: 29, loss: 1.6625264883041382, acc: 43.75, f1: 14.476190476190476, r: 0.258792881709966
06/02/2019 09:16:57 *** evaluating ***
06/02/2019 09:16:58 step: 18, epoch: 17, acc: 57.26495726495726, f1: 16.68269230769231, r: 0.3070090162452372
06/02/2019 09:16:58 *** epoch: 19 ***
06/02/2019 09:16:58 *** training ***
06/02/2019 09:16:59 step: 599, epoch: 18, batch: 4, loss: 1.5982414484024048, acc: 39.0625, f1: 14.222747497219135, r: 0.29012432152331336
06/02/2019 09:17:00 step: 604, epoch: 18, batch: 9, loss: 1.5289115905761719, acc: 43.75, f1: 15.318980414746541, r: 0.2960501283853165
06/02/2019 09:17:01 step: 609, epoch: 18, batch: 14, loss: 1.7399916648864746, acc: 37.5, f1: 13.249269005847955, r: 0.26502823420038146
06/02/2019 09:17:02 step: 614, epoch: 18, batch: 19, loss: 1.5378930568695068, acc: 53.125, f1: 16.112883218146376, r: 0.1773075356922644
06/02/2019 09:17:04 step: 619, epoch: 18, batch: 24, loss: 1.6377254724502563, acc: 42.1875, f1: 13.375538793103448, r: 0.27280537674340877
06/02/2019 09:17:05 step: 624, epoch: 18, batch: 29, loss: 1.490548849105835, acc: 45.3125, f1: 15.206866197183096, r: 0.29669181132658523
06/02/2019 09:17:05 *** evaluating ***
06/02/2019 09:17:06 step: 19, epoch: 18, acc: 56.837606837606835, f1: 16.53694968553459, r: 0.32236115256652864
06/02/2019 09:17:06 *** epoch: 20 ***
06/02/2019 09:17:06 *** training ***
06/02/2019 09:17:07 step: 632, epoch: 19, batch: 4, loss: 1.8059850931167603, acc: 34.375, f1: 11.376488095238097, r: 0.1898648579067786
06/02/2019 09:17:08 step: 637, epoch: 19, batch: 9, loss: 1.543430209159851, acc: 42.1875, f1: 14.03688524590164, r: 0.23520920319255503
06/02/2019 09:17:10 step: 642, epoch: 19, batch: 14, loss: 1.4842256307601929, acc: 46.875, f1: 21.98412698412698, r: 0.282362290274498
06/02/2019 09:17:11 step: 647, epoch: 19, batch: 19, loss: 1.341947078704834, acc: 57.8125, f1: 30.025856496444725, r: 0.2820457040487389
06/02/2019 09:17:12 step: 652, epoch: 19, batch: 24, loss: 1.5490885972976685, acc: 54.6875, f1: 27.063492063492067, r: 0.25748106458952036
06/02/2019 09:17:13 step: 657, epoch: 19, batch: 29, loss: 1.4074506759643555, acc: 57.8125, f1: 22.377926421404688, r: 0.3304790505316577
06/02/2019 09:17:13 *** evaluating ***
06/02/2019 09:17:14 step: 20, epoch: 19, acc: 56.837606837606835, f1: 17.042055555690695, r: 0.3310057963362723
06/02/2019 09:17:14 *** epoch: 21 ***
06/02/2019 09:17:14 *** training ***
06/02/2019 09:17:15 step: 665, epoch: 20, batch: 4, loss: 1.4846482276916504, acc: 48.4375, f1: 19.16264090177134, r: 0.2432079964913386
06/02/2019 09:17:16 step: 670, epoch: 20, batch: 9, loss: 1.4699740409851074, acc: 46.875, f1: 16.885456885456883, r: 0.2993494669469428
06/02/2019 09:17:17 step: 675, epoch: 20, batch: 14, loss: 1.4378846883773804, acc: 46.875, f1: 18.59447004608295, r: 0.2831411682213358
06/02/2019 09:17:18 step: 680, epoch: 20, batch: 19, loss: 1.5963166952133179, acc: 45.3125, f1: 17.972164711295147, r: 0.17921286662023747
06/02/2019 09:17:20 step: 685, epoch: 20, batch: 24, loss: 1.5137704610824585, acc: 45.3125, f1: 15.510356255178129, r: 0.27853928058047855
06/02/2019 09:17:21 step: 690, epoch: 20, batch: 29, loss: 1.5999208688735962, acc: 42.1875, f1: 15.331890331890333, r: 0.3144043904534022
06/02/2019 09:17:21 *** evaluating ***
06/02/2019 09:17:22 step: 21, epoch: 20, acc: 57.26495726495726, f1: 16.777067506639494, r: 0.32707957607520316
06/02/2019 09:17:22 *** epoch: 22 ***
06/02/2019 09:17:22 *** training ***
06/02/2019 09:17:23 step: 698, epoch: 21, batch: 4, loss: 1.42124342918396, acc: 48.4375, f1: 22.664835164835168, r: 0.32334683594065633
06/02/2019 09:17:24 step: 703, epoch: 21, batch: 9, loss: 1.7211765050888062, acc: 43.75, f1: 17.61514441842311, r: 0.25129179976519034
06/02/2019 09:17:25 step: 708, epoch: 21, batch: 14, loss: 1.4881346225738525, acc: 45.3125, f1: 18.271221532091094, r: 0.2164847255260634
06/02/2019 09:17:26 step: 713, epoch: 21, batch: 19, loss: 1.763590931892395, acc: 34.375, f1: 13.04883512544803, r: 0.23996554482691962
06/02/2019 09:17:27 step: 718, epoch: 21, batch: 24, loss: 1.6409891843795776, acc: 37.5, f1: 13.608149727552712, r: 0.2770156734643753
06/02/2019 09:17:29 step: 723, epoch: 21, batch: 29, loss: 1.3338673114776611, acc: 62.5, f1: 24.340569561157803, r: 0.3116898675760671
06/02/2019 09:17:29 *** evaluating ***
06/02/2019 09:17:29 step: 22, epoch: 21, acc: 57.692307692307686, f1: 16.933112251282694, r: 0.3412624689227193
06/02/2019 09:17:29 *** epoch: 23 ***
06/02/2019 09:17:29 *** training ***
06/02/2019 09:17:31 step: 731, epoch: 22, batch: 4, loss: 1.5595062971115112, acc: 48.4375, f1: 20.934886499402626, r: 0.35923408369162413
06/02/2019 09:17:32 step: 736, epoch: 22, batch: 9, loss: 1.5351648330688477, acc: 42.1875, f1: 16.87074829931973, r: 0.18930663539399034
06/02/2019 09:17:33 step: 741, epoch: 22, batch: 14, loss: 1.491276741027832, acc: 42.1875, f1: 14.568168871447556, r: 0.29760388761873235
06/02/2019 09:17:34 step: 746, epoch: 22, batch: 19, loss: 1.5440233945846558, acc: 43.75, f1: 15.09765196401141, r: 0.21000070160230494
06/02/2019 09:17:35 step: 751, epoch: 22, batch: 24, loss: 1.5057307481765747, acc: 37.5, f1: 14.168618266978921, r: 0.236877339163031
06/02/2019 09:17:36 step: 756, epoch: 22, batch: 29, loss: 1.4779478311538696, acc: 45.3125, f1: 17.629513343799058, r: 0.30519977649827296
06/02/2019 09:17:37 *** evaluating ***
06/02/2019 09:17:37 step: 23, epoch: 22, acc: 57.26495726495726, f1: 17.62656282025005, r: 0.337445951179663
06/02/2019 09:17:37 *** epoch: 24 ***
06/02/2019 09:17:37 *** training ***
06/02/2019 09:17:38 step: 764, epoch: 23, batch: 4, loss: 1.4779163599014282, acc: 45.3125, f1: 14.930555555555557, r: 0.31902845990574424
06/02/2019 09:17:39 step: 769, epoch: 23, batch: 9, loss: 1.4117109775543213, acc: 43.75, f1: 20.175438596491226, r: 0.35158242556767044
06/02/2019 09:17:40 step: 774, epoch: 23, batch: 14, loss: 1.3992313146591187, acc: 50.0, f1: 20.876128891103786, r: 0.32659215423171867
06/02/2019 09:17:42 step: 779, epoch: 23, batch: 19, loss: 1.3869684934616089, acc: 46.875, f1: 17.998866213151928, r: 0.2274002390689936
06/02/2019 09:17:43 step: 784, epoch: 23, batch: 24, loss: 1.5777415037155151, acc: 39.0625, f1: 14.324095022624434, r: 0.16979647441276674
06/02/2019 09:17:44 step: 789, epoch: 23, batch: 29, loss: 1.399366021156311, acc: 48.4375, f1: 13.427033492822964, r: 0.3321519865386939
06/02/2019 09:17:45 *** evaluating ***
06/02/2019 09:17:45 step: 24, epoch: 23, acc: 57.692307692307686, f1: 16.81082245752278, r: 0.34283678706843135
06/02/2019 09:17:45 *** epoch: 25 ***
06/02/2019 09:17:45 *** training ***
06/02/2019 09:17:46 step: 797, epoch: 24, batch: 4, loss: 1.4610556364059448, acc: 50.0, f1: 25.238095238095237, r: 0.2889713775623003
06/02/2019 09:17:47 step: 802, epoch: 24, batch: 9, loss: 1.418585181236267, acc: 51.5625, f1: 18.533549783549784, r: 0.3172365953669091
06/02/2019 09:17:48 step: 807, epoch: 24, batch: 14, loss: 1.4225492477416992, acc: 50.0, f1: 14.866434378629497, r: 0.23276410030987485
06/02/2019 09:17:49 step: 812, epoch: 24, batch: 19, loss: 1.4267114400863647, acc: 40.625, f1: 12.797619047619047, r: 0.3288638755822745
06/02/2019 09:17:50 step: 817, epoch: 24, batch: 24, loss: 1.4807997941970825, acc: 48.4375, f1: 19.49194092051235, r: 0.29175711240040736
06/02/2019 09:17:51 step: 822, epoch: 24, batch: 29, loss: 1.388807773590088, acc: 54.6875, f1: 17.38348676639816, r: 0.2979374445080972
06/02/2019 09:17:52 *** evaluating ***
06/02/2019 09:17:52 step: 25, epoch: 24, acc: 57.692307692307686, f1: 17.509781884781887, r: 0.3528057510189067
06/02/2019 09:17:52 *** epoch: 26 ***
06/02/2019 09:17:52 *** training ***
06/02/2019 09:17:53 step: 830, epoch: 25, batch: 4, loss: 1.4021517038345337, acc: 53.125, f1: 14.474358974358974, r: 0.3032888947063864
06/02/2019 09:17:55 step: 835, epoch: 25, batch: 9, loss: 1.3877063989639282, acc: 48.4375, f1: 17.16378859236002, r: 0.3078201363426345
06/02/2019 09:17:56 step: 840, epoch: 25, batch: 14, loss: 1.461753010749817, acc: 56.25, f1: 33.1524197195839, r: 0.3065949281910232
06/02/2019 09:17:57 step: 845, epoch: 25, batch: 19, loss: 1.461776852607727, acc: 48.4375, f1: 24.270541549953318, r: 0.3830880796297998
06/02/2019 09:17:58 step: 850, epoch: 25, batch: 24, loss: 1.505905032157898, acc: 40.625, f1: 13.984497908752545, r: 0.27994208852737756
06/02/2019 09:17:59 step: 855, epoch: 25, batch: 29, loss: 1.3778802156448364, acc: 45.3125, f1: 17.223252937538653, r: 0.3874879854520845
06/02/2019 09:18:00 *** evaluating ***
06/02/2019 09:18:00 step: 26, epoch: 25, acc: 56.837606837606835, f1: 16.622807017543863, r: 0.35315347166500516
06/02/2019 09:18:00 *** epoch: 27 ***
06/02/2019 09:18:00 *** training ***
06/02/2019 09:18:01 step: 863, epoch: 26, batch: 4, loss: 1.457917332649231, acc: 48.4375, f1: 30.484826932195354, r: 0.3597610006300539
06/02/2019 09:18:02 step: 868, epoch: 26, batch: 9, loss: 1.5253986120224, acc: 42.1875, f1: 17.142857142857142, r: 0.20387917131836206
06/02/2019 09:18:04 step: 873, epoch: 26, batch: 14, loss: 1.703689694404602, acc: 48.4375, f1: 14.84375, r: 0.19223483602859956
06/02/2019 09:18:05 step: 878, epoch: 26, batch: 19, loss: 1.2899900674819946, acc: 53.125, f1: 27.51114238798968, r: 0.3688993286406531
06/02/2019 09:18:06 step: 883, epoch: 26, batch: 24, loss: 1.2851793766021729, acc: 56.25, f1: 20.852008501146816, r: 0.2723508577314661
06/02/2019 09:18:07 step: 888, epoch: 26, batch: 29, loss: 1.4659310579299927, acc: 45.3125, f1: 18.994708994708994, r: 0.23902721426334697
06/02/2019 09:18:08 *** evaluating ***
06/02/2019 09:18:08 step: 27, epoch: 26, acc: 58.119658119658126, f1: 19.149080231903305, r: 0.3499654775175009
06/02/2019 09:18:08 *** epoch: 28 ***
06/02/2019 09:18:08 *** training ***
06/02/2019 09:18:09 step: 896, epoch: 27, batch: 4, loss: 1.5923175811767578, acc: 43.75, f1: 20.625092142120003, r: 0.2980537939283256
06/02/2019 09:18:11 step: 901, epoch: 27, batch: 9, loss: 1.7723898887634277, acc: 37.5, f1: 17.127317127317124, r: 0.2608149619247207
06/02/2019 09:18:12 step: 906, epoch: 27, batch: 14, loss: 1.6340886354446411, acc: 48.4375, f1: 24.066576698155647, r: 0.26263881381896964
06/02/2019 09:18:13 step: 911, epoch: 27, batch: 19, loss: 1.3776888847351074, acc: 51.5625, f1: 21.080401725563018, r: 0.35350976393684275
06/02/2019 09:18:14 step: 916, epoch: 27, batch: 24, loss: 1.4776545763015747, acc: 42.1875, f1: 16.564625850340136, r: 0.2205423722388829
06/02/2019 09:18:15 step: 921, epoch: 27, batch: 29, loss: 1.343535304069519, acc: 53.125, f1: 31.161518661518663, r: 0.35035641795655637
06/02/2019 09:18:15 *** evaluating ***
06/02/2019 09:18:16 step: 28, epoch: 27, acc: 58.54700854700855, f1: 18.92221892221892, r: 0.3580931800810154
06/02/2019 09:18:16 *** epoch: 29 ***
06/02/2019 09:18:16 *** training ***
06/02/2019 09:18:17 step: 929, epoch: 28, batch: 4, loss: 1.3422656059265137, acc: 46.875, f1: 17.01982228298018, r: 0.28923348953587036
06/02/2019 09:18:18 step: 934, epoch: 28, batch: 9, loss: 1.4060709476470947, acc: 40.625, f1: 19.594155844155846, r: 0.34568242108461555
06/02/2019 09:18:19 step: 939, epoch: 28, batch: 14, loss: 1.505627155303955, acc: 37.5, f1: 23.699802501645824, r: 0.33373942359349895
06/02/2019 09:18:20 step: 944, epoch: 28, batch: 19, loss: 1.3859362602233887, acc: 46.875, f1: 15.975422427035332, r: 0.23947840293327735
06/02/2019 09:18:21 step: 949, epoch: 28, batch: 24, loss: 1.4712961912155151, acc: 45.3125, f1: 21.71840716361264, r: 0.3047997157587461
06/02/2019 09:18:22 step: 954, epoch: 28, batch: 29, loss: 1.5807868242263794, acc: 40.625, f1: 12.920478536242081, r: 0.29184919646373586
06/02/2019 09:18:23 *** evaluating ***
06/02/2019 09:18:23 step: 29, epoch: 28, acc: 58.119658119658126, f1: 18.107393107393108, r: 0.36526239061174115
06/02/2019 09:18:23 *** epoch: 30 ***
06/02/2019 09:18:23 *** training ***
06/02/2019 09:18:24 step: 962, epoch: 29, batch: 4, loss: 1.4874889850616455, acc: 39.0625, f1: 15.890458747601604, r: 0.2625645487883104
06/02/2019 09:18:25 step: 967, epoch: 29, batch: 9, loss: 1.3388760089874268, acc: 46.875, f1: 17.014607775477337, r: 0.33825911543056164
06/02/2019 09:18:26 step: 972, epoch: 29, batch: 14, loss: 1.3982230424880981, acc: 50.0, f1: 17.251461988304094, r: 0.26069245257860657
06/02/2019 09:18:27 step: 977, epoch: 29, batch: 19, loss: 1.356610655784607, acc: 51.5625, f1: 24.721177944862156, r: 0.34610351367743936
06/02/2019 09:18:29 step: 982, epoch: 29, batch: 24, loss: 1.443914532661438, acc: 48.4375, f1: 16.929271708683473, r: 0.28870140024366825
06/02/2019 09:18:30 step: 987, epoch: 29, batch: 29, loss: 1.2408841848373413, acc: 54.6875, f1: 21.984743942179907, r: 0.40140468493326054
06/02/2019 09:18:30 *** evaluating ***
06/02/2019 09:18:31 step: 30, epoch: 29, acc: 58.54700854700855, f1: 19.62813953404786, r: 0.3628851772982495
06/02/2019 09:18:31 *** epoch: 31 ***
06/02/2019 09:18:31 *** training ***
06/02/2019 09:18:32 step: 995, epoch: 30, batch: 4, loss: 1.3067147731781006, acc: 54.6875, f1: 36.32988827714994, r: 0.2968075582378116
06/02/2019 09:18:33 step: 1000, epoch: 30, batch: 9, loss: 1.3362082242965698, acc: 45.3125, f1: 18.857142857142854, r: 0.217916243002039
06/02/2019 09:18:34 step: 1005, epoch: 30, batch: 14, loss: 1.5579956769943237, acc: 37.5, f1: 18.12891919274898, r: 0.2373971685147577
06/02/2019 09:18:35 step: 1010, epoch: 30, batch: 19, loss: 1.3968991041183472, acc: 45.3125, f1: 18.492258972554538, r: 0.32756710999274946
06/02/2019 09:18:36 step: 1015, epoch: 30, batch: 24, loss: 1.2157119512557983, acc: 56.25, f1: 23.06887667347641, r: 0.3568353516386271
06/02/2019 09:18:38 step: 1020, epoch: 30, batch: 29, loss: 1.4371705055236816, acc: 42.1875, f1: 24.50340136054422, r: 0.33693562904820884
06/02/2019 09:18:38 *** evaluating ***
06/02/2019 09:18:39 step: 31, epoch: 30, acc: 58.54700854700855, f1: 21.05006105006105, r: 0.36338773308544386
06/02/2019 09:18:39 *** epoch: 32 ***
06/02/2019 09:18:39 *** training ***
06/02/2019 09:18:40 step: 1028, epoch: 31, batch: 4, loss: 1.422936201095581, acc: 43.75, f1: 21.537213164251206, r: 0.41268231460186056
06/02/2019 09:18:41 step: 1033, epoch: 31, batch: 9, loss: 1.3036103248596191, acc: 59.375, f1: 22.32947232947233, r: 0.24034272633791345
06/02/2019 09:18:42 step: 1038, epoch: 31, batch: 14, loss: 1.3305519819259644, acc: 56.25, f1: 25.919886565047857, r: 0.3005643077910697
06/02/2019 09:18:43 step: 1043, epoch: 31, batch: 19, loss: 1.202238917350769, acc: 60.9375, f1: 20.396825396825395, r: 0.38719621517015035
06/02/2019 09:18:44 step: 1048, epoch: 31, batch: 24, loss: 1.5853257179260254, acc: 45.3125, f1: 16.45006250269408, r: 0.2777106739615705
06/02/2019 09:18:45 step: 1053, epoch: 31, batch: 29, loss: 1.2714723348617554, acc: 50.0, f1: 20.449134199134196, r: 0.3387562861222862
06/02/2019 09:18:46 *** evaluating ***
06/02/2019 09:18:46 step: 32, epoch: 31, acc: 59.82905982905983, f1: 22.598518281440874, r: 0.37133475486940964
06/02/2019 09:18:46 *** epoch: 33 ***
06/02/2019 09:18:46 *** training ***
06/02/2019 09:18:48 step: 1061, epoch: 32, batch: 4, loss: 1.3342599868774414, acc: 48.4375, f1: 19.180672268907564, r: 0.34972014325338197
06/02/2019 09:18:49 step: 1066, epoch: 32, batch: 9, loss: 1.247926115989685, acc: 57.8125, f1: 28.26530612244898, r: 0.33464402019457234
06/02/2019 09:18:50 step: 1071, epoch: 32, batch: 14, loss: 1.259517788887024, acc: 57.8125, f1: 37.30579277377307, r: 0.3674002605589503
06/02/2019 09:18:51 step: 1076, epoch: 32, batch: 19, loss: 1.445237159729004, acc: 45.3125, f1: 18.0047123015873, r: 0.33199181609682715
06/02/2019 09:18:52 step: 1081, epoch: 32, batch: 24, loss: 1.2101317644119263, acc: 60.9375, f1: 38.17701032241621, r: 0.35529744457202267
06/02/2019 09:18:53 step: 1086, epoch: 32, batch: 29, loss: 1.457781434059143, acc: 42.1875, f1: 14.328984156570362, r: 0.2927425607090928
06/02/2019 09:18:54 *** evaluating ***
06/02/2019 09:18:54 step: 33, epoch: 32, acc: 58.97435897435898, f1: 20.120538345403315, r: 0.368380880472382
06/02/2019 09:18:54 *** epoch: 34 ***
06/02/2019 09:18:54 *** training ***
06/02/2019 09:18:55 step: 1094, epoch: 33, batch: 4, loss: 1.2628449201583862, acc: 54.6875, f1: 28.286564625850342, r: 0.26813899838862004
06/02/2019 09:18:56 step: 1099, epoch: 33, batch: 9, loss: 1.2663135528564453, acc: 48.4375, f1: 14.987496975074613, r: 0.3060020556324221
06/02/2019 09:18:57 step: 1104, epoch: 33, batch: 14, loss: 1.2048465013504028, acc: 51.5625, f1: 17.55712641788591, r: 0.34088046691560897
06/02/2019 09:18:58 step: 1109, epoch: 33, batch: 19, loss: 1.431620478630066, acc: 43.75, f1: 17.085650723025584, r: 0.2943031923499528
06/02/2019 09:19:00 step: 1114, epoch: 33, batch: 24, loss: 1.3210461139678955, acc: 48.4375, f1: 21.201430783810647, r: 0.3568048599279966
06/02/2019 09:19:01 step: 1119, epoch: 33, batch: 29, loss: 1.2902145385742188, acc: 51.5625, f1: 22.247474747474747, r: 0.3390049955246948
06/02/2019 09:19:02 *** evaluating ***
06/02/2019 09:19:02 step: 34, epoch: 33, acc: 60.256410256410255, f1: 22.602200855396955, r: 0.36869055274417617
06/02/2019 09:19:02 *** epoch: 35 ***
06/02/2019 09:19:02 *** training ***
06/02/2019 09:19:03 step: 1127, epoch: 34, batch: 4, loss: 1.443834662437439, acc: 39.0625, f1: 13.258141112618723, r: 0.33891012543663995
06/02/2019 09:19:05 step: 1132, epoch: 34, batch: 9, loss: 1.4165948629379272, acc: 45.3125, f1: 16.373865860167232, r: 0.31102691792595977
06/02/2019 09:19:06 step: 1137, epoch: 34, batch: 14, loss: 1.353431224822998, acc: 50.0, f1: 20.86408333825333, r: 0.30254711440176596
06/02/2019 09:19:07 step: 1142, epoch: 34, batch: 19, loss: 1.4646879434585571, acc: 46.875, f1: 22.42323506254921, r: 0.22981447701111776
06/02/2019 09:19:08 step: 1147, epoch: 34, batch: 24, loss: 1.2754483222961426, acc: 53.125, f1: 25.44796334270018, r: 0.33932090602975273
06/02/2019 09:19:09 step: 1152, epoch: 34, batch: 29, loss: 1.2348062992095947, acc: 59.375, f1: 27.006802721088434, r: 0.35631707215361796
06/02/2019 09:19:10 *** evaluating ***
06/02/2019 09:19:10 step: 35, epoch: 34, acc: 59.82905982905983, f1: 22.791666666666668, r: 0.37641490531763067
06/02/2019 09:19:10 *** epoch: 36 ***
06/02/2019 09:19:10 *** training ***
06/02/2019 09:19:12 step: 1160, epoch: 35, batch: 4, loss: 1.3879549503326416, acc: 45.3125, f1: 24.86394557823129, r: 0.3743300987683538
06/02/2019 09:19:13 step: 1165, epoch: 35, batch: 9, loss: 1.51483154296875, acc: 48.4375, f1: 23.270232592266492, r: 0.26707368183545127
06/02/2019 09:19:14 step: 1170, epoch: 35, batch: 14, loss: 1.3148897886276245, acc: 48.4375, f1: 24.032340208810794, r: 0.4179728292469333
06/02/2019 09:19:15 step: 1175, epoch: 35, batch: 19, loss: 1.454892873764038, acc: 43.75, f1: 14.566486972147349, r: 0.27026392884484596
06/02/2019 09:19:16 step: 1180, epoch: 35, batch: 24, loss: 1.2549684047698975, acc: 53.125, f1: 17.737862495927015, r: 0.3188612683979696
06/02/2019 09:19:17 step: 1185, epoch: 35, batch: 29, loss: 1.2435894012451172, acc: 53.125, f1: 17.32456140350877, r: 0.28013252087264123
06/02/2019 09:19:18 *** evaluating ***
06/02/2019 09:19:18 step: 36, epoch: 35, acc: 59.82905982905983, f1: 22.890049473705993, r: 0.3693596562795283
06/02/2019 09:19:18 *** epoch: 37 ***
06/02/2019 09:19:18 *** training ***
06/02/2019 09:19:19 step: 1193, epoch: 36, batch: 4, loss: 1.3378329277038574, acc: 56.25, f1: 27.07693422196465, r: 0.3553879083453142
06/02/2019 09:19:20 step: 1198, epoch: 36, batch: 9, loss: 1.2571275234222412, acc: 57.8125, f1: 28.453710032657405, r: 0.3022432298267872
06/02/2019 09:19:22 step: 1203, epoch: 36, batch: 14, loss: 1.2784099578857422, acc: 50.0, f1: 15.55183946488294, r: 0.2952963107591849
06/02/2019 09:19:23 step: 1208, epoch: 36, batch: 19, loss: 1.291821002960205, acc: 56.25, f1: 25.168930686172065, r: 0.38975239255196087
06/02/2019 09:19:24 step: 1213, epoch: 36, batch: 24, loss: 1.122357726097107, acc: 62.5, f1: 25.539215686274513, r: 0.4483354281528368
06/02/2019 09:19:25 step: 1218, epoch: 36, batch: 29, loss: 1.2972557544708252, acc: 51.5625, f1: 25.54762186098592, r: 0.39529391120354157
06/02/2019 09:19:26 *** evaluating ***
06/02/2019 09:19:26 step: 37, epoch: 36, acc: 60.68376068376068, f1: 24.750306680049068, r: 0.35653722953299993
06/02/2019 09:19:26 *** epoch: 38 ***
06/02/2019 09:19:26 *** training ***
06/02/2019 09:19:27 step: 1226, epoch: 37, batch: 4, loss: 1.2000418901443481, acc: 56.25, f1: 20.521541950113374, r: 0.2143079668548799
06/02/2019 09:19:28 step: 1231, epoch: 37, batch: 9, loss: 1.3808673620224, acc: 54.6875, f1: 26.626984126984127, r: 0.33642385006122666
06/02/2019 09:19:30 step: 1236, epoch: 37, batch: 14, loss: 1.4678808450698853, acc: 46.875, f1: 19.426406926406926, r: 0.2937327974431724
06/02/2019 09:19:31 step: 1241, epoch: 37, batch: 19, loss: 1.2189186811447144, acc: 56.25, f1: 43.6734693877551, r: 0.4430064642764457
06/02/2019 09:19:32 step: 1246, epoch: 37, batch: 24, loss: 1.345505952835083, acc: 54.6875, f1: 26.65528233151184, r: 0.3866498567388466
06/02/2019 09:19:33 step: 1251, epoch: 37, batch: 29, loss: 1.2242584228515625, acc: 57.8125, f1: 22.8473611666889, r: 0.31364057929431355
06/02/2019 09:19:34 *** evaluating ***
06/02/2019 09:19:34 step: 38, epoch: 37, acc: 59.82905982905983, f1: 23.185096153846153, r: 0.3772531456944591
06/02/2019 09:19:34 *** epoch: 39 ***
06/02/2019 09:19:34 *** training ***
06/02/2019 09:19:35 step: 1259, epoch: 38, batch: 4, loss: 1.2952563762664795, acc: 51.5625, f1: 21.18895067698259, r: 0.3924950809963218
06/02/2019 09:19:37 step: 1264, epoch: 38, batch: 9, loss: 1.3548824787139893, acc: 48.4375, f1: 24.334733893557424, r: 0.3523262539662531
06/02/2019 09:19:38 step: 1269, epoch: 38, batch: 14, loss: 1.2126963138580322, acc: 53.125, f1: 26.269841269841265, r: 0.4526630824043793
06/02/2019 09:19:39 step: 1274, epoch: 38, batch: 19, loss: 1.260907530784607, acc: 51.5625, f1: 18.45238095238095, r: 0.2961311906619996
06/02/2019 09:19:40 step: 1279, epoch: 38, batch: 24, loss: 1.1050111055374146, acc: 59.375, f1: 33.83571936237182, r: 0.4251693631491963
06/02/2019 09:19:41 step: 1284, epoch: 38, batch: 29, loss: 1.3529611825942993, acc: 45.3125, f1: 18.59605911330049, r: 0.3487808445905174
06/02/2019 09:19:42 *** evaluating ***
06/02/2019 09:19:42 step: 39, epoch: 38, acc: 60.256410256410255, f1: 22.86597671816303, r: 0.37977242837571257
06/02/2019 09:19:42 *** epoch: 40 ***
06/02/2019 09:19:42 *** training ***
06/02/2019 09:19:43 step: 1292, epoch: 39, batch: 4, loss: 1.203091025352478, acc: 65.625, f1: 26.28341807446285, r: 0.3502463540776833
06/02/2019 09:19:44 step: 1297, epoch: 39, batch: 9, loss: 1.2818697690963745, acc: 46.875, f1: 13.904109589041097, r: 0.3721242274658261
06/02/2019 09:19:45 step: 1302, epoch: 39, batch: 14, loss: 1.3646037578582764, acc: 54.6875, f1: 35.67951318458418, r: 0.29812863719271904
06/02/2019 09:19:46 step: 1307, epoch: 39, batch: 19, loss: 1.4660872220993042, acc: 51.5625, f1: 21.351766513056837, r: 0.30804063581881463
06/02/2019 09:19:47 step: 1312, epoch: 39, batch: 24, loss: 1.2246721982955933, acc: 54.6875, f1: 18.376068376068375, r: 0.290380078139287
06/02/2019 09:19:49 step: 1317, epoch: 39, batch: 29, loss: 1.1945059299468994, acc: 56.25, f1: 35.05323505323505, r: 0.3040756554121063
06/02/2019 09:19:49 *** evaluating ***
06/02/2019 09:19:49 step: 40, epoch: 39, acc: 57.692307692307686, f1: 23.21912883896496, r: 0.361187417338655
06/02/2019 09:19:49 *** epoch: 41 ***
06/02/2019 09:19:49 *** training ***
06/02/2019 09:19:51 step: 1325, epoch: 40, batch: 4, loss: 1.2159606218338013, acc: 59.375, f1: 33.0573186212284, r: 0.3374797740223615
06/02/2019 09:19:52 step: 1330, epoch: 40, batch: 9, loss: 1.2881624698638916, acc: 51.5625, f1: 22.883090652103004, r: 0.3998760163459083
06/02/2019 09:19:53 step: 1335, epoch: 40, batch: 14, loss: 1.182797908782959, acc: 57.8125, f1: 18.185075793771443, r: 0.33359679365987194
06/02/2019 09:19:54 step: 1340, epoch: 40, batch: 19, loss: 1.2281179428100586, acc: 46.875, f1: 17.191558441558442, r: 0.38803799297395486
06/02/2019 09:19:55 step: 1345, epoch: 40, batch: 24, loss: 1.3324226140975952, acc: 59.375, f1: 34.35897435897436, r: 0.3948418485810329
06/02/2019 09:19:56 step: 1350, epoch: 40, batch: 29, loss: 1.3026446104049683, acc: 43.75, f1: 17.223502304147466, r: 0.3547781252798842
06/02/2019 09:19:57 *** evaluating ***
06/02/2019 09:19:57 step: 41, epoch: 40, acc: 59.82905982905983, f1: 22.801652248460762, r: 0.3848994915759566
06/02/2019 09:19:57 *** epoch: 42 ***
06/02/2019 09:19:57 *** training ***
06/02/2019 09:19:58 step: 1358, epoch: 41, batch: 4, loss: 1.2195850610733032, acc: 53.125, f1: 26.329022988505745, r: 0.42746091605304276
06/02/2019 09:19:59 step: 1363, epoch: 41, batch: 9, loss: 1.2079609632492065, acc: 56.25, f1: 25.970534841502584, r: 0.3762172070576694
06/02/2019 09:20:01 step: 1368, epoch: 41, batch: 14, loss: 1.465580940246582, acc: 54.6875, f1: 29.82142857142857, r: 0.34371874756534615
06/02/2019 09:20:02 step: 1373, epoch: 41, batch: 19, loss: 1.2130532264709473, acc: 50.0, f1: 17.75621118012422, r: 0.388826247102403
06/02/2019 09:20:03 step: 1378, epoch: 41, batch: 24, loss: 1.3225829601287842, acc: 51.5625, f1: 18.40476190476191, r: 0.2748834590328965
06/02/2019 09:20:04 step: 1383, epoch: 41, batch: 29, loss: 1.17228102684021, acc: 48.4375, f1: 21.537857567269334, r: 0.45868142403285955
06/02/2019 09:20:05 *** evaluating ***
06/02/2019 09:20:05 step: 42, epoch: 41, acc: 60.68376068376068, f1: 23.821898227558602, r: 0.38483216299214795
06/02/2019 09:20:05 *** epoch: 43 ***
06/02/2019 09:20:05 *** training ***
06/02/2019 09:20:06 step: 1391, epoch: 42, batch: 4, loss: 1.4142074584960938, acc: 45.3125, f1: 16.922514619883042, r: 0.36363243757023006
06/02/2019 09:20:07 step: 1396, epoch: 42, batch: 9, loss: 1.119735598564148, acc: 56.25, f1: 29.037267080745337, r: 0.4674440541976595
06/02/2019 09:20:08 step: 1401, epoch: 42, batch: 14, loss: 1.2205265760421753, acc: 65.625, f1: 33.375850340136054, r: 0.4131682426189602
06/02/2019 09:20:09 step: 1406, epoch: 42, batch: 19, loss: 1.2778236865997314, acc: 51.5625, f1: 29.173727970720453, r: 0.4084936771348526
06/02/2019 09:20:11 step: 1411, epoch: 42, batch: 24, loss: 1.0573755502700806, acc: 65.625, f1: 39.970772000423594, r: 0.4088023530511192
06/02/2019 09:20:12 step: 1416, epoch: 42, batch: 29, loss: 1.2004384994506836, acc: 62.5, f1: 30.553053503873173, r: 0.3753437497594394
06/02/2019 09:20:12 *** evaluating ***
06/02/2019 09:20:13 step: 43, epoch: 42, acc: 59.82905982905983, f1: 22.622651839086505, r: 0.3824816001971272
06/02/2019 09:20:13 *** epoch: 44 ***
06/02/2019 09:20:13 *** training ***
06/02/2019 09:20:14 step: 1424, epoch: 43, batch: 4, loss: 1.1570342779159546, acc: 57.8125, f1: 24.92694805194805, r: 0.40868502491178227
06/02/2019 09:20:15 step: 1429, epoch: 43, batch: 9, loss: 0.9656619429588318, acc: 70.3125, f1: 45.817357245928676, r: 0.47637946058333275
06/02/2019 09:20:16 step: 1434, epoch: 43, batch: 14, loss: 1.227981686592102, acc: 57.8125, f1: 24.66742629255092, r: 0.3324439335112131
06/02/2019 09:20:17 step: 1439, epoch: 43, batch: 19, loss: 1.3700096607208252, acc: 51.5625, f1: 29.730134158926724, r: 0.4575168689462447
06/02/2019 09:20:18 step: 1444, epoch: 43, batch: 24, loss: 1.2839144468307495, acc: 51.5625, f1: 20.952380952380953, r: 0.38682632469666195
06/02/2019 09:20:19 step: 1449, epoch: 43, batch: 29, loss: 1.2295424938201904, acc: 53.125, f1: 29.807256235827666, r: 0.40282303221444105
06/02/2019 09:20:20 *** evaluating ***
06/02/2019 09:20:20 step: 44, epoch: 43, acc: 61.111111111111114, f1: 24.096216258386644, r: 0.38969651323665944
06/02/2019 09:20:20 *** epoch: 45 ***
06/02/2019 09:20:20 *** training ***
06/02/2019 09:20:21 step: 1457, epoch: 44, batch: 4, loss: 1.160156488418579, acc: 51.5625, f1: 27.192331478045766, r: 0.39542267958762484
06/02/2019 09:20:23 step: 1462, epoch: 44, batch: 9, loss: 1.1575937271118164, acc: 60.9375, f1: 23.791602662570405, r: 0.4620260825257949
06/02/2019 09:20:24 step: 1467, epoch: 44, batch: 14, loss: 1.2314708232879639, acc: 60.9375, f1: 30.743028811867067, r: 0.3516259796196862
06/02/2019 09:20:25 step: 1472, epoch: 44, batch: 19, loss: 1.1641244888305664, acc: 59.375, f1: 30.89956771100839, r: 0.46533516097882144
06/02/2019 09:20:26 step: 1477, epoch: 44, batch: 24, loss: 1.3384976387023926, acc: 57.8125, f1: 22.93233082706767, r: 0.34051669024822895
06/02/2019 09:20:27 step: 1482, epoch: 44, batch: 29, loss: 1.4629826545715332, acc: 42.1875, f1: 29.016048212110558, r: 0.38148498309456447
06/02/2019 09:20:28 *** evaluating ***
06/02/2019 09:20:28 step: 45, epoch: 44, acc: 60.68376068376068, f1: 23.239075950746432, r: 0.38751849359889945
06/02/2019 09:20:28 *** epoch: 46 ***
06/02/2019 09:20:28 *** training ***
06/02/2019 09:20:29 step: 1490, epoch: 45, batch: 4, loss: 1.1054949760437012, acc: 53.125, f1: 29.37791202716576, r: 0.40254901796863984
06/02/2019 09:20:30 step: 1495, epoch: 45, batch: 9, loss: 1.2657819986343384, acc: 59.375, f1: 27.509850830284265, r: 0.3947647894345832
06/02/2019 09:20:31 step: 1500, epoch: 45, batch: 14, loss: 1.1678965091705322, acc: 51.5625, f1: 16.731366459627328, r: 0.43232210856667064
06/02/2019 09:20:33 step: 1505, epoch: 45, batch: 19, loss: 1.1110038757324219, acc: 64.0625, f1: 28.307565450422594, r: 0.43111897402398974
06/02/2019 09:20:34 step: 1510, epoch: 45, batch: 24, loss: 1.318615436553955, acc: 54.6875, f1: 32.38095238095238, r: 0.37825666144699904
06/02/2019 09:20:35 step: 1515, epoch: 45, batch: 29, loss: 1.2782851457595825, acc: 56.25, f1: 21.675084175084177, r: 0.3997874513870824
06/02/2019 09:20:36 *** evaluating ***
06/02/2019 09:20:36 step: 46, epoch: 45, acc: 60.256410256410255, f1: 23.32904672743359, r: 0.3920163623988254
06/02/2019 09:20:36 *** epoch: 47 ***
06/02/2019 09:20:36 *** training ***
06/02/2019 09:20:37 step: 1523, epoch: 46, batch: 4, loss: 1.1824712753295898, acc: 59.375, f1: 24.337606837606838, r: 0.43422490869083286
06/02/2019 09:20:38 step: 1528, epoch: 46, batch: 9, loss: 1.2765876054763794, acc: 51.5625, f1: 35.18518518518519, r: 0.3648946589071626
06/02/2019 09:20:39 step: 1533, epoch: 46, batch: 14, loss: 1.2308562994003296, acc: 59.375, f1: 26.009264592933945, r: 0.3487718019954059
06/02/2019 09:20:40 step: 1538, epoch: 46, batch: 19, loss: 1.3137701749801636, acc: 46.875, f1: 21.14231828109139, r: 0.39003715861188004
06/02/2019 09:20:41 step: 1543, epoch: 46, batch: 24, loss: 1.0571062564849854, acc: 62.5, f1: 38.609068164547686, r: 0.3784932104960226
06/02/2019 09:20:43 step: 1548, epoch: 46, batch: 29, loss: 1.2244269847869873, acc: 56.25, f1: 34.71861471861472, r: 0.33353172709214346
06/02/2019 09:20:43 *** evaluating ***
06/02/2019 09:20:44 step: 47, epoch: 46, acc: 55.98290598290598, f1: 22.530806083809285, r: 0.35630024166867036
06/02/2019 09:20:44 *** epoch: 48 ***
06/02/2019 09:20:44 *** training ***
06/02/2019 09:20:45 step: 1556, epoch: 47, batch: 4, loss: 1.1379787921905518, acc: 57.8125, f1: 21.864171864171865, r: 0.3491901160298623
06/02/2019 09:20:46 step: 1561, epoch: 47, batch: 9, loss: 0.9773654937744141, acc: 68.75, f1: 39.87107832178255, r: 0.4728691337075762
06/02/2019 09:20:47 step: 1566, epoch: 47, batch: 14, loss: 1.3479259014129639, acc: 45.3125, f1: 19.797754329004327, r: 0.35600416860336886
06/02/2019 09:20:48 step: 1571, epoch: 47, batch: 19, loss: 1.0854933261871338, acc: 57.8125, f1: 30.629305466112445, r: 0.4457764106606912
06/02/2019 09:20:49 step: 1576, epoch: 47, batch: 24, loss: 1.2613064050674438, acc: 48.4375, f1: 23.598387971789506, r: 0.3548735790373941
06/02/2019 09:20:51 step: 1581, epoch: 47, batch: 29, loss: 1.0327975749969482, acc: 59.375, f1: 24.465240641711226, r: 0.40888318621764874
06/02/2019 09:20:51 *** evaluating ***
06/02/2019 09:20:51 step: 48, epoch: 47, acc: 60.68376068376068, f1: 23.249620487201547, r: 0.3907786117587176
06/02/2019 09:20:51 *** epoch: 49 ***
06/02/2019 09:20:51 *** training ***
06/02/2019 09:20:52 step: 1589, epoch: 48, batch: 4, loss: 1.1557788848876953, acc: 59.375, f1: 28.19290643556411, r: 0.3620495190767744
06/02/2019 09:20:54 step: 1594, epoch: 48, batch: 9, loss: 1.0053766965866089, acc: 62.5, f1: 33.77614379084968, r: 0.45664996487877463
06/02/2019 09:20:55 step: 1599, epoch: 48, batch: 14, loss: 1.1961942911148071, acc: 46.875, f1: 18.228174603174605, r: 0.4429082092607993
06/02/2019 09:20:56 step: 1604, epoch: 48, batch: 19, loss: 1.048959732055664, acc: 57.8125, f1: 25.487428842504745, r: 0.46452304057228466
06/02/2019 09:20:57 step: 1609, epoch: 48, batch: 24, loss: 1.091925024986267, acc: 59.375, f1: 24.093247054919527, r: 0.4016776423788426
06/02/2019 09:20:58 step: 1614, epoch: 48, batch: 29, loss: 1.3475922346115112, acc: 53.125, f1: 29.466879590032303, r: 0.39307294920363434
06/02/2019 09:20:59 *** evaluating ***
06/02/2019 09:20:59 step: 49, epoch: 48, acc: 57.26495726495726, f1: 21.483802588174672, r: 0.3605616160573324
06/02/2019 09:20:59 *** epoch: 50 ***
06/02/2019 09:20:59 *** training ***
06/02/2019 09:21:00 step: 1622, epoch: 49, batch: 4, loss: 1.1086560487747192, acc: 57.8125, f1: 24.831932773109244, r: 0.34735141830718297
06/02/2019 09:21:02 step: 1627, epoch: 49, batch: 9, loss: 1.1446088552474976, acc: 57.8125, f1: 41.31700023004371, r: 0.3906911042267208
06/02/2019 09:21:03 step: 1632, epoch: 49, batch: 14, loss: 1.2020479440689087, acc: 62.5, f1: 28.4155328798186, r: 0.35645622484588263
06/02/2019 09:21:04 step: 1637, epoch: 49, batch: 19, loss: 1.1109822988510132, acc: 53.125, f1: 25.74041110697152, r: 0.34550528287146665
06/02/2019 09:21:05 step: 1642, epoch: 49, batch: 24, loss: 1.0827770233154297, acc: 64.0625, f1: 46.167018531550546, r: 0.46994864980440015
06/02/2019 09:21:06 step: 1647, epoch: 49, batch: 29, loss: 1.2537652254104614, acc: 62.5, f1: 23.50708502024291, r: 0.3559530874104257
06/02/2019 09:21:07 *** evaluating ***
06/02/2019 09:21:07 step: 50, epoch: 49, acc: 61.111111111111114, f1: 26.18952340447469, r: 0.3867600991036267
06/02/2019 09:21:07 *** epoch: 51 ***
06/02/2019 09:21:07 *** training ***
06/02/2019 09:21:08 step: 1655, epoch: 50, batch: 4, loss: 0.9950793981552124, acc: 62.5, f1: 37.03754578754578, r: 0.44347269341248363
06/02/2019 09:21:09 step: 1660, epoch: 50, batch: 9, loss: 1.064071536064148, acc: 56.25, f1: 34.168884945282464, r: 0.34569547303351733
06/02/2019 09:21:10 step: 1665, epoch: 50, batch: 14, loss: 1.1455078125, acc: 54.6875, f1: 21.75544794188862, r: 0.40705475060376434
06/02/2019 09:21:11 step: 1670, epoch: 50, batch: 19, loss: 0.8693644404411316, acc: 65.625, f1: 25.0177607274794, r: 0.35576365505803537
06/02/2019 09:21:13 step: 1675, epoch: 50, batch: 24, loss: 1.1864434480667114, acc: 57.8125, f1: 27.435376324442817, r: 0.40436900797709807
06/02/2019 09:21:14 step: 1680, epoch: 50, batch: 29, loss: 1.2098857164382935, acc: 50.0, f1: 30.50228896590083, r: 0.41820413123579253
06/02/2019 09:21:14 *** evaluating ***
06/02/2019 09:21:15 step: 51, epoch: 50, acc: 60.68376068376068, f1: 23.566694485312045, r: 0.39755778383996637
06/02/2019 09:21:15 *** epoch: 52 ***
06/02/2019 09:21:15 *** training ***
06/02/2019 09:21:16 step: 1688, epoch: 51, batch: 4, loss: 1.1630336046218872, acc: 53.125, f1: 20.724946695095948, r: 0.3922988846140684
06/02/2019 09:21:17 step: 1693, epoch: 51, batch: 9, loss: 1.2949938774108887, acc: 43.75, f1: 25.230004967709892, r: 0.3841084521334566
06/02/2019 09:21:18 step: 1698, epoch: 51, batch: 14, loss: 1.189500093460083, acc: 48.4375, f1: 26.82001614205004, r: 0.45816789660109114
06/02/2019 09:21:19 step: 1703, epoch: 51, batch: 19, loss: 0.9751117825508118, acc: 60.9375, f1: 36.875, r: 0.5026992508669504
06/02/2019 09:21:20 step: 1708, epoch: 51, batch: 24, loss: 1.1151328086853027, acc: 56.25, f1: 24.529196293902178, r: 0.44204800430331237
06/02/2019 09:21:21 step: 1713, epoch: 51, batch: 29, loss: 1.2677141427993774, acc: 54.6875, f1: 25.742424242424246, r: 0.4104955698381031
06/02/2019 09:21:22 *** evaluating ***
06/02/2019 09:21:22 step: 52, epoch: 51, acc: 59.401709401709404, f1: 23.34319321568549, r: 0.39029354618813256
06/02/2019 09:21:22 *** epoch: 53 ***
06/02/2019 09:21:22 *** training ***
06/02/2019 09:21:23 step: 1721, epoch: 52, batch: 4, loss: 1.3006353378295898, acc: 48.4375, f1: 17.639419404125288, r: 0.35951378870867723
06/02/2019 09:21:25 step: 1726, epoch: 52, batch: 9, loss: 1.1458522081375122, acc: 64.0625, f1: 29.789272030651343, r: 0.4230421451237673
06/02/2019 09:21:26 step: 1731, epoch: 52, batch: 14, loss: 1.1428794860839844, acc: 53.125, f1: 28.608327946563243, r: 0.533303543155782
06/02/2019 09:21:27 step: 1736, epoch: 52, batch: 19, loss: 1.5220110416412354, acc: 46.875, f1: 23.224796037296038, r: 0.3288543965888526
06/02/2019 09:21:28 step: 1741, epoch: 52, batch: 24, loss: 0.891976535320282, acc: 68.75, f1: 31.984126984126988, r: 0.3910677805169952
06/02/2019 09:21:29 step: 1746, epoch: 52, batch: 29, loss: 1.086652398109436, acc: 56.25, f1: 34.79375696767001, r: 0.4425572479535182
06/02/2019 09:21:30 *** evaluating ***
06/02/2019 09:21:30 step: 53, epoch: 52, acc: 59.82905982905983, f1: 23.03017653865712, r: 0.38832127799999083
06/02/2019 09:21:30 *** epoch: 54 ***
06/02/2019 09:21:30 *** training ***
06/02/2019 09:21:31 step: 1754, epoch: 53, batch: 4, loss: 0.9500963091850281, acc: 62.5, f1: 27.286231884057965, r: 0.5053030714289287
06/02/2019 09:21:32 step: 1759, epoch: 53, batch: 9, loss: 0.9839332699775696, acc: 57.8125, f1: 31.55028546332894, r: 0.35690411884635254
06/02/2019 09:21:34 step: 1764, epoch: 53, batch: 14, loss: 1.4322149753570557, acc: 51.5625, f1: 30.4421768707483, r: 0.35779126403099404
06/02/2019 09:21:35 step: 1769, epoch: 53, batch: 19, loss: 1.2346495389938354, acc: 50.0, f1: 26.454365079365083, r: 0.4346894930944446
06/02/2019 09:21:36 step: 1774, epoch: 53, batch: 24, loss: 1.1120290756225586, acc: 59.375, f1: 25.911566032533774, r: 0.3371067145068096
06/02/2019 09:21:37 step: 1779, epoch: 53, batch: 29, loss: 1.1674190759658813, acc: 56.25, f1: 33.64135397748843, r: 0.37957200314440454
06/02/2019 09:21:37 *** evaluating ***
06/02/2019 09:21:38 step: 54, epoch: 53, acc: 58.54700854700855, f1: 25.13393521808146, r: 0.37628220827711245
06/02/2019 09:21:38 *** epoch: 55 ***
06/02/2019 09:21:38 *** training ***
06/02/2019 09:21:39 step: 1787, epoch: 54, batch: 4, loss: 1.1126468181610107, acc: 60.9375, f1: 28.743019595898062, r: 0.42920526745301213
06/02/2019 09:21:40 step: 1792, epoch: 54, batch: 9, loss: 1.0634430646896362, acc: 64.0625, f1: 39.84705353126405, r: 0.5445949686005124
06/02/2019 09:21:41 step: 1797, epoch: 54, batch: 14, loss: 1.098779320716858, acc: 51.5625, f1: 24.49436763952893, r: 0.40472550004110264
06/02/2019 09:21:42 step: 1802, epoch: 54, batch: 19, loss: 1.1881473064422607, acc: 50.0, f1: 25.586374832064486, r: 0.3898785450242243
06/02/2019 09:21:44 step: 1807, epoch: 54, batch: 24, loss: 1.1313081979751587, acc: 60.9375, f1: 29.38775510204082, r: 0.44049227586886075
06/02/2019 09:21:45 step: 1812, epoch: 54, batch: 29, loss: 0.9698687791824341, acc: 65.625, f1: 28.116349988271168, r: 0.416018343971411
06/02/2019 09:21:45 *** evaluating ***
06/02/2019 09:21:45 step: 55, epoch: 54, acc: 61.53846153846154, f1: 24.62062466779448, r: 0.4034381866822824
06/02/2019 09:21:45 *** epoch: 56 ***
06/02/2019 09:21:45 *** training ***
06/02/2019 09:21:46 step: 1820, epoch: 55, batch: 4, loss: 1.253956913948059, acc: 46.875, f1: 22.706980519480517, r: 0.4032531105851602
06/02/2019 09:21:48 step: 1825, epoch: 55, batch: 9, loss: 1.0807981491088867, acc: 57.8125, f1: 29.06509826152684, r: 0.3928588675469735
06/02/2019 09:21:49 step: 1830, epoch: 55, batch: 14, loss: 1.3266626596450806, acc: 45.3125, f1: 23.25036075036075, r: 0.38138530792782277
06/02/2019 09:21:50 step: 1835, epoch: 55, batch: 19, loss: 1.111374020576477, acc: 50.0, f1: 21.050454921422663, r: 0.4029444923673843
06/02/2019 09:21:51 step: 1840, epoch: 55, batch: 24, loss: 1.1254576444625854, acc: 53.125, f1: 25.52939671166272, r: 0.4327057111200503
06/02/2019 09:21:53 step: 1845, epoch: 55, batch: 29, loss: 1.2492583990097046, acc: 56.25, f1: 40.238095238095234, r: 0.37522185968795074
06/02/2019 09:21:53 *** evaluating ***
06/02/2019 09:21:53 step: 56, epoch: 55, acc: 60.68376068376068, f1: 26.18062493062493, r: 0.3943149258890326
06/02/2019 09:21:53 *** epoch: 57 ***
06/02/2019 09:21:53 *** training ***
06/02/2019 09:21:55 step: 1853, epoch: 56, batch: 4, loss: 0.9674341678619385, acc: 64.0625, f1: 37.775716347144915, r: 0.49122505908389413
06/02/2019 09:21:56 step: 1858, epoch: 56, batch: 9, loss: 1.0279269218444824, acc: 59.375, f1: 45.44967139398132, r: 0.46283991831737115
06/02/2019 09:21:57 step: 1863, epoch: 56, batch: 14, loss: 1.0502508878707886, acc: 60.9375, f1: 35.470085470085465, r: 0.4675011499866577
06/02/2019 09:21:58 step: 1868, epoch: 56, batch: 19, loss: 1.0315381288528442, acc: 67.1875, f1: 36.26089732080516, r: 0.3636979183478175
06/02/2019 09:21:59 step: 1873, epoch: 56, batch: 24, loss: 1.099603533744812, acc: 62.5, f1: 48.229111464405584, r: 0.37081278836222886
06/02/2019 09:22:00 step: 1878, epoch: 56, batch: 29, loss: 1.045230746269226, acc: 54.6875, f1: 26.609857978279027, r: 0.42245559483084094
06/02/2019 09:22:01 *** evaluating ***
06/02/2019 09:22:01 step: 57, epoch: 56, acc: 59.82905982905983, f1: 24.10404844479224, r: 0.4052571974688921
06/02/2019 09:22:01 *** epoch: 58 ***
06/02/2019 09:22:01 *** training ***
06/02/2019 09:22:02 step: 1886, epoch: 57, batch: 4, loss: 1.0427204370498657, acc: 54.6875, f1: 21.23076923076923, r: 0.487920586658965
06/02/2019 09:22:03 step: 1891, epoch: 57, batch: 9, loss: 1.127053141593933, acc: 48.4375, f1: 21.085972850678733, r: 0.3600029750796213
06/02/2019 09:22:05 step: 1896, epoch: 57, batch: 14, loss: 1.1943548917770386, acc: 60.9375, f1: 36.53448275862069, r: 0.4087076450763345
06/02/2019 09:22:06 step: 1901, epoch: 57, batch: 19, loss: 1.0360217094421387, acc: 64.0625, f1: 32.69492773635275, r: 0.43996619973396833
06/02/2019 09:22:07 step: 1906, epoch: 57, batch: 24, loss: 1.2033747434616089, acc: 59.375, f1: 39.41609977324263, r: 0.4212869161603435
06/02/2019 09:22:08 step: 1911, epoch: 57, batch: 29, loss: 1.0115704536437988, acc: 57.8125, f1: 30.957613814756673, r: 0.4021253393653645
06/02/2019 09:22:09 *** evaluating ***
06/02/2019 09:22:09 step: 58, epoch: 57, acc: 60.68376068376068, f1: 23.927774035822978, r: 0.4003317880753728
06/02/2019 09:22:09 *** epoch: 59 ***
06/02/2019 09:22:09 *** training ***
06/02/2019 09:22:10 step: 1919, epoch: 58, batch: 4, loss: 1.0328834056854248, acc: 57.8125, f1: 33.333333333333336, r: 0.41247012563715746
06/02/2019 09:22:12 step: 1924, epoch: 58, batch: 9, loss: 1.098387598991394, acc: 54.6875, f1: 25.012430809644425, r: 0.4200647738626648
06/02/2019 09:22:13 step: 1929, epoch: 58, batch: 14, loss: 1.1300532817840576, acc: 59.375, f1: 27.61904761904762, r: 0.3446661421844586
06/02/2019 09:22:14 step: 1934, epoch: 58, batch: 19, loss: 1.1002962589263916, acc: 59.375, f1: 23.993316624895574, r: 0.3727277050803059
06/02/2019 09:22:15 step: 1939, epoch: 58, batch: 24, loss: 0.958303689956665, acc: 65.625, f1: 28.80467435712569, r: 0.3847906496962324
06/02/2019 09:22:16 step: 1944, epoch: 58, batch: 29, loss: 1.0536550283432007, acc: 65.625, f1: 40.27210884353741, r: 0.4609969770079943
06/02/2019 09:22:16 *** evaluating ***
06/02/2019 09:22:17 step: 59, epoch: 58, acc: 58.97435897435898, f1: 23.44523410118923, r: 0.4020562795041646
06/02/2019 09:22:17 *** epoch: 60 ***
06/02/2019 09:22:17 *** training ***
06/02/2019 09:22:18 step: 1952, epoch: 59, batch: 4, loss: 0.9796562790870667, acc: 56.25, f1: 27.480158730158728, r: 0.46712450780064013
06/02/2019 09:22:19 step: 1957, epoch: 59, batch: 9, loss: 1.0012342929840088, acc: 65.625, f1: 58.22344960275995, r: 0.40162366608111477
06/02/2019 09:22:20 step: 1962, epoch: 59, batch: 14, loss: 1.1363787651062012, acc: 57.8125, f1: 22.665965404394576, r: 0.4059324868305715
06/02/2019 09:22:21 step: 1967, epoch: 59, batch: 19, loss: 1.1045305728912354, acc: 57.8125, f1: 34.16721711803679, r: 0.4042197054524444
06/02/2019 09:22:22 step: 1972, epoch: 59, batch: 24, loss: 1.0862782001495361, acc: 62.5, f1: 36.567192192192195, r: 0.4688042797725855
06/02/2019 09:22:23 step: 1977, epoch: 59, batch: 29, loss: 1.0213191509246826, acc: 57.8125, f1: 29.82808857808858, r: 0.452411035368345
06/02/2019 09:22:24 *** evaluating ***
06/02/2019 09:22:24 step: 60, epoch: 59, acc: 60.68376068376068, f1: 27.05371031075735, r: 0.4056776877511016
06/02/2019 09:22:24 *** epoch: 61 ***
06/02/2019 09:22:24 *** training ***
06/02/2019 09:22:25 step: 1985, epoch: 60, batch: 4, loss: 1.014901041984558, acc: 62.5, f1: 24.198021181716832, r: 0.43728686380421156
06/02/2019 09:22:26 step: 1990, epoch: 60, batch: 9, loss: 1.099562168121338, acc: 54.6875, f1: 27.988883847549907, r: 0.3930145942140071
06/02/2019 09:22:27 step: 1995, epoch: 60, batch: 14, loss: 0.9859403967857361, acc: 62.5, f1: 27.46288798920378, r: 0.495264673677735
06/02/2019 09:22:29 step: 2000, epoch: 60, batch: 19, loss: 0.9517504572868347, acc: 64.0625, f1: 44.926206798127986, r: 0.49583468174863166
06/02/2019 09:22:30 step: 2005, epoch: 60, batch: 24, loss: 0.9799120426177979, acc: 62.5, f1: 34.41625298768156, r: 0.3628308391916866
06/02/2019 09:22:31 step: 2010, epoch: 60, batch: 29, loss: 1.006125807762146, acc: 64.0625, f1: 59.79476019798601, r: 0.41956551251084967
06/02/2019 09:22:32 *** evaluating ***
06/02/2019 09:22:32 step: 61, epoch: 60, acc: 60.68376068376068, f1: 26.149700151628792, r: 0.4107636988302864
06/02/2019 09:22:32 *** epoch: 62 ***
06/02/2019 09:22:32 *** training ***
06/02/2019 09:22:33 step: 2018, epoch: 61, batch: 4, loss: 0.9047973155975342, acc: 65.625, f1: 36.3986013986014, r: 0.39496440255303006
06/02/2019 09:22:34 step: 2023, epoch: 61, batch: 9, loss: 0.9661243557929993, acc: 60.9375, f1: 32.452437375782154, r: 0.41813523158396537
06/02/2019 09:22:36 step: 2028, epoch: 61, batch: 14, loss: 0.9116083979606628, acc: 64.0625, f1: 39.88331859299601, r: 0.4000711957910289
06/02/2019 09:22:37 step: 2033, epoch: 61, batch: 19, loss: 0.9658138155937195, acc: 62.5, f1: 34.564983888292154, r: 0.4221227960673646
06/02/2019 09:22:38 step: 2038, epoch: 61, batch: 24, loss: 1.1808338165283203, acc: 51.5625, f1: 27.516393442622956, r: 0.37504794626225973
06/02/2019 09:22:39 step: 2043, epoch: 61, batch: 29, loss: 1.0138670206069946, acc: 59.375, f1: 30.315686565686566, r: 0.4506541182437157
06/02/2019 09:22:39 *** evaluating ***
06/02/2019 09:22:40 step: 62, epoch: 61, acc: 59.401709401709404, f1: 23.325288841593185, r: 0.4097416217640661
06/02/2019 09:22:40 *** epoch: 63 ***
06/02/2019 09:22:40 *** training ***
06/02/2019 09:22:41 step: 2051, epoch: 62, batch: 4, loss: 1.0041162967681885, acc: 57.8125, f1: 36.06269342118399, r: 0.4449994633981319
06/02/2019 09:22:42 step: 2056, epoch: 62, batch: 9, loss: 1.0143121480941772, acc: 57.8125, f1: 35.666955740847364, r: 0.47078861240580755
06/02/2019 09:22:43 step: 2061, epoch: 62, batch: 14, loss: 1.1307481527328491, acc: 51.5625, f1: 27.91416083341549, r: 0.4709119278781201
06/02/2019 09:22:44 step: 2066, epoch: 62, batch: 19, loss: 0.884442925453186, acc: 64.0625, f1: 40.68376068376068, r: 0.4439832194445566
06/02/2019 09:22:45 step: 2071, epoch: 62, batch: 24, loss: 0.9231948256492615, acc: 59.375, f1: 37.54661825629568, r: 0.46418792183507257
06/02/2019 09:22:46 step: 2076, epoch: 62, batch: 29, loss: 1.1279280185699463, acc: 57.8125, f1: 33.26839826839826, r: 0.45552873035532715
06/02/2019 09:22:47 *** evaluating ***
06/02/2019 09:22:47 step: 63, epoch: 62, acc: 58.97435897435898, f1: 25.007936507936506, r: 0.3772213748958385
06/02/2019 09:22:47 *** epoch: 64 ***
06/02/2019 09:22:47 *** training ***
06/02/2019 09:22:48 step: 2084, epoch: 63, batch: 4, loss: 0.9582157731056213, acc: 60.9375, f1: 36.806318681318686, r: 0.5274002893843365
06/02/2019 09:22:49 step: 2089, epoch: 63, batch: 9, loss: 1.1527060270309448, acc: 59.375, f1: 30.919471153846157, r: 0.41968812728453575
06/02/2019 09:22:50 step: 2094, epoch: 63, batch: 14, loss: 0.8395426273345947, acc: 59.375, f1: 32.91005291005291, r: 0.48193605571623666
06/02/2019 09:22:52 step: 2099, epoch: 63, batch: 19, loss: 0.8289912939071655, acc: 70.3125, f1: 38.960821786908745, r: 0.4892308492137643
06/02/2019 09:22:53 step: 2104, epoch: 63, batch: 24, loss: 0.971311092376709, acc: 56.25, f1: 35.12645249487355, r: 0.4668826323886131
06/02/2019 09:22:54 step: 2109, epoch: 63, batch: 29, loss: 1.0634270906448364, acc: 60.9375, f1: 46.74481074481075, r: 0.4272201917278356
06/02/2019 09:22:55 *** evaluating ***
06/02/2019 09:22:55 step: 64, epoch: 63, acc: 59.82905982905983, f1: 25.494049809839282, r: 0.36840566589465656
06/02/2019 09:22:55 *** epoch: 65 ***
06/02/2019 09:22:55 *** training ***
06/02/2019 09:22:56 step: 2117, epoch: 64, batch: 4, loss: 1.085864782333374, acc: 57.8125, f1: 33.69942586963864, r: 0.4137481992276844
06/02/2019 09:22:57 step: 2122, epoch: 64, batch: 9, loss: 0.8892946839332581, acc: 68.75, f1: 45.67514846228873, r: 0.44841916801852094
06/02/2019 09:22:58 step: 2127, epoch: 64, batch: 14, loss: 1.0410981178283691, acc: 62.5, f1: 34.25806790026319, r: 0.3891061161106243
06/02/2019 09:22:59 step: 2132, epoch: 64, batch: 19, loss: 0.9440575242042542, acc: 60.9375, f1: 41.26808618184116, r: 0.4948145544103938
06/02/2019 09:23:01 step: 2137, epoch: 64, batch: 24, loss: 0.9946022629737854, acc: 60.9375, f1: 33.31357048748353, r: 0.442052051566473
06/02/2019 09:23:02 step: 2142, epoch: 64, batch: 29, loss: 0.9852147698402405, acc: 65.625, f1: 40.416666666666664, r: 0.47890695561450813
06/02/2019 09:23:02 *** evaluating ***
06/02/2019 09:23:03 step: 65, epoch: 64, acc: 58.54700854700855, f1: 24.449484852767405, r: 0.41760402306619565
06/02/2019 09:23:03 *** epoch: 66 ***
06/02/2019 09:23:03 *** training ***
06/02/2019 09:23:04 step: 2150, epoch: 65, batch: 4, loss: 1.0992960929870605, acc: 54.6875, f1: 28.719192663555226, r: 0.4507284150671692
06/02/2019 09:23:05 step: 2155, epoch: 65, batch: 9, loss: 1.0358127355575562, acc: 71.875, f1: 49.18154761904762, r: 0.5053964102419916
06/02/2019 09:23:06 step: 2160, epoch: 65, batch: 14, loss: 0.9129916429519653, acc: 64.0625, f1: 41.0, r: 0.5353673466240032
06/02/2019 09:23:07 step: 2165, epoch: 65, batch: 19, loss: 0.9361649751663208, acc: 59.375, f1: 31.949103528050898, r: 0.4567214015450433
06/02/2019 09:23:08 step: 2170, epoch: 65, batch: 24, loss: 0.881517767906189, acc: 68.75, f1: 38.168430833872016, r: 0.41244818839104724
06/02/2019 09:23:09 step: 2175, epoch: 65, batch: 29, loss: 0.754008412361145, acc: 76.5625, f1: 36.86028257456829, r: 0.45613026984309357
06/02/2019 09:23:10 *** evaluating ***
06/02/2019 09:23:10 step: 66, epoch: 65, acc: 59.82905982905983, f1: 25.979339951208193, r: 0.407211592523641
06/02/2019 09:23:10 *** epoch: 67 ***
06/02/2019 09:23:10 *** training ***
06/02/2019 09:23:11 step: 2183, epoch: 66, batch: 4, loss: 1.0469534397125244, acc: 60.9375, f1: 32.203947368421055, r: 0.4333770780256328
06/02/2019 09:23:12 step: 2188, epoch: 66, batch: 9, loss: 0.7981703281402588, acc: 73.4375, f1: 56.292517006802726, r: 0.5060946021316903
06/02/2019 09:23:13 step: 2193, epoch: 66, batch: 14, loss: 0.8357678055763245, acc: 70.3125, f1: 44.17226292226292, r: 0.5424546957127052
06/02/2019 09:23:14 step: 2198, epoch: 66, batch: 19, loss: 1.098882794380188, acc: 59.375, f1: 39.76828231292517, r: 0.48881886846981987
06/02/2019 09:23:16 step: 2203, epoch: 66, batch: 24, loss: 0.9062869548797607, acc: 60.9375, f1: 33.180641821946175, r: 0.5249258791822793
06/02/2019 09:23:17 step: 2208, epoch: 66, batch: 29, loss: 0.769463300704956, acc: 68.75, f1: 37.45195071637114, r: 0.4805494099578883
06/02/2019 09:23:17 *** evaluating ***
06/02/2019 09:23:18 step: 67, epoch: 66, acc: 60.256410256410255, f1: 25.679819322553875, r: 0.3913484613581856
06/02/2019 09:23:18 *** epoch: 68 ***
06/02/2019 09:23:18 *** training ***
06/02/2019 09:23:19 step: 2216, epoch: 67, batch: 4, loss: 0.8956519961357117, acc: 62.5, f1: 40.08040935672514, r: 0.534995143056915
06/02/2019 09:23:20 step: 2221, epoch: 67, batch: 9, loss: 1.0281774997711182, acc: 60.9375, f1: 38.89831183309444, r: 0.5041297074019839
06/02/2019 09:23:21 step: 2226, epoch: 67, batch: 14, loss: 1.133791208267212, acc: 53.125, f1: 42.51865459102301, r: 0.4574800731115183
06/02/2019 09:23:22 step: 2231, epoch: 67, batch: 19, loss: 1.039279818534851, acc: 54.6875, f1: 29.67031617363231, r: 0.4221351751555857
06/02/2019 09:23:23 step: 2236, epoch: 67, batch: 24, loss: 1.0533959865570068, acc: 62.5, f1: 39.33259722733408, r: 0.46471681520811375
06/02/2019 09:23:25 step: 2241, epoch: 67, batch: 29, loss: 0.8887407183647156, acc: 68.75, f1: 38.63224637681159, r: 0.4750457329706746
06/02/2019 09:23:25 *** evaluating ***
06/02/2019 09:23:26 step: 68, epoch: 67, acc: 60.256410256410255, f1: 28.955020379668095, r: 0.4134334123534372
06/02/2019 09:23:26 *** epoch: 69 ***
06/02/2019 09:23:26 *** training ***
06/02/2019 09:23:26 step: 2249, epoch: 68, batch: 4, loss: 0.8271687626838684, acc: 62.5, f1: 38.46668682194998, r: 0.48854704866652277
06/02/2019 09:23:28 step: 2254, epoch: 68, batch: 9, loss: 1.0245792865753174, acc: 56.25, f1: 27.749999999999996, r: 0.4801404571463342
06/02/2019 09:23:29 step: 2259, epoch: 68, batch: 14, loss: 0.9776038527488708, acc: 62.5, f1: 40.98124098124098, r: 0.3709046456072573
06/02/2019 09:23:30 step: 2264, epoch: 68, batch: 19, loss: 0.8730289936065674, acc: 65.625, f1: 56.017022193492785, r: 0.4885779554548249
06/02/2019 09:23:31 step: 2269, epoch: 68, batch: 24, loss: 1.0999536514282227, acc: 56.25, f1: 37.43163759607956, r: 0.4124602192929143
06/02/2019 09:23:32 step: 2274, epoch: 68, batch: 29, loss: 0.9177605509757996, acc: 64.0625, f1: 48.020067787509646, r: 0.5140823666491724
06/02/2019 09:23:33 *** evaluating ***
06/02/2019 09:23:33 step: 69, epoch: 68, acc: 61.111111111111114, f1: 26.79644174071004, r: 0.4080882176356738
06/02/2019 09:23:33 *** epoch: 70 ***
06/02/2019 09:23:33 *** training ***
06/02/2019 09:23:34 step: 2282, epoch: 69, batch: 4, loss: 0.7412213087081909, acc: 71.875, f1: 48.720238095238095, r: 0.48710475741075726
06/02/2019 09:23:35 step: 2287, epoch: 69, batch: 9, loss: 0.8113444447517395, acc: 70.3125, f1: 49.09297052154195, r: 0.5082193731491198
06/02/2019 09:23:36 step: 2292, epoch: 69, batch: 14, loss: 0.8640355467796326, acc: 62.5, f1: 37.773530817009075, r: 0.49508137025173776
06/02/2019 09:23:37 step: 2297, epoch: 69, batch: 19, loss: 1.0372611284255981, acc: 60.9375, f1: 33.07823340637122, r: 0.47035189322372944
06/02/2019 09:23:38 step: 2302, epoch: 69, batch: 24, loss: 0.8939899206161499, acc: 64.0625, f1: 38.68030118030118, r: 0.37549395858006696
06/02/2019 09:23:40 step: 2307, epoch: 69, batch: 29, loss: 0.9716538786888123, acc: 62.5, f1: 48.08244916003537, r: 0.5527409564534835
06/02/2019 09:23:40 *** evaluating ***
06/02/2019 09:23:41 step: 70, epoch: 69, acc: 59.401709401709404, f1: 25.787234425556427, r: 0.4046083463807864
06/02/2019 09:23:41 *** epoch: 71 ***
06/02/2019 09:23:41 *** training ***
06/02/2019 09:23:42 step: 2315, epoch: 70, batch: 4, loss: 0.7481791377067566, acc: 75.0, f1: 56.37426668780721, r: 0.5553358850305944
06/02/2019 09:23:43 step: 2320, epoch: 70, batch: 9, loss: 1.0023372173309326, acc: 62.5, f1: 46.00706311232627, r: 0.45785889548852665
06/02/2019 09:23:44 step: 2325, epoch: 70, batch: 14, loss: 0.6705213785171509, acc: 76.5625, f1: 45.48941798941799, r: 0.4383162257904831
06/02/2019 09:23:45 step: 2330, epoch: 70, batch: 19, loss: 0.8544523119926453, acc: 67.1875, f1: 41.534903268774244, r: 0.465763559377201
06/02/2019 09:23:46 step: 2335, epoch: 70, batch: 24, loss: 0.867181658744812, acc: 71.875, f1: 40.13343052738336, r: 0.6146511441257215
06/02/2019 09:23:47 step: 2340, epoch: 70, batch: 29, loss: 0.8893099427223206, acc: 71.875, f1: 37.42448390014312, r: 0.5019579915285639
06/02/2019 09:23:48 *** evaluating ***
06/02/2019 09:23:48 step: 71, epoch: 70, acc: 61.111111111111114, f1: 25.7211313086194, r: 0.386101105821551
06/02/2019 09:23:48 *** epoch: 72 ***
06/02/2019 09:23:48 *** training ***
06/02/2019 09:23:49 step: 2348, epoch: 71, batch: 4, loss: 1.1501948833465576, acc: 59.375, f1: 42.596721119959504, r: 0.42626127170182604
06/02/2019 09:23:51 step: 2353, epoch: 71, batch: 9, loss: 1.1615474224090576, acc: 60.9375, f1: 33.100907029478456, r: 0.5053668081950983
06/02/2019 09:23:52 step: 2358, epoch: 71, batch: 14, loss: 0.9000195860862732, acc: 60.9375, f1: 30.73838830948339, r: 0.42569359190292366
06/02/2019 09:23:53 step: 2363, epoch: 71, batch: 19, loss: 0.7116473317146301, acc: 75.0, f1: 46.41269841269842, r: 0.40753086882515926
06/02/2019 09:23:54 step: 2368, epoch: 71, batch: 24, loss: 0.9548930525779724, acc: 60.9375, f1: 36.98206555349412, r: 0.3695798287982788
06/02/2019 09:23:55 step: 2373, epoch: 71, batch: 29, loss: 0.8724977374076843, acc: 59.375, f1: 33.395580454403984, r: 0.4426999602603739
06/02/2019 09:23:56 *** evaluating ***
06/02/2019 09:23:56 step: 72, epoch: 71, acc: 59.82905982905983, f1: 26.522692660249845, r: 0.41003309355656237
06/02/2019 09:23:56 *** epoch: 73 ***
06/02/2019 09:23:56 *** training ***
06/02/2019 09:23:57 step: 2381, epoch: 72, batch: 4, loss: 1.0474905967712402, acc: 62.5, f1: 41.08111374121226, r: 0.4332314486732332
06/02/2019 09:23:58 step: 2386, epoch: 72, batch: 9, loss: 0.7652762532234192, acc: 78.125, f1: 48.55738993710691, r: 0.514186694709916
06/02/2019 09:23:59 step: 2391, epoch: 72, batch: 14, loss: 0.8343610167503357, acc: 73.4375, f1: 39.60013440860215, r: 0.5023841819172943
06/02/2019 09:24:00 step: 2396, epoch: 72, batch: 19, loss: 1.1551434993743896, acc: 45.3125, f1: 29.42765567765567, r: 0.45710930571600433
06/02/2019 09:24:01 step: 2401, epoch: 72, batch: 24, loss: 0.8301714062690735, acc: 64.0625, f1: 42.542016806722685, r: 0.5877745677767481
06/02/2019 09:24:02 step: 2406, epoch: 72, batch: 29, loss: 0.8777382373809814, acc: 62.5, f1: 48.00795978215333, r: 0.5399737904411829
06/02/2019 09:24:03 *** evaluating ***
06/02/2019 09:24:03 step: 73, epoch: 72, acc: 60.256410256410255, f1: 27.019847412649266, r: 0.41324647069505316
06/02/2019 09:24:03 *** epoch: 74 ***
06/02/2019 09:24:03 *** training ***
06/02/2019 09:24:04 step: 2414, epoch: 73, batch: 4, loss: 0.9224932789802551, acc: 59.375, f1: 32.40165631469979, r: 0.4821240978448036
06/02/2019 09:24:06 step: 2419, epoch: 73, batch: 9, loss: 1.140351414680481, acc: 53.125, f1: 32.90099309836152, r: 0.492611539170763
06/02/2019 09:24:07 step: 2424, epoch: 73, batch: 14, loss: 0.8428974151611328, acc: 70.3125, f1: 38.419676593972206, r: 0.4888357878654159
06/02/2019 09:24:08 step: 2429, epoch: 73, batch: 19, loss: 0.6968486905097961, acc: 70.3125, f1: 39.005956449565474, r: 0.4709274643171374
06/02/2019 09:24:09 step: 2434, epoch: 73, batch: 24, loss: 0.594638466835022, acc: 75.0, f1: 47.54098360655737, r: 0.3787055317231271
06/02/2019 09:24:10 step: 2439, epoch: 73, batch: 29, loss: 0.9604337215423584, acc: 65.625, f1: 37.22888555722139, r: 0.4931190329324166
06/02/2019 09:24:10 *** evaluating ***
06/02/2019 09:24:11 step: 74, epoch: 73, acc: 61.53846153846154, f1: 29.958512459662334, r: 0.40969269972131955
06/02/2019 09:24:11 *** epoch: 75 ***
06/02/2019 09:24:11 *** training ***
06/02/2019 09:24:12 step: 2447, epoch: 74, batch: 4, loss: 0.8396009206771851, acc: 67.1875, f1: 39.399719756862616, r: 0.4585422573336468
06/02/2019 09:24:13 step: 2452, epoch: 74, batch: 9, loss: 1.0344018936157227, acc: 56.25, f1: 34.16632457580734, r: 0.49309418981431546
06/02/2019 09:24:14 step: 2457, epoch: 74, batch: 14, loss: 0.9174758195877075, acc: 62.5, f1: 39.6882183908046, r: 0.5330907360821295
06/02/2019 09:24:15 step: 2462, epoch: 74, batch: 19, loss: 0.7553991079330444, acc: 73.4375, f1: 43.858085090961794, r: 0.49776883006177675
06/02/2019 09:24:16 step: 2467, epoch: 74, batch: 24, loss: 1.084875464439392, acc: 56.25, f1: 38.38669950738917, r: 0.47994699741211383
06/02/2019 09:24:18 step: 2472, epoch: 74, batch: 29, loss: 0.9303537607192993, acc: 64.0625, f1: 43.778958132235196, r: 0.5434076266663347
06/02/2019 09:24:18 *** evaluating ***
06/02/2019 09:24:19 step: 75, epoch: 74, acc: 60.256410256410255, f1: 28.00653485867821, r: 0.4161151221224743
06/02/2019 09:24:19 *** epoch: 76 ***
06/02/2019 09:24:19 *** training ***
06/02/2019 09:24:20 step: 2480, epoch: 75, batch: 4, loss: 0.9355112910270691, acc: 62.5, f1: 50.652851191934744, r: 0.4876484316847427
06/02/2019 09:24:21 step: 2485, epoch: 75, batch: 9, loss: 0.9702385663986206, acc: 59.375, f1: 43.40544871794872, r: 0.5057003042536525
06/02/2019 09:24:22 step: 2490, epoch: 75, batch: 14, loss: 0.7605427503585815, acc: 65.625, f1: 41.690456086740916, r: 0.49906740901449526
06/02/2019 09:24:23 step: 2495, epoch: 75, batch: 19, loss: 0.937531590461731, acc: 60.9375, f1: 31.831168831168828, r: 0.43563322715429964
06/02/2019 09:24:24 step: 2500, epoch: 75, batch: 24, loss: 0.6778779029846191, acc: 75.0, f1: 53.19101062676209, r: 0.5395842828468729
06/02/2019 09:24:25 step: 2505, epoch: 75, batch: 29, loss: 0.7320293188095093, acc: 71.875, f1: 43.125795772854595, r: 0.45679169676004544
06/02/2019 09:24:26 *** evaluating ***
06/02/2019 09:24:27 step: 76, epoch: 75, acc: 60.68376068376068, f1: 27.702818580644383, r: 0.4149893115607678
06/02/2019 09:24:27 *** epoch: 77 ***
06/02/2019 09:24:27 *** training ***
06/02/2019 09:24:28 step: 2513, epoch: 76, batch: 4, loss: 0.9945746660232544, acc: 64.0625, f1: 35.03472222222223, r: 0.4873428059086
06/02/2019 09:24:28 step: 2518, epoch: 76, batch: 9, loss: 0.7740045189857483, acc: 67.1875, f1: 42.65088007374759, r: 0.43055328887070876
06/02/2019 09:24:30 step: 2523, epoch: 76, batch: 14, loss: 0.7547773718833923, acc: 71.875, f1: 61.212121212121204, r: 0.6262747827065731
06/02/2019 09:24:31 step: 2528, epoch: 76, batch: 19, loss: 0.8286888003349304, acc: 68.75, f1: 57.90426215463695, r: 0.43259945541223477
06/02/2019 09:24:32 step: 2533, epoch: 76, batch: 24, loss: 0.8522204756736755, acc: 67.1875, f1: 43.84539033004506, r: 0.5328149181836404
06/02/2019 09:24:33 step: 2538, epoch: 76, batch: 29, loss: 0.8775990009307861, acc: 78.125, f1: 58.783068783068785, r: 0.5160339585278045
06/02/2019 09:24:34 *** evaluating ***
06/02/2019 09:24:34 step: 77, epoch: 76, acc: 60.256410256410255, f1: 27.274805989633, r: 0.3809261183214638
06/02/2019 09:24:34 *** epoch: 78 ***
06/02/2019 09:24:34 *** training ***
06/02/2019 09:24:35 step: 2546, epoch: 77, batch: 4, loss: 0.7982375621795654, acc: 75.0, f1: 54.7157358241102, r: 0.535004931616264
06/02/2019 09:24:36 step: 2551, epoch: 77, batch: 9, loss: 0.8389216661453247, acc: 64.0625, f1: 49.942358366271414, r: 0.518014079929436
06/02/2019 09:24:38 step: 2556, epoch: 77, batch: 14, loss: 0.7780791521072388, acc: 71.875, f1: 56.39566894886043, r: 0.5500114572218372
06/02/2019 09:24:39 step: 2561, epoch: 77, batch: 19, loss: 1.083687424659729, acc: 50.0, f1: 26.190476190476193, r: 0.4091537377859831
06/02/2019 09:24:40 step: 2566, epoch: 77, batch: 24, loss: 0.8324641585350037, acc: 65.625, f1: 44.085667600373476, r: 0.4542497780353399
06/02/2019 09:24:41 step: 2571, epoch: 77, batch: 29, loss: 0.7859557867050171, acc: 65.625, f1: 41.280622727991144, r: 0.5032857701596242
06/02/2019 09:24:42 *** evaluating ***
06/02/2019 09:24:42 step: 78, epoch: 77, acc: 58.97435897435898, f1: 25.53652291105121, r: 0.39050534528591
06/02/2019 09:24:42 *** epoch: 79 ***
06/02/2019 09:24:42 *** training ***
06/02/2019 09:24:43 step: 2579, epoch: 78, batch: 4, loss: 0.8073663115501404, acc: 65.625, f1: 43.37823313223771, r: 0.5259407075442996
06/02/2019 09:24:44 step: 2584, epoch: 78, batch: 9, loss: 0.9328106045722961, acc: 60.9375, f1: 38.98601398601399, r: 0.5498199798411766
06/02/2019 09:24:46 step: 2589, epoch: 78, batch: 14, loss: 1.0756725072860718, acc: 62.5, f1: 35.717012214721954, r: 0.42190611676456014
06/02/2019 09:24:47 step: 2594, epoch: 78, batch: 19, loss: 0.9744709730148315, acc: 59.375, f1: 41.698764089683905, r: 0.35523939007223726
06/02/2019 09:24:48 step: 2599, epoch: 78, batch: 24, loss: 0.7260878682136536, acc: 65.625, f1: 45.317460317460316, r: 0.47045969017839256
06/02/2019 09:24:49 step: 2604, epoch: 78, batch: 29, loss: 0.9728544354438782, acc: 65.625, f1: 38.91356107660455, r: 0.492399594099047
06/02/2019 09:24:49 *** evaluating ***
06/02/2019 09:24:50 step: 79, epoch: 78, acc: 61.53846153846154, f1: 28.54966717869944, r: 0.40218564497828674
06/02/2019 09:24:50 *** epoch: 80 ***
06/02/2019 09:24:50 *** training ***
06/02/2019 09:24:51 step: 2612, epoch: 79, batch: 4, loss: 0.7613511085510254, acc: 70.3125, f1: 35.84062590553463, r: 0.5172820013465572
06/02/2019 09:24:52 step: 2617, epoch: 79, batch: 9, loss: 0.9260793328285217, acc: 57.8125, f1: 40.0515799540155, r: 0.5734199490435277
06/02/2019 09:24:53 step: 2622, epoch: 79, batch: 14, loss: 0.6415256857872009, acc: 73.4375, f1: 56.34227927461011, r: 0.5995736736574065
06/02/2019 09:24:54 step: 2627, epoch: 79, batch: 19, loss: 0.6465339660644531, acc: 75.0, f1: 51.1976911976912, r: 0.5197355933302339
06/02/2019 09:24:56 step: 2632, epoch: 79, batch: 24, loss: 0.6633055210113525, acc: 76.5625, f1: 53.624712111893594, r: 0.559167249973056
06/02/2019 09:24:57 step: 2637, epoch: 79, batch: 29, loss: 0.7869529128074646, acc: 62.5, f1: 45.84453293512527, r: 0.4665048480698376
06/02/2019 09:24:57 *** evaluating ***
06/02/2019 09:24:58 step: 80, epoch: 79, acc: 60.256410256410255, f1: 30.383586190327318, r: 0.41379258871280555
06/02/2019 09:24:58 *** epoch: 81 ***
06/02/2019 09:24:58 *** training ***
06/02/2019 09:24:59 step: 2645, epoch: 80, batch: 4, loss: 0.7497849464416504, acc: 73.4375, f1: 56.44015444015443, r: 0.48503046607170164
06/02/2019 09:25:00 step: 2650, epoch: 80, batch: 9, loss: 0.9119251370429993, acc: 76.5625, f1: 60.346662186379916, r: 0.5447030486003125
06/02/2019 09:25:01 step: 2655, epoch: 80, batch: 14, loss: 0.8391137719154358, acc: 65.625, f1: 42.3623902334547, r: 0.4207544762095496
06/02/2019 09:25:02 step: 2660, epoch: 80, batch: 19, loss: 1.0215778350830078, acc: 54.6875, f1: 33.58843537414966, r: 0.5040744124410421
06/02/2019 09:25:03 step: 2665, epoch: 80, batch: 24, loss: 0.7886698842048645, acc: 70.3125, f1: 39.820367955961174, r: 0.5522312635244768
06/02/2019 09:25:05 step: 2670, epoch: 80, batch: 29, loss: 0.6926762461662292, acc: 78.125, f1: 51.911302982731556, r: 0.536846520040509
06/02/2019 09:25:05 *** evaluating ***
06/02/2019 09:25:06 step: 81, epoch: 80, acc: 61.111111111111114, f1: 30.003932324844435, r: 0.4065159300479038
06/02/2019 09:25:06 *** epoch: 82 ***
06/02/2019 09:25:06 *** training ***
06/02/2019 09:25:07 step: 2678, epoch: 81, batch: 4, loss: 0.8264844417572021, acc: 62.5, f1: 46.402533413879034, r: 0.6197234804057747
06/02/2019 09:25:08 step: 2683, epoch: 81, batch: 9, loss: 0.8479236364364624, acc: 62.5, f1: 38.34281789638933, r: 0.5846550667148118
06/02/2019 09:25:09 step: 2688, epoch: 81, batch: 14, loss: 0.7328988909721375, acc: 71.875, f1: 38.41277890466531, r: 0.5061780654952878
06/02/2019 09:25:10 step: 2693, epoch: 81, batch: 19, loss: 0.8703904747962952, acc: 56.25, f1: 44.96461575086027, r: 0.5061610526102485
06/02/2019 09:25:12 step: 2698, epoch: 81, batch: 24, loss: 0.8356879353523254, acc: 54.6875, f1: 39.40705128205128, r: 0.5615158604559948
06/02/2019 09:25:13 step: 2703, epoch: 81, batch: 29, loss: 0.7743511199951172, acc: 67.1875, f1: 43.37805052090767, r: 0.5193912704011248
06/02/2019 09:25:13 *** evaluating ***
06/02/2019 09:25:14 step: 82, epoch: 81, acc: 60.68376068376068, f1: 27.188585002568054, r: 0.40664147560422625
06/02/2019 09:25:14 *** epoch: 83 ***
06/02/2019 09:25:14 *** training ***
06/02/2019 09:25:15 step: 2711, epoch: 82, batch: 4, loss: 0.9265252351760864, acc: 54.6875, f1: 46.92799383904505, r: 0.49357068973960133
06/02/2019 09:25:16 step: 2716, epoch: 82, batch: 9, loss: 0.9616166949272156, acc: 56.25, f1: 30.865384615384617, r: 0.4582301911307988
06/02/2019 09:25:17 step: 2721, epoch: 82, batch: 14, loss: 0.6636884808540344, acc: 67.1875, f1: 49.46505357996042, r: 0.48334988708985427
06/02/2019 09:25:18 step: 2726, epoch: 82, batch: 19, loss: 1.0319634675979614, acc: 53.125, f1: 33.53002070393375, r: 0.4666531869217949
06/02/2019 09:25:19 step: 2731, epoch: 82, batch: 24, loss: 0.7360870838165283, acc: 68.75, f1: 40.4497234465474, r: 0.5323211300900155
06/02/2019 09:25:20 step: 2736, epoch: 82, batch: 29, loss: 0.7227188944816589, acc: 67.1875, f1: 38.82842245648564, r: 0.5278717878259443
06/02/2019 09:25:21 *** evaluating ***
06/02/2019 09:25:21 step: 83, epoch: 82, acc: 60.256410256410255, f1: 30.696176052213204, r: 0.4230740082092171
06/02/2019 09:25:21 *** epoch: 84 ***
06/02/2019 09:25:21 *** training ***
06/02/2019 09:25:22 step: 2744, epoch: 83, batch: 4, loss: 0.7658604383468628, acc: 68.75, f1: 48.25332405689549, r: 0.5938740791870932
06/02/2019 09:25:24 step: 2749, epoch: 83, batch: 9, loss: 0.7951152324676514, acc: 68.75, f1: 46.20905292072119, r: 0.5139239018520156
06/02/2019 09:25:25 step: 2754, epoch: 83, batch: 14, loss: 0.7146836519241333, acc: 73.4375, f1: 61.85969935215129, r: 0.5163344678913632
06/02/2019 09:25:26 step: 2759, epoch: 83, batch: 19, loss: 0.5163542628288269, acc: 84.375, f1: 53.37229437229437, r: 0.48367156293312674
06/02/2019 09:25:27 step: 2764, epoch: 83, batch: 24, loss: 0.7177177667617798, acc: 73.4375, f1: 54.887218045112775, r: 0.5602091942388152
06/02/2019 09:25:28 step: 2769, epoch: 83, batch: 29, loss: 0.7985262274742126, acc: 70.3125, f1: 54.6267539273774, r: 0.5095704511933559
06/02/2019 09:25:29 *** evaluating ***
06/02/2019 09:25:29 step: 84, epoch: 83, acc: 59.82905982905983, f1: 27.200221293110836, r: 0.39801277276511876
06/02/2019 09:25:29 *** epoch: 85 ***
06/02/2019 09:25:29 *** training ***
06/02/2019 09:25:31 step: 2777, epoch: 84, batch: 4, loss: 0.7184005379676819, acc: 75.0, f1: 45.72642543859649, r: 0.6226351918006774
06/02/2019 09:25:32 step: 2782, epoch: 84, batch: 9, loss: 0.8200368881225586, acc: 62.5, f1: 58.37241190182366, r: 0.49305951588770686
06/02/2019 09:25:33 step: 2787, epoch: 84, batch: 14, loss: 0.8298056721687317, acc: 65.625, f1: 43.07769423558897, r: 0.46669920304692164
06/02/2019 09:25:34 step: 2792, epoch: 84, batch: 19, loss: 0.8880925178527832, acc: 70.3125, f1: 47.70346003898636, r: 0.5445466734827558
06/02/2019 09:25:35 step: 2797, epoch: 84, batch: 24, loss: 0.8958919048309326, acc: 68.75, f1: 49.49971655328799, r: 0.4814561781652513
06/02/2019 09:25:36 step: 2802, epoch: 84, batch: 29, loss: 0.608967125415802, acc: 78.125, f1: 52.224513528740445, r: 0.4961540614054135
06/02/2019 09:25:37 *** evaluating ***
06/02/2019 09:25:37 step: 85, epoch: 84, acc: 60.68376068376068, f1: 28.042543750173337, r: 0.4135287925904236
06/02/2019 09:25:37 *** epoch: 86 ***
06/02/2019 09:25:37 *** training ***
06/02/2019 09:25:38 step: 2810, epoch: 85, batch: 4, loss: 0.5661229491233826, acc: 79.6875, f1: 47.78195488721804, r: 0.5831147838122258
06/02/2019 09:25:39 step: 2815, epoch: 85, batch: 9, loss: 0.8161286115646362, acc: 60.9375, f1: 49.73860606513668, r: 0.4936175642909702
06/02/2019 09:25:40 step: 2820, epoch: 85, batch: 14, loss: 0.9792572259902954, acc: 57.8125, f1: 32.46446488294315, r: 0.4934472679022439
06/02/2019 09:25:41 step: 2825, epoch: 85, batch: 19, loss: 0.6620723009109497, acc: 75.0, f1: 51.3390619405657, r: 0.5940733377630589
06/02/2019 09:25:42 step: 2830, epoch: 85, batch: 24, loss: 0.730468213558197, acc: 73.4375, f1: 50.26197713127804, r: 0.4320517403598063
06/02/2019 09:25:44 step: 2835, epoch: 85, batch: 29, loss: 0.6502296924591064, acc: 78.125, f1: 49.33898514172926, r: 0.6160463149251286
06/02/2019 09:25:44 *** evaluating ***
06/02/2019 09:25:45 step: 86, epoch: 85, acc: 61.111111111111114, f1: 30.521775670822244, r: 0.4175482451198175
06/02/2019 09:25:45 *** epoch: 87 ***
06/02/2019 09:25:45 *** training ***
06/02/2019 09:25:46 step: 2843, epoch: 86, batch: 4, loss: 0.7564613223075867, acc: 76.5625, f1: 53.40069267538713, r: 0.46327204788203025
06/02/2019 09:25:47 step: 2848, epoch: 86, batch: 9, loss: 0.7734664678573608, acc: 65.625, f1: 47.15740500676979, r: 0.46447635447496355
06/02/2019 09:25:48 step: 2853, epoch: 86, batch: 14, loss: 0.7588921189308167, acc: 68.75, f1: 41.2037962037962, r: 0.4553481532462656
06/02/2019 09:25:49 step: 2858, epoch: 86, batch: 19, loss: 0.8514256477355957, acc: 64.0625, f1: 40.30072256175197, r: 0.48243950834150473
06/02/2019 09:25:51 step: 2863, epoch: 86, batch: 24, loss: 0.5908918380737305, acc: 76.5625, f1: 57.51064854253861, r: 0.5424575603846301
06/02/2019 09:25:52 step: 2868, epoch: 86, batch: 29, loss: 0.9344882369041443, acc: 62.5, f1: 39.858613613849705, r: 0.49622363439254485
06/02/2019 09:25:53 *** evaluating ***
06/02/2019 09:25:53 step: 87, epoch: 86, acc: 60.256410256410255, f1: 28.622079772571617, r: 0.412122086860586
06/02/2019 09:25:53 *** epoch: 88 ***
06/02/2019 09:25:53 *** training ***
06/02/2019 09:25:54 step: 2876, epoch: 87, batch: 4, loss: 0.7605022192001343, acc: 67.1875, f1: 44.29787940817352, r: 0.5587930014340236
06/02/2019 09:25:55 step: 2881, epoch: 87, batch: 9, loss: 0.7348345518112183, acc: 71.875, f1: 43.86387784953358, r: 0.5261153512780219
06/02/2019 09:25:57 step: 2886, epoch: 87, batch: 14, loss: 0.6841109991073608, acc: 76.5625, f1: 54.19833048157516, r: 0.4962407852998807
06/02/2019 09:25:58 step: 2891, epoch: 87, batch: 19, loss: 0.7558156251907349, acc: 70.3125, f1: 48.8781907747425, r: 0.45442557277279516
06/02/2019 09:25:59 step: 2896, epoch: 87, batch: 24, loss: 0.7113609313964844, acc: 75.0, f1: 56.67800453514739, r: 0.6295719628978218
06/02/2019 09:26:00 step: 2901, epoch: 87, batch: 29, loss: 0.8708421587944031, acc: 62.5, f1: 43.86433946488294, r: 0.49357258840866675
06/02/2019 09:26:01 *** evaluating ***
06/02/2019 09:26:01 step: 88, epoch: 87, acc: 61.53846153846154, f1: 29.324804206105036, r: 0.3870083727473971
06/02/2019 09:26:01 *** epoch: 89 ***
06/02/2019 09:26:01 *** training ***
06/02/2019 09:26:02 step: 2909, epoch: 88, batch: 4, loss: 0.72707200050354, acc: 68.75, f1: 33.27724358974359, r: 0.5075043530849277
06/02/2019 09:26:03 step: 2914, epoch: 88, batch: 9, loss: 0.5689873099327087, acc: 78.125, f1: 52.85650022492128, r: 0.5409029353458733
06/02/2019 09:26:05 step: 2919, epoch: 88, batch: 14, loss: 0.49900150299072266, acc: 78.125, f1: 65.00949667616335, r: 0.5199761538405977
06/02/2019 09:26:06 step: 2924, epoch: 88, batch: 19, loss: 0.8265103101730347, acc: 67.1875, f1: 45.43346774193549, r: 0.5568456621546168
06/02/2019 09:26:07 step: 2929, epoch: 88, batch: 24, loss: 0.8930478692054749, acc: 62.5, f1: 41.89872325741891, r: 0.5698365366083716
06/02/2019 09:26:08 step: 2934, epoch: 88, batch: 29, loss: 0.5107108950614929, acc: 82.8125, f1: 64.49483568075118, r: 0.3836123590088694
06/02/2019 09:26:09 *** evaluating ***
06/02/2019 09:26:09 step: 89, epoch: 88, acc: 59.82905982905983, f1: 28.455476753349096, r: 0.4117319642208789
06/02/2019 09:26:09 *** epoch: 90 ***
06/02/2019 09:26:09 *** training ***
06/02/2019 09:26:10 step: 2942, epoch: 89, batch: 4, loss: 0.6354942321777344, acc: 76.5625, f1: 42.825386978612784, r: 0.46256020127581854
06/02/2019 09:26:11 step: 2947, epoch: 89, batch: 9, loss: 0.8766159415245056, acc: 64.0625, f1: 48.85064223057644, r: 0.5287869651702695
06/02/2019 09:26:12 step: 2952, epoch: 89, batch: 14, loss: 0.7320942282676697, acc: 73.4375, f1: 51.23287685399487, r: 0.4733386682130995
06/02/2019 09:26:13 step: 2957, epoch: 89, batch: 19, loss: 0.6148636341094971, acc: 75.0, f1: 60.473292765382034, r: 0.6204931796557218
06/02/2019 09:26:14 step: 2962, epoch: 89, batch: 24, loss: 0.7918394207954407, acc: 70.3125, f1: 39.26256062478973, r: 0.4906020746648377
06/02/2019 09:26:16 step: 2967, epoch: 89, batch: 29, loss: 0.6220831871032715, acc: 78.125, f1: 53.60552115583075, r: 0.5681892619732629
06/02/2019 09:26:16 *** evaluating ***
06/02/2019 09:26:16 step: 90, epoch: 89, acc: 62.39316239316239, f1: 28.443737771769, r: 0.3838890268651199
06/02/2019 09:26:16 *** epoch: 91 ***
06/02/2019 09:26:16 *** training ***
06/02/2019 09:26:18 step: 2975, epoch: 90, batch: 4, loss: 0.542848527431488, acc: 76.5625, f1: 63.66834141900689, r: 0.6314506234905968
06/02/2019 09:26:19 step: 2980, epoch: 90, batch: 9, loss: 0.6008083820343018, acc: 81.25, f1: 62.40143369175627, r: 0.5520563850880205
06/02/2019 09:26:20 step: 2985, epoch: 90, batch: 14, loss: 0.7460017800331116, acc: 76.5625, f1: 54.141615416781406, r: 0.5742216760458319
06/02/2019 09:26:21 step: 2990, epoch: 90, batch: 19, loss: 0.6801759004592896, acc: 73.4375, f1: 54.29816735275359, r: 0.5348266546553928
06/02/2019 09:26:22 step: 2995, epoch: 90, batch: 24, loss: 0.6934571862220764, acc: 78.125, f1: 55.74150114591292, r: 0.6423375779332765
06/02/2019 09:26:23 step: 3000, epoch: 90, batch: 29, loss: 0.580619752407074, acc: 75.0, f1: 58.08033161806747, r: 0.6248163079859971
06/02/2019 09:26:24 *** evaluating ***
06/02/2019 09:26:24 step: 91, epoch: 90, acc: 58.97435897435898, f1: 28.736569802746274, r: 0.43023263921393845
06/02/2019 09:26:24 *** epoch: 92 ***
06/02/2019 09:26:24 *** training ***
06/02/2019 09:26:26 step: 3008, epoch: 91, batch: 4, loss: 0.7089540362358093, acc: 68.75, f1: 43.847600721170515, r: 0.5889766975846421
06/02/2019 09:26:27 step: 3013, epoch: 91, batch: 9, loss: 0.6476161479949951, acc: 76.5625, f1: 51.69312169312169, r: 0.4403468501591786
06/02/2019 09:26:28 step: 3018, epoch: 91, batch: 14, loss: 0.6047285795211792, acc: 75.0, f1: 55.62078272604588, r: 0.6163700259341209
06/02/2019 09:26:29 step: 3023, epoch: 91, batch: 19, loss: 0.6106693148612976, acc: 75.0, f1: 62.687802393684755, r: 0.47857404953611427
06/02/2019 09:26:30 step: 3028, epoch: 91, batch: 24, loss: 0.6014519333839417, acc: 79.6875, f1: 64.93431380649426, r: 0.49519668784468074
06/02/2019 09:26:32 step: 3033, epoch: 91, batch: 29, loss: 0.7557578682899475, acc: 71.875, f1: 66.45458183273308, r: 0.45364778218985025
06/02/2019 09:26:32 *** evaluating ***
06/02/2019 09:26:33 step: 92, epoch: 91, acc: 60.68376068376068, f1: 29.040032294509654, r: 0.40422315515352936
06/02/2019 09:26:33 *** epoch: 93 ***
06/02/2019 09:26:33 *** training ***
06/02/2019 09:26:33 step: 3041, epoch: 92, batch: 4, loss: 0.6730521321296692, acc: 73.4375, f1: 51.31975867269985, r: 0.6143635653753498
06/02/2019 09:26:35 step: 3046, epoch: 92, batch: 9, loss: 0.5993654131889343, acc: 75.0, f1: 59.45839874411304, r: 0.5691216875160865
06/02/2019 09:26:36 step: 3051, epoch: 92, batch: 14, loss: 0.6044365763664246, acc: 71.875, f1: 39.51149425287356, r: 0.5200007215173152
06/02/2019 09:26:37 step: 3056, epoch: 92, batch: 19, loss: 0.6247935891151428, acc: 71.875, f1: 48.54195011337868, r: 0.5330436456547484
06/02/2019 09:26:38 step: 3061, epoch: 92, batch: 24, loss: 0.7533055543899536, acc: 68.75, f1: 40.01683501683502, r: 0.5609136623371543
06/02/2019 09:26:40 step: 3066, epoch: 92, batch: 29, loss: 0.7099324464797974, acc: 73.4375, f1: 54.21118233618234, r: 0.5426043775067038
06/02/2019 09:26:40 *** evaluating ***
06/02/2019 09:26:41 step: 93, epoch: 92, acc: 61.965811965811966, f1: 29.798896904108936, r: 0.40504428482245247
06/02/2019 09:26:41 *** epoch: 94 ***
06/02/2019 09:26:41 *** training ***
06/02/2019 09:26:42 step: 3074, epoch: 93, batch: 4, loss: 0.6635443568229675, acc: 68.75, f1: 55.302301358626295, r: 0.48033789233511176
06/02/2019 09:26:43 step: 3079, epoch: 93, batch: 9, loss: 0.6902341842651367, acc: 70.3125, f1: 48.86059474294768, r: 0.43812857825627444
06/02/2019 09:26:44 step: 3084, epoch: 93, batch: 14, loss: 0.8256083726882935, acc: 75.0, f1: 49.37527162103434, r: 0.5497900074438079
06/02/2019 09:26:46 step: 3089, epoch: 93, batch: 19, loss: 0.6057673096656799, acc: 78.125, f1: 59.22902494331065, r: 0.5655444375497798
06/02/2019 09:26:47 step: 3094, epoch: 93, batch: 24, loss: 0.6834418773651123, acc: 62.5, f1: 47.26925764661614, r: 0.5367403091010112
06/02/2019 09:26:48 step: 3099, epoch: 93, batch: 29, loss: 0.7129945755004883, acc: 71.875, f1: 46.918538836642284, r: 0.623897578644422
06/02/2019 09:26:48 *** evaluating ***
06/02/2019 09:26:49 step: 94, epoch: 93, acc: 58.54700854700855, f1: 27.5617894779563, r: 0.40017955880737777
06/02/2019 09:26:49 *** epoch: 95 ***
06/02/2019 09:26:49 *** training ***
06/02/2019 09:26:50 step: 3107, epoch: 94, batch: 4, loss: 0.9169939160346985, acc: 65.625, f1: 47.61189582215866, r: 0.5020848174806517
06/02/2019 09:26:51 step: 3112, epoch: 94, batch: 9, loss: 0.6262579560279846, acc: 79.6875, f1: 62.76209677419355, r: 0.5133267073978293
06/02/2019 09:26:52 step: 3117, epoch: 94, batch: 14, loss: 0.6215992569923401, acc: 76.5625, f1: 57.85597914630173, r: 0.5927356469625633
06/02/2019 09:26:53 step: 3122, epoch: 94, batch: 19, loss: 0.6255505084991455, acc: 71.875, f1: 38.57140243009809, r: 0.5132046349938932
06/02/2019 09:26:54 step: 3127, epoch: 94, batch: 24, loss: 0.5847756266593933, acc: 82.8125, f1: 65.25172544839985, r: 0.600463582233956
06/02/2019 09:26:56 step: 3132, epoch: 94, batch: 29, loss: 0.7925722599029541, acc: 68.75, f1: 51.220834303541075, r: 0.5667458596377661
06/02/2019 09:26:56 *** evaluating ***
06/02/2019 09:26:57 step: 95, epoch: 94, acc: 58.119658119658126, f1: 27.636120836699885, r: 0.3957724261774027
06/02/2019 09:26:57 *** epoch: 96 ***
06/02/2019 09:26:57 *** training ***
06/02/2019 09:26:58 step: 3140, epoch: 95, batch: 4, loss: 0.4241771101951599, acc: 87.5, f1: 66.46436896436896, r: 0.6803497394770687
06/02/2019 09:26:59 step: 3145, epoch: 95, batch: 9, loss: 0.5355947613716125, acc: 84.375, f1: 75.47619047619048, r: 0.5662302559161626
06/02/2019 09:27:00 step: 3150, epoch: 95, batch: 14, loss: 0.7926121950149536, acc: 71.875, f1: 42.685274109643856, r: 0.4795598330563682
06/02/2019 09:27:01 step: 3155, epoch: 95, batch: 19, loss: 0.5967255234718323, acc: 78.125, f1: 53.44094439839121, r: 0.5582282845532396
06/02/2019 09:27:02 step: 3160, epoch: 95, batch: 24, loss: 0.5324552655220032, acc: 79.6875, f1: 60.92270241429906, r: 0.515664114888417
06/02/2019 09:27:03 step: 3165, epoch: 95, batch: 29, loss: 0.7228793501853943, acc: 70.3125, f1: 44.3753947202223, r: 0.5182283911576533
06/02/2019 09:27:04 *** evaluating ***
06/02/2019 09:27:05 step: 96, epoch: 95, acc: 58.54700854700855, f1: 29.190186944342955, r: 0.40843024268098915
06/02/2019 09:27:05 *** epoch: 97 ***
06/02/2019 09:27:05 *** training ***
06/02/2019 09:27:06 step: 3173, epoch: 96, batch: 4, loss: 0.5142315030097961, acc: 78.125, f1: 63.56037151702787, r: 0.6351430207114305
06/02/2019 09:27:07 step: 3178, epoch: 96, batch: 9, loss: 0.5735035538673401, acc: 75.0, f1: 56.70542135214054, r: 0.5682050833773636
06/02/2019 09:27:08 step: 3183, epoch: 96, batch: 14, loss: 0.6164469718933105, acc: 73.4375, f1: 55.8843537414966, r: 0.48384458542333764
06/02/2019 09:27:09 step: 3188, epoch: 96, batch: 19, loss: 0.7695555686950684, acc: 73.4375, f1: 52.48137530746227, r: 0.4339397245394555
06/02/2019 09:27:10 step: 3193, epoch: 96, batch: 24, loss: 0.5726103186607361, acc: 81.25, f1: 67.10760859367672, r: 0.4691687137924319
06/02/2019 09:27:12 step: 3198, epoch: 96, batch: 29, loss: 0.53734290599823, acc: 79.6875, f1: 67.05286100281998, r: 0.5790775408445206
06/02/2019 09:27:12 *** evaluating ***
06/02/2019 09:27:13 step: 97, epoch: 96, acc: 60.68376068376068, f1: 28.31858983262727, r: 0.3999569406927611
06/02/2019 09:27:13 *** epoch: 98 ***
06/02/2019 09:27:13 *** training ***
06/02/2019 09:27:14 step: 3206, epoch: 97, batch: 4, loss: 0.7267876863479614, acc: 67.1875, f1: 46.511243386243386, r: 0.5802996417077354
06/02/2019 09:27:15 step: 3211, epoch: 97, batch: 9, loss: 0.6802021861076355, acc: 75.0, f1: 52.54760656883298, r: 0.5544636517381479
06/02/2019 09:27:16 step: 3216, epoch: 97, batch: 14, loss: 0.6344857215881348, acc: 78.125, f1: 66.42302867404602, r: 0.6026774677742963
06/02/2019 09:27:17 step: 3221, epoch: 97, batch: 19, loss: 0.46371468901634216, acc: 82.8125, f1: 54.658452510168296, r: 0.509414432017691
06/02/2019 09:27:18 step: 3226, epoch: 97, batch: 24, loss: 0.6927690505981445, acc: 71.875, f1: 50.66025641025641, r: 0.5361251307907601
06/02/2019 09:27:19 step: 3231, epoch: 97, batch: 29, loss: 0.7689982652664185, acc: 76.5625, f1: 60.38647342995169, r: 0.5373460625200569
06/02/2019 09:27:20 *** evaluating ***
06/02/2019 09:27:21 step: 98, epoch: 97, acc: 57.26495726495726, f1: 27.542734846709926, r: 0.4016066951794749
06/02/2019 09:27:21 *** epoch: 99 ***
06/02/2019 09:27:21 *** training ***
06/02/2019 09:27:22 step: 3239, epoch: 98, batch: 4, loss: 0.7182301878929138, acc: 68.75, f1: 43.19372211820725, r: 0.5175991089815621
06/02/2019 09:27:23 step: 3244, epoch: 98, batch: 9, loss: 0.730221152305603, acc: 70.3125, f1: 50.17358727979868, r: 0.5657799515886872
06/02/2019 09:27:24 step: 3249, epoch: 98, batch: 14, loss: 0.8113634586334229, acc: 70.3125, f1: 43.86984685178419, r: 0.5746657858113865
06/02/2019 09:27:25 step: 3254, epoch: 98, batch: 19, loss: 0.5238344669342041, acc: 76.5625, f1: 70.27200577200577, r: 0.5530320109000121
06/02/2019 09:27:26 step: 3259, epoch: 98, batch: 24, loss: 0.6790526509284973, acc: 73.4375, f1: 54.133064516129025, r: 0.5709325894840783
06/02/2019 09:27:27 step: 3264, epoch: 98, batch: 29, loss: 0.7052615880966187, acc: 73.4375, f1: 50.71428571428571, r: 0.4984479199040783
06/02/2019 09:27:28 *** evaluating ***
06/02/2019 09:27:28 step: 99, epoch: 98, acc: 61.111111111111114, f1: 33.4657560856191, r: 0.41576017503185925
06/02/2019 09:27:28 *** epoch: 100 ***
06/02/2019 09:27:28 *** training ***
06/02/2019 09:27:29 step: 3272, epoch: 99, batch: 4, loss: 0.697022557258606, acc: 76.5625, f1: 57.578167115902964, r: 0.5905772288222777
06/02/2019 09:27:31 step: 3277, epoch: 99, batch: 9, loss: 0.6044871807098389, acc: 79.6875, f1: 60.79527478542257, r: 0.5343491602633428
06/02/2019 09:27:32 step: 3282, epoch: 99, batch: 14, loss: 0.4912482500076294, acc: 84.375, f1: 57.33353947639662, r: 0.5302498511746747
06/02/2019 09:27:33 step: 3287, epoch: 99, batch: 19, loss: 0.5483042001724243, acc: 78.125, f1: 55.52694924123496, r: 0.6372690866381754
06/02/2019 09:27:34 step: 3292, epoch: 99, batch: 24, loss: 0.7634144425392151, acc: 68.75, f1: 57.04110704110704, r: 0.5032290132555424
06/02/2019 09:27:35 step: 3297, epoch: 99, batch: 29, loss: 0.6018183827400208, acc: 81.25, f1: 67.63581910751722, r: 0.5645181966710313
06/02/2019 09:27:36 *** evaluating ***
06/02/2019 09:27:36 step: 100, epoch: 99, acc: 60.68376068376068, f1: 28.058104939411066, r: 0.3857613938490147
06/02/2019 09:27:36 *** epoch: 101 ***
06/02/2019 09:27:36 *** training ***
06/02/2019 09:27:37 step: 3305, epoch: 100, batch: 4, loss: 0.8206942677497864, acc: 67.1875, f1: 46.28811569636038, r: 0.5226803283862709
06/02/2019 09:27:38 step: 3310, epoch: 100, batch: 9, loss: 0.7119553089141846, acc: 68.75, f1: 51.25256322624744, r: 0.5410004975807304
06/02/2019 09:27:39 step: 3315, epoch: 100, batch: 14, loss: 0.49697378277778625, acc: 76.5625, f1: 55.69673222883543, r: 0.5273642295074058
06/02/2019 09:27:40 step: 3320, epoch: 100, batch: 19, loss: 0.4907580614089966, acc: 82.8125, f1: 64.88240357092818, r: 0.5895948804218142
06/02/2019 09:27:42 step: 3325, epoch: 100, batch: 24, loss: 0.6709811687469482, acc: 68.75, f1: 53.96755271755273, r: 0.6206796197106681
06/02/2019 09:27:43 step: 3330, epoch: 100, batch: 29, loss: 0.7514733076095581, acc: 76.5625, f1: 55.039372216791584, r: 0.6085321436218971
06/02/2019 09:27:44 *** evaluating ***
06/02/2019 09:27:44 step: 101, epoch: 100, acc: 61.111111111111114, f1: 29.892762763191726, r: 0.4106652697214971
06/02/2019 09:27:44 *** epoch: 102 ***
06/02/2019 09:27:44 *** training ***
06/02/2019 09:27:45 step: 3338, epoch: 101, batch: 4, loss: 0.6243862509727478, acc: 73.4375, f1: 51.946305726793526, r: 0.5556284509370089
06/02/2019 09:27:46 step: 3343, epoch: 101, batch: 9, loss: 0.7121458649635315, acc: 65.625, f1: 42.53075871496924, r: 0.49614590963534033
06/02/2019 09:27:47 step: 3348, epoch: 101, batch: 14, loss: 0.5903905034065247, acc: 73.4375, f1: 58.85321930190515, r: 0.6736790190497559
06/02/2019 09:27:49 step: 3353, epoch: 101, batch: 19, loss: 0.804350733757019, acc: 67.1875, f1: 38.980217086834735, r: 0.5034923172706056
06/02/2019 09:27:50 step: 3358, epoch: 101, batch: 24, loss: 0.844611406326294, acc: 78.125, f1: 61.40642873484329, r: 0.5785368311672948
06/02/2019 09:27:51 step: 3363, epoch: 101, batch: 29, loss: 0.39895308017730713, acc: 82.8125, f1: 62.886444187107415, r: 0.5982988633699118
06/02/2019 09:27:52 *** evaluating ***
06/02/2019 09:27:52 step: 102, epoch: 101, acc: 58.97435897435898, f1: 30.128016733521324, r: 0.43034267951777305
06/02/2019 09:27:52 *** epoch: 103 ***
06/02/2019 09:27:52 *** training ***
06/02/2019 09:27:53 step: 3371, epoch: 102, batch: 4, loss: 0.6863922476768494, acc: 64.0625, f1: 34.70495837223778, r: 0.5223673914984073
06/02/2019 09:27:54 step: 3376, epoch: 102, batch: 9, loss: 0.5420974493026733, acc: 78.125, f1: 46.958443854995586, r: 0.6404256681603331
06/02/2019 09:27:55 step: 3381, epoch: 102, batch: 14, loss: 0.6222095489501953, acc: 73.4375, f1: 45.185431110243144, r: 0.4756626498492398
06/02/2019 09:27:56 step: 3386, epoch: 102, batch: 19, loss: 0.5382418632507324, acc: 81.25, f1: 67.06349206349206, r: 0.5074661271047796
06/02/2019 09:27:58 step: 3391, epoch: 102, batch: 24, loss: 0.5221909880638123, acc: 78.125, f1: 63.13109272678005, r: 0.5681159515894231
06/02/2019 09:27:59 step: 3396, epoch: 102, batch: 29, loss: 0.6843411326408386, acc: 76.5625, f1: 54.18740501519756, r: 0.4998823605942284
06/02/2019 09:27:59 *** evaluating ***
06/02/2019 09:28:00 step: 103, epoch: 102, acc: 60.256410256410255, f1: 27.67592158700406, r: 0.39217049956020406
06/02/2019 09:28:00 *** epoch: 104 ***
06/02/2019 09:28:00 *** training ***
06/02/2019 09:28:01 step: 3404, epoch: 103, batch: 4, loss: 0.4657711684703827, acc: 84.375, f1: 71.4331277745912, r: 0.5228428356753847
06/02/2019 09:28:02 step: 3409, epoch: 103, batch: 9, loss: 0.6526386737823486, acc: 73.4375, f1: 57.13274820417678, r: 0.5488805627226059
06/02/2019 09:28:04 step: 3414, epoch: 103, batch: 14, loss: 0.6176275610923767, acc: 79.6875, f1: 49.66248037676609, r: 0.5080514373916595
06/02/2019 09:28:05 step: 3419, epoch: 103, batch: 19, loss: 0.661523699760437, acc: 85.9375, f1: 70.17385417566906, r: 0.5808666238628303
06/02/2019 09:28:06 step: 3424, epoch: 103, batch: 24, loss: 0.6596202850341797, acc: 73.4375, f1: 63.0009276437848, r: 0.5261713797817611
06/02/2019 09:28:07 step: 3429, epoch: 103, batch: 29, loss: 0.6101018190383911, acc: 71.875, f1: 48.01323676323676, r: 0.570539561429552
06/02/2019 09:28:07 *** evaluating ***
06/02/2019 09:28:08 step: 104, epoch: 103, acc: 61.965811965811966, f1: 34.0502794860593, r: 0.409228031337285
06/02/2019 09:28:08 *** epoch: 105 ***
06/02/2019 09:28:08 *** training ***
06/02/2019 09:28:09 step: 3437, epoch: 104, batch: 4, loss: 0.6317991018295288, acc: 79.6875, f1: 49.22979797979798, r: 0.5742427696018538
06/02/2019 09:28:10 step: 3442, epoch: 104, batch: 9, loss: 0.5930183529853821, acc: 76.5625, f1: 56.003043176956226, r: 0.468361660629297
06/02/2019 09:28:11 step: 3447, epoch: 104, batch: 14, loss: 0.5589987635612488, acc: 79.6875, f1: 50.94332298136646, r: 0.5751964995237837
06/02/2019 09:28:12 step: 3452, epoch: 104, batch: 19, loss: 0.7129389047622681, acc: 67.1875, f1: 51.73053073053073, r: 0.44980910445857103
06/02/2019 09:28:13 step: 3457, epoch: 104, batch: 24, loss: 0.5517359375953674, acc: 78.125, f1: 62.43783068783068, r: 0.4709657214591522
06/02/2019 09:28:15 step: 3462, epoch: 104, batch: 29, loss: 0.47667139768600464, acc: 81.25, f1: 65.59309309309309, r: 0.6259232055523746
06/02/2019 09:28:15 *** evaluating ***
06/02/2019 09:28:16 step: 105, epoch: 104, acc: 58.54700854700855, f1: 27.98489111950193, r: 0.40752104229677166
06/02/2019 09:28:16 *** epoch: 106 ***
06/02/2019 09:28:16 *** training ***
06/02/2019 09:28:17 step: 3470, epoch: 105, batch: 4, loss: 0.5741552114486694, acc: 78.125, f1: 61.71248256387576, r: 0.5840917507057021
06/02/2019 09:28:18 step: 3475, epoch: 105, batch: 9, loss: 0.5137556791305542, acc: 84.375, f1: 73.50461133069828, r: 0.6042613943867379
06/02/2019 09:28:19 step: 3480, epoch: 105, batch: 14, loss: 0.6745004057884216, acc: 78.125, f1: 58.890118832397874, r: 0.6158563920472943
06/02/2019 09:28:20 step: 3485, epoch: 105, batch: 19, loss: 0.589381992816925, acc: 71.875, f1: 49.119817509040445, r: 0.652742876735586
06/02/2019 09:28:21 step: 3490, epoch: 105, batch: 24, loss: 0.6667238473892212, acc: 75.0, f1: 47.179962894248604, r: 0.4529572719920187
06/02/2019 09:28:23 step: 3495, epoch: 105, batch: 29, loss: 0.587175726890564, acc: 71.875, f1: 44.300755741433704, r: 0.4890538952306088
06/02/2019 09:28:23 *** evaluating ***
06/02/2019 09:28:24 step: 106, epoch: 105, acc: 59.401709401709404, f1: 28.771558790740382, r: 0.3954333276702612
06/02/2019 09:28:24 *** epoch: 107 ***
06/02/2019 09:28:24 *** training ***
06/02/2019 09:28:24 step: 3503, epoch: 106, batch: 4, loss: 0.457844614982605, acc: 87.5, f1: 69.65771801298116, r: 0.7120973326078817
06/02/2019 09:28:26 step: 3508, epoch: 106, batch: 9, loss: 0.44905704259872437, acc: 81.25, f1: 69.50256074970034, r: 0.5206135086514607
06/02/2019 09:28:27 step: 3513, epoch: 106, batch: 14, loss: 0.5066943764686584, acc: 79.6875, f1: 68.24417284445336, r: 0.5565512104532043
06/02/2019 09:28:28 step: 3518, epoch: 106, batch: 19, loss: 0.5563092231750488, acc: 76.5625, f1: 59.032077553944674, r: 0.6822953480772934
06/02/2019 09:28:29 step: 3523, epoch: 106, batch: 24, loss: 0.5994910597801208, acc: 81.25, f1: 70.67617666021921, r: 0.5692860926188978
06/02/2019 09:28:31 step: 3528, epoch: 106, batch: 29, loss: 0.6435436010360718, acc: 73.4375, f1: 56.98572261072261, r: 0.6157107648763137
06/02/2019 09:28:31 *** evaluating ***
06/02/2019 09:28:32 step: 107, epoch: 106, acc: 59.82905982905983, f1: 29.068840213073454, r: 0.41948402128614726
06/02/2019 09:28:32 *** epoch: 108 ***
06/02/2019 09:28:32 *** training ***
06/02/2019 09:28:33 step: 3536, epoch: 107, batch: 4, loss: 0.6216514706611633, acc: 78.125, f1: 63.30812045097759, r: 0.5315831491394868
06/02/2019 09:28:34 step: 3541, epoch: 107, batch: 9, loss: 0.7830621600151062, acc: 65.625, f1: 46.865251150965435, r: 0.4707182264676655
06/02/2019 09:28:35 step: 3546, epoch: 107, batch: 14, loss: 0.5799692273139954, acc: 73.4375, f1: 59.46242440004488, r: 0.6527061223737368
06/02/2019 09:28:36 step: 3551, epoch: 107, batch: 19, loss: 0.4778325855731964, acc: 85.9375, f1: 66.3138245074893, r: 0.6577798360892717
06/02/2019 09:28:37 step: 3556, epoch: 107, batch: 24, loss: 0.5831339359283447, acc: 78.125, f1: 58.03658642944358, r: 0.6199231598088112
06/02/2019 09:28:39 step: 3561, epoch: 107, batch: 29, loss: 0.47925183176994324, acc: 81.25, f1: 74.83942127575668, r: 0.608366573651294
06/02/2019 09:28:39 *** evaluating ***
06/02/2019 09:28:40 step: 108, epoch: 107, acc: 58.97435897435898, f1: 28.274843233646173, r: 0.413774640180498
06/02/2019 09:28:40 *** epoch: 109 ***
06/02/2019 09:28:40 *** training ***
06/02/2019 09:28:41 step: 3569, epoch: 108, batch: 4, loss: 0.7159331440925598, acc: 73.4375, f1: 55.69402228976696, r: 0.6364798226023366
06/02/2019 09:28:42 step: 3574, epoch: 108, batch: 9, loss: 0.658755898475647, acc: 70.3125, f1: 59.64009066895124, r: 0.5644583443008413
06/02/2019 09:28:43 step: 3579, epoch: 108, batch: 14, loss: 0.5123651623725891, acc: 85.9375, f1: 71.03232959850607, r: 0.6752755714163823
06/02/2019 09:28:44 step: 3584, epoch: 108, batch: 19, loss: 0.46718356013298035, acc: 79.6875, f1: 58.54669140383425, r: 0.5261204089254615
06/02/2019 09:28:45 step: 3589, epoch: 108, batch: 24, loss: 0.46797215938568115, acc: 75.0, f1: 41.80555555555555, r: 0.5831821387189312
06/02/2019 09:28:47 step: 3594, epoch: 108, batch: 29, loss: 0.51395583152771, acc: 76.5625, f1: 69.4551282051282, r: 0.637194498396235
06/02/2019 09:28:47 *** evaluating ***
06/02/2019 09:28:48 step: 109, epoch: 108, acc: 55.98290598290598, f1: 27.113480763994247, r: 0.4079316981695899
06/02/2019 09:28:48 *** epoch: 110 ***
06/02/2019 09:28:48 *** training ***
06/02/2019 09:28:49 step: 3602, epoch: 109, batch: 4, loss: 0.5437719821929932, acc: 76.5625, f1: 58.770923255079474, r: 0.499575837187609
06/02/2019 09:28:50 step: 3607, epoch: 109, batch: 9, loss: 0.5294057130813599, acc: 78.125, f1: 53.32589220095835, r: 0.6287535134190919
06/02/2019 09:28:51 step: 3612, epoch: 109, batch: 14, loss: 0.6511561274528503, acc: 68.75, f1: 49.228896103896105, r: 0.5512912356755482
06/02/2019 09:28:52 step: 3617, epoch: 109, batch: 19, loss: 0.5021246671676636, acc: 81.25, f1: 67.12835954053753, r: 0.5732567123249742
06/02/2019 09:28:53 step: 3622, epoch: 109, batch: 24, loss: 0.4974464476108551, acc: 76.5625, f1: 57.799433271131385, r: 0.6417071056499916
06/02/2019 09:28:54 step: 3627, epoch: 109, batch: 29, loss: 0.39825066924095154, acc: 85.9375, f1: 83.20299891728463, r: 0.5720968195717159
06/02/2019 09:28:55 *** evaluating ***
06/02/2019 09:28:55 step: 110, epoch: 109, acc: 56.41025641025641, f1: 30.787065812951685, r: 0.424504654045958
06/02/2019 09:28:55 *** epoch: 111 ***
06/02/2019 09:28:55 *** training ***
06/02/2019 09:28:56 step: 3635, epoch: 110, batch: 4, loss: 0.4799417555332184, acc: 81.25, f1: 63.43304314732886, r: 0.5060773009489851
06/02/2019 09:28:57 step: 3640, epoch: 110, batch: 9, loss: 0.543133556842804, acc: 73.4375, f1: 54.9577922077922, r: 0.647738296107216
06/02/2019 09:28:58 step: 3645, epoch: 110, batch: 14, loss: 0.5913574695587158, acc: 75.0, f1: 58.563560991486376, r: 0.5695312889561182
06/02/2019 09:29:00 step: 3650, epoch: 110, batch: 19, loss: 0.5348531007766724, acc: 78.125, f1: 52.35180565454978, r: 0.5894869509299263
06/02/2019 09:29:01 step: 3655, epoch: 110, batch: 24, loss: 0.7088077664375305, acc: 70.3125, f1: 50.241656491656485, r: 0.535890409147638
06/02/2019 09:29:02 step: 3660, epoch: 110, batch: 29, loss: 0.4171066880226135, acc: 84.375, f1: 69.08289241622575, r: 0.48084577654645344
06/02/2019 09:29:03 *** evaluating ***
06/02/2019 09:29:03 step: 111, epoch: 110, acc: 57.692307692307686, f1: 27.378220611916266, r: 0.3907673026731343
06/02/2019 09:29:03 *** epoch: 112 ***
06/02/2019 09:29:03 *** training ***
06/02/2019 09:29:04 step: 3668, epoch: 111, batch: 4, loss: 0.5899450778961182, acc: 82.8125, f1: 58.32213529224399, r: 0.6165952106777768
06/02/2019 09:29:05 step: 3673, epoch: 111, batch: 9, loss: 0.4235961437225342, acc: 89.0625, f1: 86.20718462823726, r: 0.5513564612568512
06/02/2019 09:29:06 step: 3678, epoch: 111, batch: 14, loss: 0.48220452666282654, acc: 79.6875, f1: 61.504969045291624, r: 0.6320574643919593
06/02/2019 09:29:08 step: 3683, epoch: 111, batch: 19, loss: 0.4379160702228546, acc: 84.375, f1: 54.964985994397765, r: 0.659407639937218
06/02/2019 09:29:09 step: 3688, epoch: 111, batch: 24, loss: 0.4219529628753662, acc: 85.9375, f1: 66.58008658008657, r: 0.549592225930483
06/02/2019 09:29:10 step: 3693, epoch: 111, batch: 29, loss: 0.6382644176483154, acc: 76.5625, f1: 61.000583547167395, r: 0.6009377530302513
06/02/2019 09:29:10 *** evaluating ***
06/02/2019 09:29:11 step: 112, epoch: 111, acc: 59.82905982905983, f1: 30.214767704065647, r: 0.40362985936191353
06/02/2019 09:29:11 *** epoch: 113 ***
06/02/2019 09:29:11 *** training ***
06/02/2019 09:29:12 step: 3701, epoch: 112, batch: 4, loss: 0.5441100001335144, acc: 82.8125, f1: 76.91742143081488, r: 0.5766053766047833
06/02/2019 09:29:13 step: 3706, epoch: 112, batch: 9, loss: 0.6000730395317078, acc: 76.5625, f1: 54.752572898799315, r: 0.633918455549777
06/02/2019 09:29:14 step: 3711, epoch: 112, batch: 14, loss: 0.4689260721206665, acc: 79.6875, f1: 66.24130297280202, r: 0.6065414080647847
06/02/2019 09:29:15 step: 3716, epoch: 112, batch: 19, loss: 0.4925409257411957, acc: 76.5625, f1: 45.82427536231884, r: 0.5735735526512495
06/02/2019 09:29:16 step: 3721, epoch: 112, batch: 24, loss: 0.5557492971420288, acc: 76.5625, f1: 50.10204081632653, r: 0.5332752681770792
06/02/2019 09:29:17 step: 3726, epoch: 112, batch: 29, loss: 0.6562122106552124, acc: 78.125, f1: 60.841294298921426, r: 0.5678342313892587
06/02/2019 09:29:18 *** evaluating ***
06/02/2019 09:29:18 step: 113, epoch: 112, acc: 59.401709401709404, f1: 27.599281792665785, r: 0.37462121205561705
06/02/2019 09:29:18 *** epoch: 114 ***
06/02/2019 09:29:18 *** training ***
06/02/2019 09:29:19 step: 3734, epoch: 113, batch: 4, loss: 0.662084698677063, acc: 70.3125, f1: 44.8483385983386, r: 0.6128263612476754
06/02/2019 09:29:21 step: 3739, epoch: 113, batch: 9, loss: 0.5454853773117065, acc: 78.125, f1: 64.79591836734694, r: 0.5954212458528585
06/02/2019 09:29:22 step: 3744, epoch: 113, batch: 14, loss: 0.6190919876098633, acc: 75.0, f1: 50.55186042028147, r: 0.567821109900164
06/02/2019 09:29:23 step: 3749, epoch: 113, batch: 19, loss: 0.5830176472663879, acc: 85.9375, f1: 83.26904924649286, r: 0.6202518439253714
06/02/2019 09:29:24 step: 3754, epoch: 113, batch: 24, loss: 0.5711849331855774, acc: 79.6875, f1: 65.30414220482713, r: 0.5658787313640982
06/02/2019 09:29:25 step: 3759, epoch: 113, batch: 29, loss: 0.4821367859840393, acc: 76.5625, f1: 54.819021948402, r: 0.5449869181522619
06/02/2019 09:29:25 *** evaluating ***
06/02/2019 09:29:26 step: 114, epoch: 113, acc: 58.97435897435898, f1: 30.033286920890873, r: 0.3854412702158546
06/02/2019 09:29:26 *** epoch: 115 ***
06/02/2019 09:29:26 *** training ***
06/02/2019 09:29:27 step: 3767, epoch: 114, batch: 4, loss: 0.5236068367958069, acc: 79.6875, f1: 53.559829059829056, r: 0.6341730938227627
06/02/2019 09:29:28 step: 3772, epoch: 114, batch: 9, loss: 0.5906907916069031, acc: 68.75, f1: 54.74292615596963, r: 0.6480981776402832
06/02/2019 09:29:29 step: 3777, epoch: 114, batch: 14, loss: 0.3267625570297241, acc: 85.9375, f1: 67.2746598639456, r: 0.6114539887312702
06/02/2019 09:29:30 step: 3782, epoch: 114, batch: 19, loss: 0.38464927673339844, acc: 87.5, f1: 85.97840459591612, r: 0.6417300720603543
06/02/2019 09:29:31 step: 3787, epoch: 114, batch: 24, loss: 0.7917080521583557, acc: 76.5625, f1: 70.55558341272626, r: 0.5312638025286804
06/02/2019 09:29:32 step: 3792, epoch: 114, batch: 29, loss: 0.40832003951072693, acc: 87.5, f1: 73.49412492269634, r: 0.6265484043741214
06/02/2019 09:29:33 *** evaluating ***
06/02/2019 09:29:33 step: 115, epoch: 114, acc: 59.82905982905983, f1: 28.669982437150278, r: 0.38142426956542397
06/02/2019 09:29:33 *** epoch: 116 ***
06/02/2019 09:29:33 *** training ***
06/02/2019 09:29:34 step: 3800, epoch: 115, batch: 4, loss: 0.4919176697731018, acc: 75.0, f1: 56.86250257678829, r: 0.563522661511807
06/02/2019 09:29:35 step: 3805, epoch: 115, batch: 9, loss: 0.4272628426551819, acc: 84.375, f1: 67.05553918197596, r: 0.5219417296009358
06/02/2019 09:29:36 step: 3810, epoch: 115, batch: 14, loss: 0.45968756079673767, acc: 81.25, f1: 67.86095848595849, r: 0.6367569810162945
06/02/2019 09:29:37 step: 3815, epoch: 115, batch: 19, loss: 0.43418705463409424, acc: 82.8125, f1: 68.66969009826151, r: 0.655552910395739
06/02/2019 09:29:38 step: 3820, epoch: 115, batch: 24, loss: 0.4517717659473419, acc: 82.8125, f1: 65.59321997564783, r: 0.6686700230863611
06/02/2019 09:29:40 step: 3825, epoch: 115, batch: 29, loss: 0.493742972612381, acc: 84.375, f1: 65.15932872757138, r: 0.601232541001812
06/02/2019 09:29:40 *** evaluating ***
06/02/2019 09:29:41 step: 116, epoch: 115, acc: 61.111111111111114, f1: 32.416471073791335, r: 0.3972557217601139
06/02/2019 09:29:41 *** epoch: 117 ***
06/02/2019 09:29:41 *** training ***
06/02/2019 09:29:42 step: 3833, epoch: 116, batch: 4, loss: 0.3402939736843109, acc: 89.0625, f1: 78.64357281374087, r: 0.5508296606175097
06/02/2019 09:29:43 step: 3838, epoch: 116, batch: 9, loss: 0.45064279437065125, acc: 79.6875, f1: 55.14935064935065, r: 0.5890128528229676
06/02/2019 09:29:44 step: 3843, epoch: 116, batch: 14, loss: 0.5812081694602966, acc: 78.125, f1: 64.88677536231884, r: 0.651522810783096
06/02/2019 09:29:45 step: 3848, epoch: 116, batch: 19, loss: 0.48744258284568787, acc: 81.25, f1: 72.3469047958844, r: 0.664346309451403
06/02/2019 09:29:47 step: 3853, epoch: 116, batch: 24, loss: 0.39372020959854126, acc: 81.25, f1: 71.77307935715623, r: 0.6544193602665327
06/02/2019 09:29:48 step: 3858, epoch: 116, batch: 29, loss: 0.3965659737586975, acc: 85.9375, f1: 66.87291938051816, r: 0.5918788358453997
06/02/2019 09:29:48 *** evaluating ***
06/02/2019 09:29:49 step: 117, epoch: 116, acc: 60.256410256410255, f1: 28.16124168705424, r: 0.3812375049981929
06/02/2019 09:29:49 *** epoch: 118 ***
06/02/2019 09:29:49 *** training ***
06/02/2019 09:29:50 step: 3866, epoch: 117, batch: 4, loss: 0.4998716413974762, acc: 81.25, f1: 70.0477683236304, r: 0.5538622562558515
06/02/2019 09:29:51 step: 3871, epoch: 117, batch: 9, loss: 0.45748135447502136, acc: 79.6875, f1: 68.67757447806709, r: 0.5524833461370469
06/02/2019 09:29:52 step: 3876, epoch: 117, batch: 14, loss: 0.6201850771903992, acc: 82.8125, f1: 66.86523621306229, r: 0.6650234463971781
06/02/2019 09:29:53 step: 3881, epoch: 117, batch: 19, loss: 0.5525662899017334, acc: 79.6875, f1: 56.369234214061805, r: 0.6120192507373055
06/02/2019 09:29:54 step: 3886, epoch: 117, batch: 24, loss: 0.4458388090133667, acc: 87.5, f1: 80.03355188229138, r: 0.6234483727048334
06/02/2019 09:29:55 step: 3891, epoch: 117, batch: 29, loss: 0.5163828134536743, acc: 76.5625, f1: 58.93695277153923, r: 0.6814540604033124
06/02/2019 09:29:56 *** evaluating ***
06/02/2019 09:29:56 step: 118, epoch: 117, acc: 56.837606837606835, f1: 27.3390862571113, r: 0.3779386162820581
06/02/2019 09:29:56 *** epoch: 119 ***
06/02/2019 09:29:56 *** training ***
06/02/2019 09:29:57 step: 3899, epoch: 118, batch: 4, loss: 0.37187135219573975, acc: 90.625, f1: 73.01910142588109, r: 0.5456925049401703
06/02/2019 09:29:58 step: 3904, epoch: 118, batch: 9, loss: 0.5158340930938721, acc: 85.9375, f1: 72.06009676597913, r: 0.5674301731144411
06/02/2019 09:29:59 step: 3909, epoch: 118, batch: 14, loss: 0.6824766993522644, acc: 67.1875, f1: 50.235819735819746, r: 0.5669215399716757
06/02/2019 09:30:00 step: 3914, epoch: 118, batch: 19, loss: 0.7304062247276306, acc: 73.4375, f1: 48.19276675294794, r: 0.5505938003714055
06/02/2019 09:30:02 step: 3919, epoch: 118, batch: 24, loss: 0.42049434781074524, acc: 85.9375, f1: 70.66304066304066, r: 0.5797793774426897
06/02/2019 09:30:03 step: 3924, epoch: 118, batch: 29, loss: 0.3631892502307892, acc: 87.5, f1: 72.0981240981241, r: 0.5745858455488008
06/02/2019 09:30:03 *** evaluating ***
06/02/2019 09:30:04 step: 119, epoch: 118, acc: 61.111111111111114, f1: 31.59926470588236, r: 0.40388928120847
06/02/2019 09:30:04 *** epoch: 120 ***
06/02/2019 09:30:04 *** training ***
06/02/2019 09:30:05 step: 3932, epoch: 119, batch: 4, loss: 0.6096873879432678, acc: 75.0, f1: 57.1585804132974, r: 0.5128154072911059
06/02/2019 09:30:06 step: 3937, epoch: 119, batch: 9, loss: 0.5960557460784912, acc: 68.75, f1: 42.305754590237356, r: 0.6299573858131579
06/02/2019 09:30:07 step: 3942, epoch: 119, batch: 14, loss: 0.39891317486763, acc: 87.5, f1: 72.85929951690821, r: 0.5989592379282134
06/02/2019 09:30:09 step: 3947, epoch: 119, batch: 19, loss: 0.5779193639755249, acc: 82.8125, f1: 70.52141805338736, r: 0.6266940469365585
06/02/2019 09:30:10 step: 3952, epoch: 119, batch: 24, loss: 0.5209327936172485, acc: 79.6875, f1: 53.62745098039217, r: 0.5391320310876349
06/02/2019 09:30:11 step: 3957, epoch: 119, batch: 29, loss: 0.5337874293327332, acc: 76.5625, f1: 58.10734463276837, r: 0.6322005895613603
06/02/2019 09:30:12 *** evaluating ***
06/02/2019 09:30:12 step: 120, epoch: 119, acc: 56.837606837606835, f1: 30.676805500598398, r: 0.40796577555074753
06/02/2019 09:30:12 *** epoch: 121 ***
06/02/2019 09:30:12 *** training ***
06/02/2019 09:30:13 step: 3965, epoch: 120, batch: 4, loss: 0.3751063644886017, acc: 87.5, f1: 84.5699964196205, r: 0.5860069406696724
06/02/2019 09:30:15 step: 3970, epoch: 120, batch: 9, loss: 0.548745334148407, acc: 81.25, f1: 56.44361590790162, r: 0.5870068638279536
06/02/2019 09:30:16 step: 3975, epoch: 120, batch: 14, loss: 0.4941850006580353, acc: 78.125, f1: 62.54595588235294, r: 0.6654531560989969
06/02/2019 09:30:17 step: 3980, epoch: 120, batch: 19, loss: 0.40623363852500916, acc: 82.8125, f1: 59.45990073145245, r: 0.6628999284150872
06/02/2019 09:30:18 step: 3985, epoch: 120, batch: 24, loss: 0.5165319442749023, acc: 84.375, f1: 69.27952999381571, r: 0.6565356699602082
06/02/2019 09:30:19 step: 3990, epoch: 120, batch: 29, loss: 0.3527677059173584, acc: 84.375, f1: 65.79320840190405, r: 0.6272644545151286
06/02/2019 09:30:20 *** evaluating ***
06/02/2019 09:30:20 step: 121, epoch: 120, acc: 59.401709401709404, f1: 26.73727418391529, r: 0.38979672683771177
06/02/2019 09:30:20 *** epoch: 122 ***
06/02/2019 09:30:20 *** training ***
06/02/2019 09:30:21 step: 3998, epoch: 121, batch: 4, loss: 0.5767794251441956, acc: 75.0, f1: 55.269328624591786, r: 0.618474135339188
06/02/2019 09:30:22 step: 4003, epoch: 121, batch: 9, loss: 0.7021815776824951, acc: 65.625, f1: 41.86404550733819, r: 0.4794109101271625
06/02/2019 09:30:23 step: 4008, epoch: 121, batch: 14, loss: 0.38844382762908936, acc: 85.9375, f1: 62.367909867909866, r: 0.7365604197589704
06/02/2019 09:30:24 step: 4013, epoch: 121, batch: 19, loss: 0.24914324283599854, acc: 92.1875, f1: 85.44330775788576, r: 0.5850282051456717
06/02/2019 09:30:25 step: 4018, epoch: 121, batch: 24, loss: 0.588017463684082, acc: 76.5625, f1: 58.29519686662543, r: 0.6685774976495235
06/02/2019 09:30:27 step: 4023, epoch: 121, batch: 29, loss: 0.5068160891532898, acc: 76.5625, f1: 58.61977170116705, r: 0.5862231069254151
06/02/2019 09:30:27 *** evaluating ***
06/02/2019 09:30:28 step: 122, epoch: 121, acc: 59.82905982905983, f1: 30.416432457139592, r: 0.41444055468546176
06/02/2019 09:30:28 *** epoch: 123 ***
06/02/2019 09:30:28 *** training ***
06/02/2019 09:30:29 step: 4031, epoch: 122, batch: 4, loss: 0.37974658608436584, acc: 85.9375, f1: 73.78572745712705, r: 0.626391230750349
06/02/2019 09:30:30 step: 4036, epoch: 122, batch: 9, loss: 0.42910802364349365, acc: 82.8125, f1: 67.7062463139047, r: 0.6313683076805762
06/02/2019 09:30:31 step: 4041, epoch: 122, batch: 14, loss: 0.5535765886306763, acc: 78.125, f1: 63.12833714721586, r: 0.6766767961508415
06/02/2019 09:30:32 step: 4046, epoch: 122, batch: 19, loss: 0.5498911142349243, acc: 76.5625, f1: 62.69640723422235, r: 0.6010308845048069
06/02/2019 09:30:33 step: 4051, epoch: 122, batch: 24, loss: 0.4163728654384613, acc: 84.375, f1: 67.72358867186453, r: 0.6930894320043864
06/02/2019 09:30:35 step: 4056, epoch: 122, batch: 29, loss: 0.5893558263778687, acc: 81.25, f1: 62.11393540669856, r: 0.569182394425664
06/02/2019 09:30:35 *** evaluating ***
06/02/2019 09:30:36 step: 123, epoch: 122, acc: 58.119658119658126, f1: 27.206573332967853, r: 0.3691677055858815
06/02/2019 09:30:36 *** epoch: 124 ***
06/02/2019 09:30:36 *** training ***
06/02/2019 09:30:37 step: 4064, epoch: 123, batch: 4, loss: 0.283086359500885, acc: 87.5, f1: 64.66823406478579, r: 0.6928046142734223
06/02/2019 09:30:38 step: 4069, epoch: 123, batch: 9, loss: 0.3579480051994324, acc: 84.375, f1: 76.59302101407364, r: 0.6211536683875418
06/02/2019 09:30:39 step: 4074, epoch: 123, batch: 14, loss: 0.5007483959197998, acc: 76.5625, f1: 48.904669018521005, r: 0.5144452726106951
06/02/2019 09:30:40 step: 4079, epoch: 123, batch: 19, loss: 0.2803538739681244, acc: 90.625, f1: 83.47537878787878, r: 0.6855214367698865
06/02/2019 09:30:42 step: 4084, epoch: 123, batch: 24, loss: 0.4219815731048584, acc: 79.6875, f1: 45.925324675324674, r: 0.5913624101274358
06/02/2019 09:30:43 step: 4089, epoch: 123, batch: 29, loss: 0.4063642621040344, acc: 84.375, f1: 77.81385281385282, r: 0.6549266770811366
06/02/2019 09:30:43 *** evaluating ***
06/02/2019 09:30:44 step: 124, epoch: 123, acc: 59.82905982905983, f1: 28.10474352923492, r: 0.37624544051509445
06/02/2019 09:30:44 *** epoch: 125 ***
06/02/2019 09:30:44 *** training ***
06/02/2019 09:30:45 step: 4097, epoch: 124, batch: 4, loss: 0.32891541719436646, acc: 90.625, f1: 86.06613137058102, r: 0.614066986429068
06/02/2019 09:30:46 step: 4102, epoch: 124, batch: 9, loss: 0.332798570394516, acc: 84.375, f1: 64.9969285373012, r: 0.527003939189263
06/02/2019 09:30:47 step: 4107, epoch: 124, batch: 14, loss: 0.3361464738845825, acc: 89.0625, f1: 67.3490860990861, r: 0.7005981589647237
06/02/2019 09:30:49 step: 4112, epoch: 124, batch: 19, loss: 0.39211544394493103, acc: 82.8125, f1: 75.5568092742375, r: 0.6310740796474004
06/02/2019 09:30:50 step: 4117, epoch: 124, batch: 24, loss: 0.35205960273742676, acc: 89.0625, f1: 84.16099773242631, r: 0.6508801063893417
06/02/2019 09:30:51 step: 4122, epoch: 124, batch: 29, loss: 0.4371277093887329, acc: 81.25, f1: 74.60043787629994, r: 0.6170988005831369
06/02/2019 09:30:52 *** evaluating ***
06/02/2019 09:30:52 step: 125, epoch: 124, acc: 58.54700854700855, f1: 27.801878276431196, r: 0.36291632167379745
06/02/2019 09:30:52 *** epoch: 126 ***
06/02/2019 09:30:52 *** training ***
06/02/2019 09:30:53 step: 4130, epoch: 125, batch: 4, loss: 0.35737496614456177, acc: 84.375, f1: 70.32553798858146, r: 0.6783959999729388
06/02/2019 09:30:54 step: 4135, epoch: 125, batch: 9, loss: 0.41270655393600464, acc: 85.9375, f1: 67.60288730876967, r: 0.6152449217381077
06/02/2019 09:30:55 step: 4140, epoch: 125, batch: 14, loss: 0.4080694317817688, acc: 81.25, f1: 68.37909900802427, r: 0.5946728776503344
06/02/2019 09:30:56 step: 4145, epoch: 125, batch: 19, loss: 0.47096627950668335, acc: 81.25, f1: 66.8331043956044, r: 0.6558533027194247
06/02/2019 09:30:57 step: 4150, epoch: 125, batch: 24, loss: 0.388918399810791, acc: 85.9375, f1: 68.86980879308246, r: 0.5453519427781943
06/02/2019 09:30:58 step: 4155, epoch: 125, batch: 29, loss: 0.36764270067214966, acc: 84.375, f1: 67.68680594767551, r: 0.5899382332017414
06/02/2019 09:30:59 *** evaluating ***
06/02/2019 09:30:59 step: 126, epoch: 125, acc: 58.54700854700855, f1: 27.644998370804817, r: 0.3743031237096017
06/02/2019 09:30:59 *** epoch: 127 ***
06/02/2019 09:30:59 *** training ***
06/02/2019 09:31:00 step: 4163, epoch: 126, batch: 4, loss: 0.4616013169288635, acc: 82.8125, f1: 61.14039855072464, r: 0.6226520716776823
06/02/2019 09:31:01 step: 4168, epoch: 126, batch: 9, loss: 0.4327743649482727, acc: 82.8125, f1: 57.34280925013684, r: 0.6004891167260918
06/02/2019 09:31:02 step: 4173, epoch: 126, batch: 14, loss: 0.3058374226093292, acc: 87.5, f1: 68.09208683473389, r: 0.6855962606097945
06/02/2019 09:31:03 step: 4178, epoch: 126, batch: 19, loss: 0.3032001852989197, acc: 90.625, f1: 89.92696310742173, r: 0.5974273737463062
06/02/2019 09:31:05 step: 4183, epoch: 126, batch: 24, loss: 0.4708346724510193, acc: 82.8125, f1: 65.91128117913833, r: 0.6549293776697301
06/02/2019 09:31:06 step: 4188, epoch: 126, batch: 29, loss: 0.46035996079444885, acc: 79.6875, f1: 58.77742412626134, r: 0.6322713801132024
06/02/2019 09:31:07 *** evaluating ***
06/02/2019 09:31:07 step: 127, epoch: 126, acc: 59.82905982905983, f1: 30.595024760785627, r: 0.4049868617301733
06/02/2019 09:31:07 *** epoch: 128 ***
06/02/2019 09:31:07 *** training ***
06/02/2019 09:31:08 step: 4196, epoch: 127, batch: 4, loss: 0.33432716131210327, acc: 89.0625, f1: 79.04238618524333, r: 0.6715132591008252
06/02/2019 09:31:09 step: 4201, epoch: 127, batch: 9, loss: 0.3411236107349396, acc: 85.9375, f1: 62.211386918833725, r: 0.6651622927998322
06/02/2019 09:31:10 step: 4206, epoch: 127, batch: 14, loss: 0.5732839703559875, acc: 73.4375, f1: 53.64989177489178, r: 0.5585071867425947
06/02/2019 09:31:12 step: 4211, epoch: 127, batch: 19, loss: 0.49834832549095154, acc: 76.5625, f1: 57.1957671957672, r: 0.6841492868899554
06/02/2019 09:31:13 step: 4216, epoch: 127, batch: 24, loss: 0.3901407718658447, acc: 85.9375, f1: 77.58244969524924, r: 0.5659108720561201
06/02/2019 09:31:14 step: 4221, epoch: 127, batch: 29, loss: 0.4126516282558441, acc: 82.8125, f1: 65.27323973067489, r: 0.5785253992626419
06/02/2019 09:31:14 *** evaluating ***
06/02/2019 09:31:15 step: 128, epoch: 127, acc: 58.54700854700855, f1: 28.515867077664442, r: 0.37405179313554665
06/02/2019 09:31:15 *** epoch: 129 ***
06/02/2019 09:31:15 *** training ***
06/02/2019 09:31:16 step: 4229, epoch: 128, batch: 4, loss: 0.4608100652694702, acc: 79.6875, f1: 57.194444444444436, r: 0.5448867642680242
06/02/2019 09:31:17 step: 4234, epoch: 128, batch: 9, loss: 0.5953483581542969, acc: 79.6875, f1: 67.20955614843561, r: 0.6962581289470933
06/02/2019 09:31:18 step: 4239, epoch: 128, batch: 14, loss: 0.4053826630115509, acc: 81.25, f1: 70.79840080688979, r: 0.5765857223017802
06/02/2019 09:31:19 step: 4244, epoch: 128, batch: 19, loss: 0.4410281181335449, acc: 79.6875, f1: 73.81261595547309, r: 0.5735675737552148
06/02/2019 09:31:20 step: 4249, epoch: 128, batch: 24, loss: 0.45388051867485046, acc: 81.25, f1: 59.109848484848484, r: 0.622587966382828
06/02/2019 09:31:21 step: 4254, epoch: 128, batch: 29, loss: 0.31268373131752014, acc: 84.375, f1: 61.12745098039215, r: 0.6439554977963978
06/02/2019 09:31:22 *** evaluating ***
06/02/2019 09:31:22 step: 129, epoch: 128, acc: 58.119658119658126, f1: 26.795400031999893, r: 0.3797098593286278
06/02/2019 09:31:22 *** epoch: 130 ***
06/02/2019 09:31:22 *** training ***
06/02/2019 09:31:23 step: 4262, epoch: 129, batch: 4, loss: 0.39726945757865906, acc: 85.9375, f1: 79.20791190376443, r: 0.6435882302333273
06/02/2019 09:31:24 step: 4267, epoch: 129, batch: 9, loss: 0.3964242935180664, acc: 84.375, f1: 64.36879297173415, r: 0.7077752453333632
06/02/2019 09:31:26 step: 4272, epoch: 129, batch: 14, loss: 0.48766079545021057, acc: 78.125, f1: 64.39309056956117, r: 0.5450321906849123
06/02/2019 09:31:27 step: 4277, epoch: 129, batch: 19, loss: 0.42228642106056213, acc: 85.9375, f1: 62.27875765690891, r: 0.6555034276299856
06/02/2019 09:31:28 step: 4282, epoch: 129, batch: 24, loss: 0.41367724537849426, acc: 81.25, f1: 68.38744588744589, r: 0.6728183866079993
06/02/2019 09:31:29 step: 4287, epoch: 129, batch: 29, loss: 0.4807979464530945, acc: 76.5625, f1: 58.80884995507637, r: 0.6582096012744916
06/02/2019 09:31:30 *** evaluating ***
06/02/2019 09:31:30 step: 130, epoch: 129, acc: 55.98290598290598, f1: 27.78873769009848, r: 0.3907790809544813
06/02/2019 09:31:30 *** epoch: 131 ***
06/02/2019 09:31:30 *** training ***
06/02/2019 09:31:31 step: 4295, epoch: 130, batch: 4, loss: 0.34885677695274353, acc: 84.375, f1: 65.52619756939518, r: 0.595137100930855
06/02/2019 09:31:32 step: 4300, epoch: 130, batch: 9, loss: 0.24643908441066742, acc: 92.1875, f1: 88.40702947845806, r: 0.6946974470625202
06/02/2019 09:31:34 step: 4305, epoch: 130, batch: 14, loss: 0.25851908326148987, acc: 89.0625, f1: 54.60768398268399, r: 0.6229196615519059
06/02/2019 09:31:35 step: 4310, epoch: 130, batch: 19, loss: 0.45022425055503845, acc: 84.375, f1: 60.73338426279603, r: 0.5737132385806183
06/02/2019 09:31:36 step: 4315, epoch: 130, batch: 24, loss: 0.42243194580078125, acc: 84.375, f1: 65.81563180827888, r: 0.6497571273286923
06/02/2019 09:31:37 step: 4320, epoch: 130, batch: 29, loss: 0.3601152002811432, acc: 85.9375, f1: 70.67821272429568, r: 0.555689372861301
06/02/2019 09:31:38 *** evaluating ***
06/02/2019 09:31:38 step: 131, epoch: 130, acc: 59.82905982905983, f1: 30.89266183623987, r: 0.37781326529607717
06/02/2019 09:31:38 *** epoch: 132 ***
06/02/2019 09:31:38 *** training ***
06/02/2019 09:31:39 step: 4328, epoch: 131, batch: 4, loss: 0.5398220419883728, acc: 79.6875, f1: 73.20312710064263, r: 0.650168383178442
06/02/2019 09:31:40 step: 4333, epoch: 131, batch: 9, loss: 0.2859587073326111, acc: 90.625, f1: 60.24811518971697, r: 0.5818247496524439
06/02/2019 09:31:41 step: 4338, epoch: 131, batch: 14, loss: 0.37360692024230957, acc: 84.375, f1: 69.28571428571429, r: 0.6634047442278841
06/02/2019 09:31:42 step: 4343, epoch: 131, batch: 19, loss: 0.475790411233902, acc: 84.375, f1: 74.87394957983193, r: 0.6059357085208812
06/02/2019 09:31:44 step: 4348, epoch: 131, batch: 24, loss: 0.371315598487854, acc: 82.8125, f1: 55.79527879527879, r: 0.6746989068389444
06/02/2019 09:31:45 step: 4353, epoch: 131, batch: 29, loss: 0.3075491487979889, acc: 89.0625, f1: 73.690946734425, r: 0.5675908889365386
06/02/2019 09:31:45 *** evaluating ***
06/02/2019 09:31:46 step: 132, epoch: 131, acc: 59.401709401709404, f1: 28.818317099567103, r: 0.36960854804745774
06/02/2019 09:31:46 *** epoch: 133 ***
06/02/2019 09:31:46 *** training ***
06/02/2019 09:31:47 step: 4361, epoch: 132, batch: 4, loss: 0.2914840281009674, acc: 89.0625, f1: 64.90115276879982, r: 0.6599107364372824
06/02/2019 09:31:48 step: 4366, epoch: 132, batch: 9, loss: 0.7431248426437378, acc: 68.75, f1: 49.04990842490843, r: 0.5823748477569564
06/02/2019 09:31:49 step: 4371, epoch: 132, batch: 14, loss: 0.38713374733924866, acc: 87.5, f1: 79.69505043396668, r: 0.6126236361815572
06/02/2019 09:31:50 step: 4376, epoch: 132, batch: 19, loss: 0.3202439546585083, acc: 89.0625, f1: 91.45617667356797, r: 0.6434937249975687
06/02/2019 09:31:51 step: 4381, epoch: 132, batch: 24, loss: 0.37787383794784546, acc: 84.375, f1: 77.41496598639455, r: 0.6532466927564383
06/02/2019 09:31:52 step: 4386, epoch: 132, batch: 29, loss: 0.29493650794029236, acc: 92.1875, f1: 75.91861051890842, r: 0.6655895146710041
06/02/2019 09:31:53 *** evaluating ***
06/02/2019 09:31:53 step: 133, epoch: 132, acc: 61.111111111111114, f1: 29.528670020639836, r: 0.371220561658003
06/02/2019 09:31:53 *** epoch: 134 ***
06/02/2019 09:31:53 *** training ***
06/02/2019 09:31:54 step: 4394, epoch: 133, batch: 4, loss: 0.3277953863143921, acc: 87.5, f1: 66.2812720225511, r: 0.6654553477463141
06/02/2019 09:31:55 step: 4399, epoch: 133, batch: 9, loss: 0.3105391263961792, acc: 90.625, f1: 72.47474747474747, r: 0.6716463845715525
06/02/2019 09:31:56 step: 4404, epoch: 133, batch: 14, loss: 0.35629796981811523, acc: 78.125, f1: 50.4703548085901, r: 0.621700691077158
06/02/2019 09:31:57 step: 4409, epoch: 133, batch: 19, loss: 0.42442283034324646, acc: 84.375, f1: 79.76497187023503, r: 0.6022426269679007
06/02/2019 09:31:59 step: 4414, epoch: 133, batch: 24, loss: 0.2603999078273773, acc: 95.3125, f1: 90.94437775110043, r: 0.6205236674253253
06/02/2019 09:32:00 step: 4419, epoch: 133, batch: 29, loss: 0.3440306484699249, acc: 85.9375, f1: 78.98194684572394, r: 0.5931425617403213
06/02/2019 09:32:00 *** evaluating ***
06/02/2019 09:32:01 step: 134, epoch: 133, acc: 59.401709401709404, f1: 27.55510499394266, r: 0.34897266896714424
06/02/2019 09:32:01 *** epoch: 135 ***
06/02/2019 09:32:01 *** training ***
06/02/2019 09:32:02 step: 4427, epoch: 134, batch: 4, loss: 0.24667860567569733, acc: 89.0625, f1: 84.11639090664515, r: 0.6582924179566613
06/02/2019 09:32:03 step: 4432, epoch: 134, batch: 9, loss: 0.2987804710865021, acc: 87.5, f1: 71.0014619883041, r: 0.737019674572851
06/02/2019 09:32:04 step: 4437, epoch: 134, batch: 14, loss: 0.29792654514312744, acc: 87.5, f1: 74.86081274615108, r: 0.7273771090285912
06/02/2019 09:32:05 step: 4442, epoch: 134, batch: 19, loss: 0.33953389525413513, acc: 84.375, f1: 66.64561152603422, r: 0.7121151806718825
06/02/2019 09:32:07 step: 4447, epoch: 134, batch: 24, loss: 0.2756117284297943, acc: 87.5, f1: 79.62620712620712, r: 0.5722989826427727
06/02/2019 09:32:08 step: 4452, epoch: 134, batch: 29, loss: 0.37959522008895874, acc: 84.375, f1: 76.56818181818181, r: 0.577331747882676
06/02/2019 09:32:08 *** evaluating ***
06/02/2019 09:32:09 step: 135, epoch: 134, acc: 60.256410256410255, f1: 27.869031287282496, r: 0.3510811030479424
06/02/2019 09:32:09 *** epoch: 136 ***
06/02/2019 09:32:09 *** training ***
06/02/2019 09:32:10 step: 4460, epoch: 135, batch: 4, loss: 0.4756450951099396, acc: 81.25, f1: 54.77828841915448, r: 0.5481555486377697
06/02/2019 09:32:11 step: 4465, epoch: 135, batch: 9, loss: 0.4656106233596802, acc: 84.375, f1: 61.00932480334536, r: 0.6026337697086737
06/02/2019 09:32:12 step: 4470, epoch: 135, batch: 14, loss: 0.5649592280387878, acc: 82.8125, f1: 62.16804029304029, r: 0.5843185620042707
06/02/2019 09:32:13 step: 4475, epoch: 135, batch: 19, loss: 0.2904176712036133, acc: 85.9375, f1: 83.72111978431926, r: 0.6533520336881803
06/02/2019 09:32:15 step: 4480, epoch: 135, batch: 24, loss: 0.3566914200782776, acc: 81.25, f1: 71.53123067408781, r: 0.5959923711537646
06/02/2019 09:32:16 step: 4485, epoch: 135, batch: 29, loss: 0.503000020980835, acc: 81.25, f1: 68.93262534566882, r: 0.5962727397301335
06/02/2019 09:32:16 *** evaluating ***
06/02/2019 09:32:17 step: 136, epoch: 135, acc: 56.837606837606835, f1: 26.430148850580203, r: 0.36543909692992926
06/02/2019 09:32:17 *** epoch: 137 ***
06/02/2019 09:32:17 *** training ***
06/02/2019 09:32:18 step: 4493, epoch: 136, batch: 4, loss: 0.33790743350982666, acc: 87.5, f1: 54.11796536796536, r: 0.5254341095695393
06/02/2019 09:32:19 step: 4498, epoch: 136, batch: 9, loss: 0.4001067876815796, acc: 90.625, f1: 80.27324027324028, r: 0.5680957976542923
06/02/2019 09:32:20 step: 4503, epoch: 136, batch: 14, loss: 0.31451183557510376, acc: 89.0625, f1: 80.75139146567719, r: 0.6236108461931511
06/02/2019 09:32:21 step: 4508, epoch: 136, batch: 19, loss: 0.3195728361606598, acc: 89.0625, f1: 81.82898662036109, r: 0.7070212179944156
06/02/2019 09:32:23 step: 4513, epoch: 136, batch: 24, loss: 0.44069844484329224, acc: 84.375, f1: 70.60075708301025, r: 0.5567573996875143
06/02/2019 09:32:24 step: 4518, epoch: 136, batch: 29, loss: 0.34713006019592285, acc: 85.9375, f1: 63.56833504010923, r: 0.6739167595951252
06/02/2019 09:32:24 *** evaluating ***
06/02/2019 09:32:24 step: 137, epoch: 136, acc: 61.965811965811966, f1: 28.337069334896626, r: 0.39048474788539994
06/02/2019 09:32:24 *** epoch: 138 ***
06/02/2019 09:32:24 *** training ***
06/02/2019 09:32:25 step: 4526, epoch: 137, batch: 4, loss: 0.21597920358181, acc: 93.75, f1: 89.18352179221743, r: 0.6615695296902618
06/02/2019 09:32:26 step: 4531, epoch: 137, batch: 9, loss: 0.37923285365104675, acc: 85.9375, f1: 81.01388776854614, r: 0.5891136164916206
06/02/2019 09:32:27 step: 4536, epoch: 137, batch: 14, loss: 0.18263427913188934, acc: 93.75, f1: 91.22090941639813, r: 0.6613478786246957
06/02/2019 09:32:29 step: 4541, epoch: 137, batch: 19, loss: 0.46204203367233276, acc: 84.375, f1: 68.69271533064637, r: 0.6621450394942038
06/02/2019 09:32:30 step: 4546, epoch: 137, batch: 24, loss: 0.31519749760627747, acc: 87.5, f1: 66.7167277167277, r: 0.6215389237775983
06/02/2019 09:32:31 step: 4551, epoch: 137, batch: 29, loss: 0.3408063054084778, acc: 85.9375, f1: 63.10146394823815, r: 0.5849346588128052
06/02/2019 09:32:32 *** evaluating ***
06/02/2019 09:32:32 step: 138, epoch: 137, acc: 60.68376068376068, f1: 28.55268235931192, r: 0.3883584196200429
06/02/2019 09:32:32 *** epoch: 139 ***
06/02/2019 09:32:32 *** training ***
06/02/2019 09:32:33 step: 4559, epoch: 138, batch: 4, loss: 0.33591189980506897, acc: 87.5, f1: 86.48857735279688, r: 0.6996826345383238
06/02/2019 09:32:34 step: 4564, epoch: 138, batch: 9, loss: 0.18554231524467468, acc: 93.75, f1: 78.09424894102314, r: 0.7329173528100317
06/02/2019 09:32:35 step: 4569, epoch: 138, batch: 14, loss: 0.24863095581531525, acc: 87.5, f1: 75.098858256753, r: 0.7084931316686813
06/02/2019 09:32:37 step: 4574, epoch: 138, batch: 19, loss: 0.2996629476547241, acc: 84.375, f1: 67.1347832062118, r: 0.579321040859744
06/02/2019 09:32:38 step: 4579, epoch: 138, batch: 24, loss: 0.3253280520439148, acc: 95.3125, f1: 96.96915987238569, r: 0.6567587293642859
06/02/2019 09:32:39 step: 4584, epoch: 138, batch: 29, loss: 0.2518165409564972, acc: 85.9375, f1: 74.33406647116325, r: 0.6218182103345731
06/02/2019 09:32:39 *** evaluating ***
06/02/2019 09:32:40 step: 139, epoch: 138, acc: 61.111111111111114, f1: 29.571377734492053, r: 0.3605151299287187
06/02/2019 09:32:40 *** epoch: 140 ***
06/02/2019 09:32:40 *** training ***
06/02/2019 09:32:41 step: 4592, epoch: 139, batch: 4, loss: 0.3825835883617401, acc: 79.6875, f1: 59.048636548636544, r: 0.6730075639374449
06/02/2019 09:32:42 step: 4597, epoch: 139, batch: 9, loss: 0.24998950958251953, acc: 92.1875, f1: 82.49941355852685, r: 0.7153842730911923
06/02/2019 09:32:43 step: 4602, epoch: 139, batch: 14, loss: 0.33274656534194946, acc: 90.625, f1: 86.28535624776227, r: 0.6491599206336162
06/02/2019 09:32:44 step: 4607, epoch: 139, batch: 19, loss: 0.5376710295677185, acc: 81.25, f1: 70.84862227719371, r: 0.6089796799571848
06/02/2019 09:32:45 step: 4612, epoch: 139, batch: 24, loss: 0.42018431425094604, acc: 82.8125, f1: 54.3956043956044, r: 0.6610453352502267
06/02/2019 09:32:46 step: 4617, epoch: 139, batch: 29, loss: 0.29935315251350403, acc: 89.0625, f1: 73.1494990340634, r: 0.671349553094879
06/02/2019 09:32:47 *** evaluating ***
06/02/2019 09:32:47 step: 140, epoch: 139, acc: 56.837606837606835, f1: 27.428041582857, r: 0.38036153508894655
06/02/2019 09:32:47 *** epoch: 141 ***
06/02/2019 09:32:47 *** training ***
06/02/2019 09:32:49 step: 4625, epoch: 140, batch: 4, loss: 0.2705783247947693, acc: 93.75, f1: 72.70833333333333, r: 0.6447480786372453
06/02/2019 09:32:50 step: 4630, epoch: 140, batch: 9, loss: 0.30763739347457886, acc: 87.5, f1: 71.37605872388481, r: 0.6896920126762803
06/02/2019 09:32:51 step: 4635, epoch: 140, batch: 14, loss: 0.29659590125083923, acc: 93.75, f1: 81.3395979020979, r: 0.6834337312732812
06/02/2019 09:32:52 step: 4640, epoch: 140, batch: 19, loss: 0.4254280626773834, acc: 81.25, f1: 57.05982905982907, r: 0.5428293915976489
06/02/2019 09:32:53 step: 4645, epoch: 140, batch: 24, loss: 0.2663828432559967, acc: 87.5, f1: 68.86189727463314, r: 0.6460411148918057
06/02/2019 09:32:54 step: 4650, epoch: 140, batch: 29, loss: 0.3675801455974579, acc: 85.9375, f1: 81.35079206507777, r: 0.6083928175907521
06/02/2019 09:32:55 *** evaluating ***
06/02/2019 09:32:55 step: 141, epoch: 140, acc: 61.53846153846154, f1: 30.28979698162033, r: 0.3790741645426655
06/02/2019 09:32:55 *** epoch: 142 ***
06/02/2019 09:32:55 *** training ***
06/02/2019 09:32:56 step: 4658, epoch: 141, batch: 4, loss: 0.3582319915294647, acc: 85.9375, f1: 77.62507333935905, r: 0.6627035178717422
06/02/2019 09:32:57 step: 4663, epoch: 141, batch: 9, loss: 0.2907055914402008, acc: 89.0625, f1: 85.7263550581523, r: 0.5830696980918184
06/02/2019 09:32:58 step: 4668, epoch: 141, batch: 14, loss: 0.35884761810302734, acc: 90.625, f1: 88.26968214186259, r: 0.5956496250655539
06/02/2019 09:32:59 step: 4673, epoch: 141, batch: 19, loss: 0.2633882462978363, acc: 90.625, f1: 86.68246915991277, r: 0.7062703511809658
06/02/2019 09:33:00 step: 4678, epoch: 141, batch: 24, loss: 0.39510002732276917, acc: 79.6875, f1: 58.50340136054422, r: 0.5859762984652179
06/02/2019 09:33:01 step: 4683, epoch: 141, batch: 29, loss: 0.35666903853416443, acc: 89.0625, f1: 71.20587027914614, r: 0.7082647157781201
06/02/2019 09:33:02 *** evaluating ***
06/02/2019 09:33:02 step: 142, epoch: 141, acc: 57.26495726495726, f1: 28.64927749652607, r: 0.3609676781159094
06/02/2019 09:33:02 *** epoch: 143 ***
06/02/2019 09:33:02 *** training ***
06/02/2019 09:33:03 step: 4691, epoch: 142, batch: 4, loss: 0.4560431241989136, acc: 84.375, f1: 72.13833194096352, r: 0.5734369930871117
06/02/2019 09:33:05 step: 4696, epoch: 142, batch: 9, loss: 0.31528064608573914, acc: 85.9375, f1: 70.94555246340961, r: 0.6996129026410836
06/02/2019 09:33:06 step: 4701, epoch: 142, batch: 14, loss: 0.3660319149494171, acc: 81.25, f1: 66.76712160836874, r: 0.6943843598860368
06/02/2019 09:33:07 step: 4706, epoch: 142, batch: 19, loss: 0.6657689213752747, acc: 76.5625, f1: 52.460317460317455, r: 0.6483261271374334
06/02/2019 09:33:08 step: 4711, epoch: 142, batch: 24, loss: 0.463375449180603, acc: 85.9375, f1: 77.95787545787546, r: 0.6321847392917526
06/02/2019 09:33:09 step: 4716, epoch: 142, batch: 29, loss: 0.4027208387851715, acc: 87.5, f1: 68.54764858797117, r: 0.6683383394286853
06/02/2019 09:33:10 *** evaluating ***
06/02/2019 09:33:10 step: 143, epoch: 142, acc: 60.68376068376068, f1: 28.487951509902633, r: 0.3891330777819331
06/02/2019 09:33:10 *** epoch: 144 ***
06/02/2019 09:33:10 *** training ***
06/02/2019 09:33:11 step: 4724, epoch: 143, batch: 4, loss: 0.4113241732120514, acc: 81.25, f1: 58.5071152206702, r: 0.6412906466810764
06/02/2019 09:33:12 step: 4729, epoch: 143, batch: 9, loss: 0.4965283274650574, acc: 84.375, f1: 68.88392857142858, r: 0.643064603162022
06/02/2019 09:33:13 step: 4734, epoch: 143, batch: 14, loss: 0.428609699010849, acc: 82.8125, f1: 56.50757450569481, r: 0.5522452419898723
06/02/2019 09:33:15 step: 4739, epoch: 143, batch: 19, loss: 0.4488639235496521, acc: 79.6875, f1: 60.528307476836886, r: 0.665784721951114
06/02/2019 09:33:16 step: 4744, epoch: 143, batch: 24, loss: 0.35214537382125854, acc: 90.625, f1: 79.73173884938592, r: 0.7049949251684361
06/02/2019 09:33:17 step: 4749, epoch: 143, batch: 29, loss: 0.39906251430511475, acc: 85.9375, f1: 67.96883552271483, r: 0.6400203917019155
06/02/2019 09:33:17 *** evaluating ***
06/02/2019 09:33:18 step: 144, epoch: 143, acc: 57.26495726495726, f1: 28.544326201644942, r: 0.4072037059326021
06/02/2019 09:33:18 *** epoch: 145 ***
06/02/2019 09:33:18 *** training ***
06/02/2019 09:33:19 step: 4757, epoch: 144, batch: 4, loss: 0.3835608959197998, acc: 85.9375, f1: 60.340299913068684, r: 0.583786458950901
06/02/2019 09:33:20 step: 4762, epoch: 144, batch: 9, loss: 0.3173750042915344, acc: 89.0625, f1: 76.72224038356762, r: 0.5753881429148157
06/02/2019 09:33:21 step: 4767, epoch: 144, batch: 14, loss: 0.39085492491722107, acc: 87.5, f1: 63.97400820793433, r: 0.6255378581050327
06/02/2019 09:33:22 step: 4772, epoch: 144, batch: 19, loss: 0.42398348450660706, acc: 78.125, f1: 65.5844155844156, r: 0.625510182330186
06/02/2019 09:33:23 step: 4777, epoch: 144, batch: 24, loss: 0.4988716244697571, acc: 76.5625, f1: 54.233827369331564, r: 0.659141400626747
06/02/2019 09:33:24 step: 4782, epoch: 144, batch: 29, loss: 0.4083854556083679, acc: 85.9375, f1: 70.00696767001115, r: 0.6160007223568458
06/02/2019 09:33:25 *** evaluating ***
06/02/2019 09:33:25 step: 145, epoch: 144, acc: 57.692307692307686, f1: 28.853363274209915, r: 0.40196833778552904
06/02/2019 09:33:25 *** epoch: 146 ***
06/02/2019 09:33:25 *** training ***
06/02/2019 09:33:27 step: 4790, epoch: 145, batch: 4, loss: 0.43497830629348755, acc: 85.9375, f1: 68.06239737274221, r: 0.513903948741993
06/02/2019 09:33:28 step: 4795, epoch: 145, batch: 9, loss: 0.3277221620082855, acc: 90.625, f1: 76.80654761904762, r: 0.6696904899589926
06/02/2019 09:33:29 step: 4800, epoch: 145, batch: 14, loss: 0.32474562525749207, acc: 92.1875, f1: 79.75587731798912, r: 0.6420411053465789
06/02/2019 09:33:30 step: 4805, epoch: 145, batch: 19, loss: 0.42768603563308716, acc: 85.9375, f1: 52.5094696969697, r: 0.583574603922916
06/02/2019 09:33:31 step: 4810, epoch: 145, batch: 24, loss: 0.6170566082000732, acc: 87.5, f1: 77.51709164818921, r: 0.6090774178263801
06/02/2019 09:33:32 step: 4815, epoch: 145, batch: 29, loss: 0.3804174065589905, acc: 84.375, f1: 64.42727090836334, r: 0.6949854682162141
06/02/2019 09:33:33 *** evaluating ***
06/02/2019 09:33:33 step: 146, epoch: 145, acc: 58.97435897435898, f1: 28.809403315982262, r: 0.3923683228555568
06/02/2019 09:33:33 *** epoch: 147 ***
06/02/2019 09:33:33 *** training ***
06/02/2019 09:33:34 step: 4823, epoch: 146, batch: 4, loss: 0.41622623801231384, acc: 89.0625, f1: 79.95514409630542, r: 0.675537348702247
06/02/2019 09:33:36 step: 4828, epoch: 146, batch: 9, loss: 0.3331098258495331, acc: 85.9375, f1: 72.41421843462659, r: 0.5968950678836064
06/02/2019 09:33:37 step: 4833, epoch: 146, batch: 14, loss: 0.2814282774925232, acc: 96.875, f1: 83.96331738437001, r: 0.6138230936431458
06/02/2019 09:33:38 step: 4838, epoch: 146, batch: 19, loss: 0.31264162063598633, acc: 90.625, f1: 68.73722371193428, r: 0.5987930110867231
06/02/2019 09:33:39 step: 4843, epoch: 146, batch: 24, loss: 0.2564579248428345, acc: 90.625, f1: 83.18341182626898, r: 0.6125150573493416
06/02/2019 09:33:40 step: 4848, epoch: 146, batch: 29, loss: 0.24289119243621826, acc: 92.1875, f1: 81.63486173690255, r: 0.6276222312161956
06/02/2019 09:33:40 *** evaluating ***
06/02/2019 09:33:41 step: 147, epoch: 146, acc: 57.692307692307686, f1: 28.69755837397681, r: 0.3841919153533368
06/02/2019 09:33:41 *** epoch: 148 ***
06/02/2019 09:33:41 *** training ***
06/02/2019 09:33:42 step: 4856, epoch: 147, batch: 4, loss: 0.28297045826911926, acc: 89.0625, f1: 76.11455031940207, r: 0.6045665298664222
06/02/2019 09:33:43 step: 4861, epoch: 147, batch: 9, loss: 0.15983280539512634, acc: 93.75, f1: 91.08320251177395, r: 0.6997496569300171
06/02/2019 09:33:44 step: 4866, epoch: 147, batch: 14, loss: 0.3779797852039337, acc: 89.0625, f1: 81.95635742219595, r: 0.6193311671468071
06/02/2019 09:33:45 step: 4871, epoch: 147, batch: 19, loss: 0.3658636212348938, acc: 84.375, f1: 74.66891069048638, r: 0.6634664794610692
06/02/2019 09:33:46 step: 4876, epoch: 147, batch: 24, loss: 0.4818589389324188, acc: 82.8125, f1: 65.69028797289667, r: 0.6236861197759873
06/02/2019 09:33:47 step: 4881, epoch: 147, batch: 29, loss: 0.31305059790611267, acc: 90.625, f1: 76.43654591023012, r: 0.7150473119895856
06/02/2019 09:33:48 *** evaluating ***
06/02/2019 09:33:48 step: 148, epoch: 147, acc: 57.692307692307686, f1: 27.52416139751666, r: 0.3835165214792839
06/02/2019 09:33:48 *** epoch: 149 ***
06/02/2019 09:33:48 *** training ***
06/02/2019 09:33:49 step: 4889, epoch: 148, batch: 4, loss: 0.3730587363243103, acc: 87.5, f1: 71.52594069570024, r: 0.6628637496136599
06/02/2019 09:33:50 step: 4894, epoch: 148, batch: 9, loss: 0.2656882703304291, acc: 93.75, f1: 90.43859649122808, r: 0.5930111532476618
06/02/2019 09:33:51 step: 4899, epoch: 148, batch: 14, loss: 0.2890430986881256, acc: 87.5, f1: 78.38374672312966, r: 0.6085391453720649
06/02/2019 09:33:52 step: 4904, epoch: 148, batch: 19, loss: 0.3682531416416168, acc: 82.8125, f1: 62.973942321768405, r: 0.691526997028393
06/02/2019 09:33:53 step: 4909, epoch: 148, batch: 24, loss: 0.4445415735244751, acc: 90.625, f1: 66.43176765797483, r: 0.6266036723751075
06/02/2019 09:33:55 step: 4914, epoch: 148, batch: 29, loss: 0.36615100502967834, acc: 82.8125, f1: 67.13946972567662, r: 0.5757911868398018
06/02/2019 09:33:55 *** evaluating ***
06/02/2019 09:33:56 step: 149, epoch: 148, acc: 57.26495726495726, f1: 31.192479479353928, r: 0.3804264743179833
06/02/2019 09:33:56 *** epoch: 150 ***
06/02/2019 09:33:56 *** training ***
06/02/2019 09:33:57 step: 4922, epoch: 149, batch: 4, loss: 0.29143044352531433, acc: 85.9375, f1: 72.73062691948891, r: 0.6134144951717753
06/02/2019 09:33:58 step: 4927, epoch: 149, batch: 9, loss: 0.4252135753631592, acc: 81.25, f1: 63.459119496855344, r: 0.6017307125800347
06/02/2019 09:33:59 step: 4932, epoch: 149, batch: 14, loss: 0.32847899198532104, acc: 89.0625, f1: 75.8208525345622, r: 0.5838600983876336
06/02/2019 09:34:00 step: 4937, epoch: 149, batch: 19, loss: 0.2545337677001953, acc: 92.1875, f1: 84.81136777055144, r: 0.6372246026309152
06/02/2019 09:34:01 step: 4942, epoch: 149, batch: 24, loss: 0.2675778865814209, acc: 90.625, f1: 79.22551789077212, r: 0.7173617575142018
06/02/2019 09:34:02 step: 4947, epoch: 149, batch: 29, loss: 0.41054996848106384, acc: 81.25, f1: 68.94747122795903, r: 0.6701368857247998
06/02/2019 09:34:03 *** evaluating ***
06/02/2019 09:34:03 step: 150, epoch: 149, acc: 58.119658119658126, f1: 28.629908103592317, r: 0.3902884736135924
06/02/2019 09:34:03 *** epoch: 151 ***
06/02/2019 09:34:03 *** training ***
06/02/2019 09:34:05 step: 4955, epoch: 150, batch: 4, loss: 0.24538090825080872, acc: 93.75, f1: 92.52910052910053, r: 0.7392790818401119
06/02/2019 09:34:06 step: 4960, epoch: 150, batch: 9, loss: 0.3247748017311096, acc: 84.375, f1: 71.92677389660147, r: 0.7164859980284222
06/02/2019 09:34:07 step: 4965, epoch: 150, batch: 14, loss: 0.33621397614479065, acc: 85.9375, f1: 68.88653084323713, r: 0.6764226077657726
06/02/2019 09:34:08 step: 4970, epoch: 150, batch: 19, loss: 0.2169165313243866, acc: 93.75, f1: 85.68707482993197, r: 0.6491628821540477
06/02/2019 09:34:09 step: 4975, epoch: 150, batch: 24, loss: 0.22022894024848938, acc: 90.625, f1: 80.63973063973063, r: 0.6793316203075223
06/02/2019 09:34:10 step: 4980, epoch: 150, batch: 29, loss: 0.49674075841903687, acc: 90.625, f1: 74.83806566104703, r: 0.6090656064235883
06/02/2019 09:34:11 *** evaluating ***
06/02/2019 09:34:11 step: 151, epoch: 150, acc: 58.54700854700855, f1: 27.28101158794953, r: 0.38109648878258523
06/02/2019 09:34:11 *** epoch: 152 ***
06/02/2019 09:34:11 *** training ***
06/02/2019 09:34:12 step: 4988, epoch: 151, batch: 4, loss: 0.2816289961338043, acc: 89.0625, f1: 86.66352602315716, r: 0.8108502508048155
06/02/2019 09:34:13 step: 4993, epoch: 151, batch: 9, loss: 0.34878379106521606, acc: 84.375, f1: 76.24258662865388, r: 0.6844354458792111
06/02/2019 09:34:14 step: 4998, epoch: 151, batch: 14, loss: 0.2682983875274658, acc: 90.625, f1: 77.48072733366851, r: 0.7069247321389502
06/02/2019 09:34:16 step: 5003, epoch: 151, batch: 19, loss: 0.30837762355804443, acc: 90.625, f1: 87.84475965327029, r: 0.6219547822557034
06/02/2019 09:34:17 step: 5008, epoch: 151, batch: 24, loss: 0.2566686272621155, acc: 90.625, f1: 68.43164696938283, r: 0.6544836117904356
06/02/2019 09:34:18 step: 5013, epoch: 151, batch: 29, loss: 0.15414738655090332, acc: 93.75, f1: 79.98662740768003, r: 0.7683172234546017
06/02/2019 09:34:18 *** evaluating ***
06/02/2019 09:34:18 step: 152, epoch: 151, acc: 60.256410256410255, f1: 28.23195548909355, r: 0.38058010859393204
06/02/2019 09:34:18 *** epoch: 153 ***
06/02/2019 09:34:18 *** training ***
06/02/2019 09:34:20 step: 5021, epoch: 152, batch: 4, loss: 0.3741069734096527, acc: 85.9375, f1: 58.24894561653036, r: 0.5426381889043435
06/02/2019 09:34:21 step: 5026, epoch: 152, batch: 9, loss: 0.39436039328575134, acc: 82.8125, f1: 70.4172335600907, r: 0.5364093423033252
06/02/2019 09:34:22 step: 5031, epoch: 152, batch: 14, loss: 0.40022769570350647, acc: 85.9375, f1: 76.28131414509124, r: 0.5693095135291573
06/02/2019 09:34:23 step: 5036, epoch: 152, batch: 19, loss: 0.33162611722946167, acc: 89.0625, f1: 68.5648148148148, r: 0.6041189212925786
06/02/2019 09:34:24 step: 5041, epoch: 152, batch: 24, loss: 0.31494271755218506, acc: 87.5, f1: 84.11167064228289, r: 0.6478013214425566
06/02/2019 09:34:25 step: 5046, epoch: 152, batch: 29, loss: 0.3632044792175293, acc: 87.5, f1: 65.16368116977873, r: 0.7214877039803907
06/02/2019 09:34:26 *** evaluating ***
06/02/2019 09:34:26 step: 153, epoch: 152, acc: 57.692307692307686, f1: 27.516454724601424, r: 0.3923925535121399
06/02/2019 09:34:26 *** epoch: 154 ***
06/02/2019 09:34:26 *** training ***
06/02/2019 09:34:27 step: 5054, epoch: 153, batch: 4, loss: 0.3037396967411041, acc: 85.9375, f1: 81.58730158730158, r: 0.7609058967214841
06/02/2019 09:34:28 step: 5059, epoch: 153, batch: 9, loss: 0.2741137742996216, acc: 87.5, f1: 80.39369593991442, r: 0.6744596663815916
06/02/2019 09:34:29 step: 5064, epoch: 153, batch: 14, loss: 0.23035235702991486, acc: 90.625, f1: 74.34924426093775, r: 0.6405157198841664
06/02/2019 09:34:30 step: 5069, epoch: 153, batch: 19, loss: 0.4265827238559723, acc: 90.625, f1: 55.98320158102767, r: 0.5359860079143618
06/02/2019 09:34:31 step: 5074, epoch: 153, batch: 24, loss: 0.25204023718833923, acc: 90.625, f1: 75.29887095459362, r: 0.7289225949078841
06/02/2019 09:34:32 step: 5079, epoch: 153, batch: 29, loss: 0.47171008586883545, acc: 82.8125, f1: 67.47474747474747, r: 0.7054996842473326
06/02/2019 09:34:33 *** evaluating ***
06/02/2019 09:34:33 step: 154, epoch: 153, acc: 61.111111111111114, f1: 30.098629658723926, r: 0.3749808481720262
06/02/2019 09:34:33 *** epoch: 155 ***
06/02/2019 09:34:33 *** training ***
06/02/2019 09:34:34 step: 5087, epoch: 154, batch: 4, loss: 0.29236501455307007, acc: 87.5, f1: 70.12934981684981, r: 0.7743403520506267
06/02/2019 09:34:35 step: 5092, epoch: 154, batch: 9, loss: 0.34618085622787476, acc: 85.9375, f1: 78.09904381332953, r: 0.6287477351022303
06/02/2019 09:34:36 step: 5097, epoch: 154, batch: 14, loss: 0.21658644080162048, acc: 92.1875, f1: 87.7438446969697, r: 0.7263992671964201
06/02/2019 09:34:37 step: 5102, epoch: 154, batch: 19, loss: 0.3070261478424072, acc: 89.0625, f1: 72.54782254782255, r: 0.5623801647148036
06/02/2019 09:34:38 step: 5107, epoch: 154, batch: 24, loss: 0.2806186079978943, acc: 89.0625, f1: 77.62169312169313, r: 0.721224587683571
06/02/2019 09:34:40 step: 5112, epoch: 154, batch: 29, loss: 0.23685571551322937, acc: 90.625, f1: 80.49590850833087, r: 0.6507861682742858
06/02/2019 09:34:40 *** evaluating ***
06/02/2019 09:34:41 step: 155, epoch: 154, acc: 60.256410256410255, f1: 29.028458828229997, r: 0.3922813830539081
06/02/2019 09:34:41 *** epoch: 156 ***
06/02/2019 09:34:41 *** training ***
06/02/2019 09:34:42 step: 5120, epoch: 155, batch: 4, loss: 0.15462282299995422, acc: 95.3125, f1: 69.8718557190318, r: 0.607071420166125
06/02/2019 09:34:43 step: 5125, epoch: 155, batch: 9, loss: 0.30408892035484314, acc: 85.9375, f1: 54.73389071133432, r: 0.6301714488342331
06/02/2019 09:34:44 step: 5130, epoch: 155, batch: 14, loss: 0.3176366090774536, acc: 89.0625, f1: 65.82277097902099, r: 0.7206890841712251
06/02/2019 09:34:45 step: 5135, epoch: 155, batch: 19, loss: 0.4153748154640198, acc: 84.375, f1: 63.92192192192192, r: 0.5675563140130737
06/02/2019 09:34:46 step: 5140, epoch: 155, batch: 24, loss: 0.26420968770980835, acc: 87.5, f1: 76.14325951825951, r: 0.6722751364813846
06/02/2019 09:34:47 step: 5145, epoch: 155, batch: 29, loss: 0.3192000091075897, acc: 84.375, f1: 64.210226391226, r: 0.675307518500894
06/02/2019 09:34:47 *** evaluating ***
06/02/2019 09:34:48 step: 156, epoch: 155, acc: 59.82905982905983, f1: 31.524644467694518, r: 0.39323595234203895
06/02/2019 09:34:48 *** epoch: 157 ***
06/02/2019 09:34:48 *** training ***
06/02/2019 09:34:49 step: 5153, epoch: 156, batch: 4, loss: 0.2168758064508438, acc: 93.75, f1: 89.67074009090815, r: 0.6182224951399298
06/02/2019 09:34:50 step: 5158, epoch: 156, batch: 9, loss: 0.18278659880161285, acc: 93.75, f1: 89.80470320515913, r: 0.6874381469738446
06/02/2019 09:34:51 step: 5163, epoch: 156, batch: 14, loss: 0.3909376561641693, acc: 84.375, f1: 52.280844155844164, r: 0.5503509350035534
06/02/2019 09:34:52 step: 5168, epoch: 156, batch: 19, loss: 0.3054613769054413, acc: 89.0625, f1: 64.30326503414959, r: 0.6489944252195375
06/02/2019 09:34:53 step: 5173, epoch: 156, batch: 24, loss: 0.25991785526275635, acc: 93.75, f1: 90.78167385521135, r: 0.6286577756594525
06/02/2019 09:34:54 step: 5178, epoch: 156, batch: 29, loss: 0.21653932332992554, acc: 93.75, f1: 75.60846560846561, r: 0.6233865101657319
06/02/2019 09:34:55 *** evaluating ***
06/02/2019 09:34:55 step: 157, epoch: 156, acc: 59.401709401709404, f1: 28.133449900390556, r: 0.37944700913262064
06/02/2019 09:34:55 *** epoch: 158 ***
06/02/2019 09:34:55 *** training ***
06/02/2019 09:34:56 step: 5186, epoch: 157, batch: 4, loss: 0.2931603789329529, acc: 89.0625, f1: 79.34186018584997, r: 0.6282749235108444
06/02/2019 09:34:57 step: 5191, epoch: 157, batch: 9, loss: 0.39530909061431885, acc: 84.375, f1: 55.509128552606825, r: 0.4807370636616311
06/02/2019 09:34:59 step: 5196, epoch: 157, batch: 14, loss: 0.21963368356227875, acc: 93.75, f1: 85.48611111111111, r: 0.7670995634245006
06/02/2019 09:35:00 step: 5201, epoch: 157, batch: 19, loss: 0.31811168789863586, acc: 89.0625, f1: 75.82539682539682, r: 0.7152346283298778
06/02/2019 09:35:01 step: 5206, epoch: 157, batch: 24, loss: 0.253518283367157, acc: 93.75, f1: 81.58972253799841, r: 0.7589257940159735
06/02/2019 09:35:02 step: 5211, epoch: 157, batch: 29, loss: 0.19332380592823029, acc: 93.75, f1: 79.20280390107976, r: 0.6547866145933118
06/02/2019 09:35:02 *** evaluating ***
06/02/2019 09:35:03 step: 158, epoch: 157, acc: 58.97435897435898, f1: 27.584278152193995, r: 0.3688973165528008
06/02/2019 09:35:03 *** epoch: 159 ***
06/02/2019 09:35:03 *** training ***
06/02/2019 09:35:04 step: 5219, epoch: 158, batch: 4, loss: 0.257800817489624, acc: 89.0625, f1: 66.41737891737893, r: 0.4924762271537811
06/02/2019 09:35:05 step: 5224, epoch: 158, batch: 9, loss: 0.3196978271007538, acc: 89.0625, f1: 76.65229885057471, r: 0.6406997683362046
06/02/2019 09:35:06 step: 5229, epoch: 158, batch: 14, loss: 0.3264802098274231, acc: 85.9375, f1: 60.59794372294371, r: 0.6669032610523749
06/02/2019 09:35:07 step: 5234, epoch: 158, batch: 19, loss: 0.16603705286979675, acc: 95.3125, f1: 83.62179487179488, r: 0.7694048374480054
06/02/2019 09:35:08 step: 5239, epoch: 158, batch: 24, loss: 0.32010355591773987, acc: 90.625, f1: 90.69601256933805, r: 0.5961238067716528
06/02/2019 09:35:09 step: 5244, epoch: 158, batch: 29, loss: 0.3030569851398468, acc: 87.5, f1: 66.5940184602269, r: 0.7047934969871807
06/02/2019 09:35:10 *** evaluating ***
06/02/2019 09:35:10 step: 159, epoch: 158, acc: 58.54700854700855, f1: 28.602681810311392, r: 0.38153709514446926
06/02/2019 09:35:10 *** epoch: 160 ***
06/02/2019 09:35:10 *** training ***
06/02/2019 09:35:11 step: 5252, epoch: 159, batch: 4, loss: 0.20029842853546143, acc: 95.3125, f1: 89.99999999999999, r: 0.5491769994466295
06/02/2019 09:35:12 step: 5257, epoch: 159, batch: 9, loss: 0.2872236669063568, acc: 89.0625, f1: 88.74931318681318, r: 0.6841447132624204
06/02/2019 09:35:13 step: 5262, epoch: 159, batch: 14, loss: 0.18992917239665985, acc: 90.625, f1: 85.6165629201938, r: 0.6623992164463053
06/02/2019 09:35:14 step: 5267, epoch: 159, batch: 19, loss: 0.25092828273773193, acc: 89.0625, f1: 85.0573774052035, r: 0.6902806198223029
06/02/2019 09:35:15 step: 5272, epoch: 159, batch: 24, loss: 0.25504955649375916, acc: 89.0625, f1: 76.46838668577799, r: 0.6493848244016478
06/02/2019 09:35:16 step: 5277, epoch: 159, batch: 29, loss: 0.20064520835876465, acc: 96.875, f1: 93.47222222222223, r: 0.7491727194192878
06/02/2019 09:35:17 *** evaluating ***
06/02/2019 09:35:17 step: 160, epoch: 159, acc: 57.692307692307686, f1: 27.403194213636827, r: 0.3811731081308325
06/02/2019 09:35:17 *** epoch: 161 ***
06/02/2019 09:35:17 *** training ***
06/02/2019 09:35:18 step: 5285, epoch: 160, batch: 4, loss: 0.35032686591148376, acc: 89.0625, f1: 82.25091293770893, r: 0.6780178978899029
06/02/2019 09:35:20 step: 5290, epoch: 160, batch: 9, loss: 0.2758937478065491, acc: 90.625, f1: 65.75200123152709, r: 0.6868016696571879
06/02/2019 09:35:21 step: 5295, epoch: 160, batch: 14, loss: 0.3833145499229431, acc: 87.5, f1: 61.32834699077894, r: 0.5314343601849704
06/02/2019 09:35:22 step: 5300, epoch: 160, batch: 19, loss: 0.30021533370018005, acc: 89.0625, f1: 73.51611268077025, r: 0.7064427880629551
06/02/2019 09:35:23 step: 5305, epoch: 160, batch: 24, loss: 0.2285355031490326, acc: 90.625, f1: 83.21123321123322, r: 0.5011095668162103
06/02/2019 09:35:24 step: 5310, epoch: 160, batch: 29, loss: 0.28252801299095154, acc: 87.5, f1: 77.72837841480141, r: 0.7159102297844075
06/02/2019 09:35:24 *** evaluating ***
06/02/2019 09:35:25 step: 161, epoch: 160, acc: 57.692307692307686, f1: 27.503887310054658, r: 0.3821037531058299
06/02/2019 09:35:25 *** epoch: 162 ***
06/02/2019 09:35:25 *** training ***
06/02/2019 09:35:26 step: 5318, epoch: 161, batch: 4, loss: 0.2317800521850586, acc: 92.1875, f1: 86.62695775984812, r: 0.5755724211156457
06/02/2019 09:35:27 step: 5323, epoch: 161, batch: 9, loss: 0.24840761721134186, acc: 90.625, f1: 80.93143312558206, r: 0.7110768146168938
06/02/2019 09:35:28 step: 5328, epoch: 161, batch: 14, loss: 0.18329356610774994, acc: 93.75, f1: 92.1957671957672, r: 0.7896908086486275
06/02/2019 09:35:29 step: 5333, epoch: 161, batch: 19, loss: 0.18617108464241028, acc: 90.625, f1: 84.40336134453781, r: 0.5435534316379537
06/02/2019 09:35:30 step: 5338, epoch: 161, batch: 24, loss: 0.33815744519233704, acc: 90.625, f1: 75.055476736149, r: 0.5531881767882596
06/02/2019 09:35:31 step: 5343, epoch: 161, batch: 29, loss: 0.3400551378726959, acc: 89.0625, f1: 71.84651133180544, r: 0.6978609594735747
06/02/2019 09:35:32 *** evaluating ***
06/02/2019 09:35:32 step: 162, epoch: 161, acc: 55.98290598290598, f1: 27.196040674301546, r: 0.37490675137201607
06/02/2019 09:35:32 *** epoch: 163 ***
06/02/2019 09:35:32 *** training ***
06/02/2019 09:35:33 step: 5351, epoch: 162, batch: 4, loss: 0.26997509598731995, acc: 89.0625, f1: 73.17066015595427, r: 0.6411694840454264
06/02/2019 09:35:34 step: 5356, epoch: 162, batch: 9, loss: 0.24466900527477264, acc: 90.625, f1: 76.34669796434503, r: 0.6738377398784721
06/02/2019 09:35:35 step: 5361, epoch: 162, batch: 14, loss: 0.26548799872398376, acc: 93.75, f1: 86.68459471433125, r: 0.5770906008029796
06/02/2019 09:35:37 step: 5366, epoch: 162, batch: 19, loss: 0.24493154883384705, acc: 92.1875, f1: 70.2116704805492, r: 0.6089122629983144
06/02/2019 09:35:38 step: 5371, epoch: 162, batch: 24, loss: 0.20465081930160522, acc: 89.0625, f1: 79.3728516425793, r: 0.6974581975580153
06/02/2019 09:35:39 step: 5376, epoch: 162, batch: 29, loss: 0.23790673911571503, acc: 87.5, f1: 72.33288620612564, r: 0.6663839759978245
06/02/2019 09:35:39 *** evaluating ***
06/02/2019 09:35:39 step: 163, epoch: 162, acc: 54.27350427350427, f1: 26.258204594706875, r: 0.3614699241231392
06/02/2019 09:35:39 *** epoch: 164 ***
06/02/2019 09:35:39 *** training ***
06/02/2019 09:35:41 step: 5384, epoch: 163, batch: 4, loss: 0.3368070721626282, acc: 84.375, f1: 79.19291235080709, r: 0.5798165964735964
06/02/2019 09:35:42 step: 5389, epoch: 163, batch: 9, loss: 0.1833113580942154, acc: 95.3125, f1: 87.97871698568562, r: 0.684438049059122
06/02/2019 09:35:43 step: 5394, epoch: 163, batch: 14, loss: 0.3413352966308594, acc: 89.0625, f1: 83.65352577812735, r: 0.6404892748656335
06/02/2019 09:35:44 step: 5399, epoch: 163, batch: 19, loss: 0.21304751932621002, acc: 93.75, f1: 78.89491460920031, r: 0.6709762901653723
06/02/2019 09:35:45 step: 5404, epoch: 163, batch: 24, loss: 0.28564682602882385, acc: 90.625, f1: 65.8625681455378, r: 0.6391508456358567
06/02/2019 09:35:46 step: 5409, epoch: 163, batch: 29, loss: 0.1586257517337799, acc: 96.875, f1: 96.17398474541332, r: 0.6630373944430453
06/02/2019 09:35:46 *** evaluating ***
06/02/2019 09:35:47 step: 164, epoch: 163, acc: 60.256410256410255, f1: 29.65293489190548, r: 0.38318196109810126
06/02/2019 09:35:47 *** epoch: 165 ***
06/02/2019 09:35:47 *** training ***
06/02/2019 09:35:48 step: 5417, epoch: 164, batch: 4, loss: 0.34230682253837585, acc: 81.25, f1: 68.16743563454088, r: 0.6218047495735171
06/02/2019 09:35:49 step: 5422, epoch: 164, batch: 9, loss: 0.3311524987220764, acc: 85.9375, f1: 61.544851611174366, r: 0.5180828675978856
06/02/2019 09:35:50 step: 5427, epoch: 164, batch: 14, loss: 0.25887539982795715, acc: 89.0625, f1: 55.00000000000001, r: 0.5867202830078668
06/02/2019 09:35:51 step: 5432, epoch: 164, batch: 19, loss: 0.4091494083404541, acc: 90.625, f1: 89.16891284815813, r: 0.7151281057044511
06/02/2019 09:35:52 step: 5437, epoch: 164, batch: 24, loss: 0.2556392550468445, acc: 87.5, f1: 74.40359477124183, r: 0.6348772454467304
06/02/2019 09:35:53 step: 5442, epoch: 164, batch: 29, loss: 0.3852120637893677, acc: 82.8125, f1: 75.93308865047995, r: 0.609902930173119
06/02/2019 09:35:53 *** evaluating ***
06/02/2019 09:35:54 step: 165, epoch: 164, acc: 56.837606837606835, f1: 28.801872978862193, r: 0.3789361423148088
06/02/2019 09:35:54 *** epoch: 166 ***
06/02/2019 09:35:54 *** training ***
06/02/2019 09:35:55 step: 5450, epoch: 165, batch: 4, loss: 0.2693149447441101, acc: 90.625, f1: 90.18371766497702, r: 0.7298239741329804
06/02/2019 09:35:56 step: 5455, epoch: 165, batch: 9, loss: 0.17954258620738983, acc: 92.1875, f1: 68.56400966183575, r: 0.6600003493989737
06/02/2019 09:35:57 step: 5460, epoch: 165, batch: 14, loss: 0.2114720493555069, acc: 92.1875, f1: 90.15717398070339, r: 0.7236846024203861
06/02/2019 09:35:58 step: 5465, epoch: 165, batch: 19, loss: 0.21146264672279358, acc: 90.625, f1: 79.08804035798717, r: 0.7404307670163564
06/02/2019 09:35:59 step: 5470, epoch: 165, batch: 24, loss: 0.24477513134479523, acc: 87.5, f1: 59.0538698589546, r: 0.5772945630430225
06/02/2019 09:36:00 step: 5475, epoch: 165, batch: 29, loss: 0.3465439975261688, acc: 87.5, f1: 74.01853285173718, r: 0.6461736971867128
06/02/2019 09:36:01 *** evaluating ***
06/02/2019 09:36:01 step: 166, epoch: 165, acc: 56.41025641025641, f1: 27.987908124222315, r: 0.3522936734534411
06/02/2019 09:36:01 *** epoch: 167 ***
06/02/2019 09:36:01 *** training ***
06/02/2019 09:36:02 step: 5483, epoch: 166, batch: 4, loss: 0.3055206537246704, acc: 84.375, f1: 63.30583297148647, r: 0.6318557796740758
06/02/2019 09:36:03 step: 5488, epoch: 166, batch: 9, loss: 0.18131673336029053, acc: 90.625, f1: 82.91160645197911, r: 0.6535753980734301
06/02/2019 09:36:04 step: 5493, epoch: 166, batch: 14, loss: 0.31306520104408264, acc: 89.0625, f1: 74.25713012477719, r: 0.7421238631388176
06/02/2019 09:36:06 step: 5498, epoch: 166, batch: 19, loss: 0.20435169339179993, acc: 93.75, f1: 89.70766076029236, r: 0.6114653310176017
06/02/2019 09:36:07 step: 5503, epoch: 166, batch: 24, loss: 0.29197362065315247, acc: 90.625, f1: 80.5769330129944, r: 0.6678502122213846
06/02/2019 09:36:08 step: 5508, epoch: 166, batch: 29, loss: 0.2244042456150055, acc: 92.1875, f1: 80.2903205169215, r: 0.6573625046167668
06/02/2019 09:36:08 *** evaluating ***
06/02/2019 09:36:09 step: 167, epoch: 166, acc: 58.54700854700855, f1: 27.898223096015624, r: 0.3807854634420076
06/02/2019 09:36:09 *** epoch: 168 ***
06/02/2019 09:36:09 *** training ***
06/02/2019 09:36:10 step: 5516, epoch: 167, batch: 4, loss: 0.2939382493495941, acc: 84.375, f1: 66.01641414141413, r: 0.6641830554570624
06/02/2019 09:36:11 step: 5521, epoch: 167, batch: 9, loss: 0.33775240182876587, acc: 87.5, f1: 80.5026455026455, r: 0.6927273058864626
06/02/2019 09:36:12 step: 5526, epoch: 167, batch: 14, loss: 0.2398625910282135, acc: 93.75, f1: 80.5474465836308, r: 0.6672993939182615
06/02/2019 09:36:13 step: 5531, epoch: 167, batch: 19, loss: 0.12839201092720032, acc: 95.3125, f1: 91.20370370370371, r: 0.5907521847867541
06/02/2019 09:36:14 step: 5536, epoch: 167, batch: 24, loss: 0.23077833652496338, acc: 90.625, f1: 60.021864067966014, r: 0.560935501579971
06/02/2019 09:36:15 step: 5541, epoch: 167, batch: 29, loss: 0.25130605697631836, acc: 89.0625, f1: 73.86724386724389, r: 0.5857491128262796
06/02/2019 09:36:16 *** evaluating ***
06/02/2019 09:36:16 step: 168, epoch: 167, acc: 53.41880341880342, f1: 25.866545871464712, r: 0.3477053077701525
06/02/2019 09:36:16 *** epoch: 169 ***
06/02/2019 09:36:16 *** training ***
06/02/2019 09:36:17 step: 5549, epoch: 168, batch: 4, loss: 0.26685482263565063, acc: 87.5, f1: 82.57904690202827, r: 0.6744744311029022
06/02/2019 09:36:19 step: 5554, epoch: 168, batch: 9, loss: 0.2190731316804886, acc: 92.1875, f1: 88.55367585630745, r: 0.6258519658051173
06/02/2019 09:36:20 step: 5559, epoch: 168, batch: 14, loss: 0.1819559633731842, acc: 92.1875, f1: 81.67182024324883, r: 0.6562350327648685
06/02/2019 09:36:21 step: 5564, epoch: 168, batch: 19, loss: 0.18780525028705597, acc: 92.1875, f1: 78.04788110212638, r: 0.6922363073748307
06/02/2019 09:36:22 step: 5569, epoch: 168, batch: 24, loss: 0.26259681582450867, acc: 87.5, f1: 69.73199907982517, r: 0.6918439558656787
06/02/2019 09:36:23 step: 5574, epoch: 168, batch: 29, loss: 0.23925846815109253, acc: 90.625, f1: 84.64976549626061, r: 0.6801692402267007
06/02/2019 09:36:24 *** evaluating ***
06/02/2019 09:36:24 step: 169, epoch: 168, acc: 58.54700854700855, f1: 29.75173536526084, r: 0.3503149409541793
06/02/2019 09:36:24 *** epoch: 170 ***
06/02/2019 09:36:24 *** training ***
06/02/2019 09:36:25 step: 5582, epoch: 169, batch: 4, loss: 0.2984004020690918, acc: 85.9375, f1: 67.14647380920967, r: 0.7261680269349807
06/02/2019 09:36:26 step: 5587, epoch: 169, batch: 9, loss: 0.28234460949897766, acc: 89.0625, f1: 79.52635188087773, r: 0.6630408163545843
06/02/2019 09:36:27 step: 5592, epoch: 169, batch: 14, loss: 0.2504862844944, acc: 89.0625, f1: 71.30252100840336, r: 0.5514734951022097
06/02/2019 09:36:28 step: 5597, epoch: 169, batch: 19, loss: 0.1527690440416336, acc: 96.875, f1: 98.48698099929628, r: 0.6706536222027971
06/02/2019 09:36:29 step: 5602, epoch: 169, batch: 24, loss: 0.11974712461233139, acc: 96.875, f1: 97.96651386404294, r: 0.7419393127494758
06/02/2019 09:36:31 step: 5607, epoch: 169, batch: 29, loss: 0.18054988980293274, acc: 93.75, f1: 94.90996925779535, r: 0.737135825718469
06/02/2019 09:36:31 *** evaluating ***
06/02/2019 09:36:31 step: 170, epoch: 169, acc: 58.119658119658126, f1: 29.02229139175342, r: 0.3574704757565659
06/02/2019 09:36:31 *** epoch: 171 ***
06/02/2019 09:36:31 *** training ***
06/02/2019 09:36:32 step: 5615, epoch: 170, batch: 4, loss: 0.27374544739723206, acc: 85.9375, f1: 67.48168498168499, r: 0.5214684990937547
06/02/2019 09:36:33 step: 5620, epoch: 170, batch: 9, loss: 0.2296234369277954, acc: 95.3125, f1: 80.16440704378994, r: 0.7120075032987517
06/02/2019 09:36:35 step: 5625, epoch: 170, batch: 14, loss: 0.2871207296848297, acc: 89.0625, f1: 80.12905831228811, r: 0.6338423431265944
06/02/2019 09:36:36 step: 5630, epoch: 170, batch: 19, loss: 0.3446913957595825, acc: 89.0625, f1: 77.02322308233637, r: 0.7041480954840984
06/02/2019 09:36:37 step: 5635, epoch: 170, batch: 24, loss: 0.32372480630874634, acc: 85.9375, f1: 71.92914438502675, r: 0.6956274165176122
06/02/2019 09:36:38 step: 5640, epoch: 170, batch: 29, loss: 0.25951775908470154, acc: 87.5, f1: 75.75222396650967, r: 0.6095802238288082
06/02/2019 09:36:38 *** evaluating ***
06/02/2019 09:36:39 step: 171, epoch: 170, acc: 55.98290598290598, f1: 27.60267412556145, r: 0.34689626604043877
06/02/2019 09:36:39 *** epoch: 172 ***
06/02/2019 09:36:39 *** training ***
06/02/2019 09:36:40 step: 5648, epoch: 171, batch: 4, loss: 0.23474273085594177, acc: 93.75, f1: 81.95084485407067, r: 0.6517041848369474
06/02/2019 09:36:41 step: 5653, epoch: 171, batch: 9, loss: 0.21600574254989624, acc: 92.1875, f1: 89.96527777777779, r: 0.7270240703234165
06/02/2019 09:36:42 step: 5658, epoch: 171, batch: 14, loss: 0.3266071081161499, acc: 84.375, f1: 57.16094250577009, r: 0.5718401976081169
06/02/2019 09:36:43 step: 5663, epoch: 171, batch: 19, loss: 0.4011590778827667, acc: 84.375, f1: 74.75877192982456, r: 0.7526160807817455
06/02/2019 09:36:44 step: 5668, epoch: 171, batch: 24, loss: 0.4242875874042511, acc: 84.375, f1: 67.46680402930403, r: 0.6318104120102059
06/02/2019 09:36:45 step: 5673, epoch: 171, batch: 29, loss: 0.32885751128196716, acc: 85.9375, f1: 63.96576249837119, r: 0.6391725679852669
06/02/2019 09:36:46 *** evaluating ***
06/02/2019 09:36:46 step: 172, epoch: 171, acc: 58.119658119658126, f1: 28.011665680192156, r: 0.3689320398831404
06/02/2019 09:36:46 *** epoch: 173 ***
06/02/2019 09:36:46 *** training ***
06/02/2019 09:36:47 step: 5681, epoch: 172, batch: 4, loss: 0.16320538520812988, acc: 96.875, f1: 96.98563218390805, r: 0.7482784626893711
06/02/2019 09:36:48 step: 5686, epoch: 172, batch: 9, loss: 0.19670386612415314, acc: 95.3125, f1: 90.54709207626713, r: 0.6423770685154304
06/02/2019 09:36:49 step: 5691, epoch: 172, batch: 14, loss: 0.2826663553714752, acc: 90.625, f1: 88.29107400535973, r: 0.638985560161298
06/02/2019 09:36:50 step: 5696, epoch: 172, batch: 19, loss: 0.23028011620044708, acc: 87.5, f1: 66.16326530612245, r: 0.7213864043146034
06/02/2019 09:36:51 step: 5701, epoch: 172, batch: 24, loss: 0.3168316185474396, acc: 87.5, f1: 77.94895736072208, r: 0.5911641936611861
06/02/2019 09:36:53 step: 5706, epoch: 172, batch: 29, loss: 0.2747197449207306, acc: 87.5, f1: 73.78335949764521, r: 0.5474450730040589
06/02/2019 09:36:53 *** evaluating ***
06/02/2019 09:36:54 step: 173, epoch: 172, acc: 56.837606837606835, f1: 27.032767808958447, r: 0.3555364582544622
06/02/2019 09:36:54 *** epoch: 174 ***
06/02/2019 09:36:54 *** training ***
06/02/2019 09:36:55 step: 5714, epoch: 173, batch: 4, loss: 0.2726191282272339, acc: 89.0625, f1: 85.99347496357463, r: 0.6465865427641554
06/02/2019 09:36:56 step: 5719, epoch: 173, batch: 9, loss: 0.28749823570251465, acc: 90.625, f1: 88.21552863569671, r: 0.6550854220641028
06/02/2019 09:36:57 step: 5724, epoch: 173, batch: 14, loss: 0.3490513563156128, acc: 87.5, f1: 75.9469696969697, r: 0.6814732080399633
06/02/2019 09:36:58 step: 5729, epoch: 173, batch: 19, loss: 0.22682206332683563, acc: 92.1875, f1: 77.93541000062739, r: 0.7106696159103487
06/02/2019 09:36:59 step: 5734, epoch: 173, batch: 24, loss: 0.2754347324371338, acc: 89.0625, f1: 74.64880952380952, r: 0.6181522239520671
06/02/2019 09:37:00 step: 5739, epoch: 173, batch: 29, loss: 0.3309178352355957, acc: 93.75, f1: 78.2470569263022, r: 0.6807528054524198
06/02/2019 09:37:01 *** evaluating ***
06/02/2019 09:37:01 step: 174, epoch: 173, acc: 57.692307692307686, f1: 28.943140245150545, r: 0.35751625525558933
06/02/2019 09:37:01 *** epoch: 175 ***
06/02/2019 09:37:01 *** training ***
06/02/2019 09:37:02 step: 5747, epoch: 174, batch: 4, loss: 0.2960367798805237, acc: 87.5, f1: 73.72685185185186, r: 0.7177662703499996
06/02/2019 09:37:03 step: 5752, epoch: 174, batch: 9, loss: 0.1826106607913971, acc: 95.3125, f1: 83.64447060875632, r: 0.624852560390542
06/02/2019 09:37:04 step: 5757, epoch: 174, batch: 14, loss: 0.14310422539710999, acc: 96.875, f1: 93.03573945434411, r: 0.6338129572083092
06/02/2019 09:37:05 step: 5762, epoch: 174, batch: 19, loss: 0.2529858350753784, acc: 90.625, f1: 73.17115251897862, r: 0.6712327487402066
06/02/2019 09:37:06 step: 5767, epoch: 174, batch: 24, loss: 0.20568819344043732, acc: 92.1875, f1: 79.11112700228833, r: 0.7039159097861156
06/02/2019 09:37:07 step: 5772, epoch: 174, batch: 29, loss: 0.202679842710495, acc: 93.75, f1: 63.96825396825396, r: 0.6193366644394065
06/02/2019 09:37:08 *** evaluating ***
06/02/2019 09:37:08 step: 175, epoch: 174, acc: 59.401709401709404, f1: 30.121638111412885, r: 0.37798453863381637
06/02/2019 09:37:08 *** epoch: 176 ***
06/02/2019 09:37:08 *** training ***
06/02/2019 09:37:09 step: 5780, epoch: 175, batch: 4, loss: 0.31026649475097656, acc: 84.375, f1: 70.83494140422175, r: 0.6616245134243264
06/02/2019 09:37:10 step: 5785, epoch: 175, batch: 9, loss: 0.13970130681991577, acc: 96.875, f1: 92.10884353741496, r: 0.7049780009671905
06/02/2019 09:37:11 step: 5790, epoch: 175, batch: 14, loss: 0.22990262508392334, acc: 90.625, f1: 77.79331908201877, r: 0.7286376296666818
06/02/2019 09:37:13 step: 5795, epoch: 175, batch: 19, loss: 0.20532721281051636, acc: 92.1875, f1: 82.01496793686027, r: 0.7535694536877547
06/02/2019 09:37:14 step: 5800, epoch: 175, batch: 24, loss: 0.18319082260131836, acc: 93.75, f1: 90.5173831336622, r: 0.6897669125212881
06/02/2019 09:37:15 step: 5805, epoch: 175, batch: 29, loss: 0.3180050253868103, acc: 93.75, f1: 84.4006734006734, r: 0.8014730564386194
06/02/2019 09:37:15 *** evaluating ***
06/02/2019 09:37:16 step: 176, epoch: 175, acc: 58.54700854700855, f1: 30.981458151850305, r: 0.37296027863541803
06/02/2019 09:37:16 *** epoch: 177 ***
06/02/2019 09:37:16 *** training ***
06/02/2019 09:37:17 step: 5813, epoch: 176, batch: 4, loss: 0.2993188798427582, acc: 84.375, f1: 66.99198273203824, r: 0.6864645131407483
06/02/2019 09:37:18 step: 5818, epoch: 176, batch: 9, loss: 0.2603112757205963, acc: 85.9375, f1: 74.62643584565609, r: 0.7195845081722537
06/02/2019 09:37:19 step: 5823, epoch: 176, batch: 14, loss: 0.20274294912815094, acc: 93.75, f1: 90.83484345112251, r: 0.7769665142794033
06/02/2019 09:37:20 step: 5828, epoch: 176, batch: 19, loss: 0.16062025725841522, acc: 93.75, f1: 95.28947243232957, r: 0.7015169716102408
06/02/2019 09:37:21 step: 5833, epoch: 176, batch: 24, loss: 0.2300035059452057, acc: 92.1875, f1: 87.11484593837534, r: 0.6369598297980233
06/02/2019 09:37:22 step: 5838, epoch: 176, batch: 29, loss: 0.15215103328227997, acc: 95.3125, f1: 94.09126984126985, r: 0.7098494868976404
06/02/2019 09:37:23 *** evaluating ***
06/02/2019 09:37:23 step: 177, epoch: 176, acc: 56.837606837606835, f1: 26.97196807436837, r: 0.3427931284877411
06/02/2019 09:37:23 *** epoch: 178 ***
06/02/2019 09:37:23 *** training ***
06/02/2019 09:37:24 step: 5846, epoch: 177, batch: 4, loss: 0.21447694301605225, acc: 92.1875, f1: 76.07537577365163, r: 0.6674648607746262
06/02/2019 09:37:25 step: 5851, epoch: 177, batch: 9, loss: 0.24632655084133148, acc: 89.0625, f1: 74.96064272703617, r: 0.794791266075373
06/02/2019 09:37:26 step: 5856, epoch: 177, batch: 14, loss: 0.16419237852096558, acc: 96.875, f1: 82.9020979020979, r: 0.7880101129439951
06/02/2019 09:37:27 step: 5861, epoch: 177, batch: 19, loss: 0.17816796898841858, acc: 93.75, f1: 88.19597069597069, r: 0.5440107167238167
06/02/2019 09:37:29 step: 5866, epoch: 177, batch: 24, loss: 0.19072876870632172, acc: 90.625, f1: 75.55318584923847, r: 0.7618176949177571
06/02/2019 09:37:30 step: 5871, epoch: 177, batch: 29, loss: 0.15419386327266693, acc: 95.3125, f1: 86.99649556792414, r: 0.5624988303064099
06/02/2019 09:37:30 *** evaluating ***
06/02/2019 09:37:31 step: 178, epoch: 177, acc: 54.700854700854705, f1: 27.932386670776765, r: 0.3530768014620783
06/02/2019 09:37:31 *** epoch: 179 ***
06/02/2019 09:37:31 *** training ***
06/02/2019 09:37:32 step: 5879, epoch: 178, batch: 4, loss: 0.10466580092906952, acc: 95.3125, f1: 81.04391284815813, r: 0.6769701337317325
06/02/2019 09:37:33 step: 5884, epoch: 178, batch: 9, loss: 0.3104681968688965, acc: 87.5, f1: 81.29335439118049, r: 0.7080565571638607
06/02/2019 09:37:34 step: 5889, epoch: 178, batch: 14, loss: 0.26839831471443176, acc: 93.75, f1: 90.08427815570673, r: 0.712158027563861
06/02/2019 09:37:35 step: 5894, epoch: 178, batch: 19, loss: 0.2365885078907013, acc: 89.0625, f1: 86.15904859967702, r: 0.632639170038256
06/02/2019 09:37:37 step: 5899, epoch: 178, batch: 24, loss: 0.20109029114246368, acc: 95.3125, f1: 79.26587301587301, r: 0.6959156099379019
06/02/2019 09:37:38 step: 5904, epoch: 178, batch: 29, loss: 0.21095338463783264, acc: 92.1875, f1: 74.47619047619048, r: 0.6700225155786365
06/02/2019 09:37:38 *** evaluating ***
06/02/2019 09:37:38 step: 179, epoch: 178, acc: 58.97435897435898, f1: 31.01645033710251, r: 0.3881063575578993
06/02/2019 09:37:38 *** epoch: 180 ***
06/02/2019 09:37:38 *** training ***
06/02/2019 09:37:39 step: 5912, epoch: 179, batch: 4, loss: 0.23175100982189178, acc: 92.1875, f1: 87.5868945868946, r: 0.6573773254254969
06/02/2019 09:37:40 step: 5917, epoch: 179, batch: 9, loss: 0.3133937418460846, acc: 85.9375, f1: 75.75, r: 0.773417174128346
06/02/2019 09:37:42 step: 5922, epoch: 179, batch: 14, loss: 0.11109966039657593, acc: 92.1875, f1: 84.97932506335869, r: 0.7123018388241384
06/02/2019 09:37:43 step: 5927, epoch: 179, batch: 19, loss: 0.26618561148643494, acc: 87.5, f1: 84.69136752971339, r: 0.6611222089985093
06/02/2019 09:37:44 step: 5932, epoch: 179, batch: 24, loss: 0.24173703789710999, acc: 92.1875, f1: 78.67628893662729, r: 0.7318863279236059
06/02/2019 09:37:45 step: 5937, epoch: 179, batch: 29, loss: 0.4874454140663147, acc: 84.375, f1: 60.22995283018868, r: 0.5819073403553526
06/02/2019 09:37:46 *** evaluating ***
06/02/2019 09:37:46 step: 180, epoch: 179, acc: 55.55555555555556, f1: 27.23048409440636, r: 0.3703048248141834
06/02/2019 09:37:46 *** epoch: 181 ***
06/02/2019 09:37:46 *** training ***
06/02/2019 09:37:47 step: 5945, epoch: 180, batch: 4, loss: 0.12733012437820435, acc: 95.3125, f1: 77.29500891265597, r: 0.6551752073514698
06/02/2019 09:37:48 step: 5950, epoch: 180, batch: 9, loss: 0.14537791907787323, acc: 93.75, f1: 93.89174800354925, r: 0.6461802176696938
06/02/2019 09:37:49 step: 5955, epoch: 180, batch: 14, loss: 0.21957695484161377, acc: 90.625, f1: 84.8923223135408, r: 0.6235079347518674
06/02/2019 09:37:50 step: 5960, epoch: 180, batch: 19, loss: 0.17370302975177765, acc: 93.75, f1: 88.08809082967684, r: 0.7328006453592762
06/02/2019 09:37:51 step: 5965, epoch: 180, batch: 24, loss: 0.23904897272586823, acc: 90.625, f1: 82.92970750128008, r: 0.7680270261747348
06/02/2019 09:37:52 step: 5970, epoch: 180, batch: 29, loss: 0.22250811755657196, acc: 89.0625, f1: 75.21530462291331, r: 0.7053407635178229
06/02/2019 09:37:53 *** evaluating ***
06/02/2019 09:37:53 step: 181, epoch: 180, acc: 55.55555555555556, f1: 26.453026828362404, r: 0.34443002852524096
06/02/2019 09:37:53 *** epoch: 182 ***
06/02/2019 09:37:53 *** training ***
06/02/2019 09:37:54 step: 5978, epoch: 181, batch: 4, loss: 0.22796815633773804, acc: 89.0625, f1: 85.72173794306913, r: 0.6441384125988722
06/02/2019 09:37:55 step: 5983, epoch: 181, batch: 9, loss: 0.21036550402641296, acc: 87.5, f1: 65.73824637742722, r: 0.6323318857776571
06/02/2019 09:37:56 step: 5988, epoch: 181, batch: 14, loss: 0.0886496901512146, acc: 98.4375, f1: 93.93939393939394, r: 0.5980341755486956
06/02/2019 09:37:57 step: 5993, epoch: 181, batch: 19, loss: 0.1969764679670334, acc: 90.625, f1: 67.50320275930032, r: 0.682330524368868
06/02/2019 09:37:58 step: 5998, epoch: 181, batch: 24, loss: 0.2001810222864151, acc: 93.75, f1: 90.12518805071996, r: 0.6292468344129716
06/02/2019 09:37:59 step: 6003, epoch: 181, batch: 29, loss: 0.12023309618234634, acc: 93.75, f1: 84.34584283640888, r: 0.6369336551309353
06/02/2019 09:38:00 *** evaluating ***
06/02/2019 09:38:00 step: 182, epoch: 181, acc: 58.119658119658126, f1: 26.93306335241819, r: 0.3659271405233117
06/02/2019 09:38:00 *** epoch: 183 ***
06/02/2019 09:38:00 *** training ***
06/02/2019 09:38:01 step: 6011, epoch: 182, batch: 4, loss: 0.1566614806652069, acc: 95.3125, f1: 92.90849673202615, r: 0.6001982016185369
06/02/2019 09:38:03 step: 6016, epoch: 182, batch: 9, loss: 0.12923982739448547, acc: 96.875, f1: 93.27067669172932, r: 0.7462267972136897
06/02/2019 09:38:04 step: 6021, epoch: 182, batch: 14, loss: 0.19280573725700378, acc: 95.3125, f1: 90.16400266400267, r: 0.7357371252156459
06/02/2019 09:38:05 step: 6026, epoch: 182, batch: 19, loss: 0.21074378490447998, acc: 92.1875, f1: 90.22222222222221, r: 0.6834056523957817
06/02/2019 09:38:06 step: 6031, epoch: 182, batch: 24, loss: 0.17639262974262238, acc: 90.625, f1: 87.64996907854051, r: 0.6498080368187438
06/02/2019 09:38:07 step: 6036, epoch: 182, batch: 29, loss: 0.1766720861196518, acc: 92.1875, f1: 87.85501700680271, r: 0.7026071422748469
06/02/2019 09:38:08 *** evaluating ***
06/02/2019 09:38:08 step: 183, epoch: 182, acc: 58.119658119658126, f1: 27.80901935485719, r: 0.3594361107687659
06/02/2019 09:38:08 *** epoch: 184 ***
06/02/2019 09:38:08 *** training ***
06/02/2019 09:38:09 step: 6044, epoch: 183, batch: 4, loss: 0.27804848551750183, acc: 87.5, f1: 71.7582008134737, r: 0.6941641023353025
06/02/2019 09:38:10 step: 6049, epoch: 183, batch: 9, loss: 0.20825646817684174, acc: 92.1875, f1: 90.08799471205485, r: 0.6815205712467334
06/02/2019 09:38:11 step: 6054, epoch: 183, batch: 14, loss: 0.08599115908145905, acc: 96.875, f1: 79.61943398048875, r: 0.6547048934067438
06/02/2019 09:38:12 step: 6059, epoch: 183, batch: 19, loss: 0.18962112069129944, acc: 93.75, f1: 90.8639784416182, r: 0.6751203889862981
06/02/2019 09:38:14 step: 6064, epoch: 183, batch: 24, loss: 0.26655957102775574, acc: 89.0625, f1: 64.30255629383537, r: 0.649984527743441
06/02/2019 09:38:15 step: 6069, epoch: 183, batch: 29, loss: 0.17908993363380432, acc: 95.3125, f1: 95.07858533905922, r: 0.7616309452953447
06/02/2019 09:38:15 *** evaluating ***
06/02/2019 09:38:16 step: 184, epoch: 183, acc: 57.26495726495726, f1: 28.72339833369245, r: 0.3562763583195397
06/02/2019 09:38:16 *** epoch: 185 ***
06/02/2019 09:38:16 *** training ***
06/02/2019 09:38:17 step: 6077, epoch: 184, batch: 4, loss: 0.15998467803001404, acc: 95.3125, f1: 87.07482993197279, r: 0.6607198812674856
06/02/2019 09:38:18 step: 6082, epoch: 184, batch: 9, loss: 0.22987963259220123, acc: 90.625, f1: 84.53027950310559, r: 0.6847352360102608
06/02/2019 09:38:19 step: 6087, epoch: 184, batch: 14, loss: 0.2100854218006134, acc: 92.1875, f1: 89.02127461910071, r: 0.7106295706481934
06/02/2019 09:38:20 step: 6092, epoch: 184, batch: 19, loss: 0.29859790205955505, acc: 89.0625, f1: 76.83004636129635, r: 0.6865665034068564
06/02/2019 09:38:21 step: 6097, epoch: 184, batch: 24, loss: 0.16955024003982544, acc: 90.625, f1: 79.04816017316017, r: 0.6969090531315261
06/02/2019 09:38:22 step: 6102, epoch: 184, batch: 29, loss: 0.29775309562683105, acc: 84.375, f1: 64.69907407407408, r: 0.6308760990932393
06/02/2019 09:38:22 *** evaluating ***
06/02/2019 09:38:23 step: 185, epoch: 184, acc: 52.991452991452995, f1: 26.548195367726407, r: 0.34749292677438093
06/02/2019 09:38:23 *** epoch: 186 ***
06/02/2019 09:38:23 *** training ***
06/02/2019 09:38:24 step: 6110, epoch: 185, batch: 4, loss: 0.3713817596435547, acc: 89.0625, f1: 71.96108761487051, r: 0.596840280930595
06/02/2019 09:38:25 step: 6115, epoch: 185, batch: 9, loss: 0.17936567962169647, acc: 89.0625, f1: 78.46979107848674, r: 0.6281513740152362
06/02/2019 09:38:26 step: 6120, epoch: 185, batch: 14, loss: 0.1519058495759964, acc: 96.875, f1: 94.65367965367966, r: 0.6477621542313116
06/02/2019 09:38:27 step: 6125, epoch: 185, batch: 19, loss: 0.19072556495666504, acc: 90.625, f1: 72.93010752688173, r: 0.6509312226846081
06/02/2019 09:38:28 step: 6130, epoch: 185, batch: 24, loss: 0.1405143141746521, acc: 95.3125, f1: 79.92777337604923, r: 0.7491420226991754
06/02/2019 09:38:29 step: 6135, epoch: 185, batch: 29, loss: 0.36218294501304626, acc: 87.5, f1: 83.44801193638403, r: 0.6498713154732304
06/02/2019 09:38:30 *** evaluating ***
06/02/2019 09:38:30 step: 186, epoch: 185, acc: 54.27350427350427, f1: 26.61383379742518, r: 0.3456882941917006
06/02/2019 09:38:30 *** epoch: 187 ***
06/02/2019 09:38:30 *** training ***
06/02/2019 09:38:31 step: 6143, epoch: 186, batch: 4, loss: 0.22657009959220886, acc: 92.1875, f1: 90.56965679248287, r: 0.7853213461447759
06/02/2019 09:38:32 step: 6148, epoch: 186, batch: 9, loss: 0.09246186167001724, acc: 98.4375, f1: 81.81818181818183, r: 0.60315869302889
06/02/2019 09:38:33 step: 6153, epoch: 186, batch: 14, loss: 0.2028702199459076, acc: 92.1875, f1: 88.52106227106228, r: 0.623402261203985
06/02/2019 09:38:34 step: 6158, epoch: 186, batch: 19, loss: 0.2600898742675781, acc: 92.1875, f1: 90.45454545454545, r: 0.7350465099881874
06/02/2019 09:38:36 step: 6163, epoch: 186, batch: 24, loss: 0.2712070643901825, acc: 87.5, f1: 77.09656084656085, r: 0.7118295167139606
06/02/2019 09:38:37 step: 6168, epoch: 186, batch: 29, loss: 0.07459082454442978, acc: 96.875, f1: 82.23602484472049, r: 0.6342611062964162
06/02/2019 09:38:37 *** evaluating ***
06/02/2019 09:38:38 step: 187, epoch: 186, acc: 57.692307692307686, f1: 27.613334985831855, r: 0.34631287994435334
06/02/2019 09:38:38 *** epoch: 188 ***
06/02/2019 09:38:38 *** training ***
06/02/2019 09:38:39 step: 6176, epoch: 187, batch: 4, loss: 0.3317102789878845, acc: 92.1875, f1: 90.56763285024154, r: 0.7785121395838107
06/02/2019 09:38:40 step: 6181, epoch: 187, batch: 9, loss: 0.22752542793750763, acc: 90.625, f1: 83.39549466551756, r: 0.6118961268467035
06/02/2019 09:38:41 step: 6186, epoch: 187, batch: 14, loss: 0.26515835523605347, acc: 93.75, f1: 83.65134860604742, r: 0.6897110037804113
06/02/2019 09:38:42 step: 6191, epoch: 187, batch: 19, loss: 0.25299400091171265, acc: 85.9375, f1: 67.24012099012099, r: 0.6691983129122158
06/02/2019 09:38:43 step: 6196, epoch: 187, batch: 24, loss: 0.1348756104707718, acc: 95.3125, f1: 90.55555555555554, r: 0.7384319627398839
06/02/2019 09:38:44 step: 6201, epoch: 187, batch: 29, loss: 0.11077971756458282, acc: 96.875, f1: 98.10989010989012, r: 0.5345612728942832
06/02/2019 09:38:45 *** evaluating ***
06/02/2019 09:38:45 step: 188, epoch: 187, acc: 55.12820512820513, f1: 27.524258306640764, r: 0.3632735278911478
06/02/2019 09:38:45 *** epoch: 189 ***
06/02/2019 09:38:45 *** training ***
06/02/2019 09:38:46 step: 6209, epoch: 188, batch: 4, loss: 0.2477867305278778, acc: 87.5, f1: 74.5806458034719, r: 0.7843528536621427
06/02/2019 09:38:47 step: 6214, epoch: 188, batch: 9, loss: 0.3120526969432831, acc: 89.0625, f1: 78.51112756930729, r: 0.7280039643601657
06/02/2019 09:38:48 step: 6219, epoch: 188, batch: 14, loss: 0.3347501754760742, acc: 92.1875, f1: 80.56559036658142, r: 0.6755858541193199
06/02/2019 09:38:49 step: 6224, epoch: 188, batch: 19, loss: 0.16245876252651215, acc: 93.75, f1: 84.80746012660907, r: 0.6698386067251394
06/02/2019 09:38:51 step: 6229, epoch: 188, batch: 24, loss: 0.2684725821018219, acc: 85.9375, f1: 66.09778319930122, r: 0.5825265472930593
06/02/2019 09:38:52 step: 6234, epoch: 188, batch: 29, loss: 0.11956901848316193, acc: 93.75, f1: 89.90889285006932, r: 0.7004836489721866
06/02/2019 09:38:52 *** evaluating ***
06/02/2019 09:38:53 step: 189, epoch: 188, acc: 56.41025641025641, f1: 27.35394109125423, r: 0.3488438637455303
06/02/2019 09:38:53 *** epoch: 190 ***
06/02/2019 09:38:53 *** training ***
06/02/2019 09:38:54 step: 6242, epoch: 189, batch: 4, loss: 0.2159489393234253, acc: 92.1875, f1: 76.36159451046971, r: 0.6287953675048995
06/02/2019 09:38:55 step: 6247, epoch: 189, batch: 9, loss: 0.16749876737594604, acc: 95.3125, f1: 95.83905677655677, r: 0.7584032674530875
06/02/2019 09:38:56 step: 6252, epoch: 189, batch: 14, loss: 0.12198502570390701, acc: 98.4375, f1: 95.37037037037037, r: 0.7664110047212318
06/02/2019 09:38:57 step: 6257, epoch: 189, batch: 19, loss: 0.17347188293933868, acc: 90.625, f1: 81.1738198811267, r: 0.6554216032249798
06/02/2019 09:38:58 step: 6262, epoch: 189, batch: 24, loss: 0.21522639691829681, acc: 93.75, f1: 79.81770833333333, r: 0.65629346601633
06/02/2019 09:38:59 step: 6267, epoch: 189, batch: 29, loss: 0.17205026745796204, acc: 92.1875, f1: 71.80470596726754, r: 0.6504457577665268
06/02/2019 09:39:00 *** evaluating ***
06/02/2019 09:39:00 step: 190, epoch: 189, acc: 58.54700854700855, f1: 27.408727308345306, r: 0.33679358636872203
06/02/2019 09:39:00 *** epoch: 191 ***
06/02/2019 09:39:00 *** training ***
06/02/2019 09:39:01 step: 6275, epoch: 190, batch: 4, loss: 0.21290810406208038, acc: 89.0625, f1: 81.46390013495277, r: 0.7861557406276443
06/02/2019 09:39:02 step: 6280, epoch: 190, batch: 9, loss: 0.16723474860191345, acc: 93.75, f1: 94.68091998704243, r: 0.6652158216501478
06/02/2019 09:39:03 step: 6285, epoch: 190, batch: 14, loss: 0.1799686998128891, acc: 90.625, f1: 72.41422508180374, r: 0.5923972671566413
06/02/2019 09:39:04 step: 6290, epoch: 190, batch: 19, loss: 0.39387500286102295, acc: 81.25, f1: 63.782467532467535, r: 0.5668068006955144
06/02/2019 09:39:05 step: 6295, epoch: 190, batch: 24, loss: 0.1581735759973526, acc: 93.75, f1: 88.59756228177281, r: 0.6793784254751284
06/02/2019 09:39:07 step: 6300, epoch: 190, batch: 29, loss: 0.20048028230667114, acc: 93.75, f1: 90.89714049391469, r: 0.7344322617231247
06/02/2019 09:39:07 *** evaluating ***
06/02/2019 09:39:07 step: 191, epoch: 190, acc: 56.837606837606835, f1: 26.600940663355722, r: 0.3722930643010469
06/02/2019 09:39:07 *** epoch: 192 ***
06/02/2019 09:39:07 *** training ***
06/02/2019 09:39:09 step: 6308, epoch: 191, batch: 4, loss: 0.17695792019367218, acc: 92.1875, f1: 89.31197478991596, r: 0.8138486235627671
06/02/2019 09:39:10 step: 6313, epoch: 191, batch: 9, loss: 0.17767786979675293, acc: 93.75, f1: 91.35999337638683, r: 0.6913322645984769
06/02/2019 09:39:11 step: 6318, epoch: 191, batch: 14, loss: 0.16987861692905426, acc: 93.75, f1: 81.97044750914719, r: 0.6778452221671631
06/02/2019 09:39:12 step: 6323, epoch: 191, batch: 19, loss: 0.4009919762611389, acc: 87.5, f1: 62.365319865319854, r: 0.5754312264578436
06/02/2019 09:39:13 step: 6328, epoch: 191, batch: 24, loss: 0.22247405350208282, acc: 90.625, f1: 74.13031462585033, r: 0.6648857697244146
06/02/2019 09:39:14 step: 6333, epoch: 191, batch: 29, loss: 0.14768274128437042, acc: 95.3125, f1: 89.9896480331263, r: 0.6827978756412099
06/02/2019 09:39:15 *** evaluating ***
06/02/2019 09:39:15 step: 192, epoch: 191, acc: 57.692307692307686, f1: 27.52912912425086, r: 0.3648759590450368
06/02/2019 09:39:15 *** epoch: 193 ***
06/02/2019 09:39:15 *** training ***
06/02/2019 09:39:16 step: 6341, epoch: 192, batch: 4, loss: 0.1778411567211151, acc: 95.3125, f1: 83.75021017234133, r: 0.7706900917965623
06/02/2019 09:39:17 step: 6346, epoch: 192, batch: 9, loss: 0.20692653954029083, acc: 92.1875, f1: 75.4393424036281, r: 0.6685907779862231
06/02/2019 09:39:18 step: 6351, epoch: 192, batch: 14, loss: 0.06320267170667648, acc: 100.0, f1: 100.0, r: 0.6794568027692868
06/02/2019 09:39:19 step: 6356, epoch: 192, batch: 19, loss: 0.26283615827560425, acc: 90.625, f1: 86.59448394742513, r: 0.7484451957348691
06/02/2019 09:39:20 step: 6361, epoch: 192, batch: 24, loss: 0.20355381071567535, acc: 92.1875, f1: 73.45004095004096, r: 0.5959480714658444
06/02/2019 09:39:21 step: 6366, epoch: 192, batch: 29, loss: 0.14846564829349518, acc: 95.3125, f1: 92.53301863415602, r: 0.7173456441811205
06/02/2019 09:39:22 *** evaluating ***
06/02/2019 09:39:22 step: 193, epoch: 192, acc: 54.27350427350427, f1: 27.37940162729186, r: 0.35468269542419706
06/02/2019 09:39:22 *** epoch: 194 ***
06/02/2019 09:39:22 *** training ***
06/02/2019 09:39:24 step: 6374, epoch: 193, batch: 4, loss: 0.1263238787651062, acc: 96.875, f1: 83.09227614490773, r: 0.6430895523749592
06/02/2019 09:39:25 step: 6379, epoch: 193, batch: 9, loss: 0.20785042643547058, acc: 95.3125, f1: 88.25, r: 0.7268132729100326
06/02/2019 09:39:26 step: 6384, epoch: 193, batch: 14, loss: 0.32935968041419983, acc: 85.9375, f1: 71.25564791133844, r: 0.7055387444721578
06/02/2019 09:39:27 step: 6389, epoch: 193, batch: 19, loss: 0.297023206949234, acc: 85.9375, f1: 75.58760683760684, r: 0.7134232912209582
06/02/2019 09:39:28 step: 6394, epoch: 193, batch: 24, loss: 0.180646613240242, acc: 92.1875, f1: 86.36054421768708, r: 0.6354529952959008
06/02/2019 09:39:29 step: 6399, epoch: 193, batch: 29, loss: 0.26598459482192993, acc: 87.5, f1: 71.00976874003189, r: 0.779359012391918
06/02/2019 09:39:30 *** evaluating ***
06/02/2019 09:39:30 step: 194, epoch: 193, acc: 58.119658119658126, f1: 28.227144070870803, r: 0.3425692872796905
06/02/2019 09:39:30 *** epoch: 195 ***
06/02/2019 09:39:30 *** training ***
06/02/2019 09:39:31 step: 6407, epoch: 194, batch: 4, loss: 0.136956125497818, acc: 96.875, f1: 84.57152166829587, r: 0.6859732686048671
06/02/2019 09:39:32 step: 6412, epoch: 194, batch: 9, loss: 0.3422906696796417, acc: 87.5, f1: 67.4412393162393, r: 0.6515640844979843
06/02/2019 09:39:33 step: 6417, epoch: 194, batch: 14, loss: 0.14711251854896545, acc: 92.1875, f1: 89.08662900188324, r: 0.7600022221443374
06/02/2019 09:39:34 step: 6422, epoch: 194, batch: 19, loss: 0.07458516955375671, acc: 98.4375, f1: 99.11268855368235, r: 0.6664180477116093
06/02/2019 09:39:35 step: 6427, epoch: 194, batch: 24, loss: 0.17305858433246613, acc: 93.75, f1: 80.08928571428571, r: 0.7953991779712527
06/02/2019 09:39:36 step: 6432, epoch: 194, batch: 29, loss: 0.19711890816688538, acc: 87.5, f1: 79.86733823964369, r: 0.7124742590897801
06/02/2019 09:39:37 *** evaluating ***
06/02/2019 09:39:37 step: 195, epoch: 194, acc: 56.837606837606835, f1: 27.43028544479349, r: 0.34902882126383356
06/02/2019 09:39:37 *** epoch: 196 ***
06/02/2019 09:39:37 *** training ***
06/02/2019 09:39:38 step: 6440, epoch: 195, batch: 4, loss: 0.22034305334091187, acc: 92.1875, f1: 91.79346533383801, r: 0.6194732411100758
06/02/2019 09:39:39 step: 6445, epoch: 195, batch: 9, loss: 0.2299467921257019, acc: 90.625, f1: 82.76862026862027, r: 0.7363741250396021
06/02/2019 09:39:40 step: 6450, epoch: 195, batch: 14, loss: 0.2249624878168106, acc: 89.0625, f1: 74.66391318432134, r: 0.6269836347452941
06/02/2019 09:39:41 step: 6455, epoch: 195, batch: 19, loss: 0.128487229347229, acc: 95.3125, f1: 93.61201298701299, r: 0.7434876201481928
06/02/2019 09:39:42 step: 6460, epoch: 195, batch: 24, loss: 0.16281335055828094, acc: 93.75, f1: 80.4861111111111, r: 0.6696602725752575
06/02/2019 09:39:44 step: 6465, epoch: 195, batch: 29, loss: 0.10838428884744644, acc: 95.3125, f1: 89.20143495994888, r: 0.6646630035795529
06/02/2019 09:39:44 *** evaluating ***
06/02/2019 09:39:44 step: 196, epoch: 195, acc: 55.55555555555556, f1: 28.087372837842175, r: 0.3457994918711506
06/02/2019 09:39:44 *** epoch: 197 ***
06/02/2019 09:39:44 *** training ***
06/02/2019 09:39:45 step: 6473, epoch: 196, batch: 4, loss: 0.1855163872241974, acc: 92.1875, f1: 87.16864653797502, r: 0.680617995396803
06/02/2019 09:39:46 step: 6478, epoch: 196, batch: 9, loss: 0.29053163528442383, acc: 93.75, f1: 87.84655823771956, r: 0.7553596801860564
06/02/2019 09:39:48 step: 6483, epoch: 196, batch: 14, loss: 0.21751165390014648, acc: 90.625, f1: 85.08547008547009, r: 0.745790240519643
06/02/2019 09:39:49 step: 6488, epoch: 196, batch: 19, loss: 0.09702127426862717, acc: 98.4375, f1: 96.33699633699634, r: 0.6275971406096936
06/02/2019 09:39:50 step: 6493, epoch: 196, batch: 24, loss: 0.2599605917930603, acc: 87.5, f1: 80.54421768707483, r: 0.6739644855591607
06/02/2019 09:39:51 step: 6498, epoch: 196, batch: 29, loss: 0.368122935295105, acc: 85.9375, f1: 69.80510752688171, r: 0.6084875051341372
06/02/2019 09:39:51 *** evaluating ***
06/02/2019 09:39:52 step: 197, epoch: 196, acc: 57.692307692307686, f1: 29.316521750732278, r: 0.3698397689685507
06/02/2019 09:39:52 *** epoch: 198 ***
06/02/2019 09:39:52 *** training ***
06/02/2019 09:39:53 step: 6506, epoch: 197, batch: 4, loss: 0.12785016000270844, acc: 93.75, f1: 87.59218975107575, r: 0.7958916209077147
06/02/2019 09:39:54 step: 6511, epoch: 197, batch: 9, loss: 0.16257667541503906, acc: 95.3125, f1: 84.11485208118334, r: 0.7513493772839898
06/02/2019 09:39:55 step: 6516, epoch: 197, batch: 14, loss: 0.18920868635177612, acc: 93.75, f1: 87.00757575757576, r: 0.7708244464991857
06/02/2019 09:39:56 step: 6521, epoch: 197, batch: 19, loss: 0.10869527608156204, acc: 96.875, f1: 84.21875, r: 0.7293291342049016
06/02/2019 09:39:57 step: 6526, epoch: 197, batch: 24, loss: 0.1364019811153412, acc: 96.875, f1: 95.22727272727273, r: 0.7224323146528515
06/02/2019 09:39:58 step: 6531, epoch: 197, batch: 29, loss: 0.19057577848434448, acc: 92.1875, f1: 77.65257299091884, r: 0.6224699645125252
06/02/2019 09:39:59 *** evaluating ***
06/02/2019 09:39:59 step: 198, epoch: 197, acc: 56.41025641025641, f1: 27.672281323016502, r: 0.35616212836119654
06/02/2019 09:39:59 *** epoch: 199 ***
06/02/2019 09:39:59 *** training ***
06/02/2019 09:40:00 step: 6539, epoch: 198, batch: 4, loss: 0.1448104828596115, acc: 92.1875, f1: 84.645013443043, r: 0.7137024443421056
06/02/2019 09:40:01 step: 6544, epoch: 198, batch: 9, loss: 0.151957705616951, acc: 96.875, f1: 95.06944444444444, r: 0.7488645768863093
06/02/2019 09:40:02 step: 6549, epoch: 198, batch: 14, loss: 0.18889231979846954, acc: 95.3125, f1: 93.81359381359383, r: 0.7942374084579754
06/02/2019 09:40:04 step: 6554, epoch: 198, batch: 19, loss: 0.18085719645023346, acc: 90.625, f1: 80.41833166833166, r: 0.6695646808693497
06/02/2019 09:40:05 step: 6559, epoch: 198, batch: 24, loss: 0.20902195572853088, acc: 93.75, f1: 83.07046024151288, r: 0.6624758668364358
06/02/2019 09:40:06 step: 6564, epoch: 198, batch: 29, loss: 0.21845698356628418, acc: 90.625, f1: 76.41219891219892, r: 0.7509767262905448
06/02/2019 09:40:06 *** evaluating ***
06/02/2019 09:40:07 step: 199, epoch: 198, acc: 56.41025641025641, f1: 28.733857158859955, r: 0.35563742893402306
06/02/2019 09:40:07 *** epoch: 200 ***
06/02/2019 09:40:07 *** training ***
06/02/2019 09:40:08 step: 6572, epoch: 199, batch: 4, loss: 0.26427900791168213, acc: 95.3125, f1: 94.50515690147026, r: 0.6713731415369756
06/02/2019 09:40:09 step: 6577, epoch: 199, batch: 9, loss: 0.1880354881286621, acc: 93.75, f1: 88.47647501004519, r: 0.6991181485026644
06/02/2019 09:40:10 step: 6582, epoch: 199, batch: 14, loss: 0.2047368586063385, acc: 92.1875, f1: 76.15362811791384, r: 0.7415736050925821
06/02/2019 09:40:11 step: 6587, epoch: 199, batch: 19, loss: 0.20609025657176971, acc: 92.1875, f1: 77.9563492063492, r: 0.6346000502904362
06/02/2019 09:40:12 step: 6592, epoch: 199, batch: 24, loss: 0.15475629270076752, acc: 96.875, f1: 93.73543123543124, r: 0.778958907145059
06/02/2019 09:40:13 step: 6597, epoch: 199, batch: 29, loss: 0.15755745768547058, acc: 93.75, f1: 79.76689976689977, r: 0.6503988714230484
06/02/2019 09:40:14 *** evaluating ***
06/02/2019 09:40:14 step: 200, epoch: 199, acc: 58.119658119658126, f1: 30.03567795129137, r: 0.35716589849773944
06/02/2019 09:40:14 *** epoch: 201 ***
06/02/2019 09:40:14 *** training ***
06/02/2019 09:40:15 step: 6605, epoch: 200, batch: 4, loss: 0.13881826400756836, acc: 95.3125, f1: 90.7521645021645, r: 0.7669214821194106
06/02/2019 09:40:16 step: 6610, epoch: 200, batch: 9, loss: 0.115982785820961, acc: 98.4375, f1: 93.33333333333333, r: 0.7010755559144536
06/02/2019 09:40:17 step: 6615, epoch: 200, batch: 14, loss: 0.13422051072120667, acc: 95.3125, f1: 84.40973811339944, r: 0.6935594113431103
06/02/2019 09:40:18 step: 6620, epoch: 200, batch: 19, loss: 0.13163794577121735, acc: 95.3125, f1: 92.05456095481671, r: 0.7334333845734661
06/02/2019 09:40:19 step: 6625, epoch: 200, batch: 24, loss: 0.13320478796958923, acc: 93.75, f1: 92.6006343268902, r: 0.687403089089251
06/02/2019 09:40:20 step: 6630, epoch: 200, batch: 29, loss: 0.27092865109443665, acc: 90.625, f1: 86.27907337584757, r: 0.6640404325208649
06/02/2019 09:40:21 *** evaluating ***
06/02/2019 09:40:21 step: 201, epoch: 200, acc: 55.55555555555556, f1: 26.484029823269516, r: 0.3494483037375854
06/02/2019 09:40:21 *** epoch: 202 ***
06/02/2019 09:40:21 *** training ***
06/02/2019 09:40:23 step: 6638, epoch: 201, batch: 4, loss: 0.19217121601104736, acc: 93.75, f1: 82.68453768453769, r: 0.8264379295857353
06/02/2019 09:40:24 step: 6643, epoch: 201, batch: 9, loss: 0.12220624089241028, acc: 95.3125, f1: 81.92485754985755, r: 0.7476652331264174
06/02/2019 09:40:25 step: 6648, epoch: 201, batch: 14, loss: 0.14540070295333862, acc: 95.3125, f1: 91.10295927156392, r: 0.6508278838063237
06/02/2019 09:40:26 step: 6653, epoch: 201, batch: 19, loss: 0.12951163947582245, acc: 92.1875, f1: 76.70818764568766, r: 0.7271808389326608
06/02/2019 09:40:27 step: 6658, epoch: 201, batch: 24, loss: 0.33403411507606506, acc: 87.5, f1: 70.70652173913044, r: 0.6566239815045684
06/02/2019 09:40:28 step: 6663, epoch: 201, batch: 29, loss: 0.13887453079223633, acc: 93.75, f1: 90.67659098271344, r: 0.6067306766337566
06/02/2019 09:40:29 *** evaluating ***
06/02/2019 09:40:29 step: 202, epoch: 201, acc: 56.41025641025641, f1: 29.458263820190805, r: 0.3592239999146491
06/02/2019 09:40:29 *** epoch: 203 ***
06/02/2019 09:40:29 *** training ***
06/02/2019 09:40:30 step: 6671, epoch: 202, batch: 4, loss: 0.3250623941421509, acc: 87.5, f1: 70.60533277638541, r: 0.7010517532022797
06/02/2019 09:40:31 step: 6676, epoch: 202, batch: 9, loss: 0.11162234842777252, acc: 96.875, f1: 88.90269151138718, r: 0.6516478846965061
06/02/2019 09:40:32 step: 6681, epoch: 202, batch: 14, loss: 0.20306779444217682, acc: 90.625, f1: 69.61598746081505, r: 0.5390846915006162
06/02/2019 09:40:33 step: 6686, epoch: 202, batch: 19, loss: 0.10398096591234207, acc: 98.4375, f1: 95.0, r: 0.7924781451851239
06/02/2019 09:40:34 step: 6691, epoch: 202, batch: 24, loss: 0.22908812761306763, acc: 92.1875, f1: 75.76280685737473, r: 0.6456853981341738
06/02/2019 09:40:35 step: 6696, epoch: 202, batch: 29, loss: 0.22873449325561523, acc: 90.625, f1: 72.96176912769225, r: 0.5973108504617418
06/02/2019 09:40:36 *** evaluating ***
06/02/2019 09:40:36 step: 203, epoch: 202, acc: 56.837606837606835, f1: 27.930320704495237, r: 0.363089534926503
06/02/2019 09:40:36 *** epoch: 204 ***
06/02/2019 09:40:36 *** training ***
06/02/2019 09:40:37 step: 6704, epoch: 203, batch: 4, loss: 0.11669538915157318, acc: 96.875, f1: 91.70068027210885, r: 0.6074373986481131
06/02/2019 09:40:38 step: 6709, epoch: 203, batch: 9, loss: 0.1824653148651123, acc: 93.75, f1: 87.66387195121952, r: 0.6979821632065498
06/02/2019 09:40:39 step: 6714, epoch: 203, batch: 14, loss: 0.2523631155490875, acc: 90.625, f1: 86.77845528455285, r: 0.726714972285462
06/02/2019 09:40:40 step: 6719, epoch: 203, batch: 19, loss: 0.17112386226654053, acc: 93.75, f1: 87.4908424908425, r: 0.7877499925921032
06/02/2019 09:40:41 step: 6724, epoch: 203, batch: 24, loss: 0.20344138145446777, acc: 90.625, f1: 74.29900181488203, r: 0.7464795072157265
06/02/2019 09:40:42 step: 6729, epoch: 203, batch: 29, loss: 0.19921298325061798, acc: 92.1875, f1: 70.7473544973545, r: 0.5511365861789159
06/02/2019 09:40:43 *** evaluating ***
06/02/2019 09:40:43 step: 204, epoch: 203, acc: 56.41025641025641, f1: 27.958423140385158, r: 0.3638974324189593
06/02/2019 09:40:43 *** epoch: 205 ***
06/02/2019 09:40:43 *** training ***
06/02/2019 09:40:44 step: 6737, epoch: 204, batch: 4, loss: 0.21655163168907166, acc: 87.5, f1: 74.484126984127, r: 0.6220942753972997
06/02/2019 09:40:45 step: 6742, epoch: 204, batch: 9, loss: 0.22821290791034698, acc: 95.3125, f1: 93.76102292768958, r: 0.5758021031119736
06/02/2019 09:40:46 step: 6747, epoch: 204, batch: 14, loss: 0.13303595781326294, acc: 93.75, f1: 90.0604686318972, r: 0.6676417105403862
06/02/2019 09:40:47 step: 6752, epoch: 204, batch: 19, loss: 0.11962702870368958, acc: 96.875, f1: 89.54248366013073, r: 0.7968971586978153
06/02/2019 09:40:49 step: 6757, epoch: 204, batch: 24, loss: 0.1997343748807907, acc: 92.1875, f1: 90.21284271284271, r: 0.7717477215079451
06/02/2019 09:40:50 step: 6762, epoch: 204, batch: 29, loss: 0.18533265590667725, acc: 92.1875, f1: 75.69144518272427, r: 0.6988629916962237
06/02/2019 09:40:50 *** evaluating ***
06/02/2019 09:40:51 step: 205, epoch: 204, acc: 53.41880341880342, f1: 28.425325759765297, r: 0.33758009552091445
06/02/2019 09:40:51 *** epoch: 206 ***
06/02/2019 09:40:51 *** training ***
06/02/2019 09:40:52 step: 6770, epoch: 205, batch: 4, loss: 0.13228663802146912, acc: 96.875, f1: 82.9235880398671, r: 0.798361969947066
06/02/2019 09:40:53 step: 6775, epoch: 205, batch: 9, loss: 0.17715699970722198, acc: 92.1875, f1: 84.62585034013605, r: 0.6992312613426345
06/02/2019 09:40:54 step: 6780, epoch: 205, batch: 14, loss: 0.17314434051513672, acc: 93.75, f1: 92.5457323904529, r: 0.7704241430016633
06/02/2019 09:40:55 step: 6785, epoch: 205, batch: 19, loss: 0.20561185479164124, acc: 92.1875, f1: 78.99626204997048, r: 0.616157957409278
06/02/2019 09:40:56 step: 6790, epoch: 205, batch: 24, loss: 0.20412912964820862, acc: 92.1875, f1: 80.10500828342042, r: 0.7773944487211838
06/02/2019 09:40:57 step: 6795, epoch: 205, batch: 29, loss: 0.1309354156255722, acc: 95.3125, f1: 80.299543946932, r: 0.7296538243450192
06/02/2019 09:40:58 *** evaluating ***
06/02/2019 09:40:58 step: 206, epoch: 205, acc: 54.27350427350427, f1: 27.082188722952605, r: 0.3494595127104907
06/02/2019 09:40:58 *** epoch: 207 ***
06/02/2019 09:40:58 *** training ***
06/02/2019 09:40:59 step: 6803, epoch: 206, batch: 4, loss: 0.13036024570465088, acc: 95.3125, f1: 94.35071154898742, r: 0.6918138275832426
06/02/2019 09:41:00 step: 6808, epoch: 206, batch: 9, loss: 0.19883663952350616, acc: 93.75, f1: 89.28956465902279, r: 0.6036586372986104
06/02/2019 09:41:01 step: 6813, epoch: 206, batch: 14, loss: 0.27170422673225403, acc: 87.5, f1: 62.39815541601256, r: 0.6985503725202102
06/02/2019 09:41:03 step: 6818, epoch: 206, batch: 19, loss: 0.1674092561006546, acc: 95.3125, f1: 81.96428571428571, r: 0.7526022325851184
06/02/2019 09:41:04 step: 6823, epoch: 206, batch: 24, loss: 0.12555935978889465, acc: 95.3125, f1: 89.74089635854341, r: 0.8011676101975937
06/02/2019 09:41:05 step: 6828, epoch: 206, batch: 29, loss: 0.1765381544828415, acc: 90.625, f1: 91.69858523119393, r: 0.7627413487622017
06/02/2019 09:41:05 *** evaluating ***
06/02/2019 09:41:06 step: 207, epoch: 206, acc: 55.55555555555556, f1: 28.259613121130165, r: 0.351201976323052
06/02/2019 09:41:06 *** epoch: 208 ***
06/02/2019 09:41:06 *** training ***
06/02/2019 09:41:07 step: 6836, epoch: 207, batch: 4, loss: 0.1630527377128601, acc: 93.75, f1: 79.50692137320044, r: 0.788876552037103
06/02/2019 09:41:08 step: 6841, epoch: 207, batch: 9, loss: 0.10487763583660126, acc: 96.875, f1: 85.23908523908523, r: 0.7739764429585478
06/02/2019 09:41:09 step: 6846, epoch: 207, batch: 14, loss: 0.1361536979675293, acc: 95.3125, f1: 86.42857142857142, r: 0.751123273461097
06/02/2019 09:41:10 step: 6851, epoch: 207, batch: 19, loss: 0.14799286425113678, acc: 93.75, f1: 79.44768772893774, r: 0.7816373341074064
06/02/2019 09:41:11 step: 6856, epoch: 207, batch: 24, loss: 0.09897013008594513, acc: 96.875, f1: 94.58056995002808, r: 0.7118259290965226
06/02/2019 09:41:12 step: 6861, epoch: 207, batch: 29, loss: 0.15779858827590942, acc: 95.3125, f1: 92.15873015873015, r: 0.7425867299631892
06/02/2019 09:41:13 *** evaluating ***
06/02/2019 09:41:13 step: 208, epoch: 207, acc: 55.12820512820513, f1: 27.389642811448635, r: 0.35759775822307927
06/02/2019 09:41:13 *** epoch: 209 ***
06/02/2019 09:41:13 *** training ***
06/02/2019 09:41:14 step: 6869, epoch: 208, batch: 4, loss: 0.226133793592453, acc: 93.75, f1: 89.48107448107449, r: 0.7280210786168203
06/02/2019 09:41:16 step: 6874, epoch: 208, batch: 9, loss: 0.23353512585163116, acc: 90.625, f1: 73.87397456931912, r: 0.6657308049406526
06/02/2019 09:41:17 step: 6879, epoch: 208, batch: 14, loss: 0.18165907263755798, acc: 93.75, f1: 87.7237274220033, r: 0.7294526310389091
06/02/2019 09:41:18 step: 6884, epoch: 208, batch: 19, loss: 0.11094340682029724, acc: 96.875, f1: 94.50980392156863, r: 0.7895889299226129
06/02/2019 09:41:19 step: 6889, epoch: 208, batch: 24, loss: 0.09877792745828629, acc: 93.75, f1: 78.64202284687461, r: 0.6867069481310383
06/02/2019 09:41:20 step: 6894, epoch: 208, batch: 29, loss: 0.30015066266059875, acc: 92.1875, f1: 80.04273504273505, r: 0.6792856801778151
06/02/2019 09:41:21 *** evaluating ***
06/02/2019 09:41:21 step: 209, epoch: 208, acc: 58.119658119658126, f1: 28.184254471019177, r: 0.3553303738395545
06/02/2019 09:41:21 *** epoch: 210 ***
06/02/2019 09:41:21 *** training ***
06/02/2019 09:41:22 step: 6902, epoch: 209, batch: 4, loss: 0.08281651884317398, acc: 98.4375, f1: 95.10204081632654, r: 0.7452607906397883
06/02/2019 09:41:23 step: 6907, epoch: 209, batch: 9, loss: 0.1132243201136589, acc: 95.3125, f1: 93.84523809523809, r: 0.7281576094375108
06/02/2019 09:41:24 step: 6912, epoch: 209, batch: 14, loss: 0.07295933365821838, acc: 96.875, f1: 98.4860248447205, r: 0.7083005295187618
06/02/2019 09:41:25 step: 6917, epoch: 209, batch: 19, loss: 0.25361740589141846, acc: 93.75, f1: 88.62794612794613, r: 0.7890936264932448
06/02/2019 09:41:26 step: 6922, epoch: 209, batch: 24, loss: 0.3636947274208069, acc: 93.75, f1: 80.09276437847866, r: 0.6135854937705169
06/02/2019 09:41:27 step: 6927, epoch: 209, batch: 29, loss: 0.15423868596553802, acc: 92.1875, f1: 78.52474323062559, r: 0.6917649214031295
06/02/2019 09:41:28 *** evaluating ***
06/02/2019 09:41:28 step: 210, epoch: 209, acc: 54.27350427350427, f1: 26.13929263565891, r: 0.33876070854610685
06/02/2019 09:41:28 *** epoch: 211 ***
06/02/2019 09:41:28 *** training ***
06/02/2019 09:41:30 step: 6935, epoch: 210, batch: 4, loss: 0.08244023472070694, acc: 98.4375, f1: 95.17543859649122, r: 0.7748708892488195
06/02/2019 09:41:31 step: 6940, epoch: 210, batch: 9, loss: 0.13161970674991608, acc: 96.875, f1: 95.5388301892861, r: 0.6629773699502206
06/02/2019 09:41:32 step: 6945, epoch: 210, batch: 14, loss: 0.1589246541261673, acc: 92.1875, f1: 85.7954971037651, r: 0.7142166613572426
06/02/2019 09:41:33 step: 6950, epoch: 210, batch: 19, loss: 0.23462440073490143, acc: 90.625, f1: 68.87896825396825, r: 0.6388037626894423
06/02/2019 09:41:34 step: 6955, epoch: 210, batch: 24, loss: 0.16040802001953125, acc: 92.1875, f1: 79.16298261125849, r: 0.7887751310805191
06/02/2019 09:41:35 step: 6960, epoch: 210, batch: 29, loss: 0.26567211747169495, acc: 84.375, f1: 79.27131348183981, r: 0.6920803930221082
06/02/2019 09:41:36 *** evaluating ***
06/02/2019 09:41:36 step: 211, epoch: 210, acc: 54.700854700854705, f1: 26.017961876832846, r: 0.33734892526997334
06/02/2019 09:41:36 *** epoch: 212 ***
06/02/2019 09:41:36 *** training ***
06/02/2019 09:41:37 step: 6968, epoch: 211, batch: 4, loss: 0.10500483214855194, acc: 96.875, f1: 98.29931972789116, r: 0.6748337451026603
06/02/2019 09:41:38 step: 6973, epoch: 211, batch: 9, loss: 0.08625654131174088, acc: 96.875, f1: 94.69979296066253, r: 0.7243098990280542
06/02/2019 09:41:39 step: 6978, epoch: 211, batch: 14, loss: 0.20682430267333984, acc: 90.625, f1: 73.13852813852813, r: 0.7251341457452262
06/02/2019 09:41:40 step: 6983, epoch: 211, batch: 19, loss: 0.10169261693954468, acc: 95.3125, f1: 81.3190404366875, r: 0.7431703370942399
06/02/2019 09:41:41 step: 6988, epoch: 211, batch: 24, loss: 0.12823902070522308, acc: 96.875, f1: 93.229883218685, r: 0.6579842144336912
06/02/2019 09:41:43 step: 6993, epoch: 211, batch: 29, loss: 0.12514565885066986, acc: 95.3125, f1: 94.25547996976569, r: 0.6958702102221684
06/02/2019 09:41:43 *** evaluating ***
06/02/2019 09:41:44 step: 212, epoch: 211, acc: 58.97435897435898, f1: 28.450435465909564, r: 0.3763702565155956
06/02/2019 09:41:44 *** epoch: 213 ***
06/02/2019 09:41:44 *** training ***
06/02/2019 09:41:45 step: 7001, epoch: 212, batch: 4, loss: 0.1701362282037735, acc: 92.1875, f1: 82.50595238095238, r: 0.7687389830507745
06/02/2019 09:41:46 step: 7006, epoch: 212, batch: 9, loss: 0.15284942090511322, acc: 92.1875, f1: 81.25375939849624, r: 0.7025941851692531
06/02/2019 09:41:47 step: 7011, epoch: 212, batch: 14, loss: 0.2590281367301941, acc: 90.625, f1: 72.41750208855471, r: 0.649419114533692
06/02/2019 09:41:48 step: 7016, epoch: 212, batch: 19, loss: 0.10394351929426193, acc: 98.4375, f1: 98.25396825396825, r: 0.746443702491593
06/02/2019 09:41:49 step: 7021, epoch: 212, batch: 24, loss: 0.11255545914173126, acc: 93.75, f1: 79.60420531849103, r: 0.688043261551933
06/02/2019 09:41:50 step: 7026, epoch: 212, batch: 29, loss: 0.16532795131206512, acc: 95.3125, f1: 87.29500891265597, r: 0.7166403942731723
06/02/2019 09:41:51 *** evaluating ***
06/02/2019 09:41:51 step: 213, epoch: 212, acc: 56.41025641025641, f1: 27.789552628146602, r: 0.33602624402959375
06/02/2019 09:41:51 *** epoch: 214 ***
06/02/2019 09:41:51 *** training ***
06/02/2019 09:41:52 step: 7034, epoch: 213, batch: 4, loss: 0.07017039507627487, acc: 96.875, f1: 93.5515873015873, r: 0.7168179426234166
06/02/2019 09:41:53 step: 7039, epoch: 213, batch: 9, loss: 0.10673821717500687, acc: 96.875, f1: 81.28654970760235, r: 0.6817941934837308
06/02/2019 09:41:55 step: 7044, epoch: 213, batch: 14, loss: 0.14713133871555328, acc: 95.3125, f1: 83.27380952380953, r: 0.6682426818002685
06/02/2019 09:41:56 step: 7049, epoch: 213, batch: 19, loss: 0.2527792155742645, acc: 87.5, f1: 77.21177699438569, r: 0.738174247694458
06/02/2019 09:41:57 step: 7054, epoch: 213, batch: 24, loss: 0.20215997099876404, acc: 93.75, f1: 86.26340996168582, r: 0.6980505288138688
06/02/2019 09:41:58 step: 7059, epoch: 213, batch: 29, loss: 0.2373584657907486, acc: 87.5, f1: 67.50730994152048, r: 0.6956051031641565
06/02/2019 09:41:59 *** evaluating ***
06/02/2019 09:41:59 step: 214, epoch: 213, acc: 53.84615384615385, f1: 25.886606813713797, r: 0.32750872412545207
06/02/2019 09:41:59 *** epoch: 215 ***
06/02/2019 09:41:59 *** training ***
06/02/2019 09:42:00 step: 7067, epoch: 214, batch: 4, loss: 0.05717003345489502, acc: 100.0, f1: 100.0, r: 0.7112194392905036
06/02/2019 09:42:01 step: 7072, epoch: 214, batch: 9, loss: 0.12414092570543289, acc: 95.3125, f1: 84.8263515932689, r: 0.6965133758570152
06/02/2019 09:42:02 step: 7077, epoch: 214, batch: 14, loss: 0.2196706086397171, acc: 93.75, f1: 80.38421341307715, r: 0.6127370796776072
06/02/2019 09:42:03 step: 7082, epoch: 214, batch: 19, loss: 0.34982848167419434, acc: 93.75, f1: 74.30187150014737, r: 0.6960163463101416
06/02/2019 09:42:05 step: 7087, epoch: 214, batch: 24, loss: 0.14102192223072052, acc: 95.3125, f1: 86.8421052631579, r: 0.7402328394019124
06/02/2019 09:42:05 step: 7092, epoch: 214, batch: 29, loss: 0.20223037898540497, acc: 90.625, f1: 79.68208874458874, r: 0.7514980224367634
06/02/2019 09:42:06 *** evaluating ***
06/02/2019 09:42:07 step: 215, epoch: 214, acc: 57.692307692307686, f1: 27.74308519150752, r: 0.3430859960403822
06/02/2019 09:42:07 *** epoch: 216 ***
06/02/2019 09:42:07 *** training ***
06/02/2019 09:42:08 step: 7100, epoch: 215, batch: 4, loss: 0.22063466906547546, acc: 90.625, f1: 88.31876456876458, r: 0.7131390579850377
06/02/2019 09:42:09 step: 7105, epoch: 215, batch: 9, loss: 0.11744537204504013, acc: 93.75, f1: 85.86395312005068, r: 0.6685508069805133
06/02/2019 09:42:10 step: 7110, epoch: 215, batch: 14, loss: 0.11528143286705017, acc: 95.3125, f1: 84.14814814814817, r: 0.5762979006504619
06/02/2019 09:42:11 step: 7115, epoch: 215, batch: 19, loss: 0.09993214905261993, acc: 96.875, f1: 93.80333951762523, r: 0.7185822916219781
06/02/2019 09:42:12 step: 7120, epoch: 215, batch: 24, loss: 0.06790804862976074, acc: 98.4375, f1: 96.39097744360903, r: 0.6952292049559186
06/02/2019 09:42:13 step: 7125, epoch: 215, batch: 29, loss: 0.2708311378955841, acc: 89.0625, f1: 82.87243425798412, r: 0.7471476267434868
06/02/2019 09:42:14 *** evaluating ***
06/02/2019 09:42:14 step: 216, epoch: 215, acc: 57.692307692307686, f1: 28.178613414980752, r: 0.34123343640694537
06/02/2019 09:42:14 *** epoch: 217 ***
06/02/2019 09:42:14 *** training ***
06/02/2019 09:42:15 step: 7133, epoch: 216, batch: 4, loss: 0.12635032832622528, acc: 92.1875, f1: 89.68664553901262, r: 0.5833481887102387
06/02/2019 09:42:16 step: 7138, epoch: 216, batch: 9, loss: 0.11716721951961517, acc: 95.3125, f1: 90.91123628938756, r: 0.6773846627457567
06/02/2019 09:42:18 step: 7143, epoch: 216, batch: 14, loss: 0.12228506058454514, acc: 93.75, f1: 89.11220747591335, r: 0.6649306597456086
06/02/2019 09:42:19 step: 7148, epoch: 216, batch: 19, loss: 0.3252870738506317, acc: 92.1875, f1: 86.41450216450217, r: 0.6323056196576857
06/02/2019 09:42:20 step: 7153, epoch: 216, batch: 24, loss: 0.09597591310739517, acc: 96.875, f1: 94.13915094339622, r: 0.7630975776174327
06/02/2019 09:42:21 step: 7158, epoch: 216, batch: 29, loss: 0.1379391849040985, acc: 93.75, f1: 78.02102659245517, r: 0.6835026866152919
06/02/2019 09:42:21 *** evaluating ***
06/02/2019 09:42:22 step: 217, epoch: 216, acc: 55.98290598290598, f1: 29.135160932820703, r: 0.34057230419547035
06/02/2019 09:42:22 *** epoch: 218 ***
06/02/2019 09:42:22 *** training ***
06/02/2019 09:42:23 step: 7166, epoch: 217, batch: 4, loss: 0.13829445838928223, acc: 95.3125, f1: 88.19086255191732, r: 0.6047396306509157
06/02/2019 09:42:24 step: 7171, epoch: 217, batch: 9, loss: 0.08609893172979355, acc: 95.3125, f1: 94.79166666666666, r: 0.6390227961394719
06/02/2019 09:42:25 step: 7176, epoch: 217, batch: 14, loss: 0.21217502653598785, acc: 96.875, f1: 85.43628374136848, r: 0.6642106912382483
06/02/2019 09:42:26 step: 7181, epoch: 217, batch: 19, loss: 0.092892125248909, acc: 96.875, f1: 98.1161953988041, r: 0.7889703210554206
06/02/2019 09:42:28 step: 7186, epoch: 217, batch: 24, loss: 0.11955509334802628, acc: 95.3125, f1: 92.81896551724138, r: 0.7799954422520975
06/02/2019 09:42:29 step: 7191, epoch: 217, batch: 29, loss: 0.17915622889995575, acc: 92.1875, f1: 86.50727593090022, r: 0.6444090740591439
06/02/2019 09:42:29 *** evaluating ***
06/02/2019 09:42:29 step: 218, epoch: 217, acc: 56.41025641025641, f1: 26.281081398076473, r: 0.3320428858349127
06/02/2019 09:42:29 *** epoch: 219 ***
06/02/2019 09:42:29 *** training ***
06/02/2019 09:42:31 step: 7199, epoch: 218, batch: 4, loss: 0.21043407917022705, acc: 87.5, f1: 72.87090028615452, r: 0.6793596901283805
06/02/2019 09:42:32 step: 7204, epoch: 218, batch: 9, loss: 0.1999468207359314, acc: 93.75, f1: 89.97251133879041, r: 0.7289372707665719
06/02/2019 09:42:33 step: 7209, epoch: 218, batch: 14, loss: 0.26572033762931824, acc: 89.0625, f1: 82.88915945165944, r: 0.7369713981362972
06/02/2019 09:42:34 step: 7214, epoch: 218, batch: 19, loss: 0.08892783522605896, acc: 95.3125, f1: 94.03520499108734, r: 0.7732981414933835
06/02/2019 09:42:35 step: 7219, epoch: 218, batch: 24, loss: 0.1342778503894806, acc: 95.3125, f1: 78.55113636363636, r: 0.6465672882534534
06/02/2019 09:42:37 step: 7224, epoch: 218, batch: 29, loss: 0.07447496801614761, acc: 98.4375, f1: 95.62146892655367, r: 0.82416899632916
06/02/2019 09:42:37 *** evaluating ***
06/02/2019 09:42:37 step: 219, epoch: 218, acc: 57.26495726495726, f1: 26.490047636954156, r: 0.3391760707445595
06/02/2019 09:42:37 *** epoch: 220 ***
06/02/2019 09:42:37 *** training ***
06/02/2019 09:42:39 step: 7232, epoch: 219, batch: 4, loss: 0.18409985303878784, acc: 92.1875, f1: 89.6969696969697, r: 0.7371757535887803
06/02/2019 09:42:40 step: 7237, epoch: 219, batch: 9, loss: 0.20232146978378296, acc: 93.75, f1: 66.16883116883116, r: 0.6022850986802999
06/02/2019 09:42:41 step: 7242, epoch: 219, batch: 14, loss: 0.16280081868171692, acc: 95.3125, f1: 84.53588681849553, r: 0.6888278606087881
06/02/2019 09:42:42 step: 7247, epoch: 219, batch: 19, loss: 0.13492919504642487, acc: 95.3125, f1: 91.27125597713834, r: 0.6674688231379563
06/02/2019 09:42:43 step: 7252, epoch: 219, batch: 24, loss: 0.14739099144935608, acc: 93.75, f1: 86.50318625473903, r: 0.6810079137501157
06/02/2019 09:42:44 step: 7257, epoch: 219, batch: 29, loss: 0.08722782880067825, acc: 95.3125, f1: 74.41818832796277, r: 0.647273003542676
06/02/2019 09:42:44 *** evaluating ***
06/02/2019 09:42:45 step: 220, epoch: 219, acc: 56.41025641025641, f1: 26.68559237854434, r: 0.3306605195183356
06/02/2019 09:42:45 *** epoch: 221 ***
06/02/2019 09:42:45 *** training ***
06/02/2019 09:42:45 step: 7265, epoch: 220, batch: 4, loss: 0.21969911456108093, acc: 90.625, f1: 71.87950937950937, r: 0.6818983925797706
06/02/2019 09:42:47 step: 7270, epoch: 220, batch: 9, loss: 0.09898526966571808, acc: 93.75, f1: 92.3140928906774, r: 0.7042980909359785
06/02/2019 09:42:48 step: 7275, epoch: 220, batch: 14, loss: 0.11633773893117905, acc: 96.875, f1: 94.36300897170463, r: 0.634155223513765
06/02/2019 09:42:49 step: 7280, epoch: 220, batch: 19, loss: 0.11795390397310257, acc: 95.3125, f1: 85.58823529411764, r: 0.5852521648351215
06/02/2019 09:42:50 step: 7285, epoch: 220, batch: 24, loss: 0.1487317681312561, acc: 93.75, f1: 90.30448717948718, r: 0.7502557666898757
06/02/2019 09:42:51 step: 7290, epoch: 220, batch: 29, loss: 0.0653325542807579, acc: 100.0, f1: 100.0, r: 0.5715375473255634
06/02/2019 09:42:52 *** evaluating ***
06/02/2019 09:42:52 step: 221, epoch: 220, acc: 57.692307692307686, f1: 27.0649635452267, r: 0.3257990305428997
06/02/2019 09:42:52 *** epoch: 222 ***
06/02/2019 09:42:52 *** training ***
06/02/2019 09:42:53 step: 7298, epoch: 221, batch: 4, loss: 0.1001296266913414, acc: 96.875, f1: 92.58928571428571, r: 0.7497191937259386
06/02/2019 09:42:54 step: 7303, epoch: 221, batch: 9, loss: 0.30723461508750916, acc: 92.1875, f1: 89.22448466566112, r: 0.5803313805754831
06/02/2019 09:42:55 step: 7308, epoch: 221, batch: 14, loss: 0.20494496822357178, acc: 90.625, f1: 86.69795482295483, r: 0.7197995624749898
06/02/2019 09:42:56 step: 7313, epoch: 221, batch: 19, loss: 0.09319906681776047, acc: 96.875, f1: 96.72701766318788, r: 0.7152004122549649
06/02/2019 09:42:57 step: 7318, epoch: 221, batch: 24, loss: 0.15938544273376465, acc: 93.75, f1: 94.78137631864341, r: 0.6329111864000677
06/02/2019 09:42:58 step: 7323, epoch: 221, batch: 29, loss: 0.16059617698192596, acc: 95.3125, f1: 90.97938645062268, r: 0.6660184447625446
06/02/2019 09:42:59 *** evaluating ***
06/02/2019 09:42:59 step: 222, epoch: 221, acc: 55.98290598290598, f1: 27.326144572661605, r: 0.32763668563243603
06/02/2019 09:42:59 *** epoch: 223 ***
06/02/2019 09:42:59 *** training ***
06/02/2019 09:43:00 step: 7331, epoch: 222, batch: 4, loss: 0.16572701930999756, acc: 98.4375, f1: 99.09922589725547, r: 0.6584172397074394
06/02/2019 09:43:01 step: 7336, epoch: 222, batch: 9, loss: 0.15056107938289642, acc: 92.1875, f1: 82.79790385053543, r: 0.6715292540507475
06/02/2019 09:43:02 step: 7341, epoch: 222, batch: 14, loss: 0.11644942313432693, acc: 93.75, f1: 85.82107843137254, r: 0.748030731097199
06/02/2019 09:43:03 step: 7346, epoch: 222, batch: 19, loss: 0.263222873210907, acc: 96.875, f1: 97.1218487394958, r: 0.7372393345410078
06/02/2019 09:43:04 step: 7351, epoch: 222, batch: 24, loss: 0.17667818069458008, acc: 92.1875, f1: 86.68643099205934, r: 0.6826235625520491
06/02/2019 09:43:06 step: 7356, epoch: 222, batch: 29, loss: 0.14269518852233887, acc: 95.3125, f1: 79.43223443223442, r: 0.613945151471715
06/02/2019 09:43:06 *** evaluating ***
06/02/2019 09:43:06 step: 223, epoch: 222, acc: 55.55555555555556, f1: 27.134209744503863, r: 0.3338658528187816
06/02/2019 09:43:06 *** epoch: 224 ***
06/02/2019 09:43:06 *** training ***
06/02/2019 09:43:07 step: 7364, epoch: 223, batch: 4, loss: 0.12333951890468597, acc: 95.3125, f1: 69.20289855072464, r: 0.7470200402982369
06/02/2019 09:43:08 step: 7369, epoch: 223, batch: 9, loss: 0.12003690749406815, acc: 96.875, f1: 94.22222222222221, r: 0.7668422767303704
06/02/2019 09:43:10 step: 7374, epoch: 223, batch: 14, loss: 0.03956717997789383, acc: 100.0, f1: 100.0, r: 0.7048283042095201
06/02/2019 09:43:11 step: 7379, epoch: 223, batch: 19, loss: 0.13603855669498444, acc: 93.75, f1: 86.1988674780257, r: 0.7294554402283595
06/02/2019 09:43:12 step: 7384, epoch: 223, batch: 24, loss: 0.15989425778388977, acc: 92.1875, f1: 77.65158553202032, r: 0.6851968725836435
06/02/2019 09:43:13 step: 7389, epoch: 223, batch: 29, loss: 0.19274874031543732, acc: 96.875, f1: 94.28953399541635, r: 0.685263931797333
06/02/2019 09:43:14 *** evaluating ***
06/02/2019 09:43:14 step: 224, epoch: 223, acc: 54.700854700854705, f1: 28.175581403440404, r: 0.3179507674204288
06/02/2019 09:43:14 *** epoch: 225 ***
06/02/2019 09:43:14 *** training ***
06/02/2019 09:43:15 step: 7397, epoch: 224, batch: 4, loss: 0.0905260443687439, acc: 96.875, f1: 95.86621872336157, r: 0.6653458186665201
06/02/2019 09:43:16 step: 7402, epoch: 224, batch: 9, loss: 0.18982243537902832, acc: 92.1875, f1: 88.8301282051282, r: 0.7351953682347219
06/02/2019 09:43:18 step: 7407, epoch: 224, batch: 14, loss: 0.15551766753196716, acc: 93.75, f1: 71.13197918463574, r: 0.5903300354266605
06/02/2019 09:43:19 step: 7412, epoch: 224, batch: 19, loss: 0.11604069173336029, acc: 93.75, f1: 94.54365079365078, r: 0.7598011432879024
06/02/2019 09:43:20 step: 7417, epoch: 224, batch: 24, loss: 0.12649643421173096, acc: 95.3125, f1: 89.47089947089947, r: 0.6619633825167344
06/02/2019 09:43:21 step: 7422, epoch: 224, batch: 29, loss: 0.1746790111064911, acc: 92.1875, f1: 77.99150910364145, r: 0.7501921782169156
06/02/2019 09:43:22 *** evaluating ***
06/02/2019 09:43:22 step: 225, epoch: 224, acc: 56.837606837606835, f1: 27.667017738228928, r: 0.33313675184975516
06/02/2019 09:43:22 *** epoch: 226 ***
06/02/2019 09:43:22 *** training ***
06/02/2019 09:43:23 step: 7430, epoch: 225, batch: 4, loss: 0.09288378059864044, acc: 100.0, f1: 100.0, r: 0.6727956361985817
06/02/2019 09:43:24 step: 7435, epoch: 225, batch: 9, loss: 0.10126761347055435, acc: 93.75, f1: 79.44047619047619, r: 0.6985746057463953
06/02/2019 09:43:25 step: 7440, epoch: 225, batch: 14, loss: 0.06370420008897781, acc: 96.875, f1: 98.66666666666667, r: 0.7156923452438871
06/02/2019 09:43:26 step: 7445, epoch: 225, batch: 19, loss: 0.10153856873512268, acc: 98.4375, f1: 98.76344086021506, r: 0.7666073200975487
06/02/2019 09:43:28 step: 7450, epoch: 225, batch: 24, loss: 0.09255009144544601, acc: 96.875, f1: 91.9062720225511, r: 0.7745147759150133
06/02/2019 09:43:29 step: 7455, epoch: 225, batch: 29, loss: 0.08984112739562988, acc: 96.875, f1: 93.1230674087817, r: 0.6600182372893963
06/02/2019 09:43:29 *** evaluating ***
06/02/2019 09:43:30 step: 226, epoch: 225, acc: 58.54700854700855, f1: 27.98723827102621, r: 0.33076841465578866
06/02/2019 09:43:30 *** epoch: 227 ***
06/02/2019 09:43:30 *** training ***
06/02/2019 09:43:31 step: 7463, epoch: 226, batch: 4, loss: 0.11358477175235748, acc: 96.875, f1: 97.25132275132276, r: 0.7201059461029605
06/02/2019 09:43:32 step: 7468, epoch: 226, batch: 9, loss: 0.14547628164291382, acc: 95.3125, f1: 90.01072501072501, r: 0.6382220767059509
06/02/2019 09:43:33 step: 7473, epoch: 226, batch: 14, loss: 0.12039527297019958, acc: 96.875, f1: 92.3202614379085, r: 0.7674543736219105
06/02/2019 09:43:34 step: 7478, epoch: 226, batch: 19, loss: 0.12926620244979858, acc: 92.1875, f1: 72.55208333333334, r: 0.6768651884026745
06/02/2019 09:43:35 step: 7483, epoch: 226, batch: 24, loss: 0.16440150141716003, acc: 92.1875, f1: 85.63000638852033, r: 0.7470921429880203
06/02/2019 09:43:37 step: 7488, epoch: 226, batch: 29, loss: 0.09280628710985184, acc: 96.875, f1: 92.30734154126768, r: 0.7473087981586415
06/02/2019 09:43:37 *** evaluating ***
06/02/2019 09:43:37 step: 227, epoch: 226, acc: 56.41025641025641, f1: 27.925543024227235, r: 0.3373597366111168
06/02/2019 09:43:37 *** epoch: 228 ***
06/02/2019 09:43:37 *** training ***
06/02/2019 09:43:38 step: 7496, epoch: 227, batch: 4, loss: 0.02752172201871872, acc: 100.0, f1: 100.0, r: 0.7981224470996376
06/02/2019 09:43:39 step: 7501, epoch: 227, batch: 9, loss: 0.2561505138874054, acc: 89.0625, f1: 76.3263658176449, r: 0.7206871416692602
06/02/2019 09:43:41 step: 7506, epoch: 227, batch: 14, loss: 0.13574892282485962, acc: 92.1875, f1: 75.46952657066394, r: 0.7401481816950578
06/02/2019 09:43:42 step: 7511, epoch: 227, batch: 19, loss: 0.09466865658760071, acc: 95.3125, f1: 93.37484335839599, r: 0.7964485156108762
06/02/2019 09:43:43 step: 7516, epoch: 227, batch: 24, loss: 0.06925586611032486, acc: 96.875, f1: 89.52380952380953, r: 0.6742289630579743
06/02/2019 09:43:44 step: 7521, epoch: 227, batch: 29, loss: 0.099721759557724, acc: 95.3125, f1: 95.78671413348833, r: 0.7705929596955177
06/02/2019 09:43:44 *** evaluating ***
06/02/2019 09:43:45 step: 228, epoch: 227, acc: 57.692307692307686, f1: 27.80327342747112, r: 0.3213068305663462
06/02/2019 09:43:45 *** epoch: 229 ***
06/02/2019 09:43:45 *** training ***
06/02/2019 09:43:46 step: 7529, epoch: 228, batch: 4, loss: 0.1784890741109848, acc: 93.75, f1: 79.75786265923374, r: 0.6198036511492218
06/02/2019 09:43:47 step: 7534, epoch: 228, batch: 9, loss: 0.15644118189811707, acc: 93.75, f1: 78.52648298479096, r: 0.6002970106788006
06/02/2019 09:43:48 step: 7539, epoch: 228, batch: 14, loss: 0.07537728548049927, acc: 96.875, f1: 95.84278155706727, r: 0.7129521969628175
06/02/2019 09:43:49 step: 7544, epoch: 228, batch: 19, loss: 0.1936226487159729, acc: 90.625, f1: 85.60214022716005, r: 0.7079224054070036
06/02/2019 09:43:50 step: 7549, epoch: 228, batch: 24, loss: 0.09892236441373825, acc: 98.4375, f1: 84.76190476190476, r: 0.6444823742471442
06/02/2019 09:43:51 step: 7554, epoch: 228, batch: 29, loss: 0.20627538859844208, acc: 93.75, f1: 88.83830455259026, r: 0.6489589390463669
06/02/2019 09:43:52 *** evaluating ***
06/02/2019 09:43:52 step: 229, epoch: 228, acc: 58.97435897435898, f1: 28.81773277155768, r: 0.3395865930021497
06/02/2019 09:43:52 *** epoch: 230 ***
06/02/2019 09:43:52 *** training ***
06/02/2019 09:43:53 step: 7562, epoch: 229, batch: 4, loss: 0.09010662138462067, acc: 98.4375, f1: 87.17948717948718, r: 0.7325394603184936
06/02/2019 09:43:54 step: 7567, epoch: 229, batch: 9, loss: 0.11110257357358932, acc: 95.3125, f1: 77.82587782587782, r: 0.7028674547072307
06/02/2019 09:43:55 step: 7572, epoch: 229, batch: 14, loss: 0.14371009171009064, acc: 93.75, f1: 86.88492063492063, r: 0.7550349901395382
06/02/2019 09:43:56 step: 7577, epoch: 229, batch: 19, loss: 0.1260777711868286, acc: 93.75, f1: 90.65954576824141, r: 0.6521953720615852
06/02/2019 09:43:57 step: 7582, epoch: 229, batch: 24, loss: 0.07841389626264572, acc: 96.875, f1: 97.73391812865498, r: 0.7469190805948507
06/02/2019 09:43:59 step: 7587, epoch: 229, batch: 29, loss: 0.17046993970870972, acc: 95.3125, f1: 87.6654151022111, r: 0.758436484426724
06/02/2019 09:43:59 *** evaluating ***
06/02/2019 09:43:59 step: 230, epoch: 229, acc: 52.13675213675214, f1: 26.94316267698824, r: 0.3286896164494414
06/02/2019 09:43:59 *** epoch: 231 ***
06/02/2019 09:43:59 *** training ***
06/02/2019 09:44:01 step: 7595, epoch: 230, batch: 4, loss: 0.21335667371749878, acc: 92.1875, f1: 77.2887457370216, r: 0.7365491922284175
06/02/2019 09:44:01 step: 7600, epoch: 230, batch: 9, loss: 0.11167959123849869, acc: 96.875, f1: 85.09316770186335, r: 0.6501982975557772
06/02/2019 09:44:03 step: 7605, epoch: 230, batch: 14, loss: 0.15345653891563416, acc: 92.1875, f1: 78.13095970095357, r: 0.6205648224656546
06/02/2019 09:44:04 step: 7610, epoch: 230, batch: 19, loss: 0.16078104078769684, acc: 93.75, f1: 90.71479885057472, r: 0.7543752799045189
06/02/2019 09:44:05 step: 7615, epoch: 230, batch: 24, loss: 0.13059267401695251, acc: 95.3125, f1: 93.47129327921647, r: 0.6678026604251672
06/02/2019 09:44:06 step: 7620, epoch: 230, batch: 29, loss: 0.10113228112459183, acc: 96.875, f1: 94.0200933108166, r: 0.6885984381198158
06/02/2019 09:44:07 *** evaluating ***
06/02/2019 09:44:07 step: 231, epoch: 230, acc: 56.41025641025641, f1: 28.00483772339334, r: 0.3182185109557812
06/02/2019 09:44:07 *** epoch: 232 ***
06/02/2019 09:44:07 *** training ***
06/02/2019 09:44:08 step: 7628, epoch: 231, batch: 4, loss: 0.13363316655158997, acc: 95.3125, f1: 80.4920634920635, r: 0.7311469071791111
06/02/2019 09:44:09 step: 7633, epoch: 231, batch: 9, loss: 0.08493516594171524, acc: 96.875, f1: 94.39393939393939, r: 0.7832569767526695
06/02/2019 09:44:11 step: 7638, epoch: 231, batch: 14, loss: 0.1114286258816719, acc: 96.875, f1: 98.0152601581173, r: 0.7454468011164781
06/02/2019 09:44:12 step: 7643, epoch: 231, batch: 19, loss: 0.09245608001947403, acc: 95.3125, f1: 77.26090046661103, r: 0.6562139991367298
06/02/2019 09:44:13 step: 7648, epoch: 231, batch: 24, loss: 0.08307842910289764, acc: 98.4375, f1: 97.84126984126985, r: 0.6649872900534288
06/02/2019 09:44:14 step: 7653, epoch: 231, batch: 29, loss: 0.05299365147948265, acc: 100.0, f1: 100.0, r: 0.65702678079083
06/02/2019 09:44:14 *** evaluating ***
06/02/2019 09:44:15 step: 232, epoch: 231, acc: 58.54700854700855, f1: 29.440173625657494, r: 0.3225316311360467
06/02/2019 09:44:15 *** epoch: 233 ***
06/02/2019 09:44:15 *** training ***
06/02/2019 09:44:16 step: 7661, epoch: 232, batch: 4, loss: 0.05518548563122749, acc: 96.875, f1: 79.46428571428572, r: 0.696739034966408
06/02/2019 09:44:17 step: 7666, epoch: 232, batch: 9, loss: 0.14040891826152802, acc: 92.1875, f1: 78.19701629020005, r: 0.7710270546259853
06/02/2019 09:44:18 step: 7671, epoch: 232, batch: 14, loss: 0.10389237850904465, acc: 98.4375, f1: 87.06896551724138, r: 0.692640776394115
06/02/2019 09:44:19 step: 7676, epoch: 232, batch: 19, loss: 0.0410483293235302, acc: 100.0, f1: 100.0, r: 0.7027891794426339
06/02/2019 09:44:21 step: 7681, epoch: 232, batch: 24, loss: 0.21425685286521912, acc: 96.875, f1: 92.49915739804517, r: 0.7227902777986752
06/02/2019 09:44:22 step: 7686, epoch: 232, batch: 29, loss: 0.11751773208379745, acc: 95.3125, f1: 91.67728909745716, r: 0.6892149067089212
06/02/2019 09:44:22 *** evaluating ***
06/02/2019 09:44:23 step: 233, epoch: 232, acc: 56.41025641025641, f1: 27.536479519462816, r: 0.33853629766055926
06/02/2019 09:44:23 *** epoch: 234 ***
06/02/2019 09:44:23 *** training ***
06/02/2019 09:44:24 step: 7694, epoch: 233, batch: 4, loss: 0.10379701852798462, acc: 96.875, f1: 94.33747412008282, r: 0.7795255893602585
06/02/2019 09:44:25 step: 7699, epoch: 233, batch: 9, loss: 0.11974115669727325, acc: 96.875, f1: 71.95652173913044, r: 0.6190906277545638
06/02/2019 09:44:26 step: 7704, epoch: 233, batch: 14, loss: 0.10042227059602737, acc: 96.875, f1: 95.74659182036889, r: 0.6873843232326967
06/02/2019 09:44:27 step: 7709, epoch: 233, batch: 19, loss: 0.14872011542320251, acc: 95.3125, f1: 90.44871794871796, r: 0.7703899659213171
06/02/2019 09:44:28 step: 7714, epoch: 233, batch: 24, loss: 0.20883335173130035, acc: 90.625, f1: 87.43284493284493, r: 0.6863250423268734
06/02/2019 09:44:29 step: 7719, epoch: 233, batch: 29, loss: 0.17884697020053864, acc: 93.75, f1: 88.57502252460236, r: 0.6353276577580074
06/02/2019 09:44:30 *** evaluating ***
06/02/2019 09:44:30 step: 234, epoch: 233, acc: 54.27350427350427, f1: 27.248302423465198, r: 0.3191357198289788
06/02/2019 09:44:30 *** epoch: 235 ***
06/02/2019 09:44:30 *** training ***
06/02/2019 09:44:31 step: 7727, epoch: 234, batch: 4, loss: 0.2168346643447876, acc: 90.625, f1: 72.19611528822054, r: 0.6878539631352777
06/02/2019 09:44:32 step: 7732, epoch: 234, batch: 9, loss: 0.1861804872751236, acc: 93.75, f1: 92.43135993135994, r: 0.6706524146655418
06/02/2019 09:44:33 step: 7737, epoch: 234, batch: 14, loss: 0.09396198391914368, acc: 96.875, f1: 97.7593130553657, r: 0.7283256308082912
06/02/2019 09:44:34 step: 7742, epoch: 234, batch: 19, loss: 0.12287598848342896, acc: 92.1875, f1: 90.36693260554802, r: 0.6497009480175959
06/02/2019 09:44:36 step: 7747, epoch: 234, batch: 24, loss: 0.26642540097236633, acc: 90.625, f1: 75.46355570530099, r: 0.7832247408563218
06/02/2019 09:44:37 step: 7752, epoch: 234, batch: 29, loss: 0.12101994454860687, acc: 96.875, f1: 94.66700473292767, r: 0.7330711517023416
06/02/2019 09:44:37 *** evaluating ***
06/02/2019 09:44:38 step: 235, epoch: 234, acc: 56.837606837606835, f1: 28.17851892097795, r: 0.32651145279132215
06/02/2019 09:44:38 *** epoch: 236 ***
06/02/2019 09:44:38 *** training ***
06/02/2019 09:44:39 step: 7760, epoch: 235, batch: 4, loss: 0.15525111556053162, acc: 90.625, f1: 85.9502296216292, r: 0.6037995532395956
06/02/2019 09:44:40 step: 7765, epoch: 235, batch: 9, loss: 0.16822674870491028, acc: 93.75, f1: 84.93506493506493, r: 0.7139204330589226
06/02/2019 09:44:41 step: 7770, epoch: 235, batch: 14, loss: 0.20497068762779236, acc: 92.1875, f1: 89.0909090909091, r: 0.7537974170671637
06/02/2019 09:44:42 step: 7775, epoch: 235, batch: 19, loss: 0.1603327840566635, acc: 92.1875, f1: 82.3917748917749, r: 0.6589336708088437
06/02/2019 09:44:43 step: 7780, epoch: 235, batch: 24, loss: 0.07667484134435654, acc: 98.4375, f1: 96.66048237476808, r: 0.7065136038258618
06/02/2019 09:44:45 step: 7785, epoch: 235, batch: 29, loss: 0.1877075582742691, acc: 90.625, f1: 82.7190170940171, r: 0.6857283930100636
06/02/2019 09:44:45 *** evaluating ***
06/02/2019 09:44:45 step: 236, epoch: 235, acc: 56.837606837606835, f1: 29.072740518379803, r: 0.3345717044116416
06/02/2019 09:44:45 *** epoch: 237 ***
06/02/2019 09:44:45 *** training ***
06/02/2019 09:44:47 step: 7793, epoch: 236, batch: 4, loss: 0.14006789028644562, acc: 92.1875, f1: 85.1822933010557, r: 0.7613218764919083
06/02/2019 09:44:48 step: 7798, epoch: 236, batch: 9, loss: 0.12908399105072021, acc: 95.3125, f1: 89.81762314626495, r: 0.6548369927013634
06/02/2019 09:44:49 step: 7803, epoch: 236, batch: 14, loss: 0.19077445566654205, acc: 90.625, f1: 81.10998320675739, r: 0.7303935364948285
06/02/2019 09:44:50 step: 7808, epoch: 236, batch: 19, loss: 0.126198410987854, acc: 92.1875, f1: 77.4114774114774, r: 0.733552454703141
06/02/2019 09:44:52 step: 7813, epoch: 236, batch: 24, loss: 0.14065930247306824, acc: 98.4375, f1: 99.04247104247104, r: 0.7238703527248987
06/02/2019 09:44:53 step: 7818, epoch: 236, batch: 29, loss: 0.14441455900669098, acc: 90.625, f1: 86.50786713286713, r: 0.7662799416458582
06/02/2019 09:44:53 *** evaluating ***
06/02/2019 09:44:54 step: 237, epoch: 236, acc: 54.700854700854705, f1: 29.173753465782237, r: 0.3497716842774633
06/02/2019 09:44:54 *** epoch: 238 ***
06/02/2019 09:44:54 *** training ***
06/02/2019 09:44:55 step: 7826, epoch: 237, batch: 4, loss: 0.07748100161552429, acc: 95.3125, f1: 91.09554933084345, r: 0.7227882276111225
06/02/2019 09:44:56 step: 7831, epoch: 237, batch: 9, loss: 0.08966311812400818, acc: 96.875, f1: 93.48148148148148, r: 0.7205782945645947
06/02/2019 09:44:57 step: 7836, epoch: 237, batch: 14, loss: 0.08110903203487396, acc: 95.3125, f1: 84.36574586128157, r: 0.7455795769661211
06/02/2019 09:44:58 step: 7841, epoch: 237, batch: 19, loss: 0.09816145896911621, acc: 95.3125, f1: 88.3365909452866, r: 0.6902770105710248
06/02/2019 09:44:59 step: 7846, epoch: 237, batch: 24, loss: 0.1334112286567688, acc: 95.3125, f1: 83.1244671781756, r: 0.8020224053608491
06/02/2019 09:45:00 step: 7851, epoch: 237, batch: 29, loss: 0.1434701383113861, acc: 92.1875, f1: 83.09295700396403, r: 0.6346238551324453
06/02/2019 09:45:01 *** evaluating ***
06/02/2019 09:45:01 step: 238, epoch: 237, acc: 55.55555555555556, f1: 27.29818164934678, r: 0.3305562479970811
06/02/2019 09:45:01 *** epoch: 239 ***
06/02/2019 09:45:01 *** training ***
06/02/2019 09:45:02 step: 7859, epoch: 238, batch: 4, loss: 0.0958525538444519, acc: 96.875, f1: 91.5256423310593, r: 0.6850239976286784
06/02/2019 09:45:04 step: 7864, epoch: 238, batch: 9, loss: 0.10035791248083115, acc: 96.875, f1: 84.28813559322033, r: 0.6856358611597508
06/02/2019 09:45:05 step: 7869, epoch: 238, batch: 14, loss: 0.15093980729579926, acc: 92.1875, f1: 78.79942279942281, r: 0.7700290310590749
06/02/2019 09:45:06 step: 7874, epoch: 238, batch: 19, loss: 0.2132672369480133, acc: 96.875, f1: 82.78985507246377, r: 0.7581755640418699
06/02/2019 09:45:07 step: 7879, epoch: 238, batch: 24, loss: 0.17532826960086823, acc: 96.875, f1: 95.00381970970207, r: 0.5866380159502967
06/02/2019 09:45:08 step: 7884, epoch: 238, batch: 29, loss: 0.09333756566047668, acc: 96.875, f1: 83.75, r: 0.7554208771631362
06/02/2019 09:45:09 *** evaluating ***
06/02/2019 09:45:09 step: 239, epoch: 238, acc: 54.27350427350427, f1: 26.18036862250693, r: 0.31505933498544675
06/02/2019 09:45:09 *** epoch: 240 ***
06/02/2019 09:45:09 *** training ***
06/02/2019 09:45:10 step: 7892, epoch: 239, batch: 4, loss: 0.19616681337356567, acc: 93.75, f1: 77.43055555555556, r: 0.6407871745114468
06/02/2019 09:45:11 step: 7897, epoch: 239, batch: 9, loss: 0.09107784181833267, acc: 96.875, f1: 96.69709989258861, r: 0.7379734952459693
06/02/2019 09:45:13 step: 7902, epoch: 239, batch: 14, loss: 0.11099470406770706, acc: 96.875, f1: 97.24512202773073, r: 0.6431302826119422
06/02/2019 09:45:13 step: 7907, epoch: 239, batch: 19, loss: 0.09097529202699661, acc: 93.75, f1: 84.92226890756302, r: 0.6684969565453261
06/02/2019 09:45:15 step: 7912, epoch: 239, batch: 24, loss: 0.11048603802919388, acc: 95.3125, f1: 92.04144979397094, r: 0.7552771457419267
06/02/2019 09:45:16 step: 7917, epoch: 239, batch: 29, loss: 0.11800947785377502, acc: 95.3125, f1: 89.47916666666666, r: 0.5868570548211005
06/02/2019 09:45:16 *** evaluating ***
06/02/2019 09:45:17 step: 240, epoch: 239, acc: 54.700854700854705, f1: 27.551233112862374, r: 0.32415968719819666
06/02/2019 09:45:17 *** epoch: 241 ***
06/02/2019 09:45:17 *** training ***
06/02/2019 09:45:18 step: 7925, epoch: 240, batch: 4, loss: 0.13391488790512085, acc: 92.1875, f1: 85.35162506347905, r: 0.8071618545456382
06/02/2019 09:45:19 step: 7930, epoch: 240, batch: 9, loss: 0.13594132661819458, acc: 92.1875, f1: 80.10176957545379, r: 0.819501816303374
06/02/2019 09:45:20 step: 7935, epoch: 240, batch: 14, loss: 0.11330171674489975, acc: 98.4375, f1: 98.7878787878788, r: 0.7250596665023913
06/02/2019 09:45:21 step: 7940, epoch: 240, batch: 19, loss: 0.04686972498893738, acc: 96.875, f1: 96.64366032290562, r: 0.7608217119433213
06/02/2019 09:45:22 step: 7945, epoch: 240, batch: 24, loss: 0.05983835458755493, acc: 98.4375, f1: 99.05018611218071, r: 0.6878660992478665
06/02/2019 09:45:23 step: 7950, epoch: 240, batch: 29, loss: 0.10604383051395416, acc: 92.1875, f1: 80.84325396825398, r: 0.8020025614517506
06/02/2019 09:45:24 *** evaluating ***
06/02/2019 09:45:24 step: 241, epoch: 240, acc: 57.26495726495726, f1: 27.571580063626723, r: 0.32535306843480216
06/02/2019 09:45:24 *** epoch: 242 ***
06/02/2019 09:45:24 *** training ***
06/02/2019 09:45:25 step: 7958, epoch: 241, batch: 4, loss: 0.11185183376073837, acc: 93.75, f1: 75.68139097744361, r: 0.6150480886747478
06/02/2019 09:45:26 step: 7963, epoch: 241, batch: 9, loss: 0.11725789308547974, acc: 95.3125, f1: 80.95765345765346, r: 0.6725003850731709
06/02/2019 09:45:27 step: 7968, epoch: 241, batch: 14, loss: 0.20002731680870056, acc: 92.1875, f1: 69.08755530184102, r: 0.652614287232941
06/02/2019 09:45:28 step: 7973, epoch: 241, batch: 19, loss: 0.16302014887332916, acc: 93.75, f1: 79.29481858554188, r: 0.6009233652549182
06/02/2019 09:45:29 step: 7978, epoch: 241, batch: 24, loss: 0.08933102339506149, acc: 98.4375, f1: 98.20868786386028, r: 0.6730139736849577
06/02/2019 09:45:31 step: 7983, epoch: 241, batch: 29, loss: 0.3640955090522766, acc: 92.1875, f1: 92.99135991682577, r: 0.7542294385377731
06/02/2019 09:45:31 *** evaluating ***
06/02/2019 09:45:32 step: 242, epoch: 241, acc: 55.12820512820513, f1: 29.139068310359328, r: 0.3634269172940883
06/02/2019 09:45:32 *** epoch: 243 ***
06/02/2019 09:45:32 *** training ***
06/02/2019 09:45:33 step: 7991, epoch: 242, batch: 4, loss: 0.17730677127838135, acc: 92.1875, f1: 77.30798771121353, r: 0.6068864042319131
06/02/2019 09:45:34 step: 7996, epoch: 242, batch: 9, loss: 0.032851237803697586, acc: 100.0, f1: 100.0, r: 0.6744240240941313
06/02/2019 09:45:35 step: 8001, epoch: 242, batch: 14, loss: 0.2267138510942459, acc: 92.1875, f1: 88.67832933350174, r: 0.636087204694941
06/02/2019 09:45:36 step: 8006, epoch: 242, batch: 19, loss: 0.12465386837720871, acc: 95.3125, f1: 89.65873015873017, r: 0.7242988671666143
06/02/2019 09:45:37 step: 8011, epoch: 242, batch: 24, loss: 0.06786088645458221, acc: 98.4375, f1: 97.67907162865147, r: 0.6325159251024022
06/02/2019 09:45:39 step: 8016, epoch: 242, batch: 29, loss: 0.12423660606145859, acc: 98.4375, f1: 97.46031746031747, r: 0.7823563687334549
06/02/2019 09:45:39 *** evaluating ***
06/02/2019 09:45:39 step: 243, epoch: 242, acc: 54.27350427350427, f1: 27.737930467824544, r: 0.32572295680395985
06/02/2019 09:45:39 *** epoch: 244 ***
06/02/2019 09:45:39 *** training ***
06/02/2019 09:45:41 step: 8024, epoch: 243, batch: 4, loss: 0.1698284149169922, acc: 92.1875, f1: 79.91147741147742, r: 0.7189909922086115
06/02/2019 09:45:42 step: 8029, epoch: 243, batch: 9, loss: 0.0964808315038681, acc: 96.875, f1: 92.93933759129006, r: 0.702899333470951
06/02/2019 09:45:43 step: 8034, epoch: 243, batch: 14, loss: 0.1166963279247284, acc: 92.1875, f1: 87.80623306233062, r: 0.7695763922879568
06/02/2019 09:45:44 step: 8039, epoch: 243, batch: 19, loss: 0.06377014517784119, acc: 100.0, f1: 100.0, r: 0.7450084563735032
06/02/2019 09:45:45 step: 8044, epoch: 243, batch: 24, loss: 0.1193530261516571, acc: 95.3125, f1: 80.19444444444444, r: 0.6835078140656383
06/02/2019 09:45:46 step: 8049, epoch: 243, batch: 29, loss: 0.15376773476600647, acc: 93.75, f1: 81.20748299319727, r: 0.5781513661726189
06/02/2019 09:45:47 *** evaluating ***
06/02/2019 09:45:47 step: 244, epoch: 243, acc: 59.401709401709404, f1: 28.881053066943522, r: 0.3361025811357629
06/02/2019 09:45:47 *** epoch: 245 ***
06/02/2019 09:45:47 *** training ***
06/02/2019 09:45:48 step: 8057, epoch: 244, batch: 4, loss: 0.11639290302991867, acc: 95.3125, f1: 85.23033126293996, r: 0.7844341467224459
06/02/2019 09:45:49 step: 8062, epoch: 244, batch: 9, loss: 0.12328710407018661, acc: 95.3125, f1: 79.3611111111111, r: 0.6562762334754638
06/02/2019 09:45:50 step: 8067, epoch: 244, batch: 14, loss: 0.15808232128620148, acc: 92.1875, f1: 89.67032967032966, r: 0.7723669613661008
06/02/2019 09:45:51 step: 8072, epoch: 244, batch: 19, loss: 0.04922989755868912, acc: 98.4375, f1: 97.77777777777777, r: 0.6127179008027375
06/02/2019 09:45:52 step: 8077, epoch: 244, batch: 24, loss: 0.13292548060417175, acc: 95.3125, f1: 91.66222840242997, r: 0.7642617152011544
06/02/2019 09:45:54 step: 8082, epoch: 244, batch: 29, loss: 0.14508213102817535, acc: 93.75, f1: 89.38192137320044, r: 0.8069059242094335
06/02/2019 09:45:54 *** evaluating ***
06/02/2019 09:45:55 step: 245, epoch: 244, acc: 58.119658119658126, f1: 29.323381297065502, r: 0.3495385619539343
06/02/2019 09:45:55 *** epoch: 246 ***
06/02/2019 09:45:55 *** training ***
06/02/2019 09:45:56 step: 8090, epoch: 245, batch: 4, loss: 0.11545445024967194, acc: 95.3125, f1: 85.255111292443, r: 0.8039363676793432
06/02/2019 09:45:57 step: 8095, epoch: 245, batch: 9, loss: 0.0952531024813652, acc: 95.3125, f1: 79.41190269211134, r: 0.7803540432433006
06/02/2019 09:45:58 step: 8100, epoch: 245, batch: 14, loss: 0.12458612024784088, acc: 92.1875, f1: 91.40789872247673, r: 0.709247219913115
06/02/2019 09:45:59 step: 8105, epoch: 245, batch: 19, loss: 0.15780265629291534, acc: 92.1875, f1: 66.26344086021506, r: 0.7417169834845534
06/02/2019 09:46:00 step: 8110, epoch: 245, batch: 24, loss: 0.12933923304080963, acc: 93.75, f1: 82.95439819614347, r: 0.7702784509235548
06/02/2019 09:46:01 step: 8115, epoch: 245, batch: 29, loss: 0.03314506635069847, acc: 98.4375, f1: 94.28571428571428, r: 0.674449999521086
06/02/2019 09:46:02 *** evaluating ***
06/02/2019 09:46:02 step: 246, epoch: 245, acc: 56.837606837606835, f1: 28.218970137636223, r: 0.3427434890294645
06/02/2019 09:46:02 *** epoch: 247 ***
06/02/2019 09:46:02 *** training ***
06/02/2019 09:46:03 step: 8123, epoch: 246, batch: 4, loss: 0.06973884999752045, acc: 98.4375, f1: 97.8937728937729, r: 0.7280733228354599
06/02/2019 09:46:05 step: 8128, epoch: 246, batch: 9, loss: 0.0749957263469696, acc: 98.4375, f1: 98.95652173913044, r: 0.77967671232653
06/02/2019 09:46:06 step: 8133, epoch: 246, batch: 14, loss: 0.09252946823835373, acc: 98.4375, f1: 86.53846153846155, r: 0.748033250128437
06/02/2019 09:46:07 step: 8138, epoch: 246, batch: 19, loss: 0.1148785874247551, acc: 93.75, f1: 90.77908202908202, r: 0.8042608656277151
06/02/2019 09:46:08 step: 8143, epoch: 246, batch: 24, loss: 0.11358479410409927, acc: 95.3125, f1: 78.511396011396, r: 0.6520716918469276
06/02/2019 09:46:09 step: 8148, epoch: 246, batch: 29, loss: 0.13603739440441132, acc: 93.75, f1: 89.43310657596372, r: 0.7055597730556119
06/02/2019 09:46:10 *** evaluating ***
06/02/2019 09:46:10 step: 247, epoch: 246, acc: 56.837606837606835, f1: 27.83743386283063, r: 0.3330931343874142
06/02/2019 09:46:10 *** epoch: 248 ***
06/02/2019 09:46:10 *** training ***
06/02/2019 09:46:11 step: 8156, epoch: 247, batch: 4, loss: 0.15392619371414185, acc: 90.625, f1: 84.77891156462584, r: 0.6594311735375948
06/02/2019 09:46:12 step: 8161, epoch: 247, batch: 9, loss: 0.06977133452892303, acc: 98.4375, f1: 94.04761904761905, r: 0.7104898342974304
06/02/2019 09:46:13 step: 8166, epoch: 247, batch: 14, loss: 0.22024628520011902, acc: 95.3125, f1: 92.38095238095238, r: 0.7185456820615278
06/02/2019 09:46:14 step: 8171, epoch: 247, batch: 19, loss: 0.10623136162757874, acc: 95.3125, f1: 90.33100368130161, r: 0.78073019500634
06/02/2019 09:46:16 step: 8176, epoch: 247, batch: 24, loss: 0.13754186034202576, acc: 90.625, f1: 86.59007187780772, r: 0.760495776801696
06/02/2019 09:46:17 step: 8181, epoch: 247, batch: 29, loss: 0.1094430610537529, acc: 96.875, f1: 85.51587301587303, r: 0.7736865984370872
06/02/2019 09:46:17 *** evaluating ***
06/02/2019 09:46:18 step: 248, epoch: 247, acc: 56.41025641025641, f1: 27.40524272387017, r: 0.32372245566449487
06/02/2019 09:46:18 *** epoch: 249 ***
06/02/2019 09:46:18 *** training ***
06/02/2019 09:46:19 step: 8189, epoch: 248, batch: 4, loss: 0.11861324310302734, acc: 95.3125, f1: 91.13926030182188, r: 0.7319417233069705
06/02/2019 09:46:20 step: 8194, epoch: 248, batch: 9, loss: 0.1346244215965271, acc: 93.75, f1: 91.30411255411256, r: 0.786504105763817
06/02/2019 09:46:21 step: 8199, epoch: 248, batch: 14, loss: 0.08054465055465698, acc: 96.875, f1: 96.36456733230926, r: 0.6463835418557724
06/02/2019 09:46:22 step: 8204, epoch: 248, batch: 19, loss: 0.1878475695848465, acc: 90.625, f1: 80.53757167202545, r: 0.6522219833830447
06/02/2019 09:46:23 step: 8209, epoch: 248, batch: 24, loss: 0.08270465582609177, acc: 96.875, f1: 93.9888658417376, r: 0.7377740146403903
06/02/2019 09:46:24 step: 8214, epoch: 248, batch: 29, loss: 0.20855802297592163, acc: 89.0625, f1: 74.25106326422116, r: 0.6820873378854257
06/02/2019 09:46:25 *** evaluating ***
06/02/2019 09:46:25 step: 249, epoch: 248, acc: 56.837606837606835, f1: 29.082093266066, r: 0.3234076115401201
06/02/2019 09:46:25 *** epoch: 250 ***
06/02/2019 09:46:25 *** training ***
06/02/2019 09:46:26 step: 8222, epoch: 249, batch: 4, loss: 0.14940814673900604, acc: 90.625, f1: 84.3073593073593, r: 0.6496100909749412
06/02/2019 09:46:27 step: 8227, epoch: 249, batch: 9, loss: 0.100234754383564, acc: 95.3125, f1: 87.56613756613757, r: 0.6580295242228604
06/02/2019 09:46:29 step: 8232, epoch: 249, batch: 14, loss: 0.02863495983183384, acc: 100.0, f1: 100.0, r: 0.6833686631572008
06/02/2019 09:46:29 step: 8237, epoch: 249, batch: 19, loss: 0.10024071484804153, acc: 95.3125, f1: 93.98701298701299, r: 0.7468798889186887
06/02/2019 09:46:31 step: 8242, epoch: 249, batch: 24, loss: 0.11012737452983856, acc: 95.3125, f1: 82.73809523809523, r: 0.7477887192050645
06/02/2019 09:46:31 step: 8247, epoch: 249, batch: 29, loss: 0.10477877408266068, acc: 95.3125, f1: 81.75033806626098, r: 0.693717905676454
06/02/2019 09:46:32 *** evaluating ***
06/02/2019 09:46:32 step: 250, epoch: 249, acc: 59.82905982905983, f1: 28.868131253225044, r: 0.33196319559651116
06/02/2019 09:46:32 *** epoch: 251 ***
06/02/2019 09:46:32 *** training ***
06/02/2019 09:46:34 step: 8255, epoch: 250, batch: 4, loss: 0.16582706570625305, acc: 93.75, f1: 88.88235806339254, r: 0.6256488211685133
06/02/2019 09:46:35 step: 8260, epoch: 250, batch: 9, loss: 0.11601068824529648, acc: 95.3125, f1: 89.01340996168582, r: 0.8245289255913888
06/02/2019 09:46:36 step: 8265, epoch: 250, batch: 14, loss: 0.22306129336357117, acc: 90.625, f1: 73.4469696969697, r: 0.6737145696097637
06/02/2019 09:46:37 step: 8270, epoch: 250, batch: 19, loss: 0.10035042464733124, acc: 96.875, f1: 91.70068027210885, r: 0.6685827673237562
06/02/2019 09:46:38 step: 8275, epoch: 250, batch: 24, loss: 0.07364507019519806, acc: 98.4375, f1: 95.91836734693878, r: 0.6877237660142411
06/02/2019 09:46:39 step: 8280, epoch: 250, batch: 29, loss: 0.1173030287027359, acc: 93.75, f1: 77.2752009894867, r: 0.6945027224505963
06/02/2019 09:46:39 *** evaluating ***
06/02/2019 09:46:40 step: 251, epoch: 250, acc: 56.41025641025641, f1: 27.698909313855406, r: 0.3337732020838563
06/02/2019 09:46:40 *** epoch: 252 ***
06/02/2019 09:46:40 *** training ***
06/02/2019 09:46:41 step: 8288, epoch: 251, batch: 4, loss: 0.19825288653373718, acc: 90.625, f1: 87.95951262516611, r: 0.6457283213445218
06/02/2019 09:46:42 step: 8293, epoch: 251, batch: 9, loss: 0.07500288635492325, acc: 96.875, f1: 94.81783824640966, r: 0.6307606135529953
06/02/2019 09:46:43 step: 8298, epoch: 251, batch: 14, loss: 0.13076266646385193, acc: 96.875, f1: 96.73726094211268, r: 0.6938468384072536
06/02/2019 09:46:44 step: 8303, epoch: 251, batch: 19, loss: 0.1320660263299942, acc: 96.875, f1: 84.57792207792207, r: 0.7480702798383168
06/02/2019 09:46:45 step: 8308, epoch: 251, batch: 24, loss: 0.19617398083209991, acc: 90.625, f1: 84.31251288394147, r: 0.6231706475813943
06/02/2019 09:46:46 step: 8313, epoch: 251, batch: 29, loss: 0.11992176622152328, acc: 96.875, f1: 97.07633053221288, r: 0.7423046945954135
06/02/2019 09:46:47 *** evaluating ***
06/02/2019 09:46:47 step: 252, epoch: 251, acc: 56.837606837606835, f1: 28.029063300924285, r: 0.32660637329906594
06/02/2019 09:46:47 *** epoch: 253 ***
06/02/2019 09:46:47 *** training ***
06/02/2019 09:46:48 step: 8321, epoch: 252, batch: 4, loss: 0.06957272440195084, acc: 96.875, f1: 93.40676883780333, r: 0.6042990220687288
06/02/2019 09:46:49 step: 8326, epoch: 252, batch: 9, loss: 0.13601860404014587, acc: 93.75, f1: 84.37888198757764, r: 0.6939674423199046
06/02/2019 09:46:50 step: 8331, epoch: 252, batch: 14, loss: 0.07141728699207306, acc: 98.4375, f1: 95.52845528455285, r: 0.6979413086593326
06/02/2019 09:46:51 step: 8336, epoch: 252, batch: 19, loss: 0.09181363880634308, acc: 95.3125, f1: 80.65476190476191, r: 0.6586028967558948
06/02/2019 09:46:53 step: 8341, epoch: 252, batch: 24, loss: 0.1328771561384201, acc: 95.3125, f1: 96.6156462585034, r: 0.7160321621364724
06/02/2019 09:46:54 step: 8346, epoch: 252, batch: 29, loss: 0.13484740257263184, acc: 95.3125, f1: 95.19300144300145, r: 0.7900680127168012
06/02/2019 09:46:54 *** evaluating ***
06/02/2019 09:46:55 step: 253, epoch: 252, acc: 56.837606837606835, f1: 27.348450754406073, r: 0.3360026282873928
06/02/2019 09:46:55 *** epoch: 254 ***
06/02/2019 09:46:55 *** training ***
06/02/2019 09:46:56 step: 8354, epoch: 253, batch: 4, loss: 0.15819130837917328, acc: 93.75, f1: 91.36813186813187, r: 0.769086225786245
06/02/2019 09:46:57 step: 8359, epoch: 253, batch: 9, loss: 0.15342988073825836, acc: 100.0, f1: 100.0, r: 0.6651694812206802
06/02/2019 09:46:58 step: 8364, epoch: 253, batch: 14, loss: 0.07523643970489502, acc: 96.875, f1: 95.95368916797489, r: 0.6587007949876231
06/02/2019 09:46:59 step: 8369, epoch: 253, batch: 19, loss: 0.23147352039813995, acc: 89.0625, f1: 82.07275953859804, r: 0.6248830042811393
06/02/2019 09:47:00 step: 8374, epoch: 253, batch: 24, loss: 0.08659125119447708, acc: 95.3125, f1: 92.20785440613027, r: 0.7945172597170953
06/02/2019 09:47:01 step: 8379, epoch: 253, batch: 29, loss: 0.10969993472099304, acc: 96.875, f1: 93.76909384443432, r: 0.6133922155603813
06/02/2019 09:47:02 *** evaluating ***
06/02/2019 09:47:02 step: 254, epoch: 253, acc: 56.837606837606835, f1: 26.488088118022336, r: 0.32571094810550244
06/02/2019 09:47:02 *** epoch: 255 ***
06/02/2019 09:47:02 *** training ***
06/02/2019 09:47:03 step: 8387, epoch: 254, batch: 4, loss: 0.23803368210792542, acc: 92.1875, f1: 77.25694444444444, r: 0.6559473543135906
06/02/2019 09:47:05 step: 8392, epoch: 254, batch: 9, loss: 0.06140752136707306, acc: 98.4375, f1: 96.1111111111111, r: 0.8268528204296799
06/02/2019 09:47:06 step: 8397, epoch: 254, batch: 14, loss: 0.0740545392036438, acc: 96.875, f1: 90.73584863058547, r: 0.5928615158182737
06/02/2019 09:47:07 step: 8402, epoch: 254, batch: 19, loss: 0.07646167278289795, acc: 95.3125, f1: 84.67261904761905, r: 0.7135629604194202
06/02/2019 09:47:08 step: 8407, epoch: 254, batch: 24, loss: 0.13504506647586823, acc: 92.1875, f1: 75.50617925617927, r: 0.7067974473384662
06/02/2019 09:47:09 step: 8412, epoch: 254, batch: 29, loss: 0.1817636936903, acc: 90.625, f1: 72.16314084238611, r: 0.5945257187234275
06/02/2019 09:47:10 *** evaluating ***
06/02/2019 09:47:10 step: 255, epoch: 254, acc: 55.12820512820513, f1: 27.591281267751853, r: 0.3201503219322891
06/02/2019 09:47:10 *** epoch: 256 ***
06/02/2019 09:47:10 *** training ***
06/02/2019 09:47:11 step: 8420, epoch: 255, batch: 4, loss: 0.1008673682808876, acc: 95.3125, f1: 89.75414078674947, r: 0.7318094945718574
06/02/2019 09:47:12 step: 8425, epoch: 255, batch: 9, loss: 0.10735826939344406, acc: 95.3125, f1: 95.90570719602978, r: 0.7340172669345962
06/02/2019 09:47:14 step: 8430, epoch: 255, batch: 14, loss: 0.17234636843204498, acc: 93.75, f1: 88.49203525429242, r: 0.5861928255891328
06/02/2019 09:47:15 step: 8435, epoch: 255, batch: 19, loss: 0.03273613750934601, acc: 100.0, f1: 100.0, r: 0.8187877569835535
06/02/2019 09:47:16 step: 8440, epoch: 255, batch: 24, loss: 0.09421184659004211, acc: 96.875, f1: 95.17080745341615, r: 0.7790755464626422
06/02/2019 09:47:17 step: 8445, epoch: 255, batch: 29, loss: 0.10554502159357071, acc: 96.875, f1: 93.38202170500305, r: 0.7456638256863407
06/02/2019 09:47:17 *** evaluating ***
06/02/2019 09:47:18 step: 256, epoch: 255, acc: 59.82905982905983, f1: 29.459545057179092, r: 0.3281623797988799
06/02/2019 09:47:18 *** epoch: 257 ***
06/02/2019 09:47:18 *** training ***
06/02/2019 09:47:19 step: 8453, epoch: 256, batch: 4, loss: 0.09842134267091751, acc: 95.3125, f1: 78.56784819190834, r: 0.7405390437261853
06/02/2019 09:47:20 step: 8458, epoch: 256, batch: 9, loss: 0.14215366542339325, acc: 95.3125, f1: 88.26636904761904, r: 0.7240773602155955
06/02/2019 09:47:21 step: 8463, epoch: 256, batch: 14, loss: 0.09524593502283096, acc: 98.4375, f1: 97.6608187134503, r: 0.6127724266818786
06/02/2019 09:47:22 step: 8468, epoch: 256, batch: 19, loss: 0.05817393213510513, acc: 98.4375, f1: 87.0, r: 0.7202848104321975
06/02/2019 09:47:23 step: 8473, epoch: 256, batch: 24, loss: 0.0969579666852951, acc: 96.875, f1: 91.6971916971917, r: 0.7091680030080384
06/02/2019 09:47:24 step: 8478, epoch: 256, batch: 29, loss: 0.10523328930139542, acc: 95.3125, f1: 91.65096807953951, r: 0.664914191935143
06/02/2019 09:47:25 *** evaluating ***
06/02/2019 09:47:25 step: 257, epoch: 256, acc: 58.54700854700855, f1: 28.235139264551023, r: 0.32758688413695647
06/02/2019 09:47:25 *** epoch: 258 ***
06/02/2019 09:47:25 *** training ***
06/02/2019 09:47:27 step: 8486, epoch: 257, batch: 4, loss: 0.08474593609571457, acc: 95.3125, f1: 85.03514503514504, r: 0.5739500263403989
06/02/2019 09:47:28 step: 8491, epoch: 257, batch: 9, loss: 0.18184657394886017, acc: 93.75, f1: 78.54043353063508, r: 0.7737442164533178
06/02/2019 09:47:29 step: 8496, epoch: 257, batch: 14, loss: 0.07737638056278229, acc: 98.4375, f1: 97.46657283603096, r: 0.6937746076756127
06/02/2019 09:47:30 step: 8501, epoch: 257, batch: 19, loss: 0.0741250291466713, acc: 96.875, f1: 95.12295081967214, r: 0.6380629416402853
06/02/2019 09:47:31 step: 8506, epoch: 257, batch: 24, loss: 0.15874417126178741, acc: 92.1875, f1: 75.59149184149183, r: 0.6720564220934466
06/02/2019 09:47:32 step: 8511, epoch: 257, batch: 29, loss: 0.0883452445268631, acc: 98.4375, f1: 95.0, r: 0.7282004052947262
06/02/2019 09:47:33 *** evaluating ***
06/02/2019 09:47:33 step: 258, epoch: 257, acc: 57.26495726495726, f1: 28.08328844956387, r: 0.3297179017869498
06/02/2019 09:47:33 *** epoch: 259 ***
06/02/2019 09:47:33 *** training ***
06/02/2019 09:47:34 step: 8519, epoch: 258, batch: 4, loss: 0.08049343526363373, acc: 96.875, f1: 94.80990274093722, r: 0.5370358315791356
06/02/2019 09:47:35 step: 8524, epoch: 258, batch: 9, loss: 0.12706220149993896, acc: 95.3125, f1: 73.76186301522362, r: 0.6401322454996967
06/02/2019 09:47:36 step: 8529, epoch: 258, batch: 14, loss: 0.05815086513757706, acc: 100.0, f1: 100.0, r: 0.7571485698819023
06/02/2019 09:47:37 step: 8534, epoch: 258, batch: 19, loss: 0.020187057554721832, acc: 100.0, f1: 100.0, r: 0.6639109519177769
06/02/2019 09:47:38 step: 8539, epoch: 258, batch: 24, loss: 0.06502367556095123, acc: 98.4375, f1: 99.32234432234432, r: 0.7547274306446455
06/02/2019 09:47:39 step: 8544, epoch: 258, batch: 29, loss: 0.10622311383485794, acc: 96.875, f1: 96.09628466771323, r: 0.718248137642743
06/02/2019 09:47:40 *** evaluating ***
06/02/2019 09:47:40 step: 259, epoch: 258, acc: 57.692307692307686, f1: 27.736118370793296, r: 0.32388124067230106
06/02/2019 09:47:41 *** epoch: 260 ***
06/02/2019 09:47:41 *** training ***
06/02/2019 09:47:42 step: 8552, epoch: 259, batch: 4, loss: 0.15563206374645233, acc: 93.75, f1: 67.36453201970444, r: 0.6051681389231223
06/02/2019 09:47:43 step: 8557, epoch: 259, batch: 9, loss: 0.14083082973957062, acc: 93.75, f1: 86.88803949673515, r: 0.6898870006671595
06/02/2019 09:47:44 step: 8562, epoch: 259, batch: 14, loss: 0.1858256459236145, acc: 92.1875, f1: 80.80357142857143, r: 0.63797704158596
06/02/2019 09:47:45 step: 8567, epoch: 259, batch: 19, loss: 0.11485421657562256, acc: 96.875, f1: 86.66666666666667, r: 0.7327145286854907
06/02/2019 09:47:46 step: 8572, epoch: 259, batch: 24, loss: 0.07974003255367279, acc: 96.875, f1: 94.55782312925169, r: 0.6753912918931986
06/02/2019 09:47:47 step: 8577, epoch: 259, batch: 29, loss: 0.0997677743434906, acc: 96.875, f1: 94.26839826839827, r: 0.6307214529416986
06/02/2019 09:47:48 *** evaluating ***
06/02/2019 09:47:48 step: 260, epoch: 259, acc: 58.54700854700855, f1: 28.713953079670894, r: 0.3311637880367745
06/02/2019 09:47:48 *** epoch: 261 ***
06/02/2019 09:47:48 *** training ***
06/02/2019 09:47:49 step: 8585, epoch: 260, batch: 4, loss: 0.057037461549043655, acc: 98.4375, f1: 95.17543859649122, r: 0.7988150942102618
06/02/2019 09:47:50 step: 8590, epoch: 260, batch: 9, loss: 0.11175338178873062, acc: 95.3125, f1: 94.02905484538138, r: 0.646512200035449
06/02/2019 09:47:51 step: 8595, epoch: 260, batch: 14, loss: 0.16951815783977509, acc: 89.0625, f1: 86.89484126984127, r: 0.6902293232420299
06/02/2019 09:47:52 step: 8600, epoch: 260, batch: 19, loss: 0.13905496895313263, acc: 92.1875, f1: 92.94092603303129, r: 0.7722223277479382
06/02/2019 09:47:53 step: 8605, epoch: 260, batch: 24, loss: 0.18385879695415497, acc: 92.1875, f1: 88.00396825396825, r: 0.7789981458590189
06/02/2019 09:47:54 step: 8610, epoch: 260, batch: 29, loss: 0.05656401813030243, acc: 98.4375, f1: 99.36507936507937, r: 0.7850608947000292
06/02/2019 09:47:55 *** evaluating ***
06/02/2019 09:47:55 step: 261, epoch: 260, acc: 58.119658119658126, f1: 28.809330986304833, r: 0.3228149105776086
06/02/2019 09:47:55 *** epoch: 262 ***
06/02/2019 09:47:55 *** training ***
06/02/2019 09:47:56 step: 8618, epoch: 261, batch: 4, loss: 0.04838970676064491, acc: 98.4375, f1: 96.65024630541872, r: 0.7716085360619972
06/02/2019 09:47:58 step: 8623, epoch: 261, batch: 9, loss: 0.1154404878616333, acc: 93.75, f1: 90.9064940227731, r: 0.7584083682525001
06/02/2019 09:47:59 step: 8628, epoch: 261, batch: 14, loss: 0.06734316796064377, acc: 98.4375, f1: 97.07792207792207, r: 0.7489182592910026
06/02/2019 09:48:00 step: 8633, epoch: 261, batch: 19, loss: 0.13575071096420288, acc: 98.4375, f1: 97.6023976023976, r: 0.7072943238424921
06/02/2019 09:48:01 step: 8638, epoch: 261, batch: 24, loss: 0.13572978973388672, acc: 95.3125, f1: 90.10808270676691, r: 0.6969060886965226
06/02/2019 09:48:02 step: 8643, epoch: 261, batch: 29, loss: 0.053780872374773026, acc: 98.4375, f1: 98.92156862745098, r: 0.720817227604951
06/02/2019 09:48:03 *** evaluating ***
06/02/2019 09:48:03 step: 262, epoch: 261, acc: 55.55555555555556, f1: 28.02535155476332, r: 0.3230740512611287
06/02/2019 09:48:03 *** epoch: 263 ***
06/02/2019 09:48:03 *** training ***
06/02/2019 09:48:04 step: 8651, epoch: 262, batch: 4, loss: 0.09432725608348846, acc: 95.3125, f1: 87.09242317616702, r: 0.5810390457608287
06/02/2019 09:48:05 step: 8656, epoch: 262, batch: 9, loss: 0.1119510754942894, acc: 95.3125, f1: 85.12432012432012, r: 0.7707975957851637
06/02/2019 09:48:06 step: 8661, epoch: 262, batch: 14, loss: 0.07238712906837463, acc: 96.875, f1: 91.56565656565657, r: 0.7591237647132929
06/02/2019 09:48:07 step: 8666, epoch: 262, batch: 19, loss: 0.07117178291082382, acc: 98.4375, f1: 96.85131195335276, r: 0.6024327774482732
06/02/2019 09:48:09 step: 8671, epoch: 262, batch: 24, loss: 0.09357621520757675, acc: 96.875, f1: 96.89281117852548, r: 0.7001432555489696
06/02/2019 09:48:10 step: 8676, epoch: 262, batch: 29, loss: 0.10757526755332947, acc: 95.3125, f1: 96.66053921568628, r: 0.7831187836148961
06/02/2019 09:48:10 *** evaluating ***
06/02/2019 09:48:11 step: 263, epoch: 262, acc: 57.692307692307686, f1: 29.005788689060168, r: 0.3286471808127737
06/02/2019 09:48:11 *** epoch: 264 ***
06/02/2019 09:48:11 *** training ***
06/02/2019 09:48:12 step: 8684, epoch: 263, batch: 4, loss: 0.13082027435302734, acc: 93.75, f1: 81.06990068754774, r: 0.7706425402129125
06/02/2019 09:48:13 step: 8689, epoch: 263, batch: 9, loss: 0.04527809098362923, acc: 100.0, f1: 100.0, r: 0.7228787509485161
06/02/2019 09:48:14 step: 8694, epoch: 263, batch: 14, loss: 0.08603512495756149, acc: 98.4375, f1: 96.52173913043478, r: 0.6245191823270373
06/02/2019 09:48:15 step: 8699, epoch: 263, batch: 19, loss: 0.24922960996627808, acc: 93.75, f1: 76.88242784380306, r: 0.7069756038447385
06/02/2019 09:48:16 step: 8704, epoch: 263, batch: 24, loss: 0.059938207268714905, acc: 98.4375, f1: 92.38095238095238, r: 0.6125661464456631
06/02/2019 09:48:17 step: 8709, epoch: 263, batch: 29, loss: 0.040242742747068405, acc: 98.4375, f1: 95.28985507246377, r: 0.7501326473366651
06/02/2019 09:48:18 *** evaluating ***
06/02/2019 09:48:19 step: 264, epoch: 263, acc: 58.119658119658126, f1: 28.133280805833643, r: 0.3295586781346337
06/02/2019 09:48:19 *** epoch: 265 ***
06/02/2019 09:48:19 *** training ***
06/02/2019 09:48:20 step: 8717, epoch: 264, batch: 4, loss: 0.07915494590997696, acc: 96.875, f1: 91.66666666666666, r: 0.7089338729646886
06/02/2019 09:48:21 step: 8722, epoch: 264, batch: 9, loss: 0.23581121861934662, acc: 95.3125, f1: 83.84343434343434, r: 0.6675114396788802
06/02/2019 09:48:22 step: 8727, epoch: 264, batch: 14, loss: 0.0873020812869072, acc: 96.875, f1: 90.92261904761905, r: 0.7600017050022889
06/02/2019 09:48:23 step: 8732, epoch: 264, batch: 19, loss: 0.10722654312849045, acc: 95.3125, f1: 90.04658467160448, r: 0.6871706147478917
06/02/2019 09:48:24 step: 8737, epoch: 264, batch: 24, loss: 0.07983115315437317, acc: 96.875, f1: 93.96825396825396, r: 0.708860480055299
06/02/2019 09:48:26 step: 8742, epoch: 264, batch: 29, loss: 0.029717370867729187, acc: 100.0, f1: 100.0, r: 0.7034856214667445
06/02/2019 09:48:26 *** evaluating ***
06/02/2019 09:48:27 step: 265, epoch: 264, acc: 57.692307692307686, f1: 28.827797998373168, r: 0.32433025084463296
06/02/2019 09:48:27 *** epoch: 266 ***
06/02/2019 09:48:27 *** training ***
06/02/2019 09:48:28 step: 8750, epoch: 265, batch: 4, loss: 0.08594287186861038, acc: 96.875, f1: 97.91176854754441, r: 0.6976404181054585
06/02/2019 09:48:29 step: 8755, epoch: 265, batch: 9, loss: 0.1630157083272934, acc: 93.75, f1: 89.95330598081782, r: 0.7114725822427398
06/02/2019 09:48:30 step: 8760, epoch: 265, batch: 14, loss: 0.12620334327220917, acc: 93.75, f1: 91.94726598453306, r: 0.739570508993399
06/02/2019 09:48:31 step: 8765, epoch: 265, batch: 19, loss: 0.048934705555438995, acc: 100.0, f1: 100.0, r: 0.7341064517635352
06/02/2019 09:48:32 step: 8770, epoch: 265, batch: 24, loss: 0.10008590668439865, acc: 96.875, f1: 95.76985943118666, r: 0.7462581830942683
06/02/2019 09:48:34 step: 8775, epoch: 265, batch: 29, loss: 0.07843860983848572, acc: 98.4375, f1: 83.33333333333333, r: 0.6201024688705628
06/02/2019 09:48:34 *** evaluating ***
06/02/2019 09:48:35 step: 266, epoch: 265, acc: 58.54700854700855, f1: 28.489480898742475, r: 0.32286992892118976
06/02/2019 09:48:35 *** epoch: 267 ***
06/02/2019 09:48:35 *** training ***
06/02/2019 09:48:36 step: 8783, epoch: 266, batch: 4, loss: 0.1298954337835312, acc: 95.3125, f1: 87.6533189033189, r: 0.7742240870497088
06/02/2019 09:48:37 step: 8788, epoch: 266, batch: 9, loss: 0.023363379761576653, acc: 100.0, f1: 100.0, r: 0.6424982721923084
06/02/2019 09:48:38 step: 8793, epoch: 266, batch: 14, loss: 0.0734674260020256, acc: 96.875, f1: 89.20634920634922, r: 0.6782987295265199
06/02/2019 09:48:39 step: 8798, epoch: 266, batch: 19, loss: 0.13329365849494934, acc: 96.875, f1: 85.51587301587303, r: 0.5797812038551882
06/02/2019 09:48:40 step: 8803, epoch: 266, batch: 24, loss: 0.06428002566099167, acc: 96.875, f1: 96.90846761627162, r: 0.7161660314560716
06/02/2019 09:48:41 step: 8808, epoch: 266, batch: 29, loss: 0.06776906549930573, acc: 95.3125, f1: 80.66790352504638, r: 0.5875788788298055
06/02/2019 09:48:42 *** evaluating ***
06/02/2019 09:48:42 step: 267, epoch: 266, acc: 54.700854700854705, f1: 27.419529112729325, r: 0.3188158872895643
06/02/2019 09:48:42 *** epoch: 268 ***
06/02/2019 09:48:42 *** training ***
06/02/2019 09:48:43 step: 8816, epoch: 267, batch: 4, loss: 0.27978572249412537, acc: 95.3125, f1: 92.3699874686717, r: 0.7501764857208418
06/02/2019 09:48:44 step: 8821, epoch: 267, batch: 9, loss: 0.06624594330787659, acc: 98.4375, f1: 98.44322344322345, r: 0.7802688187902994
06/02/2019 09:48:46 step: 8826, epoch: 267, batch: 14, loss: 0.11313283443450928, acc: 96.875, f1: 94.79166666666666, r: 0.8151624663851279
06/02/2019 09:48:47 step: 8831, epoch: 267, batch: 19, loss: 0.04655031859874725, acc: 98.4375, f1: 96.3718820861678, r: 0.6732395208922176
06/02/2019 09:48:48 step: 8836, epoch: 267, batch: 24, loss: 0.13195529580116272, acc: 95.3125, f1: 71.64772727272728, r: 0.6396881301311347
06/02/2019 09:48:49 step: 8841, epoch: 267, batch: 29, loss: 0.0663951113820076, acc: 98.4375, f1: 93.19727891156461, r: 0.692122422432413
06/02/2019 09:48:49 *** evaluating ***
06/02/2019 09:48:50 step: 268, epoch: 267, acc: 56.41025641025641, f1: 28.250829838480296, r: 0.32419157063002746
06/02/2019 09:48:50 *** epoch: 269 ***
06/02/2019 09:48:50 *** training ***
06/02/2019 09:48:51 step: 8849, epoch: 268, batch: 4, loss: 0.07164999842643738, acc: 98.4375, f1: 93.19727891156462, r: 0.6361998481148934
06/02/2019 09:48:52 step: 8854, epoch: 268, batch: 9, loss: 0.16701117157936096, acc: 92.1875, f1: 87.79644432051718, r: 0.7812902195029564
06/02/2019 09:48:53 step: 8859, epoch: 268, batch: 14, loss: 0.3052836060523987, acc: 98.4375, f1: 95.28985507246377, r: 0.7969122009946192
06/02/2019 09:48:54 step: 8864, epoch: 268, batch: 19, loss: 0.07148449867963791, acc: 98.4375, f1: 95.09803921568627, r: 0.7793900649064328
06/02/2019 09:48:55 step: 8869, epoch: 268, batch: 24, loss: 0.0840546116232872, acc: 96.875, f1: 95.87301587301589, r: 0.6370936410474962
06/02/2019 09:48:57 step: 8874, epoch: 268, batch: 29, loss: 0.03407793492078781, acc: 98.4375, f1: 97.33806566104703, r: 0.7103049004576067
06/02/2019 09:48:57 *** evaluating ***
06/02/2019 09:48:58 step: 269, epoch: 268, acc: 56.41025641025641, f1: 26.695477986912287, r: 0.3120857467194862
06/02/2019 09:48:58 *** epoch: 270 ***
06/02/2019 09:48:58 *** training ***
06/02/2019 09:48:59 step: 8882, epoch: 269, batch: 4, loss: 0.04594185948371887, acc: 98.4375, f1: 82.05128205128206, r: 0.5749658125343107
06/02/2019 09:49:00 step: 8887, epoch: 269, batch: 9, loss: 0.06700360774993896, acc: 98.4375, f1: 96.1111111111111, r: 0.7954508723734025
06/02/2019 09:49:01 step: 8892, epoch: 269, batch: 14, loss: 0.19040139019489288, acc: 96.875, f1: 95.8274304778864, r: 0.7646624055979865
06/02/2019 09:49:02 step: 8897, epoch: 269, batch: 19, loss: 0.0859101191163063, acc: 95.3125, f1: 93.55943937858832, r: 0.7941592103269217
06/02/2019 09:49:03 step: 8902, epoch: 269, batch: 24, loss: 0.10417159646749496, acc: 95.3125, f1: 92.30079509117621, r: 0.714839598793204
06/02/2019 09:49:04 step: 8907, epoch: 269, batch: 29, loss: 0.19038446247577667, acc: 93.75, f1: 90.28901161767294, r: 0.7719568068549588
06/02/2019 09:49:05 *** evaluating ***
06/02/2019 09:49:05 step: 270, epoch: 269, acc: 55.98290598290598, f1: 27.91361192813212, r: 0.32271541509039475
06/02/2019 09:49:05 *** epoch: 271 ***
06/02/2019 09:49:05 *** training ***
06/02/2019 09:49:06 step: 8915, epoch: 270, batch: 4, loss: 0.2189411222934723, acc: 95.3125, f1: 93.61111111111111, r: 0.7816296515464471
06/02/2019 09:49:08 step: 8920, epoch: 270, batch: 9, loss: 0.1152493804693222, acc: 93.75, f1: 83.74406366320113, r: 0.6276188210381429
06/02/2019 09:49:09 step: 8925, epoch: 270, batch: 14, loss: 0.2109672725200653, acc: 96.875, f1: 98.26465201465201, r: 0.7206131221969732
06/02/2019 09:49:10 step: 8930, epoch: 270, batch: 19, loss: 0.11910092085599899, acc: 98.4375, f1: 97.89377289377289, r: 0.7482353546257856
06/02/2019 09:49:11 step: 8935, epoch: 270, batch: 24, loss: 0.20667168498039246, acc: 92.1875, f1: 84.48861717478738, r: 0.6952637854260239
06/02/2019 09:49:12 step: 8940, epoch: 270, batch: 29, loss: 0.07443990558385849, acc: 98.4375, f1: 96.39097744360903, r: 0.6508106985636053
06/02/2019 09:49:13 *** evaluating ***
06/02/2019 09:49:13 step: 271, epoch: 270, acc: 57.26495726495726, f1: 28.314151747655586, r: 0.32330506061026054
06/02/2019 09:49:13 *** epoch: 272 ***
06/02/2019 09:49:13 *** training ***
06/02/2019 09:49:14 step: 8948, epoch: 271, batch: 4, loss: 0.08466599881649017, acc: 98.4375, f1: 98.53846153846153, r: 0.8093874317552499
06/02/2019 09:49:15 step: 8953, epoch: 271, batch: 9, loss: 0.07352593541145325, acc: 96.875, f1: 80.41125541125541, r: 0.7262678685821424
06/02/2019 09:49:17 step: 8958, epoch: 271, batch: 14, loss: 0.15418033301830292, acc: 93.75, f1: 78.06987903173301, r: 0.6288399895485978
06/02/2019 09:49:18 step: 8963, epoch: 271, batch: 19, loss: 0.13568396866321564, acc: 93.75, f1: 92.23960535588442, r: 0.7121124618271043
06/02/2019 09:49:19 step: 8968, epoch: 271, batch: 24, loss: 0.13903947174549103, acc: 93.75, f1: 82.12928921568627, r: 0.7297212467117417
06/02/2019 09:49:20 step: 8973, epoch: 271, batch: 29, loss: 0.1647215485572815, acc: 98.4375, f1: 96.73469387755101, r: 0.6500492469098557
06/02/2019 09:49:20 *** evaluating ***
06/02/2019 09:49:21 step: 272, epoch: 271, acc: 56.837606837606835, f1: 27.971218807889286, r: 0.3297011829508647
06/02/2019 09:49:21 *** epoch: 273 ***
06/02/2019 09:49:21 *** training ***
06/02/2019 09:49:22 step: 8981, epoch: 272, batch: 4, loss: 0.13362088799476624, acc: 95.3125, f1: 95.56060694371587, r: 0.6567255443427802
06/02/2019 09:49:23 step: 8986, epoch: 272, batch: 9, loss: 0.04013346880674362, acc: 100.0, f1: 100.0, r: 0.803720290673335
06/02/2019 09:49:24 step: 8991, epoch: 272, batch: 14, loss: 0.04856770485639572, acc: 98.4375, f1: 97.92008757525998, r: 0.65796116947159
06/02/2019 09:49:25 step: 8996, epoch: 272, batch: 19, loss: 0.07436233758926392, acc: 98.4375, f1: 97.87581699346406, r: 0.7658707256907756
06/02/2019 09:49:26 step: 9001, epoch: 272, batch: 24, loss: 0.28524547815322876, acc: 90.625, f1: 83.68429189857761, r: 0.6424441563307781
06/02/2019 09:49:27 step: 9006, epoch: 272, batch: 29, loss: 0.2346435785293579, acc: 90.625, f1: 77.63236075929574, r: 0.5788085112315531
06/02/2019 09:49:28 *** evaluating ***
06/02/2019 09:49:28 step: 273, epoch: 272, acc: 56.41025641025641, f1: 28.070106722743688, r: 0.3250818588131171
06/02/2019 09:49:28 *** epoch: 274 ***
06/02/2019 09:49:28 *** training ***
06/02/2019 09:49:29 step: 9014, epoch: 273, batch: 4, loss: 0.05167457461357117, acc: 98.4375, f1: 98.27998088867655, r: 0.722132501216034
06/02/2019 09:49:30 step: 9019, epoch: 273, batch: 9, loss: 0.09176808595657349, acc: 96.875, f1: 92.62452107279692, r: 0.7429126215594081
06/02/2019 09:49:31 step: 9024, epoch: 273, batch: 14, loss: 0.08897752314805984, acc: 96.875, f1: 97.83728832442068, r: 0.7625408756039107
06/02/2019 09:49:33 step: 9029, epoch: 273, batch: 19, loss: 0.0813215970993042, acc: 98.4375, f1: 97.55639097744361, r: 0.7917233745082097
06/02/2019 09:49:34 step: 9034, epoch: 273, batch: 24, loss: 0.09535970538854599, acc: 98.4375, f1: 95.23809523809523, r: 0.7729760595909527
06/02/2019 09:49:35 step: 9039, epoch: 273, batch: 29, loss: 0.24546486139297485, acc: 93.75, f1: 76.30508807296505, r: 0.6456319986510023
06/02/2019 09:49:35 *** evaluating ***
06/02/2019 09:49:36 step: 274, epoch: 273, acc: 57.692307692307686, f1: 27.36728527169704, r: 0.3207618639883493
06/02/2019 09:49:36 *** epoch: 275 ***
06/02/2019 09:49:36 *** training ***
06/02/2019 09:49:37 step: 9047, epoch: 274, batch: 4, loss: 0.1630491465330124, acc: 96.875, f1: 94.46115288220551, r: 0.6969880763796737
06/02/2019 09:49:38 step: 9052, epoch: 274, batch: 9, loss: 0.05192827060818672, acc: 98.4375, f1: 86.11111111111111, r: 0.7788235486425077
06/02/2019 09:49:39 step: 9057, epoch: 274, batch: 14, loss: 0.026660045608878136, acc: 100.0, f1: 100.0, r: 0.6897480138848435
06/02/2019 09:49:40 step: 9062, epoch: 274, batch: 19, loss: 0.1431209295988083, acc: 92.1875, f1: 65.07207078635649, r: 0.6339069610232909
06/02/2019 09:49:41 step: 9067, epoch: 274, batch: 24, loss: 0.05587964132428169, acc: 98.4375, f1: 97.1139971139971, r: 0.6169417691210177
06/02/2019 09:49:42 step: 9072, epoch: 274, batch: 29, loss: 0.11499221622943878, acc: 96.875, f1: 95.16388373531231, r: 0.6275005127735543
06/02/2019 09:49:43 *** evaluating ***
06/02/2019 09:49:43 step: 275, epoch: 274, acc: 55.55555555555556, f1: 27.072454258146482, r: 0.30771387202332773
06/02/2019 09:49:43 *** epoch: 276 ***
06/02/2019 09:49:43 *** training ***
06/02/2019 09:49:44 step: 9080, epoch: 275, batch: 4, loss: 0.14293915033340454, acc: 96.875, f1: 93.078231292517, r: 0.7397714603660533
06/02/2019 09:49:46 step: 9085, epoch: 275, batch: 9, loss: 0.14383211731910706, acc: 95.3125, f1: 95.35014005602241, r: 0.6757096593664896
06/02/2019 09:49:47 step: 9090, epoch: 275, batch: 14, loss: 0.12305346876382828, acc: 93.75, f1: 82.96207264957265, r: 0.6238174559916055
06/02/2019 09:49:48 step: 9095, epoch: 275, batch: 19, loss: 0.1080646887421608, acc: 93.75, f1: 88.86167800453516, r: 0.7044883980312585
06/02/2019 09:49:49 step: 9100, epoch: 275, batch: 24, loss: 0.037226174026727676, acc: 98.4375, f1: 99.0559186637618, r: 0.636464642909577
06/02/2019 09:49:50 step: 9105, epoch: 275, batch: 29, loss: 0.1132906824350357, acc: 96.875, f1: 83.11111111111111, r: 0.7859556377759287
06/02/2019 09:49:51 *** evaluating ***
06/02/2019 09:49:51 step: 276, epoch: 275, acc: 58.119658119658126, f1: 28.34717622891928, r: 0.325964531051323
06/02/2019 09:49:51 *** epoch: 277 ***
06/02/2019 09:49:51 *** training ***
06/02/2019 09:49:52 step: 9113, epoch: 276, batch: 4, loss: 0.11118530482053757, acc: 95.3125, f1: 92.43645243645246, r: 0.6825437497501725
06/02/2019 09:49:53 step: 9118, epoch: 276, batch: 9, loss: 0.06726273149251938, acc: 98.4375, f1: 85.0, r: 0.7667428565768115
06/02/2019 09:49:55 step: 9123, epoch: 276, batch: 14, loss: 0.09186265617609024, acc: 96.875, f1: 90.58608058608058, r: 0.8201972906786232
06/02/2019 09:49:56 step: 9128, epoch: 276, batch: 19, loss: 0.04528040811419487, acc: 98.4375, f1: 85.71428571428572, r: 0.6355075955291638
06/02/2019 09:49:57 step: 9133, epoch: 276, batch: 24, loss: 0.16232505440711975, acc: 95.3125, f1: 70.10421189752498, r: 0.5653306163755322
06/02/2019 09:49:58 step: 9138, epoch: 276, batch: 29, loss: 0.13471896946430206, acc: 95.3125, f1: 82.169869425967, r: 0.6772390891476493
06/02/2019 09:49:59 *** evaluating ***
06/02/2019 09:49:59 step: 277, epoch: 276, acc: 54.27350427350427, f1: 26.690536306265628, r: 0.3089022985596193
06/02/2019 09:49:59 *** epoch: 278 ***
06/02/2019 09:49:59 *** training ***
06/02/2019 09:50:00 step: 9146, epoch: 277, batch: 4, loss: 0.06149344518780708, acc: 96.875, f1: 83.86128364389234, r: 0.8004428971095867
06/02/2019 09:50:01 step: 9151, epoch: 277, batch: 9, loss: 0.07550977915525436, acc: 96.875, f1: 91.61765981438113, r: 0.6153548458428842
06/02/2019 09:50:02 step: 9156, epoch: 277, batch: 14, loss: 0.0815119594335556, acc: 96.875, f1: 96.63352520495377, r: 0.6278825059124754
06/02/2019 09:50:03 step: 9161, epoch: 277, batch: 19, loss: 0.08295636624097824, acc: 95.3125, f1: 81.6653117670694, r: 0.6841633700932548
06/02/2019 09:50:04 step: 9166, epoch: 277, batch: 24, loss: 0.06754061579704285, acc: 96.875, f1: 96.28848149429263, r: 0.6617933981094601
06/02/2019 09:50:05 step: 9171, epoch: 277, batch: 29, loss: 0.09669439494609833, acc: 100.0, f1: 100.0, r: 0.7303906425523606
06/02/2019 09:50:06 *** evaluating ***
06/02/2019 09:50:06 step: 278, epoch: 277, acc: 56.837606837606835, f1: 28.6366730917799, r: 0.3212333278656182
06/02/2019 09:50:06 *** epoch: 279 ***
06/02/2019 09:50:06 *** training ***
06/02/2019 09:50:07 step: 9179, epoch: 278, batch: 4, loss: 0.09616521000862122, acc: 98.4375, f1: 98.81521986785145, r: 0.7151656433510178
06/02/2019 09:50:08 step: 9184, epoch: 278, batch: 9, loss: 0.12763814628124237, acc: 98.4375, f1: 86.53846153846155, r: 0.7280225855391729
06/02/2019 09:50:10 step: 9189, epoch: 278, batch: 14, loss: 0.09161794185638428, acc: 95.3125, f1: 80.94774273345703, r: 0.6252633809746329
06/02/2019 09:50:11 step: 9194, epoch: 278, batch: 19, loss: 0.03007359802722931, acc: 100.0, f1: 100.0, r: 0.7131542739349801
06/02/2019 09:50:12 step: 9199, epoch: 278, batch: 24, loss: 0.10741017758846283, acc: 95.3125, f1: 80.42874396135265, r: 0.7298601163638033
06/02/2019 09:50:13 step: 9204, epoch: 278, batch: 29, loss: 0.09577243030071259, acc: 95.3125, f1: 89.0377943569433, r: 0.7798556550176902
06/02/2019 09:50:13 *** evaluating ***
06/02/2019 09:50:14 step: 279, epoch: 278, acc: 55.98290598290598, f1: 27.720356201594964, r: 0.3176752298242945
06/02/2019 09:50:14 *** epoch: 280 ***
06/02/2019 09:50:14 *** training ***
06/02/2019 09:50:15 step: 9212, epoch: 279, batch: 4, loss: 0.05737699940800667, acc: 96.875, f1: 96.35416666666666, r: 0.8237075978607566
06/02/2019 09:50:16 step: 9217, epoch: 279, batch: 9, loss: 0.07886067032814026, acc: 95.3125, f1: 84.6108942384154, r: 0.6234053913035742
06/02/2019 09:50:17 step: 9222, epoch: 279, batch: 14, loss: 0.08715415745973587, acc: 98.4375, f1: 86.53846153846155, r: 0.6410007258058981
06/02/2019 09:50:18 step: 9227, epoch: 279, batch: 19, loss: 0.09513738006353378, acc: 95.3125, f1: 91.76587301587303, r: 0.8199938227059684
06/02/2019 09:50:19 step: 9232, epoch: 279, batch: 24, loss: 0.23209461569786072, acc: 93.75, f1: 78.3961038961039, r: 0.7028256804337235
06/02/2019 09:50:21 step: 9237, epoch: 279, batch: 29, loss: 0.10682927817106247, acc: 92.1875, f1: 84.45153061224492, r: 0.6973201121968927
06/02/2019 09:50:21 *** evaluating ***
06/02/2019 09:50:21 step: 280, epoch: 279, acc: 55.98290598290598, f1: 25.741404831319308, r: 0.3037417665181158
06/02/2019 09:50:21 *** epoch: 281 ***
06/02/2019 09:50:21 *** training ***
06/02/2019 09:50:23 step: 9245, epoch: 280, batch: 4, loss: 0.0876242071390152, acc: 93.75, f1: 85.40966386554622, r: 0.71126555762111
06/02/2019 09:50:24 step: 9250, epoch: 280, batch: 9, loss: 0.15154807269573212, acc: 95.3125, f1: 91.5842198948545, r: 0.6084149074937769
06/02/2019 09:50:25 step: 9255, epoch: 280, batch: 14, loss: 0.13400617241859436, acc: 95.3125, f1: 94.30926216640503, r: 0.6602990554851086
06/02/2019 09:50:26 step: 9260, epoch: 280, batch: 19, loss: 0.04390259459614754, acc: 98.4375, f1: 99.1282554211616, r: 0.661899332959325
06/02/2019 09:50:27 step: 9265, epoch: 280, batch: 24, loss: 0.0311221182346344, acc: 100.0, f1: 100.0, r: 0.6566949864785181
06/02/2019 09:50:28 step: 9270, epoch: 280, batch: 29, loss: 0.06314115226268768, acc: 98.4375, f1: 98.14921920185078, r: 0.6673254474316469
06/02/2019 09:50:29 *** evaluating ***
06/02/2019 09:50:29 step: 281, epoch: 280, acc: 55.98290598290598, f1: 29.022084559413265, r: 0.3277467681660684
06/02/2019 09:50:29 *** epoch: 282 ***
06/02/2019 09:50:29 *** training ***
06/02/2019 09:50:30 step: 9278, epoch: 281, batch: 4, loss: 0.06723804771900177, acc: 98.4375, f1: 97.94832826747721, r: 0.7985840401197701
06/02/2019 09:50:31 step: 9283, epoch: 281, batch: 9, loss: 0.11312824487686157, acc: 96.875, f1: 95.64814814814815, r: 0.7670277200149941
06/02/2019 09:50:32 step: 9288, epoch: 281, batch: 14, loss: 0.07902391254901886, acc: 98.4375, f1: 99.08733679807327, r: 0.6516033065907506
06/02/2019 09:50:33 step: 9293, epoch: 281, batch: 19, loss: 0.10428912937641144, acc: 93.75, f1: 89.86442129299273, r: 0.6409448233680101
06/02/2019 09:50:35 step: 9298, epoch: 281, batch: 24, loss: 0.08541913330554962, acc: 96.875, f1: 95.51557465091301, r: 0.7643134133634197
06/02/2019 09:50:36 step: 9303, epoch: 281, batch: 29, loss: 0.05627279356122017, acc: 98.4375, f1: 99.3073593073593, r: 0.7319734853825918
06/02/2019 09:50:37 *** evaluating ***
06/02/2019 09:50:37 step: 282, epoch: 281, acc: 55.98290598290598, f1: 28.74515503875969, r: 0.32276886969358104
06/02/2019 09:50:37 *** epoch: 283 ***
06/02/2019 09:50:37 *** training ***
06/02/2019 09:50:38 step: 9311, epoch: 282, batch: 4, loss: 0.12381432205438614, acc: 96.875, f1: 93.79310344827586, r: 0.7096782140797171
06/02/2019 09:50:39 step: 9316, epoch: 282, batch: 9, loss: 0.059963420033454895, acc: 98.4375, f1: 97.0, r: 0.794499012254508
06/02/2019 09:50:40 step: 9321, epoch: 282, batch: 14, loss: 0.08348231762647629, acc: 96.875, f1: 97.47135217723454, r: 0.6671277360684633
06/02/2019 09:50:42 step: 9326, epoch: 282, batch: 19, loss: 0.10126542299985886, acc: 93.75, f1: 86.47171907040328, r: 0.756187145429493
06/02/2019 09:50:43 step: 9331, epoch: 282, batch: 24, loss: 0.0695299282670021, acc: 98.4375, f1: 99.32386747802569, r: 0.7711754008614148
06/02/2019 09:50:44 step: 9336, epoch: 282, batch: 29, loss: 0.08672372996807098, acc: 95.3125, f1: 95.60064935064935, r: 0.6568649473662207
06/02/2019 09:50:44 *** evaluating ***
06/02/2019 09:50:45 step: 283, epoch: 282, acc: 58.119658119658126, f1: 28.650085562001397, r: 0.32838520136892013
06/02/2019 09:50:45 *** epoch: 284 ***
06/02/2019 09:50:45 *** training ***
06/02/2019 09:50:46 step: 9344, epoch: 283, batch: 4, loss: 0.0702967643737793, acc: 95.3125, f1: 92.24300831443689, r: 0.7217076527451259
06/02/2019 09:50:47 step: 9349, epoch: 283, batch: 9, loss: 0.06562477350234985, acc: 96.875, f1: 93.70477475740634, r: 0.7010969648440077
06/02/2019 09:50:48 step: 9354, epoch: 283, batch: 14, loss: 0.0505102314054966, acc: 98.4375, f1: 95.23809523809523, r: 0.7458699245072818
06/02/2019 09:50:49 step: 9359, epoch: 283, batch: 19, loss: 0.03873997554183006, acc: 98.4375, f1: 96.3718820861678, r: 0.7312937763736931
06/02/2019 09:50:50 step: 9364, epoch: 283, batch: 24, loss: 0.05274295061826706, acc: 98.4375, f1: 98.48484848484848, r: 0.7614444355698367
06/02/2019 09:50:51 step: 9369, epoch: 283, batch: 29, loss: 0.093106210231781, acc: 96.875, f1: 81.28654970760235, r: 0.6413628786154882
06/02/2019 09:50:52 *** evaluating ***
06/02/2019 09:50:52 step: 284, epoch: 283, acc: 58.119658119658126, f1: 28.642989660909375, r: 0.32781307240544894
06/02/2019 09:50:52 *** epoch: 285 ***
06/02/2019 09:50:52 *** training ***
06/02/2019 09:50:53 step: 9377, epoch: 284, batch: 4, loss: 0.038309697061777115, acc: 100.0, f1: 100.0, r: 0.7016012827131468
06/02/2019 09:50:54 step: 9382, epoch: 284, batch: 9, loss: 0.08303861320018768, acc: 96.875, f1: 94.73977297065753, r: 0.716550303101354
06/02/2019 09:50:55 step: 9387, epoch: 284, batch: 14, loss: 0.051591385155916214, acc: 98.4375, f1: 94.13919413919413, r: 0.6867518946269895
06/02/2019 09:50:57 step: 9392, epoch: 284, batch: 19, loss: 0.08328448235988617, acc: 95.3125, f1: 86.26743626743627, r: 0.7702063352040074
06/02/2019 09:50:58 step: 9397, epoch: 284, batch: 24, loss: 0.04963737726211548, acc: 98.4375, f1: 98.80745341614906, r: 0.7030519811305962
06/02/2019 09:50:59 step: 9402, epoch: 284, batch: 29, loss: 0.2249247431755066, acc: 95.3125, f1: 94.75897196485433, r: 0.7816105348529105
06/02/2019 09:50:59 *** evaluating ***
06/02/2019 09:51:00 step: 285, epoch: 284, acc: 55.98290598290598, f1: 27.723834992200857, r: 0.32253685796427994
06/02/2019 09:51:00 *** epoch: 286 ***
06/02/2019 09:51:00 *** training ***
06/02/2019 09:51:01 step: 9410, epoch: 285, batch: 4, loss: 0.04878831282258034, acc: 98.4375, f1: 86.76470588235294, r: 0.7794533100666305
06/02/2019 09:51:02 step: 9415, epoch: 285, batch: 9, loss: 0.13496927917003632, acc: 95.3125, f1: 82.5378787878788, r: 0.7052541167962678
06/02/2019 09:51:03 step: 9420, epoch: 285, batch: 14, loss: 0.09381907433271408, acc: 95.3125, f1: 90.79097650526222, r: 0.7145345018402917
06/02/2019 09:51:04 step: 9425, epoch: 285, batch: 19, loss: 0.08318637311458588, acc: 96.875, f1: 90.625, r: 0.693839157326182
06/02/2019 09:51:05 step: 9430, epoch: 285, batch: 24, loss: 0.02432229183614254, acc: 100.0, f1: 100.0, r: 0.7975783438935495
06/02/2019 09:51:06 step: 9435, epoch: 285, batch: 29, loss: 0.034803230315446854, acc: 100.0, f1: 100.0, r: 0.8198166891256934
06/02/2019 09:51:07 *** evaluating ***
06/02/2019 09:51:07 step: 286, epoch: 285, acc: 55.55555555555556, f1: 26.123644443171617, r: 0.314071779532454
06/02/2019 09:51:07 *** epoch: 287 ***
06/02/2019 09:51:07 *** training ***
06/02/2019 09:51:09 step: 9443, epoch: 286, batch: 4, loss: 0.27698779106140137, acc: 89.0625, f1: 87.17837244425266, r: 0.7205416271208976
06/02/2019 09:51:10 step: 9448, epoch: 286, batch: 9, loss: 0.06434742361307144, acc: 96.875, f1: 95.3514739229025, r: 0.6863620334953899
06/02/2019 09:51:11 step: 9453, epoch: 286, batch: 14, loss: 0.06473790854215622, acc: 96.875, f1: 95.31746031746032, r: 0.6473496082008416
06/02/2019 09:51:12 step: 9458, epoch: 286, batch: 19, loss: 0.06716350466012955, acc: 98.4375, f1: 96.39097744360903, r: 0.6639037409302768
06/02/2019 09:51:13 step: 9463, epoch: 286, batch: 24, loss: 0.08452261239290237, acc: 98.4375, f1: 96.4625850340136, r: 0.6849026954792606
06/02/2019 09:51:14 step: 9468, epoch: 286, batch: 29, loss: 0.02563527040183544, acc: 100.0, f1: 100.0, r: 0.7055588542536236
06/02/2019 09:51:15 *** evaluating ***
06/02/2019 09:51:15 step: 287, epoch: 286, acc: 56.41025641025641, f1: 28.35027016698593, r: 0.3179027352178269
06/02/2019 09:51:15 *** epoch: 288 ***
06/02/2019 09:51:15 *** training ***
06/02/2019 09:51:17 step: 9476, epoch: 287, batch: 4, loss: 0.2213386595249176, acc: 95.3125, f1: 93.28483835005574, r: 0.7129789383563395
06/02/2019 09:51:18 step: 9481, epoch: 287, batch: 9, loss: 0.06293461471796036, acc: 98.4375, f1: 98.1111111111111, r: 0.7084626101323944
06/02/2019 09:51:19 step: 9486, epoch: 287, batch: 14, loss: 0.04189974442124367, acc: 96.875, f1: 95.0186741363212, r: 0.7749600408235597
06/02/2019 09:51:20 step: 9491, epoch: 287, batch: 19, loss: 0.08220825344324112, acc: 95.3125, f1: 93.85022385022386, r: 0.5145154299115564
06/02/2019 09:51:21 step: 9496, epoch: 287, batch: 24, loss: 0.054776631295681, acc: 98.4375, f1: 99.22027290448344, r: 0.6962233478191971
06/02/2019 09:51:22 step: 9501, epoch: 287, batch: 29, loss: 0.11020219326019287, acc: 95.3125, f1: 96.013209013209, r: 0.7426442660033565
06/02/2019 09:51:23 *** evaluating ***
06/02/2019 09:51:24 step: 288, epoch: 287, acc: 56.41025641025641, f1: 28.993605798189193, r: 0.328598603872287
06/02/2019 09:51:24 *** epoch: 289 ***
06/02/2019 09:51:24 *** training ***
06/02/2019 09:51:25 step: 9509, epoch: 288, batch: 4, loss: 0.16943347454071045, acc: 93.75, f1: 83.46967473709952, r: 0.7345690883569104
06/02/2019 09:51:26 step: 9514, epoch: 288, batch: 9, loss: 0.09242482483386993, acc: 95.3125, f1: 77.75391275391274, r: 0.6051328571024617
06/02/2019 09:51:27 step: 9519, epoch: 288, batch: 14, loss: 0.1105097234249115, acc: 95.3125, f1: 94.23977297065753, r: 0.7546302078090741
06/02/2019 09:51:28 step: 9524, epoch: 288, batch: 19, loss: 0.056351982057094574, acc: 98.4375, f1: 97.16216216216216, r: 0.7531665091030194
06/02/2019 09:51:29 step: 9529, epoch: 288, batch: 24, loss: 0.11883469671010971, acc: 96.875, f1: 94.5982603567743, r: 0.7640991730685402
06/02/2019 09:51:30 step: 9534, epoch: 288, batch: 29, loss: 0.03574700653553009, acc: 100.0, f1: 100.0, r: 0.6780666473019764
06/02/2019 09:51:31 *** evaluating ***
06/02/2019 09:51:31 step: 289, epoch: 288, acc: 56.41025641025641, f1: 27.167354710701485, r: 0.3059347413349681
06/02/2019 09:51:31 *** epoch: 290 ***
06/02/2019 09:51:31 *** training ***
06/02/2019 09:51:33 step: 9542, epoch: 289, batch: 4, loss: 0.10476697236299515, acc: 96.875, f1: 94.75357018460467, r: 0.5848814098353026
06/02/2019 09:51:34 step: 9547, epoch: 289, batch: 9, loss: 0.011390571482479572, acc: 100.0, f1: 100.0, r: 0.7022227564042753
06/02/2019 09:51:35 step: 9552, epoch: 289, batch: 14, loss: 0.06755881011486053, acc: 96.875, f1: 81.04395604395604, r: 0.6729812136426085
06/02/2019 09:51:36 step: 9557, epoch: 289, batch: 19, loss: 0.1160873994231224, acc: 93.75, f1: 87.2244684037137, r: 0.760814081270789
06/02/2019 09:51:37 step: 9562, epoch: 289, batch: 24, loss: 0.09354197233915329, acc: 95.3125, f1: 96.22076023391813, r: 0.8117328583580959
06/02/2019 09:51:39 step: 9567, epoch: 289, batch: 29, loss: 0.194952592253685, acc: 95.3125, f1: 91.87861711848919, r: 0.6539172073719703
06/02/2019 09:51:39 *** evaluating ***
06/02/2019 09:51:39 step: 290, epoch: 289, acc: 57.26495726495726, f1: 27.987855166124188, r: 0.3206083408614794
06/02/2019 09:51:39 *** epoch: 291 ***
06/02/2019 09:51:39 *** training ***
06/02/2019 09:51:40 step: 9575, epoch: 290, batch: 4, loss: 0.03335738927125931, acc: 100.0, f1: 100.0, r: 0.6151275277594406
06/02/2019 09:51:42 step: 9580, epoch: 290, batch: 9, loss: 0.11329960078001022, acc: 95.3125, f1: 82.06439393939394, r: 0.7090621229214603
06/02/2019 09:51:43 step: 9585, epoch: 290, batch: 14, loss: 0.08181773871183395, acc: 96.875, f1: 90.7936507936508, r: 0.6828202839772616
06/02/2019 09:51:44 step: 9590, epoch: 290, batch: 19, loss: 0.14560164511203766, acc: 92.1875, f1: 90.49740829346094, r: 0.7346346117848399
06/02/2019 09:51:45 step: 9595, epoch: 290, batch: 24, loss: 0.12313183397054672, acc: 96.875, f1: 93.62193362193364, r: 0.7096846777326017
06/02/2019 09:51:46 step: 9600, epoch: 290, batch: 29, loss: 0.06800077110528946, acc: 98.4375, f1: 98.90476190476191, r: 0.8049031497250525
06/02/2019 09:51:47 *** evaluating ***
06/02/2019 09:51:47 step: 291, epoch: 290, acc: 55.98290598290598, f1: 26.941889237449985, r: 0.313266748451577
06/02/2019 09:51:47 *** epoch: 292 ***
06/02/2019 09:51:47 *** training ***
06/02/2019 09:51:48 step: 9608, epoch: 291, batch: 4, loss: 0.0527089387178421, acc: 100.0, f1: 100.0, r: 0.7641288639374281
06/02/2019 09:51:50 step: 9613, epoch: 291, batch: 9, loss: 0.08227203786373138, acc: 98.4375, f1: 99.24465733235077, r: 0.7924354225286329
06/02/2019 09:51:51 step: 9618, epoch: 291, batch: 14, loss: 0.1303015649318695, acc: 93.75, f1: 88.96290491118077, r: 0.7657085717708245
06/02/2019 09:51:52 step: 9623, epoch: 291, batch: 19, loss: 0.16420669853687286, acc: 93.75, f1: 95.24481885777567, r: 0.6944881456256489
06/02/2019 09:51:53 step: 9628, epoch: 291, batch: 24, loss: 0.10968225449323654, acc: 98.4375, f1: 94.04761904761905, r: 0.7432738011927358
06/02/2019 09:51:54 step: 9633, epoch: 291, batch: 29, loss: 0.09454931318759918, acc: 98.4375, f1: 97.12121212121212, r: 0.682847787829482
06/02/2019 09:51:55 *** evaluating ***
06/02/2019 09:51:55 step: 292, epoch: 291, acc: 56.837606837606835, f1: 26.427758690731256, r: 0.2989927375153056
06/02/2019 09:51:55 *** epoch: 293 ***
06/02/2019 09:51:55 *** training ***
06/02/2019 09:51:56 step: 9641, epoch: 292, batch: 4, loss: 0.11241453140974045, acc: 95.3125, f1: 92.79248366013071, r: 0.8127525673122571
06/02/2019 09:51:57 step: 9646, epoch: 292, batch: 9, loss: 0.08188458532094955, acc: 96.875, f1: 96.72619047619048, r: 0.76444055177478
06/02/2019 09:51:58 step: 9651, epoch: 292, batch: 14, loss: 0.037597812712192535, acc: 98.4375, f1: 97.57236227824464, r: 0.6389515776665782
06/02/2019 09:52:00 step: 9656, epoch: 292, batch: 19, loss: 0.046015556901693344, acc: 96.875, f1: 92.25274725274726, r: 0.791683886698763
06/02/2019 09:52:01 step: 9661, epoch: 292, batch: 24, loss: 0.09819920361042023, acc: 95.3125, f1: 90.12987012987014, r: 0.6938971930937167
06/02/2019 09:52:02 step: 9666, epoch: 292, batch: 29, loss: 0.013819357380270958, acc: 100.0, f1: 100.0, r: 0.7085011705965495
06/02/2019 09:52:03 *** evaluating ***
06/02/2019 09:52:03 step: 293, epoch: 292, acc: 57.692307692307686, f1: 28.33535096567963, r: 0.33946867819882637
06/02/2019 09:52:03 *** epoch: 294 ***
06/02/2019 09:52:03 *** training ***
06/02/2019 09:52:04 step: 9674, epoch: 293, batch: 4, loss: 0.21651726961135864, acc: 93.75, f1: 91.81737588652481, r: 0.8264001629278774
06/02/2019 09:52:05 step: 9679, epoch: 293, batch: 9, loss: 0.10800156742334366, acc: 95.3125, f1: 89.22077922077922, r: 0.7539921844004096
06/02/2019 09:52:06 step: 9684, epoch: 293, batch: 14, loss: 0.15956133604049683, acc: 95.3125, f1: 90.15873015873017, r: 0.6772774401271654
06/02/2019 09:52:07 step: 9689, epoch: 293, batch: 19, loss: 0.05807454511523247, acc: 98.4375, f1: 95.43010752688173, r: 0.7567309535064688
06/02/2019 09:52:09 step: 9694, epoch: 293, batch: 24, loss: 0.05455518886446953, acc: 98.4375, f1: 97.8937728937729, r: 0.7555553695850185
06/02/2019 09:52:10 step: 9699, epoch: 293, batch: 29, loss: 0.05279621109366417, acc: 98.4375, f1: 95.91836734693878, r: 0.661739204235611
06/02/2019 09:52:11 *** evaluating ***
06/02/2019 09:52:11 step: 294, epoch: 293, acc: 55.55555555555556, f1: 27.936439617528773, r: 0.3276187028684656
06/02/2019 09:52:11 *** epoch: 295 ***
06/02/2019 09:52:11 *** training ***
06/02/2019 09:52:12 step: 9707, epoch: 294, batch: 4, loss: 0.05170578509569168, acc: 100.0, f1: 100.0, r: 0.7652802342180355
06/02/2019 09:52:13 step: 9712, epoch: 294, batch: 9, loss: 0.12638957798480988, acc: 95.3125, f1: 92.49686716791979, r: 0.7906409284386507
06/02/2019 09:52:15 step: 9717, epoch: 294, batch: 14, loss: 0.03887638449668884, acc: 98.4375, f1: 96.66048237476808, r: 0.6598396830419664
06/02/2019 09:52:16 step: 9722, epoch: 294, batch: 19, loss: 0.09824128448963165, acc: 95.3125, f1: 93.68377741588996, r: 0.7770813859704938
06/02/2019 09:52:17 step: 9727, epoch: 294, batch: 24, loss: 0.13913202285766602, acc: 93.75, f1: 78.70028011204482, r: 0.6875873936100491
06/02/2019 09:52:18 step: 9732, epoch: 294, batch: 29, loss: 0.20361363887786865, acc: 95.3125, f1: 80.94192406692406, r: 0.6873210907375191
06/02/2019 09:52:19 *** evaluating ***
06/02/2019 09:52:19 step: 295, epoch: 294, acc: 58.119658119658126, f1: 28.843379446640316, r: 0.31545268517795383
06/02/2019 09:52:19 *** epoch: 296 ***
06/02/2019 09:52:19 *** training ***
06/02/2019 09:52:20 step: 9740, epoch: 295, batch: 4, loss: 0.01887829788029194, acc: 100.0, f1: 100.0, r: 0.6396614551133438
06/02/2019 09:52:21 step: 9745, epoch: 295, batch: 9, loss: 0.03467178717255592, acc: 100.0, f1: 100.0, r: 0.7430484951330132
06/02/2019 09:52:22 step: 9750, epoch: 295, batch: 14, loss: 0.11556322127580643, acc: 95.3125, f1: 90.64814814814814, r: 0.7743588997484
06/02/2019 09:52:23 step: 9755, epoch: 295, batch: 19, loss: 0.08186598122119904, acc: 96.875, f1: 92.0, r: 0.7913959600078078
06/02/2019 09:52:25 step: 9760, epoch: 295, batch: 24, loss: 0.07392453402280807, acc: 96.875, f1: 82.5108225108225, r: 0.7195270616916664
06/02/2019 09:52:26 step: 9765, epoch: 295, batch: 29, loss: 0.09224934130907059, acc: 96.875, f1: 86.20286110174887, r: 0.7802943510921155
06/02/2019 09:52:27 *** evaluating ***
06/02/2019 09:52:27 step: 296, epoch: 295, acc: 57.692307692307686, f1: 28.581536991901135, r: 0.3372119233503698
06/02/2019 09:52:27 *** epoch: 297 ***
06/02/2019 09:52:27 *** training ***
06/02/2019 09:52:28 step: 9773, epoch: 296, batch: 4, loss: 0.055438000708818436, acc: 98.4375, f1: 87.28813559322035, r: 0.6642507236261682
06/02/2019 09:52:29 step: 9778, epoch: 296, batch: 9, loss: 0.08383432775735855, acc: 96.875, f1: 88.35978835978837, r: 0.7140885432110995
06/02/2019 09:52:30 step: 9783, epoch: 296, batch: 14, loss: 0.04653835669159889, acc: 100.0, f1: 100.0, r: 0.7672602222557353
06/02/2019 09:52:31 step: 9788, epoch: 296, batch: 19, loss: 0.01904030703008175, acc: 100.0, f1: 100.0, r: 0.5897332694167426
06/02/2019 09:52:33 step: 9793, epoch: 296, batch: 24, loss: 0.06993313133716583, acc: 96.875, f1: 96.46780303030303, r: 0.7659290499454381
06/02/2019 09:52:34 step: 9798, epoch: 296, batch: 29, loss: 0.11836827546358109, acc: 95.3125, f1: 89.38157081014224, r: 0.6483228679268646
06/02/2019 09:52:34 *** evaluating ***
06/02/2019 09:52:35 step: 297, epoch: 296, acc: 56.837606837606835, f1: 28.277773414661812, r: 0.3286843566716421
06/02/2019 09:52:35 *** epoch: 298 ***
06/02/2019 09:52:35 *** training ***
06/02/2019 09:52:36 step: 9806, epoch: 297, batch: 4, loss: 0.037510763853788376, acc: 100.0, f1: 100.0, r: 0.6851895513868392
06/02/2019 09:52:37 step: 9811, epoch: 297, batch: 9, loss: 0.18006563186645508, acc: 96.875, f1: 97.89502164502166, r: 0.6148359193814595
06/02/2019 09:52:38 step: 9816, epoch: 297, batch: 14, loss: 0.19712691009044647, acc: 92.1875, f1: 76.1320066427825, r: 0.59127668786223
06/02/2019 09:52:39 step: 9821, epoch: 297, batch: 19, loss: 0.05112919211387634, acc: 100.0, f1: 100.0, r: 0.7598043455290754
06/02/2019 09:52:40 step: 9826, epoch: 297, batch: 24, loss: 0.07890859246253967, acc: 95.3125, f1: 91.9615608977311, r: 0.6692655539599247
06/02/2019 09:52:41 step: 9831, epoch: 297, batch: 29, loss: 0.047405973076820374, acc: 98.4375, f1: 95.84415584415584, r: 0.7759212334628223
06/02/2019 09:52:42 *** evaluating ***
06/02/2019 09:52:42 step: 298, epoch: 297, acc: 56.41025641025641, f1: 28.279919338087655, r: 0.36296516507466725
06/02/2019 09:52:42 *** epoch: 299 ***
06/02/2019 09:52:42 *** training ***
06/02/2019 09:52:43 step: 9839, epoch: 298, batch: 4, loss: 0.01897384412586689, acc: 100.0, f1: 100.0, r: 0.7155602266311955
06/02/2019 09:52:45 step: 9844, epoch: 298, batch: 9, loss: 0.05963006615638733, acc: 98.4375, f1: 97.88359788359789, r: 0.6280706592041079
06/02/2019 09:52:46 step: 9849, epoch: 298, batch: 14, loss: 0.08535803854465485, acc: 95.3125, f1: 81.10042425458246, r: 0.7013721972528505
06/02/2019 09:52:47 step: 9854, epoch: 298, batch: 19, loss: 0.05756024643778801, acc: 100.0, f1: 100.0, r: 0.6953292399518787
06/02/2019 09:52:48 step: 9859, epoch: 298, batch: 24, loss: 0.13151666522026062, acc: 93.75, f1: 92.05023547880691, r: 0.6884320996538773
06/02/2019 09:52:49 step: 9864, epoch: 298, batch: 29, loss: 0.08481155335903168, acc: 98.4375, f1: 95.84415584415584, r: 0.7476483015099247
06/02/2019 09:52:50 *** evaluating ***
06/02/2019 09:52:50 step: 299, epoch: 298, acc: 58.54700854700855, f1: 28.705236791742948, r: 0.3256903581769456
06/02/2019 09:52:50 *** epoch: 300 ***
06/02/2019 09:52:50 *** training ***
06/02/2019 09:52:51 step: 9872, epoch: 299, batch: 4, loss: 0.24909912049770355, acc: 93.75, f1: 79.5268308080808, r: 0.773229123553313
06/02/2019 09:52:52 step: 9877, epoch: 299, batch: 9, loss: 0.07853687554597855, acc: 96.875, f1: 97.36111111111111, r: 0.7536413718760545
06/02/2019 09:52:53 step: 9882, epoch: 299, batch: 14, loss: 0.10567597299814224, acc: 96.875, f1: 92.59803921568627, r: 0.8200274563226081
06/02/2019 09:52:55 step: 9887, epoch: 299, batch: 19, loss: 0.1299867480993271, acc: 93.75, f1: 88.7202380952381, r: 0.6826053109972796
06/02/2019 09:52:56 step: 9892, epoch: 299, batch: 24, loss: 0.07456609606742859, acc: 95.3125, f1: 95.29892484212246, r: 0.7148776355169866
06/02/2019 09:52:57 step: 9897, epoch: 299, batch: 29, loss: 0.01894126459956169, acc: 98.4375, f1: 98.24046920821115, r: 0.6743124920430822
06/02/2019 09:52:58 *** evaluating ***
06/02/2019 09:52:58 step: 300, epoch: 299, acc: 57.26495726495726, f1: 27.558958668249332, r: 0.33393409124133155
06/02/2019 09:52:58 
*** Best acc model ***
epoch: 90
acc: 62.39316239316239
f1: 28.443737771769
corr: 0.3838890268651199
06/02/2019 09:52:58 Loading Test Data
06/02/2019 09:52:58 load data from data/word2vec_temp/test_text.npy, data/word2vec_temp/test_label.npy, training: False
06/02/2019 09:53:19 loaded. total len: 2228
06/02/2019 09:53:19 Test: length: 2228, total batch: 35, batch size: 64
06/02/2019 09:53:19 
*** Test Result ***
acc: 57.26495726495726
f1: 27.558958668249332
corr: 0.33393409124133155
