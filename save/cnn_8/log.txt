06/02/2019 03:46:56 {'input_path': 'data/word2vec_temp', 'output_path': 'save/cnn_8', 'gpu': True, 'seed': 20000125, 'display_per_batch': 5, 'optimizer': 'adagrad', 'lr': 0.01, 'lr_decay': 0, 'weight_decay': 0.0001, 'momentum': 0.985, 'num_epochs': 300, 'batch_size': 64, 'num_labels': 8, 'embedding_size': 300, 'type': 'cnn', 'cnn': {'max_length': 512, 'conv_1': {'size': 512, 'kernel_size': 3}, 'max_pool_1': {'kernel_size': 2, 'stride': 2}, 'fc': {'hidden_size': 512, 'dropout': 0.9}, 'loss': 'cross_entropy'}}
06/02/2019 03:46:56 Loading Train Data
06/02/2019 03:46:56 load data from data/word2vec_temp/train_text.npy, data/word2vec_temp/train_label.npy, training: True
06/02/2019 03:47:21 loaded. total len: 2342
06/02/2019 03:47:21 Train: length: 2108, total batch: 33, batch size: 64
06/02/2019 03:47:21 Dev: length: 234, total batch: 4, batch size: 64
06/02/2019 03:47:21 Loading model cnn
06/02/2019 03:47:31 *** epoch: 1 ***
06/02/2019 03:47:31 *** training ***
06/02/2019 03:47:32 step: 5, epoch: 0, batch: 4, loss: 2.309593915939331, acc: 20.3125, f1: 8.761214630779849, r: -0.01320542826929088
06/02/2019 03:47:32 step: 10, epoch: 0, batch: 9, loss: 1.80499267578125, acc: 32.8125, f1: 11.334213845458224, r: 0.026019836316072575
06/02/2019 03:47:32 step: 15, epoch: 0, batch: 14, loss: 1.9241055250167847, acc: 18.75, f1: 6.145552560646901, r: -0.029461837051799868
06/02/2019 03:47:33 step: 20, epoch: 0, batch: 19, loss: 1.8849014043807983, acc: 28.125, f1: 11.126373626373626, r: 0.020694455394672528
06/02/2019 03:47:33 step: 25, epoch: 0, batch: 24, loss: 1.942413330078125, acc: 21.875, f1: 7.974413646055438, r: -0.0077507580698741654
06/02/2019 03:47:33 step: 30, epoch: 0, batch: 29, loss: 1.9731357097625732, acc: 29.6875, f1: 9.880050505050505, r: 0.06525483777068018
06/02/2019 03:47:33 *** evaluating ***
06/02/2019 03:47:34 step: 1, epoch: 0, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.1008351711739501
06/02/2019 03:47:34 *** epoch: 2 ***
06/02/2019 03:47:34 *** training ***
06/02/2019 03:47:34 step: 38, epoch: 1, batch: 4, loss: 1.7689183950424194, acc: 28.125, f1: 9.739729225023343, r: 0.09139527286804441
06/02/2019 03:47:34 step: 43, epoch: 1, batch: 9, loss: 1.758923053741455, acc: 35.9375, f1: 10.276482345103265, r: 0.03584376468397599
06/02/2019 03:47:34 step: 48, epoch: 1, batch: 14, loss: 1.6195210218429565, acc: 31.25, f1: 8.15018315018315, r: 0.19348135501804964
06/02/2019 03:47:35 step: 53, epoch: 1, batch: 19, loss: 1.513193964958191, acc: 45.3125, f1: 10.929705215419501, r: 0.1324694000178831
06/02/2019 03:47:35 step: 58, epoch: 1, batch: 24, loss: 1.6732251644134521, acc: 40.625, f1: 12.82506673555498, r: 0.08424911934349424
06/02/2019 03:47:35 step: 63, epoch: 1, batch: 29, loss: 1.7356715202331543, acc: 25.0, f1: 7.4444444444444455, r: 0.18290113516862758
06/02/2019 03:47:36 *** evaluating ***
06/02/2019 03:47:36 step: 2, epoch: 1, acc: 44.871794871794876, f1: 7.743362831858408, r: 0.22713839220288334
06/02/2019 03:47:36 *** epoch: 3 ***
06/02/2019 03:47:36 *** training ***
06/02/2019 03:47:36 step: 71, epoch: 2, batch: 4, loss: 1.6772773265838623, acc: 29.6875, f1: 8.65079365079365, r: 0.18928520710269173
06/02/2019 03:47:36 step: 76, epoch: 2, batch: 9, loss: 1.5404187440872192, acc: 46.875, f1: 12.698412698412694, r: 0.11177890792418275
06/02/2019 03:47:37 step: 81, epoch: 2, batch: 14, loss: 1.701469898223877, acc: 42.1875, f1: 10.892857142857142, r: 0.22077867020597533
06/02/2019 03:47:37 step: 86, epoch: 2, batch: 19, loss: 1.448252558708191, acc: 51.5625, f1: 10.778985507246377, r: 0.17596815206842956
06/02/2019 03:47:37 step: 91, epoch: 2, batch: 24, loss: 1.691860318183899, acc: 34.375, f1: 7.836045810729356, r: 0.19048653729303347
06/02/2019 03:47:38 step: 96, epoch: 2, batch: 29, loss: 1.536586046218872, acc: 39.0625, f1: 13.546176046176045, r: 0.275568544520403
06/02/2019 03:47:38 *** evaluating ***
06/02/2019 03:47:38 step: 3, epoch: 2, acc: 45.2991452991453, f1: 8.470394736842104, r: 0.221354625075474
06/02/2019 03:47:38 *** epoch: 4 ***
06/02/2019 03:47:38 *** training ***
06/02/2019 03:47:38 step: 104, epoch: 3, batch: 4, loss: 1.4110443592071533, acc: 40.625, f1: 14.087775578458809, r: 0.23027051309870986
06/02/2019 03:47:38 step: 109, epoch: 3, batch: 9, loss: 1.5715023279190063, acc: 48.4375, f1: 17.78336905538471, r: 0.20312156809928572
06/02/2019 03:47:39 step: 114, epoch: 3, batch: 14, loss: 1.482345700263977, acc: 48.4375, f1: 17.663622526636225, r: 0.2767426032308824
06/02/2019 03:47:39 step: 119, epoch: 3, batch: 19, loss: 1.3283047676086426, acc: 56.25, f1: 16.731811548884718, r: 0.2077489291066563
06/02/2019 03:47:39 step: 124, epoch: 3, batch: 24, loss: 1.5036911964416504, acc: 45.3125, f1: 16.147691953493727, r: 0.33063587189092636
06/02/2019 03:47:40 step: 129, epoch: 3, batch: 29, loss: 1.5734776258468628, acc: 45.3125, f1: 14.777777777777779, r: 0.21453257394296893
06/02/2019 03:47:40 *** evaluating ***
06/02/2019 03:47:40 step: 4, epoch: 3, acc: 46.15384615384615, f1: 10.15432098765432, r: 0.22187780925683903
06/02/2019 03:47:40 *** epoch: 5 ***
06/02/2019 03:47:40 *** training ***
06/02/2019 03:47:40 step: 137, epoch: 4, batch: 4, loss: 1.2128442525863647, acc: 51.5625, f1: 19.161410018552875, r: 0.34106925102804747
06/02/2019 03:47:41 step: 142, epoch: 4, batch: 9, loss: 1.452112078666687, acc: 46.875, f1: 19.674452475995253, r: 0.3146239525870396
06/02/2019 03:47:41 step: 147, epoch: 4, batch: 14, loss: 1.1800196170806885, acc: 57.8125, f1: 30.3968253968254, r: 0.27812993989035295
06/02/2019 03:47:41 step: 152, epoch: 4, batch: 19, loss: 1.4330871105194092, acc: 50.0, f1: 17.016384018396092, r: 0.23043990717598697
06/02/2019 03:47:42 step: 157, epoch: 4, batch: 24, loss: 1.320755958557129, acc: 51.5625, f1: 20.230848861283643, r: 0.3315619783249561
06/02/2019 03:47:42 step: 162, epoch: 4, batch: 29, loss: 1.5119976997375488, acc: 48.4375, f1: 20.708041958041957, r: 0.2802882223313845
06/02/2019 03:47:42 *** evaluating ***
06/02/2019 03:47:42 step: 5, epoch: 4, acc: 51.70940170940172, f1: 15.574348005823463, r: 0.2529428971792447
06/02/2019 03:47:42 *** epoch: 6 ***
06/02/2019 03:47:42 *** training ***
06/02/2019 03:47:43 step: 170, epoch: 5, batch: 4, loss: 1.2773818969726562, acc: 51.5625, f1: 18.978310502283104, r: 0.37312394668908366
06/02/2019 03:47:43 step: 175, epoch: 5, batch: 9, loss: 1.1699585914611816, acc: 60.9375, f1: 23.23040848527349, r: 0.36947903603900994
06/02/2019 03:47:43 step: 180, epoch: 5, batch: 14, loss: 1.136296272277832, acc: 60.9375, f1: 31.092970521541957, r: 0.48010684899791434
06/02/2019 03:47:44 step: 185, epoch: 5, batch: 19, loss: 1.2914843559265137, acc: 56.25, f1: 22.752925339132236, r: 0.39267260175694596
06/02/2019 03:47:44 step: 190, epoch: 5, batch: 24, loss: 1.2129018306732178, acc: 59.375, f1: 29.876543209876544, r: 0.3320918230135509
06/02/2019 03:47:44 step: 195, epoch: 5, batch: 29, loss: 1.1421685218811035, acc: 67.1875, f1: 39.5486960704352, r: 0.30327001570942896
06/02/2019 03:47:44 *** evaluating ***
06/02/2019 03:47:44 step: 6, epoch: 5, acc: 54.27350427350427, f1: 17.025534597373678, r: 0.3015639939953365
06/02/2019 03:47:44 *** epoch: 7 ***
06/02/2019 03:47:44 *** training ***
06/02/2019 03:47:45 step: 203, epoch: 6, batch: 4, loss: 1.0715093612670898, acc: 62.5, f1: 28.038628038628037, r: 0.3918823980553186
06/02/2019 03:47:45 step: 208, epoch: 6, batch: 9, loss: 0.9616252183914185, acc: 71.875, f1: 36.836158192090394, r: 0.5101837870462386
06/02/2019 03:47:45 step: 213, epoch: 6, batch: 14, loss: 1.1160820722579956, acc: 60.9375, f1: 44.91373360938579, r: 0.4210086770357329
06/02/2019 03:47:46 step: 218, epoch: 6, batch: 19, loss: 1.2781250476837158, acc: 62.5, f1: 36.00519451812555, r: 0.36413906190444884
06/02/2019 03:47:46 step: 223, epoch: 6, batch: 24, loss: 1.0074130296707153, acc: 65.625, f1: 42.686179735051915, r: 0.4574912205369744
06/02/2019 03:47:46 step: 228, epoch: 6, batch: 29, loss: 1.19399094581604, acc: 53.125, f1: 22.59899888765295, r: 0.466842418607388
06/02/2019 03:47:46 *** evaluating ***
06/02/2019 03:47:47 step: 7, epoch: 6, acc: 53.84615384615385, f1: 17.361280601572915, r: 0.3242271883647681
06/02/2019 03:47:47 *** epoch: 8 ***
06/02/2019 03:47:47 *** training ***
06/02/2019 03:47:47 step: 236, epoch: 7, batch: 4, loss: 1.0136784315109253, acc: 60.9375, f1: 26.595853858784892, r: 0.42379193269773074
06/02/2019 03:47:47 step: 241, epoch: 7, batch: 9, loss: 0.9637618064880371, acc: 71.875, f1: 40.030107047279216, r: 0.4526375687913448
06/02/2019 03:47:47 step: 246, epoch: 7, batch: 14, loss: 0.9295845031738281, acc: 67.1875, f1: 41.69643769073378, r: 0.571847813065193
06/02/2019 03:47:48 step: 251, epoch: 7, batch: 19, loss: 1.0775291919708252, acc: 60.9375, f1: 32.871916443345015, r: 0.39891909768688916
06/02/2019 03:47:48 step: 256, epoch: 7, batch: 24, loss: 1.147367000579834, acc: 62.5, f1: 31.900793650793652, r: 0.5185803593185419
06/02/2019 03:47:48 step: 261, epoch: 7, batch: 29, loss: 0.9943177700042725, acc: 65.625, f1: 31.040602814796358, r: 0.4279416924934989
06/02/2019 03:47:48 *** evaluating ***
06/02/2019 03:47:49 step: 8, epoch: 7, acc: 52.13675213675214, f1: 18.784775173854257, r: 0.34199745081163313
06/02/2019 03:47:49 *** epoch: 9 ***
06/02/2019 03:47:49 *** training ***
06/02/2019 03:47:49 step: 269, epoch: 8, batch: 4, loss: 1.1382348537445068, acc: 51.5625, f1: 26.878927524088812, r: 0.39751086827059223
06/02/2019 03:47:49 step: 274, epoch: 8, batch: 9, loss: 1.0856555700302124, acc: 59.375, f1: 28.730769230769234, r: 0.43707694756005383
06/02/2019 03:47:50 step: 279, epoch: 8, batch: 14, loss: 1.0937106609344482, acc: 59.375, f1: 37.58701719014016, r: 0.3600180518130513
06/02/2019 03:47:50 step: 284, epoch: 8, batch: 19, loss: 0.8779093623161316, acc: 70.3125, f1: 52.86538857967429, r: 0.5602902466628575
06/02/2019 03:47:50 step: 289, epoch: 8, batch: 24, loss: 0.9071063995361328, acc: 65.625, f1: 32.648809523809526, r: 0.517920542934269
06/02/2019 03:47:51 step: 294, epoch: 8, batch: 29, loss: 0.8137840628623962, acc: 75.0, f1: 43.21640789725896, r: 0.5015011217471017
06/02/2019 03:47:51 *** evaluating ***
06/02/2019 03:47:51 step: 9, epoch: 8, acc: 54.27350427350427, f1: 19.54975454770369, r: 0.3465968274555672
06/02/2019 03:47:51 *** epoch: 10 ***
06/02/2019 03:47:51 *** training ***
06/02/2019 03:47:51 step: 302, epoch: 9, batch: 4, loss: 0.8595112562179565, acc: 70.3125, f1: 43.496646795827125, r: 0.5155807095128053
06/02/2019 03:47:52 step: 307, epoch: 9, batch: 9, loss: 0.820303738117218, acc: 70.3125, f1: 39.59529505582137, r: 0.612469281444913
06/02/2019 03:47:52 step: 312, epoch: 9, batch: 14, loss: 0.5667276382446289, acc: 85.9375, f1: 57.25172901813133, r: 0.4694060012342203
06/02/2019 03:47:52 step: 317, epoch: 9, batch: 19, loss: 1.0342333316802979, acc: 54.6875, f1: 35.52838164251208, r: 0.5030251720267813
06/02/2019 03:47:52 step: 322, epoch: 9, batch: 24, loss: 1.0342402458190918, acc: 65.625, f1: 43.2032736466234, r: 0.4549925106307247
06/02/2019 03:47:53 step: 327, epoch: 9, batch: 29, loss: 0.8965674638748169, acc: 70.3125, f1: 48.263882297104885, r: 0.5096438057942573
06/02/2019 03:47:53 *** evaluating ***
06/02/2019 03:47:53 step: 10, epoch: 9, acc: 54.27350427350427, f1: 21.891862485196, r: 0.3658509307824757
06/02/2019 03:47:53 *** epoch: 11 ***
06/02/2019 03:47:53 *** training ***
06/02/2019 03:47:53 step: 335, epoch: 10, batch: 4, loss: 0.8058620095252991, acc: 70.3125, f1: 54.48153899240855, r: 0.5359769496636076
06/02/2019 03:47:54 step: 340, epoch: 10, batch: 9, loss: 0.6896758675575256, acc: 71.875, f1: 70.35029065389597, r: 0.559195638185895
06/02/2019 03:47:54 step: 345, epoch: 10, batch: 14, loss: 0.7074457406997681, acc: 75.0, f1: 55.76513764013764, r: 0.625432577102546
06/02/2019 03:47:54 step: 350, epoch: 10, batch: 19, loss: 0.7568027377128601, acc: 82.8125, f1: 62.41579926231557, r: 0.5904871174433125
06/02/2019 03:47:55 step: 355, epoch: 10, batch: 24, loss: 0.6208347678184509, acc: 79.6875, f1: 55.445436507936506, r: 0.6094811609408359
06/02/2019 03:47:55 step: 360, epoch: 10, batch: 29, loss: 0.6161152124404907, acc: 76.5625, f1: 48.48195549406203, r: 0.4462729965136026
06/02/2019 03:47:55 *** evaluating ***
06/02/2019 03:47:55 step: 11, epoch: 10, acc: 55.12820512820513, f1: 23.222075346992728, r: 0.3637555072307656
06/02/2019 03:47:55 *** epoch: 12 ***
06/02/2019 03:47:55 *** training ***
06/02/2019 03:47:56 step: 368, epoch: 11, batch: 4, loss: 0.7885787487030029, acc: 75.0, f1: 56.035353535353536, r: 0.5808094019126648
06/02/2019 03:47:56 step: 373, epoch: 11, batch: 9, loss: 0.6952990293502808, acc: 75.0, f1: 61.08614951356886, r: 0.5816527718644844
06/02/2019 03:47:56 step: 378, epoch: 11, batch: 14, loss: 0.7039711475372314, acc: 73.4375, f1: 67.4141193439439, r: 0.528898775803877
06/02/2019 03:47:56 step: 383, epoch: 11, batch: 19, loss: 0.7525370121002197, acc: 73.4375, f1: 59.62433458231777, r: 0.5238978476055076
06/02/2019 03:47:57 step: 388, epoch: 11, batch: 24, loss: 0.766991376876831, acc: 75.0, f1: 62.49905891210239, r: 0.5650663917193245
06/02/2019 03:47:57 step: 393, epoch: 11, batch: 29, loss: 0.5206953883171082, acc: 84.375, f1: 69.04573453080917, r: 0.6078051125205659
06/02/2019 03:47:57 *** evaluating ***
06/02/2019 03:47:57 step: 12, epoch: 11, acc: 53.84615384615385, f1: 23.536181216518436, r: 0.3647886036510952
06/02/2019 03:47:57 *** epoch: 13 ***
06/02/2019 03:47:57 *** training ***
06/02/2019 03:47:58 step: 401, epoch: 12, batch: 4, loss: 0.6260728240013123, acc: 78.125, f1: 49.68223266095607, r: 0.43437078849795796
06/02/2019 03:47:58 step: 406, epoch: 12, batch: 9, loss: 0.7728186845779419, acc: 70.3125, f1: 57.22025462626965, r: 0.4906214491664216
06/02/2019 03:47:58 step: 411, epoch: 12, batch: 14, loss: 0.4859524071216583, acc: 85.9375, f1: 70.61714516315, r: 0.567283926556303
06/02/2019 03:47:58 step: 416, epoch: 12, batch: 19, loss: 0.6105653047561646, acc: 78.125, f1: 67.0022041450613, r: 0.6184938628205268
06/02/2019 03:47:59 step: 421, epoch: 12, batch: 24, loss: 0.6012730002403259, acc: 79.6875, f1: 62.18379316535998, r: 0.6192783685446349
06/02/2019 03:47:59 step: 426, epoch: 12, batch: 29, loss: 0.4852297306060791, acc: 89.0625, f1: 86.46291398571033, r: 0.6160856045616495
06/02/2019 03:47:59 *** evaluating ***
06/02/2019 03:47:59 step: 13, epoch: 12, acc: 54.700854700854705, f1: 23.69307280021566, r: 0.37450719765344487
06/02/2019 03:47:59 *** epoch: 14 ***
06/02/2019 03:47:59 *** training ***
06/02/2019 03:48:00 step: 434, epoch: 13, batch: 4, loss: 0.47047412395477295, acc: 84.375, f1: 69.63419129179998, r: 0.7185973505527069
06/02/2019 03:48:00 step: 439, epoch: 13, batch: 9, loss: 0.5675899386405945, acc: 78.125, f1: 58.208826293675365, r: 0.6570750242298014
06/02/2019 03:48:00 step: 444, epoch: 13, batch: 14, loss: 0.3935595750808716, acc: 84.375, f1: 62.60429446503878, r: 0.5762241469507361
06/02/2019 03:48:00 step: 449, epoch: 13, batch: 19, loss: 0.6928498148918152, acc: 78.125, f1: 71.68667810177244, r: 0.6032294657915525
06/02/2019 03:48:01 step: 454, epoch: 13, batch: 24, loss: 0.6020724177360535, acc: 81.25, f1: 68.95292207792207, r: 0.7112582875846475
06/02/2019 03:48:01 step: 459, epoch: 13, batch: 29, loss: 0.5322247743606567, acc: 84.375, f1: 72.3560658343267, r: 0.6465292526995502
06/02/2019 03:48:01 *** evaluating ***
06/02/2019 03:48:01 step: 14, epoch: 13, acc: 58.54700854700855, f1: 27.92544178261467, r: 0.37721257276021164
06/02/2019 03:48:01 *** epoch: 15 ***
06/02/2019 03:48:01 *** training ***
06/02/2019 03:48:02 step: 467, epoch: 14, batch: 4, loss: 0.5797771215438843, acc: 81.25, f1: 58.549783549783555, r: 0.6471076875706527
06/02/2019 03:48:02 step: 472, epoch: 14, batch: 9, loss: 0.4572761058807373, acc: 92.1875, f1: 74.16895604395604, r: 0.5949671859420742
06/02/2019 03:48:02 step: 477, epoch: 14, batch: 14, loss: 0.5390497446060181, acc: 81.25, f1: 63.39191605495953, r: 0.716548494452089
06/02/2019 03:48:02 step: 482, epoch: 14, batch: 19, loss: 0.38273632526397705, acc: 87.5, f1: 82.14855320118478, r: 0.599412517808093
06/02/2019 03:48:03 step: 487, epoch: 14, batch: 24, loss: 0.36000871658325195, acc: 87.5, f1: 64.70716393330132, r: 0.7039008700970584
06/02/2019 03:48:03 step: 492, epoch: 14, batch: 29, loss: 0.4925403892993927, acc: 84.375, f1: 80.38593481989709, r: 0.6138309835278822
06/02/2019 03:48:03 *** evaluating ***
06/02/2019 03:48:03 step: 15, epoch: 14, acc: 53.84615384615385, f1: 25.43262987012987, r: 0.3711144565160333
06/02/2019 03:48:03 *** epoch: 16 ***
06/02/2019 03:48:03 *** training ***
06/02/2019 03:48:04 step: 500, epoch: 15, batch: 4, loss: 0.23969200253486633, acc: 92.1875, f1: 92.93002915451895, r: 0.6447932040955132
06/02/2019 03:48:04 step: 505, epoch: 15, batch: 9, loss: 0.4484224021434784, acc: 87.5, f1: 77.50885348540312, r: 0.7242955249810722
06/02/2019 03:48:04 step: 510, epoch: 15, batch: 14, loss: 0.41755983233451843, acc: 87.5, f1: 75.43456543456544, r: 0.6819976629222733
06/02/2019 03:48:04 step: 515, epoch: 15, batch: 19, loss: 0.3094032108783722, acc: 89.0625, f1: 78.04552651099965, r: 0.7011784187887887
06/02/2019 03:48:05 step: 520, epoch: 15, batch: 24, loss: 0.4034690260887146, acc: 87.5, f1: 78.51641414141413, r: 0.7022726015722526
06/02/2019 03:48:05 step: 525, epoch: 15, batch: 29, loss: 0.37038424611091614, acc: 85.9375, f1: 70.53241466077958, r: 0.6703980633601937
06/02/2019 03:48:05 *** evaluating ***
06/02/2019 03:48:05 step: 16, epoch: 15, acc: 56.41025641025641, f1: 24.97207302461178, r: 0.37129528911827336
06/02/2019 03:48:05 *** epoch: 17 ***
06/02/2019 03:48:05 *** training ***
06/02/2019 03:48:05 step: 533, epoch: 16, batch: 4, loss: 0.383263498544693, acc: 90.625, f1: 75.58631528046422, r: 0.7296237047077102
06/02/2019 03:48:06 step: 538, epoch: 16, batch: 9, loss: 0.30739229917526245, acc: 90.625, f1: 85.16941391941391, r: 0.7316426482614479
06/02/2019 03:48:06 step: 543, epoch: 16, batch: 14, loss: 0.3942186236381531, acc: 85.9375, f1: 72.10343822843824, r: 0.7158843154020382
06/02/2019 03:48:06 step: 548, epoch: 16, batch: 19, loss: 0.449889600276947, acc: 85.9375, f1: 75.81536689911076, r: 0.5968478765186056
06/02/2019 03:48:07 step: 553, epoch: 16, batch: 24, loss: 0.43308696150779724, acc: 84.375, f1: 68.00794561988592, r: 0.6290953131656539
06/02/2019 03:48:07 step: 558, epoch: 16, batch: 29, loss: 0.3309016823768616, acc: 85.9375, f1: 83.16441728206433, r: 0.6444321505398648
06/02/2019 03:48:07 *** evaluating ***
06/02/2019 03:48:07 step: 17, epoch: 16, acc: 56.41025641025641, f1: 25.933238681065085, r: 0.372924849272658
06/02/2019 03:48:07 *** epoch: 18 ***
06/02/2019 03:48:07 *** training ***
06/02/2019 03:48:07 step: 566, epoch: 17, batch: 4, loss: 0.24299535155296326, acc: 93.75, f1: 92.57912976511359, r: 0.6702439912602772
06/02/2019 03:48:08 step: 571, epoch: 17, batch: 9, loss: 0.2869682013988495, acc: 93.75, f1: 80.45723398984269, r: 0.7129376226228786
06/02/2019 03:48:08 step: 576, epoch: 17, batch: 14, loss: 0.37837499380111694, acc: 87.5, f1: 72.95187451437451, r: 0.6606435635718729
06/02/2019 03:48:08 step: 581, epoch: 17, batch: 19, loss: 0.3515869379043579, acc: 90.625, f1: 70.34260443307757, r: 0.6357212431788298
06/02/2019 03:48:08 step: 586, epoch: 17, batch: 24, loss: 0.4206300973892212, acc: 87.5, f1: 80.6820163963021, r: 0.6318798595503039
06/02/2019 03:48:09 step: 591, epoch: 17, batch: 29, loss: 0.35813722014427185, acc: 87.5, f1: 73.16017316017316, r: 0.5311790770579244
06/02/2019 03:48:09 *** evaluating ***
06/02/2019 03:48:09 step: 18, epoch: 17, acc: 56.41025641025641, f1: 25.318996400212978, r: 0.37640441830729116
06/02/2019 03:48:09 *** epoch: 19 ***
06/02/2019 03:48:09 *** training ***
06/02/2019 03:48:09 step: 599, epoch: 18, batch: 4, loss: 0.30086997151374817, acc: 90.625, f1: 88.28892240656947, r: 0.7622193713214778
06/02/2019 03:48:10 step: 604, epoch: 18, batch: 9, loss: 0.4537777006626129, acc: 82.8125, f1: 71.77897899972368, r: 0.6944762122105176
06/02/2019 03:48:10 step: 609, epoch: 18, batch: 14, loss: 0.208783358335495, acc: 93.75, f1: 89.45296385974353, r: 0.5191589576389245
06/02/2019 03:48:10 step: 614, epoch: 18, batch: 19, loss: 0.3141557276248932, acc: 85.9375, f1: 69.90315650878348, r: 0.5375229234855462
06/02/2019 03:48:10 step: 619, epoch: 18, batch: 24, loss: 0.27571460604667664, acc: 90.625, f1: 78.0390565317036, r: 0.762667411326657
06/02/2019 03:48:11 step: 624, epoch: 18, batch: 29, loss: 0.3476998805999756, acc: 85.9375, f1: 68.41948621553885, r: 0.6969574589997579
06/02/2019 03:48:11 *** evaluating ***
06/02/2019 03:48:11 step: 19, epoch: 18, acc: 54.27350427350427, f1: 25.293820387941835, r: 0.3628883400716579
06/02/2019 03:48:11 *** epoch: 20 ***
06/02/2019 03:48:11 *** training ***
06/02/2019 03:48:11 step: 632, epoch: 19, batch: 4, loss: 0.373695969581604, acc: 87.5, f1: 85.1702152722561, r: 0.6242968706438274
06/02/2019 03:48:11 step: 637, epoch: 19, batch: 9, loss: 0.22472964227199554, acc: 93.75, f1: 93.47943722943724, r: 0.7159023162056142
06/02/2019 03:48:12 step: 642, epoch: 19, batch: 14, loss: 0.32140621542930603, acc: 90.625, f1: 79.93291601665986, r: 0.6077340373360917
06/02/2019 03:48:12 step: 647, epoch: 19, batch: 19, loss: 0.2807329595088959, acc: 89.0625, f1: 85.82367470111099, r: 0.7186271348874355
06/02/2019 03:48:12 step: 652, epoch: 19, batch: 24, loss: 0.21579128503799438, acc: 92.1875, f1: 89.65802537231109, r: 0.6885457811659936
06/02/2019 03:48:13 step: 657, epoch: 19, batch: 29, loss: 0.3739059269428253, acc: 85.9375, f1: 79.85203321268895, r: 0.6778789633826782
06/02/2019 03:48:13 *** evaluating ***
06/02/2019 03:48:13 step: 20, epoch: 19, acc: 52.991452991452995, f1: 22.968824853885213, r: 0.3655035244171096
06/02/2019 03:48:13 *** epoch: 21 ***
06/02/2019 03:48:13 *** training ***
06/02/2019 03:48:13 step: 665, epoch: 20, batch: 4, loss: 0.24166813492774963, acc: 93.75, f1: 91.8391542544085, r: 0.7986794267660404
06/02/2019 03:48:13 step: 670, epoch: 20, batch: 9, loss: 0.19916334748268127, acc: 95.3125, f1: 80.14208014208015, r: 0.6965104496576016
06/02/2019 03:48:14 step: 675, epoch: 20, batch: 14, loss: 0.1638958752155304, acc: 95.3125, f1: 93.8333730364091, r: 0.6942686287906649
06/02/2019 03:48:14 step: 680, epoch: 20, batch: 19, loss: 0.25871220231056213, acc: 90.625, f1: 85.56181093126905, r: 0.6297821751542304
06/02/2019 03:48:14 step: 685, epoch: 20, batch: 24, loss: 0.19976630806922913, acc: 92.1875, f1: 79.91346153846153, r: 0.7294859898702499
06/02/2019 03:48:15 step: 690, epoch: 20, batch: 29, loss: 0.22811853885650635, acc: 90.625, f1: 71.19263285024154, r: 0.7580181631830453
06/02/2019 03:48:15 *** evaluating ***
06/02/2019 03:48:15 step: 21, epoch: 20, acc: 55.12820512820513, f1: 24.736607882099683, r: 0.37068733966059453
06/02/2019 03:48:15 *** epoch: 22 ***
06/02/2019 03:48:15 *** training ***
06/02/2019 03:48:15 step: 698, epoch: 21, batch: 4, loss: 0.20981267094612122, acc: 96.875, f1: 94.90229885057472, r: 0.7177524463089664
06/02/2019 03:48:16 step: 703, epoch: 21, batch: 9, loss: 0.3314451277256012, acc: 87.5, f1: 86.94868856633562, r: 0.6840729375231968
06/02/2019 03:48:16 step: 708, epoch: 21, batch: 14, loss: 0.14928042888641357, acc: 96.875, f1: 96.58489658489658, r: 0.6253040348004545
06/02/2019 03:48:16 step: 713, epoch: 21, batch: 19, loss: 0.3014562129974365, acc: 92.1875, f1: 80.14437087966499, r: 0.7275388798352996
06/02/2019 03:48:17 step: 718, epoch: 21, batch: 24, loss: 0.1493019461631775, acc: 93.75, f1: 77.69660894660895, r: 0.696299604347182
06/02/2019 03:48:17 step: 723, epoch: 21, batch: 29, loss: 0.3086206018924713, acc: 90.625, f1: 83.34880538005538, r: 0.7488972528261776
06/02/2019 03:48:17 *** evaluating ***
06/02/2019 03:48:17 step: 22, epoch: 21, acc: 56.41025641025641, f1: 23.754299766842646, r: 0.3679142903847613
06/02/2019 03:48:17 *** epoch: 23 ***
06/02/2019 03:48:17 *** training ***
06/02/2019 03:48:18 step: 731, epoch: 22, batch: 4, loss: 0.13315513730049133, acc: 96.875, f1: 94.47265221878226, r: 0.5173630266265791
06/02/2019 03:48:18 step: 736, epoch: 22, batch: 9, loss: 0.08487416803836823, acc: 98.4375, f1: 87.03703703703704, r: 0.8376412226067355
06/02/2019 03:48:18 step: 741, epoch: 22, batch: 14, loss: 0.16746345162391663, acc: 98.4375, f1: 97.43008314436887, r: 0.7072250249399007
06/02/2019 03:48:18 step: 746, epoch: 22, batch: 19, loss: 0.27892717719078064, acc: 90.625, f1: 87.11735423407522, r: 0.73983127737613
06/02/2019 03:48:19 step: 751, epoch: 22, batch: 24, loss: 0.17627498507499695, acc: 95.3125, f1: 80.64076817946787, r: 0.7378784980934364
06/02/2019 03:48:19 step: 756, epoch: 22, batch: 29, loss: 0.14494164288043976, acc: 96.875, f1: 96.95196480981343, r: 0.7842542965108167
06/02/2019 03:48:19 *** evaluating ***
06/02/2019 03:48:19 step: 23, epoch: 22, acc: 55.12820512820513, f1: 24.217913832199546, r: 0.36492467273226226
06/02/2019 03:48:19 *** epoch: 24 ***
06/02/2019 03:48:19 *** training ***
06/02/2019 03:48:20 step: 764, epoch: 23, batch: 4, loss: 0.16570082306861877, acc: 98.4375, f1: 96.36363636363636, r: 0.7770211213617448
06/02/2019 03:48:20 step: 769, epoch: 23, batch: 9, loss: 0.28662559390068054, acc: 93.75, f1: 91.36845435848757, r: 0.6535417269401875
06/02/2019 03:48:20 step: 774, epoch: 23, batch: 14, loss: 0.10308481007814407, acc: 98.4375, f1: 86.36363636363636, r: 0.807498093565959
06/02/2019 03:48:20 step: 779, epoch: 23, batch: 19, loss: 0.19642293453216553, acc: 93.75, f1: 88.71622495151908, r: 0.645370113393769
06/02/2019 03:48:21 step: 784, epoch: 23, batch: 24, loss: 0.1908240020275116, acc: 93.75, f1: 80.6161953988041, r: 0.7067290480664427
06/02/2019 03:48:21 step: 789, epoch: 23, batch: 29, loss: 0.09990545362234116, acc: 98.4375, f1: 97.94941900205059, r: 0.6886971918272973
06/02/2019 03:48:21 *** evaluating ***
06/02/2019 03:48:21 step: 24, epoch: 23, acc: 56.837606837606835, f1: 25.159843371029485, r: 0.3616337110137802
06/02/2019 03:48:21 *** epoch: 25 ***
06/02/2019 03:48:21 *** training ***
06/02/2019 03:48:22 step: 797, epoch: 24, batch: 4, loss: 0.2345375418663025, acc: 90.625, f1: 76.87769283513964, r: 0.6857899852482783
06/02/2019 03:48:22 step: 802, epoch: 24, batch: 9, loss: 0.1340952217578888, acc: 95.3125, f1: 96.8042229230784, r: 0.6777471567706336
06/02/2019 03:48:22 step: 807, epoch: 24, batch: 14, loss: 0.18069933354854584, acc: 96.875, f1: 91.27745960044096, r: 0.6884055921707316
06/02/2019 03:48:22 step: 812, epoch: 24, batch: 19, loss: 0.16613604128360748, acc: 92.1875, f1: 75.00988433650973, r: 0.7339767738025371
06/02/2019 03:48:23 step: 817, epoch: 24, batch: 24, loss: 0.20143547654151917, acc: 93.75, f1: 84.64646464646465, r: 0.6377431647134811
06/02/2019 03:48:23 step: 822, epoch: 24, batch: 29, loss: 0.16305722296237946, acc: 95.3125, f1: 96.08948429385772, r: 0.7694673775942587
06/02/2019 03:48:23 *** evaluating ***
06/02/2019 03:48:23 step: 25, epoch: 24, acc: 57.692307692307686, f1: 25.48716452907384, r: 0.36135805626212175
06/02/2019 03:48:23 *** epoch: 26 ***
06/02/2019 03:48:23 *** training ***
06/02/2019 03:48:23 step: 830, epoch: 25, batch: 4, loss: 0.258100688457489, acc: 89.0625, f1: 83.65498272413166, r: 0.6816768354169145
06/02/2019 03:48:24 step: 835, epoch: 25, batch: 9, loss: 0.10236652940511703, acc: 98.4375, f1: 94.6969696969697, r: 0.7271995543461086
06/02/2019 03:48:24 step: 840, epoch: 25, batch: 14, loss: 0.15241128206253052, acc: 95.3125, f1: 94.91307634164777, r: 0.6934194667893904
06/02/2019 03:48:24 step: 845, epoch: 25, batch: 19, loss: 0.1952541619539261, acc: 95.3125, f1: 93.08694209391074, r: 0.6667986388752798
06/02/2019 03:48:25 step: 850, epoch: 25, batch: 24, loss: 0.15584923326969147, acc: 96.875, f1: 94.59270887842315, r: 0.7278504835617269
06/02/2019 03:48:25 step: 855, epoch: 25, batch: 29, loss: 0.12690477073192596, acc: 98.4375, f1: 97.33806566104703, r: 0.6808757365975597
06/02/2019 03:48:25 *** evaluating ***
06/02/2019 03:48:25 step: 26, epoch: 25, acc: 56.41025641025641, f1: 25.319780919420136, r: 0.36041759350945624
06/02/2019 03:48:25 *** epoch: 27 ***
06/02/2019 03:48:25 *** training ***
06/02/2019 03:48:26 step: 863, epoch: 26, batch: 4, loss: 0.15616294741630554, acc: 95.3125, f1: 97.38095238095238, r: 0.745657570263091
06/02/2019 03:48:26 step: 868, epoch: 26, batch: 9, loss: 0.13204219937324524, acc: 93.75, f1: 95.52507583119828, r: 0.6204108011024863
06/02/2019 03:48:26 step: 873, epoch: 26, batch: 14, loss: 0.13460621237754822, acc: 98.4375, f1: 97.28070175438597, r: 0.8115039418972607
06/02/2019 03:48:26 step: 878, epoch: 26, batch: 19, loss: 0.1197425052523613, acc: 98.4375, f1: 93.93939393939394, r: 0.6556053692661663
06/02/2019 03:48:27 step: 883, epoch: 26, batch: 24, loss: 0.19402523338794708, acc: 93.75, f1: 91.28975247573631, r: 0.6519494663702676
06/02/2019 03:48:27 step: 888, epoch: 26, batch: 29, loss: 0.10052447021007538, acc: 96.875, f1: 83.94660894660895, r: 0.777244964437894
06/02/2019 03:48:27 *** evaluating ***
06/02/2019 03:48:27 step: 27, epoch: 26, acc: 55.12820512820513, f1: 24.22607043887837, r: 0.3662641871711225
06/02/2019 03:48:27 *** epoch: 28 ***
06/02/2019 03:48:27 *** training ***
06/02/2019 03:48:28 step: 896, epoch: 27, batch: 4, loss: 0.09512493759393692, acc: 100.0, f1: 100.0, r: 0.7241222109689792
06/02/2019 03:48:28 step: 901, epoch: 27, batch: 9, loss: 0.04317321628332138, acc: 100.0, f1: 100.0, r: 0.7461085101772318
06/02/2019 03:48:28 step: 906, epoch: 27, batch: 14, loss: 0.03376686945557594, acc: 100.0, f1: 100.0, r: 0.6275269174799332
06/02/2019 03:48:28 step: 911, epoch: 27, batch: 19, loss: 0.16572630405426025, acc: 93.75, f1: 81.16568230420222, r: 0.597346462384228
06/02/2019 03:48:29 step: 916, epoch: 27, batch: 24, loss: 0.1545024961233139, acc: 96.875, f1: 96.6186839012926, r: 0.8207352639117658
06/02/2019 03:48:29 step: 921, epoch: 27, batch: 29, loss: 0.1660965085029602, acc: 93.75, f1: 84.82127192982456, r: 0.6886566232088437
06/02/2019 03:48:29 *** evaluating ***
06/02/2019 03:48:29 step: 28, epoch: 27, acc: 55.12820512820513, f1: 24.383401495287337, r: 0.35472941053721924
06/02/2019 03:48:29 *** epoch: 29 ***
06/02/2019 03:48:29 *** training ***
06/02/2019 03:48:30 step: 929, epoch: 28, batch: 4, loss: 0.1621619164943695, acc: 92.1875, f1: 69.46849672883508, r: 0.6851355103585991
06/02/2019 03:48:30 step: 934, epoch: 28, batch: 9, loss: 0.21114398539066315, acc: 92.1875, f1: 80.41549405878673, r: 0.7939912136279553
06/02/2019 03:48:30 step: 939, epoch: 28, batch: 14, loss: 0.07606273889541626, acc: 98.4375, f1: 97.64309764309765, r: 0.5133895393348069
06/02/2019 03:48:30 step: 944, epoch: 28, batch: 19, loss: 0.11833939701318741, acc: 96.875, f1: 95.57214973634007, r: 0.6566537470248335
06/02/2019 03:48:31 step: 949, epoch: 28, batch: 24, loss: 0.15554949641227722, acc: 93.75, f1: 90.08919843597263, r: 0.7446472744412022
06/02/2019 03:48:31 step: 954, epoch: 28, batch: 29, loss: 0.11982936412096024, acc: 100.0, f1: 100.0, r: 0.810284881006181
06/02/2019 03:48:31 *** evaluating ***
06/02/2019 03:48:31 step: 29, epoch: 28, acc: 57.26495726495726, f1: 25.28763526359008, r: 0.3621530552879833
06/02/2019 03:48:31 *** epoch: 30 ***
06/02/2019 03:48:31 *** training ***
06/02/2019 03:48:31 step: 962, epoch: 29, batch: 4, loss: 0.09063808619976044, acc: 98.4375, f1: 95.84415584415584, r: 0.6993050733297753
06/02/2019 03:48:32 step: 967, epoch: 29, batch: 9, loss: 0.08623528480529785, acc: 96.875, f1: 94.94655004859086, r: 0.654703633497926
06/02/2019 03:48:32 step: 972, epoch: 29, batch: 14, loss: 0.13415837287902832, acc: 95.3125, f1: 93.54790137398832, r: 0.6399227467560853
06/02/2019 03:48:32 step: 977, epoch: 29, batch: 19, loss: 0.11020106077194214, acc: 93.75, f1: 90.23124389051809, r: 0.7601560527118524
06/02/2019 03:48:33 step: 982, epoch: 29, batch: 24, loss: 0.17471244931221008, acc: 90.625, f1: 80.68757288022655, r: 0.6899464163440333
06/02/2019 03:48:33 step: 987, epoch: 29, batch: 29, loss: 0.11680017411708832, acc: 96.875, f1: 94.26099792313897, r: 0.7042703582853173
06/02/2019 03:48:33 *** evaluating ***
06/02/2019 03:48:33 step: 30, epoch: 29, acc: 56.41025641025641, f1: 25.493048643488684, r: 0.36087552031038495
06/02/2019 03:48:33 *** epoch: 31 ***
06/02/2019 03:48:33 *** training ***
06/02/2019 03:48:33 step: 995, epoch: 30, batch: 4, loss: 0.042574476450681686, acc: 100.0, f1: 100.0, r: 0.7146370759939639
06/02/2019 03:48:34 step: 1000, epoch: 30, batch: 9, loss: 0.11903475970029831, acc: 100.0, f1: 100.0, r: 0.7900782418000458
06/02/2019 03:48:34 step: 1005, epoch: 30, batch: 14, loss: 0.12331903725862503, acc: 96.875, f1: 95.59044130472702, r: 0.7034573414689226
06/02/2019 03:48:34 step: 1010, epoch: 30, batch: 19, loss: 0.08114787936210632, acc: 96.875, f1: 94.34968971914785, r: 0.644135662236918
06/02/2019 03:48:35 step: 1015, epoch: 30, batch: 24, loss: 0.19226022064685822, acc: 95.3125, f1: 90.67665488150664, r: 0.6377456240627224
06/02/2019 03:48:35 step: 1020, epoch: 30, batch: 29, loss: 0.05116206035017967, acc: 98.4375, f1: 96.57142857142857, r: 0.6648057297355258
06/02/2019 03:48:35 *** evaluating ***
06/02/2019 03:48:35 step: 31, epoch: 30, acc: 56.41025641025641, f1: 24.98151307507479, r: 0.3592166876284686
06/02/2019 03:48:35 *** epoch: 32 ***
06/02/2019 03:48:35 *** training ***
06/02/2019 03:48:35 step: 1028, epoch: 31, batch: 4, loss: 0.11675477027893066, acc: 96.875, f1: 93.10634204251225, r: 0.6405344029808121
06/02/2019 03:48:36 step: 1033, epoch: 31, batch: 9, loss: 0.11073421686887741, acc: 96.875, f1: 95.47390938491641, r: 0.6711135996500405
06/02/2019 03:48:36 step: 1038, epoch: 31, batch: 14, loss: 0.07653836905956268, acc: 98.4375, f1: 99.23784738358583, r: 0.6754494261884946
06/02/2019 03:48:36 step: 1043, epoch: 31, batch: 19, loss: 0.1258823573589325, acc: 95.3125, f1: 94.47870363064081, r: 0.8650601452680327
06/02/2019 03:48:36 step: 1048, epoch: 31, batch: 24, loss: 0.10403161495923996, acc: 95.3125, f1: 88.20050125313284, r: 0.6844147416480545
06/02/2019 03:48:37 step: 1053, epoch: 31, batch: 29, loss: 0.08055907487869263, acc: 98.4375, f1: 98.6086956521739, r: 0.6569776101463563
06/02/2019 03:48:37 *** evaluating ***
06/02/2019 03:48:37 step: 32, epoch: 31, acc: 55.55555555555556, f1: 24.117379883291424, r: 0.3579713704176525
06/02/2019 03:48:37 *** epoch: 33 ***
06/02/2019 03:48:37 *** training ***
06/02/2019 03:48:37 step: 1061, epoch: 32, batch: 4, loss: 0.07406112551689148, acc: 98.4375, f1: 96.1111111111111, r: 0.8624407925069748
06/02/2019 03:48:38 step: 1066, epoch: 32, batch: 9, loss: 0.09107451885938644, acc: 98.4375, f1: 99.28167370027835, r: 0.7223635720835844
06/02/2019 03:48:38 step: 1071, epoch: 32, batch: 14, loss: 0.13649988174438477, acc: 96.875, f1: 96.03743315508022, r: 0.7798518957675283
06/02/2019 03:48:38 step: 1076, epoch: 32, batch: 19, loss: 0.12442061305046082, acc: 93.75, f1: 88.1382275132275, r: 0.7985600633354942
06/02/2019 03:48:38 step: 1081, epoch: 32, batch: 24, loss: 0.06763328611850739, acc: 98.4375, f1: 99.25961082107261, r: 0.7397105115661796
06/02/2019 03:48:39 step: 1086, epoch: 32, batch: 29, loss: 0.08182888478040695, acc: 96.875, f1: 97.25490196078432, r: 0.6533364788342312
06/02/2019 03:48:39 *** evaluating ***
06/02/2019 03:48:39 step: 33, epoch: 32, acc: 58.119658119658126, f1: 25.900181060371953, r: 0.3530803083142973
06/02/2019 03:48:39 *** epoch: 34 ***
06/02/2019 03:48:39 *** training ***
06/02/2019 03:48:39 step: 1094, epoch: 33, batch: 4, loss: 0.09210105240345001, acc: 98.4375, f1: 97.64172335600908, r: 0.6756013592869635
06/02/2019 03:48:40 step: 1099, epoch: 33, batch: 9, loss: 0.13754867017269135, acc: 95.3125, f1: 83.73015873015873, r: 0.7511324936285121
06/02/2019 03:48:40 step: 1104, epoch: 33, batch: 14, loss: 0.05065164715051651, acc: 98.4375, f1: 96.3718820861678, r: 0.7326921185537184
06/02/2019 03:48:40 step: 1109, epoch: 33, batch: 19, loss: 0.1378958821296692, acc: 96.875, f1: 89.50738916256158, r: 0.6628504577573621
06/02/2019 03:48:41 step: 1114, epoch: 33, batch: 24, loss: 0.13854455947875977, acc: 95.3125, f1: 95.48903089663959, r: 0.778695827398986
06/02/2019 03:48:41 step: 1119, epoch: 33, batch: 29, loss: 0.060542285442352295, acc: 100.0, f1: 100.0, r: 0.7935403835613094
06/02/2019 03:48:41 *** evaluating ***
06/02/2019 03:48:41 step: 34, epoch: 33, acc: 55.98290598290598, f1: 24.336080586080584, r: 0.3557695436058672
06/02/2019 03:48:41 *** epoch: 35 ***
06/02/2019 03:48:41 *** training ***
06/02/2019 03:48:41 step: 1127, epoch: 34, batch: 4, loss: 0.15930062532424927, acc: 95.3125, f1: 92.84874355311698, r: 0.8214557114133391
06/02/2019 03:48:42 step: 1132, epoch: 34, batch: 9, loss: 0.11533361673355103, acc: 98.4375, f1: 97.27891156462584, r: 0.6799227765586476
06/02/2019 03:48:42 step: 1137, epoch: 34, batch: 14, loss: 0.07029489427804947, acc: 98.4375, f1: 86.36363636363636, r: 0.7383992583529417
06/02/2019 03:48:42 step: 1142, epoch: 34, batch: 19, loss: 0.09418763965368271, acc: 98.4375, f1: 95.55555555555556, r: 0.7054328734325339
06/02/2019 03:48:43 step: 1147, epoch: 34, batch: 24, loss: 0.08833103626966476, acc: 98.4375, f1: 99.10627007401202, r: 0.6566878491980113
06/02/2019 03:48:43 step: 1152, epoch: 34, batch: 29, loss: 0.10494720190763474, acc: 98.4375, f1: 99.31899641577061, r: 0.7414434884636273
06/02/2019 03:48:43 *** evaluating ***
06/02/2019 03:48:43 step: 35, epoch: 34, acc: 56.41025641025641, f1: 25.12767989169315, r: 0.3502023142036145
06/02/2019 03:48:43 *** epoch: 36 ***
06/02/2019 03:48:43 *** training ***
06/02/2019 03:48:44 step: 1160, epoch: 35, batch: 4, loss: 0.0952664464712143, acc: 98.4375, f1: 86.90476190476191, r: 0.7835734136031034
06/02/2019 03:48:44 step: 1165, epoch: 35, batch: 9, loss: 0.05660371109843254, acc: 98.4375, f1: 97.99498746867168, r: 0.7742236106629744
06/02/2019 03:48:44 step: 1170, epoch: 35, batch: 14, loss: 0.05685599148273468, acc: 98.4375, f1: 99.12462006079028, r: 0.647387587032015
06/02/2019 03:48:44 step: 1175, epoch: 35, batch: 19, loss: 0.128942608833313, acc: 96.875, f1: 93.73358348968105, r: 0.7195202738053933
06/02/2019 03:48:45 step: 1180, epoch: 35, batch: 24, loss: 0.11041096597909927, acc: 96.875, f1: 84.80128893662729, r: 0.7218298557559246
06/02/2019 03:48:45 step: 1185, epoch: 35, batch: 29, loss: 0.11487709730863571, acc: 95.3125, f1: 94.8098661977762, r: 0.6750613668027313
06/02/2019 03:48:45 *** evaluating ***
06/02/2019 03:48:45 step: 36, epoch: 35, acc: 55.12820512820513, f1: 24.702834010638945, r: 0.3471228206437734
06/02/2019 03:48:45 *** epoch: 37 ***
06/02/2019 03:48:45 *** training ***
06/02/2019 03:48:46 step: 1193, epoch: 36, batch: 4, loss: 0.08893388509750366, acc: 98.4375, f1: 96.3718820861678, r: 0.6522179219713623
06/02/2019 03:48:46 step: 1198, epoch: 36, batch: 9, loss: 0.0749015361070633, acc: 98.4375, f1: 86.95652173913044, r: 0.7440964397669729
06/02/2019 03:48:46 step: 1203, epoch: 36, batch: 14, loss: 0.04350889474153519, acc: 100.0, f1: 100.0, r: 0.7824593928170878
06/02/2019 03:48:47 step: 1208, epoch: 36, batch: 19, loss: 0.027853840962052345, acc: 100.0, f1: 100.0, r: 0.6521913338268857
06/02/2019 03:48:47 step: 1213, epoch: 36, batch: 24, loss: 0.055454663932323456, acc: 100.0, f1: 100.0, r: 0.729768247116092
06/02/2019 03:48:47 step: 1218, epoch: 36, batch: 29, loss: 0.10129058361053467, acc: 96.875, f1: 97.33401084010839, r: 0.7707040939718169
06/02/2019 03:48:47 *** evaluating ***
06/02/2019 03:48:47 step: 37, epoch: 36, acc: 56.837606837606835, f1: 25.012700664435716, r: 0.34606708016212906
06/02/2019 03:48:47 *** epoch: 38 ***
06/02/2019 03:48:47 *** training ***
06/02/2019 03:48:48 step: 1226, epoch: 37, batch: 4, loss: 0.08895087242126465, acc: 96.875, f1: 97.24515537209037, r: 0.7517435200201403
06/02/2019 03:48:48 step: 1231, epoch: 37, batch: 9, loss: 0.06318364292383194, acc: 100.0, f1: 100.0, r: 0.7361151792436393
06/02/2019 03:48:48 step: 1236, epoch: 37, batch: 14, loss: 0.08814938366413116, acc: 96.875, f1: 93.73543123543124, r: 0.760028846199455
06/02/2019 03:48:49 step: 1241, epoch: 37, batch: 19, loss: 0.0547039620578289, acc: 98.4375, f1: 97.88359788359789, r: 0.6941871053064267
06/02/2019 03:48:49 step: 1246, epoch: 37, batch: 24, loss: 0.12776514887809753, acc: 95.3125, f1: 94.33564482703652, r: 0.8127325927713464
06/02/2019 03:48:49 step: 1251, epoch: 37, batch: 29, loss: 0.04819352924823761, acc: 100.0, f1: 100.0, r: 0.6758108394602971
06/02/2019 03:48:49 *** evaluating ***
06/02/2019 03:48:50 step: 38, epoch: 37, acc: 56.837606837606835, f1: 25.62883181236526, r: 0.35110911421004337
06/02/2019 03:48:50 *** epoch: 39 ***
06/02/2019 03:48:50 *** training ***
06/02/2019 03:48:50 step: 1259, epoch: 38, batch: 4, loss: 0.0625474750995636, acc: 98.4375, f1: 97.55639097744361, r: 0.776213814475757
06/02/2019 03:48:50 step: 1264, epoch: 38, batch: 9, loss: 0.039576370269060135, acc: 100.0, f1: 100.0, r: 0.7774886543725722
06/02/2019 03:48:51 step: 1269, epoch: 38, batch: 14, loss: 0.15596413612365723, acc: 95.3125, f1: 91.09264638869902, r: 0.7680240544725423
06/02/2019 03:48:51 step: 1274, epoch: 38, batch: 19, loss: 0.055263493210077286, acc: 100.0, f1: 100.0, r: 0.6446600736161646
06/02/2019 03:48:51 step: 1279, epoch: 38, batch: 24, loss: 0.0545816533267498, acc: 98.4375, f1: 96.11111111111111, r: 0.829176860383933
06/02/2019 03:48:51 step: 1284, epoch: 38, batch: 29, loss: 0.05362977832555771, acc: 100.0, f1: 100.0, r: 0.7136807106257672
06/02/2019 03:48:51 *** evaluating ***
06/02/2019 03:48:52 step: 39, epoch: 38, acc: 58.119658119658126, f1: 26.07150788363313, r: 0.3435406152386069
06/02/2019 03:48:52 *** epoch: 40 ***
06/02/2019 03:48:52 *** training ***
06/02/2019 03:48:52 step: 1292, epoch: 39, batch: 4, loss: 0.08317479491233826, acc: 96.875, f1: 93.3068783068783, r: 0.749885228972955
06/02/2019 03:48:52 step: 1297, epoch: 39, batch: 9, loss: 0.061788834631443024, acc: 98.4375, f1: 86.53846153846155, r: 0.7387664196814782
06/02/2019 03:48:52 step: 1302, epoch: 39, batch: 14, loss: 0.041713885962963104, acc: 98.4375, f1: 99.12462006079028, r: 0.6391845322270111
06/02/2019 03:48:53 step: 1307, epoch: 39, batch: 19, loss: 0.07947337627410889, acc: 96.875, f1: 97.16899085320138, r: 0.6959731689951167
06/02/2019 03:48:53 step: 1312, epoch: 39, batch: 24, loss: 0.13183800876140594, acc: 95.3125, f1: 97.06766917293234, r: 0.6671478555573486
06/02/2019 03:48:53 step: 1317, epoch: 39, batch: 29, loss: 0.06699630618095398, acc: 100.0, f1: 100.0, r: 0.7400025760519247
06/02/2019 03:48:53 *** evaluating ***
06/02/2019 03:48:54 step: 40, epoch: 39, acc: 54.700854700854705, f1: 24.17598344816411, r: 0.3448804832948362
06/02/2019 03:48:54 *** epoch: 41 ***
06/02/2019 03:48:54 *** training ***
06/02/2019 03:48:54 step: 1325, epoch: 40, batch: 4, loss: 0.07092602550983429, acc: 96.875, f1: 94.37843480396671, r: 0.59399479092395
06/02/2019 03:48:54 step: 1330, epoch: 40, batch: 9, loss: 0.14217078685760498, acc: 93.75, f1: 90.51739926739927, r: 0.7584224311156581
06/02/2019 03:48:54 step: 1335, epoch: 40, batch: 14, loss: 0.08438476175069809, acc: 96.875, f1: 98.32967032967034, r: 0.599704753744088
06/02/2019 03:48:55 step: 1340, epoch: 40, batch: 19, loss: 0.06862061470746994, acc: 98.4375, f1: 98.74776386404294, r: 0.8145010261226864
06/02/2019 03:48:55 step: 1345, epoch: 40, batch: 24, loss: 0.054089225828647614, acc: 98.4375, f1: 97.99498746867168, r: 0.736073366448824
06/02/2019 03:48:55 step: 1350, epoch: 40, batch: 29, loss: 0.06255380809307098, acc: 98.4375, f1: 98.27998088867655, r: 0.6628081337580684
06/02/2019 03:48:55 *** evaluating ***
06/02/2019 03:48:55 step: 41, epoch: 40, acc: 56.837606837606835, f1: 25.47123015873016, r: 0.3480143223616977
06/02/2019 03:48:55 *** epoch: 42 ***
06/02/2019 03:48:55 *** training ***
06/02/2019 03:48:56 step: 1358, epoch: 41, batch: 4, loss: 0.09372105449438095, acc: 95.3125, f1: 85.8871473354232, r: 0.7378883341280217
06/02/2019 03:48:56 step: 1363, epoch: 41, batch: 9, loss: 0.03216315060853958, acc: 98.4375, f1: 98.33499833499833, r: 0.716497625249265
06/02/2019 03:48:56 step: 1368, epoch: 41, batch: 14, loss: 0.05898212641477585, acc: 100.0, f1: 100.0, r: 0.6501075864594971
06/02/2019 03:48:57 step: 1373, epoch: 41, batch: 19, loss: 0.1365411877632141, acc: 93.75, f1: 90.3342670401494, r: 0.7091677530887339
06/02/2019 03:48:57 step: 1378, epoch: 41, batch: 24, loss: 0.06153840571641922, acc: 98.4375, f1: 97.20730397422128, r: 0.6936897390935595
06/02/2019 03:48:57 step: 1383, epoch: 41, batch: 29, loss: 0.07897758483886719, acc: 98.4375, f1: 86.66666666666667, r: 0.7536776083449042
06/02/2019 03:48:57 *** evaluating ***
06/02/2019 03:48:57 step: 42, epoch: 41, acc: 57.692307692307686, f1: 25.601543583535108, r: 0.35231321463813825
06/02/2019 03:48:57 *** epoch: 43 ***
06/02/2019 03:48:57 *** training ***
06/02/2019 03:48:58 step: 1391, epoch: 42, batch: 4, loss: 0.10856965184211731, acc: 95.3125, f1: 94.17480035492459, r: 0.7107555101447594
06/02/2019 03:48:58 step: 1396, epoch: 42, batch: 9, loss: 0.0490906685590744, acc: 98.4375, f1: 99.12462006079028, r: 0.7184121953592983
06/02/2019 03:48:58 step: 1401, epoch: 42, batch: 14, loss: 0.10790514945983887, acc: 95.3125, f1: 89.58333333333333, r: 0.6978777440783144
06/02/2019 03:48:59 step: 1406, epoch: 42, batch: 19, loss: 0.038530778139829636, acc: 100.0, f1: 100.0, r: 0.711503846734579
06/02/2019 03:48:59 step: 1411, epoch: 42, batch: 24, loss: 0.035709790885448456, acc: 100.0, f1: 100.0, r: 0.6784770808216217
06/02/2019 03:48:59 step: 1416, epoch: 42, batch: 29, loss: 0.058152422308921814, acc: 98.4375, f1: 97.40259740259741, r: 0.6751729735634244
06/02/2019 03:48:59 *** evaluating ***
06/02/2019 03:48:59 step: 43, epoch: 42, acc: 56.837606837606835, f1: 25.379886862191654, r: 0.34691286997235815
06/02/2019 03:48:59 *** epoch: 44 ***
06/02/2019 03:48:59 *** training ***
06/02/2019 03:49:00 step: 1424, epoch: 43, batch: 4, loss: 0.08532410860061646, acc: 96.875, f1: 92.73809523809524, r: 0.7213934381660982
06/02/2019 03:49:00 step: 1429, epoch: 43, batch: 9, loss: 0.07674562931060791, acc: 98.4375, f1: 99.24764890282131, r: 0.6555538620459633
06/02/2019 03:49:00 step: 1434, epoch: 43, batch: 14, loss: 0.041438404470682144, acc: 98.4375, f1: 98.80745341614906, r: 0.6989928831145347
06/02/2019 03:49:01 step: 1439, epoch: 43, batch: 19, loss: 0.062335528433322906, acc: 96.875, f1: 85.58238636363636, r: 0.7273872511247099
06/02/2019 03:49:01 step: 1444, epoch: 43, batch: 24, loss: 0.12564077973365784, acc: 95.3125, f1: 93.21637426900585, r: 0.6461080916311608
06/02/2019 03:49:01 step: 1449, epoch: 43, batch: 29, loss: 0.11047475039958954, acc: 96.875, f1: 84.57792207792207, r: 0.7360053578477914
06/02/2019 03:49:01 *** evaluating ***
06/02/2019 03:49:01 step: 44, epoch: 43, acc: 56.41025641025641, f1: 25.476420791206568, r: 0.34774673453033234
06/02/2019 03:49:01 *** epoch: 45 ***
06/02/2019 03:49:01 *** training ***
06/02/2019 03:49:02 step: 1457, epoch: 44, batch: 4, loss: 0.11068933457136154, acc: 95.3125, f1: 82.1655328798186, r: 0.7922950951544581
06/02/2019 03:49:02 step: 1462, epoch: 44, batch: 9, loss: 0.055217135697603226, acc: 98.4375, f1: 83.33333333333333, r: 0.7636397810147132
06/02/2019 03:49:02 step: 1467, epoch: 44, batch: 14, loss: 0.038277145475149155, acc: 98.4375, f1: 98.80745341614906, r: 0.7008291745438814
06/02/2019 03:49:03 step: 1472, epoch: 44, batch: 19, loss: 0.1025640070438385, acc: 98.4375, f1: 96.1111111111111, r: 0.7561815786738725
06/02/2019 03:49:03 step: 1477, epoch: 44, batch: 24, loss: 0.08433403074741364, acc: 98.4375, f1: 99.01960784313727, r: 0.7983288864997433
06/02/2019 03:49:03 step: 1482, epoch: 44, batch: 29, loss: 0.06797397136688232, acc: 98.4375, f1: 86.8421052631579, r: 0.8055062999092182
06/02/2019 03:49:03 *** evaluating ***
06/02/2019 03:49:03 step: 45, epoch: 44, acc: 57.26495726495726, f1: 25.373133316742468, r: 0.34508249216820147
06/02/2019 03:49:03 *** epoch: 46 ***
06/02/2019 03:49:03 *** training ***
06/02/2019 03:49:04 step: 1490, epoch: 45, batch: 4, loss: 0.12772446870803833, acc: 96.875, f1: 94.92435689708533, r: 0.6790706112768983
06/02/2019 03:49:04 step: 1495, epoch: 45, batch: 9, loss: 0.05371187627315521, acc: 98.4375, f1: 98.1111111111111, r: 0.739220871606223
06/02/2019 03:49:04 step: 1500, epoch: 45, batch: 14, loss: 0.020038451999425888, acc: 100.0, f1: 100.0, r: 0.6870355819215792
06/02/2019 03:49:05 step: 1505, epoch: 45, batch: 19, loss: 0.0849984735250473, acc: 96.875, f1: 81.83543149060391, r: 0.6251260902322342
06/02/2019 03:49:05 step: 1510, epoch: 45, batch: 24, loss: 0.08728104829788208, acc: 98.4375, f1: 99.25490196078431, r: 0.7458788845329459
06/02/2019 03:49:05 step: 1515, epoch: 45, batch: 29, loss: 0.09284693747758865, acc: 96.875, f1: 97.78846153846153, r: 0.8085483355654283
06/02/2019 03:49:05 *** evaluating ***
06/02/2019 03:49:06 step: 46, epoch: 45, acc: 56.837606837606835, f1: 25.56591583670038, r: 0.3494925449898851
06/02/2019 03:49:06 *** epoch: 47 ***
06/02/2019 03:49:06 *** training ***
06/02/2019 03:49:06 step: 1523, epoch: 46, batch: 4, loss: 0.0786009430885315, acc: 96.875, f1: 97.8182636077373, r: 0.6388698907642724
06/02/2019 03:49:06 step: 1528, epoch: 46, batch: 9, loss: 0.04525340348482132, acc: 98.4375, f1: 95.56737588652481, r: 0.7349945377450581
06/02/2019 03:49:06 step: 1533, epoch: 46, batch: 14, loss: 0.05312938988208771, acc: 100.0, f1: 100.0, r: 0.7947181699280961
06/02/2019 03:49:07 step: 1538, epoch: 46, batch: 19, loss: 0.05465291440486908, acc: 98.4375, f1: 97.94871794871796, r: 0.6827541339571153
06/02/2019 03:49:07 step: 1543, epoch: 46, batch: 24, loss: 0.08238198608160019, acc: 100.0, f1: 100.0, r: 0.784699348305882
06/02/2019 03:49:07 step: 1548, epoch: 46, batch: 29, loss: 0.051709529012441635, acc: 98.4375, f1: 97.84126984126983, r: 0.7692943120509278
06/02/2019 03:49:08 *** evaluating ***
06/02/2019 03:49:08 step: 47, epoch: 46, acc: 56.837606837606835, f1: 25.487056701022283, r: 0.3516857938190113
06/02/2019 03:49:08 *** epoch: 48 ***
06/02/2019 03:49:08 *** training ***
06/02/2019 03:49:08 step: 1556, epoch: 47, batch: 4, loss: 0.015428073704242706, acc: 100.0, f1: 100.0, r: 0.6507004699404106
06/02/2019 03:49:08 step: 1561, epoch: 47, batch: 9, loss: 0.06837711483240128, acc: 98.4375, f1: 96.3718820861678, r: 0.6617951076796704
06/02/2019 03:49:09 step: 1566, epoch: 47, batch: 14, loss: 0.025929778814315796, acc: 100.0, f1: 100.0, r: 0.7273373028483261
06/02/2019 03:49:09 step: 1571, epoch: 47, batch: 19, loss: 0.0754769816994667, acc: 96.875, f1: 97.96142369991475, r: 0.7877540821792614
06/02/2019 03:49:09 step: 1576, epoch: 47, batch: 24, loss: 0.04702587425708771, acc: 100.0, f1: 100.0, r: 0.7607761232154209
06/02/2019 03:49:09 step: 1581, epoch: 47, batch: 29, loss: 0.04340583458542824, acc: 100.0, f1: 100.0, r: 0.7257216667478541
06/02/2019 03:49:10 *** evaluating ***
06/02/2019 03:49:10 step: 48, epoch: 47, acc: 56.837606837606835, f1: 25.388771301986495, r: 0.34389737074570226
06/02/2019 03:49:10 *** epoch: 49 ***
06/02/2019 03:49:10 *** training ***
06/02/2019 03:49:10 step: 1589, epoch: 48, batch: 4, loss: 0.09637921303510666, acc: 96.875, f1: 98.19447965521613, r: 0.7437255230603406
06/02/2019 03:49:10 step: 1594, epoch: 48, batch: 9, loss: 0.04233163967728615, acc: 98.4375, f1: 96.6137566137566, r: 0.6587694436040029
06/02/2019 03:49:11 step: 1599, epoch: 48, batch: 14, loss: 0.06931544095277786, acc: 98.4375, f1: 99.08700322234156, r: 0.836894332682222
06/02/2019 03:49:11 step: 1604, epoch: 48, batch: 19, loss: 0.09588086605072021, acc: 98.4375, f1: 97.57236227824464, r: 0.6752384233477493
06/02/2019 03:49:11 step: 1609, epoch: 48, batch: 24, loss: 0.06386882811784744, acc: 96.875, f1: 97.6493982208268, r: 0.6750866580368033
06/02/2019 03:49:12 step: 1614, epoch: 48, batch: 29, loss: 0.08504398912191391, acc: 96.875, f1: 96.97474747474747, r: 0.7939386128764458
06/02/2019 03:49:12 *** evaluating ***
06/02/2019 03:49:12 step: 49, epoch: 48, acc: 57.692307692307686, f1: 25.6178397091054, r: 0.3468309546776956
06/02/2019 03:49:12 *** epoch: 50 ***
06/02/2019 03:49:12 *** training ***
06/02/2019 03:49:12 step: 1622, epoch: 49, batch: 4, loss: 0.11337326467037201, acc: 95.3125, f1: 96.65264780154486, r: 0.7449668648395833
06/02/2019 03:49:13 step: 1627, epoch: 49, batch: 9, loss: 0.06315289437770844, acc: 98.4375, f1: 98.90476190476191, r: 0.8568859004188445
06/02/2019 03:49:13 step: 1632, epoch: 49, batch: 14, loss: 0.02070918306708336, acc: 100.0, f1: 100.0, r: 0.7030403176462509
06/02/2019 03:49:13 step: 1637, epoch: 49, batch: 19, loss: 0.022063368931412697, acc: 100.0, f1: 100.0, r: 0.7517951685598545
06/02/2019 03:49:13 step: 1642, epoch: 49, batch: 24, loss: 0.06904831528663635, acc: 98.4375, f1: 98.06763285024155, r: 0.8233566024681215
06/02/2019 03:49:14 step: 1647, epoch: 49, batch: 29, loss: 0.05897143483161926, acc: 98.4375, f1: 96.6137566137566, r: 0.7049614942357066
06/02/2019 03:49:14 *** evaluating ***
06/02/2019 03:49:14 step: 50, epoch: 49, acc: 57.692307692307686, f1: 25.44377670850229, r: 0.3349403940484371
06/02/2019 03:49:14 *** epoch: 51 ***
06/02/2019 03:49:14 *** training ***
06/02/2019 03:49:14 step: 1655, epoch: 50, batch: 4, loss: 0.09893679618835449, acc: 98.4375, f1: 94.97835497835497, r: 0.6087904944818642
06/02/2019 03:49:15 step: 1660, epoch: 50, batch: 9, loss: 0.016358818858861923, acc: 100.0, f1: 100.0, r: 0.6048438500808943
06/02/2019 03:49:15 step: 1665, epoch: 50, batch: 14, loss: 0.044537462294101715, acc: 98.4375, f1: 95.10204081632654, r: 0.6696070919060678
06/02/2019 03:49:15 step: 1670, epoch: 50, batch: 19, loss: 0.01087871938943863, acc: 100.0, f1: 100.0, r: 0.5953375759851912
06/02/2019 03:49:15 step: 1675, epoch: 50, batch: 24, loss: 0.07948305457830429, acc: 96.875, f1: 73.23232323232322, r: 0.8024322332006029
06/02/2019 03:49:16 step: 1680, epoch: 50, batch: 29, loss: 0.12223903834819794, acc: 95.3125, f1: 84.07152166829586, r: 0.7639565285419564
06/02/2019 03:49:16 *** evaluating ***
06/02/2019 03:49:16 step: 51, epoch: 50, acc: 55.98290598290598, f1: 24.433037985957206, r: 0.3375667236070043
06/02/2019 03:49:16 *** epoch: 52 ***
06/02/2019 03:49:16 *** training ***
06/02/2019 03:49:16 step: 1688, epoch: 51, batch: 4, loss: 0.06838861107826233, acc: 98.4375, f1: 98.29313543599258, r: 0.7028320596508935
06/02/2019 03:49:17 step: 1693, epoch: 51, batch: 9, loss: 0.03480185568332672, acc: 98.4375, f1: 97.67080745341615, r: 0.7672476369122483
06/02/2019 03:49:17 step: 1698, epoch: 51, batch: 14, loss: 0.05251653119921684, acc: 96.875, f1: 93.04761904761905, r: 0.7796772445505618
06/02/2019 03:49:17 step: 1703, epoch: 51, batch: 19, loss: 0.05464749410748482, acc: 100.0, f1: 100.0, r: 0.5876288403254558
06/02/2019 03:49:17 step: 1708, epoch: 51, batch: 24, loss: 0.06873148679733276, acc: 100.0, f1: 100.0, r: 0.7886341585458106
06/02/2019 03:49:18 step: 1713, epoch: 51, batch: 29, loss: 0.035533007234334946, acc: 100.0, f1: 100.0, r: 0.6341214110762414
06/02/2019 03:49:18 *** evaluating ***
06/02/2019 03:49:18 step: 52, epoch: 51, acc: 56.837606837606835, f1: 25.174465736812557, r: 0.3432747822953329
06/02/2019 03:49:18 *** epoch: 53 ***
06/02/2019 03:49:18 *** training ***
06/02/2019 03:49:18 step: 1721, epoch: 52, batch: 4, loss: 0.10537796467542648, acc: 96.875, f1: 91.08465608465607, r: 0.7952584397264454
06/02/2019 03:49:19 step: 1726, epoch: 52, batch: 9, loss: 0.02265828475356102, acc: 100.0, f1: 100.0, r: 0.6122502169018322
06/02/2019 03:49:19 step: 1731, epoch: 52, batch: 14, loss: 0.07012525200843811, acc: 98.4375, f1: 99.25905846209452, r: 0.6337418550813663
06/02/2019 03:49:19 step: 1736, epoch: 52, batch: 19, loss: 0.01903626322746277, acc: 100.0, f1: 100.0, r: 0.6774435379572493
06/02/2019 03:49:19 step: 1741, epoch: 52, batch: 24, loss: 0.07689814269542694, acc: 98.4375, f1: 98.37199837199837, r: 0.7380871238151978
06/02/2019 03:49:20 step: 1746, epoch: 52, batch: 29, loss: 0.04957962408661842, acc: 98.4375, f1: 95.71428571428571, r: 0.8116046988344813
06/02/2019 03:49:20 *** evaluating ***
06/02/2019 03:49:20 step: 53, epoch: 52, acc: 55.98290598290598, f1: 25.403913540481494, r: 0.34502364418109316
06/02/2019 03:49:20 *** epoch: 54 ***
06/02/2019 03:49:20 *** training ***
06/02/2019 03:49:20 step: 1754, epoch: 53, batch: 4, loss: 0.031197931617498398, acc: 100.0, f1: 100.0, r: 0.7757283149908718
06/02/2019 03:49:21 step: 1759, epoch: 53, batch: 9, loss: 0.02835327386856079, acc: 100.0, f1: 100.0, r: 0.7603319375590911
06/02/2019 03:49:21 step: 1764, epoch: 53, batch: 14, loss: 0.024616118520498276, acc: 100.0, f1: 100.0, r: 0.7915251137536672
06/02/2019 03:49:21 step: 1769, epoch: 53, batch: 19, loss: 0.050476886332035065, acc: 98.4375, f1: 97.94871794871796, r: 0.695259992034486
06/02/2019 03:49:21 step: 1774, epoch: 53, batch: 24, loss: 0.05224188417196274, acc: 98.4375, f1: 94.9685534591195, r: 0.6593667368228338
06/02/2019 03:49:22 step: 1779, epoch: 53, batch: 29, loss: 0.04063359275460243, acc: 98.4375, f1: 97.28813559322033, r: 0.7238191904484018
06/02/2019 03:49:22 *** evaluating ***
06/02/2019 03:49:22 step: 54, epoch: 53, acc: 55.55555555555556, f1: 25.30037642230265, r: 0.3469209132484808
06/02/2019 03:49:22 *** epoch: 55 ***
06/02/2019 03:49:22 *** training ***
06/02/2019 03:49:22 step: 1787, epoch: 54, batch: 4, loss: 0.113728828728199, acc: 95.3125, f1: 82.83641493671286, r: 0.6882683215750423
06/02/2019 03:49:22 step: 1792, epoch: 54, batch: 9, loss: 0.06410964578390121, acc: 98.4375, f1: 87.24489795918367, r: 0.783500018132932
06/02/2019 03:49:23 step: 1797, epoch: 54, batch: 14, loss: 0.0375518761575222, acc: 100.0, f1: 100.0, r: 0.802465727564214
06/02/2019 03:49:23 step: 1802, epoch: 54, batch: 19, loss: 0.05971001088619232, acc: 96.875, f1: 97.38070666863237, r: 0.6388756403155006
06/02/2019 03:49:23 step: 1807, epoch: 54, batch: 24, loss: 0.026157906278967857, acc: 98.4375, f1: 86.53846153846155, r: 0.7393639876767962
06/02/2019 03:49:23 step: 1812, epoch: 54, batch: 29, loss: 0.05012107640504837, acc: 98.4375, f1: 99.30118798043326, r: 0.8400695651595891
06/02/2019 03:49:24 *** evaluating ***
06/02/2019 03:49:24 step: 55, epoch: 54, acc: 56.41025641025641, f1: 25.389059113862363, r: 0.3352524880842116
06/02/2019 03:49:24 *** epoch: 56 ***
06/02/2019 03:49:24 *** training ***
06/02/2019 03:49:24 step: 1820, epoch: 55, batch: 4, loss: 0.01922321692109108, acc: 100.0, f1: 100.0, r: 0.7807208014710249
06/02/2019 03:49:24 step: 1825, epoch: 55, batch: 9, loss: 0.04035305231809616, acc: 98.4375, f1: 93.33333333333333, r: 0.7807596574069937
06/02/2019 03:49:25 step: 1830, epoch: 55, batch: 14, loss: 0.0631868839263916, acc: 98.4375, f1: 97.46031746031747, r: 0.6769327599512779
06/02/2019 03:49:25 step: 1835, epoch: 55, batch: 19, loss: 0.12078536301851273, acc: 93.75, f1: 87.76079244829245, r: 0.7967488400175349
06/02/2019 03:49:25 step: 1840, epoch: 55, batch: 24, loss: 0.047978293150663376, acc: 98.4375, f1: 96.86274509803921, r: 0.6837345711676465
06/02/2019 03:49:25 step: 1845, epoch: 55, batch: 29, loss: 0.06880076229572296, acc: 96.875, f1: 91.9578853046595, r: 0.7553476117319052
06/02/2019 03:49:26 *** evaluating ***
06/02/2019 03:49:26 step: 56, epoch: 55, acc: 56.837606837606835, f1: 25.32921810699589, r: 0.33623362263723744
06/02/2019 03:49:26 *** epoch: 57 ***
06/02/2019 03:49:26 *** training ***
06/02/2019 03:49:26 step: 1853, epoch: 56, batch: 4, loss: 0.04929427057504654, acc: 100.0, f1: 100.0, r: 0.7984887991749271
06/02/2019 03:49:26 step: 1858, epoch: 56, batch: 9, loss: 0.024139368906617165, acc: 100.0, f1: 100.0, r: 0.7321329506134596
06/02/2019 03:49:27 step: 1863, epoch: 56, batch: 14, loss: 0.03905709087848663, acc: 100.0, f1: 100.0, r: 0.6555854031129416
06/02/2019 03:49:27 step: 1868, epoch: 56, batch: 19, loss: 0.03473817929625511, acc: 100.0, f1: 100.0, r: 0.6696025428110242
06/02/2019 03:49:27 step: 1873, epoch: 56, batch: 24, loss: 0.07521732151508331, acc: 96.875, f1: 98.30033090902656, r: 0.7111940140252575
06/02/2019 03:49:27 step: 1878, epoch: 56, batch: 29, loss: 0.05881114676594734, acc: 100.0, f1: 100.0, r: 0.7927838514174244
06/02/2019 03:49:28 *** evaluating ***
06/02/2019 03:49:28 step: 57, epoch: 56, acc: 56.837606837606835, f1: 25.232898007684167, r: 0.3416050402136812
06/02/2019 03:49:28 *** epoch: 58 ***
06/02/2019 03:49:28 *** training ***
06/02/2019 03:49:28 step: 1886, epoch: 57, batch: 4, loss: 0.04398297518491745, acc: 100.0, f1: 100.0, r: 0.7405468270032461
06/02/2019 03:49:28 step: 1891, epoch: 57, batch: 9, loss: 0.061516135931015015, acc: 98.4375, f1: 96.8944099378882, r: 0.5851325083874414
06/02/2019 03:49:29 step: 1896, epoch: 57, batch: 14, loss: 0.03629934415221214, acc: 100.0, f1: 100.0, r: 0.860369072131494
06/02/2019 03:49:29 step: 1901, epoch: 57, batch: 19, loss: 0.11693689972162247, acc: 93.75, f1: 81.20471014492753, r: 0.766255261772752
06/02/2019 03:49:29 step: 1906, epoch: 57, batch: 24, loss: 0.06650834530591965, acc: 98.4375, f1: 99.19073845116331, r: 0.716214256585598
06/02/2019 03:49:29 step: 1911, epoch: 57, batch: 29, loss: 0.03145996108651161, acc: 98.4375, f1: 98.02659802659804, r: 0.714808588070963
06/02/2019 03:49:30 *** evaluating ***
06/02/2019 03:49:30 step: 58, epoch: 57, acc: 56.41025641025641, f1: 25.58208708032186, r: 0.3407424809953963
06/02/2019 03:49:30 *** epoch: 59 ***
06/02/2019 03:49:30 *** training ***
06/02/2019 03:49:30 step: 1919, epoch: 58, batch: 4, loss: 0.07099945843219757, acc: 98.4375, f1: 98.94416893297074, r: 0.6727579960696392
06/02/2019 03:49:30 step: 1924, epoch: 58, batch: 9, loss: 0.04948898404836655, acc: 98.4375, f1: 95.60606060606061, r: 0.7651069609327021
06/02/2019 03:49:31 step: 1929, epoch: 58, batch: 14, loss: 0.02591404877603054, acc: 98.4375, f1: 97.38775510204081, r: 0.7507506097452334
06/02/2019 03:49:31 step: 1934, epoch: 58, batch: 19, loss: 0.020001962780952454, acc: 100.0, f1: 100.0, r: 0.7294769257173084
06/02/2019 03:49:31 step: 1939, epoch: 58, batch: 24, loss: 0.03085324354469776, acc: 100.0, f1: 100.0, r: 0.7909594203339654
06/02/2019 03:49:31 step: 1944, epoch: 58, batch: 29, loss: 0.09326136857271194, acc: 95.3125, f1: 71.15750915750915, r: 0.6321037435270355
06/02/2019 03:49:32 *** evaluating ***
06/02/2019 03:49:32 step: 59, epoch: 58, acc: 57.692307692307686, f1: 25.33552038976606, r: 0.3343566632040401
06/02/2019 03:49:32 *** epoch: 60 ***
06/02/2019 03:49:32 *** training ***
06/02/2019 03:49:32 step: 1952, epoch: 59, batch: 4, loss: 0.034073375165462494, acc: 100.0, f1: 100.0, r: 0.7827475145196298
06/02/2019 03:49:32 step: 1957, epoch: 59, batch: 9, loss: 0.04155619442462921, acc: 98.4375, f1: 97.38775510204081, r: 0.6958037826358442
06/02/2019 03:49:33 step: 1962, epoch: 59, batch: 14, loss: 0.02196171134710312, acc: 100.0, f1: 100.0, r: 0.7965734343558375
06/02/2019 03:49:33 step: 1967, epoch: 59, batch: 19, loss: 0.04149575158953667, acc: 100.0, f1: 100.0, r: 0.7195153540041429
06/02/2019 03:49:33 step: 1972, epoch: 59, batch: 24, loss: 0.0292641744017601, acc: 100.0, f1: 100.0, r: 0.7990114519289125
06/02/2019 03:49:33 step: 1977, epoch: 59, batch: 29, loss: 0.015706468373537064, acc: 100.0, f1: 100.0, r: 0.7861688020425363
06/02/2019 03:49:34 *** evaluating ***
06/02/2019 03:49:34 step: 60, epoch: 59, acc: 57.692307692307686, f1: 25.67862401858987, r: 0.3409142264445224
06/02/2019 03:49:34 *** epoch: 61 ***
06/02/2019 03:49:34 *** training ***
06/02/2019 03:49:34 step: 1985, epoch: 60, batch: 4, loss: 0.038630735129117966, acc: 100.0, f1: 100.0, r: 0.6785664442587406
06/02/2019 03:49:34 step: 1990, epoch: 60, batch: 9, loss: 0.031908027827739716, acc: 100.0, f1: 100.0, r: 0.7041151647166197
06/02/2019 03:49:35 step: 1995, epoch: 60, batch: 14, loss: 0.05563829839229584, acc: 100.0, f1: 100.0, r: 0.73371701091055
06/02/2019 03:49:35 step: 2000, epoch: 60, batch: 19, loss: 0.013600189238786697, acc: 100.0, f1: 100.0, r: 0.6448076421494056
06/02/2019 03:49:35 step: 2005, epoch: 60, batch: 24, loss: 0.04114161431789398, acc: 98.4375, f1: 85.18518518518519, r: 0.6610648101030906
06/02/2019 03:49:35 step: 2010, epoch: 60, batch: 29, loss: 0.09506779164075851, acc: 96.875, f1: 93.16567206592782, r: 0.7992880863608449
06/02/2019 03:49:36 *** evaluating ***
06/02/2019 03:49:36 step: 61, epoch: 60, acc: 56.837606837606835, f1: 25.355208479245462, r: 0.3367481777426497
06/02/2019 03:49:36 *** epoch: 62 ***
06/02/2019 03:49:36 *** training ***
06/02/2019 03:49:36 step: 2018, epoch: 61, batch: 4, loss: 0.05748055875301361, acc: 98.4375, f1: 99.15966386554622, r: 0.7888321531722701
06/02/2019 03:49:36 step: 2023, epoch: 61, batch: 9, loss: 0.07026516646146774, acc: 98.4375, f1: 98.98692810457517, r: 0.7683617686854682
06/02/2019 03:49:36 step: 2028, epoch: 61, batch: 14, loss: 0.0301920548081398, acc: 100.0, f1: 100.0, r: 0.8356019386117094
06/02/2019 03:49:37 step: 2033, epoch: 61, batch: 19, loss: 0.012789282947778702, acc: 100.0, f1: 100.0, r: 0.6519243895800558
06/02/2019 03:49:37 step: 2038, epoch: 61, batch: 24, loss: 0.00997387245297432, acc: 100.0, f1: 100.0, r: 0.677992916953398
06/02/2019 03:49:37 step: 2043, epoch: 61, batch: 29, loss: 0.06304316967725754, acc: 96.875, f1: 98.09647129291969, r: 0.7800296816760821
06/02/2019 03:49:37 *** evaluating ***
06/02/2019 03:49:38 step: 62, epoch: 61, acc: 56.41025641025641, f1: 25.226337448559672, r: 0.33966719104759013
06/02/2019 03:49:38 *** epoch: 63 ***
06/02/2019 03:49:38 *** training ***
06/02/2019 03:49:38 step: 2051, epoch: 62, batch: 4, loss: 0.05626968294382095, acc: 98.4375, f1: 86.53846153846155, r: 0.792645369780394
06/02/2019 03:49:38 step: 2056, epoch: 62, batch: 9, loss: 0.03597370535135269, acc: 100.0, f1: 100.0, r: 0.772238089812771
06/02/2019 03:49:38 step: 2061, epoch: 62, batch: 14, loss: 0.030154390260577202, acc: 98.4375, f1: 99.24414210128496, r: 0.7206686566162137
06/02/2019 03:49:39 step: 2066, epoch: 62, batch: 19, loss: 0.07332070171833038, acc: 98.4375, f1: 98.64135864135865, r: 0.7201191881875237
06/02/2019 03:49:39 step: 2071, epoch: 62, batch: 24, loss: 0.02407105267047882, acc: 100.0, f1: 100.0, r: 0.702609247974992
06/02/2019 03:49:39 step: 2076, epoch: 62, batch: 29, loss: 0.03314550593495369, acc: 100.0, f1: 100.0, r: 0.7046646803704452
06/02/2019 03:49:40 *** evaluating ***
06/02/2019 03:49:40 step: 63, epoch: 62, acc: 56.837606837606835, f1: 25.48120887699342, r: 0.3337221712726304
06/02/2019 03:49:40 *** epoch: 64 ***
06/02/2019 03:49:40 *** training ***
06/02/2019 03:49:40 step: 2084, epoch: 63, batch: 4, loss: 0.03709938004612923, acc: 98.4375, f1: 97.25274725274726, r: 0.7466363993944702
06/02/2019 03:49:40 step: 2089, epoch: 63, batch: 9, loss: 0.05564814805984497, acc: 100.0, f1: 100.0, r: 0.6939743119739917
06/02/2019 03:49:40 step: 2094, epoch: 63, batch: 14, loss: 0.024926189333200455, acc: 100.0, f1: 100.0, r: 0.7410829095325991
06/02/2019 03:49:41 step: 2099, epoch: 63, batch: 19, loss: 0.05048064887523651, acc: 98.4375, f1: 98.73358348968104, r: 0.8290684022497108
06/02/2019 03:49:41 step: 2104, epoch: 63, batch: 24, loss: 0.05251587554812431, acc: 98.4375, f1: 94.6969696969697, r: 0.7495982668781825
06/02/2019 03:49:41 step: 2109, epoch: 63, batch: 29, loss: 0.05533170327544212, acc: 98.4375, f1: 99.06273620559335, r: 0.7133662976329809
06/02/2019 03:49:42 *** evaluating ***
06/02/2019 03:49:42 step: 64, epoch: 63, acc: 55.98290598290598, f1: 25.127908814349492, r: 0.33939665457003665
06/02/2019 03:49:42 *** epoch: 65 ***
06/02/2019 03:49:42 *** training ***
06/02/2019 03:49:42 step: 2117, epoch: 64, batch: 4, loss: 0.04289337992668152, acc: 100.0, f1: 100.0, r: 0.7657938647967257
06/02/2019 03:49:42 step: 2122, epoch: 64, batch: 9, loss: 0.031580742448568344, acc: 100.0, f1: 100.0, r: 0.7904765866156254
06/02/2019 03:49:42 step: 2127, epoch: 64, batch: 14, loss: 0.1112070381641388, acc: 98.4375, f1: 98.9534275248561, r: 0.6928694561809766
06/02/2019 03:49:43 step: 2132, epoch: 64, batch: 19, loss: 0.05176534503698349, acc: 98.4375, f1: 98.95652173913044, r: 0.8274401188716248
06/02/2019 03:49:43 step: 2137, epoch: 64, batch: 24, loss: 0.07715335488319397, acc: 95.3125, f1: 81.39567669172934, r: 0.7269555102582486
06/02/2019 03:49:43 step: 2142, epoch: 64, batch: 29, loss: 0.15926262736320496, acc: 96.875, f1: 98.27466740576496, r: 0.7231512721083764
06/02/2019 03:49:43 *** evaluating ***
06/02/2019 03:49:44 step: 65, epoch: 64, acc: 56.41025641025641, f1: 25.345378438849504, r: 0.33857588195109256
06/02/2019 03:49:44 *** epoch: 66 ***
06/02/2019 03:49:44 *** training ***
06/02/2019 03:49:44 step: 2150, epoch: 65, batch: 4, loss: 0.018714288249611855, acc: 100.0, f1: 100.0, r: 0.7225483929619824
06/02/2019 03:49:44 step: 2155, epoch: 65, batch: 9, loss: 0.021964557468891144, acc: 100.0, f1: 100.0, r: 0.7159529251506769
06/02/2019 03:49:44 step: 2160, epoch: 65, batch: 14, loss: 0.029981687664985657, acc: 100.0, f1: 100.0, r: 0.7629712042419368
06/02/2019 03:49:45 step: 2165, epoch: 65, batch: 19, loss: 0.03770419582724571, acc: 100.0, f1: 100.0, r: 0.7376839029023585
06/02/2019 03:49:45 step: 2170, epoch: 65, batch: 24, loss: 0.008488785475492477, acc: 100.0, f1: 100.0, r: 0.7196631652255778
06/02/2019 03:49:45 step: 2175, epoch: 65, batch: 29, loss: 0.0352851040661335, acc: 100.0, f1: 100.0, r: 0.7988569942460545
06/02/2019 03:49:46 *** evaluating ***
06/02/2019 03:49:46 step: 66, epoch: 65, acc: 56.837606837606835, f1: 25.466027874564457, r: 0.3364195431817109
06/02/2019 03:49:46 *** epoch: 67 ***
06/02/2019 03:49:46 *** training ***
06/02/2019 03:49:46 step: 2183, epoch: 66, batch: 4, loss: 0.05150194093585014, acc: 98.4375, f1: 97.11399711399712, r: 0.6952366843098651
06/02/2019 03:49:46 step: 2188, epoch: 66, batch: 9, loss: 0.08615253865718842, acc: 96.875, f1: 97.13690963690964, r: 0.7558991842639212
06/02/2019 03:49:47 step: 2193, epoch: 66, batch: 14, loss: 0.054023873060941696, acc: 98.4375, f1: 98.0045351473923, r: 0.7195141330664602
06/02/2019 03:49:47 step: 2198, epoch: 66, batch: 19, loss: 0.044292423874139786, acc: 100.0, f1: 100.0, r: 0.7255620181496304
06/02/2019 03:49:47 step: 2203, epoch: 66, batch: 24, loss: 0.06752457469701767, acc: 98.4375, f1: 87.16216216216216, r: 0.7441910819953884
06/02/2019 03:49:47 step: 2208, epoch: 66, batch: 29, loss: 0.03806478530168533, acc: 98.4375, f1: 95.17543859649122, r: 0.7938124739966735
06/02/2019 03:49:47 *** evaluating ***
06/02/2019 03:49:48 step: 67, epoch: 66, acc: 57.692307692307686, f1: 25.40911506174865, r: 0.3392395927025338
06/02/2019 03:49:48 *** epoch: 68 ***
06/02/2019 03:49:48 *** training ***
06/02/2019 03:49:48 step: 2216, epoch: 67, batch: 4, loss: 0.02893524244427681, acc: 100.0, f1: 100.0, r: 0.6772075738565051
06/02/2019 03:49:48 step: 2221, epoch: 67, batch: 9, loss: 0.031367283314466476, acc: 100.0, f1: 100.0, r: 0.7802755500852611
06/02/2019 03:49:49 step: 2226, epoch: 67, batch: 14, loss: 0.05754457041621208, acc: 98.4375, f1: 99.1388044579534, r: 0.7526806967757798
06/02/2019 03:49:49 step: 2231, epoch: 67, batch: 19, loss: 0.022902976721525192, acc: 100.0, f1: 100.0, r: 0.6964394981269479
06/02/2019 03:49:49 step: 2236, epoch: 67, batch: 24, loss: 0.05130452290177345, acc: 98.4375, f1: 98.61949956859362, r: 0.5395028217830901
06/02/2019 03:49:49 step: 2241, epoch: 67, batch: 29, loss: 0.04200233519077301, acc: 98.4375, f1: 98.97400820793433, r: 0.7965781990511527
06/02/2019 03:49:49 *** evaluating ***
06/02/2019 03:49:50 step: 68, epoch: 67, acc: 57.26495726495726, f1: 25.561382519109337, r: 0.3345412873122007
06/02/2019 03:49:50 *** epoch: 69 ***
06/02/2019 03:49:50 *** training ***
06/02/2019 03:49:50 step: 2249, epoch: 68, batch: 4, loss: 0.020086906850337982, acc: 100.0, f1: 100.0, r: 0.7271338169322519
06/02/2019 03:49:50 step: 2254, epoch: 68, batch: 9, loss: 0.03327488899230957, acc: 98.4375, f1: 97.88359788359789, r: 0.7285920119495356
06/02/2019 03:49:50 step: 2259, epoch: 68, batch: 14, loss: 0.06655450165271759, acc: 98.4375, f1: 86.66666666666667, r: 0.7246254476447462
06/02/2019 03:49:51 step: 2264, epoch: 68, batch: 19, loss: 0.01748044043779373, acc: 100.0, f1: 100.0, r: 0.7604200132079125
06/02/2019 03:49:51 step: 2269, epoch: 68, batch: 24, loss: 0.03671003133058548, acc: 100.0, f1: 100.0, r: 0.6911765753026164
06/02/2019 03:49:51 step: 2274, epoch: 68, batch: 29, loss: 0.012665476649999619, acc: 100.0, f1: 100.0, r: 0.7463128812066623
06/02/2019 03:49:51 *** evaluating ***
06/02/2019 03:49:52 step: 69, epoch: 68, acc: 56.41025641025641, f1: 25.42861701473257, r: 0.33408732304048333
06/02/2019 03:49:52 *** epoch: 70 ***
06/02/2019 03:49:52 *** training ***
06/02/2019 03:49:52 step: 2282, epoch: 69, batch: 4, loss: 0.05444859713315964, acc: 98.4375, f1: 97.59288330716902, r: 0.690556636373629
06/02/2019 03:49:52 step: 2287, epoch: 69, batch: 9, loss: 0.04398523271083832, acc: 98.4375, f1: 86.90476190476191, r: 0.8023519701503874
06/02/2019 03:49:52 step: 2292, epoch: 69, batch: 14, loss: 0.028805460780858994, acc: 98.4375, f1: 99.10775566231983, r: 0.5762481155946941
06/02/2019 03:49:53 step: 2297, epoch: 69, batch: 19, loss: 0.033093467354774475, acc: 98.4375, f1: 98.35286664554957, r: 0.7336276637453399
06/02/2019 03:49:53 step: 2302, epoch: 69, batch: 24, loss: 0.05768471583724022, acc: 96.875, f1: 98.37848932676519, r: 0.7512115480504616
06/02/2019 03:49:53 step: 2307, epoch: 69, batch: 29, loss: 0.03992031514644623, acc: 98.4375, f1: 93.71980676328504, r: 0.5700854017488622
06/02/2019 03:49:53 *** evaluating ***
06/02/2019 03:49:54 step: 70, epoch: 69, acc: 56.837606837606835, f1: 25.062493780166196, r: 0.33042264745420685
06/02/2019 03:49:54 *** epoch: 71 ***
06/02/2019 03:49:54 *** training ***
06/02/2019 03:49:54 step: 2315, epoch: 70, batch: 4, loss: 0.09015294909477234, acc: 96.875, f1: 95.71428571428572, r: 0.7002473746992957
06/02/2019 03:49:54 step: 2320, epoch: 70, batch: 9, loss: 0.019443415105342865, acc: 100.0, f1: 100.0, r: 0.6107346285368976
06/02/2019 03:49:54 step: 2325, epoch: 70, batch: 14, loss: 0.028793221339583397, acc: 100.0, f1: 100.0, r: 0.7646806329542176
06/02/2019 03:49:55 step: 2330, epoch: 70, batch: 19, loss: 0.010952621698379517, acc: 100.0, f1: 100.0, r: 0.7117292898899946
06/02/2019 03:49:55 step: 2335, epoch: 70, batch: 24, loss: 0.09315192699432373, acc: 96.875, f1: 97.17261904761905, r: 0.8077695529546094
06/02/2019 03:49:55 step: 2340, epoch: 70, batch: 29, loss: 0.05196591839194298, acc: 96.875, f1: 98.09000523286238, r: 0.6722980748921015
06/02/2019 03:49:55 *** evaluating ***
06/02/2019 03:49:55 step: 71, epoch: 70, acc: 56.837606837606835, f1: 25.60170263223406, r: 0.33704316098800835
06/02/2019 03:49:55 *** epoch: 72 ***
06/02/2019 03:49:55 *** training ***
06/02/2019 03:49:56 step: 2348, epoch: 71, batch: 4, loss: 0.024222128093242645, acc: 98.4375, f1: 86.76470588235294, r: 0.7146065032654111
06/02/2019 03:49:56 step: 2353, epoch: 71, batch: 9, loss: 0.06940923631191254, acc: 96.875, f1: 95.81377686640845, r: 0.7723505280956107
06/02/2019 03:49:56 step: 2358, epoch: 71, batch: 14, loss: 0.039323821663856506, acc: 98.4375, f1: 94.04761904761905, r: 0.6654331790125176
06/02/2019 03:49:57 step: 2363, epoch: 71, batch: 19, loss: 0.02876107022166252, acc: 98.4375, f1: 87.03703703703704, r: 0.716530811418312
06/02/2019 03:49:57 step: 2368, epoch: 71, batch: 24, loss: 0.0595628060400486, acc: 100.0, f1: 100.0, r: 0.6883389979573948
06/02/2019 03:49:57 step: 2373, epoch: 71, batch: 29, loss: 0.04633311927318573, acc: 96.875, f1: 80.47619047619048, r: 0.8203789735187144
06/02/2019 03:49:57 *** evaluating ***
06/02/2019 03:49:58 step: 72, epoch: 71, acc: 56.41025641025641, f1: 25.27496626180837, r: 0.3288341826761386
06/02/2019 03:49:58 *** epoch: 73 ***
06/02/2019 03:49:58 *** training ***
06/02/2019 03:49:58 step: 2381, epoch: 72, batch: 4, loss: 0.05619993805885315, acc: 98.4375, f1: 98.91304347826086, r: 0.7585236752571304
06/02/2019 03:49:58 step: 2386, epoch: 72, batch: 9, loss: 0.03666013479232788, acc: 100.0, f1: 100.0, r: 0.8042987397645467
06/02/2019 03:49:58 step: 2391, epoch: 72, batch: 14, loss: 0.017108594998717308, acc: 100.0, f1: 100.0, r: 0.6835129581936418
06/02/2019 03:49:59 step: 2396, epoch: 72, batch: 19, loss: 0.014721212908625603, acc: 100.0, f1: 100.0, r: 0.7942286242298353
06/02/2019 03:49:59 step: 2401, epoch: 72, batch: 24, loss: 0.08014258742332458, acc: 96.875, f1: 85.02415458937197, r: 0.7963973069725664
06/02/2019 03:49:59 step: 2406, epoch: 72, batch: 29, loss: 0.02762461081147194, acc: 100.0, f1: 100.0, r: 0.6468979243210807
06/02/2019 03:49:59 *** evaluating ***
06/02/2019 03:50:00 step: 73, epoch: 72, acc: 55.98290598290598, f1: 25.29692791485244, r: 0.33673416267016115
06/02/2019 03:50:00 *** epoch: 74 ***
06/02/2019 03:50:00 *** training ***
06/02/2019 03:50:00 step: 2414, epoch: 73, batch: 4, loss: 0.01149854063987732, acc: 100.0, f1: 100.0, r: 0.8129486678876094
06/02/2019 03:50:00 step: 2419, epoch: 73, batch: 9, loss: 0.06646186858415604, acc: 96.875, f1: 95.98837943665531, r: 0.7649167469161126
06/02/2019 03:50:00 step: 2424, epoch: 73, batch: 14, loss: 0.02037186361849308, acc: 100.0, f1: 100.0, r: 0.6933414691249294
06/02/2019 03:50:01 step: 2429, epoch: 73, batch: 19, loss: 0.00639575719833374, acc: 100.0, f1: 100.0, r: 0.7271501378122275
06/02/2019 03:50:01 step: 2434, epoch: 73, batch: 24, loss: 0.009543851017951965, acc: 100.0, f1: 100.0, r: 0.6827898222783267
06/02/2019 03:50:01 step: 2439, epoch: 73, batch: 29, loss: 0.019123461097478867, acc: 100.0, f1: 100.0, r: 0.6849347574871012
06/02/2019 03:50:01 *** evaluating ***
06/02/2019 03:50:02 step: 74, epoch: 73, acc: 56.41025641025641, f1: 25.41633615371056, r: 0.3430938881752742
06/02/2019 03:50:02 *** epoch: 75 ***
06/02/2019 03:50:02 *** training ***
06/02/2019 03:50:02 step: 2447, epoch: 74, batch: 4, loss: 0.04089014604687691, acc: 98.4375, f1: 86.36363636363636, r: 0.8054621105993743
06/02/2019 03:50:02 step: 2452, epoch: 74, batch: 9, loss: 0.0479101687669754, acc: 100.0, f1: 100.0, r: 0.746116390925805
06/02/2019 03:50:02 step: 2457, epoch: 74, batch: 14, loss: 0.04500056430697441, acc: 98.4375, f1: 97.1188475390156, r: 0.7225233595806276
06/02/2019 03:50:03 step: 2462, epoch: 74, batch: 19, loss: 0.03344648331403732, acc: 98.4375, f1: 98.78335949764521, r: 0.7715649942444395
06/02/2019 03:50:03 step: 2467, epoch: 74, batch: 24, loss: 0.04457263648509979, acc: 98.4375, f1: 98.62098685628098, r: 0.7005548960281539
06/02/2019 03:50:03 step: 2472, epoch: 74, batch: 29, loss: 0.0534980446100235, acc: 98.4375, f1: 97.12121212121212, r: 0.8211454867328604
06/02/2019 03:50:03 *** evaluating ***
06/02/2019 03:50:04 step: 75, epoch: 74, acc: 56.837606837606835, f1: 25.39376018246986, r: 0.3285434029111811
06/02/2019 03:50:04 *** epoch: 76 ***
06/02/2019 03:50:04 *** training ***
06/02/2019 03:50:04 step: 2480, epoch: 75, batch: 4, loss: 0.016964729875326157, acc: 100.0, f1: 100.0, r: 0.6715195753243903
06/02/2019 03:50:04 step: 2485, epoch: 75, batch: 9, loss: 0.02138786017894745, acc: 100.0, f1: 100.0, r: 0.831125653968253
06/02/2019 03:50:04 step: 2490, epoch: 75, batch: 14, loss: 0.028966298326849937, acc: 98.4375, f1: 86.66666666666667, r: 0.7085289983073033
06/02/2019 03:50:05 step: 2495, epoch: 75, batch: 19, loss: 0.05968796834349632, acc: 98.4375, f1: 98.83367139959432, r: 0.7845973085192213
06/02/2019 03:50:05 step: 2500, epoch: 75, batch: 24, loss: 0.046882472932338715, acc: 98.4375, f1: 94.6969696969697, r: 0.788577272438992
06/02/2019 03:50:05 step: 2505, epoch: 75, batch: 29, loss: 0.053652454167604446, acc: 96.875, f1: 84.09485251590513, r: 0.7205382292926072
06/02/2019 03:50:05 *** evaluating ***
06/02/2019 03:50:06 step: 76, epoch: 75, acc: 56.837606837606835, f1: 25.439706957252213, r: 0.32769208249855636
06/02/2019 03:50:06 *** epoch: 77 ***
06/02/2019 03:50:06 *** training ***
06/02/2019 03:50:06 step: 2513, epoch: 76, batch: 4, loss: 0.01222614198923111, acc: 100.0, f1: 100.0, r: 0.7982976800123874
06/02/2019 03:50:06 step: 2518, epoch: 76, batch: 9, loss: 0.08236372470855713, acc: 98.4375, f1: 99.23521913913127, r: 0.6742447423760254
06/02/2019 03:50:07 step: 2523, epoch: 76, batch: 14, loss: 0.06595299392938614, acc: 96.875, f1: 92.37522377372001, r: 0.6157268645423465
06/02/2019 03:50:07 step: 2528, epoch: 76, batch: 19, loss: 0.039908722043037415, acc: 100.0, f1: 100.0, r: 0.7606421515394688
06/02/2019 03:50:07 step: 2533, epoch: 76, batch: 24, loss: 0.07219700515270233, acc: 96.875, f1: 95.76168788920903, r: 0.8192850736914128
06/02/2019 03:50:07 step: 2538, epoch: 76, batch: 29, loss: 0.032872699201107025, acc: 98.4375, f1: 98.14921920185078, r: 0.7910764419600476
06/02/2019 03:50:08 *** evaluating ***
06/02/2019 03:50:08 step: 77, epoch: 76, acc: 56.41025641025641, f1: 25.280274252131328, r: 0.3404979657121876
06/02/2019 03:50:08 *** epoch: 78 ***
06/02/2019 03:50:08 *** training ***
06/02/2019 03:50:08 step: 2546, epoch: 77, batch: 4, loss: 0.012718357145786285, acc: 100.0, f1: 100.0, r: 0.7248235099658343
06/02/2019 03:50:08 step: 2551, epoch: 77, batch: 9, loss: 0.029382769018411636, acc: 100.0, f1: 100.0, r: 0.6539132505726861
06/02/2019 03:50:09 step: 2556, epoch: 77, batch: 14, loss: 0.018015339970588684, acc: 100.0, f1: 100.0, r: 0.8138427464044338
06/02/2019 03:50:09 step: 2561, epoch: 77, batch: 19, loss: 0.04939430579543114, acc: 98.4375, f1: 98.9254718280755, r: 0.6753344261044888
06/02/2019 03:50:09 step: 2566, epoch: 77, batch: 24, loss: 0.03937382623553276, acc: 100.0, f1: 100.0, r: 0.7009410840966621
06/02/2019 03:50:09 step: 2571, epoch: 77, batch: 29, loss: 0.017598770558834076, acc: 100.0, f1: 100.0, r: 0.8044142401176552
06/02/2019 03:50:10 *** evaluating ***
06/02/2019 03:50:10 step: 78, epoch: 77, acc: 57.692307692307686, f1: 25.47208684191934, r: 0.33286889332920994
06/02/2019 03:50:10 *** epoch: 79 ***
06/02/2019 03:50:10 *** training ***
06/02/2019 03:50:10 step: 2579, epoch: 78, batch: 4, loss: 0.0250703152269125, acc: 100.0, f1: 100.0, r: 0.7073557035901104
06/02/2019 03:50:10 step: 2584, epoch: 78, batch: 9, loss: 0.04778183996677399, acc: 98.4375, f1: 93.19727891156461, r: 0.6511670376973188
06/02/2019 03:50:11 step: 2589, epoch: 78, batch: 14, loss: 0.037350043654441833, acc: 98.4375, f1: 98.01587301587303, r: 0.849822812247431
06/02/2019 03:50:11 step: 2594, epoch: 78, batch: 19, loss: 0.0458325631916523, acc: 100.0, f1: 100.0, r: 0.7785555394911743
06/02/2019 03:50:11 step: 2599, epoch: 78, batch: 24, loss: 0.06021551787853241, acc: 98.4375, f1: 97.78325123152709, r: 0.777695002884297
06/02/2019 03:50:12 step: 2604, epoch: 78, batch: 29, loss: 0.06559669971466064, acc: 100.0, f1: 100.0, r: 0.8032294833060798
06/02/2019 03:50:12 *** evaluating ***
06/02/2019 03:50:12 step: 79, epoch: 78, acc: 57.26495726495726, f1: 25.460263940836906, r: 0.33591230184237875
06/02/2019 03:50:12 *** epoch: 80 ***
06/02/2019 03:50:12 *** training ***
06/02/2019 03:50:12 step: 2612, epoch: 79, batch: 4, loss: 0.0244683139026165, acc: 100.0, f1: 100.0, r: 0.7054878171332762
06/02/2019 03:50:12 step: 2617, epoch: 79, batch: 9, loss: 0.04473933205008507, acc: 98.4375, f1: 94.28571428571428, r: 0.6692549238280805
06/02/2019 03:50:13 step: 2622, epoch: 79, batch: 14, loss: 0.02227657660841942, acc: 100.0, f1: 100.0, r: 0.7939515222643341
06/02/2019 03:50:13 step: 2627, epoch: 79, batch: 19, loss: 0.05976257845759392, acc: 100.0, f1: 100.0, r: 0.6842446728315106
06/02/2019 03:50:13 step: 2632, epoch: 79, batch: 24, loss: 0.08898228406906128, acc: 98.4375, f1: 97.07792207792207, r: 0.7762442625351126
06/02/2019 03:50:14 step: 2637, epoch: 79, batch: 29, loss: 0.023631900548934937, acc: 98.4375, f1: 99.02818270165209, r: 0.6808413597076631
06/02/2019 03:50:14 *** evaluating ***
06/02/2019 03:50:14 step: 80, epoch: 79, acc: 56.837606837606835, f1: 25.388771301986495, r: 0.3366552803235396
06/02/2019 03:50:14 *** epoch: 81 ***
06/02/2019 03:50:14 *** training ***
06/02/2019 03:50:14 step: 2645, epoch: 80, batch: 4, loss: 0.016436217352747917, acc: 100.0, f1: 100.0, r: 0.8280270853952401
06/02/2019 03:50:14 step: 2650, epoch: 80, batch: 9, loss: 0.03179012984037399, acc: 100.0, f1: 100.0, r: 0.648579193950073
06/02/2019 03:50:15 step: 2655, epoch: 80, batch: 14, loss: 0.0353584848344326, acc: 98.4375, f1: 99.00598955014655, r: 0.7209314581593079
06/02/2019 03:50:15 step: 2660, epoch: 80, batch: 19, loss: 0.026777075603604317, acc: 100.0, f1: 100.0, r: 0.7004753493535965
06/02/2019 03:50:15 step: 2665, epoch: 80, batch: 24, loss: 0.008145108819007874, acc: 100.0, f1: 100.0, r: 0.7390417373684536
06/02/2019 03:50:15 step: 2670, epoch: 80, batch: 29, loss: 0.051096513867378235, acc: 96.875, f1: 97.14564665324544, r: 0.6791265876541543
06/02/2019 03:50:16 *** evaluating ***
06/02/2019 03:50:16 step: 81, epoch: 80, acc: 58.54700854700855, f1: 25.828024077878336, r: 0.3278110345045894
06/02/2019 03:50:16 *** epoch: 82 ***
06/02/2019 03:50:16 *** training ***
06/02/2019 03:50:16 step: 2678, epoch: 81, batch: 4, loss: 0.06758980453014374, acc: 96.875, f1: 98.315617446715, r: 0.7381143789505649
06/02/2019 03:50:16 step: 2683, epoch: 81, batch: 9, loss: 0.017986157909035683, acc: 100.0, f1: 100.0, r: 0.8027603570586049
06/02/2019 03:50:17 step: 2688, epoch: 81, batch: 14, loss: 0.0193600095808506, acc: 100.0, f1: 100.0, r: 0.792066777035249
06/02/2019 03:50:17 step: 2693, epoch: 81, batch: 19, loss: 0.08168597519397736, acc: 96.875, f1: 95.01587301587301, r: 0.7792062191872413
06/02/2019 03:50:17 step: 2698, epoch: 81, batch: 24, loss: 0.016575580462813377, acc: 100.0, f1: 100.0, r: 0.6278305553534713
06/02/2019 03:50:17 step: 2703, epoch: 81, batch: 29, loss: 0.006355080753564835, acc: 100.0, f1: 100.0, r: 0.607267974594712
06/02/2019 03:50:18 *** evaluating ***
06/02/2019 03:50:18 step: 82, epoch: 81, acc: 58.97435897435898, f1: 25.96392817010347, r: 0.33170647641016876
06/02/2019 03:50:18 *** epoch: 83 ***
06/02/2019 03:50:18 *** training ***
06/02/2019 03:50:18 step: 2711, epoch: 82, batch: 4, loss: 0.005660071969032288, acc: 100.0, f1: 100.0, r: 0.7478495724112904
06/02/2019 03:50:18 step: 2716, epoch: 82, batch: 9, loss: 0.05604017153382301, acc: 98.4375, f1: 95.71428571428572, r: 0.7725031824734653
06/02/2019 03:50:18 step: 2721, epoch: 82, batch: 14, loss: 0.015430696308612823, acc: 100.0, f1: 100.0, r: 0.7043763389238529
06/02/2019 03:50:19 step: 2726, epoch: 82, batch: 19, loss: 0.04963686689734459, acc: 98.4375, f1: 97.47474747474747, r: 0.7100869058377262
06/02/2019 03:50:19 step: 2731, epoch: 82, batch: 24, loss: 0.014904975891113281, acc: 100.0, f1: 100.0, r: 0.7026720167635607
06/02/2019 03:50:19 step: 2736, epoch: 82, batch: 29, loss: 0.016604337841272354, acc: 100.0, f1: 100.0, r: 0.7379428143984029
06/02/2019 03:50:19 *** evaluating ***
06/02/2019 03:50:20 step: 83, epoch: 82, acc: 55.55555555555556, f1: 25.30404638707335, r: 0.34220496427499836
06/02/2019 03:50:20 *** epoch: 84 ***
06/02/2019 03:50:20 *** training ***
06/02/2019 03:50:20 step: 2744, epoch: 83, batch: 4, loss: 0.045972153544425964, acc: 98.4375, f1: 97.33806566104703, r: 0.7429900873311415
06/02/2019 03:50:20 step: 2749, epoch: 83, batch: 9, loss: 0.017807528376579285, acc: 100.0, f1: 100.0, r: 0.8574308151705731
06/02/2019 03:50:20 step: 2754, epoch: 83, batch: 14, loss: 0.037992123514413834, acc: 98.4375, f1: 97.22222222222221, r: 0.789082413622493
06/02/2019 03:50:21 step: 2759, epoch: 83, batch: 19, loss: 0.0044662803411483765, acc: 100.0, f1: 100.0, r: 0.7547644110638307
06/02/2019 03:50:21 step: 2764, epoch: 83, batch: 24, loss: 0.01311422511935234, acc: 100.0, f1: 100.0, r: 0.6492100944216689
06/02/2019 03:50:21 step: 2769, epoch: 83, batch: 29, loss: 0.023813124746084213, acc: 100.0, f1: 100.0, r: 0.7470873364733572
06/02/2019 03:50:21 *** evaluating ***
06/02/2019 03:50:21 step: 84, epoch: 83, acc: 57.26495726495726, f1: 25.42200920412659, r: 0.3249230273311887
06/02/2019 03:50:21 *** epoch: 85 ***
06/02/2019 03:50:21 *** training ***
06/02/2019 03:50:22 step: 2777, epoch: 84, batch: 4, loss: 0.028079843148589134, acc: 100.0, f1: 100.0, r: 0.7785782261248839
06/02/2019 03:50:22 step: 2782, epoch: 84, batch: 9, loss: 0.037568558007478714, acc: 98.4375, f1: 97.6023976023976, r: 0.8045063789563752
06/02/2019 03:50:22 step: 2787, epoch: 84, batch: 14, loss: 0.04111488163471222, acc: 96.875, f1: 95.16250944822373, r: 0.6648017665153094
06/02/2019 03:50:23 step: 2792, epoch: 84, batch: 19, loss: 0.02961389720439911, acc: 100.0, f1: 100.0, r: 0.7316970172101246
06/02/2019 03:50:23 step: 2797, epoch: 84, batch: 24, loss: 0.017434153705835342, acc: 100.0, f1: 100.0, r: 0.8160192844061891
06/02/2019 03:50:23 step: 2802, epoch: 84, batch: 29, loss: 0.035626284778118134, acc: 98.4375, f1: 96.42857142857143, r: 0.827297604120412
06/02/2019 03:50:23 *** evaluating ***
06/02/2019 03:50:23 step: 85, epoch: 84, acc: 56.41025641025641, f1: 25.329702490344225, r: 0.33351167592002784
06/02/2019 03:50:23 *** epoch: 86 ***
06/02/2019 03:50:23 *** training ***
06/02/2019 03:50:24 step: 2810, epoch: 85, batch: 4, loss: 0.018092945218086243, acc: 100.0, f1: 100.0, r: 0.6663186959747829
06/02/2019 03:50:24 step: 2815, epoch: 85, batch: 9, loss: 0.011194486171007156, acc: 100.0, f1: 100.0, r: 0.6776182526229471
06/02/2019 03:50:24 step: 2820, epoch: 85, batch: 14, loss: 0.051023293286561966, acc: 98.4375, f1: 95.84415584415584, r: 0.6319093941729887
06/02/2019 03:50:25 step: 2825, epoch: 85, batch: 19, loss: 0.026829291135072708, acc: 100.0, f1: 100.0, r: 0.7274380481638306
06/02/2019 03:50:25 step: 2830, epoch: 85, batch: 24, loss: 0.01410045474767685, acc: 100.0, f1: 100.0, r: 0.8181462630537374
06/02/2019 03:50:25 step: 2835, epoch: 85, batch: 29, loss: 0.02836562693119049, acc: 98.4375, f1: 97.47899159663866, r: 0.7156425826999513
06/02/2019 03:50:25 *** evaluating ***
06/02/2019 03:50:25 step: 86, epoch: 85, acc: 55.98290598290598, f1: 24.604825786643996, r: 0.3334519975530708
06/02/2019 03:50:25 *** epoch: 87 ***
06/02/2019 03:50:25 *** training ***
06/02/2019 03:50:26 step: 2843, epoch: 86, batch: 4, loss: 0.0426068976521492, acc: 98.4375, f1: 96.52173913043478, r: 0.704857984748849
06/02/2019 03:50:26 step: 2848, epoch: 86, batch: 9, loss: 0.03955770283937454, acc: 98.4375, f1: 98.06763285024154, r: 0.7839779031427936
06/02/2019 03:50:26 step: 2853, epoch: 86, batch: 14, loss: 0.024697065353393555, acc: 100.0, f1: 100.0, r: 0.7057493664377265
06/02/2019 03:50:26 step: 2858, epoch: 86, batch: 19, loss: 0.017224252223968506, acc: 100.0, f1: 100.0, r: 0.7007770119767955
06/02/2019 03:50:27 step: 2863, epoch: 86, batch: 24, loss: 0.01843583956360817, acc: 100.0, f1: 100.0, r: 0.7631597139946596
06/02/2019 03:50:27 step: 2868, epoch: 86, batch: 29, loss: 0.0544714517891407, acc: 98.4375, f1: 91.66666666666666, r: 0.740391150738933
06/02/2019 03:50:27 *** evaluating ***
06/02/2019 03:50:27 step: 87, epoch: 86, acc: 56.837606837606835, f1: 24.82756301429592, r: 0.3330843344672865
06/02/2019 03:50:27 *** epoch: 88 ***
06/02/2019 03:50:27 *** training ***
06/02/2019 03:50:28 step: 2876, epoch: 87, batch: 4, loss: 0.01034902036190033, acc: 100.0, f1: 100.0, r: 0.7274322580026392
06/02/2019 03:50:28 step: 2881, epoch: 87, batch: 9, loss: 0.06747845560312271, acc: 96.875, f1: 97.33486943164363, r: 0.7915652117579517
06/02/2019 03:50:28 step: 2886, epoch: 87, batch: 14, loss: 0.04771149158477783, acc: 98.4375, f1: 96.82539682539682, r: 0.78656820744877
06/02/2019 03:50:28 step: 2891, epoch: 87, batch: 19, loss: 0.021896187216043472, acc: 100.0, f1: 100.0, r: 0.7363944192973423
06/02/2019 03:50:29 step: 2896, epoch: 87, batch: 24, loss: 0.02481972426176071, acc: 98.4375, f1: 97.69119769119769, r: 0.5814234835252667
06/02/2019 03:50:29 step: 2901, epoch: 87, batch: 29, loss: 0.007959049195051193, acc: 100.0, f1: 100.0, r: 0.7278253895192595
06/02/2019 03:50:29 *** evaluating ***
06/02/2019 03:50:29 step: 88, epoch: 87, acc: 55.98290598290598, f1: 24.51216774425287, r: 0.325936757965571
06/02/2019 03:50:29 *** epoch: 89 ***
06/02/2019 03:50:29 *** training ***
06/02/2019 03:50:30 step: 2909, epoch: 88, batch: 4, loss: 0.008648619055747986, acc: 100.0, f1: 100.0, r: 0.7381547423888469
06/02/2019 03:50:30 step: 2914, epoch: 88, batch: 9, loss: 0.07080952823162079, acc: 98.4375, f1: 98.72122762148338, r: 0.7265067026985808
06/02/2019 03:50:30 step: 2919, epoch: 88, batch: 14, loss: 0.028109554201364517, acc: 100.0, f1: 100.0, r: 0.603947561211521
06/02/2019 03:50:30 step: 2924, epoch: 88, batch: 19, loss: 0.02387462928891182, acc: 100.0, f1: 100.0, r: 0.7884315981325611
06/02/2019 03:50:31 step: 2929, epoch: 88, batch: 24, loss: 0.014322444796562195, acc: 100.0, f1: 100.0, r: 0.7858052250105925
06/02/2019 03:50:31 step: 2934, epoch: 88, batch: 29, loss: 0.038988448679447174, acc: 100.0, f1: 100.0, r: 0.7684792187197784
06/02/2019 03:50:31 *** evaluating ***
06/02/2019 03:50:31 step: 89, epoch: 88, acc: 57.26495726495726, f1: 25.511058550476385, r: 0.3391400854266
06/02/2019 03:50:31 *** epoch: 90 ***
06/02/2019 03:50:31 *** training ***
06/02/2019 03:50:32 step: 2942, epoch: 89, batch: 4, loss: 0.016839047893881798, acc: 100.0, f1: 100.0, r: 0.8159968449760208
06/02/2019 03:50:32 step: 2947, epoch: 89, batch: 9, loss: 0.017335519194602966, acc: 100.0, f1: 100.0, r: 0.6948411849611754
06/02/2019 03:50:32 step: 2952, epoch: 89, batch: 14, loss: 0.006553787738084793, acc: 100.0, f1: 100.0, r: 0.7834861891595454
06/02/2019 03:50:33 step: 2957, epoch: 89, batch: 19, loss: 0.02628207951784134, acc: 100.0, f1: 100.0, r: 0.8186712797239264
06/02/2019 03:50:33 step: 2962, epoch: 89, batch: 24, loss: 0.019859731197357178, acc: 100.0, f1: 100.0, r: 0.6093141085404679
06/02/2019 03:50:33 step: 2967, epoch: 89, batch: 29, loss: 0.08312822878360748, acc: 96.875, f1: 95.66579492208783, r: 0.7525092509772464
06/02/2019 03:50:33 *** evaluating ***
06/02/2019 03:50:34 step: 90, epoch: 89, acc: 57.26495726495726, f1: 25.5057316163184, r: 0.3318301770748233
06/02/2019 03:50:34 *** epoch: 91 ***
06/02/2019 03:50:34 *** training ***
06/02/2019 03:50:34 step: 2975, epoch: 90, batch: 4, loss: 0.0585775189101696, acc: 98.4375, f1: 94.80519480519482, r: 0.6892587412153671
06/02/2019 03:50:34 step: 2980, epoch: 90, batch: 9, loss: 0.01134459674358368, acc: 100.0, f1: 100.0, r: 0.7092070729133403
06/02/2019 03:50:35 step: 2985, epoch: 90, batch: 14, loss: 0.026071667671203613, acc: 100.0, f1: 100.0, r: 0.7797075573110355
06/02/2019 03:50:35 step: 2990, epoch: 90, batch: 19, loss: 0.0057218037545681, acc: 100.0, f1: 100.0, r: 0.8659446730632882
06/02/2019 03:50:35 step: 2995, epoch: 90, batch: 24, loss: 0.026061363518238068, acc: 98.4375, f1: 99.05018611218071, r: 0.7035372019975693
06/02/2019 03:50:35 step: 3000, epoch: 90, batch: 29, loss: 0.02123212069272995, acc: 100.0, f1: 100.0, r: 0.8319691756064793
06/02/2019 03:50:36 *** evaluating ***
06/02/2019 03:50:36 step: 91, epoch: 90, acc: 56.837606837606835, f1: 25.489865565475267, r: 0.32587571175363167
06/02/2019 03:50:36 *** epoch: 92 ***
06/02/2019 03:50:36 *** training ***
06/02/2019 03:50:36 step: 3008, epoch: 91, batch: 4, loss: 0.06467217952013016, acc: 96.875, f1: 96.24162708561686, r: 0.658564849813687
06/02/2019 03:50:36 step: 3013, epoch: 91, batch: 9, loss: 0.012695100158452988, acc: 100.0, f1: 100.0, r: 0.6061612613275247
06/02/2019 03:50:36 step: 3018, epoch: 91, batch: 14, loss: 0.07031607627868652, acc: 96.875, f1: 96.16161616161617, r: 0.6764793903563434
06/02/2019 03:50:37 step: 3023, epoch: 91, batch: 19, loss: 0.041868213564157486, acc: 98.4375, f1: 98.67434153148439, r: 0.7427290051845212
06/02/2019 03:50:37 step: 3028, epoch: 91, batch: 24, loss: 0.02075788751244545, acc: 100.0, f1: 100.0, r: 0.7211205775416125
06/02/2019 03:50:37 step: 3033, epoch: 91, batch: 29, loss: 0.016240481287240982, acc: 100.0, f1: 100.0, r: 0.7385546459520071
06/02/2019 03:50:38 *** evaluating ***
06/02/2019 03:50:38 step: 92, epoch: 91, acc: 56.837606837606835, f1: 24.902842822405606, r: 0.3238910268499625
06/02/2019 03:50:38 *** epoch: 93 ***
06/02/2019 03:50:38 *** training ***
06/02/2019 03:50:38 step: 3041, epoch: 92, batch: 4, loss: 0.026723502203822136, acc: 100.0, f1: 100.0, r: 0.8027188168384474
06/02/2019 03:50:38 step: 3046, epoch: 92, batch: 9, loss: 0.04512824863195419, acc: 98.4375, f1: 95.33333333333334, r: 0.7633822073638817
06/02/2019 03:50:39 step: 3051, epoch: 92, batch: 14, loss: 0.0041535384953022, acc: 100.0, f1: 100.0, r: 0.7301378030224472
06/02/2019 03:50:39 step: 3056, epoch: 92, batch: 19, loss: 0.04525614529848099, acc: 98.4375, f1: 98.0952380952381, r: 0.661940870947668
06/02/2019 03:50:39 step: 3061, epoch: 92, batch: 24, loss: 0.023759376257658005, acc: 100.0, f1: 100.0, r: 0.6683405953640774
06/02/2019 03:50:40 step: 3066, epoch: 92, batch: 29, loss: 0.05961171165108681, acc: 98.4375, f1: 98.17056766209308, r: 0.6710356713394156
06/02/2019 03:50:40 *** evaluating ***
06/02/2019 03:50:40 step: 93, epoch: 92, acc: 57.26495726495726, f1: 25.530103434816553, r: 0.3375649758439364
06/02/2019 03:50:40 *** epoch: 94 ***
06/02/2019 03:50:40 *** training ***
06/02/2019 03:50:40 step: 3074, epoch: 93, batch: 4, loss: 0.03635479137301445, acc: 98.4375, f1: 97.83549783549783, r: 0.8239299831543005
06/02/2019 03:50:40 step: 3079, epoch: 93, batch: 9, loss: 0.03543657809495926, acc: 98.4375, f1: 98.62318840579711, r: 0.8039943826200331
06/02/2019 03:50:41 step: 3084, epoch: 93, batch: 14, loss: 0.06150313466787338, acc: 98.4375, f1: 97.75132275132276, r: 0.7790435159668215
06/02/2019 03:50:41 step: 3089, epoch: 93, batch: 19, loss: 0.03606114163994789, acc: 100.0, f1: 100.0, r: 0.6727984628755114
06/02/2019 03:50:41 step: 3094, epoch: 93, batch: 24, loss: 0.06465746462345123, acc: 96.875, f1: 95.21919879062736, r: 0.6392409880516121
06/02/2019 03:50:42 step: 3099, epoch: 93, batch: 29, loss: 0.01984246075153351, acc: 100.0, f1: 100.0, r: 0.8348533066081163
06/02/2019 03:50:42 *** evaluating ***
06/02/2019 03:50:42 step: 94, epoch: 93, acc: 57.26495726495726, f1: 25.734963604690954, r: 0.3294468232604497
06/02/2019 03:50:42 *** epoch: 95 ***
06/02/2019 03:50:42 *** training ***
06/02/2019 03:50:42 step: 3107, epoch: 94, batch: 4, loss: 0.013795584440231323, acc: 100.0, f1: 100.0, r: 0.6762344705666653
06/02/2019 03:50:43 step: 3112, epoch: 94, batch: 9, loss: 0.07594543695449829, acc: 95.3125, f1: 73.21856775727402, r: 0.4910373962728363
06/02/2019 03:50:43 step: 3117, epoch: 94, batch: 14, loss: 0.043339330703020096, acc: 98.4375, f1: 96.82539682539684, r: 0.6457075560389474
06/02/2019 03:50:43 step: 3122, epoch: 94, batch: 19, loss: 0.019388459622859955, acc: 98.4375, f1: 97.667638483965, r: 0.7161148462862034
06/02/2019 03:50:43 step: 3127, epoch: 94, batch: 24, loss: 0.024364911019802094, acc: 100.0, f1: 100.0, r: 0.7472426479767507
06/02/2019 03:50:44 step: 3132, epoch: 94, batch: 29, loss: 0.03532271832227707, acc: 100.0, f1: 100.0, r: 0.8177295772889898
06/02/2019 03:50:44 *** evaluating ***
06/02/2019 03:50:44 step: 95, epoch: 94, acc: 58.119658119658126, f1: 25.081773000462317, r: 0.31981627649449174
06/02/2019 03:50:44 *** epoch: 96 ***
06/02/2019 03:50:44 *** training ***
06/02/2019 03:50:44 step: 3140, epoch: 95, batch: 4, loss: 0.021529417484998703, acc: 98.4375, f1: 98.4077841662981, r: 0.6982886363410387
06/02/2019 03:50:45 step: 3145, epoch: 95, batch: 9, loss: 0.04582838714122772, acc: 98.4375, f1: 98.94416893297074, r: 0.6956158051348057
06/02/2019 03:50:45 step: 3150, epoch: 95, batch: 14, loss: 0.007820598781108856, acc: 100.0, f1: 100.0, r: 0.6851487934603703
06/02/2019 03:50:45 step: 3155, epoch: 95, batch: 19, loss: 0.008695237338542938, acc: 100.0, f1: 100.0, r: 0.7110264376017067
06/02/2019 03:50:46 step: 3160, epoch: 95, batch: 24, loss: 0.03450668230652809, acc: 98.4375, f1: 97.0, r: 0.8337536672223024
06/02/2019 03:50:46 step: 3165, epoch: 95, batch: 29, loss: 0.04701968654990196, acc: 100.0, f1: 100.0, r: 0.7911544389563355
06/02/2019 03:50:46 *** evaluating ***
06/02/2019 03:50:46 step: 96, epoch: 95, acc: 56.837606837606835, f1: 25.373001889586845, r: 0.3306323190460208
06/02/2019 03:50:46 *** epoch: 97 ***
06/02/2019 03:50:46 *** training ***
06/02/2019 03:50:46 step: 3173, epoch: 96, batch: 4, loss: 0.012688647955656052, acc: 100.0, f1: 100.0, r: 0.6750477419166167
06/02/2019 03:50:47 step: 3178, epoch: 96, batch: 9, loss: 0.0830230712890625, acc: 96.875, f1: 98.03862932732902, r: 0.8411353404035616
06/02/2019 03:50:47 step: 3183, epoch: 96, batch: 14, loss: 0.045104075223207474, acc: 96.875, f1: 78.76344086021506, r: 0.6604335570913255
06/02/2019 03:50:47 step: 3188, epoch: 96, batch: 19, loss: 0.019609538838267326, acc: 100.0, f1: 100.0, r: 0.675674090937008
06/02/2019 03:50:48 step: 3193, epoch: 96, batch: 24, loss: 0.028633136302232742, acc: 100.0, f1: 100.0, r: 0.7261020981253633
06/02/2019 03:50:48 step: 3198, epoch: 96, batch: 29, loss: 0.016136303544044495, acc: 100.0, f1: 100.0, r: 0.737171307713006
06/02/2019 03:50:48 *** evaluating ***
06/02/2019 03:50:48 step: 97, epoch: 96, acc: 58.54700854700855, f1: 26.25059051398337, r: 0.33467674291732363
06/02/2019 03:50:48 *** epoch: 98 ***
06/02/2019 03:50:48 *** training ***
06/02/2019 03:50:48 step: 3206, epoch: 97, batch: 4, loss: 0.021117720752954483, acc: 100.0, f1: 100.0, r: 0.671189099205471
06/02/2019 03:50:49 step: 3211, epoch: 97, batch: 9, loss: 0.02267695777118206, acc: 100.0, f1: 100.0, r: 0.7084550898045937
06/02/2019 03:50:49 step: 3216, epoch: 97, batch: 14, loss: 0.02094263583421707, acc: 98.4375, f1: 98.94909688013136, r: 0.5055313261879698
06/02/2019 03:50:49 step: 3221, epoch: 97, batch: 19, loss: 0.036073483526706696, acc: 100.0, f1: 100.0, r: 0.8475476995598815
06/02/2019 03:50:50 step: 3226, epoch: 97, batch: 24, loss: 0.014035489410161972, acc: 100.0, f1: 100.0, r: 0.7956137321914648
06/02/2019 03:50:50 step: 3231, epoch: 97, batch: 29, loss: 0.0394553542137146, acc: 98.4375, f1: 96.66666666666667, r: 0.8246184587507677
06/02/2019 03:50:50 *** evaluating ***
06/02/2019 03:50:50 step: 98, epoch: 97, acc: 58.119658119658126, f1: 25.160384970431828, r: 0.3303194973624392
06/02/2019 03:50:50 *** epoch: 99 ***
06/02/2019 03:50:50 *** training ***
06/02/2019 03:50:50 step: 3239, epoch: 98, batch: 4, loss: 0.021901488304138184, acc: 100.0, f1: 100.0, r: 0.7851777293038282
06/02/2019 03:50:51 step: 3244, epoch: 98, batch: 9, loss: 0.02807478792965412, acc: 100.0, f1: 100.0, r: 0.6957044757981118
06/02/2019 03:50:51 step: 3249, epoch: 98, batch: 14, loss: 0.033468060195446014, acc: 98.4375, f1: 85.2216748768473, r: 0.6580756343473424
06/02/2019 03:50:51 step: 3254, epoch: 98, batch: 19, loss: 0.02186906710267067, acc: 100.0, f1: 100.0, r: 0.7130521540153745
06/02/2019 03:50:52 step: 3259, epoch: 98, batch: 24, loss: 0.02802497148513794, acc: 100.0, f1: 100.0, r: 0.7163950357448573
06/02/2019 03:50:52 step: 3264, epoch: 98, batch: 29, loss: 0.025729402899742126, acc: 98.4375, f1: 96.9047619047619, r: 0.7844159317798094
06/02/2019 03:50:52 *** evaluating ***
06/02/2019 03:50:52 step: 99, epoch: 98, acc: 56.837606837606835, f1: 24.968547744652106, r: 0.33069445719020496
06/02/2019 03:50:52 *** epoch: 100 ***
06/02/2019 03:50:52 *** training ***
06/02/2019 03:50:53 step: 3272, epoch: 99, batch: 4, loss: 0.04882664233446121, acc: 96.875, f1: 73.62318840579711, r: 0.7405219147741297
06/02/2019 03:50:53 step: 3277, epoch: 99, batch: 9, loss: 0.026376910507678986, acc: 98.4375, f1: 99.28193499622071, r: 0.8201776891308926
06/02/2019 03:50:53 step: 3282, epoch: 99, batch: 14, loss: 0.030472658574581146, acc: 100.0, f1: 100.0, r: 0.8374077725918456
06/02/2019 03:50:53 step: 3287, epoch: 99, batch: 19, loss: 0.01695450022816658, acc: 100.0, f1: 100.0, r: 0.6258596161314167
06/02/2019 03:50:54 step: 3292, epoch: 99, batch: 24, loss: 0.03658522292971611, acc: 98.4375, f1: 96.04938271604938, r: 0.5516159082757004
06/02/2019 03:50:54 step: 3297, epoch: 99, batch: 29, loss: 0.054949235171079636, acc: 96.875, f1: 96.90166975881262, r: 0.6737184752102282
06/02/2019 03:50:54 *** evaluating ***
06/02/2019 03:50:54 step: 100, epoch: 99, acc: 57.26495726495726, f1: 24.25630127475494, r: 0.318139125750914
06/02/2019 03:50:54 *** epoch: 101 ***
06/02/2019 03:50:54 *** training ***
06/02/2019 03:50:55 step: 3305, epoch: 100, batch: 4, loss: 0.005073104053735733, acc: 100.0, f1: 100.0, r: 0.7088482925611712
06/02/2019 03:50:55 step: 3310, epoch: 100, batch: 9, loss: 0.02140689641237259, acc: 100.0, f1: 100.0, r: 0.739949438738419
06/02/2019 03:50:55 step: 3315, epoch: 100, batch: 14, loss: 0.014786183834075928, acc: 100.0, f1: 100.0, r: 0.7798360042227858
06/02/2019 03:50:55 step: 3320, epoch: 100, batch: 19, loss: 0.022267194464802742, acc: 100.0, f1: 100.0, r: 0.6783111452045736
06/02/2019 03:50:56 step: 3325, epoch: 100, batch: 24, loss: 0.03666888549923897, acc: 98.4375, f1: 98.7878787878788, r: 0.7477719497182919
06/02/2019 03:50:56 step: 3330, epoch: 100, batch: 29, loss: 0.022177185863256454, acc: 100.0, f1: 100.0, r: 0.6161443174895096
06/02/2019 03:50:56 *** evaluating ***
06/02/2019 03:50:56 step: 101, epoch: 100, acc: 58.119658119658126, f1: 25.214840893095403, r: 0.3197522298312225
06/02/2019 03:50:56 *** epoch: 102 ***
06/02/2019 03:50:56 *** training ***
06/02/2019 03:50:56 step: 3338, epoch: 101, batch: 4, loss: 0.025268888100981712, acc: 100.0, f1: 100.0, r: 0.7558088123020476
06/02/2019 03:50:57 step: 3343, epoch: 101, batch: 9, loss: 0.00825183093547821, acc: 100.0, f1: 100.0, r: 0.8041164158489996
06/02/2019 03:50:57 step: 3348, epoch: 101, batch: 14, loss: 0.003801770508289337, acc: 100.0, f1: 100.0, r: 0.6611730205496271
06/02/2019 03:50:57 step: 3353, epoch: 101, batch: 19, loss: 0.014397108927369118, acc: 100.0, f1: 100.0, r: 0.6545847104091351
06/02/2019 03:50:58 step: 3358, epoch: 101, batch: 24, loss: 0.01756969839334488, acc: 100.0, f1: 100.0, r: 0.7043636389130786
06/02/2019 03:50:58 step: 3363, epoch: 101, batch: 29, loss: 0.014607097953557968, acc: 100.0, f1: 100.0, r: 0.6735681612120646
06/02/2019 03:50:58 *** evaluating ***
06/02/2019 03:50:58 step: 102, epoch: 101, acc: 58.97435897435898, f1: 25.82009802578087, r: 0.3227766551157227
06/02/2019 03:50:58 *** epoch: 103 ***
06/02/2019 03:50:58 *** training ***
06/02/2019 03:50:58 step: 3371, epoch: 102, batch: 4, loss: 0.016483400017023087, acc: 100.0, f1: 100.0, r: 0.7673843244448094
06/02/2019 03:50:59 step: 3376, epoch: 102, batch: 9, loss: 0.03350696340203285, acc: 98.4375, f1: 97.40259740259741, r: 0.7217831065090031
06/02/2019 03:50:59 step: 3381, epoch: 102, batch: 14, loss: 0.024268902838230133, acc: 100.0, f1: 100.0, r: 0.7228728394230399
06/02/2019 03:50:59 step: 3386, epoch: 102, batch: 19, loss: 0.009717907756567001, acc: 100.0, f1: 100.0, r: 0.6867478133821605
06/02/2019 03:50:59 step: 3391, epoch: 102, batch: 24, loss: 0.005783256143331528, acc: 100.0, f1: 100.0, r: 0.6751540885101353
06/02/2019 03:51:00 step: 3396, epoch: 102, batch: 29, loss: 0.06404188275337219, acc: 96.875, f1: 96.63381123058542, r: 0.7779542619495357
06/02/2019 03:51:00 *** evaluating ***
06/02/2019 03:51:00 step: 103, epoch: 102, acc: 57.692307692307686, f1: 25.864030230078427, r: 0.3254483114039184
06/02/2019 03:51:00 *** epoch: 104 ***
06/02/2019 03:51:00 *** training ***
06/02/2019 03:51:00 step: 3404, epoch: 103, batch: 4, loss: 0.027081605046987534, acc: 100.0, f1: 100.0, r: 0.701726094219922
06/02/2019 03:51:01 step: 3409, epoch: 103, batch: 9, loss: 0.04020668566226959, acc: 98.4375, f1: 97.20730397422128, r: 0.6814928716550518
06/02/2019 03:51:01 step: 3414, epoch: 103, batch: 14, loss: 0.014433279633522034, acc: 100.0, f1: 100.0, r: 0.7134913794019004
06/02/2019 03:51:01 step: 3419, epoch: 103, batch: 19, loss: 0.01764402724802494, acc: 100.0, f1: 100.0, r: 0.7278683472998198
06/02/2019 03:51:01 step: 3424, epoch: 103, batch: 24, loss: 0.016521506011486053, acc: 100.0, f1: 100.0, r: 0.6845631561613394
06/02/2019 03:51:02 step: 3429, epoch: 103, batch: 29, loss: 0.0075400881469249725, acc: 100.0, f1: 100.0, r: 0.7416336451220611
06/02/2019 03:51:02 *** evaluating ***
06/02/2019 03:51:02 step: 104, epoch: 103, acc: 57.692307692307686, f1: 25.733495063513057, r: 0.33287336823840924
06/02/2019 03:51:02 *** epoch: 105 ***
06/02/2019 03:51:02 *** training ***
06/02/2019 03:51:02 step: 3437, epoch: 104, batch: 4, loss: 0.0211857371032238, acc: 100.0, f1: 100.0, r: 0.8322441222618796
06/02/2019 03:51:02 step: 3442, epoch: 104, batch: 9, loss: 0.01106688380241394, acc: 100.0, f1: 100.0, r: 0.8396205266193887
06/02/2019 03:51:03 step: 3447, epoch: 104, batch: 14, loss: 0.02982761710882187, acc: 98.4375, f1: 97.55102040816325, r: 0.6654382727192886
06/02/2019 03:51:03 step: 3452, epoch: 104, batch: 19, loss: 0.05189318582415581, acc: 98.4375, f1: 87.03703703703704, r: 0.7796218581428029
06/02/2019 03:51:03 step: 3457, epoch: 104, batch: 24, loss: 0.015257814899086952, acc: 100.0, f1: 100.0, r: 0.6523298930565256
06/02/2019 03:51:04 step: 3462, epoch: 104, batch: 29, loss: 0.021702099591493607, acc: 100.0, f1: 100.0, r: 0.7468738732077598
06/02/2019 03:51:04 *** evaluating ***
06/02/2019 03:51:04 step: 105, epoch: 104, acc: 58.54700854700855, f1: 25.89014546248655, r: 0.32649315484099906
06/02/2019 03:51:04 *** epoch: 106 ***
06/02/2019 03:51:04 *** training ***
06/02/2019 03:51:04 step: 3470, epoch: 105, batch: 4, loss: 0.017812617123126984, acc: 100.0, f1: 100.0, r: 0.853212896337823
06/02/2019 03:51:04 step: 3475, epoch: 105, batch: 9, loss: 0.010112892836332321, acc: 100.0, f1: 100.0, r: 0.8019320405224578
06/02/2019 03:51:05 step: 3480, epoch: 105, batch: 14, loss: 0.030372988432645798, acc: 100.0, f1: 100.0, r: 0.7795512760185203
06/02/2019 03:51:05 step: 3485, epoch: 105, batch: 19, loss: 0.0196547731757164, acc: 100.0, f1: 100.0, r: 0.6938130517122676
06/02/2019 03:51:05 step: 3490, epoch: 105, batch: 24, loss: 0.008199825882911682, acc: 100.0, f1: 100.0, r: 0.6661198321555243
06/02/2019 03:51:06 step: 3495, epoch: 105, batch: 29, loss: 0.04554435983300209, acc: 98.4375, f1: 83.33333333333333, r: 0.7260022643389499
06/02/2019 03:51:06 *** evaluating ***
06/02/2019 03:51:06 step: 106, epoch: 105, acc: 58.54700854700855, f1: 25.90466452173669, r: 0.3312185349549276
06/02/2019 03:51:06 *** epoch: 107 ***
06/02/2019 03:51:06 *** training ***
06/02/2019 03:51:06 step: 3503, epoch: 106, batch: 4, loss: 0.031114406883716583, acc: 100.0, f1: 100.0, r: 0.7710517703049993
06/02/2019 03:51:06 step: 3508, epoch: 106, batch: 9, loss: 0.009165845811367035, acc: 100.0, f1: 100.0, r: 0.7468026571261667
06/02/2019 03:51:07 step: 3513, epoch: 106, batch: 14, loss: 0.01764417439699173, acc: 100.0, f1: 100.0, r: 0.7910387939074561
06/02/2019 03:51:07 step: 3518, epoch: 106, batch: 19, loss: 0.019342586398124695, acc: 100.0, f1: 100.0, r: 0.7910704620605609
06/02/2019 03:51:07 step: 3523, epoch: 106, batch: 24, loss: 0.011327028274536133, acc: 100.0, f1: 100.0, r: 0.7895767047471545
06/02/2019 03:51:08 step: 3528, epoch: 106, batch: 29, loss: 0.010415229946374893, acc: 100.0, f1: 100.0, r: 0.691708158642479
06/02/2019 03:51:08 *** evaluating ***
06/02/2019 03:51:08 step: 107, epoch: 106, acc: 58.54700854700855, f1: 25.954580209929727, r: 0.3291137741694984
06/02/2019 03:51:08 *** epoch: 108 ***
06/02/2019 03:51:08 *** training ***
06/02/2019 03:51:08 step: 3536, epoch: 107, batch: 4, loss: 0.021103516221046448, acc: 100.0, f1: 100.0, r: 0.7584484107277312
06/02/2019 03:51:08 step: 3541, epoch: 107, batch: 9, loss: 0.04381838068366051, acc: 98.4375, f1: 98.90070921985816, r: 0.8292414960674894
06/02/2019 03:51:09 step: 3546, epoch: 107, batch: 14, loss: 0.010202784091234207, acc: 100.0, f1: 100.0, r: 0.7050525932972627
06/02/2019 03:51:09 step: 3551, epoch: 107, batch: 19, loss: 0.051738839596509933, acc: 98.4375, f1: 98.88591800356507, r: 0.7873291337311832
06/02/2019 03:51:09 step: 3556, epoch: 107, batch: 24, loss: 0.048398081213235855, acc: 98.4375, f1: 98.49293563579279, r: 0.7149842345574071
06/02/2019 03:51:09 step: 3561, epoch: 107, batch: 29, loss: 0.025120703503489494, acc: 100.0, f1: 100.0, r: 0.8222000947297542
06/02/2019 03:51:10 *** evaluating ***
06/02/2019 03:51:10 step: 108, epoch: 107, acc: 58.97435897435898, f1: 26.432062625063985, r: 0.31696683211994203
06/02/2019 03:51:10 *** epoch: 109 ***
06/02/2019 03:51:10 *** training ***
06/02/2019 03:51:10 step: 3569, epoch: 108, batch: 4, loss: 0.011877419427037239, acc: 100.0, f1: 100.0, r: 0.7204398992646117
06/02/2019 03:51:10 step: 3574, epoch: 108, batch: 9, loss: 0.020890116691589355, acc: 100.0, f1: 100.0, r: 0.6409259854478195
06/02/2019 03:51:11 step: 3579, epoch: 108, batch: 14, loss: 0.00872073695063591, acc: 100.0, f1: 100.0, r: 0.6728654402562106
06/02/2019 03:51:11 step: 3584, epoch: 108, batch: 19, loss: 0.01830187439918518, acc: 100.0, f1: 100.0, r: 0.7546929550306155
06/02/2019 03:51:11 step: 3589, epoch: 108, batch: 24, loss: 0.029913414269685745, acc: 100.0, f1: 100.0, r: 0.6796373491000448
06/02/2019 03:51:12 step: 3594, epoch: 108, batch: 29, loss: 0.022208478301763535, acc: 98.4375, f1: 96.52173913043478, r: 0.6893669535021115
06/02/2019 03:51:12 *** evaluating ***
06/02/2019 03:51:12 step: 109, epoch: 108, acc: 57.26495726495726, f1: 25.58324969331995, r: 0.3251342079281534
06/02/2019 03:51:12 *** epoch: 110 ***
06/02/2019 03:51:12 *** training ***
06/02/2019 03:51:12 step: 3602, epoch: 109, batch: 4, loss: 0.008248116821050644, acc: 100.0, f1: 100.0, r: 0.8411130402494371
06/02/2019 03:51:12 step: 3607, epoch: 109, batch: 9, loss: 0.04842647910118103, acc: 98.4375, f1: 96.30252100840336, r: 0.7569244323904585
06/02/2019 03:51:13 step: 3612, epoch: 109, batch: 14, loss: 0.010218579322099686, acc: 100.0, f1: 100.0, r: 0.8211450815176822
06/02/2019 03:51:13 step: 3617, epoch: 109, batch: 19, loss: 0.051093168556690216, acc: 96.875, f1: 80.40816326530611, r: 0.5870825571578356
06/02/2019 03:51:13 step: 3622, epoch: 109, batch: 24, loss: 0.006569065153598785, acc: 100.0, f1: 100.0, r: 0.7963671118592226
06/02/2019 03:51:14 step: 3627, epoch: 109, batch: 29, loss: 0.019510194659233093, acc: 100.0, f1: 100.0, r: 0.6712018902224456
06/02/2019 03:51:14 *** evaluating ***
06/02/2019 03:51:14 step: 110, epoch: 109, acc: 58.97435897435898, f1: 25.976686909581648, r: 0.31761319521438935
06/02/2019 03:51:14 *** epoch: 111 ***
06/02/2019 03:51:14 *** training ***
06/02/2019 03:51:14 step: 3635, epoch: 110, batch: 4, loss: 0.007434718310832977, acc: 100.0, f1: 100.0, r: 0.8122793234908271
06/02/2019 03:51:15 step: 3640, epoch: 110, batch: 9, loss: 0.0027197599411010742, acc: 100.0, f1: 100.0, r: 0.7765964518434945
06/02/2019 03:51:15 step: 3645, epoch: 110, batch: 14, loss: 0.021643878892064095, acc: 98.4375, f1: 97.38775510204081, r: 0.7414251733378753
06/02/2019 03:51:15 step: 3650, epoch: 110, batch: 19, loss: 0.05951949954032898, acc: 98.4375, f1: 95.28985507246377, r: 0.733205824812977
06/02/2019 03:51:16 step: 3655, epoch: 110, batch: 24, loss: 0.042915672063827515, acc: 96.875, f1: 97.26984126984128, r: 0.6907261509904743
06/02/2019 03:51:16 step: 3660, epoch: 110, batch: 29, loss: 0.06985577940940857, acc: 96.875, f1: 93.92912868522625, r: 0.7950245723691705
06/02/2019 03:51:16 *** evaluating ***
06/02/2019 03:51:16 step: 111, epoch: 110, acc: 57.26495726495726, f1: 25.112386988324015, r: 0.3214976983038802
06/02/2019 03:51:16 *** epoch: 112 ***
06/02/2019 03:51:16 *** training ***
06/02/2019 03:51:16 step: 3668, epoch: 111, batch: 4, loss: 0.02896389737725258, acc: 98.4375, f1: 95.71428571428571, r: 0.7782116326441697
06/02/2019 03:51:17 step: 3673, epoch: 111, batch: 9, loss: 0.012753497809171677, acc: 100.0, f1: 100.0, r: 0.7813589759956069
06/02/2019 03:51:17 step: 3678, epoch: 111, batch: 14, loss: 0.01703464984893799, acc: 98.4375, f1: 96.79442508710801, r: 0.6323821638445527
06/02/2019 03:51:17 step: 3683, epoch: 111, batch: 19, loss: 0.009517854079604149, acc: 100.0, f1: 100.0, r: 0.6822463580070205
06/02/2019 03:51:18 step: 3688, epoch: 111, batch: 24, loss: 0.02638758346438408, acc: 100.0, f1: 100.0, r: 0.7490247378098789
06/02/2019 03:51:18 step: 3693, epoch: 111, batch: 29, loss: 0.02068902738392353, acc: 100.0, f1: 100.0, r: 0.69004394559493
06/02/2019 03:51:18 *** evaluating ***
06/02/2019 03:51:18 step: 112, epoch: 111, acc: 57.26495726495726, f1: 26.115487873564586, r: 0.3264585206428365
06/02/2019 03:51:18 *** epoch: 113 ***
06/02/2019 03:51:18 *** training ***
06/02/2019 03:51:18 step: 3701, epoch: 112, batch: 4, loss: 0.02854931727051735, acc: 100.0, f1: 100.0, r: 0.6958994932763464
06/02/2019 03:51:19 step: 3706, epoch: 112, batch: 9, loss: 0.008631564676761627, acc: 100.0, f1: 100.0, r: 0.7785866771184338
06/02/2019 03:51:19 step: 3711, epoch: 112, batch: 14, loss: 0.003040481358766556, acc: 100.0, f1: 100.0, r: 0.791479516575429
06/02/2019 03:51:19 step: 3716, epoch: 112, batch: 19, loss: 0.061557475477457047, acc: 98.4375, f1: 97.67080745341616, r: 0.8121799501551995
06/02/2019 03:51:20 step: 3721, epoch: 112, batch: 24, loss: 0.015863168984651566, acc: 100.0, f1: 100.0, r: 0.6914963269870591
06/02/2019 03:51:20 step: 3726, epoch: 112, batch: 29, loss: 0.016931824386119843, acc: 98.4375, f1: 86.11111111111111, r: 0.6717196301271688
06/02/2019 03:51:20 *** evaluating ***
06/02/2019 03:51:20 step: 113, epoch: 112, acc: 56.837606837606835, f1: 25.274162631154155, r: 0.3208586200628439
06/02/2019 03:51:20 *** epoch: 114 ***
06/02/2019 03:51:20 *** training ***
06/02/2019 03:51:20 step: 3734, epoch: 113, batch: 4, loss: 0.00921575352549553, acc: 100.0, f1: 100.0, r: 0.7479897467921072
06/02/2019 03:51:21 step: 3739, epoch: 113, batch: 9, loss: 0.004847198724746704, acc: 100.0, f1: 100.0, r: 0.7253655280613807
06/02/2019 03:51:21 step: 3744, epoch: 113, batch: 14, loss: 0.03114726021885872, acc: 98.4375, f1: 97.25274725274726, r: 0.8156908754625541
06/02/2019 03:51:21 step: 3749, epoch: 113, batch: 19, loss: 0.08985152840614319, acc: 96.875, f1: 95.92335102539184, r: 0.6560724660648833
06/02/2019 03:51:22 step: 3754, epoch: 113, batch: 24, loss: 0.0338766947388649, acc: 98.4375, f1: 97.96918767507003, r: 0.7679230669613191
06/02/2019 03:51:22 step: 3759, epoch: 113, batch: 29, loss: 0.018396444618701935, acc: 100.0, f1: 100.0, r: 0.8006782200764425
06/02/2019 03:51:22 *** evaluating ***
06/02/2019 03:51:22 step: 114, epoch: 113, acc: 57.692307692307686, f1: 25.727557755775578, r: 0.31222690159990424
06/02/2019 03:51:22 *** epoch: 115 ***
06/02/2019 03:51:22 *** training ***
06/02/2019 03:51:22 step: 3767, epoch: 114, batch: 4, loss: 0.12153136730194092, acc: 92.1875, f1: 90.8202614379085, r: 0.7426264851114175
06/02/2019 03:51:23 step: 3772, epoch: 114, batch: 9, loss: 0.04980124533176422, acc: 98.4375, f1: 95.55555555555556, r: 0.6306143012325441
06/02/2019 03:51:23 step: 3777, epoch: 114, batch: 14, loss: 0.019411742687225342, acc: 100.0, f1: 100.0, r: 0.6724939591829566
06/02/2019 03:51:23 step: 3782, epoch: 114, batch: 19, loss: 0.012549445033073425, acc: 100.0, f1: 100.0, r: 0.762037560582953
06/02/2019 03:51:23 step: 3787, epoch: 114, batch: 24, loss: 0.04254670813679695, acc: 98.4375, f1: 99.32234432234432, r: 0.7700826815615679
06/02/2019 03:51:24 step: 3792, epoch: 114, batch: 29, loss: 0.02595537155866623, acc: 100.0, f1: 100.0, r: 0.7347202995306628
06/02/2019 03:51:24 *** evaluating ***
06/02/2019 03:51:24 step: 115, epoch: 114, acc: 57.26495726495726, f1: 26.070493487455828, r: 0.3235625811320375
06/02/2019 03:51:24 *** epoch: 116 ***
06/02/2019 03:51:24 *** training ***
06/02/2019 03:51:24 step: 3800, epoch: 115, batch: 4, loss: 0.07401455193758011, acc: 96.875, f1: 96.83107371045661, r: 0.8099392964119424
06/02/2019 03:51:24 step: 3805, epoch: 115, batch: 9, loss: 0.04443734139204025, acc: 98.4375, f1: 86.66666666666667, r: 0.6708212607565991
06/02/2019 03:51:25 step: 3810, epoch: 115, batch: 14, loss: 0.01803518645465374, acc: 100.0, f1: 100.0, r: 0.7478331705840967
06/02/2019 03:51:25 step: 3815, epoch: 115, batch: 19, loss: 0.015808522701263428, acc: 100.0, f1: 100.0, r: 0.699076621814597
06/02/2019 03:51:25 step: 3820, epoch: 115, batch: 24, loss: 0.02740800939500332, acc: 100.0, f1: 100.0, r: 0.7701720942354455
06/02/2019 03:51:26 step: 3825, epoch: 115, batch: 29, loss: 0.01728670857846737, acc: 100.0, f1: 100.0, r: 0.6739490518853122
06/02/2019 03:51:26 *** evaluating ***
06/02/2019 03:51:26 step: 116, epoch: 115, acc: 56.837606837606835, f1: 25.418591965087924, r: 0.31754622841257657
06/02/2019 03:51:26 *** epoch: 117 ***
06/02/2019 03:51:26 *** training ***
06/02/2019 03:51:26 step: 3833, epoch: 116, batch: 4, loss: 0.0353802815079689, acc: 96.875, f1: 97.53463203463204, r: 0.7114647474284796
06/02/2019 03:51:26 step: 3838, epoch: 116, batch: 9, loss: 0.01298338919878006, acc: 100.0, f1: 100.0, r: 0.7859338319321185
06/02/2019 03:51:27 step: 3843, epoch: 116, batch: 14, loss: 0.001900259405374527, acc: 100.0, f1: 100.0, r: 0.6208765961757737
06/02/2019 03:51:27 step: 3848, epoch: 116, batch: 19, loss: 0.007685702294111252, acc: 100.0, f1: 100.0, r: 0.8726832395655562
06/02/2019 03:51:27 step: 3853, epoch: 116, batch: 24, loss: 0.03089548461139202, acc: 100.0, f1: 100.0, r: 0.6047006344890585
06/02/2019 03:51:28 step: 3858, epoch: 116, batch: 29, loss: 0.03776415437459946, acc: 98.4375, f1: 97.75132275132275, r: 0.7122220731001463
06/02/2019 03:51:28 *** evaluating ***
06/02/2019 03:51:28 step: 117, epoch: 116, acc: 57.26495726495726, f1: 25.58324969331995, r: 0.3320945737211217
06/02/2019 03:51:28 *** epoch: 118 ***
06/02/2019 03:51:28 *** training ***
06/02/2019 03:51:28 step: 3866, epoch: 117, batch: 4, loss: 0.04312237352132797, acc: 96.875, f1: 95.66544566544566, r: 0.8014502299751749
06/02/2019 03:51:28 step: 3871, epoch: 117, batch: 9, loss: 0.03607778996229172, acc: 98.4375, f1: 98.14921920185078, r: 0.7032201600086367
06/02/2019 03:51:29 step: 3876, epoch: 117, batch: 14, loss: 0.01470121368765831, acc: 100.0, f1: 100.0, r: 0.7930162479413339
06/02/2019 03:51:29 step: 3881, epoch: 117, batch: 19, loss: 0.0031022168695926666, acc: 100.0, f1: 100.0, r: 0.7311280575891186
06/02/2019 03:51:29 step: 3886, epoch: 117, batch: 24, loss: 0.0603766068816185, acc: 98.4375, f1: 99.06896551724138, r: 0.8124273094516897
06/02/2019 03:51:29 step: 3891, epoch: 117, batch: 29, loss: 0.0541367270052433, acc: 98.4375, f1: 98.42118665648077, r: 0.696957879131063
06/02/2019 03:51:30 *** evaluating ***
06/02/2019 03:51:30 step: 118, epoch: 117, acc: 58.54700854700855, f1: 25.32805968245245, r: 0.31509326260745923
06/02/2019 03:51:30 *** epoch: 119 ***
06/02/2019 03:51:30 *** training ***
06/02/2019 03:51:30 step: 3899, epoch: 118, batch: 4, loss: 0.029339749366044998, acc: 98.4375, f1: 96.04395604395604, r: 0.6745216221500189
06/02/2019 03:51:30 step: 3904, epoch: 118, batch: 9, loss: 0.009625226259231567, acc: 100.0, f1: 100.0, r: 0.7241364138521057
06/02/2019 03:51:31 step: 3909, epoch: 118, batch: 14, loss: 0.012382052838802338, acc: 100.0, f1: 100.0, r: 0.7580037117891989
06/02/2019 03:51:31 step: 3914, epoch: 118, batch: 19, loss: 0.024218380451202393, acc: 98.4375, f1: 99.12462006079026, r: 0.6870794185988285
06/02/2019 03:51:31 step: 3919, epoch: 118, batch: 24, loss: 0.006804116070270538, acc: 100.0, f1: 100.0, r: 0.7738570649145039
06/02/2019 03:51:31 step: 3924, epoch: 118, batch: 29, loss: 0.011310577392578125, acc: 100.0, f1: 100.0, r: 0.7416733477211119
06/02/2019 03:51:32 *** evaluating ***
06/02/2019 03:51:32 step: 119, epoch: 118, acc: 56.41025641025641, f1: 24.70844498939544, r: 0.3250624718424835
06/02/2019 03:51:32 *** epoch: 120 ***
06/02/2019 03:51:32 *** training ***
06/02/2019 03:51:32 step: 3932, epoch: 119, batch: 4, loss: 0.009808510541915894, acc: 100.0, f1: 100.0, r: 0.8351929665025036
06/02/2019 03:51:32 step: 3937, epoch: 119, batch: 9, loss: 0.012067247182130814, acc: 100.0, f1: 100.0, r: 0.7347876833517207
06/02/2019 03:51:33 step: 3942, epoch: 119, batch: 14, loss: 0.011981680989265442, acc: 100.0, f1: 100.0, r: 0.7701321610397907
06/02/2019 03:51:33 step: 3947, epoch: 119, batch: 19, loss: 0.024043582379817963, acc: 98.4375, f1: 99.10775566231983, r: 0.6063986093193893
06/02/2019 03:51:33 step: 3952, epoch: 119, batch: 24, loss: 0.03824230283498764, acc: 96.875, f1: 96.54741896758703, r: 0.7370090513374513
06/02/2019 03:51:33 step: 3957, epoch: 119, batch: 29, loss: 0.006340421736240387, acc: 100.0, f1: 100.0, r: 0.6587267163103562
06/02/2019 03:51:33 *** evaluating ***
06/02/2019 03:51:34 step: 120, epoch: 119, acc: 57.26495726495726, f1: 25.12628410652359, r: 0.32694874759687187
06/02/2019 03:51:34 *** epoch: 121 ***
06/02/2019 03:51:34 *** training ***
06/02/2019 03:51:34 step: 3965, epoch: 120, batch: 4, loss: 0.03008721023797989, acc: 96.875, f1: 92.81117852546424, r: 0.6811445345729765
06/02/2019 03:51:34 step: 3970, epoch: 120, batch: 9, loss: 0.041528865694999695, acc: 98.4375, f1: 96.66666666666667, r: 0.7963060237330565
06/02/2019 03:51:34 step: 3975, epoch: 120, batch: 14, loss: 0.005756232887506485, acc: 100.0, f1: 100.0, r: 0.6973855963441228
06/02/2019 03:51:35 step: 3980, epoch: 120, batch: 19, loss: 0.01292465254664421, acc: 100.0, f1: 100.0, r: 0.7799729831240023
06/02/2019 03:51:35 step: 3985, epoch: 120, batch: 24, loss: 0.031960271298885345, acc: 98.4375, f1: 95.10204081632652, r: 0.7012614759110333
06/02/2019 03:51:35 step: 3990, epoch: 120, batch: 29, loss: 0.016105178743600845, acc: 100.0, f1: 100.0, r: 0.6363711127558109
06/02/2019 03:51:35 *** evaluating ***
06/02/2019 03:51:36 step: 121, epoch: 120, acc: 56.41025641025641, f1: 24.70844498939544, r: 0.3302253418759926
06/02/2019 03:51:36 *** epoch: 122 ***
06/02/2019 03:51:36 *** training ***
06/02/2019 03:51:36 step: 3998, epoch: 121, batch: 4, loss: 0.011147905141115189, acc: 100.0, f1: 100.0, r: 0.7210447525392071
06/02/2019 03:51:36 step: 4003, epoch: 121, batch: 9, loss: 0.03225906193256378, acc: 98.4375, f1: 94.74548440065682, r: 0.7428076695070555
06/02/2019 03:51:36 step: 4008, epoch: 121, batch: 14, loss: 0.008357372134923935, acc: 100.0, f1: 100.0, r: 0.873223298868523
06/02/2019 03:51:37 step: 4013, epoch: 121, batch: 19, loss: 0.015476066619157791, acc: 100.0, f1: 100.0, r: 0.7571663527922484
06/02/2019 03:51:37 step: 4018, epoch: 121, batch: 24, loss: 0.025874588638544083, acc: 100.0, f1: 100.0, r: 0.8144758650829838
06/02/2019 03:51:37 step: 4023, epoch: 121, batch: 29, loss: 0.05300308018922806, acc: 96.875, f1: 85.44973544973544, r: 0.6602291110875914
06/02/2019 03:51:37 *** evaluating ***
06/02/2019 03:51:38 step: 122, epoch: 121, acc: 58.97435897435898, f1: 25.995172761615976, r: 0.31794572610314437
06/02/2019 03:51:38 *** epoch: 123 ***
06/02/2019 03:51:38 *** training ***
06/02/2019 03:51:38 step: 4031, epoch: 122, batch: 4, loss: 0.03331328183412552, acc: 98.4375, f1: 99.04665607395637, r: 0.6700120818794242
06/02/2019 03:51:38 step: 4036, epoch: 122, batch: 9, loss: 0.008561357855796814, acc: 100.0, f1: 100.0, r: 0.6951087011256122
06/02/2019 03:51:38 step: 4041, epoch: 122, batch: 14, loss: 0.014148551970720291, acc: 100.0, f1: 100.0, r: 0.6982838664244111
06/02/2019 03:51:39 step: 4046, epoch: 122, batch: 19, loss: 0.016683295369148254, acc: 100.0, f1: 100.0, r: 0.6237009438999489
06/02/2019 03:51:39 step: 4051, epoch: 122, batch: 24, loss: 0.0197780542075634, acc: 100.0, f1: 100.0, r: 0.6568007002684284
06/02/2019 03:51:39 step: 4056, epoch: 122, batch: 29, loss: 0.007466282695531845, acc: 100.0, f1: 100.0, r: 0.8463969910693597
06/02/2019 03:51:39 *** evaluating ***
06/02/2019 03:51:40 step: 123, epoch: 122, acc: 56.837606837606835, f1: 25.388771301986495, r: 0.3284056381268184
06/02/2019 03:51:40 *** epoch: 124 ***
06/02/2019 03:51:40 *** training ***
06/02/2019 03:51:40 step: 4064, epoch: 123, batch: 4, loss: 0.0016874708235263824, acc: 100.0, f1: 100.0, r: 0.6798442066307322
06/02/2019 03:51:40 step: 4069, epoch: 123, batch: 9, loss: 0.019937656819820404, acc: 100.0, f1: 100.0, r: 0.7860263761857886
06/02/2019 03:51:40 step: 4074, epoch: 123, batch: 14, loss: 0.023317500948905945, acc: 100.0, f1: 100.0, r: 0.6786323006476782
06/02/2019 03:51:41 step: 4079, epoch: 123, batch: 19, loss: 0.01659557595849037, acc: 100.0, f1: 100.0, r: 0.7264374089854181
06/02/2019 03:51:41 step: 4084, epoch: 123, batch: 24, loss: 0.008246377110481262, acc: 100.0, f1: 100.0, r: 0.668166761839859
06/02/2019 03:51:41 step: 4089, epoch: 123, batch: 29, loss: 0.021476365625858307, acc: 98.4375, f1: 98.20574162679426, r: 0.7793783258313968
06/02/2019 03:51:41 *** evaluating ***
06/02/2019 03:51:41 step: 124, epoch: 123, acc: 56.837606837606835, f1: 24.906418476399285, r: 0.32474240248018776
06/02/2019 03:51:41 *** epoch: 125 ***
06/02/2019 03:51:41 *** training ***
06/02/2019 03:51:42 step: 4097, epoch: 124, batch: 4, loss: 0.009336043149232864, acc: 100.0, f1: 100.0, r: 0.6729571436074595
06/02/2019 03:51:42 step: 4102, epoch: 124, batch: 9, loss: 0.004402652382850647, acc: 100.0, f1: 100.0, r: 0.8133607055922217
06/02/2019 03:51:42 step: 4107, epoch: 124, batch: 14, loss: 0.06442046165466309, acc: 95.3125, f1: 89.6734693877551, r: 0.6617020960011049
06/02/2019 03:51:43 step: 4112, epoch: 124, batch: 19, loss: 0.010342512279748917, acc: 100.0, f1: 100.0, r: 0.6993977502641118
06/02/2019 03:51:43 step: 4117, epoch: 124, batch: 24, loss: 0.06688512116670609, acc: 96.875, f1: 97.68939393939394, r: 0.7882685886315066
06/02/2019 03:51:43 step: 4122, epoch: 124, batch: 29, loss: 0.006027527153491974, acc: 100.0, f1: 100.0, r: 0.7205728288026314
06/02/2019 03:51:43 *** evaluating ***
06/02/2019 03:51:43 step: 125, epoch: 124, acc: 57.692307692307686, f1: 25.63514545162327, r: 0.3287459784978346
06/02/2019 03:51:43 *** epoch: 126 ***
06/02/2019 03:51:43 *** training ***
06/02/2019 03:51:44 step: 4130, epoch: 125, batch: 4, loss: 0.024907369166612625, acc: 100.0, f1: 100.0, r: 0.789998277165312
06/02/2019 03:51:44 step: 4135, epoch: 125, batch: 9, loss: 0.0435265451669693, acc: 100.0, f1: 100.0, r: 0.7701164429422431
06/02/2019 03:51:44 step: 4140, epoch: 125, batch: 14, loss: 0.010758068412542343, acc: 100.0, f1: 100.0, r: 0.7579857604414727
06/02/2019 03:51:45 step: 4145, epoch: 125, batch: 19, loss: 0.03560860455036163, acc: 98.4375, f1: 97.12121212121212, r: 0.8361205510696321
06/02/2019 03:51:45 step: 4150, epoch: 125, batch: 24, loss: 0.02509322203695774, acc: 100.0, f1: 100.0, r: 0.7095294548263856
06/02/2019 03:51:45 step: 4155, epoch: 125, batch: 29, loss: 0.02182755619287491, acc: 100.0, f1: 100.0, r: 0.7592515148602621
06/02/2019 03:51:45 *** evaluating ***
06/02/2019 03:51:46 step: 126, epoch: 125, acc: 56.837606837606835, f1: 25.546085059333905, r: 0.32379390254401164
06/02/2019 03:51:46 *** epoch: 127 ***
06/02/2019 03:51:46 *** training ***
06/02/2019 03:51:46 step: 4163, epoch: 126, batch: 4, loss: 0.015435773879289627, acc: 100.0, f1: 100.0, r: 0.6432374824950923
06/02/2019 03:51:46 step: 4168, epoch: 126, batch: 9, loss: 0.02982771396636963, acc: 98.4375, f1: 97.94941900205059, r: 0.6618742122313886
06/02/2019 03:51:46 step: 4173, epoch: 126, batch: 14, loss: 0.00537872314453125, acc: 100.0, f1: 100.0, r: 0.6791968509318529
06/02/2019 03:51:47 step: 4178, epoch: 126, batch: 19, loss: 0.008735258132219315, acc: 100.0, f1: 100.0, r: 0.7170878335894575
06/02/2019 03:51:47 step: 4183, epoch: 126, batch: 24, loss: 0.04751453921198845, acc: 100.0, f1: 100.0, r: 0.7944533507417704
06/02/2019 03:51:47 step: 4188, epoch: 126, batch: 29, loss: 0.02856455370783806, acc: 100.0, f1: 100.0, r: 0.7983545216795922
06/02/2019 03:51:48 *** evaluating ***
06/02/2019 03:51:48 step: 127, epoch: 126, acc: 57.26495726495726, f1: 26.162335373070434, r: 0.3219863424350742
06/02/2019 03:51:48 *** epoch: 128 ***
06/02/2019 03:51:48 *** training ***
06/02/2019 03:51:48 step: 4196, epoch: 127, batch: 4, loss: 0.005938112735748291, acc: 100.0, f1: 100.0, r: 0.7768198910656784
06/02/2019 03:51:48 step: 4201, epoch: 127, batch: 9, loss: 0.016405442729592323, acc: 100.0, f1: 100.0, r: 0.7488314847260298
06/02/2019 03:51:49 step: 4206, epoch: 127, batch: 14, loss: 0.04518773779273033, acc: 98.4375, f1: 86.66666666666667, r: 0.7273851487630094
06/02/2019 03:51:49 step: 4211, epoch: 127, batch: 19, loss: 0.008488062769174576, acc: 100.0, f1: 100.0, r: 0.7465130351244791
06/02/2019 03:51:49 step: 4216, epoch: 127, batch: 24, loss: 0.015553891658782959, acc: 100.0, f1: 100.0, r: 0.6286261996668355
06/02/2019 03:51:50 step: 4221, epoch: 127, batch: 29, loss: 0.007379263639450073, acc: 100.0, f1: 100.0, r: 0.6245494317845579
06/02/2019 03:51:50 *** evaluating ***
06/02/2019 03:51:50 step: 128, epoch: 127, acc: 56.837606837606835, f1: 25.466027874564457, r: 0.3218106003534468
06/02/2019 03:51:50 *** epoch: 129 ***
06/02/2019 03:51:50 *** training ***
06/02/2019 03:51:50 step: 4229, epoch: 128, batch: 4, loss: 0.013508237898349762, acc: 100.0, f1: 100.0, r: 0.7967310149367349
06/02/2019 03:51:50 step: 4234, epoch: 128, batch: 9, loss: 0.012923173606395721, acc: 100.0, f1: 100.0, r: 0.70746213067444
06/02/2019 03:51:51 step: 4239, epoch: 128, batch: 14, loss: 0.0034286752343177795, acc: 100.0, f1: 100.0, r: 0.7502496016497626
06/02/2019 03:51:51 step: 4244, epoch: 128, batch: 19, loss: 0.006786450743675232, acc: 100.0, f1: 100.0, r: 0.8085808452196582
06/02/2019 03:51:51 step: 4249, epoch: 128, batch: 24, loss: 0.012509182095527649, acc: 100.0, f1: 100.0, r: 0.6740440366235778
06/02/2019 03:51:52 step: 4254, epoch: 128, batch: 29, loss: 0.028691742569208145, acc: 100.0, f1: 100.0, r: 0.7181659817028345
06/02/2019 03:51:52 *** evaluating ***
06/02/2019 03:51:52 step: 129, epoch: 128, acc: 56.837606837606835, f1: 25.010086900990725, r: 0.3140650210679098
06/02/2019 03:51:52 *** epoch: 130 ***
06/02/2019 03:51:52 *** training ***
06/02/2019 03:51:52 step: 4262, epoch: 129, batch: 4, loss: 0.007637962698936462, acc: 100.0, f1: 100.0, r: 0.8143707737445249
06/02/2019 03:51:52 step: 4267, epoch: 129, batch: 9, loss: 0.028623994439840317, acc: 98.4375, f1: 99.25490196078431, r: 0.8337436646737834
06/02/2019 03:51:53 step: 4272, epoch: 129, batch: 14, loss: 0.028651535511016846, acc: 98.4375, f1: 98.40848806366049, r: 0.6878059196262498
06/02/2019 03:51:53 step: 4277, epoch: 129, batch: 19, loss: 0.03329453989863396, acc: 100.0, f1: 100.0, r: 0.7196235610366262
06/02/2019 03:51:53 step: 4282, epoch: 129, batch: 24, loss: 0.003936905413866043, acc: 100.0, f1: 100.0, r: 0.756880809737245
06/02/2019 03:51:53 step: 4287, epoch: 129, batch: 29, loss: 0.025658201426267624, acc: 98.4375, f1: 99.02885682574917, r: 0.7530297010838112
06/02/2019 03:51:54 *** evaluating ***
06/02/2019 03:51:54 step: 130, epoch: 129, acc: 57.26495726495726, f1: 25.49087183308495, r: 0.32377342223724676
06/02/2019 03:51:54 *** epoch: 131 ***
06/02/2019 03:51:54 *** training ***
06/02/2019 03:51:54 step: 4295, epoch: 130, batch: 4, loss: 0.026175156235694885, acc: 100.0, f1: 100.0, r: 0.7054995600988426
06/02/2019 03:51:54 step: 4300, epoch: 130, batch: 9, loss: 0.010782334953546524, acc: 100.0, f1: 100.0, r: 0.7186638144196124
06/02/2019 03:51:55 step: 4305, epoch: 130, batch: 14, loss: 0.0037623345851898193, acc: 100.0, f1: 100.0, r: 0.6988960309009189
06/02/2019 03:51:55 step: 4310, epoch: 130, batch: 19, loss: 0.02928091585636139, acc: 100.0, f1: 100.0, r: 0.8227527945700512
06/02/2019 03:51:55 step: 4315, epoch: 130, batch: 24, loss: 0.012730944901704788, acc: 100.0, f1: 100.0, r: 0.701208204877157
06/02/2019 03:51:55 step: 4320, epoch: 130, batch: 29, loss: 0.02000926434993744, acc: 100.0, f1: 100.0, r: 0.8179426961373056
06/02/2019 03:51:56 *** evaluating ***
06/02/2019 03:51:56 step: 131, epoch: 130, acc: 57.692307692307686, f1: 25.67577454675335, r: 0.32399361769013485
06/02/2019 03:51:56 *** epoch: 132 ***
06/02/2019 03:51:56 *** training ***
06/02/2019 03:51:56 step: 4328, epoch: 131, batch: 4, loss: 0.03546835109591484, acc: 98.4375, f1: 93.33333333333333, r: 0.7808520668509066
06/02/2019 03:51:56 step: 4333, epoch: 131, batch: 9, loss: 0.032476529479026794, acc: 98.4375, f1: 96.4625850340136, r: 0.669766250552669
06/02/2019 03:51:56 step: 4338, epoch: 131, batch: 14, loss: 0.01564415916800499, acc: 98.4375, f1: 98.46710666382798, r: 0.6704683690805524
06/02/2019 03:51:57 step: 4343, epoch: 131, batch: 19, loss: 0.006479144096374512, acc: 100.0, f1: 100.0, r: 0.705410420788184
06/02/2019 03:51:57 step: 4348, epoch: 131, batch: 24, loss: 0.012856870889663696, acc: 100.0, f1: 100.0, r: 0.7896886443756956
06/02/2019 03:51:57 step: 4353, epoch: 131, batch: 29, loss: 0.024515243247151375, acc: 100.0, f1: 100.0, r: 0.746471634993141
06/02/2019 03:51:58 *** evaluating ***
06/02/2019 03:51:58 step: 132, epoch: 131, acc: 56.837606837606835, f1: 24.30806785991233, r: 0.3201256362895299
06/02/2019 03:51:58 *** epoch: 133 ***
06/02/2019 03:51:58 *** training ***
06/02/2019 03:51:58 step: 4361, epoch: 132, batch: 4, loss: 0.0050411224365234375, acc: 100.0, f1: 100.0, r: 0.7514976655843119
06/02/2019 03:51:58 step: 4366, epoch: 132, batch: 9, loss: 0.01611672341823578, acc: 100.0, f1: 100.0, r: 0.8224777440004651
06/02/2019 03:51:59 step: 4371, epoch: 132, batch: 14, loss: 0.045676179230213165, acc: 98.4375, f1: 97.76021080368906, r: 0.6063164594511068
06/02/2019 03:51:59 step: 4376, epoch: 132, batch: 19, loss: 0.04661893844604492, acc: 98.4375, f1: 99.28193499622071, r: 0.7228849109531046
06/02/2019 03:51:59 step: 4381, epoch: 132, batch: 24, loss: 0.020733922719955444, acc: 98.4375, f1: 95.57823129251702, r: 0.7826310221038242
06/02/2019 03:52:00 step: 4386, epoch: 132, batch: 29, loss: 0.006397850811481476, acc: 100.0, f1: 100.0, r: 0.7077548342533317
06/02/2019 03:52:00 *** evaluating ***
06/02/2019 03:52:00 step: 133, epoch: 132, acc: 57.26495726495726, f1: 24.389466402906535, r: 0.3223698802974092
06/02/2019 03:52:00 *** epoch: 134 ***
06/02/2019 03:52:00 *** training ***
06/02/2019 03:52:00 step: 4394, epoch: 133, batch: 4, loss: 0.016307003796100616, acc: 100.0, f1: 100.0, r: 0.8197927398305912
06/02/2019 03:52:00 step: 4399, epoch: 133, batch: 9, loss: 0.008498571813106537, acc: 100.0, f1: 100.0, r: 0.7387016590337409
06/02/2019 03:52:01 step: 4404, epoch: 133, batch: 14, loss: 0.010570026934146881, acc: 100.0, f1: 100.0, r: 0.6591506798050969
06/02/2019 03:52:01 step: 4409, epoch: 133, batch: 19, loss: 0.033185236155986786, acc: 98.4375, f1: 97.38775510204081, r: 0.7058906832043133
06/02/2019 03:52:01 step: 4414, epoch: 133, batch: 24, loss: 0.012509055435657501, acc: 100.0, f1: 100.0, r: 0.7608163141563509
06/02/2019 03:52:02 step: 4419, epoch: 133, batch: 29, loss: 0.03044917806982994, acc: 96.875, f1: 94.57055682684974, r: 0.7521045770516621
06/02/2019 03:52:02 *** evaluating ***
06/02/2019 03:52:02 step: 134, epoch: 133, acc: 56.41025641025641, f1: 24.800210393061473, r: 0.32400387638889616
06/02/2019 03:52:02 *** epoch: 135 ***
06/02/2019 03:52:02 *** training ***
06/02/2019 03:52:02 step: 4427, epoch: 134, batch: 4, loss: 0.012463156133890152, acc: 100.0, f1: 100.0, r: 0.7759901284977213
06/02/2019 03:52:02 step: 4432, epoch: 134, batch: 9, loss: 0.013118766248226166, acc: 100.0, f1: 100.0, r: 0.675136233510311
06/02/2019 03:52:03 step: 4437, epoch: 134, batch: 14, loss: 0.016279716044664383, acc: 100.0, f1: 100.0, r: 0.7073853331776692
06/02/2019 03:52:03 step: 4442, epoch: 134, batch: 19, loss: 0.03916142135858536, acc: 98.4375, f1: 99.22727711774365, r: 0.7209079986918767
06/02/2019 03:52:03 step: 4447, epoch: 134, batch: 24, loss: 0.016380619257688522, acc: 100.0, f1: 100.0, r: 0.7279906608576748
06/02/2019 03:52:03 step: 4452, epoch: 134, batch: 29, loss: 0.0247994102537632, acc: 100.0, f1: 100.0, r: 0.8241676703553276
06/02/2019 03:52:04 *** evaluating ***
06/02/2019 03:52:04 step: 135, epoch: 134, acc: 56.41025641025641, f1: 25.279078844652613, r: 0.32068147533646757
06/02/2019 03:52:04 *** epoch: 136 ***
06/02/2019 03:52:04 *** training ***
06/02/2019 03:52:04 step: 4460, epoch: 135, batch: 4, loss: 0.0033283084630966187, acc: 100.0, f1: 100.0, r: 0.7123570564654994
06/02/2019 03:52:04 step: 4465, epoch: 135, batch: 9, loss: 0.00597536563873291, acc: 100.0, f1: 100.0, r: 0.7465798772763937
06/02/2019 03:52:05 step: 4470, epoch: 135, batch: 14, loss: 0.03728146106004715, acc: 96.875, f1: 96.26722483865342, r: 0.6893331777564765
06/02/2019 03:52:05 step: 4475, epoch: 135, batch: 19, loss: 0.006453059613704681, acc: 100.0, f1: 100.0, r: 0.6758893848650145
06/02/2019 03:52:05 step: 4480, epoch: 135, batch: 24, loss: 0.031747762113809586, acc: 98.4375, f1: 98.91156462585033, r: 0.7654668591486533
06/02/2019 03:52:05 step: 4485, epoch: 135, batch: 29, loss: 0.013659082353115082, acc: 100.0, f1: 100.0, r: 0.5687582780448404
06/02/2019 03:52:06 *** evaluating ***
06/02/2019 03:52:06 step: 136, epoch: 135, acc: 58.119658119658126, f1: 25.83394743306313, r: 0.3233492324069529
06/02/2019 03:52:06 *** epoch: 137 ***
06/02/2019 03:52:06 *** training ***
06/02/2019 03:52:06 step: 4493, epoch: 136, batch: 4, loss: 0.012639336287975311, acc: 100.0, f1: 100.0, r: 0.7130146633674247
06/02/2019 03:52:06 step: 4498, epoch: 136, batch: 9, loss: 0.00553489476442337, acc: 100.0, f1: 100.0, r: 0.6847327637921223
06/02/2019 03:52:07 step: 4503, epoch: 136, batch: 14, loss: 0.033950988203287125, acc: 98.4375, f1: 97.97979797979798, r: 0.6582772674024389
06/02/2019 03:52:07 step: 4508, epoch: 136, batch: 19, loss: 0.018322858959436417, acc: 98.4375, f1: 99.0514075887393, r: 0.8056758757786694
06/02/2019 03:52:07 step: 4513, epoch: 136, batch: 24, loss: 0.01706863008439541, acc: 100.0, f1: 100.0, r: 0.8167032240902291
06/02/2019 03:52:07 step: 4518, epoch: 136, batch: 29, loss: 0.015890218317508698, acc: 100.0, f1: 100.0, r: 0.6285275628519688
06/02/2019 03:52:08 *** evaluating ***
06/02/2019 03:52:08 step: 137, epoch: 136, acc: 58.119658119658126, f1: 25.321811416523964, r: 0.3076926269424451
06/02/2019 03:52:08 *** epoch: 138 ***
06/02/2019 03:52:08 *** training ***
06/02/2019 03:52:08 step: 4526, epoch: 137, batch: 4, loss: 0.013041909784078598, acc: 100.0, f1: 100.0, r: 0.6368054725451558
06/02/2019 03:52:08 step: 4531, epoch: 137, batch: 9, loss: 0.014063142240047455, acc: 100.0, f1: 100.0, r: 0.8085496786430587
06/02/2019 03:52:08 step: 4536, epoch: 137, batch: 14, loss: 0.005859989672899246, acc: 100.0, f1: 100.0, r: 0.7189185500660142
06/02/2019 03:52:09 step: 4541, epoch: 137, batch: 19, loss: 0.0041833072900772095, acc: 100.0, f1: 100.0, r: 0.7466001126026794
06/02/2019 03:52:09 step: 4546, epoch: 137, batch: 24, loss: 0.059401802718639374, acc: 98.4375, f1: 99.11483253588517, r: 0.799197416925488
06/02/2019 03:52:09 step: 4551, epoch: 137, batch: 29, loss: 0.038387708365917206, acc: 98.4375, f1: 97.28813559322033, r: 0.8270247338901151
06/02/2019 03:52:09 *** evaluating ***
06/02/2019 03:52:09 step: 138, epoch: 137, acc: 57.692307692307686, f1: 25.654841216966467, r: 0.3188730372254281
06/02/2019 03:52:09 *** epoch: 139 ***
06/02/2019 03:52:09 *** training ***
06/02/2019 03:52:10 step: 4559, epoch: 138, batch: 4, loss: 0.05152637138962746, acc: 98.4375, f1: 99.1386735572782, r: 0.7110972503618215
06/02/2019 03:52:10 step: 4564, epoch: 138, batch: 9, loss: 0.004105377942323685, acc: 100.0, f1: 100.0, r: 0.7804132021635473
06/02/2019 03:52:10 step: 4569, epoch: 138, batch: 14, loss: 0.0022600889205932617, acc: 100.0, f1: 100.0, r: 0.7993186514132692
06/02/2019 03:52:10 step: 4574, epoch: 138, batch: 19, loss: 0.04156067967414856, acc: 96.875, f1: 98.21174196174196, r: 0.6878891380427912
06/02/2019 03:52:10 step: 4579, epoch: 138, batch: 24, loss: 0.01803695410490036, acc: 100.0, f1: 100.0, r: 0.7350818975250927
06/02/2019 03:52:11 step: 4584, epoch: 138, batch: 29, loss: 0.03568702191114426, acc: 98.4375, f1: 97.87644787644787, r: 0.7484158160073503
06/02/2019 03:52:11 *** evaluating ***
06/02/2019 03:52:11 step: 139, epoch: 138, acc: 56.41025641025641, f1: 24.37718513700301, r: 0.3112711216307616
06/02/2019 03:52:11 *** epoch: 140 ***
06/02/2019 03:52:11 *** training ***
06/02/2019 03:52:11 step: 4592, epoch: 139, batch: 4, loss: 0.03163490444421768, acc: 98.4375, f1: 95.0, r: 0.769282100301307
06/02/2019 03:52:11 step: 4597, epoch: 139, batch: 9, loss: 0.04469725489616394, acc: 98.4375, f1: 98.71901977165135, r: 0.6679793179846213
06/02/2019 03:52:11 step: 4602, epoch: 139, batch: 14, loss: 0.0377916544675827, acc: 98.4375, f1: 98.63945578231294, r: 0.6912860373208821
06/02/2019 03:52:12 step: 4607, epoch: 139, batch: 19, loss: 0.030698947608470917, acc: 100.0, f1: 100.0, r: 0.8228495548178305
06/02/2019 03:52:12 step: 4612, epoch: 139, batch: 24, loss: 0.04776349663734436, acc: 98.4375, f1: 97.40259740259741, r: 0.7286267612415768
06/02/2019 03:52:12 step: 4617, epoch: 139, batch: 29, loss: 0.020649582147598267, acc: 98.4375, f1: 96.1111111111111, r: 0.8187703235389484
06/02/2019 03:52:12 *** evaluating ***
06/02/2019 03:52:12 step: 140, epoch: 139, acc: 57.26495726495726, f1: 24.29382585656229, r: 0.30918840741923
06/02/2019 03:52:12 *** epoch: 141 ***
06/02/2019 03:52:12 *** training ***
06/02/2019 03:52:12 step: 4625, epoch: 140, batch: 4, loss: 0.026617448776960373, acc: 100.0, f1: 100.0, r: 0.75785742350842
06/02/2019 03:52:12 step: 4630, epoch: 140, batch: 9, loss: 0.015130981802940369, acc: 100.0, f1: 100.0, r: 0.763060514768456
06/02/2019 03:52:13 step: 4635, epoch: 140, batch: 14, loss: 0.045754022896289825, acc: 96.875, f1: 83.35600907029479, r: 0.7702893734840482
06/02/2019 03:52:13 step: 4640, epoch: 140, batch: 19, loss: 0.03988645225763321, acc: 96.875, f1: 96.77775554518034, r: 0.7835369222517319
06/02/2019 03:52:13 step: 4645, epoch: 140, batch: 24, loss: 0.0157729871571064, acc: 100.0, f1: 100.0, r: 0.7998284678137185
06/02/2019 03:52:13 step: 4650, epoch: 140, batch: 29, loss: 0.00918913260102272, acc: 100.0, f1: 100.0, r: 0.841432353133059
06/02/2019 03:52:13 *** evaluating ***
06/02/2019 03:52:13 step: 141, epoch: 140, acc: 56.837606837606835, f1: 24.834783200385235, r: 0.31760803978289087
06/02/2019 03:52:13 *** epoch: 142 ***
06/02/2019 03:52:13 *** training ***
06/02/2019 03:52:14 step: 4658, epoch: 141, batch: 4, loss: 0.007123991847038269, acc: 100.0, f1: 100.0, r: 0.700279253679198
06/02/2019 03:52:14 step: 4663, epoch: 141, batch: 9, loss: 0.03624480962753296, acc: 98.4375, f1: 93.33333333333333, r: 0.7809198559477131
06/02/2019 03:52:14 step: 4668, epoch: 141, batch: 14, loss: 0.01266457885503769, acc: 100.0, f1: 100.0, r: 0.6708244258764804
06/02/2019 03:52:14 step: 4673, epoch: 141, batch: 19, loss: 0.013718090951442719, acc: 100.0, f1: 100.0, r: 0.7663128252911559
06/02/2019 03:52:14 step: 4678, epoch: 141, batch: 24, loss: 0.04906057193875313, acc: 96.875, f1: 97.69928484214198, r: 0.694359043560089
06/02/2019 03:52:15 step: 4683, epoch: 141, batch: 29, loss: 0.009740516543388367, acc: 100.0, f1: 100.0, r: 0.8343489250228069
06/02/2019 03:52:15 *** evaluating ***
06/02/2019 03:52:15 step: 142, epoch: 141, acc: 56.837606837606835, f1: 24.957263505568612, r: 0.3176418264172998
06/02/2019 03:52:15 *** epoch: 143 ***
06/02/2019 03:52:15 *** training ***
06/02/2019 03:52:15 step: 4691, epoch: 142, batch: 4, loss: 0.004612758755683899, acc: 100.0, f1: 100.0, r: 0.8224975123955709
06/02/2019 03:52:15 step: 4696, epoch: 142, batch: 9, loss: 0.0268816240131855, acc: 100.0, f1: 100.0, r: 0.7402284118390221
06/02/2019 03:52:15 step: 4701, epoch: 142, batch: 14, loss: 0.05627002567052841, acc: 98.4375, f1: 97.64957264957265, r: 0.6933224266072521
06/02/2019 03:52:15 step: 4706, epoch: 142, batch: 19, loss: 0.020448539406061172, acc: 98.4375, f1: 98.25396825396825, r: 0.7695163629218391
06/02/2019 03:52:16 step: 4711, epoch: 142, batch: 24, loss: 0.012308277189731598, acc: 100.0, f1: 100.0, r: 0.7984572073344477
06/02/2019 03:52:16 step: 4716, epoch: 142, batch: 29, loss: 0.018467113375663757, acc: 100.0, f1: 100.0, r: 0.8055712858397888
06/02/2019 03:52:16 *** evaluating ***
06/02/2019 03:52:16 step: 143, epoch: 142, acc: 56.41025641025641, f1: 25.19168733732016, r: 0.32248390291622897
06/02/2019 03:52:16 *** epoch: 144 ***
06/02/2019 03:52:16 *** training ***
06/02/2019 03:52:16 step: 4724, epoch: 143, batch: 4, loss: 0.000528387725353241, acc: 100.0, f1: 100.0, r: 0.6477017679359897
06/02/2019 03:52:16 step: 4729, epoch: 143, batch: 9, loss: 0.015186332166194916, acc: 100.0, f1: 100.0, r: 0.7493955151783029
06/02/2019 03:52:17 step: 4734, epoch: 143, batch: 14, loss: 0.018982145935297012, acc: 100.0, f1: 100.0, r: 0.8055753711511392
06/02/2019 03:52:17 step: 4739, epoch: 143, batch: 19, loss: 0.010515682399272919, acc: 100.0, f1: 100.0, r: 0.6526010853953348
06/02/2019 03:52:17 step: 4744, epoch: 143, batch: 24, loss: 0.008623436093330383, acc: 100.0, f1: 100.0, r: 0.7156019198992273
06/02/2019 03:52:17 step: 4749, epoch: 143, batch: 29, loss: 0.004577670246362686, acc: 100.0, f1: 100.0, r: 0.7408563634865769
06/02/2019 03:52:17 *** evaluating ***
06/02/2019 03:52:17 step: 144, epoch: 143, acc: 58.54700854700855, f1: 25.72469186463105, r: 0.3120756004212199
06/02/2019 03:52:17 *** epoch: 145 ***
06/02/2019 03:52:17 *** training ***
06/02/2019 03:52:17 step: 4757, epoch: 144, batch: 4, loss: 0.018397320061922073, acc: 100.0, f1: 100.0, r: 0.7078098765515641
06/02/2019 03:52:18 step: 4762, epoch: 144, batch: 9, loss: 0.013468373566865921, acc: 100.0, f1: 100.0, r: 0.795691602770904
06/02/2019 03:52:18 step: 4767, epoch: 144, batch: 14, loss: 0.008156731724739075, acc: 100.0, f1: 100.0, r: 0.8104207152082351
06/02/2019 03:52:18 step: 4772, epoch: 144, batch: 19, loss: 0.016238972544670105, acc: 100.0, f1: 100.0, r: 0.6974465482874548
06/02/2019 03:52:18 step: 4777, epoch: 144, batch: 24, loss: 0.016066312789916992, acc: 100.0, f1: 100.0, r: 0.7299919748408268
06/02/2019 03:52:18 step: 4782, epoch: 144, batch: 29, loss: 0.012799981981515884, acc: 100.0, f1: 100.0, r: 0.7495151927123866
06/02/2019 03:52:19 *** evaluating ***
06/02/2019 03:52:19 step: 145, epoch: 144, acc: 56.837606837606835, f1: 25.420369170484868, r: 0.32213974604808754
06/02/2019 03:52:19 *** epoch: 146 ***
06/02/2019 03:52:19 *** training ***
06/02/2019 03:52:19 step: 4790, epoch: 145, batch: 4, loss: 0.010665632784366608, acc: 100.0, f1: 100.0, r: 0.7009252216717288
06/02/2019 03:52:19 step: 4795, epoch: 145, batch: 9, loss: 0.014606412500143051, acc: 100.0, f1: 100.0, r: 0.7128274123074552
06/02/2019 03:52:19 step: 4800, epoch: 145, batch: 14, loss: 0.01958524063229561, acc: 100.0, f1: 100.0, r: 0.8191848981267971
06/02/2019 03:52:19 step: 4805, epoch: 145, batch: 19, loss: 0.02631506137549877, acc: 98.4375, f1: 98.64433811802233, r: 0.8317335975477982
06/02/2019 03:52:20 step: 4810, epoch: 145, batch: 24, loss: 0.05392646789550781, acc: 98.4375, f1: 97.24489795918367, r: 0.6927265451661266
06/02/2019 03:52:20 step: 4815, epoch: 145, batch: 29, loss: 0.04522591084241867, acc: 98.4375, f1: 97.47474747474747, r: 0.7772463610545881
06/02/2019 03:52:20 *** evaluating ***
06/02/2019 03:52:20 step: 146, epoch: 145, acc: 57.692307692307686, f1: 25.529176025182103, r: 0.3163981591644588
06/02/2019 03:52:20 *** epoch: 147 ***
06/02/2019 03:52:20 *** training ***
06/02/2019 03:52:20 step: 4823, epoch: 146, batch: 4, loss: 0.049618661403656006, acc: 98.4375, f1: 99.03961584633853, r: 0.653479400654292
06/02/2019 03:52:20 step: 4828, epoch: 146, batch: 9, loss: 0.027534376829862595, acc: 100.0, f1: 100.0, r: 0.762270788577821
06/02/2019 03:52:20 step: 4833, epoch: 146, batch: 14, loss: 0.009920340031385422, acc: 100.0, f1: 100.0, r: 0.7993711382027838
06/02/2019 03:52:21 step: 4838, epoch: 146, batch: 19, loss: 0.041131943464279175, acc: 98.4375, f1: 99.0, r: 0.7178087123401057
06/02/2019 03:52:21 step: 4843, epoch: 146, batch: 24, loss: 0.021310824900865555, acc: 100.0, f1: 100.0, r: 0.7008885586167004
06/02/2019 03:52:21 step: 4848, epoch: 146, batch: 29, loss: 0.004808906465768814, acc: 100.0, f1: 100.0, r: 0.6550243636394674
06/02/2019 03:52:21 *** evaluating ***
06/02/2019 03:52:21 step: 147, epoch: 146, acc: 57.692307692307686, f1: 25.62811305097131, r: 0.31297560893424375
06/02/2019 03:52:21 *** epoch: 148 ***
06/02/2019 03:52:21 *** training ***
06/02/2019 03:52:21 step: 4856, epoch: 147, batch: 4, loss: 0.022831179201602936, acc: 100.0, f1: 100.0, r: 0.7563506388869214
06/02/2019 03:52:22 step: 4861, epoch: 147, batch: 9, loss: 0.02758239582180977, acc: 100.0, f1: 100.0, r: 0.7756563667878951
06/02/2019 03:52:22 step: 4866, epoch: 147, batch: 14, loss: 0.0081922747194767, acc: 100.0, f1: 100.0, r: 0.7410285618804896
06/02/2019 03:52:22 step: 4871, epoch: 147, batch: 19, loss: 0.012635637074708939, acc: 100.0, f1: 100.0, r: 0.791864356263548
06/02/2019 03:52:22 step: 4876, epoch: 147, batch: 24, loss: 0.015591088682413101, acc: 100.0, f1: 100.0, r: 0.6993160886881595
06/02/2019 03:52:22 step: 4881, epoch: 147, batch: 29, loss: 0.020628273487091064, acc: 100.0, f1: 100.0, r: 0.682496677317484
06/02/2019 03:52:23 *** evaluating ***
06/02/2019 03:52:23 step: 148, epoch: 147, acc: 57.692307692307686, f1: 25.614403682095578, r: 0.31109524574889763
06/02/2019 03:52:23 *** epoch: 149 ***
06/02/2019 03:52:23 *** training ***
06/02/2019 03:52:23 step: 4889, epoch: 148, batch: 4, loss: 0.020519260317087173, acc: 100.0, f1: 100.0, r: 0.7536456630237889
06/02/2019 03:52:23 step: 4894, epoch: 148, batch: 9, loss: 0.011949550360441208, acc: 100.0, f1: 100.0, r: 0.7849476970936436
06/02/2019 03:52:23 step: 4899, epoch: 148, batch: 14, loss: 0.01929226517677307, acc: 98.4375, f1: 98.40975351179434, r: 0.6789999699503763
06/02/2019 03:52:23 step: 4904, epoch: 148, batch: 19, loss: 0.005114279687404633, acc: 100.0, f1: 100.0, r: 0.7058280324206962
06/02/2019 03:52:24 step: 4909, epoch: 148, batch: 24, loss: 0.012365955859422684, acc: 100.0, f1: 100.0, r: 0.7494393693176549
06/02/2019 03:52:24 step: 4914, epoch: 148, batch: 29, loss: 0.008417438715696335, acc: 100.0, f1: 100.0, r: 0.712020958953947
06/02/2019 03:52:24 *** evaluating ***
06/02/2019 03:52:24 step: 149, epoch: 148, acc: 57.26495726495726, f1: 25.09563894523327, r: 0.31509009749740713
06/02/2019 03:52:24 *** epoch: 150 ***
06/02/2019 03:52:24 *** training ***
06/02/2019 03:52:24 step: 4922, epoch: 149, batch: 4, loss: 0.00824500247836113, acc: 100.0, f1: 100.0, r: 0.5814848048053574
06/02/2019 03:52:24 step: 4927, epoch: 149, batch: 9, loss: 0.0038436949253082275, acc: 100.0, f1: 100.0, r: 0.6917800713428158
06/02/2019 03:52:25 step: 4932, epoch: 149, batch: 14, loss: 0.052202507853507996, acc: 96.875, f1: 85.17080745341615, r: 0.7368432694718448
06/02/2019 03:52:25 step: 4937, epoch: 149, batch: 19, loss: 0.018496673554182053, acc: 98.4375, f1: 93.16239316239316, r: 0.6400797397895269
06/02/2019 03:52:25 step: 4942, epoch: 149, batch: 24, loss: 0.004190854728221893, acc: 100.0, f1: 100.0, r: 0.6874320110133614
06/02/2019 03:52:25 step: 4947, epoch: 149, batch: 29, loss: 0.011483315378427505, acc: 100.0, f1: 100.0, r: 0.7491252727474152
06/02/2019 03:52:25 *** evaluating ***
06/02/2019 03:52:25 step: 150, epoch: 149, acc: 57.26495726495726, f1: 25.028780459202682, r: 0.31748783220783144
06/02/2019 03:52:25 *** epoch: 151 ***
06/02/2019 03:52:25 *** training ***
06/02/2019 03:52:26 step: 4955, epoch: 150, batch: 4, loss: 0.016539573669433594, acc: 100.0, f1: 100.0, r: 0.7910406202734299
06/02/2019 03:52:26 step: 4960, epoch: 150, batch: 9, loss: 0.02302556484937668, acc: 100.0, f1: 100.0, r: 0.810925244732418
06/02/2019 03:52:26 step: 4965, epoch: 150, batch: 14, loss: 0.02036208286881447, acc: 98.4375, f1: 98.13258636788049, r: 0.7147537102391021
06/02/2019 03:52:26 step: 4970, epoch: 150, batch: 19, loss: 0.02738449163734913, acc: 98.4375, f1: 97.87644787644788, r: 0.7825207338841151
06/02/2019 03:52:26 step: 4975, epoch: 150, batch: 24, loss: 0.02332763746380806, acc: 98.4375, f1: 96.9047619047619, r: 0.8213644486521475
06/02/2019 03:52:27 step: 4980, epoch: 150, batch: 29, loss: 0.032340291887521744, acc: 98.4375, f1: 99.23521913913127, r: 0.701026199722087
06/02/2019 03:52:27 *** evaluating ***
06/02/2019 03:52:27 step: 151, epoch: 150, acc: 57.26495726495726, f1: 25.13357342900198, r: 0.31733147929356514
06/02/2019 03:52:27 *** epoch: 152 ***
06/02/2019 03:52:27 *** training ***
06/02/2019 03:52:27 step: 4988, epoch: 151, batch: 4, loss: 0.013419896364212036, acc: 100.0, f1: 100.0, r: 0.7769507073780173
06/02/2019 03:52:27 step: 4993, epoch: 151, batch: 9, loss: 0.02453562617301941, acc: 100.0, f1: 100.0, r: 0.7913425901160587
06/02/2019 03:52:27 step: 4998, epoch: 151, batch: 14, loss: 0.005714483559131622, acc: 100.0, f1: 100.0, r: 0.702991423567924
06/02/2019 03:52:27 step: 5003, epoch: 151, batch: 19, loss: 0.0114307701587677, acc: 100.0, f1: 100.0, r: 0.6957649306520753
06/02/2019 03:52:28 step: 5008, epoch: 151, batch: 24, loss: 0.013137124478816986, acc: 100.0, f1: 100.0, r: 0.7513059153572437
06/02/2019 03:52:28 step: 5013, epoch: 151, batch: 29, loss: 0.015775401145219803, acc: 100.0, f1: 100.0, r: 0.7176316899032419
06/02/2019 03:52:28 *** evaluating ***
06/02/2019 03:52:28 step: 152, epoch: 151, acc: 56.41025641025641, f1: 24.800210393061473, r: 0.3194451073932639
06/02/2019 03:52:28 *** epoch: 153 ***
06/02/2019 03:52:28 *** training ***
06/02/2019 03:52:28 step: 5021, epoch: 152, batch: 4, loss: 0.022618604823946953, acc: 98.4375, f1: 83.33333333333333, r: 0.6352982442362975
06/02/2019 03:52:28 step: 5026, epoch: 152, batch: 9, loss: 0.026433337479829788, acc: 100.0, f1: 100.0, r: 0.79304013456454
06/02/2019 03:52:29 step: 5031, epoch: 152, batch: 14, loss: 0.020604155957698822, acc: 100.0, f1: 100.0, r: 0.7782294816983116
06/02/2019 03:52:29 step: 5036, epoch: 152, batch: 19, loss: 0.028234055265784264, acc: 100.0, f1: 100.0, r: 0.7430391177145907
06/02/2019 03:52:29 step: 5041, epoch: 152, batch: 24, loss: 0.0021895021200180054, acc: 100.0, f1: 100.0, r: 0.7577966840242234
06/02/2019 03:52:29 step: 5046, epoch: 152, batch: 29, loss: 0.016622457653284073, acc: 100.0, f1: 100.0, r: 0.6770719749002116
06/02/2019 03:52:29 *** evaluating ***
06/02/2019 03:52:29 step: 153, epoch: 152, acc: 58.119658119658126, f1: 25.701748342042073, r: 0.316137222678889
06/02/2019 03:52:29 *** epoch: 154 ***
06/02/2019 03:52:29 *** training ***
06/02/2019 03:52:30 step: 5054, epoch: 153, batch: 4, loss: 0.006081543862819672, acc: 100.0, f1: 100.0, r: 0.6658973824686236
06/02/2019 03:52:30 step: 5059, epoch: 153, batch: 9, loss: 0.0017875880002975464, acc: 100.0, f1: 100.0, r: 0.587857808922628
06/02/2019 03:52:30 step: 5064, epoch: 153, batch: 14, loss: 0.007508210837841034, acc: 100.0, f1: 100.0, r: 0.7726016674020852
06/02/2019 03:52:30 step: 5069, epoch: 153, batch: 19, loss: 0.0224052332341671, acc: 100.0, f1: 100.0, r: 0.7703913555396157
06/02/2019 03:52:30 step: 5074, epoch: 153, batch: 24, loss: 0.01435154676437378, acc: 100.0, f1: 100.0, r: 0.6311906106171025
06/02/2019 03:52:31 step: 5079, epoch: 153, batch: 29, loss: 0.017739448696374893, acc: 98.4375, f1: 94.44444444444444, r: 0.7830508905704495
06/02/2019 03:52:31 *** evaluating ***
06/02/2019 03:52:31 step: 154, epoch: 153, acc: 56.837606837606835, f1: 25.835959465272417, r: 0.3171787356331636
06/02/2019 03:52:31 *** epoch: 155 ***
06/02/2019 03:52:31 *** training ***
06/02/2019 03:52:31 step: 5087, epoch: 154, batch: 4, loss: 0.04914378002285957, acc: 98.4375, f1: 97.47899159663866, r: 0.7845361031615152
06/02/2019 03:52:31 step: 5092, epoch: 154, batch: 9, loss: 0.0069255344569683075, acc: 100.0, f1: 100.0, r: 0.81525313662273
06/02/2019 03:52:31 step: 5097, epoch: 154, batch: 14, loss: 0.007471993565559387, acc: 100.0, f1: 100.0, r: 0.7222018436095916
06/02/2019 03:52:32 step: 5102, epoch: 154, batch: 19, loss: 0.013185661286115646, acc: 100.0, f1: 100.0, r: 0.6934885485714335
06/02/2019 03:52:32 step: 5107, epoch: 154, batch: 24, loss: 0.013636063784360886, acc: 100.0, f1: 100.0, r: 0.700270191369459
06/02/2019 03:52:32 step: 5112, epoch: 154, batch: 29, loss: 0.0048927366733551025, acc: 100.0, f1: 100.0, r: 0.6672651727774496
06/02/2019 03:52:32 *** evaluating ***
06/02/2019 03:52:32 step: 155, epoch: 154, acc: 57.692307692307686, f1: 26.19285364781081, r: 0.3136881335205862
06/02/2019 03:52:32 *** epoch: 156 ***
06/02/2019 03:52:32 *** training ***
06/02/2019 03:52:32 step: 5120, epoch: 155, batch: 4, loss: 0.010350776836276054, acc: 100.0, f1: 100.0, r: 0.6895703071569022
06/02/2019 03:52:32 step: 5125, epoch: 155, batch: 9, loss: 0.04063534736633301, acc: 98.4375, f1: 98.81123180979709, r: 0.661289625493594
06/02/2019 03:52:33 step: 5130, epoch: 155, batch: 14, loss: 0.0018541514873504639, acc: 100.0, f1: 100.0, r: 0.6654639139586498
06/02/2019 03:52:33 step: 5135, epoch: 155, batch: 19, loss: 0.025238599628210068, acc: 100.0, f1: 100.0, r: 0.6517185832847094
06/02/2019 03:52:33 step: 5140, epoch: 155, batch: 24, loss: 0.0182097926735878, acc: 100.0, f1: 100.0, r: 0.7259195825756076
06/02/2019 03:52:33 step: 5145, epoch: 155, batch: 29, loss: 0.01779850944876671, acc: 100.0, f1: 100.0, r: 0.7236677110449479
06/02/2019 03:52:33 *** evaluating ***
06/02/2019 03:52:33 step: 156, epoch: 155, acc: 58.54700854700855, f1: 25.853936695045444, r: 0.3094071280299986
06/02/2019 03:52:33 *** epoch: 157 ***
06/02/2019 03:52:33 *** training ***
06/02/2019 03:52:34 step: 5153, epoch: 156, batch: 4, loss: 0.01635858416557312, acc: 100.0, f1: 100.0, r: 0.6988241738484301
06/02/2019 03:52:34 step: 5158, epoch: 156, batch: 9, loss: 0.042174406349658966, acc: 98.4375, f1: 84.41558441558442, r: 0.716810009322991
06/02/2019 03:52:34 step: 5163, epoch: 156, batch: 14, loss: 0.03906269371509552, acc: 98.4375, f1: 98.57549857549857, r: 0.789870509776529
06/02/2019 03:52:34 step: 5168, epoch: 156, batch: 19, loss: 0.03221820294857025, acc: 100.0, f1: 100.0, r: 0.7133575842040916
06/02/2019 03:52:34 step: 5173, epoch: 156, batch: 24, loss: 0.029553035274147987, acc: 98.4375, f1: 98.81521986785145, r: 0.7260916850552998
06/02/2019 03:52:35 step: 5178, epoch: 156, batch: 29, loss: 0.007681664079427719, acc: 100.0, f1: 100.0, r: 0.7109373935883522
06/02/2019 03:52:35 *** evaluating ***
06/02/2019 03:52:35 step: 157, epoch: 156, acc: 57.692307692307686, f1: 25.563678303528135, r: 0.31595259384785174
06/02/2019 03:52:35 *** epoch: 158 ***
06/02/2019 03:52:35 *** training ***
06/02/2019 03:52:35 step: 5186, epoch: 157, batch: 4, loss: 0.008532002568244934, acc: 100.0, f1: 100.0, r: 0.6746349236025712
06/02/2019 03:52:35 step: 5191, epoch: 157, batch: 9, loss: 0.005821622908115387, acc: 100.0, f1: 100.0, r: 0.662035091571815
06/02/2019 03:52:35 step: 5196, epoch: 157, batch: 14, loss: 0.006471700966358185, acc: 100.0, f1: 100.0, r: 0.7393178520565348
06/02/2019 03:52:35 step: 5201, epoch: 157, batch: 19, loss: 0.010163307189941406, acc: 100.0, f1: 100.0, r: 0.7338690055772645
06/02/2019 03:52:36 step: 5206, epoch: 157, batch: 24, loss: 0.025385867804288864, acc: 98.4375, f1: 97.0, r: 0.7921737984518631
06/02/2019 03:52:36 step: 5211, epoch: 157, batch: 29, loss: 0.01656506583094597, acc: 100.0, f1: 100.0, r: 0.8079500155188146
06/02/2019 03:52:36 *** evaluating ***
06/02/2019 03:52:36 step: 158, epoch: 157, acc: 57.26495726495726, f1: 25.47646157018909, r: 0.3168981489158768
06/02/2019 03:52:36 *** epoch: 159 ***
06/02/2019 03:52:36 *** training ***
06/02/2019 03:52:36 step: 5219, epoch: 158, batch: 4, loss: 0.03609143942594528, acc: 96.875, f1: 98.43014128728414, r: 0.6879888756115617
06/02/2019 03:52:36 step: 5224, epoch: 158, batch: 9, loss: 0.016533248126506805, acc: 100.0, f1: 100.0, r: 0.7111084775539456
06/02/2019 03:52:37 step: 5229, epoch: 158, batch: 14, loss: 0.0385703407227993, acc: 98.4375, f1: 94.44444444444444, r: 0.7598212974986379
06/02/2019 03:52:37 step: 5234, epoch: 158, batch: 19, loss: 0.014473088085651398, acc: 100.0, f1: 100.0, r: 0.7029995938279422
06/02/2019 03:52:37 step: 5239, epoch: 158, batch: 24, loss: 0.004996493458747864, acc: 100.0, f1: 100.0, r: 0.6973576724015438
06/02/2019 03:52:37 step: 5244, epoch: 158, batch: 29, loss: 0.017502497881650925, acc: 100.0, f1: 100.0, r: 0.6482674160991184
06/02/2019 03:52:37 *** evaluating ***
06/02/2019 03:52:37 step: 159, epoch: 158, acc: 58.119658119658126, f1: 26.27515417593899, r: 0.32173044166170944
06/02/2019 03:52:37 *** epoch: 160 ***
06/02/2019 03:52:37 *** training ***
06/02/2019 03:52:38 step: 5252, epoch: 159, batch: 4, loss: 0.019930288195610046, acc: 100.0, f1: 100.0, r: 0.7796644027080304
06/02/2019 03:52:38 step: 5257, epoch: 159, batch: 9, loss: 0.03189385309815407, acc: 100.0, f1: 100.0, r: 0.7756208695471724
06/02/2019 03:52:38 step: 5262, epoch: 159, batch: 14, loss: 0.009402520954608917, acc: 100.0, f1: 100.0, r: 0.7922309106984647
06/02/2019 03:52:38 step: 5267, epoch: 159, batch: 19, loss: 0.02110326662659645, acc: 100.0, f1: 100.0, r: 0.6849702176803003
06/02/2019 03:52:38 step: 5272, epoch: 159, batch: 24, loss: 0.014259159564971924, acc: 100.0, f1: 100.0, r: 0.7890374608692596
06/02/2019 03:52:38 step: 5277, epoch: 159, batch: 29, loss: 0.005414515733718872, acc: 100.0, f1: 100.0, r: 0.6991474604443754
06/02/2019 03:52:39 *** evaluating ***
06/02/2019 03:52:39 step: 160, epoch: 159, acc: 57.692307692307686, f1: 25.563678303528135, r: 0.3205144519858192
06/02/2019 03:52:39 *** epoch: 161 ***
06/02/2019 03:52:39 *** training ***
06/02/2019 03:52:39 step: 5285, epoch: 160, batch: 4, loss: 0.05750937759876251, acc: 96.875, f1: 93.38509316770187, r: 0.7711779767762371
06/02/2019 03:52:39 step: 5290, epoch: 160, batch: 9, loss: 0.03820054233074188, acc: 98.4375, f1: 94.6969696969697, r: 0.7237868718963572
06/02/2019 03:52:39 step: 5295, epoch: 160, batch: 14, loss: 0.02293272688984871, acc: 98.4375, f1: 98.20574162679426, r: 0.8466791038113777
06/02/2019 03:52:39 step: 5300, epoch: 160, batch: 19, loss: 0.009607415646314621, acc: 100.0, f1: 100.0, r: 0.7648099418652806
06/02/2019 03:52:40 step: 5305, epoch: 160, batch: 24, loss: 0.013305991888046265, acc: 100.0, f1: 100.0, r: 0.8111940397348996
06/02/2019 03:54:00 {'input_path': 'data/word2vec_temp', 'output_path': 'save/cnn_8', 'gpu': True, 'seed': 20000125, 'display_per_batch': 5, 'optimizer': 'adagrad', 'lr': 0.01, 'lr_decay': 0, 'weight_decay': 0.0001, 'momentum': 0.985, 'num_epochs': 300, 'batch_size': 64, 'num_labels': 8, 'embedding_size': 300, 'type': 'cnn', 'cnn': {'max_length': 512, 'conv_1': {'size': 512, 'kernel_size': 3}, 'max_pool_1': {'kernel_size': 2, 'stride': 2}, 'fc': {'hidden_size': 512, 'dropout': 0.5}, 'loss': 'cross_entropy'}}
06/02/2019 03:54:00 Loading Train Data
06/02/2019 03:54:00 load data from data/word2vec_temp/train_text.npy, data/word2vec_temp/train_label.npy, training: True
06/02/2019 03:54:24 loaded. total len: 2342
06/02/2019 03:54:24 Train: length: 2108, total batch: 33, batch size: 64
06/02/2019 03:54:24 Dev: length: 234, total batch: 4, batch size: 64
06/02/2019 03:54:24 Loading model cnn
06/02/2019 03:54:36 *** epoch: 1 ***
06/02/2019 03:54:36 *** training ***
06/02/2019 03:54:36 step: 5, epoch: 0, batch: 4, loss: 2.546051502227783, acc: 23.4375, f1: 6.396396396396396, r: 0.06032080217361301
06/02/2019 03:54:37 step: 10, epoch: 0, batch: 9, loss: 1.9668920040130615, acc: 29.6875, f1: 10.261904761904763, r: 0.021200190255740693
06/02/2019 03:54:37 step: 15, epoch: 0, batch: 14, loss: 2.0098984241485596, acc: 18.75, f1: 5.90909090909091, r: -0.029327880278091335
06/02/2019 03:54:38 step: 20, epoch: 0, batch: 19, loss: 1.7561140060424805, acc: 31.25, f1: 10.004001600640255, r: 0.048354176565206186
06/02/2019 03:54:38 step: 25, epoch: 0, batch: 24, loss: 1.7327138185501099, acc: 37.5, f1: 11.21848739495798, r: 0.0841918369066631
06/02/2019 03:54:38 step: 30, epoch: 0, batch: 29, loss: 1.7161345481872559, acc: 37.5, f1: 12.217171717171718, r: 0.19875398206804085
06/02/2019 03:54:38 *** evaluating ***
06/02/2019 03:54:38 step: 1, epoch: 0, acc: 45.2991452991453, f1: 8.552147239263803, r: 0.23607931774859633
06/02/2019 03:54:38 *** epoch: 2 ***
06/02/2019 03:54:38 *** training ***
06/02/2019 03:54:39 step: 38, epoch: 1, batch: 4, loss: 1.7374154329299927, acc: 39.0625, f1: 16.440170940170944, r: 0.18600934689221182
06/02/2019 03:54:39 step: 43, epoch: 1, batch: 9, loss: 1.6258082389831543, acc: 45.3125, f1: 18.78434065934066, r: 0.18958317794107227
06/02/2019 03:54:40 step: 48, epoch: 1, batch: 14, loss: 1.5090034008026123, acc: 34.375, f1: 12.584267941410799, r: 0.27941329308715657
06/02/2019 03:54:40 step: 53, epoch: 1, batch: 19, loss: 1.3900110721588135, acc: 54.6875, f1: 21.555153707052447, r: 0.27730768604333284
06/02/2019 03:54:40 step: 58, epoch: 1, batch: 24, loss: 1.4811310768127441, acc: 50.0, f1: 19.70390538585893, r: 0.24618377263486785
06/02/2019 03:54:41 step: 63, epoch: 1, batch: 29, loss: 1.652823805809021, acc: 42.1875, f1: 17.763157894736842, r: 0.30208240697082184
06/02/2019 03:54:41 *** evaluating ***
06/02/2019 03:54:41 step: 2, epoch: 1, acc: 49.572649572649574, f1: 13.300432665212858, r: 0.26311994512496906
06/02/2019 03:54:41 *** epoch: 3 ***
06/02/2019 03:54:41 *** training ***
06/02/2019 03:54:41 step: 71, epoch: 2, batch: 4, loss: 1.4895737171173096, acc: 42.1875, f1: 16.381752305665348, r: 0.35605571841994826
06/02/2019 03:54:42 step: 76, epoch: 2, batch: 9, loss: 1.2943956851959229, acc: 57.8125, f1: 22.014430014430015, r: 0.30026407231021945
06/02/2019 03:54:42 step: 81, epoch: 2, batch: 14, loss: 1.4900214672088623, acc: 46.875, f1: 17.3115193264447, r: 0.39635352893963055
06/02/2019 03:54:43 step: 86, epoch: 2, batch: 19, loss: 1.1485381126403809, acc: 57.8125, f1: 19.736842105263158, r: 0.346959629606737
06/02/2019 03:54:43 step: 91, epoch: 2, batch: 24, loss: 1.3435852527618408, acc: 54.6875, f1: 20.992822966507173, r: 0.4057545333259973
06/02/2019 03:54:43 step: 96, epoch: 2, batch: 29, loss: 1.3958523273468018, acc: 56.25, f1: 23.683574879227052, r: 0.3544463195969293
06/02/2019 03:54:43 *** evaluating ***
06/02/2019 03:54:44 step: 3, epoch: 2, acc: 52.13675213675214, f1: 17.058734830268644, r: 0.2891256275108669
06/02/2019 03:54:44 *** epoch: 4 ***
06/02/2019 03:54:44 *** training ***
06/02/2019 03:54:44 step: 104, epoch: 3, batch: 4, loss: 1.0833157300949097, acc: 60.9375, f1: 28.663605243383156, r: 0.372698624203018
06/02/2019 03:54:44 step: 109, epoch: 3, batch: 9, loss: 1.1034046411514282, acc: 59.375, f1: 31.97278911564626, r: 0.46609626230794354
06/02/2019 03:54:45 step: 114, epoch: 3, batch: 14, loss: 1.2113324403762817, acc: 64.0625, f1: 31.819331983805665, r: 0.3943720082097078
06/02/2019 03:54:45 step: 119, epoch: 3, batch: 19, loss: 0.9679597616195679, acc: 70.3125, f1: 39.778927940940456, r: 0.4872619431407208
06/02/2019 03:54:45 step: 124, epoch: 3, batch: 24, loss: 1.0581907033920288, acc: 60.9375, f1: 31.283596923342838, r: 0.5291825063727664
06/02/2019 03:54:46 step: 129, epoch: 3, batch: 29, loss: 1.2297008037567139, acc: 50.0, f1: 23.799649750353975, r: 0.4888544002779506
06/02/2019 03:54:46 *** evaluating ***
06/02/2019 03:54:46 step: 4, epoch: 3, acc: 51.70940170940172, f1: 18.36147920706744, r: 0.2892790269612506
06/02/2019 03:54:46 *** epoch: 5 ***
06/02/2019 03:54:46 *** training ***
06/02/2019 03:54:46 step: 137, epoch: 4, batch: 4, loss: 0.7529356479644775, acc: 73.4375, f1: 54.209061128303404, r: 0.5492375081356109
06/02/2019 03:54:47 step: 142, epoch: 4, batch: 9, loss: 0.999416708946228, acc: 64.0625, f1: 41.607142857142854, r: 0.4556412895165296
06/02/2019 03:54:47 step: 147, epoch: 4, batch: 14, loss: 0.6153056025505066, acc: 81.25, f1: 64.46982083564102, r: 0.5247179563135034
06/02/2019 03:54:47 step: 152, epoch: 4, batch: 19, loss: 0.9630779027938843, acc: 67.1875, f1: 39.79263347129896, r: 0.3723766212214445
06/02/2019 03:54:48 step: 157, epoch: 4, batch: 24, loss: 0.7813166379928589, acc: 73.4375, f1: 56.28167944660019, r: 0.5056633049613322
06/02/2019 03:54:48 step: 162, epoch: 4, batch: 29, loss: 0.8604642748832703, acc: 71.875, f1: 58.1992881400209, r: 0.5785172811054315
06/02/2019 03:54:48 *** evaluating ***
06/02/2019 03:54:48 step: 5, epoch: 4, acc: 52.56410256410257, f1: 20.441647491527366, r: 0.2852433679577273
06/02/2019 03:54:48 *** epoch: 6 ***
06/02/2019 03:54:48 *** training ***
06/02/2019 03:54:48 step: 170, epoch: 5, batch: 4, loss: 0.5835673213005066, acc: 84.375, f1: 71.99074074074075, r: 0.63400943267328
06/02/2019 03:54:49 step: 175, epoch: 5, batch: 9, loss: 0.47594547271728516, acc: 89.0625, f1: 77.44943910578586, r: 0.64070223596426
06/02/2019 03:54:49 step: 180, epoch: 5, batch: 14, loss: 0.5962519645690918, acc: 75.0, f1: 62.518122002899524, r: 0.6150851753475012
06/02/2019 03:54:49 step: 185, epoch: 5, batch: 19, loss: 0.4744800627231598, acc: 93.75, f1: 81.3961038961039, r: 0.6577239130115674
06/02/2019 03:54:50 step: 190, epoch: 5, batch: 24, loss: 0.4825449287891388, acc: 90.625, f1: 74.0919540229885, r: 0.6285379140515092
06/02/2019 03:54:50 step: 195, epoch: 5, batch: 29, loss: 0.564714252948761, acc: 81.25, f1: 69.92733146770414, r: 0.5141375044315674
06/02/2019 03:54:50 *** evaluating ***
06/02/2019 03:54:50 step: 6, epoch: 5, acc: 55.12820512820513, f1: 22.01479854629121, r: 0.29176771704261495
06/02/2019 03:54:50 *** epoch: 7 ***
06/02/2019 03:54:50 *** training ***
06/02/2019 03:54:51 step: 203, epoch: 6, batch: 4, loss: 0.2546808421611786, acc: 95.3125, f1: 97.18996689584925, r: 0.5659906258134088
06/02/2019 03:54:51 step: 208, epoch: 6, batch: 9, loss: 0.2675221264362335, acc: 93.75, f1: 82.40229885057471, r: 0.7128869331127466
06/02/2019 03:54:51 step: 213, epoch: 6, batch: 14, loss: 0.19643664360046387, acc: 96.875, f1: 96.2266608771168, r: 0.6670300396798557
06/02/2019 03:54:52 step: 218, epoch: 6, batch: 19, loss: 0.3551018238067627, acc: 89.0625, f1: 92.9584965299251, r: 0.7243419839147665
06/02/2019 03:54:52 step: 223, epoch: 6, batch: 24, loss: 0.2847038209438324, acc: 90.625, f1: 84.52107279693487, r: 0.6723280642922423
06/02/2019 03:54:52 step: 228, epoch: 6, batch: 29, loss: 0.3218519985675812, acc: 92.1875, f1: 90.67261904761904, r: 0.723594737916061
06/02/2019 03:54:52 *** evaluating ***
06/02/2019 03:54:53 step: 7, epoch: 6, acc: 55.12820512820513, f1: 23.1065848694978, r: 0.27487884789651534
06/02/2019 03:54:53 *** epoch: 8 ***
06/02/2019 03:54:53 *** training ***
06/02/2019 03:54:53 step: 236, epoch: 7, batch: 4, loss: 0.1501120924949646, acc: 98.4375, f1: 94.87179487179486, r: 0.770053641360789
06/02/2019 03:54:53 step: 241, epoch: 7, batch: 9, loss: 0.11425521224737167, acc: 100.0, f1: 100.0, r: 0.8125841461201729
06/02/2019 03:54:53 step: 246, epoch: 7, batch: 14, loss: 0.1482020616531372, acc: 96.875, f1: 94.65367965367966, r: 0.6914441801483431
06/02/2019 03:54:54 step: 251, epoch: 7, batch: 19, loss: 0.2173559069633484, acc: 98.4375, f1: 99.17287014061208, r: 0.6426502836066299
06/02/2019 03:54:54 step: 256, epoch: 7, batch: 24, loss: 0.11364343017339706, acc: 98.4375, f1: 96.84210526315789, r: 0.8166600827145901
06/02/2019 03:54:54 step: 261, epoch: 7, batch: 29, loss: 0.16627314686775208, acc: 98.4375, f1: 93.65079365079366, r: 0.635183225524731
06/02/2019 03:54:54 *** evaluating ***
06/02/2019 03:54:55 step: 8, epoch: 7, acc: 52.991452991452995, f1: 24.414086280237836, r: 0.2729021993294968
06/02/2019 03:54:55 *** epoch: 9 ***
06/02/2019 03:54:55 *** training ***
06/02/2019 03:54:55 step: 269, epoch: 8, batch: 4, loss: 0.14600932598114014, acc: 95.3125, f1: 93.61932288761557, r: 0.7608033656310363
06/02/2019 03:54:55 step: 274, epoch: 8, batch: 9, loss: 0.12433159351348877, acc: 98.4375, f1: 97.83549783549783, r: 0.7533548480312601
06/02/2019 03:54:55 step: 279, epoch: 8, batch: 14, loss: 0.08912929147481918, acc: 98.4375, f1: 98.8994708994709, r: 0.6410146699425107
06/02/2019 03:54:56 step: 284, epoch: 8, batch: 19, loss: 0.10411001741886139, acc: 100.0, f1: 100.0, r: 0.7379623347245889
06/02/2019 03:54:56 step: 289, epoch: 8, batch: 24, loss: 0.08846957236528397, acc: 100.0, f1: 100.0, r: 0.7744253063240087
06/02/2019 03:54:56 step: 294, epoch: 8, batch: 29, loss: 0.08215108513832092, acc: 100.0, f1: 100.0, r: 0.6908536092410728
06/02/2019 03:54:56 *** evaluating ***
06/02/2019 03:54:56 step: 9, epoch: 8, acc: 53.41880341880342, f1: 25.104221175649744, r: 0.2625041203579237
06/02/2019 03:54:56 *** epoch: 10 ***
06/02/2019 03:54:56 *** training ***
06/02/2019 03:54:57 step: 302, epoch: 9, batch: 4, loss: 0.06655348092317581, acc: 100.0, f1: 100.0, r: 0.8223816570557064
06/02/2019 03:54:57 step: 307, epoch: 9, batch: 9, loss: 0.10007725656032562, acc: 96.875, f1: 96.81899641577061, r: 0.7632019297349273
06/02/2019 03:54:57 step: 312, epoch: 9, batch: 14, loss: 0.030561499297618866, acc: 100.0, f1: 100.0, r: 0.6108196825027826
06/02/2019 03:54:58 step: 317, epoch: 9, batch: 19, loss: 0.058181166648864746, acc: 98.4375, f1: 98.26839826839826, r: 0.8001564792210714
06/02/2019 03:54:58 step: 322, epoch: 9, batch: 24, loss: 0.10747529566287994, acc: 96.875, f1: 98.16950934963357, r: 0.7097878274402033
06/02/2019 03:54:59 step: 327, epoch: 9, batch: 29, loss: 0.03423447161912918, acc: 100.0, f1: 100.0, r: 0.7073108320694121
06/02/2019 03:54:59 *** evaluating ***
06/02/2019 03:54:59 step: 10, epoch: 9, acc: 49.572649572649574, f1: 22.032254385648258, r: 0.2639555313112481
06/02/2019 03:54:59 *** epoch: 11 ***
06/02/2019 03:54:59 *** training ***
06/02/2019 03:54:59 step: 335, epoch: 10, batch: 4, loss: 0.07724110782146454, acc: 100.0, f1: 100.0, r: 0.6909203876756653
06/02/2019 03:54:59 step: 340, epoch: 10, batch: 9, loss: 0.05536270886659622, acc: 98.4375, f1: 99.22727711774365, r: 0.6530418490395127
06/02/2019 03:55:00 step: 345, epoch: 10, batch: 14, loss: 0.02352878451347351, acc: 100.0, f1: 100.0, r: 0.8563594389872827
06/02/2019 03:55:00 step: 350, epoch: 10, batch: 19, loss: 0.04324907436966896, acc: 98.4375, f1: 98.30623306233062, r: 0.7740875192418952
06/02/2019 03:55:00 step: 355, epoch: 10, batch: 24, loss: 0.019718032330274582, acc: 100.0, f1: 100.0, r: 0.8426412776481619
06/02/2019 03:55:01 step: 360, epoch: 10, batch: 29, loss: 0.029292866587638855, acc: 100.0, f1: 100.0, r: 0.6649379468488941
06/02/2019 03:55:01 *** evaluating ***
06/02/2019 03:55:01 step: 11, epoch: 10, acc: 53.41880341880342, f1: 23.950825966436916, r: 0.2610967374511012
06/02/2019 03:55:01 *** epoch: 12 ***
06/02/2019 03:55:01 *** training ***
06/02/2019 03:55:01 step: 368, epoch: 11, batch: 4, loss: 0.027890712022781372, acc: 100.0, f1: 100.0, r: 0.8164247390621114
06/02/2019 03:55:01 step: 373, epoch: 11, batch: 9, loss: 0.04646061360836029, acc: 100.0, f1: 100.0, r: 0.7578820940291136
06/02/2019 03:55:02 step: 378, epoch: 11, batch: 14, loss: 0.016079910099506378, acc: 100.0, f1: 100.0, r: 0.6558707486340569
06/02/2019 03:55:02 step: 383, epoch: 11, batch: 19, loss: 0.015400748699903488, acc: 100.0, f1: 100.0, r: 0.7250878169486465
06/02/2019 03:55:02 step: 388, epoch: 11, batch: 24, loss: 0.03152009844779968, acc: 100.0, f1: 100.0, r: 0.8275769502218331
06/02/2019 03:55:03 step: 393, epoch: 11, batch: 29, loss: 0.026344627141952515, acc: 100.0, f1: 100.0, r: 0.6642446081593162
06/02/2019 03:55:03 *** evaluating ***
06/02/2019 03:55:03 step: 12, epoch: 11, acc: 50.85470085470085, f1: 21.36386515465253, r: 0.24815987834456477
06/02/2019 03:55:03 *** epoch: 13 ***
06/02/2019 03:55:03 *** training ***
06/02/2019 03:55:03 step: 401, epoch: 12, batch: 4, loss: 0.026474393904209137, acc: 100.0, f1: 100.0, r: 0.6459193860957856
06/02/2019 03:55:04 step: 406, epoch: 12, batch: 9, loss: 0.025323089212179184, acc: 100.0, f1: 100.0, r: 0.6992515514091577
06/02/2019 03:55:04 step: 411, epoch: 12, batch: 14, loss: 0.030009474605321884, acc: 100.0, f1: 100.0, r: 0.7064517648317363
06/02/2019 03:55:04 step: 416, epoch: 12, batch: 19, loss: 0.019720159471035004, acc: 100.0, f1: 100.0, r: 0.7255259006966397
06/02/2019 03:55:05 step: 421, epoch: 12, batch: 24, loss: 0.015356853604316711, acc: 100.0, f1: 100.0, r: 0.7591417539799195
06/02/2019 03:55:05 step: 426, epoch: 12, batch: 29, loss: 0.014241810888051987, acc: 100.0, f1: 100.0, r: 0.6944889278108921
06/02/2019 03:55:05 *** evaluating ***
06/02/2019 03:55:05 step: 13, epoch: 12, acc: 53.84615384615385, f1: 23.507093099035423, r: 0.25485179178195144
06/02/2019 03:55:05 *** epoch: 14 ***
06/02/2019 03:55:05 *** training ***
06/02/2019 03:55:06 step: 434, epoch: 13, batch: 4, loss: 0.018803197890520096, acc: 100.0, f1: 100.0, r: 0.8016579341343948
06/02/2019 03:55:06 step: 439, epoch: 13, batch: 9, loss: 0.03560023754835129, acc: 100.0, f1: 100.0, r: 0.8262278788287436
06/02/2019 03:55:06 step: 444, epoch: 13, batch: 14, loss: 0.014756083488464355, acc: 100.0, f1: 100.0, r: 0.676048593177008
06/02/2019 03:55:07 step: 449, epoch: 13, batch: 19, loss: 0.01705864444375038, acc: 100.0, f1: 100.0, r: 0.6947165620830925
06/02/2019 03:55:07 step: 454, epoch: 13, batch: 24, loss: 0.01610541343688965, acc: 100.0, f1: 100.0, r: 0.8306315652102475
06/02/2019 03:55:07 step: 459, epoch: 13, batch: 29, loss: 0.017098531126976013, acc: 100.0, f1: 100.0, r: 0.7382905810068623
06/02/2019 03:55:07 *** evaluating ***
06/02/2019 03:55:08 step: 14, epoch: 13, acc: 52.56410256410257, f1: 21.42948625006041, r: 0.24875352094763128
06/02/2019 03:55:08 *** epoch: 15 ***
06/02/2019 03:55:08 *** training ***
06/02/2019 03:55:08 step: 467, epoch: 14, batch: 4, loss: 0.0543723963201046, acc: 96.875, f1: 95.26785714285714, r: 0.7771296643656785
06/02/2019 03:55:08 step: 472, epoch: 14, batch: 9, loss: 0.01164601743221283, acc: 100.0, f1: 100.0, r: 0.6402696196056876
06/02/2019 03:55:09 step: 477, epoch: 14, batch: 14, loss: 0.01028035581111908, acc: 100.0, f1: 100.0, r: 0.8543483121719244
06/02/2019 03:55:09 step: 482, epoch: 14, batch: 19, loss: 0.008055679500102997, acc: 100.0, f1: 100.0, r: 0.7344647577692677
06/02/2019 03:55:09 step: 487, epoch: 14, batch: 24, loss: 0.013871811330318451, acc: 100.0, f1: 100.0, r: 0.7541684026774417
06/02/2019 03:55:10 step: 492, epoch: 14, batch: 29, loss: 0.00576387345790863, acc: 100.0, f1: 100.0, r: 0.6936995118473681
06/02/2019 03:55:10 *** evaluating ***
06/02/2019 03:55:10 step: 15, epoch: 14, acc: 52.56410256410257, f1: 20.881975228523395, r: 0.24640732868037746
06/02/2019 03:55:10 *** epoch: 16 ***
06/02/2019 03:55:10 *** training ***
06/02/2019 03:55:11 step: 500, epoch: 15, batch: 4, loss: 0.0074897706508636475, acc: 100.0, f1: 100.0, r: 0.6987793709822036
06/02/2019 03:55:11 step: 505, epoch: 15, batch: 9, loss: 0.010208018124103546, acc: 100.0, f1: 100.0, r: 0.8241619413750295
06/02/2019 03:55:11 step: 510, epoch: 15, batch: 14, loss: 0.019768796861171722, acc: 100.0, f1: 100.0, r: 0.8033961159139914
06/02/2019 03:55:12 step: 515, epoch: 15, batch: 19, loss: 0.006800003349781036, acc: 100.0, f1: 100.0, r: 0.7573365993550504
06/02/2019 03:55:12 step: 520, epoch: 15, batch: 24, loss: 0.011344671249389648, acc: 100.0, f1: 100.0, r: 0.8179617338433374
06/02/2019 03:55:12 step: 525, epoch: 15, batch: 29, loss: 0.012244358658790588, acc: 100.0, f1: 100.0, r: 0.7592212241615779
06/02/2019 03:55:12 *** evaluating ***
06/02/2019 03:55:13 step: 16, epoch: 15, acc: 53.41880341880342, f1: 22.12824873539159, r: 0.24730301498310295
06/02/2019 03:55:13 *** epoch: 17 ***
06/02/2019 03:55:13 *** training ***
06/02/2019 03:55:13 step: 533, epoch: 16, batch: 4, loss: 0.007813841104507446, acc: 100.0, f1: 100.0, r: 0.8465337813760363
06/02/2019 03:55:13 step: 538, epoch: 16, batch: 9, loss: 0.009247824549674988, acc: 100.0, f1: 100.0, r: 0.7845330073561468
06/02/2019 03:55:14 step: 543, epoch: 16, batch: 14, loss: 0.008218720555305481, acc: 100.0, f1: 100.0, r: 0.8149910934088727
06/02/2019 03:55:14 step: 548, epoch: 16, batch: 19, loss: 0.006682343780994415, acc: 100.0, f1: 100.0, r: 0.7133949266014888
06/02/2019 03:55:14 step: 553, epoch: 16, batch: 24, loss: 0.007977016270160675, acc: 100.0, f1: 100.0, r: 0.7981427507644159
06/02/2019 03:55:15 step: 558, epoch: 16, batch: 29, loss: 0.009067967534065247, acc: 100.0, f1: 100.0, r: 0.7321216939875124
06/02/2019 03:55:15 *** evaluating ***
06/02/2019 03:55:15 step: 17, epoch: 16, acc: 52.56410256410257, f1: 21.645807188944875, r: 0.2403418578951472
06/02/2019 03:55:15 *** epoch: 18 ***
06/02/2019 03:55:15 *** training ***
06/02/2019 03:55:15 step: 566, epoch: 17, batch: 4, loss: 0.004180051386356354, acc: 100.0, f1: 100.0, r: 0.7317588141481725
06/02/2019 03:55:16 step: 571, epoch: 17, batch: 9, loss: 0.015035375952720642, acc: 100.0, f1: 100.0, r: 0.7948935989231424
06/02/2019 03:55:16 step: 576, epoch: 17, batch: 14, loss: 0.010781094431877136, acc: 100.0, f1: 100.0, r: 0.7672845090917181
06/02/2019 03:55:16 step: 581, epoch: 17, batch: 19, loss: 0.013769999146461487, acc: 100.0, f1: 100.0, r: 0.7383010477036357
06/02/2019 03:55:17 step: 586, epoch: 17, batch: 24, loss: 0.013387657701969147, acc: 100.0, f1: 100.0, r: 0.6780749374043014
06/02/2019 03:55:17 step: 591, epoch: 17, batch: 29, loss: 0.015539340674877167, acc: 100.0, f1: 100.0, r: 0.7200363591685163
06/02/2019 03:55:17 *** evaluating ***
06/02/2019 03:55:17 step: 18, epoch: 17, acc: 52.991452991452995, f1: 22.501938687753, r: 0.23780681061202313
06/02/2019 03:55:17 *** epoch: 19 ***
06/02/2019 03:55:17 *** training ***
06/02/2019 03:55:18 step: 599, epoch: 18, batch: 4, loss: 0.01152046024799347, acc: 100.0, f1: 100.0, r: 0.8404875146544263
06/02/2019 03:55:18 step: 604, epoch: 18, batch: 9, loss: 0.009138472378253937, acc: 100.0, f1: 100.0, r: 0.8010473278870908
06/02/2019 03:55:18 step: 609, epoch: 18, batch: 14, loss: 0.016704175621271133, acc: 100.0, f1: 100.0, r: 0.60422669934786
06/02/2019 03:55:19 step: 614, epoch: 18, batch: 19, loss: 0.008880682289600372, acc: 100.0, f1: 100.0, r: 0.6419236466025015
06/02/2019 03:55:19 step: 619, epoch: 18, batch: 24, loss: 0.007598675787448883, acc: 100.0, f1: 100.0, r: 0.8397023215728318
06/02/2019 03:55:19 step: 624, epoch: 18, batch: 29, loss: 0.008025340735912323, acc: 100.0, f1: 100.0, r: 0.8183178184120612
06/02/2019 03:55:19 *** evaluating ***
06/02/2019 03:55:20 step: 19, epoch: 18, acc: 52.991452991452995, f1: 22.61291063668179, r: 0.2411772110190395
06/02/2019 03:55:20 *** epoch: 20 ***
06/02/2019 03:55:20 *** training ***
06/02/2019 03:55:20 step: 632, epoch: 19, batch: 4, loss: 0.010244060307741165, acc: 100.0, f1: 100.0, r: 0.7547649708023475
06/02/2019 03:55:20 step: 637, epoch: 19, batch: 9, loss: 0.005912452936172485, acc: 100.0, f1: 100.0, r: 0.7693247438533456
06/02/2019 03:55:20 step: 642, epoch: 19, batch: 14, loss: 0.0037658587098121643, acc: 100.0, f1: 100.0, r: 0.6922070168502681
06/02/2019 03:55:21 step: 647, epoch: 19, batch: 19, loss: 0.011133924126625061, acc: 100.0, f1: 100.0, r: 0.8149125832393602
06/02/2019 03:55:21 step: 652, epoch: 19, batch: 24, loss: 0.0055410414934158325, acc: 100.0, f1: 100.0, r: 0.7056001479654368
06/02/2019 03:55:21 step: 657, epoch: 19, batch: 29, loss: 0.006386183202266693, acc: 100.0, f1: 100.0, r: 0.7915679479225355
06/02/2019 03:55:22 *** evaluating ***
06/02/2019 03:55:22 step: 20, epoch: 19, acc: 53.41880341880342, f1: 22.042396745932418, r: 0.2387327059679237
06/02/2019 03:55:22 *** epoch: 21 ***
06/02/2019 03:55:22 *** training ***
06/02/2019 03:55:22 step: 665, epoch: 20, batch: 4, loss: 0.00839202105998993, acc: 100.0, f1: 100.0, r: 0.8152803675682767
06/02/2019 03:55:22 step: 670, epoch: 20, batch: 9, loss: 0.02454337850213051, acc: 100.0, f1: 100.0, r: 0.7160344348020143
06/02/2019 03:55:23 step: 675, epoch: 20, batch: 14, loss: 0.009160935878753662, acc: 100.0, f1: 100.0, r: 0.7024238995714361
06/02/2019 03:55:23 step: 680, epoch: 20, batch: 19, loss: 0.006044827401638031, acc: 100.0, f1: 100.0, r: 0.6894479441354227
06/02/2019 03:55:23 step: 685, epoch: 20, batch: 24, loss: 0.005153156816959381, acc: 100.0, f1: 100.0, r: 0.8028794994920438
06/02/2019 03:55:24 step: 690, epoch: 20, batch: 29, loss: 0.0026864632964134216, acc: 100.0, f1: 100.0, r: 0.8244036282481426
06/02/2019 03:55:24 *** evaluating ***
06/02/2019 03:55:24 step: 21, epoch: 20, acc: 51.28205128205128, f1: 21.557081895688405, r: 0.2353189063071686
06/02/2019 03:55:24 *** epoch: 22 ***
06/02/2019 03:55:24 *** training ***
06/02/2019 03:55:24 step: 698, epoch: 21, batch: 4, loss: 0.0049038901925086975, acc: 100.0, f1: 100.0, r: 0.7857981638915421
06/02/2019 03:55:25 step: 703, epoch: 21, batch: 9, loss: 0.009404271841049194, acc: 100.0, f1: 100.0, r: 0.7524006548962551
06/02/2019 03:55:25 step: 708, epoch: 21, batch: 14, loss: 0.006020471453666687, acc: 100.0, f1: 100.0, r: 0.7023889435982764
06/02/2019 03:55:25 step: 713, epoch: 21, batch: 19, loss: 0.007872991263866425, acc: 100.0, f1: 100.0, r: 0.7698774267573216
06/02/2019 03:55:26 step: 718, epoch: 21, batch: 24, loss: 0.013115718960762024, acc: 100.0, f1: 100.0, r: 0.7937797341085591
06/02/2019 03:55:26 step: 723, epoch: 21, batch: 29, loss: 0.0064543262124061584, acc: 100.0, f1: 100.0, r: 0.81073505407979
06/02/2019 03:55:26 *** evaluating ***
06/02/2019 03:55:26 step: 22, epoch: 21, acc: 53.41880341880342, f1: 21.925333486221, r: 0.2358001922813401
06/02/2019 03:55:26 *** epoch: 23 ***
06/02/2019 03:55:26 *** training ***
06/02/2019 03:55:27 step: 731, epoch: 22, batch: 4, loss: 0.008610296994447708, acc: 100.0, f1: 100.0, r: 0.493633630394123
06/02/2019 03:55:27 step: 736, epoch: 22, batch: 9, loss: 0.0040784478187561035, acc: 100.0, f1: 100.0, r: 0.8348611534033583
06/02/2019 03:55:27 step: 741, epoch: 22, batch: 14, loss: 0.005919143557548523, acc: 100.0, f1: 100.0, r: 0.7291870720437806
06/02/2019 03:55:28 step: 746, epoch: 22, batch: 19, loss: 0.007181406021118164, acc: 100.0, f1: 100.0, r: 0.8293504160019
06/02/2019 03:55:28 step: 751, epoch: 22, batch: 24, loss: 0.0045736804604530334, acc: 100.0, f1: 100.0, r: 0.7708524423794846
06/02/2019 03:55:28 step: 756, epoch: 22, batch: 29, loss: 0.0056116580963134766, acc: 100.0, f1: 100.0, r: 0.8277960034858531
06/02/2019 03:55:29 *** evaluating ***
06/02/2019 03:55:29 step: 23, epoch: 22, acc: 51.70940170940172, f1: 22.067709536472076, r: 0.23411316813671193
06/02/2019 03:55:29 *** epoch: 24 ***
06/02/2019 03:55:29 *** training ***
06/02/2019 03:55:29 step: 764, epoch: 23, batch: 4, loss: 0.0030819550156593323, acc: 100.0, f1: 100.0, r: 0.8067968402466507
06/02/2019 03:55:29 step: 769, epoch: 23, batch: 9, loss: 0.0034638643264770508, acc: 100.0, f1: 100.0, r: 0.7591720252300862
06/02/2019 03:55:30 step: 774, epoch: 23, batch: 14, loss: 0.00987415760755539, acc: 100.0, f1: 100.0, r: 0.821031716936733
06/02/2019 03:55:30 step: 779, epoch: 23, batch: 19, loss: 0.003670603036880493, acc: 100.0, f1: 100.0, r: 0.7015794009966158
06/02/2019 03:55:30 step: 784, epoch: 23, batch: 24, loss: 0.006882406771183014, acc: 100.0, f1: 100.0, r: 0.777396214163849
06/02/2019 03:55:31 step: 789, epoch: 23, batch: 29, loss: 0.002739265561103821, acc: 100.0, f1: 100.0, r: 0.717834452920416
06/02/2019 03:55:31 *** evaluating ***
06/02/2019 03:55:31 step: 24, epoch: 23, acc: 52.991452991452995, f1: 21.942107883108978, r: 0.22994974684988795
06/02/2019 03:55:31 *** epoch: 25 ***
06/02/2019 03:55:31 *** training ***
06/02/2019 03:55:31 step: 797, epoch: 24, batch: 4, loss: 0.009983193129301071, acc: 100.0, f1: 100.0, r: 0.7835462819680077
06/02/2019 03:55:32 step: 802, epoch: 24, batch: 9, loss: 0.0029462575912475586, acc: 100.0, f1: 100.0, r: 0.7013872859799963
06/02/2019 03:55:32 step: 807, epoch: 24, batch: 14, loss: 0.011602252721786499, acc: 100.0, f1: 100.0, r: 0.7326385830453028
06/02/2019 03:55:32 step: 812, epoch: 24, batch: 19, loss: 0.007414229214191437, acc: 100.0, f1: 100.0, r: 0.8311700568887304
06/02/2019 03:55:33 step: 817, epoch: 24, batch: 24, loss: 0.00609927624464035, acc: 100.0, f1: 100.0, r: 0.7223953486392006
06/02/2019 03:55:33 step: 822, epoch: 24, batch: 29, loss: 0.006350085139274597, acc: 100.0, f1: 100.0, r: 0.777073895050459
06/02/2019 03:55:33 *** evaluating ***
06/02/2019 03:55:33 step: 25, epoch: 24, acc: 52.13675213675214, f1: 21.569941420639612, r: 0.22955335366857688
06/02/2019 03:55:33 *** epoch: 26 ***
06/02/2019 03:55:33 *** training ***
06/02/2019 03:55:34 step: 830, epoch: 25, batch: 4, loss: 0.004012554883956909, acc: 100.0, f1: 100.0, r: 0.7614494204502908
06/02/2019 03:55:34 step: 835, epoch: 25, batch: 9, loss: 0.0027372166514396667, acc: 100.0, f1: 100.0, r: 0.7397692081751361
06/02/2019 03:55:34 step: 840, epoch: 25, batch: 14, loss: 0.0026424750685691833, acc: 100.0, f1: 100.0, r: 0.7324881348992638
06/02/2019 03:55:35 step: 845, epoch: 25, batch: 19, loss: 0.01147913932800293, acc: 100.0, f1: 100.0, r: 0.7362568806924839
06/02/2019 03:55:35 step: 850, epoch: 25, batch: 24, loss: 0.0076086074113845825, acc: 100.0, f1: 100.0, r: 0.7830321028672764
06/02/2019 03:55:35 step: 855, epoch: 25, batch: 29, loss: 0.012746196240186691, acc: 100.0, f1: 100.0, r: 0.7073192457916829
06/02/2019 03:55:35 *** evaluating ***
06/02/2019 03:55:36 step: 26, epoch: 25, acc: 51.28205128205128, f1: 21.93189165484883, r: 0.22748937774550826
06/02/2019 03:55:36 *** epoch: 27 ***
06/02/2019 03:55:36 *** training ***
06/02/2019 03:55:36 step: 863, epoch: 26, batch: 4, loss: 0.003309696912765503, acc: 100.0, f1: 100.0, r: 0.7845391824830914
06/02/2019 03:55:36 step: 868, epoch: 26, batch: 9, loss: 0.003078065812587738, acc: 100.0, f1: 100.0, r: 0.6615198648640932
06/02/2019 03:55:37 step: 873, epoch: 26, batch: 14, loss: 0.0038688406348228455, acc: 100.0, f1: 100.0, r: 0.8248094544119613
06/02/2019 03:55:37 step: 878, epoch: 26, batch: 19, loss: 0.005121313035488129, acc: 100.0, f1: 100.0, r: 0.6890623645569642
06/02/2019 03:55:37 step: 883, epoch: 26, batch: 24, loss: 0.010226111859083176, acc: 100.0, f1: 100.0, r: 0.7066853980547544
06/02/2019 03:55:38 step: 888, epoch: 26, batch: 29, loss: 0.00272524356842041, acc: 100.0, f1: 100.0, r: 0.7936124386805643
06/02/2019 03:55:38 *** evaluating ***
06/02/2019 03:55:38 step: 27, epoch: 26, acc: 53.84615384615385, f1: 22.099992283348726, r: 0.23295410887272583
06/02/2019 03:55:38 *** epoch: 28 ***
06/02/2019 03:55:38 *** training ***
06/02/2019 03:55:38 step: 896, epoch: 27, batch: 4, loss: 0.005247652530670166, acc: 100.0, f1: 100.0, r: 0.7316204786439707
06/02/2019 03:55:39 step: 901, epoch: 27, batch: 9, loss: 0.004605002701282501, acc: 100.0, f1: 100.0, r: 0.715973711204323
06/02/2019 03:55:39 step: 906, epoch: 27, batch: 14, loss: 0.0027280449867248535, acc: 100.0, f1: 100.0, r: 0.6189341711664763
06/02/2019 03:55:39 step: 911, epoch: 27, batch: 19, loss: 0.0034242942929267883, acc: 100.0, f1: 100.0, r: 0.6490542636695623
06/02/2019 03:55:40 step: 916, epoch: 27, batch: 24, loss: 0.005948677659034729, acc: 100.0, f1: 100.0, r: 0.8639010230170502
06/02/2019 03:55:40 step: 921, epoch: 27, batch: 29, loss: 0.003336504101753235, acc: 100.0, f1: 100.0, r: 0.7801716521105732
06/02/2019 03:55:40 *** evaluating ***
06/02/2019 03:55:40 step: 28, epoch: 27, acc: 52.56410256410257, f1: 22.405651134537756, r: 0.23013528902900052
06/02/2019 03:55:40 *** epoch: 29 ***
06/02/2019 03:55:40 *** training ***
06/02/2019 03:55:41 step: 929, epoch: 28, batch: 4, loss: 0.0017308145761489868, acc: 100.0, f1: 100.0, r: 0.7152496688038895
06/02/2019 03:55:41 step: 934, epoch: 28, batch: 9, loss: 0.0020507201552391052, acc: 100.0, f1: 100.0, r: 0.8192299889226375
06/02/2019 03:55:41 step: 939, epoch: 28, batch: 14, loss: 0.0017306283116340637, acc: 100.0, f1: 100.0, r: 0.5479309523019648
06/02/2019 03:55:41 step: 944, epoch: 28, batch: 19, loss: 0.0032425299286842346, acc: 100.0, f1: 100.0, r: 0.7012362369673149
06/02/2019 03:55:42 step: 949, epoch: 28, batch: 24, loss: 0.0030505284667015076, acc: 100.0, f1: 100.0, r: 0.79399957646976
06/02/2019 03:55:42 step: 954, epoch: 28, batch: 29, loss: 0.0025117844343185425, acc: 100.0, f1: 100.0, r: 0.8393991908413545
06/02/2019 03:55:42 *** evaluating ***
06/02/2019 03:55:42 step: 29, epoch: 28, acc: 51.28205128205128, f1: 21.45456683708091, r: 0.22442242320251407
06/02/2019 03:55:42 *** epoch: 30 ***
06/02/2019 03:55:42 *** training ***
06/02/2019 03:55:43 step: 962, epoch: 29, batch: 4, loss: 0.0026300624012947083, acc: 100.0, f1: 100.0, r: 0.6884287569227129
06/02/2019 03:55:43 step: 967, epoch: 29, batch: 9, loss: 0.0036340653896331787, acc: 100.0, f1: 100.0, r: 0.6748393178889698
06/02/2019 03:55:43 step: 972, epoch: 29, batch: 14, loss: 0.0017247274518013, acc: 100.0, f1: 100.0, r: 0.6857943008697375
06/02/2019 03:55:44 step: 977, epoch: 29, batch: 19, loss: 0.0035943835973739624, acc: 100.0, f1: 100.0, r: 0.7975340512511384
06/02/2019 03:55:44 step: 982, epoch: 29, batch: 24, loss: 0.004409804940223694, acc: 100.0, f1: 100.0, r: 0.7491389706171505
06/02/2019 03:55:44 step: 987, epoch: 29, batch: 29, loss: 0.0028580129146575928, acc: 100.0, f1: 100.0, r: 0.7417268177293266
06/02/2019 03:55:45 *** evaluating ***
06/02/2019 03:55:45 step: 30, epoch: 29, acc: 50.427350427350426, f1: 21.81133817163229, r: 0.21732276455481814
06/02/2019 03:55:45 *** epoch: 31 ***
06/02/2019 03:55:45 *** training ***
06/02/2019 03:55:45 step: 995, epoch: 30, batch: 4, loss: 0.004029162228107452, acc: 100.0, f1: 100.0, r: 0.7061165837187724
06/02/2019 03:55:45 step: 1000, epoch: 30, batch: 9, loss: 0.0074328407645225525, acc: 100.0, f1: 100.0, r: 0.8119010445449638
06/02/2019 03:55:46 step: 1005, epoch: 30, batch: 14, loss: 0.008804045617580414, acc: 100.0, f1: 100.0, r: 0.7223298498699829
06/02/2019 03:55:46 step: 1010, epoch: 30, batch: 19, loss: 0.0031468644738197327, acc: 100.0, f1: 100.0, r: 0.6832835340232049
06/02/2019 03:55:46 step: 1015, epoch: 30, batch: 24, loss: 0.003548145294189453, acc: 100.0, f1: 100.0, r: 0.7326590693782697
06/02/2019 03:55:47 step: 1020, epoch: 30, batch: 29, loss: 0.005800962448120117, acc: 100.0, f1: 100.0, r: 0.713587896021452
06/02/2019 03:55:47 *** evaluating ***
06/02/2019 03:55:47 step: 31, epoch: 30, acc: 51.70940170940172, f1: 21.150500070131805, r: 0.23273940082900604
06/02/2019 03:55:47 *** epoch: 32 ***
06/02/2019 03:55:47 *** training ***
06/02/2019 03:55:47 step: 1028, epoch: 31, batch: 4, loss: 0.0037365704774856567, acc: 100.0, f1: 100.0, r: 0.6778509343113764
06/02/2019 03:55:48 step: 1033, epoch: 31, batch: 9, loss: 0.003604792058467865, acc: 100.0, f1: 100.0, r: 0.7620168515811474
06/02/2019 03:55:48 step: 1038, epoch: 31, batch: 14, loss: 0.0060986727476119995, acc: 100.0, f1: 100.0, r: 0.6776250932517641
06/02/2019 03:55:48 step: 1043, epoch: 31, batch: 19, loss: 0.0008433312177658081, acc: 100.0, f1: 100.0, r: 0.8679403573518817
06/02/2019 03:55:49 step: 1048, epoch: 31, batch: 24, loss: 0.0028664395213127136, acc: 100.0, f1: 100.0, r: 0.6839692256753782
06/02/2019 03:55:49 step: 1053, epoch: 31, batch: 29, loss: 0.007443122565746307, acc: 100.0, f1: 100.0, r: 0.6875433136483404
06/02/2019 03:55:49 *** evaluating ***
06/02/2019 03:55:49 step: 32, epoch: 31, acc: 54.27350427350427, f1: 21.822199730094464, r: 0.2441509421281075
06/02/2019 03:55:49 *** epoch: 33 ***
06/02/2019 03:55:49 *** training ***
06/02/2019 03:55:49 step: 1061, epoch: 32, batch: 4, loss: 0.0020717084407806396, acc: 100.0, f1: 100.0, r: 0.8709301179526294
06/02/2019 03:55:50 step: 1066, epoch: 32, batch: 9, loss: 0.003929831087589264, acc: 100.0, f1: 100.0, r: 0.7714163190210026
06/02/2019 03:55:50 step: 1071, epoch: 32, batch: 14, loss: 0.006378382444381714, acc: 100.0, f1: 100.0, r: 0.7953788733775308
06/02/2019 03:55:50 step: 1076, epoch: 32, batch: 19, loss: 0.0019031539559364319, acc: 100.0, f1: 100.0, r: 0.8340934484894164
06/02/2019 03:55:51 step: 1081, epoch: 32, batch: 24, loss: 0.0025532618165016174, acc: 100.0, f1: 100.0, r: 0.7277723159493317
06/02/2019 03:55:51 step: 1086, epoch: 32, batch: 29, loss: 0.0033415332436561584, acc: 100.0, f1: 100.0, r: 0.667234242340355
06/02/2019 03:55:51 *** evaluating ***
06/02/2019 03:55:51 step: 33, epoch: 32, acc: 51.28205128205128, f1: 21.34868870938125, r: 0.23663002215936674
06/02/2019 03:55:51 *** epoch: 34 ***
06/02/2019 03:55:51 *** training ***
06/02/2019 03:55:52 step: 1094, epoch: 33, batch: 4, loss: 0.002332232892513275, acc: 100.0, f1: 100.0, r: 0.7137755623066194
06/02/2019 03:55:52 step: 1099, epoch: 33, batch: 9, loss: 0.0018334463238716125, acc: 100.0, f1: 100.0, r: 0.7573998218965885
06/02/2019 03:55:52 step: 1104, epoch: 33, batch: 14, loss: 0.005071073770523071, acc: 100.0, f1: 100.0, r: 0.7329264865341876
06/02/2019 03:55:52 step: 1109, epoch: 33, batch: 19, loss: 0.0032911747694015503, acc: 100.0, f1: 100.0, r: 0.7101574981228445
06/02/2019 03:55:53 step: 1114, epoch: 33, batch: 24, loss: 0.003918886184692383, acc: 100.0, f1: 100.0, r: 0.7894180291651147
06/02/2019 03:55:53 step: 1119, epoch: 33, batch: 29, loss: 0.005755074322223663, acc: 100.0, f1: 100.0, r: 0.8041289662101117
06/02/2019 03:55:53 *** evaluating ***
06/02/2019 03:55:53 step: 34, epoch: 33, acc: 51.70940170940172, f1: 21.546857449535622, r: 0.2290762121151409
06/02/2019 03:55:53 *** epoch: 35 ***
06/02/2019 03:55:53 *** training ***
06/02/2019 03:55:54 step: 1127, epoch: 34, batch: 4, loss: 0.004227057099342346, acc: 100.0, f1: 100.0, r: 0.844344987178554
06/02/2019 03:55:54 step: 1132, epoch: 34, batch: 9, loss: 0.0023573413491249084, acc: 100.0, f1: 100.0, r: 0.7251101598341297
06/02/2019 03:55:54 step: 1137, epoch: 34, batch: 14, loss: 0.006998177617788315, acc: 100.0, f1: 100.0, r: 0.7523147865061263
06/02/2019 03:55:55 step: 1142, epoch: 34, batch: 19, loss: 0.002429187297821045, acc: 100.0, f1: 100.0, r: 0.6857847116292368
06/02/2019 03:55:55 step: 1147, epoch: 34, batch: 24, loss: 0.007054932415485382, acc: 100.0, f1: 100.0, r: 0.6578264498324348
06/02/2019 03:55:55 step: 1152, epoch: 34, batch: 29, loss: 0.004359401762485504, acc: 100.0, f1: 100.0, r: 0.7459961786664
06/02/2019 03:55:56 *** evaluating ***
06/02/2019 03:55:56 step: 35, epoch: 34, acc: 52.56410256410257, f1: 21.972134121624194, r: 0.23145778407221101
06/02/2019 03:55:56 *** epoch: 36 ***
06/02/2019 03:55:56 *** training ***
06/02/2019 03:55:56 step: 1160, epoch: 35, batch: 4, loss: 0.0016861483454704285, acc: 100.0, f1: 100.0, r: 0.7711243760038798
06/02/2019 03:55:56 step: 1165, epoch: 35, batch: 9, loss: 0.0032081902027130127, acc: 100.0, f1: 100.0, r: 0.7918924170317292
06/02/2019 03:55:57 step: 1170, epoch: 35, batch: 14, loss: 0.0016666203737258911, acc: 100.0, f1: 100.0, r: 0.6336740464241587
06/02/2019 03:55:57 step: 1175, epoch: 35, batch: 19, loss: 0.002623647451400757, acc: 100.0, f1: 100.0, r: 0.7517319060070122
06/02/2019 03:55:57 step: 1180, epoch: 35, batch: 24, loss: 0.0020224153995513916, acc: 100.0, f1: 100.0, r: 0.7438956243874687
06/02/2019 03:55:58 step: 1185, epoch: 35, batch: 29, loss: 0.004504051059484482, acc: 100.0, f1: 100.0, r: 0.7192503398376204
06/02/2019 03:55:58 *** evaluating ***
06/02/2019 03:55:58 step: 36, epoch: 35, acc: 51.28205128205128, f1: 21.237395965930546, r: 0.22143825533302353
06/02/2019 03:55:58 *** epoch: 37 ***
06/02/2019 03:55:58 *** training ***
06/02/2019 03:55:58 step: 1193, epoch: 36, batch: 4, loss: 0.002304859459400177, acc: 100.0, f1: 100.0, r: 0.7073054666608313
06/02/2019 03:55:59 step: 1198, epoch: 36, batch: 9, loss: 0.002124696969985962, acc: 100.0, f1: 100.0, r: 0.7523638104178805
06/02/2019 03:55:59 step: 1203, epoch: 36, batch: 14, loss: 0.0024913176894187927, acc: 100.0, f1: 100.0, r: 0.790998081043315
06/02/2019 03:55:59 step: 1208, epoch: 36, batch: 19, loss: 0.0018182694911956787, acc: 100.0, f1: 100.0, r: 0.6651609025650649
06/02/2019 03:56:00 step: 1213, epoch: 36, batch: 24, loss: 0.0025208964943885803, acc: 100.0, f1: 100.0, r: 0.71132155663453
06/02/2019 03:56:00 step: 1218, epoch: 36, batch: 29, loss: 0.001995094120502472, acc: 100.0, f1: 100.0, r: 0.7782380466170199
06/02/2019 03:56:00 *** evaluating ***
06/02/2019 03:56:00 step: 37, epoch: 36, acc: 52.56410256410257, f1: 21.744938082373615, r: 0.22935295068387088
06/02/2019 03:56:00 *** epoch: 38 ***
06/02/2019 03:56:00 *** training ***
06/02/2019 03:56:01 step: 1226, epoch: 37, batch: 4, loss: 0.006718441843986511, acc: 100.0, f1: 100.0, r: 0.7748247363553833
06/02/2019 03:56:01 step: 1231, epoch: 37, batch: 9, loss: 0.0021261200308799744, acc: 100.0, f1: 100.0, r: 0.7593701176392624
06/02/2019 03:56:01 step: 1236, epoch: 37, batch: 14, loss: 0.0013069063425064087, acc: 100.0, f1: 100.0, r: 0.7980404897935464
06/02/2019 03:56:02 step: 1241, epoch: 37, batch: 19, loss: 0.0035572946071624756, acc: 100.0, f1: 100.0, r: 0.7195068217692203
06/02/2019 03:56:02 step: 1246, epoch: 37, batch: 24, loss: 0.00687757134437561, acc: 100.0, f1: 100.0, r: 0.8151244908402793
06/02/2019 03:56:02 step: 1251, epoch: 37, batch: 29, loss: 0.0011025890707969666, acc: 100.0, f1: 100.0, r: 0.6887631473231638
06/02/2019 03:56:03 *** evaluating ***
06/02/2019 03:56:03 step: 38, epoch: 37, acc: 52.991452991452995, f1: 21.82840514523819, r: 0.23307032684834686
06/02/2019 03:56:03 *** epoch: 39 ***
06/02/2019 03:56:03 *** training ***
06/02/2019 03:56:03 step: 1259, epoch: 38, batch: 4, loss: 0.0013085603713989258, acc: 100.0, f1: 100.0, r: 0.7784036829542632
06/02/2019 03:56:03 step: 1264, epoch: 38, batch: 9, loss: 0.0036271438002586365, acc: 100.0, f1: 100.0, r: 0.7805589272159761
06/02/2019 03:56:04 step: 1269, epoch: 38, batch: 14, loss: 0.0015714168548583984, acc: 100.0, f1: 100.0, r: 0.8103927475494399
06/02/2019 03:56:04 step: 1274, epoch: 38, batch: 19, loss: 0.0016531124711036682, acc: 100.0, f1: 100.0, r: 0.6365120051761222
06/02/2019 03:56:04 step: 1279, epoch: 38, batch: 24, loss: 0.00171748548746109, acc: 100.0, f1: 100.0, r: 0.8513709224027486
06/02/2019 03:56:05 step: 1284, epoch: 38, batch: 29, loss: 0.002000190317630768, acc: 100.0, f1: 100.0, r: 0.7505756228931229
06/02/2019 03:56:05 *** evaluating ***
06/02/2019 03:56:05 step: 39, epoch: 38, acc: 52.991452991452995, f1: 22.319356343846415, r: 0.23289037724742004
06/02/2019 03:56:05 *** epoch: 40 ***
06/02/2019 03:56:05 *** training ***
06/02/2019 03:56:05 step: 1292, epoch: 39, batch: 4, loss: 0.00196751207113266, acc: 100.0, f1: 100.0, r: 0.7685006614913561
06/02/2019 03:56:06 step: 1297, epoch: 39, batch: 9, loss: 0.0013746023178100586, acc: 100.0, f1: 100.0, r: 0.8070284079091056
06/02/2019 03:56:06 step: 1302, epoch: 39, batch: 14, loss: 0.0017021074891090393, acc: 100.0, f1: 100.0, r: 0.632003046190139
06/02/2019 03:56:06 step: 1307, epoch: 39, batch: 19, loss: 0.002009183168411255, acc: 100.0, f1: 100.0, r: 0.6979342217351667
06/02/2019 03:56:07 step: 1312, epoch: 39, batch: 24, loss: 0.0030346810817718506, acc: 100.0, f1: 100.0, r: 0.6741057119794118
06/02/2019 03:56:07 step: 1317, epoch: 39, batch: 29, loss: 0.001791633665561676, acc: 100.0, f1: 100.0, r: 0.7441449225615142
06/02/2019 03:56:07 *** evaluating ***
06/02/2019 03:56:07 step: 40, epoch: 39, acc: 52.13675213675214, f1: 21.883900954059857, r: 0.2302607151799561
06/02/2019 03:56:07 *** epoch: 41 ***
06/02/2019 03:56:07 *** training ***
06/02/2019 03:56:08 step: 1325, epoch: 40, batch: 4, loss: 0.0034348145127296448, acc: 100.0, f1: 100.0, r: 0.5909134719886863
06/02/2019 03:56:08 step: 1330, epoch: 40, batch: 9, loss: 0.0030508041381835938, acc: 100.0, f1: 100.0, r: 0.7973298877671045
06/02/2019 03:56:08 step: 1335, epoch: 40, batch: 14, loss: 0.001356668770313263, acc: 100.0, f1: 100.0, r: 0.6973607090550125
06/02/2019 03:56:09 step: 1340, epoch: 40, batch: 19, loss: 0.0035993754863739014, acc: 100.0, f1: 100.0, r: 0.8413933758192792
06/02/2019 03:56:09 step: 1345, epoch: 40, batch: 24, loss: 0.0022630244493484497, acc: 100.0, f1: 100.0, r: 0.776473064809891
06/02/2019 03:56:09 step: 1350, epoch: 40, batch: 29, loss: 0.0011062920093536377, acc: 100.0, f1: 100.0, r: 0.6953230713600727
06/02/2019 03:56:09 *** evaluating ***
06/02/2019 03:56:10 step: 41, epoch: 40, acc: 50.0, f1: 21.63340504653965, r: 0.22517095031278553
06/02/2019 03:56:10 *** epoch: 42 ***
06/02/2019 03:56:10 *** training ***
06/02/2019 03:56:10 step: 1358, epoch: 41, batch: 4, loss: 0.0018344447016716003, acc: 100.0, f1: 100.0, r: 0.7537773239791902
06/02/2019 03:56:10 step: 1363, epoch: 41, batch: 9, loss: 0.0021715089678764343, acc: 100.0, f1: 100.0, r: 0.7221029186602607
06/02/2019 03:56:11 step: 1368, epoch: 41, batch: 14, loss: 0.0021873489022254944, acc: 100.0, f1: 100.0, r: 0.643293568019748
06/02/2019 03:56:11 step: 1373, epoch: 41, batch: 19, loss: 0.002064347267150879, acc: 100.0, f1: 100.0, r: 0.6999156553246668
06/02/2019 03:56:11 step: 1378, epoch: 41, batch: 24, loss: 0.0011294633150100708, acc: 100.0, f1: 100.0, r: 0.6718401083251795
06/02/2019 03:56:12 step: 1383, epoch: 41, batch: 29, loss: 0.0028571560978889465, acc: 100.0, f1: 100.0, r: 0.7084513370836378
06/02/2019 03:56:12 *** evaluating ***
06/02/2019 03:56:12 step: 42, epoch: 41, acc: 51.70940170940172, f1: 21.520506289377163, r: 0.23024484458904368
06/02/2019 03:56:12 *** epoch: 43 ***
06/02/2019 03:56:12 *** training ***
06/02/2019 03:56:12 step: 1391, epoch: 42, batch: 4, loss: 0.003909029066562653, acc: 100.0, f1: 100.0, r: 0.7530627427298527
06/02/2019 03:56:13 step: 1396, epoch: 42, batch: 9, loss: 0.0009629055857658386, acc: 100.0, f1: 100.0, r: 0.7788792242283409
06/02/2019 03:56:13 step: 1401, epoch: 42, batch: 14, loss: 0.0017420724034309387, acc: 100.0, f1: 100.0, r: 0.727611656831519
06/02/2019 03:56:13 step: 1406, epoch: 42, batch: 19, loss: 0.004413984715938568, acc: 100.0, f1: 100.0, r: 0.7210123391417095
06/02/2019 03:56:14 step: 1411, epoch: 42, batch: 24, loss: 0.0010908767580986023, acc: 100.0, f1: 100.0, r: 0.7104244458210329
06/02/2019 03:56:14 step: 1416, epoch: 42, batch: 29, loss: 0.002446889877319336, acc: 100.0, f1: 100.0, r: 0.7004445317498799
06/02/2019 03:56:14 *** evaluating ***
06/02/2019 03:56:14 step: 43, epoch: 42, acc: 49.572649572649574, f1: 21.481961682860035, r: 0.23045528445179703
06/02/2019 03:56:14 *** epoch: 44 ***
06/02/2019 03:56:14 *** training ***
06/02/2019 03:56:15 step: 1424, epoch: 43, batch: 4, loss: 0.0029609352350234985, acc: 100.0, f1: 100.0, r: 0.7591330416779567
06/02/2019 03:56:15 step: 1429, epoch: 43, batch: 9, loss: 0.002928435802459717, acc: 100.0, f1: 100.0, r: 0.6760579137516409
06/02/2019 03:56:15 step: 1434, epoch: 43, batch: 14, loss: 0.0014695003628730774, acc: 100.0, f1: 100.0, r: 0.6903931546696146
06/02/2019 03:56:16 step: 1439, epoch: 43, batch: 19, loss: 0.0013732388615608215, acc: 100.0, f1: 100.0, r: 0.7453058470346836
06/02/2019 03:56:16 step: 1444, epoch: 43, batch: 24, loss: 0.002989158034324646, acc: 100.0, f1: 100.0, r: 0.6954995841873683
06/02/2019 03:56:16 step: 1449, epoch: 43, batch: 29, loss: 0.0015464574098587036, acc: 100.0, f1: 100.0, r: 0.8503962156796929
06/02/2019 03:56:16 *** evaluating ***
06/02/2019 03:56:17 step: 44, epoch: 43, acc: 52.56410256410257, f1: 21.902527036455606, r: 0.23606653125262456
06/02/2019 03:56:17 *** epoch: 45 ***
06/02/2019 03:56:17 *** training ***
06/02/2019 03:56:17 step: 1457, epoch: 44, batch: 4, loss: 0.0036449208855628967, acc: 100.0, f1: 100.0, r: 0.8238987454137872
06/02/2019 03:56:17 step: 1462, epoch: 44, batch: 9, loss: 0.001984812319278717, acc: 100.0, f1: 100.0, r: 0.8089791618699742
06/02/2019 03:56:18 step: 1467, epoch: 44, batch: 14, loss: 0.0021976009011268616, acc: 100.0, f1: 100.0, r: 0.7104526735503043
06/02/2019 03:56:18 step: 1472, epoch: 44, batch: 19, loss: 0.005547448992729187, acc: 100.0, f1: 100.0, r: 0.7891508062385616
06/02/2019 03:56:18 step: 1477, epoch: 44, batch: 24, loss: 0.0019178390502929688, acc: 100.0, f1: 100.0, r: 0.8097961215783138
06/02/2019 03:56:19 step: 1482, epoch: 44, batch: 29, loss: 0.0015428215265274048, acc: 100.0, f1: 100.0, r: 0.8242871057992405
06/02/2019 03:56:19 *** evaluating ***
06/02/2019 03:56:19 step: 45, epoch: 44, acc: 54.27350427350427, f1: 23.187680939558287, r: 0.23915835416070494
06/02/2019 03:56:19 *** epoch: 46 ***
06/02/2019 03:56:19 *** training ***
06/02/2019 03:56:19 step: 1490, epoch: 45, batch: 4, loss: 0.0025222226977348328, acc: 100.0, f1: 100.0, r: 0.7321828593182081
06/02/2019 03:56:20 step: 1495, epoch: 45, batch: 9, loss: 0.0028533264994621277, acc: 100.0, f1: 100.0, r: 0.7384581898744772
06/02/2019 03:56:20 step: 1500, epoch: 45, batch: 14, loss: 0.0027110204100608826, acc: 100.0, f1: 100.0, r: 0.6951541781346172
06/02/2019 03:56:20 step: 1505, epoch: 45, batch: 19, loss: 0.005115240812301636, acc: 100.0, f1: 100.0, r: 0.637792828588378
06/02/2019 03:56:20 step: 1510, epoch: 45, batch: 24, loss: 0.001876533031463623, acc: 100.0, f1: 100.0, r: 0.759622615204091
06/02/2019 03:56:21 step: 1515, epoch: 45, batch: 29, loss: 0.004937693476676941, acc: 100.0, f1: 100.0, r: 0.8232576183886675
06/02/2019 03:56:21 *** evaluating ***
06/02/2019 03:56:21 step: 46, epoch: 45, acc: 52.13675213675214, f1: 22.404288163763358, r: 0.23260761619805756
06/02/2019 03:56:21 *** epoch: 47 ***
06/02/2019 03:56:21 *** training ***
06/02/2019 03:56:21 step: 1523, epoch: 46, batch: 4, loss: 0.001651667058467865, acc: 100.0, f1: 100.0, r: 0.6425569113817353
06/02/2019 03:56:22 step: 1528, epoch: 46, batch: 9, loss: 0.0036495551466941833, acc: 100.0, f1: 100.0, r: 0.7482275764040993
06/02/2019 03:56:22 step: 1533, epoch: 46, batch: 14, loss: 0.0020370110869407654, acc: 100.0, f1: 100.0, r: 0.7962215291659731
06/02/2019 03:56:22 step: 1538, epoch: 46, batch: 19, loss: 0.0013140887022018433, acc: 100.0, f1: 100.0, r: 0.665099605393515
06/02/2019 03:56:23 step: 1543, epoch: 46, batch: 24, loss: 0.001687467098236084, acc: 100.0, f1: 100.0, r: 0.8164293277308428
06/02/2019 03:56:23 step: 1548, epoch: 46, batch: 29, loss: 0.0014878138899803162, acc: 100.0, f1: 100.0, r: 0.7281234715309247
06/02/2019 03:56:23 *** evaluating ***
06/02/2019 03:56:23 step: 47, epoch: 46, acc: 54.27350427350427, f1: 22.395395153277885, r: 0.23474562937501167
06/02/2019 03:56:23 *** epoch: 48 ***
06/02/2019 03:56:23 *** training ***
06/02/2019 03:56:23 step: 1556, epoch: 47, batch: 4, loss: 0.001245833933353424, acc: 100.0, f1: 100.0, r: 0.6655599202278112
06/02/2019 03:56:24 step: 1561, epoch: 47, batch: 9, loss: 0.0016367733478546143, acc: 100.0, f1: 100.0, r: 0.6670005481858426
06/02/2019 03:56:24 step: 1566, epoch: 47, batch: 14, loss: 0.0012455284595489502, acc: 100.0, f1: 100.0, r: 0.736307088430555
06/02/2019 03:56:25 step: 1571, epoch: 47, batch: 19, loss: 0.0017834901809692383, acc: 100.0, f1: 100.0, r: 0.7985766838195811
06/02/2019 03:56:25 step: 1576, epoch: 47, batch: 24, loss: 0.0012632310390472412, acc: 100.0, f1: 100.0, r: 0.7807057280167361
06/02/2019 03:56:25 step: 1581, epoch: 47, batch: 29, loss: 0.002580881118774414, acc: 100.0, f1: 100.0, r: 0.7283828502412879
06/02/2019 03:56:25 *** evaluating ***
06/02/2019 03:56:26 step: 48, epoch: 47, acc: 50.0, f1: 21.355840459995886, r: 0.23414573551192813
06/02/2019 03:56:26 *** epoch: 49 ***
06/02/2019 03:56:26 *** training ***
06/02/2019 03:56:26 step: 1589, epoch: 48, batch: 4, loss: 0.0015545636415481567, acc: 100.0, f1: 100.0, r: 0.8162886098661505
06/02/2019 03:56:26 step: 1594, epoch: 48, batch: 9, loss: 0.0010400786995887756, acc: 100.0, f1: 100.0, r: 0.647045843422534
06/02/2019 03:56:26 step: 1599, epoch: 48, batch: 14, loss: 0.0014012977480888367, acc: 100.0, f1: 100.0, r: 0.8339005152855492
06/02/2019 03:56:27 step: 1604, epoch: 48, batch: 19, loss: 0.0021081045269966125, acc: 100.0, f1: 100.0, r: 0.7084543022394806
06/02/2019 03:56:27 step: 1609, epoch: 48, batch: 24, loss: 0.0026343241333961487, acc: 100.0, f1: 100.0, r: 0.6797445692072659
06/02/2019 03:56:27 step: 1614, epoch: 48, batch: 29, loss: 0.0035344213247299194, acc: 100.0, f1: 100.0, r: 0.8120334047485165
06/02/2019 03:56:28 *** evaluating ***
06/02/2019 03:56:28 step: 49, epoch: 48, acc: 52.13675213675214, f1: 21.769838072215187, r: 0.2353664706106523
06/02/2019 03:56:28 *** epoch: 50 ***
06/02/2019 03:56:28 *** training ***
06/02/2019 03:56:28 step: 1622, epoch: 49, batch: 4, loss: 0.002901829779148102, acc: 100.0, f1: 100.0, r: 0.7520837997388498
06/02/2019 03:56:28 step: 1627, epoch: 49, batch: 9, loss: 0.0013768523931503296, acc: 100.0, f1: 100.0, r: 0.8608586536766857
06/02/2019 03:56:29 step: 1632, epoch: 49, batch: 14, loss: 0.0019119009375572205, acc: 100.0, f1: 100.0, r: 0.7536227428905506
06/02/2019 03:56:29 step: 1637, epoch: 49, batch: 19, loss: 0.004680998623371124, acc: 100.0, f1: 100.0, r: 0.7568075801112402
06/02/2019 03:56:29 step: 1642, epoch: 49, batch: 24, loss: 0.004592776298522949, acc: 100.0, f1: 100.0, r: 0.8306191719764423
06/02/2019 03:56:30 step: 1647, epoch: 49, batch: 29, loss: 0.002247154712677002, acc: 100.0, f1: 100.0, r: 0.7350661897794404
06/02/2019 03:56:30 *** evaluating ***
06/02/2019 03:56:30 step: 50, epoch: 49, acc: 51.70940170940172, f1: 21.655476697995418, r: 0.23044628017090102
06/02/2019 03:56:30 *** epoch: 51 ***
06/02/2019 03:56:30 *** training ***
06/02/2019 03:56:30 step: 1655, epoch: 50, batch: 4, loss: 0.0022805780172348022, acc: 100.0, f1: 100.0, r: 0.6295658747407583
06/02/2019 03:56:31 step: 1660, epoch: 50, batch: 9, loss: 0.0010347068309783936, acc: 100.0, f1: 100.0, r: 0.6631900132560078
06/02/2019 03:56:31 step: 1665, epoch: 50, batch: 14, loss: 0.0012583062052726746, acc: 100.0, f1: 100.0, r: 0.6895710433304043
06/02/2019 03:56:31 step: 1670, epoch: 50, batch: 19, loss: 0.0012434646487236023, acc: 100.0, f1: 100.0, r: 0.6395745219308846
06/02/2019 03:56:32 step: 1675, epoch: 50, batch: 24, loss: 0.0036987513303756714, acc: 100.0, f1: 100.0, r: 0.8229243527068094
06/02/2019 03:56:32 step: 1680, epoch: 50, batch: 29, loss: 0.0011844560503959656, acc: 100.0, f1: 100.0, r: 0.8205248660332755
06/02/2019 03:56:32 *** evaluating ***
06/02/2019 03:56:32 step: 51, epoch: 50, acc: 50.85470085470085, f1: 21.275987345292727, r: 0.230862893770637
06/02/2019 03:56:32 *** epoch: 52 ***
06/02/2019 03:56:32 *** training ***
06/02/2019 03:56:33 step: 1688, epoch: 51, batch: 4, loss: 0.0021256357431411743, acc: 100.0, f1: 100.0, r: 0.7694009138824993
06/02/2019 03:56:33 step: 1693, epoch: 51, batch: 9, loss: 0.014984898269176483, acc: 100.0, f1: 100.0, r: 0.7827874308609708
06/02/2019 03:56:33 step: 1698, epoch: 51, batch: 14, loss: 0.0007668137550354004, acc: 100.0, f1: 100.0, r: 0.8069732136982516
06/02/2019 03:56:34 step: 1703, epoch: 51, batch: 19, loss: 0.002347975969314575, acc: 100.0, f1: 100.0, r: 0.6419275290549719
06/02/2019 03:56:34 step: 1708, epoch: 51, batch: 24, loss: 0.0011974051594734192, acc: 100.0, f1: 100.0, r: 0.8016338287203697
06/02/2019 03:56:34 step: 1713, epoch: 51, batch: 29, loss: 0.0017442479729652405, acc: 100.0, f1: 100.0, r: 0.6402791593503984
06/02/2019 03:56:34 *** evaluating ***
06/02/2019 03:56:35 step: 52, epoch: 51, acc: 53.84615384615385, f1: 22.330639748036663, r: 0.24167154287824869
06/02/2019 03:56:35 *** epoch: 53 ***
06/02/2019 03:56:35 *** training ***
06/02/2019 03:56:35 step: 1721, epoch: 52, batch: 4, loss: 0.001864798367023468, acc: 100.0, f1: 100.0, r: 0.8398069336187592
06/02/2019 03:56:35 step: 1726, epoch: 52, batch: 9, loss: 0.001063913106918335, acc: 100.0, f1: 100.0, r: 0.5984797252505281
06/02/2019 03:56:36 step: 1731, epoch: 52, batch: 14, loss: 0.000910736620426178, acc: 100.0, f1: 100.0, r: 0.6513172418439733
06/02/2019 03:56:36 step: 1736, epoch: 52, batch: 19, loss: 0.0016794130206108093, acc: 100.0, f1: 100.0, r: 0.698915547029582
06/02/2019 03:56:36 step: 1741, epoch: 52, batch: 24, loss: 0.0011952891945838928, acc: 100.0, f1: 100.0, r: 0.7432648410518681
06/02/2019 03:56:37 step: 1746, epoch: 52, batch: 29, loss: 0.0027927234768867493, acc: 100.0, f1: 100.0, r: 0.7978755706534457
06/02/2019 03:56:37 *** evaluating ***
06/02/2019 03:56:37 step: 53, epoch: 52, acc: 52.56410256410257, f1: 22.234180444102353, r: 0.2410371753343966
06/02/2019 03:56:37 *** epoch: 54 ***
06/02/2019 03:56:37 *** training ***
06/02/2019 03:56:37 step: 1754, epoch: 53, batch: 4, loss: 0.0038042962551116943, acc: 100.0, f1: 100.0, r: 0.7835006213008159
06/02/2019 03:56:38 step: 1759, epoch: 53, batch: 9, loss: 0.001809343695640564, acc: 100.0, f1: 100.0, r: 0.719365000796134
06/02/2019 03:56:38 step: 1764, epoch: 53, batch: 14, loss: 0.001977689564228058, acc: 100.0, f1: 100.0, r: 0.8040493285836574
06/02/2019 03:56:38 step: 1769, epoch: 53, batch: 19, loss: 0.0018528327345848083, acc: 100.0, f1: 100.0, r: 0.7114861325688174
06/02/2019 03:56:39 step: 1774, epoch: 53, batch: 24, loss: 0.0013696923851966858, acc: 100.0, f1: 100.0, r: 0.6890704793602069
06/02/2019 03:56:39 step: 1779, epoch: 53, batch: 29, loss: 0.0010063424706459045, acc: 100.0, f1: 100.0, r: 0.7465768458251125
06/02/2019 03:56:39 *** evaluating ***
06/02/2019 03:56:39 step: 54, epoch: 53, acc: 53.41880341880342, f1: 22.185824261571856, r: 0.24496895244462238
06/02/2019 03:56:39 *** epoch: 55 ***
06/02/2019 03:56:39 *** training ***
06/02/2019 03:56:40 step: 1787, epoch: 54, batch: 4, loss: 0.002690032124519348, acc: 100.0, f1: 100.0, r: 0.7174467429157707
06/02/2019 03:56:40 step: 1792, epoch: 54, batch: 9, loss: 0.00276009738445282, acc: 100.0, f1: 100.0, r: 0.7967065233924326
06/02/2019 03:56:40 step: 1797, epoch: 54, batch: 14, loss: 0.0014228969812393188, acc: 100.0, f1: 100.0, r: 0.805778055961192
06/02/2019 03:56:41 step: 1802, epoch: 54, batch: 19, loss: 0.0022486895322799683, acc: 100.0, f1: 100.0, r: 0.6567763762609221
06/02/2019 03:56:41 step: 1807, epoch: 54, batch: 24, loss: 0.0008863434195518494, acc: 100.0, f1: 100.0, r: 0.724545328573088
06/02/2019 03:56:41 step: 1812, epoch: 54, batch: 29, loss: 0.001669473946094513, acc: 100.0, f1: 100.0, r: 0.84227118865212
06/02/2019 03:56:41 *** evaluating ***
06/02/2019 03:56:42 step: 55, epoch: 54, acc: 52.991452991452995, f1: 22.271402174206784, r: 0.22940639963010748
06/02/2019 03:56:42 *** epoch: 56 ***
06/02/2019 03:56:42 *** training ***
06/02/2019 03:56:42 step: 1820, epoch: 55, batch: 4, loss: 0.001245640218257904, acc: 100.0, f1: 100.0, r: 0.7773117583775974
06/02/2019 03:56:42 step: 1825, epoch: 55, batch: 9, loss: 0.0018551051616668701, acc: 100.0, f1: 100.0, r: 0.797089224902256
06/02/2019 03:56:43 step: 1830, epoch: 55, batch: 14, loss: 0.0018015801906585693, acc: 100.0, f1: 100.0, r: 0.7006069825892809
06/02/2019 03:56:43 step: 1835, epoch: 55, batch: 19, loss: 0.0012469515204429626, acc: 100.0, f1: 100.0, r: 0.858350845780739
06/02/2019 03:56:43 step: 1840, epoch: 55, batch: 24, loss: 0.002839609980583191, acc: 100.0, f1: 100.0, r: 0.6830196845295426
06/02/2019 03:56:44 step: 1845, epoch: 55, batch: 29, loss: 0.001950681209564209, acc: 100.0, f1: 100.0, r: 0.7784291168727043
06/02/2019 03:56:44 *** evaluating ***
06/02/2019 03:56:44 step: 56, epoch: 55, acc: 51.28205128205128, f1: 21.3773017960214, r: 0.23039515995984391
06/02/2019 03:56:44 *** epoch: 57 ***
06/02/2019 03:56:44 *** training ***
06/02/2019 03:56:44 step: 1853, epoch: 56, batch: 4, loss: 0.004454687237739563, acc: 100.0, f1: 100.0, r: 0.830328032594029
06/02/2019 03:56:45 step: 1858, epoch: 56, batch: 9, loss: 0.0017222091555595398, acc: 100.0, f1: 100.0, r: 0.7304278542690616
06/02/2019 03:56:45 step: 1863, epoch: 56, batch: 14, loss: 0.0018916428089141846, acc: 100.0, f1: 100.0, r: 0.6419185497227038
06/02/2019 03:56:45 step: 1868, epoch: 56, batch: 19, loss: 0.0023022443056106567, acc: 100.0, f1: 100.0, r: 0.6613046032418453
06/02/2019 03:56:46 step: 1873, epoch: 56, batch: 24, loss: 0.003547683358192444, acc: 100.0, f1: 100.0, r: 0.7298221443056814
06/02/2019 03:56:46 step: 1878, epoch: 56, batch: 29, loss: 0.0010152682662010193, acc: 100.0, f1: 100.0, r: 0.7937540585910128
06/02/2019 03:56:46 *** evaluating ***
06/02/2019 03:56:46 step: 57, epoch: 56, acc: 52.13675213675214, f1: 21.762266665559505, r: 0.2235084536783383
06/02/2019 03:56:46 *** epoch: 58 ***
06/02/2019 03:56:46 *** training ***
06/02/2019 03:56:47 step: 1886, epoch: 57, batch: 4, loss: 0.0021823719143867493, acc: 100.0, f1: 100.0, r: 0.7280002165145856
06/02/2019 03:56:47 step: 1891, epoch: 57, batch: 9, loss: 0.0012416467070579529, acc: 100.0, f1: 100.0, r: 0.6341931210627109
06/02/2019 03:56:47 step: 1896, epoch: 57, batch: 14, loss: 0.0006215423345565796, acc: 100.0, f1: 100.0, r: 0.8588165410744013
06/02/2019 03:56:48 step: 1901, epoch: 57, batch: 19, loss: 0.002766624093055725, acc: 100.0, f1: 100.0, r: 0.7759818460262884
06/02/2019 03:56:48 step: 1906, epoch: 57, batch: 24, loss: 0.0016103088855743408, acc: 100.0, f1: 100.0, r: 0.7297822133345445
06/02/2019 03:56:48 step: 1911, epoch: 57, batch: 29, loss: 0.0015084221959114075, acc: 100.0, f1: 100.0, r: 0.6974949304697093
06/02/2019 03:56:48 *** evaluating ***
06/02/2019 03:56:49 step: 58, epoch: 57, acc: 53.41880341880342, f1: 22.68441353789426, r: 0.23230509873298955
06/02/2019 03:56:49 *** epoch: 59 ***
06/02/2019 03:56:49 *** training ***
06/02/2019 03:56:49 step: 1919, epoch: 58, batch: 4, loss: 0.0020835772156715393, acc: 100.0, f1: 100.0, r: 0.7171771776707446
06/02/2019 03:56:49 step: 1924, epoch: 58, batch: 9, loss: 0.0004630237817764282, acc: 100.0, f1: 100.0, r: 0.8201832671065821
06/02/2019 03:56:49 step: 1929, epoch: 58, batch: 14, loss: 0.0007923245429992676, acc: 100.0, f1: 100.0, r: 0.6963995196870931
06/02/2019 03:56:50 step: 1934, epoch: 58, batch: 19, loss: 0.0011601150035858154, acc: 100.0, f1: 100.0, r: 0.8339583339517207
06/02/2019 03:56:50 step: 1939, epoch: 58, batch: 24, loss: 0.0014500245451927185, acc: 100.0, f1: 100.0, r: 0.7946054503191511
06/02/2019 03:56:50 step: 1944, epoch: 58, batch: 29, loss: 0.0014777407050132751, acc: 100.0, f1: 100.0, r: 0.6391299237811167
06/02/2019 03:56:51 *** evaluating ***
06/02/2019 03:56:51 step: 59, epoch: 58, acc: 53.84615384615385, f1: 22.846573301617163, r: 0.2306968015127738
06/02/2019 03:56:51 *** epoch: 60 ***
06/02/2019 03:56:51 *** training ***
06/02/2019 03:56:51 step: 1952, epoch: 59, batch: 4, loss: 0.000609077513217926, acc: 100.0, f1: 100.0, r: 0.780447322626707
06/02/2019 03:56:51 step: 1957, epoch: 59, batch: 9, loss: 0.0014010965824127197, acc: 100.0, f1: 100.0, r: 0.6792207186350536
06/02/2019 03:56:52 step: 1962, epoch: 59, batch: 14, loss: 0.0013784989714622498, acc: 100.0, f1: 100.0, r: 0.7971337404426686
06/02/2019 03:56:52 step: 1967, epoch: 59, batch: 19, loss: 0.002146601676940918, acc: 100.0, f1: 100.0, r: 0.7364487702052861
06/02/2019 03:56:52 step: 1972, epoch: 59, batch: 24, loss: 0.001096375286579132, acc: 100.0, f1: 100.0, r: 0.8033371664217976
06/02/2019 03:56:53 step: 1977, epoch: 59, batch: 29, loss: 0.0017355158925056458, acc: 100.0, f1: 100.0, r: 0.7831881180367756
06/02/2019 03:56:53 *** evaluating ***
06/02/2019 03:56:53 step: 60, epoch: 59, acc: 52.56410256410257, f1: 22.054132591246407, r: 0.2282843585398605
06/02/2019 03:56:53 *** epoch: 61 ***
06/02/2019 03:56:53 *** training ***
06/02/2019 03:56:53 step: 1985, epoch: 60, batch: 4, loss: 0.0011265352368354797, acc: 100.0, f1: 100.0, r: 0.713595595959713
06/02/2019 03:56:54 step: 1990, epoch: 60, batch: 9, loss: 0.0015657618641853333, acc: 100.0, f1: 100.0, r: 0.6979823971750302
06/02/2019 03:56:54 step: 1995, epoch: 60, batch: 14, loss: 0.0015219375491142273, acc: 100.0, f1: 100.0, r: 0.7550542622046581
06/02/2019 03:56:55 step: 2000, epoch: 60, batch: 19, loss: 0.001076430082321167, acc: 100.0, f1: 100.0, r: 0.645247215397352
06/02/2019 03:56:55 step: 2005, epoch: 60, batch: 24, loss: 0.0011622309684753418, acc: 100.0, f1: 100.0, r: 0.6750410178397326
06/02/2019 03:56:55 step: 2010, epoch: 60, batch: 29, loss: 0.002390854060649872, acc: 100.0, f1: 100.0, r: 0.8508543458241902
06/02/2019 03:56:55 *** evaluating ***
06/02/2019 03:56:56 step: 61, epoch: 60, acc: 52.991452991452995, f1: 22.222380838038124, r: 0.23442882432076262
06/02/2019 03:56:56 *** epoch: 62 ***
06/02/2019 03:56:56 *** training ***
06/02/2019 03:56:56 step: 2018, epoch: 61, batch: 4, loss: 0.0008311569690704346, acc: 100.0, f1: 100.0, r: 0.8147468345343613
06/02/2019 03:56:56 step: 2023, epoch: 61, batch: 9, loss: 0.0007889196276664734, acc: 100.0, f1: 100.0, r: 0.7774106465308636
06/02/2019 03:56:57 step: 2028, epoch: 61, batch: 14, loss: 0.0019174963235855103, acc: 100.0, f1: 100.0, r: 0.8369044109257471
06/02/2019 03:56:57 step: 2033, epoch: 61, batch: 19, loss: 0.0023495107889175415, acc: 100.0, f1: 100.0, r: 0.6535023327871642
06/02/2019 03:56:57 step: 2038, epoch: 61, batch: 24, loss: 0.0012212768197059631, acc: 100.0, f1: 100.0, r: 0.6830591291732857
06/02/2019 03:56:58 step: 2043, epoch: 61, batch: 29, loss: 0.0016452893614768982, acc: 100.0, f1: 100.0, r: 0.7248385491834674
06/02/2019 03:56:58 *** evaluating ***
06/02/2019 03:56:58 step: 62, epoch: 61, acc: 52.991452991452995, f1: 23.00923559011623, r: 0.2388253831566595
06/02/2019 03:56:58 *** epoch: 63 ***
06/02/2019 03:56:58 *** training ***
06/02/2019 03:56:58 step: 2051, epoch: 62, batch: 4, loss: 0.0014387965202331543, acc: 100.0, f1: 100.0, r: 0.7972083210747256
06/02/2019 03:56:58 step: 2056, epoch: 62, batch: 9, loss: 0.0018435940146446228, acc: 100.0, f1: 100.0, r: 0.7767168584999063
06/02/2019 03:56:59 step: 2061, epoch: 62, batch: 14, loss: 0.0019124411046504974, acc: 100.0, f1: 100.0, r: 0.7284611793355764
06/02/2019 03:56:59 step: 2066, epoch: 62, batch: 19, loss: 0.002413451671600342, acc: 100.0, f1: 100.0, r: 0.727760209660517
06/02/2019 03:56:59 step: 2071, epoch: 62, batch: 24, loss: 0.0011301562190055847, acc: 100.0, f1: 100.0, r: 0.6973918926495121
06/02/2019 03:57:00 step: 2076, epoch: 62, batch: 29, loss: 0.0021467432379722595, acc: 100.0, f1: 100.0, r: 0.708147280908817
06/02/2019 03:57:00 *** evaluating ***
06/02/2019 03:57:00 step: 63, epoch: 62, acc: 52.56410256410257, f1: 22.787366533633556, r: 0.23734251095106731
06/02/2019 03:57:00 *** epoch: 64 ***
06/02/2019 03:57:00 *** training ***
06/02/2019 03:57:00 step: 2084, epoch: 63, batch: 4, loss: 0.0017827674746513367, acc: 100.0, f1: 100.0, r: 0.7483415086915792
06/02/2019 03:57:01 step: 2089, epoch: 63, batch: 9, loss: 0.0013411715626716614, acc: 100.0, f1: 100.0, r: 0.7068831224693817
06/02/2019 03:57:01 step: 2094, epoch: 63, batch: 14, loss: 0.0013086572289466858, acc: 100.0, f1: 100.0, r: 0.7101387171973692
06/02/2019 03:57:01 step: 2099, epoch: 63, batch: 19, loss: 0.0017280951142311096, acc: 100.0, f1: 100.0, r: 0.8186899140967171
06/02/2019 03:57:02 step: 2104, epoch: 63, batch: 24, loss: 0.000484466552734375, acc: 100.0, f1: 100.0, r: 0.7670489589952034
06/02/2019 03:57:02 step: 2109, epoch: 63, batch: 29, loss: 0.0015718117356300354, acc: 100.0, f1: 100.0, r: 0.7345030717946325
06/02/2019 03:57:02 *** evaluating ***
06/02/2019 03:57:03 step: 64, epoch: 63, acc: 52.56410256410257, f1: 22.586045241604374, r: 0.2411066026928998
06/02/2019 03:57:03 *** epoch: 65 ***
06/02/2019 03:57:03 *** training ***
06/02/2019 03:57:03 step: 2117, epoch: 64, batch: 4, loss: 0.0006173029541969299, acc: 100.0, f1: 100.0, r: 0.6909952621453297
06/02/2019 03:57:03 step: 2122, epoch: 64, batch: 9, loss: 0.001688539981842041, acc: 100.0, f1: 100.0, r: 0.7983529702450977
06/02/2019 03:57:04 step: 2127, epoch: 64, batch: 14, loss: 0.004290618002414703, acc: 100.0, f1: 100.0, r: 0.7101721912693446
06/02/2019 03:57:04 step: 2132, epoch: 64, batch: 19, loss: 0.0011231452226638794, acc: 100.0, f1: 100.0, r: 0.8401621208164382
06/02/2019 03:57:04 step: 2137, epoch: 64, batch: 24, loss: 0.001835741102695465, acc: 100.0, f1: 100.0, r: 0.7245803603929023
06/02/2019 03:57:04 step: 2142, epoch: 64, batch: 29, loss: 0.0025834739208221436, acc: 100.0, f1: 100.0, r: 0.7606457633363247
06/02/2019 03:57:05 *** evaluating ***
06/02/2019 03:57:05 step: 65, epoch: 64, acc: 54.700854700854705, f1: 23.324692308052704, r: 0.2425319785509202
06/02/2019 03:57:05 *** epoch: 66 ***
06/02/2019 03:57:05 *** training ***
06/02/2019 03:57:05 step: 2150, epoch: 65, batch: 4, loss: 0.0012176558375358582, acc: 100.0, f1: 100.0, r: 0.7231189460365353
06/02/2019 03:57:06 step: 2155, epoch: 65, batch: 9, loss: 0.001255922019481659, acc: 100.0, f1: 100.0, r: 0.7076600784435167
06/02/2019 03:57:06 step: 2160, epoch: 65, batch: 14, loss: 0.0026929080486297607, acc: 100.0, f1: 100.0, r: 0.7545156848302197
06/02/2019 03:57:06 step: 2165, epoch: 65, batch: 19, loss: 0.002049751579761505, acc: 100.0, f1: 100.0, r: 0.739404537650622
06/02/2019 03:57:07 step: 2170, epoch: 65, batch: 24, loss: 0.0014101341366767883, acc: 100.0, f1: 100.0, r: 0.7467898368046968
06/02/2019 03:57:07 step: 2175, epoch: 65, batch: 29, loss: 0.0019336119294166565, acc: 100.0, f1: 100.0, r: 0.8063620728234833
06/02/2019 03:57:07 *** evaluating ***
06/02/2019 03:57:07 step: 66, epoch: 65, acc: 52.56410256410257, f1: 22.286357623139967, r: 0.24251459197295563
06/02/2019 03:57:07 *** epoch: 67 ***
06/02/2019 03:57:07 *** training ***
06/02/2019 03:57:08 step: 2183, epoch: 66, batch: 4, loss: 0.0026376843452453613, acc: 100.0, f1: 100.0, r: 0.6946427914310762
06/02/2019 03:57:08 step: 2188, epoch: 66, batch: 9, loss: 0.0006437748670578003, acc: 100.0, f1: 100.0, r: 0.7720422281980214
06/02/2019 03:57:08 step: 2193, epoch: 66, batch: 14, loss: 0.0014932453632354736, acc: 100.0, f1: 100.0, r: 0.7412066559391486
06/02/2019 03:57:09 step: 2198, epoch: 66, batch: 19, loss: 0.0022740438580513, acc: 100.0, f1: 100.0, r: 0.7169011264699162
06/02/2019 03:57:09 step: 2203, epoch: 66, batch: 24, loss: 0.002355121076107025, acc: 100.0, f1: 100.0, r: 0.7081839251429688
06/02/2019 03:57:09 step: 2208, epoch: 66, batch: 29, loss: 0.002078317105770111, acc: 100.0, f1: 100.0, r: 0.8282223517728995
06/02/2019 03:57:09 *** evaluating ***
06/02/2019 03:57:10 step: 67, epoch: 66, acc: 53.84615384615385, f1: 23.38409994457799, r: 0.24342105180764867
06/02/2019 03:57:10 *** epoch: 68 ***
06/02/2019 03:57:10 *** training ***
06/02/2019 03:57:10 step: 2216, epoch: 67, batch: 4, loss: 0.002017192542552948, acc: 100.0, f1: 100.0, r: 0.6811313608830208
06/02/2019 03:57:10 step: 2221, epoch: 67, batch: 9, loss: 0.0015614032745361328, acc: 100.0, f1: 100.0, r: 0.783043859630185
06/02/2019 03:57:11 step: 2226, epoch: 67, batch: 14, loss: 0.0019668489694595337, acc: 100.0, f1: 100.0, r: 0.7708806016102424
06/02/2019 03:57:11 step: 2231, epoch: 67, batch: 19, loss: 0.0017293915152549744, acc: 100.0, f1: 100.0, r: 0.688651060331308
06/02/2019 03:57:11 step: 2236, epoch: 67, batch: 24, loss: 0.0014578551054000854, acc: 100.0, f1: 100.0, r: 0.49859960815012966
06/02/2019 03:57:12 step: 2241, epoch: 67, batch: 29, loss: 0.0014714151620864868, acc: 100.0, f1: 100.0, r: 0.796665803204152
06/02/2019 03:57:12 *** evaluating ***
06/02/2019 03:57:12 step: 68, epoch: 67, acc: 53.84615384615385, f1: 22.83983451536643, r: 0.23119795469920348
06/02/2019 03:57:12 *** epoch: 69 ***
06/02/2019 03:57:12 *** training ***
06/02/2019 03:57:12 step: 2249, epoch: 68, batch: 4, loss: 0.0038357675075531006, acc: 100.0, f1: 100.0, r: 0.7345617011235853
06/02/2019 03:57:13 step: 2254, epoch: 68, batch: 9, loss: 0.003887556493282318, acc: 100.0, f1: 100.0, r: 0.7457747310652747
06/02/2019 03:57:13 step: 2259, epoch: 68, batch: 14, loss: 0.0012776851654052734, acc: 100.0, f1: 100.0, r: 0.7458174356283411
06/02/2019 03:57:13 step: 2264, epoch: 68, batch: 19, loss: 0.0028034597635269165, acc: 100.0, f1: 100.0, r: 0.7610592826334456
06/02/2019 03:57:14 step: 2269, epoch: 68, batch: 24, loss: 0.0025777667760849, acc: 100.0, f1: 100.0, r: 0.7005106740404777
06/02/2019 03:57:14 step: 2274, epoch: 68, batch: 29, loss: 0.001738518476486206, acc: 100.0, f1: 100.0, r: 0.7646853327798665
06/02/2019 03:57:14 *** evaluating ***
06/02/2019 03:57:14 step: 69, epoch: 68, acc: 51.28205128205128, f1: 22.063782905601297, r: 0.23874149592716884
06/02/2019 03:57:14 *** epoch: 70 ***
06/02/2019 03:57:14 *** training ***
06/02/2019 03:57:15 step: 2282, epoch: 69, batch: 4, loss: 0.0016345903277397156, acc: 100.0, f1: 100.0, r: 0.7268027673934454
06/02/2019 03:57:15 step: 2287, epoch: 69, batch: 9, loss: 0.0034227222204208374, acc: 100.0, f1: 100.0, r: 0.8025866788486659
06/02/2019 03:57:15 step: 2292, epoch: 69, batch: 14, loss: 0.0018559545278549194, acc: 100.0, f1: 100.0, r: 0.6264936963392843
06/02/2019 03:57:16 step: 2297, epoch: 69, batch: 19, loss: 0.0023770704865455627, acc: 100.0, f1: 100.0, r: 0.7481414913852615
06/02/2019 03:57:16 step: 2302, epoch: 69, batch: 24, loss: 0.001060396432876587, acc: 100.0, f1: 100.0, r: 0.7616125552255972
06/02/2019 03:57:16 step: 2307, epoch: 69, batch: 29, loss: 0.0012142360210418701, acc: 100.0, f1: 100.0, r: 0.5843363419875993
06/02/2019 03:57:16 *** evaluating ***
06/02/2019 03:57:17 step: 70, epoch: 69, acc: 52.991452991452995, f1: 22.605528871320402, r: 0.23652987640342218
06/02/2019 03:57:17 *** epoch: 71 ***
06/02/2019 03:57:17 *** training ***
06/02/2019 03:57:17 step: 2315, epoch: 70, batch: 4, loss: 0.000648200511932373, acc: 100.0, f1: 100.0, r: 0.7361436184089585
06/02/2019 03:57:17 step: 2320, epoch: 70, batch: 9, loss: 0.0006854459643363953, acc: 100.0, f1: 100.0, r: 0.6328272552616935
06/02/2019 03:57:17 step: 2325, epoch: 70, batch: 14, loss: 0.002230040729045868, acc: 100.0, f1: 100.0, r: 0.7607304523917878
06/02/2019 03:57:18 step: 2330, epoch: 70, batch: 19, loss: 0.00173138827085495, acc: 100.0, f1: 100.0, r: 0.6966400430280073
06/02/2019 03:57:18 step: 2335, epoch: 70, batch: 24, loss: 0.0014892667531967163, acc: 100.0, f1: 100.0, r: 0.8258888762726908
06/02/2019 03:57:18 step: 2340, epoch: 70, batch: 29, loss: 0.002404928207397461, acc: 100.0, f1: 100.0, r: 0.6770924114169704
06/02/2019 03:57:19 *** evaluating ***
06/02/2019 03:57:19 step: 71, epoch: 70, acc: 51.70940170940172, f1: 22.11014145189677, r: 0.2321971194496126
06/02/2019 03:57:19 *** epoch: 72 ***
06/02/2019 03:57:19 *** training ***
06/02/2019 03:57:19 step: 2348, epoch: 71, batch: 4, loss: 0.0011066421866416931, acc: 100.0, f1: 100.0, r: 0.6965097701209695
06/02/2019 03:57:19 step: 2353, epoch: 71, batch: 9, loss: 0.0031474754214286804, acc: 100.0, f1: 100.0, r: 0.7281264423934055
06/02/2019 03:57:20 step: 2358, epoch: 71, batch: 14, loss: 0.0016270428895950317, acc: 100.0, f1: 100.0, r: 0.6995476851937837
06/02/2019 03:57:20 step: 2363, epoch: 71, batch: 19, loss: 0.0010957494378089905, acc: 100.0, f1: 100.0, r: 0.7086752197664649
06/02/2019 03:57:20 step: 2368, epoch: 71, batch: 24, loss: 0.0008900538086891174, acc: 100.0, f1: 100.0, r: 0.7233327651260593
06/02/2019 03:57:21 step: 2373, epoch: 71, batch: 29, loss: 0.0012557506561279297, acc: 100.0, f1: 100.0, r: 0.8388168101131684
06/02/2019 03:57:21 *** evaluating ***
06/02/2019 03:57:21 step: 72, epoch: 71, acc: 51.70940170940172, f1: 21.991150463776382, r: 0.23737446678570645
06/02/2019 03:57:21 *** epoch: 73 ***
06/02/2019 03:57:21 *** training ***
06/02/2019 03:57:21 step: 2381, epoch: 72, batch: 4, loss: 0.0010675489902496338, acc: 100.0, f1: 100.0, r: 0.7641824929581232
06/02/2019 03:57:22 step: 2386, epoch: 72, batch: 9, loss: 0.00044880807399749756, acc: 100.0, f1: 100.0, r: 0.8128276294606945
06/02/2019 03:57:22 step: 2391, epoch: 72, batch: 14, loss: 0.0012680068612098694, acc: 100.0, f1: 100.0, r: 0.6682952153024381
06/02/2019 03:57:22 step: 2396, epoch: 72, batch: 19, loss: 0.0009946897625923157, acc: 100.0, f1: 100.0, r: 0.7958330589883227
06/02/2019 03:57:23 step: 2401, epoch: 72, batch: 24, loss: 0.0011309534311294556, acc: 100.0, f1: 100.0, r: 0.8194977793189293
06/02/2019 03:57:23 step: 2406, epoch: 72, batch: 29, loss: 0.0008877143263816833, acc: 100.0, f1: 100.0, r: 0.6490332071395506
06/02/2019 03:57:23 *** evaluating ***
06/02/2019 03:57:23 step: 73, epoch: 72, acc: 52.56410256410257, f1: 22.370817996827963, r: 0.23790237452135993
06/02/2019 03:57:23 *** epoch: 74 ***
06/02/2019 03:57:23 *** training ***
06/02/2019 03:57:24 step: 2414, epoch: 73, batch: 4, loss: 0.000800400972366333, acc: 100.0, f1: 100.0, r: 0.8117226954733145
06/02/2019 03:57:24 step: 2419, epoch: 73, batch: 9, loss: 0.0012542977929115295, acc: 100.0, f1: 100.0, r: 0.7594813310574539
06/02/2019 03:57:24 step: 2424, epoch: 73, batch: 14, loss: 0.0020044073462486267, acc: 100.0, f1: 100.0, r: 0.6985618945960911
06/02/2019 03:57:25 step: 2429, epoch: 73, batch: 19, loss: 0.0017796307802200317, acc: 100.0, f1: 100.0, r: 0.7307540155506221
06/02/2019 03:57:25 step: 2434, epoch: 73, batch: 24, loss: 0.0017476975917816162, acc: 100.0, f1: 100.0, r: 0.6805877433244985
06/02/2019 03:57:25 step: 2439, epoch: 73, batch: 29, loss: 0.0027439147233963013, acc: 100.0, f1: 100.0, r: 0.7057746640080071
06/02/2019 03:57:25 *** evaluating ***
06/02/2019 03:57:25 step: 74, epoch: 73, acc: 52.991452991452995, f1: 23.03710116609733, r: 0.24479223101343006
06/02/2019 03:57:25 *** epoch: 75 ***
06/02/2019 03:57:25 *** training ***
06/02/2019 03:57:26 step: 2447, epoch: 74, batch: 4, loss: 0.0009238570928573608, acc: 100.0, f1: 100.0, r: 0.8033583401648813
06/02/2019 03:57:26 step: 2452, epoch: 74, batch: 9, loss: 0.0013105496764183044, acc: 100.0, f1: 100.0, r: 0.7775169478432322
06/02/2019 03:57:26 step: 2457, epoch: 74, batch: 14, loss: 0.0006169900298118591, acc: 100.0, f1: 100.0, r: 0.7140393044219387
06/02/2019 03:57:27 step: 2462, epoch: 74, batch: 19, loss: 0.002154141664505005, acc: 100.0, f1: 100.0, r: 0.7816158877045087
06/02/2019 03:57:27 step: 2467, epoch: 74, batch: 24, loss: 0.0011136308312416077, acc: 100.0, f1: 100.0, r: 0.7005471524295483
06/02/2019 03:57:27 step: 2472, epoch: 74, batch: 29, loss: 0.0013766810297966003, acc: 100.0, f1: 100.0, r: 0.8410924045586293
06/02/2019 03:57:27 *** evaluating ***
06/02/2019 03:57:28 step: 75, epoch: 74, acc: 51.70940170940172, f1: 22.148703724952373, r: 0.24227292973150177
06/02/2019 03:57:28 *** epoch: 76 ***
06/02/2019 03:57:28 *** training ***
06/02/2019 03:57:28 step: 2480, epoch: 75, batch: 4, loss: 0.0013439953327178955, acc: 100.0, f1: 100.0, r: 0.6890770181166483
06/02/2019 03:57:28 step: 2485, epoch: 75, batch: 9, loss: 0.001356370747089386, acc: 100.0, f1: 100.0, r: 0.8287370716060986
06/02/2019 03:57:29 step: 2490, epoch: 75, batch: 14, loss: 0.002789899706840515, acc: 100.0, f1: 100.0, r: 0.6871633859517114
06/02/2019 03:57:29 step: 2495, epoch: 75, batch: 19, loss: 0.0018624141812324524, acc: 100.0, f1: 100.0, r: 0.7930968761151403
06/02/2019 03:57:29 step: 2500, epoch: 75, batch: 24, loss: 0.004566080868244171, acc: 100.0, f1: 100.0, r: 0.7930353679802233
06/02/2019 03:57:30 step: 2505, epoch: 75, batch: 29, loss: 0.0009401962161064148, acc: 100.0, f1: 100.0, r: 0.7464367304405676
06/02/2019 03:57:30 *** evaluating ***
06/02/2019 03:57:30 step: 76, epoch: 75, acc: 53.84615384615385, f1: 24.486352890608213, r: 0.23913006734384118
06/02/2019 03:57:30 *** epoch: 77 ***
06/02/2019 03:57:30 *** training ***
06/02/2019 03:57:30 step: 2513, epoch: 76, batch: 4, loss: 0.0011875703930854797, acc: 100.0, f1: 100.0, r: 0.8006967317460567
06/02/2019 03:57:30 step: 2518, epoch: 76, batch: 9, loss: 0.0012223497033119202, acc: 100.0, f1: 100.0, r: 0.686903830144481
06/02/2019 03:57:31 step: 2523, epoch: 76, batch: 14, loss: 0.0007353425025939941, acc: 100.0, f1: 100.0, r: 0.6784539533523769
06/02/2019 03:57:31 step: 2528, epoch: 76, batch: 19, loss: 0.0006254985928535461, acc: 100.0, f1: 100.0, r: 0.7664240892766454
06/02/2019 03:57:31 step: 2533, epoch: 76, batch: 24, loss: 0.0017405375838279724, acc: 100.0, f1: 100.0, r: 0.8476264162100106
06/02/2019 03:57:32 step: 2538, epoch: 76, batch: 29, loss: 0.0012314841151237488, acc: 100.0, f1: 100.0, r: 0.687452311898154
06/02/2019 03:57:32 *** evaluating ***
06/02/2019 03:57:32 step: 77, epoch: 76, acc: 52.991452991452995, f1: 23.146463839158855, r: 0.24077340755977505
06/02/2019 03:57:32 *** epoch: 78 ***
06/02/2019 03:57:32 *** training ***
06/02/2019 03:57:32 step: 2546, epoch: 77, batch: 4, loss: 0.0012848526239395142, acc: 100.0, f1: 100.0, r: 0.7265692811846828
06/02/2019 03:57:33 step: 2551, epoch: 77, batch: 9, loss: 0.0016027912497520447, acc: 100.0, f1: 100.0, r: 0.6648438727512994
06/02/2019 03:57:33 step: 2556, epoch: 77, batch: 14, loss: 0.0018522441387176514, acc: 100.0, f1: 100.0, r: 0.7351093106329476
06/02/2019 03:57:33 step: 2561, epoch: 77, batch: 19, loss: 0.0037944093346595764, acc: 100.0, f1: 100.0, r: 0.6720084043857041
06/02/2019 03:57:34 step: 2566, epoch: 77, batch: 24, loss: 0.001316756010055542, acc: 100.0, f1: 100.0, r: 0.7092883472464053
06/02/2019 03:57:34 step: 2571, epoch: 77, batch: 29, loss: 0.0012737587094306946, acc: 100.0, f1: 100.0, r: 0.8047267697001594
06/02/2019 03:57:34 *** evaluating ***
06/02/2019 03:57:34 step: 78, epoch: 77, acc: 51.70940170940172, f1: 23.128224992283243, r: 0.21955283117576227
06/02/2019 03:57:34 *** epoch: 79 ***
06/02/2019 03:57:34 *** training ***
06/02/2019 03:57:35 step: 2579, epoch: 78, batch: 4, loss: 0.0009316504001617432, acc: 100.0, f1: 100.0, r: 0.7016401555832125
06/02/2019 03:57:35 step: 2584, epoch: 78, batch: 9, loss: 0.0013903528451919556, acc: 100.0, f1: 100.0, r: 0.6484046218487634
06/02/2019 03:57:35 step: 2589, epoch: 78, batch: 14, loss: 0.0009161755442619324, acc: 100.0, f1: 100.0, r: 0.8529216693017885
06/02/2019 03:57:35 step: 2594, epoch: 78, batch: 19, loss: 0.0014395788311958313, acc: 100.0, f1: 100.0, r: 0.7774937796350332
06/02/2019 03:57:36 step: 2599, epoch: 78, batch: 24, loss: 0.0012387186288833618, acc: 100.0, f1: 100.0, r: 0.7928234544060023
06/02/2019 03:57:36 step: 2604, epoch: 78, batch: 29, loss: 0.001214146614074707, acc: 100.0, f1: 100.0, r: 0.8136825628005923
06/02/2019 03:57:36 *** evaluating ***
06/02/2019 03:57:36 step: 79, epoch: 78, acc: 54.27350427350427, f1: 23.79110307455402, r: 0.22726153405223615
06/02/2019 03:57:36 *** epoch: 80 ***
06/02/2019 03:57:36 *** training ***
06/02/2019 03:57:36 step: 2612, epoch: 79, batch: 4, loss: 0.0018111616373062134, acc: 100.0, f1: 100.0, r: 0.6810481947953198
06/02/2019 03:57:37 step: 2617, epoch: 79, batch: 9, loss: 0.0006563514471054077, acc: 100.0, f1: 100.0, r: 0.7445672944488461
06/02/2019 03:57:37 step: 2622, epoch: 79, batch: 14, loss: 0.0007505565881729126, acc: 100.0, f1: 100.0, r: 0.7968589730575306
06/02/2019 03:57:37 step: 2627, epoch: 79, batch: 19, loss: 0.0018726810812950134, acc: 100.0, f1: 100.0, r: 0.6861468108354807
06/02/2019 03:57:38 step: 2632, epoch: 79, batch: 24, loss: 0.0026200711727142334, acc: 100.0, f1: 100.0, r: 0.8208654965949093
06/02/2019 03:57:38 step: 2637, epoch: 79, batch: 29, loss: 0.0024871304631233215, acc: 100.0, f1: 100.0, r: 0.7205151520889919
06/02/2019 03:57:38 *** evaluating ***
06/02/2019 03:57:38 step: 80, epoch: 79, acc: 52.991452991452995, f1: 23.441202300592366, r: 0.22959332012905104
06/02/2019 03:57:38 *** epoch: 81 ***
06/02/2019 03:57:38 *** training ***
06/02/2019 03:57:38 step: 2645, epoch: 80, batch: 4, loss: 0.0010419711470603943, acc: 100.0, f1: 100.0, r: 0.8288263311999444
06/02/2019 03:57:39 step: 2650, epoch: 80, batch: 9, loss: 0.0021484270691871643, acc: 100.0, f1: 100.0, r: 0.6728328828960832
06/02/2019 03:57:39 step: 2655, epoch: 80, batch: 14, loss: 0.0012760311365127563, acc: 100.0, f1: 100.0, r: 0.7118105088874396
06/02/2019 03:57:39 step: 2660, epoch: 80, batch: 19, loss: 0.00329764187335968, acc: 100.0, f1: 100.0, r: 0.7249052891253996
06/02/2019 03:57:40 step: 2665, epoch: 80, batch: 24, loss: 0.0009146407246589661, acc: 100.0, f1: 100.0, r: 0.728801732349502
06/02/2019 03:57:40 step: 2670, epoch: 80, batch: 29, loss: 0.0017867609858512878, acc: 100.0, f1: 100.0, r: 0.7563849249624761
06/02/2019 03:57:40 *** evaluating ***
06/02/2019 03:57:40 step: 81, epoch: 80, acc: 53.41880341880342, f1: 24.1420994202541, r: 0.2218351911834886
06/02/2019 03:57:40 *** epoch: 82 ***
06/02/2019 03:57:40 *** training ***
06/02/2019 03:57:41 step: 2678, epoch: 81, batch: 4, loss: 0.0011255443096160889, acc: 100.0, f1: 100.0, r: 0.7605777375680267
06/02/2019 03:57:41 step: 2683, epoch: 81, batch: 9, loss: 0.0012479498982429504, acc: 100.0, f1: 100.0, r: 0.7976950732322449
06/02/2019 03:57:41 step: 2688, epoch: 81, batch: 14, loss: 0.0011489912867546082, acc: 100.0, f1: 100.0, r: 0.7944934450015105
06/02/2019 03:57:41 step: 2693, epoch: 81, batch: 19, loss: 0.00248042494058609, acc: 100.0, f1: 100.0, r: 0.8109461847927041
06/02/2019 03:57:42 step: 2698, epoch: 81, batch: 24, loss: 0.003219418227672577, acc: 100.0, f1: 100.0, r: 0.6198751220527385
06/02/2019 03:57:42 step: 2703, epoch: 81, batch: 29, loss: 0.0018831565976142883, acc: 100.0, f1: 100.0, r: 0.6645147367977059
06/02/2019 03:57:42 *** evaluating ***
06/02/2019 03:57:42 step: 82, epoch: 81, acc: 55.12820512820513, f1: 24.512370927937905, r: 0.2349239040203868
06/02/2019 03:57:42 *** epoch: 83 ***
06/02/2019 03:57:42 *** training ***
06/02/2019 03:57:43 step: 2711, epoch: 82, batch: 4, loss: 0.0007752478122711182, acc: 100.0, f1: 100.0, r: 0.6963495086253195
06/02/2019 03:57:43 step: 2716, epoch: 82, batch: 9, loss: 0.0014768987894058228, acc: 100.0, f1: 100.0, r: 0.7898017554498193
06/02/2019 03:57:43 step: 2721, epoch: 82, batch: 14, loss: 0.0010158196091651917, acc: 100.0, f1: 100.0, r: 0.6951799270212777
06/02/2019 03:57:43 step: 2726, epoch: 82, batch: 19, loss: 0.0037354454398155212, acc: 100.0, f1: 100.0, r: 0.7293451471827872
06/02/2019 03:57:44 step: 2731, epoch: 82, batch: 24, loss: 0.0014614611864089966, acc: 100.0, f1: 100.0, r: 0.6559438541705213
06/02/2019 03:57:44 step: 2736, epoch: 82, batch: 29, loss: 0.0006483122706413269, acc: 100.0, f1: 100.0, r: 0.7197553227265766
06/02/2019 03:57:44 *** evaluating ***
06/02/2019 03:57:44 step: 83, epoch: 82, acc: 52.991452991452995, f1: 24.21106562687898, r: 0.24608006706260385
06/02/2019 03:57:44 *** epoch: 84 ***
06/02/2019 03:57:44 *** training ***
06/02/2019 03:57:45 step: 2744, epoch: 83, batch: 4, loss: 0.0006494000554084778, acc: 100.0, f1: 100.0, r: 0.7460968788195391
06/02/2019 03:57:45 step: 2749, epoch: 83, batch: 9, loss: 0.0012692064046859741, acc: 100.0, f1: 100.0, r: 0.8557815668607269
06/02/2019 03:57:45 step: 2754, epoch: 83, batch: 14, loss: 0.0009039342403411865, acc: 100.0, f1: 100.0, r: 0.7950458144629484
06/02/2019 03:57:46 step: 2759, epoch: 83, batch: 19, loss: 0.0030088797211647034, acc: 100.0, f1: 100.0, r: 0.7238209301698085
06/02/2019 03:57:46 step: 2764, epoch: 83, batch: 24, loss: 0.001189403235912323, acc: 100.0, f1: 100.0, r: 0.6507759219085484
06/02/2019 03:57:46 step: 2769, epoch: 83, batch: 29, loss: 0.0010133832693099976, acc: 100.0, f1: 100.0, r: 0.7062584804910927
06/02/2019 03:57:47 *** evaluating ***
06/02/2019 03:57:47 step: 84, epoch: 83, acc: 53.84615384615385, f1: 24.473816154958307, r: 0.2332003177987259
06/02/2019 03:57:47 *** epoch: 85 ***
06/02/2019 03:57:47 *** training ***
06/02/2019 03:57:47 step: 2777, epoch: 84, batch: 4, loss: 0.0030735135078430176, acc: 100.0, f1: 100.0, r: 0.7817998552955068
06/02/2019 03:57:47 step: 2782, epoch: 84, batch: 9, loss: 0.0022270455956459045, acc: 100.0, f1: 100.0, r: 0.7297863728720843
06/02/2019 03:57:48 step: 2787, epoch: 84, batch: 14, loss: 0.00305725634098053, acc: 100.0, f1: 100.0, r: 0.6788114702437352
06/02/2019 03:57:48 step: 2792, epoch: 84, batch: 19, loss: 0.0011977702379226685, acc: 100.0, f1: 100.0, r: 0.7376624724156304
06/02/2019 03:57:48 step: 2797, epoch: 84, batch: 24, loss: 0.0007565245032310486, acc: 100.0, f1: 100.0, r: 0.815571032866889
06/02/2019 03:57:49 step: 2802, epoch: 84, batch: 29, loss: 0.0005467385053634644, acc: 100.0, f1: 100.0, r: 0.8387126865662577
06/02/2019 03:57:49 *** evaluating ***
06/02/2019 03:57:49 step: 85, epoch: 84, acc: 52.13675213675214, f1: 24.22955538797123, r: 0.22886123823370866
06/02/2019 03:57:49 *** epoch: 86 ***
06/02/2019 03:57:49 *** training ***
06/02/2019 03:57:49 step: 2810, epoch: 85, batch: 4, loss: 0.003259986639022827, acc: 100.0, f1: 100.0, r: 0.688886571590088
06/02/2019 03:57:49 step: 2815, epoch: 85, batch: 9, loss: 0.0075236037373542786, acc: 100.0, f1: 100.0, r: 0.6827375655496934
06/02/2019 03:57:50 step: 2820, epoch: 85, batch: 14, loss: 0.004253275692462921, acc: 100.0, f1: 100.0, r: 0.6632976871087892
06/02/2019 03:57:50 step: 2825, epoch: 85, batch: 19, loss: 0.001925051212310791, acc: 100.0, f1: 100.0, r: 0.727638634344971
06/02/2019 03:57:50 step: 2830, epoch: 85, batch: 24, loss: 0.0013894438743591309, acc: 100.0, f1: 100.0, r: 0.8200307362033256
06/02/2019 03:57:51 step: 2835, epoch: 85, batch: 29, loss: 0.0023942366242408752, acc: 100.0, f1: 100.0, r: 0.7321735486144613
06/02/2019 03:57:51 *** evaluating ***
06/02/2019 03:57:51 step: 86, epoch: 85, acc: 52.56410256410257, f1: 24.59408437282322, r: 0.23559960478155229
06/02/2019 03:57:51 *** epoch: 87 ***
06/02/2019 03:57:51 *** training ***
06/02/2019 03:57:51 step: 2843, epoch: 86, batch: 4, loss: 0.0033340752124786377, acc: 100.0, f1: 100.0, r: 0.7389150410501927
06/02/2019 03:57:52 step: 2848, epoch: 86, batch: 9, loss: 0.0015369802713394165, acc: 100.0, f1: 100.0, r: 0.7836518545435238
06/02/2019 03:57:52 step: 2853, epoch: 86, batch: 14, loss: 0.001016959547996521, acc: 100.0, f1: 100.0, r: 0.6820323549136074
06/02/2019 03:57:52 step: 2858, epoch: 86, batch: 19, loss: 0.002501867711544037, acc: 100.0, f1: 100.0, r: 0.7060913331326502
06/02/2019 03:57:53 step: 2863, epoch: 86, batch: 24, loss: 0.0018458440899848938, acc: 100.0, f1: 100.0, r: 0.7707209313613114
06/02/2019 03:57:53 step: 2868, epoch: 86, batch: 29, loss: 0.0017535090446472168, acc: 100.0, f1: 100.0, r: 0.7681251334030909
06/02/2019 03:57:53 *** evaluating ***
06/02/2019 03:57:53 step: 87, epoch: 86, acc: 51.70940170940172, f1: 22.571154835780728, r: 0.23149012660143484
06/02/2019 03:57:53 *** epoch: 88 ***
06/02/2019 03:57:53 *** training ***
06/02/2019 03:57:54 step: 2876, epoch: 87, batch: 4, loss: 0.0007123425602912903, acc: 100.0, f1: 100.0, r: 0.7292082748044103
06/02/2019 03:57:54 step: 2881, epoch: 87, batch: 9, loss: 0.00141829252243042, acc: 100.0, f1: 100.0, r: 0.8127310099826692
06/02/2019 03:57:54 step: 2886, epoch: 87, batch: 14, loss: 0.0014580115675926208, acc: 100.0, f1: 100.0, r: 0.7808490128568681
06/02/2019 03:57:54 step: 2891, epoch: 87, batch: 19, loss: 0.0024176090955734253, acc: 100.0, f1: 100.0, r: 0.740353866015119
06/02/2019 03:57:55 step: 2896, epoch: 87, batch: 24, loss: 0.0012609511613845825, acc: 100.0, f1: 100.0, r: 0.6328569670097816
06/02/2019 03:57:55 step: 2901, epoch: 87, batch: 29, loss: 0.0011195391416549683, acc: 100.0, f1: 100.0, r: 0.7401755697250474
06/02/2019 03:57:55 *** evaluating ***
06/02/2019 03:57:55 step: 88, epoch: 87, acc: 49.14529914529914, f1: 23.800653394599003, r: 0.24361402690714024
06/02/2019 03:57:55 *** epoch: 89 ***
06/02/2019 03:57:55 *** training ***
06/02/2019 03:57:56 step: 2909, epoch: 88, batch: 4, loss: 0.001569986343383789, acc: 100.0, f1: 100.0, r: 0.7436994459400403
06/02/2019 03:57:56 step: 2914, epoch: 88, batch: 9, loss: 0.0031515508890151978, acc: 100.0, f1: 100.0, r: 0.7572231905261195
06/02/2019 03:57:56 step: 2919, epoch: 88, batch: 14, loss: 0.0010267272591590881, acc: 100.0, f1: 100.0, r: 0.6272118684488951
06/02/2019 03:57:56 step: 2924, epoch: 88, batch: 19, loss: 0.00259988009929657, acc: 100.0, f1: 100.0, r: 0.7883000198748078
06/02/2019 03:57:57 step: 2929, epoch: 88, batch: 24, loss: 0.0007861629128456116, acc: 100.0, f1: 100.0, r: 0.782660819535011
06/02/2019 03:57:57 step: 2934, epoch: 88, batch: 29, loss: 0.0014122501015663147, acc: 100.0, f1: 100.0, r: 0.7760870702800029
06/02/2019 03:57:57 *** evaluating ***
06/02/2019 03:57:57 step: 89, epoch: 88, acc: 52.13675213675214, f1: 22.72025899252597, r: 0.24288479581235786
06/02/2019 03:57:57 *** epoch: 90 ***
06/02/2019 03:57:57 *** training ***
06/02/2019 03:57:58 step: 2942, epoch: 89, batch: 4, loss: 0.0019743070006370544, acc: 100.0, f1: 100.0, r: 0.8135148375136166
06/02/2019 03:57:58 step: 2947, epoch: 89, batch: 9, loss: 0.0009389147162437439, acc: 100.0, f1: 100.0, r: 0.7003062911102982
06/02/2019 03:57:58 step: 2952, epoch: 89, batch: 14, loss: 0.0006587952375411987, acc: 100.0, f1: 100.0, r: 0.784845296409297
06/02/2019 03:57:59 step: 2957, epoch: 89, batch: 19, loss: 0.0013550743460655212, acc: 100.0, f1: 100.0, r: 0.8296700097414168
06/02/2019 03:57:59 step: 2962, epoch: 89, batch: 24, loss: 0.0012592077255249023, acc: 100.0, f1: 100.0, r: 0.6111103039542566
06/02/2019 03:57:59 step: 2967, epoch: 89, batch: 29, loss: 0.001235179603099823, acc: 100.0, f1: 100.0, r: 0.7789829607712244
06/02/2019 03:57:59 *** evaluating ***
06/02/2019 03:57:59 step: 90, epoch: 89, acc: 53.41880341880342, f1: 23.31376832647953, r: 0.24987375404379714
06/02/2019 03:57:59 *** epoch: 91 ***
06/02/2019 03:57:59 *** training ***
06/02/2019 03:58:00 step: 2975, epoch: 90, batch: 4, loss: 0.0009440779685974121, acc: 100.0, f1: 100.0, r: 0.6701904734836355
06/02/2019 03:58:00 step: 2980, epoch: 90, batch: 9, loss: 0.0012635812163352966, acc: 100.0, f1: 100.0, r: 0.6997805640465878
06/02/2019 03:58:00 step: 2985, epoch: 90, batch: 14, loss: 0.0023635104298591614, acc: 100.0, f1: 100.0, r: 0.7879225674012316
06/02/2019 03:58:01 step: 2990, epoch: 90, batch: 19, loss: 0.0007881522178649902, acc: 100.0, f1: 100.0, r: 0.8664136302878463
06/02/2019 03:58:01 step: 2995, epoch: 90, batch: 24, loss: 0.0012128502130508423, acc: 100.0, f1: 100.0, r: 0.6904687416414379
06/02/2019 03:58:01 step: 3000, epoch: 90, batch: 29, loss: 0.0015154406428337097, acc: 100.0, f1: 100.0, r: 0.832132617423869
06/02/2019 03:58:01 *** evaluating ***
06/02/2019 03:58:02 step: 91, epoch: 90, acc: 54.27350427350427, f1: 25.99950396825397, r: 0.24835164697553191
06/02/2019 03:58:02 *** epoch: 92 ***
06/02/2019 03:58:02 *** training ***
06/02/2019 03:58:02 step: 3008, epoch: 91, batch: 4, loss: 0.0028043463826179504, acc: 100.0, f1: 100.0, r: 0.6800035548449218
06/02/2019 03:58:02 step: 3013, epoch: 91, batch: 9, loss: 0.0007984563708305359, acc: 100.0, f1: 100.0, r: 0.5869756638484127
06/02/2019 03:58:02 step: 3018, epoch: 91, batch: 14, loss: 0.0011320635676383972, acc: 100.0, f1: 100.0, r: 0.6940641538578671
06/02/2019 03:58:03 step: 3023, epoch: 91, batch: 19, loss: 0.002890363335609436, acc: 100.0, f1: 100.0, r: 0.7013193576819329
06/02/2019 03:58:03 step: 3028, epoch: 91, batch: 24, loss: 0.0005289316177368164, acc: 100.0, f1: 100.0, r: 0.7227874793807493
06/02/2019 03:58:03 step: 3033, epoch: 91, batch: 29, loss: 0.0014580413699150085, acc: 100.0, f1: 100.0, r: 0.7404956604191395
06/02/2019 03:58:04 *** evaluating ***
06/02/2019 03:58:04 step: 92, epoch: 91, acc: 47.43589743589743, f1: 20.59578324700278, r: 0.24363830401385392
06/02/2019 03:58:04 *** epoch: 93 ***
06/02/2019 03:58:04 *** training ***
06/02/2019 03:58:04 step: 3041, epoch: 92, batch: 4, loss: 0.0009113401174545288, acc: 100.0, f1: 100.0, r: 0.8029155336247793
06/02/2019 03:58:04 step: 3046, epoch: 92, batch: 9, loss: 0.002121739089488983, acc: 100.0, f1: 100.0, r: 0.7784040218839814
06/02/2019 03:58:05 step: 3051, epoch: 92, batch: 14, loss: 0.0025384724140167236, acc: 100.0, f1: 100.0, r: 0.7295422515331778
06/02/2019 03:58:05 step: 3056, epoch: 92, batch: 19, loss: 0.0010421425104141235, acc: 100.0, f1: 100.0, r: 0.6740072829148511
06/02/2019 03:58:05 step: 3061, epoch: 92, batch: 24, loss: 0.002459011971950531, acc: 100.0, f1: 100.0, r: 0.6684809591071439
06/02/2019 03:58:06 step: 3066, epoch: 92, batch: 29, loss: 0.0011664479970932007, acc: 100.0, f1: 100.0, r: 0.6698278711316445
06/02/2019 03:58:06 *** evaluating ***
06/02/2019 03:58:06 step: 93, epoch: 92, acc: 52.13675213675214, f1: 24.813267178899434, r: 0.24769745804064577
06/02/2019 03:58:06 *** epoch: 94 ***
06/02/2019 03:58:06 *** training ***
06/02/2019 03:58:06 step: 3074, epoch: 93, batch: 4, loss: 0.004553437232971191, acc: 100.0, f1: 100.0, r: 0.8283701758488312
06/02/2019 03:58:07 step: 3079, epoch: 93, batch: 9, loss: 0.0008025765419006348, acc: 100.0, f1: 100.0, r: 0.797517446567109
06/02/2019 03:58:07 step: 3084, epoch: 93, batch: 14, loss: 0.0020436719059944153, acc: 100.0, f1: 100.0, r: 0.793537291354731
06/02/2019 03:58:07 step: 3089, epoch: 93, batch: 19, loss: 0.001605793833732605, acc: 100.0, f1: 100.0, r: 0.6802496695101405
06/02/2019 03:58:08 step: 3094, epoch: 93, batch: 24, loss: 0.0010879933834075928, acc: 100.0, f1: 100.0, r: 0.6605354897192228
06/02/2019 03:58:08 step: 3099, epoch: 93, batch: 29, loss: 0.0008273869752883911, acc: 100.0, f1: 100.0, r: 0.843117191333265
06/02/2019 03:58:08 *** evaluating ***
06/02/2019 03:58:08 step: 94, epoch: 93, acc: 54.700854700854705, f1: 25.034064433245508, r: 0.24276486470563796
06/02/2019 03:58:08 *** epoch: 95 ***
06/02/2019 03:58:08 *** training ***
06/02/2019 03:58:09 step: 3107, epoch: 94, batch: 4, loss: 0.001516878604888916, acc: 100.0, f1: 100.0, r: 0.673963355046797
06/02/2019 03:58:09 step: 3112, epoch: 94, batch: 9, loss: 0.003549136221408844, acc: 100.0, f1: 100.0, r: 0.5028841912366261
06/02/2019 03:58:09 step: 3117, epoch: 94, batch: 14, loss: 0.002689950168132782, acc: 100.0, f1: 100.0, r: 0.6432009360102873
06/02/2019 03:58:10 step: 3122, epoch: 94, batch: 19, loss: 0.001261100172996521, acc: 100.0, f1: 100.0, r: 0.7309830465198304
06/02/2019 03:58:10 step: 3127, epoch: 94, batch: 24, loss: 0.0018259510397911072, acc: 100.0, f1: 100.0, r: 0.751491942265772
06/02/2019 03:58:10 step: 3132, epoch: 94, batch: 29, loss: 0.001374661922454834, acc: 100.0, f1: 100.0, r: 0.823981884109435
06/02/2019 03:58:11 *** evaluating ***
06/02/2019 03:58:11 step: 95, epoch: 94, acc: 52.56410256410257, f1: 24.05897806336611, r: 0.2449187004284325
06/02/2019 03:58:11 *** epoch: 96 ***
06/02/2019 03:58:11 *** training ***
06/02/2019 03:58:11 step: 3140, epoch: 95, batch: 4, loss: 0.0028000622987747192, acc: 100.0, f1: 100.0, r: 0.7026112053975245
06/02/2019 03:58:11 step: 3145, epoch: 95, batch: 9, loss: 0.0008472353219985962, acc: 100.0, f1: 100.0, r: 0.712826176897444
06/02/2019 03:58:12 step: 3150, epoch: 95, batch: 14, loss: 0.0009608864784240723, acc: 100.0, f1: 100.0, r: 0.7131690186140317
06/02/2019 03:58:12 step: 3155, epoch: 95, batch: 19, loss: 0.0009936913847923279, acc: 100.0, f1: 100.0, r: 0.7774360740317218
06/02/2019 03:58:12 step: 3160, epoch: 95, batch: 24, loss: 0.0007846802473068237, acc: 100.0, f1: 100.0, r: 0.840824695345671
06/02/2019 03:58:13 step: 3165, epoch: 95, batch: 29, loss: 0.0017351657152175903, acc: 100.0, f1: 100.0, r: 0.7992879415107648
06/02/2019 03:58:13 *** evaluating ***
06/02/2019 03:58:13 step: 96, epoch: 95, acc: 48.717948717948715, f1: 19.589240752215435, r: 0.2414379192056618
06/02/2019 03:58:13 *** epoch: 97 ***
06/02/2019 03:58:13 *** training ***
06/02/2019 03:58:13 step: 3173, epoch: 96, batch: 4, loss: 0.000607319176197052, acc: 100.0, f1: 100.0, r: 0.6812642652569156
06/02/2019 03:58:14 step: 3178, epoch: 96, batch: 9, loss: 0.002049356698989868, acc: 100.0, f1: 100.0, r: 0.8479177040751656
06/02/2019 03:58:14 step: 3183, epoch: 96, batch: 14, loss: 0.0019224733114242554, acc: 100.0, f1: 100.0, r: 0.7022824229165625
06/02/2019 03:58:14 step: 3188, epoch: 96, batch: 19, loss: 0.0011606588959693909, acc: 100.0, f1: 100.0, r: 0.7049651828995375
06/02/2019 03:58:15 step: 3193, epoch: 96, batch: 24, loss: 0.0009462907910346985, acc: 100.0, f1: 100.0, r: 0.7239507897791526
06/02/2019 03:58:15 step: 3198, epoch: 96, batch: 29, loss: 0.0007254257798194885, acc: 100.0, f1: 100.0, r: 0.7115092368228474
06/02/2019 03:58:15 *** evaluating ***
06/02/2019 03:58:15 step: 97, epoch: 96, acc: 51.70940170940172, f1: 21.195747552663647, r: 0.2365121238791306
06/02/2019 03:58:15 *** epoch: 98 ***
06/02/2019 03:58:15 *** training ***
06/02/2019 03:58:15 step: 3206, epoch: 97, batch: 4, loss: 0.0006481856107711792, acc: 100.0, f1: 100.0, r: 0.6755884933245425
06/02/2019 03:58:16 step: 3211, epoch: 97, batch: 9, loss: 0.0010982006788253784, acc: 100.0, f1: 100.0, r: 0.7549907251092196
06/02/2019 03:58:16 step: 3216, epoch: 97, batch: 14, loss: 0.00190763920545578, acc: 100.0, f1: 100.0, r: 0.5983476581192894
06/02/2019 03:58:16 step: 3221, epoch: 97, batch: 19, loss: 0.0015371814370155334, acc: 100.0, f1: 100.0, r: 0.8445705474394035
06/02/2019 03:58:16 step: 3226, epoch: 97, batch: 24, loss: 0.0012900903820991516, acc: 100.0, f1: 100.0, r: 0.7972228824579203
06/02/2019 03:58:17 step: 3231, epoch: 97, batch: 29, loss: 0.0013568848371505737, acc: 100.0, f1: 100.0, r: 0.8235631672224055
06/02/2019 03:58:17 *** evaluating ***
06/02/2019 03:58:17 step: 98, epoch: 97, acc: 50.427350427350426, f1: 21.824054048788092, r: 0.23096833863822377
06/02/2019 03:58:17 *** epoch: 99 ***
06/02/2019 03:58:17 *** training ***
06/02/2019 03:58:17 step: 3239, epoch: 98, batch: 4, loss: 0.002608783543109894, acc: 100.0, f1: 100.0, r: 0.7936927441227607
06/02/2019 03:58:18 step: 3244, epoch: 98, batch: 9, loss: 0.0019499287009239197, acc: 100.0, f1: 100.0, r: 0.7042794661412504
06/02/2019 03:58:18 step: 3249, epoch: 98, batch: 14, loss: 0.00184553861618042, acc: 100.0, f1: 100.0, r: 0.612440818610533
06/02/2019 03:58:18 step: 3254, epoch: 98, batch: 19, loss: 0.0016322135925292969, acc: 100.0, f1: 100.0, r: 0.7161610734142351
06/02/2019 03:58:19 step: 3259, epoch: 98, batch: 24, loss: 0.002216324210166931, acc: 100.0, f1: 100.0, r: 0.7195204688479993
06/02/2019 03:58:19 step: 3264, epoch: 98, batch: 29, loss: 0.001585155725479126, acc: 100.0, f1: 100.0, r: 0.7896255607966229
06/02/2019 03:58:19 *** evaluating ***
06/02/2019 03:58:19 step: 99, epoch: 98, acc: 52.13675213675214, f1: 22.911944599584373, r: 0.23603399502676586
06/02/2019 03:58:19 *** epoch: 100 ***
06/02/2019 03:58:19 *** training ***
06/02/2019 03:58:20 step: 3272, epoch: 99, batch: 4, loss: 0.0008293390274047852, acc: 100.0, f1: 100.0, r: 0.7856395322212666
06/02/2019 03:58:20 step: 3277, epoch: 99, batch: 9, loss: 0.0004757493734359741, acc: 100.0, f1: 100.0, r: 0.8181050537115975
06/02/2019 03:58:20 step: 3282, epoch: 99, batch: 14, loss: 0.0009269937872886658, acc: 100.0, f1: 100.0, r: 0.8500131677583731
06/02/2019 03:58:21 step: 3287, epoch: 99, batch: 19, loss: 0.002257987856864929, acc: 100.0, f1: 100.0, r: 0.6435428237568717
06/02/2019 03:58:21 step: 3292, epoch: 99, batch: 24, loss: 0.0017390847206115723, acc: 100.0, f1: 100.0, r: 0.5710582341112325
06/02/2019 03:58:21 step: 3297, epoch: 99, batch: 29, loss: 0.0030752867460250854, acc: 100.0, f1: 100.0, r: 0.6730558380186288
06/02/2019 03:58:21 *** evaluating ***
06/02/2019 03:58:21 step: 100, epoch: 99, acc: 52.991452991452995, f1: 21.660028244558603, r: 0.23756446401969916
06/02/2019 03:58:21 *** epoch: 101 ***
06/02/2019 03:58:21 *** training ***
06/02/2019 03:58:22 step: 3305, epoch: 100, batch: 4, loss: 0.0008339658379554749, acc: 100.0, f1: 100.0, r: 0.7608430551982955
06/02/2019 03:58:22 step: 3310, epoch: 100, batch: 9, loss: 0.0005183815956115723, acc: 100.0, f1: 100.0, r: 0.7405900898121699
06/02/2019 03:58:22 step: 3315, epoch: 100, batch: 14, loss: 0.0008122101426124573, acc: 100.0, f1: 100.0, r: 0.7814420623941148
06/02/2019 03:58:23 step: 3320, epoch: 100, batch: 19, loss: 0.0008792653679847717, acc: 100.0, f1: 100.0, r: 0.6585641003304358
06/02/2019 03:58:23 step: 3325, epoch: 100, batch: 24, loss: 0.0006616562604904175, acc: 100.0, f1: 100.0, r: 0.7454450044577258
06/02/2019 03:58:23 step: 3330, epoch: 100, batch: 29, loss: 0.0008844062685966492, acc: 100.0, f1: 100.0, r: 0.6228946589614046
06/02/2019 03:58:23 *** evaluating ***
06/02/2019 03:58:24 step: 101, epoch: 100, acc: 54.27350427350427, f1: 23.648717258300408, r: 0.23949671396818603
06/02/2019 03:58:24 *** epoch: 102 ***
06/02/2019 03:58:24 *** training ***
06/02/2019 03:58:24 step: 3338, epoch: 101, batch: 4, loss: 0.0020754188299179077, acc: 100.0, f1: 100.0, r: 0.7578434077013892
06/02/2019 03:58:24 step: 3343, epoch: 101, batch: 9, loss: 0.0018731504678726196, acc: 100.0, f1: 100.0, r: 0.8037863665046008
06/02/2019 03:58:24 step: 3348, epoch: 101, batch: 14, loss: 0.0008673593401908875, acc: 100.0, f1: 100.0, r: 0.6637939826932638
06/02/2019 03:58:25 step: 3353, epoch: 101, batch: 19, loss: 0.0018491968512535095, acc: 100.0, f1: 100.0, r: 0.6599465637604454
06/02/2019 03:58:25 step: 3358, epoch: 101, batch: 24, loss: 0.0011321306228637695, acc: 100.0, f1: 100.0, r: 0.721417816643897
06/02/2019 03:58:25 step: 3363, epoch: 101, batch: 29, loss: 0.006352029740810394, acc: 100.0, f1: 100.0, r: 0.6650780408133624
06/02/2019 03:58:26 *** evaluating ***
06/02/2019 03:58:26 step: 102, epoch: 101, acc: 52.13675213675214, f1: 25.54981589343569, r: 0.2557646526285823
06/02/2019 03:58:26 *** epoch: 103 ***
06/02/2019 03:58:26 *** training ***
06/02/2019 03:58:26 step: 3371, epoch: 102, batch: 4, loss: 0.003462061285972595, acc: 100.0, f1: 100.0, r: 0.7679166915435676
06/02/2019 03:58:26 step: 3376, epoch: 102, batch: 9, loss: 0.0017372742295265198, acc: 100.0, f1: 100.0, r: 0.7228507854745393
06/02/2019 03:58:27 step: 3381, epoch: 102, batch: 14, loss: 0.000761277973651886, acc: 100.0, f1: 100.0, r: 0.7102205055325583
06/02/2019 03:58:27 step: 3386, epoch: 102, batch: 19, loss: 0.0018034502863883972, acc: 100.0, f1: 100.0, r: 0.7153835646356418
06/02/2019 03:58:27 step: 3391, epoch: 102, batch: 24, loss: 0.0005965307354927063, acc: 100.0, f1: 100.0, r: 0.7109765888111025
06/02/2019 03:58:28 step: 3396, epoch: 102, batch: 29, loss: 0.003599494695663452, acc: 100.0, f1: 100.0, r: 0.7808772350390444
06/02/2019 03:58:28 *** evaluating ***
06/02/2019 03:58:28 step: 103, epoch: 102, acc: 50.85470085470085, f1: 23.48881314539546, r: 0.23438831515060432
06/02/2019 03:58:28 *** epoch: 104 ***
06/02/2019 03:58:28 *** training ***
06/02/2019 03:58:28 step: 3404, epoch: 103, batch: 4, loss: 0.0017428025603294373, acc: 100.0, f1: 100.0, r: 0.7310360123828548
06/02/2019 03:58:29 step: 3409, epoch: 103, batch: 9, loss: 0.0005475059151649475, acc: 100.0, f1: 100.0, r: 0.7085186862955607
06/02/2019 03:58:29 step: 3414, epoch: 103, batch: 14, loss: 0.003658868372440338, acc: 100.0, f1: 100.0, r: 0.7087990200391918
06/02/2019 03:58:29 step: 3419, epoch: 103, batch: 19, loss: 0.0008742883801460266, acc: 100.0, f1: 100.0, r: 0.7203982560635922
06/02/2019 03:58:30 step: 3424, epoch: 103, batch: 24, loss: 0.0015887990593910217, acc: 100.0, f1: 100.0, r: 0.6885717233349364
06/02/2019 03:58:30 step: 3429, epoch: 103, batch: 29, loss: 0.0025605708360671997, acc: 100.0, f1: 100.0, r: 0.6939502231366046
06/02/2019 03:58:30 *** evaluating ***
06/02/2019 03:58:30 step: 104, epoch: 103, acc: 53.84615384615385, f1: 24.39821131624863, r: 0.23828937969713543
06/02/2019 03:58:30 *** epoch: 105 ***
06/02/2019 03:58:30 *** training ***
06/02/2019 03:58:30 step: 3437, epoch: 104, batch: 4, loss: 0.007743239402770996, acc: 100.0, f1: 100.0, r: 0.8283198050546048
06/02/2019 03:58:31 step: 3442, epoch: 104, batch: 9, loss: 0.0006936118006706238, acc: 100.0, f1: 100.0, r: 0.8391388978905592
06/02/2019 03:58:31 step: 3447, epoch: 104, batch: 14, loss: 0.0012951195240020752, acc: 100.0, f1: 100.0, r: 0.7148063635841296
06/02/2019 03:58:32 step: 3452, epoch: 104, batch: 19, loss: 0.0012770593166351318, acc: 100.0, f1: 100.0, r: 0.7874969674385124
06/02/2019 03:58:32 step: 3457, epoch: 104, batch: 24, loss: 0.0004918128252029419, acc: 100.0, f1: 100.0, r: 0.6503822508684843
06/02/2019 03:58:32 step: 3462, epoch: 104, batch: 29, loss: 0.0007799491286277771, acc: 100.0, f1: 100.0, r: 0.7526086365606514
06/02/2019 03:58:32 *** evaluating ***
06/02/2019 03:58:33 step: 105, epoch: 104, acc: 54.27350427350427, f1: 23.315616796901942, r: 0.2407104412091356
06/02/2019 03:58:33 *** epoch: 106 ***
06/02/2019 03:58:33 *** training ***
06/02/2019 03:58:33 step: 3470, epoch: 105, batch: 4, loss: 0.0016889199614524841, acc: 100.0, f1: 100.0, r: 0.8528384182304678
06/02/2019 03:58:33 step: 3475, epoch: 105, batch: 9, loss: 0.0009952634572982788, acc: 100.0, f1: 100.0, r: 0.8035191708565863
06/02/2019 03:58:34 step: 3480, epoch: 105, batch: 14, loss: 0.0008159354329109192, acc: 100.0, f1: 100.0, r: 0.77387392846136
06/02/2019 03:58:34 step: 3485, epoch: 105, batch: 19, loss: 0.0005655288696289062, acc: 100.0, f1: 100.0, r: 0.7391853424663506
06/02/2019 03:58:34 step: 3490, epoch: 105, batch: 24, loss: 0.0015356317162513733, acc: 100.0, f1: 100.0, r: 0.6946523331853244
06/02/2019 03:58:35 step: 3495, epoch: 105, batch: 29, loss: 0.0009062141180038452, acc: 100.0, f1: 100.0, r: 0.7841251275946238
06/02/2019 03:58:35 *** evaluating ***
06/02/2019 03:58:35 step: 106, epoch: 105, acc: 53.41880341880342, f1: 24.548206164912372, r: 0.2383764768827244
06/02/2019 03:58:35 *** epoch: 107 ***
06/02/2019 03:58:35 *** training ***
06/02/2019 03:58:35 step: 3503, epoch: 106, batch: 4, loss: 0.0008361116051673889, acc: 100.0, f1: 100.0, r: 0.766168175093908
06/02/2019 03:58:35 step: 3508, epoch: 106, batch: 9, loss: 0.0016492903232574463, acc: 100.0, f1: 100.0, r: 0.7471481910552376
06/02/2019 03:58:36 step: 3513, epoch: 106, batch: 14, loss: 0.0020506680011749268, acc: 100.0, f1: 100.0, r: 0.7928302473319145
06/02/2019 03:58:36 step: 3518, epoch: 106, batch: 19, loss: 0.0008363425731658936, acc: 100.0, f1: 100.0, r: 0.7950008802463038
06/02/2019 03:58:36 step: 3523, epoch: 106, batch: 24, loss: 0.0015549957752227783, acc: 100.0, f1: 100.0, r: 0.7856156811762443
06/02/2019 03:58:37 step: 3528, epoch: 106, batch: 29, loss: 0.0009174644947052002, acc: 100.0, f1: 100.0, r: 0.6962418370859319
06/02/2019 03:58:37 *** evaluating ***
06/02/2019 03:58:37 step: 107, epoch: 106, acc: 52.56410256410257, f1: 24.104467652537366, r: 0.24129759887115418
06/02/2019 03:58:37 *** epoch: 108 ***
06/02/2019 03:58:37 *** training ***
06/02/2019 03:58:37 step: 3536, epoch: 107, batch: 4, loss: 0.0016800612211227417, acc: 100.0, f1: 100.0, r: 0.7583430774767052
06/02/2019 03:58:37 step: 3541, epoch: 107, batch: 9, loss: 0.002513214945793152, acc: 100.0, f1: 100.0, r: 0.8402891716324934
06/02/2019 03:58:38 step: 3546, epoch: 107, batch: 14, loss: 0.0016323104500770569, acc: 100.0, f1: 100.0, r: 0.721276377097703
06/02/2019 03:58:38 step: 3551, epoch: 107, batch: 19, loss: 0.0012424886226654053, acc: 100.0, f1: 100.0, r: 0.795481878307192
06/02/2019 03:58:38 step: 3556, epoch: 107, batch: 24, loss: 0.004319444298744202, acc: 100.0, f1: 100.0, r: 0.7394165412032877
06/02/2019 03:58:39 step: 3561, epoch: 107, batch: 29, loss: 0.0010038018226623535, acc: 100.0, f1: 100.0, r: 0.8288958417085137
06/02/2019 03:58:39 *** evaluating ***
06/02/2019 03:58:39 step: 108, epoch: 107, acc: 53.41880341880342, f1: 22.968818209101823, r: 0.2311874090802221
06/02/2019 03:58:39 *** epoch: 109 ***
06/02/2019 03:58:39 *** training ***
06/02/2019 03:58:39 step: 3569, epoch: 108, batch: 4, loss: 0.0012264177203178406, acc: 100.0, f1: 100.0, r: 0.7422720402023452
06/02/2019 03:58:39 step: 3574, epoch: 108, batch: 9, loss: 0.0007514655590057373, acc: 100.0, f1: 100.0, r: 0.6026070474582671
06/02/2019 03:58:40 step: 3579, epoch: 108, batch: 14, loss: 0.000861823558807373, acc: 100.0, f1: 100.0, r: 0.6935462894752493
06/02/2019 03:58:40 step: 3584, epoch: 108, batch: 19, loss: 0.0035943016409873962, acc: 100.0, f1: 100.0, r: 0.7527540998814657
06/02/2019 03:58:40 step: 3589, epoch: 108, batch: 24, loss: 0.004165224730968475, acc: 100.0, f1: 100.0, r: 0.6817808609383839
06/02/2019 03:58:41 step: 3594, epoch: 108, batch: 29, loss: 0.0017905160784721375, acc: 100.0, f1: 100.0, r: 0.7201538696750273
06/02/2019 03:58:41 *** evaluating ***
06/02/2019 03:58:41 step: 109, epoch: 108, acc: 52.13675213675214, f1: 23.626767362746925, r: 0.2430908835706578
06/02/2019 03:58:41 *** epoch: 110 ***
06/02/2019 03:58:41 *** training ***
06/02/2019 03:58:41 step: 3602, epoch: 109, batch: 4, loss: 0.003108680248260498, acc: 100.0, f1: 100.0, r: 0.8441149341628545
06/02/2019 03:58:42 step: 3607, epoch: 109, batch: 9, loss: 0.0026845037937164307, acc: 100.0, f1: 100.0, r: 0.7629231156158752
06/02/2019 03:58:42 step: 3612, epoch: 109, batch: 14, loss: 0.0009558945894241333, acc: 100.0, f1: 100.0, r: 0.8197788861590651
06/02/2019 03:58:42 step: 3617, epoch: 109, batch: 19, loss: 0.0008225440979003906, acc: 100.0, f1: 100.0, r: 0.5688198156738029
06/02/2019 03:58:42 step: 3622, epoch: 109, batch: 24, loss: 0.0010584592819213867, acc: 100.0, f1: 100.0, r: 0.7974548942183708
06/02/2019 03:58:43 step: 3627, epoch: 109, batch: 29, loss: 0.0009089410305023193, acc: 100.0, f1: 100.0, r: 0.6520582500133837
06/02/2019 03:58:43 *** evaluating ***
06/02/2019 03:58:43 step: 110, epoch: 109, acc: 52.56410256410257, f1: 21.67080620124934, r: 0.24253217811443048
06/02/2019 03:58:43 *** epoch: 111 ***
06/02/2019 03:58:43 *** training ***
06/02/2019 03:58:43 step: 3635, epoch: 110, batch: 4, loss: 0.001651167869567871, acc: 100.0, f1: 100.0, r: 0.811422295745769
06/02/2019 03:58:44 step: 3640, epoch: 110, batch: 9, loss: 0.0014222115278244019, acc: 100.0, f1: 100.0, r: 0.7760692595648988
06/02/2019 03:58:44 step: 3645, epoch: 110, batch: 14, loss: 0.0011155828833580017, acc: 100.0, f1: 100.0, r: 0.7499241556771352
06/02/2019 03:58:44 step: 3650, epoch: 110, batch: 19, loss: 0.0007479116320610046, acc: 100.0, f1: 100.0, r: 0.7679665960186969
06/02/2019 03:58:44 step: 3655, epoch: 110, batch: 24, loss: 0.0009012967348098755, acc: 100.0, f1: 100.0, r: 0.695202016269363
06/02/2019 03:58:45 step: 3660, epoch: 110, batch: 29, loss: 0.0009087622165679932, acc: 100.0, f1: 100.0, r: 0.8190618192272411
06/02/2019 03:58:45 *** evaluating ***
06/02/2019 03:58:45 step: 111, epoch: 110, acc: 52.13675213675214, f1: 20.769949677704506, r: 0.2382811399247398
06/02/2019 03:58:45 *** epoch: 112 ***
06/02/2019 03:58:45 *** training ***
06/02/2019 03:58:45 step: 3668, epoch: 111, batch: 4, loss: 0.0007213130593299866, acc: 100.0, f1: 100.0, r: 0.7988299883079322
06/02/2019 03:58:46 step: 3673, epoch: 111, batch: 9, loss: 0.0007069259881973267, acc: 100.0, f1: 100.0, r: 0.7891731848220124
06/02/2019 03:58:46 step: 3678, epoch: 111, batch: 14, loss: 0.004858806729316711, acc: 100.0, f1: 100.0, r: 0.627846610232542
06/02/2019 03:58:46 step: 3683, epoch: 111, batch: 19, loss: 0.0006426647305488586, acc: 100.0, f1: 100.0, r: 0.6994309785109751
06/02/2019 03:58:47 step: 3688, epoch: 111, batch: 24, loss: 0.0011487379670143127, acc: 100.0, f1: 100.0, r: 0.7493231675427078
06/02/2019 03:58:47 step: 3693, epoch: 111, batch: 29, loss: 0.0006945878267288208, acc: 100.0, f1: 100.0, r: 0.696761239365524
06/02/2019 03:58:47 *** evaluating ***
06/02/2019 03:58:47 step: 112, epoch: 111, acc: 53.84615384615385, f1: 25.636620673254907, r: 0.23926183578355353
06/02/2019 03:58:47 *** epoch: 113 ***
06/02/2019 03:58:47 *** training ***
06/02/2019 03:58:48 step: 3701, epoch: 112, batch: 4, loss: 0.0030077695846557617, acc: 100.0, f1: 100.0, r: 0.7081727251649478
06/02/2019 03:58:48 step: 3706, epoch: 112, batch: 9, loss: 0.0017910823225975037, acc: 100.0, f1: 100.0, r: 0.7854620603514344
06/02/2019 03:58:48 step: 3711, epoch: 112, batch: 14, loss: 0.0011966228485107422, acc: 100.0, f1: 100.0, r: 0.7371446368890325
06/02/2019 03:58:48 step: 3716, epoch: 112, batch: 19, loss: 0.00171564519405365, acc: 100.0, f1: 100.0, r: 0.847942238488613
06/02/2019 03:58:49 step: 3721, epoch: 112, batch: 24, loss: 0.001322127878665924, acc: 100.0, f1: 100.0, r: 0.6863241594515577
06/02/2019 03:58:49 step: 3726, epoch: 112, batch: 29, loss: 0.0008887052536010742, acc: 100.0, f1: 100.0, r: 0.7190324946952895
06/02/2019 03:58:49 *** evaluating ***
06/02/2019 03:58:49 step: 113, epoch: 112, acc: 51.28205128205128, f1: 24.159290890269155, r: 0.23689617587258738
06/02/2019 03:58:49 *** epoch: 114 ***
06/02/2019 03:58:49 *** training ***
06/02/2019 03:58:50 step: 3734, epoch: 113, batch: 4, loss: 0.0020660310983657837, acc: 100.0, f1: 100.0, r: 0.7520001595724665
06/02/2019 03:58:50 step: 3739, epoch: 113, batch: 9, loss: 0.0011561661958694458, acc: 100.0, f1: 100.0, r: 0.7558354447413188
06/02/2019 03:58:50 step: 3744, epoch: 113, batch: 14, loss: 0.0008824989199638367, acc: 100.0, f1: 100.0, r: 0.830138143298465
06/02/2019 03:58:50 step: 3749, epoch: 113, batch: 19, loss: 0.0009014308452606201, acc: 100.0, f1: 100.0, r: 0.6798566010749877
06/02/2019 03:58:51 step: 3754, epoch: 113, batch: 24, loss: 0.0005844011902809143, acc: 100.0, f1: 100.0, r: 0.7787072179595969
06/02/2019 03:58:51 step: 3759, epoch: 113, batch: 29, loss: 0.0013658404350280762, acc: 100.0, f1: 100.0, r: 0.8038762256301726
06/02/2019 03:58:51 *** evaluating ***
06/02/2019 03:58:51 step: 114, epoch: 113, acc: 52.56410256410257, f1: 22.296653304114358, r: 0.24186080710142732
06/02/2019 03:58:51 *** epoch: 115 ***
06/02/2019 03:58:51 *** training ***
06/02/2019 03:58:52 step: 3767, epoch: 114, batch: 4, loss: 0.0009752064943313599, acc: 100.0, f1: 100.0, r: 0.7894501683678452
06/02/2019 03:58:52 step: 3772, epoch: 114, batch: 9, loss: 0.0006083771586418152, acc: 100.0, f1: 100.0, r: 0.636149486832802
06/02/2019 03:58:52 step: 3777, epoch: 114, batch: 14, loss: 0.0014189258217811584, acc: 100.0, f1: 100.0, r: 0.6520036077576598
06/02/2019 03:58:53 step: 3782, epoch: 114, batch: 19, loss: 0.0010073855519294739, acc: 100.0, f1: 100.0, r: 0.7626031960625046
06/02/2019 03:58:53 step: 3787, epoch: 114, batch: 24, loss: 0.0023166462779045105, acc: 100.0, f1: 100.0, r: 0.7696058398249475
06/02/2019 03:58:53 step: 3792, epoch: 114, batch: 29, loss: 0.0012645497918128967, acc: 100.0, f1: 100.0, r: 0.7556190187470107
06/02/2019 03:58:53 *** evaluating ***
06/02/2019 03:58:54 step: 115, epoch: 114, acc: 52.13675213675214, f1: 22.420195063065503, r: 0.24566808036405752
06/02/2019 03:58:54 *** epoch: 116 ***
06/02/2019 03:58:54 *** training ***
06/02/2019 03:58:54 step: 3800, epoch: 115, batch: 4, loss: 0.0008110776543617249, acc: 100.0, f1: 100.0, r: 0.8213243256449442
06/02/2019 03:58:54 step: 3805, epoch: 115, batch: 9, loss: 0.0023111552000045776, acc: 100.0, f1: 100.0, r: 0.7147613121014759
06/02/2019 03:58:54 step: 3810, epoch: 115, batch: 14, loss: 0.002306535840034485, acc: 100.0, f1: 100.0, r: 0.7356302474527241
06/02/2019 03:58:55 step: 3815, epoch: 115, batch: 19, loss: 0.001792415976524353, acc: 100.0, f1: 100.0, r: 0.6995679633723695
06/02/2019 03:58:55 step: 3820, epoch: 115, batch: 24, loss: 0.005918942391872406, acc: 100.0, f1: 100.0, r: 0.7823453477250938
06/02/2019 03:58:55 step: 3825, epoch: 115, batch: 29, loss: 0.0013324469327926636, acc: 100.0, f1: 100.0, r: 0.7450931621985877
06/02/2019 03:58:56 *** evaluating ***
06/02/2019 03:58:56 step: 116, epoch: 115, acc: 53.41880341880342, f1: 24.51174728587837, r: 0.25079969708842453
06/02/2019 03:58:56 *** epoch: 117 ***
06/02/2019 03:58:56 *** training ***
06/02/2019 03:58:56 step: 3833, epoch: 116, batch: 4, loss: 0.0009675994515419006, acc: 100.0, f1: 100.0, r: 0.7439310817352368
06/02/2019 03:58:56 step: 3838, epoch: 116, batch: 9, loss: 0.0014623180031776428, acc: 100.0, f1: 100.0, r: 0.7845295755099172
06/02/2019 03:58:57 step: 3843, epoch: 116, batch: 14, loss: 0.0008574724197387695, acc: 100.0, f1: 100.0, r: 0.6923296390446226
06/02/2019 03:58:57 step: 3848, epoch: 116, batch: 19, loss: 0.0009802579879760742, acc: 100.0, f1: 100.0, r: 0.873497233319789
06/02/2019 03:58:57 step: 3853, epoch: 116, batch: 24, loss: 0.0009669363498687744, acc: 100.0, f1: 100.0, r: 0.6128478371807319
06/02/2019 03:58:58 step: 3858, epoch: 116, batch: 29, loss: 0.0018608644604682922, acc: 100.0, f1: 100.0, r: 0.737219802885638
06/02/2019 03:58:58 *** evaluating ***
06/02/2019 03:58:58 step: 117, epoch: 116, acc: 53.84615384615385, f1: 21.218342525676736, r: 0.23810496836422723
06/02/2019 03:58:58 *** epoch: 118 ***
06/02/2019 03:58:58 *** training ***
06/02/2019 03:58:58 step: 3866, epoch: 117, batch: 4, loss: 0.0011284798383712769, acc: 100.0, f1: 100.0, r: 0.829954302136882
06/02/2019 03:58:59 step: 3871, epoch: 117, batch: 9, loss: 0.005811579525470734, acc: 100.0, f1: 100.0, r: 0.6803245001715724
06/02/2019 03:58:59 step: 3876, epoch: 117, batch: 14, loss: 0.0008369535207748413, acc: 100.0, f1: 100.0, r: 0.8124562374026623
06/02/2019 03:58:59 step: 3881, epoch: 117, batch: 19, loss: 0.0015434473752975464, acc: 100.0, f1: 100.0, r: 0.7110536272003525
06/02/2019 03:59:00 step: 3886, epoch: 117, batch: 24, loss: 0.0018879249691963196, acc: 100.0, f1: 100.0, r: 0.8230612680535154
06/02/2019 03:59:00 step: 3891, epoch: 117, batch: 29, loss: 0.001163683831691742, acc: 100.0, f1: 100.0, r: 0.695656938286857
06/02/2019 03:59:00 *** evaluating ***
06/02/2019 03:59:00 step: 118, epoch: 117, acc: 52.991452991452995, f1: 21.188877840021043, r: 0.24546246936256813
06/02/2019 03:59:00 *** epoch: 119 ***
06/02/2019 03:59:00 *** training ***
06/02/2019 03:59:00 step: 3899, epoch: 118, batch: 4, loss: 0.0006355494260787964, acc: 100.0, f1: 100.0, r: 0.6743437360783641
06/02/2019 03:59:01 step: 3904, epoch: 118, batch: 9, loss: 0.0012443363666534424, acc: 100.0, f1: 100.0, r: 0.7556294701170969
06/02/2019 03:59:01 step: 3909, epoch: 118, batch: 14, loss: 0.0005750805139541626, acc: 100.0, f1: 100.0, r: 0.7584488697152676
06/02/2019 03:59:01 step: 3914, epoch: 118, batch: 19, loss: 0.0007473528385162354, acc: 100.0, f1: 100.0, r: 0.7539925814987632
06/02/2019 03:59:02 step: 3919, epoch: 118, batch: 24, loss: 0.0007495284080505371, acc: 100.0, f1: 100.0, r: 0.7728741875833611
06/02/2019 03:59:02 step: 3924, epoch: 118, batch: 29, loss: 0.0016544163227081299, acc: 100.0, f1: 100.0, r: 0.7485403404416063
06/02/2019 03:59:02 *** evaluating ***
06/02/2019 03:59:02 step: 119, epoch: 118, acc: 52.56410256410257, f1: 20.2884591819229, r: 0.23227867041480899
06/02/2019 03:59:02 *** epoch: 120 ***
06/02/2019 03:59:02 *** training ***
06/02/2019 03:59:03 step: 3932, epoch: 119, batch: 4, loss: 0.0014992058277130127, acc: 100.0, f1: 100.0, r: 0.8364843129365807
06/02/2019 03:59:03 step: 3937, epoch: 119, batch: 9, loss: 0.003054603934288025, acc: 100.0, f1: 100.0, r: 0.7117300198919916
06/02/2019 03:59:03 step: 3942, epoch: 119, batch: 14, loss: 0.0017474740743637085, acc: 100.0, f1: 100.0, r: 0.7183026549115923
06/02/2019 03:59:04 step: 3947, epoch: 119, batch: 19, loss: 0.0007343068718910217, acc: 100.0, f1: 100.0, r: 0.6681485449565809
06/02/2019 03:59:04 step: 3952, epoch: 119, batch: 24, loss: 0.000767834484577179, acc: 100.0, f1: 100.0, r: 0.7343670733841041
06/02/2019 03:59:04 step: 3957, epoch: 119, batch: 29, loss: 0.001013234257698059, acc: 100.0, f1: 100.0, r: 0.6635589093847925
06/02/2019 03:59:04 *** evaluating ***
06/02/2019 03:59:05 step: 120, epoch: 119, acc: 54.27350427350427, f1: 23.819791045480326, r: 0.25459618301585635
06/02/2019 03:59:05 *** epoch: 121 ***
06/02/2019 03:59:05 *** training ***
06/02/2019 03:59:05 step: 3965, epoch: 120, batch: 4, loss: 0.001529879868030548, acc: 100.0, f1: 100.0, r: 0.6903319265856348
06/02/2019 03:59:05 step: 3970, epoch: 120, batch: 9, loss: 0.0017429739236831665, acc: 100.0, f1: 100.0, r: 0.8221342448889482
06/02/2019 03:59:05 step: 3975, epoch: 120, batch: 14, loss: 0.0005553737282752991, acc: 100.0, f1: 100.0, r: 0.7408423086341998
06/02/2019 03:59:06 step: 3980, epoch: 120, batch: 19, loss: 0.0007061660289764404, acc: 100.0, f1: 100.0, r: 0.7811631651410241
06/02/2019 03:59:06 step: 3985, epoch: 120, batch: 24, loss: 0.0021554529666900635, acc: 100.0, f1: 100.0, r: 0.7104049697626004
06/02/2019 03:59:06 step: 3990, epoch: 120, batch: 29, loss: 0.0005960464477539062, acc: 100.0, f1: 100.0, r: 0.669469691447583
06/02/2019 03:59:07 *** evaluating ***
06/02/2019 03:59:07 step: 121, epoch: 120, acc: 53.84615384615385, f1: 22.95705274349475, r: 0.24862069193545702
06/02/2019 03:59:07 *** epoch: 122 ***
06/02/2019 03:59:07 *** training ***
06/02/2019 03:59:07 step: 3998, epoch: 121, batch: 4, loss: 0.0007028356194496155, acc: 100.0, f1: 100.0, r: 0.7173857200912255
06/02/2019 03:59:07 step: 4003, epoch: 121, batch: 9, loss: 0.0007579848170280457, acc: 100.0, f1: 100.0, r: 0.7362074864565447
06/02/2019 03:59:08 step: 4008, epoch: 121, batch: 14, loss: 0.001090213656425476, acc: 100.0, f1: 100.0, r: 0.8731995725640453
06/02/2019 03:59:08 step: 4013, epoch: 121, batch: 19, loss: 0.0013868361711502075, acc: 100.0, f1: 100.0, r: 0.7571052880443395
06/02/2019 03:59:08 step: 4018, epoch: 121, batch: 24, loss: 0.002531580626964569, acc: 100.0, f1: 100.0, r: 0.8124046047144132
06/02/2019 03:59:09 step: 4023, epoch: 121, batch: 29, loss: 0.0015551671385765076, acc: 100.0, f1: 100.0, r: 0.7211676187995125
06/02/2019 03:59:09 *** evaluating ***
06/02/2019 03:59:09 step: 122, epoch: 121, acc: 53.84615384615385, f1: 23.32052139037433, r: 0.24964869482344032
06/02/2019 03:59:09 *** epoch: 123 ***
06/02/2019 03:59:09 *** training ***
06/02/2019 03:59:09 step: 4031, epoch: 122, batch: 4, loss: 0.0006886273622512817, acc: 100.0, f1: 100.0, r: 0.670350273941906
06/02/2019 03:59:10 step: 4036, epoch: 122, batch: 9, loss: 0.0008108988404273987, acc: 100.0, f1: 100.0, r: 0.7164096772075579
06/02/2019 03:59:10 step: 4041, epoch: 122, batch: 14, loss: 0.000981166958808899, acc: 100.0, f1: 100.0, r: 0.7444959485278616
06/02/2019 03:59:10 step: 4046, epoch: 122, batch: 19, loss: 0.0005568116903305054, acc: 100.0, f1: 100.0, r: 0.7210213764946724
06/02/2019 03:59:11 step: 4051, epoch: 122, batch: 24, loss: 0.0016025900840759277, acc: 100.0, f1: 100.0, r: 0.6254241606697407
06/02/2019 03:59:11 step: 4056, epoch: 122, batch: 29, loss: 0.0015601962804794312, acc: 100.0, f1: 100.0, r: 0.8453828044685743
06/02/2019 03:59:11 *** evaluating ***
06/02/2019 03:59:11 step: 123, epoch: 122, acc: 53.84615384615385, f1: 22.634778849040483, r: 0.24169765613230862
06/02/2019 03:59:11 *** epoch: 124 ***
06/02/2019 03:59:11 *** training ***
06/02/2019 03:59:12 step: 4064, epoch: 123, batch: 4, loss: 0.0007135495543479919, acc: 100.0, f1: 100.0, r: 0.7165297664528764
06/02/2019 03:59:12 step: 4069, epoch: 123, batch: 9, loss: 0.0007520318031311035, acc: 100.0, f1: 100.0, r: 0.7931816102400416
06/02/2019 03:59:12 step: 4074, epoch: 123, batch: 14, loss: 0.002068154513835907, acc: 100.0, f1: 100.0, r: 0.6929314606735835
06/02/2019 03:59:13 step: 4079, epoch: 123, batch: 19, loss: 0.000950448215007782, acc: 100.0, f1: 100.0, r: 0.663770717296977
06/02/2019 03:59:13 step: 4084, epoch: 123, batch: 24, loss: 0.0034263059496879578, acc: 100.0, f1: 100.0, r: 0.6859443870945708
06/02/2019 03:59:13 step: 4089, epoch: 123, batch: 29, loss: 0.0005795732140541077, acc: 100.0, f1: 100.0, r: 0.7896157432706689
06/02/2019 03:59:13 *** evaluating ***
06/02/2019 03:59:13 step: 124, epoch: 123, acc: 52.56410256410257, f1: 22.89338206724057, r: 0.2408929408177302
06/02/2019 03:59:13 *** epoch: 125 ***
06/02/2019 03:59:13 *** training ***
06/02/2019 03:59:14 step: 4097, epoch: 124, batch: 4, loss: 0.0006691813468933105, acc: 100.0, f1: 100.0, r: 0.7069386007694927
06/02/2019 03:59:14 step: 4102, epoch: 124, batch: 9, loss: 0.0009189099073410034, acc: 100.0, f1: 100.0, r: 0.8123867455857138
06/02/2019 03:59:14 step: 4107, epoch: 124, batch: 14, loss: 0.001734711229801178, acc: 100.0, f1: 100.0, r: 0.7373622772246945
06/02/2019 03:59:15 step: 4112, epoch: 124, batch: 19, loss: 0.001394525170326233, acc: 100.0, f1: 100.0, r: 0.725294806359836
06/02/2019 03:59:15 step: 4117, epoch: 124, batch: 24, loss: 0.0014657601714134216, acc: 100.0, f1: 100.0, r: 0.7991623573287051
06/02/2019 03:59:15 step: 4122, epoch: 124, batch: 29, loss: 0.0005774497985839844, acc: 100.0, f1: 100.0, r: 0.7186496818932064
06/02/2019 03:59:15 *** evaluating ***
06/02/2019 03:59:16 step: 125, epoch: 124, acc: 51.70940170940172, f1: 21.878553987895, r: 0.24457652714814493
06/02/2019 03:59:16 *** epoch: 126 ***
06/02/2019 03:59:16 *** training ***
06/02/2019 03:59:16 step: 4130, epoch: 125, batch: 4, loss: 0.001249462366104126, acc: 100.0, f1: 100.0, r: 0.794208948739587
06/02/2019 03:59:16 step: 4135, epoch: 125, batch: 9, loss: 0.0005841106176376343, acc: 100.0, f1: 100.0, r: 0.7773575017038769
06/02/2019 03:59:17 step: 4140, epoch: 125, batch: 14, loss: 0.0008681192994117737, acc: 100.0, f1: 100.0, r: 0.7422875166340519
06/02/2019 03:59:17 step: 4145, epoch: 125, batch: 19, loss: 0.000712864100933075, acc: 100.0, f1: 100.0, r: 0.8555475303512673
06/02/2019 03:59:17 step: 4150, epoch: 125, batch: 24, loss: 0.0006554797291755676, acc: 100.0, f1: 100.0, r: 0.7134545954739651
06/02/2019 03:59:17 step: 4155, epoch: 125, batch: 29, loss: 0.0004942789673805237, acc: 100.0, f1: 100.0, r: 0.7618405558190776
06/02/2019 03:59:18 *** evaluating ***
06/02/2019 03:59:18 step: 126, epoch: 125, acc: 52.56410256410257, f1: 22.968675239710233, r: 0.23841946414676463
06/02/2019 03:59:18 *** epoch: 127 ***
06/02/2019 03:59:18 *** training ***
06/02/2019 03:59:18 step: 4163, epoch: 126, batch: 4, loss: 0.000602036714553833, acc: 100.0, f1: 100.0, r: 0.644349328697429
06/02/2019 03:59:18 step: 4168, epoch: 126, batch: 9, loss: 0.001082397997379303, acc: 100.0, f1: 100.0, r: 0.709485774014809
06/02/2019 03:59:19 step: 4173, epoch: 126, batch: 14, loss: 0.0014395788311958313, acc: 100.0, f1: 100.0, r: 0.7039488817971767
06/02/2019 03:59:19 step: 4178, epoch: 126, batch: 19, loss: 0.0009715855121612549, acc: 100.0, f1: 100.0, r: 0.7133622170645034
06/02/2019 03:59:19 step: 4183, epoch: 126, batch: 24, loss: 0.001769237220287323, acc: 100.0, f1: 100.0, r: 0.8078593188510549
06/02/2019 03:59:20 step: 4188, epoch: 126, batch: 29, loss: 0.004042372107505798, acc: 100.0, f1: 100.0, r: 0.8219639308608978
06/02/2019 03:59:20 *** evaluating ***
06/02/2019 03:59:20 step: 127, epoch: 126, acc: 54.700854700854705, f1: 23.104905066494112, r: 0.24255578006461792
06/02/2019 03:59:20 *** epoch: 128 ***
06/02/2019 03:59:20 *** training ***
06/02/2019 03:59:20 step: 4196, epoch: 127, batch: 4, loss: 0.0017037615180015564, acc: 100.0, f1: 100.0, r: 0.7834496038639479
06/02/2019 03:59:20 step: 4201, epoch: 127, batch: 9, loss: 0.001697055995464325, acc: 100.0, f1: 100.0, r: 0.7326978262895176
06/02/2019 03:59:21 step: 4206, epoch: 127, batch: 14, loss: 0.003500785678625107, acc: 100.0, f1: 100.0, r: 0.7206891391070488
06/02/2019 03:59:21 step: 4211, epoch: 127, batch: 19, loss: 0.0008332878351211548, acc: 100.0, f1: 100.0, r: 0.7484348233285298
06/02/2019 03:59:21 step: 4216, epoch: 127, batch: 24, loss: 0.001003853976726532, acc: 100.0, f1: 100.0, r: 0.6083474351260116
06/02/2019 03:59:22 step: 4221, epoch: 127, batch: 29, loss: 0.0011309310793876648, acc: 100.0, f1: 100.0, r: 0.6437254756816498
06/02/2019 03:59:22 *** evaluating ***
06/02/2019 03:59:22 step: 128, epoch: 127, acc: 52.991452991452995, f1: 23.08395202336431, r: 0.23698602714592618
06/02/2019 03:59:22 *** epoch: 129 ***
06/02/2019 03:59:22 *** training ***
06/02/2019 03:59:22 step: 4229, epoch: 128, batch: 4, loss: 0.0022749677300453186, acc: 100.0, f1: 100.0, r: 0.7972291780572325
06/02/2019 03:59:22 step: 4234, epoch: 128, batch: 9, loss: 0.0009312555193901062, acc: 100.0, f1: 100.0, r: 0.7133841662133583
06/02/2019 03:59:23 step: 4239, epoch: 128, batch: 14, loss: 0.001519232988357544, acc: 100.0, f1: 100.0, r: 0.762343852693623
06/02/2019 03:59:23 step: 4244, epoch: 128, batch: 19, loss: 0.002184569835662842, acc: 100.0, f1: 100.0, r: 0.8073979505553086
06/02/2019 03:59:23 step: 4249, epoch: 128, batch: 24, loss: 0.0010534897446632385, acc: 100.0, f1: 100.0, r: 0.675790461416924
06/02/2019 03:59:24 step: 4254, epoch: 128, batch: 29, loss: 0.00456121563911438, acc: 100.0, f1: 100.0, r: 0.7316058571266185
06/02/2019 03:59:24 *** evaluating ***
06/02/2019 03:59:24 step: 129, epoch: 128, acc: 51.70940170940172, f1: 24.15671652513758, r: 0.23729996244346935
06/02/2019 03:59:24 *** epoch: 130 ***
06/02/2019 03:59:24 *** training ***
06/02/2019 03:59:24 step: 4262, epoch: 129, batch: 4, loss: 0.000618956983089447, acc: 100.0, f1: 100.0, r: 0.8145354576818331
06/02/2019 03:59:24 step: 4267, epoch: 129, batch: 9, loss: 0.0013182982802391052, acc: 100.0, f1: 100.0, r: 0.8406243819309372
06/02/2019 03:59:25 step: 4272, epoch: 129, batch: 14, loss: 0.0008016005158424377, acc: 100.0, f1: 100.0, r: 0.7274851782315994
06/02/2019 03:59:25 step: 4277, epoch: 129, batch: 19, loss: 0.0009465962648391724, acc: 100.0, f1: 100.0, r: 0.7197525038780805
06/02/2019 03:59:25 step: 4282, epoch: 129, batch: 24, loss: 0.0011940598487854004, acc: 100.0, f1: 100.0, r: 0.7568312852272384
06/02/2019 03:59:26 step: 4287, epoch: 129, batch: 29, loss: 0.0008581802248954773, acc: 100.0, f1: 100.0, r: 0.757767856398099
06/02/2019 03:59:26 *** evaluating ***
06/02/2019 03:59:26 step: 130, epoch: 129, acc: 53.84615384615385, f1: 23.334357570007473, r: 0.2423005543664982
06/02/2019 03:59:26 *** epoch: 131 ***
06/02/2019 03:59:26 *** training ***
06/02/2019 03:59:26 step: 4295, epoch: 130, batch: 4, loss: 0.0025862902402877808, acc: 100.0, f1: 100.0, r: 0.7109166597481745
06/02/2019 03:59:27 step: 4300, epoch: 130, batch: 9, loss: 0.0010716989636421204, acc: 100.0, f1: 100.0, r: 0.7014127184039113
06/02/2019 03:59:27 step: 4305, epoch: 130, batch: 14, loss: 0.0006732568144798279, acc: 100.0, f1: 100.0, r: 0.6939212164019326
06/02/2019 03:59:27 step: 4310, epoch: 130, batch: 19, loss: 0.0016712620854377747, acc: 100.0, f1: 100.0, r: 0.830218265486779
06/02/2019 03:59:27 step: 4315, epoch: 130, batch: 24, loss: 0.0005920678377151489, acc: 100.0, f1: 100.0, r: 0.6839769145329485
06/02/2019 03:59:28 step: 4320, epoch: 130, batch: 29, loss: 0.0010699182748794556, acc: 100.0, f1: 100.0, r: 0.8182342981783381
06/02/2019 03:59:28 *** evaluating ***
06/02/2019 03:59:28 step: 131, epoch: 130, acc: 52.13675213675214, f1: 21.771627344826623, r: 0.22920677838498785
06/02/2019 03:59:28 *** epoch: 132 ***
06/02/2019 03:59:28 *** training ***
06/02/2019 03:59:28 step: 4328, epoch: 131, batch: 4, loss: 0.0009950324892997742, acc: 100.0, f1: 100.0, r: 0.7963231741573897
06/02/2019 03:59:29 step: 4333, epoch: 131, batch: 9, loss: 0.0015477240085601807, acc: 100.0, f1: 100.0, r: 0.6910771218444803
06/02/2019 03:59:29 step: 4338, epoch: 131, batch: 14, loss: 0.0014327391982078552, acc: 100.0, f1: 100.0, r: 0.6996059006425897
06/02/2019 03:59:29 step: 4343, epoch: 131, batch: 19, loss: 0.0012686178088188171, acc: 100.0, f1: 100.0, r: 0.7072401945570004
06/02/2019 03:59:30 step: 4348, epoch: 131, batch: 24, loss: 0.0007100850343704224, acc: 100.0, f1: 100.0, r: 0.793638112009606
06/02/2019 03:59:30 step: 4353, epoch: 131, batch: 29, loss: 0.0018349513411521912, acc: 100.0, f1: 100.0, r: 0.7511739703785227
06/02/2019 03:59:30 *** evaluating ***
06/02/2019 03:59:30 step: 132, epoch: 131, acc: 52.56410256410257, f1: 22.418341611175446, r: 0.23552310636401325
06/02/2019 03:59:30 *** epoch: 133 ***
06/02/2019 03:59:30 *** training ***
06/02/2019 03:59:30 step: 4361, epoch: 132, batch: 4, loss: 0.0008367300033569336, acc: 100.0, f1: 100.0, r: 0.7509711362964542
06/02/2019 03:59:31 step: 4366, epoch: 132, batch: 9, loss: 0.0022099390625953674, acc: 100.0, f1: 100.0, r: 0.8226763916621487
06/02/2019 03:59:31 step: 4371, epoch: 132, batch: 14, loss: 0.0007798448204994202, acc: 100.0, f1: 100.0, r: 0.6240894219669187
06/02/2019 03:59:31 step: 4376, epoch: 132, batch: 19, loss: 0.002074114978313446, acc: 100.0, f1: 100.0, r: 0.7330664178825416
06/02/2019 03:59:32 step: 4381, epoch: 132, batch: 24, loss: 0.0007204189896583557, acc: 100.0, f1: 100.0, r: 0.7820419083471106
06/02/2019 03:59:32 step: 4386, epoch: 132, batch: 29, loss: 0.0011919662356376648, acc: 100.0, f1: 100.0, r: 0.6967576058634861
06/02/2019 03:59:32 *** evaluating ***
06/02/2019 03:59:32 step: 133, epoch: 132, acc: 53.41880341880342, f1: 23.394016805432013, r: 0.2429683187889133
06/02/2019 03:59:32 *** epoch: 134 ***
06/02/2019 03:59:32 *** training ***
06/02/2019 03:59:32 step: 4394, epoch: 133, batch: 4, loss: 0.0010981559753417969, acc: 100.0, f1: 100.0, r: 0.823177810035356
06/02/2019 03:59:33 step: 4399, epoch: 133, batch: 9, loss: 0.0013833865523338318, acc: 100.0, f1: 100.0, r: 0.7374999569723067
06/02/2019 03:59:33 step: 4404, epoch: 133, batch: 14, loss: 0.001034557819366455, acc: 100.0, f1: 100.0, r: 0.668653068756963
06/02/2019 03:59:33 step: 4409, epoch: 133, batch: 19, loss: 0.0006499812006950378, acc: 100.0, f1: 100.0, r: 0.7503454814366778
06/02/2019 03:59:33 step: 4414, epoch: 133, batch: 24, loss: 0.0007831677794456482, acc: 100.0, f1: 100.0, r: 0.7610758366686179
06/02/2019 03:59:34 step: 4419, epoch: 133, batch: 29, loss: 0.00043910741806030273, acc: 100.0, f1: 100.0, r: 0.7703469591919847
06/02/2019 03:59:34 *** evaluating ***
06/02/2019 03:59:34 step: 134, epoch: 133, acc: 53.41880341880342, f1: 24.526500992315594, r: 0.23833412500319517
06/02/2019 03:59:34 *** epoch: 135 ***
06/02/2019 03:59:34 *** training ***
06/02/2019 03:59:34 step: 4427, epoch: 134, batch: 4, loss: 0.0009294673800468445, acc: 100.0, f1: 100.0, r: 0.7789822952009409
06/02/2019 03:59:35 step: 4432, epoch: 134, batch: 9, loss: 0.0007711350917816162, acc: 100.0, f1: 100.0, r: 0.6462293349513507
06/02/2019 03:59:35 step: 4437, epoch: 134, batch: 14, loss: 0.00036219507455825806, acc: 100.0, f1: 100.0, r: 0.725384785420772
06/02/2019 03:59:35 step: 4442, epoch: 134, batch: 19, loss: 0.0006478056311607361, acc: 100.0, f1: 100.0, r: 0.7201521809511175
06/02/2019 03:59:36 step: 4447, epoch: 134, batch: 24, loss: 0.001781456172466278, acc: 100.0, f1: 100.0, r: 0.7121855783131973
06/02/2019 03:59:36 step: 4452, epoch: 134, batch: 29, loss: 0.0010873675346374512, acc: 100.0, f1: 100.0, r: 0.8254793324019668
06/02/2019 03:59:36 *** evaluating ***
06/02/2019 03:59:36 step: 135, epoch: 134, acc: 53.84615384615385, f1: 21.859727444102333, r: 0.24923500625308756
06/02/2019 03:59:36 *** epoch: 136 ***
06/02/2019 03:59:36 *** training ***
06/02/2019 03:59:37 step: 4460, epoch: 135, batch: 4, loss: 0.002384267747402191, acc: 100.0, f1: 100.0, r: 0.7132111073065904
06/02/2019 03:59:37 step: 4465, epoch: 135, batch: 9, loss: 0.0008736997842788696, acc: 100.0, f1: 100.0, r: 0.6315341042720438
06/02/2019 03:59:37 step: 4470, epoch: 135, batch: 14, loss: 0.0007811412215232849, acc: 100.0, f1: 100.0, r: 0.7110044024133799
06/02/2019 03:59:38 step: 4475, epoch: 135, batch: 19, loss: 0.0015655755996704102, acc: 100.0, f1: 100.0, r: 0.688838118798178
06/02/2019 03:59:38 step: 4480, epoch: 135, batch: 24, loss: 0.001529783010482788, acc: 100.0, f1: 100.0, r: 0.7686694327973647
06/02/2019 03:59:38 step: 4485, epoch: 135, batch: 29, loss: 0.0006977468729019165, acc: 100.0, f1: 100.0, r: 0.5671250440706395
06/02/2019 03:59:38 *** evaluating ***
06/02/2019 03:59:38 step: 136, epoch: 135, acc: 52.56410256410257, f1: 23.52068111949535, r: 0.2419396009873452
06/02/2019 03:59:38 *** epoch: 137 ***
06/02/2019 03:59:38 *** training ***
06/02/2019 03:59:39 step: 4493, epoch: 136, batch: 4, loss: 0.00041449815034866333, acc: 100.0, f1: 100.0, r: 0.7144110106604827
06/02/2019 03:59:39 step: 4498, epoch: 136, batch: 9, loss: 0.0015076547861099243, acc: 100.0, f1: 100.0, r: 0.6726229155093614
06/02/2019 03:59:39 step: 4503, epoch: 136, batch: 14, loss: 0.0009958669543266296, acc: 100.0, f1: 100.0, r: 0.6990127920256459
06/02/2019 03:59:40 step: 4508, epoch: 136, batch: 19, loss: 0.000560462474822998, acc: 100.0, f1: 100.0, r: 0.8082059396801812
06/02/2019 03:59:40 step: 4513, epoch: 136, batch: 24, loss: 0.0007694661617279053, acc: 100.0, f1: 100.0, r: 0.821298165895233
06/02/2019 03:59:40 step: 4518, epoch: 136, batch: 29, loss: 0.0005195960402488708, acc: 100.0, f1: 100.0, r: 0.598720935224264
06/02/2019 03:59:41 *** evaluating ***
06/02/2019 03:59:41 step: 137, epoch: 136, acc: 52.56410256410257, f1: 22.799797407521357, r: 0.2433369567890163
06/02/2019 03:59:41 *** epoch: 138 ***
06/02/2019 03:59:41 *** training ***
06/02/2019 03:59:41 step: 4526, epoch: 137, batch: 4, loss: 0.0005401894450187683, acc: 100.0, f1: 100.0, r: 0.6872466854389062
06/02/2019 03:59:41 step: 4531, epoch: 137, batch: 9, loss: 0.0015929043292999268, acc: 100.0, f1: 100.0, r: 0.8099012517425821
06/02/2019 03:59:42 step: 4536, epoch: 137, batch: 14, loss: 0.0006961151957511902, acc: 100.0, f1: 100.0, r: 0.7056224777090538
06/02/2019 03:59:42 step: 4541, epoch: 137, batch: 19, loss: 0.0018823668360710144, acc: 100.0, f1: 100.0, r: 0.7481044381092052
06/02/2019 03:59:42 step: 4546, epoch: 137, batch: 24, loss: 0.0009602457284927368, acc: 100.0, f1: 100.0, r: 0.8107420474402496
06/02/2019 03:59:43 step: 4551, epoch: 137, batch: 29, loss: 0.002276495099067688, acc: 100.0, f1: 100.0, r: 0.8482649319190414
06/02/2019 03:59:43 *** evaluating ***
06/02/2019 03:59:43 step: 138, epoch: 137, acc: 52.13675213675214, f1: 22.601888474757263, r: 0.2375337685347368
06/02/2019 03:59:43 *** epoch: 139 ***
06/02/2019 03:59:43 *** training ***
06/02/2019 03:59:43 step: 4559, epoch: 138, batch: 4, loss: 0.0013371556997299194, acc: 100.0, f1: 100.0, r: 0.7333439308525598
06/02/2019 03:59:43 step: 4564, epoch: 138, batch: 9, loss: 0.0015282928943634033, acc: 100.0, f1: 100.0, r: 0.7801859842594497
06/02/2019 03:59:44 step: 4569, epoch: 138, batch: 14, loss: 0.002409204840660095, acc: 100.0, f1: 100.0, r: 0.7981353552859942
06/02/2019 03:59:44 step: 4574, epoch: 138, batch: 19, loss: 0.0011458024382591248, acc: 100.0, f1: 100.0, r: 0.7100745719336345
06/02/2019 03:59:44 step: 4579, epoch: 138, batch: 24, loss: 0.0008141994476318359, acc: 100.0, f1: 100.0, r: 0.7121200697083215
06/02/2019 03:59:45 step: 4584, epoch: 138, batch: 29, loss: 0.0007377713918685913, acc: 100.0, f1: 100.0, r: 0.7587437648939692
06/02/2019 03:59:45 *** evaluating ***
06/02/2019 03:59:45 step: 139, epoch: 138, acc: 53.41880341880342, f1: 22.84210458376899, r: 0.23940962756071257
06/02/2019 03:59:45 *** epoch: 140 ***
06/02/2019 03:59:45 *** training ***
06/02/2019 03:59:45 step: 4592, epoch: 139, batch: 4, loss: 0.0010459870100021362, acc: 100.0, f1: 100.0, r: 0.8092606056983213
06/02/2019 03:59:45 step: 4597, epoch: 139, batch: 9, loss: 0.001030750572681427, acc: 100.0, f1: 100.0, r: 0.6744318502411449
06/02/2019 03:59:46 step: 4602, epoch: 139, batch: 14, loss: 0.000658676028251648, acc: 100.0, f1: 100.0, r: 0.686122745161556
06/02/2019 03:59:46 step: 4607, epoch: 139, batch: 19, loss: 0.002045251429080963, acc: 100.0, f1: 100.0, r: 0.8276029869816114
06/02/2019 03:59:46 step: 4612, epoch: 139, batch: 24, loss: 0.0021549686789512634, acc: 100.0, f1: 100.0, r: 0.7343717118223485
06/02/2019 03:59:47 step: 4617, epoch: 139, batch: 29, loss: 0.0020661503076553345, acc: 100.0, f1: 100.0, r: 0.8210936217154037
06/02/2019 03:59:47 *** evaluating ***
06/02/2019 03:59:47 step: 140, epoch: 139, acc: 51.70940170940172, f1: 22.41242011795047, r: 0.23851248972054714
06/02/2019 03:59:47 *** epoch: 141 ***
06/02/2019 03:59:47 *** training ***
06/02/2019 03:59:47 step: 4625, epoch: 140, batch: 4, loss: 0.0011761710047721863, acc: 100.0, f1: 100.0, r: 0.7520912493014889
06/02/2019 03:59:48 step: 4630, epoch: 140, batch: 9, loss: 0.0011764690279960632, acc: 100.0, f1: 100.0, r: 0.764041403351608
06/02/2019 03:59:48 step: 4635, epoch: 140, batch: 14, loss: 0.0020181573927402496, acc: 100.0, f1: 100.0, r: 0.7829175426397563
06/02/2019 03:59:48 step: 4640, epoch: 140, batch: 19, loss: 0.00047169625759124756, acc: 100.0, f1: 100.0, r: 0.7834942939386401
06/02/2019 03:59:49 step: 4645, epoch: 140, batch: 24, loss: 0.0007418468594551086, acc: 100.0, f1: 100.0, r: 0.8004239132327271
06/02/2019 03:59:49 step: 4650, epoch: 140, batch: 29, loss: 0.0012462660670280457, acc: 100.0, f1: 100.0, r: 0.8410533033939643
06/02/2019 03:59:49 *** evaluating ***
06/02/2019 03:59:49 step: 141, epoch: 140, acc: 52.991452991452995, f1: 20.585821480172186, r: 0.2391668998197838
06/02/2019 03:59:49 *** epoch: 142 ***
06/02/2019 03:59:49 *** training ***
06/02/2019 03:59:50 step: 4658, epoch: 141, batch: 4, loss: 0.0008414611220359802, acc: 100.0, f1: 100.0, r: 0.7032351713451372
06/02/2019 03:59:50 step: 4663, epoch: 141, batch: 9, loss: 0.00047934800386428833, acc: 100.0, f1: 100.0, r: 0.7890859522332767
06/02/2019 03:59:50 step: 4668, epoch: 141, batch: 14, loss: 0.0011427327990531921, acc: 100.0, f1: 100.0, r: 0.7065050241178219
06/02/2019 03:59:50 step: 4673, epoch: 141, batch: 19, loss: 0.0019812434911727905, acc: 100.0, f1: 100.0, r: 0.7655328653094179
06/02/2019 03:59:51 step: 4678, epoch: 141, batch: 24, loss: 0.0019721761345863342, acc: 100.0, f1: 100.0, r: 0.730903085664105
06/02/2019 03:59:51 step: 4683, epoch: 141, batch: 29, loss: 0.0012773945927619934, acc: 100.0, f1: 100.0, r: 0.8318754455879135
06/02/2019 03:59:51 *** evaluating ***
06/02/2019 03:59:51 step: 142, epoch: 141, acc: 50.0, f1: 22.336465598719162, r: 0.22435058995604054
06/02/2019 03:59:51 *** epoch: 143 ***
06/02/2019 03:59:51 *** training ***
06/02/2019 03:59:52 step: 4691, epoch: 142, batch: 4, loss: 0.0014124363660812378, acc: 100.0, f1: 100.0, r: 0.822707449394743
06/02/2019 03:59:52 step: 4696, epoch: 142, batch: 9, loss: 0.0009234994649887085, acc: 100.0, f1: 100.0, r: 0.7514553682970286
06/02/2019 03:59:52 step: 4701, epoch: 142, batch: 14, loss: 0.0009639114141464233, acc: 100.0, f1: 100.0, r: 0.7214274150442078
06/02/2019 03:59:53 step: 4706, epoch: 142, batch: 19, loss: 0.0014906376600265503, acc: 100.0, f1: 100.0, r: 0.7744738781651298
06/02/2019 03:59:53 step: 4711, epoch: 142, batch: 24, loss: 0.0011833012104034424, acc: 100.0, f1: 100.0, r: 0.7976762417423601
06/02/2019 03:59:53 step: 4716, epoch: 142, batch: 29, loss: 0.0009482800960540771, acc: 100.0, f1: 100.0, r: 0.8062628619698942
06/02/2019 03:59:53 *** evaluating ***
06/02/2019 03:59:54 step: 143, epoch: 142, acc: 51.28205128205128, f1: 22.31596734863178, r: 0.2394194771027306
06/02/2019 03:59:54 *** epoch: 144 ***
06/02/2019 03:59:54 *** training ***
06/02/2019 03:59:54 step: 4724, epoch: 143, batch: 4, loss: 0.000878751277923584, acc: 100.0, f1: 100.0, r: 0.6288457053809121
06/02/2019 03:59:54 step: 4729, epoch: 143, batch: 9, loss: 0.0003454238176345825, acc: 100.0, f1: 100.0, r: 0.7699271573865255
06/02/2019 03:59:54 step: 4734, epoch: 143, batch: 14, loss: 0.0015496239066123962, acc: 100.0, f1: 100.0, r: 0.8084311939099066
06/02/2019 03:59:55 step: 4739, epoch: 143, batch: 19, loss: 0.0012187138199806213, acc: 100.0, f1: 100.0, r: 0.6291111605512657
06/02/2019 03:59:55 step: 4744, epoch: 143, batch: 24, loss: 0.0026316121220588684, acc: 100.0, f1: 100.0, r: 0.7162615283906255
06/02/2019 03:59:55 step: 4749, epoch: 143, batch: 29, loss: 0.0028609000146389008, acc: 100.0, f1: 100.0, r: 0.7038013026282834
06/02/2019 03:59:55 *** evaluating ***
06/02/2019 03:59:55 step: 144, epoch: 143, acc: 52.56410256410257, f1: 22.379125304771858, r: 0.2417031951283761
06/02/2019 03:59:55 *** epoch: 145 ***
06/02/2019 03:59:55 *** training ***
06/02/2019 03:59:56 step: 4757, epoch: 144, batch: 4, loss: 0.0004890710115432739, acc: 100.0, f1: 100.0, r: 0.7193834047615548
06/02/2019 03:59:56 step: 4762, epoch: 144, batch: 9, loss: 0.0006217211484909058, acc: 100.0, f1: 100.0, r: 0.7992299877412204
06/02/2019 03:59:56 step: 4767, epoch: 144, batch: 14, loss: 0.0005963966250419617, acc: 100.0, f1: 100.0, r: 0.8051614770282658
06/02/2019 03:59:57 step: 4772, epoch: 144, batch: 19, loss: 0.001054137945175171, acc: 100.0, f1: 100.0, r: 0.7009087631414118
06/02/2019 03:59:57 step: 4777, epoch: 144, batch: 24, loss: 0.09547790884971619, acc: 93.75, f1: 96.91666666666666, r: 0.7188062789276713
06/02/2019 03:59:57 step: 4782, epoch: 144, batch: 29, loss: 0.25834813714027405, acc: 96.875, f1: 95.15873015873015, r: 0.7311508849508925
06/02/2019 03:59:57 *** evaluating ***
06/02/2019 03:59:57 step: 145, epoch: 144, acc: 48.717948717948715, f1: 20.631895372461408, r: 0.20093571013877887
06/02/2019 03:59:57 *** epoch: 146 ***
06/02/2019 03:59:57 *** training ***
06/02/2019 03:59:58 step: 4790, epoch: 145, batch: 4, loss: 0.1679890900850296, acc: 98.4375, f1: 98.29573934837093, r: 0.6965605204771863
06/02/2019 03:59:58 step: 4795, epoch: 145, batch: 9, loss: 0.1259031891822815, acc: 100.0, f1: 100.0, r: 0.7320604463305798
06/02/2019 03:59:58 step: 4800, epoch: 145, batch: 14, loss: 0.13921862840652466, acc: 93.75, f1: 91.0090851039127, r: 0.7964203852198717
06/02/2019 03:59:59 step: 4805, epoch: 145, batch: 19, loss: 0.0695158988237381, acc: 98.4375, f1: 98.64433811802233, r: 0.8231051655578415
06/02/2019 03:59:59 step: 4810, epoch: 145, batch: 24, loss: 0.06989537179470062, acc: 98.4375, f1: 99.13793103448276, r: 0.7150618657481344
06/02/2019 03:59:59 step: 4815, epoch: 145, batch: 29, loss: 0.04393302649259567, acc: 100.0, f1: 100.0, r: 0.7844407102764525
06/02/2019 04:00:00 *** evaluating ***
06/02/2019 04:00:00 step: 146, epoch: 145, acc: 50.85470085470085, f1: 25.07687194527569, r: 0.22815559271558586
06/02/2019 04:00:00 *** epoch: 147 ***
06/02/2019 04:00:00 *** training ***
06/02/2019 04:00:00 step: 4823, epoch: 146, batch: 4, loss: 0.032862432301044464, acc: 100.0, f1: 100.0, r: 0.7300055791591994
06/02/2019 04:00:00 step: 4828, epoch: 146, batch: 9, loss: 0.023807521909475327, acc: 100.0, f1: 100.0, r: 0.7689187584398928
06/02/2019 04:00:00 step: 4833, epoch: 146, batch: 14, loss: 0.01765456795692444, acc: 100.0, f1: 100.0, r: 0.8013347712563708
06/02/2019 04:00:01 step: 4838, epoch: 146, batch: 19, loss: 0.023903030902147293, acc: 100.0, f1: 100.0, r: 0.7337047278313933
06/02/2019 04:00:01 step: 4843, epoch: 146, batch: 24, loss: 0.02284366637468338, acc: 100.0, f1: 100.0, r: 0.7144962677292009
06/02/2019 04:00:01 step: 4848, epoch: 146, batch: 29, loss: 0.038521021604537964, acc: 100.0, f1: 100.0, r: 0.6403989061944743
06/02/2019 04:00:01 *** evaluating ***
06/02/2019 04:00:02 step: 147, epoch: 146, acc: 50.0, f1: 23.205047876100508, r: 0.22522133899176208
06/02/2019 04:00:02 *** epoch: 148 ***
06/02/2019 04:00:02 *** training ***
06/02/2019 04:00:02 step: 4856, epoch: 147, batch: 4, loss: 0.006209284067153931, acc: 100.0, f1: 100.0, r: 0.7409812517233396
06/02/2019 04:00:02 step: 4861, epoch: 147, batch: 9, loss: 0.01895509660243988, acc: 100.0, f1: 100.0, r: 0.7659773074113797
06/02/2019 04:00:02 step: 4866, epoch: 147, batch: 14, loss: 0.011719293892383575, acc: 100.0, f1: 100.0, r: 0.7344921042650613
06/02/2019 04:00:03 step: 4871, epoch: 147, batch: 19, loss: 0.015335910022258759, acc: 100.0, f1: 100.0, r: 0.7923609931605343
06/02/2019 04:00:03 step: 4876, epoch: 147, batch: 24, loss: 0.007374100387096405, acc: 100.0, f1: 100.0, r: 0.7232447397244838
06/02/2019 04:00:03 step: 4881, epoch: 147, batch: 29, loss: 0.013156667351722717, acc: 100.0, f1: 100.0, r: 0.6996145452779912
06/02/2019 04:00:04 *** evaluating ***
06/02/2019 04:00:04 step: 148, epoch: 147, acc: 49.572649572649574, f1: 21.772743722149063, r: 0.21553519596746534
06/02/2019 04:00:04 *** epoch: 149 ***
06/02/2019 04:00:04 *** training ***
06/02/2019 04:00:04 step: 4889, epoch: 148, batch: 4, loss: 0.0060098543763160706, acc: 100.0, f1: 100.0, r: 0.7590433045355637
06/02/2019 04:00:04 step: 4894, epoch: 148, batch: 9, loss: 0.010011814534664154, acc: 100.0, f1: 100.0, r: 0.7885266243513201
06/02/2019 04:00:05 step: 4899, epoch: 148, batch: 14, loss: 0.004535555839538574, acc: 100.0, f1: 100.0, r: 0.7150279442616068
06/02/2019 04:00:05 step: 4904, epoch: 148, batch: 19, loss: 0.017410457134246826, acc: 100.0, f1: 100.0, r: 0.7468939826792939
06/02/2019 04:00:05 step: 4909, epoch: 148, batch: 24, loss: 0.03537876904010773, acc: 98.4375, f1: 99.25490196078431, r: 0.7565138612047593
06/02/2019 04:00:06 step: 4914, epoch: 148, batch: 29, loss: 0.007292062044143677, acc: 100.0, f1: 100.0, r: 0.698649633939703
06/02/2019 04:00:06 *** evaluating ***
06/02/2019 04:00:06 step: 149, epoch: 148, acc: 49.572649572649574, f1: 20.90350709417165, r: 0.2128520469094201
06/02/2019 04:00:06 *** epoch: 150 ***
06/02/2019 04:00:06 *** training ***
06/02/2019 04:00:06 step: 4922, epoch: 149, batch: 4, loss: 0.002443477511405945, acc: 100.0, f1: 100.0, r: 0.6549313791427042
06/02/2019 04:00:06 step: 4927, epoch: 149, batch: 9, loss: 0.004327848553657532, acc: 100.0, f1: 100.0, r: 0.6919648512663832
06/02/2019 04:00:07 step: 4932, epoch: 149, batch: 14, loss: 0.00681682676076889, acc: 100.0, f1: 100.0, r: 0.7639111212047122
06/02/2019 04:00:07 step: 4937, epoch: 149, batch: 19, loss: 0.0037289410829544067, acc: 100.0, f1: 100.0, r: 0.6891014735380279
06/02/2019 04:00:07 step: 4942, epoch: 149, batch: 24, loss: 0.0017880648374557495, acc: 100.0, f1: 100.0, r: 0.699722840382746
06/02/2019 04:00:08 step: 4947, epoch: 149, batch: 29, loss: 0.005491003394126892, acc: 100.0, f1: 100.0, r: 0.77728918717009
06/02/2019 04:00:08 *** evaluating ***
06/02/2019 04:00:08 step: 150, epoch: 149, acc: 49.572649572649574, f1: 21.14624488035505, r: 0.21773257544159377
06/02/2019 04:00:08 *** epoch: 151 ***
06/02/2019 04:00:08 *** training ***
06/02/2019 04:00:08 step: 4955, epoch: 150, batch: 4, loss: 0.004845716059207916, acc: 100.0, f1: 100.0, r: 0.7934954665586799
06/02/2019 04:00:09 step: 4960, epoch: 150, batch: 9, loss: 0.006625615060329437, acc: 100.0, f1: 100.0, r: 0.8288716364779884
06/02/2019 04:00:09 step: 4965, epoch: 150, batch: 14, loss: 0.0036552995443344116, acc: 100.0, f1: 100.0, r: 0.7486692658260246
06/02/2019 04:00:09 step: 4970, epoch: 150, batch: 19, loss: 0.006581366062164307, acc: 100.0, f1: 100.0, r: 0.7968377507277722
06/02/2019 04:00:10 step: 4975, epoch: 150, batch: 24, loss: 0.005356684327125549, acc: 100.0, f1: 100.0, r: 0.8270119149293375
06/02/2019 04:00:10 step: 4980, epoch: 150, batch: 29, loss: 0.005856737494468689, acc: 100.0, f1: 100.0, r: 0.704758667842087
06/02/2019 04:00:10 *** evaluating ***
06/02/2019 04:00:10 step: 151, epoch: 150, acc: 48.717948717948715, f1: 22.410729485869936, r: 0.21338247726050363
06/02/2019 04:00:10 *** epoch: 152 ***
06/02/2019 04:00:10 *** training ***
06/02/2019 04:00:11 step: 4988, epoch: 151, batch: 4, loss: 0.005053475499153137, acc: 100.0, f1: 100.0, r: 0.7828605943863347
06/02/2019 04:00:11 step: 4993, epoch: 151, batch: 9, loss: 0.009955070912837982, acc: 100.0, f1: 100.0, r: 0.7948629536247714
06/02/2019 04:00:11 step: 4998, epoch: 151, batch: 14, loss: 0.002572454512119293, acc: 100.0, f1: 100.0, r: 0.6905263967056212
06/02/2019 04:00:12 step: 5003, epoch: 151, batch: 19, loss: 0.0025848224759101868, acc: 100.0, f1: 100.0, r: 0.7124401087625845
06/02/2019 04:00:12 step: 5008, epoch: 151, batch: 24, loss: 0.0029670819640159607, acc: 100.0, f1: 100.0, r: 0.7535236154883516
06/02/2019 04:00:12 step: 5013, epoch: 151, batch: 29, loss: 0.0033790916204452515, acc: 100.0, f1: 100.0, r: 0.7197163258248969
06/02/2019 04:00:12 *** evaluating ***
06/02/2019 04:00:13 step: 152, epoch: 151, acc: 50.0, f1: 23.57787752153949, r: 0.21558608608593194
06/02/2019 04:00:13 *** epoch: 153 ***
06/02/2019 04:00:13 *** training ***
06/02/2019 04:00:13 step: 5021, epoch: 152, batch: 4, loss: 0.0033905431628227234, acc: 100.0, f1: 100.0, r: 0.6532027181821365
06/02/2019 04:00:13 step: 5026, epoch: 152, batch: 9, loss: 0.00405457615852356, acc: 100.0, f1: 100.0, r: 0.8077123937849326
06/02/2019 04:00:13 step: 5031, epoch: 152, batch: 14, loss: 0.0026567280292510986, acc: 100.0, f1: 100.0, r: 0.7853084397480774
06/02/2019 04:00:14 step: 5036, epoch: 152, batch: 19, loss: 0.00709940493106842, acc: 100.0, f1: 100.0, r: 0.7520522155645601
06/02/2019 04:00:14 step: 5041, epoch: 152, batch: 24, loss: 0.0032225102186203003, acc: 100.0, f1: 100.0, r: 0.7585653276280284
06/02/2019 04:00:15 step: 5046, epoch: 152, batch: 29, loss: 0.003487236797809601, acc: 100.0, f1: 100.0, r: 0.6866333565278178
06/02/2019 04:00:15 *** evaluating ***
06/02/2019 04:00:15 step: 153, epoch: 152, acc: 49.572649572649574, f1: 23.409920506572217, r: 0.21183842909103173
06/02/2019 04:00:15 *** epoch: 154 ***
06/02/2019 04:00:15 *** training ***
06/02/2019 04:00:15 step: 5054, epoch: 153, batch: 4, loss: 0.0035878047347068787, acc: 100.0, f1: 100.0, r: 0.6802031891159351
06/02/2019 04:00:15 step: 5059, epoch: 153, batch: 9, loss: 0.0014899671077728271, acc: 100.0, f1: 100.0, r: 0.6048414825375088
06/02/2019 04:00:16 step: 5064, epoch: 153, batch: 14, loss: 0.004449993371963501, acc: 100.0, f1: 100.0, r: 0.7761726911837905
06/02/2019 04:00:16 step: 5069, epoch: 153, batch: 19, loss: 0.002307027578353882, acc: 100.0, f1: 100.0, r: 0.7787999140806363
06/02/2019 04:00:16 step: 5074, epoch: 153, batch: 24, loss: 0.004376061260700226, acc: 100.0, f1: 100.0, r: 0.6421492789635614
06/02/2019 04:00:17 step: 5079, epoch: 153, batch: 29, loss: 0.00423809140920639, acc: 100.0, f1: 100.0, r: 0.7833122726649616
06/02/2019 04:00:17 *** evaluating ***
06/02/2019 04:00:17 step: 154, epoch: 153, acc: 50.85470085470085, f1: 23.781577373486247, r: 0.21473034803921592
06/02/2019 04:00:17 *** epoch: 155 ***
06/02/2019 04:00:17 *** training ***
06/02/2019 04:00:17 step: 5087, epoch: 154, batch: 4, loss: 0.004119068384170532, acc: 100.0, f1: 100.0, r: 0.8160257334898405
06/02/2019 04:00:18 step: 5092, epoch: 154, batch: 9, loss: 0.0018644481897354126, acc: 100.0, f1: 100.0, r: 0.8197409004966099
06/02/2019 04:00:18 step: 5097, epoch: 154, batch: 14, loss: 0.0018622428178787231, acc: 100.0, f1: 100.0, r: 0.7186277362912084
06/02/2019 04:00:18 step: 5102, epoch: 154, batch: 19, loss: 0.003519102931022644, acc: 100.0, f1: 100.0, r: 0.6940792104393899
06/02/2019 04:00:18 step: 5107, epoch: 154, batch: 24, loss: 0.0038374587893486023, acc: 100.0, f1: 100.0, r: 0.671816456082599
06/02/2019 04:00:19 step: 5112, epoch: 154, batch: 29, loss: 0.0011765733361244202, acc: 100.0, f1: 100.0, r: 0.6823058946366521
06/02/2019 04:00:19 *** evaluating ***
06/02/2019 04:00:19 step: 155, epoch: 154, acc: 50.427350427350426, f1: 21.870952457795095, r: 0.21178105529925556
06/02/2019 04:00:19 *** epoch: 156 ***
06/02/2019 04:00:19 *** training ***
06/02/2019 04:00:19 step: 5120, epoch: 155, batch: 4, loss: 0.002321995794773102, acc: 100.0, f1: 100.0, r: 0.6887657099721183
06/02/2019 04:00:20 step: 5125, epoch: 155, batch: 9, loss: 0.00160902738571167, acc: 100.0, f1: 100.0, r: 0.6634168843090704
06/02/2019 04:00:20 step: 5130, epoch: 155, batch: 14, loss: 0.0018342137336730957, acc: 100.0, f1: 100.0, r: 0.651532933838495
06/02/2019 04:00:20 step: 5135, epoch: 155, batch: 19, loss: 0.003387928009033203, acc: 100.0, f1: 100.0, r: 0.6877947892431373
06/02/2019 04:00:21 step: 5140, epoch: 155, batch: 24, loss: 0.0018125101923942566, acc: 100.0, f1: 100.0, r: 0.7233603256672058
06/02/2019 04:00:21 step: 5145, epoch: 155, batch: 29, loss: 0.00254107266664505, acc: 100.0, f1: 100.0, r: 0.7252987369081616
06/02/2019 04:00:21 *** evaluating ***
06/02/2019 04:00:21 step: 156, epoch: 155, acc: 49.14529914529914, f1: 22.163239900371266, r: 0.21077327097796536
06/02/2019 04:00:21 *** epoch: 157 ***
06/02/2019 04:00:21 *** training ***
06/02/2019 04:00:22 step: 5153, epoch: 156, batch: 4, loss: 0.0009655803442001343, acc: 100.0, f1: 100.0, r: 0.7341040877344447
06/02/2019 04:00:22 step: 5158, epoch: 156, batch: 9, loss: 0.0025522634387016296, acc: 100.0, f1: 100.0, r: 0.7374917563666213
06/02/2019 04:00:22 step: 5163, epoch: 156, batch: 14, loss: 0.006108365952968597, acc: 100.0, f1: 100.0, r: 0.8040834265489222
06/02/2019 04:00:22 step: 5168, epoch: 156, batch: 19, loss: 0.002467699348926544, acc: 100.0, f1: 100.0, r: 0.7709491837077733
06/02/2019 04:00:23 step: 5173, epoch: 156, batch: 24, loss: 0.0054128095507621765, acc: 100.0, f1: 100.0, r: 0.7024542067789679
06/02/2019 04:00:23 step: 5178, epoch: 156, batch: 29, loss: 0.008029825985431671, acc: 100.0, f1: 100.0, r: 0.7058755450551994
06/02/2019 04:00:23 *** evaluating ***
06/02/2019 04:00:23 step: 157, epoch: 156, acc: 49.572649572649574, f1: 23.14478808473385, r: 0.21735375692689637
06/02/2019 04:00:23 *** epoch: 158 ***
06/02/2019 04:00:23 *** training ***
06/02/2019 04:00:24 step: 5186, epoch: 157, batch: 4, loss: 0.0025713592767715454, acc: 100.0, f1: 100.0, r: 0.6076870028763544
06/02/2019 04:00:24 step: 5191, epoch: 157, batch: 9, loss: 0.0015101432800292969, acc: 100.0, f1: 100.0, r: 0.7129588210251143
06/02/2019 04:00:24 step: 5196, epoch: 157, batch: 14, loss: 0.0020805299282073975, acc: 100.0, f1: 100.0, r: 0.739506153898318
06/02/2019 04:00:24 step: 5201, epoch: 157, batch: 19, loss: 0.001355096697807312, acc: 100.0, f1: 100.0, r: 0.6901551165488817
06/02/2019 04:00:25 step: 5206, epoch: 157, batch: 24, loss: 0.004030831158161163, acc: 100.0, f1: 100.0, r: 0.7944074221292212
06/02/2019 04:00:25 step: 5211, epoch: 157, batch: 29, loss: 0.0014437437057495117, acc: 100.0, f1: 100.0, r: 0.8098100791982304
06/02/2019 04:00:25 *** evaluating ***
06/02/2019 04:00:25 step: 158, epoch: 157, acc: 48.717948717948715, f1: 22.914266656919203, r: 0.21587996027905923
06/02/2019 04:00:25 *** epoch: 159 ***
06/02/2019 04:00:25 *** training ***
06/02/2019 04:00:26 step: 5219, epoch: 158, batch: 4, loss: 0.004417978227138519, acc: 100.0, f1: 100.0, r: 0.694130177386161
06/02/2019 04:00:26 step: 5224, epoch: 158, batch: 9, loss: 0.003147132694721222, acc: 100.0, f1: 100.0, r: 0.715895931297354
06/02/2019 04:00:26 step: 5229, epoch: 158, batch: 14, loss: 0.0009173080325126648, acc: 100.0, f1: 100.0, r: 0.767413367813701
06/02/2019 04:00:27 step: 5234, epoch: 158, batch: 19, loss: 0.0022245869040489197, acc: 100.0, f1: 100.0, r: 0.7051619337397547
06/02/2019 04:00:27 step: 5239, epoch: 158, batch: 24, loss: 0.002362079918384552, acc: 100.0, f1: 100.0, r: 0.7102392170637947
06/02/2019 04:00:27 step: 5244, epoch: 158, batch: 29, loss: 0.0017162561416625977, acc: 100.0, f1: 100.0, r: 0.6561095919472463
06/02/2019 04:00:27 *** evaluating ***
06/02/2019 04:00:27 step: 159, epoch: 158, acc: 51.28205128205128, f1: 22.49775320107924, r: 0.2155564128558869
06/02/2019 04:00:27 *** epoch: 160 ***
06/02/2019 04:00:27 *** training ***
06/02/2019 04:00:28 step: 5252, epoch: 159, batch: 4, loss: 0.0018981248140335083, acc: 100.0, f1: 100.0, r: 0.7902322875559029
06/02/2019 04:00:28 step: 5257, epoch: 159, batch: 9, loss: 0.0012175068259239197, acc: 100.0, f1: 100.0, r: 0.7804950488326771
06/02/2019 04:00:28 step: 5262, epoch: 159, batch: 14, loss: 0.0022201836109161377, acc: 100.0, f1: 100.0, r: 0.7930492498317556
06/02/2019 04:00:29 step: 5267, epoch: 159, batch: 19, loss: 0.00284663587808609, acc: 100.0, f1: 100.0, r: 0.7029889299255188
06/02/2019 04:00:29 step: 5272, epoch: 159, batch: 24, loss: 0.0017362982034683228, acc: 100.0, f1: 100.0, r: 0.7874754053932633
06/02/2019 04:00:29 step: 5277, epoch: 159, batch: 29, loss: 0.0027138739824295044, acc: 100.0, f1: 100.0, r: 0.6638660565698848
06/02/2019 04:00:29 *** evaluating ***
06/02/2019 04:00:30 step: 160, epoch: 159, acc: 50.85470085470085, f1: 21.975399253601246, r: 0.21706475128414948
06/02/2019 04:00:30 *** epoch: 161 ***
06/02/2019 04:00:30 *** training ***
06/02/2019 04:00:30 step: 5285, epoch: 160, batch: 4, loss: 0.0020806267857551575, acc: 100.0, f1: 100.0, r: 0.7912269639177376
06/02/2019 04:00:30 step: 5290, epoch: 160, batch: 9, loss: 0.0012632161378860474, acc: 100.0, f1: 100.0, r: 0.7397264365242848
06/02/2019 04:00:31 step: 5295, epoch: 160, batch: 14, loss: 0.0012134537100791931, acc: 100.0, f1: 100.0, r: 0.8502598425429917
06/02/2019 04:00:31 step: 5300, epoch: 160, batch: 19, loss: 0.0015974342823028564, acc: 100.0, f1: 100.0, r: 0.7671829159263198
06/02/2019 04:00:31 step: 5305, epoch: 160, batch: 24, loss: 0.0023323819041252136, acc: 100.0, f1: 100.0, r: 0.8125027418275365
06/02/2019 04:00:32 step: 5310, epoch: 160, batch: 29, loss: 0.002559177577495575, acc: 100.0, f1: 100.0, r: 0.721333870240403
06/02/2019 04:00:32 *** evaluating ***
06/02/2019 04:00:32 step: 161, epoch: 160, acc: 50.85470085470085, f1: 22.76119473815047, r: 0.21546972767469036
06/02/2019 04:00:32 *** epoch: 162 ***
06/02/2019 04:00:32 *** training ***
06/02/2019 04:00:32 step: 5318, epoch: 161, batch: 4, loss: 0.0017281696200370789, acc: 100.0, f1: 100.0, r: 0.8135615717717695
06/02/2019 04:00:33 step: 5323, epoch: 161, batch: 9, loss: 0.0014448240399360657, acc: 100.0, f1: 100.0, r: 0.7502510191262107
06/02/2019 04:00:33 step: 5328, epoch: 161, batch: 14, loss: 0.0019945278763771057, acc: 100.0, f1: 100.0, r: 0.7205688064390129
06/02/2019 04:00:33 step: 5333, epoch: 161, batch: 19, loss: 0.0018322914838790894, acc: 100.0, f1: 100.0, r: 0.6006868168453113
06/02/2019 04:00:34 step: 5338, epoch: 161, batch: 24, loss: 0.002713121473789215, acc: 100.0, f1: 100.0, r: 0.6747071431697332
06/02/2019 04:00:34 step: 5343, epoch: 161, batch: 29, loss: 0.002239622175693512, acc: 100.0, f1: 100.0, r: 0.7650582206395328
06/02/2019 04:00:34 *** evaluating ***
06/02/2019 04:00:34 step: 162, epoch: 161, acc: 50.85470085470085, f1: 23.07182183636951, r: 0.21953017356516666
06/02/2019 04:00:34 *** epoch: 163 ***
06/02/2019 04:00:34 *** training ***
06/02/2019 04:00:34 step: 5351, epoch: 162, batch: 4, loss: 0.0026944130659103394, acc: 100.0, f1: 100.0, r: 0.8162640596641001
06/02/2019 04:00:35 step: 5356, epoch: 162, batch: 9, loss: 0.0011625736951828003, acc: 100.0, f1: 100.0, r: 0.7941856459538884
06/02/2019 04:00:35 step: 5361, epoch: 162, batch: 14, loss: 0.0021090805530548096, acc: 100.0, f1: 100.0, r: 0.6870146194677847
06/02/2019 04:00:36 step: 5366, epoch: 162, batch: 19, loss: 0.0011664703488349915, acc: 100.0, f1: 100.0, r: 0.7154460363686518
06/02/2019 04:00:36 step: 5371, epoch: 162, batch: 24, loss: 0.0009986311197280884, acc: 100.0, f1: 100.0, r: 0.6992990795013018
06/02/2019 04:00:36 step: 5376, epoch: 162, batch: 29, loss: 0.001228533685207367, acc: 100.0, f1: 100.0, r: 0.6534596886579499
06/02/2019 04:00:36 *** evaluating ***
06/02/2019 04:00:37 step: 163, epoch: 162, acc: 50.85470085470085, f1: 23.102415762340193, r: 0.2209818279366836
06/02/2019 04:00:37 *** epoch: 164 ***
06/02/2019 04:00:37 *** training ***
06/02/2019 04:00:37 step: 5384, epoch: 163, batch: 4, loss: 0.001049831509590149, acc: 100.0, f1: 100.0, r: 0.7179004576590802
06/02/2019 04:00:37 step: 5389, epoch: 163, batch: 9, loss: 0.0012825429439544678, acc: 100.0, f1: 100.0, r: 0.8466796480368548
06/02/2019 04:00:38 step: 5394, epoch: 163, batch: 14, loss: 0.0029504820704460144, acc: 100.0, f1: 100.0, r: 0.8220386228388661
06/02/2019 04:00:38 step: 5399, epoch: 163, batch: 19, loss: 0.00353434681892395, acc: 100.0, f1: 100.0, r: 0.7484863787315529
06/02/2019 04:00:38 step: 5404, epoch: 163, batch: 24, loss: 0.0017629340291023254, acc: 100.0, f1: 100.0, r: 0.7193670753537217
06/02/2019 04:00:38 step: 5409, epoch: 163, batch: 29, loss: 0.0016646087169647217, acc: 100.0, f1: 100.0, r: 0.732120970928399
06/02/2019 04:00:39 *** evaluating ***
06/02/2019 04:00:39 step: 164, epoch: 163, acc: 51.28205128205128, f1: 23.20928045337875, r: 0.21560688826023583
06/02/2019 04:00:39 *** epoch: 165 ***
06/02/2019 04:00:39 *** training ***
06/02/2019 04:00:39 step: 5417, epoch: 164, batch: 4, loss: 0.001806013286113739, acc: 100.0, f1: 100.0, r: 0.7196789862835795
06/02/2019 04:00:39 step: 5422, epoch: 164, batch: 9, loss: 0.0021559372544288635, acc: 100.0, f1: 100.0, r: 0.7315485941917154
06/02/2019 04:00:40 step: 5427, epoch: 164, batch: 14, loss: 0.0010516196489334106, acc: 100.0, f1: 100.0, r: 0.7820650241827017
06/02/2019 04:00:40 step: 5432, epoch: 164, batch: 19, loss: 0.001073010265827179, acc: 100.0, f1: 100.0, r: 0.7683510194303517
06/02/2019 04:00:40 step: 5437, epoch: 164, batch: 24, loss: 0.0010672733187675476, acc: 100.0, f1: 100.0, r: 0.7281579976202656
06/02/2019 04:00:41 step: 5442, epoch: 164, batch: 29, loss: 0.0022740438580513, acc: 100.0, f1: 100.0, r: 0.7922869751610339
06/02/2019 04:00:41 *** evaluating ***
06/02/2019 04:00:41 step: 165, epoch: 164, acc: 49.14529914529914, f1: 22.957301611800762, r: 0.21842886448188326
06/02/2019 04:00:41 *** epoch: 166 ***
06/02/2019 04:00:41 *** training ***
06/02/2019 04:00:41 step: 5450, epoch: 165, batch: 4, loss: 0.0023243576288223267, acc: 100.0, f1: 100.0, r: 0.7894003335560559
06/02/2019 04:00:42 step: 5455, epoch: 165, batch: 9, loss: 0.0009738802909851074, acc: 100.0, f1: 100.0, r: 0.7633364932330967
06/02/2019 04:00:42 step: 5460, epoch: 165, batch: 14, loss: 0.001603543758392334, acc: 100.0, f1: 100.0, r: 0.7355682642791089
06/02/2019 04:00:42 step: 5465, epoch: 165, batch: 19, loss: 0.0020658597350120544, acc: 100.0, f1: 100.0, r: 0.7316707916313543
06/02/2019 04:00:43 step: 5470, epoch: 165, batch: 24, loss: 0.004040665924549103, acc: 100.0, f1: 100.0, r: 0.8184290469693426
06/02/2019 04:00:43 step: 5475, epoch: 165, batch: 29, loss: 0.004253052175045013, acc: 100.0, f1: 100.0, r: 0.6639444520898629
06/02/2019 04:00:43 *** evaluating ***
06/02/2019 04:00:43 step: 166, epoch: 165, acc: 50.427350427350426, f1: 22.743867643941577, r: 0.20732517090563032
06/02/2019 04:00:43 *** epoch: 167 ***
06/02/2019 04:00:43 *** training ***
06/02/2019 04:00:44 step: 5483, epoch: 166, batch: 4, loss: 0.0011183246970176697, acc: 100.0, f1: 100.0, r: 0.7676946541372488
06/02/2019 04:00:44 step: 5488, epoch: 166, batch: 9, loss: 0.0006678774952888489, acc: 100.0, f1: 100.0, r: 0.7065759251680842
06/02/2019 04:00:44 step: 5493, epoch: 166, batch: 14, loss: 0.0016024336218833923, acc: 100.0, f1: 100.0, r: 0.720456697506792
06/02/2019 04:00:45 step: 5498, epoch: 166, batch: 19, loss: 0.0013243556022644043, acc: 100.0, f1: 100.0, r: 0.6427420549152103
06/02/2019 04:00:45 step: 5503, epoch: 166, batch: 24, loss: 0.0007573142647743225, acc: 100.0, f1: 100.0, r: 0.6743727041943374
06/02/2019 04:00:45 step: 5508, epoch: 166, batch: 29, loss: 0.001224711537361145, acc: 100.0, f1: 100.0, r: 0.6838444609364482
06/02/2019 04:00:45 *** evaluating ***
06/02/2019 04:00:46 step: 167, epoch: 166, acc: 50.85470085470085, f1: 22.761342800536532, r: 0.2084735742458505
06/02/2019 04:00:46 *** epoch: 168 ***
06/02/2019 04:00:46 *** training ***
06/02/2019 04:00:46 step: 5516, epoch: 167, batch: 4, loss: 0.0011835992336273193, acc: 100.0, f1: 100.0, r: 0.7165708802707127
06/02/2019 04:00:46 step: 5521, epoch: 167, batch: 9, loss: 0.003478236496448517, acc: 100.0, f1: 100.0, r: 0.6644393802434204
06/02/2019 04:00:46 step: 5526, epoch: 167, batch: 14, loss: 0.0015951916575431824, acc: 100.0, f1: 100.0, r: 0.6742976878134274
06/02/2019 04:00:47 step: 5531, epoch: 167, batch: 19, loss: 0.001869097352027893, acc: 100.0, f1: 100.0, r: 0.7783769072342345
06/02/2019 04:00:47 step: 5536, epoch: 167, batch: 24, loss: 0.001357540488243103, acc: 100.0, f1: 100.0, r: 0.7895881521639361
06/02/2019 04:00:47 step: 5541, epoch: 167, batch: 29, loss: 0.0012269914150238037, acc: 100.0, f1: 100.0, r: 0.7921459651385878
06/02/2019 04:00:48 *** evaluating ***
06/02/2019 04:00:48 step: 168, epoch: 167, acc: 50.427350427350426, f1: 22.030065209230564, r: 0.20968807712616708
06/02/2019 04:00:48 *** epoch: 169 ***
06/02/2019 04:00:48 *** training ***
06/02/2019 04:00:48 step: 5549, epoch: 168, batch: 4, loss: 0.0013719350099563599, acc: 100.0, f1: 100.0, r: 0.8121753305810759
06/02/2019 04:00:48 step: 5554, epoch: 168, batch: 9, loss: 0.0013532936573028564, acc: 100.0, f1: 100.0, r: 0.7832201830993442
06/02/2019 04:00:49 step: 5559, epoch: 168, batch: 14, loss: 0.0012299492955207825, acc: 100.0, f1: 100.0, r: 0.6746207762272216
06/02/2019 04:00:49 step: 5564, epoch: 168, batch: 19, loss: 0.001814514398574829, acc: 100.0, f1: 100.0, r: 0.7264518139890871
06/02/2019 04:00:49 step: 5569, epoch: 168, batch: 24, loss: 0.0005457699298858643, acc: 100.0, f1: 100.0, r: 0.5755846773567049
06/02/2019 04:00:50 step: 5574, epoch: 168, batch: 29, loss: 0.0038005858659744263, acc: 100.0, f1: 100.0, r: 0.7629549319136802
06/02/2019 04:00:50 *** evaluating ***
06/02/2019 04:00:50 step: 169, epoch: 168, acc: 49.572649572649574, f1: 22.584396852118804, r: 0.21539238269093694
06/02/2019 04:00:50 *** epoch: 170 ***
06/02/2019 04:00:50 *** training ***
06/02/2019 04:00:50 step: 5582, epoch: 169, batch: 4, loss: 0.0011045709252357483, acc: 100.0, f1: 100.0, r: 0.6783220514731081
06/02/2019 04:00:51 step: 5587, epoch: 169, batch: 9, loss: 0.0008171349763870239, acc: 100.0, f1: 100.0, r: 0.8449506211256632
06/02/2019 04:00:51 step: 5592, epoch: 169, batch: 14, loss: 0.0005152076482772827, acc: 100.0, f1: 100.0, r: 0.6615524908164475
06/02/2019 04:00:51 step: 5597, epoch: 169, batch: 19, loss: 0.0009385943412780762, acc: 100.0, f1: 100.0, r: 0.6770394851833325
06/02/2019 04:00:52 step: 5602, epoch: 169, batch: 24, loss: 0.002430558204650879, acc: 100.0, f1: 100.0, r: 0.6843887240236177
06/02/2019 04:00:52 step: 5607, epoch: 169, batch: 29, loss: 0.0024184584617614746, acc: 100.0, f1: 100.0, r: 0.7260668736057299
06/02/2019 04:00:52 *** evaluating ***
06/02/2019 04:00:52 step: 170, epoch: 169, acc: 50.427350427350426, f1: 22.230519904161188, r: 0.2120551283755823
06/02/2019 04:00:52 *** epoch: 171 ***
06/02/2019 04:00:52 *** training ***
06/02/2019 04:00:52 step: 5615, epoch: 170, batch: 4, loss: 0.0011702179908752441, acc: 100.0, f1: 100.0, r: 0.7406540835704657
06/02/2019 04:00:53 step: 5620, epoch: 170, batch: 9, loss: 0.00189228355884552, acc: 100.0, f1: 100.0, r: 0.6658299887259778
06/02/2019 04:00:53 step: 5625, epoch: 170, batch: 14, loss: 0.0007956549525260925, acc: 100.0, f1: 100.0, r: 0.7637253540180775
06/02/2019 04:00:53 step: 5630, epoch: 170, batch: 19, loss: 0.0011091753840446472, acc: 100.0, f1: 100.0, r: 0.699304795281602
06/02/2019 04:00:53 step: 5635, epoch: 170, batch: 24, loss: 0.0025224238634109497, acc: 100.0, f1: 100.0, r: 0.7493341915225676
06/02/2019 04:00:54 step: 5640, epoch: 170, batch: 29, loss: 0.0011904239654541016, acc: 100.0, f1: 100.0, r: 0.7141991947659152
06/02/2019 04:00:54 *** evaluating ***
06/02/2019 04:00:54 step: 171, epoch: 170, acc: 50.85470085470085, f1: 21.89300062712041, r: 0.21007336021309467
06/02/2019 04:00:54 *** epoch: 172 ***
06/02/2019 04:00:54 *** training ***
06/02/2019 04:00:54 step: 5648, epoch: 171, batch: 4, loss: 0.002163425087928772, acc: 100.0, f1: 100.0, r: 0.6678055896102739
06/02/2019 04:00:55 step: 5653, epoch: 171, batch: 9, loss: 0.0009305626153945923, acc: 100.0, f1: 100.0, r: 0.7701922355131688
06/02/2019 04:00:55 step: 5658, epoch: 171, batch: 14, loss: 0.0021781399846076965, acc: 100.0, f1: 100.0, r: 0.6923778832860473
06/02/2019 04:00:55 step: 5663, epoch: 171, batch: 19, loss: 0.0006974935531616211, acc: 100.0, f1: 100.0, r: 0.772729533125905
06/02/2019 04:00:55 step: 5668, epoch: 171, batch: 24, loss: 0.0013278797268867493, acc: 100.0, f1: 100.0, r: 0.8019758173256609
06/02/2019 04:00:56 step: 5673, epoch: 171, batch: 29, loss: 0.0009758621454238892, acc: 100.0, f1: 100.0, r: 0.6890317258521119
06/02/2019 04:00:56 *** evaluating ***
06/02/2019 04:00:56 step: 172, epoch: 171, acc: 49.14529914529914, f1: 21.641391536925543, r: 0.20871010552606364
06/02/2019 04:00:56 *** epoch: 173 ***
06/02/2019 04:00:56 *** training ***
06/02/2019 04:00:56 step: 5681, epoch: 172, batch: 4, loss: 0.0013187229633331299, acc: 100.0, f1: 100.0, r: 0.7855740561903592
06/02/2019 04:00:57 step: 5686, epoch: 172, batch: 9, loss: 0.0007806718349456787, acc: 100.0, f1: 100.0, r: 0.7044823994262391
06/02/2019 04:00:57 step: 5691, epoch: 172, batch: 14, loss: 0.00040678679943084717, acc: 100.0, f1: 100.0, r: 0.6834864229403218
06/02/2019 04:00:57 step: 5696, epoch: 172, batch: 19, loss: 0.001596786081790924, acc: 100.0, f1: 100.0, r: 0.6878182032291447
06/02/2019 04:00:58 step: 5701, epoch: 172, batch: 24, loss: 0.0005099624395370483, acc: 100.0, f1: 100.0, r: 0.7600226415075287
06/02/2019 04:00:58 step: 5706, epoch: 172, batch: 29, loss: 0.0007144808769226074, acc: 100.0, f1: 100.0, r: 0.7983438447903952
06/02/2019 04:00:58 *** evaluating ***
06/02/2019 04:00:58 step: 173, epoch: 172, acc: 50.427350427350426, f1: 22.936033264980633, r: 0.21532856558621377
06/02/2019 04:00:58 *** epoch: 174 ***
06/02/2019 04:00:58 *** training ***
06/02/2019 04:00:58 step: 5714, epoch: 173, batch: 4, loss: 0.001371532678604126, acc: 100.0, f1: 100.0, r: 0.8279115008655212
06/02/2019 04:00:59 step: 5719, epoch: 173, batch: 9, loss: 0.0021383240818977356, acc: 100.0, f1: 100.0, r: 0.6855326111170353
06/02/2019 04:00:59 step: 5724, epoch: 173, batch: 14, loss: 0.0011734962463378906, acc: 100.0, f1: 100.0, r: 0.6374244984856123
06/02/2019 04:00:59 step: 5729, epoch: 173, batch: 19, loss: 0.0009096935391426086, acc: 100.0, f1: 100.0, r: 0.7543842196859784
06/02/2019 04:00:59 step: 5734, epoch: 173, batch: 24, loss: 0.001068398356437683, acc: 100.0, f1: 100.0, r: 0.7088022475169576
06/02/2019 04:01:00 step: 5739, epoch: 173, batch: 29, loss: 0.0017296820878982544, acc: 100.0, f1: 100.0, r: 0.7426214548282721
06/02/2019 04:01:00 *** evaluating ***
06/02/2019 04:01:00 step: 174, epoch: 173, acc: 49.572649572649574, f1: 21.497943942523566, r: 0.2113519047824094
06/02/2019 04:01:00 *** epoch: 175 ***
06/02/2019 04:01:00 *** training ***
06/02/2019 04:01:00 step: 5747, epoch: 174, batch: 4, loss: 0.0015526413917541504, acc: 100.0, f1: 100.0, r: 0.7135074099912941
06/02/2019 04:01:01 step: 5752, epoch: 174, batch: 9, loss: 0.0019143223762512207, acc: 100.0, f1: 100.0, r: 0.8098597928052131
06/02/2019 04:01:01 step: 5757, epoch: 174, batch: 14, loss: 0.001061558723449707, acc: 100.0, f1: 100.0, r: 0.7039574132266115
06/02/2019 04:01:01 step: 5762, epoch: 174, batch: 19, loss: 0.002235621213912964, acc: 100.0, f1: 100.0, r: 0.7261910003362985
06/02/2019 04:01:02 step: 5767, epoch: 174, batch: 24, loss: 0.0016630366444587708, acc: 100.0, f1: 100.0, r: 0.8091061842349739
06/02/2019 04:01:02 step: 5772, epoch: 174, batch: 29, loss: 0.0006267279386520386, acc: 100.0, f1: 100.0, r: 0.8088396946504204
06/02/2019 04:01:02 *** evaluating ***
06/02/2019 04:01:02 step: 175, epoch: 174, acc: 50.85470085470085, f1: 22.812899693703237, r: 0.21336290556674545
06/02/2019 04:01:02 *** epoch: 176 ***
06/02/2019 04:01:02 *** training ***
06/02/2019 04:01:03 step: 5780, epoch: 175, batch: 4, loss: 0.002829037606716156, acc: 100.0, f1: 100.0, r: 0.7260035102581084
06/02/2019 04:01:03 step: 5785, epoch: 175, batch: 9, loss: 0.0011096745729446411, acc: 100.0, f1: 100.0, r: 0.7824018430685618
06/02/2019 04:01:03 step: 5790, epoch: 175, batch: 14, loss: 0.0006692409515380859, acc: 100.0, f1: 100.0, r: 0.7219087084393492
06/02/2019 04:01:03 step: 5795, epoch: 175, batch: 19, loss: 0.0006034523248672485, acc: 100.0, f1: 100.0, r: 0.7238103823080528
06/02/2019 04:01:04 step: 5800, epoch: 175, batch: 24, loss: 0.0006734579801559448, acc: 100.0, f1: 100.0, r: 0.7300646974815701
06/02/2019 04:01:04 step: 5805, epoch: 175, batch: 29, loss: 0.0016197413206100464, acc: 100.0, f1: 100.0, r: 0.7768323637603817
06/02/2019 04:01:04 *** evaluating ***
06/02/2019 04:01:04 step: 176, epoch: 175, acc: 50.85470085470085, f1: 22.16104977288029, r: 0.2162324941065322
06/02/2019 04:01:04 *** epoch: 177 ***
06/02/2019 04:01:04 *** training ***
06/02/2019 04:01:05 step: 5813, epoch: 176, batch: 4, loss: 0.0006968826055526733, acc: 100.0, f1: 100.0, r: 0.6934831480975764
06/02/2019 04:01:05 step: 5818, epoch: 176, batch: 9, loss: 0.0014996379613876343, acc: 100.0, f1: 100.0, r: 0.7285711052450141
06/02/2019 04:01:05 step: 5823, epoch: 176, batch: 14, loss: 0.0013796314597129822, acc: 100.0, f1: 100.0, r: 0.7357623679875185
06/02/2019 04:01:06 step: 5828, epoch: 176, batch: 19, loss: 0.004998058080673218, acc: 100.0, f1: 100.0, r: 0.6637912461487591
06/02/2019 04:01:06 step: 5833, epoch: 176, batch: 24, loss: 0.0007353052496910095, acc: 100.0, f1: 100.0, r: 0.831648498178116
06/02/2019 04:01:06 step: 5838, epoch: 176, batch: 29, loss: 0.0008495375514030457, acc: 100.0, f1: 100.0, r: 0.6930921787066192
06/02/2019 04:01:06 *** evaluating ***
06/02/2019 04:01:07 step: 177, epoch: 176, acc: 50.427350427350426, f1: 22.808016111874196, r: 0.2152284060729588
06/02/2019 04:01:07 *** epoch: 178 ***
06/02/2019 04:01:07 *** training ***
06/02/2019 04:01:07 step: 5846, epoch: 177, batch: 4, loss: 0.0010906308889389038, acc: 100.0, f1: 100.0, r: 0.8189697251030851
06/02/2019 04:01:07 step: 5851, epoch: 177, batch: 9, loss: 0.0027121901512145996, acc: 100.0, f1: 100.0, r: 0.5377489562839153
06/02/2019 04:01:08 step: 5856, epoch: 177, batch: 14, loss: 0.0013888701796531677, acc: 100.0, f1: 100.0, r: 0.6747529111975326
06/02/2019 04:01:08 step: 5861, epoch: 177, batch: 19, loss: 0.0006068795919418335, acc: 100.0, f1: 100.0, r: 0.6553570009673575
06/02/2019 04:01:08 step: 5866, epoch: 177, batch: 24, loss: 0.001529783010482788, acc: 100.0, f1: 100.0, r: 0.6661557497941936
06/02/2019 04:01:09 step: 5871, epoch: 177, batch: 29, loss: 0.0016580671072006226, acc: 100.0, f1: 100.0, r: 0.6698621118034913
06/02/2019 04:01:09 *** evaluating ***
06/02/2019 04:01:09 step: 178, epoch: 177, acc: 50.0, f1: 22.358604629085583, r: 0.21714801768925934
06/02/2019 04:01:09 *** epoch: 179 ***
06/02/2019 04:01:09 *** training ***
06/02/2019 04:01:09 step: 5879, epoch: 178, batch: 4, loss: 0.0005344972014427185, acc: 100.0, f1: 100.0, r: 0.7764682187512395
06/02/2019 04:01:09 step: 5884, epoch: 178, batch: 9, loss: 0.0068567246198654175, acc: 100.0, f1: 100.0, r: 0.7906446295277386
06/02/2019 04:01:10 step: 5889, epoch: 178, batch: 14, loss: 0.001900985836982727, acc: 100.0, f1: 100.0, r: 0.745307314935432
06/02/2019 04:01:10 step: 5894, epoch: 178, batch: 19, loss: 0.0024638772010803223, acc: 100.0, f1: 100.0, r: 0.8478055320269652
06/02/2019 04:01:10 step: 5899, epoch: 178, batch: 24, loss: 0.0009839460253715515, acc: 100.0, f1: 100.0, r: 0.7231651487061562
06/02/2019 04:01:11 step: 5904, epoch: 178, batch: 29, loss: 0.0014650970697402954, acc: 100.0, f1: 100.0, r: 0.7794330705538393
06/02/2019 04:01:11 *** evaluating ***
06/02/2019 04:01:11 step: 179, epoch: 178, acc: 52.13675213675214, f1: 23.525239753914096, r: 0.22158327471817513
06/02/2019 04:01:11 *** epoch: 180 ***
06/02/2019 04:01:11 *** training ***
06/02/2019 04:01:11 step: 5912, epoch: 179, batch: 4, loss: 0.00048457831144332886, acc: 100.0, f1: 100.0, r: 0.7125611452335182
06/02/2019 04:01:11 step: 5917, epoch: 179, batch: 9, loss: 0.00042869895696640015, acc: 100.0, f1: 100.0, r: 0.8165462118117998
06/02/2019 04:01:12 step: 5922, epoch: 179, batch: 14, loss: 0.0012395456433296204, acc: 100.0, f1: 100.0, r: 0.8378340546726922
06/02/2019 04:01:12 step: 5927, epoch: 179, batch: 19, loss: 0.0016150623559951782, acc: 100.0, f1: 100.0, r: 0.6708675915906716
06/02/2019 04:01:12 step: 5932, epoch: 179, batch: 24, loss: 0.0011649057269096375, acc: 100.0, f1: 100.0, r: 0.7201215638084303
06/02/2019 04:01:13 step: 5937, epoch: 179, batch: 29, loss: 0.001536451280117035, acc: 100.0, f1: 100.0, r: 0.7964806072948186
06/02/2019 04:01:13 *** evaluating ***
06/02/2019 04:01:13 step: 180, epoch: 179, acc: 51.28205128205128, f1: 23.29923372779247, r: 0.22147072495880266
06/02/2019 04:01:13 *** epoch: 181 ***
06/02/2019 04:01:13 *** training ***
06/02/2019 04:01:13 step: 5945, epoch: 180, batch: 4, loss: 0.0005929619073867798, acc: 100.0, f1: 100.0, r: 0.7214371850231059
06/02/2019 04:01:14 step: 5950, epoch: 180, batch: 9, loss: 0.001576445996761322, acc: 100.0, f1: 100.0, r: 0.615402220804463
06/02/2019 04:01:14 step: 5955, epoch: 180, batch: 14, loss: 0.0007635205984115601, acc: 100.0, f1: 100.0, r: 0.7499089895536977
06/02/2019 04:01:14 step: 5960, epoch: 180, batch: 19, loss: 0.0006001442670822144, acc: 100.0, f1: 100.0, r: 0.8220631289362388
06/02/2019 04:01:14 step: 5965, epoch: 180, batch: 24, loss: 0.0006703212857246399, acc: 100.0, f1: 100.0, r: 0.7801008815540409
06/02/2019 04:01:15 step: 5970, epoch: 180, batch: 29, loss: 0.0018297433853149414, acc: 100.0, f1: 100.0, r: 0.766731053331571
06/02/2019 04:01:15 *** evaluating ***
06/02/2019 04:01:15 step: 181, epoch: 180, acc: 50.427350427350426, f1: 22.790489090981392, r: 0.2208603241992692
06/02/2019 04:01:15 *** epoch: 182 ***
06/02/2019 04:01:15 *** training ***
06/02/2019 04:01:15 step: 5978, epoch: 181, batch: 4, loss: 0.0008117258548736572, acc: 100.0, f1: 100.0, r: 0.733706452520138
06/02/2019 04:01:15 step: 5983, epoch: 181, batch: 9, loss: 0.0005599111318588257, acc: 100.0, f1: 100.0, r: 0.7939996186015092
06/02/2019 04:01:16 step: 5988, epoch: 181, batch: 14, loss: 0.00042875856161117554, acc: 100.0, f1: 100.0, r: 0.7548679016345204
06/02/2019 04:01:16 step: 5993, epoch: 181, batch: 19, loss: 0.0011449530720710754, acc: 100.0, f1: 100.0, r: 0.709352304891358
06/02/2019 04:01:16 step: 5998, epoch: 181, batch: 24, loss: 0.0008316636085510254, acc: 100.0, f1: 100.0, r: 0.8191476527051597
06/02/2019 04:01:17 step: 6003, epoch: 181, batch: 29, loss: 0.0014829710125923157, acc: 100.0, f1: 100.0, r: 0.7632085164181404
06/02/2019 04:01:17 *** evaluating ***
06/02/2019 04:01:17 step: 182, epoch: 181, acc: 50.85470085470085, f1: 22.684875975399493, r: 0.2210396371016033
06/02/2019 04:01:17 *** epoch: 183 ***
06/02/2019 04:01:17 *** training ***
06/02/2019 04:01:17 step: 6011, epoch: 182, batch: 4, loss: 0.000832684338092804, acc: 100.0, f1: 100.0, r: 0.7569710835535199
06/02/2019 04:01:18 step: 6016, epoch: 182, batch: 9, loss: 0.004168689250946045, acc: 100.0, f1: 100.0, r: 0.6797319566204919
06/02/2019 04:01:18 step: 6021, epoch: 182, batch: 14, loss: 0.001279786229133606, acc: 100.0, f1: 100.0, r: 0.7090297082272626
06/02/2019 04:01:18 step: 6026, epoch: 182, batch: 19, loss: 0.0004015788435935974, acc: 100.0, f1: 100.0, r: 0.7043753194373626
06/02/2019 04:01:19 step: 6031, epoch: 182, batch: 24, loss: 0.0005645677447319031, acc: 100.0, f1: 100.0, r: 0.7364811934487447
06/02/2019 04:01:19 step: 6036, epoch: 182, batch: 29, loss: 0.0008751228451728821, acc: 100.0, f1: 100.0, r: 0.8092889760891223
06/02/2019 04:01:19 *** evaluating ***
06/02/2019 04:01:19 step: 183, epoch: 182, acc: 52.13675213675214, f1: 23.5989660269436, r: 0.22203049375534825
06/02/2019 04:01:19 *** epoch: 184 ***
06/02/2019 04:01:19 *** training ***
06/02/2019 04:01:19 step: 6044, epoch: 183, batch: 4, loss: 0.001383177936077118, acc: 100.0, f1: 100.0, r: 0.7698665946529464
06/02/2019 04:01:20 step: 6049, epoch: 183, batch: 9, loss: 0.001925438642501831, acc: 100.0, f1: 100.0, r: 0.6539049268532331
06/02/2019 04:01:20 step: 6054, epoch: 183, batch: 14, loss: 0.0011390820145606995, acc: 100.0, f1: 100.0, r: 0.7240141203679453
06/02/2019 04:01:20 step: 6059, epoch: 183, batch: 19, loss: 0.0006300583481788635, acc: 100.0, f1: 100.0, r: 0.6900863970474315
06/02/2019 04:01:21 step: 6064, epoch: 183, batch: 24, loss: 0.0009859204292297363, acc: 100.0, f1: 100.0, r: 0.7195307445163543
06/02/2019 04:01:21 step: 6069, epoch: 183, batch: 29, loss: 0.0010108351707458496, acc: 100.0, f1: 100.0, r: 0.7613809132015616
06/02/2019 04:01:21 *** evaluating ***
06/02/2019 04:01:21 step: 184, epoch: 183, acc: 51.70940170940172, f1: 23.409433047499284, r: 0.21884837560646414
06/02/2019 04:01:21 *** epoch: 185 ***
06/02/2019 04:01:21 *** training ***
06/02/2019 04:01:21 step: 6077, epoch: 184, batch: 4, loss: 0.009449750185012817, acc: 100.0, f1: 100.0, r: 0.7546550500384908
06/02/2019 04:01:22 step: 6082, epoch: 184, batch: 9, loss: 0.0016992390155792236, acc: 100.0, f1: 100.0, r: 0.7517248843284278
06/02/2019 04:01:22 step: 6087, epoch: 184, batch: 14, loss: 0.0008687004446983337, acc: 100.0, f1: 100.0, r: 0.7316053241553292
06/02/2019 04:01:22 step: 6092, epoch: 184, batch: 19, loss: 0.001255616545677185, acc: 100.0, f1: 100.0, r: 0.7708711370000803
06/02/2019 04:01:23 step: 6097, epoch: 184, batch: 24, loss: 0.0007962062954902649, acc: 100.0, f1: 100.0, r: 0.738809317466927
06/02/2019 04:01:23 step: 6102, epoch: 184, batch: 29, loss: 0.0010519623756408691, acc: 100.0, f1: 100.0, r: 0.8634484540421187
06/02/2019 04:01:23 *** evaluating ***
06/02/2019 04:01:23 step: 185, epoch: 184, acc: 51.28205128205128, f1: 23.997146644205465, r: 0.2240084000496547
06/02/2019 04:01:23 *** epoch: 186 ***
06/02/2019 04:01:23 *** training ***
06/02/2019 04:01:24 step: 6110, epoch: 185, batch: 4, loss: 0.00032860785722732544, acc: 100.0, f1: 100.0, r: 0.6993219920617618
06/02/2019 04:01:24 step: 6115, epoch: 185, batch: 9, loss: 0.0006219819188117981, acc: 100.0, f1: 100.0, r: 0.7170353552026998
06/02/2019 04:01:24 step: 6120, epoch: 185, batch: 14, loss: 0.000886969268321991, acc: 100.0, f1: 100.0, r: 0.6357487854641449
06/02/2019 04:01:25 step: 6125, epoch: 185, batch: 19, loss: 0.0013215094804763794, acc: 100.0, f1: 100.0, r: 0.7442837151972571
06/02/2019 04:01:25 step: 6130, epoch: 185, batch: 24, loss: 0.0008481666445732117, acc: 100.0, f1: 100.0, r: 0.6989197292901851
06/02/2019 04:01:25 step: 6135, epoch: 185, batch: 29, loss: 0.0003084242343902588, acc: 100.0, f1: 100.0, r: 0.8062460660255872
06/02/2019 04:01:25 *** evaluating ***
06/02/2019 04:01:26 step: 186, epoch: 185, acc: 52.13675213675214, f1: 23.561825056326235, r: 0.22249010029024002
06/02/2019 04:01:26 *** epoch: 187 ***
06/02/2019 04:01:26 *** training ***
06/02/2019 04:01:26 step: 6143, epoch: 186, batch: 4, loss: 0.0005856603384017944, acc: 100.0, f1: 100.0, r: 0.7948377642924317
06/02/2019 04:01:26 step: 6148, epoch: 186, batch: 9, loss: 0.0008891746401786804, acc: 100.0, f1: 100.0, r: 0.7120321075415001
06/02/2019 04:01:26 step: 6153, epoch: 186, batch: 14, loss: 0.004342690110206604, acc: 100.0, f1: 100.0, r: 0.7636835537156367
06/02/2019 04:01:27 step: 6158, epoch: 186, batch: 19, loss: 0.0031997114419937134, acc: 100.0, f1: 100.0, r: 0.7189617468896511
06/02/2019 04:01:27 step: 6163, epoch: 186, batch: 24, loss: 0.0016938000917434692, acc: 100.0, f1: 100.0, r: 0.6960901925982353
06/02/2019 04:01:27 step: 6168, epoch: 186, batch: 29, loss: 0.0009311586618423462, acc: 100.0, f1: 100.0, r: 0.7257255619389377
06/02/2019 04:01:27 *** evaluating ***
06/02/2019 04:01:27 step: 187, epoch: 186, acc: 51.28205128205128, f1: 22.662458394008738, r: 0.2198558427325386
06/02/2019 04:01:27 *** epoch: 188 ***
06/02/2019 04:01:27 *** training ***
06/02/2019 04:01:28 step: 6176, epoch: 187, batch: 4, loss: 0.001028202474117279, acc: 100.0, f1: 100.0, r: 0.8238123751205768
06/02/2019 04:01:28 step: 6181, epoch: 187, batch: 9, loss: 0.0005422830581665039, acc: 100.0, f1: 100.0, r: 0.6989097838281411
06/02/2019 04:01:28 step: 6186, epoch: 187, batch: 14, loss: 0.0006707608699798584, acc: 100.0, f1: 100.0, r: 0.7337812888250486
06/02/2019 04:01:29 step: 6191, epoch: 187, batch: 19, loss: 0.0013806074857711792, acc: 100.0, f1: 100.0, r: 0.6780379682831514
06/02/2019 04:01:29 step: 6196, epoch: 187, batch: 24, loss: 0.0006641373038291931, acc: 100.0, f1: 100.0, r: 0.7440321952706944
06/02/2019 04:01:30 step: 6201, epoch: 187, batch: 29, loss: 0.001026056706905365, acc: 100.0, f1: 100.0, r: 0.764825767668015
06/02/2019 04:01:30 *** evaluating ***
06/02/2019 04:01:30 step: 188, epoch: 187, acc: 50.85470085470085, f1: 22.59263056169004, r: 0.22060826022397223
06/02/2019 04:01:30 *** epoch: 189 ***
06/02/2019 04:01:30 *** training ***
06/02/2019 04:01:30 step: 6209, epoch: 188, batch: 4, loss: 0.0010602548718452454, acc: 100.0, f1: 100.0, r: 0.6451730013455639
06/02/2019 04:01:31 step: 6214, epoch: 188, batch: 9, loss: 0.0007177442312240601, acc: 100.0, f1: 100.0, r: 0.7622289485710702
06/02/2019 04:01:31 step: 6219, epoch: 188, batch: 14, loss: 0.0011302009224891663, acc: 100.0, f1: 100.0, r: 0.778245708315202
06/02/2019 04:01:31 step: 6224, epoch: 188, batch: 19, loss: 0.0013959407806396484, acc: 100.0, f1: 100.0, r: 0.7620521291318604
06/02/2019 04:01:31 step: 6229, epoch: 188, batch: 24, loss: 0.003703482449054718, acc: 100.0, f1: 100.0, r: 0.6878705041863147
06/02/2019 04:01:32 step: 6234, epoch: 188, batch: 29, loss: 0.000887945294380188, acc: 100.0, f1: 100.0, r: 0.7215076300117989
06/02/2019 04:01:32 *** evaluating ***
06/02/2019 04:01:32 step: 189, epoch: 188, acc: 50.0, f1: 22.78078007518797, r: 0.2241588356858445
06/02/2019 04:01:32 *** epoch: 190 ***
06/02/2019 04:01:32 *** training ***
06/02/2019 04:01:32 step: 6242, epoch: 189, batch: 4, loss: 0.0030143260955810547, acc: 100.0, f1: 100.0, r: 0.6961373481769803
06/02/2019 04:01:33 step: 6247, epoch: 189, batch: 9, loss: 0.0013117417693138123, acc: 100.0, f1: 100.0, r: 0.7630478653648219
06/02/2019 04:01:33 step: 6252, epoch: 189, batch: 14, loss: 0.0006306394934654236, acc: 100.0, f1: 100.0, r: 0.6868725901457158
06/02/2019 04:01:33 step: 6257, epoch: 189, batch: 19, loss: 0.0007376000285148621, acc: 100.0, f1: 100.0, r: 0.668239511182679
06/02/2019 04:01:34 step: 6262, epoch: 189, batch: 24, loss: 0.0007083341479301453, acc: 100.0, f1: 100.0, r: 0.6782390709993636
06/02/2019 04:01:34 step: 6267, epoch: 189, batch: 29, loss: 0.0007135495543479919, acc: 100.0, f1: 100.0, r: 0.8018336393365314
06/02/2019 04:01:34 *** evaluating ***
06/02/2019 04:01:34 step: 190, epoch: 189, acc: 50.85470085470085, f1: 23.239110003269044, r: 0.22544451648513375
06/02/2019 04:01:34 *** epoch: 191 ***
06/02/2019 04:01:34 *** training ***
06/02/2019 04:01:35 step: 6275, epoch: 190, batch: 4, loss: 0.0009838640689849854, acc: 100.0, f1: 100.0, r: 0.7657468610690614
06/02/2019 04:01:35 step: 6280, epoch: 190, batch: 9, loss: 0.002520449459552765, acc: 100.0, f1: 100.0, r: 0.7098293079391371
06/02/2019 04:01:35 step: 6285, epoch: 190, batch: 14, loss: 0.0010358765721321106, acc: 100.0, f1: 100.0, r: 0.6941722651396945
06/02/2019 04:01:36 step: 6290, epoch: 190, batch: 19, loss: 0.001707218587398529, acc: 100.0, f1: 100.0, r: 0.7612879049660394
06/02/2019 04:01:36 step: 6295, epoch: 190, batch: 24, loss: 0.000894792377948761, acc: 100.0, f1: 100.0, r: 0.7445470096655051
06/02/2019 04:01:36 step: 6300, epoch: 190, batch: 29, loss: 0.0011929720640182495, acc: 100.0, f1: 100.0, r: 0.7840960650273439
06/02/2019 04:01:36 *** evaluating ***
06/02/2019 04:01:37 step: 191, epoch: 190, acc: 50.0, f1: 22.779252947863498, r: 0.22124681569352928
06/02/2019 04:01:37 *** epoch: 192 ***
06/02/2019 04:01:37 *** training ***
06/02/2019 04:01:37 step: 6308, epoch: 191, batch: 4, loss: 0.0012882798910140991, acc: 100.0, f1: 100.0, r: 0.7791895224197939
06/02/2019 04:01:37 step: 6313, epoch: 191, batch: 9, loss: 0.000710219144821167, acc: 100.0, f1: 100.0, r: 0.7393035365478366
06/02/2019 04:01:37 step: 6318, epoch: 191, batch: 14, loss: 0.0018539205193519592, acc: 100.0, f1: 100.0, r: 0.6360942427604435
06/02/2019 04:01:38 step: 6323, epoch: 191, batch: 19, loss: 0.0006749927997589111, acc: 100.0, f1: 100.0, r: 0.7755542271190465
06/02/2019 04:01:38 step: 6328, epoch: 191, batch: 24, loss: 0.000875227153301239, acc: 100.0, f1: 100.0, r: 0.739705346916609
06/02/2019 04:01:38 step: 6333, epoch: 191, batch: 29, loss: 0.0007590726017951965, acc: 100.0, f1: 100.0, r: 0.780514574175167
06/02/2019 04:01:38 *** evaluating ***
06/02/2019 04:01:39 step: 192, epoch: 191, acc: 50.427350427350426, f1: 22.93382999775764, r: 0.2217181261527632
06/02/2019 04:01:39 *** epoch: 193 ***
06/02/2019 04:01:39 *** training ***
06/02/2019 04:01:39 step: 6341, epoch: 192, batch: 4, loss: 0.0008055418729782104, acc: 100.0, f1: 100.0, r: 0.7881170859151373
06/02/2019 04:01:39 step: 6346, epoch: 192, batch: 9, loss: 0.0008244067430496216, acc: 100.0, f1: 100.0, r: 0.8058140471308847
06/02/2019 04:01:39 step: 6351, epoch: 192, batch: 14, loss: 0.0008175596594810486, acc: 100.0, f1: 100.0, r: 0.7376007729192329
06/02/2019 04:01:40 step: 6356, epoch: 192, batch: 19, loss: 0.0008543580770492554, acc: 100.0, f1: 100.0, r: 0.7829124387130065
06/02/2019 04:01:40 step: 6361, epoch: 192, batch: 24, loss: 0.00039664655923843384, acc: 100.0, f1: 100.0, r: 0.8267020285384309
06/02/2019 04:01:40 step: 6366, epoch: 192, batch: 29, loss: 0.0026648789644241333, acc: 100.0, f1: 100.0, r: 0.6702560202501686
06/02/2019 04:01:41 *** evaluating ***
06/02/2019 04:01:41 step: 193, epoch: 192, acc: 51.70940170940172, f1: 24.029342195368955, r: 0.22645920783716345
06/02/2019 04:01:41 *** epoch: 194 ***
06/02/2019 04:01:41 *** training ***
06/02/2019 04:01:41 step: 6374, epoch: 193, batch: 4, loss: 0.001523032784461975, acc: 100.0, f1: 100.0, r: 0.6757189573163509
06/02/2019 04:01:41 step: 6379, epoch: 193, batch: 9, loss: 0.0004597902297973633, acc: 100.0, f1: 100.0, r: 0.5877803908657916
06/02/2019 04:01:42 step: 6384, epoch: 193, batch: 14, loss: 0.001292683184146881, acc: 100.0, f1: 100.0, r: 0.7231780899055574
06/02/2019 04:01:42 step: 6389, epoch: 193, batch: 19, loss: 0.0007265880703926086, acc: 100.0, f1: 100.0, r: 0.8237391433466199
06/02/2019 04:01:42 step: 6394, epoch: 193, batch: 24, loss: 0.0008751899003982544, acc: 100.0, f1: 100.0, r: 0.7936381739695504
06/02/2019 04:01:43 step: 6399, epoch: 193, batch: 29, loss: 0.0013636276125907898, acc: 100.0, f1: 100.0, r: 0.7877102545456156
06/02/2019 04:01:43 *** evaluating ***
06/02/2019 04:01:43 step: 194, epoch: 193, acc: 50.427350427350426, f1: 22.953454408079022, r: 0.22051291654285593
06/02/2019 04:01:43 *** epoch: 195 ***
06/02/2019 04:01:43 *** training ***
06/02/2019 04:01:43 step: 6407, epoch: 194, batch: 4, loss: 0.0008420050144195557, acc: 100.0, f1: 100.0, r: 0.816278820599971
06/02/2019 04:01:43 step: 6412, epoch: 194, batch: 9, loss: 0.0007903054356575012, acc: 100.0, f1: 100.0, r: 0.7521680353162836
06/02/2019 04:01:44 step: 6417, epoch: 194, batch: 14, loss: 0.001033395528793335, acc: 100.0, f1: 100.0, r: 0.7389206252386138
06/02/2019 04:01:44 step: 6422, epoch: 194, batch: 19, loss: 0.00048632174730300903, acc: 100.0, f1: 100.0, r: 0.7999208177303644
06/02/2019 04:01:44 step: 6427, epoch: 194, batch: 24, loss: 0.00035855919122695923, acc: 100.0, f1: 100.0, r: 0.8387433331183415
06/02/2019 04:01:45 step: 6432, epoch: 194, batch: 29, loss: 0.0010792240500450134, acc: 100.0, f1: 100.0, r: 0.6826541683507975
06/02/2019 04:01:45 *** evaluating ***
06/02/2019 04:01:45 step: 195, epoch: 194, acc: 50.427350427350426, f1: 22.383275833641445, r: 0.2211378387169903
06/02/2019 04:01:45 *** epoch: 196 ***
06/02/2019 04:01:45 *** training ***
06/02/2019 04:01:45 step: 6440, epoch: 195, batch: 4, loss: 0.0004370585083961487, acc: 100.0, f1: 100.0, r: 0.7392247776818809
06/02/2019 04:01:46 step: 6445, epoch: 195, batch: 9, loss: 0.0006979703903198242, acc: 100.0, f1: 100.0, r: 0.7100179073744203
06/02/2019 04:01:46 step: 6450, epoch: 195, batch: 14, loss: 0.0006862804293632507, acc: 100.0, f1: 100.0, r: 0.804885924948185
06/02/2019 04:01:46 step: 6455, epoch: 195, batch: 19, loss: 0.0012783333659172058, acc: 100.0, f1: 100.0, r: 0.7178196075065288
06/02/2019 04:01:47 step: 6460, epoch: 195, batch: 24, loss: 0.0008460655808448792, acc: 100.0, f1: 100.0, r: 0.6247453797034318
06/02/2019 04:01:47 step: 6465, epoch: 195, batch: 29, loss: 0.002276133745908737, acc: 100.0, f1: 100.0, r: 0.5944146967468035
06/02/2019 04:01:47 *** evaluating ***
06/02/2019 04:01:47 step: 196, epoch: 195, acc: 51.28205128205128, f1: 22.360461176250652, r: 0.21592360623800796
06/02/2019 04:01:47 *** epoch: 197 ***
06/02/2019 04:01:47 *** training ***
06/02/2019 04:01:48 step: 6473, epoch: 196, batch: 4, loss: 0.000756971538066864, acc: 100.0, f1: 100.0, r: 0.7405806288739283
06/02/2019 04:01:48 step: 6478, epoch: 196, batch: 9, loss: 0.0005974844098091125, acc: 100.0, f1: 100.0, r: 0.7846440763779695
06/02/2019 04:01:48 step: 6483, epoch: 196, batch: 14, loss: 0.0006464272737503052, acc: 100.0, f1: 100.0, r: 0.7246740883782381
06/02/2019 04:01:49 step: 6488, epoch: 196, batch: 19, loss: 0.0005914345383644104, acc: 100.0, f1: 100.0, r: 0.8065343304408797
06/02/2019 04:01:49 step: 6493, epoch: 196, batch: 24, loss: 0.0015259310603141785, acc: 100.0, f1: 100.0, r: 0.7498872755229112
06/02/2019 04:01:49 step: 6498, epoch: 196, batch: 29, loss: 0.0010140761733055115, acc: 100.0, f1: 100.0, r: 0.7157506752506484
06/02/2019 04:01:50 *** evaluating ***
06/02/2019 04:01:50 step: 197, epoch: 196, acc: 47.863247863247864, f1: 21.121970876704097, r: 0.21086610695630081
06/02/2019 04:01:50 *** epoch: 198 ***
06/02/2019 04:01:50 *** training ***
06/02/2019 04:01:50 step: 6506, epoch: 197, batch: 4, loss: 0.0008987709879875183, acc: 100.0, f1: 100.0, r: 0.6365215513256252
06/02/2019 04:01:50 step: 6511, epoch: 197, batch: 9, loss: 0.0011195167899131775, acc: 100.0, f1: 100.0, r: 0.6816549808344208
06/02/2019 04:01:51 step: 6516, epoch: 197, batch: 14, loss: 0.0012921243906021118, acc: 100.0, f1: 100.0, r: 0.8332230109804734
06/02/2019 04:01:51 step: 6521, epoch: 197, batch: 19, loss: 0.0012964531779289246, acc: 100.0, f1: 100.0, r: 0.7148416244057693
06/02/2019 04:01:51 step: 6526, epoch: 197, batch: 24, loss: 0.0005738139152526855, acc: 100.0, f1: 100.0, r: 0.8498307850854488
06/02/2019 04:01:52 step: 6531, epoch: 197, batch: 29, loss: 0.0012564733624458313, acc: 100.0, f1: 100.0, r: 0.7407752650526951
06/02/2019 04:01:52 *** evaluating ***
06/02/2019 04:01:52 step: 198, epoch: 197, acc: 52.13675213675214, f1: 23.618349269245126, r: 0.21814477776375768
06/02/2019 04:01:52 *** epoch: 199 ***
06/02/2019 04:01:52 *** training ***
06/02/2019 04:01:52 step: 6539, epoch: 198, batch: 4, loss: 0.0007386058568954468, acc: 100.0, f1: 100.0, r: 0.818189796224415
06/02/2019 04:01:53 step: 6544, epoch: 198, batch: 9, loss: 0.0007163360714912415, acc: 100.0, f1: 100.0, r: 0.7818234766123014
06/02/2019 04:01:53 step: 6549, epoch: 198, batch: 14, loss: 0.0011914074420928955, acc: 100.0, f1: 100.0, r: 0.7305401054524377
06/02/2019 04:01:53 step: 6554, epoch: 198, batch: 19, loss: 0.0012295842170715332, acc: 100.0, f1: 100.0, r: 0.8151473074044205
06/02/2019 04:01:54 step: 6559, epoch: 198, batch: 24, loss: 0.0010251179337501526, acc: 100.0, f1: 100.0, r: 0.6777192822978276
06/02/2019 04:01:54 step: 6564, epoch: 198, batch: 29, loss: 0.0004135817289352417, acc: 100.0, f1: 100.0, r: 0.8209059890275852
06/02/2019 04:01:54 *** evaluating ***
06/02/2019 04:01:54 step: 199, epoch: 198, acc: 50.427350427350426, f1: 22.830383765738453, r: 0.22649976096650443
06/02/2019 04:01:54 *** epoch: 200 ***
06/02/2019 04:01:54 *** training ***
06/02/2019 04:01:55 step: 6572, epoch: 199, batch: 4, loss: 0.0005957037210464478, acc: 100.0, f1: 100.0, r: 0.7324683740266323
06/02/2019 04:01:55 step: 6577, epoch: 199, batch: 9, loss: 0.0011087730526924133, acc: 100.0, f1: 100.0, r: 0.7993447483961957
06/02/2019 04:01:55 step: 6582, epoch: 199, batch: 14, loss: 0.0008389800786972046, acc: 100.0, f1: 100.0, r: 0.7817904661890869
06/02/2019 04:01:56 step: 6587, epoch: 199, batch: 19, loss: 0.0014359429478645325, acc: 100.0, f1: 100.0, r: 0.7644266426256046
06/02/2019 04:01:56 step: 6592, epoch: 199, batch: 24, loss: 0.0012259259819984436, acc: 100.0, f1: 100.0, r: 0.6869688700760429
06/02/2019 04:01:56 step: 6597, epoch: 199, batch: 29, loss: 0.0017703846096992493, acc: 100.0, f1: 100.0, r: 0.7851137522512541
06/02/2019 04:01:57 *** evaluating ***
06/02/2019 04:01:57 step: 200, epoch: 199, acc: 50.0, f1: 22.112633096716948, r: 0.22453425506163252
06/02/2019 04:01:57 *** epoch: 201 ***
06/02/2019 04:01:57 *** training ***
06/02/2019 04:01:57 step: 6605, epoch: 200, batch: 4, loss: 0.0008512288331985474, acc: 100.0, f1: 100.0, r: 0.6270913380229867
06/02/2019 04:01:57 step: 6610, epoch: 200, batch: 9, loss: 0.0008001700043678284, acc: 100.0, f1: 100.0, r: 0.8187808077333181
06/02/2019 04:01:58 step: 6615, epoch: 200, batch: 14, loss: 0.0031925514340400696, acc: 100.0, f1: 100.0, r: 0.8331062015893815
06/02/2019 04:01:58 step: 6620, epoch: 200, batch: 19, loss: 0.0015772581100463867, acc: 100.0, f1: 100.0, r: 0.661847078990653
06/02/2019 04:01:58 step: 6625, epoch: 200, batch: 24, loss: 0.001051492989063263, acc: 100.0, f1: 100.0, r: 0.6856741911094087
06/02/2019 04:01:59 step: 6630, epoch: 200, batch: 29, loss: 0.0016556605696678162, acc: 100.0, f1: 100.0, r: 0.7884689026101097
06/02/2019 04:01:59 *** evaluating ***
06/02/2019 04:01:59 step: 201, epoch: 200, acc: 50.85470085470085, f1: 23.617196142007472, r: 0.22546416925901944
06/02/2019 04:01:59 *** epoch: 202 ***
06/02/2019 04:01:59 *** training ***
06/02/2019 04:01:59 step: 6638, epoch: 201, batch: 4, loss: 0.0004754588007926941, acc: 100.0, f1: 100.0, r: 0.8079948218306663
06/02/2019 04:02:00 step: 6643, epoch: 201, batch: 9, loss: 0.0007763952016830444, acc: 100.0, f1: 100.0, r: 0.7458224541143168
06/02/2019 04:02:00 step: 6648, epoch: 201, batch: 14, loss: 0.0007573887705802917, acc: 100.0, f1: 100.0, r: 0.7118597511556102
06/02/2019 04:02:00 step: 6653, epoch: 201, batch: 19, loss: 0.0009043961763381958, acc: 100.0, f1: 100.0, r: 0.8105385090223394
06/02/2019 04:02:01 step: 6658, epoch: 201, batch: 24, loss: 0.0007835477590560913, acc: 100.0, f1: 100.0, r: 0.6898443943687893
06/02/2019 04:02:01 step: 6663, epoch: 201, batch: 29, loss: 0.0008916482329368591, acc: 100.0, f1: 100.0, r: 0.7312870991070767
06/02/2019 04:02:01 *** evaluating ***
06/02/2019 04:02:01 step: 202, epoch: 201, acc: 50.85470085470085, f1: 22.144882554085463, r: 0.22449832245295295
06/02/2019 04:02:01 *** epoch: 203 ***
06/02/2019 04:02:01 *** training ***
06/02/2019 04:02:02 step: 6671, epoch: 202, batch: 4, loss: 0.0011462569236755371, acc: 100.0, f1: 100.0, r: 0.7082284270188274
06/02/2019 04:02:02 step: 6676, epoch: 202, batch: 9, loss: 0.0006095170974731445, acc: 100.0, f1: 100.0, r: 0.8344908332747478
06/02/2019 04:02:02 step: 6681, epoch: 202, batch: 14, loss: 0.000325717031955719, acc: 100.0, f1: 100.0, r: 0.6892849430823799
06/02/2019 04:02:03 step: 6686, epoch: 202, batch: 19, loss: 0.0017583519220352173, acc: 100.0, f1: 100.0, r: 0.7477218425842336
06/02/2019 04:02:03 step: 6691, epoch: 202, batch: 24, loss: 0.0006255581974983215, acc: 100.0, f1: 100.0, r: 0.7121471616555775
06/02/2019 04:02:04 step: 6696, epoch: 202, batch: 29, loss: 0.0010813996195793152, acc: 100.0, f1: 100.0, r: 0.6741553721428336
06/02/2019 04:02:04 *** evaluating ***
06/02/2019 04:02:04 step: 203, epoch: 202, acc: 50.85470085470085, f1: 22.814772048102434, r: 0.2286321524811492
06/02/2019 04:02:04 *** epoch: 204 ***
06/02/2019 04:02:04 *** training ***
06/02/2019 04:02:04 step: 6704, epoch: 203, batch: 4, loss: 0.001983560621738434, acc: 100.0, f1: 100.0, r: 0.696491002356495
06/02/2019 04:02:05 step: 6709, epoch: 203, batch: 9, loss: 0.0022500529885292053, acc: 100.0, f1: 100.0, r: 0.7493257597536844
06/02/2019 04:02:05 step: 6714, epoch: 203, batch: 14, loss: 0.0008342787623405457, acc: 100.0, f1: 100.0, r: 0.8066516525527682
06/02/2019 04:02:05 step: 6719, epoch: 203, batch: 19, loss: 0.0007005482912063599, acc: 100.0, f1: 100.0, r: 0.7390936484390663
06/02/2019 04:02:06 step: 6724, epoch: 203, batch: 24, loss: 0.0006591081619262695, acc: 100.0, f1: 100.0, r: 0.6857079253978838
06/02/2019 04:02:06 step: 6729, epoch: 203, batch: 29, loss: 0.0011342987418174744, acc: 100.0, f1: 100.0, r: 0.7961224752678188
06/02/2019 04:02:06 *** evaluating ***
06/02/2019 04:02:06 step: 204, epoch: 203, acc: 50.427350427350426, f1: 21.95307333546614, r: 0.22608032966336433
06/02/2019 04:02:06 *** epoch: 205 ***
06/02/2019 04:02:06 *** training ***
06/02/2019 04:02:07 step: 6737, epoch: 204, batch: 4, loss: 0.001004733145236969, acc: 100.0, f1: 100.0, r: 0.6369832105349451
06/02/2019 04:02:07 step: 6742, epoch: 204, batch: 9, loss: 0.0012693628668785095, acc: 100.0, f1: 100.0, r: 0.8363916013112017
06/02/2019 04:02:07 step: 6747, epoch: 204, batch: 14, loss: 0.0021229535341262817, acc: 100.0, f1: 100.0, r: 0.7885694508909205
06/02/2019 04:02:08 step: 6752, epoch: 204, batch: 19, loss: 0.001108802855014801, acc: 100.0, f1: 100.0, r: 0.7605196347158049
06/02/2019 04:02:08 step: 6757, epoch: 204, batch: 24, loss: 0.0013219639658927917, acc: 100.0, f1: 100.0, r: 0.7790139416642426
06/02/2019 04:02:09 step: 6762, epoch: 204, batch: 29, loss: 0.000699654221534729, acc: 100.0, f1: 100.0, r: 0.844297075766303
06/02/2019 04:02:09 *** evaluating ***
06/02/2019 04:02:09 step: 205, epoch: 204, acc: 50.0, f1: 22.021943463780353, r: 0.22870090060509646
06/02/2019 04:02:09 *** epoch: 206 ***
06/02/2019 04:02:09 *** training ***
06/02/2019 04:02:09 step: 6770, epoch: 205, batch: 4, loss: 0.0016058161854743958, acc: 100.0, f1: 100.0, r: 0.5945857220034944
06/02/2019 04:02:10 step: 6775, epoch: 205, batch: 9, loss: 0.0020271465182304382, acc: 100.0, f1: 100.0, r: 0.6934373823452586
06/02/2019 04:02:10 step: 6780, epoch: 205, batch: 14, loss: 0.0013441890478134155, acc: 100.0, f1: 100.0, r: 0.8440188094898908
06/02/2019 04:02:10 step: 6785, epoch: 205, batch: 19, loss: 0.0012309327721595764, acc: 100.0, f1: 100.0, r: 0.7084178690898729
06/02/2019 04:02:11 step: 6790, epoch: 205, batch: 24, loss: 0.0020895153284072876, acc: 100.0, f1: 100.0, r: 0.8666779712107557
06/02/2019 04:02:11 step: 6795, epoch: 205, batch: 29, loss: 0.0008775815367698669, acc: 100.0, f1: 100.0, r: 0.8074054937863739
06/02/2019 04:02:11 *** evaluating ***
06/02/2019 04:02:11 step: 206, epoch: 205, acc: 50.0, f1: 22.141552232318706, r: 0.22416634932518087
06/02/2019 04:02:11 *** epoch: 207 ***
06/02/2019 04:02:11 *** training ***
06/02/2019 04:02:12 step: 6803, epoch: 206, batch: 4, loss: 0.002139054238796234, acc: 100.0, f1: 100.0, r: 0.6743654194096884
06/02/2019 04:02:12 step: 6808, epoch: 206, batch: 9, loss: 0.000819765031337738, acc: 100.0, f1: 100.0, r: 0.8342211974359046
06/02/2019 04:02:12 step: 6813, epoch: 206, batch: 14, loss: 0.0004000142216682434, acc: 100.0, f1: 100.0, r: 0.7927316384551736
06/02/2019 04:02:13 step: 6818, epoch: 206, batch: 19, loss: 0.0008263140916824341, acc: 100.0, f1: 100.0, r: 0.7488267169587852
06/02/2019 04:02:13 step: 6823, epoch: 206, batch: 24, loss: 0.0006631612777709961, acc: 100.0, f1: 100.0, r: 0.7991729831478386
06/02/2019 04:02:13 step: 6828, epoch: 206, batch: 29, loss: 0.0005562379956245422, acc: 100.0, f1: 100.0, r: 0.7385189675060094
06/02/2019 04:02:13 *** evaluating ***
06/02/2019 04:02:14 step: 207, epoch: 206, acc: 50.0, f1: 21.91679732656559, r: 0.22352623800647298
06/02/2019 04:02:14 *** epoch: 208 ***
06/02/2019 04:02:14 *** training ***
06/02/2019 04:02:14 step: 6836, epoch: 207, batch: 4, loss: 0.002603471279144287, acc: 100.0, f1: 100.0, r: 0.7634150921589631
06/02/2019 04:02:14 step: 6841, epoch: 207, batch: 9, loss: 0.0008161589503288269, acc: 100.0, f1: 100.0, r: 0.6990102491304131
06/02/2019 04:02:15 step: 6846, epoch: 207, batch: 14, loss: 0.0007935017347335815, acc: 100.0, f1: 100.0, r: 0.7498960070458491
06/02/2019 04:02:15 step: 6851, epoch: 207, batch: 19, loss: 0.0004796460270881653, acc: 100.0, f1: 100.0, r: 0.721899219135844
06/02/2019 04:02:15 step: 6856, epoch: 207, batch: 24, loss: 0.0006677806377410889, acc: 100.0, f1: 100.0, r: 0.7696214068705409
06/02/2019 04:02:16 step: 6861, epoch: 207, batch: 29, loss: 0.0009841546416282654, acc: 100.0, f1: 100.0, r: 0.7594232109926075
06/02/2019 04:02:16 *** evaluating ***
06/02/2019 04:02:16 step: 208, epoch: 207, acc: 50.0, f1: 22.079915145866746, r: 0.22170114506842561
06/02/2019 04:02:16 *** epoch: 209 ***
06/02/2019 04:02:16 *** training ***
06/02/2019 04:02:16 step: 6869, epoch: 208, batch: 4, loss: 0.0007845312356948853, acc: 100.0, f1: 100.0, r: 0.8477343712151587
06/02/2019 04:02:17 step: 6874, epoch: 208, batch: 9, loss: 0.0014874860644340515, acc: 100.0, f1: 100.0, r: 0.7308804604072304
06/02/2019 04:02:17 step: 6879, epoch: 208, batch: 14, loss: 0.0006827041506767273, acc: 100.0, f1: 100.0, r: 0.7880255652552185
06/02/2019 04:02:17 step: 6884, epoch: 208, batch: 19, loss: 0.001246914267539978, acc: 100.0, f1: 100.0, r: 0.5894997377651314
06/02/2019 04:02:18 step: 6889, epoch: 208, batch: 24, loss: 0.0008747503161430359, acc: 100.0, f1: 100.0, r: 0.7421227482243904
06/02/2019 04:02:18 step: 6894, epoch: 208, batch: 29, loss: 0.0009868592023849487, acc: 100.0, f1: 100.0, r: 0.6924782641411174
06/02/2019 04:02:18 *** evaluating ***
06/02/2019 04:02:18 step: 209, epoch: 208, acc: 51.70940170940172, f1: 22.637400460160954, r: 0.22279694910226341
06/02/2019 04:02:18 *** epoch: 210 ***
06/02/2019 04:02:18 *** training ***
06/02/2019 04:02:19 step: 6902, epoch: 209, batch: 4, loss: 0.00037270039319992065, acc: 100.0, f1: 100.0, r: 0.8300641854644761
06/02/2019 04:02:19 step: 6907, epoch: 209, batch: 9, loss: 0.0008533596992492676, acc: 100.0, f1: 100.0, r: 0.6280827505362184
06/02/2019 04:02:19 step: 6912, epoch: 209, batch: 14, loss: 0.00044342875480651855, acc: 100.0, f1: 100.0, r: 0.7342961009701032
06/02/2019 04:02:20 step: 6917, epoch: 209, batch: 19, loss: 0.001094914972782135, acc: 100.0, f1: 100.0, r: 0.73927345560676
06/02/2019 04:02:20 step: 6922, epoch: 209, batch: 24, loss: 0.0008606463670730591, acc: 100.0, f1: 100.0, r: 0.7812444581983757
06/02/2019 04:02:21 step: 6927, epoch: 209, batch: 29, loss: 0.0009939298033714294, acc: 100.0, f1: 100.0, r: 0.7952248568918939
06/02/2019 04:02:21 *** evaluating ***
06/02/2019 04:02:21 step: 210, epoch: 209, acc: 51.70940170940172, f1: 23.608001530853514, r: 0.2253667604159825
06/02/2019 04:02:21 *** epoch: 211 ***
06/02/2019 04:02:21 *** training ***
06/02/2019 04:02:21 step: 6935, epoch: 210, batch: 4, loss: 0.0009621977806091309, acc: 100.0, f1: 100.0, r: 0.6847643627460257
06/02/2019 04:02:22 step: 6940, epoch: 210, batch: 9, loss: 0.0014841929078102112, acc: 100.0, f1: 100.0, r: 0.7306799910857362
06/02/2019 04:02:22 step: 6945, epoch: 210, batch: 14, loss: 0.000459134578704834, acc: 100.0, f1: 100.0, r: 0.6867709569957723
06/02/2019 04:02:22 step: 6950, epoch: 210, batch: 19, loss: 0.0009663105010986328, acc: 100.0, f1: 100.0, r: 0.8418964283108306
06/02/2019 04:02:23 step: 6955, epoch: 210, batch: 24, loss: 0.0017737597227096558, acc: 100.0, f1: 100.0, r: 0.7500356103883253
06/02/2019 04:02:23 step: 6960, epoch: 210, batch: 29, loss: 0.0005519390106201172, acc: 100.0, f1: 100.0, r: 0.7868178139801641
06/02/2019 04:02:23 *** evaluating ***
06/02/2019 04:02:23 step: 211, epoch: 210, acc: 50.0, f1: 21.90392872405939, r: 0.220762084895279
06/02/2019 04:02:23 *** epoch: 212 ***
06/02/2019 04:02:23 *** training ***
06/02/2019 04:02:24 step: 6968, epoch: 211, batch: 4, loss: 0.0007420778274536133, acc: 100.0, f1: 100.0, r: 0.7979284108426145
06/02/2019 04:02:24 step: 6973, epoch: 211, batch: 9, loss: 0.0005709975957870483, acc: 100.0, f1: 100.0, r: 0.7103964037883794
06/02/2019 04:02:24 step: 6978, epoch: 211, batch: 14, loss: 0.0008047595620155334, acc: 100.0, f1: 100.0, r: 0.6432637698836808
06/02/2019 04:02:25 step: 6983, epoch: 211, batch: 19, loss: 0.0008484944701194763, acc: 100.0, f1: 100.0, r: 0.6962087260495801
06/02/2019 04:02:25 step: 6988, epoch: 211, batch: 24, loss: 0.0005341395735740662, acc: 100.0, f1: 100.0, r: 0.8106221643843698
06/02/2019 04:02:25 step: 6993, epoch: 211, batch: 29, loss: 0.0006646215915679932, acc: 100.0, f1: 100.0, r: 0.7074527265820912
06/02/2019 04:02:26 *** evaluating ***
06/02/2019 04:02:26 step: 212, epoch: 211, acc: 51.70940170940172, f1: 22.298423544554538, r: 0.21689325182492925
06/02/2019 04:02:26 *** epoch: 213 ***
06/02/2019 04:02:26 *** training ***
06/02/2019 04:02:26 step: 7001, epoch: 212, batch: 4, loss: 0.0006547868251800537, acc: 100.0, f1: 100.0, r: 0.6804146090202654
06/02/2019 04:02:26 step: 7006, epoch: 212, batch: 9, loss: 0.0005396082997322083, acc: 100.0, f1: 100.0, r: 0.8122900850749163
06/02/2019 04:02:27 step: 7011, epoch: 212, batch: 14, loss: 0.0007415264844894409, acc: 100.0, f1: 100.0, r: 0.8309677589261513
06/02/2019 04:02:27 step: 7016, epoch: 212, batch: 19, loss: 0.0006328821182250977, acc: 100.0, f1: 100.0, r: 0.7480872608463026
06/02/2019 04:02:27 step: 7021, epoch: 212, batch: 24, loss: 0.0008096247911453247, acc: 100.0, f1: 100.0, r: 0.7824961147101268
06/02/2019 04:02:28 step: 7026, epoch: 212, batch: 29, loss: 0.00047876685857772827, acc: 100.0, f1: 100.0, r: 0.8232585838463408
06/02/2019 04:02:28 *** evaluating ***
06/02/2019 04:02:28 step: 213, epoch: 212, acc: 50.427350427350426, f1: 21.927249260439186, r: 0.2201555345235871
06/02/2019 04:02:28 *** epoch: 214 ***
06/02/2019 04:02:28 *** training ***
06/02/2019 04:02:28 step: 7034, epoch: 213, batch: 4, loss: 0.0007410719990730286, acc: 100.0, f1: 100.0, r: 0.8102039509976177
06/02/2019 04:02:29 step: 7039, epoch: 213, batch: 9, loss: 0.000771835446357727, acc: 100.0, f1: 100.0, r: 0.7755869026886236
06/02/2019 04:02:29 step: 7044, epoch: 213, batch: 14, loss: 0.00044636428356170654, acc: 100.0, f1: 100.0, r: 0.7798114213514102
06/02/2019 04:02:30 step: 7049, epoch: 213, batch: 19, loss: 0.0009740516543388367, acc: 100.0, f1: 100.0, r: 0.7548062775959508
06/02/2019 04:02:30 step: 7054, epoch: 213, batch: 24, loss: 0.0012183338403701782, acc: 100.0, f1: 100.0, r: 0.7605511335295532
06/02/2019 04:02:30 step: 7059, epoch: 213, batch: 29, loss: 0.00067177414894104, acc: 100.0, f1: 100.0, r: 0.7235051785802386
06/02/2019 04:02:30 *** evaluating ***
06/02/2019 04:02:31 step: 214, epoch: 213, acc: 51.70940170940172, f1: 23.578507369533824, r: 0.22390430158724095
06/02/2019 04:02:31 *** epoch: 215 ***
06/02/2019 04:02:31 *** training ***
06/02/2019 04:02:31 step: 7067, epoch: 214, batch: 4, loss: 0.0014456436038017273, acc: 100.0, f1: 100.0, r: 0.7712012809716873
06/02/2019 04:02:31 step: 7072, epoch: 214, batch: 9, loss: 0.0014291107654571533, acc: 100.0, f1: 100.0, r: 0.694099269887317
06/02/2019 04:02:32 step: 7077, epoch: 214, batch: 14, loss: 0.00039920955896377563, acc: 100.0, f1: 100.0, r: 0.7656119363055217
06/02/2019 04:02:32 step: 7082, epoch: 214, batch: 19, loss: 0.0005066543817520142, acc: 100.0, f1: 100.0, r: 0.6687082811800895
06/02/2019 04:02:32 step: 7087, epoch: 214, batch: 24, loss: 0.0006209090352058411, acc: 100.0, f1: 100.0, r: 0.7066527863139015
06/02/2019 04:02:33 step: 7092, epoch: 214, batch: 29, loss: 0.0004663988947868347, acc: 100.0, f1: 100.0, r: 0.7862124853897332
06/02/2019 04:02:33 *** evaluating ***
06/02/2019 04:02:33 step: 215, epoch: 214, acc: 51.28205128205128, f1: 22.816992995685695, r: 0.2240877333722004
06/02/2019 04:02:33 *** epoch: 216 ***
06/02/2019 04:02:33 *** training ***
06/02/2019 04:02:33 step: 7100, epoch: 215, batch: 4, loss: 0.0013729333877563477, acc: 100.0, f1: 100.0, r: 0.7833851655828556
06/02/2019 04:02:34 step: 7105, epoch: 215, batch: 9, loss: 0.0007888153195381165, acc: 100.0, f1: 100.0, r: 0.7273365265818774
06/02/2019 04:02:34 step: 7110, epoch: 215, batch: 14, loss: 0.00040791183710098267, acc: 100.0, f1: 100.0, r: 0.7408968239749052
06/02/2019 04:02:34 step: 7115, epoch: 215, batch: 19, loss: 0.0008225142955780029, acc: 100.0, f1: 100.0, r: 0.6219567997133607
06/02/2019 04:02:35 step: 7120, epoch: 215, batch: 24, loss: 0.0004910901188850403, acc: 100.0, f1: 100.0, r: 0.6845104149849364
06/02/2019 04:02:35 step: 7125, epoch: 215, batch: 29, loss: 0.0012356415390968323, acc: 100.0, f1: 100.0, r: 0.7817417964127202
06/02/2019 04:02:35 *** evaluating ***
06/02/2019 04:02:35 step: 216, epoch: 215, acc: 50.85470085470085, f1: 22.362544257941845, r: 0.22065464718851105
06/02/2019 04:02:35 *** epoch: 217 ***
06/02/2019 04:02:35 *** training ***
06/02/2019 04:02:36 step: 7133, epoch: 216, batch: 4, loss: 0.0007124319672584534, acc: 100.0, f1: 100.0, r: 0.8388802190020843
06/02/2019 04:02:36 step: 7138, epoch: 216, batch: 9, loss: 0.0004933476448059082, acc: 100.0, f1: 100.0, r: 0.7965140543953875
06/02/2019 04:02:36 step: 7143, epoch: 216, batch: 14, loss: 0.001068122684955597, acc: 100.0, f1: 100.0, r: 0.682733418149707
06/02/2019 04:02:37 step: 7148, epoch: 216, batch: 19, loss: 0.0011468902230262756, acc: 100.0, f1: 100.0, r: 0.7712916161357813
06/02/2019 04:02:37 step: 7153, epoch: 216, batch: 24, loss: 0.0006101354956626892, acc: 100.0, f1: 100.0, r: 0.7879830774603844
06/02/2019 04:02:37 step: 7158, epoch: 216, batch: 29, loss: 0.0013274773955345154, acc: 100.0, f1: 100.0, r: 0.7676442002779262
06/02/2019 04:02:37 *** evaluating ***
06/02/2019 04:02:38 step: 217, epoch: 216, acc: 49.572649572649574, f1: 22.066510720421046, r: 0.21966558701752792
06/02/2019 04:02:38 *** epoch: 218 ***
06/02/2019 04:02:38 *** training ***
06/02/2019 04:02:38 step: 7166, epoch: 217, batch: 4, loss: 0.0012893527746200562, acc: 100.0, f1: 100.0, r: 0.691812936228817
06/02/2019 04:02:38 step: 7171, epoch: 217, batch: 9, loss: 0.0003500506281852722, acc: 100.0, f1: 100.0, r: 0.7119071111210385
06/02/2019 04:02:38 step: 7176, epoch: 217, batch: 14, loss: 0.0017359927296638489, acc: 100.0, f1: 100.0, r: 0.5968577218224986
06/02/2019 04:02:39 step: 7181, epoch: 217, batch: 19, loss: 0.0005889981985092163, acc: 100.0, f1: 100.0, r: 0.7428942299903628
06/02/2019 04:02:39 step: 7186, epoch: 217, batch: 24, loss: 0.0004062652587890625, acc: 100.0, f1: 100.0, r: 0.7075660575232756
06/02/2019 04:02:40 step: 7191, epoch: 217, batch: 29, loss: 0.0008561462163925171, acc: 100.0, f1: 100.0, r: 0.7244641937347558
06/02/2019 04:02:40 *** evaluating ***
06/02/2019 04:02:40 step: 218, epoch: 217, acc: 51.70940170940172, f1: 22.84070824498386, r: 0.2204173380214971
06/02/2019 04:02:40 *** epoch: 219 ***
06/02/2019 04:02:40 *** training ***
06/02/2019 04:02:40 step: 7199, epoch: 218, batch: 4, loss: 0.0010551810264587402, acc: 100.0, f1: 100.0, r: 0.8433611265528629
06/02/2019 04:02:41 step: 7204, epoch: 218, batch: 9, loss: 0.0004489123821258545, acc: 100.0, f1: 100.0, r: 0.8078366445783172
06/02/2019 04:02:41 step: 7209, epoch: 218, batch: 14, loss: 0.0006857439875602722, acc: 100.0, f1: 100.0, r: 0.6881219010203388
06/02/2019 04:02:41 step: 7214, epoch: 218, batch: 19, loss: 0.0008424371480941772, acc: 100.0, f1: 100.0, r: 0.7980682105442523
06/02/2019 04:02:42 step: 7219, epoch: 218, batch: 24, loss: 0.0007267296314239502, acc: 100.0, f1: 100.0, r: 0.7962411693851404
06/02/2019 04:02:42 step: 7224, epoch: 218, batch: 29, loss: 0.0008812248706817627, acc: 100.0, f1: 100.0, r: 0.756460294183675
06/02/2019 04:02:42 *** evaluating ***
06/02/2019 04:02:42 step: 219, epoch: 218, acc: 52.56410256410257, f1: 23.456188732535797, r: 0.22474617808025302
06/02/2019 04:02:42 *** epoch: 220 ***
06/02/2019 04:02:42 *** training ***
06/02/2019 04:02:43 step: 7232, epoch: 219, batch: 4, loss: 0.0014200806617736816, acc: 100.0, f1: 100.0, r: 0.8373871491853999
06/02/2019 04:02:43 step: 7237, epoch: 219, batch: 9, loss: 0.001238808035850525, acc: 100.0, f1: 100.0, r: 0.7254672365539119
06/02/2019 04:02:43 step: 7242, epoch: 219, batch: 14, loss: 0.0004043951630592346, acc: 100.0, f1: 100.0, r: 0.7777033754873067
06/02/2019 04:02:44 step: 7247, epoch: 219, batch: 19, loss: 0.0022232234477996826, acc: 100.0, f1: 100.0, r: 0.8270171518272278
06/02/2019 04:02:44 step: 7252, epoch: 219, batch: 24, loss: 0.000749446451663971, acc: 100.0, f1: 100.0, r: 0.7885440197688802
06/02/2019 04:02:45 step: 7257, epoch: 219, batch: 29, loss: 0.0013209357857704163, acc: 100.0, f1: 100.0, r: 0.755460267556033
06/02/2019 04:02:45 *** evaluating ***
06/02/2019 04:02:45 step: 220, epoch: 219, acc: 51.70940170940172, f1: 23.467092057703372, r: 0.2270789482767878
06/02/2019 04:02:45 *** epoch: 221 ***
06/02/2019 04:02:45 *** training ***
06/02/2019 04:02:45 step: 7265, epoch: 220, batch: 4, loss: 0.00039305537939071655, acc: 100.0, f1: 100.0, r: 0.6663318219143616
06/02/2019 04:02:46 step: 7270, epoch: 220, batch: 9, loss: 0.00046368688344955444, acc: 100.0, f1: 100.0, r: 0.7659515990029189
06/02/2019 04:02:46 step: 7275, epoch: 220, batch: 14, loss: 0.0008845627307891846, acc: 100.0, f1: 100.0, r: 0.7193518072855648
06/02/2019 04:02:46 step: 7280, epoch: 220, batch: 19, loss: 0.0006237700581550598, acc: 100.0, f1: 100.0, r: 0.6935284147673053
06/02/2019 04:02:47 step: 7285, epoch: 220, batch: 24, loss: 0.0006054937839508057, acc: 100.0, f1: 100.0, r: 0.8490151522060733
06/02/2019 04:02:47 step: 7290, epoch: 220, batch: 29, loss: 0.0029431506991386414, acc: 100.0, f1: 100.0, r: 0.8234220127626602
06/02/2019 04:02:47 *** evaluating ***
06/02/2019 04:02:47 step: 221, epoch: 220, acc: 50.85470085470085, f1: 23.119529926595145, r: 0.22430961571154473
06/02/2019 04:02:47 *** epoch: 222 ***
06/02/2019 04:02:47 *** training ***
06/02/2019 04:02:48 step: 7298, epoch: 221, batch: 4, loss: 0.0011765062808990479, acc: 100.0, f1: 100.0, r: 0.775602767675465
06/02/2019 04:02:48 step: 7303, epoch: 221, batch: 9, loss: 0.0011106356978416443, acc: 100.0, f1: 100.0, r: 0.6632869428327544
06/02/2019 04:02:49 step: 7308, epoch: 221, batch: 14, loss: 0.0008475854992866516, acc: 100.0, f1: 100.0, r: 0.8025199997499073
06/02/2019 04:02:49 step: 7313, epoch: 221, batch: 19, loss: 0.0009789764881134033, acc: 100.0, f1: 100.0, r: 0.678079105752706
06/02/2019 04:02:49 step: 7318, epoch: 221, batch: 24, loss: 0.0008597150444984436, acc: 100.0, f1: 100.0, r: 0.7952656612768567
06/02/2019 04:02:50 step: 7323, epoch: 221, batch: 29, loss: 0.0006393268704414368, acc: 100.0, f1: 100.0, r: 0.7707613312736298
06/02/2019 04:02:50 *** evaluating ***
06/02/2019 04:02:50 step: 222, epoch: 221, acc: 48.717948717948715, f1: 21.902062637023036, r: 0.22320732426770976
06/02/2019 04:02:50 *** epoch: 223 ***
06/02/2019 04:02:50 *** training ***
06/02/2019 04:02:50 step: 7331, epoch: 222, batch: 4, loss: 0.0010137036442756653, acc: 100.0, f1: 100.0, r: 0.6985839221720355
06/02/2019 04:02:51 step: 7336, epoch: 222, batch: 9, loss: 0.0009515136480331421, acc: 100.0, f1: 100.0, r: 0.7691223311511363
06/02/2019 04:02:51 step: 7341, epoch: 222, batch: 14, loss: 0.0005352869629859924, acc: 100.0, f1: 100.0, r: 0.7046831711120912
06/02/2019 04:02:52 step: 7346, epoch: 222, batch: 19, loss: 0.0005495324730873108, acc: 100.0, f1: 100.0, r: 0.765712108316682
06/02/2019 04:02:52 step: 7351, epoch: 222, batch: 24, loss: 0.0007315501570701599, acc: 100.0, f1: 100.0, r: 0.7223313193324277
06/02/2019 04:02:52 step: 7356, epoch: 222, batch: 29, loss: 0.0007873028516769409, acc: 100.0, f1: 100.0, r: 0.6534729135638935
06/02/2019 04:02:52 *** evaluating ***
06/02/2019 04:02:53 step: 223, epoch: 222, acc: 51.70940170940172, f1: 23.626122598162073, r: 0.2266740687442274
06/02/2019 04:02:53 *** epoch: 224 ***
06/02/2019 04:02:53 *** training ***
06/02/2019 04:02:53 step: 7364, epoch: 223, batch: 4, loss: 0.0019199959933757782, acc: 100.0, f1: 100.0, r: 0.7064799717988397
06/02/2019 04:02:53 step: 7369, epoch: 223, batch: 9, loss: 0.0031067654490470886, acc: 100.0, f1: 100.0, r: 0.8333967040524253
06/02/2019 04:02:54 step: 7374, epoch: 223, batch: 14, loss: 0.003944717347621918, acc: 100.0, f1: 100.0, r: 0.6661962952690026
06/02/2019 04:02:54 step: 7379, epoch: 223, batch: 19, loss: 0.0011108815670013428, acc: 100.0, f1: 100.0, r: 0.7097571396360977
06/02/2019 04:02:55 step: 7384, epoch: 223, batch: 24, loss: 0.0008178427815437317, acc: 100.0, f1: 100.0, r: 0.7838453086365107
06/02/2019 04:02:55 step: 7389, epoch: 223, batch: 29, loss: 0.0006621107459068298, acc: 100.0, f1: 100.0, r: 0.7122502481154576
06/02/2019 04:02:55 *** evaluating ***
06/02/2019 04:02:55 step: 224, epoch: 223, acc: 52.13675213675214, f1: 23.526091712857582, r: 0.22563326270362088
06/02/2019 04:02:55 *** epoch: 225 ***
06/02/2019 04:02:55 *** training ***
06/02/2019 04:02:56 step: 7397, epoch: 224, batch: 4, loss: 0.002069801092147827, acc: 100.0, f1: 100.0, r: 0.7320232951624532
06/02/2019 04:02:56 step: 7402, epoch: 224, batch: 9, loss: 0.0006890296936035156, acc: 100.0, f1: 100.0, r: 0.7450212414954162
06/02/2019 04:02:56 step: 7407, epoch: 224, batch: 14, loss: 0.0006874576210975647, acc: 100.0, f1: 100.0, r: 0.657496754724303
06/02/2019 04:02:57 step: 7412, epoch: 224, batch: 19, loss: 0.0012772008776664734, acc: 100.0, f1: 100.0, r: 0.7198540271544058
06/02/2019 04:02:57 step: 7417, epoch: 224, batch: 24, loss: 0.0008491799235343933, acc: 100.0, f1: 100.0, r: 0.766635241718024
06/02/2019 04:02:57 step: 7422, epoch: 224, batch: 29, loss: 0.0006340518593788147, acc: 100.0, f1: 100.0, r: 0.7541612679877956
06/02/2019 04:02:57 *** evaluating ***
06/02/2019 04:02:58 step: 225, epoch: 224, acc: 51.70940170940172, f1: 24.175224684735554, r: 0.22848188485307014
06/02/2019 04:02:58 *** epoch: 226 ***
06/02/2019 04:02:58 *** training ***
06/02/2019 04:02:58 step: 7430, epoch: 225, batch: 4, loss: 0.0008880347013473511, acc: 100.0, f1: 100.0, r: 0.6824123018647761
06/02/2019 04:02:58 step: 7435, epoch: 225, batch: 9, loss: 0.0007769167423248291, acc: 100.0, f1: 100.0, r: 0.7852525238158277
06/02/2019 04:02:59 step: 7440, epoch: 225, batch: 14, loss: 0.0008465573191642761, acc: 100.0, f1: 100.0, r: 0.8066797500582055
06/02/2019 04:02:59 step: 7445, epoch: 225, batch: 19, loss: 0.000706501305103302, acc: 100.0, f1: 100.0, r: 0.8188070628932145
06/02/2019 04:02:59 step: 7450, epoch: 225, batch: 24, loss: 0.0005852356553077698, acc: 100.0, f1: 100.0, r: 0.8248479356669546
06/02/2019 04:03:00 step: 7455, epoch: 225, batch: 29, loss: 0.0007125958800315857, acc: 100.0, f1: 100.0, r: 0.6996122389008196
06/02/2019 04:03:00 *** evaluating ***
06/02/2019 04:03:00 step: 226, epoch: 225, acc: 51.70940170940172, f1: 23.252096277498822, r: 0.22693836034163836
06/02/2019 04:03:00 *** epoch: 227 ***
06/02/2019 04:03:00 *** training ***
06/02/2019 04:03:00 step: 7463, epoch: 226, batch: 4, loss: 0.0010257139801979065, acc: 100.0, f1: 100.0, r: 0.650624270083436
06/02/2019 04:03:01 step: 7468, epoch: 226, batch: 9, loss: 0.0009898990392684937, acc: 100.0, f1: 100.0, r: 0.6926712316198882
06/02/2019 04:03:01 step: 7473, epoch: 226, batch: 14, loss: 0.0005395412445068359, acc: 100.0, f1: 100.0, r: 0.706716490949327
06/02/2019 04:03:01 step: 7478, epoch: 226, batch: 19, loss: 0.0006028115749359131, acc: 100.0, f1: 100.0, r: 0.7427779959399564
06/02/2019 04:03:02 step: 7483, epoch: 226, batch: 24, loss: 0.0010201036930084229, acc: 100.0, f1: 100.0, r: 0.7225538151802912
06/02/2019 04:03:02 step: 7488, epoch: 226, batch: 29, loss: 0.0008311942219734192, acc: 100.0, f1: 100.0, r: 0.7581329531896023
06/02/2019 04:03:02 *** evaluating ***
06/02/2019 04:03:02 step: 227, epoch: 226, acc: 51.70940170940172, f1: 23.622510368567305, r: 0.2272122970340148
06/02/2019 04:03:02 *** epoch: 228 ***
06/02/2019 04:03:02 *** training ***
06/02/2019 04:03:03 step: 7496, epoch: 227, batch: 4, loss: 0.0005209669470787048, acc: 100.0, f1: 100.0, r: 0.6986479977578074
06/02/2019 04:03:03 step: 7501, epoch: 227, batch: 9, loss: 0.00046703964471817017, acc: 100.0, f1: 100.0, r: 0.6223133045756885
06/02/2019 04:03:03 step: 7506, epoch: 227, batch: 14, loss: 0.0007840022444725037, acc: 100.0, f1: 100.0, r: 0.6985585694016737
06/02/2019 04:03:04 step: 7511, epoch: 227, batch: 19, loss: 0.0005147233605384827, acc: 100.0, f1: 100.0, r: 0.8202152828967415
06/02/2019 04:03:04 step: 7516, epoch: 227, batch: 24, loss: 0.0008794590830802917, acc: 100.0, f1: 100.0, r: 0.6335349507029158
06/02/2019 04:03:04 step: 7521, epoch: 227, batch: 29, loss: 0.0019381418824195862, acc: 100.0, f1: 100.0, r: 0.7233380767766093
06/02/2019 04:03:04 *** evaluating ***
06/02/2019 04:03:05 step: 228, epoch: 227, acc: 50.427350427350426, f1: 23.082198217646027, r: 0.2252329400374171
06/02/2019 04:03:05 *** epoch: 229 ***
06/02/2019 04:03:05 *** training ***
06/02/2019 04:03:05 step: 7529, epoch: 228, batch: 4, loss: 0.0010966584086418152, acc: 100.0, f1: 100.0, r: 0.7882669793685401
06/02/2019 04:03:05 step: 7534, epoch: 228, batch: 9, loss: 0.0021989643573760986, acc: 100.0, f1: 100.0, r: 0.7730510830054569
06/02/2019 04:03:06 step: 7539, epoch: 228, batch: 14, loss: 0.000603802502155304, acc: 100.0, f1: 100.0, r: 0.7210191768113575
06/02/2019 04:03:06 step: 7544, epoch: 228, batch: 19, loss: 0.000964045524597168, acc: 100.0, f1: 100.0, r: 0.7722748961873676
06/02/2019 04:03:06 step: 7549, epoch: 228, batch: 24, loss: 0.0006109029054641724, acc: 100.0, f1: 100.0, r: 0.7381339017027767
06/02/2019 04:03:07 step: 7554, epoch: 228, batch: 29, loss: 0.0005072504281997681, acc: 100.0, f1: 100.0, r: 0.5474673868707499
06/02/2019 04:03:07 *** evaluating ***
06/02/2019 04:03:07 step: 229, epoch: 228, acc: 51.28205128205128, f1: 22.485619613377168, r: 0.22242602051087174
06/02/2019 04:03:07 *** epoch: 230 ***
06/02/2019 04:03:07 *** training ***
06/02/2019 04:03:07 step: 7562, epoch: 229, batch: 4, loss: 0.0032071545720100403, acc: 100.0, f1: 100.0, r: 0.8244619385786754
06/02/2019 04:03:08 step: 7567, epoch: 229, batch: 9, loss: 0.0006792843341827393, acc: 100.0, f1: 100.0, r: 0.7181405383107698
06/02/2019 04:03:08 step: 7572, epoch: 229, batch: 14, loss: 0.0018426105380058289, acc: 100.0, f1: 100.0, r: 0.7024454228064281
06/02/2019 04:03:08 step: 7577, epoch: 229, batch: 19, loss: 0.0007696375250816345, acc: 100.0, f1: 100.0, r: 0.7299784101184565
06/02/2019 04:03:09 step: 7582, epoch: 229, batch: 24, loss: 0.0011653155088424683, acc: 100.0, f1: 100.0, r: 0.8031522682769885
06/02/2019 04:03:09 step: 7587, epoch: 229, batch: 29, loss: 0.0030830204486846924, acc: 100.0, f1: 100.0, r: 0.7729163855386538
06/02/2019 04:03:09 *** evaluating ***
06/02/2019 04:03:09 step: 230, epoch: 229, acc: 52.991452991452995, f1: 23.471912318422167, r: 0.23410379142212034
06/02/2019 04:03:09 *** epoch: 231 ***
06/02/2019 04:03:09 *** training ***
06/02/2019 04:03:10 step: 7595, epoch: 230, batch: 4, loss: 0.0008319243788719177, acc: 100.0, f1: 100.0, r: 0.7995599039766628
06/02/2019 04:03:10 step: 7600, epoch: 230, batch: 9, loss: 0.0006839856505393982, acc: 100.0, f1: 100.0, r: 0.7802020199191292
06/02/2019 04:03:10 step: 7605, epoch: 230, batch: 14, loss: 0.0011154040694236755, acc: 100.0, f1: 100.0, r: 0.7344601255895647
06/02/2019 04:03:11 step: 7610, epoch: 230, batch: 19, loss: 0.0010437816381454468, acc: 100.0, f1: 100.0, r: 0.763895554936751
06/02/2019 04:03:11 step: 7615, epoch: 230, batch: 24, loss: 0.0005896911025047302, acc: 100.0, f1: 100.0, r: 0.8435294932002222
06/02/2019 04:03:12 step: 7620, epoch: 230, batch: 29, loss: 0.0010568946599960327, acc: 100.0, f1: 100.0, r: 0.795078895034178
06/02/2019 04:03:12 *** evaluating ***
06/02/2019 04:03:12 step: 231, epoch: 230, acc: 52.991452991452995, f1: 23.57781118693599, r: 0.23067877987814142
06/02/2019 04:03:12 *** epoch: 232 ***
06/02/2019 04:03:12 *** training ***
06/02/2019 04:03:12 step: 7628, epoch: 231, batch: 4, loss: 0.00071696937084198, acc: 100.0, f1: 100.0, r: 0.8309094459827594
06/02/2019 04:03:12 step: 7633, epoch: 231, batch: 9, loss: 0.0005797520279884338, acc: 100.0, f1: 100.0, r: 0.7926768156025369
06/02/2019 04:03:13 step: 7638, epoch: 231, batch: 14, loss: 0.00046915560960769653, acc: 100.0, f1: 100.0, r: 0.7264486946604802
06/02/2019 04:03:13 step: 7643, epoch: 231, batch: 19, loss: 0.0005493313074111938, acc: 100.0, f1: 100.0, r: 0.7810300374994743
06/02/2019 04:03:14 step: 7648, epoch: 231, batch: 24, loss: 0.0003188997507095337, acc: 100.0, f1: 100.0, r: 0.7096253690357597
06/02/2019 04:03:14 step: 7653, epoch: 231, batch: 29, loss: 0.0010102763772010803, acc: 100.0, f1: 100.0, r: 0.7507431581636111
06/02/2019 04:03:14 *** evaluating ***
06/02/2019 04:03:14 step: 232, epoch: 231, acc: 52.13675213675214, f1: 23.49413364748425, r: 0.22807462410661225
06/02/2019 04:03:14 *** epoch: 233 ***
06/02/2019 04:03:14 *** training ***
06/02/2019 04:03:15 step: 7661, epoch: 232, batch: 4, loss: 0.0005111619830131531, acc: 100.0, f1: 100.0, r: 0.6571405302506207
06/02/2019 04:03:15 step: 7666, epoch: 232, batch: 9, loss: 0.0008146762847900391, acc: 100.0, f1: 100.0, r: 0.6623348517160943
06/02/2019 04:03:15 step: 7671, epoch: 232, batch: 14, loss: 0.00038938969373703003, acc: 100.0, f1: 100.0, r: 0.749375838574165
06/02/2019 04:03:16 step: 7676, epoch: 232, batch: 19, loss: 0.001421608030796051, acc: 100.0, f1: 100.0, r: 0.8580549493608548
06/02/2019 04:03:16 step: 7681, epoch: 232, batch: 24, loss: 0.0009725913405418396, acc: 100.0, f1: 100.0, r: 0.746753016285651
06/02/2019 04:03:16 step: 7686, epoch: 232, batch: 29, loss: 0.0015196874737739563, acc: 100.0, f1: 100.0, r: 0.7126412484367669
06/02/2019 04:03:17 *** evaluating ***
06/02/2019 04:03:17 step: 233, epoch: 232, acc: 50.85470085470085, f1: 22.608829268865254, r: 0.2239203115830193
06/02/2019 04:03:17 *** epoch: 234 ***
06/02/2019 04:03:17 *** training ***
06/02/2019 04:03:17 step: 7694, epoch: 233, batch: 4, loss: 0.0011669620871543884, acc: 100.0, f1: 100.0, r: 0.7685271789260031
06/02/2019 04:03:17 step: 7699, epoch: 233, batch: 9, loss: 0.0024900510907173157, acc: 100.0, f1: 100.0, r: 0.7867249230802033
06/02/2019 04:03:18 step: 7704, epoch: 233, batch: 14, loss: 0.0014328733086585999, acc: 100.0, f1: 100.0, r: 0.688265165931756
06/02/2019 04:03:18 step: 7709, epoch: 233, batch: 19, loss: 0.0007326602935791016, acc: 100.0, f1: 100.0, r: 0.6898971535246509
06/02/2019 04:03:18 step: 7714, epoch: 233, batch: 24, loss: 0.0007588490843772888, acc: 100.0, f1: 100.0, r: 0.6903694355122896
06/02/2019 04:03:19 step: 7719, epoch: 233, batch: 29, loss: 0.0013421326875686646, acc: 100.0, f1: 100.0, r: 0.6587415848467263
06/02/2019 04:03:19 *** evaluating ***
06/02/2019 04:03:19 step: 234, epoch: 233, acc: 50.427350427350426, f1: 23.240431966560397, r: 0.22394256533588291
06/02/2019 04:03:19 *** epoch: 235 ***
06/02/2019 04:03:19 *** training ***
06/02/2019 04:03:19 step: 7727, epoch: 234, batch: 4, loss: 0.0008137226104736328, acc: 100.0, f1: 100.0, r: 0.7473926077927875
06/02/2019 04:03:20 step: 7732, epoch: 234, batch: 9, loss: 0.0008682161569595337, acc: 100.0, f1: 100.0, r: 0.6740795600451156
06/02/2019 04:03:20 step: 7737, epoch: 234, batch: 14, loss: 0.001111157238483429, acc: 100.0, f1: 100.0, r: 0.7416053418966905
06/02/2019 04:03:20 step: 7742, epoch: 234, batch: 19, loss: 0.0004658550024032593, acc: 100.0, f1: 100.0, r: 0.8003671018117701
06/02/2019 04:03:21 step: 7747, epoch: 234, batch: 24, loss: 0.0007103607058525085, acc: 100.0, f1: 100.0, r: 0.7580189505891751
06/02/2019 04:03:21 step: 7752, epoch: 234, batch: 29, loss: 0.0006510689854621887, acc: 100.0, f1: 100.0, r: 0.6915969738656369
06/02/2019 04:03:21 *** evaluating ***
06/02/2019 04:03:21 step: 235, epoch: 234, acc: 52.13675213675214, f1: 23.17768488500759, r: 0.22190815477264147
06/02/2019 04:03:21 *** epoch: 236 ***
06/02/2019 04:03:21 *** training ***
06/02/2019 04:03:22 step: 7760, epoch: 235, batch: 4, loss: 0.0005038753151893616, acc: 100.0, f1: 100.0, r: 0.7490740601592447
06/02/2019 04:03:22 step: 7765, epoch: 235, batch: 9, loss: 0.0004123374819755554, acc: 100.0, f1: 100.0, r: 0.7526142075989755
06/02/2019 04:03:22 step: 7770, epoch: 235, batch: 14, loss: 0.000534355640411377, acc: 100.0, f1: 100.0, r: 0.7524813089661088
06/02/2019 04:03:23 step: 7775, epoch: 235, batch: 19, loss: 0.000391349196434021, acc: 100.0, f1: 100.0, r: 0.7773704383881785
06/02/2019 04:03:23 step: 7780, epoch: 235, batch: 24, loss: 0.0004355385899543762, acc: 100.0, f1: 100.0, r: 0.7377965237572706
06/02/2019 04:03:23 step: 7785, epoch: 235, batch: 29, loss: 0.0009616166353225708, acc: 100.0, f1: 100.0, r: 0.5937785176764026
06/02/2019 04:03:24 *** evaluating ***
06/02/2019 04:03:24 step: 236, epoch: 235, acc: 51.28205128205128, f1: 22.85774229179574, r: 0.22366226586570223
06/02/2019 04:03:24 *** epoch: 237 ***
06/02/2019 04:03:24 *** training ***
06/02/2019 04:03:24 step: 7793, epoch: 236, batch: 4, loss: 0.0006806999444961548, acc: 100.0, f1: 100.0, r: 0.7586121815060548
06/02/2019 04:03:24 step: 7798, epoch: 236, batch: 9, loss: 0.00033023953437805176, acc: 100.0, f1: 100.0, r: 0.635534401981405
06/02/2019 04:03:25 step: 7803, epoch: 236, batch: 14, loss: 0.0006665885448455811, acc: 100.0, f1: 100.0, r: 0.7359490104478046
06/02/2019 04:03:25 step: 7808, epoch: 236, batch: 19, loss: 0.0007396265864372253, acc: 100.0, f1: 100.0, r: 0.8293270454428805
06/02/2019 04:03:25 step: 7813, epoch: 236, batch: 24, loss: 0.0007038936018943787, acc: 100.0, f1: 100.0, r: 0.7994277011794872
06/02/2019 04:03:26 step: 7818, epoch: 236, batch: 29, loss: 0.00180923193693161, acc: 100.0, f1: 100.0, r: 0.766043127778893
06/02/2019 04:03:26 *** evaluating ***
06/02/2019 04:03:26 step: 237, epoch: 236, acc: 51.70940170940172, f1: 23.90574210048676, r: 0.22383805645647908
06/02/2019 04:03:26 *** epoch: 238 ***
06/02/2019 04:03:26 *** training ***
06/02/2019 04:03:26 step: 7826, epoch: 237, batch: 4, loss: 0.0005397945642471313, acc: 100.0, f1: 100.0, r: 0.6973771320495298
06/02/2019 04:03:27 step: 7831, epoch: 237, batch: 9, loss: 0.002782955765724182, acc: 100.0, f1: 100.0, r: 0.7485371002264765
06/02/2019 04:03:27 step: 7836, epoch: 237, batch: 14, loss: 0.000565946102142334, acc: 100.0, f1: 100.0, r: 0.7689892760127625
06/02/2019 04:03:27 step: 7841, epoch: 237, batch: 19, loss: 0.0012446120381355286, acc: 100.0, f1: 100.0, r: 0.826365836184811
06/02/2019 04:03:28 step: 7846, epoch: 237, batch: 24, loss: 0.0009336173534393311, acc: 100.0, f1: 100.0, r: 0.7913645654324748
06/02/2019 04:03:28 step: 7851, epoch: 237, batch: 29, loss: 0.0006037130951881409, acc: 100.0, f1: 100.0, r: 0.708039200857582
06/02/2019 04:03:28 *** evaluating ***
06/02/2019 04:03:28 step: 238, epoch: 237, acc: 50.85470085470085, f1: 23.161949198235565, r: 0.21952516132917385
06/02/2019 04:03:28 *** epoch: 239 ***
06/02/2019 04:03:28 *** training ***
06/02/2019 04:03:29 step: 7859, epoch: 238, batch: 4, loss: 0.0011210590600967407, acc: 100.0, f1: 100.0, r: 0.7943932245895085
06/02/2019 04:03:29 step: 7864, epoch: 238, batch: 9, loss: 0.0005018562078475952, acc: 100.0, f1: 100.0, r: 0.6647454196025463
06/02/2019 04:03:29 step: 7869, epoch: 238, batch: 14, loss: 0.001668401062488556, acc: 100.0, f1: 100.0, r: 0.7342768468694049
06/02/2019 04:03:30 step: 7874, epoch: 238, batch: 19, loss: 0.0008583143353462219, acc: 100.0, f1: 100.0, r: 0.7264760860505638
06/02/2019 04:03:30 step: 7879, epoch: 238, batch: 24, loss: 0.0008775144815444946, acc: 100.0, f1: 100.0, r: 0.8240179589604533
06/02/2019 04:03:30 step: 7884, epoch: 238, batch: 29, loss: 0.0017406493425369263, acc: 100.0, f1: 100.0, r: 0.7602604506067897
06/02/2019 04:03:31 *** evaluating ***
06/02/2019 04:03:31 step: 239, epoch: 238, acc: 47.008547008547005, f1: 20.785849717245387, r: 0.21714160589546408
06/02/2019 04:03:31 *** epoch: 240 ***
06/02/2019 04:03:31 *** training ***
06/02/2019 04:03:31 step: 7892, epoch: 239, batch: 4, loss: 0.0005798488855361938, acc: 100.0, f1: 100.0, r: 0.7649688598454933
06/02/2019 04:03:31 step: 7897, epoch: 239, batch: 9, loss: 0.0007024556398391724, acc: 100.0, f1: 100.0, r: 0.7912340339566056
06/02/2019 04:03:32 step: 7902, epoch: 239, batch: 14, loss: 0.00084676593542099, acc: 100.0, f1: 100.0, r: 0.7612152007360399
06/02/2019 04:03:32 step: 7907, epoch: 239, batch: 19, loss: 0.0003851950168609619, acc: 100.0, f1: 100.0, r: 0.6929398493413641
06/02/2019 04:03:32 step: 7912, epoch: 239, batch: 24, loss: 0.001115061342716217, acc: 100.0, f1: 100.0, r: 0.7306893360852813
06/02/2019 04:03:33 step: 7917, epoch: 239, batch: 29, loss: 0.0006115511059761047, acc: 100.0, f1: 100.0, r: 0.828967020197765
06/02/2019 04:03:33 *** evaluating ***
06/02/2019 04:03:33 step: 240, epoch: 239, acc: 51.28205128205128, f1: 23.123119607519627, r: 0.22282020156685078
06/02/2019 04:03:33 *** epoch: 241 ***
06/02/2019 04:03:33 *** training ***
06/02/2019 04:03:33 step: 7925, epoch: 240, batch: 4, loss: 0.0033226534724235535, acc: 100.0, f1: 100.0, r: 0.5806662909353985
06/02/2019 04:03:33 step: 7930, epoch: 240, batch: 9, loss: 0.0006198287010192871, acc: 100.0, f1: 100.0, r: 0.6908636901345859
06/02/2019 04:03:34 step: 7935, epoch: 240, batch: 14, loss: 0.0006605684757232666, acc: 100.0, f1: 100.0, r: 0.7108893276211667
06/02/2019 04:03:34 step: 7940, epoch: 240, batch: 19, loss: 0.0005440264940261841, acc: 100.0, f1: 100.0, r: 0.7220100753083193
06/02/2019 04:03:34 step: 7945, epoch: 240, batch: 24, loss: 0.0003548264503479004, acc: 100.0, f1: 100.0, r: 0.822949013458392
06/02/2019 04:03:34 step: 7950, epoch: 240, batch: 29, loss: 0.00046987831592559814, acc: 100.0, f1: 100.0, r: 0.6946414161512814
06/02/2019 04:03:34 *** evaluating ***
06/02/2019 04:03:35 step: 241, epoch: 240, acc: 52.13675213675214, f1: 23.53935482110964, r: 0.2253866553562406
06/02/2019 04:03:35 *** epoch: 242 ***
06/02/2019 04:03:35 *** training ***
06/02/2019 04:03:35 step: 7958, epoch: 241, batch: 4, loss: 0.000315345823764801, acc: 100.0, f1: 100.0, r: 0.6808280035876044
06/02/2019 04:03:35 step: 7963, epoch: 241, batch: 9, loss: 0.0006873831152915955, acc: 100.0, f1: 100.0, r: 0.7544588905853065
06/02/2019 04:03:35 step: 7968, epoch: 241, batch: 14, loss: 0.0005891695618629456, acc: 100.0, f1: 100.0, r: 0.8201839988388512
06/02/2019 04:03:35 step: 7973, epoch: 241, batch: 19, loss: 0.001189984381198883, acc: 100.0, f1: 100.0, r: 0.6923684354898643
06/02/2019 04:03:36 step: 7978, epoch: 241, batch: 24, loss: 0.003049604594707489, acc: 100.0, f1: 100.0, r: 0.8315028594641312
06/02/2019 04:03:36 step: 7983, epoch: 241, batch: 29, loss: 0.0007641389966011047, acc: 100.0, f1: 100.0, r: 0.8006666795394393
06/02/2019 04:03:36 *** evaluating ***
06/02/2019 04:03:36 step: 242, epoch: 241, acc: 49.572649572649574, f1: 22.575340481019996, r: 0.22660630627390843
06/02/2019 04:03:36 *** epoch: 243 ***
06/02/2019 04:03:36 *** training ***
06/02/2019 04:03:36 step: 7991, epoch: 242, batch: 4, loss: 0.001563534140586853, acc: 100.0, f1: 100.0, r: 0.8200710097396784
06/02/2019 04:03:37 step: 7996, epoch: 242, batch: 9, loss: 0.0014439374208450317, acc: 100.0, f1: 100.0, r: 0.8269803530789338
06/02/2019 04:03:37 step: 8001, epoch: 242, batch: 14, loss: 0.0005318447947502136, acc: 100.0, f1: 100.0, r: 0.7090793803722449
06/02/2019 04:03:37 step: 8006, epoch: 242, batch: 19, loss: 0.0007317587733268738, acc: 100.0, f1: 100.0, r: 0.696388539755121
06/02/2019 04:03:37 step: 8011, epoch: 242, batch: 24, loss: 0.0004971697926521301, acc: 100.0, f1: 100.0, r: 0.7875549455920541
06/02/2019 04:03:37 step: 8016, epoch: 242, batch: 29, loss: 0.0010954886674880981, acc: 100.0, f1: 100.0, r: 0.7346121843283194
06/02/2019 04:03:38 *** evaluating ***
06/02/2019 04:03:38 step: 243, epoch: 242, acc: 52.991452991452995, f1: 24.963451726844585, r: 0.22104648148222805
06/02/2019 04:03:38 *** epoch: 244 ***
06/02/2019 04:03:38 *** training ***
06/02/2019 04:03:38 step: 8024, epoch: 243, batch: 4, loss: 0.0007227733731269836, acc: 100.0, f1: 100.0, r: 0.7839857772693213
06/02/2019 04:03:38 step: 8029, epoch: 243, batch: 9, loss: 0.0018190070986747742, acc: 100.0, f1: 100.0, r: 0.6915229082180072
06/02/2019 04:03:38 step: 8034, epoch: 243, batch: 14, loss: 0.0008878782391548157, acc: 100.0, f1: 100.0, r: 0.7365516149310551
06/02/2019 04:03:39 step: 8039, epoch: 243, batch: 19, loss: 0.0009285137057304382, acc: 100.0, f1: 100.0, r: 0.706002242705855
06/02/2019 04:03:39 step: 8044, epoch: 243, batch: 24, loss: 0.0007051378488540649, acc: 100.0, f1: 100.0, r: 0.8057291839481389
06/02/2019 04:03:39 step: 8049, epoch: 243, batch: 29, loss: 0.000850282609462738, acc: 100.0, f1: 100.0, r: 0.7061920359986181
06/02/2019 04:03:39 *** evaluating ***
06/02/2019 04:03:39 step: 244, epoch: 243, acc: 51.28205128205128, f1: 23.073526084964573, r: 0.21955939163517343
06/02/2019 04:03:39 *** epoch: 245 ***
06/02/2019 04:03:39 *** training ***
06/02/2019 04:03:40 step: 8057, epoch: 244, batch: 4, loss: 0.0008744820952415466, acc: 100.0, f1: 100.0, r: 0.8083375799774168
06/02/2019 04:03:40 step: 8062, epoch: 244, batch: 9, loss: 0.00043851882219314575, acc: 100.0, f1: 100.0, r: 0.7152070072138368
06/02/2019 04:03:40 step: 8067, epoch: 244, batch: 14, loss: 0.0002470463514328003, acc: 100.0, f1: 100.0, r: 0.747494258084805
06/02/2019 04:03:40 step: 8072, epoch: 244, batch: 19, loss: 0.000433899462223053, acc: 100.0, f1: 100.0, r: 0.6995561995087368
06/02/2019 04:03:40 step: 8077, epoch: 244, batch: 24, loss: 0.002178594470024109, acc: 100.0, f1: 100.0, r: 0.7108824016530032
06/02/2019 04:03:41 step: 8082, epoch: 244, batch: 29, loss: 0.0012661442160606384, acc: 100.0, f1: 100.0, r: 0.7034911601129503
06/02/2019 04:03:41 *** evaluating ***
06/02/2019 04:03:41 step: 245, epoch: 244, acc: 50.0, f1: 22.718715215343778, r: 0.2216254683511345
06/02/2019 04:03:41 *** epoch: 246 ***
06/02/2019 04:03:41 *** training ***
06/02/2019 04:03:41 step: 8090, epoch: 245, batch: 4, loss: 0.0004657953977584839, acc: 100.0, f1: 100.0, r: 0.7282332092909221
06/02/2019 04:03:41 step: 8095, epoch: 245, batch: 9, loss: 0.0031617209315299988, acc: 100.0, f1: 100.0, r: 0.7060135213874779
06/02/2019 04:03:42 step: 8100, epoch: 245, batch: 14, loss: 0.0005353540182113647, acc: 100.0, f1: 100.0, r: 0.813036204935409
06/02/2019 04:03:42 step: 8105, epoch: 245, batch: 19, loss: 0.0007525160908699036, acc: 100.0, f1: 100.0, r: 0.7822657406142105
06/02/2019 04:03:42 step: 8110, epoch: 245, batch: 24, loss: 0.0007691755890846252, acc: 100.0, f1: 100.0, r: 0.7496058980186555
06/02/2019 04:03:42 step: 8115, epoch: 245, batch: 29, loss: 0.00041741132736206055, acc: 100.0, f1: 100.0, r: 0.7043667173581277
06/02/2019 04:03:42 *** evaluating ***
06/02/2019 04:03:42 step: 246, epoch: 245, acc: 52.56410256410257, f1: 23.889278143594176, r: 0.22429774769232508
06/02/2019 04:03:42 *** epoch: 247 ***
06/02/2019 04:03:42 *** training ***
06/02/2019 04:03:43 step: 8123, epoch: 246, batch: 4, loss: 0.0006423965096473694, acc: 100.0, f1: 100.0, r: 0.6807593776992426
06/02/2019 04:03:43 step: 8128, epoch: 246, batch: 9, loss: 0.0005516856908798218, acc: 100.0, f1: 100.0, r: 0.829937110271882
06/02/2019 04:03:43 step: 8133, epoch: 246, batch: 14, loss: 0.0006214231252670288, acc: 100.0, f1: 100.0, r: 0.7416751078350315
06/02/2019 04:03:43 step: 8138, epoch: 246, batch: 19, loss: 0.0006869137287139893, acc: 100.0, f1: 100.0, r: 0.8043234155514007
06/02/2019 04:03:43 step: 8143, epoch: 246, batch: 24, loss: 0.0008233562111854553, acc: 100.0, f1: 100.0, r: 0.6976496038747304
06/02/2019 04:03:44 step: 8148, epoch: 246, batch: 29, loss: 0.0020272359251976013, acc: 100.0, f1: 100.0, r: 0.7092581690475076
06/02/2019 04:03:44 *** evaluating ***
06/02/2019 04:03:44 step: 247, epoch: 246, acc: 52.13675213675214, f1: 23.66097101124324, r: 0.218305158547989
06/02/2019 04:03:44 *** epoch: 248 ***
06/02/2019 04:03:44 *** training ***
06/02/2019 04:03:44 step: 8156, epoch: 247, batch: 4, loss: 0.0005933716893196106, acc: 100.0, f1: 100.0, r: 0.857300324918739
06/02/2019 04:03:44 step: 8161, epoch: 247, batch: 9, loss: 0.0006819590926170349, acc: 100.0, f1: 100.0, r: 0.786461159677756
06/02/2019 04:03:45 step: 8166, epoch: 247, batch: 14, loss: 0.0008125528693199158, acc: 100.0, f1: 100.0, r: 0.8049609382942264
06/02/2019 04:03:45 step: 8171, epoch: 247, batch: 19, loss: 0.0014900118112564087, acc: 100.0, f1: 100.0, r: 0.7305809628166139
06/02/2019 04:03:45 step: 8176, epoch: 247, batch: 24, loss: 0.0008452385663986206, acc: 100.0, f1: 100.0, r: 0.722912111718222
06/02/2019 04:03:45 step: 8181, epoch: 247, batch: 29, loss: 0.0002592802047729492, acc: 100.0, f1: 100.0, r: 0.7311636787422879
06/02/2019 04:03:45 *** evaluating ***
06/02/2019 04:03:45 step: 248, epoch: 247, acc: 50.85470085470085, f1: 23.03507943525497, r: 0.21850310272347265
06/02/2019 04:03:45 *** epoch: 249 ***
06/02/2019 04:03:45 *** training ***
06/02/2019 04:03:46 step: 8189, epoch: 248, batch: 4, loss: 0.001528427004814148, acc: 100.0, f1: 100.0, r: 0.7643872738303007
06/02/2019 04:03:46 step: 8194, epoch: 248, batch: 9, loss: 0.00038701295852661133, acc: 100.0, f1: 100.0, r: 0.798382661695109
06/02/2019 04:03:46 step: 8199, epoch: 248, batch: 14, loss: 0.0005066841840744019, acc: 100.0, f1: 100.0, r: 0.7300679315233904
06/02/2019 04:03:46 step: 8204, epoch: 248, batch: 19, loss: 0.0008513554930686951, acc: 100.0, f1: 100.0, r: 0.7550694833293959
06/02/2019 04:03:47 step: 8209, epoch: 248, batch: 24, loss: 0.0008136928081512451, acc: 100.0, f1: 100.0, r: 0.6631127715991293
06/02/2019 04:03:47 step: 8214, epoch: 248, batch: 29, loss: 0.0008196234703063965, acc: 100.0, f1: 100.0, r: 0.7128487618169501
06/02/2019 04:03:47 *** evaluating ***
06/02/2019 04:03:47 step: 249, epoch: 248, acc: 52.13675213675214, f1: 23.69499507046802, r: 0.21996027164491108
06/02/2019 04:03:47 *** epoch: 250 ***
06/02/2019 04:03:47 *** training ***
06/02/2019 04:03:47 step: 8222, epoch: 249, batch: 4, loss: 0.0007341057062149048, acc: 100.0, f1: 100.0, r: 0.658615323350883
06/02/2019 04:03:47 step: 8227, epoch: 249, batch: 9, loss: 0.00046062469482421875, acc: 100.0, f1: 100.0, r: 0.7497538916181202
06/02/2019 04:03:48 step: 8232, epoch: 249, batch: 14, loss: 0.0004095733165740967, acc: 100.0, f1: 100.0, r: 0.7588272581860809
06/02/2019 04:03:48 step: 8237, epoch: 249, batch: 19, loss: 0.0006347000598907471, acc: 100.0, f1: 100.0, r: 0.705590418162245
06/02/2019 04:03:48 step: 8242, epoch: 249, batch: 24, loss: 0.001155845820903778, acc: 100.0, f1: 100.0, r: 0.7103313366537374
06/02/2019 04:03:48 step: 8247, epoch: 249, batch: 29, loss: 0.0004138350486755371, acc: 100.0, f1: 100.0, r: 0.6087361504709806
06/02/2019 04:03:48 *** evaluating ***
06/02/2019 04:03:48 step: 250, epoch: 249, acc: 51.70940170940172, f1: 23.523068410286452, r: 0.21871024650551402
06/02/2019 04:03:48 *** epoch: 251 ***
06/02/2019 04:03:48 *** training ***
06/02/2019 04:03:49 step: 8255, epoch: 250, batch: 4, loss: 0.0034213699400424957, acc: 100.0, f1: 100.0, r: 0.8090419639140805
06/02/2019 04:03:49 step: 8260, epoch: 250, batch: 9, loss: 0.0008609294891357422, acc: 100.0, f1: 100.0, r: 0.803510885728187
06/02/2019 04:03:49 step: 8265, epoch: 250, batch: 14, loss: 0.0008471757173538208, acc: 100.0, f1: 100.0, r: 0.7486193206494971
06/02/2019 04:03:49 step: 8270, epoch: 250, batch: 19, loss: 0.0006640255451202393, acc: 100.0, f1: 100.0, r: 0.795808900301131
06/02/2019 04:03:49 step: 8275, epoch: 250, batch: 24, loss: 0.0003192126750946045, acc: 100.0, f1: 100.0, r: 0.6824898256674453
06/02/2019 04:03:50 step: 8280, epoch: 250, batch: 29, loss: 0.0021017491817474365, acc: 100.0, f1: 100.0, r: 0.6721726273003977
06/02/2019 04:03:50 *** evaluating ***
06/02/2019 04:03:50 step: 251, epoch: 250, acc: 52.13675213675214, f1: 23.73199373851597, r: 0.21938267017982246
06/02/2019 04:03:50 *** epoch: 252 ***
06/02/2019 04:03:50 *** training ***
06/02/2019 04:03:50 step: 8288, epoch: 251, batch: 4, loss: 0.0011289790272712708, acc: 100.0, f1: 100.0, r: 0.7004906432502446
06/02/2019 04:03:50 step: 8293, epoch: 251, batch: 9, loss: 0.0008091926574707031, acc: 100.0, f1: 100.0, r: 0.742887808945523
06/02/2019 04:03:50 step: 8298, epoch: 251, batch: 14, loss: 0.0008979812264442444, acc: 100.0, f1: 100.0, r: 0.5798804135729723
06/02/2019 04:03:51 step: 8303, epoch: 251, batch: 19, loss: 0.0014852136373519897, acc: 100.0, f1: 100.0, r: 0.5986566923985811
06/02/2019 04:03:51 step: 8308, epoch: 251, batch: 24, loss: 0.0005122646689414978, acc: 100.0, f1: 100.0, r: 0.7117597700137838
06/02/2019 04:03:51 step: 8313, epoch: 251, batch: 29, loss: 0.0005532950162887573, acc: 100.0, f1: 100.0, r: 0.7528741748821802
06/02/2019 04:03:51 *** evaluating ***
06/02/2019 04:03:51 step: 252, epoch: 251, acc: 51.70940170940172, f1: 23.381905819616165, r: 0.2194015790561861
06/02/2019 04:03:51 *** epoch: 253 ***
06/02/2019 04:03:51 *** training ***
06/02/2019 04:03:51 step: 8321, epoch: 252, batch: 4, loss: 0.00019088387489318848, acc: 100.0, f1: 100.0, r: 0.6315848814736094
06/02/2019 04:03:52 step: 8326, epoch: 252, batch: 9, loss: 0.0005041807889938354, acc: 100.0, f1: 100.0, r: 0.7109781955402761
06/02/2019 04:03:52 step: 8331, epoch: 252, batch: 14, loss: 0.0004860907793045044, acc: 100.0, f1: 100.0, r: 0.7138261172723298
06/02/2019 04:03:52 step: 8336, epoch: 252, batch: 19, loss: 0.0006805285811424255, acc: 100.0, f1: 100.0, r: 0.8128448678249675
06/02/2019 04:03:52 step: 8341, epoch: 252, batch: 24, loss: 0.0006956383585929871, acc: 100.0, f1: 100.0, r: 0.8341844580599116
06/02/2019 04:03:52 step: 8346, epoch: 252, batch: 29, loss: 0.0007835403084754944, acc: 100.0, f1: 100.0, r: 0.775591357650342
06/02/2019 04:03:52 *** evaluating ***
06/02/2019 04:03:53 step: 253, epoch: 252, acc: 51.70940170940172, f1: 23.559126102594146, r: 0.22528542833771742
06/02/2019 04:03:53 *** epoch: 254 ***
06/02/2019 04:03:53 *** training ***
06/02/2019 04:03:53 step: 8354, epoch: 253, batch: 4, loss: 0.0011894255876541138, acc: 100.0, f1: 100.0, r: 0.8227251595457484
06/02/2019 04:03:53 step: 8359, epoch: 253, batch: 9, loss: 0.0005139783024787903, acc: 100.0, f1: 100.0, r: 0.6949467778811045
06/02/2019 04:03:53 step: 8364, epoch: 253, batch: 14, loss: 0.0010842978954315186, acc: 100.0, f1: 100.0, r: 0.8010076251712145
06/02/2019 04:03:53 step: 8369, epoch: 253, batch: 19, loss: 0.0010480433702468872, acc: 100.0, f1: 100.0, r: 0.780661911670274
06/02/2019 04:03:54 step: 8374, epoch: 253, batch: 24, loss: 0.0006361901760101318, acc: 100.0, f1: 100.0, r: 0.6742311723626062
06/02/2019 04:03:54 step: 8379, epoch: 253, batch: 29, loss: 0.0007707849144935608, acc: 100.0, f1: 100.0, r: 0.7585580438981456
06/02/2019 04:03:54 *** evaluating ***
06/02/2019 04:03:54 step: 254, epoch: 253, acc: 52.13675213675214, f1: 23.769312741352216, r: 0.2197681003000901
06/02/2019 04:03:54 *** epoch: 255 ***
06/02/2019 04:03:54 *** training ***
06/02/2019 04:03:54 step: 8387, epoch: 254, batch: 4, loss: 0.001285359263420105, acc: 100.0, f1: 100.0, r: 0.6796100211985581
06/02/2019 04:03:54 step: 8392, epoch: 254, batch: 9, loss: 0.0009401068091392517, acc: 100.0, f1: 100.0, r: 0.6588259229701571
06/02/2019 04:03:55 step: 8397, epoch: 254, batch: 14, loss: 0.0006101354956626892, acc: 100.0, f1: 100.0, r: 0.7783870180287326
06/02/2019 04:03:55 step: 8402, epoch: 254, batch: 19, loss: 0.000606149435043335, acc: 100.0, f1: 100.0, r: 0.6759528485479092
06/02/2019 04:03:55 step: 8407, epoch: 254, batch: 24, loss: 0.0006680041551589966, acc: 100.0, f1: 100.0, r: 0.7477054095138438
06/02/2019 04:03:55 step: 8412, epoch: 254, batch: 29, loss: 0.0006955713033676147, acc: 100.0, f1: 100.0, r: 0.8269138812888189
06/02/2019 04:03:55 *** evaluating ***
06/02/2019 04:03:55 step: 255, epoch: 254, acc: 51.70940170940172, f1: 23.525746891225808, r: 0.22085120587969603
06/02/2019 04:03:55 *** epoch: 256 ***
06/02/2019 04:03:55 *** training ***
06/02/2019 04:03:56 step: 8420, epoch: 255, batch: 4, loss: 0.000723615288734436, acc: 100.0, f1: 100.0, r: 0.6661193624928455
06/02/2019 04:03:56 step: 8425, epoch: 255, batch: 9, loss: 0.0006040334701538086, acc: 100.0, f1: 100.0, r: 0.6356307313535188
06/02/2019 04:03:56 step: 8430, epoch: 255, batch: 14, loss: 0.0007129758596420288, acc: 100.0, f1: 100.0, r: 0.841209587293918
06/02/2019 04:03:56 step: 8435, epoch: 255, batch: 19, loss: 0.002307616174221039, acc: 100.0, f1: 100.0, r: 0.7382427385964169
06/02/2019 04:03:57 step: 8440, epoch: 255, batch: 24, loss: 0.0004963576793670654, acc: 100.0, f1: 100.0, r: 0.7304324214892813
06/02/2019 04:03:57 step: 8445, epoch: 255, batch: 29, loss: 0.0007960572838783264, acc: 100.0, f1: 100.0, r: 0.6275794409107045
06/02/2019 04:03:57 *** evaluating ***
06/02/2019 04:03:57 step: 256, epoch: 255, acc: 52.13675213675214, f1: 24.58664414140482, r: 0.22640525104579404
06/02/2019 04:03:57 *** epoch: 257 ***
06/02/2019 04:03:57 *** training ***
06/02/2019 04:03:57 step: 8453, epoch: 256, batch: 4, loss: 0.0018218383193016052, acc: 100.0, f1: 100.0, r: 0.7244313353906207
06/02/2019 04:03:57 step: 8458, epoch: 256, batch: 9, loss: 0.000691629946231842, acc: 100.0, f1: 100.0, r: 0.6910336460895006
06/02/2019 04:03:58 step: 8463, epoch: 256, batch: 14, loss: 0.0013242065906524658, acc: 100.0, f1: 100.0, r: 0.8112284810270313
06/02/2019 04:03:58 step: 8468, epoch: 256, batch: 19, loss: 0.0005547180771827698, acc: 100.0, f1: 100.0, r: 0.7639838766105262
06/02/2019 04:03:58 step: 8473, epoch: 256, batch: 24, loss: 0.0005247518420219421, acc: 100.0, f1: 100.0, r: 0.8102944617947576
06/02/2019 04:03:58 step: 8478, epoch: 256, batch: 29, loss: 0.0006829053163528442, acc: 100.0, f1: 100.0, r: 0.6833773758808248
06/02/2019 04:03:59 *** evaluating ***
06/02/2019 04:03:59 step: 257, epoch: 256, acc: 52.13675213675214, f1: 24.834631962864723, r: 0.22386571831545404
06/02/2019 04:03:59 *** epoch: 258 ***
06/02/2019 04:03:59 *** training ***
06/02/2019 04:03:59 step: 8486, epoch: 257, batch: 4, loss: 0.0004348382353782654, acc: 100.0, f1: 100.0, r: 0.8070910269195406
06/02/2019 04:03:59 step: 8491, epoch: 257, batch: 9, loss: 0.0015188977122306824, acc: 100.0, f1: 100.0, r: 0.718172689214006
06/02/2019 04:03:59 step: 8496, epoch: 257, batch: 14, loss: 0.004288427531719208, acc: 100.0, f1: 100.0, r: 0.6871773885599548
06/02/2019 04:04:00 step: 8501, epoch: 257, batch: 19, loss: 0.0053254663944244385, acc: 100.0, f1: 100.0, r: 0.7110675813021593
06/02/2019 04:04:00 step: 8506, epoch: 257, batch: 24, loss: 0.00047869980335235596, acc: 100.0, f1: 100.0, r: 0.7718508448852488
06/02/2019 04:04:00 step: 8511, epoch: 257, batch: 29, loss: 0.000499136745929718, acc: 100.0, f1: 100.0, r: 0.7446561975930274
06/02/2019 04:04:00 *** evaluating ***
06/02/2019 04:04:00 step: 258, epoch: 257, acc: 50.427350427350426, f1: 24.952472989622017, r: 0.21856760916340753
06/02/2019 04:04:00 *** epoch: 259 ***
06/02/2019 04:04:00 *** training ***
06/02/2019 04:04:00 step: 8519, epoch: 258, batch: 4, loss: 0.0008756294846534729, acc: 100.0, f1: 100.0, r: 0.7500220136689588
06/02/2019 04:04:01 step: 8524, epoch: 258, batch: 9, loss: 0.0017263144254684448, acc: 100.0, f1: 100.0, r: 0.8324265515563388
06/02/2019 04:04:01 step: 8529, epoch: 258, batch: 14, loss: 0.0008914545178413391, acc: 100.0, f1: 100.0, r: 0.8045964243893003
06/02/2019 04:04:01 step: 8534, epoch: 258, batch: 19, loss: 0.0008544549345970154, acc: 100.0, f1: 100.0, r: 0.8077159236619686
06/02/2019 04:04:01 step: 8539, epoch: 258, batch: 24, loss: 0.001015685498714447, acc: 100.0, f1: 100.0, r: 0.5683189114793669
06/02/2019 04:04:02 step: 8544, epoch: 258, batch: 29, loss: 0.0009433254599571228, acc: 100.0, f1: 100.0, r: 0.8352660177632989
06/02/2019 04:04:02 *** evaluating ***
06/02/2019 04:04:02 step: 259, epoch: 258, acc: 51.28205128205128, f1: 23.205105032332995, r: 0.21925135967944537
06/02/2019 04:04:02 *** epoch: 260 ***
06/02/2019 04:04:02 *** training ***
06/02/2019 04:04:02 step: 8552, epoch: 259, batch: 4, loss: 0.0003511384129524231, acc: 100.0, f1: 100.0, r: 0.7729930397186263
06/02/2019 04:04:02 step: 8557, epoch: 259, batch: 9, loss: 0.0007716566324234009, acc: 100.0, f1: 100.0, r: 0.7788544938365954
06/02/2019 04:04:02 step: 8562, epoch: 259, batch: 14, loss: 0.0004413723945617676, acc: 100.0, f1: 100.0, r: 0.7597038106824803
06/02/2019 04:04:03 step: 8567, epoch: 259, batch: 19, loss: 0.0012345537543296814, acc: 100.0, f1: 100.0, r: 0.5781533550515547
06/02/2019 04:04:03 step: 8572, epoch: 259, batch: 24, loss: 0.00032766908407211304, acc: 100.0, f1: 100.0, r: 0.7859976671391463
06/02/2019 04:04:03 step: 8577, epoch: 259, batch: 29, loss: 0.0015250444412231445, acc: 100.0, f1: 100.0, r: 0.6616596289955432
06/02/2019 04:04:03 *** evaluating ***
06/02/2019 04:04:03 step: 260, epoch: 259, acc: 51.28205128205128, f1: 22.97968128494213, r: 0.22523847595131302
06/02/2019 04:04:03 *** epoch: 261 ***
06/02/2019 04:04:03 *** training ***
06/02/2019 04:04:04 step: 8585, epoch: 260, batch: 4, loss: 0.0014947131276130676, acc: 100.0, f1: 100.0, r: 0.6801091097797674
06/02/2019 04:04:04 step: 8590, epoch: 260, batch: 9, loss: 0.0009943842887878418, acc: 100.0, f1: 100.0, r: 0.8103816409845781
06/02/2019 04:04:04 step: 8595, epoch: 260, batch: 14, loss: 0.00047744810581207275, acc: 100.0, f1: 100.0, r: 0.7964530824894078
06/02/2019 04:04:04 step: 8600, epoch: 260, batch: 19, loss: 0.0010923147201538086, acc: 100.0, f1: 100.0, r: 0.6919784237718237
06/02/2019 04:04:04 step: 8605, epoch: 260, batch: 24, loss: 0.0009007230401039124, acc: 100.0, f1: 100.0, r: 0.6967808274615462
06/02/2019 04:04:05 step: 8610, epoch: 260, batch: 29, loss: 0.0004282146692276001, acc: 100.0, f1: 100.0, r: 0.7650156373052269
06/02/2019 04:04:05 *** evaluating ***
06/02/2019 04:04:05 step: 261, epoch: 260, acc: 52.13675213675214, f1: 24.080302246063116, r: 0.22624207772436292
06/02/2019 04:04:05 *** epoch: 262 ***
06/02/2019 04:04:05 *** training ***
06/02/2019 04:04:05 step: 8618, epoch: 261, batch: 4, loss: 0.0005019828677177429, acc: 100.0, f1: 100.0, r: 0.6560617032104861
06/02/2019 04:04:05 step: 8623, epoch: 261, batch: 9, loss: 0.000772625207901001, acc: 100.0, f1: 100.0, r: 0.7412317086422872
06/02/2019 04:04:06 step: 8628, epoch: 261, batch: 14, loss: 0.001124061644077301, acc: 100.0, f1: 100.0, r: 0.7570253341971235
06/02/2019 04:04:06 step: 8633, epoch: 261, batch: 19, loss: 0.0006477311253547668, acc: 100.0, f1: 100.0, r: 0.7638178666981985
06/02/2019 04:04:06 step: 8638, epoch: 261, batch: 24, loss: 0.0030989423394203186, acc: 100.0, f1: 100.0, r: 0.6570409241569284
06/02/2019 04:04:06 step: 8643, epoch: 261, batch: 29, loss: 0.0008876621723175049, acc: 100.0, f1: 100.0, r: 0.7722893703383621
06/02/2019 04:04:06 *** evaluating ***
06/02/2019 04:04:06 step: 262, epoch: 261, acc: 51.70940170940172, f1: 23.63487389803179, r: 0.2296823545198025
06/02/2019 04:04:06 *** epoch: 263 ***
06/02/2019 04:04:06 *** training ***
06/02/2019 04:04:07 step: 8651, epoch: 262, batch: 4, loss: 0.0005317479372024536, acc: 100.0, f1: 100.0, r: 0.5655219596537766
06/02/2019 04:04:07 step: 8656, epoch: 262, batch: 9, loss: 0.00040749460458755493, acc: 100.0, f1: 100.0, r: 0.732974276523231
06/02/2019 04:04:07 step: 8661, epoch: 262, batch: 14, loss: 0.0019382983446121216, acc: 100.0, f1: 100.0, r: 0.794318680092598
06/02/2019 04:04:07 step: 8666, epoch: 262, batch: 19, loss: 0.0006721764802932739, acc: 100.0, f1: 100.0, r: 0.7506613673421916
06/02/2019 04:04:08 step: 8671, epoch: 262, batch: 24, loss: 0.002013087272644043, acc: 100.0, f1: 100.0, r: 0.7585762279354308
06/02/2019 04:04:08 step: 8676, epoch: 262, batch: 29, loss: 0.0013283416628837585, acc: 100.0, f1: 100.0, r: 0.771723799958827
06/02/2019 04:04:08 *** evaluating ***
06/02/2019 04:04:08 step: 263, epoch: 262, acc: 52.13675213675214, f1: 22.81610184457493, r: 0.23171402052025714
06/02/2019 04:04:08 *** epoch: 264 ***
06/02/2019 04:04:08 *** training ***
06/02/2019 04:04:08 step: 8684, epoch: 263, batch: 4, loss: 0.0016710609197616577, acc: 100.0, f1: 100.0, r: 0.6766226064225339
06/02/2019 04:04:09 step: 8689, epoch: 263, batch: 9, loss: 0.0016682744026184082, acc: 100.0, f1: 100.0, r: 0.6810829295937121
06/02/2019 04:04:09 step: 8694, epoch: 263, batch: 14, loss: 0.0005595013499259949, acc: 100.0, f1: 100.0, r: 0.834050069124143
06/02/2019 04:04:09 step: 8699, epoch: 263, batch: 19, loss: 0.0013078674674034119, acc: 100.0, f1: 100.0, r: 0.7327634479257459
06/02/2019 04:04:09 step: 8704, epoch: 263, batch: 24, loss: 0.0015262067317962646, acc: 100.0, f1: 100.0, r: 0.7774176054138693
06/02/2019 04:04:09 step: 8709, epoch: 263, batch: 29, loss: 0.0008038729429244995, acc: 100.0, f1: 100.0, r: 0.7993185342859008
06/02/2019 04:04:09 *** evaluating ***
06/02/2019 04:04:10 step: 264, epoch: 263, acc: 52.13675213675214, f1: 23.859737088256843, r: 0.23228065681517465
06/02/2019 04:04:10 *** epoch: 265 ***
06/02/2019 04:04:10 *** training ***
06/02/2019 04:04:10 step: 8717, epoch: 264, batch: 4, loss: 0.0006081759929656982, acc: 100.0, f1: 100.0, r: 0.7045159746194648
06/02/2019 04:04:10 step: 8722, epoch: 264, batch: 9, loss: 0.001477755606174469, acc: 100.0, f1: 100.0, r: 0.6965226529408296
06/02/2019 04:04:10 step: 8727, epoch: 264, batch: 14, loss: 0.00044670701026916504, acc: 100.0, f1: 100.0, r: 0.7305178259318768
06/02/2019 04:04:10 step: 8732, epoch: 264, batch: 19, loss: 0.0031041353940963745, acc: 100.0, f1: 100.0, r: 0.4932320012667174
06/02/2019 04:04:11 step: 8737, epoch: 264, batch: 24, loss: 0.0010433122515678406, acc: 100.0, f1: 100.0, r: 0.7009472922163025
06/02/2019 04:04:11 step: 8742, epoch: 264, batch: 29, loss: 0.0011397674679756165, acc: 100.0, f1: 100.0, r: 0.7879750426101123
06/02/2019 04:04:11 *** evaluating ***
06/02/2019 04:04:11 step: 265, epoch: 264, acc: 52.13675213675214, f1: 23.590805781914952, r: 0.22884155657228633
06/02/2019 04:04:11 *** epoch: 266 ***
06/02/2019 04:04:11 *** training ***
06/02/2019 04:04:11 step: 8750, epoch: 265, batch: 4, loss: 0.0007790252566337585, acc: 100.0, f1: 100.0, r: 0.8192240836741351
06/02/2019 04:04:11 step: 8755, epoch: 265, batch: 9, loss: 0.0008526593446731567, acc: 100.0, f1: 100.0, r: 0.8350092150414473
06/02/2019 04:04:12 step: 8760, epoch: 265, batch: 14, loss: 0.001095890998840332, acc: 100.0, f1: 100.0, r: 0.6883642131704397
06/02/2019 04:04:12 step: 8765, epoch: 265, batch: 19, loss: 0.0005584061145782471, acc: 100.0, f1: 100.0, r: 0.6686512467385126
06/02/2019 04:04:12 step: 8770, epoch: 265, batch: 24, loss: 0.0009659677743911743, acc: 100.0, f1: 100.0, r: 0.7832152344526245
06/02/2019 04:04:12 step: 8775, epoch: 265, batch: 29, loss: 0.0004278793931007385, acc: 100.0, f1: 100.0, r: 0.7585934903608783
06/02/2019 04:04:12 *** evaluating ***
06/02/2019 04:04:12 step: 266, epoch: 265, acc: 51.70940170940172, f1: 23.233901515151512, r: 0.22859397676821486
06/02/2019 04:04:12 *** epoch: 267 ***
06/02/2019 04:04:12 *** training ***
06/02/2019 04:04:13 step: 8783, epoch: 266, batch: 4, loss: 0.000752963125705719, acc: 100.0, f1: 100.0, r: 0.5854234506792815
06/02/2019 04:04:13 step: 8788, epoch: 266, batch: 9, loss: 0.0011556297540664673, acc: 100.0, f1: 100.0, r: 0.730179367298997
06/02/2019 04:04:13 step: 8793, epoch: 266, batch: 14, loss: 0.0008481442928314209, acc: 100.0, f1: 100.0, r: 0.7391524761763525
06/02/2019 04:04:13 step: 8798, epoch: 266, batch: 19, loss: 0.0006750151515007019, acc: 100.0, f1: 100.0, r: 0.8110882896770514
06/02/2019 04:04:14 step: 8803, epoch: 266, batch: 24, loss: 0.000514298677444458, acc: 100.0, f1: 100.0, r: 0.6562935367250913
06/02/2019 04:04:14 step: 8808, epoch: 266, batch: 29, loss: 0.0004093870520591736, acc: 100.0, f1: 100.0, r: 0.8127302682294998
06/02/2019 04:04:14 *** evaluating ***
06/02/2019 04:04:14 step: 267, epoch: 266, acc: 51.70940170940172, f1: 23.219415342656152, r: 0.22857901149300752
06/02/2019 04:04:14 *** epoch: 268 ***
06/02/2019 04:04:14 *** training ***
06/02/2019 04:04:14 step: 8816, epoch: 267, batch: 4, loss: 0.0005560517311096191, acc: 100.0, f1: 100.0, r: 0.6304065917581443
06/02/2019 04:04:14 step: 8821, epoch: 267, batch: 9, loss: 0.0016110539436340332, acc: 100.0, f1: 100.0, r: 0.6984182451623876
06/02/2019 04:04:15 step: 8826, epoch: 267, batch: 14, loss: 0.0011961907148361206, acc: 100.0, f1: 100.0, r: 0.7773291678223353
06/02/2019 04:04:15 step: 8831, epoch: 267, batch: 19, loss: 0.0011191219091415405, acc: 100.0, f1: 100.0, r: 0.7166510690500091
06/02/2019 04:04:15 step: 8836, epoch: 267, batch: 24, loss: 0.00042845308780670166, acc: 100.0, f1: 100.0, r: 0.7755441099848465
06/02/2019 04:04:15 step: 8841, epoch: 267, batch: 29, loss: 0.00031331926584243774, acc: 100.0, f1: 100.0, r: 0.7702423817850548
06/02/2019 04:04:15 *** evaluating ***
06/02/2019 04:04:15 step: 268, epoch: 267, acc: 51.70940170940172, f1: 23.46434997750787, r: 0.22280126331605277
06/02/2019 04:04:15 *** epoch: 269 ***
06/02/2019 04:04:15 *** training ***
06/02/2019 04:04:16 step: 8849, epoch: 268, batch: 4, loss: 0.0007519647479057312, acc: 100.0, f1: 100.0, r: 0.8073874204958522
06/02/2019 04:04:16 step: 8854, epoch: 268, batch: 9, loss: 0.00083179771900177, acc: 100.0, f1: 100.0, r: 0.7581959156275794
06/02/2019 04:04:16 step: 8859, epoch: 268, batch: 14, loss: 0.00037375837564468384, acc: 100.0, f1: 100.0, r: 0.7679546851221084
06/02/2019 04:04:16 step: 8864, epoch: 268, batch: 19, loss: 0.0007003024220466614, acc: 100.0, f1: 100.0, r: 0.7849850310595106
06/02/2019 04:04:16 step: 8869, epoch: 268, batch: 24, loss: 0.0007369890809059143, acc: 100.0, f1: 100.0, r: 0.7460897015573643
06/02/2019 04:04:17 step: 8874, epoch: 268, batch: 29, loss: 0.0010140538215637207, acc: 100.0, f1: 100.0, r: 0.7365057719481919
06/02/2019 04:04:17 *** evaluating ***
06/02/2019 04:04:17 step: 269, epoch: 268, acc: 51.70940170940172, f1: 23.213852576517052, r: 0.2230593674817935
06/02/2019 04:04:17 *** epoch: 270 ***
06/02/2019 04:04:17 *** training ***
06/02/2019 04:04:17 step: 8882, epoch: 269, batch: 4, loss: 0.00099925696849823, acc: 100.0, f1: 100.0, r: 0.846404773945121
06/02/2019 04:04:17 step: 8887, epoch: 269, batch: 9, loss: 0.0015923455357551575, acc: 100.0, f1: 100.0, r: 0.700291959770734
06/02/2019 04:04:18 step: 8892, epoch: 269, batch: 14, loss: 0.0010596886277198792, acc: 100.0, f1: 100.0, r: 0.799952345583897
06/02/2019 04:04:18 step: 8897, epoch: 269, batch: 19, loss: 0.0007338002324104309, acc: 100.0, f1: 100.0, r: 0.702056197186252
06/02/2019 04:04:18 step: 8902, epoch: 269, batch: 24, loss: 0.0006564930081367493, acc: 100.0, f1: 100.0, r: 0.6924008569391995
06/02/2019 04:04:18 step: 8907, epoch: 269, batch: 29, loss: 0.0009610727429389954, acc: 100.0, f1: 100.0, r: 0.6793201080082614
06/02/2019 04:04:18 *** evaluating ***
06/02/2019 04:04:18 step: 270, epoch: 269, acc: 51.70940170940172, f1: 23.261619333047904, r: 0.22081933888371483
06/02/2019 04:04:18 *** epoch: 271 ***
06/02/2019 04:04:18 *** training ***
06/02/2019 04:04:19 step: 8915, epoch: 270, batch: 4, loss: 0.0008016005158424377, acc: 100.0, f1: 100.0, r: 0.8386547464865075
06/02/2019 04:04:19 step: 8920, epoch: 270, batch: 9, loss: 0.0013657733798027039, acc: 100.0, f1: 100.0, r: 0.7118191087030674
06/02/2019 04:04:19 step: 8925, epoch: 270, batch: 14, loss: 0.0010476559400558472, acc: 100.0, f1: 100.0, r: 0.813812534756555
06/02/2019 04:04:19 step: 8930, epoch: 270, batch: 19, loss: 0.0008799806237220764, acc: 100.0, f1: 100.0, r: 0.6810826843435474
06/02/2019 04:04:20 step: 8935, epoch: 270, batch: 24, loss: 0.0009766370058059692, acc: 100.0, f1: 100.0, r: 0.6571013142106201
06/02/2019 04:04:20 step: 8940, epoch: 270, batch: 29, loss: 0.0007476955652236938, acc: 100.0, f1: 100.0, r: 0.6620388786587835
06/02/2019 04:04:20 *** evaluating ***
06/02/2019 04:04:20 step: 271, epoch: 270, acc: 51.70940170940172, f1: 23.318636577825284, r: 0.22167415811745453
06/02/2019 04:04:20 *** epoch: 272 ***
06/02/2019 04:04:20 *** training ***
06/02/2019 04:04:20 step: 8948, epoch: 271, batch: 4, loss: 0.0007209405303001404, acc: 100.0, f1: 100.0, r: 0.6719102855050799
06/02/2019 04:04:21 step: 8953, epoch: 271, batch: 9, loss: 0.0010834485292434692, acc: 100.0, f1: 100.0, r: 0.6827594716057999
06/02/2019 04:04:21 step: 8958, epoch: 271, batch: 14, loss: 0.0009983032941818237, acc: 100.0, f1: 100.0, r: 0.6281647480881271
06/02/2019 04:04:21 step: 8963, epoch: 271, batch: 19, loss: 0.0005569308996200562, acc: 100.0, f1: 100.0, r: 0.6500901413178387
06/02/2019 04:04:21 step: 8968, epoch: 271, batch: 24, loss: 0.0005511417984962463, acc: 100.0, f1: 100.0, r: 0.6890850201112226
06/02/2019 04:04:21 step: 8973, epoch: 271, batch: 29, loss: 0.0005727261304855347, acc: 100.0, f1: 100.0, r: 0.6297671200138677
06/02/2019 04:04:22 *** evaluating ***
06/02/2019 04:04:22 step: 272, epoch: 271, acc: 52.13675213675214, f1: 22.834287191810237, r: 0.2265547602992986
06/02/2019 04:04:22 *** epoch: 273 ***
06/02/2019 04:04:22 *** training ***
06/02/2019 04:04:22 step: 8981, epoch: 272, batch: 4, loss: 0.00046603381633758545, acc: 100.0, f1: 100.0, r: 0.7569152074074352
06/02/2019 04:04:22 step: 8986, epoch: 272, batch: 9, loss: 0.0007611215114593506, acc: 100.0, f1: 100.0, r: 0.7208267073880061
06/02/2019 04:04:22 step: 8991, epoch: 272, batch: 14, loss: 0.0005267634987831116, acc: 100.0, f1: 100.0, r: 0.8161146427523105
06/02/2019 04:04:22 step: 8996, epoch: 272, batch: 19, loss: 0.0010334327816963196, acc: 100.0, f1: 100.0, r: 0.6725844196222325
06/02/2019 04:04:23 step: 9001, epoch: 272, batch: 24, loss: 0.000581696629524231, acc: 100.0, f1: 100.0, r: 0.7137952751655829
06/02/2019 04:04:23 step: 9006, epoch: 272, batch: 29, loss: 0.00040362775325775146, acc: 100.0, f1: 100.0, r: 0.7437740574940557
06/02/2019 04:04:23 *** evaluating ***
06/02/2019 04:04:23 step: 273, epoch: 272, acc: 52.56410256410257, f1: 24.21933742317502, r: 0.22853771413801235
06/02/2019 04:04:23 *** epoch: 274 ***
06/02/2019 04:04:23 *** training ***
06/02/2019 04:04:23 step: 9014, epoch: 273, batch: 4, loss: 0.001264907419681549, acc: 100.0, f1: 100.0, r: 0.8064008544246553
06/02/2019 04:04:23 step: 9019, epoch: 273, batch: 9, loss: 0.0008853748440742493, acc: 100.0, f1: 100.0, r: 0.7146546856857192
06/02/2019 04:04:24 step: 9024, epoch: 273, batch: 14, loss: 0.0030273273587226868, acc: 100.0, f1: 100.0, r: 0.7633355383106445
06/02/2019 04:04:24 step: 9029, epoch: 273, batch: 19, loss: 0.0015941187739372253, acc: 100.0, f1: 100.0, r: 0.7494270512755634
06/02/2019 04:04:24 step: 9034, epoch: 273, batch: 24, loss: 0.0018111765384674072, acc: 100.0, f1: 100.0, r: 0.6314980649151198
06/02/2019 04:04:24 step: 9039, epoch: 273, batch: 29, loss: 0.0007538273930549622, acc: 100.0, f1: 100.0, r: 0.7142285292790239
06/02/2019 04:04:24 *** evaluating ***
06/02/2019 04:04:24 step: 274, epoch: 273, acc: 51.28205128205128, f1: 23.121684085372824, r: 0.23776318325199183
06/02/2019 04:04:24 *** epoch: 275 ***
06/02/2019 04:04:24 *** training ***
06/02/2019 04:04:25 step: 9047, epoch: 274, batch: 4, loss: 0.001306183636188507, acc: 100.0, f1: 100.0, r: 0.8139812974180248
06/02/2019 04:04:25 step: 9052, epoch: 274, batch: 9, loss: 0.0023724883794784546, acc: 100.0, f1: 100.0, r: 0.7442290918438111
06/02/2019 04:04:25 step: 9057, epoch: 274, batch: 14, loss: 0.0011838898062705994, acc: 100.0, f1: 100.0, r: 0.6263474042027093
06/02/2019 04:04:25 step: 9062, epoch: 274, batch: 19, loss: 0.004255443811416626, acc: 100.0, f1: 100.0, r: 0.7812481405786185
06/02/2019 04:04:26 step: 9067, epoch: 274, batch: 24, loss: 0.0007818788290023804, acc: 100.0, f1: 100.0, r: 0.7121539498656173
06/02/2019 04:04:26 step: 9072, epoch: 274, batch: 29, loss: 0.001002676784992218, acc: 100.0, f1: 100.0, r: 0.6216475748009394
06/02/2019 04:04:26 *** evaluating ***
06/02/2019 04:04:26 step: 275, epoch: 274, acc: 51.70940170940172, f1: 23.474774299752475, r: 0.2350967241240387
06/02/2019 04:04:26 *** epoch: 276 ***
06/02/2019 04:04:26 *** training ***
06/02/2019 04:04:26 step: 9080, epoch: 275, batch: 4, loss: 0.0005539730191230774, acc: 100.0, f1: 100.0, r: 0.679245294719022
06/02/2019 04:04:27 step: 9085, epoch: 275, batch: 9, loss: 0.0009237527847290039, acc: 100.0, f1: 100.0, r: 0.7875583335382121
06/02/2019 04:04:27 step: 9090, epoch: 275, batch: 14, loss: 0.0007148832082748413, acc: 100.0, f1: 100.0, r: 0.8125533786639322
06/02/2019 04:04:27 step: 9095, epoch: 275, batch: 19, loss: 0.000685572624206543, acc: 100.0, f1: 100.0, r: 0.6782503783776186
06/02/2019 04:04:27 step: 9100, epoch: 275, batch: 24, loss: 0.0014672130346298218, acc: 100.0, f1: 100.0, r: 0.7128870158015413
06/02/2019 04:04:27 step: 9105, epoch: 275, batch: 29, loss: 0.0008786320686340332, acc: 100.0, f1: 100.0, r: 0.7320379854074901
06/02/2019 04:04:27 *** evaluating ***
06/02/2019 04:04:28 step: 276, epoch: 275, acc: 52.13675213675214, f1: 23.631445094293394, r: 0.23418989364275458
06/02/2019 04:04:28 *** epoch: 277 ***
06/02/2019 04:04:28 *** training ***
06/02/2019 04:04:28 step: 9113, epoch: 276, batch: 4, loss: 0.0007848292589187622, acc: 100.0, f1: 100.0, r: 0.6866957119984002
06/02/2019 04:04:28 step: 9118, epoch: 276, batch: 9, loss: 0.001749090850353241, acc: 100.0, f1: 100.0, r: 0.8058897980624411
06/02/2019 04:04:28 step: 9123, epoch: 276, batch: 14, loss: 0.0014010518789291382, acc: 100.0, f1: 100.0, r: 0.6314931474910677
06/02/2019 04:04:28 step: 9128, epoch: 276, batch: 19, loss: 0.0008289888501167297, acc: 100.0, f1: 100.0, r: 0.8048622769865209
06/02/2019 04:04:28 step: 9133, epoch: 276, batch: 24, loss: 0.0007636398077011108, acc: 100.0, f1: 100.0, r: 0.7380581700403489
06/02/2019 04:04:29 step: 9138, epoch: 276, batch: 29, loss: 0.0003143101930618286, acc: 100.0, f1: 100.0, r: 0.770098889030267
06/02/2019 04:04:29 *** evaluating ***
06/02/2019 04:04:29 step: 277, epoch: 276, acc: 51.70940170940172, f1: 23.6248046859817, r: 0.23470209752458976
06/02/2019 04:04:29 *** epoch: 278 ***
06/02/2019 04:04:29 *** training ***
06/02/2019 04:04:29 step: 9146, epoch: 277, batch: 4, loss: 0.0007512494921684265, acc: 100.0, f1: 100.0, r: 0.8262321817461451
06/02/2019 04:04:29 step: 9151, epoch: 277, batch: 9, loss: 0.00038792937994003296, acc: 100.0, f1: 100.0, r: 0.7798859066159021
06/02/2019 04:04:29 step: 9156, epoch: 277, batch: 14, loss: 0.000870831310749054, acc: 100.0, f1: 100.0, r: 0.6876702349909378
06/02/2019 04:04:30 step: 9161, epoch: 277, batch: 19, loss: 0.0013427734375, acc: 100.0, f1: 100.0, r: 0.6872724137606531
06/02/2019 04:04:30 step: 9166, epoch: 277, batch: 24, loss: 0.0009070634841918945, acc: 100.0, f1: 100.0, r: 0.7814607559320176
06/02/2019 04:04:30 step: 9171, epoch: 277, batch: 29, loss: 0.0007079541683197021, acc: 100.0, f1: 100.0, r: 0.7067767564128495
06/02/2019 04:04:30 *** evaluating ***
06/02/2019 04:04:30 step: 278, epoch: 277, acc: 52.13675213675214, f1: 23.933855098683132, r: 0.2268092660830541
06/02/2019 04:04:30 *** epoch: 279 ***
06/02/2019 04:04:30 *** training ***
06/02/2019 04:04:31 step: 9179, epoch: 278, batch: 4, loss: 0.0011765435338020325, acc: 100.0, f1: 100.0, r: 0.8403136371971622
06/02/2019 04:04:31 step: 9184, epoch: 278, batch: 9, loss: 0.00047181546688079834, acc: 100.0, f1: 100.0, r: 0.7013276283971529
06/02/2019 04:04:31 step: 9189, epoch: 278, batch: 14, loss: 0.0007129684090614319, acc: 100.0, f1: 100.0, r: 0.8341060035753008
06/02/2019 04:04:31 step: 9194, epoch: 278, batch: 19, loss: 0.0009546577930450439, acc: 100.0, f1: 100.0, r: 0.7074086366127996
06/02/2019 04:04:32 step: 9199, epoch: 278, batch: 24, loss: 0.0010009482502937317, acc: 100.0, f1: 100.0, r: 0.6942299561672873
06/02/2019 04:04:32 step: 9204, epoch: 278, batch: 29, loss: 0.0010351836681365967, acc: 100.0, f1: 100.0, r: 0.793070208071474
06/02/2019 04:04:32 *** evaluating ***
06/02/2019 04:04:32 step: 279, epoch: 278, acc: 52.56410256410257, f1: 24.15911028442998, r: 0.22578666729221292
06/02/2019 04:04:32 *** epoch: 280 ***
06/02/2019 04:04:32 *** training ***
06/02/2019 04:04:32 step: 9212, epoch: 279, batch: 4, loss: 0.00464872270822525, acc: 100.0, f1: 100.0, r: 0.7906652846474708
06/02/2019 04:04:32 step: 9217, epoch: 279, batch: 9, loss: 0.0005856454372406006, acc: 100.0, f1: 100.0, r: 0.8182537321039418
06/02/2019 04:04:33 step: 9222, epoch: 279, batch: 14, loss: 0.0013069435954093933, acc: 100.0, f1: 100.0, r: 0.7679886191669573
06/02/2019 04:04:33 step: 9227, epoch: 279, batch: 19, loss: 0.0015399232506752014, acc: 100.0, f1: 100.0, r: 0.7473537895922916
06/02/2019 04:04:33 step: 9232, epoch: 279, batch: 24, loss: 0.0006767511367797852, acc: 100.0, f1: 100.0, r: 0.6890541304143316
06/02/2019 04:04:33 step: 9237, epoch: 279, batch: 29, loss: 0.001081317663192749, acc: 100.0, f1: 100.0, r: 0.721102996883196
06/02/2019 04:04:33 *** evaluating ***
06/02/2019 04:04:33 step: 280, epoch: 279, acc: 50.85470085470085, f1: 23.040623115686103, r: 0.2281461721816757
06/02/2019 04:04:33 *** epoch: 281 ***
06/02/2019 04:04:33 *** training ***
06/02/2019 04:04:34 step: 9245, epoch: 280, batch: 4, loss: 0.0004309713840484619, acc: 100.0, f1: 100.0, r: 0.7210679388558369
06/02/2019 04:04:34 step: 9250, epoch: 280, batch: 9, loss: 0.0005678907036781311, acc: 100.0, f1: 100.0, r: 0.8293900947579196
06/02/2019 04:04:34 step: 9255, epoch: 280, batch: 14, loss: 0.0009181872010231018, acc: 100.0, f1: 100.0, r: 0.6650980486301724
06/02/2019 04:04:34 step: 9260, epoch: 280, batch: 19, loss: 0.000999554991722107, acc: 100.0, f1: 100.0, r: 0.7837567411539039
06/02/2019 04:04:34 step: 9265, epoch: 280, batch: 24, loss: 0.0005996301770210266, acc: 100.0, f1: 100.0, r: 0.7395795595143014
06/02/2019 04:04:35 step: 9270, epoch: 280, batch: 29, loss: 0.0007099583745002747, acc: 100.0, f1: 100.0, r: 0.7346951924083517
06/02/2019 04:04:35 *** evaluating ***
06/02/2019 04:04:35 step: 281, epoch: 280, acc: 52.13675213675214, f1: 23.933855098683132, r: 0.22806151827085486
06/02/2019 04:04:35 *** epoch: 282 ***
06/02/2019 04:04:35 *** training ***
06/02/2019 04:04:35 step: 9278, epoch: 281, batch: 4, loss: 0.0005574002861976624, acc: 100.0, f1: 100.0, r: 0.7659924077897148
06/02/2019 04:04:35 step: 9283, epoch: 281, batch: 9, loss: 0.0007551908493041992, acc: 100.0, f1: 100.0, r: 0.8501872341811328
06/02/2019 04:04:36 step: 9288, epoch: 281, batch: 14, loss: 0.001019984483718872, acc: 100.0, f1: 100.0, r: 0.7925186417496323
06/02/2019 04:04:36 step: 9293, epoch: 281, batch: 19, loss: 0.0007464438676834106, acc: 100.0, f1: 100.0, r: 0.6979373369945876
06/02/2019 04:04:36 step: 9298, epoch: 281, batch: 24, loss: 0.0006790608167648315, acc: 100.0, f1: 100.0, r: 0.796463953126176
06/02/2019 04:04:36 step: 9303, epoch: 281, batch: 29, loss: 0.0005382373929023743, acc: 100.0, f1: 100.0, r: 0.7492858549105545
06/02/2019 04:04:36 *** evaluating ***
06/02/2019 04:04:36 step: 282, epoch: 281, acc: 51.70940170940172, f1: 23.488936943310662, r: 0.2301756495202016
06/02/2019 04:04:36 *** epoch: 283 ***
06/02/2019 04:04:36 *** training ***
06/02/2019 04:04:37 step: 9311, epoch: 282, batch: 4, loss: 0.0019880682229995728, acc: 100.0, f1: 100.0, r: 0.6893750395217018
06/02/2019 04:04:37 step: 9316, epoch: 282, batch: 9, loss: 0.0007845982909202576, acc: 100.0, f1: 100.0, r: 0.7004369441288222
06/02/2019 04:04:37 step: 9321, epoch: 282, batch: 14, loss: 0.0005507022142410278, acc: 100.0, f1: 100.0, r: 0.8463001997715107
06/02/2019 04:04:37 step: 9326, epoch: 282, batch: 19, loss: 0.0007551312446594238, acc: 100.0, f1: 100.0, r: 0.7589020040329497
06/02/2019 04:04:37 step: 9331, epoch: 282, batch: 24, loss: 0.0006718561053276062, acc: 100.0, f1: 100.0, r: 0.6864100509407441
06/02/2019 04:04:38 step: 9336, epoch: 282, batch: 29, loss: 0.0012683719396591187, acc: 100.0, f1: 100.0, r: 0.6938814111484719
06/02/2019 04:04:38 *** evaluating ***
06/02/2019 04:04:38 step: 283, epoch: 282, acc: 51.28205128205128, f1: 23.31803983130905, r: 0.22701374842258337
06/02/2019 04:04:38 *** epoch: 284 ***
06/02/2019 04:04:38 *** training ***
06/02/2019 04:04:38 step: 9344, epoch: 283, batch: 4, loss: 0.0008021071553230286, acc: 100.0, f1: 100.0, r: 0.7712090762551183
06/02/2019 04:04:38 step: 9349, epoch: 283, batch: 9, loss: 0.0003805235028266907, acc: 100.0, f1: 100.0, r: 0.8264240237437195
06/02/2019 04:04:38 step: 9354, epoch: 283, batch: 14, loss: 0.00041113048791885376, acc: 100.0, f1: 100.0, r: 0.656042272941812
06/02/2019 04:04:39 step: 9359, epoch: 283, batch: 19, loss: 0.0008814632892608643, acc: 100.0, f1: 100.0, r: 0.8145503479530514
06/02/2019 04:04:39 step: 9364, epoch: 283, batch: 24, loss: 0.0006446167826652527, acc: 100.0, f1: 100.0, r: 0.7416338955197864
06/02/2019 04:04:39 step: 9369, epoch: 283, batch: 29, loss: 0.0005729794502258301, acc: 100.0, f1: 100.0, r: 0.8403781078751535
06/02/2019 04:04:39 *** evaluating ***
06/02/2019 04:04:39 step: 284, epoch: 283, acc: 51.70940170940172, f1: 23.44844324249359, r: 0.22619895391616499
06/02/2019 04:04:39 *** epoch: 285 ***
06/02/2019 04:04:39 *** training ***
06/02/2019 04:04:40 step: 9377, epoch: 284, batch: 4, loss: 0.0005356594920158386, acc: 100.0, f1: 100.0, r: 0.8382481320798032
06/02/2019 04:04:40 step: 9382, epoch: 284, batch: 9, loss: 0.0009623691439628601, acc: 100.0, f1: 100.0, r: 0.7265843283675604
06/02/2019 04:04:40 step: 9387, epoch: 284, batch: 14, loss: 0.001697339117527008, acc: 100.0, f1: 100.0, r: 0.7069607114226956
06/02/2019 04:04:40 step: 9392, epoch: 284, batch: 19, loss: 0.0008907243609428406, acc: 100.0, f1: 100.0, r: 0.7279469731487204
06/02/2019 04:04:41 step: 9397, epoch: 284, batch: 24, loss: 0.0006514787673950195, acc: 100.0, f1: 100.0, r: 0.6394983287809174
06/02/2019 04:04:41 step: 9402, epoch: 284, batch: 29, loss: 0.0017007812857627869, acc: 100.0, f1: 100.0, r: 0.8278874350395489
06/02/2019 04:04:41 *** evaluating ***
06/02/2019 04:04:41 step: 285, epoch: 284, acc: 51.28205128205128, f1: 23.50356535918287, r: 0.23029888471293053
06/02/2019 04:04:41 *** epoch: 286 ***
06/02/2019 04:04:41 *** training ***
06/02/2019 04:04:41 step: 9410, epoch: 285, batch: 4, loss: 0.0008354783058166504, acc: 100.0, f1: 100.0, r: 0.8088881056339151
06/02/2019 04:04:42 step: 9415, epoch: 285, batch: 9, loss: 0.0017320513725280762, acc: 100.0, f1: 100.0, r: 0.8303070265270631
06/02/2019 04:04:42 step: 9420, epoch: 285, batch: 14, loss: 0.0005782246589660645, acc: 100.0, f1: 100.0, r: 0.7937105998237505
06/02/2019 04:04:42 step: 9425, epoch: 285, batch: 19, loss: 0.00048269331455230713, acc: 100.0, f1: 100.0, r: 0.7333670536970589
06/02/2019 04:04:42 step: 9430, epoch: 285, batch: 24, loss: 0.0009534731507301331, acc: 100.0, f1: 100.0, r: 0.7704002705921248
06/02/2019 04:04:42 step: 9435, epoch: 285, batch: 29, loss: 0.0008125752210617065, acc: 100.0, f1: 100.0, r: 0.6587325621106699
06/02/2019 04:04:43 *** evaluating ***
06/02/2019 04:04:43 step: 286, epoch: 285, acc: 51.70940170940172, f1: 23.50551461921862, r: 0.23342148393282958
06/02/2019 04:04:43 *** epoch: 287 ***
06/02/2019 04:04:43 *** training ***
06/02/2019 04:04:43 step: 9443, epoch: 286, batch: 4, loss: 0.0009300857782363892, acc: 100.0, f1: 100.0, r: 0.7471830326891833
06/02/2019 04:04:43 step: 9448, epoch: 286, batch: 9, loss: 0.0008058175444602966, acc: 100.0, f1: 100.0, r: 0.8122251439310652
06/02/2019 04:04:43 step: 9453, epoch: 286, batch: 14, loss: 0.0008744001388549805, acc: 100.0, f1: 100.0, r: 0.712885296638213
06/02/2019 04:04:43 step: 9458, epoch: 286, batch: 19, loss: 0.0006081536412239075, acc: 100.0, f1: 100.0, r: 0.7238641871561365
06/02/2019 04:04:44 step: 9463, epoch: 286, batch: 24, loss: 0.0012934952974319458, acc: 100.0, f1: 100.0, r: 0.7650325824396211
06/02/2019 04:04:44 step: 9468, epoch: 286, batch: 29, loss: 0.0005940496921539307, acc: 100.0, f1: 100.0, r: 0.7990738819684099
06/02/2019 04:04:44 *** evaluating ***
06/02/2019 04:04:44 step: 287, epoch: 286, acc: 51.28205128205128, f1: 23.27246927479126, r: 0.23084246603968941
06/02/2019 04:04:44 *** epoch: 288 ***
06/02/2019 04:04:44 *** training ***
06/02/2019 04:04:44 step: 9476, epoch: 287, batch: 4, loss: 0.000272504985332489, acc: 100.0, f1: 100.0, r: 0.7823522233492839
06/02/2019 04:04:44 step: 9481, epoch: 287, batch: 9, loss: 0.0005246549844741821, acc: 100.0, f1: 100.0, r: 0.7805531573604616
06/02/2019 04:04:45 step: 9486, epoch: 287, batch: 14, loss: 0.0005383193492889404, acc: 100.0, f1: 100.0, r: 0.7710082326162788
06/02/2019 04:04:45 step: 9491, epoch: 287, batch: 19, loss: 0.0007252469658851624, acc: 100.0, f1: 100.0, r: 0.735859291502469
06/02/2019 04:04:45 step: 9496, epoch: 287, batch: 24, loss: 0.0004548579454421997, acc: 100.0, f1: 100.0, r: 0.6514219362951692
06/02/2019 04:04:45 step: 9501, epoch: 287, batch: 29, loss: 0.0006472989916801453, acc: 100.0, f1: 100.0, r: 0.7491424137733211
06/02/2019 04:04:45 *** evaluating ***
06/02/2019 04:04:45 step: 288, epoch: 287, acc: 51.70940170940172, f1: 23.44287508320619, r: 0.23008225995303352
06/02/2019 04:04:45 *** epoch: 289 ***
06/02/2019 04:04:45 *** training ***
06/02/2019 04:04:46 step: 9509, epoch: 288, batch: 4, loss: 0.0017940104007720947, acc: 100.0, f1: 100.0, r: 0.6132782168577857
06/02/2019 04:04:46 step: 9514, epoch: 288, batch: 9, loss: 0.0007520467042922974, acc: 100.0, f1: 100.0, r: 0.7642628263168216
06/02/2019 04:04:46 step: 9519, epoch: 288, batch: 14, loss: 0.0015923380851745605, acc: 100.0, f1: 100.0, r: 0.7385455217129977
06/02/2019 04:04:46 step: 9524, epoch: 288, batch: 19, loss: 0.0009943842887878418, acc: 100.0, f1: 100.0, r: 0.8520173111490292
06/02/2019 04:04:46 step: 9529, epoch: 288, batch: 24, loss: 0.0009875893592834473, acc: 100.0, f1: 100.0, r: 0.7344171669949808
06/02/2019 04:04:47 step: 9534, epoch: 288, batch: 29, loss: 0.002469561994075775, acc: 100.0, f1: 100.0, r: 0.7032503277806297
06/02/2019 04:04:47 *** evaluating ***
06/02/2019 04:04:47 step: 289, epoch: 288, acc: 50.85470085470085, f1: 23.40907845688521, r: 0.2259981630606416
06/02/2019 04:04:47 *** epoch: 290 ***
06/02/2019 04:04:47 *** training ***
06/02/2019 04:04:47 step: 9542, epoch: 289, batch: 4, loss: 0.0011776462197303772, acc: 100.0, f1: 100.0, r: 0.7893543172231893
06/02/2019 04:04:47 step: 9547, epoch: 289, batch: 9, loss: 0.000705033540725708, acc: 100.0, f1: 100.0, r: 0.7604428000911605
06/02/2019 04:04:47 step: 9552, epoch: 289, batch: 14, loss: 0.0004539862275123596, acc: 100.0, f1: 100.0, r: 0.7185697658162367
06/02/2019 04:04:47 step: 9557, epoch: 289, batch: 19, loss: 0.0006460100412368774, acc: 100.0, f1: 100.0, r: 0.7196013729364564
06/02/2019 04:04:48 step: 9562, epoch: 289, batch: 24, loss: 0.0008290782570838928, acc: 100.0, f1: 100.0, r: 0.8183507173117972
06/02/2019 04:04:48 step: 9567, epoch: 289, batch: 29, loss: 0.0006073266267776489, acc: 100.0, f1: 100.0, r: 0.8206478283410131
06/02/2019 04:04:48 *** evaluating ***
06/02/2019 04:04:48 step: 290, epoch: 289, acc: 52.13675213675214, f1: 22.776519806594997, r: 0.23348621889770207
06/02/2019 04:04:48 *** epoch: 291 ***
06/02/2019 04:04:48 *** training ***
06/02/2019 04:04:48 step: 9575, epoch: 290, batch: 4, loss: 0.0015142112970352173, acc: 100.0, f1: 100.0, r: 0.7638938885861153
06/02/2019 04:04:48 step: 9580, epoch: 290, batch: 9, loss: 0.0009935200214385986, acc: 100.0, f1: 100.0, r: 0.6271336736818491
06/02/2019 04:04:49 step: 9585, epoch: 290, batch: 14, loss: 0.0012108013033866882, acc: 100.0, f1: 100.0, r: 0.7630000434086766
06/02/2019 04:04:49 step: 9590, epoch: 290, batch: 19, loss: 0.0007250532507896423, acc: 100.0, f1: 100.0, r: 0.7681419733724825
06/02/2019 04:04:49 step: 9595, epoch: 290, batch: 24, loss: 0.0005317479372024536, acc: 100.0, f1: 100.0, r: 0.7628679384412278
06/02/2019 04:04:50 step: 9600, epoch: 290, batch: 29, loss: 0.0005109608173370361, acc: 100.0, f1: 100.0, r: 0.743978713973539
06/02/2019 04:04:50 *** evaluating ***
06/02/2019 04:04:50 step: 291, epoch: 290, acc: 51.28205128205128, f1: 23.272633535476736, r: 0.2280480251217866
06/02/2019 04:04:50 *** epoch: 292 ***
06/02/2019 04:04:50 *** training ***
06/02/2019 04:04:50 step: 9608, epoch: 291, batch: 4, loss: 0.0004674196243286133, acc: 100.0, f1: 100.0, r: 0.7553236919573609
06/02/2019 04:04:50 step: 9613, epoch: 291, batch: 9, loss: 0.000604778528213501, acc: 100.0, f1: 100.0, r: 0.7345535297118222
06/02/2019 04:04:50 step: 9618, epoch: 291, batch: 14, loss: 0.0006814822554588318, acc: 100.0, f1: 100.0, r: 0.7552384542003232
06/02/2019 04:04:51 step: 9623, epoch: 291, batch: 19, loss: 0.0003182068467140198, acc: 100.0, f1: 100.0, r: 0.7227252542416129
06/02/2019 04:04:51 step: 9628, epoch: 291, batch: 24, loss: 0.00044046342372894287, acc: 100.0, f1: 100.0, r: 0.7186012650614901
06/02/2019 04:04:51 step: 9633, epoch: 291, batch: 29, loss: 0.0006171241402626038, acc: 100.0, f1: 100.0, r: 0.6750791147535115
06/02/2019 04:04:51 *** evaluating ***
06/02/2019 04:04:51 step: 292, epoch: 291, acc: 51.28205128205128, f1: 23.101134022834824, r: 0.23188967387403941
06/02/2019 04:04:51 *** epoch: 293 ***
06/02/2019 04:04:51 *** training ***
06/02/2019 04:04:52 step: 9641, epoch: 292, batch: 4, loss: 0.0005343332886695862, acc: 100.0, f1: 100.0, r: 0.8188551394221713
06/02/2019 04:04:52 step: 9646, epoch: 292, batch: 9, loss: 0.0006922632455825806, acc: 100.0, f1: 100.0, r: 0.6978254462255572
06/02/2019 04:04:52 step: 9651, epoch: 292, batch: 14, loss: 0.0002094358205795288, acc: 100.0, f1: 100.0, r: 0.6949473016805046
06/02/2019 04:04:52 step: 9656, epoch: 292, batch: 19, loss: 0.001240357756614685, acc: 100.0, f1: 100.0, r: 0.8077845210013651
06/02/2019 04:04:52 step: 9661, epoch: 292, batch: 24, loss: 0.0005195587873458862, acc: 100.0, f1: 100.0, r: 0.7560136595309014
06/02/2019 04:04:53 step: 9666, epoch: 292, batch: 29, loss: 0.0006517842411994934, acc: 100.0, f1: 100.0, r: 0.6481112118460283
06/02/2019 04:04:53 *** evaluating ***
06/02/2019 04:04:53 step: 293, epoch: 292, acc: 48.29059829059829, f1: 23.034031224820698, r: 0.23098193396635008
06/02/2019 04:04:53 *** epoch: 294 ***
06/02/2019 04:04:53 *** training ***
06/02/2019 04:04:53 step: 9674, epoch: 293, batch: 4, loss: 0.0013976618647575378, acc: 100.0, f1: 100.0, r: 0.6856309553975106
06/02/2019 04:04:53 step: 9679, epoch: 293, batch: 9, loss: 0.0008011758327484131, acc: 100.0, f1: 100.0, r: 0.6779252980825907
06/02/2019 04:04:53 step: 9684, epoch: 293, batch: 14, loss: 0.0013481676578521729, acc: 100.0, f1: 100.0, r: 0.6936260194297622
06/02/2019 04:04:54 step: 9689, epoch: 293, batch: 19, loss: 0.0009524375200271606, acc: 100.0, f1: 100.0, r: 0.7037140656701678
06/02/2019 04:04:54 step: 9694, epoch: 293, batch: 24, loss: 0.001113690435886383, acc: 100.0, f1: 100.0, r: 0.7946075009100512
06/02/2019 04:04:54 step: 9699, epoch: 293, batch: 29, loss: 0.0005755051970481873, acc: 100.0, f1: 100.0, r: 0.7332706977281709
06/02/2019 04:04:54 *** evaluating ***
06/02/2019 04:04:54 step: 294, epoch: 293, acc: 51.70940170940172, f1: 23.481822453861927, r: 0.23667050354222002
06/02/2019 04:04:54 *** epoch: 295 ***
06/02/2019 04:04:54 *** training ***
06/02/2019 04:04:55 step: 9707, epoch: 294, batch: 4, loss: 0.0005033388733863831, acc: 100.0, f1: 100.0, r: 0.7070036219463214
06/02/2019 04:04:55 step: 9712, epoch: 294, batch: 9, loss: 0.00041410326957702637, acc: 100.0, f1: 100.0, r: 0.7299396418173665
06/02/2019 04:04:55 step: 9717, epoch: 294, batch: 14, loss: 0.0015942156314849854, acc: 100.0, f1: 100.0, r: 0.558760014025964
06/02/2019 04:04:55 step: 9722, epoch: 294, batch: 19, loss: 0.0008979067206382751, acc: 100.0, f1: 100.0, r: 0.6460839215290906
06/02/2019 04:04:56 step: 9727, epoch: 294, batch: 24, loss: 0.0016104057431221008, acc: 100.0, f1: 100.0, r: 0.7626791194498974
06/02/2019 04:04:56 step: 9732, epoch: 294, batch: 29, loss: 0.0010485053062438965, acc: 100.0, f1: 100.0, r: 0.704689146940828
06/02/2019 04:04:56 *** evaluating ***
06/02/2019 04:04:56 step: 295, epoch: 294, acc: 52.13675213675214, f1: 23.527457262601573, r: 0.23042416691308157
06/02/2019 04:04:56 *** epoch: 296 ***
06/02/2019 04:04:56 *** training ***
06/02/2019 04:04:56 step: 9740, epoch: 295, batch: 4, loss: 0.0005683377385139465, acc: 100.0, f1: 100.0, r: 0.6976207185991834
06/02/2019 04:04:57 step: 9745, epoch: 295, batch: 9, loss: 0.0007667243480682373, acc: 100.0, f1: 100.0, r: 0.6806503511741373
06/02/2019 04:04:57 step: 9750, epoch: 295, batch: 14, loss: 0.001154966652393341, acc: 100.0, f1: 100.0, r: 0.787511999545697
06/02/2019 04:04:57 step: 9755, epoch: 295, batch: 19, loss: 0.0006542876362800598, acc: 100.0, f1: 100.0, r: 0.6264508278092144
06/02/2019 04:04:58 step: 9760, epoch: 295, batch: 24, loss: 0.001368016004562378, acc: 100.0, f1: 100.0, r: 0.7062613949895972
06/02/2019 04:04:58 step: 9765, epoch: 295, batch: 29, loss: 0.0013299435377120972, acc: 100.0, f1: 100.0, r: 0.8158510308363118
06/02/2019 04:04:58 *** evaluating ***
06/02/2019 04:04:58 step: 296, epoch: 295, acc: 51.70940170940172, f1: 23.39981178101185, r: 0.23330699477848701
06/02/2019 04:04:58 *** epoch: 297 ***
06/02/2019 04:04:58 *** training ***
06/02/2019 04:04:58 step: 9773, epoch: 296, batch: 4, loss: 0.0011657625436782837, acc: 100.0, f1: 100.0, r: 0.7031582522619507
06/02/2019 04:04:59 step: 9778, epoch: 296, batch: 9, loss: 0.0007672533392906189, acc: 100.0, f1: 100.0, r: 0.6624374896335504
06/02/2019 04:04:59 step: 9783, epoch: 296, batch: 14, loss: 0.0011520609259605408, acc: 100.0, f1: 100.0, r: 0.7126444308692765
06/02/2019 04:04:59 step: 9788, epoch: 296, batch: 19, loss: 0.0011609122157096863, acc: 100.0, f1: 100.0, r: 0.6816449329467124
06/02/2019 04:04:59 step: 9793, epoch: 296, batch: 24, loss: 0.001184806227684021, acc: 100.0, f1: 100.0, r: 0.633527382125157
06/02/2019 04:05:00 step: 9798, epoch: 296, batch: 29, loss: 0.0003828704357147217, acc: 100.0, f1: 100.0, r: 0.762161064856494
06/02/2019 04:05:00 *** evaluating ***
06/02/2019 04:05:00 step: 297, epoch: 296, acc: 51.70940170940172, f1: 23.42125845553532, r: 0.23063082753479788
06/02/2019 04:05:00 *** epoch: 298 ***
06/02/2019 04:05:00 *** training ***
06/02/2019 04:05:00 step: 9806, epoch: 297, batch: 4, loss: 0.0009663030505180359, acc: 100.0, f1: 100.0, r: 0.6925128285136777
06/02/2019 04:05:01 step: 9811, epoch: 297, batch: 9, loss: 0.0006484538316726685, acc: 100.0, f1: 100.0, r: 0.6939398166965544
06/02/2019 04:05:01 step: 9816, epoch: 297, batch: 14, loss: 0.0005833283066749573, acc: 100.0, f1: 100.0, r: 0.7049375062251474
06/02/2019 04:05:01 step: 9821, epoch: 297, batch: 19, loss: 0.0006119459867477417, acc: 100.0, f1: 100.0, r: 0.774564805335232
06/02/2019 04:05:01 step: 9826, epoch: 297, batch: 24, loss: 0.00023469328880310059, acc: 100.0, f1: 100.0, r: 0.795373258108666
06/02/2019 04:05:02 step: 9831, epoch: 297, batch: 29, loss: 0.0005329623818397522, acc: 100.0, f1: 100.0, r: 0.7338110284064402
06/02/2019 04:05:02 *** evaluating ***
06/02/2019 04:05:02 step: 298, epoch: 297, acc: 51.28205128205128, f1: 23.11084508780082, r: 0.23319616710952062
06/02/2019 04:05:02 *** epoch: 299 ***
06/02/2019 04:05:02 *** training ***
06/02/2019 04:05:02 step: 9839, epoch: 298, batch: 4, loss: 0.0015562251210212708, acc: 100.0, f1: 100.0, r: 0.8068736406201591
06/02/2019 04:05:02 step: 9844, epoch: 298, batch: 9, loss: 0.0004855543375015259, acc: 100.0, f1: 100.0, r: 0.6119505613218208
06/02/2019 04:05:03 step: 9849, epoch: 298, batch: 14, loss: 0.0007003098726272583, acc: 100.0, f1: 100.0, r: 0.7307332930009911
06/02/2019 04:05:03 step: 9854, epoch: 298, batch: 19, loss: 0.00037107616662979126, acc: 100.0, f1: 100.0, r: 0.7455356283789104
06/02/2019 04:05:03 step: 9859, epoch: 298, batch: 24, loss: 0.0005131661891937256, acc: 100.0, f1: 100.0, r: 0.7336691883185951
06/02/2019 04:05:03 step: 9864, epoch: 298, batch: 29, loss: 0.000861339271068573, acc: 100.0, f1: 100.0, r: 0.661193038360095
06/02/2019 04:05:04 *** evaluating ***
06/02/2019 04:05:04 step: 299, epoch: 298, acc: 51.28205128205128, f1: 23.2153335234913, r: 0.23128000115671973
06/02/2019 04:05:04 *** epoch: 300 ***
06/02/2019 04:05:04 *** training ***
06/02/2019 04:05:04 step: 9872, epoch: 299, batch: 4, loss: 0.0005040988326072693, acc: 100.0, f1: 100.0, r: 0.6913257961119342
06/02/2019 04:05:04 step: 9877, epoch: 299, batch: 9, loss: 0.000766463577747345, acc: 100.0, f1: 100.0, r: 0.7897333342442265
06/02/2019 04:05:04 step: 9882, epoch: 299, batch: 14, loss: 0.0005636364221572876, acc: 100.0, f1: 100.0, r: 0.8259539851069969
06/02/2019 04:05:05 step: 9887, epoch: 299, batch: 19, loss: 0.0007595494389533997, acc: 100.0, f1: 100.0, r: 0.824984961472164
06/02/2019 04:05:05 step: 9892, epoch: 299, batch: 24, loss: 0.0006220042705535889, acc: 100.0, f1: 100.0, r: 0.6897228378006386
06/02/2019 04:05:05 step: 9897, epoch: 299, batch: 29, loss: 0.0011045262217521667, acc: 100.0, f1: 100.0, r: 0.6814992833110931
06/02/2019 04:05:05 *** evaluating ***
06/02/2019 04:05:05 step: 300, epoch: 299, acc: 51.70940170940172, f1: 23.773274261857853, r: 0.23343648357810234
06/02/2019 04:05:05 
*** Best acc model ***
epoch: 6
acc: 55.12820512820513
f1: 22.01479854629121
corr: 0.29176771704261495
06/02/2019 04:05:05 Loading Test Data
06/02/2019 04:05:05 load data from data/word2vec_temp/test_text.npy, data/word2vec_temp/test_label.npy, training: False
06/02/2019 04:05:29 loaded. total len: 2228
06/02/2019 04:05:29 Test: length: 2228, total batch: 35, batch size: 64
06/02/2019 04:05:29 
*** Test Result ***
acc: 51.70940170940172
f1: 23.773274261857853
corr: 0.23343648357810234
