06/02/2019 01:19:01 {'input_path': 'data/word2vec_temp', 'output_path': 'save/cnn_3', 'gpu': True, 'seed': 20000125, 'display_per_batch': 5, 'optimizer': 'adagrad', 'lr': 0.01, 'lr_decay': 0, 'weight_decay': 0.0001, 'momentum': 0.985, 'num_epochs': 300, 'batch_size': 64, 'num_labels': 8, 'embedding_size': 300, 'type': 'cnn', 'cnn': {'max_length': 512, 'conv_1': {'size': 1024, 'kernel_size': 3, 'dropout': 0.5}, 'max_pool_1': {'kernel_size': 2, 'stride': 2}, 'fc': {'hidden_size': 2000, 'dropout': 0.5}, 'loss': 'cross_entropy'}}
06/02/2019 01:19:01 Loading Train Data
06/02/2019 01:19:01 load data from data/word2vec_temp/train_text.npy, data/word2vec_temp/train_label.npy, training: True
06/02/2019 01:19:31 loaded. total len: 2342
06/02/2019 01:19:31 Train: length: 2108, total batch: 33, batch size: 64
06/02/2019 01:19:31 Dev: length: 234, total batch: 4, batch size: 64
06/02/2019 01:19:31 Loading model cnn
06/02/2019 01:19:56 *** epoch: 1 ***
06/02/2019 01:19:56 *** training ***
06/02/2019 01:19:59 step: 5, epoch: 0, batch: 4, loss: 12.040116310119629, acc: 28.125, f1: 13.181089743589745, r: 0.06256150891092332
06/02/2019 01:20:00 step: 10, epoch: 0, batch: 9, loss: 4.465641975402832, acc: 14.0625, f1: 14.976472802559762, r: -0.010022939239863674
06/02/2019 01:20:02 step: 15, epoch: 0, batch: 14, loss: 2.5148534774780273, acc: 37.5, f1: 16.336405529953918, r: 0.04881528762627842
06/02/2019 01:20:03 step: 20, epoch: 0, batch: 19, loss: 2.213840961456299, acc: 26.5625, f1: 10.1881823202073, r: 0.09884381667706896
06/02/2019 01:20:05 step: 25, epoch: 0, batch: 24, loss: 1.93641197681427, acc: 29.6875, f1: 15.625904405708052, r: 0.12761708301147226
06/02/2019 01:20:06 step: 30, epoch: 0, batch: 29, loss: 1.6336901187896729, acc: 50.0, f1: 18.24926900584795, r: 0.08174212836620759
06/02/2019 01:20:07 *** evaluating ***
06/02/2019 01:20:07 step: 1, epoch: 0, acc: 49.572649572649574, f1: 15.797198963317385, r: 0.20030488837061475
06/02/2019 01:20:07 *** epoch: 2 ***
06/02/2019 01:20:07 *** training ***
06/02/2019 01:20:08 step: 38, epoch: 1, batch: 4, loss: 1.9768385887145996, acc: 35.9375, f1: 12.541731872717786, r: 0.1547318603394235
06/02/2019 01:20:09 step: 43, epoch: 1, batch: 9, loss: 1.5497900247573853, acc: 43.75, f1: 18.06813047799765, r: 0.19953523624587793
06/02/2019 01:20:10 step: 48, epoch: 1, batch: 14, loss: 1.6645461320877075, acc: 39.0625, f1: 21.418512025995742, r: 0.09440590212146817
06/02/2019 01:20:11 step: 53, epoch: 1, batch: 19, loss: 1.6076871156692505, acc: 46.875, f1: 21.27594627594628, r: 0.201511611464352
06/02/2019 01:20:12 step: 58, epoch: 1, batch: 24, loss: 1.6365509033203125, acc: 51.5625, f1: 29.961538461538456, r: 0.2181945985266448
06/02/2019 01:20:12 step: 63, epoch: 1, batch: 29, loss: 1.4924206733703613, acc: 48.4375, f1: 15.979236812570145, r: 0.194284324807348
06/02/2019 01:20:13 *** evaluating ***
06/02/2019 01:20:13 step: 2, epoch: 1, acc: 52.13675213675214, f1: 17.59484046777551, r: 0.22962412336324736
06/02/2019 01:20:13 *** epoch: 3 ***
06/02/2019 01:20:13 *** training ***
06/02/2019 01:20:14 step: 71, epoch: 2, batch: 4, loss: 1.558838129043579, acc: 51.5625, f1: 22.60335395928616, r: 0.2917676049586282
06/02/2019 01:20:15 step: 76, epoch: 2, batch: 9, loss: 1.6189265251159668, acc: 40.625, f1: 15.56293445878848, r: 0.28804895646399387
06/02/2019 01:20:16 step: 81, epoch: 2, batch: 14, loss: 1.613589882850647, acc: 48.4375, f1: 25.757850241545892, r: 0.3115350559762214
06/02/2019 01:20:17 step: 86, epoch: 2, batch: 19, loss: 1.4661107063293457, acc: 51.5625, f1: 15.131578947368418, r: 0.2961257079846477
06/02/2019 01:20:18 step: 91, epoch: 2, batch: 24, loss: 1.5650731325149536, acc: 42.1875, f1: 16.011385874399572, r: 0.20697968600881522
06/02/2019 01:20:18 step: 96, epoch: 2, batch: 29, loss: 1.3501821756362915, acc: 46.875, f1: 17.27272727272727, r: 0.18827399418271473
06/02/2019 01:20:19 *** evaluating ***
06/02/2019 01:20:19 step: 3, epoch: 2, acc: 53.84615384615385, f1: 17.098657121467337, r: 0.23674373679114524
06/02/2019 01:20:19 *** epoch: 4 ***
06/02/2019 01:20:19 *** training ***
06/02/2019 01:20:20 step: 104, epoch: 3, batch: 4, loss: 1.4752843379974365, acc: 46.875, f1: 18.86316457156037, r: 0.26952924312886506
06/02/2019 01:20:21 step: 109, epoch: 3, batch: 9, loss: 1.459113597869873, acc: 50.0, f1: 26.182266009852217, r: 0.3503173004826954
06/02/2019 01:20:22 step: 114, epoch: 3, batch: 14, loss: 1.317315697669983, acc: 51.5625, f1: 28.435559006211182, r: 0.289419339756773
06/02/2019 01:20:23 step: 119, epoch: 3, batch: 19, loss: 1.3596985340118408, acc: 57.8125, f1: 23.220575901593516, r: 0.21446125454438741
06/02/2019 01:20:24 step: 124, epoch: 3, batch: 24, loss: 1.5897430181503296, acc: 43.75, f1: 14.686032863849766, r: 0.21806046585513156
06/02/2019 01:20:24 step: 129, epoch: 3, batch: 29, loss: 1.3999392986297607, acc: 43.75, f1: 19.514066496163682, r: 0.3788248827103048
06/02/2019 01:20:25 *** evaluating ***
06/02/2019 01:20:25 step: 4, epoch: 3, acc: 50.0, f1: 17.91399703130284, r: 0.25170485696252437
06/02/2019 01:20:25 *** epoch: 5 ***
06/02/2019 01:20:25 *** training ***
06/02/2019 01:20:26 step: 137, epoch: 4, batch: 4, loss: 1.31825852394104, acc: 56.25, f1: 23.513251454427923, r: 0.29599598060480475
06/02/2019 01:20:27 step: 142, epoch: 4, batch: 9, loss: 1.2190654277801514, acc: 54.6875, f1: 26.033388253537503, r: 0.3711750220589162
06/02/2019 01:20:28 step: 147, epoch: 4, batch: 14, loss: 1.355654239654541, acc: 45.3125, f1: 20.42708535245849, r: 0.33345931347041446
06/02/2019 01:20:29 step: 152, epoch: 4, batch: 19, loss: 1.1222302913665771, acc: 64.0625, f1: 27.268893387314442, r: 0.36694096447971053
06/02/2019 01:20:30 step: 157, epoch: 4, batch: 24, loss: 1.0274946689605713, acc: 62.5, f1: 26.759946402803543, r: 0.35232948827720223
06/02/2019 01:20:30 step: 162, epoch: 4, batch: 29, loss: 1.265234351158142, acc: 57.8125, f1: 29.534161490683232, r: 0.4286734426439913
06/02/2019 01:20:31 *** evaluating ***
06/02/2019 01:20:31 step: 5, epoch: 4, acc: 52.56410256410257, f1: 18.047688972799104, r: 0.23240759322717333
06/02/2019 01:20:31 *** epoch: 6 ***
06/02/2019 01:20:31 *** training ***
06/02/2019 01:20:32 step: 170, epoch: 5, batch: 4, loss: 1.0079641342163086, acc: 64.0625, f1: 52.86163522012578, r: 0.5633273618757486
06/02/2019 01:20:33 step: 175, epoch: 5, batch: 9, loss: 0.9618296027183533, acc: 62.5, f1: 34.32212465195973, r: 0.41843749737017855
06/02/2019 01:20:34 step: 180, epoch: 5, batch: 14, loss: 0.8071596026420593, acc: 71.875, f1: 41.29729729729729, r: 0.42891885772890526
06/02/2019 01:20:35 step: 185, epoch: 5, batch: 19, loss: 1.058835506439209, acc: 64.0625, f1: 40.60606060606061, r: 0.4947163349724256
06/02/2019 01:20:36 step: 190, epoch: 5, batch: 24, loss: 0.9255537390708923, acc: 71.875, f1: 47.470926783910734, r: 0.47826447417577517
06/02/2019 01:20:36 step: 195, epoch: 5, batch: 29, loss: 0.9387277364730835, acc: 68.75, f1: 46.866359447004605, r: 0.5144427899420189
06/02/2019 01:20:37 *** evaluating ***
06/02/2019 01:20:37 step: 6, epoch: 5, acc: 52.56410256410257, f1: 19.470487192261384, r: 0.2694128862619533
06/02/2019 01:20:37 *** epoch: 7 ***
06/02/2019 01:20:37 *** training ***
06/02/2019 01:20:38 step: 203, epoch: 6, batch: 4, loss: 0.7178077101707458, acc: 79.6875, f1: 69.15465486894058, r: 0.5964386411014445
06/02/2019 01:20:39 step: 208, epoch: 6, batch: 9, loss: 0.6504163146018982, acc: 78.125, f1: 46.69600667693888, r: 0.624972052666373
06/02/2019 01:20:40 step: 213, epoch: 6, batch: 14, loss: 0.6596272587776184, acc: 81.25, f1: 63.60954174513497, r: 0.6437391604581811
06/02/2019 01:20:41 step: 218, epoch: 6, batch: 19, loss: 0.9422646164894104, acc: 70.3125, f1: 45.56547619047619, r: 0.5043157220810532
06/02/2019 01:20:42 step: 223, epoch: 6, batch: 24, loss: 0.9250404238700867, acc: 75.0, f1: 51.99412827461608, r: 0.5064010268272249
06/02/2019 01:20:42 step: 228, epoch: 6, batch: 29, loss: 0.6969078183174133, acc: 81.25, f1: 65.89546783625731, r: 0.6115410696909147
06/02/2019 01:20:43 *** evaluating ***
06/02/2019 01:20:43 step: 7, epoch: 6, acc: 52.991452991452995, f1: 20.651427873202067, r: 0.2966645238517698
06/02/2019 01:20:43 *** epoch: 8 ***
06/02/2019 01:20:43 *** training ***
06/02/2019 01:20:44 step: 236, epoch: 7, batch: 4, loss: 0.6978884935379028, acc: 76.5625, f1: 64.9816924132201, r: 0.4989526930316722
06/02/2019 01:20:45 step: 241, epoch: 7, batch: 9, loss: 0.4887854754924774, acc: 82.8125, f1: 82.50865258384054, r: 0.5816475220910103
06/02/2019 01:20:46 step: 246, epoch: 7, batch: 14, loss: 0.7033937573432922, acc: 78.125, f1: 54.619269949066215, r: 0.47518299762691196
06/02/2019 01:20:47 step: 251, epoch: 7, batch: 19, loss: 0.6627907752990723, acc: 81.25, f1: 70.57792207792208, r: 0.7239887509853744
06/02/2019 01:20:47 step: 256, epoch: 7, batch: 24, loss: 0.7282463312149048, acc: 76.5625, f1: 46.55975877192983, r: 0.5786539372717707
06/02/2019 01:20:48 step: 261, epoch: 7, batch: 29, loss: 0.5455535054206848, acc: 84.375, f1: 70.92705000313696, r: 0.5072064740245383
06/02/2019 01:20:49 *** evaluating ***
06/02/2019 01:20:49 step: 8, epoch: 7, acc: 48.717948717948715, f1: 21.346874851339138, r: 0.30473785881305043
06/02/2019 01:20:49 *** epoch: 9 ***
06/02/2019 01:20:49 *** training ***
06/02/2019 01:20:50 step: 269, epoch: 8, batch: 4, loss: 0.3608110845088959, acc: 92.1875, f1: 81.92147034252298, r: 0.6296694518926105
06/02/2019 01:20:51 step: 274, epoch: 8, batch: 9, loss: 0.5425795316696167, acc: 81.25, f1: 82.35976129582268, r: 0.5664089344497534
06/02/2019 01:20:52 step: 279, epoch: 8, batch: 14, loss: 0.45224708318710327, acc: 87.5, f1: 69.11564625850339, r: 0.5344626777626111
06/02/2019 01:20:53 step: 284, epoch: 8, batch: 19, loss: 0.4056268632411957, acc: 89.0625, f1: 83.2746089888947, r: 0.6064921935815434
06/02/2019 01:20:54 step: 289, epoch: 8, batch: 24, loss: 0.3983367085456848, acc: 90.625, f1: 78.114731272626, r: 0.6635051035109656
06/02/2019 01:20:54 step: 294, epoch: 8, batch: 29, loss: 0.5228291153907776, acc: 81.25, f1: 67.58597883597885, r: 0.7123371944743135
06/02/2019 01:20:55 *** evaluating ***
06/02/2019 01:20:55 step: 9, epoch: 8, acc: 49.572649572649574, f1: 20.295409138312728, r: 0.3335213224226186
06/02/2019 01:20:55 *** epoch: 10 ***
06/02/2019 01:20:55 *** training ***
06/02/2019 01:20:56 step: 302, epoch: 9, batch: 4, loss: 0.33511942625045776, acc: 90.625, f1: 79.66745283018868, r: 0.7400942213330212
06/02/2019 01:20:57 step: 307, epoch: 9, batch: 9, loss: 0.2796688973903656, acc: 92.1875, f1: 87.6795634920635, r: 0.7244461745907194
06/02/2019 01:20:58 step: 312, epoch: 9, batch: 14, loss: 0.273855596780777, acc: 93.75, f1: 89.26391069248211, r: 0.6789215468966254
06/02/2019 01:20:59 step: 317, epoch: 9, batch: 19, loss: 0.43681538105010986, acc: 89.0625, f1: 78.00414078674949, r: 0.6747192805542132
06/02/2019 01:20:59 step: 322, epoch: 9, batch: 24, loss: 0.2795751094818115, acc: 92.1875, f1: 87.2068488412314, r: 0.6188259973175365
06/02/2019 01:21:00 step: 327, epoch: 9, batch: 29, loss: 0.27311044931411743, acc: 92.1875, f1: 90.60732530560117, r: 0.6678315317338918
06/02/2019 01:21:01 *** evaluating ***
06/02/2019 01:21:01 step: 10, epoch: 9, acc: 55.12820512820513, f1: 27.35720649628347, r: 0.3353820629176759
06/02/2019 01:21:01 *** epoch: 11 ***
06/02/2019 01:21:01 *** training ***
06/02/2019 01:21:02 step: 335, epoch: 10, batch: 4, loss: 0.2853884696960449, acc: 92.1875, f1: 94.27126823269354, r: 0.6160771065522975
06/02/2019 01:21:03 step: 340, epoch: 10, batch: 9, loss: 0.21281452476978302, acc: 95.3125, f1: 95.47252747252747, r: 0.6895563749100834
06/02/2019 01:21:04 step: 345, epoch: 10, batch: 14, loss: 0.3006024956703186, acc: 93.75, f1: 79.93595896856766, r: 0.7318772980141327
06/02/2019 01:21:05 step: 350, epoch: 10, batch: 19, loss: 0.1754780113697052, acc: 96.875, f1: 96.96062507690415, r: 0.771475998706346
06/02/2019 01:21:06 step: 355, epoch: 10, batch: 24, loss: 0.1870371699333191, acc: 95.3125, f1: 95.25537910713673, r: 0.7338534610922968
06/02/2019 01:21:06 step: 360, epoch: 10, batch: 29, loss: 0.26502907276153564, acc: 95.3125, f1: 96.81566639959753, r: 0.6629532908884088
06/02/2019 01:21:07 *** evaluating ***
06/02/2019 01:21:07 step: 11, epoch: 10, acc: 55.55555555555556, f1: 27.852678420738776, r: 0.3458955451384374
06/02/2019 01:21:07 *** epoch: 12 ***
06/02/2019 01:21:07 *** training ***
06/02/2019 01:21:08 step: 368, epoch: 11, batch: 4, loss: 0.15412645041942596, acc: 95.3125, f1: 95.019292301901, r: 0.7855377062652226
06/02/2019 01:21:09 step: 373, epoch: 11, batch: 9, loss: 0.21403604745864868, acc: 92.1875, f1: 89.42244091312415, r: 0.7690015050007418
06/02/2019 01:21:10 step: 378, epoch: 11, batch: 14, loss: 0.14431233704090118, acc: 98.4375, f1: 97.20730397422128, r: 0.6920205831010404
06/02/2019 01:21:11 step: 383, epoch: 11, batch: 19, loss: 0.22547698020935059, acc: 93.75, f1: 94.30388371564842, r: 0.5890739256953156
06/02/2019 01:21:11 step: 388, epoch: 11, batch: 24, loss: 0.1834309995174408, acc: 93.75, f1: 90.57419578000692, r: 0.6795623596424595
06/02/2019 01:21:12 step: 393, epoch: 11, batch: 29, loss: 0.19240649044513702, acc: 93.75, f1: 88.2082309195185, r: 0.6178131596511147
06/02/2019 01:21:13 *** evaluating ***
06/02/2019 01:21:13 step: 12, epoch: 11, acc: 53.84615384615385, f1: 23.360183435740172, r: 0.3344516242535074
06/02/2019 01:21:13 *** epoch: 13 ***
06/02/2019 01:21:13 *** training ***
06/02/2019 01:21:14 step: 401, epoch: 12, batch: 4, loss: 0.16016580164432526, acc: 93.75, f1: 90.03819444444446, r: 0.793059159927336
06/02/2019 01:21:15 step: 406, epoch: 12, batch: 9, loss: 0.12689699232578278, acc: 98.4375, f1: 87.2340425531915, r: 0.7374979635375828
06/02/2019 01:21:16 step: 411, epoch: 12, batch: 14, loss: 0.12544739246368408, acc: 98.4375, f1: 99.0514075887393, r: 0.8193339503773892
06/02/2019 01:21:17 step: 416, epoch: 12, batch: 19, loss: 0.09566798806190491, acc: 96.875, f1: 80.23809523809523, r: 0.6054534390425628
06/02/2019 01:21:17 step: 421, epoch: 12, batch: 24, loss: 0.13518401980400085, acc: 98.4375, f1: 99.15902964959568, r: 0.6889187523601684
06/02/2019 01:21:18 step: 426, epoch: 12, batch: 29, loss: 0.20258353650569916, acc: 93.75, f1: 90.57422969187675, r: 0.7404643741078243
06/02/2019 01:21:19 *** evaluating ***
06/02/2019 01:21:19 step: 13, epoch: 12, acc: 55.12820512820513, f1: 27.647037764889077, r: 0.33679941543121145
06/02/2019 01:21:19 *** epoch: 14 ***
06/02/2019 01:21:19 *** training ***
06/02/2019 01:21:20 step: 434, epoch: 13, batch: 4, loss: 0.0576096810400486, acc: 100.0, f1: 100.0, r: 0.7999125654583686
06/02/2019 01:21:21 step: 439, epoch: 13, batch: 9, loss: 0.09296718239784241, acc: 96.875, f1: 97.21981721981722, r: 0.7355610249126161
06/02/2019 01:21:22 step: 444, epoch: 13, batch: 14, loss: 0.10069670528173447, acc: 98.4375, f1: 96.83890577507597, r: 0.704460889855219
06/02/2019 01:21:22 step: 449, epoch: 13, batch: 19, loss: 0.0736747533082962, acc: 100.0, f1: 100.0, r: 0.7996538414902306
06/02/2019 01:21:23 step: 454, epoch: 13, batch: 24, loss: 0.05437789484858513, acc: 100.0, f1: 100.0, r: 0.7788157036307422
06/02/2019 01:21:24 step: 459, epoch: 13, batch: 29, loss: 0.13093402981758118, acc: 96.875, f1: 95.33333333333334, r: 0.777868727220162
06/02/2019 01:21:25 *** evaluating ***
06/02/2019 01:21:25 step: 14, epoch: 13, acc: 54.700854700854705, f1: 27.86305035535028, r: 0.33884660870587463
06/02/2019 01:21:25 *** epoch: 15 ***
06/02/2019 01:21:25 *** training ***
06/02/2019 01:21:26 step: 467, epoch: 14, batch: 4, loss: 0.11780127137899399, acc: 96.875, f1: 94.87581699346406, r: 0.7642539047380684
06/02/2019 01:21:27 step: 472, epoch: 14, batch: 9, loss: 0.11192721873521805, acc: 98.4375, f1: 98.08018068887634, r: 0.670161837244861
06/02/2019 01:21:27 step: 477, epoch: 14, batch: 14, loss: 0.05569930747151375, acc: 100.0, f1: 100.0, r: 0.7817564927340028
06/02/2019 01:21:28 step: 482, epoch: 14, batch: 19, loss: 0.08950959891080856, acc: 98.4375, f1: 99.2158439730572, r: 0.6769120181889315
06/02/2019 01:21:29 step: 487, epoch: 14, batch: 24, loss: 0.10438702255487442, acc: 96.875, f1: 92.46042711088303, r: 0.6648964563921137
06/02/2019 01:21:30 step: 492, epoch: 14, batch: 29, loss: 0.050180863589048386, acc: 100.0, f1: 100.0, r: 0.8051627938995486
06/02/2019 01:21:30 *** evaluating ***
06/02/2019 01:21:31 step: 15, epoch: 14, acc: 55.12820512820513, f1: 26.490112622109773, r: 0.3392013060243734
06/02/2019 01:21:31 *** epoch: 16 ***
06/02/2019 01:21:31 *** training ***
06/02/2019 01:21:32 step: 500, epoch: 15, batch: 4, loss: 0.050545286387205124, acc: 100.0, f1: 100.0, r: 0.8206140520858505
06/02/2019 01:21:33 step: 505, epoch: 15, batch: 9, loss: 0.10901570320129395, acc: 95.3125, f1: 97.44664308943861, r: 0.5697503841702608
06/02/2019 01:21:33 step: 510, epoch: 15, batch: 14, loss: 0.0499301515519619, acc: 100.0, f1: 100.0, r: 0.6725299800412295
06/02/2019 01:21:34 step: 515, epoch: 15, batch: 19, loss: 0.08450069278478622, acc: 98.4375, f1: 98.06763285024155, r: 0.7048604758764664
06/02/2019 01:21:35 step: 520, epoch: 15, batch: 24, loss: 0.056956831365823746, acc: 98.4375, f1: 99.28193499622071, r: 0.836038124796687
06/02/2019 01:21:36 step: 525, epoch: 15, batch: 29, loss: 0.06522490829229355, acc: 98.4375, f1: 98.43175692232296, r: 0.7619089243365536
06/02/2019 01:21:37 *** evaluating ***
06/02/2019 01:21:37 step: 16, epoch: 15, acc: 55.55555555555556, f1: 27.44864454881314, r: 0.3437087416247967
06/02/2019 01:21:37 *** epoch: 17 ***
06/02/2019 01:21:37 *** training ***
06/02/2019 01:21:38 step: 533, epoch: 16, batch: 4, loss: 0.0648043230175972, acc: 100.0, f1: 100.0, r: 0.7222908937782214
06/02/2019 01:21:39 step: 538, epoch: 16, batch: 9, loss: 0.04399953782558441, acc: 100.0, f1: 100.0, r: 0.6869458769596256
06/02/2019 01:21:40 step: 543, epoch: 16, batch: 14, loss: 0.0435691699385643, acc: 100.0, f1: 100.0, r: 0.8029883606801705
06/02/2019 01:21:40 step: 548, epoch: 16, batch: 19, loss: 0.0360456146299839, acc: 100.0, f1: 100.0, r: 0.7577811088015407
06/02/2019 01:21:41 step: 553, epoch: 16, batch: 24, loss: 0.03691780939698219, acc: 100.0, f1: 100.0, r: 0.7962726276197573
06/02/2019 01:21:42 step: 558, epoch: 16, batch: 29, loss: 0.036879487335681915, acc: 100.0, f1: 100.0, r: 0.6979404740292617
06/02/2019 01:21:43 *** evaluating ***
06/02/2019 01:21:43 step: 17, epoch: 16, acc: 55.98290598290598, f1: 29.053461803074804, r: 0.3375616127909913
06/02/2019 01:21:43 *** epoch: 18 ***
06/02/2019 01:21:43 *** training ***
06/02/2019 01:21:44 step: 566, epoch: 17, batch: 4, loss: 0.06113830953836441, acc: 98.4375, f1: 96.4625850340136, r: 0.6858732245191915
06/02/2019 01:21:45 step: 571, epoch: 17, batch: 9, loss: 0.05281704664230347, acc: 98.4375, f1: 98.91589438713062, r: 0.691666242367967
06/02/2019 01:21:46 step: 576, epoch: 17, batch: 14, loss: 0.0356011763215065, acc: 100.0, f1: 100.0, r: 0.7324428225280131
06/02/2019 01:21:46 step: 581, epoch: 17, batch: 19, loss: 0.03727538138628006, acc: 100.0, f1: 100.0, r: 0.7772340777280066
06/02/2019 01:21:47 step: 586, epoch: 17, batch: 24, loss: 0.03156166151165962, acc: 100.0, f1: 100.0, r: 0.5965593107620399
06/02/2019 01:21:48 step: 591, epoch: 17, batch: 29, loss: 0.031049512326717377, acc: 100.0, f1: 100.0, r: 0.7910162629851992
06/02/2019 01:21:49 *** evaluating ***
06/02/2019 01:21:49 step: 18, epoch: 17, acc: 55.12820512820513, f1: 27.88248062624616, r: 0.33847394670992575
06/02/2019 01:21:49 *** epoch: 19 ***
06/02/2019 01:21:49 *** training ***
06/02/2019 01:21:50 step: 599, epoch: 18, batch: 4, loss: 0.02948468178510666, acc: 100.0, f1: 100.0, r: 0.7402868164218027
06/02/2019 01:21:51 step: 604, epoch: 18, batch: 9, loss: 0.018973983824253082, acc: 100.0, f1: 100.0, r: 0.7123674520494465
06/02/2019 01:21:52 step: 609, epoch: 18, batch: 14, loss: 0.03805353119969368, acc: 98.4375, f1: 98.00242130750605, r: 0.7098041869313161
06/02/2019 01:21:52 step: 614, epoch: 18, batch: 19, loss: 0.031008169054985046, acc: 98.4375, f1: 98.63636363636363, r: 0.8370096206338256
06/02/2019 01:21:53 step: 619, epoch: 18, batch: 24, loss: 0.05932626128196716, acc: 98.4375, f1: 99.22312556458898, r: 0.5525481850540058
06/02/2019 01:21:54 step: 624, epoch: 18, batch: 29, loss: 0.03577693924307823, acc: 100.0, f1: 100.0, r: 0.8533133150265686
06/02/2019 01:21:54 *** evaluating ***
06/02/2019 01:21:55 step: 19, epoch: 18, acc: 54.700854700854705, f1: 26.818144074323673, r: 0.33581924707555333
06/02/2019 01:21:55 *** epoch: 20 ***
06/02/2019 01:21:55 *** training ***
06/02/2019 01:21:56 step: 632, epoch: 19, batch: 4, loss: 0.05110645666718483, acc: 98.4375, f1: 98.63056333644569, r: 0.6573586123654527
06/02/2019 01:21:57 step: 637, epoch: 19, batch: 9, loss: 0.026387345045804977, acc: 100.0, f1: 100.0, r: 0.7323306909429378
06/02/2019 01:21:57 step: 642, epoch: 19, batch: 14, loss: 0.05649607256054878, acc: 98.4375, f1: 97.94832826747721, r: 0.7689460426098793
06/02/2019 01:21:58 step: 647, epoch: 19, batch: 19, loss: 0.08397296071052551, acc: 96.875, f1: 98.47619047619048, r: 0.6856586214495117
06/02/2019 01:21:59 step: 652, epoch: 19, batch: 24, loss: 0.028635572642087936, acc: 100.0, f1: 100.0, r: 0.7620047715980639
06/02/2019 01:22:00 step: 657, epoch: 19, batch: 29, loss: 0.03529597073793411, acc: 100.0, f1: 100.0, r: 0.6808960271951412
06/02/2019 01:22:00 *** evaluating ***
06/02/2019 01:22:01 step: 20, epoch: 19, acc: 53.41880341880342, f1: 27.273855146274023, r: 0.32736965427191056
06/02/2019 01:22:01 *** epoch: 21 ***
06/02/2019 01:22:01 *** training ***
06/02/2019 01:22:02 step: 665, epoch: 20, batch: 4, loss: 0.017364386469125748, acc: 100.0, f1: 100.0, r: 0.6551115339617036
06/02/2019 01:22:02 step: 670, epoch: 20, batch: 9, loss: 0.029036886990070343, acc: 100.0, f1: 100.0, r: 0.794439661942759
06/02/2019 01:22:03 step: 675, epoch: 20, batch: 14, loss: 0.052084121853113174, acc: 98.4375, f1: 97.95321637426902, r: 0.7351864867260237
06/02/2019 01:22:04 step: 680, epoch: 20, batch: 19, loss: 0.028448529541492462, acc: 100.0, f1: 100.0, r: 0.8072425017764333
06/02/2019 01:22:05 step: 685, epoch: 20, batch: 24, loss: 0.020607177168130875, acc: 100.0, f1: 100.0, r: 0.7639073245012489
06/02/2019 01:22:06 step: 690, epoch: 20, batch: 29, loss: 0.019187621772289276, acc: 100.0, f1: 100.0, r: 0.7505591916163923
06/02/2019 01:22:06 *** evaluating ***
06/02/2019 01:22:07 step: 21, epoch: 20, acc: 55.55555555555556, f1: 26.40653907451575, r: 0.3411363342388522
06/02/2019 01:22:07 *** epoch: 22 ***
06/02/2019 01:22:07 *** training ***
06/02/2019 01:22:07 step: 698, epoch: 21, batch: 4, loss: 0.06940677016973495, acc: 98.4375, f1: 97.60239760239759, r: 0.6642816949487552
06/02/2019 01:22:08 step: 703, epoch: 21, batch: 9, loss: 0.03213011473417282, acc: 100.0, f1: 100.0, r: 0.7117111725166219
06/02/2019 01:22:09 step: 708, epoch: 21, batch: 14, loss: 0.06619040668010712, acc: 98.4375, f1: 99.02548725637182, r: 0.808852023680143
06/02/2019 01:22:10 step: 713, epoch: 21, batch: 19, loss: 0.09986598789691925, acc: 95.3125, f1: 97.3444976076555, r: 0.7712919436844506
06/02/2019 01:22:11 step: 718, epoch: 21, batch: 24, loss: 0.012740075588226318, acc: 100.0, f1: 100.0, r: 0.7571515510522794
06/02/2019 01:22:12 step: 723, epoch: 21, batch: 29, loss: 0.022501517087221146, acc: 100.0, f1: 100.0, r: 0.716528278241898
06/02/2019 01:22:12 *** evaluating ***
06/02/2019 01:22:13 step: 22, epoch: 21, acc: 55.12820512820513, f1: 27.84054822026727, r: 0.35003511991050484
06/02/2019 01:22:13 *** epoch: 23 ***
06/02/2019 01:22:13 *** training ***
06/02/2019 01:22:13 step: 731, epoch: 22, batch: 4, loss: 0.019740581512451172, acc: 100.0, f1: 100.0, r: 0.7620824077136783
06/02/2019 01:22:14 step: 736, epoch: 22, batch: 9, loss: 0.01708149164915085, acc: 100.0, f1: 100.0, r: 0.713596671749329
06/02/2019 01:22:15 step: 741, epoch: 22, batch: 14, loss: 0.028586946427822113, acc: 100.0, f1: 100.0, r: 0.7166165924270976
06/02/2019 01:22:16 step: 746, epoch: 22, batch: 19, loss: 0.03571534901857376, acc: 98.4375, f1: 99.23521913913127, r: 0.7517829845055458
06/02/2019 01:22:17 step: 751, epoch: 22, batch: 24, loss: 0.021422002464532852, acc: 100.0, f1: 100.0, r: 0.77874728891397
06/02/2019 01:22:18 step: 756, epoch: 22, batch: 29, loss: 0.011914104223251343, acc: 100.0, f1: 100.0, r: 0.7184924651688527
06/02/2019 01:22:18 *** evaluating ***
06/02/2019 01:22:18 step: 23, epoch: 22, acc: 54.700854700854705, f1: 27.401449162774945, r: 0.3417923701443678
06/02/2019 01:22:18 *** epoch: 24 ***
06/02/2019 01:22:18 *** training ***
06/02/2019 01:22:19 step: 764, epoch: 23, batch: 4, loss: 0.014313969761133194, acc: 100.0, f1: 100.0, r: 0.7690659805108763
06/02/2019 01:22:20 step: 769, epoch: 23, batch: 9, loss: 0.034932736307382584, acc: 98.4375, f1: 97.98701298701299, r: 0.7862310505444248
06/02/2019 01:22:21 step: 774, epoch: 23, batch: 14, loss: 0.0361219123005867, acc: 98.4375, f1: 97.92358803986711, r: 0.8159676336103204
06/02/2019 01:22:22 step: 779, epoch: 23, batch: 19, loss: 0.02830587700009346, acc: 100.0, f1: 100.0, r: 0.6962258944531664
06/02/2019 01:22:23 step: 784, epoch: 23, batch: 24, loss: 0.0321911983191967, acc: 100.0, f1: 100.0, r: 0.8090590599088757
06/02/2019 01:22:24 step: 789, epoch: 23, batch: 29, loss: 0.02326691895723343, acc: 98.4375, f1: 97.96918767507003, r: 0.8106021796998439
06/02/2019 01:22:24 *** evaluating ***
06/02/2019 01:22:24 step: 24, epoch: 23, acc: 55.55555555555556, f1: 26.369056079235886, r: 0.3518693209831805
06/02/2019 01:22:24 *** epoch: 25 ***
06/02/2019 01:22:24 *** training ***
06/02/2019 01:22:25 step: 797, epoch: 24, batch: 4, loss: 0.020103201270103455, acc: 100.0, f1: 100.0, r: 0.7345918716133077
06/02/2019 01:22:26 step: 802, epoch: 24, batch: 9, loss: 0.023564238101243973, acc: 100.0, f1: 100.0, r: 0.6509652943877394
06/02/2019 01:22:27 step: 807, epoch: 24, batch: 14, loss: 0.024261388927698135, acc: 100.0, f1: 100.0, r: 0.7919512066477332
06/02/2019 01:22:28 step: 812, epoch: 24, batch: 19, loss: 0.027851924300193787, acc: 100.0, f1: 100.0, r: 0.7527244203706137
06/02/2019 01:22:29 step: 817, epoch: 24, batch: 24, loss: 0.024180877953767776, acc: 100.0, f1: 100.0, r: 0.6971468022889589
06/02/2019 01:22:29 step: 822, epoch: 24, batch: 29, loss: 0.05685838311910629, acc: 98.4375, f1: 86.8421052631579, r: 0.7429617889946337
06/02/2019 01:22:30 *** evaluating ***
06/02/2019 01:22:30 step: 25, epoch: 24, acc: 55.55555555555556, f1: 27.830743551267567, r: 0.3385977426250133
06/02/2019 01:22:30 *** epoch: 26 ***
06/02/2019 01:22:30 *** training ***
06/02/2019 01:22:31 step: 830, epoch: 25, batch: 4, loss: 0.019551649689674377, acc: 100.0, f1: 100.0, r: 0.6794293184742377
06/02/2019 01:22:32 step: 835, epoch: 25, batch: 9, loss: 0.017765432596206665, acc: 100.0, f1: 100.0, r: 0.6513476776759557
06/02/2019 01:22:33 step: 840, epoch: 25, batch: 14, loss: 0.010770760476589203, acc: 100.0, f1: 100.0, r: 0.6835226232236071
06/02/2019 01:22:34 step: 845, epoch: 25, batch: 19, loss: 0.0446140393614769, acc: 100.0, f1: 100.0, r: 0.6535527554083644
06/02/2019 01:22:35 step: 850, epoch: 25, batch: 24, loss: 0.010689213871955872, acc: 100.0, f1: 100.0, r: 0.8070218294164075
06/02/2019 01:22:36 step: 855, epoch: 25, batch: 29, loss: 0.015005405992269516, acc: 100.0, f1: 100.0, r: 0.7976199659393798
06/02/2019 01:22:36 *** evaluating ***
06/02/2019 01:22:36 step: 26, epoch: 25, acc: 55.12820512820513, f1: 28.15297053034958, r: 0.34249639683427563
06/02/2019 01:22:36 *** epoch: 27 ***
06/02/2019 01:22:36 *** training ***
06/02/2019 01:22:37 step: 863, epoch: 26, batch: 4, loss: 0.020346947014331818, acc: 100.0, f1: 100.0, r: 0.7401587683065316
06/02/2019 01:22:38 step: 868, epoch: 26, batch: 9, loss: 0.03292134404182434, acc: 100.0, f1: 100.0, r: 0.6886037077201562
06/02/2019 01:22:39 step: 873, epoch: 26, batch: 14, loss: 0.023487821221351624, acc: 100.0, f1: 100.0, r: 0.7861282409032495
06/02/2019 01:22:40 step: 878, epoch: 26, batch: 19, loss: 0.018191926181316376, acc: 100.0, f1: 100.0, r: 0.7375519691198376
06/02/2019 01:22:41 step: 883, epoch: 26, batch: 24, loss: 0.01569594070315361, acc: 100.0, f1: 100.0, r: 0.7867090255977178
06/02/2019 01:22:41 step: 888, epoch: 26, batch: 29, loss: 0.010192260146141052, acc: 100.0, f1: 100.0, r: 0.831234126954589
06/02/2019 01:22:42 *** evaluating ***
06/02/2019 01:22:42 step: 27, epoch: 26, acc: 52.991452991452995, f1: 26.72924100248447, r: 0.34647953268587367
06/02/2019 01:22:42 *** epoch: 28 ***
06/02/2019 01:22:42 *** training ***
06/02/2019 01:22:43 step: 896, epoch: 27, batch: 4, loss: 0.018934525549411774, acc: 100.0, f1: 100.0, r: 0.7328060647610829
06/02/2019 01:22:44 step: 901, epoch: 27, batch: 9, loss: 0.010251395404338837, acc: 100.0, f1: 100.0, r: 0.667643054003092
06/02/2019 01:22:45 step: 906, epoch: 27, batch: 14, loss: 0.013947129249572754, acc: 100.0, f1: 100.0, r: 0.8051907713370203
06/02/2019 01:22:46 step: 911, epoch: 27, batch: 19, loss: 0.02815847098827362, acc: 100.0, f1: 100.0, r: 0.6759743054489095
06/02/2019 01:22:46 step: 916, epoch: 27, batch: 24, loss: 0.023209013044834137, acc: 100.0, f1: 100.0, r: 0.7438958896842266
06/02/2019 01:22:47 step: 921, epoch: 27, batch: 29, loss: 0.009764984250068665, acc: 100.0, f1: 100.0, r: 0.6647471623202402
06/02/2019 01:22:48 *** evaluating ***
06/02/2019 01:22:48 step: 28, epoch: 27, acc: 56.41025641025641, f1: 28.483083423573223, r: 0.3380186055984016
06/02/2019 01:22:48 *** epoch: 29 ***
06/02/2019 01:22:48 *** training ***
06/02/2019 01:22:49 step: 929, epoch: 28, batch: 4, loss: 0.03507084771990776, acc: 98.4375, f1: 99.23784738358583, r: 0.6872024161335639
06/02/2019 01:22:50 step: 934, epoch: 28, batch: 9, loss: 0.0329270139336586, acc: 98.4375, f1: 96.82539682539684, r: 0.6569982524599225
06/02/2019 01:22:50 step: 939, epoch: 28, batch: 14, loss: 0.017771724611520767, acc: 100.0, f1: 100.0, r: 0.7387889547426132
06/02/2019 01:22:51 step: 944, epoch: 28, batch: 19, loss: 0.016196593642234802, acc: 100.0, f1: 100.0, r: 0.6607219681563286
06/02/2019 01:22:52 step: 949, epoch: 28, batch: 24, loss: 0.010486289858818054, acc: 100.0, f1: 100.0, r: 0.7184041993852518
06/02/2019 01:22:53 step: 954, epoch: 28, batch: 29, loss: 0.01563071459531784, acc: 100.0, f1: 100.0, r: 0.8166503071465104
06/02/2019 01:22:53 *** evaluating ***
06/02/2019 01:22:54 step: 29, epoch: 28, acc: 56.837606837606835, f1: 29.06948270584634, r: 0.3524094761633935
06/02/2019 01:22:54 *** epoch: 30 ***
06/02/2019 01:22:54 *** training ***
06/02/2019 01:22:54 step: 962, epoch: 29, batch: 4, loss: 0.01183021068572998, acc: 100.0, f1: 100.0, r: 0.7052982804710766
06/02/2019 01:22:55 step: 967, epoch: 29, batch: 9, loss: 0.016795124858617783, acc: 100.0, f1: 100.0, r: 0.6288240868610167
06/02/2019 01:22:56 step: 972, epoch: 29, batch: 14, loss: 0.011158816516399384, acc: 100.0, f1: 100.0, r: 0.7697853494410446
06/02/2019 01:22:57 step: 977, epoch: 29, batch: 19, loss: 0.025780022144317627, acc: 98.4375, f1: 98.57142857142858, r: 0.7796004699677074
06/02/2019 01:22:58 step: 982, epoch: 29, batch: 24, loss: 0.010783161967992783, acc: 100.0, f1: 100.0, r: 0.7476570307998498
06/02/2019 01:22:59 step: 987, epoch: 29, batch: 29, loss: 0.01531931385397911, acc: 100.0, f1: 100.0, r: 0.807823146804585
06/02/2019 01:22:59 *** evaluating ***
06/02/2019 01:23:00 step: 30, epoch: 29, acc: 55.12820512820513, f1: 26.34387901693262, r: 0.3498018835130173
06/02/2019 01:23:00 *** epoch: 31 ***
06/02/2019 01:23:00 *** training ***
06/02/2019 01:23:00 step: 995, epoch: 30, batch: 4, loss: 0.024922862648963928, acc: 100.0, f1: 100.0, r: 0.7754559425517902
06/02/2019 01:23:01 step: 1000, epoch: 30, batch: 9, loss: 0.007284991443157196, acc: 100.0, f1: 100.0, r: 0.717003453161838
06/02/2019 01:23:02 step: 1005, epoch: 30, batch: 14, loss: 0.015383362770080566, acc: 100.0, f1: 100.0, r: 0.8516020153271224
06/02/2019 01:23:03 step: 1010, epoch: 30, batch: 19, loss: 0.017936162650585175, acc: 100.0, f1: 100.0, r: 0.6716552040592442
06/02/2019 01:23:04 step: 1015, epoch: 30, batch: 24, loss: 0.011457458138465881, acc: 100.0, f1: 100.0, r: 0.8234000651883177
06/02/2019 01:23:05 step: 1020, epoch: 30, batch: 29, loss: 0.011805541813373566, acc: 100.0, f1: 100.0, r: 0.7839841417605191
06/02/2019 01:23:05 *** evaluating ***
06/02/2019 01:23:05 step: 31, epoch: 30, acc: 57.26495726495726, f1: 29.294937562571167, r: 0.3434298161925881
06/02/2019 01:23:05 *** epoch: 32 ***
06/02/2019 01:23:05 *** training ***
06/02/2019 01:23:06 step: 1028, epoch: 31, batch: 4, loss: 0.018018942326307297, acc: 100.0, f1: 100.0, r: 0.7649422920976743
06/02/2019 01:23:07 step: 1033, epoch: 31, batch: 9, loss: 0.020923234522342682, acc: 100.0, f1: 100.0, r: 0.7388938988271508
06/02/2019 01:23:08 step: 1038, epoch: 31, batch: 14, loss: 0.01709645614027977, acc: 100.0, f1: 100.0, r: 0.6823398040382775
06/02/2019 01:23:09 step: 1043, epoch: 31, batch: 19, loss: 0.009138181805610657, acc: 100.0, f1: 100.0, r: 0.6930012748891694
06/02/2019 01:23:10 step: 1048, epoch: 31, batch: 24, loss: 0.042227938771247864, acc: 98.4375, f1: 99.2158439730572, r: 0.7262472645283218
06/02/2019 01:23:10 step: 1053, epoch: 31, batch: 29, loss: 0.006085306406021118, acc: 100.0, f1: 100.0, r: 0.804515216986417
06/02/2019 01:23:11 *** evaluating ***
06/02/2019 01:23:11 step: 32, epoch: 31, acc: 58.119658119658126, f1: 30.805788624105823, r: 0.34548371594588806
06/02/2019 01:23:11 *** epoch: 33 ***
06/02/2019 01:23:11 *** training ***
06/02/2019 01:23:12 step: 1061, epoch: 32, batch: 4, loss: 0.008905105292797089, acc: 100.0, f1: 100.0, r: 0.7766323035865921
06/02/2019 01:23:13 step: 1066, epoch: 32, batch: 9, loss: 0.01743742823600769, acc: 100.0, f1: 100.0, r: 0.6692367086826521
06/02/2019 01:23:14 step: 1071, epoch: 32, batch: 14, loss: 0.011095091700553894, acc: 100.0, f1: 100.0, r: 0.7073754080248318
06/02/2019 01:23:15 step: 1076, epoch: 32, batch: 19, loss: 0.008896566927433014, acc: 100.0, f1: 100.0, r: 0.7888648102723709
06/02/2019 01:23:16 step: 1081, epoch: 32, batch: 24, loss: 0.01912381872534752, acc: 98.4375, f1: 99.17874396135265, r: 0.8264380638686181
06/02/2019 01:23:16 step: 1086, epoch: 32, batch: 29, loss: 0.04174939915537834, acc: 98.4375, f1: 99.24762531740808, r: 0.6571618085464443
06/02/2019 01:23:17 *** evaluating ***
06/02/2019 01:23:17 step: 33, epoch: 32, acc: 57.26495726495726, f1: 29.45409243362689, r: 0.3504859540164002
06/02/2019 01:23:17 *** epoch: 34 ***
06/02/2019 01:23:17 *** training ***
06/02/2019 01:23:18 step: 1094, epoch: 33, batch: 4, loss: 0.010679878294467926, acc: 100.0, f1: 100.0, r: 0.8086664028430479
06/02/2019 01:23:19 step: 1099, epoch: 33, batch: 9, loss: 0.015956014394760132, acc: 100.0, f1: 100.0, r: 0.8132501404100553
06/02/2019 01:23:20 step: 1104, epoch: 33, batch: 14, loss: 0.008942600339651108, acc: 100.0, f1: 100.0, r: 0.7016897267788686
06/02/2019 01:23:21 step: 1109, epoch: 33, batch: 19, loss: 0.009051613509654999, acc: 100.0, f1: 100.0, r: 0.7093155401818257
06/02/2019 01:23:22 step: 1114, epoch: 33, batch: 24, loss: 0.007209412753582001, acc: 100.0, f1: 100.0, r: 0.6860628269642146
06/02/2019 01:23:22 step: 1119, epoch: 33, batch: 29, loss: 0.00898662954568863, acc: 100.0, f1: 100.0, r: 0.6894489475314648
06/02/2019 01:23:23 *** evaluating ***
06/02/2019 01:23:23 step: 34, epoch: 33, acc: 57.26495726495726, f1: 29.918146354587034, r: 0.34153962744743804
06/02/2019 01:23:23 *** epoch: 35 ***
06/02/2019 01:23:23 *** training ***
06/02/2019 01:23:24 step: 1127, epoch: 34, batch: 4, loss: 0.00386839359998703, acc: 100.0, f1: 100.0, r: 0.6968150092138172
06/02/2019 01:23:25 step: 1132, epoch: 34, batch: 9, loss: 0.013172805309295654, acc: 100.0, f1: 100.0, r: 0.8056320894171946
06/02/2019 01:23:26 step: 1137, epoch: 34, batch: 14, loss: 0.005597002804279327, acc: 100.0, f1: 100.0, r: 0.7567071434422055
06/02/2019 01:23:27 step: 1142, epoch: 34, batch: 19, loss: 0.013408742845058441, acc: 100.0, f1: 100.0, r: 0.8328184898241812
06/02/2019 01:23:27 step: 1147, epoch: 34, batch: 24, loss: 0.010201744735240936, acc: 100.0, f1: 100.0, r: 0.6868610680402828
06/02/2019 01:23:28 step: 1152, epoch: 34, batch: 29, loss: 0.00872868299484253, acc: 100.0, f1: 100.0, r: 0.8276376071438445
06/02/2019 01:23:29 *** evaluating ***
06/02/2019 01:23:29 step: 35, epoch: 34, acc: 56.837606837606835, f1: 26.761438240785395, r: 0.3485865942908202
06/02/2019 01:23:29 *** epoch: 36 ***
06/02/2019 01:23:29 *** training ***
06/02/2019 01:23:30 step: 1160, epoch: 35, batch: 4, loss: 0.010028816759586334, acc: 100.0, f1: 100.0, r: 0.7255174268114468
06/02/2019 01:23:31 step: 1165, epoch: 35, batch: 9, loss: 0.012477811425924301, acc: 100.0, f1: 100.0, r: 0.7245920076014241
06/02/2019 01:23:32 step: 1170, epoch: 35, batch: 14, loss: 0.006182368844747543, acc: 100.0, f1: 100.0, r: 0.8037287529480901
06/02/2019 01:23:32 step: 1175, epoch: 35, batch: 19, loss: 0.026706315577030182, acc: 100.0, f1: 100.0, r: 0.6963089927998652
06/02/2019 01:23:33 step: 1180, epoch: 35, batch: 24, loss: 0.003976397216320038, acc: 100.0, f1: 100.0, r: 0.7745491765869132
06/02/2019 01:23:34 step: 1185, epoch: 35, batch: 29, loss: 0.024754144251346588, acc: 100.0, f1: 100.0, r: 0.8057714779845117
06/02/2019 01:23:34 *** evaluating ***
06/02/2019 01:23:35 step: 36, epoch: 35, acc: 55.55555555555556, f1: 27.211713179898567, r: 0.34083509328176115
06/02/2019 01:23:35 *** epoch: 37 ***
06/02/2019 01:23:35 *** training ***
06/02/2019 01:23:36 step: 1193, epoch: 36, batch: 4, loss: 0.01473119854927063, acc: 100.0, f1: 100.0, r: 0.6711931352919512
06/02/2019 01:23:37 step: 1198, epoch: 36, batch: 9, loss: 0.010935395956039429, acc: 100.0, f1: 100.0, r: 0.7827743910281306
06/02/2019 01:23:37 step: 1203, epoch: 36, batch: 14, loss: 0.00452364981174469, acc: 100.0, f1: 100.0, r: 0.8073447137879541
06/02/2019 01:23:38 step: 1208, epoch: 36, batch: 19, loss: 0.02214093506336212, acc: 98.4375, f1: 96.30252100840336, r: 0.6407664329492833
06/02/2019 01:23:39 step: 1213, epoch: 36, batch: 24, loss: 0.011407531797885895, acc: 100.0, f1: 100.0, r: 0.7786114443719564
06/02/2019 01:23:40 step: 1218, epoch: 36, batch: 29, loss: 0.008946787565946579, acc: 100.0, f1: 100.0, r: 0.7679484840264665
06/02/2019 01:23:40 *** evaluating ***
06/02/2019 01:23:41 step: 37, epoch: 36, acc: 55.98290598290598, f1: 29.471981099176492, r: 0.34333020897085703
06/02/2019 01:23:41 *** epoch: 38 ***
06/02/2019 01:23:41 *** training ***
06/02/2019 01:23:41 step: 1226, epoch: 37, batch: 4, loss: 0.006798572838306427, acc: 100.0, f1: 100.0, r: 0.6945481344217114
06/02/2019 01:23:42 step: 1231, epoch: 37, batch: 9, loss: 0.005866743624210358, acc: 100.0, f1: 100.0, r: 0.7601018372093784
06/02/2019 01:23:43 step: 1236, epoch: 37, batch: 14, loss: 0.002995043992996216, acc: 100.0, f1: 100.0, r: 0.7184479652994584
06/02/2019 01:23:44 step: 1241, epoch: 37, batch: 19, loss: 0.00481492280960083, acc: 100.0, f1: 100.0, r: 0.81271796968129
06/02/2019 01:23:45 step: 1246, epoch: 37, batch: 24, loss: 0.0034004896879196167, acc: 100.0, f1: 100.0, r: 0.7056870714730538
06/02/2019 01:23:46 step: 1251, epoch: 37, batch: 29, loss: 0.00872889906167984, acc: 100.0, f1: 100.0, r: 0.7959864672314624
06/02/2019 01:23:46 *** evaluating ***
06/02/2019 01:23:46 step: 38, epoch: 37, acc: 55.98290598290598, f1: 28.171863617241772, r: 0.34349627275909744
06/02/2019 01:23:46 *** epoch: 39 ***
06/02/2019 01:23:46 *** training ***
06/02/2019 01:23:47 step: 1259, epoch: 38, batch: 4, loss: 0.005296476185321808, acc: 100.0, f1: 100.0, r: 0.7402048804779349
06/02/2019 01:23:48 step: 1264, epoch: 38, batch: 9, loss: 0.018032211810350418, acc: 100.0, f1: 100.0, r: 0.6980107210853124
06/02/2019 01:23:49 step: 1269, epoch: 38, batch: 14, loss: 0.010885253548622131, acc: 100.0, f1: 100.0, r: 0.7516276907710167
06/02/2019 01:23:50 step: 1274, epoch: 38, batch: 19, loss: 0.009346149861812592, acc: 100.0, f1: 100.0, r: 0.6838983025422267
06/02/2019 01:23:51 step: 1279, epoch: 38, batch: 24, loss: 0.017567340284585953, acc: 100.0, f1: 100.0, r: 0.7398968268373554
06/02/2019 01:23:52 step: 1284, epoch: 38, batch: 29, loss: 0.015897929668426514, acc: 98.4375, f1: 99.3015873015873, r: 0.7119081598183468
06/02/2019 01:23:52 *** evaluating ***
06/02/2019 01:23:52 step: 39, epoch: 38, acc: 55.55555555555556, f1: 26.545335903084883, r: 0.34769197379705485
06/02/2019 01:23:52 *** epoch: 40 ***
06/02/2019 01:23:52 *** training ***
06/02/2019 01:23:53 step: 1292, epoch: 39, batch: 4, loss: 0.013834875077009201, acc: 100.0, f1: 100.0, r: 0.6418077724265001
06/02/2019 01:23:54 step: 1297, epoch: 39, batch: 9, loss: 0.02116525173187256, acc: 100.0, f1: 100.0, r: 0.7660873484936013
06/02/2019 01:23:55 step: 1302, epoch: 39, batch: 14, loss: 0.0038888677954673767, acc: 100.0, f1: 100.0, r: 0.7883429658107459
06/02/2019 01:23:56 step: 1307, epoch: 39, batch: 19, loss: 0.011096455156803131, acc: 100.0, f1: 100.0, r: 0.8106885516280992
06/02/2019 01:23:57 step: 1312, epoch: 39, batch: 24, loss: 0.013825595378875732, acc: 100.0, f1: 100.0, r: 0.7257880410066372
06/02/2019 01:23:57 step: 1317, epoch: 39, batch: 29, loss: 0.005215652287006378, acc: 100.0, f1: 100.0, r: 0.7061179057814239
06/02/2019 01:23:58 *** evaluating ***
06/02/2019 01:23:58 step: 40, epoch: 39, acc: 55.55555555555556, f1: 25.22804870689453, r: 0.3409832064532245
06/02/2019 01:23:58 *** epoch: 41 ***
06/02/2019 01:23:58 *** training ***
06/02/2019 01:23:59 step: 1325, epoch: 40, batch: 4, loss: 0.02194075845181942, acc: 100.0, f1: 100.0, r: 0.8048616548925908
06/02/2019 01:24:00 step: 1330, epoch: 40, batch: 9, loss: 0.005743533372879028, acc: 100.0, f1: 100.0, r: 0.7882677598039691
06/02/2019 01:24:01 step: 1335, epoch: 40, batch: 14, loss: 0.013211429119110107, acc: 100.0, f1: 100.0, r: 0.8017615149013171
06/02/2019 01:24:01 step: 1340, epoch: 40, batch: 19, loss: 0.020990777760744095, acc: 100.0, f1: 100.0, r: 0.7236837544977689
06/02/2019 01:24:02 step: 1345, epoch: 40, batch: 24, loss: 0.008821479976177216, acc: 100.0, f1: 100.0, r: 0.7173516191247823
06/02/2019 01:24:03 step: 1350, epoch: 40, batch: 29, loss: 0.009842373430728912, acc: 100.0, f1: 100.0, r: 0.6087992331720933
06/02/2019 01:24:04 *** evaluating ***
06/02/2019 01:24:04 step: 41, epoch: 40, acc: 57.26495726495726, f1: 29.626323776758053, r: 0.3430983368817431
06/02/2019 01:24:04 *** epoch: 42 ***
06/02/2019 01:24:04 *** training ***
06/02/2019 01:24:05 step: 1358, epoch: 41, batch: 4, loss: 0.006314404308795929, acc: 100.0, f1: 100.0, r: 0.8454045136264521
06/02/2019 01:24:06 step: 1363, epoch: 41, batch: 9, loss: 0.02040497213602066, acc: 100.0, f1: 100.0, r: 0.7723100955460573
06/02/2019 01:24:07 step: 1368, epoch: 41, batch: 14, loss: 0.006649509072303772, acc: 100.0, f1: 100.0, r: 0.6936011125169268
06/02/2019 01:24:07 step: 1373, epoch: 41, batch: 19, loss: 0.011707209050655365, acc: 100.0, f1: 100.0, r: 0.829679464631593
06/02/2019 01:24:08 step: 1378, epoch: 41, batch: 24, loss: 0.0021236762404441833, acc: 100.0, f1: 100.0, r: 0.8293299881826879
06/02/2019 01:24:09 step: 1383, epoch: 41, batch: 29, loss: 0.0068968236446380615, acc: 100.0, f1: 100.0, r: 0.6632871541311158
06/02/2019 01:24:09 *** evaluating ***
06/02/2019 01:24:10 step: 42, epoch: 41, acc: 56.41025641025641, f1: 27.3606951467383, r: 0.33850233719532796
06/02/2019 01:24:10 *** epoch: 43 ***
06/02/2019 01:24:10 *** training ***
06/02/2019 01:24:11 step: 1391, epoch: 42, batch: 4, loss: 0.012417368590831757, acc: 100.0, f1: 100.0, r: 0.7836200400485223
06/02/2019 01:24:11 step: 1396, epoch: 42, batch: 9, loss: 0.004885397851467133, acc: 100.0, f1: 100.0, r: 0.6764506278236834
06/02/2019 01:24:12 step: 1401, epoch: 42, batch: 14, loss: 0.0052327960729599, acc: 100.0, f1: 100.0, r: 0.8041085700146136
06/02/2019 01:24:13 step: 1406, epoch: 42, batch: 19, loss: 0.015096388757228851, acc: 100.0, f1: 100.0, r: 0.7865090168828621
06/02/2019 01:24:14 step: 1411, epoch: 42, batch: 24, loss: 0.014676913619041443, acc: 100.0, f1: 100.0, r: 0.8005377700121241
06/02/2019 01:24:15 step: 1416, epoch: 42, batch: 29, loss: 0.024891749024391174, acc: 98.4375, f1: 99.3331164606376, r: 0.7825684963411144
06/02/2019 01:24:15 *** evaluating ***
06/02/2019 01:24:16 step: 43, epoch: 42, acc: 57.26495726495726, f1: 29.34096043914256, r: 0.33887463792695427
06/02/2019 01:24:16 *** epoch: 44 ***
06/02/2019 01:24:16 *** training ***
06/02/2019 01:24:16 step: 1424, epoch: 43, batch: 4, loss: 0.024742741137742996, acc: 98.4375, f1: 99.15343915343915, r: 0.6919108924707618
06/02/2019 01:24:17 step: 1429, epoch: 43, batch: 9, loss: 0.0073537677526474, acc: 100.0, f1: 100.0, r: 0.8034174530102504
06/02/2019 01:24:18 step: 1434, epoch: 43, batch: 14, loss: 0.01603572815656662, acc: 100.0, f1: 100.0, r: 0.830180152976591
06/02/2019 01:24:19 step: 1439, epoch: 43, batch: 19, loss: 0.0029816478490829468, acc: 100.0, f1: 100.0, r: 0.8323323478499108
06/02/2019 01:24:20 step: 1444, epoch: 43, batch: 24, loss: 0.00816374272108078, acc: 100.0, f1: 100.0, r: 0.8042534519840934
06/02/2019 01:24:21 step: 1449, epoch: 43, batch: 29, loss: 0.013131571933627129, acc: 100.0, f1: 100.0, r: 0.7910668929798761
06/02/2019 01:24:21 *** evaluating ***
06/02/2019 01:24:21 step: 44, epoch: 43, acc: 55.98290598290598, f1: 27.386704601842965, r: 0.3374860580517814
06/02/2019 01:24:21 *** epoch: 45 ***
06/02/2019 01:24:21 *** training ***
06/02/2019 01:24:22 step: 1457, epoch: 44, batch: 4, loss: 0.010788608342409134, acc: 100.0, f1: 100.0, r: 0.7166597551492608
06/02/2019 01:24:23 step: 1462, epoch: 44, batch: 9, loss: 0.0029603317379951477, acc: 100.0, f1: 100.0, r: 0.6897084775159904
06/02/2019 01:24:24 step: 1467, epoch: 44, batch: 14, loss: 0.00904460996389389, acc: 100.0, f1: 100.0, r: 0.701777622864386
06/02/2019 01:24:25 step: 1472, epoch: 44, batch: 19, loss: 0.00725293904542923, acc: 100.0, f1: 100.0, r: 0.559451444125628
06/02/2019 01:24:26 step: 1477, epoch: 44, batch: 24, loss: 0.018723666667938232, acc: 98.4375, f1: 97.75132275132275, r: 0.7894802414509868
06/02/2019 01:24:26 step: 1482, epoch: 44, batch: 29, loss: 0.0048719123005867004, acc: 100.0, f1: 100.0, r: 0.7892977638551568
06/02/2019 01:24:27 *** evaluating ***
06/02/2019 01:24:27 step: 45, epoch: 44, acc: 55.98290598290598, f1: 26.941268958736497, r: 0.34169683277698704
06/02/2019 01:24:27 *** epoch: 46 ***
06/02/2019 01:24:27 *** training ***
06/02/2019 01:24:28 step: 1490, epoch: 45, batch: 4, loss: 0.007153533399105072, acc: 100.0, f1: 100.0, r: 0.7946417623090011
06/02/2019 01:24:29 step: 1495, epoch: 45, batch: 9, loss: 0.01285187155008316, acc: 100.0, f1: 100.0, r: 0.5993166081276242
06/02/2019 01:24:30 step: 1500, epoch: 45, batch: 14, loss: 0.002480126917362213, acc: 100.0, f1: 100.0, r: 0.6783339097008607
06/02/2019 01:24:31 step: 1505, epoch: 45, batch: 19, loss: 0.011626742780208588, acc: 100.0, f1: 100.0, r: 0.796458415371817
06/02/2019 01:24:32 step: 1510, epoch: 45, batch: 24, loss: 0.006324678659439087, acc: 100.0, f1: 100.0, r: 0.6234103752260587
06/02/2019 01:24:32 step: 1515, epoch: 45, batch: 29, loss: 0.008363213390111923, acc: 100.0, f1: 100.0, r: 0.6712524876026528
06/02/2019 01:24:33 *** evaluating ***
06/02/2019 01:24:33 step: 46, epoch: 45, acc: 58.54700854700855, f1: 29.877793046375363, r: 0.3342796769415808
06/02/2019 01:24:33 *** epoch: 47 ***
06/02/2019 01:24:33 *** training ***
06/02/2019 01:24:34 step: 1523, epoch: 46, batch: 4, loss: 0.00829026848077774, acc: 100.0, f1: 100.0, r: 0.7621399509756168
06/02/2019 01:24:35 step: 1528, epoch: 46, batch: 9, loss: 0.0013009309768676758, acc: 100.0, f1: 100.0, r: 0.6550842794002745
06/02/2019 01:24:36 step: 1533, epoch: 46, batch: 14, loss: 0.012294776737689972, acc: 100.0, f1: 100.0, r: 0.6500226595779124
06/02/2019 01:24:36 step: 1538, epoch: 46, batch: 19, loss: 0.018328752368688583, acc: 100.0, f1: 100.0, r: 0.6521211381652898
06/02/2019 01:24:37 step: 1543, epoch: 46, batch: 24, loss: 0.00412788987159729, acc: 100.0, f1: 100.0, r: 0.7387151208682822
06/02/2019 01:24:38 step: 1548, epoch: 46, batch: 29, loss: 0.01218365877866745, acc: 100.0, f1: 100.0, r: 0.7890484307588156
06/02/2019 01:24:38 *** evaluating ***
06/02/2019 01:24:39 step: 47, epoch: 46, acc: 56.41025641025641, f1: 28.260576278181915, r: 0.333739530603501
06/02/2019 01:24:39 *** epoch: 48 ***
06/02/2019 01:24:39 *** training ***
06/02/2019 01:24:40 step: 1556, epoch: 47, batch: 4, loss: 0.0024510324001312256, acc: 100.0, f1: 100.0, r: 0.8109985241021647
06/02/2019 01:24:40 step: 1561, epoch: 47, batch: 9, loss: 0.004225306212902069, acc: 100.0, f1: 100.0, r: 0.8101740926528389
06/02/2019 01:24:41 step: 1566, epoch: 47, batch: 14, loss: 0.013478875160217285, acc: 100.0, f1: 100.0, r: 0.6435342923323689
06/02/2019 01:24:42 step: 1571, epoch: 47, batch: 19, loss: 0.009257480502128601, acc: 100.0, f1: 100.0, r: 0.7176633693274987
06/02/2019 01:24:43 step: 1576, epoch: 47, batch: 24, loss: 0.004330851137638092, acc: 100.0, f1: 100.0, r: 0.740548336355633
06/02/2019 01:24:44 step: 1581, epoch: 47, batch: 29, loss: 0.0040299370884895325, acc: 100.0, f1: 100.0, r: 0.7363272482550897
06/02/2019 01:24:44 *** evaluating ***
06/02/2019 01:24:45 step: 48, epoch: 47, acc: 55.12820512820513, f1: 27.846724808146067, r: 0.3271891140311927
06/02/2019 01:24:45 *** epoch: 49 ***
06/02/2019 01:24:45 *** training ***
06/02/2019 01:24:45 step: 1589, epoch: 48, batch: 4, loss: 0.008969958871603012, acc: 100.0, f1: 100.0, r: 0.7270022220483932
06/02/2019 01:24:46 step: 1594, epoch: 48, batch: 9, loss: 0.006460078060626984, acc: 100.0, f1: 100.0, r: 0.7448324475917307
06/02/2019 01:24:47 step: 1599, epoch: 48, batch: 14, loss: 0.010020092129707336, acc: 100.0, f1: 100.0, r: 0.7068802091894177
06/02/2019 01:24:48 step: 1604, epoch: 48, batch: 19, loss: 0.004018664360046387, acc: 100.0, f1: 100.0, r: 0.7599499577016909
06/02/2019 01:24:49 step: 1609, epoch: 48, batch: 24, loss: 0.008980557322502136, acc: 100.0, f1: 100.0, r: 0.778614198776426
06/02/2019 01:24:50 step: 1614, epoch: 48, batch: 29, loss: 0.00378505140542984, acc: 100.0, f1: 100.0, r: 0.6845843280737614
06/02/2019 01:24:50 *** evaluating ***
06/02/2019 01:24:50 step: 49, epoch: 48, acc: 56.837606837606835, f1: 29.263391093949448, r: 0.3271124545076121
06/02/2019 01:24:50 *** epoch: 50 ***
06/02/2019 01:24:50 *** training ***
06/02/2019 01:24:51 step: 1622, epoch: 49, batch: 4, loss: 0.010674521327018738, acc: 100.0, f1: 100.0, r: 0.7713690553364327
06/02/2019 01:24:52 step: 1627, epoch: 49, batch: 9, loss: 0.006193187087774277, acc: 100.0, f1: 100.0, r: 0.6789357346689909
06/02/2019 01:24:53 step: 1632, epoch: 49, batch: 14, loss: 0.004160933196544647, acc: 100.0, f1: 100.0, r: 0.7497250275293217
06/02/2019 01:24:54 step: 1637, epoch: 49, batch: 19, loss: 0.005333907902240753, acc: 100.0, f1: 100.0, r: 0.8162905130101177
06/02/2019 01:24:55 step: 1642, epoch: 49, batch: 24, loss: 0.00778670608997345, acc: 100.0, f1: 100.0, r: 0.6646719354461511
06/02/2019 01:24:56 step: 1647, epoch: 49, batch: 29, loss: 0.007783904671669006, acc: 100.0, f1: 100.0, r: 0.7056744171553683
06/02/2019 01:24:56 *** evaluating ***
06/02/2019 01:24:56 step: 50, epoch: 49, acc: 55.98290598290598, f1: 29.560139579489427, r: 0.33148296472916483
06/02/2019 01:24:56 *** epoch: 51 ***
06/02/2019 01:24:56 *** training ***
06/02/2019 01:24:57 step: 1655, epoch: 50, batch: 4, loss: 0.029130078852176666, acc: 98.4375, f1: 98.0952380952381, r: 0.7254303058568313
06/02/2019 01:24:58 step: 1660, epoch: 50, batch: 9, loss: 0.010030500590801239, acc: 100.0, f1: 100.0, r: 0.777752194908505
06/02/2019 01:24:59 step: 1665, epoch: 50, batch: 14, loss: 0.014441948384046555, acc: 100.0, f1: 100.0, r: 0.8645117062706815
06/02/2019 01:25:00 step: 1670, epoch: 50, batch: 19, loss: 0.00384356826543808, acc: 100.0, f1: 100.0, r: 0.8153910775171243
06/02/2019 01:25:01 step: 1675, epoch: 50, batch: 24, loss: 0.0038151815533638, acc: 100.0, f1: 100.0, r: 0.7724660980115681
06/02/2019 01:25:01 step: 1680, epoch: 50, batch: 29, loss: 0.012339688837528229, acc: 100.0, f1: 100.0, r: 0.6949099416618688
06/02/2019 01:25:02 *** evaluating ***
06/02/2019 01:25:02 step: 51, epoch: 50, acc: 56.41025641025641, f1: 26.92042010628547, r: 0.33628361489084385
06/02/2019 01:25:02 *** epoch: 52 ***
06/02/2019 01:25:02 *** training ***
06/02/2019 01:25:03 step: 1688, epoch: 51, batch: 4, loss: 0.008071370422840118, acc: 100.0, f1: 100.0, r: 0.8037998822384294
06/02/2019 01:25:04 step: 1693, epoch: 51, batch: 9, loss: 0.00452665239572525, acc: 100.0, f1: 100.0, r: 0.6722644492980467
06/02/2019 01:25:05 step: 1698, epoch: 51, batch: 14, loss: 0.011808589100837708, acc: 100.0, f1: 100.0, r: 0.748095948350485
06/02/2019 01:25:06 step: 1703, epoch: 51, batch: 19, loss: 0.003683537244796753, acc: 100.0, f1: 100.0, r: 0.6499137751287266
06/02/2019 01:25:06 step: 1708, epoch: 51, batch: 24, loss: 0.009764991700649261, acc: 100.0, f1: 100.0, r: 0.7607565440590806
06/02/2019 01:25:07 step: 1713, epoch: 51, batch: 29, loss: 0.022734105587005615, acc: 98.4375, f1: 99.11111111111111, r: 0.697422269683124
06/02/2019 01:25:08 *** evaluating ***
06/02/2019 01:25:08 step: 52, epoch: 51, acc: 56.41025641025641, f1: 28.0999990399111, r: 0.33682396164509776
06/02/2019 01:25:08 *** epoch: 53 ***
06/02/2019 01:25:08 *** training ***
06/02/2019 01:25:09 step: 1721, epoch: 52, batch: 4, loss: 0.010732270777225494, acc: 100.0, f1: 100.0, r: 0.7991128346755797
06/02/2019 01:25:10 step: 1726, epoch: 52, batch: 9, loss: 0.005996778607368469, acc: 100.0, f1: 100.0, r: 0.8104941400862303
06/02/2019 01:25:11 step: 1731, epoch: 52, batch: 14, loss: 0.015174902975559235, acc: 100.0, f1: 100.0, r: 0.7802768759309016
06/02/2019 01:25:11 step: 1736, epoch: 52, batch: 19, loss: 0.009272538125514984, acc: 100.0, f1: 100.0, r: 0.745606931549632
06/02/2019 01:25:12 step: 1741, epoch: 52, batch: 24, loss: 0.004525378346443176, acc: 100.0, f1: 100.0, r: 0.8100847202299108
06/02/2019 01:25:13 step: 1746, epoch: 52, batch: 29, loss: 0.007078841328620911, acc: 100.0, f1: 100.0, r: 0.8085565875586179
06/02/2019 01:25:13 *** evaluating ***
06/02/2019 01:25:14 step: 53, epoch: 52, acc: 55.55555555555556, f1: 27.796885139519794, r: 0.33284677434228394
06/02/2019 01:25:14 *** epoch: 54 ***
06/02/2019 01:25:14 *** training ***
06/02/2019 01:25:15 step: 1754, epoch: 53, batch: 4, loss: 0.0034411177039146423, acc: 100.0, f1: 100.0, r: 0.8085158694290024
06/02/2019 01:25:15 step: 1759, epoch: 53, batch: 9, loss: 0.007548891007900238, acc: 100.0, f1: 100.0, r: 0.6884658766278257
06/02/2019 01:25:16 step: 1764, epoch: 53, batch: 14, loss: 0.00287744402885437, acc: 100.0, f1: 100.0, r: 0.7322948377853228
06/02/2019 01:25:17 step: 1769, epoch: 53, batch: 19, loss: 0.003266632556915283, acc: 100.0, f1: 100.0, r: 0.7604264270948663
06/02/2019 01:25:18 step: 1774, epoch: 53, batch: 24, loss: 0.004070445895195007, acc: 100.0, f1: 100.0, r: 0.7611637907033845
06/02/2019 01:25:19 step: 1779, epoch: 53, batch: 29, loss: 0.007033728063106537, acc: 100.0, f1: 100.0, r: 0.7237211694518947
06/02/2019 01:25:19 *** evaluating ***
06/02/2019 01:25:20 step: 54, epoch: 53, acc: 56.41025641025641, f1: 28.42313551911583, r: 0.33802740900656514
06/02/2019 01:25:20 *** epoch: 55 ***
06/02/2019 01:25:20 *** training ***
06/02/2019 01:25:21 step: 1787, epoch: 54, batch: 4, loss: 0.014010988175868988, acc: 100.0, f1: 100.0, r: 0.7932657696491603
06/02/2019 01:25:22 step: 1792, epoch: 54, batch: 9, loss: 0.005483552813529968, acc: 100.0, f1: 100.0, r: 0.6759727221743342
06/02/2019 01:25:22 step: 1797, epoch: 54, batch: 14, loss: 0.0027463436126708984, acc: 100.0, f1: 100.0, r: 0.6538222319342517
06/02/2019 01:25:23 step: 1802, epoch: 54, batch: 19, loss: 0.011370263993740082, acc: 100.0, f1: 100.0, r: 0.752825112283057
06/02/2019 01:25:24 step: 1807, epoch: 54, batch: 24, loss: 0.010106749832630157, acc: 100.0, f1: 100.0, r: 0.7377926264074364
06/02/2019 01:25:25 step: 1812, epoch: 54, batch: 29, loss: 0.017655465751886368, acc: 100.0, f1: 100.0, r: 0.8131995018161948
06/02/2019 01:25:25 *** evaluating ***
06/02/2019 01:25:26 step: 55, epoch: 54, acc: 56.837606837606835, f1: 28.444865319865322, r: 0.3460396261240729
06/02/2019 01:25:26 *** epoch: 56 ***
06/02/2019 01:25:26 *** training ***
06/02/2019 01:25:27 step: 1820, epoch: 55, batch: 4, loss: 0.04218244552612305, acc: 98.4375, f1: 98.25396825396825, r: 0.7763760849209832
06/02/2019 01:25:28 step: 1825, epoch: 55, batch: 9, loss: 0.006451837718486786, acc: 100.0, f1: 100.0, r: 0.700822075152696
06/02/2019 01:25:28 step: 1830, epoch: 55, batch: 14, loss: 0.007055096328258514, acc: 100.0, f1: 100.0, r: 0.7617214634384839
06/02/2019 01:25:29 step: 1835, epoch: 55, batch: 19, loss: 0.0032820850610733032, acc: 100.0, f1: 100.0, r: 0.8121553503828742
06/02/2019 01:25:30 step: 1840, epoch: 55, batch: 24, loss: 0.012111164629459381, acc: 100.0, f1: 100.0, r: 0.7834819576686602
06/02/2019 01:25:31 step: 1845, epoch: 55, batch: 29, loss: 0.010283827781677246, acc: 100.0, f1: 100.0, r: 0.7542421387117518
06/02/2019 01:25:31 *** evaluating ***
06/02/2019 01:25:32 step: 56, epoch: 55, acc: 57.26495726495726, f1: 27.541682228446934, r: 0.3500343767020583
06/02/2019 01:25:32 *** epoch: 57 ***
06/02/2019 01:25:32 *** training ***
06/02/2019 01:25:33 step: 1853, epoch: 56, batch: 4, loss: 0.004294373095035553, acc: 100.0, f1: 100.0, r: 0.7795215867984432
06/02/2019 01:25:33 step: 1858, epoch: 56, batch: 9, loss: 0.006215251982212067, acc: 100.0, f1: 100.0, r: 0.7132385295912168
06/02/2019 01:25:34 step: 1863, epoch: 56, batch: 14, loss: 0.007146403193473816, acc: 100.0, f1: 100.0, r: 0.7954661756702461
06/02/2019 01:25:35 step: 1868, epoch: 56, batch: 19, loss: 0.030247308313846588, acc: 98.4375, f1: 83.6734693877551, r: 0.6551349936626538
06/02/2019 01:25:36 step: 1873, epoch: 56, batch: 24, loss: 0.008382558822631836, acc: 100.0, f1: 100.0, r: 0.7140971398166719
06/02/2019 01:25:37 step: 1878, epoch: 56, batch: 29, loss: 0.006582409143447876, acc: 100.0, f1: 100.0, r: 0.8446049615839242
06/02/2019 01:25:37 *** evaluating ***
06/02/2019 01:25:38 step: 57, epoch: 56, acc: 57.26495726495726, f1: 27.690723498531955, r: 0.3445242299925398
06/02/2019 01:25:38 *** epoch: 58 ***
06/02/2019 01:25:38 *** training ***
06/02/2019 01:25:38 step: 1886, epoch: 57, batch: 4, loss: 0.006165765225887299, acc: 100.0, f1: 100.0, r: 0.8312105655945826
06/02/2019 01:25:39 step: 1891, epoch: 57, batch: 9, loss: 0.004434935748577118, acc: 100.0, f1: 100.0, r: 0.7293599122924355
06/02/2019 01:25:40 step: 1896, epoch: 57, batch: 14, loss: 0.005423534661531448, acc: 100.0, f1: 100.0, r: 0.7052773879610318
06/02/2019 01:25:41 step: 1901, epoch: 57, batch: 19, loss: 0.0060213059186935425, acc: 100.0, f1: 100.0, r: 0.6960399108011033
06/02/2019 01:25:42 step: 1906, epoch: 57, batch: 24, loss: 0.0033336952328681946, acc: 100.0, f1: 100.0, r: 0.6933321171672857
06/02/2019 01:25:43 step: 1911, epoch: 57, batch: 29, loss: 0.003650825470685959, acc: 100.0, f1: 100.0, r: 0.7208799436511374
06/02/2019 01:25:43 *** evaluating ***
06/02/2019 01:25:43 step: 58, epoch: 57, acc: 55.98290598290598, f1: 28.105419799498744, r: 0.34315937892475457
06/02/2019 01:25:43 *** epoch: 59 ***
06/02/2019 01:25:43 *** training ***
06/02/2019 01:25:44 step: 1919, epoch: 58, batch: 4, loss: 0.019860394299030304, acc: 98.4375, f1: 99.15343915343915, r: 0.7361735081071046
06/02/2019 01:25:45 step: 1924, epoch: 58, batch: 9, loss: 0.007280014455318451, acc: 100.0, f1: 100.0, r: 0.6548829511320723
06/02/2019 01:25:46 step: 1929, epoch: 58, batch: 14, loss: 0.013446856290102005, acc: 100.0, f1: 100.0, r: 0.5575868988960174
06/02/2019 01:25:47 step: 1934, epoch: 58, batch: 19, loss: 0.02011575549840927, acc: 98.4375, f1: 94.74548440065682, r: 0.6652019230743124
06/02/2019 01:25:48 step: 1939, epoch: 58, batch: 24, loss: 0.00582972913980484, acc: 100.0, f1: 100.0, r: 0.7387467891768515
06/02/2019 01:25:49 step: 1944, epoch: 58, batch: 29, loss: 0.010503627359867096, acc: 100.0, f1: 100.0, r: 0.6802037278612221
06/02/2019 01:25:49 *** evaluating ***
06/02/2019 01:25:49 step: 59, epoch: 58, acc: 57.26495726495726, f1: 28.459074470727007, r: 0.3397680465418446
06/02/2019 01:25:49 *** epoch: 60 ***
06/02/2019 01:25:49 *** training ***
06/02/2019 01:25:50 step: 1952, epoch: 59, batch: 4, loss: 0.03665659949183464, acc: 98.4375, f1: 98.42118665648077, r: 0.6794402679404139
06/02/2019 01:25:51 step: 1957, epoch: 59, batch: 9, loss: 0.0043259188532829285, acc: 100.0, f1: 100.0, r: 0.7927051049494331
06/02/2019 01:25:52 step: 1962, epoch: 59, batch: 14, loss: 0.005718611180782318, acc: 100.0, f1: 100.0, r: 0.6847495194123141
06/02/2019 01:25:53 step: 1967, epoch: 59, batch: 19, loss: 0.00946759432554245, acc: 100.0, f1: 100.0, r: 0.8175439013756158
06/02/2019 01:25:54 step: 1972, epoch: 59, batch: 24, loss: 0.004082970321178436, acc: 100.0, f1: 100.0, r: 0.8270264659000904
06/02/2019 01:25:55 step: 1977, epoch: 59, batch: 29, loss: 0.003674842417240143, acc: 100.0, f1: 100.0, r: 0.6562013085743686
06/02/2019 01:25:55 *** evaluating ***
06/02/2019 01:25:55 step: 60, epoch: 59, acc: 56.41025641025641, f1: 27.787563928357294, r: 0.3470493372988051
06/02/2019 01:25:55 *** epoch: 61 ***
06/02/2019 01:25:55 *** training ***
06/02/2019 01:25:56 step: 1985, epoch: 60, batch: 4, loss: 0.010519593954086304, acc: 100.0, f1: 100.0, r: 0.6785708375626011
06/02/2019 01:25:57 step: 1990, epoch: 60, batch: 9, loss: 0.007083818316459656, acc: 100.0, f1: 100.0, r: 0.8367429332359878
06/02/2019 01:25:58 step: 1995, epoch: 60, batch: 14, loss: 0.014515921473503113, acc: 100.0, f1: 100.0, r: 0.7810772507337747
06/02/2019 01:25:59 step: 2000, epoch: 60, batch: 19, loss: 0.013603754341602325, acc: 100.0, f1: 100.0, r: 0.8019699006839353
06/02/2019 01:25:59 step: 2005, epoch: 60, batch: 24, loss: 0.009976066648960114, acc: 100.0, f1: 100.0, r: 0.6702693866412445
06/02/2019 01:26:00 step: 2010, epoch: 60, batch: 29, loss: 0.04505818337202072, acc: 98.4375, f1: 97.38775510204081, r: 0.6860270802021736
06/02/2019 01:26:01 *** evaluating ***
06/02/2019 01:26:01 step: 61, epoch: 60, acc: 57.26495726495726, f1: 28.803726858988625, r: 0.34470605242915114
06/02/2019 01:26:01 *** epoch: 62 ***
06/02/2019 01:26:01 *** training ***
06/02/2019 01:26:02 step: 2018, epoch: 61, batch: 4, loss: 0.007207371294498444, acc: 100.0, f1: 100.0, r: 0.6831839455311977
06/02/2019 01:26:03 step: 2023, epoch: 61, batch: 9, loss: 0.020825082436203957, acc: 100.0, f1: 100.0, r: 0.7937972569139001
06/02/2019 01:26:04 step: 2028, epoch: 61, batch: 14, loss: 0.0071725547313690186, acc: 100.0, f1: 100.0, r: 0.6972136135376961
06/02/2019 01:26:05 step: 2033, epoch: 61, batch: 19, loss: 0.004980258643627167, acc: 100.0, f1: 100.0, r: 0.7502148137434344
06/02/2019 01:26:05 step: 2038, epoch: 61, batch: 24, loss: 0.019909225404262543, acc: 100.0, f1: 100.0, r: 0.7098510277254884
06/02/2019 01:26:06 step: 2043, epoch: 61, batch: 29, loss: 0.0037189722061157227, acc: 100.0, f1: 100.0, r: 0.843749859607485
06/02/2019 01:26:07 *** evaluating ***
06/02/2019 01:26:07 step: 62, epoch: 61, acc: 55.55555555555556, f1: 26.839934555382435, r: 0.34452820958920805
06/02/2019 01:26:07 *** epoch: 63 ***
06/02/2019 01:26:07 *** training ***
06/02/2019 01:26:08 step: 2051, epoch: 62, batch: 4, loss: 0.0035254955291748047, acc: 100.0, f1: 100.0, r: 0.8093466522118962
06/02/2019 01:26:09 step: 2056, epoch: 62, batch: 9, loss: 0.03454144671559334, acc: 98.4375, f1: 99.26314819931841, r: 0.7118738593008567
06/02/2019 01:26:09 step: 2061, epoch: 62, batch: 14, loss: 0.017492108047008514, acc: 100.0, f1: 100.0, r: 0.7007434969543135
06/02/2019 01:26:10 step: 2066, epoch: 62, batch: 19, loss: 0.004061564803123474, acc: 100.0, f1: 100.0, r: 0.7060633015136576
06/02/2019 01:26:11 step: 2071, epoch: 62, batch: 24, loss: 0.0037666037678718567, acc: 100.0, f1: 100.0, r: 0.6095753961236304
06/02/2019 01:26:12 step: 2076, epoch: 62, batch: 29, loss: 0.0041018277406692505, acc: 100.0, f1: 100.0, r: 0.7787584115864863
06/02/2019 01:26:12 *** evaluating ***
06/02/2019 01:26:13 step: 63, epoch: 62, acc: 55.55555555555556, f1: 27.67021695266964, r: 0.3427646624247532
06/02/2019 01:26:13 *** epoch: 64 ***
06/02/2019 01:26:13 *** training ***
06/02/2019 01:26:13 step: 2084, epoch: 63, batch: 4, loss: 0.0045858025550842285, acc: 100.0, f1: 100.0, r: 0.6350420883569048
06/02/2019 01:26:14 step: 2089, epoch: 63, batch: 9, loss: 0.005559302866458893, acc: 100.0, f1: 100.0, r: 0.6876473926408518
06/02/2019 01:26:15 step: 2094, epoch: 63, batch: 14, loss: 0.00892261415719986, acc: 100.0, f1: 100.0, r: 0.7400181467293795
06/02/2019 01:26:16 step: 2099, epoch: 63, batch: 19, loss: 0.02581123262643814, acc: 98.4375, f1: 96.84210526315789, r: 0.7182411579009218
06/02/2019 01:26:17 step: 2104, epoch: 63, batch: 24, loss: 0.011429674923419952, acc: 100.0, f1: 100.0, r: 0.7193607096471497
06/02/2019 01:26:18 step: 2109, epoch: 63, batch: 29, loss: 0.015566214919090271, acc: 100.0, f1: 100.0, r: 0.8125710855861901
06/02/2019 01:26:18 *** evaluating ***
06/02/2019 01:26:19 step: 64, epoch: 63, acc: 55.55555555555556, f1: 28.11020269029892, r: 0.3384121745339427
06/02/2019 01:26:19 *** epoch: 65 ***
06/02/2019 01:26:19 *** training ***
06/02/2019 01:26:19 step: 2117, epoch: 64, batch: 4, loss: 0.008137539029121399, acc: 100.0, f1: 100.0, r: 0.6945616763628346
06/02/2019 01:26:20 step: 2122, epoch: 64, batch: 9, loss: 0.012565091252326965, acc: 100.0, f1: 100.0, r: 0.7580787469799998
06/02/2019 01:26:21 step: 2127, epoch: 64, batch: 14, loss: 0.0013945847749710083, acc: 100.0, f1: 100.0, r: 0.7939226922341464
06/02/2019 01:26:22 step: 2132, epoch: 64, batch: 19, loss: 0.004979737102985382, acc: 100.0, f1: 100.0, r: 0.6037795018753763
06/02/2019 01:26:23 step: 2137, epoch: 64, batch: 24, loss: 0.01349622756242752, acc: 100.0, f1: 100.0, r: 0.8244562547394475
06/02/2019 01:26:24 step: 2142, epoch: 64, batch: 29, loss: 0.01687387004494667, acc: 100.0, f1: 100.0, r: 0.6859112139140078
06/02/2019 01:26:24 *** evaluating ***
06/02/2019 01:26:24 step: 65, epoch: 64, acc: 55.12820512820513, f1: 27.65067165448657, r: 0.34463959523512544
06/02/2019 01:26:24 *** epoch: 66 ***
06/02/2019 01:26:24 *** training ***
06/02/2019 01:26:25 step: 2150, epoch: 65, batch: 4, loss: 0.0040827468037605286, acc: 100.0, f1: 100.0, r: 0.7776076109412493
06/02/2019 01:26:26 step: 2155, epoch: 65, batch: 9, loss: 0.008821509778499603, acc: 100.0, f1: 100.0, r: 0.7011949051370807
06/02/2019 01:26:27 step: 2160, epoch: 65, batch: 14, loss: 0.004482381045818329, acc: 100.0, f1: 100.0, r: 0.772043149856206
06/02/2019 01:26:28 step: 2165, epoch: 65, batch: 19, loss: 0.005857989192008972, acc: 100.0, f1: 100.0, r: 0.6918723626899678
06/02/2019 01:26:29 step: 2170, epoch: 65, batch: 24, loss: 0.00560692697763443, acc: 100.0, f1: 100.0, r: 0.8617585612466943
06/02/2019 01:26:29 step: 2175, epoch: 65, batch: 29, loss: 0.015777938067913055, acc: 100.0, f1: 100.0, r: 0.8625622324507463
06/02/2019 01:26:30 *** evaluating ***
06/02/2019 01:26:30 step: 66, epoch: 65, acc: 56.41025641025641, f1: 27.406233363082656, r: 0.3426150853631808
06/02/2019 01:26:30 *** epoch: 67 ***
06/02/2019 01:26:30 *** training ***
06/02/2019 01:26:31 step: 2183, epoch: 66, batch: 4, loss: 0.0032767876982688904, acc: 100.0, f1: 100.0, r: 0.778421118755601
06/02/2019 01:26:32 step: 2188, epoch: 66, batch: 9, loss: 0.0029339194297790527, acc: 100.0, f1: 100.0, r: 0.7122477827004344
06/02/2019 01:26:33 step: 2193, epoch: 66, batch: 14, loss: 0.009351998567581177, acc: 100.0, f1: 100.0, r: 0.809355441810706
06/02/2019 01:26:34 step: 2198, epoch: 66, batch: 19, loss: 0.005523309111595154, acc: 100.0, f1: 100.0, r: 0.7588086684936335
06/02/2019 01:26:35 step: 2203, epoch: 66, batch: 24, loss: 0.0016755163669586182, acc: 100.0, f1: 100.0, r: 0.7124924993450346
06/02/2019 01:26:35 step: 2208, epoch: 66, batch: 29, loss: 0.006564773619174957, acc: 100.0, f1: 100.0, r: 0.7358855354545624
06/02/2019 01:26:36 *** evaluating ***
06/02/2019 01:26:36 step: 67, epoch: 66, acc: 56.41025641025641, f1: 28.337530756983643, r: 0.3382577170864753
06/02/2019 01:26:36 *** epoch: 68 ***
06/02/2019 01:26:36 *** training ***
06/02/2019 01:26:37 step: 2216, epoch: 67, batch: 4, loss: 0.006788410246372223, acc: 100.0, f1: 100.0, r: 0.8122626145485552
06/02/2019 01:26:38 step: 2221, epoch: 67, batch: 9, loss: 0.0014699026942253113, acc: 100.0, f1: 100.0, r: 0.7010068715876299
06/02/2019 01:26:39 step: 2226, epoch: 67, batch: 14, loss: 0.02164643257856369, acc: 98.4375, f1: 98.04639804639805, r: 0.7013068021071795
06/02/2019 01:26:40 step: 2231, epoch: 67, batch: 19, loss: 0.0022687092423439026, acc: 100.0, f1: 100.0, r: 0.7893062325213143
06/02/2019 01:26:41 step: 2236, epoch: 67, batch: 24, loss: 0.018849346786737442, acc: 100.0, f1: 100.0, r: 0.8163498710941648
06/02/2019 01:26:42 step: 2241, epoch: 67, batch: 29, loss: 0.0038633421063423157, acc: 100.0, f1: 100.0, r: 0.6623134697513596
06/02/2019 01:26:42 *** evaluating ***
06/02/2019 01:26:42 step: 68, epoch: 67, acc: 54.27350427350427, f1: 29.441003584372215, r: 0.3512152053905371
06/02/2019 01:26:42 *** epoch: 69 ***
06/02/2019 01:26:42 *** training ***
06/02/2019 01:26:43 step: 2249, epoch: 68, batch: 4, loss: 0.011786524206399918, acc: 100.0, f1: 100.0, r: 0.7700928118433359
06/02/2019 01:26:44 step: 2254, epoch: 68, batch: 9, loss: 0.003645956516265869, acc: 100.0, f1: 100.0, r: 0.711243019833466
06/02/2019 01:26:45 step: 2259, epoch: 68, batch: 14, loss: 0.008481848984956741, acc: 100.0, f1: 100.0, r: 0.7085037089187268
06/02/2019 01:26:46 step: 2264, epoch: 68, batch: 19, loss: 0.005049005150794983, acc: 100.0, f1: 100.0, r: 0.6824444131651459
06/02/2019 01:26:46 step: 2269, epoch: 68, batch: 24, loss: 0.004923418164253235, acc: 100.0, f1: 100.0, r: 0.7605630285612333
06/02/2019 01:26:47 step: 2274, epoch: 68, batch: 29, loss: 0.005494058132171631, acc: 100.0, f1: 100.0, r: 0.8347743216291965
06/02/2019 01:26:48 *** evaluating ***
06/02/2019 01:26:48 step: 69, epoch: 68, acc: 58.119658119658126, f1: 29.434364793556643, r: 0.3418019921085087
06/02/2019 01:26:48 *** epoch: 70 ***
06/02/2019 01:26:48 *** training ***
06/02/2019 01:26:49 step: 2282, epoch: 69, batch: 4, loss: 0.004449203610420227, acc: 100.0, f1: 100.0, r: 0.6402526531800066
06/02/2019 01:26:50 step: 2287, epoch: 69, batch: 9, loss: 0.002098873257637024, acc: 100.0, f1: 100.0, r: 0.7421099052045625
06/02/2019 01:26:51 step: 2292, epoch: 69, batch: 14, loss: 0.005275942385196686, acc: 100.0, f1: 100.0, r: 0.7431487684376451
06/02/2019 01:26:51 step: 2297, epoch: 69, batch: 19, loss: 0.005415409803390503, acc: 100.0, f1: 100.0, r: 0.7529151203713726
06/02/2019 01:26:52 step: 2302, epoch: 69, batch: 24, loss: 0.005376800894737244, acc: 100.0, f1: 100.0, r: 0.6211211402988118
06/02/2019 01:26:53 step: 2307, epoch: 69, batch: 29, loss: 0.015724115073680878, acc: 100.0, f1: 100.0, r: 0.8191482528488045
06/02/2019 01:26:54 *** evaluating ***
06/02/2019 01:26:54 step: 70, epoch: 69, acc: 57.692307692307686, f1: 28.415731587629057, r: 0.34469034561988804
06/02/2019 01:26:54 *** epoch: 71 ***
06/02/2019 01:26:54 *** training ***
06/02/2019 01:26:55 step: 2315, epoch: 70, batch: 4, loss: 0.015771888196468353, acc: 100.0, f1: 100.0, r: 0.7092842300826375
06/02/2019 01:26:56 step: 2320, epoch: 70, batch: 9, loss: 0.004208177328109741, acc: 100.0, f1: 100.0, r: 0.7447792375893817
06/02/2019 01:26:56 step: 2325, epoch: 70, batch: 14, loss: 0.004248186945915222, acc: 100.0, f1: 100.0, r: 0.7005399657135616
06/02/2019 01:26:57 step: 2330, epoch: 70, batch: 19, loss: 0.013072296977043152, acc: 100.0, f1: 100.0, r: 0.7440582774979217
06/02/2019 01:26:58 step: 2335, epoch: 70, batch: 24, loss: 0.004963971674442291, acc: 100.0, f1: 100.0, r: 0.7560802956792217
06/02/2019 01:26:59 step: 2340, epoch: 70, batch: 29, loss: 0.007862411439418793, acc: 100.0, f1: 100.0, r: 0.839715544551741
06/02/2019 01:27:00 *** evaluating ***
06/02/2019 01:27:00 step: 71, epoch: 70, acc: 59.82905982905983, f1: 30.67783327134584, r: 0.3501823514341943
06/02/2019 01:27:00 *** epoch: 72 ***
06/02/2019 01:27:00 *** training ***
06/02/2019 01:27:01 step: 2348, epoch: 71, batch: 4, loss: 0.006807014346122742, acc: 100.0, f1: 100.0, r: 0.7546649968188791
06/02/2019 01:27:02 step: 2353, epoch: 71, batch: 9, loss: 0.01827313005924225, acc: 100.0, f1: 100.0, r: 0.7805238363806104
06/02/2019 01:27:02 step: 2358, epoch: 71, batch: 14, loss: 0.0041074976325035095, acc: 100.0, f1: 100.0, r: 0.6845740352150537
06/02/2019 01:27:03 step: 2363, epoch: 71, batch: 19, loss: 0.0048201680183410645, acc: 100.0, f1: 100.0, r: 0.6817678874401253
06/02/2019 01:27:04 step: 2368, epoch: 71, batch: 24, loss: 0.002351619303226471, acc: 100.0, f1: 100.0, r: 0.7907089800685305
06/02/2019 01:27:05 step: 2373, epoch: 71, batch: 29, loss: 0.0030981451272964478, acc: 100.0, f1: 100.0, r: 0.7977877848185262
06/02/2019 01:27:05 *** evaluating ***
06/02/2019 01:27:06 step: 72, epoch: 71, acc: 57.692307692307686, f1: 28.294688080032714, r: 0.34931228231763983
06/02/2019 01:27:06 *** epoch: 73 ***
06/02/2019 01:27:06 *** training ***
06/02/2019 01:27:07 step: 2381, epoch: 72, batch: 4, loss: 0.0030497387051582336, acc: 100.0, f1: 100.0, r: 0.7158445685604745
06/02/2019 01:27:08 step: 2386, epoch: 72, batch: 9, loss: 0.002389654517173767, acc: 100.0, f1: 100.0, r: 0.7840574796669547
06/02/2019 01:27:08 step: 2391, epoch: 72, batch: 14, loss: 0.008241917937994003, acc: 100.0, f1: 100.0, r: 0.7362216200015124
06/02/2019 01:27:09 step: 2396, epoch: 72, batch: 19, loss: 0.028138108551502228, acc: 98.4375, f1: 99.05978784956606, r: 0.7880754709067254
06/02/2019 01:27:10 step: 2401, epoch: 72, batch: 24, loss: 0.009204752743244171, acc: 100.0, f1: 100.0, r: 0.6975354431835192
06/02/2019 01:27:11 step: 2406, epoch: 72, batch: 29, loss: 0.005506269633769989, acc: 100.0, f1: 100.0, r: 0.7748097801407764
06/02/2019 01:27:12 *** evaluating ***
06/02/2019 01:27:12 step: 73, epoch: 72, acc: 54.27350427350427, f1: 27.677243974512884, r: 0.3360043687924739
06/02/2019 01:27:12 *** epoch: 74 ***
06/02/2019 01:27:12 *** training ***
06/02/2019 01:27:13 step: 2414, epoch: 73, batch: 4, loss: 0.0048042237758636475, acc: 100.0, f1: 100.0, r: 0.8118852333456715
06/02/2019 01:27:14 step: 2419, epoch: 73, batch: 9, loss: 0.009133957326412201, acc: 100.0, f1: 100.0, r: 0.6704876179439686
06/02/2019 01:27:14 step: 2424, epoch: 73, batch: 14, loss: 0.03527345508337021, acc: 98.4375, f1: 99.28193499622071, r: 0.7724072405014598
06/02/2019 01:27:15 step: 2429, epoch: 73, batch: 19, loss: 0.006491485983133316, acc: 100.0, f1: 100.0, r: 0.753032720865055
06/02/2019 01:27:16 step: 2434, epoch: 73, batch: 24, loss: 0.008827827870845795, acc: 100.0, f1: 100.0, r: 0.6994067959858932
06/02/2019 01:27:17 step: 2439, epoch: 73, batch: 29, loss: 0.005022294819355011, acc: 100.0, f1: 100.0, r: 0.8161366228570861
06/02/2019 01:27:17 *** evaluating ***
06/02/2019 01:27:18 step: 74, epoch: 73, acc: 58.54700854700855, f1: 27.942087997467702, r: 0.3487684999708029
06/02/2019 01:27:18 *** epoch: 75 ***
06/02/2019 01:27:18 *** training ***
06/02/2019 01:27:19 step: 2447, epoch: 74, batch: 4, loss: 0.011441163718700409, acc: 100.0, f1: 100.0, r: 0.6908567222930937
06/02/2019 01:27:19 step: 2452, epoch: 74, batch: 9, loss: 0.006223149597644806, acc: 100.0, f1: 100.0, r: 0.6356496562796131
06/02/2019 01:27:20 step: 2457, epoch: 74, batch: 14, loss: 0.02375757321715355, acc: 98.4375, f1: 97.55639097744361, r: 0.8137771279996256
06/02/2019 01:27:21 step: 2462, epoch: 74, batch: 19, loss: 0.0014652833342552185, acc: 100.0, f1: 100.0, r: 0.6946350570322574
06/02/2019 01:27:22 step: 2467, epoch: 74, batch: 24, loss: 0.00322568416595459, acc: 100.0, f1: 100.0, r: 0.7669064672812657
06/02/2019 01:27:23 step: 2472, epoch: 74, batch: 29, loss: 0.004644721746444702, acc: 100.0, f1: 100.0, r: 0.717292045681114
06/02/2019 01:27:23 *** evaluating ***
06/02/2019 01:27:24 step: 75, epoch: 74, acc: 57.692307692307686, f1: 29.326959782224517, r: 0.34742677446754544
06/02/2019 01:27:24 *** epoch: 76 ***
06/02/2019 01:27:24 *** training ***
06/02/2019 01:27:24 step: 2480, epoch: 75, batch: 4, loss: 0.004388727247714996, acc: 100.0, f1: 100.0, r: 0.688047942008593
06/02/2019 01:27:25 step: 2485, epoch: 75, batch: 9, loss: 0.003434382379055023, acc: 100.0, f1: 100.0, r: 0.7601056571243638
06/02/2019 01:27:26 step: 2490, epoch: 75, batch: 14, loss: 0.006051219999790192, acc: 100.0, f1: 100.0, r: 0.7541337289190694
06/02/2019 01:27:27 step: 2495, epoch: 75, batch: 19, loss: 0.0013058185577392578, acc: 100.0, f1: 100.0, r: 0.7051326519479987
06/02/2019 01:27:28 step: 2500, epoch: 75, batch: 24, loss: 0.005730129778385162, acc: 100.0, f1: 100.0, r: 0.6595436859815185
06/02/2019 01:27:29 step: 2505, epoch: 75, batch: 29, loss: 0.003036022186279297, acc: 100.0, f1: 100.0, r: 0.7126309420554207
06/02/2019 01:27:29 *** evaluating ***
06/02/2019 01:27:29 step: 76, epoch: 75, acc: 58.119658119658126, f1: 28.62287867274569, r: 0.3505818626540735
06/02/2019 01:27:29 *** epoch: 77 ***
06/02/2019 01:27:29 *** training ***
06/02/2019 01:27:30 step: 2513, epoch: 76, batch: 4, loss: 0.009126193821430206, acc: 100.0, f1: 100.0, r: 0.7036081156919634
06/02/2019 01:27:31 step: 2518, epoch: 76, batch: 9, loss: 0.004237651824951172, acc: 100.0, f1: 100.0, r: 0.7458843781223261
06/02/2019 01:27:32 step: 2523, epoch: 76, batch: 14, loss: 0.002047508955001831, acc: 100.0, f1: 100.0, r: 0.7847893546284157
06/02/2019 01:27:33 step: 2528, epoch: 76, batch: 19, loss: 0.013481814414262772, acc: 100.0, f1: 100.0, r: 0.7961207254669482
06/02/2019 01:27:34 step: 2533, epoch: 76, batch: 24, loss: 0.0033589228987693787, acc: 100.0, f1: 100.0, r: 0.7178527006682133
06/02/2019 01:27:35 step: 2538, epoch: 76, batch: 29, loss: 0.0028739571571350098, acc: 100.0, f1: 100.0, r: 0.6398463012326495
06/02/2019 01:27:35 *** evaluating ***
06/02/2019 01:27:35 step: 77, epoch: 76, acc: 57.692307692307686, f1: 28.976298332284895, r: 0.3440002673206321
06/02/2019 01:27:35 *** epoch: 78 ***
06/02/2019 01:27:35 *** training ***
06/02/2019 01:27:36 step: 2546, epoch: 77, batch: 4, loss: 0.003952845931053162, acc: 100.0, f1: 100.0, r: 0.6658835098090754
06/02/2019 01:27:37 step: 2551, epoch: 77, batch: 9, loss: 0.006320454180240631, acc: 100.0, f1: 100.0, r: 0.7038923471303222
06/02/2019 01:27:38 step: 2556, epoch: 77, batch: 14, loss: 0.0047430843114852905, acc: 100.0, f1: 100.0, r: 0.6884537262082348
06/02/2019 01:27:39 step: 2561, epoch: 77, batch: 19, loss: 0.016703784465789795, acc: 98.4375, f1: 99.31633407243163, r: 0.7403558386993673
06/02/2019 01:27:40 step: 2566, epoch: 77, batch: 24, loss: 0.007673211395740509, acc: 100.0, f1: 100.0, r: 0.811990840009648
06/02/2019 01:27:41 step: 2571, epoch: 77, batch: 29, loss: 0.0028755664825439453, acc: 100.0, f1: 100.0, r: 0.6911999010746133
06/02/2019 01:27:41 *** evaluating ***
06/02/2019 01:27:41 step: 78, epoch: 77, acc: 58.54700854700855, f1: 29.33332383192363, r: 0.3464953615481455
06/02/2019 01:27:41 *** epoch: 79 ***
06/02/2019 01:27:41 *** training ***
06/02/2019 01:27:42 step: 2579, epoch: 78, batch: 4, loss: 0.0020517781376838684, acc: 100.0, f1: 100.0, r: 0.696859298502389
06/02/2019 01:27:43 step: 2584, epoch: 78, batch: 9, loss: 0.003640204668045044, acc: 100.0, f1: 100.0, r: 0.6890034238355204
06/02/2019 01:27:44 step: 2589, epoch: 78, batch: 14, loss: 0.008465565741062164, acc: 100.0, f1: 100.0, r: 0.6730406672224029
06/02/2019 01:27:45 step: 2594, epoch: 78, batch: 19, loss: 0.00323689728975296, acc: 100.0, f1: 100.0, r: 0.7377878150935936
06/02/2019 01:27:45 step: 2599, epoch: 78, batch: 24, loss: 0.0023422986268997192, acc: 100.0, f1: 100.0, r: 0.7536542128672252
06/02/2019 01:27:46 step: 2604, epoch: 78, batch: 29, loss: 0.0015252530574798584, acc: 100.0, f1: 100.0, r: 0.6856809609848955
06/02/2019 01:27:47 *** evaluating ***
06/02/2019 01:27:47 step: 79, epoch: 78, acc: 58.119658119658126, f1: 29.332213425152194, r: 0.3506996603145417
06/02/2019 01:27:47 *** epoch: 80 ***
06/02/2019 01:27:47 *** training ***
06/02/2019 01:27:48 step: 2612, epoch: 79, batch: 4, loss: 0.03018631786108017, acc: 98.4375, f1: 99.30300807043287, r: 0.7440785226231537
06/02/2019 01:27:49 step: 2617, epoch: 79, batch: 9, loss: 0.0035610273480415344, acc: 100.0, f1: 100.0, r: 0.7786725572057988
06/02/2019 01:27:50 step: 2622, epoch: 79, batch: 14, loss: 0.005308575928211212, acc: 100.0, f1: 100.0, r: 0.7188858891279609
06/02/2019 01:27:51 step: 2627, epoch: 79, batch: 19, loss: 0.005420371890068054, acc: 100.0, f1: 100.0, r: 0.7149317221210163
06/02/2019 01:27:51 step: 2632, epoch: 79, batch: 24, loss: 0.002138875424861908, acc: 100.0, f1: 100.0, r: 0.6950691863510552
06/02/2019 01:27:52 step: 2637, epoch: 79, batch: 29, loss: 0.003411203622817993, acc: 100.0, f1: 100.0, r: 0.8027972140354237
06/02/2019 01:27:53 *** evaluating ***
06/02/2019 01:27:53 step: 80, epoch: 79, acc: 55.55555555555556, f1: 24.84330759196109, r: 0.348054174231152
06/02/2019 01:27:53 *** epoch: 81 ***
06/02/2019 01:27:53 *** training ***
06/02/2019 01:27:54 step: 2645, epoch: 80, batch: 4, loss: 0.0016282573342323303, acc: 100.0, f1: 100.0, r: 0.7266571584562405
06/02/2019 01:27:55 step: 2650, epoch: 80, batch: 9, loss: 0.0077371746301651, acc: 100.0, f1: 100.0, r: 0.6955591468691782
06/02/2019 01:27:56 step: 2655, epoch: 80, batch: 14, loss: 0.007266782224178314, acc: 100.0, f1: 100.0, r: 0.7101623637792478
06/02/2019 01:27:57 step: 2660, epoch: 80, batch: 19, loss: 0.002515062689781189, acc: 100.0, f1: 100.0, r: 0.7345733057368437
06/02/2019 01:27:58 step: 2665, epoch: 80, batch: 24, loss: 0.0034377947449684143, acc: 100.0, f1: 100.0, r: 0.6805907478419307
06/02/2019 01:27:58 step: 2670, epoch: 80, batch: 29, loss: 0.016215696930885315, acc: 100.0, f1: 100.0, r: 0.6169584644096114
06/02/2019 01:27:59 *** evaluating ***
06/02/2019 01:27:59 step: 81, epoch: 80, acc: 57.692307692307686, f1: 29.368453064505996, r: 0.34495228757529656
06/02/2019 01:27:59 *** epoch: 82 ***
06/02/2019 01:27:59 *** training ***
06/02/2019 01:28:00 step: 2678, epoch: 81, batch: 4, loss: 0.005059339106082916, acc: 100.0, f1: 100.0, r: 0.6715475875306048
06/02/2019 01:28:01 step: 2683, epoch: 81, batch: 9, loss: 0.006090864539146423, acc: 100.0, f1: 100.0, r: 0.7719595547377586
06/02/2019 01:28:02 step: 2688, epoch: 81, batch: 14, loss: 0.007854238152503967, acc: 100.0, f1: 100.0, r: 0.7016255938852121
06/02/2019 01:28:02 step: 2693, epoch: 81, batch: 19, loss: 0.0021006017923355103, acc: 100.0, f1: 100.0, r: 0.8090884729204901
06/02/2019 01:28:03 step: 2698, epoch: 81, batch: 24, loss: 0.00570988655090332, acc: 100.0, f1: 100.0, r: 0.805372448333224
06/02/2019 01:28:04 step: 2703, epoch: 81, batch: 29, loss: 0.001669563353061676, acc: 100.0, f1: 100.0, r: 0.773778202323536
06/02/2019 01:28:05 *** evaluating ***
06/02/2019 01:28:05 step: 82, epoch: 81, acc: 58.97435897435898, f1: 28.52431428770194, r: 0.3547944800002866
06/02/2019 01:28:05 *** epoch: 83 ***
06/02/2019 01:28:05 *** training ***
06/02/2019 01:28:06 step: 2711, epoch: 82, batch: 4, loss: 0.006931975483894348, acc: 100.0, f1: 100.0, r: 0.6696950173704692
06/02/2019 01:28:07 step: 2716, epoch: 82, batch: 9, loss: 0.013145286589860916, acc: 100.0, f1: 100.0, r: 0.6788846455128141
06/02/2019 01:28:08 step: 2721, epoch: 82, batch: 14, loss: 0.003958664834499359, acc: 100.0, f1: 100.0, r: 0.7345946723835686
06/02/2019 01:28:08 step: 2726, epoch: 82, batch: 19, loss: 0.006897620856761932, acc: 100.0, f1: 100.0, r: 0.828554105100688
06/02/2019 01:28:09 step: 2731, epoch: 82, batch: 24, loss: 0.0028458014130592346, acc: 100.0, f1: 100.0, r: 0.728711629452556
06/02/2019 01:28:10 step: 2736, epoch: 82, batch: 29, loss: 0.0035165995359420776, acc: 100.0, f1: 100.0, r: 0.7649222990909412
06/02/2019 01:28:11 *** evaluating ***
06/02/2019 01:28:11 step: 83, epoch: 82, acc: 58.54700854700855, f1: 28.91398338044975, r: 0.34890813651640373
06/02/2019 01:28:11 *** epoch: 84 ***
06/02/2019 01:28:11 *** training ***
06/02/2019 01:28:12 step: 2744, epoch: 83, batch: 4, loss: 0.012733258306980133, acc: 98.4375, f1: 98.97857852177614, r: 0.6834528750873458
06/02/2019 01:28:13 step: 2749, epoch: 83, batch: 9, loss: 0.0016424506902694702, acc: 100.0, f1: 100.0, r: 0.7176117019516135
06/02/2019 01:28:13 step: 2754, epoch: 83, batch: 14, loss: 0.002900540828704834, acc: 100.0, f1: 100.0, r: 0.6776403314249849
06/02/2019 01:28:14 step: 2759, epoch: 83, batch: 19, loss: 0.00979544222354889, acc: 100.0, f1: 100.0, r: 0.7905380711894342
06/02/2019 01:28:15 step: 2764, epoch: 83, batch: 24, loss: 0.029911808669567108, acc: 98.4375, f1: 99.15966386554622, r: 0.8228987615408195
06/02/2019 01:28:16 step: 2769, epoch: 83, batch: 29, loss: 0.002764865756034851, acc: 100.0, f1: 100.0, r: 0.6883259332034318
06/02/2019 01:28:16 *** evaluating ***
06/02/2019 01:28:17 step: 84, epoch: 83, acc: 58.97435897435898, f1: 29.333347922502334, r: 0.34634351690540427
06/02/2019 01:28:17 *** epoch: 85 ***
06/02/2019 01:28:17 *** training ***
06/02/2019 01:28:18 step: 2777, epoch: 84, batch: 4, loss: 0.009766824543476105, acc: 100.0, f1: 100.0, r: 0.8024331025421574
06/02/2019 01:28:18 step: 2782, epoch: 84, batch: 9, loss: 0.0029941946268081665, acc: 100.0, f1: 100.0, r: 0.7022015941545067
06/02/2019 01:28:19 step: 2787, epoch: 84, batch: 14, loss: 0.0027947425842285156, acc: 100.0, f1: 100.0, r: 0.8300975539401071
06/02/2019 01:28:20 step: 2792, epoch: 84, batch: 19, loss: 0.00284779816865921, acc: 100.0, f1: 100.0, r: 0.8659699116561159
06/02/2019 01:28:21 step: 2797, epoch: 84, batch: 24, loss: 0.0037629231810569763, acc: 100.0, f1: 100.0, r: 0.8344985683032586
06/02/2019 01:28:22 step: 2802, epoch: 84, batch: 29, loss: 0.0033594295382499695, acc: 100.0, f1: 100.0, r: 0.6752571967679092
06/02/2019 01:28:22 *** evaluating ***
06/02/2019 01:28:23 step: 85, epoch: 84, acc: 56.837606837606835, f1: 28.784606416185365, r: 0.34150612475843534
06/02/2019 01:28:23 *** epoch: 86 ***
06/02/2019 01:28:23 *** training ***
06/02/2019 01:28:24 step: 2810, epoch: 85, batch: 4, loss: 0.002019748091697693, acc: 100.0, f1: 100.0, r: 0.8535614806303123
06/02/2019 01:28:24 step: 2815, epoch: 85, batch: 9, loss: 0.003504350781440735, acc: 100.0, f1: 100.0, r: 0.6715629779890845
06/02/2019 01:28:25 step: 2820, epoch: 85, batch: 14, loss: 0.003851167857646942, acc: 100.0, f1: 100.0, r: 0.6733533175214391
06/02/2019 01:28:26 step: 2825, epoch: 85, batch: 19, loss: 0.004050783812999725, acc: 100.0, f1: 100.0, r: 0.8016983794319485
06/02/2019 01:28:27 step: 2830, epoch: 85, batch: 24, loss: 0.004034794867038727, acc: 100.0, f1: 100.0, r: 0.7954911164420801
06/02/2019 01:28:28 step: 2835, epoch: 85, batch: 29, loss: 0.004428446292877197, acc: 100.0, f1: 100.0, r: 0.7641247916960499
06/02/2019 01:28:28 *** evaluating ***
06/02/2019 01:28:29 step: 86, epoch: 85, acc: 58.119658119658126, f1: 29.094595287452357, r: 0.34567611678339993
06/02/2019 01:28:29 *** epoch: 87 ***
06/02/2019 01:28:29 *** training ***
06/02/2019 01:28:30 step: 2843, epoch: 86, batch: 4, loss: 0.0030707716941833496, acc: 100.0, f1: 100.0, r: 0.6188419084726671
06/02/2019 01:28:30 step: 2848, epoch: 86, batch: 9, loss: 0.0027648285031318665, acc: 100.0, f1: 100.0, r: 0.7067815041471923
06/02/2019 01:28:31 step: 2853, epoch: 86, batch: 14, loss: 0.00933278352022171, acc: 100.0, f1: 100.0, r: 0.7373532876328349
06/02/2019 01:28:32 step: 2858, epoch: 86, batch: 19, loss: 0.0037572234869003296, acc: 100.0, f1: 100.0, r: 0.649968567835445
06/02/2019 01:28:33 step: 2863, epoch: 86, batch: 24, loss: 0.00789167732000351, acc: 100.0, f1: 100.0, r: 0.8369463750190046
06/02/2019 01:28:34 step: 2868, epoch: 86, batch: 29, loss: 0.011024095118045807, acc: 100.0, f1: 100.0, r: 0.782525814213728
06/02/2019 01:28:34 *** evaluating ***
06/02/2019 01:28:35 step: 87, epoch: 86, acc: 58.97435897435898, f1: 29.761382729476125, r: 0.3521593200693816
06/02/2019 01:28:35 *** epoch: 88 ***
06/02/2019 01:28:35 *** training ***
06/02/2019 01:28:36 step: 2876, epoch: 87, batch: 4, loss: 0.003170020878314972, acc: 100.0, f1: 100.0, r: 0.7095105928324482
06/02/2019 01:28:36 step: 2881, epoch: 87, batch: 9, loss: 0.001039460301399231, acc: 100.0, f1: 100.0, r: 0.745371762970543
06/02/2019 01:28:37 step: 2886, epoch: 87, batch: 14, loss: 0.004045039415359497, acc: 100.0, f1: 100.0, r: 0.698952039920734
06/02/2019 01:28:38 step: 2891, epoch: 87, batch: 19, loss: 0.006275787949562073, acc: 100.0, f1: 100.0, r: 0.6168364739355315
06/02/2019 01:28:39 step: 2896, epoch: 87, batch: 24, loss: 0.002896130084991455, acc: 100.0, f1: 100.0, r: 0.6621623172867711
06/02/2019 01:28:40 step: 2901, epoch: 87, batch: 29, loss: 0.006879180669784546, acc: 100.0, f1: 100.0, r: 0.8554866827664249
06/02/2019 01:28:40 *** evaluating ***
06/02/2019 01:28:41 step: 88, epoch: 87, acc: 57.692307692307686, f1: 28.255094633484205, r: 0.35808451705163863
06/02/2019 01:28:41 *** epoch: 89 ***
06/02/2019 01:28:41 *** training ***
06/02/2019 01:28:42 step: 2909, epoch: 88, batch: 4, loss: 0.002498909831047058, acc: 100.0, f1: 100.0, r: 0.7496212531178454
06/02/2019 01:28:42 step: 2914, epoch: 88, batch: 9, loss: 0.013661973178386688, acc: 100.0, f1: 100.0, r: 0.6655545579032741
06/02/2019 01:28:43 step: 2919, epoch: 88, batch: 14, loss: 0.0027138590812683105, acc: 100.0, f1: 100.0, r: 0.6834932034774303
06/02/2019 01:28:44 step: 2924, epoch: 88, batch: 19, loss: 0.005921646952629089, acc: 100.0, f1: 100.0, r: 0.7333738997483121
06/02/2019 01:28:45 step: 2929, epoch: 88, batch: 24, loss: 0.0018134936690330505, acc: 100.0, f1: 100.0, r: 0.7466542217221992
06/02/2019 01:28:46 step: 2934, epoch: 88, batch: 29, loss: 0.006085358560085297, acc: 100.0, f1: 100.0, r: 0.7161905828806765
06/02/2019 01:28:46 *** evaluating ***
06/02/2019 01:28:47 step: 89, epoch: 88, acc: 58.119658119658126, f1: 28.616332497911444, r: 0.3392004799602404
06/02/2019 01:28:47 *** epoch: 90 ***
06/02/2019 01:28:47 *** training ***
06/02/2019 01:28:47 step: 2942, epoch: 89, batch: 4, loss: 0.006211563944816589, acc: 100.0, f1: 100.0, r: 0.8282542679747137
06/02/2019 01:28:48 step: 2947, epoch: 89, batch: 9, loss: 0.004227668046951294, acc: 100.0, f1: 100.0, r: 0.5897445099940123
06/02/2019 01:28:49 step: 2952, epoch: 89, batch: 14, loss: 0.0021241307258605957, acc: 100.0, f1: 100.0, r: 0.6433643310104152
06/02/2019 01:28:50 step: 2957, epoch: 89, batch: 19, loss: 0.008246265351772308, acc: 100.0, f1: 100.0, r: 0.8366073156253587
06/02/2019 01:28:51 step: 2962, epoch: 89, batch: 24, loss: 0.0033337101340293884, acc: 100.0, f1: 100.0, r: 0.6727989851123841
06/02/2019 01:28:52 step: 2967, epoch: 89, batch: 29, loss: 0.0016262084245681763, acc: 100.0, f1: 100.0, r: 0.8085193751774179
06/02/2019 01:28:52 *** evaluating ***
06/02/2019 01:28:53 step: 90, epoch: 89, acc: 58.97435897435898, f1: 30.197814207003454, r: 0.3428216064032256
06/02/2019 01:28:53 *** epoch: 91 ***
06/02/2019 01:28:53 *** training ***
06/02/2019 01:28:53 step: 2975, epoch: 90, batch: 4, loss: 0.0034070461988449097, acc: 100.0, f1: 100.0, r: 0.6757183073716772
06/02/2019 01:28:54 step: 2980, epoch: 90, batch: 9, loss: 0.0025609955191612244, acc: 100.0, f1: 100.0, r: 0.7057663294361168
06/02/2019 01:28:55 step: 2985, epoch: 90, batch: 14, loss: 0.00461164116859436, acc: 100.0, f1: 100.0, r: 0.8129804632735051
06/02/2019 01:28:56 step: 2990, epoch: 90, batch: 19, loss: 0.012511283159255981, acc: 100.0, f1: 100.0, r: 0.7711010435289241
06/02/2019 01:28:57 step: 2995, epoch: 90, batch: 24, loss: 0.007369387894868851, acc: 100.0, f1: 100.0, r: 0.6695907744745201
06/02/2019 01:28:58 step: 3000, epoch: 90, batch: 29, loss: 0.003958702087402344, acc: 100.0, f1: 100.0, r: 0.6790984597122516
06/02/2019 01:28:58 *** evaluating ***
06/02/2019 01:28:59 step: 91, epoch: 90, acc: 58.54700854700855, f1: 29.756139918826058, r: 0.33494996138614724
06/02/2019 01:28:59 *** epoch: 92 ***
06/02/2019 01:28:59 *** training ***
06/02/2019 01:28:59 step: 3008, epoch: 91, batch: 4, loss: 0.005087368190288544, acc: 100.0, f1: 100.0, r: 0.8422264247780451
06/02/2019 01:29:00 step: 3013, epoch: 91, batch: 9, loss: 0.010289408266544342, acc: 100.0, f1: 100.0, r: 0.584960813982923
06/02/2019 01:29:01 step: 3018, epoch: 91, batch: 14, loss: 0.0013285279273986816, acc: 100.0, f1: 100.0, r: 0.6619640631342594
06/02/2019 01:29:02 step: 3023, epoch: 91, batch: 19, loss: 0.004815578460693359, acc: 100.0, f1: 100.0, r: 0.6118321178386144
06/02/2019 01:29:03 step: 3028, epoch: 91, batch: 24, loss: 0.005919300019741058, acc: 100.0, f1: 100.0, r: 0.8011912733548034
06/02/2019 01:29:04 step: 3033, epoch: 91, batch: 29, loss: 0.004154488444328308, acc: 100.0, f1: 100.0, r: 0.8399299993761513
06/02/2019 01:29:04 *** evaluating ***
06/02/2019 01:29:04 step: 92, epoch: 91, acc: 59.401709401709404, f1: 28.350162379574144, r: 0.35161928345297305
06/02/2019 01:29:04 *** epoch: 93 ***
06/02/2019 01:29:04 *** training ***
06/02/2019 01:29:05 step: 3041, epoch: 92, batch: 4, loss: 0.0036584138870239258, acc: 100.0, f1: 100.0, r: 0.8149570802308962
06/02/2019 01:29:06 step: 3046, epoch: 92, batch: 9, loss: 0.003773629665374756, acc: 100.0, f1: 100.0, r: 0.8354013737789123
06/02/2019 01:29:07 step: 3051, epoch: 92, batch: 14, loss: 0.0044016167521476746, acc: 100.0, f1: 100.0, r: 0.7697526773136116
06/02/2019 01:29:08 step: 3056, epoch: 92, batch: 19, loss: 0.012403145432472229, acc: 100.0, f1: 100.0, r: 0.7550870548599146
06/02/2019 01:29:09 step: 3061, epoch: 92, batch: 24, loss: 0.006796456873416901, acc: 100.0, f1: 100.0, r: 0.7981533926154143
06/02/2019 01:29:10 step: 3066, epoch: 92, batch: 29, loss: 0.0017179474234580994, acc: 100.0, f1: 100.0, r: 0.7085136238774651
06/02/2019 01:29:10 *** evaluating ***
06/02/2019 01:29:10 step: 93, epoch: 92, acc: 58.119658119658126, f1: 29.583604993721625, r: 0.34115295723082967
06/02/2019 01:29:10 *** epoch: 94 ***
06/02/2019 01:29:10 *** training ***
06/02/2019 01:29:11 step: 3074, epoch: 93, batch: 4, loss: 0.0028323009610176086, acc: 100.0, f1: 100.0, r: 0.7377432474817476
06/02/2019 01:29:12 step: 3079, epoch: 93, batch: 9, loss: 0.003217756748199463, acc: 100.0, f1: 100.0, r: 0.6694682241898182
06/02/2019 01:29:13 step: 3084, epoch: 93, batch: 14, loss: 0.011293977499008179, acc: 100.0, f1: 100.0, r: 0.7587340436348108
06/02/2019 01:29:14 step: 3089, epoch: 93, batch: 19, loss: 0.00915171205997467, acc: 100.0, f1: 100.0, r: 0.8231289031869045
06/02/2019 01:29:15 step: 3094, epoch: 93, batch: 24, loss: 0.0049726516008377075, acc: 100.0, f1: 100.0, r: 0.7628031473150071
06/02/2019 01:29:16 step: 3099, epoch: 93, batch: 29, loss: 0.010435178875923157, acc: 100.0, f1: 100.0, r: 0.6069968209695497
06/02/2019 01:29:16 *** evaluating ***
06/02/2019 01:29:16 step: 94, epoch: 93, acc: 58.97435897435898, f1: 28.498814793760076, r: 0.34857081671701423
06/02/2019 01:29:16 *** epoch: 95 ***
06/02/2019 01:29:16 *** training ***
06/02/2019 01:29:17 step: 3107, epoch: 94, batch: 4, loss: 0.005978066474199295, acc: 100.0, f1: 100.0, r: 0.8221536517918472
06/02/2019 01:29:18 step: 3112, epoch: 94, batch: 9, loss: 0.006803646683692932, acc: 100.0, f1: 100.0, r: 0.8395398566286798
06/02/2019 01:29:19 step: 3117, epoch: 94, batch: 14, loss: 0.018854424357414246, acc: 100.0, f1: 100.0, r: 0.7469601684907924
06/02/2019 01:29:20 step: 3122, epoch: 94, batch: 19, loss: 0.006362520158290863, acc: 100.0, f1: 100.0, r: 0.8553412750505376
06/02/2019 01:29:21 step: 3127, epoch: 94, batch: 24, loss: 0.0029816552996635437, acc: 100.0, f1: 100.0, r: 0.7174725141201803
06/02/2019 01:29:21 step: 3132, epoch: 94, batch: 29, loss: 0.020888060331344604, acc: 100.0, f1: 100.0, r: 0.7935743896017005
06/02/2019 01:29:22 *** evaluating ***
06/02/2019 01:29:22 step: 95, epoch: 94, acc: 56.41025641025641, f1: 27.803402737613265, r: 0.33600991922006007
06/02/2019 01:29:22 *** epoch: 96 ***
06/02/2019 01:29:22 *** training ***
06/02/2019 01:29:23 step: 3140, epoch: 95, batch: 4, loss: 0.012063167989253998, acc: 100.0, f1: 100.0, r: 0.7832054975415083
06/02/2019 01:29:24 step: 3145, epoch: 95, batch: 9, loss: 0.001697339117527008, acc: 100.0, f1: 100.0, r: 0.6594405523896631
06/02/2019 01:29:25 step: 3150, epoch: 95, batch: 14, loss: 0.0020133033394813538, acc: 100.0, f1: 100.0, r: 0.740207248438387
06/02/2019 01:29:26 step: 3155, epoch: 95, batch: 19, loss: 0.0023813992738723755, acc: 100.0, f1: 100.0, r: 0.6364129490706347
06/02/2019 01:29:26 step: 3160, epoch: 95, batch: 24, loss: 0.001906663179397583, acc: 100.0, f1: 100.0, r: 0.770283568495855
06/02/2019 01:29:27 step: 3165, epoch: 95, batch: 29, loss: 0.000840112566947937, acc: 100.0, f1: 100.0, r: 0.7556250666972901
06/02/2019 01:29:28 *** evaluating ***
06/02/2019 01:29:28 step: 96, epoch: 95, acc: 56.41025641025641, f1: 28.356205412910835, r: 0.3432432972888729
06/02/2019 01:29:28 *** epoch: 97 ***
06/02/2019 01:29:28 *** training ***
06/02/2019 01:29:29 step: 3173, epoch: 96, batch: 4, loss: 0.008505702018737793, acc: 100.0, f1: 100.0, r: 0.713139469491092
06/02/2019 01:29:30 step: 3178, epoch: 96, batch: 9, loss: 0.002606809139251709, acc: 100.0, f1: 100.0, r: 0.7561027331712405
06/02/2019 01:29:31 step: 3183, epoch: 96, batch: 14, loss: 0.021697834134101868, acc: 98.4375, f1: 98.61853832442068, r: 0.786995485029161
06/02/2019 01:29:32 step: 3188, epoch: 96, batch: 19, loss: 0.0019915029406547546, acc: 100.0, f1: 100.0, r: 0.718168064527666
06/02/2019 01:29:32 step: 3193, epoch: 96, batch: 24, loss: 0.02880505472421646, acc: 98.4375, f1: 98.86128364389234, r: 0.8275282868985836
06/02/2019 01:29:33 step: 3198, epoch: 96, batch: 29, loss: 0.012002333998680115, acc: 100.0, f1: 100.0, r: 0.7177352222937803
06/02/2019 01:29:34 *** evaluating ***
06/02/2019 01:29:34 step: 97, epoch: 96, acc: 57.26495726495726, f1: 27.540903534973527, r: 0.34654197101376566
06/02/2019 01:29:34 *** epoch: 98 ***
06/02/2019 01:29:34 *** training ***
06/02/2019 01:29:35 step: 3206, epoch: 97, batch: 4, loss: 0.0039014145731925964, acc: 100.0, f1: 100.0, r: 0.7515112268335071
06/02/2019 01:29:36 step: 3211, epoch: 97, batch: 9, loss: 0.02061200514435768, acc: 98.4375, f1: 98.55875831485588, r: 0.7218916166630029
06/02/2019 01:29:37 step: 3216, epoch: 97, batch: 14, loss: 0.010882847011089325, acc: 100.0, f1: 100.0, r: 0.6794160912235864
06/02/2019 01:29:37 step: 3221, epoch: 97, batch: 19, loss: 0.009373646229505539, acc: 100.0, f1: 100.0, r: 0.8205779103625734
06/02/2019 01:29:38 step: 3226, epoch: 97, batch: 24, loss: 0.00888853520154953, acc: 100.0, f1: 100.0, r: 0.7961336751919188
06/02/2019 01:29:39 step: 3231, epoch: 97, batch: 29, loss: 0.0057732537388801575, acc: 100.0, f1: 100.0, r: 0.6025946650507921
06/02/2019 01:29:39 *** evaluating ***
06/02/2019 01:29:40 step: 98, epoch: 97, acc: 56.41025641025641, f1: 28.20540179695109, r: 0.3343781005665576
06/02/2019 01:29:40 *** epoch: 99 ***
06/02/2019 01:29:40 *** training ***
06/02/2019 01:29:41 step: 3239, epoch: 98, batch: 4, loss: 0.0023503825068473816, acc: 100.0, f1: 100.0, r: 0.7371720144801132
06/02/2019 01:29:41 step: 3244, epoch: 98, batch: 9, loss: 0.0023804977536201477, acc: 100.0, f1: 100.0, r: 0.7629053848897998
06/02/2019 01:29:42 step: 3249, epoch: 98, batch: 14, loss: 0.005047760903835297, acc: 100.0, f1: 100.0, r: 0.777634910678745
06/02/2019 01:29:43 step: 3254, epoch: 98, batch: 19, loss: 0.004765786230564117, acc: 100.0, f1: 100.0, r: 0.7147483907609951
06/02/2019 01:29:44 step: 3259, epoch: 98, batch: 24, loss: 0.0020022988319396973, acc: 100.0, f1: 100.0, r: 0.5992761512310395
06/02/2019 01:29:45 step: 3264, epoch: 98, batch: 29, loss: 0.0040891095995903015, acc: 100.0, f1: 100.0, r: 0.7732860635083916
06/02/2019 01:29:45 *** evaluating ***
06/02/2019 01:29:46 step: 99, epoch: 98, acc: 59.401709401709404, f1: 30.41505956449565, r: 0.3492626903503268
06/02/2019 01:29:46 *** epoch: 100 ***
06/02/2019 01:29:46 *** training ***
06/02/2019 01:29:47 step: 3272, epoch: 99, batch: 4, loss: 0.0048705339431762695, acc: 100.0, f1: 100.0, r: 0.7712320652656572
06/02/2019 01:29:47 step: 3277, epoch: 99, batch: 9, loss: 0.004760861396789551, acc: 100.0, f1: 100.0, r: 0.7975207123732841
06/02/2019 01:29:48 step: 3282, epoch: 99, batch: 14, loss: 0.007004052400588989, acc: 100.0, f1: 100.0, r: 0.7473149882790773
06/02/2019 01:29:49 step: 3287, epoch: 99, batch: 19, loss: 0.0031128525733947754, acc: 100.0, f1: 100.0, r: 0.6448584714094899
06/02/2019 01:29:50 step: 3292, epoch: 99, batch: 24, loss: 0.0048828646540641785, acc: 100.0, f1: 100.0, r: 0.8168924818286235
06/02/2019 01:29:51 step: 3297, epoch: 99, batch: 29, loss: 0.00799630582332611, acc: 100.0, f1: 100.0, r: 0.772499121803851
06/02/2019 01:29:51 *** evaluating ***
06/02/2019 01:29:51 step: 100, epoch: 99, acc: 56.837606837606835, f1: 27.547486894850447, r: 0.341981510215489
06/02/2019 01:29:51 *** epoch: 101 ***
06/02/2019 01:29:51 *** training ***
06/02/2019 01:29:52 step: 3305, epoch: 100, batch: 4, loss: 0.0027904212474823, acc: 100.0, f1: 100.0, r: 0.7756576261059792
06/02/2019 01:29:53 step: 3310, epoch: 100, batch: 9, loss: 0.002416655421257019, acc: 100.0, f1: 100.0, r: 0.7758441098711611
06/02/2019 01:29:54 step: 3315, epoch: 100, batch: 14, loss: 0.0017119348049163818, acc: 100.0, f1: 100.0, r: 0.7711663627251842
06/02/2019 01:29:55 step: 3320, epoch: 100, batch: 19, loss: 0.015570081770420074, acc: 98.4375, f1: 98.65047233468286, r: 0.661751670815558
06/02/2019 01:29:56 step: 3325, epoch: 100, batch: 24, loss: 0.006678648293018341, acc: 100.0, f1: 100.0, r: 0.7985730927508855
06/02/2019 01:29:56 step: 3330, epoch: 100, batch: 29, loss: 0.0024532005190849304, acc: 100.0, f1: 100.0, r: 0.7077927469507894
06/02/2019 01:29:57 *** evaluating ***
06/02/2019 01:29:57 step: 101, epoch: 100, acc: 55.98290598290598, f1: 27.05467503987241, r: 0.3311775291130921
06/02/2019 01:29:57 *** epoch: 102 ***
06/02/2019 01:29:57 *** training ***
06/02/2019 01:29:58 step: 3338, epoch: 101, batch: 4, loss: 0.003545120358467102, acc: 100.0, f1: 100.0, r: 0.6402647074986481
06/02/2019 01:29:59 step: 3343, epoch: 101, batch: 9, loss: 0.0047798678278923035, acc: 100.0, f1: 100.0, r: 0.8081410448130186
06/02/2019 01:30:00 step: 3348, epoch: 101, batch: 14, loss: 0.004909813404083252, acc: 100.0, f1: 100.0, r: 0.7191745031042867
06/02/2019 01:30:00 step: 3353, epoch: 101, batch: 19, loss: 0.0024942010641098022, acc: 100.0, f1: 100.0, r: 0.7090730730704794
06/02/2019 01:30:01 step: 3358, epoch: 101, batch: 24, loss: 0.004198864102363586, acc: 100.0, f1: 100.0, r: 0.8010547229602688
06/02/2019 01:30:02 step: 3363, epoch: 101, batch: 29, loss: 0.009008601307868958, acc: 100.0, f1: 100.0, r: 0.7919256859046908
06/02/2019 01:30:03 *** evaluating ***
06/02/2019 01:30:03 step: 102, epoch: 101, acc: 56.41025641025641, f1: 28.452349661489446, r: 0.3328410823234075
06/02/2019 01:30:03 *** epoch: 103 ***
06/02/2019 01:30:03 *** training ***
06/02/2019 01:30:04 step: 3371, epoch: 102, batch: 4, loss: 0.004038318991661072, acc: 100.0, f1: 100.0, r: 0.6791599626925158
06/02/2019 01:30:05 step: 3376, epoch: 102, batch: 9, loss: 0.003398999571800232, acc: 100.0, f1: 100.0, r: 0.7544783417641763
06/02/2019 01:30:06 step: 3381, epoch: 102, batch: 14, loss: 0.0021555721759796143, acc: 100.0, f1: 100.0, r: 0.6692266188092789
06/02/2019 01:30:06 step: 3386, epoch: 102, batch: 19, loss: 0.0030544623732566833, acc: 100.0, f1: 100.0, r: 0.6868516913398691
06/02/2019 01:30:07 step: 3391, epoch: 102, batch: 24, loss: 0.01473061740398407, acc: 100.0, f1: 100.0, r: 0.6319534780565849
06/02/2019 01:30:08 step: 3396, epoch: 102, batch: 29, loss: 0.0024278610944747925, acc: 100.0, f1: 100.0, r: 0.7899493143288945
06/02/2019 01:30:08 *** evaluating ***
06/02/2019 01:30:09 step: 103, epoch: 102, acc: 55.55555555555556, f1: 28.81275628362454, r: 0.32926166993887884
06/02/2019 01:30:09 *** epoch: 104 ***
06/02/2019 01:30:09 *** training ***
06/02/2019 01:30:10 step: 3404, epoch: 103, batch: 4, loss: 0.006074190139770508, acc: 100.0, f1: 100.0, r: 0.6918814046565999
06/02/2019 01:30:10 step: 3409, epoch: 103, batch: 9, loss: 0.004431113600730896, acc: 100.0, f1: 100.0, r: 0.7923703004457046
06/02/2019 01:30:11 step: 3414, epoch: 103, batch: 14, loss: 0.002102896571159363, acc: 100.0, f1: 100.0, r: 0.774248720464744
06/02/2019 01:30:12 step: 3419, epoch: 103, batch: 19, loss: 0.01331871747970581, acc: 100.0, f1: 100.0, r: 0.7977490412893988
06/02/2019 01:30:13 step: 3424, epoch: 103, batch: 24, loss: 0.011054076254367828, acc: 100.0, f1: 100.0, r: 0.8050275183528021
06/02/2019 01:30:14 step: 3429, epoch: 103, batch: 29, loss: 0.004093877971172333, acc: 100.0, f1: 100.0, r: 0.7315558932673137
06/02/2019 01:30:14 *** evaluating ***
06/02/2019 01:30:15 step: 104, epoch: 103, acc: 57.692307692307686, f1: 29.147590022254988, r: 0.3469386465831967
06/02/2019 01:30:15 *** epoch: 105 ***
06/02/2019 01:30:15 *** training ***
06/02/2019 01:30:15 step: 3437, epoch: 104, batch: 4, loss: 0.006317794322967529, acc: 100.0, f1: 100.0, r: 0.7139101364768581
06/02/2019 01:30:16 step: 3442, epoch: 104, batch: 9, loss: 0.0032860413193702698, acc: 100.0, f1: 100.0, r: 0.7619462572871811
06/02/2019 01:30:17 step: 3447, epoch: 104, batch: 14, loss: 0.0022618919610977173, acc: 100.0, f1: 100.0, r: 0.6958759109942205
06/02/2019 01:30:18 step: 3452, epoch: 104, batch: 19, loss: 0.006627615541219711, acc: 100.0, f1: 100.0, r: 0.6619668471248021
06/02/2019 01:30:19 step: 3457, epoch: 104, batch: 24, loss: 0.008560694754123688, acc: 100.0, f1: 100.0, r: 0.8000323914880512
06/02/2019 01:30:20 step: 3462, epoch: 104, batch: 29, loss: 0.011103622615337372, acc: 100.0, f1: 100.0, r: 0.7617704810779224
06/02/2019 01:30:20 *** evaluating ***
06/02/2019 01:30:20 step: 105, epoch: 104, acc: 57.26495726495726, f1: 29.865209174832604, r: 0.34661879836929
06/02/2019 01:30:20 *** epoch: 106 ***
06/02/2019 01:30:20 *** training ***
06/02/2019 01:30:21 step: 3470, epoch: 105, batch: 4, loss: 0.0029978975653648376, acc: 100.0, f1: 100.0, r: 0.709888001437329
06/02/2019 01:30:22 step: 3475, epoch: 105, batch: 9, loss: 0.004501625895500183, acc: 100.0, f1: 100.0, r: 0.6912343702623809
06/02/2019 01:30:23 step: 3480, epoch: 105, batch: 14, loss: 0.004667453467845917, acc: 100.0, f1: 100.0, r: 0.6744152321793162
06/02/2019 01:30:24 step: 3485, epoch: 105, batch: 19, loss: 0.00203908234834671, acc: 100.0, f1: 100.0, r: 0.8266890362815671
06/02/2019 01:30:25 step: 3490, epoch: 105, batch: 24, loss: 0.003770604729652405, acc: 100.0, f1: 100.0, r: 0.783474291240344
06/02/2019 01:30:25 step: 3495, epoch: 105, batch: 29, loss: 0.0018796250224113464, acc: 100.0, f1: 100.0, r: 0.7488299617730059
06/02/2019 01:30:26 *** evaluating ***
06/02/2019 01:30:26 step: 106, epoch: 105, acc: 55.98290598290598, f1: 29.08098588897925, r: 0.33629667971324273
06/02/2019 01:30:26 *** epoch: 107 ***
06/02/2019 01:30:26 *** training ***
06/02/2019 01:30:27 step: 3503, epoch: 106, batch: 4, loss: 0.0030303969979286194, acc: 100.0, f1: 100.0, r: 0.7598409294943946
06/02/2019 01:30:28 step: 3508, epoch: 106, batch: 9, loss: 0.0024930164217948914, acc: 100.0, f1: 100.0, r: 0.8166515950369322
06/02/2019 01:30:29 step: 3513, epoch: 106, batch: 14, loss: 0.003733322024345398, acc: 100.0, f1: 100.0, r: 0.7673575125234221
06/02/2019 01:30:30 step: 3518, epoch: 106, batch: 19, loss: 0.008387751877307892, acc: 100.0, f1: 100.0, r: 0.7150174359805058
06/02/2019 01:30:30 step: 3523, epoch: 106, batch: 24, loss: 0.004382334649562836, acc: 100.0, f1: 100.0, r: 0.6818056601392845
06/02/2019 01:30:31 step: 3528, epoch: 106, batch: 29, loss: 0.006974995136260986, acc: 100.0, f1: 100.0, r: 0.6913539466676005
06/02/2019 01:30:32 *** evaluating ***
06/02/2019 01:30:32 step: 107, epoch: 106, acc: 58.97435897435898, f1: 29.954622736880797, r: 0.3568753060639022
06/02/2019 01:30:32 *** epoch: 108 ***
06/02/2019 01:30:32 *** training ***
06/02/2019 01:30:33 step: 3536, epoch: 107, batch: 4, loss: 0.0037589147686958313, acc: 100.0, f1: 100.0, r: 0.8246582307377832
06/02/2019 01:30:34 step: 3541, epoch: 107, batch: 9, loss: 0.0034651905298233032, acc: 100.0, f1: 100.0, r: 0.7264262085759231
06/02/2019 01:30:35 step: 3546, epoch: 107, batch: 14, loss: 0.0034498274326324463, acc: 100.0, f1: 100.0, r: 0.6751357911127389
06/02/2019 01:30:35 step: 3551, epoch: 107, batch: 19, loss: 0.006322428584098816, acc: 100.0, f1: 100.0, r: 0.7075108017953671
06/02/2019 01:30:36 step: 3556, epoch: 107, batch: 24, loss: 0.012329228222370148, acc: 100.0, f1: 100.0, r: 0.735774743187704
06/02/2019 01:30:37 step: 3561, epoch: 107, batch: 29, loss: 0.0030181705951690674, acc: 100.0, f1: 100.0, r: 0.7117728757255762
06/02/2019 01:30:38 *** evaluating ***
06/02/2019 01:30:38 step: 108, epoch: 107, acc: 59.401709401709404, f1: 30.53952991452991, r: 0.35566796279887414
06/02/2019 01:30:38 *** epoch: 109 ***
06/02/2019 01:30:38 *** training ***
06/02/2019 01:30:39 step: 3569, epoch: 108, batch: 4, loss: 0.0015053078532218933, acc: 100.0, f1: 100.0, r: 0.6889095541300474
06/02/2019 01:30:40 step: 3574, epoch: 108, batch: 9, loss: 0.021736864000558853, acc: 98.4375, f1: 99.25490196078431, r: 0.7996313325316782
06/02/2019 01:30:40 step: 3579, epoch: 108, batch: 14, loss: 0.0038640424609184265, acc: 100.0, f1: 100.0, r: 0.791897616899935
06/02/2019 01:30:41 step: 3584, epoch: 108, batch: 19, loss: 0.00263252854347229, acc: 100.0, f1: 100.0, r: 0.6486711426871667
06/02/2019 01:30:42 step: 3589, epoch: 108, batch: 24, loss: 0.013151030987501144, acc: 100.0, f1: 100.0, r: 0.550112665863826
06/02/2019 01:30:43 step: 3594, epoch: 108, batch: 29, loss: 0.011083610355854034, acc: 100.0, f1: 100.0, r: 0.7481351079708128
06/02/2019 01:30:43 *** evaluating ***
06/02/2019 01:30:44 step: 109, epoch: 108, acc: 57.26495726495726, f1: 29.640693435740484, r: 0.34053063664399036
06/02/2019 01:30:44 *** epoch: 110 ***
06/02/2019 01:30:44 *** training ***
06/02/2019 01:30:44 step: 3602, epoch: 109, batch: 4, loss: 0.0033937618136405945, acc: 100.0, f1: 100.0, r: 0.7811097468928938
06/02/2019 01:30:45 step: 3607, epoch: 109, batch: 9, loss: 0.0019585713744163513, acc: 100.0, f1: 100.0, r: 0.7188986930177892
06/02/2019 01:30:46 step: 3612, epoch: 109, batch: 14, loss: 0.005853921175003052, acc: 100.0, f1: 100.0, r: 0.7816091893483206
06/02/2019 01:30:47 step: 3617, epoch: 109, batch: 19, loss: 0.005027689039707184, acc: 100.0, f1: 100.0, r: 0.8230944780351367
06/02/2019 01:30:48 step: 3622, epoch: 109, batch: 24, loss: 0.0022918730974197388, acc: 100.0, f1: 100.0, r: 0.680016442617405
06/02/2019 01:30:49 step: 3627, epoch: 109, batch: 29, loss: 0.0027911216020584106, acc: 100.0, f1: 100.0, r: 0.768871325960206
06/02/2019 01:30:49 *** evaluating ***
06/02/2019 01:30:49 step: 110, epoch: 109, acc: 59.401709401709404, f1: 30.470958779782308, r: 0.3496920833134757
06/02/2019 01:30:49 *** epoch: 111 ***
06/02/2019 01:30:49 *** training ***
06/02/2019 01:30:50 step: 3635, epoch: 110, batch: 4, loss: 0.0032210201025009155, acc: 100.0, f1: 100.0, r: 0.7231231958341401
06/02/2019 01:30:51 step: 3640, epoch: 110, batch: 9, loss: 0.0024671852588653564, acc: 100.0, f1: 100.0, r: 0.7857804259462423
06/02/2019 01:30:52 step: 3645, epoch: 110, batch: 14, loss: 0.005984082818031311, acc: 100.0, f1: 100.0, r: 0.7941312944353567
06/02/2019 01:30:53 step: 3650, epoch: 110, batch: 19, loss: 0.0032549574971199036, acc: 100.0, f1: 100.0, r: 0.8036430608283693
06/02/2019 01:30:54 step: 3655, epoch: 110, batch: 24, loss: 0.0030440092086791992, acc: 100.0, f1: 100.0, r: 0.7236348110316122
06/02/2019 01:30:55 step: 3660, epoch: 110, batch: 29, loss: 0.0032001063227653503, acc: 100.0, f1: 100.0, r: 0.7778839206273421
06/02/2019 01:30:55 *** evaluating ***
06/02/2019 01:30:55 step: 111, epoch: 110, acc: 56.837606837606835, f1: 29.07359009032437, r: 0.3460453668945753
06/02/2019 01:30:55 *** epoch: 112 ***
06/02/2019 01:30:55 *** training ***
06/02/2019 01:30:56 step: 3668, epoch: 111, batch: 4, loss: 0.0058243535459041595, acc: 100.0, f1: 100.0, r: 0.6890489573899742
06/02/2019 01:30:57 step: 3673, epoch: 111, batch: 9, loss: 0.004381172358989716, acc: 100.0, f1: 100.0, r: 0.7779840959666278
06/02/2019 01:30:58 step: 3678, epoch: 111, batch: 14, loss: 0.0037415549159049988, acc: 100.0, f1: 100.0, r: 0.6774242918867511
06/02/2019 01:30:59 step: 3683, epoch: 111, batch: 19, loss: 0.0011216551065444946, acc: 100.0, f1: 100.0, r: 0.7065063023336321
06/02/2019 01:31:00 step: 3688, epoch: 111, batch: 24, loss: 0.0059691667556762695, acc: 100.0, f1: 100.0, r: 0.825282751529446
06/02/2019 01:31:00 step: 3693, epoch: 111, batch: 29, loss: 0.005791306495666504, acc: 100.0, f1: 100.0, r: 0.681271769111261
06/02/2019 01:31:01 *** evaluating ***
06/02/2019 01:31:01 step: 112, epoch: 111, acc: 58.97435897435898, f1: 29.38856419773288, r: 0.3394952907898135
06/02/2019 01:31:01 *** epoch: 113 ***
06/02/2019 01:31:01 *** training ***
06/02/2019 01:31:02 step: 3701, epoch: 112, batch: 4, loss: 0.004685111343860626, acc: 100.0, f1: 100.0, r: 0.6866556288702492
06/02/2019 01:31:03 step: 3706, epoch: 112, batch: 9, loss: 0.0035875365138053894, acc: 100.0, f1: 100.0, r: 0.7505911529171349
06/02/2019 01:31:04 step: 3711, epoch: 112, batch: 14, loss: 0.0017940923571586609, acc: 100.0, f1: 100.0, r: 0.6702144116735601
06/02/2019 01:31:05 step: 3716, epoch: 112, batch: 19, loss: 0.0061584338545799255, acc: 100.0, f1: 100.0, r: 0.6006013614300731
06/02/2019 01:31:05 step: 3721, epoch: 112, batch: 24, loss: 0.009962819516658783, acc: 100.0, f1: 100.0, r: 0.6703408473964838
06/02/2019 01:31:06 step: 3726, epoch: 112, batch: 29, loss: 0.00467468798160553, acc: 100.0, f1: 100.0, r: 0.7728510596848761
06/02/2019 01:31:07 *** evaluating ***
06/02/2019 01:31:07 step: 113, epoch: 112, acc: 57.26495726495726, f1: 29.19612052381244, r: 0.3564597452050813
06/02/2019 01:31:07 *** epoch: 114 ***
06/02/2019 01:31:07 *** training ***
06/02/2019 01:31:08 step: 3734, epoch: 113, batch: 4, loss: 0.002885483205318451, acc: 100.0, f1: 100.0, r: 0.671270702725812
06/02/2019 01:31:09 step: 3739, epoch: 113, batch: 9, loss: 0.006613515317440033, acc: 100.0, f1: 100.0, r: 0.6814329010201576
06/02/2019 01:31:10 step: 3744, epoch: 113, batch: 14, loss: 0.005223706364631653, acc: 100.0, f1: 100.0, r: 0.7745721985629148
06/02/2019 01:31:10 step: 3749, epoch: 113, batch: 19, loss: 0.005510397255420685, acc: 100.0, f1: 100.0, r: 0.7733408357464238
06/02/2019 01:31:11 step: 3754, epoch: 113, batch: 24, loss: 0.008329324424266815, acc: 100.0, f1: 100.0, r: 0.8339489550209405
06/02/2019 01:31:12 step: 3759, epoch: 113, batch: 29, loss: 0.0028372034430503845, acc: 100.0, f1: 100.0, r: 0.7114034072509801
06/02/2019 01:31:12 *** evaluating ***
06/02/2019 01:31:13 step: 114, epoch: 113, acc: 57.692307692307686, f1: 29.550969142203233, r: 0.34263244788192976
06/02/2019 01:31:13 *** epoch: 115 ***
06/02/2019 01:31:13 *** training ***
06/02/2019 01:31:14 step: 3767, epoch: 114, batch: 4, loss: 0.0015141665935516357, acc: 100.0, f1: 100.0, r: 0.7038461811067632
06/02/2019 01:31:14 step: 3772, epoch: 114, batch: 9, loss: 0.002551048994064331, acc: 100.0, f1: 100.0, r: 0.7318813580504087
06/02/2019 01:31:15 step: 3777, epoch: 114, batch: 14, loss: 0.006075337529182434, acc: 100.0, f1: 100.0, r: 0.6437376569484561
06/02/2019 01:31:16 step: 3782, epoch: 114, batch: 19, loss: 0.003486163914203644, acc: 100.0, f1: 100.0, r: 0.7053228408098298
06/02/2019 01:31:17 step: 3787, epoch: 114, batch: 24, loss: 0.0023835450410842896, acc: 100.0, f1: 100.0, r: 0.6652323698654944
06/02/2019 01:31:18 step: 3792, epoch: 114, batch: 29, loss: 0.004258677363395691, acc: 100.0, f1: 100.0, r: 0.5369499798565517
06/02/2019 01:31:18 *** evaluating ***
06/02/2019 01:31:19 step: 115, epoch: 114, acc: 57.692307692307686, f1: 28.52184017862601, r: 0.3426433255685081
06/02/2019 01:31:19 *** epoch: 116 ***
06/02/2019 01:31:19 *** training ***
06/02/2019 01:31:20 step: 3800, epoch: 115, batch: 4, loss: 0.0034083276987075806, acc: 100.0, f1: 100.0, r: 0.6925404833150013
06/02/2019 01:31:20 step: 3805, epoch: 115, batch: 9, loss: 0.0018217191100120544, acc: 100.0, f1: 100.0, r: 0.7237108814086254
06/02/2019 01:31:21 step: 3810, epoch: 115, batch: 14, loss: 0.005078885704278946, acc: 100.0, f1: 100.0, r: 0.7199515888970989
06/02/2019 01:31:22 step: 3815, epoch: 115, batch: 19, loss: 0.005420170724391937, acc: 100.0, f1: 100.0, r: 0.6683272189988484
06/02/2019 01:31:23 step: 3820, epoch: 115, batch: 24, loss: 0.0032599791884422302, acc: 100.0, f1: 100.0, r: 0.8055891013371613
06/02/2019 01:31:24 step: 3825, epoch: 115, batch: 29, loss: 0.009585089981555939, acc: 100.0, f1: 100.0, r: 0.6914642754710797
06/02/2019 01:31:24 *** evaluating ***
06/02/2019 01:31:25 step: 116, epoch: 115, acc: 58.119658119658126, f1: 30.087723477436146, r: 0.3419112601806132
06/02/2019 01:31:25 *** epoch: 117 ***
06/02/2019 01:31:25 *** training ***
06/02/2019 01:31:25 step: 3833, epoch: 116, batch: 4, loss: 0.012888316065073013, acc: 100.0, f1: 100.0, r: 0.7095593552320787
06/02/2019 01:31:26 step: 3838, epoch: 116, batch: 9, loss: 0.0030388236045837402, acc: 100.0, f1: 100.0, r: 0.7662146342681134
06/02/2019 01:31:27 step: 3843, epoch: 116, batch: 14, loss: 0.004596292972564697, acc: 100.0, f1: 100.0, r: 0.7156451316615822
06/02/2019 01:31:28 step: 3848, epoch: 116, batch: 19, loss: 0.002238243818283081, acc: 100.0, f1: 100.0, r: 0.7304242164155309
06/02/2019 01:31:29 step: 3853, epoch: 116, batch: 24, loss: 0.001277424395084381, acc: 100.0, f1: 100.0, r: 0.8003482682478706
06/02/2019 01:31:30 step: 3858, epoch: 116, batch: 29, loss: 0.005199059844017029, acc: 100.0, f1: 100.0, r: 0.7002632233963167
06/02/2019 01:31:30 *** evaluating ***
06/02/2019 01:31:30 step: 117, epoch: 116, acc: 56.41025641025641, f1: 29.385946949757514, r: 0.34230498033915135
06/02/2019 01:31:30 *** epoch: 118 ***
06/02/2019 01:31:30 *** training ***
06/02/2019 01:31:31 step: 3866, epoch: 117, batch: 4, loss: 0.003725484013557434, acc: 100.0, f1: 100.0, r: 0.7718849407608304
06/02/2019 01:31:32 step: 3871, epoch: 117, batch: 9, loss: 0.003710933029651642, acc: 100.0, f1: 100.0, r: 0.7336205727117251
06/02/2019 01:31:33 step: 3876, epoch: 117, batch: 14, loss: 0.005070596933364868, acc: 100.0, f1: 100.0, r: 0.8290160361852463
06/02/2019 01:31:34 step: 3881, epoch: 117, batch: 19, loss: 0.002014756202697754, acc: 100.0, f1: 100.0, r: 0.779460998205405
06/02/2019 01:31:35 step: 3886, epoch: 117, batch: 24, loss: 0.004821032285690308, acc: 100.0, f1: 100.0, r: 0.6713066414728236
06/02/2019 01:31:36 step: 3891, epoch: 117, batch: 29, loss: 0.008002713322639465, acc: 100.0, f1: 100.0, r: 0.756747006595884
06/02/2019 01:31:36 *** evaluating ***
06/02/2019 01:31:36 step: 118, epoch: 117, acc: 57.692307692307686, f1: 29.683670256313366, r: 0.33345677905984183
06/02/2019 01:31:36 *** epoch: 119 ***
06/02/2019 01:31:36 *** training ***
06/02/2019 01:31:37 step: 3899, epoch: 118, batch: 4, loss: 0.0017095357179641724, acc: 100.0, f1: 100.0, r: 0.8161823578035523
06/02/2019 01:31:38 step: 3904, epoch: 118, batch: 9, loss: 0.008623629808425903, acc: 100.0, f1: 100.0, r: 0.694010583874971
06/02/2019 01:31:39 step: 3909, epoch: 118, batch: 14, loss: 0.0042247474193573, acc: 100.0, f1: 100.0, r: 0.8249129154043984
06/02/2019 01:31:40 step: 3914, epoch: 118, batch: 19, loss: 0.002632632851600647, acc: 100.0, f1: 100.0, r: 0.7095434788639151
06/02/2019 01:31:41 step: 3919, epoch: 118, batch: 24, loss: 0.010734349489212036, acc: 100.0, f1: 100.0, r: 0.6928300498129752
06/02/2019 01:31:42 step: 3924, epoch: 118, batch: 29, loss: 0.005306273698806763, acc: 100.0, f1: 100.0, r: 0.7162337544893758
06/02/2019 01:31:42 *** evaluating ***
06/02/2019 01:31:42 step: 119, epoch: 118, acc: 57.26495726495726, f1: 28.17979779240073, r: 0.3496778247118211
06/02/2019 01:31:42 *** epoch: 120 ***
06/02/2019 01:31:42 *** training ***
06/02/2019 01:31:43 step: 3932, epoch: 119, batch: 4, loss: 0.0027053356170654297, acc: 100.0, f1: 100.0, r: 0.7180242947490398
06/02/2019 01:31:44 step: 3937, epoch: 119, batch: 9, loss: 0.0014599338173866272, acc: 100.0, f1: 100.0, r: 0.7062561722536951
06/02/2019 01:31:45 step: 3942, epoch: 119, batch: 14, loss: 0.0016924068331718445, acc: 100.0, f1: 100.0, r: 0.8058512104726832
06/02/2019 01:31:46 step: 3947, epoch: 119, batch: 19, loss: 0.007288910448551178, acc: 100.0, f1: 100.0, r: 0.7920642430027585
06/02/2019 01:31:47 step: 3952, epoch: 119, batch: 24, loss: 0.0016058310866355896, acc: 100.0, f1: 100.0, r: 0.7212050447816574
06/02/2019 01:31:48 step: 3957, epoch: 119, batch: 29, loss: 0.0013265907764434814, acc: 100.0, f1: 100.0, r: 0.7571013732127561
06/02/2019 01:31:48 *** evaluating ***
06/02/2019 01:31:48 step: 120, epoch: 119, acc: 58.119658119658126, f1: 28.9051790689573, r: 0.34767889964720017
06/02/2019 01:31:48 *** epoch: 121 ***
06/02/2019 01:31:48 *** training ***
06/02/2019 01:31:49 step: 3965, epoch: 120, batch: 4, loss: 0.002336747944355011, acc: 100.0, f1: 100.0, r: 0.7174835765630334
06/02/2019 01:31:50 step: 3970, epoch: 120, batch: 9, loss: 0.004279613494873047, acc: 100.0, f1: 100.0, r: 0.7306186870967324
06/02/2019 01:31:51 step: 3975, epoch: 120, batch: 14, loss: 0.0009326860308647156, acc: 100.0, f1: 100.0, r: 0.7037749040914151
06/02/2019 01:31:52 step: 3980, epoch: 120, batch: 19, loss: 0.010793320834636688, acc: 100.0, f1: 100.0, r: 0.8103583636419248
06/02/2019 01:31:52 step: 3985, epoch: 120, batch: 24, loss: 0.0015372484922409058, acc: 100.0, f1: 100.0, r: 0.7354502137870264
06/02/2019 01:31:53 step: 3990, epoch: 120, batch: 29, loss: 0.006946593523025513, acc: 100.0, f1: 100.0, r: 0.7305538589868253
06/02/2019 01:31:54 *** evaluating ***
06/02/2019 01:31:54 step: 121, epoch: 120, acc: 54.27350427350427, f1: 26.613005859341637, r: 0.33696350850402457
06/02/2019 01:31:54 *** epoch: 122 ***
06/02/2019 01:31:54 *** training ***
06/02/2019 01:31:55 step: 3998, epoch: 121, batch: 4, loss: 0.0015245378017425537, acc: 100.0, f1: 100.0, r: 0.7229528266927607
06/02/2019 01:31:56 step: 4003, epoch: 121, batch: 9, loss: 0.006511256098747253, acc: 100.0, f1: 100.0, r: 0.7550887021955623
06/02/2019 01:31:57 step: 4008, epoch: 121, batch: 14, loss: 0.0019206181168556213, acc: 100.0, f1: 100.0, r: 0.7056503364479545
06/02/2019 01:31:58 step: 4013, epoch: 121, batch: 19, loss: 0.003198668360710144, acc: 100.0, f1: 100.0, r: 0.7322387424043366
06/02/2019 01:31:58 step: 4018, epoch: 121, batch: 24, loss: 0.002407871186733246, acc: 100.0, f1: 100.0, r: 0.8120751809769365
06/02/2019 01:31:59 step: 4023, epoch: 121, batch: 29, loss: 0.0088820680975914, acc: 100.0, f1: 100.0, r: 0.718902805896694
06/02/2019 01:32:00 *** evaluating ***
06/02/2019 01:32:00 step: 122, epoch: 121, acc: 57.26495726495726, f1: 29.846265803452166, r: 0.3374434799777496
06/02/2019 01:32:00 *** epoch: 123 ***
06/02/2019 01:32:00 *** training ***
06/02/2019 01:32:01 step: 4031, epoch: 122, batch: 4, loss: 0.002869773656129837, acc: 100.0, f1: 100.0, r: 0.5866346889080954
06/02/2019 01:32:02 step: 4036, epoch: 122, batch: 9, loss: 0.010280556976795197, acc: 100.0, f1: 100.0, r: 0.7719999751584571
06/02/2019 01:32:03 step: 4041, epoch: 122, batch: 14, loss: 0.004044145345687866, acc: 100.0, f1: 100.0, r: 0.8077902994513123
06/02/2019 01:32:03 step: 4046, epoch: 122, batch: 19, loss: 0.005698747932910919, acc: 100.0, f1: 100.0, r: 0.7547158995411838
06/02/2019 01:32:04 step: 4051, epoch: 122, batch: 24, loss: 0.0007646083831787109, acc: 100.0, f1: 100.0, r: 0.7192239086025721
06/02/2019 01:32:05 step: 4056, epoch: 122, batch: 29, loss: 0.002319425344467163, acc: 100.0, f1: 100.0, r: 0.6765426175513806
06/02/2019 01:32:06 *** evaluating ***
06/02/2019 01:32:06 step: 123, epoch: 122, acc: 57.692307692307686, f1: 28.914418398938523, r: 0.3355129125763592
06/02/2019 01:32:06 *** epoch: 124 ***
06/02/2019 01:32:06 *** training ***
06/02/2019 01:32:07 step: 4064, epoch: 123, batch: 4, loss: 0.002475328743457794, acc: 100.0, f1: 100.0, r: 0.8027605472486522
06/02/2019 01:32:08 step: 4069, epoch: 123, batch: 9, loss: 0.0008214861154556274, acc: 100.0, f1: 100.0, r: 0.6725043283040998
06/02/2019 01:32:09 step: 4074, epoch: 123, batch: 14, loss: 0.0012932568788528442, acc: 100.0, f1: 100.0, r: 0.7896797821450089
06/02/2019 01:32:09 step: 4079, epoch: 123, batch: 19, loss: 0.0009807273745536804, acc: 100.0, f1: 100.0, r: 0.7436294460503506
06/02/2019 01:32:10 step: 4084, epoch: 123, batch: 24, loss: 0.0037315040826797485, acc: 100.0, f1: 100.0, r: 0.8390533621884864
06/02/2019 01:32:11 step: 4089, epoch: 123, batch: 29, loss: 0.0007313713431358337, acc: 100.0, f1: 100.0, r: 0.7836219033108053
06/02/2019 01:32:11 *** evaluating ***
06/02/2019 01:32:12 step: 124, epoch: 123, acc: 56.837606837606835, f1: 28.39878375592661, r: 0.3371238037891537
06/02/2019 01:32:12 *** epoch: 125 ***
06/02/2019 01:32:12 *** training ***
06/02/2019 01:32:13 step: 4097, epoch: 124, batch: 4, loss: 0.002414882183074951, acc: 100.0, f1: 100.0, r: 0.8034555794241893
06/02/2019 01:32:14 step: 4102, epoch: 124, batch: 9, loss: 0.002574153244495392, acc: 100.0, f1: 100.0, r: 0.6508065267031347
06/02/2019 01:32:14 step: 4107, epoch: 124, batch: 14, loss: 0.004773549735546112, acc: 100.0, f1: 100.0, r: 0.6664083202923488
06/02/2019 01:32:15 step: 4112, epoch: 124, batch: 19, loss: 0.0022532865405082703, acc: 100.0, f1: 100.0, r: 0.8043934305105036
06/02/2019 01:32:16 step: 4117, epoch: 124, batch: 24, loss: 0.00497300922870636, acc: 100.0, f1: 100.0, r: 0.7427008792878443
06/02/2019 01:32:17 step: 4122, epoch: 124, batch: 29, loss: 0.0047819241881370544, acc: 100.0, f1: 100.0, r: 0.7096699672762037
06/02/2019 01:32:17 *** evaluating ***
06/02/2019 01:32:18 step: 125, epoch: 124, acc: 57.26495726495726, f1: 29.606310857741814, r: 0.3360482989344833
06/02/2019 01:32:18 *** epoch: 126 ***
06/02/2019 01:32:18 *** training ***
06/02/2019 01:32:19 step: 4130, epoch: 125, batch: 4, loss: 0.0011237040162086487, acc: 100.0, f1: 100.0, r: 0.6173249950973275
06/02/2019 01:32:19 step: 4135, epoch: 125, batch: 9, loss: 0.0032736286520957947, acc: 100.0, f1: 100.0, r: 0.7971169911456073
06/02/2019 01:32:20 step: 4140, epoch: 125, batch: 14, loss: 0.0010796040296554565, acc: 100.0, f1: 100.0, r: 0.7862001569594993
06/02/2019 01:32:21 step: 4145, epoch: 125, batch: 19, loss: 0.007223665714263916, acc: 100.0, f1: 100.0, r: 0.7159814434067207
06/02/2019 01:32:22 step: 4150, epoch: 125, batch: 24, loss: 0.003277689218521118, acc: 100.0, f1: 100.0, r: 0.8186840265290007
06/02/2019 01:32:23 step: 4155, epoch: 125, batch: 29, loss: 0.0027040690183639526, acc: 100.0, f1: 100.0, r: 0.7986215800767038
06/02/2019 01:32:23 *** evaluating ***
06/02/2019 01:32:23 step: 126, epoch: 125, acc: 57.26495726495726, f1: 28.52423206937096, r: 0.33744519020857155
06/02/2019 01:32:23 *** epoch: 127 ***
06/02/2019 01:32:23 *** training ***
06/02/2019 01:32:24 step: 4163, epoch: 126, batch: 4, loss: 0.0013415142893791199, acc: 100.0, f1: 100.0, r: 0.804783903533686
06/02/2019 01:32:25 step: 4168, epoch: 126, batch: 9, loss: 0.0011149346828460693, acc: 100.0, f1: 100.0, r: 0.8475190923632914
06/02/2019 01:32:26 step: 4173, epoch: 126, batch: 14, loss: 0.007017910480499268, acc: 100.0, f1: 100.0, r: 0.6304318849471925
06/02/2019 01:32:27 step: 4178, epoch: 126, batch: 19, loss: 0.004937753081321716, acc: 100.0, f1: 100.0, r: 0.8365524697094123
06/02/2019 01:32:28 step: 4183, epoch: 126, batch: 24, loss: 0.007631182670593262, acc: 100.0, f1: 100.0, r: 0.6799623496453203
06/02/2019 01:32:29 step: 4188, epoch: 126, batch: 29, loss: 0.01294725388288498, acc: 98.4375, f1: 99.07692307692308, r: 0.5767285847228875
06/02/2019 01:32:29 *** evaluating ***
06/02/2019 01:32:29 step: 127, epoch: 126, acc: 57.26495726495726, f1: 27.85289994096812, r: 0.3337514627225416
06/02/2019 01:32:29 *** epoch: 128 ***
06/02/2019 01:32:29 *** training ***
06/02/2019 01:32:30 step: 4196, epoch: 127, batch: 4, loss: 0.0034006163477897644, acc: 100.0, f1: 100.0, r: 0.7882214068272827
06/02/2019 01:32:31 step: 4201, epoch: 127, batch: 9, loss: 0.0017406120896339417, acc: 100.0, f1: 100.0, r: 0.6682364005365098
06/02/2019 01:32:32 step: 4206, epoch: 127, batch: 14, loss: 0.01019313931465149, acc: 100.0, f1: 100.0, r: 0.7710375094371685
06/02/2019 01:32:33 step: 4211, epoch: 127, batch: 19, loss: 0.004180051386356354, acc: 100.0, f1: 100.0, r: 0.6737646372788559
06/02/2019 01:32:34 step: 4216, epoch: 127, batch: 24, loss: 0.015301123261451721, acc: 98.4375, f1: 98.80174291938998, r: 0.8570360581430743
06/02/2019 01:32:34 step: 4221, epoch: 127, batch: 29, loss: 0.01657097041606903, acc: 98.4375, f1: 99.37146448774355, r: 0.7926771901512951
06/02/2019 01:32:35 *** evaluating ***
06/02/2019 01:32:35 step: 128, epoch: 127, acc: 55.55555555555556, f1: 29.00504814720501, r: 0.32795248232387797
06/02/2019 01:32:35 *** epoch: 129 ***
06/02/2019 01:32:35 *** training ***
06/02/2019 01:32:36 step: 4229, epoch: 128, batch: 4, loss: 0.0014157295227050781, acc: 100.0, f1: 100.0, r: 0.7392883024349216
06/02/2019 01:32:37 step: 4234, epoch: 128, batch: 9, loss: 0.0031363442540168762, acc: 100.0, f1: 100.0, r: 0.8436244913159295
06/02/2019 01:32:38 step: 4239, epoch: 128, batch: 14, loss: 0.0031241029500961304, acc: 100.0, f1: 100.0, r: 0.6741343423069291
06/02/2019 01:32:39 step: 4244, epoch: 128, batch: 19, loss: 0.0034924522042274475, acc: 100.0, f1: 100.0, r: 0.707986563302727
06/02/2019 01:32:40 step: 4249, epoch: 128, batch: 24, loss: 0.0024777501821517944, acc: 100.0, f1: 100.0, r: 0.8088444542067723
06/02/2019 01:32:40 step: 4254, epoch: 128, batch: 29, loss: 0.0037244781851768494, acc: 100.0, f1: 100.0, r: 0.7122541912786762
06/02/2019 01:32:41 *** evaluating ***
06/02/2019 01:32:41 step: 129, epoch: 128, acc: 58.119658119658126, f1: 28.980184091374905, r: 0.34033389374472306
06/02/2019 01:32:41 *** epoch: 130 ***
06/02/2019 01:32:41 *** training ***
06/02/2019 01:32:42 step: 4262, epoch: 129, batch: 4, loss: 0.0010502785444259644, acc: 100.0, f1: 100.0, r: 0.7704305615598266
06/02/2019 01:32:43 step: 4267, epoch: 129, batch: 9, loss: 0.0019578784704208374, acc: 100.0, f1: 100.0, r: 0.7074458202937082
06/02/2019 01:32:44 step: 4272, epoch: 129, batch: 14, loss: 0.0037768781185150146, acc: 100.0, f1: 100.0, r: 0.8412857108147761
06/02/2019 01:32:44 step: 4277, epoch: 129, batch: 19, loss: 0.0027635619044303894, acc: 100.0, f1: 100.0, r: 0.759075095274479
06/02/2019 01:32:45 step: 4282, epoch: 129, batch: 24, loss: 0.002856343984603882, acc: 100.0, f1: 100.0, r: 0.7578236150559883
06/02/2019 01:32:46 step: 4287, epoch: 129, batch: 29, loss: 0.0026352182030677795, acc: 100.0, f1: 100.0, r: 0.7674999420980309
06/02/2019 01:32:46 *** evaluating ***
06/02/2019 01:32:47 step: 130, epoch: 129, acc: 56.41025641025641, f1: 28.343723273577716, r: 0.34828430857830467
06/02/2019 01:32:47 *** epoch: 131 ***
06/02/2019 01:32:47 *** training ***
06/02/2019 01:32:48 step: 4295, epoch: 130, batch: 4, loss: 0.04504265636205673, acc: 98.4375, f1: 99.15824915824916, r: 0.793229074740004
06/02/2019 01:32:48 step: 4300, epoch: 130, batch: 9, loss: 0.005836799740791321, acc: 100.0, f1: 100.0, r: 0.8043028646768309
06/02/2019 01:32:49 step: 4305, epoch: 130, batch: 14, loss: 0.0020998716354370117, acc: 100.0, f1: 100.0, r: 0.7147514099542432
06/02/2019 01:32:50 step: 4310, epoch: 130, batch: 19, loss: 0.0075077638030052185, acc: 100.0, f1: 100.0, r: 0.7074517148833457
06/02/2019 01:32:51 step: 4315, epoch: 130, batch: 24, loss: 0.006286926567554474, acc: 100.0, f1: 100.0, r: 0.8378416669076834
06/02/2019 01:32:52 step: 4320, epoch: 130, batch: 29, loss: 0.003805294632911682, acc: 100.0, f1: 100.0, r: 0.8240972353541571
06/02/2019 01:32:52 *** evaluating ***
06/02/2019 01:32:53 step: 131, epoch: 130, acc: 57.692307692307686, f1: 27.970518193423423, r: 0.3421124198178914
06/02/2019 01:32:53 *** epoch: 132 ***
06/02/2019 01:32:53 *** training ***
06/02/2019 01:32:54 step: 4328, epoch: 131, batch: 4, loss: 0.0035531893372535706, acc: 100.0, f1: 100.0, r: 0.7136766271620147
06/02/2019 01:32:54 step: 4333, epoch: 131, batch: 9, loss: 0.0037064626812934875, acc: 100.0, f1: 100.0, r: 0.7081269065331568
06/02/2019 01:32:55 step: 4338, epoch: 131, batch: 14, loss: 0.0029102861881256104, acc: 100.0, f1: 100.0, r: 0.8018986500800805
06/02/2019 01:32:56 step: 4343, epoch: 131, batch: 19, loss: 0.002064153552055359, acc: 100.0, f1: 100.0, r: 0.8340902796623837
06/02/2019 01:32:57 step: 4348, epoch: 131, batch: 24, loss: 0.005853839218616486, acc: 100.0, f1: 100.0, r: 0.7922725672171087
06/02/2019 01:32:58 step: 4353, epoch: 131, batch: 29, loss: 0.005147326737642288, acc: 100.0, f1: 100.0, r: 0.7633472626524005
06/02/2019 01:32:58 *** evaluating ***
06/02/2019 01:32:59 step: 132, epoch: 131, acc: 56.837606837606835, f1: 28.24876724320702, r: 0.33847287878271076
06/02/2019 01:32:59 *** epoch: 133 ***
06/02/2019 01:32:59 *** training ***
06/02/2019 01:32:59 step: 4361, epoch: 132, batch: 4, loss: 0.004397742450237274, acc: 100.0, f1: 100.0, r: 0.7311712278645233
06/02/2019 01:33:00 step: 4366, epoch: 132, batch: 9, loss: 0.002071589231491089, acc: 100.0, f1: 100.0, r: 0.8271157204618276
06/02/2019 01:33:01 step: 4371, epoch: 132, batch: 14, loss: 0.005450315773487091, acc: 100.0, f1: 100.0, r: 0.7864144428705817
06/02/2019 01:33:02 step: 4376, epoch: 132, batch: 19, loss: 0.004142548888921738, acc: 100.0, f1: 100.0, r: 0.7162362922109249
06/02/2019 01:33:03 step: 4381, epoch: 132, batch: 24, loss: 0.013708986341953278, acc: 100.0, f1: 100.0, r: 0.7913644230952589
06/02/2019 01:33:04 step: 4386, epoch: 132, batch: 29, loss: 0.007433094084262848, acc: 100.0, f1: 100.0, r: 0.6778307582385811
06/02/2019 01:33:04 *** evaluating ***
06/02/2019 01:33:04 step: 133, epoch: 132, acc: 58.119658119658126, f1: 29.440280162470025, r: 0.3521700529606533
06/02/2019 01:33:04 *** epoch: 134 ***
06/02/2019 01:33:04 *** training ***
06/02/2019 01:33:05 step: 4394, epoch: 133, batch: 4, loss: 0.006438054144382477, acc: 100.0, f1: 100.0, r: 0.6802155952904234
06/02/2019 01:33:06 step: 4399, epoch: 133, batch: 9, loss: 0.006207458674907684, acc: 100.0, f1: 100.0, r: 0.7723894363035732
06/02/2019 01:33:07 step: 4404, epoch: 133, batch: 14, loss: 0.002984762191772461, acc: 100.0, f1: 100.0, r: 0.7619339459386689
06/02/2019 01:33:08 step: 4409, epoch: 133, batch: 19, loss: 0.004849620163440704, acc: 100.0, f1: 100.0, r: 0.7959161063289497
06/02/2019 01:33:09 step: 4414, epoch: 133, batch: 24, loss: 0.008581660687923431, acc: 100.0, f1: 100.0, r: 0.8120239348947103
06/02/2019 01:33:10 step: 4419, epoch: 133, batch: 29, loss: 0.001720227301120758, acc: 100.0, f1: 100.0, r: 0.7225066763666451
06/02/2019 01:33:10 *** evaluating ***
06/02/2019 01:33:10 step: 134, epoch: 133, acc: 57.692307692307686, f1: 30.11956887532622, r: 0.3430940379389684
06/02/2019 01:33:10 *** epoch: 135 ***
06/02/2019 01:33:10 *** training ***
06/02/2019 01:33:11 step: 4427, epoch: 134, batch: 4, loss: 0.010140679776668549, acc: 100.0, f1: 100.0, r: 0.6203916619004404
06/02/2019 01:33:12 step: 4432, epoch: 134, batch: 9, loss: 0.005926251411437988, acc: 100.0, f1: 100.0, r: 0.7654337375022833
06/02/2019 01:33:13 step: 4437, epoch: 134, batch: 14, loss: 0.006012823432683945, acc: 100.0, f1: 100.0, r: 0.693804057288798
06/02/2019 01:33:14 step: 4442, epoch: 134, batch: 19, loss: 0.002801060676574707, acc: 100.0, f1: 100.0, r: 0.7303195779424102
06/02/2019 01:33:15 step: 4447, epoch: 134, batch: 24, loss: 0.0044631510972976685, acc: 100.0, f1: 100.0, r: 0.6637128221707794
06/02/2019 01:33:15 step: 4452, epoch: 134, batch: 29, loss: 0.004444051533937454, acc: 100.0, f1: 100.0, r: 0.7711160639583726
06/02/2019 01:33:16 *** evaluating ***
06/02/2019 01:33:16 step: 135, epoch: 134, acc: 58.119658119658126, f1: 30.0377216144851, r: 0.3504546209437924
06/02/2019 01:33:16 *** epoch: 136 ***
06/02/2019 01:33:16 *** training ***
06/02/2019 01:33:17 step: 4460, epoch: 135, batch: 4, loss: 0.002774342894554138, acc: 100.0, f1: 100.0, r: 0.6903221376175698
06/02/2019 01:33:18 step: 4465, epoch: 135, batch: 9, loss: 0.003987118601799011, acc: 100.0, f1: 100.0, r: 0.5959564701840079
06/02/2019 01:33:19 step: 4470, epoch: 135, batch: 14, loss: 0.0023376047611236572, acc: 100.0, f1: 100.0, r: 0.6728766968913804
06/02/2019 01:33:20 step: 4475, epoch: 135, batch: 19, loss: 0.006311211735010147, acc: 100.0, f1: 100.0, r: 0.613494659967511
06/02/2019 01:33:21 step: 4480, epoch: 135, batch: 24, loss: 0.005267634987831116, acc: 100.0, f1: 100.0, r: 0.7115559679688763
06/02/2019 01:33:21 step: 4485, epoch: 135, batch: 29, loss: 0.004803366959095001, acc: 100.0, f1: 100.0, r: 0.8139334009379358
06/02/2019 01:33:22 *** evaluating ***
06/02/2019 01:33:22 step: 136, epoch: 135, acc: 58.54700854700855, f1: 29.70617627789126, r: 0.3475663160859666
06/02/2019 01:33:22 *** epoch: 137 ***
06/02/2019 01:33:22 *** training ***
06/02/2019 01:33:23 step: 4493, epoch: 136, batch: 4, loss: 0.01006612554192543, acc: 100.0, f1: 100.0, r: 0.712530377223263
06/02/2019 01:33:24 step: 4498, epoch: 136, batch: 9, loss: 0.0022440850734710693, acc: 100.0, f1: 100.0, r: 0.7101999655511928
06/02/2019 01:33:25 step: 4503, epoch: 136, batch: 14, loss: 0.00197022408246994, acc: 100.0, f1: 100.0, r: 0.7180859750751439
06/02/2019 01:33:26 step: 4508, epoch: 136, batch: 19, loss: 0.010607365518808365, acc: 100.0, f1: 100.0, r: 0.7041877063221499
06/02/2019 01:33:27 step: 4513, epoch: 136, batch: 24, loss: 0.005473442375659943, acc: 100.0, f1: 100.0, r: 0.8142420055316365
06/02/2019 01:33:27 step: 4518, epoch: 136, batch: 29, loss: 0.011117443442344666, acc: 100.0, f1: 100.0, r: 0.8578297228377412
06/02/2019 01:33:28 *** evaluating ***
06/02/2019 01:33:28 step: 137, epoch: 136, acc: 58.119658119658126, f1: 29.96021470461503, r: 0.3437902400080618
06/02/2019 01:33:28 *** epoch: 138 ***
06/02/2019 01:33:28 *** training ***
06/02/2019 01:33:29 step: 4526, epoch: 137, batch: 4, loss: 0.000893227756023407, acc: 100.0, f1: 100.0, r: 0.7499184461437393
06/02/2019 01:33:30 step: 4531, epoch: 137, batch: 9, loss: 0.010871164500713348, acc: 100.0, f1: 100.0, r: 0.8013818906440378
06/02/2019 01:33:31 step: 4536, epoch: 137, batch: 14, loss: 0.011529117822647095, acc: 100.0, f1: 100.0, r: 0.807018778934837
06/02/2019 01:33:32 step: 4541, epoch: 137, batch: 19, loss: 0.007874630391597748, acc: 100.0, f1: 100.0, r: 0.8402475080850738
06/02/2019 01:33:32 step: 4546, epoch: 137, batch: 24, loss: 0.002236761152744293, acc: 100.0, f1: 100.0, r: 0.7906432178614599
06/02/2019 01:33:33 step: 4551, epoch: 137, batch: 29, loss: 0.004156708717346191, acc: 100.0, f1: 100.0, r: 0.7026426598092934
06/02/2019 01:33:34 *** evaluating ***
06/02/2019 01:33:34 step: 138, epoch: 137, acc: 57.692307692307686, f1: 29.75477258265237, r: 0.34429485483378736
06/02/2019 01:33:34 *** epoch: 139 ***
06/02/2019 01:33:34 *** training ***
06/02/2019 01:33:35 step: 4559, epoch: 138, batch: 4, loss: 0.001337401568889618, acc: 100.0, f1: 100.0, r: 0.8448982620685849
06/02/2019 01:33:36 step: 4564, epoch: 138, batch: 9, loss: 0.004428155720233917, acc: 100.0, f1: 100.0, r: 0.7944968194043535
06/02/2019 01:33:37 step: 4569, epoch: 138, batch: 14, loss: 0.0022355318069458008, acc: 100.0, f1: 100.0, r: 0.7923787771002937
06/02/2019 01:33:37 step: 4574, epoch: 138, batch: 19, loss: 0.0034883469343185425, acc: 100.0, f1: 100.0, r: 0.7962985132173386
06/02/2019 01:33:38 step: 4579, epoch: 138, batch: 24, loss: 0.0042213574051856995, acc: 100.0, f1: 100.0, r: 0.7660615975511095
06/02/2019 01:33:39 step: 4584, epoch: 138, batch: 29, loss: 0.0048195235431194305, acc: 100.0, f1: 100.0, r: 0.8310292872361467
06/02/2019 01:33:40 *** evaluating ***
06/02/2019 01:33:40 step: 139, epoch: 138, acc: 55.98290598290598, f1: 29.057629107596256, r: 0.34305199014176824
06/02/2019 01:33:40 *** epoch: 140 ***
06/02/2019 01:33:40 *** training ***
06/02/2019 01:33:41 step: 4592, epoch: 139, batch: 4, loss: 0.0009580478072166443, acc: 100.0, f1: 100.0, r: 0.7637492777737063
06/02/2019 01:33:42 step: 4597, epoch: 139, batch: 9, loss: 0.010135561227798462, acc: 100.0, f1: 100.0, r: 0.7527853940536441
06/02/2019 01:33:43 step: 4602, epoch: 139, batch: 14, loss: 0.003797568380832672, acc: 100.0, f1: 100.0, r: 0.721625767431153
06/02/2019 01:33:44 step: 4607, epoch: 139, batch: 19, loss: 0.0016704574227333069, acc: 100.0, f1: 100.0, r: 0.7779417990478681
06/02/2019 01:33:44 step: 4612, epoch: 139, batch: 24, loss: 0.0025304406881332397, acc: 100.0, f1: 100.0, r: 0.6578500844941502
06/02/2019 01:33:45 step: 4617, epoch: 139, batch: 29, loss: 0.0040971264243125916, acc: 100.0, f1: 100.0, r: 0.7755935902203361
06/02/2019 01:33:46 *** evaluating ***
06/02/2019 01:33:46 step: 140, epoch: 139, acc: 58.97435897435898, f1: 30.199600236364944, r: 0.3489340815667063
06/02/2019 01:33:46 *** epoch: 141 ***
06/02/2019 01:33:46 *** training ***
06/02/2019 01:33:47 step: 4625, epoch: 140, batch: 4, loss: 0.007086955010890961, acc: 100.0, f1: 100.0, r: 0.753525093063719
06/02/2019 01:33:48 step: 4630, epoch: 140, batch: 9, loss: 0.0067196376621723175, acc: 100.0, f1: 100.0, r: 0.7058497074945334
06/02/2019 01:33:48 step: 4635, epoch: 140, batch: 14, loss: 0.0012206435203552246, acc: 100.0, f1: 100.0, r: 0.8062999025535051
06/02/2019 01:33:49 step: 4640, epoch: 140, batch: 19, loss: 0.002364523708820343, acc: 100.0, f1: 100.0, r: 0.7018570272555801
06/02/2019 01:33:50 step: 4645, epoch: 140, batch: 24, loss: 0.0029746219515800476, acc: 100.0, f1: 100.0, r: 0.7133571714456366
06/02/2019 01:33:51 step: 4650, epoch: 140, batch: 29, loss: 0.003640376031398773, acc: 100.0, f1: 100.0, r: 0.7085251621732682
06/02/2019 01:33:51 *** evaluating ***
06/02/2019 01:33:52 step: 141, epoch: 140, acc: 58.119658119658126, f1: 29.892769607843135, r: 0.35345014662430163
06/02/2019 01:33:52 *** epoch: 142 ***
06/02/2019 01:33:52 *** training ***
06/02/2019 01:33:52 step: 4658, epoch: 141, batch: 4, loss: 0.004523314535617828, acc: 100.0, f1: 100.0, r: 0.6925890600909567
06/02/2019 01:33:53 step: 4663, epoch: 141, batch: 9, loss: 0.002101130783557892, acc: 100.0, f1: 100.0, r: 0.772729323780469
06/02/2019 01:33:54 step: 4668, epoch: 141, batch: 14, loss: 0.0014998838305473328, acc: 100.0, f1: 100.0, r: 0.7271808059494886
06/02/2019 01:33:55 step: 4673, epoch: 141, batch: 19, loss: 0.022633396089076996, acc: 98.4375, f1: 98.90070921985816, r: 0.7955151565171674
06/02/2019 01:33:56 step: 4678, epoch: 141, batch: 24, loss: 0.002424784004688263, acc: 100.0, f1: 100.0, r: 0.7340428011138922
06/02/2019 01:33:57 step: 4683, epoch: 141, batch: 29, loss: 0.006596464663743973, acc: 100.0, f1: 100.0, r: 0.7176297839518893
06/02/2019 01:33:57 *** evaluating ***
06/02/2019 01:33:57 step: 142, epoch: 141, acc: 57.692307692307686, f1: 27.63135090487262, r: 0.34597667746152677
06/02/2019 01:33:57 *** epoch: 143 ***
06/02/2019 01:33:57 *** training ***
06/02/2019 01:33:58 step: 4691, epoch: 142, batch: 4, loss: 0.002720773220062256, acc: 100.0, f1: 100.0, r: 0.7676370796601971
06/02/2019 01:33:59 step: 4696, epoch: 142, batch: 9, loss: 0.0014196783304214478, acc: 100.0, f1: 100.0, r: 0.79000139221829
06/02/2019 01:34:00 step: 4701, epoch: 142, batch: 14, loss: 0.0039858147501945496, acc: 100.0, f1: 100.0, r: 0.8298396020675829
06/02/2019 01:34:01 step: 4706, epoch: 142, batch: 19, loss: 0.005494758486747742, acc: 100.0, f1: 100.0, r: 0.7540099196285539
06/02/2019 01:34:02 step: 4711, epoch: 142, batch: 24, loss: 0.001842409372329712, acc: 100.0, f1: 100.0, r: 0.7654268233599227
06/02/2019 01:34:03 step: 4716, epoch: 142, batch: 29, loss: 0.008547589182853699, acc: 100.0, f1: 100.0, r: 0.8470179272283861
06/02/2019 01:34:03 *** evaluating ***
06/02/2019 01:34:03 step: 143, epoch: 142, acc: 57.26495726495726, f1: 27.801405371448475, r: 0.34677165328761644
06/02/2019 01:34:03 *** epoch: 144 ***
06/02/2019 01:34:03 *** training ***
06/02/2019 01:34:04 step: 4724, epoch: 143, batch: 4, loss: 0.011999033391475677, acc: 100.0, f1: 100.0, r: 0.7764924144613135
06/02/2019 01:34:05 step: 4729, epoch: 143, batch: 9, loss: 0.003382645547389984, acc: 100.0, f1: 100.0, r: 0.7225922038572589
06/02/2019 01:34:06 step: 4734, epoch: 143, batch: 14, loss: 0.010072343051433563, acc: 100.0, f1: 100.0, r: 0.6621401999944789
06/02/2019 01:34:07 step: 4739, epoch: 143, batch: 19, loss: 0.003283753991127014, acc: 100.0, f1: 100.0, r: 0.8283494862959779
06/02/2019 01:34:08 step: 4744, epoch: 143, batch: 24, loss: 0.01579747349023819, acc: 98.4375, f1: 96.52173913043478, r: 0.7052490760127622
06/02/2019 01:34:08 step: 4749, epoch: 143, batch: 29, loss: 0.002657376229763031, acc: 100.0, f1: 100.0, r: 0.715447472095269
06/02/2019 01:34:09 *** evaluating ***
06/02/2019 01:34:09 step: 144, epoch: 143, acc: 56.41025641025641, f1: 28.473481766335002, r: 0.3473441314374905
06/02/2019 01:34:09 *** epoch: 145 ***
06/02/2019 01:34:09 *** training ***
06/02/2019 01:34:10 step: 4757, epoch: 144, batch: 4, loss: 0.0022256597876548767, acc: 100.0, f1: 100.0, r: 0.7617017239193881
06/02/2019 01:34:11 step: 4762, epoch: 144, batch: 9, loss: 0.002277277410030365, acc: 100.0, f1: 100.0, r: 0.6984201177997519
06/02/2019 01:34:12 step: 4767, epoch: 144, batch: 14, loss: 0.0016269609332084656, acc: 100.0, f1: 100.0, r: 0.8334449449470558
06/02/2019 01:34:12 step: 4772, epoch: 144, batch: 19, loss: 0.007044337689876556, acc: 100.0, f1: 100.0, r: 0.6315634807195403
06/02/2019 01:34:13 step: 4777, epoch: 144, batch: 24, loss: 0.004032664000988007, acc: 100.0, f1: 100.0, r: 0.6764289371797455
06/02/2019 01:34:14 step: 4782, epoch: 144, batch: 29, loss: 0.003054112195968628, acc: 100.0, f1: 100.0, r: 0.7708180540840688
06/02/2019 01:34:14 *** evaluating ***
06/02/2019 01:34:15 step: 145, epoch: 144, acc: 57.26495726495726, f1: 28.748734817813766, r: 0.34567433068769166
06/02/2019 01:34:15 *** epoch: 146 ***
06/02/2019 01:34:15 *** training ***
06/02/2019 01:34:16 step: 4790, epoch: 145, batch: 4, loss: 0.012614183127880096, acc: 100.0, f1: 100.0, r: 0.7311074410862808
06/02/2019 01:34:17 step: 4795, epoch: 145, batch: 9, loss: 0.010196641087532043, acc: 100.0, f1: 100.0, r: 0.6487293000988924
06/02/2019 01:34:17 step: 4800, epoch: 145, batch: 14, loss: 0.0044456347823143005, acc: 100.0, f1: 100.0, r: 0.7116113411318815
06/02/2019 01:34:18 step: 4805, epoch: 145, batch: 19, loss: 0.02091459557414055, acc: 100.0, f1: 100.0, r: 0.7884545366282998
06/02/2019 01:34:19 step: 4810, epoch: 145, batch: 24, loss: 0.016242533922195435, acc: 98.4375, f1: 99.17516324894031, r: 0.755603756616915
06/02/2019 01:34:20 step: 4815, epoch: 145, batch: 29, loss: 0.0041264817118644714, acc: 100.0, f1: 100.0, r: 0.6476639026291172
06/02/2019 01:34:20 *** evaluating ***
06/02/2019 01:34:21 step: 146, epoch: 145, acc: 57.26495726495726, f1: 29.62839594217105, r: 0.34279676039075674
06/02/2019 01:34:21 *** epoch: 147 ***
06/02/2019 01:34:21 *** training ***
06/02/2019 01:34:22 step: 4823, epoch: 146, batch: 4, loss: 0.012976415455341339, acc: 100.0, f1: 100.0, r: 0.7609110429559727
06/02/2019 01:34:22 step: 4828, epoch: 146, batch: 9, loss: 0.0032842084765434265, acc: 100.0, f1: 100.0, r: 0.7012643431440013
06/02/2019 01:34:23 step: 4833, epoch: 146, batch: 14, loss: 0.0010903030633926392, acc: 100.0, f1: 100.0, r: 0.6657941996359386
06/02/2019 01:34:24 step: 4838, epoch: 146, batch: 19, loss: 0.005585759878158569, acc: 100.0, f1: 100.0, r: 0.7748264476057983
06/02/2019 01:34:25 step: 4843, epoch: 146, batch: 24, loss: 0.002746671438217163, acc: 100.0, f1: 100.0, r: 0.677493795605226
06/02/2019 01:34:26 step: 4848, epoch: 146, batch: 29, loss: 0.002369120717048645, acc: 100.0, f1: 100.0, r: 0.840980841398346
06/02/2019 01:34:26 *** evaluating ***
06/02/2019 01:34:27 step: 147, epoch: 146, acc: 57.26495726495726, f1: 29.191900059661823, r: 0.34117715374459917
06/02/2019 01:34:27 *** epoch: 148 ***
06/02/2019 01:34:27 *** training ***
06/02/2019 01:34:27 step: 4856, epoch: 147, batch: 4, loss: 0.004871763288974762, acc: 100.0, f1: 100.0, r: 0.8412033743277098
06/02/2019 01:34:28 step: 4861, epoch: 147, batch: 9, loss: 0.0013755187392234802, acc: 100.0, f1: 100.0, r: 0.6581771281164487
06/02/2019 01:34:29 step: 4866, epoch: 147, batch: 14, loss: 0.005868125706911087, acc: 100.0, f1: 100.0, r: 0.7959070502501058
06/02/2019 01:34:30 step: 4871, epoch: 147, batch: 19, loss: 0.001255914568901062, acc: 100.0, f1: 100.0, r: 0.7552580966836081
06/02/2019 01:34:31 step: 4876, epoch: 147, batch: 24, loss: 0.0040289536118507385, acc: 100.0, f1: 100.0, r: 0.8318804145420958
06/02/2019 01:34:32 step: 4881, epoch: 147, batch: 29, loss: 0.0029054731130599976, acc: 100.0, f1: 100.0, r: 0.7125298583665708
06/02/2019 01:34:32 *** evaluating ***
06/02/2019 01:34:32 step: 148, epoch: 147, acc: 56.837606837606835, f1: 28.446930577901462, r: 0.3386797995470383
06/02/2019 01:34:32 *** epoch: 149 ***
06/02/2019 01:34:32 *** training ***
06/02/2019 01:34:33 step: 4889, epoch: 148, batch: 4, loss: 0.0010622963309288025, acc: 100.0, f1: 100.0, r: 0.8571508798306363
06/02/2019 01:34:34 step: 4894, epoch: 148, batch: 9, loss: 0.006495296955108643, acc: 100.0, f1: 100.0, r: 0.7857538103637924
06/02/2019 01:34:35 step: 4899, epoch: 148, batch: 14, loss: 0.006643064320087433, acc: 100.0, f1: 100.0, r: 0.6811901678890973
06/02/2019 01:34:36 step: 4904, epoch: 148, batch: 19, loss: 0.0018292516469955444, acc: 100.0, f1: 100.0, r: 0.7880259688529967
06/02/2019 01:34:37 step: 4909, epoch: 148, batch: 24, loss: 0.001459360122680664, acc: 100.0, f1: 100.0, r: 0.7877948270757089
06/02/2019 01:34:37 step: 4914, epoch: 148, batch: 29, loss: 0.012555055320262909, acc: 100.0, f1: 100.0, r: 0.5883376637652857
06/02/2019 01:34:38 *** evaluating ***
06/02/2019 01:34:38 step: 149, epoch: 148, acc: 56.837606837606835, f1: 27.469583082710187, r: 0.3398113495779414
06/02/2019 01:34:38 *** epoch: 150 ***
06/02/2019 01:34:38 *** training ***
06/02/2019 01:34:39 step: 4922, epoch: 149, batch: 4, loss: 0.007758870720863342, acc: 100.0, f1: 100.0, r: 0.6389386903328558
06/02/2019 01:34:40 step: 4927, epoch: 149, batch: 9, loss: 0.003081604838371277, acc: 100.0, f1: 100.0, r: 0.8072354162646778
06/02/2019 01:34:41 step: 4932, epoch: 149, batch: 14, loss: 0.0031041279435157776, acc: 100.0, f1: 100.0, r: 0.8087169206053892
06/02/2019 01:34:42 step: 4937, epoch: 149, batch: 19, loss: 0.0017317980527877808, acc: 100.0, f1: 100.0, r: 0.6930887749752653
06/02/2019 01:34:43 step: 4942, epoch: 149, batch: 24, loss: 0.0012419596314430237, acc: 100.0, f1: 100.0, r: 0.7530588349439288
06/02/2019 01:34:43 step: 4947, epoch: 149, batch: 29, loss: 0.006971493363380432, acc: 100.0, f1: 100.0, r: 0.7034985354312853
06/02/2019 01:34:44 *** evaluating ***
06/02/2019 01:34:44 step: 150, epoch: 149, acc: 57.26495726495726, f1: 28.81741680479408, r: 0.3323428965834996
06/02/2019 01:34:44 *** epoch: 151 ***
06/02/2019 01:34:44 *** training ***
06/02/2019 01:34:45 step: 4955, epoch: 150, batch: 4, loss: 0.0011693984270095825, acc: 100.0, f1: 100.0, r: 0.6201281940487933
06/02/2019 01:34:46 step: 4960, epoch: 150, batch: 9, loss: 0.0012832358479499817, acc: 100.0, f1: 100.0, r: 0.7551253227263272
06/02/2019 01:34:47 step: 4965, epoch: 150, batch: 14, loss: 0.0010985732078552246, acc: 100.0, f1: 100.0, r: 0.77316412373276
06/02/2019 01:34:48 step: 4970, epoch: 150, batch: 19, loss: 0.0032484233379364014, acc: 100.0, f1: 100.0, r: 0.7420383825854004
06/02/2019 01:34:48 step: 4975, epoch: 150, batch: 24, loss: 0.019353099167346954, acc: 98.4375, f1: 99.19056429232191, r: 0.8261576646822804
06/02/2019 01:34:49 step: 4980, epoch: 150, batch: 29, loss: 0.0013599097728729248, acc: 100.0, f1: 100.0, r: 0.7812729832839685
06/02/2019 01:34:50 *** evaluating ***
06/02/2019 01:34:50 step: 151, epoch: 150, acc: 55.12820512820513, f1: 27.7023582581302, r: 0.329339103945031
06/02/2019 01:34:50 *** epoch: 152 ***
06/02/2019 01:34:50 *** training ***
06/02/2019 01:34:51 step: 4988, epoch: 151, batch: 4, loss: 0.008341461420059204, acc: 100.0, f1: 100.0, r: 0.6522932593185812
06/02/2019 01:34:52 step: 4993, epoch: 151, batch: 9, loss: 0.0021411776542663574, acc: 100.0, f1: 100.0, r: 0.7175806722577047
06/02/2019 01:34:53 step: 4998, epoch: 151, batch: 14, loss: 0.0036604441702365875, acc: 100.0, f1: 100.0, r: 0.7652844214736184
06/02/2019 01:34:54 step: 5003, epoch: 151, batch: 19, loss: 0.0031979158520698547, acc: 100.0, f1: 100.0, r: 0.6998577301909891
06/02/2019 01:34:54 step: 5008, epoch: 151, batch: 24, loss: 0.0028972551226615906, acc: 100.0, f1: 100.0, r: 0.6992769402373796
06/02/2019 01:34:55 step: 5013, epoch: 151, batch: 29, loss: 0.0027343928813934326, acc: 100.0, f1: 100.0, r: 0.8106184250882774
06/02/2019 01:34:56 *** evaluating ***
06/02/2019 01:34:56 step: 152, epoch: 151, acc: 56.837606837606835, f1: 28.051837165566674, r: 0.3450426071019463
06/02/2019 01:34:56 *** epoch: 153 ***
06/02/2019 01:34:56 *** training ***
06/02/2019 01:34:57 step: 5021, epoch: 152, batch: 4, loss: 0.0025833845138549805, acc: 100.0, f1: 100.0, r: 0.6939669776518888
06/02/2019 01:34:58 step: 5026, epoch: 152, batch: 9, loss: 0.00111456960439682, acc: 100.0, f1: 100.0, r: 0.7529448418336168
06/02/2019 01:34:58 step: 5031, epoch: 152, batch: 14, loss: 0.0036096572875976562, acc: 100.0, f1: 100.0, r: 0.7676506816398523
06/02/2019 01:34:59 step: 5036, epoch: 152, batch: 19, loss: 0.0027866214513778687, acc: 100.0, f1: 100.0, r: 0.6356719422060143
06/02/2019 01:35:00 step: 5041, epoch: 152, batch: 24, loss: 0.011520631611347198, acc: 100.0, f1: 100.0, r: 0.7194362307024086
06/02/2019 01:35:01 step: 5046, epoch: 152, batch: 29, loss: 0.0016981959342956543, acc: 100.0, f1: 100.0, r: 0.7801212998262099
06/02/2019 01:35:01 *** evaluating ***
06/02/2019 01:35:02 step: 153, epoch: 152, acc: 58.97435897435898, f1: 29.623768004547678, r: 0.33542370324630705
06/02/2019 01:35:02 *** epoch: 154 ***
06/02/2019 01:35:02 *** training ***
06/02/2019 01:35:03 step: 5054, epoch: 153, batch: 4, loss: 0.011997886002063751, acc: 100.0, f1: 100.0, r: 0.8101496270300439
06/02/2019 01:35:03 step: 5059, epoch: 153, batch: 9, loss: 0.004142336547374725, acc: 100.0, f1: 100.0, r: 0.7169344832790978
06/02/2019 01:35:04 step: 5064, epoch: 153, batch: 14, loss: 0.0021050944924354553, acc: 100.0, f1: 100.0, r: 0.7015641101850024
06/02/2019 01:35:05 step: 5069, epoch: 153, batch: 19, loss: 0.004633165895938873, acc: 100.0, f1: 100.0, r: 0.68178625582238
06/02/2019 01:35:06 step: 5074, epoch: 153, batch: 24, loss: 0.0021502599120140076, acc: 100.0, f1: 100.0, r: 0.7926488550568354
06/02/2019 01:35:07 step: 5079, epoch: 153, batch: 29, loss: 0.001510806381702423, acc: 100.0, f1: 100.0, r: 0.7814667960351225
06/02/2019 01:35:07 *** evaluating ***
06/02/2019 01:35:08 step: 154, epoch: 153, acc: 57.26495726495726, f1: 29.161619121297406, r: 0.3380035370325099
06/02/2019 01:35:08 *** epoch: 155 ***
06/02/2019 01:35:08 *** training ***
06/02/2019 01:35:08 step: 5087, epoch: 154, batch: 4, loss: 0.004365637898445129, acc: 100.0, f1: 100.0, r: 0.6173387202244753
06/02/2019 01:35:09 step: 5092, epoch: 154, batch: 9, loss: 0.007403053343296051, acc: 100.0, f1: 100.0, r: 0.7651969965398573
06/02/2019 01:35:10 step: 5097, epoch: 154, batch: 14, loss: 0.0013307183980941772, acc: 100.0, f1: 100.0, r: 0.8057616829686026
06/02/2019 01:35:11 step: 5102, epoch: 154, batch: 19, loss: 0.007134832441806793, acc: 100.0, f1: 100.0, r: 0.7228008448736352
06/02/2019 01:35:12 step: 5107, epoch: 154, batch: 24, loss: 0.003829769790172577, acc: 100.0, f1: 100.0, r: 0.6848096175052697
06/02/2019 01:35:13 step: 5112, epoch: 154, batch: 29, loss: 0.0011090189218521118, acc: 100.0, f1: 100.0, r: 0.6813407681064917
06/02/2019 01:35:13 *** evaluating ***
06/02/2019 01:35:13 step: 155, epoch: 154, acc: 58.54700854700855, f1: 30.147559614038833, r: 0.3389587802128929
06/02/2019 01:35:13 *** epoch: 156 ***
06/02/2019 01:35:13 *** training ***
06/02/2019 01:35:14 step: 5120, epoch: 155, batch: 4, loss: 0.011576034128665924, acc: 100.0, f1: 100.0, r: 0.7293330960965426
06/02/2019 01:35:15 step: 5125, epoch: 155, batch: 9, loss: 0.008397415280342102, acc: 100.0, f1: 100.0, r: 0.692220937147729
06/02/2019 01:35:16 step: 5130, epoch: 155, batch: 14, loss: 0.005051553249359131, acc: 100.0, f1: 100.0, r: 0.7022078940888293
06/02/2019 01:35:17 step: 5135, epoch: 155, batch: 19, loss: 0.007049448788166046, acc: 100.0, f1: 100.0, r: 0.7031645233756739
06/02/2019 01:35:18 step: 5140, epoch: 155, batch: 24, loss: 0.0020008832216262817, acc: 100.0, f1: 100.0, r: 0.7305747589658076
06/02/2019 01:35:19 step: 5145, epoch: 155, batch: 29, loss: 0.003979980945587158, acc: 100.0, f1: 100.0, r: 0.7355051266193265
06/02/2019 01:35:19 *** evaluating ***
06/02/2019 01:35:19 step: 156, epoch: 155, acc: 56.837606837606835, f1: 28.22010590981499, r: 0.3327478849404254
06/02/2019 01:35:19 *** epoch: 157 ***
06/02/2019 01:35:19 *** training ***
06/02/2019 01:35:20 step: 5153, epoch: 156, batch: 4, loss: 0.0024201050400733948, acc: 100.0, f1: 100.0, r: 0.6942224406099603
06/02/2019 01:35:21 step: 5158, epoch: 156, batch: 9, loss: 0.006396248936653137, acc: 100.0, f1: 100.0, r: 0.7790151340154868
06/02/2019 01:35:22 step: 5163, epoch: 156, batch: 14, loss: 0.0025133267045021057, acc: 100.0, f1: 100.0, r: 0.7129805793508298
06/02/2019 01:35:23 step: 5168, epoch: 156, batch: 19, loss: 0.002491883933544159, acc: 100.0, f1: 100.0, r: 0.8132700185183144
06/02/2019 01:35:23 step: 5173, epoch: 156, batch: 24, loss: 0.0028221383690834045, acc: 100.0, f1: 100.0, r: 0.7543898184960202
06/02/2019 01:35:24 step: 5178, epoch: 156, batch: 29, loss: 0.004826754331588745, acc: 100.0, f1: 100.0, r: 0.7959107523091892
06/02/2019 01:35:25 *** evaluating ***
06/02/2019 01:35:25 step: 157, epoch: 156, acc: 57.692307692307686, f1: 28.63082022818378, r: 0.34043257525020365
06/02/2019 01:35:25 *** epoch: 158 ***
06/02/2019 01:35:25 *** training ***
06/02/2019 01:35:26 step: 5186, epoch: 157, batch: 4, loss: 0.004809573292732239, acc: 100.0, f1: 100.0, r: 0.6775210847313866
06/02/2019 01:35:27 step: 5191, epoch: 157, batch: 9, loss: 0.009334232658147812, acc: 100.0, f1: 100.0, r: 0.7534528719648181
06/02/2019 01:35:28 step: 5196, epoch: 157, batch: 14, loss: 0.002635650336742401, acc: 100.0, f1: 100.0, r: 0.8251247111084858
06/02/2019 01:35:28 step: 5201, epoch: 157, batch: 19, loss: 0.0008747726678848267, acc: 100.0, f1: 100.0, r: 0.6911113488126484
06/02/2019 01:35:29 step: 5206, epoch: 157, batch: 24, loss: 0.0025717541575431824, acc: 100.0, f1: 100.0, r: 0.7173168308973077
06/02/2019 01:35:30 step: 5211, epoch: 157, batch: 29, loss: 0.0005277097225189209, acc: 100.0, f1: 100.0, r: 0.7145319425706695
06/02/2019 01:35:31 *** evaluating ***
06/02/2019 01:35:31 step: 158, epoch: 157, acc: 56.837606837606835, f1: 26.975278055370495, r: 0.3382017170538076
06/02/2019 01:35:31 *** epoch: 159 ***
06/02/2019 01:35:31 *** training ***
06/02/2019 01:35:32 step: 5219, epoch: 158, batch: 4, loss: 0.005537137389183044, acc: 100.0, f1: 100.0, r: 0.7777491202789732
06/02/2019 01:35:33 step: 5224, epoch: 158, batch: 9, loss: 0.00507909432053566, acc: 100.0, f1: 100.0, r: 0.7488508651112318
06/02/2019 01:35:34 step: 5229, epoch: 158, batch: 14, loss: 0.0013562515377998352, acc: 100.0, f1: 100.0, r: 0.8182900263611508
06/02/2019 01:35:34 step: 5234, epoch: 158, batch: 19, loss: 0.0012264475226402283, acc: 100.0, f1: 100.0, r: 0.6254040584414032
06/02/2019 01:35:35 step: 5239, epoch: 158, batch: 24, loss: 0.0009709224104881287, acc: 100.0, f1: 100.0, r: 0.7862040496980607
06/02/2019 01:35:36 step: 5244, epoch: 158, batch: 29, loss: 0.0029968321323394775, acc: 100.0, f1: 100.0, r: 0.8198814783873609
06/02/2019 01:35:37 *** evaluating ***
06/02/2019 01:35:37 step: 159, epoch: 158, acc: 57.26495726495726, f1: 28.943499334318034, r: 0.33578987402666954
06/02/2019 01:35:37 *** epoch: 160 ***
06/02/2019 01:35:37 *** training ***
06/02/2019 01:35:38 step: 5252, epoch: 159, batch: 4, loss: 0.0028131231665611267, acc: 100.0, f1: 100.0, r: 0.7175021549548556
06/02/2019 01:35:39 step: 5257, epoch: 159, batch: 9, loss: 0.002002127468585968, acc: 100.0, f1: 100.0, r: 0.7144269679039595
06/02/2019 01:35:39 step: 5262, epoch: 159, batch: 14, loss: 0.0024806857109069824, acc: 100.0, f1: 100.0, r: 0.6839509909778315
06/02/2019 01:35:40 step: 5267, epoch: 159, batch: 19, loss: 0.006266079843044281, acc: 100.0, f1: 100.0, r: 0.8300851875547499
06/02/2019 01:35:41 step: 5272, epoch: 159, batch: 24, loss: 0.0025721564888954163, acc: 100.0, f1: 100.0, r: 0.7092099937722294
06/02/2019 01:35:42 step: 5277, epoch: 159, batch: 29, loss: 0.0015511736273765564, acc: 100.0, f1: 100.0, r: 0.6992158993230597
06/02/2019 01:35:42 *** evaluating ***
06/02/2019 01:35:43 step: 160, epoch: 159, acc: 57.26495726495726, f1: 28.512199402560135, r: 0.3390771135277089
06/02/2019 01:35:43 *** epoch: 161 ***
06/02/2019 01:35:43 *** training ***
06/02/2019 01:35:44 step: 5285, epoch: 160, batch: 4, loss: 0.001431286334991455, acc: 100.0, f1: 100.0, r: 0.6887689201730627
06/02/2019 01:35:44 step: 5290, epoch: 160, batch: 9, loss: 0.0028499215841293335, acc: 100.0, f1: 100.0, r: 0.7664233259508583
06/02/2019 01:35:45 step: 5295, epoch: 160, batch: 14, loss: 0.0035956203937530518, acc: 100.0, f1: 100.0, r: 0.8007041206720982
06/02/2019 01:35:46 step: 5300, epoch: 160, batch: 19, loss: 0.0012198463082313538, acc: 100.0, f1: 100.0, r: 0.7166772844101842
06/02/2019 01:35:47 step: 5305, epoch: 160, batch: 24, loss: 0.007556408643722534, acc: 100.0, f1: 100.0, r: 0.8169882871368637
06/02/2019 01:35:48 step: 5310, epoch: 160, batch: 29, loss: 0.0027661994099617004, acc: 100.0, f1: 100.0, r: 0.8404832628275215
06/02/2019 01:35:48 *** evaluating ***
06/02/2019 01:35:49 step: 161, epoch: 160, acc: 58.119658119658126, f1: 29.756782675849497, r: 0.3474452445318019
06/02/2019 01:35:49 *** epoch: 162 ***
06/02/2019 01:35:49 *** training ***
06/02/2019 01:35:49 step: 5318, epoch: 161, batch: 4, loss: 0.00684472918510437, acc: 100.0, f1: 100.0, r: 0.8373258970925634
06/02/2019 01:35:50 step: 5323, epoch: 161, batch: 9, loss: 0.0021089017391204834, acc: 100.0, f1: 100.0, r: 0.7624510118321981
06/02/2019 01:35:51 step: 5328, epoch: 161, batch: 14, loss: 0.002864256501197815, acc: 100.0, f1: 100.0, r: 0.6638054481888263
06/02/2019 01:35:52 step: 5333, epoch: 161, batch: 19, loss: 0.001520104706287384, acc: 100.0, f1: 100.0, r: 0.7780928596250198
06/02/2019 01:35:53 step: 5338, epoch: 161, batch: 24, loss: 0.001309812068939209, acc: 100.0, f1: 100.0, r: 0.6514220015512817
06/02/2019 01:35:54 step: 5343, epoch: 161, batch: 29, loss: 0.001796439290046692, acc: 100.0, f1: 100.0, r: 0.8416795494070956
06/02/2019 01:35:54 *** evaluating ***
06/02/2019 01:35:55 step: 162, epoch: 161, acc: 56.837606837606835, f1: 28.410057204700063, r: 0.35211692314969717
06/02/2019 01:35:55 *** epoch: 163 ***
06/02/2019 01:35:55 *** training ***
06/02/2019 01:35:55 step: 5351, epoch: 162, batch: 4, loss: 0.0015299692749977112, acc: 100.0, f1: 100.0, r: 0.7912747788131389
06/02/2019 01:35:56 step: 5356, epoch: 162, batch: 9, loss: 0.0028293058276176453, acc: 100.0, f1: 100.0, r: 0.8530796682542585
06/02/2019 01:35:57 step: 5361, epoch: 162, batch: 14, loss: 0.0014507696032524109, acc: 100.0, f1: 100.0, r: 0.6668895383299891
06/02/2019 01:35:58 step: 5366, epoch: 162, batch: 19, loss: 0.003299891948699951, acc: 100.0, f1: 100.0, r: 0.6775739836069365
06/02/2019 01:35:59 step: 5371, epoch: 162, batch: 24, loss: 0.01242072507739067, acc: 100.0, f1: 100.0, r: 0.6861787927410766
06/02/2019 01:36:00 step: 5376, epoch: 162, batch: 29, loss: 0.001347661018371582, acc: 100.0, f1: 100.0, r: 0.6736499808881257
06/02/2019 01:36:00 *** evaluating ***
06/02/2019 01:36:00 step: 163, epoch: 162, acc: 57.692307692307686, f1: 29.5456043569367, r: 0.3442565798574565
06/02/2019 01:36:00 *** epoch: 164 ***
06/02/2019 01:36:00 *** training ***
06/02/2019 01:36:01 step: 5384, epoch: 163, batch: 4, loss: 0.0028867051005363464, acc: 100.0, f1: 100.0, r: 0.7257936518034853
06/02/2019 01:36:02 step: 5389, epoch: 163, batch: 9, loss: 0.004761025309562683, acc: 100.0, f1: 100.0, r: 0.7074753913174375
06/02/2019 01:36:03 step: 5394, epoch: 163, batch: 14, loss: 0.0019392520189285278, acc: 100.0, f1: 100.0, r: 0.7989695682691405
06/02/2019 01:36:04 step: 5399, epoch: 163, batch: 19, loss: 0.003033384680747986, acc: 100.0, f1: 100.0, r: 0.7569548425863065
06/02/2019 01:36:05 step: 5404, epoch: 163, batch: 24, loss: 0.002414099872112274, acc: 100.0, f1: 100.0, r: 0.7949899854881273
06/02/2019 01:36:05 step: 5409, epoch: 163, batch: 29, loss: 0.006568290293216705, acc: 100.0, f1: 100.0, r: 0.7188166619482896
06/02/2019 01:36:06 *** evaluating ***
06/02/2019 01:36:06 step: 164, epoch: 163, acc: 57.692307692307686, f1: 29.586066642593483, r: 0.338015967135047
06/02/2019 01:36:06 *** epoch: 165 ***
06/02/2019 01:36:06 *** training ***
06/02/2019 01:36:07 step: 5417, epoch: 164, batch: 4, loss: 0.0035763680934906006, acc: 100.0, f1: 100.0, r: 0.8223450723212025
06/02/2019 01:36:08 step: 5422, epoch: 164, batch: 9, loss: 0.0017116889357566833, acc: 100.0, f1: 100.0, r: 0.8168111323396667
06/02/2019 01:36:09 step: 5427, epoch: 164, batch: 14, loss: 0.0021640658378601074, acc: 100.0, f1: 100.0, r: 0.8025695977213732
06/02/2019 01:36:10 step: 5432, epoch: 164, batch: 19, loss: 0.0015004649758338928, acc: 100.0, f1: 100.0, r: 0.57598248199423
06/02/2019 01:36:10 step: 5437, epoch: 164, batch: 24, loss: 0.0021629109978675842, acc: 100.0, f1: 100.0, r: 0.8012776931414112
06/02/2019 01:36:11 step: 5442, epoch: 164, batch: 29, loss: 0.0032861754298210144, acc: 100.0, f1: 100.0, r: 0.7041622362787567
06/02/2019 01:36:12 *** evaluating ***
06/02/2019 01:36:12 step: 165, epoch: 164, acc: 58.119658119658126, f1: 28.867262954564538, r: 0.3382402563016722
06/02/2019 01:36:12 *** epoch: 166 ***
06/02/2019 01:36:12 *** training ***
06/02/2019 01:36:13 step: 5450, epoch: 165, batch: 4, loss: 0.009096343070268631, acc: 100.0, f1: 100.0, r: 0.7265608498307974
06/02/2019 01:36:14 step: 5455, epoch: 165, batch: 9, loss: 0.0016978085041046143, acc: 100.0, f1: 100.0, r: 0.7961707367694693
06/02/2019 01:36:15 step: 5460, epoch: 165, batch: 14, loss: 0.001630827784538269, acc: 100.0, f1: 100.0, r: 0.7120652722400093
06/02/2019 01:36:15 step: 5465, epoch: 165, batch: 19, loss: 0.003094911575317383, acc: 100.0, f1: 100.0, r: 0.6630097577098473
06/02/2019 01:36:16 step: 5470, epoch: 165, batch: 24, loss: 0.0068949684500694275, acc: 100.0, f1: 100.0, r: 0.8037278858706364
06/02/2019 01:36:17 step: 5475, epoch: 165, batch: 29, loss: 0.007527194917201996, acc: 100.0, f1: 100.0, r: 0.8188246834263875
06/02/2019 01:36:18 *** evaluating ***
06/02/2019 01:36:18 step: 166, epoch: 165, acc: 57.26495726495726, f1: 29.02272827320266, r: 0.3362827925487259
06/02/2019 01:36:18 *** epoch: 167 ***
06/02/2019 01:36:18 *** training ***
06/02/2019 01:36:19 step: 5483, epoch: 166, batch: 4, loss: 0.00206582248210907, acc: 100.0, f1: 100.0, r: 0.775899228359105
06/02/2019 01:36:20 step: 5488, epoch: 166, batch: 9, loss: 0.004323519766330719, acc: 100.0, f1: 100.0, r: 0.7287790311960014
06/02/2019 01:36:20 step: 5493, epoch: 166, batch: 14, loss: 0.0011186599731445312, acc: 100.0, f1: 100.0, r: 0.765725496830775
06/02/2019 01:36:21 step: 5498, epoch: 166, batch: 19, loss: 0.016656942665576935, acc: 98.4375, f1: 98.9254718280755, r: 0.6757293523686173
06/02/2019 01:36:22 step: 5503, epoch: 166, batch: 24, loss: 0.00199262797832489, acc: 100.0, f1: 100.0, r: 0.7585514895689484
06/02/2019 01:36:23 step: 5508, epoch: 166, batch: 29, loss: 0.010081000626087189, acc: 100.0, f1: 100.0, r: 0.7839120231500604
06/02/2019 01:36:23 *** evaluating ***
06/02/2019 01:36:24 step: 167, epoch: 166, acc: 56.837606837606835, f1: 29.140737734487733, r: 0.3240314345779219
06/02/2019 01:36:24 *** epoch: 168 ***
06/02/2019 01:36:24 *** training ***
06/02/2019 01:36:25 step: 5516, epoch: 167, batch: 4, loss: 0.0037468746304512024, acc: 100.0, f1: 100.0, r: 0.7621600210481199
06/02/2019 01:36:25 step: 5521, epoch: 167, batch: 9, loss: 0.0029559284448623657, acc: 100.0, f1: 100.0, r: 0.7165397887640822
06/02/2019 01:36:26 step: 5526, epoch: 167, batch: 14, loss: 0.0033547207713127136, acc: 100.0, f1: 100.0, r: 0.6499462116796121
06/02/2019 01:36:27 step: 5531, epoch: 167, batch: 19, loss: 0.0008490309119224548, acc: 100.0, f1: 100.0, r: 0.7085335583248941
06/02/2019 01:36:28 step: 5536, epoch: 167, batch: 24, loss: 0.0030478015542030334, acc: 100.0, f1: 100.0, r: 0.8016500565706208
06/02/2019 01:36:29 step: 5541, epoch: 167, batch: 29, loss: 0.002504020929336548, acc: 100.0, f1: 100.0, r: 0.7431120550986867
06/02/2019 01:36:29 *** evaluating ***
06/02/2019 01:36:30 step: 168, epoch: 167, acc: 57.692307692307686, f1: 29.427995391705064, r: 0.33775089784223716
06/02/2019 01:36:30 *** epoch: 169 ***
06/02/2019 01:36:30 *** training ***
06/02/2019 01:36:30 step: 5549, epoch: 168, batch: 4, loss: 0.003412507474422455, acc: 100.0, f1: 100.0, r: 0.8142271659031811
06/02/2019 01:36:31 step: 5554, epoch: 168, batch: 9, loss: 0.0009481236338615417, acc: 100.0, f1: 100.0, r: 0.7688931149267373
06/02/2019 01:36:32 step: 5559, epoch: 168, batch: 14, loss: 0.0008038505911827087, acc: 100.0, f1: 100.0, r: 0.8085186269531313
06/02/2019 01:36:33 step: 5564, epoch: 168, batch: 19, loss: 0.0060640424489974976, acc: 100.0, f1: 100.0, r: 0.8078203902466647
06/02/2019 01:36:34 step: 5569, epoch: 168, batch: 24, loss: 0.003244742751121521, acc: 100.0, f1: 100.0, r: 0.6026048954493031
06/02/2019 01:36:35 step: 5574, epoch: 168, batch: 29, loss: 0.002626948058605194, acc: 100.0, f1: 100.0, r: 0.6480958082353949
06/02/2019 01:36:35 *** evaluating ***
06/02/2019 01:36:35 step: 169, epoch: 168, acc: 57.692307692307686, f1: 30.03617863937179, r: 0.3322527902949361
06/02/2019 01:36:35 *** epoch: 170 ***
06/02/2019 01:36:35 *** training ***
06/02/2019 01:36:36 step: 5582, epoch: 169, batch: 4, loss: 0.002663448452949524, acc: 100.0, f1: 100.0, r: 0.6285312371913471
06/02/2019 01:36:37 step: 5587, epoch: 169, batch: 9, loss: 0.0020672008395195007, acc: 100.0, f1: 100.0, r: 0.7166300728355844
06/02/2019 01:36:38 step: 5592, epoch: 169, batch: 14, loss: 0.0034638866782188416, acc: 100.0, f1: 100.0, r: 0.6367981471086234
06/02/2019 01:36:39 step: 5597, epoch: 169, batch: 19, loss: 0.0022976770997047424, acc: 100.0, f1: 100.0, r: 0.7910320360252021
06/02/2019 01:36:40 step: 5602, epoch: 169, batch: 24, loss: 0.014619529247283936, acc: 98.4375, f1: 98.14315663372267, r: 0.6955385785639149
06/02/2019 01:36:41 step: 5607, epoch: 169, batch: 29, loss: 0.0006749406456947327, acc: 100.0, f1: 100.0, r: 0.7119963995675511
06/02/2019 01:36:41 *** evaluating ***
06/02/2019 01:36:41 step: 170, epoch: 169, acc: 57.692307692307686, f1: 29.554939775005383, r: 0.3364352008013526
06/02/2019 01:36:41 *** epoch: 171 ***
06/02/2019 01:36:41 *** training ***
06/02/2019 01:36:42 step: 5615, epoch: 170, batch: 4, loss: 0.002564936876296997, acc: 100.0, f1: 100.0, r: 0.5545158825386316
06/02/2019 01:36:43 step: 5620, epoch: 170, batch: 9, loss: 0.0028968267142772675, acc: 100.0, f1: 100.0, r: 0.7613999346439435
06/02/2019 01:36:44 step: 5625, epoch: 170, batch: 14, loss: 0.006670907139778137, acc: 100.0, f1: 100.0, r: 0.8065008410693364
06/02/2019 01:36:45 step: 5630, epoch: 170, batch: 19, loss: 0.004730239510536194, acc: 100.0, f1: 100.0, r: 0.6292436009075931
06/02/2019 01:36:45 step: 5635, epoch: 170, batch: 24, loss: 0.0025669485330581665, acc: 100.0, f1: 100.0, r: 0.7162923629827752
06/02/2019 01:36:46 step: 5640, epoch: 170, batch: 29, loss: 0.004493728280067444, acc: 100.0, f1: 100.0, r: 0.6917819864300548
06/02/2019 01:36:47 *** evaluating ***
06/02/2019 01:36:47 step: 171, epoch: 170, acc: 57.26495726495726, f1: 26.46400025009005, r: 0.34164653565720865
06/02/2019 01:36:47 *** epoch: 172 ***
06/02/2019 01:36:47 *** training ***
06/02/2019 01:36:48 step: 5648, epoch: 171, batch: 4, loss: 0.01825115457177162, acc: 100.0, f1: 100.0, r: 0.7971511206791391
06/02/2019 01:36:49 step: 5653, epoch: 171, batch: 9, loss: 0.004006676375865936, acc: 100.0, f1: 100.0, r: 0.8300567969731839
06/02/2019 01:36:49 step: 5658, epoch: 171, batch: 14, loss: 0.0014432892203330994, acc: 100.0, f1: 100.0, r: 0.5679620627163335
06/02/2019 01:36:50 step: 5663, epoch: 171, batch: 19, loss: 0.005403831601142883, acc: 100.0, f1: 100.0, r: 0.6721999099543503
06/02/2019 01:36:51 step: 5668, epoch: 171, batch: 24, loss: 0.0037234723567962646, acc: 100.0, f1: 100.0, r: 0.8063910207605768
06/02/2019 01:36:52 step: 5673, epoch: 171, batch: 29, loss: 0.001982562243938446, acc: 100.0, f1: 100.0, r: 0.6788735570937803
06/02/2019 01:36:52 *** evaluating ***
06/02/2019 01:36:53 step: 172, epoch: 171, acc: 57.26495726495726, f1: 27.313816883502707, r: 0.34883952648524513
06/02/2019 01:36:53 *** epoch: 173 ***
06/02/2019 01:36:53 *** training ***
06/02/2019 01:36:53 step: 5681, epoch: 172, batch: 4, loss: 0.0058704763650894165, acc: 100.0, f1: 100.0, r: 0.6922201268937171
06/02/2019 01:36:54 step: 5686, epoch: 172, batch: 9, loss: 0.007361792027950287, acc: 100.0, f1: 100.0, r: 0.6639586900381129
06/02/2019 01:36:55 step: 5691, epoch: 172, batch: 14, loss: 0.015560194849967957, acc: 100.0, f1: 100.0, r: 0.7125580531105197
06/02/2019 01:36:56 step: 5696, epoch: 172, batch: 19, loss: 0.001763850450515747, acc: 100.0, f1: 100.0, r: 0.7268070092912083
06/02/2019 01:36:57 step: 5701, epoch: 172, batch: 24, loss: 0.0038535967469215393, acc: 100.0, f1: 100.0, r: 0.7886341055234416
06/02/2019 01:36:58 step: 5706, epoch: 172, batch: 29, loss: 0.003587007522583008, acc: 100.0, f1: 100.0, r: 0.6946285410509438
06/02/2019 01:36:58 *** evaluating ***
06/02/2019 01:36:59 step: 173, epoch: 172, acc: 56.837606837606835, f1: 28.234231917566316, r: 0.3379448508450331
06/02/2019 01:36:59 *** epoch: 174 ***
06/02/2019 01:36:59 *** training ***
06/02/2019 01:37:00 step: 5714, epoch: 173, batch: 4, loss: 0.0037461519241333008, acc: 100.0, f1: 100.0, r: 0.7299820027254278
06/02/2019 01:37:00 step: 5719, epoch: 173, batch: 9, loss: 0.008182317018508911, acc: 100.0, f1: 100.0, r: 0.7344752023857274
06/02/2019 01:37:01 step: 5724, epoch: 173, batch: 14, loss: 0.0030332952737808228, acc: 100.0, f1: 100.0, r: 0.8081991391182682
06/02/2019 01:37:02 step: 5729, epoch: 173, batch: 19, loss: 0.004496358335018158, acc: 100.0, f1: 100.0, r: 0.7864927073346296
06/02/2019 01:37:03 step: 5734, epoch: 173, batch: 24, loss: 0.001628674566745758, acc: 100.0, f1: 100.0, r: 0.7847084347481528
06/02/2019 01:37:04 step: 5739, epoch: 173, batch: 29, loss: 0.0022007524967193604, acc: 100.0, f1: 100.0, r: 0.7937745191624656
06/02/2019 01:37:04 *** evaluating ***
06/02/2019 01:37:05 step: 174, epoch: 173, acc: 57.26495726495726, f1: 28.090629707749702, r: 0.34608906652203386
06/02/2019 01:37:05 *** epoch: 175 ***
06/02/2019 01:37:05 *** training ***
06/02/2019 01:37:05 step: 5747, epoch: 174, batch: 4, loss: 0.0024157576262950897, acc: 100.0, f1: 100.0, r: 0.6848948053566388
06/02/2019 01:37:06 step: 5752, epoch: 174, batch: 9, loss: 0.0020718201994895935, acc: 100.0, f1: 100.0, r: 0.6876120987137136
06/02/2019 01:37:07 step: 5757, epoch: 174, batch: 14, loss: 0.001590147614479065, acc: 100.0, f1: 100.0, r: 0.8035729025217817
06/02/2019 01:37:08 step: 5762, epoch: 174, batch: 19, loss: 0.0038432851433753967, acc: 100.0, f1: 100.0, r: 0.7679559347834962
06/02/2019 01:37:09 step: 5767, epoch: 174, batch: 24, loss: 0.001519426703453064, acc: 100.0, f1: 100.0, r: 0.5766851757396391
06/02/2019 01:37:10 step: 5772, epoch: 174, batch: 29, loss: 0.008761368691921234, acc: 100.0, f1: 100.0, r: 0.7664458191528739
06/02/2019 01:37:10 *** evaluating ***
06/02/2019 01:37:10 step: 175, epoch: 174, acc: 56.41025641025641, f1: 28.13915443719568, r: 0.33520894013279795
06/02/2019 01:37:10 *** epoch: 176 ***
06/02/2019 01:37:10 *** training ***
06/02/2019 01:37:11 step: 5780, epoch: 175, batch: 4, loss: 0.0025971606373786926, acc: 100.0, f1: 100.0, r: 0.7178047786618653
06/02/2019 01:37:12 step: 5785, epoch: 175, batch: 9, loss: 0.001345209777355194, acc: 100.0, f1: 100.0, r: 0.666032267017525
06/02/2019 01:37:13 step: 5790, epoch: 175, batch: 14, loss: 0.004453681409358978, acc: 100.0, f1: 100.0, r: 0.8232822858055574
06/02/2019 01:37:14 step: 5795, epoch: 175, batch: 19, loss: 0.013567499816417694, acc: 100.0, f1: 100.0, r: 0.799887042631851
06/02/2019 01:37:15 step: 5800, epoch: 175, batch: 24, loss: 0.0020255520939826965, acc: 100.0, f1: 100.0, r: 0.7192124146774604
06/02/2019 01:37:16 step: 5805, epoch: 175, batch: 29, loss: 0.003750026226043701, acc: 100.0, f1: 100.0, r: 0.8148058657268067
06/02/2019 01:37:16 *** evaluating ***
06/02/2019 01:37:16 step: 176, epoch: 175, acc: 56.41025641025641, f1: 26.697497799259217, r: 0.3298809738802899
06/02/2019 01:37:16 *** epoch: 177 ***
06/02/2019 01:37:16 *** training ***
06/02/2019 01:37:17 step: 5813, epoch: 176, batch: 4, loss: 0.0035759173333644867, acc: 100.0, f1: 100.0, r: 0.6510765305116791
06/02/2019 01:37:18 step: 5818, epoch: 176, batch: 9, loss: 0.00406719371676445, acc: 100.0, f1: 100.0, r: 0.810109109108224
06/02/2019 01:37:19 step: 5823, epoch: 176, batch: 14, loss: 0.002731435000896454, acc: 100.0, f1: 100.0, r: 0.8248602826857144
06/02/2019 01:37:20 step: 5828, epoch: 176, batch: 19, loss: 0.03300044313073158, acc: 98.4375, f1: 99.09700722394221, r: 0.7761436840185847
06/02/2019 01:37:20 step: 5833, epoch: 176, batch: 24, loss: 0.0017795562744140625, acc: 100.0, f1: 100.0, r: 0.7888133243582848
06/02/2019 01:37:21 step: 5838, epoch: 176, batch: 29, loss: 0.0080205537378788, acc: 100.0, f1: 100.0, r: 0.6960731230151067
06/02/2019 01:37:22 *** evaluating ***
06/02/2019 01:37:22 step: 177, epoch: 176, acc: 58.119658119658126, f1: 29.15787724263914, r: 0.34118490308190513
06/02/2019 01:37:22 *** epoch: 178 ***
06/02/2019 01:37:22 *** training ***
06/02/2019 01:37:23 step: 5846, epoch: 177, batch: 4, loss: 0.000934600830078125, acc: 100.0, f1: 100.0, r: 0.7877455301892501
06/02/2019 01:37:24 step: 5851, epoch: 177, batch: 9, loss: 0.002538904547691345, acc: 100.0, f1: 100.0, r: 0.660621519775433
06/02/2019 01:37:24 step: 5856, epoch: 177, batch: 14, loss: 0.0042829811573028564, acc: 100.0, f1: 100.0, r: 0.8227357390527763
06/02/2019 01:37:25 step: 5861, epoch: 177, batch: 19, loss: 0.0017922371625900269, acc: 100.0, f1: 100.0, r: 0.7890487611998545
06/02/2019 01:37:26 step: 5866, epoch: 177, batch: 24, loss: 0.001251228153705597, acc: 100.0, f1: 100.0, r: 0.8072237463767076
06/02/2019 01:37:27 step: 5871, epoch: 177, batch: 29, loss: 0.005536802113056183, acc: 100.0, f1: 100.0, r: 0.8002859852362134
06/02/2019 01:37:27 *** evaluating ***
06/02/2019 01:37:28 step: 178, epoch: 177, acc: 57.692307692307686, f1: 29.800888867721465, r: 0.3387258335266326
06/02/2019 01:37:28 *** epoch: 179 ***
06/02/2019 01:37:28 *** training ***
06/02/2019 01:37:29 step: 5879, epoch: 178, batch: 4, loss: 0.0028475522994995117, acc: 100.0, f1: 100.0, r: 0.6641399237959867
06/02/2019 01:37:30 step: 5884, epoch: 178, batch: 9, loss: 0.0015947222709655762, acc: 100.0, f1: 100.0, r: 0.7389980388256845
06/02/2019 01:37:30 step: 5889, epoch: 178, batch: 14, loss: 0.002222098410129547, acc: 100.0, f1: 100.0, r: 0.8545948268453178
06/02/2019 01:37:31 step: 5894, epoch: 178, batch: 19, loss: 0.0034378841519355774, acc: 100.0, f1: 100.0, r: 0.723249623817221
06/02/2019 01:37:32 step: 5899, epoch: 178, batch: 24, loss: 0.029655005782842636, acc: 98.4375, f1: 97.0, r: 0.7171925489497465
06/02/2019 01:37:33 step: 5904, epoch: 178, batch: 29, loss: 0.0031314492225646973, acc: 100.0, f1: 100.0, r: 0.7922088950574242
06/02/2019 01:37:33 *** evaluating ***
06/02/2019 01:37:34 step: 179, epoch: 178, acc: 56.837606837606835, f1: 26.594103451682717, r: 0.3457864218448068
06/02/2019 01:37:34 *** epoch: 180 ***
06/02/2019 01:37:34 *** training ***
06/02/2019 01:37:35 step: 5912, epoch: 179, batch: 4, loss: 0.01037587970495224, acc: 100.0, f1: 100.0, r: 0.7258474496109205
06/02/2019 01:37:35 step: 5917, epoch: 179, batch: 9, loss: 0.0016113221645355225, acc: 100.0, f1: 100.0, r: 0.7022328666683679
06/02/2019 01:37:36 step: 5922, epoch: 179, batch: 14, loss: 0.0021661296486854553, acc: 100.0, f1: 100.0, r: 0.577705137755696
06/02/2019 01:37:37 step: 5927, epoch: 179, batch: 19, loss: 0.00640212744474411, acc: 100.0, f1: 100.0, r: 0.8109754193231433
06/02/2019 01:37:38 step: 5932, epoch: 179, batch: 24, loss: 0.002067767083644867, acc: 100.0, f1: 100.0, r: 0.7529098042535669
06/02/2019 01:37:39 step: 5937, epoch: 179, batch: 29, loss: 0.007881321012973785, acc: 100.0, f1: 100.0, r: 0.797434220715607
06/02/2019 01:37:39 *** evaluating ***
06/02/2019 01:37:39 step: 180, epoch: 179, acc: 58.97435897435898, f1: 29.94647861622487, r: 0.3595208603950528
06/02/2019 01:37:40 *** epoch: 181 ***
06/02/2019 01:37:40 *** training ***
06/02/2019 01:37:40 step: 5945, epoch: 180, batch: 4, loss: 0.0022676363587379456, acc: 100.0, f1: 100.0, r: 0.6894679003405086
06/02/2019 01:37:41 step: 5950, epoch: 180, batch: 9, loss: 0.0033324435353279114, acc: 100.0, f1: 100.0, r: 0.8113886027328363
06/02/2019 01:37:42 step: 5955, epoch: 180, batch: 14, loss: 0.005829505622386932, acc: 100.0, f1: 100.0, r: 0.7910387155151533
06/02/2019 01:37:43 step: 5960, epoch: 180, batch: 19, loss: 0.0010423362255096436, acc: 100.0, f1: 100.0, r: 0.8221965189608313
06/02/2019 01:37:44 step: 5965, epoch: 180, batch: 24, loss: 0.00574725866317749, acc: 100.0, f1: 100.0, r: 0.7312548845111246
06/02/2019 01:37:45 step: 5970, epoch: 180, batch: 29, loss: 0.0035422928631305695, acc: 100.0, f1: 100.0, r: 0.778648167907331
06/02/2019 01:37:45 *** evaluating ***
06/02/2019 01:37:46 step: 181, epoch: 180, acc: 56.837606837606835, f1: 28.51017536569073, r: 0.34267989878990296
06/02/2019 01:37:46 *** epoch: 182 ***
06/02/2019 01:37:46 *** training ***
06/02/2019 01:37:46 step: 5978, epoch: 181, batch: 4, loss: 0.0012274906039237976, acc: 100.0, f1: 100.0, r: 0.7642798721912745
06/02/2019 01:37:47 step: 5983, epoch: 181, batch: 9, loss: 0.002462640404701233, acc: 100.0, f1: 100.0, r: 0.5887589479332102
06/02/2019 01:37:48 step: 5988, epoch: 181, batch: 14, loss: 0.003569386899471283, acc: 100.0, f1: 100.0, r: 0.6818373038875666
06/02/2019 01:37:49 step: 5993, epoch: 181, batch: 19, loss: 0.0007293745875358582, acc: 100.0, f1: 100.0, r: 0.8158679861121988
06/02/2019 01:37:50 step: 5998, epoch: 181, batch: 24, loss: 0.006203077733516693, acc: 100.0, f1: 100.0, r: 0.601070251299649
06/02/2019 01:37:51 step: 6003, epoch: 181, batch: 29, loss: 0.0034285709261894226, acc: 100.0, f1: 100.0, r: 0.7719744098514006
06/02/2019 01:37:51 *** evaluating ***
06/02/2019 01:37:52 step: 182, epoch: 181, acc: 57.692307692307686, f1: 29.023283734844906, r: 0.3385080093783672
06/02/2019 01:37:52 *** epoch: 183 ***
06/02/2019 01:37:52 *** training ***
06/02/2019 01:37:52 step: 6011, epoch: 182, batch: 4, loss: 0.007112722843885422, acc: 100.0, f1: 100.0, r: 0.6541613210742933
06/02/2019 01:37:53 step: 6016, epoch: 182, batch: 9, loss: 0.014456160366535187, acc: 98.4375, f1: 99.19056429232191, r: 0.8236590336730208
06/02/2019 01:37:54 step: 6021, epoch: 182, batch: 14, loss: 0.011220231652259827, acc: 100.0, f1: 100.0, r: 0.7941370050847916
06/02/2019 01:37:55 step: 6026, epoch: 182, batch: 19, loss: 0.0028173699975013733, acc: 100.0, f1: 100.0, r: 0.771461646328965
06/02/2019 01:37:56 step: 6031, epoch: 182, batch: 24, loss: 0.004188645631074905, acc: 100.0, f1: 100.0, r: 0.703257239919782
06/02/2019 01:37:57 step: 6036, epoch: 182, batch: 29, loss: 0.004251815378665924, acc: 100.0, f1: 100.0, r: 0.687897402066071
06/02/2019 01:37:57 *** evaluating ***
06/02/2019 01:37:57 step: 183, epoch: 182, acc: 57.692307692307686, f1: 28.31306068792164, r: 0.3498021495533692
06/02/2019 01:37:57 *** epoch: 184 ***
06/02/2019 01:37:57 *** training ***
06/02/2019 01:37:58 step: 6044, epoch: 183, batch: 4, loss: 0.0034225061535835266, acc: 100.0, f1: 100.0, r: 0.7650642054240439
06/02/2019 01:37:59 step: 6049, epoch: 183, batch: 9, loss: 0.0019976794719696045, acc: 100.0, f1: 100.0, r: 0.8231956768658573
06/02/2019 01:38:00 step: 6054, epoch: 183, batch: 14, loss: 0.003689214587211609, acc: 100.0, f1: 100.0, r: 0.6894667155327145
06/02/2019 01:38:01 step: 6059, epoch: 183, batch: 19, loss: 0.002171128988265991, acc: 100.0, f1: 100.0, r: 0.6873852304697636
06/02/2019 01:38:02 step: 6064, epoch: 183, batch: 24, loss: 0.008626118302345276, acc: 100.0, f1: 100.0, r: 0.7979089816921721
06/02/2019 01:38:03 step: 6069, epoch: 183, batch: 29, loss: 0.005444034934043884, acc: 100.0, f1: 100.0, r: 0.5857007916907306
06/02/2019 01:38:03 *** evaluating ***
06/02/2019 01:38:03 step: 184, epoch: 183, acc: 57.26495726495726, f1: 27.599148406956864, r: 0.34521650254275754
06/02/2019 01:38:03 *** epoch: 185 ***
06/02/2019 01:38:03 *** training ***
06/02/2019 01:38:04 step: 6077, epoch: 184, batch: 4, loss: 0.001549556851387024, acc: 100.0, f1: 100.0, r: 0.8120292462991697
06/02/2019 01:38:05 step: 6082, epoch: 184, batch: 9, loss: 0.004073642194271088, acc: 100.0, f1: 100.0, r: 0.7486515903423602
06/02/2019 01:38:06 step: 6087, epoch: 184, batch: 14, loss: 0.001822054386138916, acc: 100.0, f1: 100.0, r: 0.7614263173202755
06/02/2019 01:38:07 step: 6092, epoch: 184, batch: 19, loss: 0.0017390251159667969, acc: 100.0, f1: 100.0, r: 0.712145227218725
06/02/2019 01:38:08 step: 6097, epoch: 184, batch: 24, loss: 0.004271455109119415, acc: 100.0, f1: 100.0, r: 0.7220757660100623
06/02/2019 01:38:09 step: 6102, epoch: 184, batch: 29, loss: 0.002142667770385742, acc: 100.0, f1: 100.0, r: 0.7954808956736097
06/02/2019 01:38:09 *** evaluating ***
06/02/2019 01:38:09 step: 185, epoch: 184, acc: 58.119658119658126, f1: 28.89750417710944, r: 0.3374169570193989
06/02/2019 01:38:09 *** epoch: 186 ***
06/02/2019 01:38:09 *** training ***
06/02/2019 01:38:10 step: 6110, epoch: 185, batch: 4, loss: 0.0025993138551712036, acc: 100.0, f1: 100.0, r: 0.8011244220206849
06/02/2019 01:38:11 step: 6115, epoch: 185, batch: 9, loss: 0.0029557161033153534, acc: 100.0, f1: 100.0, r: 0.8010653511620436
06/02/2019 01:38:12 step: 6120, epoch: 185, batch: 14, loss: 0.0014943927526474, acc: 100.0, f1: 100.0, r: 0.6924727998280533
06/02/2019 01:38:13 step: 6125, epoch: 185, batch: 19, loss: 0.001280665397644043, acc: 100.0, f1: 100.0, r: 0.7804192054769878
06/02/2019 01:38:14 step: 6130, epoch: 185, batch: 24, loss: 0.0023880526423454285, acc: 100.0, f1: 100.0, r: 0.6889905596487778
06/02/2019 01:38:15 step: 6135, epoch: 185, batch: 29, loss: 0.0016354620456695557, acc: 100.0, f1: 100.0, r: 0.7630471857488466
06/02/2019 01:38:15 *** evaluating ***
06/02/2019 01:38:15 step: 186, epoch: 185, acc: 57.26495726495726, f1: 28.710879343319817, r: 0.3385210769946264
06/02/2019 01:38:15 *** epoch: 187 ***
06/02/2019 01:38:15 *** training ***
06/02/2019 01:38:16 step: 6143, epoch: 186, batch: 4, loss: 0.005337975919246674, acc: 100.0, f1: 100.0, r: 0.6987947196995303
06/02/2019 01:38:17 step: 6148, epoch: 186, batch: 9, loss: 0.0007260516285896301, acc: 100.0, f1: 100.0, r: 0.7812411278940792
06/02/2019 01:38:18 step: 6153, epoch: 186, batch: 14, loss: 0.010361265391111374, acc: 100.0, f1: 100.0, r: 0.7346767986324403
06/02/2019 01:38:19 step: 6158, epoch: 186, batch: 19, loss: 0.0006296038627624512, acc: 100.0, f1: 100.0, r: 0.700721481005964
06/02/2019 01:38:20 step: 6163, epoch: 186, batch: 24, loss: 0.0013941824436187744, acc: 100.0, f1: 100.0, r: 0.7115331975513917
06/02/2019 01:38:21 step: 6168, epoch: 186, batch: 29, loss: 0.01113654300570488, acc: 100.0, f1: 100.0, r: 0.7861194909087907
06/02/2019 01:38:21 *** evaluating ***
06/02/2019 01:38:21 step: 187, epoch: 186, acc: 57.26495726495726, f1: 28.53803896977466, r: 0.33869390567879104
06/02/2019 01:38:21 *** epoch: 188 ***
06/02/2019 01:38:21 *** training ***
06/02/2019 01:38:22 step: 6176, epoch: 187, batch: 4, loss: 0.003172457218170166, acc: 100.0, f1: 100.0, r: 0.7271188069583657
06/02/2019 01:38:23 step: 6181, epoch: 187, batch: 9, loss: 0.002799995243549347, acc: 100.0, f1: 100.0, r: 0.7711029743406486
06/02/2019 01:38:24 step: 6186, epoch: 187, batch: 14, loss: 0.0011961087584495544, acc: 100.0, f1: 100.0, r: 0.6478386625593925
06/02/2019 01:38:25 step: 6191, epoch: 187, batch: 19, loss: 0.008279763162136078, acc: 100.0, f1: 100.0, r: 0.7442795518765692
06/02/2019 01:38:26 step: 6196, epoch: 187, batch: 24, loss: 0.004222169518470764, acc: 100.0, f1: 100.0, r: 0.6616337180546178
06/02/2019 01:38:27 step: 6201, epoch: 187, batch: 29, loss: 0.0026909559965133667, acc: 100.0, f1: 100.0, r: 0.745817457540279
06/02/2019 01:38:27 *** evaluating ***
06/02/2019 01:38:27 step: 188, epoch: 187, acc: 57.692307692307686, f1: 28.671240240498886, r: 0.3292595344539174
06/02/2019 01:38:27 *** epoch: 189 ***
06/02/2019 01:38:27 *** training ***
06/02/2019 01:38:28 step: 6209, epoch: 188, batch: 4, loss: 0.0015971958637237549, acc: 100.0, f1: 100.0, r: 0.6878326351740793
06/02/2019 01:38:29 step: 6214, epoch: 188, batch: 9, loss: 0.005257889628410339, acc: 100.0, f1: 100.0, r: 0.7517951492029145
06/02/2019 01:38:30 step: 6219, epoch: 188, batch: 14, loss: 0.001146651804447174, acc: 100.0, f1: 100.0, r: 0.8114424491940049
06/02/2019 01:38:31 step: 6224, epoch: 188, batch: 19, loss: 0.0037402883172035217, acc: 100.0, f1: 100.0, r: 0.6793100406475969
06/02/2019 01:38:32 step: 6229, epoch: 188, batch: 24, loss: 0.0016347318887710571, acc: 100.0, f1: 100.0, r: 0.7982521027275885
06/02/2019 01:38:32 step: 6234, epoch: 188, batch: 29, loss: 0.006747597828507423, acc: 100.0, f1: 100.0, r: 0.6658173992195454
06/02/2019 01:38:33 *** evaluating ***
06/02/2019 01:38:33 step: 189, epoch: 188, acc: 57.26495726495726, f1: 28.007881844237502, r: 0.3491949023384532
06/02/2019 01:38:33 *** epoch: 190 ***
06/02/2019 01:38:33 *** training ***
06/02/2019 01:38:34 step: 6242, epoch: 189, batch: 4, loss: 0.0010535567998886108, acc: 100.0, f1: 100.0, r: 0.7070315441758305
06/02/2019 01:38:35 step: 6247, epoch: 189, batch: 9, loss: 0.02521635591983795, acc: 98.4375, f1: 99.11914172783737, r: 0.7589481963287317
06/02/2019 01:38:36 step: 6252, epoch: 189, batch: 14, loss: 0.0007086917757987976, acc: 100.0, f1: 100.0, r: 0.7161280139556092
06/02/2019 01:38:37 step: 6257, epoch: 189, batch: 19, loss: 0.005077920854091644, acc: 100.0, f1: 100.0, r: 0.6072347938443956
06/02/2019 01:38:38 step: 6262, epoch: 189, batch: 24, loss: 0.0014410018920898438, acc: 100.0, f1: 100.0, r: 0.7817723039310639
06/02/2019 01:38:38 step: 6267, epoch: 189, batch: 29, loss: 0.009902320802211761, acc: 100.0, f1: 100.0, r: 0.8251819466670602
06/02/2019 01:38:39 *** evaluating ***
06/02/2019 01:38:39 step: 190, epoch: 189, acc: 58.54700854700855, f1: 29.95376275510204, r: 0.3469133876715116
06/02/2019 01:38:39 *** epoch: 191 ***
06/02/2019 01:38:39 *** training ***
06/02/2019 01:38:40 step: 6275, epoch: 190, batch: 4, loss: 0.0011987388134002686, acc: 100.0, f1: 100.0, r: 0.663757060792375
06/02/2019 01:38:41 step: 6280, epoch: 190, batch: 9, loss: 0.004196383059024811, acc: 100.0, f1: 100.0, r: 0.5951263329339564
06/02/2019 01:38:42 step: 6285, epoch: 190, batch: 14, loss: 0.0012878552079200745, acc: 100.0, f1: 100.0, r: 0.7930292573459363
06/02/2019 01:38:42 step: 6290, epoch: 190, batch: 19, loss: 0.050848644226789474, acc: 96.875, f1: 98.16946778711485, r: 0.7573692086179096
06/02/2019 01:38:43 step: 6295, epoch: 190, batch: 24, loss: 0.010794516652822495, acc: 100.0, f1: 100.0, r: 0.75820299004843
06/02/2019 01:38:44 step: 6300, epoch: 190, batch: 29, loss: 0.0023472458124160767, acc: 100.0, f1: 100.0, r: 0.6361828926195229
06/02/2019 01:38:45 *** evaluating ***
06/02/2019 01:38:45 step: 191, epoch: 190, acc: 58.54700854700855, f1: 29.845965489751713, r: 0.34303467444328484
06/02/2019 01:38:45 *** epoch: 192 ***
06/02/2019 01:38:45 *** training ***
06/02/2019 01:38:46 step: 6308, epoch: 191, batch: 4, loss: 0.002014070749282837, acc: 100.0, f1: 100.0, r: 0.7579964511129
06/02/2019 01:38:47 step: 6313, epoch: 191, batch: 9, loss: 0.0013050884008407593, acc: 100.0, f1: 100.0, r: 0.8023142727101115
06/02/2019 01:38:47 step: 6318, epoch: 191, batch: 14, loss: 0.0016397163271903992, acc: 100.0, f1: 100.0, r: 0.6951856756212823
06/02/2019 01:38:48 step: 6323, epoch: 191, batch: 19, loss: 0.004058748483657837, acc: 100.0, f1: 100.0, r: 0.7357407913112762
06/02/2019 01:38:49 step: 6328, epoch: 191, batch: 24, loss: 0.0019478052854537964, acc: 100.0, f1: 100.0, r: 0.8273081240535846
06/02/2019 01:38:50 step: 6333, epoch: 191, batch: 29, loss: 0.001419156789779663, acc: 100.0, f1: 100.0, r: 0.8086897752537847
06/02/2019 01:38:50 *** evaluating ***
06/02/2019 01:38:51 step: 192, epoch: 191, acc: 57.692307692307686, f1: 28.505801040756428, r: 0.347652172626471
06/02/2019 01:38:51 *** epoch: 193 ***
06/02/2019 01:38:51 *** training ***
06/02/2019 01:38:52 step: 6341, epoch: 192, batch: 4, loss: 0.010085918009281158, acc: 100.0, f1: 100.0, r: 0.6128399357307793
06/02/2019 01:38:53 step: 6346, epoch: 192, batch: 9, loss: 0.0014005154371261597, acc: 100.0, f1: 100.0, r: 0.7992829972893823
06/02/2019 01:38:53 step: 6351, epoch: 192, batch: 14, loss: 0.0009394288063049316, acc: 100.0, f1: 100.0, r: 0.8476219534903291
06/02/2019 01:38:54 step: 6356, epoch: 192, batch: 19, loss: 0.0025160908699035645, acc: 100.0, f1: 100.0, r: 0.7645774696697012
06/02/2019 01:38:55 step: 6361, epoch: 192, batch: 24, loss: 0.0024435222148895264, acc: 100.0, f1: 100.0, r: 0.7267626835413692
06/02/2019 01:38:56 step: 6366, epoch: 192, batch: 29, loss: 0.00215718150138855, acc: 100.0, f1: 100.0, r: 0.8510011685038206
06/02/2019 01:38:56 *** evaluating ***
06/02/2019 01:38:57 step: 193, epoch: 192, acc: 56.837606837606835, f1: 27.944121667805877, r: 0.34588166631193107
06/02/2019 01:38:57 *** epoch: 194 ***
06/02/2019 01:38:57 *** training ***
06/02/2019 01:38:58 step: 6374, epoch: 193, batch: 4, loss: 0.0006382912397384644, acc: 100.0, f1: 100.0, r: 0.6975230020527965
06/02/2019 01:38:58 step: 6379, epoch: 193, batch: 9, loss: 0.006560973823070526, acc: 100.0, f1: 100.0, r: 0.7833606530615529
06/02/2019 01:38:59 step: 6384, epoch: 193, batch: 14, loss: 0.002398751676082611, acc: 100.0, f1: 100.0, r: 0.8440973393456312
06/02/2019 01:39:00 step: 6389, epoch: 193, batch: 19, loss: 0.0028761252760887146, acc: 100.0, f1: 100.0, r: 0.7645521594155699
06/02/2019 01:39:01 step: 6394, epoch: 193, batch: 24, loss: 0.013197749853134155, acc: 100.0, f1: 100.0, r: 0.779062956900657
06/02/2019 01:39:02 step: 6399, epoch: 193, batch: 29, loss: 0.0032783672213554382, acc: 100.0, f1: 100.0, r: 0.6873080967957116
06/02/2019 01:39:02 *** evaluating ***
06/02/2019 01:39:03 step: 194, epoch: 193, acc: 58.54700854700855, f1: 30.675115120167, r: 0.34692189862912215
06/02/2019 01:39:03 *** epoch: 195 ***
06/02/2019 01:39:03 *** training ***
06/02/2019 01:39:03 step: 6407, epoch: 194, batch: 4, loss: 0.007245026528835297, acc: 100.0, f1: 100.0, r: 0.6770582898960136
06/02/2019 01:39:04 step: 6412, epoch: 194, batch: 9, loss: 0.0013339370489120483, acc: 100.0, f1: 100.0, r: 0.7315517326215961
06/02/2019 01:39:05 step: 6417, epoch: 194, batch: 14, loss: 0.009060703217983246, acc: 100.0, f1: 100.0, r: 0.669055874936677
06/02/2019 01:39:06 step: 6422, epoch: 194, batch: 19, loss: 0.0029794201254844666, acc: 100.0, f1: 100.0, r: 0.6790126575972922
06/02/2019 01:39:07 step: 6427, epoch: 194, batch: 24, loss: 0.0008011385798454285, acc: 100.0, f1: 100.0, r: 0.7934599751462883
06/02/2019 01:39:08 step: 6432, epoch: 194, batch: 29, loss: 0.0036829710006713867, acc: 100.0, f1: 100.0, r: 0.792457749469625
06/02/2019 01:39:08 *** evaluating ***
06/02/2019 01:39:08 step: 195, epoch: 194, acc: 57.26495726495726, f1: 28.33862351873965, r: 0.3500699110640261
06/02/2019 01:39:08 *** epoch: 196 ***
06/02/2019 01:39:08 *** training ***
06/02/2019 01:39:09 step: 6440, epoch: 195, batch: 4, loss: 0.0012420564889907837, acc: 100.0, f1: 100.0, r: 0.7903414766217987
06/02/2019 01:39:10 step: 6445, epoch: 195, batch: 9, loss: 0.002385541796684265, acc: 100.0, f1: 100.0, r: 0.6878422293146277
06/02/2019 01:39:11 step: 6450, epoch: 195, batch: 14, loss: 0.004346996545791626, acc: 100.0, f1: 100.0, r: 0.8215843992120863
06/02/2019 01:39:12 step: 6455, epoch: 195, batch: 19, loss: 0.009654797613620758, acc: 100.0, f1: 100.0, r: 0.819304707492894
06/02/2019 01:39:13 step: 6460, epoch: 195, batch: 24, loss: 0.002204008400440216, acc: 100.0, f1: 100.0, r: 0.7461919852572055
06/02/2019 01:39:13 step: 6465, epoch: 195, batch: 29, loss: 0.009930618107318878, acc: 100.0, f1: 100.0, r: 0.7130754752117888
06/02/2019 01:39:14 *** evaluating ***
06/02/2019 01:39:14 step: 196, epoch: 195, acc: 56.837606837606835, f1: 27.92561360964139, r: 0.3455525186703165
06/02/2019 01:39:14 *** epoch: 197 ***
06/02/2019 01:39:14 *** training ***
06/02/2019 01:39:15 step: 6473, epoch: 196, batch: 4, loss: 0.0042496249079704285, acc: 100.0, f1: 100.0, r: 0.7668873779797718
06/02/2019 01:39:16 step: 6478, epoch: 196, batch: 9, loss: 0.0031310468912124634, acc: 100.0, f1: 100.0, r: 0.7287493248775264
06/02/2019 01:39:17 step: 6483, epoch: 196, batch: 14, loss: 0.0029155611991882324, acc: 100.0, f1: 100.0, r: 0.78991114467185
06/02/2019 01:39:18 step: 6488, epoch: 196, batch: 19, loss: 0.002427726984024048, acc: 100.0, f1: 100.0, r: 0.8248288110616514
06/02/2019 01:39:18 step: 6493, epoch: 196, batch: 24, loss: 0.0025394633412361145, acc: 100.0, f1: 100.0, r: 0.661301872584135
06/02/2019 01:39:19 step: 6498, epoch: 196, batch: 29, loss: 0.0011167749762535095, acc: 100.0, f1: 100.0, r: 0.8071013691814146
06/02/2019 01:39:20 *** evaluating ***
06/02/2019 01:39:20 step: 197, epoch: 196, acc: 58.119658119658126, f1: 29.08559113300493, r: 0.3556575091354664
06/02/2019 01:39:20 *** epoch: 198 ***
06/02/2019 01:39:20 *** training ***
06/02/2019 01:39:21 step: 6506, epoch: 197, batch: 4, loss: 0.0053932517766952515, acc: 100.0, f1: 100.0, r: 0.6756347936216035
06/02/2019 01:39:22 step: 6511, epoch: 197, batch: 9, loss: 0.004573658108711243, acc: 100.0, f1: 100.0, r: 0.651802226631463
06/02/2019 01:39:23 step: 6516, epoch: 197, batch: 14, loss: 0.0011298879981040955, acc: 100.0, f1: 100.0, r: 0.7827492312123103
06/02/2019 01:39:23 step: 6521, epoch: 197, batch: 19, loss: 0.0028479769825935364, acc: 100.0, f1: 100.0, r: 0.8119083090264055
06/02/2019 01:39:24 step: 6526, epoch: 197, batch: 24, loss: 0.0033706799149513245, acc: 100.0, f1: 100.0, r: 0.7602753937155043
06/02/2019 01:39:25 step: 6531, epoch: 197, batch: 29, loss: 0.004416301846504211, acc: 100.0, f1: 100.0, r: 0.8088367509162654
06/02/2019 01:39:25 *** evaluating ***
06/02/2019 01:39:26 step: 198, epoch: 197, acc: 58.119658119658126, f1: 29.713707884815832, r: 0.33338531731185117
06/02/2019 01:39:26 *** epoch: 199 ***
06/02/2019 01:39:26 *** training ***
06/02/2019 01:39:27 step: 6539, epoch: 198, batch: 4, loss: 0.0015775561332702637, acc: 100.0, f1: 100.0, r: 0.6214467342808183
06/02/2019 01:39:27 step: 6544, epoch: 198, batch: 9, loss: 0.002091623842716217, acc: 100.0, f1: 100.0, r: 0.7904215818022723
06/02/2019 01:39:28 step: 6549, epoch: 198, batch: 14, loss: 0.002881385385990143, acc: 100.0, f1: 100.0, r: 0.7263521957907341
06/02/2019 01:39:29 step: 6554, epoch: 198, batch: 19, loss: 0.0014600679278373718, acc: 100.0, f1: 100.0, r: 0.7007926912566311
06/02/2019 01:39:30 step: 6559, epoch: 198, batch: 24, loss: 0.003071725368499756, acc: 100.0, f1: 100.0, r: 0.6780411642331751
06/02/2019 01:39:31 step: 6564, epoch: 198, batch: 29, loss: 0.0009556561708450317, acc: 100.0, f1: 100.0, r: 0.837350868460909
06/02/2019 01:39:31 *** evaluating ***
06/02/2019 01:39:31 step: 199, epoch: 198, acc: 56.837606837606835, f1: 28.53550458207213, r: 0.32802053234346773
06/02/2019 01:39:31 *** epoch: 200 ***
06/02/2019 01:39:31 *** training ***
06/02/2019 01:39:32 step: 6572, epoch: 199, batch: 4, loss: 0.0025239065289497375, acc: 100.0, f1: 100.0, r: 0.6999686038644841
06/02/2019 01:39:33 step: 6577, epoch: 199, batch: 9, loss: 0.0033686384558677673, acc: 100.0, f1: 100.0, r: 0.7628181999590188
06/02/2019 01:39:34 step: 6582, epoch: 199, batch: 14, loss: 0.0007938891649246216, acc: 100.0, f1: 100.0, r: 0.7303034935395395
06/02/2019 01:39:35 step: 6587, epoch: 199, batch: 19, loss: 0.007934466004371643, acc: 100.0, f1: 100.0, r: 0.6831235082554988
06/02/2019 01:39:36 step: 6592, epoch: 199, batch: 24, loss: 0.001041129231452942, acc: 100.0, f1: 100.0, r: 0.6439381975100464
06/02/2019 01:39:36 step: 6597, epoch: 199, batch: 29, loss: 0.00544341653585434, acc: 100.0, f1: 100.0, r: 0.7909881124374475
06/02/2019 01:39:37 *** evaluating ***
06/02/2019 01:39:37 step: 200, epoch: 199, acc: 58.119658119658126, f1: 30.418687236869058, r: 0.34545068210641255
06/02/2019 01:39:37 *** epoch: 201 ***
06/02/2019 01:39:37 *** training ***
06/02/2019 01:39:38 step: 6605, epoch: 200, batch: 4, loss: 0.003255993127822876, acc: 100.0, f1: 100.0, r: 0.7691836682142252
06/02/2019 01:39:39 step: 6610, epoch: 200, batch: 9, loss: 0.0035486221313476562, acc: 100.0, f1: 100.0, r: 0.6750296524097051
06/02/2019 01:39:40 step: 6615, epoch: 200, batch: 14, loss: 0.0022341683506965637, acc: 100.0, f1: 100.0, r: 0.6643567007235256
06/02/2019 01:39:41 step: 6620, epoch: 200, batch: 19, loss: 0.005004256963729858, acc: 100.0, f1: 100.0, r: 0.7377464533178157
06/02/2019 01:39:41 step: 6625, epoch: 200, batch: 24, loss: 0.0012483596801757812, acc: 100.0, f1: 100.0, r: 0.8059013093208521
06/02/2019 01:39:42 step: 6630, epoch: 200, batch: 29, loss: 0.001965634524822235, acc: 100.0, f1: 100.0, r: 0.7189263388417938
06/02/2019 01:39:43 *** evaluating ***
06/02/2019 01:39:43 step: 201, epoch: 200, acc: 57.692307692307686, f1: 28.06556064127609, r: 0.3436790623866081
06/02/2019 01:39:43 *** epoch: 202 ***
06/02/2019 01:39:43 *** training ***
06/02/2019 01:39:44 step: 6638, epoch: 201, batch: 4, loss: 0.0042336806654930115, acc: 100.0, f1: 100.0, r: 0.7007952950699435
06/02/2019 01:39:45 step: 6643, epoch: 201, batch: 9, loss: 0.0019369050860404968, acc: 100.0, f1: 100.0, r: 0.6905699685040532
06/02/2019 01:39:46 step: 6648, epoch: 201, batch: 14, loss: 0.0011477842926979065, acc: 100.0, f1: 100.0, r: 0.7044231541862073
06/02/2019 01:39:46 step: 6653, epoch: 201, batch: 19, loss: 0.008663646876811981, acc: 100.0, f1: 100.0, r: 0.6723285276896118
06/02/2019 01:39:47 step: 6658, epoch: 201, batch: 24, loss: 0.001429334282875061, acc: 100.0, f1: 100.0, r: 0.8160044197375917
06/02/2019 01:39:48 step: 6663, epoch: 201, batch: 29, loss: 0.0010838881134986877, acc: 100.0, f1: 100.0, r: 0.7146988813374691
06/02/2019 01:39:48 *** evaluating ***
06/02/2019 01:39:49 step: 202, epoch: 201, acc: 58.119658119658126, f1: 29.55800539655591, r: 0.3450513627506581
06/02/2019 01:39:49 *** epoch: 203 ***
06/02/2019 01:39:49 *** training ***
06/02/2019 01:39:50 step: 6671, epoch: 202, batch: 4, loss: 0.0013709738850593567, acc: 100.0, f1: 100.0, r: 0.6479571662039773
06/02/2019 01:39:50 step: 6676, epoch: 202, batch: 9, loss: 0.009382382035255432, acc: 100.0, f1: 100.0, r: 0.8354402363567316
06/02/2019 01:39:51 step: 6681, epoch: 202, batch: 14, loss: 0.01370134949684143, acc: 100.0, f1: 100.0, r: 0.7319098056110259
06/02/2019 01:39:52 step: 6686, epoch: 202, batch: 19, loss: 0.002559155225753784, acc: 100.0, f1: 100.0, r: 0.8026450111441894
06/02/2019 01:39:53 step: 6691, epoch: 202, batch: 24, loss: 0.002232559025287628, acc: 100.0, f1: 100.0, r: 0.7398119175180793
06/02/2019 01:39:54 step: 6696, epoch: 202, batch: 29, loss: 0.004382893443107605, acc: 100.0, f1: 100.0, r: 0.5676331580466105
06/02/2019 01:39:54 *** evaluating ***
06/02/2019 01:39:55 step: 203, epoch: 202, acc: 57.26495726495726, f1: 27.91351098879773, r: 0.3443199983674073
06/02/2019 01:39:55 *** epoch: 204 ***
06/02/2019 01:39:55 *** training ***
06/02/2019 01:39:55 step: 6704, epoch: 203, batch: 4, loss: 0.0025262758135795593, acc: 100.0, f1: 100.0, r: 0.7951547013717893
06/02/2019 01:39:56 step: 6709, epoch: 203, batch: 9, loss: 0.0013278648257255554, acc: 100.0, f1: 100.0, r: 0.7091576110068105
06/02/2019 01:39:57 step: 6714, epoch: 203, batch: 14, loss: 0.0021078139543533325, acc: 100.0, f1: 100.0, r: 0.7214608606077426
06/02/2019 01:39:58 step: 6719, epoch: 203, batch: 19, loss: 0.003705047070980072, acc: 100.0, f1: 100.0, r: 0.7173778104398681
06/02/2019 01:39:59 step: 6724, epoch: 203, batch: 24, loss: 0.020020093768835068, acc: 100.0, f1: 100.0, r: 0.8000541700444678
06/02/2019 01:40:00 step: 6729, epoch: 203, batch: 29, loss: 0.0013825967907905579, acc: 100.0, f1: 100.0, r: 0.6673223092767182
06/02/2019 01:40:00 *** evaluating ***
06/02/2019 01:40:00 step: 204, epoch: 203, acc: 55.98290598290598, f1: 27.00719058897947, r: 0.3266164866221702
06/02/2019 01:40:00 *** epoch: 205 ***
06/02/2019 01:40:00 *** training ***
06/02/2019 01:40:01 step: 6737, epoch: 204, batch: 4, loss: 0.0019273385405540466, acc: 100.0, f1: 100.0, r: 0.7099370127372713
06/02/2019 01:40:02 step: 6742, epoch: 204, batch: 9, loss: 0.0013402700424194336, acc: 100.0, f1: 100.0, r: 0.6972281955290376
06/02/2019 01:40:03 step: 6747, epoch: 204, batch: 14, loss: 0.0013288706541061401, acc: 100.0, f1: 100.0, r: 0.6495412019344711
06/02/2019 01:40:04 step: 6752, epoch: 204, batch: 19, loss: 0.002260960638523102, acc: 100.0, f1: 100.0, r: 0.8178094278306959
06/02/2019 01:40:05 step: 6757, epoch: 204, batch: 24, loss: 0.0018472149968147278, acc: 100.0, f1: 100.0, r: 0.8380521287072459
06/02/2019 01:40:05 step: 6762, epoch: 204, batch: 29, loss: 0.001219525933265686, acc: 100.0, f1: 100.0, r: 0.783529535371021
06/02/2019 01:40:06 *** evaluating ***
06/02/2019 01:40:06 step: 205, epoch: 204, acc: 57.26495726495726, f1: 27.957554280485088, r: 0.33898537281248997
06/02/2019 01:40:06 *** epoch: 206 ***
06/02/2019 01:40:06 *** training ***
06/02/2019 01:40:07 step: 6770, epoch: 205, batch: 4, loss: 0.0009603127837181091, acc: 100.0, f1: 100.0, r: 0.6218511322254623
06/02/2019 01:40:08 step: 6775, epoch: 205, batch: 9, loss: 0.0030336081981658936, acc: 100.0, f1: 100.0, r: 0.6955800733705109
06/02/2019 01:40:09 step: 6780, epoch: 205, batch: 14, loss: 0.007090635597705841, acc: 100.0, f1: 100.0, r: 0.7584680500201804
06/02/2019 01:40:10 step: 6785, epoch: 205, batch: 19, loss: 0.005076520144939423, acc: 100.0, f1: 100.0, r: 0.7731648950729456
06/02/2019 01:40:11 step: 6790, epoch: 205, batch: 24, loss: 0.0012431591749191284, acc: 100.0, f1: 100.0, r: 0.7337031894530465
06/02/2019 01:40:11 step: 6795, epoch: 205, batch: 29, loss: 0.0014258474111557007, acc: 100.0, f1: 100.0, r: 0.7253002245818003
06/02/2019 01:40:12 *** evaluating ***
06/02/2019 01:40:12 step: 206, epoch: 205, acc: 51.70940170940172, f1: 27.471646903896417, r: 0.3234112705046727
06/02/2019 01:40:12 *** epoch: 207 ***
06/02/2019 01:40:12 *** training ***
06/02/2019 01:40:13 step: 6803, epoch: 206, batch: 4, loss: 0.003506302833557129, acc: 100.0, f1: 100.0, r: 0.6593446900331049
06/02/2019 01:40:14 step: 6808, epoch: 206, batch: 9, loss: 0.002547241747379303, acc: 100.0, f1: 100.0, r: 0.6591782457033728
06/02/2019 01:40:15 step: 6813, epoch: 206, batch: 14, loss: 0.001546323299407959, acc: 100.0, f1: 100.0, r: 0.7265341700033479
06/02/2019 01:40:15 step: 6818, epoch: 206, batch: 19, loss: 0.0022622719407081604, acc: 100.0, f1: 100.0, r: 0.6652519043989125
06/02/2019 01:40:16 step: 6823, epoch: 206, batch: 24, loss: 0.001119956374168396, acc: 100.0, f1: 100.0, r: 0.6775081514132016
06/02/2019 01:40:17 step: 6828, epoch: 206, batch: 29, loss: 0.0048769377171993256, acc: 100.0, f1: 100.0, r: 0.7481442414086714
06/02/2019 01:40:18 *** evaluating ***
06/02/2019 01:40:18 step: 207, epoch: 206, acc: 58.119658119658126, f1: 29.53418486335064, r: 0.3420023275514952
06/02/2019 01:40:18 *** epoch: 208 ***
06/02/2019 01:40:18 *** training ***
06/02/2019 01:40:19 step: 6836, epoch: 207, batch: 4, loss: 0.005392745137214661, acc: 100.0, f1: 100.0, r: 0.803925539383989
06/02/2019 01:40:20 step: 6841, epoch: 207, batch: 9, loss: 0.002134263515472412, acc: 100.0, f1: 100.0, r: 0.7826934146726093
06/02/2019 01:40:20 step: 6846, epoch: 207, batch: 14, loss: 0.0031168758869171143, acc: 100.0, f1: 100.0, r: 0.6649228647372887
06/02/2019 01:40:21 step: 6851, epoch: 207, batch: 19, loss: 0.003996722400188446, acc: 100.0, f1: 100.0, r: 0.6840143715754945
06/02/2019 01:40:22 step: 6856, epoch: 207, batch: 24, loss: 0.001113004982471466, acc: 100.0, f1: 100.0, r: 0.7700927731065385
06/02/2019 01:40:23 step: 6861, epoch: 207, batch: 29, loss: 0.0022496506571769714, acc: 100.0, f1: 100.0, r: 0.6875464750865964
06/02/2019 01:40:23 *** evaluating ***
06/02/2019 01:40:24 step: 208, epoch: 207, acc: 58.54700854700855, f1: 29.714403204644768, r: 0.3409096523833686
06/02/2019 01:40:24 *** epoch: 209 ***
06/02/2019 01:40:24 *** training ***
06/02/2019 01:40:24 step: 6869, epoch: 208, batch: 4, loss: 0.0022066235542297363, acc: 100.0, f1: 100.0, r: 0.7779827984967569
06/02/2019 01:40:25 step: 6874, epoch: 208, batch: 9, loss: 0.0019454807043075562, acc: 100.0, f1: 100.0, r: 0.7080746195561728
06/02/2019 01:40:26 step: 6879, epoch: 208, batch: 14, loss: 0.0034209564328193665, acc: 100.0, f1: 100.0, r: 0.7907659240991644
06/02/2019 01:40:27 step: 6884, epoch: 208, batch: 19, loss: 0.016313865780830383, acc: 98.4375, f1: 99.10625620655412, r: 0.7370848731695864
06/02/2019 01:40:28 step: 6889, epoch: 208, batch: 24, loss: 0.006066679954528809, acc: 100.0, f1: 100.0, r: 0.6968818799102178
06/02/2019 01:40:29 step: 6894, epoch: 208, batch: 29, loss: 0.0024323612451553345, acc: 100.0, f1: 100.0, r: 0.6377967195556122
06/02/2019 01:40:29 *** evaluating ***
06/02/2019 01:40:29 step: 209, epoch: 208, acc: 58.54700854700855, f1: 28.587292596432857, r: 0.3597643361368156
06/02/2019 01:40:29 *** epoch: 210 ***
06/02/2019 01:40:29 *** training ***
06/02/2019 01:40:30 step: 6902, epoch: 209, batch: 4, loss: 0.0011269375681877136, acc: 100.0, f1: 100.0, r: 0.772787111731347
06/02/2019 01:40:31 step: 6907, epoch: 209, batch: 9, loss: 0.0021725893020629883, acc: 100.0, f1: 100.0, r: 0.7194447885377562
06/02/2019 01:40:32 step: 6912, epoch: 209, batch: 14, loss: 0.003598228096961975, acc: 100.0, f1: 100.0, r: 0.6978959439987812
06/02/2019 01:40:33 step: 6917, epoch: 209, batch: 19, loss: 0.0007568150758743286, acc: 100.0, f1: 100.0, r: 0.6737899428367171
06/02/2019 01:40:34 step: 6922, epoch: 209, batch: 24, loss: 0.0019669532775878906, acc: 100.0, f1: 100.0, r: 0.6768566941107684
06/02/2019 01:40:35 step: 6927, epoch: 209, batch: 29, loss: 0.0021436139941215515, acc: 100.0, f1: 100.0, r: 0.7966630589196795
06/02/2019 01:40:35 *** evaluating ***
06/02/2019 01:40:35 step: 210, epoch: 209, acc: 58.119658119658126, f1: 29.502139657654773, r: 0.343488574706905
06/02/2019 01:40:35 *** epoch: 211 ***
06/02/2019 01:40:35 *** training ***
06/02/2019 01:40:36 step: 6935, epoch: 210, batch: 4, loss: 0.001988641917705536, acc: 100.0, f1: 100.0, r: 0.7090885525994083
06/02/2019 01:40:37 step: 6940, epoch: 210, batch: 9, loss: 0.0012852400541305542, acc: 100.0, f1: 100.0, r: 0.7172029842926567
06/02/2019 01:40:38 step: 6945, epoch: 210, batch: 14, loss: 0.02079247683286667, acc: 98.4375, f1: 99.19073845116331, r: 0.7071834114029032
06/02/2019 01:40:39 step: 6950, epoch: 210, batch: 19, loss: 0.0013593882322311401, acc: 100.0, f1: 100.0, r: 0.6578974716281746
06/02/2019 01:40:40 step: 6955, epoch: 210, batch: 24, loss: 0.005170345306396484, acc: 100.0, f1: 100.0, r: 0.8107571433501111
06/02/2019 01:40:40 step: 6960, epoch: 210, batch: 29, loss: 0.003522627055644989, acc: 100.0, f1: 100.0, r: 0.7769910713140169
06/02/2019 01:40:41 *** evaluating ***
06/02/2019 01:40:41 step: 211, epoch: 210, acc: 58.97435897435898, f1: 27.845852517836406, r: 0.33626127567659564
06/02/2019 01:40:41 *** epoch: 212 ***
06/02/2019 01:40:41 *** training ***
06/02/2019 01:40:42 step: 6968, epoch: 211, batch: 4, loss: 0.002002224326133728, acc: 100.0, f1: 100.0, r: 0.7909795928708394
06/02/2019 01:40:43 step: 6973, epoch: 211, batch: 9, loss: 0.0028032436966896057, acc: 100.0, f1: 100.0, r: 0.7172479024095019
06/02/2019 01:40:44 step: 6978, epoch: 211, batch: 14, loss: 0.003094024956226349, acc: 100.0, f1: 100.0, r: 0.8179427060685169
06/02/2019 01:40:45 step: 6983, epoch: 211, batch: 19, loss: 0.0010635405778884888, acc: 100.0, f1: 100.0, r: 0.5914870414140625
06/02/2019 01:40:46 step: 6988, epoch: 211, batch: 24, loss: 0.0014142468571662903, acc: 100.0, f1: 100.0, r: 0.7666583320028119
06/02/2019 01:40:46 step: 6993, epoch: 211, batch: 29, loss: 0.0056715309619903564, acc: 100.0, f1: 100.0, r: 0.8066281319210044
06/02/2019 01:40:47 *** evaluating ***
06/02/2019 01:40:47 step: 212, epoch: 211, acc: 58.54700854700855, f1: 27.95530492898914, r: 0.34583261730763754
06/02/2019 01:40:47 *** epoch: 213 ***
06/02/2019 01:40:47 *** training ***
06/02/2019 01:40:48 step: 7001, epoch: 212, batch: 4, loss: 0.007173188030719757, acc: 100.0, f1: 100.0, r: 0.8406682634150413
06/02/2019 01:40:49 step: 7006, epoch: 212, batch: 9, loss: 0.0007612258195877075, acc: 100.0, f1: 100.0, r: 0.7267715912683202
06/02/2019 01:40:50 step: 7011, epoch: 212, batch: 14, loss: 0.003347940742969513, acc: 100.0, f1: 100.0, r: 0.7110155482827805
06/02/2019 01:40:51 step: 7016, epoch: 212, batch: 19, loss: 0.011462084949016571, acc: 100.0, f1: 100.0, r: 0.6611308940367667
06/02/2019 01:40:51 step: 7021, epoch: 212, batch: 24, loss: 0.002645917236804962, acc: 100.0, f1: 100.0, r: 0.7257245579384282
06/02/2019 01:40:52 step: 7026, epoch: 212, batch: 29, loss: 0.008261680603027344, acc: 100.0, f1: 100.0, r: 0.6995571781709131
06/02/2019 01:40:53 *** evaluating ***
06/02/2019 01:40:53 step: 213, epoch: 212, acc: 58.119658119658126, f1: 28.75362078854595, r: 0.3364047412872104
06/02/2019 01:40:53 *** epoch: 214 ***
06/02/2019 01:40:53 *** training ***
06/02/2019 01:40:54 step: 7034, epoch: 213, batch: 4, loss: 0.008612081408500671, acc: 100.0, f1: 100.0, r: 0.7500680597602282
06/02/2019 01:40:55 step: 7039, epoch: 213, batch: 9, loss: 0.004527382552623749, acc: 100.0, f1: 100.0, r: 0.6469867868909815
06/02/2019 01:40:55 step: 7044, epoch: 213, batch: 14, loss: 0.0019920319318771362, acc: 100.0, f1: 100.0, r: 0.6456478376397703
06/02/2019 01:40:56 step: 7049, epoch: 213, batch: 19, loss: 0.003343377262353897, acc: 100.0, f1: 100.0, r: 0.7059629300623942
06/02/2019 01:40:57 step: 7054, epoch: 213, batch: 24, loss: 0.005544327199459076, acc: 100.0, f1: 100.0, r: 0.8320402669760076
06/02/2019 01:40:58 step: 7059, epoch: 213, batch: 29, loss: 0.00057230144739151, acc: 100.0, f1: 100.0, r: 0.8454183201857437
06/02/2019 01:40:58 *** evaluating ***
06/02/2019 01:40:59 step: 214, epoch: 213, acc: 57.692307692307686, f1: 26.850943213081415, r: 0.3407546152175098
06/02/2019 01:40:59 *** epoch: 215 ***
06/02/2019 01:40:59 *** training ***
06/02/2019 01:41:00 step: 7067, epoch: 214, batch: 4, loss: 0.0009282529354095459, acc: 100.0, f1: 100.0, r: 0.7607047070225644
06/02/2019 01:41:00 step: 7072, epoch: 214, batch: 9, loss: 0.001672588288784027, acc: 100.0, f1: 100.0, r: 0.8058061913839488
06/02/2019 01:41:01 step: 7077, epoch: 214, batch: 14, loss: 0.0005087926983833313, acc: 100.0, f1: 100.0, r: 0.7160176724295012
06/02/2019 01:41:02 step: 7082, epoch: 214, batch: 19, loss: 0.023696135729551315, acc: 98.4375, f1: 99.14108879626122, r: 0.7267315696332728
06/02/2019 01:41:03 step: 7087, epoch: 214, batch: 24, loss: 0.0018277838826179504, acc: 100.0, f1: 100.0, r: 0.6130204118700225
06/02/2019 01:41:04 step: 7092, epoch: 214, batch: 29, loss: 0.0022909194231033325, acc: 100.0, f1: 100.0, r: 0.685390323203859
06/02/2019 01:41:04 *** evaluating ***
06/02/2019 01:41:04 step: 215, epoch: 214, acc: 56.837606837606835, f1: 28.153438893259896, r: 0.33731861326114226
06/02/2019 01:41:04 *** epoch: 216 ***
06/02/2019 01:41:04 *** training ***
06/02/2019 01:41:05 step: 7100, epoch: 215, batch: 4, loss: 0.0032021328806877136, acc: 100.0, f1: 100.0, r: 0.7746482125369932
06/02/2019 01:41:06 step: 7105, epoch: 215, batch: 9, loss: 0.04527095705270767, acc: 98.4375, f1: 99.11914172783739, r: 0.6923819624323115
06/02/2019 01:41:07 step: 7110, epoch: 215, batch: 14, loss: 0.010842319577932358, acc: 100.0, f1: 100.0, r: 0.6665673716429008
06/02/2019 01:41:08 step: 7115, epoch: 215, batch: 19, loss: 0.005220357328653336, acc: 100.0, f1: 100.0, r: 0.6804900416137275
06/02/2019 01:41:09 step: 7120, epoch: 215, batch: 24, loss: 0.005144655704498291, acc: 100.0, f1: 100.0, r: 0.7252391415433243
06/02/2019 01:41:10 step: 7125, epoch: 215, batch: 29, loss: 0.0007206499576568604, acc: 100.0, f1: 100.0, r: 0.8268953565399866
06/02/2019 01:41:10 *** evaluating ***
06/02/2019 01:41:10 step: 216, epoch: 215, acc: 57.26495726495726, f1: 29.11372435152923, r: 0.34886323685088494
06/02/2019 01:41:10 *** epoch: 217 ***
06/02/2019 01:41:10 *** training ***
06/02/2019 01:41:11 step: 7133, epoch: 216, batch: 4, loss: 0.0009643808007240295, acc: 100.0, f1: 100.0, r: 0.796909784653082
06/02/2019 01:41:12 step: 7138, epoch: 216, batch: 9, loss: 0.004443183541297913, acc: 100.0, f1: 100.0, r: 0.7118220126409184
06/02/2019 01:41:13 step: 7143, epoch: 216, batch: 14, loss: 0.002922944724559784, acc: 100.0, f1: 100.0, r: 0.6868519359165874
06/02/2019 01:41:14 step: 7148, epoch: 216, batch: 19, loss: 0.010287381708621979, acc: 100.0, f1: 100.0, r: 0.7154301725768227
06/02/2019 01:41:15 step: 7153, epoch: 216, batch: 24, loss: 0.02940277010202408, acc: 98.4375, f1: 85.49450549450549, r: 0.6648358579344767
06/02/2019 01:41:15 step: 7158, epoch: 216, batch: 29, loss: 0.002933777868747711, acc: 100.0, f1: 100.0, r: 0.7929778528162884
06/02/2019 01:41:16 *** evaluating ***
06/02/2019 01:41:16 step: 217, epoch: 216, acc: 55.12820512820513, f1: 27.985718782579326, r: 0.3368671741292819
06/02/2019 01:41:16 *** epoch: 218 ***
06/02/2019 01:41:16 *** training ***
06/02/2019 01:41:17 step: 7166, epoch: 217, batch: 4, loss: 0.001880250871181488, acc: 100.0, f1: 100.0, r: 0.7029332840039731
06/02/2019 01:41:18 step: 7171, epoch: 217, batch: 9, loss: 0.003224700689315796, acc: 100.0, f1: 100.0, r: 0.6015530402639141
06/02/2019 01:41:19 step: 7176, epoch: 217, batch: 14, loss: 0.005629770457744598, acc: 100.0, f1: 100.0, r: 0.7722836463148678
06/02/2019 01:41:20 step: 7181, epoch: 217, batch: 19, loss: 0.0012090876698493958, acc: 100.0, f1: 100.0, r: 0.7914478831028896
06/02/2019 01:41:20 step: 7186, epoch: 217, batch: 24, loss: 0.010784365236759186, acc: 100.0, f1: 100.0, r: 0.6027764033309171
06/02/2019 01:41:21 step: 7191, epoch: 217, batch: 29, loss: 0.005746882408857346, acc: 100.0, f1: 100.0, r: 0.649739417259751
06/02/2019 01:41:22 *** evaluating ***
06/02/2019 01:41:22 step: 218, epoch: 217, acc: 57.26495726495726, f1: 29.62701440293986, r: 0.3342638695039429
06/02/2019 01:41:22 *** epoch: 219 ***
06/02/2019 01:41:22 *** training ***
06/02/2019 01:41:23 step: 7199, epoch: 218, batch: 4, loss: 0.004943870007991791, acc: 100.0, f1: 100.0, r: 0.7137997330186054
06/02/2019 01:41:24 step: 7204, epoch: 218, batch: 9, loss: 0.004362821578979492, acc: 100.0, f1: 100.0, r: 0.7041317234711861
06/02/2019 01:41:25 step: 7209, epoch: 218, batch: 14, loss: 0.0013367310166358948, acc: 100.0, f1: 100.0, r: 0.7495411234254167
06/02/2019 01:41:26 step: 7214, epoch: 218, batch: 19, loss: 0.0008595585823059082, acc: 100.0, f1: 100.0, r: 0.7171768139380397
06/02/2019 01:41:26 step: 7219, epoch: 218, batch: 24, loss: 0.004045788198709488, acc: 100.0, f1: 100.0, r: 0.8007938740178016
06/02/2019 01:41:27 step: 7224, epoch: 218, batch: 29, loss: 0.0003710240125656128, acc: 100.0, f1: 100.0, r: 0.7041361289926712
06/02/2019 01:41:28 *** evaluating ***
06/02/2019 01:41:28 step: 219, epoch: 218, acc: 58.119658119658126, f1: 30.563887582038674, r: 0.35118741321181846
06/02/2019 01:41:28 *** epoch: 220 ***
06/02/2019 01:41:28 *** training ***
06/02/2019 01:41:29 step: 7232, epoch: 219, batch: 4, loss: 0.005685500800609589, acc: 100.0, f1: 100.0, r: 0.8012255167943032
06/02/2019 01:41:30 step: 7237, epoch: 219, batch: 9, loss: 0.0018244870007038116, acc: 100.0, f1: 100.0, r: 0.6700513108017904
06/02/2019 01:41:31 step: 7242, epoch: 219, batch: 14, loss: 0.008350826799869537, acc: 100.0, f1: 100.0, r: 0.726538320837728
06/02/2019 01:41:31 step: 7247, epoch: 219, batch: 19, loss: 0.0032228082418441772, acc: 100.0, f1: 100.0, r: 0.7992767935952677
06/02/2019 01:41:32 step: 7252, epoch: 219, batch: 24, loss: 0.004324391484260559, acc: 100.0, f1: 100.0, r: 0.7735501630854291
06/02/2019 01:41:33 step: 7257, epoch: 219, batch: 29, loss: 0.002164304256439209, acc: 100.0, f1: 100.0, r: 0.6936612098539452
06/02/2019 01:41:34 *** evaluating ***
06/02/2019 01:41:34 step: 220, epoch: 219, acc: 57.692307692307686, f1: 30.226797095475156, r: 0.34185739450055797
06/02/2019 01:41:34 *** epoch: 221 ***
06/02/2019 01:41:34 *** training ***
06/02/2019 01:41:35 step: 7265, epoch: 220, batch: 4, loss: 0.006148532032966614, acc: 100.0, f1: 100.0, r: 0.7693249033777739
06/02/2019 01:41:36 step: 7270, epoch: 220, batch: 9, loss: 0.0016503334045410156, acc: 100.0, f1: 100.0, r: 0.7636667053249653
06/02/2019 01:41:37 step: 7275, epoch: 220, batch: 14, loss: 0.003959953784942627, acc: 100.0, f1: 100.0, r: 0.786140013276455
06/02/2019 01:41:37 step: 7280, epoch: 220, batch: 19, loss: 0.0032096579670906067, acc: 100.0, f1: 100.0, r: 0.7787629092480095
06/02/2019 01:41:38 step: 7285, epoch: 220, batch: 24, loss: 0.004804700613021851, acc: 100.0, f1: 100.0, r: 0.7282824686336596
06/02/2019 01:41:39 step: 7290, epoch: 220, batch: 29, loss: 0.0010701045393943787, acc: 100.0, f1: 100.0, r: 0.7126803807744528
06/02/2019 01:41:40 *** evaluating ***
06/02/2019 01:41:40 step: 221, epoch: 220, acc: 58.54700854700855, f1: 31.50751037203171, r: 0.3414977490173427
06/02/2019 01:41:40 *** epoch: 222 ***
06/02/2019 01:41:40 *** training ***
06/02/2019 01:41:41 step: 7298, epoch: 221, batch: 4, loss: 0.00455329567193985, acc: 100.0, f1: 100.0, r: 0.6748101230060978
06/02/2019 01:41:42 step: 7303, epoch: 221, batch: 9, loss: 0.004183851182460785, acc: 100.0, f1: 100.0, r: 0.8654290291882153
06/02/2019 01:41:42 step: 7308, epoch: 221, batch: 14, loss: 0.0014138221740722656, acc: 100.0, f1: 100.0, r: 0.7348726949483089
06/02/2019 01:41:43 step: 7313, epoch: 221, batch: 19, loss: 0.0022730976343154907, acc: 100.0, f1: 100.0, r: 0.8140378973846514
06/02/2019 01:41:44 step: 7318, epoch: 221, batch: 24, loss: 0.0022349581122398376, acc: 100.0, f1: 100.0, r: 0.7624608683576841
06/02/2019 01:41:45 step: 7323, epoch: 221, batch: 29, loss: 0.0021711811423301697, acc: 100.0, f1: 100.0, r: 0.7407079241556014
06/02/2019 01:41:45 *** evaluating ***
06/02/2019 01:41:46 step: 222, epoch: 221, acc: 57.26495726495726, f1: 29.60191250861254, r: 0.3362569441068987
06/02/2019 01:41:46 *** epoch: 223 ***
06/02/2019 01:41:46 *** training ***
06/02/2019 01:41:46 step: 7331, epoch: 222, batch: 4, loss: 0.001149892807006836, acc: 100.0, f1: 100.0, r: 0.6193190738074555
06/02/2019 01:41:47 step: 7336, epoch: 222, batch: 9, loss: 0.0037386417388916016, acc: 100.0, f1: 100.0, r: 0.6899966870775492
06/02/2019 01:41:48 step: 7341, epoch: 222, batch: 14, loss: 0.0016967281699180603, acc: 100.0, f1: 100.0, r: 0.7994235710796497
06/02/2019 01:41:49 step: 7346, epoch: 222, batch: 19, loss: 0.0011471882462501526, acc: 100.0, f1: 100.0, r: 0.6436563452132323
06/02/2019 01:41:50 step: 7351, epoch: 222, batch: 24, loss: 0.0015950724482536316, acc: 100.0, f1: 100.0, r: 0.662898648907203
06/02/2019 01:41:51 step: 7356, epoch: 222, batch: 29, loss: 0.006349354982376099, acc: 100.0, f1: 100.0, r: 0.7876217415346118
06/02/2019 01:41:51 *** evaluating ***
06/02/2019 01:41:51 step: 223, epoch: 222, acc: 58.119658119658126, f1: 31.34787335071706, r: 0.3429202625630075
06/02/2019 01:41:51 *** epoch: 224 ***
06/02/2019 01:41:51 *** training ***
06/02/2019 01:41:52 step: 7364, epoch: 223, batch: 4, loss: 0.001941777765750885, acc: 100.0, f1: 100.0, r: 0.7573408778974878
06/02/2019 01:41:53 step: 7369, epoch: 223, batch: 9, loss: 0.0013134405016899109, acc: 100.0, f1: 100.0, r: 0.6877606380638825
06/02/2019 01:41:54 step: 7374, epoch: 223, batch: 14, loss: 0.0014174655079841614, acc: 100.0, f1: 100.0, r: 0.8291470790020571
06/02/2019 01:41:55 step: 7379, epoch: 223, batch: 19, loss: 0.0058443546295166016, acc: 100.0, f1: 100.0, r: 0.70263225269195
06/02/2019 01:41:56 step: 7384, epoch: 223, batch: 24, loss: 0.0025367215275764465, acc: 100.0, f1: 100.0, r: 0.6648777065901604
06/02/2019 01:41:57 step: 7389, epoch: 223, batch: 29, loss: 0.001810327172279358, acc: 100.0, f1: 100.0, r: 0.7113380993436569
06/02/2019 01:41:57 *** evaluating ***
06/02/2019 01:41:57 step: 224, epoch: 223, acc: 57.26495726495726, f1: 28.929370055430216, r: 0.33757924125123706
06/02/2019 01:41:57 *** epoch: 225 ***
06/02/2019 01:41:57 *** training ***
06/02/2019 01:41:58 step: 7397, epoch: 224, batch: 4, loss: 0.0020089372992515564, acc: 100.0, f1: 100.0, r: 0.793583764177674
06/02/2019 01:41:59 step: 7402, epoch: 224, batch: 9, loss: 0.003088124096393585, acc: 100.0, f1: 100.0, r: 0.6891275759134179
06/02/2019 01:42:00 step: 7407, epoch: 224, batch: 14, loss: 0.020491361618041992, acc: 100.0, f1: 100.0, r: 0.6832511650114929
06/02/2019 01:42:00 step: 7412, epoch: 224, batch: 19, loss: 0.005222775042057037, acc: 100.0, f1: 100.0, r: 0.7859431559170338
06/02/2019 01:42:01 step: 7417, epoch: 224, batch: 24, loss: 0.0029651522636413574, acc: 100.0, f1: 100.0, r: 0.7324737795885404
06/02/2019 01:42:02 step: 7422, epoch: 224, batch: 29, loss: 0.003925733268260956, acc: 100.0, f1: 100.0, r: 0.7240619210845305
06/02/2019 01:42:03 *** evaluating ***
06/02/2019 01:42:03 step: 225, epoch: 224, acc: 57.26495726495726, f1: 27.8109015093428, r: 0.3318502723657624
06/02/2019 01:42:03 *** epoch: 226 ***
06/02/2019 01:42:03 *** training ***
06/02/2019 01:42:04 step: 7430, epoch: 225, batch: 4, loss: 0.004096187651157379, acc: 100.0, f1: 100.0, r: 0.7950380599906526
06/02/2019 01:42:05 step: 7435, epoch: 225, batch: 9, loss: 0.0036693811416625977, acc: 100.0, f1: 100.0, r: 0.8465614370457035
06/02/2019 01:42:05 step: 7440, epoch: 225, batch: 14, loss: 0.001369088888168335, acc: 100.0, f1: 100.0, r: 0.6647904553125052
06/02/2019 01:42:06 step: 7445, epoch: 225, batch: 19, loss: 0.0014387890696525574, acc: 100.0, f1: 100.0, r: 0.6732364939170222
06/02/2019 01:42:07 step: 7450, epoch: 225, batch: 24, loss: 0.001397944986820221, acc: 100.0, f1: 100.0, r: 0.7115691154814214
06/02/2019 01:42:08 step: 7455, epoch: 225, batch: 29, loss: 0.003277026116847992, acc: 100.0, f1: 100.0, r: 0.6229737182820824
06/02/2019 01:42:08 *** evaluating ***
06/02/2019 01:42:09 step: 226, epoch: 225, acc: 58.119658119658126, f1: 31.5153902599515, r: 0.33782211466584516
06/02/2019 01:42:09 *** epoch: 227 ***
06/02/2019 01:42:09 *** training ***
06/02/2019 01:42:10 step: 7463, epoch: 226, batch: 4, loss: 0.0014257803559303284, acc: 100.0, f1: 100.0, r: 0.658571525030236
06/02/2019 01:42:10 step: 7468, epoch: 226, batch: 9, loss: 0.0020663514733314514, acc: 100.0, f1: 100.0, r: 0.5899397675632928
06/02/2019 01:42:11 step: 7473, epoch: 226, batch: 14, loss: 0.0007000267505645752, acc: 100.0, f1: 100.0, r: 0.7502727969119141
06/02/2019 01:42:12 step: 7478, epoch: 226, batch: 19, loss: 0.0010841116309165955, acc: 100.0, f1: 100.0, r: 0.695797886778319
06/02/2019 01:42:13 step: 7483, epoch: 226, batch: 24, loss: 0.0029394403100013733, acc: 100.0, f1: 100.0, r: 0.6631642071588346
06/02/2019 01:42:14 step: 7488, epoch: 226, batch: 29, loss: 0.002624504268169403, acc: 100.0, f1: 100.0, r: 0.7092815336998735
06/02/2019 01:42:14 *** evaluating ***
06/02/2019 01:42:15 step: 227, epoch: 226, acc: 57.26495726495726, f1: 29.142585023255435, r: 0.33350933033019814
06/02/2019 01:42:15 *** epoch: 228 ***
06/02/2019 01:42:15 *** training ***
06/02/2019 01:42:15 step: 7496, epoch: 227, batch: 4, loss: 0.0010550469160079956, acc: 100.0, f1: 100.0, r: 0.6493096671443177
06/02/2019 01:42:16 step: 7501, epoch: 227, batch: 9, loss: 0.0009622126817703247, acc: 100.0, f1: 100.0, r: 0.6871519800580983
06/02/2019 01:42:17 step: 7506, epoch: 227, batch: 14, loss: 0.0025326311588287354, acc: 100.0, f1: 100.0, r: 0.6922983687387295
06/02/2019 01:42:18 step: 7511, epoch: 227, batch: 19, loss: 0.0026799440383911133, acc: 100.0, f1: 100.0, r: 0.7415120549597181
06/02/2019 01:42:19 step: 7516, epoch: 227, batch: 24, loss: 0.0012060478329658508, acc: 100.0, f1: 100.0, r: 0.682202152629297
06/02/2019 01:42:20 step: 7521, epoch: 227, batch: 29, loss: 0.003985047340393066, acc: 100.0, f1: 100.0, r: 0.8318714681065861
06/02/2019 01:42:20 *** evaluating ***
06/02/2019 01:42:20 step: 228, epoch: 227, acc: 58.119658119658126, f1: 29.578809626323714, r: 0.3429483626971248
06/02/2019 01:42:20 *** epoch: 229 ***
06/02/2019 01:42:20 *** training ***
06/02/2019 01:42:21 step: 7529, epoch: 228, batch: 4, loss: 0.001550823450088501, acc: 100.0, f1: 100.0, r: 0.6992501468480683
06/02/2019 01:42:22 step: 7534, epoch: 228, batch: 9, loss: 0.0016684159636497498, acc: 100.0, f1: 100.0, r: 0.7513695945826584
06/02/2019 01:42:23 step: 7539, epoch: 228, batch: 14, loss: 0.0036565959453582764, acc: 100.0, f1: 100.0, r: 0.6911978945307045
06/02/2019 01:42:24 step: 7544, epoch: 228, batch: 19, loss: 0.002671577036380768, acc: 100.0, f1: 100.0, r: 0.7237229401419631
06/02/2019 01:42:25 step: 7549, epoch: 228, batch: 24, loss: 0.004628896713256836, acc: 100.0, f1: 100.0, r: 0.7674662170127093
06/02/2019 01:42:25 step: 7554, epoch: 228, batch: 29, loss: 0.0016519725322723389, acc: 100.0, f1: 100.0, r: 0.7785505878376693
06/02/2019 01:42:26 *** evaluating ***
06/02/2019 01:42:26 step: 229, epoch: 228, acc: 57.26495726495726, f1: 28.637921201349936, r: 0.34084431994054665
06/02/2019 01:42:26 *** epoch: 230 ***
06/02/2019 01:42:26 *** training ***
06/02/2019 01:42:27 step: 7562, epoch: 229, batch: 4, loss: 0.005929641425609589, acc: 100.0, f1: 100.0, r: 0.779485132449572
06/02/2019 01:42:28 step: 7567, epoch: 229, batch: 9, loss: 0.0009335055947303772, acc: 100.0, f1: 100.0, r: 0.7985431631446148
06/02/2019 01:42:29 step: 7572, epoch: 229, batch: 14, loss: 0.002660490572452545, acc: 100.0, f1: 100.0, r: 0.7307141816092881
06/02/2019 01:42:30 step: 7577, epoch: 229, batch: 19, loss: 0.0007404163479804993, acc: 100.0, f1: 100.0, r: 0.7022390076358657
06/02/2019 01:42:30 step: 7582, epoch: 229, batch: 24, loss: 0.010198228061199188, acc: 100.0, f1: 100.0, r: 0.7395118472186845
06/02/2019 01:42:31 step: 7587, epoch: 229, batch: 29, loss: 0.0037314146757125854, acc: 100.0, f1: 100.0, r: 0.7173388925734816
06/02/2019 01:42:32 *** evaluating ***
06/02/2019 01:42:32 step: 230, epoch: 229, acc: 58.97435897435898, f1: 30.848922902494337, r: 0.34490632205986116
06/02/2019 01:42:32 *** epoch: 231 ***
06/02/2019 01:42:32 *** training ***
06/02/2019 01:42:33 step: 7595, epoch: 230, batch: 4, loss: 0.01316242665052414, acc: 100.0, f1: 100.0, r: 0.8239131022887831
06/02/2019 01:42:34 step: 7600, epoch: 230, batch: 9, loss: 0.008012469857931137, acc: 100.0, f1: 100.0, r: 0.7747288878042669
06/02/2019 01:42:35 step: 7605, epoch: 230, batch: 14, loss: 0.0013127326965332031, acc: 100.0, f1: 100.0, r: 0.6728975531388048
06/02/2019 01:42:35 step: 7610, epoch: 230, batch: 19, loss: 0.003273308277130127, acc: 100.0, f1: 100.0, r: 0.654082931036326
06/02/2019 01:42:36 step: 7615, epoch: 230, batch: 24, loss: 0.001741945743560791, acc: 100.0, f1: 100.0, r: 0.74342401429321
06/02/2019 01:42:37 step: 7620, epoch: 230, batch: 29, loss: 0.001315288245677948, acc: 100.0, f1: 100.0, r: 0.8121472741915977
06/02/2019 01:42:38 *** evaluating ***
06/02/2019 01:42:38 step: 231, epoch: 230, acc: 58.97435897435898, f1: 30.255763612498942, r: 0.3462399615374561
06/02/2019 01:42:38 *** epoch: 232 ***
06/02/2019 01:42:38 *** training ***
06/02/2019 01:42:39 step: 7628, epoch: 231, batch: 4, loss: 0.004224911332130432, acc: 100.0, f1: 100.0, r: 0.8250196467917776
06/02/2019 01:42:40 step: 7633, epoch: 231, batch: 9, loss: 0.0020407140254974365, acc: 100.0, f1: 100.0, r: 0.686812005423826
06/02/2019 01:42:40 step: 7638, epoch: 231, batch: 14, loss: 0.004098705947399139, acc: 100.0, f1: 100.0, r: 0.6819890658083908
06/02/2019 01:42:41 step: 7643, epoch: 231, batch: 19, loss: 0.00526033341884613, acc: 100.0, f1: 100.0, r: 0.7661421783352601
06/02/2019 01:42:42 step: 7648, epoch: 231, batch: 24, loss: 0.0032554566860198975, acc: 100.0, f1: 100.0, r: 0.675445546028301
06/02/2019 01:42:43 step: 7653, epoch: 231, batch: 29, loss: 0.0012072548270225525, acc: 100.0, f1: 100.0, r: 0.7182293229932734
06/02/2019 01:42:43 *** evaluating ***
06/02/2019 01:42:44 step: 232, epoch: 231, acc: 58.54700854700855, f1: 32.242573815509154, r: 0.3434006327080287
06/02/2019 01:42:44 *** epoch: 233 ***
06/02/2019 01:42:44 *** training ***
06/02/2019 01:42:44 step: 7661, epoch: 232, batch: 4, loss: 0.0016844794154167175, acc: 100.0, f1: 100.0, r: 0.7767793520474804
06/02/2019 01:42:45 step: 7666, epoch: 232, batch: 9, loss: 0.0018802210688591003, acc: 100.0, f1: 100.0, r: 0.7199792007696487
06/02/2019 01:42:46 step: 7671, epoch: 232, batch: 14, loss: 0.01868705451488495, acc: 100.0, f1: 100.0, r: 0.6975945371268122
06/02/2019 01:42:47 step: 7676, epoch: 232, batch: 19, loss: 0.005793869495391846, acc: 100.0, f1: 100.0, r: 0.8010672334623377
06/02/2019 01:42:48 step: 7681, epoch: 232, batch: 24, loss: 0.0011767446994781494, acc: 100.0, f1: 100.0, r: 0.6521894193955191
06/02/2019 01:42:49 step: 7686, epoch: 232, batch: 29, loss: 0.0021627768874168396, acc: 100.0, f1: 100.0, r: 0.693774712259557
06/02/2019 01:42:49 *** evaluating ***
06/02/2019 01:42:49 step: 233, epoch: 232, acc: 56.837606837606835, f1: 28.817427385892113, r: 0.33948381208082495
06/02/2019 01:42:49 *** epoch: 234 ***
06/02/2019 01:42:49 *** training ***
06/02/2019 01:42:50 step: 7694, epoch: 233, batch: 4, loss: 0.002555452287197113, acc: 100.0, f1: 100.0, r: 0.7810530375777276
06/02/2019 01:42:51 step: 7699, epoch: 233, batch: 9, loss: 0.0021145641803741455, acc: 100.0, f1: 100.0, r: 0.843595761118701
06/02/2019 01:42:52 step: 7704, epoch: 233, batch: 14, loss: 0.0011466220021247864, acc: 100.0, f1: 100.0, r: 0.7887376665033496
06/02/2019 01:42:53 step: 7709, epoch: 233, batch: 19, loss: 0.0022315606474876404, acc: 100.0, f1: 100.0, r: 0.602294822298501
06/02/2019 01:42:54 step: 7714, epoch: 233, batch: 24, loss: 0.001376807689666748, acc: 100.0, f1: 100.0, r: 0.7331018888645304
06/02/2019 01:42:55 step: 7719, epoch: 233, batch: 29, loss: 0.004145968705415726, acc: 100.0, f1: 100.0, r: 0.6748100012423188
06/02/2019 01:42:55 *** evaluating ***
06/02/2019 01:42:55 step: 234, epoch: 233, acc: 58.97435897435898, f1: 30.90411632144227, r: 0.33765882841145295
06/02/2019 01:42:55 *** epoch: 235 ***
06/02/2019 01:42:55 *** training ***
06/02/2019 01:42:56 step: 7727, epoch: 234, batch: 4, loss: 0.0024074986577033997, acc: 100.0, f1: 100.0, r: 0.6880512943470553
06/02/2019 01:42:57 step: 7732, epoch: 234, batch: 9, loss: 0.0009632781147956848, acc: 100.0, f1: 100.0, r: 0.8091776854388386
06/02/2019 01:42:58 step: 7737, epoch: 234, batch: 14, loss: 0.0016484260559082031, acc: 100.0, f1: 100.0, r: 0.7434771996998523
06/02/2019 01:42:59 step: 7742, epoch: 234, batch: 19, loss: 0.0011205151677131653, acc: 100.0, f1: 100.0, r: 0.8653091231797729
06/02/2019 01:43:00 step: 7747, epoch: 234, batch: 24, loss: 0.000737704336643219, acc: 100.0, f1: 100.0, r: 0.580937806144539
06/02/2019 01:43:01 step: 7752, epoch: 234, batch: 29, loss: 0.0009757131338119507, acc: 100.0, f1: 100.0, r: 0.8399290845036018
06/02/2019 01:43:01 *** evaluating ***
06/02/2019 01:43:01 step: 235, epoch: 234, acc: 56.41025641025641, f1: 26.78102186480067, r: 0.3308249390801161
06/02/2019 01:43:01 *** epoch: 236 ***
06/02/2019 01:43:01 *** training ***
06/02/2019 01:43:02 step: 7760, epoch: 235, batch: 4, loss: 0.0019587725400924683, acc: 100.0, f1: 100.0, r: 0.784513818077307
06/02/2019 01:43:03 step: 7765, epoch: 235, batch: 9, loss: 0.0015103816986083984, acc: 100.0, f1: 100.0, r: 0.600480172062874
06/02/2019 01:43:04 step: 7770, epoch: 235, batch: 14, loss: 0.0007051676511764526, acc: 100.0, f1: 100.0, r: 0.7372913001710696
06/02/2019 01:43:05 step: 7775, epoch: 235, batch: 19, loss: 0.0013501718640327454, acc: 100.0, f1: 100.0, r: 0.7596845679052203
06/02/2019 01:43:05 step: 7780, epoch: 235, batch: 24, loss: 0.0016832724213600159, acc: 100.0, f1: 100.0, r: 0.6650871499430082
06/02/2019 01:43:06 step: 7785, epoch: 235, batch: 29, loss: 0.004072636365890503, acc: 100.0, f1: 100.0, r: 0.7930152362812435
06/02/2019 01:43:07 *** evaluating ***
06/02/2019 01:43:07 step: 236, epoch: 235, acc: 55.98290598290598, f1: 26.3391720583082, r: 0.3293614733218523
06/02/2019 01:43:07 *** epoch: 237 ***
06/02/2019 01:43:07 *** training ***
06/02/2019 01:43:08 step: 7793, epoch: 236, batch: 4, loss: 0.0011397600173950195, acc: 100.0, f1: 100.0, r: 0.8308078017706776
06/02/2019 01:43:09 step: 7798, epoch: 236, batch: 9, loss: 0.0030473992228507996, acc: 100.0, f1: 100.0, r: 0.8381000217732335
06/02/2019 01:43:10 step: 7803, epoch: 236, batch: 14, loss: 0.006078086793422699, acc: 100.0, f1: 100.0, r: 0.7007477961030483
06/02/2019 01:43:10 step: 7808, epoch: 236, batch: 19, loss: 0.002062171697616577, acc: 100.0, f1: 100.0, r: 0.7265733476259946
06/02/2019 01:43:11 step: 7813, epoch: 236, batch: 24, loss: 0.0030434392392635345, acc: 100.0, f1: 100.0, r: 0.8048698871628271
06/02/2019 01:43:12 step: 7818, epoch: 236, batch: 29, loss: 0.002218380570411682, acc: 100.0, f1: 100.0, r: 0.7532602959412882
06/02/2019 01:43:13 *** evaluating ***
06/02/2019 01:43:13 step: 237, epoch: 236, acc: 55.98290598290598, f1: 27.57958035534889, r: 0.33306695988331264
06/02/2019 01:43:13 *** epoch: 238 ***
06/02/2019 01:43:13 *** training ***
06/02/2019 01:43:14 step: 7826, epoch: 237, batch: 4, loss: 0.003826126456260681, acc: 100.0, f1: 100.0, r: 0.7388268764257278
06/02/2019 01:43:15 step: 7831, epoch: 237, batch: 9, loss: 0.003425121307373047, acc: 100.0, f1: 100.0, r: 0.7698263393228133
06/02/2019 01:43:15 step: 7836, epoch: 237, batch: 14, loss: 0.00047297775745391846, acc: 100.0, f1: 100.0, r: 0.7450742250989408
06/02/2019 01:43:16 step: 7841, epoch: 237, batch: 19, loss: 0.0019373893737792969, acc: 100.0, f1: 100.0, r: 0.8108614903434878
06/02/2019 01:43:17 step: 7846, epoch: 237, batch: 24, loss: 0.0015150383114814758, acc: 100.0, f1: 100.0, r: 0.7080999284007005
06/02/2019 01:43:18 step: 7851, epoch: 237, batch: 29, loss: 0.006989613175392151, acc: 100.0, f1: 100.0, r: 0.7406261801860007
06/02/2019 01:43:18 *** evaluating ***
06/02/2019 01:43:19 step: 238, epoch: 237, acc: 56.837606837606835, f1: 29.176142169704406, r: 0.32943485124283917
06/02/2019 01:43:19 *** epoch: 239 ***
06/02/2019 01:43:19 *** training ***
06/02/2019 01:43:20 step: 7859, epoch: 238, batch: 4, loss: 0.000600084662437439, acc: 100.0, f1: 100.0, r: 0.6892482372064691
06/02/2019 01:43:20 step: 7864, epoch: 238, batch: 9, loss: 0.0008478462696075439, acc: 100.0, f1: 100.0, r: 0.8127390940352555
06/02/2019 01:43:21 step: 7869, epoch: 238, batch: 14, loss: 0.001383669674396515, acc: 100.0, f1: 100.0, r: 0.6715615086582081
06/02/2019 01:43:22 step: 7874, epoch: 238, batch: 19, loss: 0.0023716576397418976, acc: 100.0, f1: 100.0, r: 0.8466044269320646
06/02/2019 01:43:23 step: 7879, epoch: 238, batch: 24, loss: 0.004638314247131348, acc: 100.0, f1: 100.0, r: 0.7311964373477589
06/02/2019 01:43:24 step: 7884, epoch: 238, batch: 29, loss: 0.012523733079433441, acc: 98.4375, f1: 99.26406926406926, r: 0.7515824557305962
06/02/2019 01:43:24 *** evaluating ***
06/02/2019 01:43:24 step: 239, epoch: 238, acc: 59.82905982905983, f1: 31.081866174947585, r: 0.34768209936683964
06/02/2019 01:43:24 *** epoch: 240 ***
06/02/2019 01:43:24 *** training ***
06/02/2019 01:43:25 step: 7892, epoch: 239, batch: 4, loss: 0.005752824246883392, acc: 100.0, f1: 100.0, r: 0.787335773635261
06/02/2019 01:43:26 step: 7897, epoch: 239, batch: 9, loss: 0.006153210997581482, acc: 100.0, f1: 100.0, r: 0.7579163346443832
06/02/2019 01:43:27 step: 7902, epoch: 239, batch: 14, loss: 0.003078930079936981, acc: 100.0, f1: 100.0, r: 0.7299546796050264
06/02/2019 01:43:28 step: 7907, epoch: 239, batch: 19, loss: 0.0011077150702476501, acc: 100.0, f1: 100.0, r: 0.6850209477510593
06/02/2019 01:43:29 step: 7912, epoch: 239, batch: 24, loss: 0.0005568712949752808, acc: 100.0, f1: 100.0, r: 0.8122070773304143
06/02/2019 01:43:29 step: 7917, epoch: 239, batch: 29, loss: 0.0037026405334472656, acc: 100.0, f1: 100.0, r: 0.6812133452685564
06/02/2019 01:43:30 *** evaluating ***
06/02/2019 01:43:30 step: 240, epoch: 239, acc: 57.692307692307686, f1: 29.721568991480495, r: 0.34769656551883954
06/02/2019 01:43:30 *** epoch: 241 ***
06/02/2019 01:43:30 *** training ***
06/02/2019 01:43:31 step: 7925, epoch: 240, batch: 4, loss: 0.004504017531871796, acc: 100.0, f1: 100.0, r: 0.6591883416978944
06/02/2019 01:43:32 step: 7930, epoch: 240, batch: 9, loss: 0.007755771279335022, acc: 100.0, f1: 100.0, r: 0.8043391997788814
06/02/2019 01:43:33 step: 7935, epoch: 240, batch: 14, loss: 0.005400478839874268, acc: 100.0, f1: 100.0, r: 0.6466638004921397
06/02/2019 01:43:34 step: 7940, epoch: 240, batch: 19, loss: 0.0009387359023094177, acc: 100.0, f1: 100.0, r: 0.7120415495577973
06/02/2019 01:43:34 step: 7945, epoch: 240, batch: 24, loss: 0.0010235458612442017, acc: 100.0, f1: 100.0, r: 0.8261855671308104
06/02/2019 01:43:35 step: 7950, epoch: 240, batch: 29, loss: 0.001999504864215851, acc: 100.0, f1: 100.0, r: 0.7906952376584689
06/02/2019 01:43:36 *** evaluating ***
06/02/2019 01:43:36 step: 241, epoch: 240, acc: 58.119658119658126, f1: 29.74343464928814, r: 0.35005827091276426
06/02/2019 01:43:36 *** epoch: 242 ***
06/02/2019 01:43:36 *** training ***
06/02/2019 01:43:37 step: 7958, epoch: 241, batch: 4, loss: 0.0014472529292106628, acc: 100.0, f1: 100.0, r: 0.5938376129358319
06/02/2019 01:43:38 step: 7963, epoch: 241, batch: 9, loss: 0.002698242664337158, acc: 100.0, f1: 100.0, r: 0.7953128762303961
06/02/2019 01:43:38 step: 7968, epoch: 241, batch: 14, loss: 0.0013968050479888916, acc: 100.0, f1: 100.0, r: 0.7647312381480043
06/02/2019 01:43:39 step: 7973, epoch: 241, batch: 19, loss: 0.0015674233436584473, acc: 100.0, f1: 100.0, r: 0.6938053963051142
06/02/2019 01:43:40 step: 7978, epoch: 241, batch: 24, loss: 0.0021399706602096558, acc: 100.0, f1: 100.0, r: 0.6328109179603619
06/02/2019 01:43:41 step: 7983, epoch: 241, batch: 29, loss: 0.0021107271313667297, acc: 100.0, f1: 100.0, r: 0.7794321016698869
06/02/2019 01:43:41 *** evaluating ***
06/02/2019 01:43:41 step: 242, epoch: 241, acc: 57.692307692307686, f1: 30.525626019188252, r: 0.34910980129931257
06/02/2019 01:43:41 *** epoch: 243 ***
06/02/2019 01:43:41 *** training ***
06/02/2019 01:43:42 step: 7991, epoch: 242, batch: 4, loss: 0.000769883394241333, acc: 100.0, f1: 100.0, r: 0.8094359104618177
06/02/2019 01:43:43 step: 7996, epoch: 242, batch: 9, loss: 0.0006245523691177368, acc: 100.0, f1: 100.0, r: 0.7021462599298846
06/02/2019 01:43:44 step: 8001, epoch: 242, batch: 14, loss: 0.0016024783253669739, acc: 100.0, f1: 100.0, r: 0.6586624803850005
06/02/2019 01:43:45 step: 8006, epoch: 242, batch: 19, loss: 0.0024556592106819153, acc: 100.0, f1: 100.0, r: 0.6901204188457359
06/02/2019 01:43:46 step: 8011, epoch: 242, batch: 24, loss: 0.0024899914860725403, acc: 100.0, f1: 100.0, r: 0.6618645430142578
06/02/2019 01:43:46 step: 8016, epoch: 242, batch: 29, loss: 0.008535578846931458, acc: 100.0, f1: 100.0, r: 0.749237444604582
06/02/2019 01:43:47 *** evaluating ***
06/02/2019 01:43:47 step: 243, epoch: 242, acc: 55.55555555555556, f1: 30.665575542361278, r: 0.339069982608517
06/02/2019 01:43:47 *** epoch: 244 ***
06/02/2019 01:43:47 *** training ***
06/02/2019 01:43:48 step: 8024, epoch: 243, batch: 4, loss: 0.020746354013681412, acc: 98.4375, f1: 99.16883116883116, r: 0.6858659831856405
06/02/2019 01:43:49 step: 8029, epoch: 243, batch: 9, loss: 0.012101829051971436, acc: 100.0, f1: 100.0, r: 0.7050548383569448
06/02/2019 01:43:50 step: 8034, epoch: 243, batch: 14, loss: 0.003402411937713623, acc: 100.0, f1: 100.0, r: 0.8134011764281185
06/02/2019 01:43:50 step: 8039, epoch: 243, batch: 19, loss: 0.00522170215845108, acc: 100.0, f1: 100.0, r: 0.8030504109462163
06/02/2019 01:43:51 step: 8044, epoch: 243, batch: 24, loss: 0.0039849430322647095, acc: 100.0, f1: 100.0, r: 0.8055122077817676
06/02/2019 01:43:52 step: 8049, epoch: 243, batch: 29, loss: 0.0014784187078475952, acc: 100.0, f1: 100.0, r: 0.7029891474020651
06/02/2019 01:43:53 *** evaluating ***
06/02/2019 01:43:53 step: 244, epoch: 243, acc: 57.692307692307686, f1: 27.54399501841023, r: 0.3439744686799601
06/02/2019 01:43:53 *** epoch: 245 ***
06/02/2019 01:43:53 *** training ***
06/02/2019 01:43:54 step: 8057, epoch: 244, batch: 4, loss: 0.0030085742473602295, acc: 100.0, f1: 100.0, r: 0.6893427712360309
06/02/2019 01:43:55 step: 8062, epoch: 244, batch: 9, loss: 0.008308380842208862, acc: 100.0, f1: 100.0, r: 0.76076303623598
06/02/2019 01:43:55 step: 8067, epoch: 244, batch: 14, loss: 0.03311767801642418, acc: 98.4375, f1: 99.21115921115921, r: 0.749963616555259
06/02/2019 01:43:56 step: 8072, epoch: 244, batch: 19, loss: 0.0020134299993515015, acc: 100.0, f1: 100.0, r: 0.754320795024154
06/02/2019 01:43:57 step: 8077, epoch: 244, batch: 24, loss: 0.0012064799666404724, acc: 100.0, f1: 100.0, r: 0.7782304163587355
06/02/2019 01:43:58 step: 8082, epoch: 244, batch: 29, loss: 0.0037854090332984924, acc: 100.0, f1: 100.0, r: 0.687295999504029
06/02/2019 01:43:58 *** evaluating ***
06/02/2019 01:43:59 step: 245, epoch: 244, acc: 59.82905982905983, f1: 31.171350530372077, r: 0.35293127017892806
06/02/2019 01:43:59 *** epoch: 246 ***
06/02/2019 01:43:59 *** training ***
06/02/2019 01:43:59 step: 8090, epoch: 245, batch: 4, loss: 0.007112674415111542, acc: 100.0, f1: 100.0, r: 0.682167248205036
06/02/2019 01:44:00 step: 8095, epoch: 245, batch: 9, loss: 0.0017367750406265259, acc: 100.0, f1: 100.0, r: 0.7456035532136112
06/02/2019 01:44:01 step: 8100, epoch: 245, batch: 14, loss: 0.001355282962322235, acc: 100.0, f1: 100.0, r: 0.8129167282824405
06/02/2019 01:44:02 step: 8105, epoch: 245, batch: 19, loss: 0.005618184804916382, acc: 100.0, f1: 100.0, r: 0.7768116423100228
06/02/2019 01:44:03 step: 8110, epoch: 245, batch: 24, loss: 0.0024863332509994507, acc: 100.0, f1: 100.0, r: 0.7421599969019234
06/02/2019 01:44:03 step: 8115, epoch: 245, batch: 29, loss: 0.0012300089001655579, acc: 100.0, f1: 100.0, r: 0.692208050671678
06/02/2019 01:44:04 *** evaluating ***
06/02/2019 01:44:04 step: 246, epoch: 245, acc: 57.692307692307686, f1: 29.74324526048664, r: 0.3537745286373323
06/02/2019 01:44:04 *** epoch: 247 ***
06/02/2019 01:44:04 *** training ***
06/02/2019 01:44:05 step: 8123, epoch: 246, batch: 4, loss: 0.00117599219083786, acc: 100.0, f1: 100.0, r: 0.7876365723447389
06/02/2019 01:44:06 step: 8128, epoch: 246, batch: 9, loss: 0.004821568727493286, acc: 100.0, f1: 100.0, r: 0.7453303018369936
06/02/2019 01:44:07 step: 8133, epoch: 246, batch: 14, loss: 0.0007375925779342651, acc: 100.0, f1: 100.0, r: 0.7852118344140198
06/02/2019 01:44:07 step: 8138, epoch: 246, batch: 19, loss: 0.010025262832641602, acc: 100.0, f1: 100.0, r: 0.7869937799613321
06/02/2019 01:44:08 step: 8143, epoch: 246, batch: 24, loss: 0.016086459159851074, acc: 98.4375, f1: 97.78325123152709, r: 0.7982710602481669
06/02/2019 01:44:09 step: 8148, epoch: 246, batch: 29, loss: 0.004786945879459381, acc: 100.0, f1: 100.0, r: 0.7247502167634895
06/02/2019 01:44:09 *** evaluating ***
06/02/2019 01:44:10 step: 247, epoch: 246, acc: 56.837606837606835, f1: 30.300824701546414, r: 0.3341335605246732
06/02/2019 01:44:10 *** epoch: 248 ***
06/02/2019 01:44:10 *** training ***
06/02/2019 01:44:11 step: 8156, epoch: 247, batch: 4, loss: 0.004235565662384033, acc: 100.0, f1: 100.0, r: 0.7493679493080744
06/02/2019 01:44:11 step: 8161, epoch: 247, batch: 9, loss: 0.004897691309452057, acc: 100.0, f1: 100.0, r: 0.6724307694277472
06/02/2019 01:44:12 step: 8166, epoch: 247, batch: 14, loss: 0.012687459588050842, acc: 100.0, f1: 100.0, r: 0.7471644786408059
06/02/2019 01:44:13 step: 8171, epoch: 247, batch: 19, loss: 0.0008119717240333557, acc: 100.0, f1: 100.0, r: 0.7617774277654812
06/02/2019 01:44:14 step: 8176, epoch: 247, batch: 24, loss: 0.00159367173910141, acc: 100.0, f1: 100.0, r: 0.826167030393965
06/02/2019 01:44:15 step: 8181, epoch: 247, batch: 29, loss: 0.0027201324701309204, acc: 100.0, f1: 100.0, r: 0.7042188370277972
06/02/2019 01:44:15 *** evaluating ***
06/02/2019 01:44:16 step: 248, epoch: 247, acc: 57.692307692307686, f1: 29.939317439317435, r: 0.3428864352419075
06/02/2019 01:44:16 *** epoch: 249 ***
06/02/2019 01:44:16 *** training ***
06/02/2019 01:44:16 step: 8189, epoch: 248, batch: 4, loss: 0.002119973301887512, acc: 100.0, f1: 100.0, r: 0.7015950198837253
06/02/2019 01:44:17 step: 8194, epoch: 248, batch: 9, loss: 0.011874817311763763, acc: 100.0, f1: 100.0, r: 0.7920662416668963
06/02/2019 01:44:18 step: 8199, epoch: 248, batch: 14, loss: 0.0041932612657547, acc: 100.0, f1: 100.0, r: 0.8264353803999438
06/02/2019 01:44:19 step: 8204, epoch: 248, batch: 19, loss: 0.0018814131617546082, acc: 100.0, f1: 100.0, r: 0.738223970062075
06/02/2019 01:44:20 step: 8209, epoch: 248, batch: 24, loss: 0.003802098333835602, acc: 100.0, f1: 100.0, r: 0.8339500062876383
06/02/2019 01:44:21 step: 8214, epoch: 248, batch: 29, loss: 0.0033838823437690735, acc: 100.0, f1: 100.0, r: 0.7450999901677975
06/02/2019 01:44:21 *** evaluating ***
06/02/2019 01:44:21 step: 249, epoch: 248, acc: 57.26495726495726, f1: 28.89249750633429, r: 0.3451120686756425
06/02/2019 01:44:21 *** epoch: 250 ***
06/02/2019 01:44:21 *** training ***
06/02/2019 01:44:22 step: 8222, epoch: 249, batch: 4, loss: 0.0018629059195518494, acc: 100.0, f1: 100.0, r: 0.7199007207825135
06/02/2019 01:44:23 step: 8227, epoch: 249, batch: 9, loss: 0.0028036832809448242, acc: 100.0, f1: 100.0, r: 0.8082701697931324
06/02/2019 01:44:24 step: 8232, epoch: 249, batch: 14, loss: 0.0007485747337341309, acc: 100.0, f1: 100.0, r: 0.800358309243541
06/02/2019 01:44:25 step: 8237, epoch: 249, batch: 19, loss: 0.0015801265835762024, acc: 100.0, f1: 100.0, r: 0.7495250384389648
06/02/2019 01:44:25 step: 8242, epoch: 249, batch: 24, loss: 0.0038296878337860107, acc: 100.0, f1: 100.0, r: 0.8073805724360497
06/02/2019 01:44:26 step: 8247, epoch: 249, batch: 29, loss: 0.006831187754869461, acc: 100.0, f1: 100.0, r: 0.779406515522919
06/02/2019 01:44:27 *** evaluating ***
06/02/2019 01:44:27 step: 250, epoch: 249, acc: 58.119658119658126, f1: 29.820010916523433, r: 0.34205397122658354
06/02/2019 01:44:27 *** epoch: 251 ***
06/02/2019 01:44:27 *** training ***
06/02/2019 01:44:28 step: 8255, epoch: 250, batch: 4, loss: 0.006276361644268036, acc: 100.0, f1: 100.0, r: 0.7232966464032057
06/02/2019 01:44:29 step: 8260, epoch: 250, batch: 9, loss: 0.0009265616536140442, acc: 100.0, f1: 100.0, r: 0.765841895830726
06/02/2019 01:44:29 step: 8265, epoch: 250, batch: 14, loss: 0.0043787844479084015, acc: 100.0, f1: 100.0, r: 0.6804263352041802
06/02/2019 01:44:30 step: 8270, epoch: 250, batch: 19, loss: 0.0015700459480285645, acc: 100.0, f1: 100.0, r: 0.7443579566556915
06/02/2019 01:44:31 step: 8275, epoch: 250, batch: 24, loss: 0.005235366523265839, acc: 100.0, f1: 100.0, r: 0.7215952830755371
06/02/2019 01:44:32 step: 8280, epoch: 250, batch: 29, loss: 0.0035182759165763855, acc: 100.0, f1: 100.0, r: 0.6826009925673505
06/02/2019 01:44:32 *** evaluating ***
06/02/2019 01:44:33 step: 251, epoch: 250, acc: 56.837606837606835, f1: 29.85335604251534, r: 0.33542688005279875
06/02/2019 01:44:33 *** epoch: 252 ***
06/02/2019 01:44:33 *** training ***
06/02/2019 01:44:34 step: 8288, epoch: 251, batch: 4, loss: 0.0016508102416992188, acc: 100.0, f1: 100.0, r: 0.8080101847875648
06/02/2019 01:44:34 step: 8293, epoch: 251, batch: 9, loss: 0.003785170614719391, acc: 100.0, f1: 100.0, r: 0.8315254509066609
06/02/2019 01:44:35 step: 8298, epoch: 251, batch: 14, loss: 0.0052436478435993195, acc: 100.0, f1: 100.0, r: 0.7765130113154131
06/02/2019 01:44:36 step: 8303, epoch: 251, batch: 19, loss: 0.0024447478353977203, acc: 100.0, f1: 100.0, r: 0.7531083058623689
06/02/2019 01:44:37 step: 8308, epoch: 251, batch: 24, loss: 0.0006105229258537292, acc: 100.0, f1: 100.0, r: 0.8311605726076795
06/02/2019 01:44:38 step: 8313, epoch: 251, batch: 29, loss: 0.002735935151576996, acc: 100.0, f1: 100.0, r: 0.7153490924340921
06/02/2019 01:44:38 *** evaluating ***
06/02/2019 01:44:38 step: 252, epoch: 251, acc: 58.97435897435898, f1: 30.557250255336204, r: 0.3439286254712815
06/02/2019 01:44:38 *** epoch: 253 ***
06/02/2019 01:44:38 *** training ***
06/02/2019 01:44:39 step: 8321, epoch: 252, batch: 4, loss: 0.0014938488602638245, acc: 100.0, f1: 100.0, r: 0.7694890407740341
06/02/2019 01:44:40 step: 8326, epoch: 252, batch: 9, loss: 0.0010167434811592102, acc: 100.0, f1: 100.0, r: 0.7843542736097505
06/02/2019 01:44:41 step: 8331, epoch: 252, batch: 14, loss: 0.003793582320213318, acc: 100.0, f1: 100.0, r: 0.6601058598785199
06/02/2019 01:44:42 step: 8336, epoch: 252, batch: 19, loss: 0.000992901623249054, acc: 100.0, f1: 100.0, r: 0.8192224287104758
06/02/2019 01:44:43 step: 8341, epoch: 252, batch: 24, loss: 0.004202120006084442, acc: 100.0, f1: 100.0, r: 0.8095892850327475
06/02/2019 01:44:43 step: 8346, epoch: 252, batch: 29, loss: 0.0032015666365623474, acc: 100.0, f1: 100.0, r: 0.696441591022474
06/02/2019 01:44:44 *** evaluating ***
06/02/2019 01:44:44 step: 253, epoch: 252, acc: 58.97435897435898, f1: 30.837180809503728, r: 0.34908827334607906
06/02/2019 01:44:44 *** epoch: 254 ***
06/02/2019 01:44:44 *** training ***
06/02/2019 01:44:45 step: 8354, epoch: 253, batch: 4, loss: 0.003381989896297455, acc: 100.0, f1: 100.0, r: 0.6966652315502105
06/02/2019 01:44:46 step: 8359, epoch: 253, batch: 9, loss: 0.002646900713443756, acc: 100.0, f1: 100.0, r: 0.7857451582985351
06/02/2019 01:44:47 step: 8364, epoch: 253, batch: 14, loss: 0.0009705722332000732, acc: 100.0, f1: 100.0, r: 0.8138842586184871
06/02/2019 01:44:47 step: 8369, epoch: 253, batch: 19, loss: 0.007432825863361359, acc: 100.0, f1: 100.0, r: 0.7532972784984836
06/02/2019 01:44:48 step: 8374, epoch: 253, batch: 24, loss: 0.0034750252962112427, acc: 100.0, f1: 100.0, r: 0.8001432047549621
06/02/2019 01:44:49 step: 8379, epoch: 253, batch: 29, loss: 0.0017532780766487122, acc: 100.0, f1: 100.0, r: 0.832910184780533
06/02/2019 01:44:49 *** evaluating ***
06/02/2019 01:44:50 step: 254, epoch: 253, acc: 57.26495726495726, f1: 29.54573914564819, r: 0.3616812703966096
06/02/2019 01:44:50 *** epoch: 255 ***
06/02/2019 01:44:50 *** training ***
06/02/2019 01:44:50 step: 8387, epoch: 254, batch: 4, loss: 0.0012392997741699219, acc: 100.0, f1: 100.0, r: 0.7771025588538277
06/02/2019 01:44:51 step: 8392, epoch: 254, batch: 9, loss: 0.0016586333513259888, acc: 100.0, f1: 100.0, r: 0.7913665132725717
06/02/2019 01:44:52 step: 8397, epoch: 254, batch: 14, loss: 0.0021273568272590637, acc: 100.0, f1: 100.0, r: 0.7922305086809853
06/02/2019 01:44:53 step: 8402, epoch: 254, batch: 19, loss: 0.0015731602907180786, acc: 100.0, f1: 100.0, r: 0.7224387267809145
06/02/2019 01:44:54 step: 8407, epoch: 254, batch: 24, loss: 0.007807031273841858, acc: 100.0, f1: 100.0, r: 0.7627379737879045
06/02/2019 01:44:55 step: 8412, epoch: 254, batch: 29, loss: 0.0005480200052261353, acc: 100.0, f1: 100.0, r: 0.7030590517689075
06/02/2019 01:44:55 *** evaluating ***
06/02/2019 01:44:55 step: 255, epoch: 254, acc: 57.692307692307686, f1: 29.9703190260375, r: 0.35184316277083866
06/02/2019 01:44:55 *** epoch: 256 ***
06/02/2019 01:44:55 *** training ***
06/02/2019 01:44:56 step: 8420, epoch: 255, batch: 4, loss: 0.0005509555339813232, acc: 100.0, f1: 100.0, r: 0.7099289410940257
06/02/2019 01:44:57 step: 8425, epoch: 255, batch: 9, loss: 0.0039554014801979065, acc: 100.0, f1: 100.0, r: 0.7299592978222442
06/02/2019 01:44:58 step: 8430, epoch: 255, batch: 14, loss: 0.00148676335811615, acc: 100.0, f1: 100.0, r: 0.7959990953387067
06/02/2019 01:44:59 step: 8435, epoch: 255, batch: 19, loss: 0.005055874586105347, acc: 100.0, f1: 100.0, r: 0.7197823092867849
06/02/2019 01:45:00 step: 8440, epoch: 255, batch: 24, loss: 0.002565436065196991, acc: 100.0, f1: 100.0, r: 0.7817315163665332
06/02/2019 01:45:00 step: 8445, epoch: 255, batch: 29, loss: 0.0031235218048095703, acc: 100.0, f1: 100.0, r: 0.6911468055925681
06/02/2019 01:45:01 *** evaluating ***
06/02/2019 01:45:01 step: 256, epoch: 255, acc: 57.26495726495726, f1: 30.118120098768443, r: 0.3418128055879185
06/02/2019 01:45:01 *** epoch: 257 ***
06/02/2019 01:45:01 *** training ***
06/02/2019 01:45:02 step: 8453, epoch: 256, batch: 4, loss: 0.0015022456645965576, acc: 100.0, f1: 100.0, r: 0.7554938820970148
06/02/2019 01:45:03 step: 8458, epoch: 256, batch: 9, loss: 0.0033295229077339172, acc: 100.0, f1: 100.0, r: 0.8111203369949651
06/02/2019 01:45:04 step: 8463, epoch: 256, batch: 14, loss: 0.004868894815444946, acc: 100.0, f1: 100.0, r: 0.7677109340009333
06/02/2019 01:45:04 step: 8468, epoch: 256, batch: 19, loss: 0.002634376287460327, acc: 100.0, f1: 100.0, r: 0.7211318333539517
06/02/2019 01:45:05 step: 8473, epoch: 256, batch: 24, loss: 0.0008609145879745483, acc: 100.0, f1: 100.0, r: 0.8475483404781181
06/02/2019 01:45:06 step: 8478, epoch: 256, batch: 29, loss: 0.0014583542943000793, acc: 100.0, f1: 100.0, r: 0.7869838668421175
06/02/2019 01:45:06 *** evaluating ***
06/02/2019 01:45:07 step: 257, epoch: 256, acc: 55.12820512820513, f1: 30.985404931735406, r: 0.3559352272491958
06/02/2019 01:45:07 *** epoch: 258 ***
06/02/2019 01:45:07 *** training ***
06/02/2019 01:45:07 step: 8486, epoch: 257, batch: 4, loss: 0.001921921968460083, acc: 100.0, f1: 100.0, r: 0.7644348819980371
06/02/2019 01:45:08 step: 8491, epoch: 257, batch: 9, loss: 0.003693588078022003, acc: 100.0, f1: 100.0, r: 0.6548805720881837
06/02/2019 01:45:09 step: 8496, epoch: 257, batch: 14, loss: 0.004866957664489746, acc: 100.0, f1: 100.0, r: 0.7025667582304725
06/02/2019 01:45:10 step: 8501, epoch: 257, batch: 19, loss: 0.0007898956537246704, acc: 100.0, f1: 100.0, r: 0.7143951889857995
06/02/2019 01:45:11 step: 8506, epoch: 257, batch: 24, loss: 0.002172708511352539, acc: 100.0, f1: 100.0, r: 0.6738141750759195
06/02/2019 01:45:12 step: 8511, epoch: 257, batch: 29, loss: 0.0012384355068206787, acc: 100.0, f1: 100.0, r: 0.7954713752843865
06/02/2019 01:45:12 *** evaluating ***
06/02/2019 01:45:12 step: 258, epoch: 257, acc: 56.837606837606835, f1: 30.477265743606996, r: 0.33961567754975147
06/02/2019 01:45:12 *** epoch: 259 ***
06/02/2019 01:45:12 *** training ***
06/02/2019 01:45:13 step: 8519, epoch: 258, batch: 4, loss: 0.001720711588859558, acc: 100.0, f1: 100.0, r: 0.7274368114931551
06/02/2019 01:45:14 step: 8524, epoch: 258, batch: 9, loss: 0.0010808855295181274, acc: 100.0, f1: 100.0, r: 0.7554616132188391
06/02/2019 01:45:15 step: 8529, epoch: 258, batch: 14, loss: 0.002300940454006195, acc: 100.0, f1: 100.0, r: 0.7931601213516629
06/02/2019 01:45:16 step: 8534, epoch: 258, batch: 19, loss: 0.004775680601596832, acc: 100.0, f1: 100.0, r: 0.8638303870436947
06/02/2019 01:45:16 step: 8539, epoch: 258, batch: 24, loss: 0.0038795284926891327, acc: 100.0, f1: 100.0, r: 0.8152473952964421
06/02/2019 01:45:17 step: 8544, epoch: 258, batch: 29, loss: 0.015233427286148071, acc: 100.0, f1: 100.0, r: 0.7463575288318571
06/02/2019 01:45:18 *** evaluating ***
06/02/2019 01:45:18 step: 259, epoch: 258, acc: 57.26495726495726, f1: 30.757554453420592, r: 0.3322167936914884
06/02/2019 01:45:18 *** epoch: 260 ***
06/02/2019 01:45:18 *** training ***
06/02/2019 01:45:19 step: 8552, epoch: 259, batch: 4, loss: 0.002287723124027252, acc: 100.0, f1: 100.0, r: 0.8239286549217023
06/02/2019 01:45:20 step: 8557, epoch: 259, batch: 9, loss: 0.019096732139587402, acc: 98.4375, f1: 98.63636363636363, r: 0.7572839699923122
06/02/2019 01:45:21 step: 8562, epoch: 259, batch: 14, loss: 0.001824215054512024, acc: 100.0, f1: 100.0, r: 0.6760964025240935
06/02/2019 01:45:21 step: 8567, epoch: 259, batch: 19, loss: 0.004655946046113968, acc: 100.0, f1: 100.0, r: 0.7380229809126189
06/02/2019 01:45:22 step: 8572, epoch: 259, batch: 24, loss: 0.0013390257954597473, acc: 100.0, f1: 100.0, r: 0.8074518180735013
06/02/2019 01:45:23 step: 8577, epoch: 259, batch: 29, loss: 0.0010782405734062195, acc: 100.0, f1: 100.0, r: 0.7931741243997531
06/02/2019 01:45:23 *** evaluating ***
06/02/2019 01:45:24 step: 260, epoch: 259, acc: 55.55555555555556, f1: 28.738468001844296, r: 0.33981159052618787
06/02/2019 01:45:24 *** epoch: 261 ***
06/02/2019 01:45:24 *** training ***
06/02/2019 01:45:25 step: 8585, epoch: 260, batch: 4, loss: 0.0005012378096580505, acc: 100.0, f1: 100.0, r: 0.7470108048785696
06/02/2019 01:45:25 step: 8590, epoch: 260, batch: 9, loss: 0.002109736204147339, acc: 100.0, f1: 100.0, r: 0.6728798841863504
06/02/2019 01:45:26 step: 8595, epoch: 260, batch: 14, loss: 0.0018605515360832214, acc: 100.0, f1: 100.0, r: 0.7130664744744972
06/02/2019 01:45:27 step: 8600, epoch: 260, batch: 19, loss: 0.0015104413032531738, acc: 100.0, f1: 100.0, r: 0.8124170911004095
06/02/2019 01:45:28 step: 8605, epoch: 260, batch: 24, loss: 0.00186929851770401, acc: 100.0, f1: 100.0, r: 0.8149963856461487
06/02/2019 01:45:29 step: 8610, epoch: 260, batch: 29, loss: 0.003974162042140961, acc: 100.0, f1: 100.0, r: 0.7422572642615044
06/02/2019 01:45:29 *** evaluating ***
06/02/2019 01:45:29 step: 261, epoch: 260, acc: 57.692307692307686, f1: 29.859558368299417, r: 0.34909014031128205
06/02/2019 01:45:29 *** epoch: 262 ***
06/02/2019 01:45:29 *** training ***
06/02/2019 01:45:30 step: 8618, epoch: 261, batch: 4, loss: 0.0009978115558624268, acc: 100.0, f1: 100.0, r: 0.8102714416470287
06/02/2019 01:45:31 step: 8623, epoch: 261, batch: 9, loss: 0.0018743649125099182, acc: 100.0, f1: 100.0, r: 0.7007600702691066
06/02/2019 01:45:32 step: 8628, epoch: 261, batch: 14, loss: 0.0009425431489944458, acc: 100.0, f1: 100.0, r: 0.737342395227718
06/02/2019 01:45:33 step: 8633, epoch: 261, batch: 19, loss: 0.003205806016921997, acc: 100.0, f1: 100.0, r: 0.8316342778190091
06/02/2019 01:45:34 step: 8638, epoch: 261, batch: 24, loss: 0.0025809183716773987, acc: 100.0, f1: 100.0, r: 0.7650243000586142
06/02/2019 01:45:34 step: 8643, epoch: 261, batch: 29, loss: 0.0073544979095458984, acc: 100.0, f1: 100.0, r: 0.8021228226831149
06/02/2019 01:45:35 *** evaluating ***
06/02/2019 01:45:35 step: 262, epoch: 261, acc: 58.54700854700855, f1: 28.886154030422805, r: 0.3495457677724581
06/02/2019 01:45:35 *** epoch: 263 ***
06/02/2019 01:45:35 *** training ***
06/02/2019 01:45:36 step: 8651, epoch: 262, batch: 4, loss: 0.003738619387149811, acc: 100.0, f1: 100.0, r: 0.640878123429345
06/02/2019 01:45:37 step: 8656, epoch: 262, batch: 9, loss: 0.0015299171209335327, acc: 100.0, f1: 100.0, r: 0.8064845313826042
06/02/2019 01:45:38 step: 8661, epoch: 262, batch: 14, loss: 0.002098873257637024, acc: 100.0, f1: 100.0, r: 0.7199455671243813
06/02/2019 01:45:38 step: 8666, epoch: 262, batch: 19, loss: 0.0008735433220863342, acc: 100.0, f1: 100.0, r: 0.7571979045969252
06/02/2019 01:45:39 step: 8671, epoch: 262, batch: 24, loss: 0.0031426846981048584, acc: 100.0, f1: 100.0, r: 0.6283266809537731
06/02/2019 01:45:40 step: 8676, epoch: 262, batch: 29, loss: 0.0035713911056518555, acc: 100.0, f1: 100.0, r: 0.819349321126822
06/02/2019 01:45:40 *** evaluating ***
06/02/2019 01:45:41 step: 263, epoch: 262, acc: 59.401709401709404, f1: 31.402888878243772, r: 0.34703552868918375
06/02/2019 01:45:41 *** epoch: 264 ***
06/02/2019 01:45:41 *** training ***
06/02/2019 01:45:42 step: 8684, epoch: 263, batch: 4, loss: 0.002628833055496216, acc: 100.0, f1: 100.0, r: 0.7362298101159155
06/02/2019 01:45:42 step: 8689, epoch: 263, batch: 9, loss: 0.002046734094619751, acc: 100.0, f1: 100.0, r: 0.7875595705263437
06/02/2019 01:45:43 step: 8694, epoch: 263, batch: 14, loss: 0.002951841801404953, acc: 100.0, f1: 100.0, r: 0.7706525886321252
06/02/2019 01:45:44 step: 8699, epoch: 263, batch: 19, loss: 0.0009052902460098267, acc: 100.0, f1: 100.0, r: 0.569849603237879
06/02/2019 01:45:45 step: 8704, epoch: 263, batch: 24, loss: 0.009945832192897797, acc: 100.0, f1: 100.0, r: 0.8275791760047966
06/02/2019 01:45:46 step: 8709, epoch: 263, batch: 29, loss: 0.0016184002161026, acc: 100.0, f1: 100.0, r: 0.8097670144152006
06/02/2019 01:45:46 *** evaluating ***
06/02/2019 01:45:46 step: 264, epoch: 263, acc: 56.41025641025641, f1: 29.464042337887552, r: 0.3435308489637746
06/02/2019 01:45:46 *** epoch: 265 ***
06/02/2019 01:45:46 *** training ***
06/02/2019 01:45:47 step: 8717, epoch: 264, batch: 4, loss: 0.0005128905177116394, acc: 100.0, f1: 100.0, r: 0.8397102761692266
06/02/2019 01:45:48 step: 8722, epoch: 264, batch: 9, loss: 0.0037663057446479797, acc: 100.0, f1: 100.0, r: 0.7750814314183174
06/02/2019 01:45:49 step: 8727, epoch: 264, batch: 14, loss: 0.02196895331144333, acc: 98.4375, f1: 99.1484593837535, r: 0.6654842103832163
06/02/2019 01:45:50 step: 8732, epoch: 264, batch: 19, loss: 0.0066994354128837585, acc: 100.0, f1: 100.0, r: 0.6787139089027726
06/02/2019 01:45:51 step: 8737, epoch: 264, batch: 24, loss: 0.0034690573811531067, acc: 100.0, f1: 100.0, r: 0.7284736233697857
06/02/2019 01:45:51 step: 8742, epoch: 264, batch: 29, loss: 0.0012353509664535522, acc: 100.0, f1: 100.0, r: 0.8552131856208857
06/02/2019 01:45:52 *** evaluating ***
06/02/2019 01:45:52 step: 265, epoch: 264, acc: 57.26495726495726, f1: 28.887976674707843, r: 0.352227247129237
06/02/2019 01:45:52 *** epoch: 266 ***
06/02/2019 01:45:52 *** training ***
06/02/2019 01:45:53 step: 8750, epoch: 265, batch: 4, loss: 0.0025446340441703796, acc: 100.0, f1: 100.0, r: 0.6813467665078436
06/02/2019 01:45:54 step: 8755, epoch: 265, batch: 9, loss: 0.0005374699831008911, acc: 100.0, f1: 100.0, r: 0.7571576629634473
06/02/2019 01:45:55 step: 8760, epoch: 265, batch: 14, loss: 0.0024974867701530457, acc: 100.0, f1: 100.0, r: 0.6836420590326234
06/02/2019 01:45:55 step: 8765, epoch: 265, batch: 19, loss: 0.0016338899731636047, acc: 100.0, f1: 100.0, r: 0.7835993820480482
06/02/2019 01:45:56 step: 8770, epoch: 265, batch: 24, loss: 0.002523764967918396, acc: 100.0, f1: 100.0, r: 0.6536978211137615
06/02/2019 01:45:57 step: 8775, epoch: 265, batch: 29, loss: 0.00496065616607666, acc: 100.0, f1: 100.0, r: 0.726658217490937
06/02/2019 01:45:57 *** evaluating ***
06/02/2019 01:45:58 step: 266, epoch: 265, acc: 56.41025641025641, f1: 28.470371263743992, r: 0.33992802968605973
06/02/2019 01:45:58 *** epoch: 267 ***
06/02/2019 01:45:58 *** training ***
06/02/2019 01:45:59 step: 8783, epoch: 266, batch: 4, loss: 0.0012051761150360107, acc: 100.0, f1: 100.0, r: 0.7925294132908935
06/02/2019 01:45:59 step: 8788, epoch: 266, batch: 9, loss: 0.000824563205242157, acc: 100.0, f1: 100.0, r: 0.8086927562014369
06/02/2019 01:46:00 step: 8793, epoch: 266, batch: 14, loss: 0.0021831244230270386, acc: 100.0, f1: 100.0, r: 0.7436052806689709
06/02/2019 01:46:01 step: 8798, epoch: 266, batch: 19, loss: 0.0018260478973388672, acc: 100.0, f1: 100.0, r: 0.7029548857761146
06/02/2019 01:46:02 step: 8803, epoch: 266, batch: 24, loss: 0.0019033029675483704, acc: 100.0, f1: 100.0, r: 0.5515904563855828
06/02/2019 01:46:03 step: 8808, epoch: 266, batch: 29, loss: 0.0008452236652374268, acc: 100.0, f1: 100.0, r: 0.7565542036825524
06/02/2019 01:46:03 *** evaluating ***
06/02/2019 01:46:03 step: 267, epoch: 266, acc: 54.700854700854705, f1: 29.11353652321952, r: 0.3368003350964022
06/02/2019 01:46:03 *** epoch: 268 ***
06/02/2019 01:46:03 *** training ***
06/02/2019 01:46:04 step: 8816, epoch: 267, batch: 4, loss: 0.0020604208111763, acc: 100.0, f1: 100.0, r: 0.7915851452052273
06/02/2019 01:46:05 step: 8821, epoch: 267, batch: 9, loss: 0.0012203678488731384, acc: 100.0, f1: 100.0, r: 0.6144289781007526
06/02/2019 01:46:06 step: 8826, epoch: 267, batch: 14, loss: 0.0013043954968452454, acc: 100.0, f1: 100.0, r: 0.7235015457389086
06/02/2019 01:46:07 step: 8831, epoch: 267, batch: 19, loss: 0.0006733015179634094, acc: 100.0, f1: 100.0, r: 0.7381154123047018
06/02/2019 01:46:08 step: 8836, epoch: 267, batch: 24, loss: 0.006160996854305267, acc: 100.0, f1: 100.0, r: 0.6736018743372818
06/02/2019 01:46:08 step: 8841, epoch: 267, batch: 29, loss: 0.002232559025287628, acc: 100.0, f1: 100.0, r: 0.7659790340648563
06/02/2019 01:46:09 *** evaluating ***
06/02/2019 01:46:09 step: 268, epoch: 267, acc: 57.26495726495726, f1: 30.673266078787698, r: 0.3333163321211179
06/02/2019 01:46:09 *** epoch: 269 ***
06/02/2019 01:46:09 *** training ***
06/02/2019 01:46:10 step: 8849, epoch: 268, batch: 4, loss: 0.0010446235537528992, acc: 100.0, f1: 100.0, r: 0.7764748650867573
06/02/2019 01:46:11 step: 8854, epoch: 268, batch: 9, loss: 0.0006465539336204529, acc: 100.0, f1: 100.0, r: 0.7476906404783821
06/02/2019 01:46:12 step: 8859, epoch: 268, batch: 14, loss: 0.0011766552925109863, acc: 100.0, f1: 100.0, r: 0.7028833968572161
06/02/2019 01:46:13 step: 8864, epoch: 268, batch: 19, loss: 0.0016843602061271667, acc: 100.0, f1: 100.0, r: 0.798409829481442
06/02/2019 01:46:13 step: 8869, epoch: 268, batch: 24, loss: 0.002466239035129547, acc: 100.0, f1: 100.0, r: 0.7393018985373935
06/02/2019 01:46:14 step: 8874, epoch: 268, batch: 29, loss: 0.004077829420566559, acc: 100.0, f1: 100.0, r: 0.6554169548809313
06/02/2019 01:46:15 *** evaluating ***
06/02/2019 01:46:15 step: 269, epoch: 268, acc: 58.119658119658126, f1: 32.886001820821335, r: 0.3332616415250939
06/02/2019 01:46:15 *** epoch: 270 ***
06/02/2019 01:46:15 *** training ***
06/02/2019 01:46:16 step: 8882, epoch: 269, batch: 4, loss: 0.0041940100491046906, acc: 100.0, f1: 100.0, r: 0.6930296190332061
06/02/2019 01:46:17 step: 8887, epoch: 269, batch: 9, loss: 0.003295212984085083, acc: 100.0, f1: 100.0, r: 0.7011868704446078
06/02/2019 01:46:17 step: 8892, epoch: 269, batch: 14, loss: 0.003620050847530365, acc: 100.0, f1: 100.0, r: 0.7445026354464792
06/02/2019 01:46:18 step: 8897, epoch: 269, batch: 19, loss: 0.000675104558467865, acc: 100.0, f1: 100.0, r: 0.7145632067537421
06/02/2019 01:46:19 step: 8902, epoch: 269, batch: 24, loss: 0.0028257593512535095, acc: 100.0, f1: 100.0, r: 0.7936572220892242
06/02/2019 01:46:20 step: 8907, epoch: 269, batch: 29, loss: 0.0007682442665100098, acc: 100.0, f1: 100.0, r: 0.8271117006558649
06/02/2019 01:46:20 *** evaluating ***
06/02/2019 01:46:21 step: 270, epoch: 269, acc: 55.98290598290598, f1: 29.122857372742995, r: 0.33404088278338395
06/02/2019 01:46:21 *** epoch: 271 ***
06/02/2019 01:46:21 *** training ***
06/02/2019 01:46:21 step: 8915, epoch: 270, batch: 4, loss: 0.0011830031871795654, acc: 100.0, f1: 100.0, r: 0.8469996746688596
06/02/2019 01:46:22 step: 8920, epoch: 270, batch: 9, loss: 0.0006584003567695618, acc: 100.0, f1: 100.0, r: 0.6946694746424998
06/02/2019 01:46:23 step: 8925, epoch: 270, batch: 14, loss: 0.0020186901092529297, acc: 100.0, f1: 100.0, r: 0.7727386692013463
06/02/2019 01:46:24 step: 8930, epoch: 270, batch: 19, loss: 0.0015161335468292236, acc: 100.0, f1: 100.0, r: 0.8164173940804332
06/02/2019 01:46:25 step: 8935, epoch: 270, batch: 24, loss: 0.003409057855606079, acc: 100.0, f1: 100.0, r: 0.7448915844756222
06/02/2019 01:46:26 step: 8940, epoch: 270, batch: 29, loss: 0.0045148395001888275, acc: 100.0, f1: 100.0, r: 0.7478487174560509
06/02/2019 01:46:26 *** evaluating ***
06/02/2019 01:46:26 step: 271, epoch: 270, acc: 58.119658119658126, f1: 29.858476776800323, r: 0.33604658184612707
06/02/2019 01:46:26 *** epoch: 272 ***
06/02/2019 01:46:26 *** training ***
06/02/2019 01:46:27 step: 8948, epoch: 271, batch: 4, loss: 0.007443562150001526, acc: 100.0, f1: 100.0, r: 0.695173793501337
06/02/2019 01:46:28 step: 8953, epoch: 271, batch: 9, loss: 0.0018770694732666016, acc: 100.0, f1: 100.0, r: 0.7745570096682453
06/02/2019 01:46:29 step: 8958, epoch: 271, batch: 14, loss: 0.0032147765159606934, acc: 100.0, f1: 100.0, r: 0.723190598327502
06/02/2019 01:46:29 step: 8963, epoch: 271, batch: 19, loss: 0.0013690441846847534, acc: 100.0, f1: 100.0, r: 0.7108008020967338
06/02/2019 01:46:30 step: 8968, epoch: 271, batch: 24, loss: 0.0038692355155944824, acc: 100.0, f1: 100.0, r: 0.8235032631298026
06/02/2019 01:46:31 step: 8973, epoch: 271, batch: 29, loss: 0.0018691793084144592, acc: 100.0, f1: 100.0, r: 0.771000320063767
06/02/2019 01:46:31 *** evaluating ***
06/02/2019 01:46:32 step: 272, epoch: 271, acc: 57.692307692307686, f1: 29.08943621892312, r: 0.34187154228812294
06/02/2019 01:46:32 *** epoch: 273 ***
06/02/2019 01:46:32 *** training ***
06/02/2019 01:46:33 step: 8981, epoch: 272, batch: 4, loss: 0.0009445920586585999, acc: 100.0, f1: 100.0, r: 0.6665129246579936
06/02/2019 01:46:33 step: 8986, epoch: 272, batch: 9, loss: 0.0018416345119476318, acc: 100.0, f1: 100.0, r: 0.7637450602741511
06/02/2019 01:46:34 step: 8991, epoch: 272, batch: 14, loss: 0.0019685178995132446, acc: 100.0, f1: 100.0, r: 0.6643588560085364
06/02/2019 01:46:35 step: 8996, epoch: 272, batch: 19, loss: 0.00339314341545105, acc: 100.0, f1: 100.0, r: 0.6891768653182696
06/02/2019 01:46:36 step: 9001, epoch: 272, batch: 24, loss: 0.0008003562688827515, acc: 100.0, f1: 100.0, r: 0.740956830279665
06/02/2019 01:46:37 step: 9006, epoch: 272, batch: 29, loss: 0.0017343983054161072, acc: 100.0, f1: 100.0, r: 0.6589367495609371
06/02/2019 01:46:37 *** evaluating ***
06/02/2019 01:46:38 step: 273, epoch: 272, acc: 57.692307692307686, f1: 29.913028609758996, r: 0.3406049209523728
06/02/2019 01:46:38 *** epoch: 274 ***
06/02/2019 01:46:38 *** training ***
06/02/2019 01:46:38 step: 9014, epoch: 273, batch: 4, loss: 0.0022697672247886658, acc: 100.0, f1: 100.0, r: 0.680326490605667
06/02/2019 01:46:39 step: 9019, epoch: 273, batch: 9, loss: 0.0012349188327789307, acc: 100.0, f1: 100.0, r: 0.7821769473196408
06/02/2019 01:46:40 step: 9024, epoch: 273, batch: 14, loss: 0.0010299012064933777, acc: 100.0, f1: 100.0, r: 0.7761603718541252
06/02/2019 01:46:41 step: 9029, epoch: 273, batch: 19, loss: 0.004900023341178894, acc: 100.0, f1: 100.0, r: 0.6836378307230894
06/02/2019 01:46:42 step: 9034, epoch: 273, batch: 24, loss: 0.00135108083486557, acc: 100.0, f1: 100.0, r: 0.7202153616936378
06/02/2019 01:46:43 step: 9039, epoch: 273, batch: 29, loss: 0.011664263904094696, acc: 100.0, f1: 100.0, r: 0.672726050093252
06/02/2019 01:46:43 *** evaluating ***
06/02/2019 01:46:43 step: 274, epoch: 273, acc: 57.692307692307686, f1: 28.36455474904842, r: 0.34511750413831227
06/02/2019 01:46:43 *** epoch: 275 ***
06/02/2019 01:46:43 *** training ***
06/02/2019 01:46:44 step: 9047, epoch: 274, batch: 4, loss: 0.004927739500999451, acc: 100.0, f1: 100.0, r: 0.6104705962816599
06/02/2019 01:46:45 step: 9052, epoch: 274, batch: 9, loss: 0.0031451955437660217, acc: 100.0, f1: 100.0, r: 0.7351395319474955
06/02/2019 01:46:46 step: 9057, epoch: 274, batch: 14, loss: 0.0007338151335716248, acc: 100.0, f1: 100.0, r: 0.6891001223409791
06/02/2019 01:46:46 step: 9062, epoch: 274, batch: 19, loss: 0.00259225070476532, acc: 100.0, f1: 100.0, r: 0.700141684717876
06/02/2019 01:46:47 step: 9067, epoch: 274, batch: 24, loss: 0.0011831298470497131, acc: 100.0, f1: 100.0, r: 0.6381461660764294
06/02/2019 01:46:48 step: 9072, epoch: 274, batch: 29, loss: 0.002324715256690979, acc: 100.0, f1: 100.0, r: 0.6831258931739153
06/02/2019 01:46:48 *** evaluating ***
06/02/2019 01:46:49 step: 275, epoch: 274, acc: 57.26495726495726, f1: 28.86461683894118, r: 0.33572934497660506
06/02/2019 01:46:49 *** epoch: 276 ***
06/02/2019 01:46:49 *** training ***
06/02/2019 01:46:50 step: 9080, epoch: 275, batch: 4, loss: 0.008996833115816116, acc: 100.0, f1: 100.0, r: 0.8541186150605277
06/02/2019 01:46:50 step: 9085, epoch: 275, batch: 9, loss: 0.008353959769010544, acc: 100.0, f1: 100.0, r: 0.8277521314048333
06/02/2019 01:46:51 step: 9090, epoch: 275, batch: 14, loss: 0.0022763311862945557, acc: 100.0, f1: 100.0, r: 0.8223075226535113
06/02/2019 01:46:52 step: 9095, epoch: 275, batch: 19, loss: 0.0009031742811203003, acc: 100.0, f1: 100.0, r: 0.7726713964850229
06/02/2019 01:46:53 step: 9100, epoch: 275, batch: 24, loss: 0.0008791610598564148, acc: 100.0, f1: 100.0, r: 0.8587414104495084
06/02/2019 01:46:54 step: 9105, epoch: 275, batch: 29, loss: 0.007983192801475525, acc: 100.0, f1: 100.0, r: 0.6197128745835919
06/02/2019 01:46:54 *** evaluating ***
06/02/2019 01:46:54 step: 276, epoch: 275, acc: 57.692307692307686, f1: 29.102791129668894, r: 0.34076959212185887
06/02/2019 01:46:54 *** epoch: 277 ***
06/02/2019 01:46:54 *** training ***
06/02/2019 01:46:55 step: 9113, epoch: 276, batch: 4, loss: 0.002863749861717224, acc: 100.0, f1: 100.0, r: 0.8134482692777778
06/02/2019 01:46:56 step: 9118, epoch: 276, batch: 9, loss: 0.001955263316631317, acc: 100.0, f1: 100.0, r: 0.7939251614860453
06/02/2019 01:46:57 step: 9123, epoch: 276, batch: 14, loss: 0.0018458403646945953, acc: 100.0, f1: 100.0, r: 0.8505995313503665
06/02/2019 01:46:58 step: 9128, epoch: 276, batch: 19, loss: 0.0017690584063529968, acc: 100.0, f1: 100.0, r: 0.7889574799887414
06/02/2019 01:46:58 step: 9133, epoch: 276, batch: 24, loss: 0.0027089864015579224, acc: 100.0, f1: 100.0, r: 0.7501385842443993
06/02/2019 01:46:59 step: 9138, epoch: 276, batch: 29, loss: 0.0022680163383483887, acc: 100.0, f1: 100.0, r: 0.6535333139358607
06/02/2019 01:47:00 *** evaluating ***
06/02/2019 01:47:00 step: 277, epoch: 276, acc: 55.55555555555556, f1: 29.0588028704727, r: 0.32827367007152175
06/02/2019 01:47:00 *** epoch: 278 ***
06/02/2019 01:47:00 *** training ***
06/02/2019 01:47:01 step: 9146, epoch: 277, batch: 4, loss: 0.0023977458477020264, acc: 100.0, f1: 100.0, r: 0.7016668162374483
06/02/2019 01:47:02 step: 9151, epoch: 277, batch: 9, loss: 0.002539992332458496, acc: 100.0, f1: 100.0, r: 0.737664227395247
06/02/2019 01:47:02 step: 9156, epoch: 277, batch: 14, loss: 0.0009846985340118408, acc: 100.0, f1: 100.0, r: 0.7057489333415973
06/02/2019 01:47:03 step: 9161, epoch: 277, batch: 19, loss: 0.001374267041683197, acc: 100.0, f1: 100.0, r: 0.8162862141214241
06/02/2019 01:47:04 step: 9166, epoch: 277, batch: 24, loss: 0.0020519718527793884, acc: 100.0, f1: 100.0, r: 0.7221365467370029
06/02/2019 01:47:05 step: 9171, epoch: 277, batch: 29, loss: 0.003202870488166809, acc: 100.0, f1: 100.0, r: 0.7277510538398165
06/02/2019 01:47:05 *** evaluating ***
06/02/2019 01:47:06 step: 278, epoch: 277, acc: 57.692307692307686, f1: 29.09903800878221, r: 0.3402663333809295
06/02/2019 01:47:06 *** epoch: 279 ***
06/02/2019 01:47:06 *** training ***
06/02/2019 01:47:07 step: 9179, epoch: 278, batch: 4, loss: 0.010981522500514984, acc: 100.0, f1: 100.0, r: 0.8053992149024071
06/02/2019 01:47:07 step: 9184, epoch: 278, batch: 9, loss: 0.00141085684299469, acc: 100.0, f1: 100.0, r: 0.7557933255004303
06/02/2019 01:47:08 step: 9189, epoch: 278, batch: 14, loss: 0.0017983689904212952, acc: 100.0, f1: 100.0, r: 0.7563669843025882
06/02/2019 01:47:09 step: 9194, epoch: 278, batch: 19, loss: 0.0017713308334350586, acc: 100.0, f1: 100.0, r: 0.7934004525491958
06/02/2019 01:47:10 step: 9199, epoch: 278, batch: 24, loss: 0.003481261432170868, acc: 100.0, f1: 100.0, r: 0.8070256766936263
06/02/2019 01:47:11 step: 9204, epoch: 278, batch: 29, loss: 0.012861303985118866, acc: 100.0, f1: 100.0, r: 0.7820469163314074
06/02/2019 01:47:11 *** evaluating ***
06/02/2019 01:47:12 step: 279, epoch: 278, acc: 55.98290598290598, f1: 28.372698226417175, r: 0.3413376813402141
06/02/2019 01:47:12 *** epoch: 280 ***
06/02/2019 01:47:12 *** training ***
06/02/2019 01:47:12 step: 9212, epoch: 279, batch: 4, loss: 0.0013122335076332092, acc: 100.0, f1: 100.0, r: 0.697518256179317
06/02/2019 01:47:13 step: 9217, epoch: 279, batch: 9, loss: 0.0013717412948608398, acc: 100.0, f1: 100.0, r: 0.7619613978860236
06/02/2019 01:47:14 step: 9222, epoch: 279, batch: 14, loss: 0.0006517097353935242, acc: 100.0, f1: 100.0, r: 0.6864456459601807
06/02/2019 01:47:15 step: 9227, epoch: 279, batch: 19, loss: 0.0013567134737968445, acc: 100.0, f1: 100.0, r: 0.7058211628941893
06/02/2019 01:47:16 step: 9232, epoch: 279, batch: 24, loss: 0.002401098608970642, acc: 100.0, f1: 100.0, r: 0.8083622761476907
06/02/2019 01:47:17 step: 9237, epoch: 279, batch: 29, loss: 0.00080079585313797, acc: 100.0, f1: 100.0, r: 0.8168485489314783
06/02/2019 01:47:17 *** evaluating ***
06/02/2019 01:47:17 step: 280, epoch: 279, acc: 57.692307692307686, f1: 29.795751633986928, r: 0.34156959527166325
06/02/2019 01:47:17 *** epoch: 281 ***
06/02/2019 01:47:17 *** training ***
06/02/2019 01:47:18 step: 9245, epoch: 280, batch: 4, loss: 0.0008693039417266846, acc: 100.0, f1: 100.0, r: 0.6488966975373599
06/02/2019 01:47:19 step: 9250, epoch: 280, batch: 9, loss: 0.0007289573550224304, acc: 100.0, f1: 100.0, r: 0.6354691534260561
06/02/2019 01:47:20 step: 9255, epoch: 280, batch: 14, loss: 0.00345022976398468, acc: 100.0, f1: 100.0, r: 0.8234812197076572
06/02/2019 01:47:20 step: 9260, epoch: 280, batch: 19, loss: 0.0008037388324737549, acc: 100.0, f1: 100.0, r: 0.7944613547227406
06/02/2019 01:47:21 step: 9265, epoch: 280, batch: 24, loss: 0.001500450074672699, acc: 100.0, f1: 100.0, r: 0.7038777785877073
06/02/2019 01:47:22 step: 9270, epoch: 280, batch: 29, loss: 0.001379840075969696, acc: 100.0, f1: 100.0, r: 0.714143779699916
06/02/2019 01:47:23 *** evaluating ***
06/02/2019 01:47:23 step: 281, epoch: 280, acc: 56.837606837606835, f1: 28.394820842512253, r: 0.3410914661333084
06/02/2019 01:47:23 *** epoch: 282 ***
06/02/2019 01:47:23 *** training ***
06/02/2019 01:47:24 step: 9278, epoch: 281, batch: 4, loss: 0.0018713027238845825, acc: 100.0, f1: 100.0, r: 0.7619352904526682
06/02/2019 01:47:24 step: 9283, epoch: 281, batch: 9, loss: 0.0028092488646507263, acc: 100.0, f1: 100.0, r: 0.6952168439388372
06/02/2019 01:47:25 step: 9288, epoch: 281, batch: 14, loss: 0.0021401047706604004, acc: 100.0, f1: 100.0, r: 0.6862262503903391
06/02/2019 01:47:26 step: 9293, epoch: 281, batch: 19, loss: 0.001275390386581421, acc: 100.0, f1: 100.0, r: 0.6602225531333237
06/02/2019 01:47:27 step: 9298, epoch: 281, batch: 24, loss: 0.0013051703572273254, acc: 100.0, f1: 100.0, r: 0.7774083100763637
06/02/2019 01:47:28 step: 9303, epoch: 281, batch: 29, loss: 0.009123515337705612, acc: 100.0, f1: 100.0, r: 0.6326135881531727
06/02/2019 01:47:28 *** evaluating ***
06/02/2019 01:47:29 step: 282, epoch: 281, acc: 56.41025641025641, f1: 28.840130690490962, r: 0.3324743733645961
06/02/2019 01:47:29 *** epoch: 283 ***
06/02/2019 01:47:29 *** training ***
06/02/2019 01:47:29 step: 9311, epoch: 282, batch: 4, loss: 0.0009624436497688293, acc: 100.0, f1: 100.0, r: 0.7226496810780788
06/02/2019 01:47:30 step: 9316, epoch: 282, batch: 9, loss: 0.006627388298511505, acc: 100.0, f1: 100.0, r: 0.6383086383144831
06/02/2019 01:47:31 step: 9321, epoch: 282, batch: 14, loss: 0.0018484517931938171, acc: 100.0, f1: 100.0, r: 0.7430142853320858
06/02/2019 01:47:32 step: 9326, epoch: 282, batch: 19, loss: 0.0018939971923828125, acc: 100.0, f1: 100.0, r: 0.7255873246593213
06/02/2019 01:47:33 step: 9331, epoch: 282, batch: 24, loss: 0.0006641969084739685, acc: 100.0, f1: 100.0, r: 0.8023953900960318
06/02/2019 01:47:33 step: 9336, epoch: 282, batch: 29, loss: 0.005665488541126251, acc: 100.0, f1: 100.0, r: 0.7678312053377718
06/02/2019 01:47:34 *** evaluating ***
06/02/2019 01:47:34 step: 283, epoch: 282, acc: 57.26495726495726, f1: 29.44541772635949, r: 0.32772219640507344
06/02/2019 01:47:34 *** epoch: 284 ***
06/02/2019 01:47:34 *** training ***
06/02/2019 01:47:35 step: 9344, epoch: 283, batch: 4, loss: 0.0032839328050613403, acc: 100.0, f1: 100.0, r: 0.7874484170640309
06/02/2019 01:47:36 step: 9349, epoch: 283, batch: 9, loss: 0.0013442263007164001, acc: 100.0, f1: 100.0, r: 0.5974030075416197
06/02/2019 01:47:37 step: 9354, epoch: 283, batch: 14, loss: 0.005310222506523132, acc: 100.0, f1: 100.0, r: 0.8321638731819458
06/02/2019 01:47:37 step: 9359, epoch: 283, batch: 19, loss: 0.003840707242488861, acc: 100.0, f1: 100.0, r: 0.8255257666045882
06/02/2019 01:47:38 step: 9364, epoch: 283, batch: 24, loss: 0.002052515745162964, acc: 100.0, f1: 100.0, r: 0.6261096161232622
06/02/2019 01:47:39 step: 9369, epoch: 283, batch: 29, loss: 0.0024285465478897095, acc: 100.0, f1: 100.0, r: 0.6883342324557903
06/02/2019 01:47:39 *** evaluating ***
06/02/2019 01:47:40 step: 284, epoch: 283, acc: 57.692307692307686, f1: 28.9435977021452, r: 0.33102278197677854
06/02/2019 01:47:40 *** epoch: 285 ***
06/02/2019 01:47:40 *** training ***
06/02/2019 01:47:41 step: 9377, epoch: 284, batch: 4, loss: 0.0024828314781188965, acc: 100.0, f1: 100.0, r: 0.786877880055442
06/02/2019 01:47:41 step: 9382, epoch: 284, batch: 9, loss: 0.00338069349527359, acc: 100.0, f1: 100.0, r: 0.791388604274653
06/02/2019 01:47:42 step: 9387, epoch: 284, batch: 14, loss: 0.005187615752220154, acc: 100.0, f1: 100.0, r: 0.7270935628783528
06/02/2019 01:47:43 step: 9392, epoch: 284, batch: 19, loss: 0.004969503730535507, acc: 100.0, f1: 100.0, r: 0.7293956104834535
06/02/2019 01:47:44 step: 9397, epoch: 284, batch: 24, loss: 0.0023045018315315247, acc: 100.0, f1: 100.0, r: 0.7846139780937775
06/02/2019 01:47:45 step: 9402, epoch: 284, batch: 29, loss: 0.0036694444715976715, acc: 100.0, f1: 100.0, r: 0.6896682320808285
06/02/2019 01:47:45 *** evaluating ***
06/02/2019 01:47:45 step: 285, epoch: 284, acc: 58.97435897435898, f1: 30.157107373718357, r: 0.3353694617008954
06/02/2019 01:47:45 *** epoch: 286 ***
06/02/2019 01:47:45 *** training ***
06/02/2019 01:47:46 step: 9410, epoch: 285, batch: 4, loss: 0.0017147138714790344, acc: 100.0, f1: 100.0, r: 0.7846584046855961
06/02/2019 01:47:47 step: 9415, epoch: 285, batch: 9, loss: 0.0011174529790878296, acc: 100.0, f1: 100.0, r: 0.6730432898511204
06/02/2019 01:47:48 step: 9420, epoch: 285, batch: 14, loss: 0.001914389431476593, acc: 100.0, f1: 100.0, r: 0.813427808974783
06/02/2019 01:47:49 step: 9425, epoch: 285, batch: 19, loss: 0.0022334232926368713, acc: 100.0, f1: 100.0, r: 0.7456540134806007
06/02/2019 01:47:49 step: 9430, epoch: 285, batch: 24, loss: 0.0007707849144935608, acc: 100.0, f1: 100.0, r: 0.6821555009988737
06/02/2019 01:47:50 step: 9435, epoch: 285, batch: 29, loss: 0.0021745115518569946, acc: 100.0, f1: 100.0, r: 0.7911006559149073
06/02/2019 01:47:51 *** evaluating ***
06/02/2019 01:47:51 step: 286, epoch: 285, acc: 58.97435897435898, f1: 30.01678860013103, r: 0.3434770568495429
06/02/2019 01:47:51 *** epoch: 287 ***
06/02/2019 01:47:51 *** training ***
06/02/2019 01:47:52 step: 9443, epoch: 286, batch: 4, loss: 0.014403745532035828, acc: 100.0, f1: 100.0, r: 0.7854182130724843
06/02/2019 01:47:53 step: 9448, epoch: 286, batch: 9, loss: 0.0015504658222198486, acc: 100.0, f1: 100.0, r: 0.7438521959091557
06/02/2019 01:47:53 step: 9453, epoch: 286, batch: 14, loss: 0.0006926655769348145, acc: 100.0, f1: 100.0, r: 0.8039744614575317
06/02/2019 01:47:54 step: 9458, epoch: 286, batch: 19, loss: 0.003127560019493103, acc: 100.0, f1: 100.0, r: 0.7385615094673703
06/02/2019 01:47:55 step: 9463, epoch: 286, batch: 24, loss: 0.0012251734733581543, acc: 100.0, f1: 100.0, r: 0.7964755209538932
06/02/2019 01:47:56 step: 9468, epoch: 286, batch: 29, loss: 0.0022929832339286804, acc: 100.0, f1: 100.0, r: 0.8063493789299699
06/02/2019 01:47:56 *** evaluating ***
06/02/2019 01:47:57 step: 287, epoch: 286, acc: 58.97435897435898, f1: 30.55766422230458, r: 0.33824794787200296
06/02/2019 01:47:57 *** epoch: 288 ***
06/02/2019 01:47:57 *** training ***
06/02/2019 01:47:57 step: 9476, epoch: 287, batch: 4, loss: 0.0006339997053146362, acc: 100.0, f1: 100.0, r: 0.7783604126195078
06/02/2019 01:47:58 step: 9481, epoch: 287, batch: 9, loss: 0.0034337788820266724, acc: 100.0, f1: 100.0, r: 0.677166454057768
06/02/2019 01:47:59 step: 9486, epoch: 287, batch: 14, loss: 0.0007362291216850281, acc: 100.0, f1: 100.0, r: 0.8313292648938385
06/02/2019 01:48:00 step: 9491, epoch: 287, batch: 19, loss: 0.00046688318252563477, acc: 100.0, f1: 100.0, r: 0.720805733802523
06/02/2019 01:48:01 step: 9496, epoch: 287, batch: 24, loss: 0.0023377686738967896, acc: 100.0, f1: 100.0, r: 0.7510919407192687
06/02/2019 01:48:01 step: 9501, epoch: 287, batch: 29, loss: 0.0009048357605934143, acc: 100.0, f1: 100.0, r: 0.6443484205147365
06/02/2019 01:48:02 *** evaluating ***
06/02/2019 01:48:02 step: 288, epoch: 287, acc: 58.97435897435898, f1: 29.906612521020197, r: 0.3385585038023272
06/02/2019 01:48:02 *** epoch: 289 ***
06/02/2019 01:48:02 *** training ***
06/02/2019 01:48:03 step: 9509, epoch: 288, batch: 4, loss: 0.0032717660069465637, acc: 100.0, f1: 100.0, r: 0.8421113265921414
06/02/2019 01:48:04 step: 9514, epoch: 288, batch: 9, loss: 0.0017002522945404053, acc: 100.0, f1: 100.0, r: 0.785440998409813
06/02/2019 01:48:05 step: 9519, epoch: 288, batch: 14, loss: 0.004265978932380676, acc: 100.0, f1: 100.0, r: 0.7102386258387108
06/02/2019 01:48:05 step: 9524, epoch: 288, batch: 19, loss: 0.0012758001685142517, acc: 100.0, f1: 100.0, r: 0.7863618332235893
06/02/2019 01:48:06 step: 9529, epoch: 288, batch: 24, loss: 0.0061960369348526, acc: 100.0, f1: 100.0, r: 0.5777980879351843
06/02/2019 01:48:07 step: 9534, epoch: 288, batch: 29, loss: 0.006120018661022186, acc: 100.0, f1: 100.0, r: 0.7902358283657638
06/02/2019 01:48:07 *** evaluating ***
06/02/2019 01:48:08 step: 289, epoch: 288, acc: 58.54700854700855, f1: 30.33648291868859, r: 0.33113586701953096
06/02/2019 01:48:08 *** epoch: 290 ***
06/02/2019 01:48:08 *** training ***
06/02/2019 01:48:09 step: 9542, epoch: 289, batch: 4, loss: 0.0006077289581298828, acc: 100.0, f1: 100.0, r: 0.7580668071220188
06/02/2019 01:48:09 step: 9547, epoch: 289, batch: 9, loss: 0.0023490190505981445, acc: 100.0, f1: 100.0, r: 0.7925253270104192
06/02/2019 01:48:10 step: 9552, epoch: 289, batch: 14, loss: 0.0007166787981987, acc: 100.0, f1: 100.0, r: 0.8160427085720326
06/02/2019 01:48:11 step: 9557, epoch: 289, batch: 19, loss: 0.0014648213982582092, acc: 100.0, f1: 100.0, r: 0.7399086913881002
06/02/2019 01:48:12 step: 9562, epoch: 289, batch: 24, loss: 0.002178661525249481, acc: 100.0, f1: 100.0, r: 0.8468749177454796
06/02/2019 01:48:13 step: 9567, epoch: 289, batch: 29, loss: 0.0030600279569625854, acc: 100.0, f1: 100.0, r: 0.6987920237705411
06/02/2019 01:48:13 *** evaluating ***
06/02/2019 01:48:13 step: 290, epoch: 289, acc: 56.41025641025641, f1: 29.545482761967754, r: 0.3233194320190477
06/02/2019 01:48:13 *** epoch: 291 ***
06/02/2019 01:48:13 *** training ***
06/02/2019 01:48:14 step: 9575, epoch: 290, batch: 4, loss: 0.0036874860525131226, acc: 100.0, f1: 100.0, r: 0.6641300629802479
06/02/2019 01:48:15 step: 9580, epoch: 290, batch: 9, loss: 0.0013545826077461243, acc: 100.0, f1: 100.0, r: 0.8235856622285054
06/02/2019 01:48:16 step: 9585, epoch: 290, batch: 14, loss: 0.0013773292303085327, acc: 100.0, f1: 100.0, r: 0.7477043474219155
06/02/2019 01:48:17 step: 9590, epoch: 290, batch: 19, loss: 0.003032900393009186, acc: 100.0, f1: 100.0, r: 0.6145791164727016
06/02/2019 01:48:17 step: 9595, epoch: 290, batch: 24, loss: 0.0005679875612258911, acc: 100.0, f1: 100.0, r: 0.8019395176975621
06/02/2019 01:48:18 step: 9600, epoch: 290, batch: 29, loss: 0.002086721360683441, acc: 100.0, f1: 100.0, r: 0.8011985363127158
06/02/2019 01:48:19 *** evaluating ***
06/02/2019 01:48:19 step: 291, epoch: 290, acc: 57.692307692307686, f1: 29.0300633537375, r: 0.3323047755227492
06/02/2019 01:48:19 *** epoch: 292 ***
06/02/2019 01:48:19 *** training ***
06/02/2019 01:48:20 step: 9608, epoch: 291, batch: 4, loss: 0.0013698861002922058, acc: 100.0, f1: 100.0, r: 0.706195343252804
06/02/2019 01:48:21 step: 9613, epoch: 291, batch: 9, loss: 0.002474665641784668, acc: 100.0, f1: 100.0, r: 0.6720490381335376
06/02/2019 01:48:21 step: 9618, epoch: 291, batch: 14, loss: 0.00391029566526413, acc: 100.0, f1: 100.0, r: 0.8041305537508096
06/02/2019 01:48:22 step: 9623, epoch: 291, batch: 19, loss: 0.0015240088105201721, acc: 100.0, f1: 100.0, r: 0.6729481367877251
06/02/2019 01:48:23 step: 9628, epoch: 291, batch: 24, loss: 0.0015361085534095764, acc: 100.0, f1: 100.0, r: 0.8388437400948356
06/02/2019 01:48:24 step: 9633, epoch: 291, batch: 29, loss: 0.004971593618392944, acc: 100.0, f1: 100.0, r: 0.7900451162939016
06/02/2019 01:48:24 *** evaluating ***
06/02/2019 01:48:25 step: 292, epoch: 291, acc: 59.401709401709404, f1: 31.42936585081712, r: 0.3391237254710279
06/02/2019 01:48:25 *** epoch: 293 ***
06/02/2019 01:48:25 *** training ***
06/02/2019 01:48:25 step: 9641, epoch: 292, batch: 4, loss: 0.014453798532485962, acc: 100.0, f1: 100.0, r: 0.6639441392781043
06/02/2019 01:48:26 step: 9646, epoch: 292, batch: 9, loss: 0.001875653862953186, acc: 100.0, f1: 100.0, r: 0.7853737279602447
06/02/2019 01:48:27 step: 9651, epoch: 292, batch: 14, loss: 0.00451284646987915, acc: 100.0, f1: 100.0, r: 0.6595411768193936
06/02/2019 01:48:28 step: 9656, epoch: 292, batch: 19, loss: 0.0015481933951377869, acc: 100.0, f1: 100.0, r: 0.7868453077039095
06/02/2019 01:48:29 step: 9661, epoch: 292, batch: 24, loss: 0.0022609680891036987, acc: 100.0, f1: 100.0, r: 0.7029934125521973
06/02/2019 01:48:30 step: 9666, epoch: 292, batch: 29, loss: 0.0008001700043678284, acc: 100.0, f1: 100.0, r: 0.7830652033223319
06/02/2019 01:48:30 *** evaluating ***
06/02/2019 01:48:30 step: 293, epoch: 292, acc: 57.692307692307686, f1: 30.136167729712792, r: 0.34305723954724526
06/02/2019 01:48:30 *** epoch: 294 ***
06/02/2019 01:48:30 *** training ***
06/02/2019 01:48:31 step: 9674, epoch: 293, batch: 4, loss: 0.0011880025267601013, acc: 100.0, f1: 100.0, r: 0.7297942669011634
06/02/2019 01:48:32 step: 9679, epoch: 293, batch: 9, loss: 0.0018137022852897644, acc: 100.0, f1: 100.0, r: 0.6395808389381451
06/02/2019 01:48:33 step: 9684, epoch: 293, batch: 14, loss: 0.002298131585121155, acc: 100.0, f1: 100.0, r: 0.8135522478945353
06/02/2019 01:48:34 step: 9689, epoch: 293, batch: 19, loss: 0.0009889155626296997, acc: 100.0, f1: 100.0, r: 0.7154352866810781
06/02/2019 01:48:34 step: 9694, epoch: 293, batch: 24, loss: 0.005309268832206726, acc: 100.0, f1: 100.0, r: 0.7790218098084583
06/02/2019 01:48:35 step: 9699, epoch: 293, batch: 29, loss: 0.002664625644683838, acc: 100.0, f1: 100.0, r: 0.7859668272991274
06/02/2019 01:48:36 *** evaluating ***
06/02/2019 01:48:36 step: 294, epoch: 293, acc: 59.401709401709404, f1: 30.919916231379503, r: 0.3428224166206717
06/02/2019 01:48:36 *** epoch: 295 ***
06/02/2019 01:48:36 *** training ***
06/02/2019 01:48:37 step: 9707, epoch: 294, batch: 4, loss: 0.003815092146396637, acc: 100.0, f1: 100.0, r: 0.5693379622507442
06/02/2019 01:48:37 step: 9712, epoch: 294, batch: 9, loss: 0.0009423941373825073, acc: 100.0, f1: 100.0, r: 0.7484586178342324
06/02/2019 01:48:38 step: 9717, epoch: 294, batch: 14, loss: 0.0015732049942016602, acc: 100.0, f1: 100.0, r: 0.728149389789426
06/02/2019 01:48:39 step: 9722, epoch: 294, batch: 19, loss: 0.0019202455878257751, acc: 100.0, f1: 100.0, r: 0.7084895695896588
06/02/2019 01:48:40 step: 9727, epoch: 294, batch: 24, loss: 0.0023233219981193542, acc: 100.0, f1: 100.0, r: 0.8419704767819016
06/02/2019 01:48:41 step: 9732, epoch: 294, batch: 29, loss: 0.0014792010188102722, acc: 100.0, f1: 100.0, r: 0.7871869359815642
06/02/2019 01:48:41 *** evaluating ***
06/02/2019 01:48:41 step: 295, epoch: 294, acc: 57.692307692307686, f1: 29.08611969674316, r: 0.33242150208499277
06/02/2019 01:48:41 *** epoch: 296 ***
06/02/2019 01:48:41 *** training ***
06/02/2019 01:48:42 step: 9740, epoch: 295, batch: 4, loss: 0.0022339224815368652, acc: 100.0, f1: 100.0, r: 0.832391000676179
06/02/2019 01:48:43 step: 9745, epoch: 295, batch: 9, loss: 0.002457074820995331, acc: 100.0, f1: 100.0, r: 0.706413467501218
06/02/2019 01:48:44 step: 9750, epoch: 295, batch: 14, loss: 0.002001151442527771, acc: 100.0, f1: 100.0, r: 0.6334753018158014
06/02/2019 01:48:45 step: 9755, epoch: 295, batch: 19, loss: 0.0012376755475997925, acc: 100.0, f1: 100.0, r: 0.6096479650423439
06/02/2019 01:48:46 step: 9760, epoch: 295, batch: 24, loss: 0.003129851073026657, acc: 100.0, f1: 100.0, r: 0.724206798422337
06/02/2019 01:48:46 step: 9765, epoch: 295, batch: 29, loss: 0.0009667947888374329, acc: 100.0, f1: 100.0, r: 0.8363004649401586
06/02/2019 01:48:47 *** evaluating ***
06/02/2019 01:48:47 step: 296, epoch: 295, acc: 58.54700854700855, f1: 30.362595434677957, r: 0.3303984934157394
06/02/2019 01:48:47 *** epoch: 297 ***
06/02/2019 01:48:47 *** training ***
06/02/2019 01:48:48 step: 9773, epoch: 296, batch: 4, loss: 0.0015812069177627563, acc: 100.0, f1: 100.0, r: 0.8048048312058224
06/02/2019 01:48:49 step: 9778, epoch: 296, batch: 9, loss: 0.0022362545132637024, acc: 100.0, f1: 100.0, r: 0.8000319017295225
06/02/2019 01:48:50 step: 9783, epoch: 296, batch: 14, loss: 0.0013471469283103943, acc: 100.0, f1: 100.0, r: 0.7112791259486554
06/02/2019 01:48:51 step: 9788, epoch: 296, batch: 19, loss: 0.019907765090465546, acc: 98.4375, f1: 96.1111111111111, r: 0.7520098337733288
06/02/2019 01:48:51 step: 9793, epoch: 296, batch: 24, loss: 0.0010171905159950256, acc: 100.0, f1: 100.0, r: 0.791048663880494
06/02/2019 01:48:52 step: 9798, epoch: 296, batch: 29, loss: 0.0021802783012390137, acc: 100.0, f1: 100.0, r: 0.7933456286711996
06/02/2019 01:48:53 *** evaluating ***
06/02/2019 01:48:53 step: 297, epoch: 296, acc: 54.700854700854705, f1: 27.740466272724333, r: 0.32306475652244454
06/02/2019 01:48:53 *** epoch: 298 ***
06/02/2019 01:48:53 *** training ***
06/02/2019 01:48:54 step: 9806, epoch: 297, batch: 4, loss: 0.003645487129688263, acc: 100.0, f1: 100.0, r: 0.7843773041091515
06/02/2019 01:48:55 step: 9811, epoch: 297, batch: 9, loss: 0.0019118934869766235, acc: 100.0, f1: 100.0, r: 0.7129638152460086
06/02/2019 01:48:55 step: 9816, epoch: 297, batch: 14, loss: 0.00052642822265625, acc: 100.0, f1: 100.0, r: 0.6681481086253297
06/02/2019 01:48:56 step: 9821, epoch: 297, batch: 19, loss: 0.0030878260731697083, acc: 100.0, f1: 100.0, r: 0.7917692662779539
06/02/2019 01:48:57 step: 9826, epoch: 297, batch: 24, loss: 0.002433858811855316, acc: 100.0, f1: 100.0, r: 0.8215671651342032
06/02/2019 01:48:58 step: 9831, epoch: 297, batch: 29, loss: 0.0007853582501411438, acc: 100.0, f1: 100.0, r: 0.7159794449624546
06/02/2019 01:48:58 *** evaluating ***
06/02/2019 01:48:59 step: 298, epoch: 297, acc: 57.26495726495726, f1: 29.277816393175716, r: 0.33557549148189947
06/02/2019 01:48:59 *** epoch: 299 ***
06/02/2019 01:48:59 *** training ***
06/02/2019 01:48:59 step: 9839, epoch: 298, batch: 4, loss: 0.001845717430114746, acc: 100.0, f1: 100.0, r: 0.6188029979508708
06/02/2019 01:49:00 step: 9844, epoch: 298, batch: 9, loss: 0.002661876380443573, acc: 100.0, f1: 100.0, r: 0.6968565680308146
06/02/2019 01:49:01 step: 9849, epoch: 298, batch: 14, loss: 0.0018924251198768616, acc: 100.0, f1: 100.0, r: 0.7932715797399895
06/02/2019 01:49:02 step: 9854, epoch: 298, batch: 19, loss: 0.0024397820234298706, acc: 100.0, f1: 100.0, r: 0.6208329581445867
06/02/2019 01:49:03 step: 9859, epoch: 298, batch: 24, loss: 0.002287246286869049, acc: 100.0, f1: 100.0, r: 0.745008629881709
06/02/2019 01:49:04 step: 9864, epoch: 298, batch: 29, loss: 0.0034420937299728394, acc: 100.0, f1: 100.0, r: 0.7222630633703256
06/02/2019 01:49:04 *** evaluating ***
06/02/2019 01:49:04 step: 299, epoch: 298, acc: 56.837606837606835, f1: 27.431547619047613, r: 0.33588515769801763
06/02/2019 01:49:04 *** epoch: 300 ***
06/02/2019 01:49:04 *** training ***
06/02/2019 01:49:05 step: 9872, epoch: 299, batch: 4, loss: 0.0044133104383945465, acc: 100.0, f1: 100.0, r: 0.627048240707351
06/02/2019 01:49:06 step: 9877, epoch: 299, batch: 9, loss: 0.0015283748507499695, acc: 100.0, f1: 100.0, r: 0.7074726494899877
06/02/2019 01:49:07 step: 9882, epoch: 299, batch: 14, loss: 0.0020103976130485535, acc: 100.0, f1: 100.0, r: 0.7153861863508723
06/02/2019 01:49:08 step: 9887, epoch: 299, batch: 19, loss: 0.006261937320232391, acc: 100.0, f1: 100.0, r: 0.7926679455417727
06/02/2019 01:49:09 step: 9892, epoch: 299, batch: 24, loss: 0.0013719499111175537, acc: 100.0, f1: 100.0, r: 0.7961139304837102
06/02/2019 01:49:09 step: 9897, epoch: 299, batch: 29, loss: 0.0026035457849502563, acc: 100.0, f1: 100.0, r: 0.8141714635307061
06/02/2019 01:49:10 *** evaluating ***
06/02/2019 01:49:10 step: 300, epoch: 299, acc: 57.26495726495726, f1: 29.61513674806222, r: 0.3438677861531436
06/02/2019 01:49:10 
*** Best acc model ***
epoch: 71
acc: 59.82905982905983
f1: 30.67783327134584
corr: 0.3501823514341943
06/02/2019 01:49:10 Loading Test Data
06/02/2019 01:49:10 load data from data/word2vec_temp/test_text.npy, data/word2vec_temp/test_label.npy, training: False
06/02/2019 01:49:36 loaded. total len: 2228
06/02/2019 01:49:36 Test: length: 2228, total batch: 35, batch size: 64
06/02/2019 01:49:38 
*** Test Result ***
acc: 57.26495726495726
f1: 29.61513674806222
corr: 0.3438677861531436
