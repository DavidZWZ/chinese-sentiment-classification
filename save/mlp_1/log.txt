06/01/2019 10:35:50 {'input_path': 'data/word2vec_temp', 'output_path': 'save/mlp_1', 'gpu': True, 'seed': 20000125, 'display_per_batch': 5, 'optimizer': 'adagrad', 'lr': 0.01, 'lr_decay': 0, 'weight_decay': 0.0001, 'momentum': 0.985, 'num_epochs': 300, 'batch_size': 64, 'num_labels': 8, 'embedding_size': 300, 'type': 'mlp', 'mlp': {'max_length': 512, 'dropout': 0.5, 'hidden_size': 512, 'loss': 'cross_entropy'}}
06/01/2019 10:35:50 Loading Train Data
06/01/2019 10:35:50 load data from data/word2vec_temp/train_text.npy, data/word2vec_temp/train_label.npy, training: True
06/01/2019 10:36:22 loaded. total len: 2342
06/01/2019 10:36:22 Train: length: 2108, total batch: 33, batch size: 64
06/01/2019 10:36:22 Dev: length: 234, total batch: 4, batch size: 64
06/01/2019 10:36:22 Loading model mlp
06/01/2019 10:36:30 *** epoch: 1 ***
06/01/2019 10:36:30 *** training ***
06/01/2019 10:36:30 step: 5, epoch: 0, batch: 4, loss: 18.411392211914062, acc: 12.5, f1: 4.719004719004719, r: -0.02713453698982065
06/01/2019 10:36:31 step: 10, epoch: 0, batch: 9, loss: 3.5352513790130615, acc: 7.8125, f1: 8.59548296027425, r: 0.08174245775784669
06/01/2019 10:36:31 step: 15, epoch: 0, batch: 14, loss: 1.8681751489639282, acc: 26.5625, f1: 16.31330534155619, r: 0.09654598970853355
06/01/2019 10:36:31 step: 20, epoch: 0, batch: 19, loss: 1.939516544342041, acc: 18.75, f1: 11.480519480519481, r: 0.07300946883983736
06/01/2019 10:36:31 step: 25, epoch: 0, batch: 24, loss: 1.9679388999938965, acc: 23.4375, f1: 11.397714604236343, r: 0.10861727069709004
06/01/2019 10:36:31 step: 30, epoch: 0, batch: 29, loss: 2.0352375507354736, acc: 17.1875, f1: 9.235209235209235, r: 0.05165378718493954
06/01/2019 10:36:32 *** evaluating ***
06/01/2019 10:36:32 step: 1, epoch: 0, acc: 43.162393162393165, f1: 12.086286538635845, r: 0.08966728761365382
06/01/2019 10:36:32 *** epoch: 2 ***
06/01/2019 10:36:32 *** training ***
06/01/2019 10:36:32 step: 38, epoch: 1, batch: 4, loss: 1.4825208187103271, acc: 57.8125, f1: 17.549226441631504, r: 0.18351734119797222
06/01/2019 10:36:32 step: 43, epoch: 1, batch: 9, loss: 1.325417399406433, acc: 60.9375, f1: 41.17747382641, r: 0.38919402974182116
06/01/2019 10:36:32 step: 48, epoch: 1, batch: 14, loss: 1.5198158025741577, acc: 43.75, f1: 28.607913825305125, r: 0.2015114133423468
06/01/2019 10:36:32 step: 53, epoch: 1, batch: 19, loss: 1.4115885496139526, acc: 43.75, f1: 32.210942101798274, r: 0.22807500624651014
06/01/2019 10:36:32 step: 58, epoch: 1, batch: 24, loss: 1.5526306629180908, acc: 45.3125, f1: 17.224499089253186, r: 0.20905580434075585
06/01/2019 10:36:32 step: 63, epoch: 1, batch: 29, loss: 1.301389217376709, acc: 53.125, f1: 27.1978021978022, r: 0.3189556502862366
06/01/2019 10:36:33 *** evaluating ***
06/01/2019 10:36:33 step: 2, epoch: 1, acc: 36.75213675213676, f1: 11.250642597279342, r: 0.1520329142589675
06/01/2019 10:36:33 *** epoch: 3 ***
06/01/2019 10:36:33 *** training ***
06/01/2019 10:36:33 step: 71, epoch: 2, batch: 4, loss: 1.4088947772979736, acc: 43.75, f1: 18.749999999999996, r: 0.3591862956611459
06/01/2019 10:36:33 step: 76, epoch: 2, batch: 9, loss: 1.2170624732971191, acc: 51.5625, f1: 26.051587301587304, r: 0.3610028222848316
06/01/2019 10:36:33 step: 81, epoch: 2, batch: 14, loss: 1.0817372798919678, acc: 59.375, f1: 48.96825396825397, r: 0.36537202037927774
06/01/2019 10:36:33 step: 86, epoch: 2, batch: 19, loss: 1.29457426071167, acc: 62.5, f1: 33.70754472387866, r: 0.29247518319206195
06/01/2019 10:36:33 step: 91, epoch: 2, batch: 24, loss: 1.1956546306610107, acc: 57.8125, f1: 32.38516435404009, r: 0.3037186843711278
06/01/2019 10:36:33 step: 96, epoch: 2, batch: 29, loss: 1.308166265487671, acc: 48.4375, f1: 42.27511351935315, r: 0.3363391833922504
06/01/2019 10:36:34 *** evaluating ***
06/01/2019 10:36:34 step: 3, epoch: 2, acc: 53.41880341880342, f1: 20.370907693416278, r: 0.2224473785627076
06/01/2019 10:36:34 *** epoch: 4 ***
06/01/2019 10:36:34 *** training ***
06/01/2019 10:36:34 step: 104, epoch: 3, batch: 4, loss: 0.8707942366600037, acc: 62.5, f1: 39.724799010513294, r: 0.4462027475921869
06/01/2019 10:36:34 step: 109, epoch: 3, batch: 9, loss: 1.0309367179870605, acc: 64.0625, f1: 40.495302287581694, r: 0.3591480877841909
06/01/2019 10:36:34 step: 114, epoch: 3, batch: 14, loss: 1.1056243181228638, acc: 56.25, f1: 45.39009046241447, r: 0.32353661435914394
06/01/2019 10:36:34 step: 119, epoch: 3, batch: 19, loss: 1.131118655204773, acc: 50.0, f1: 14.246575342465754, r: 0.31121670705089277
06/01/2019 10:36:34 step: 124, epoch: 3, batch: 24, loss: 1.063891053199768, acc: 56.25, f1: 49.03826734455537, r: 0.43509497151138454
06/01/2019 10:36:35 step: 129, epoch: 3, batch: 29, loss: 0.7792723774909973, acc: 71.875, f1: 42.5, r: 0.46885949083073525
06/01/2019 10:36:35 *** evaluating ***
06/01/2019 10:36:35 step: 4, epoch: 3, acc: 43.162393162393165, f1: 14.829419430720476, r: 0.21853393187163306
06/01/2019 10:36:35 *** epoch: 5 ***
06/01/2019 10:36:35 *** training ***
06/01/2019 10:36:35 step: 137, epoch: 4, batch: 4, loss: 1.047227144241333, acc: 59.375, f1: 34.799587385794275, r: 0.46798593806688027
06/01/2019 10:36:35 step: 142, epoch: 4, batch: 9, loss: 0.7303246855735779, acc: 70.3125, f1: 64.14597970335674, r: 0.5740624322338278
06/01/2019 10:36:35 step: 147, epoch: 4, batch: 14, loss: 0.7241173982620239, acc: 75.0, f1: 46.34665103415103, r: 0.47624369037744735
06/01/2019 10:36:35 step: 152, epoch: 4, batch: 19, loss: 0.7241949439048767, acc: 73.4375, f1: 45.49483322854702, r: 0.3499230660001364
06/01/2019 10:36:36 step: 157, epoch: 4, batch: 24, loss: 0.8987070322036743, acc: 60.9375, f1: 28.783248443689867, r: 0.5045800923545286
06/01/2019 10:36:36 step: 162, epoch: 4, batch: 29, loss: 0.698779821395874, acc: 68.75, f1: 61.14915881220229, r: 0.5277253702706228
06/01/2019 10:36:36 *** evaluating ***
06/01/2019 10:36:36 step: 5, epoch: 4, acc: 55.98290598290598, f1: 19.6765811587793, r: 0.2925625529808916
06/01/2019 10:36:36 *** epoch: 6 ***
06/01/2019 10:36:36 *** training ***
06/01/2019 10:36:36 step: 170, epoch: 5, batch: 4, loss: 0.8031684160232544, acc: 62.5, f1: 43.70421245421245, r: 0.5693131842651077
06/01/2019 10:36:36 step: 175, epoch: 5, batch: 9, loss: 0.7209485173225403, acc: 78.125, f1: 65.78974118382985, r: 0.5562735310476323
06/01/2019 10:36:36 step: 180, epoch: 5, batch: 14, loss: 0.813668429851532, acc: 68.75, f1: 47.36652236652237, r: 0.46523595955864444
06/01/2019 10:36:36 step: 185, epoch: 5, batch: 19, loss: 0.566807210445404, acc: 75.0, f1: 62.78923690688397, r: 0.5446937569971586
06/01/2019 10:36:37 step: 190, epoch: 5, batch: 24, loss: 0.5589163303375244, acc: 81.25, f1: 76.71194705677465, r: 0.7306461353743483
06/01/2019 10:36:37 step: 195, epoch: 5, batch: 29, loss: 0.49845486879348755, acc: 79.6875, f1: 63.713878713878714, r: 0.5429482377661126
06/01/2019 10:36:37 *** evaluating ***
06/01/2019 10:36:37 step: 6, epoch: 5, acc: 58.97435897435898, f1: 19.489074679914374, r: 0.3061308965570745
06/01/2019 10:36:37 *** epoch: 7 ***
06/01/2019 10:36:37 *** training ***
06/01/2019 10:36:37 step: 203, epoch: 6, batch: 4, loss: 0.6028875708580017, acc: 71.875, f1: 61.06734226871527, r: 0.5244660473496874
06/01/2019 10:36:37 step: 208, epoch: 6, batch: 9, loss: 0.5594107508659363, acc: 71.875, f1: 49.27995391705069, r: 0.5668547547171331
06/01/2019 10:36:37 step: 213, epoch: 6, batch: 14, loss: 0.42421862483024597, acc: 82.8125, f1: 68.66369421668962, r: 0.6128015221728753
06/01/2019 10:36:37 step: 218, epoch: 6, batch: 19, loss: 0.6870115399360657, acc: 70.3125, f1: 51.1906977431171, r: 0.5918397107414467
06/01/2019 10:36:38 step: 223, epoch: 6, batch: 24, loss: 0.5755928754806519, acc: 75.0, f1: 49.8921568627451, r: 0.5794547131338011
06/01/2019 10:36:38 step: 228, epoch: 6, batch: 29, loss: 0.7590391635894775, acc: 71.875, f1: 54.96021049929406, r: 0.5492850844558428
06/01/2019 10:36:38 *** evaluating ***
06/01/2019 10:36:38 step: 7, epoch: 6, acc: 58.119658119658126, f1: 19.676357109218582, r: 0.3290705326529015
06/01/2019 10:36:38 *** epoch: 8 ***
06/01/2019 10:36:38 *** training ***
06/01/2019 10:36:38 step: 236, epoch: 7, batch: 4, loss: 0.4666435420513153, acc: 79.6875, f1: 60.84807256235828, r: 0.541109347824202
06/01/2019 10:36:38 step: 241, epoch: 7, batch: 9, loss: 0.4963712990283966, acc: 81.25, f1: 68.7641723356009, r: 0.44954904934497036
06/01/2019 10:36:38 step: 246, epoch: 7, batch: 14, loss: 0.5649574398994446, acc: 82.8125, f1: 73.44671201814059, r: 0.5752041680178966
06/01/2019 10:36:39 step: 251, epoch: 7, batch: 19, loss: 0.3913019895553589, acc: 84.375, f1: 69.18557632843347, r: 0.5823847550391709
06/01/2019 10:36:39 step: 256, epoch: 7, batch: 24, loss: 0.41494807600975037, acc: 82.8125, f1: 77.39880952380952, r: 0.6740039387406134
06/01/2019 10:36:39 step: 261, epoch: 7, batch: 29, loss: 0.6194159984588623, acc: 70.3125, f1: 64.93348115299334, r: 0.5430211091754306
06/01/2019 10:36:39 *** evaluating ***
06/01/2019 10:36:39 step: 8, epoch: 7, acc: 60.256410256410255, f1: 20.754563233376793, r: 0.3180459649333885
06/01/2019 10:36:39 *** epoch: 9 ***
06/01/2019 10:36:39 *** training ***
06/01/2019 10:36:39 step: 269, epoch: 8, batch: 4, loss: 0.4655977785587311, acc: 76.5625, f1: 63.38611306591603, r: 0.5107093829084106
06/01/2019 10:36:39 step: 274, epoch: 8, batch: 9, loss: 0.35147368907928467, acc: 89.0625, f1: 91.12374851564968, r: 0.670718688986302
06/01/2019 10:36:40 step: 279, epoch: 8, batch: 14, loss: 0.5522393584251404, acc: 78.125, f1: 61.074264217856886, r: 0.5676000163278245
06/01/2019 10:36:40 step: 284, epoch: 8, batch: 19, loss: 0.41136348247528076, acc: 79.6875, f1: 64.21496585540926, r: 0.5490342554664414
06/01/2019 10:36:40 step: 289, epoch: 8, batch: 24, loss: 0.5297322869300842, acc: 82.8125, f1: 69.72063825530212, r: 0.7059784402192038
06/01/2019 10:36:40 step: 294, epoch: 8, batch: 29, loss: 0.4576157033443451, acc: 78.125, f1: 71.08618089145473, r: 0.5157779237393394
06/01/2019 10:36:40 *** evaluating ***
06/01/2019 10:36:40 step: 9, epoch: 8, acc: 57.26495726495726, f1: 19.78452954942762, r: 0.3212806912732783
06/01/2019 10:36:40 *** epoch: 10 ***
06/01/2019 10:36:40 *** training ***
06/01/2019 10:36:40 step: 302, epoch: 9, batch: 4, loss: 0.40958818793296814, acc: 81.25, f1: 78.1557067271353, r: 0.6639596173993553
06/01/2019 10:36:40 step: 307, epoch: 9, batch: 9, loss: 0.32173630595207214, acc: 87.5, f1: 72.10148896608386, r: 0.6244848900231602
06/01/2019 10:36:41 step: 312, epoch: 9, batch: 14, loss: 0.5078563094139099, acc: 76.5625, f1: 69.5745044429255, r: 0.6843925744994992
06/01/2019 10:36:41 step: 317, epoch: 9, batch: 19, loss: 0.38114607334136963, acc: 79.6875, f1: 73.17741782027495, r: 0.5532895192136693
06/01/2019 10:36:41 step: 322, epoch: 9, batch: 24, loss: 0.3241156041622162, acc: 89.0625, f1: 81.23829241841663, r: 0.5769097687534451
06/01/2019 10:36:41 step: 327, epoch: 9, batch: 29, loss: 0.2935320734977722, acc: 89.0625, f1: 79.18518518518518, r: 0.7366274613596331
06/01/2019 10:36:41 *** evaluating ***
06/01/2019 10:36:41 step: 10, epoch: 9, acc: 57.692307692307686, f1: 19.526862026862027, r: 0.31980492504312913
06/01/2019 10:36:41 *** epoch: 11 ***
06/01/2019 10:36:41 *** training ***
06/01/2019 10:36:41 step: 335, epoch: 10, batch: 4, loss: 0.3868163824081421, acc: 92.1875, f1: 90.28143756404626, r: 0.6875168997310113
06/01/2019 10:36:42 step: 340, epoch: 10, batch: 9, loss: 0.3635892868041992, acc: 84.375, f1: 83.07425652253238, r: 0.5254511123537822
06/01/2019 10:36:42 step: 345, epoch: 10, batch: 14, loss: 0.48778805136680603, acc: 76.5625, f1: 60.957572238060045, r: 0.6379798280496835
06/01/2019 10:36:42 step: 350, epoch: 10, batch: 19, loss: 0.29273319244384766, acc: 90.625, f1: 86.03462157809983, r: 0.6013959834770919
06/01/2019 10:36:42 step: 355, epoch: 10, batch: 24, loss: 0.39094558358192444, acc: 87.5, f1: 74.11949949530275, r: 0.62236676475972
06/01/2019 10:36:42 step: 360, epoch: 10, batch: 29, loss: 0.3889353275299072, acc: 84.375, f1: 69.99512375571697, r: 0.6288295628054459
06/01/2019 10:36:42 *** evaluating ***
06/01/2019 10:36:42 step: 11, epoch: 10, acc: 58.119658119658126, f1: 20.17389112903226, r: 0.3113260428685418
06/01/2019 10:36:42 *** epoch: 12 ***
06/01/2019 10:36:42 *** training ***
06/01/2019 10:36:42 step: 368, epoch: 11, batch: 4, loss: 0.3210690915584564, acc: 90.625, f1: 78.4564393939394, r: 0.7011234475713666
06/01/2019 10:36:43 step: 373, epoch: 11, batch: 9, loss: 0.32806435227394104, acc: 89.0625, f1: 81.92371810384232, r: 0.6072807907104584
06/01/2019 10:36:43 step: 378, epoch: 11, batch: 14, loss: 0.2574012875556946, acc: 96.875, f1: 95.0915750915751, r: 0.6276220022700216
06/01/2019 10:36:43 step: 383, epoch: 11, batch: 19, loss: 0.3776279389858246, acc: 85.9375, f1: 77.36437868865713, r: 0.6449361620594621
06/01/2019 10:36:43 step: 388, epoch: 11, batch: 24, loss: 0.3364482820034027, acc: 90.625, f1: 86.82631967515688, r: 0.8010773633466098
06/01/2019 10:36:43 step: 393, epoch: 11, batch: 29, loss: 0.23583456873893738, acc: 90.625, f1: 73.7580296048038, r: 0.7203837966831188
06/01/2019 10:36:43 *** evaluating ***
06/01/2019 10:36:43 step: 12, epoch: 11, acc: 58.97435897435898, f1: 20.52390180878553, r: 0.30796488577340236
06/01/2019 10:36:43 *** epoch: 13 ***
06/01/2019 10:36:43 *** training ***
06/01/2019 10:36:43 step: 401, epoch: 12, batch: 4, loss: 0.2473079115152359, acc: 90.625, f1: 74.38903380901235, r: 0.6292707631723767
06/01/2019 10:36:44 step: 406, epoch: 12, batch: 9, loss: 0.4240935742855072, acc: 87.5, f1: 76.1246976707503, r: 0.6344448925000856
06/01/2019 10:36:44 step: 411, epoch: 12, batch: 14, loss: 0.1965748816728592, acc: 92.1875, f1: 77.84130486171303, r: 0.6351169074176042
06/01/2019 10:36:44 step: 416, epoch: 12, batch: 19, loss: 0.27980536222457886, acc: 92.1875, f1: 86.0187647754137, r: 0.687194732703299
06/01/2019 10:36:44 step: 421, epoch: 12, batch: 24, loss: 0.37347057461738586, acc: 89.0625, f1: 82.48877348142054, r: 0.6990494211165432
06/01/2019 10:36:44 step: 426, epoch: 12, batch: 29, loss: 0.20352092385292053, acc: 90.625, f1: 80.29842221746269, r: 0.7315252424247249
06/01/2019 10:36:44 *** evaluating ***
06/01/2019 10:36:44 step: 13, epoch: 12, acc: 59.401709401709404, f1: 20.583920732658413, r: 0.3196884760873322
06/01/2019 10:36:44 *** epoch: 14 ***
06/01/2019 10:36:44 *** training ***
06/01/2019 10:36:44 step: 434, epoch: 13, batch: 4, loss: 0.24803349375724792, acc: 95.3125, f1: 96.40590958605664, r: 0.7834915240346401
06/01/2019 10:36:45 step: 439, epoch: 13, batch: 9, loss: 0.3215063810348511, acc: 89.0625, f1: 91.54796001870469, r: 0.719554364408485
06/01/2019 10:36:45 step: 444, epoch: 13, batch: 14, loss: 0.26078349351882935, acc: 89.0625, f1: 86.13054837319542, r: 0.7099282257452338
06/01/2019 10:36:45 step: 449, epoch: 13, batch: 19, loss: 0.2808169722557068, acc: 90.625, f1: 86.15427457532722, r: 0.6376574518943752
06/01/2019 10:36:45 step: 454, epoch: 13, batch: 24, loss: 0.2986604571342468, acc: 89.0625, f1: 67.86661255411255, r: 0.6730285773070419
06/01/2019 10:36:45 step: 459, epoch: 13, batch: 29, loss: 0.18835589289665222, acc: 92.1875, f1: 92.12082983054461, r: 0.6293917504944875
06/01/2019 10:36:45 *** evaluating ***
06/01/2019 10:36:45 step: 14, epoch: 13, acc: 60.256410256410255, f1: 23.39397392361404, r: 0.3156921924208478
06/01/2019 10:36:45 *** epoch: 15 ***
06/01/2019 10:36:45 *** training ***
06/01/2019 10:36:45 step: 467, epoch: 14, batch: 4, loss: 0.3653903901576996, acc: 87.5, f1: 84.1041366041366, r: 0.7028091770144796
06/01/2019 10:36:46 step: 472, epoch: 14, batch: 9, loss: 0.4072488248348236, acc: 82.8125, f1: 72.92865472025892, r: 0.6247490889641126
06/01/2019 10:36:46 step: 477, epoch: 14, batch: 14, loss: 0.36775168776512146, acc: 82.8125, f1: 77.77185996349759, r: 0.6071913306276654
06/01/2019 10:36:46 step: 482, epoch: 14, batch: 19, loss: 0.2431708574295044, acc: 89.0625, f1: 89.33677433677435, r: 0.5814436139592454
06/01/2019 10:36:46 step: 487, epoch: 14, batch: 24, loss: 0.16479478776454926, acc: 95.3125, f1: 83.48070276497697, r: 0.7836144463795603
06/01/2019 10:36:46 step: 492, epoch: 14, batch: 29, loss: 0.22238217294216156, acc: 90.625, f1: 72.2334774489506, r: 0.7201859100993491
06/01/2019 10:36:46 *** evaluating ***
06/01/2019 10:36:46 step: 15, epoch: 14, acc: 59.82905982905983, f1: 20.49296536796537, r: 0.29926967366221074
06/01/2019 10:36:46 *** epoch: 16 ***
06/01/2019 10:36:46 *** training ***
06/01/2019 10:36:46 step: 500, epoch: 15, batch: 4, loss: 0.2801145315170288, acc: 92.1875, f1: 77.2813763186434, r: 0.5743963133038706
06/01/2019 10:36:47 step: 505, epoch: 15, batch: 9, loss: 0.2581842541694641, acc: 87.5, f1: 60.461670048136206, r: 0.6077755058782169
06/01/2019 10:36:47 step: 510, epoch: 15, batch: 14, loss: 0.2121659219264984, acc: 92.1875, f1: 82.59920634920634, r: 0.681166498785591
06/01/2019 10:36:47 step: 515, epoch: 15, batch: 19, loss: 0.2633318603038788, acc: 93.75, f1: 89.63250257367905, r: 0.6529864941044856
06/01/2019 10:36:47 step: 520, epoch: 15, batch: 24, loss: 0.2764400541782379, acc: 87.5, f1: 84.9161877394636, r: 0.7159419901665818
06/01/2019 10:36:47 step: 525, epoch: 15, batch: 29, loss: 0.11413484066724777, acc: 96.875, f1: 86.32386747802569, r: 0.7199740386977784
06/01/2019 10:36:47 *** evaluating ***
06/01/2019 10:36:47 step: 16, epoch: 15, acc: 59.401709401709404, f1: 20.697189069327596, r: 0.30639659876806297
06/01/2019 10:36:47 *** epoch: 17 ***
06/01/2019 10:36:47 *** training ***
06/01/2019 10:36:47 step: 533, epoch: 16, batch: 4, loss: 0.1722009927034378, acc: 93.75, f1: 90.18474279343843, r: 0.6347018494594425
06/01/2019 10:36:48 step: 538, epoch: 16, batch: 9, loss: 0.18708683550357819, acc: 92.1875, f1: 91.82871182871183, r: 0.6624666489187725
06/01/2019 10:36:48 step: 543, epoch: 16, batch: 14, loss: 0.2701815962791443, acc: 90.625, f1: 76.24007936507937, r: 0.6527069628795179
06/01/2019 10:36:48 step: 548, epoch: 16, batch: 19, loss: 0.2760850489139557, acc: 89.0625, f1: 84.94915936776403, r: 0.6320376359210318
06/01/2019 10:36:48 step: 553, epoch: 16, batch: 24, loss: 0.24930676817893982, acc: 92.1875, f1: 87.11580086580088, r: 0.6376438261067426
06/01/2019 10:36:48 step: 558, epoch: 16, batch: 29, loss: 0.1854443997144699, acc: 96.875, f1: 92.5925925925926, r: 0.7589460578174412
06/01/2019 10:36:48 *** evaluating ***
06/01/2019 10:36:48 step: 17, epoch: 16, acc: 58.119658119658126, f1: 19.62658532106127, r: 0.29374015946689347
06/01/2019 10:36:48 *** epoch: 18 ***
06/01/2019 10:36:48 *** training ***
06/01/2019 10:36:49 step: 566, epoch: 17, batch: 4, loss: 0.1549001783132553, acc: 92.1875, f1: 87.13547309133048, r: 0.6985965531988342
06/01/2019 10:36:49 step: 571, epoch: 17, batch: 9, loss: 0.20920313894748688, acc: 93.75, f1: 83.99166118060127, r: 0.5741708286743692
06/01/2019 10:36:49 step: 576, epoch: 17, batch: 14, loss: 0.15447843074798584, acc: 93.75, f1: 80.5788509735878, r: 0.5847244619585078
06/01/2019 10:36:49 step: 581, epoch: 17, batch: 19, loss: 0.09630438685417175, acc: 98.4375, f1: 99.3331164606376, r: 0.7654513553106532
06/01/2019 10:36:49 step: 586, epoch: 17, batch: 24, loss: 0.10508982837200165, acc: 98.4375, f1: 97.17948717948718, r: 0.7523488913828653
06/01/2019 10:36:49 step: 591, epoch: 17, batch: 29, loss: 0.08193305134773254, acc: 98.4375, f1: 98.67132867132867, r: 0.7976316352569284
06/01/2019 10:36:49 *** evaluating ***
06/01/2019 10:36:50 step: 18, epoch: 17, acc: 58.54700854700855, f1: 20.127688172043012, r: 0.28729367075780726
06/01/2019 10:36:50 *** epoch: 19 ***
06/01/2019 10:36:50 *** training ***
06/01/2019 10:36:50 step: 599, epoch: 18, batch: 4, loss: 0.28636404871940613, acc: 89.0625, f1: 85.24207205068731, r: 0.6340715799814639
06/01/2019 10:36:50 step: 604, epoch: 18, batch: 9, loss: 0.21495886147022247, acc: 90.625, f1: 89.61580086580086, r: 0.7684759033458701
06/01/2019 10:36:50 step: 609, epoch: 18, batch: 14, loss: 0.16238278150558472, acc: 93.75, f1: 93.58223711997297, r: 0.7408755833473681
06/01/2019 10:36:50 step: 614, epoch: 18, batch: 19, loss: 0.24291615188121796, acc: 92.1875, f1: 91.23900654156205, r: 0.7629944565712746
06/01/2019 10:36:51 step: 619, epoch: 18, batch: 24, loss: 0.23437656462192535, acc: 95.3125, f1: 93.71428571428572, r: 0.6784578406748951
06/01/2019 10:36:51 step: 624, epoch: 18, batch: 29, loss: 0.18611972033977509, acc: 92.1875, f1: 92.14959568733153, r: 0.7896356956739479
06/01/2019 10:36:51 *** evaluating ***
06/01/2019 10:36:51 step: 19, epoch: 18, acc: 58.97435897435898, f1: 21.286728466451972, r: 0.287366802875197
06/01/2019 10:36:51 *** epoch: 20 ***
06/01/2019 10:36:51 *** training ***
06/01/2019 10:36:51 step: 632, epoch: 19, batch: 4, loss: 0.07343217730522156, acc: 98.4375, f1: 99.04247104247104, r: 0.6643434196522431
06/01/2019 10:36:52 step: 637, epoch: 19, batch: 9, loss: 0.0856352224946022, acc: 96.875, f1: 85.03202450570872, r: 0.6995480689553986
06/01/2019 10:36:52 step: 642, epoch: 19, batch: 14, loss: 0.14646482467651367, acc: 95.3125, f1: 89.71622495151908, r: 0.7074770158285951
06/01/2019 10:36:52 step: 647, epoch: 19, batch: 19, loss: 0.16249209642410278, acc: 93.75, f1: 94.330860979294, r: 0.748142154597133
06/01/2019 10:36:52 step: 652, epoch: 19, batch: 24, loss: 0.13103747367858887, acc: 93.75, f1: 93.86122943167231, r: 0.7293530585099929
06/01/2019 10:36:53 step: 657, epoch: 19, batch: 29, loss: 0.18423029780387878, acc: 95.3125, f1: 95.3814935064935, r: 0.7485729119768637
06/01/2019 10:36:53 *** evaluating ***
06/01/2019 10:36:53 step: 20, epoch: 19, acc: 59.401709401709404, f1: 21.935285841524642, r: 0.29066659781715876
06/01/2019 10:36:53 *** epoch: 21 ***
06/01/2019 10:36:53 *** training ***
06/01/2019 10:36:53 step: 665, epoch: 20, batch: 4, loss: 0.14439433813095093, acc: 93.75, f1: 81.48248792270532, r: 0.6707784898849197
06/01/2019 10:36:53 step: 670, epoch: 20, batch: 9, loss: 0.13360010087490082, acc: 95.3125, f1: 93.4126214682498, r: 0.7103830052137198
06/01/2019 10:36:53 step: 675, epoch: 20, batch: 14, loss: 0.17444796860218048, acc: 90.625, f1: 82.5705329153605, r: 0.6935693806771548
06/01/2019 10:36:54 step: 680, epoch: 20, batch: 19, loss: 0.14492154121398926, acc: 95.3125, f1: 70.10466988727859, r: 0.6303805740185215
06/01/2019 10:36:54 step: 685, epoch: 20, batch: 24, loss: 0.2580723762512207, acc: 87.5, f1: 77.87931034482759, r: 0.6262345203179006
06/01/2019 10:36:54 step: 690, epoch: 20, batch: 29, loss: 0.15217238664627075, acc: 92.1875, f1: 92.41790729595608, r: 0.7258609720427642
06/01/2019 10:36:54 *** evaluating ***
06/01/2019 10:36:54 step: 21, epoch: 20, acc: 58.54700854700855, f1: 22.295614850309693, r: 0.28664558162812526
06/01/2019 10:36:54 *** epoch: 22 ***
06/01/2019 10:36:54 *** training ***
06/01/2019 10:36:54 step: 698, epoch: 21, batch: 4, loss: 0.128062441945076, acc: 96.875, f1: 94.14451185879757, r: 0.6499199435948363
06/01/2019 10:36:55 step: 703, epoch: 21, batch: 9, loss: 0.06288529932498932, acc: 100.0, f1: 100.0, r: 0.6969238394916744
06/01/2019 10:36:55 step: 708, epoch: 21, batch: 14, loss: 0.25198304653167725, acc: 90.625, f1: 87.37919555934262, r: 0.7130000154486762
06/01/2019 10:36:55 step: 713, epoch: 21, batch: 19, loss: 0.1414082944393158, acc: 95.3125, f1: 95.51023573200993, r: 0.7416534574959032
06/01/2019 10:36:55 step: 718, epoch: 21, batch: 24, loss: 0.17979790270328522, acc: 93.75, f1: 92.29755178907723, r: 0.6621438073655119
06/01/2019 10:36:56 step: 723, epoch: 21, batch: 29, loss: 0.17472049593925476, acc: 95.3125, f1: 91.88834154351395, r: 0.6442050718623392
06/01/2019 10:36:56 *** evaluating ***
06/01/2019 10:36:56 step: 22, epoch: 21, acc: 58.54700854700855, f1: 21.645067788639093, r: 0.2859401995059126
06/01/2019 10:36:56 *** epoch: 23 ***
06/01/2019 10:36:56 *** training ***
06/01/2019 10:36:56 step: 731, epoch: 22, batch: 4, loss: 0.20423558354377747, acc: 89.0625, f1: 82.55950937250962, r: 0.6405895841324164
06/01/2019 10:36:56 step: 736, epoch: 22, batch: 9, loss: 0.08457397669553757, acc: 98.4375, f1: 99.28854394049641, r: 0.667057479096151
06/01/2019 10:36:56 step: 741, epoch: 22, batch: 14, loss: 0.14596638083457947, acc: 95.3125, f1: 88.58885017421602, r: 0.6174778070251623
06/01/2019 10:36:57 step: 746, epoch: 22, batch: 19, loss: 0.17798206210136414, acc: 92.1875, f1: 91.18338399189463, r: 0.6977287314865229
06/01/2019 10:36:57 step: 751, epoch: 22, batch: 24, loss: 0.14902503788471222, acc: 93.75, f1: 78.63584915367599, r: 0.7110546120966554
06/01/2019 10:36:57 step: 756, epoch: 22, batch: 29, loss: 0.13798850774765015, acc: 93.75, f1: 93.1036313345159, r: 0.8054775713553223
06/01/2019 10:36:57 *** evaluating ***
06/01/2019 10:36:57 step: 23, epoch: 22, acc: 57.26495726495726, f1: 19.66477075119886, r: 0.28064417887756055
06/01/2019 10:36:57 *** epoch: 24 ***
06/01/2019 10:36:57 *** training ***
06/01/2019 10:36:58 step: 764, epoch: 23, batch: 4, loss: 0.08916392922401428, acc: 96.875, f1: 97.10074505992876, r: 0.6608335656718672
06/01/2019 10:36:58 step: 769, epoch: 23, batch: 9, loss: 0.10775672644376755, acc: 98.4375, f1: 98.14315663372267, r: 0.705520367447971
06/01/2019 10:36:58 step: 774, epoch: 23, batch: 14, loss: 0.12334463000297546, acc: 96.875, f1: 94.13196033562166, r: 0.7938188833823738
06/01/2019 10:36:58 step: 779, epoch: 23, batch: 19, loss: 0.10260505974292755, acc: 96.875, f1: 98.07549857549857, r: 0.7157123348833703
06/01/2019 10:36:58 step: 784, epoch: 23, batch: 24, loss: 0.13717021048069, acc: 95.3125, f1: 95.84457671957672, r: 0.5762236098002154
06/01/2019 10:36:59 step: 789, epoch: 23, batch: 29, loss: 0.13723495602607727, acc: 95.3125, f1: 87.90586932447397, r: 0.6514313820559859
06/01/2019 10:36:59 *** evaluating ***
06/01/2019 10:36:59 step: 24, epoch: 23, acc: 58.119658119658126, f1: 19.943387798196973, r: 0.28198111367731143
06/01/2019 10:36:59 *** epoch: 25 ***
06/01/2019 10:36:59 *** training ***
06/01/2019 10:36:59 step: 797, epoch: 24, batch: 4, loss: 0.1118493303656578, acc: 96.875, f1: 82.56446101273688, r: 0.7273719896192544
06/01/2019 10:36:59 step: 802, epoch: 24, batch: 9, loss: 0.09467887878417969, acc: 96.875, f1: 97.01655052264809, r: 0.753407422506521
06/01/2019 10:37:00 step: 807, epoch: 24, batch: 14, loss: 0.0963684692978859, acc: 96.875, f1: 98.56784819190835, r: 0.6964475180755253
06/01/2019 10:37:00 step: 812, epoch: 24, batch: 19, loss: 0.12033167481422424, acc: 98.4375, f1: 98.06763285024154, r: 0.7900192213693527
06/01/2019 10:37:00 step: 817, epoch: 24, batch: 24, loss: 0.13119164109230042, acc: 96.875, f1: 97.61883181623959, r: 0.7172101765990282
06/01/2019 10:37:00 step: 822, epoch: 24, batch: 29, loss: 0.12986117601394653, acc: 93.75, f1: 91.93932154801719, r: 0.6723496478457464
06/01/2019 10:37:00 *** evaluating ***
06/01/2019 10:37:00 step: 25, epoch: 24, acc: 58.54700854700855, f1: 21.89395548535846, r: 0.28141276494484646
06/01/2019 10:37:00 *** epoch: 26 ***
06/01/2019 10:37:00 *** training ***
06/01/2019 10:37:01 step: 830, epoch: 25, batch: 4, loss: 0.1819753795862198, acc: 90.625, f1: 75.51190476190477, r: 0.7607916509021015
06/01/2019 10:37:01 step: 835, epoch: 25, batch: 9, loss: 0.06631989777088165, acc: 98.4375, f1: 94.99596448748991, r: 0.6705586417317116
06/01/2019 10:37:01 step: 840, epoch: 25, batch: 14, loss: 0.17730197310447693, acc: 92.1875, f1: 83.71417984189723, r: 0.6374792178376201
06/01/2019 10:37:01 step: 845, epoch: 25, batch: 19, loss: 0.09199821203947067, acc: 98.4375, f1: 95.10204081632654, r: 0.6293092403017548
06/01/2019 10:37:02 step: 850, epoch: 25, batch: 24, loss: 0.08212529867887497, acc: 96.875, f1: 95.56404379933792, r: 0.6534266783576732
06/01/2019 10:37:02 step: 855, epoch: 25, batch: 29, loss: 0.16787311434745789, acc: 92.1875, f1: 88.23682476943347, r: 0.6811301165506848
06/01/2019 10:37:02 *** evaluating ***
06/01/2019 10:37:02 step: 26, epoch: 25, acc: 58.97435897435898, f1: 22.286148313492067, r: 0.2797988092651817
06/01/2019 10:37:02 *** epoch: 27 ***
06/01/2019 10:37:02 *** training ***
06/01/2019 10:37:02 step: 863, epoch: 26, batch: 4, loss: 0.09824921935796738, acc: 95.3125, f1: 95.862409479921, r: 0.6639385284857223
06/01/2019 10:37:02 step: 868, epoch: 26, batch: 9, loss: 0.13064414262771606, acc: 96.875, f1: 94.74993494665627, r: 0.6096278424789126
06/01/2019 10:37:03 step: 873, epoch: 26, batch: 14, loss: 0.06664958596229553, acc: 98.4375, f1: 99.26314819931841, r: 0.7238238554428944
06/01/2019 10:37:03 step: 878, epoch: 26, batch: 19, loss: 0.03618384525179863, acc: 98.4375, f1: 94.9579831932773, r: 0.6585737864498281
06/01/2019 10:37:03 step: 883, epoch: 26, batch: 24, loss: 0.12397351861000061, acc: 95.3125, f1: 91.52790400910702, r: 0.6788878771013539
06/01/2019 10:37:03 step: 888, epoch: 26, batch: 29, loss: 0.12638160586357117, acc: 93.75, f1: 96.64835164835165, r: 0.7161747655863732
06/01/2019 10:37:03 *** evaluating ***
06/01/2019 10:37:04 step: 27, epoch: 26, acc: 58.97435897435898, f1: 22.037523471949704, r: 0.2837110848581392
06/01/2019 10:37:04 *** epoch: 28 ***
06/01/2019 10:37:04 *** training ***
06/01/2019 10:37:04 step: 896, epoch: 27, batch: 4, loss: 0.04843117669224739, acc: 100.0, f1: 100.0, r: 0.7265796037417007
06/01/2019 10:37:04 step: 901, epoch: 27, batch: 9, loss: 0.16344556212425232, acc: 92.1875, f1: 88.43127705627705, r: 0.7386639637336901
06/01/2019 10:37:04 step: 906, epoch: 27, batch: 14, loss: 0.15024518966674805, acc: 93.75, f1: 83.29144081133066, r: 0.7945963012206991
06/01/2019 10:37:04 step: 911, epoch: 27, batch: 19, loss: 0.09464270621538162, acc: 96.875, f1: 85.71835571835571, r: 0.7485027986970618
06/01/2019 10:37:05 step: 916, epoch: 27, batch: 24, loss: 0.08416477590799332, acc: 95.3125, f1: 80.89929520195174, r: 0.6415987685576152
06/01/2019 10:37:05 step: 921, epoch: 27, batch: 29, loss: 0.06721746921539307, acc: 98.4375, f1: 94.978354978355, r: 0.7135258194795545
06/01/2019 10:37:05 *** evaluating ***
06/01/2019 10:37:05 step: 28, epoch: 27, acc: 58.54700854700855, f1: 20.412495753251793, r: 0.29192663833716703
06/01/2019 10:37:05 *** epoch: 29 ***
06/01/2019 10:37:05 *** training ***
06/01/2019 10:37:05 step: 929, epoch: 28, batch: 4, loss: 0.04419833421707153, acc: 100.0, f1: 100.0, r: 0.7005118895538908
06/01/2019 10:37:06 step: 934, epoch: 28, batch: 9, loss: 0.08844154328107834, acc: 96.875, f1: 83.54166666666667, r: 0.7535624037610356
06/01/2019 10:37:06 step: 939, epoch: 28, batch: 14, loss: 0.14547866582870483, acc: 93.75, f1: 75.74903874593315, r: 0.6831032401088056
06/01/2019 10:37:06 step: 944, epoch: 28, batch: 19, loss: 0.11073387414216995, acc: 95.3125, f1: 96.45487491232171, r: 0.823327536931107
06/01/2019 10:37:06 step: 949, epoch: 28, batch: 24, loss: 0.06010807305574417, acc: 98.4375, f1: 98.15295815295816, r: 0.6850608045378735
06/01/2019 10:37:06 step: 954, epoch: 28, batch: 29, loss: 0.1928659975528717, acc: 92.1875, f1: 79.98945335710042, r: 0.7705895582708349
06/01/2019 10:37:07 *** evaluating ***
06/01/2019 10:37:07 step: 29, epoch: 28, acc: 58.97435897435898, f1: 20.316091954022987, r: 0.28336865011207807
06/01/2019 10:37:07 *** epoch: 30 ***
06/01/2019 10:37:07 *** training ***
06/01/2019 10:37:07 step: 962, epoch: 29, batch: 4, loss: 0.15522323548793793, acc: 92.1875, f1: 92.70292207792208, r: 0.761946000742385
06/01/2019 10:37:07 step: 967, epoch: 29, batch: 9, loss: 0.121769018471241, acc: 93.75, f1: 82.13324412452319, r: 0.6846780683275719
06/01/2019 10:37:07 step: 972, epoch: 29, batch: 14, loss: 0.1573009192943573, acc: 93.75, f1: 82.28998977600149, r: 0.7764810175714078
06/01/2019 10:37:08 step: 977, epoch: 29, batch: 19, loss: 0.08117318898439407, acc: 96.875, f1: 97.74719240093009, r: 0.7102604729994396
06/01/2019 10:37:08 step: 982, epoch: 29, batch: 24, loss: 0.13300003111362457, acc: 92.1875, f1: 75.74436854476578, r: 0.5902378040858904
06/01/2019 10:37:08 step: 987, epoch: 29, batch: 29, loss: 0.05025404691696167, acc: 100.0, f1: 100.0, r: 0.6938787854316438
06/01/2019 10:37:08 *** evaluating ***
06/01/2019 10:37:08 step: 30, epoch: 29, acc: 58.97435897435898, f1: 21.62977219422746, r: 0.2796724770715483
06/01/2019 10:37:08 *** epoch: 31 ***
06/01/2019 10:37:08 *** training ***
06/01/2019 10:37:08 step: 995, epoch: 30, batch: 4, loss: 0.12701188027858734, acc: 95.3125, f1: 97.11392133747722, r: 0.7291812132266897
06/01/2019 10:37:09 step: 1000, epoch: 30, batch: 9, loss: 0.16087079048156738, acc: 93.75, f1: 90.76946419448636, r: 0.6434604381577459
06/01/2019 10:37:09 step: 1005, epoch: 30, batch: 14, loss: 0.06977684795856476, acc: 96.875, f1: 96.56589908690749, r: 0.6622953888148703
06/01/2019 10:37:09 step: 1010, epoch: 30, batch: 19, loss: 0.11269710212945938, acc: 95.3125, f1: 69.24679487179488, r: 0.682321775865352
06/01/2019 10:37:09 step: 1015, epoch: 30, batch: 24, loss: 0.13718709349632263, acc: 96.875, f1: 95.06294471811714, r: 0.6984422154096053
06/01/2019 10:37:10 step: 1020, epoch: 30, batch: 29, loss: 0.06365130841732025, acc: 98.4375, f1: 97.77777777777777, r: 0.7974448970783775
06/01/2019 10:37:10 *** evaluating ***
06/01/2019 10:37:10 step: 31, epoch: 30, acc: 57.692307692307686, f1: 19.882590132827325, r: 0.28583646662598156
06/01/2019 10:37:10 *** epoch: 32 ***
06/01/2019 10:37:10 *** training ***
06/01/2019 10:37:10 step: 1028, epoch: 31, batch: 4, loss: 0.07389801740646362, acc: 98.4375, f1: 99.19073845116331, r: 0.6741635914140867
06/01/2019 10:37:10 step: 1033, epoch: 31, batch: 9, loss: 0.15960508584976196, acc: 93.75, f1: 89.94023190671618, r: 0.7180503698187601
06/01/2019 10:37:11 step: 1038, epoch: 31, batch: 14, loss: 0.04026641324162483, acc: 100.0, f1: 100.0, r: 0.7778277843087265
06/01/2019 10:37:11 step: 1043, epoch: 31, batch: 19, loss: 0.0712294653058052, acc: 100.0, f1: 100.0, r: 0.7187736368481241
06/01/2019 10:37:11 step: 1048, epoch: 31, batch: 24, loss: 0.1276126652956009, acc: 96.875, f1: 97.25396554664847, r: 0.6388265984079752
06/01/2019 10:37:11 step: 1053, epoch: 31, batch: 29, loss: 0.04849185049533844, acc: 98.4375, f1: 97.8675645342312, r: 0.6692038757126018
06/01/2019 10:37:11 *** evaluating ***
06/01/2019 10:37:11 step: 32, epoch: 31, acc: 58.97435897435898, f1: 20.501028449643137, r: 0.2796797182039762
06/01/2019 10:37:11 *** epoch: 33 ***
06/01/2019 10:37:11 *** training ***
06/01/2019 10:37:12 step: 1061, epoch: 32, batch: 4, loss: 0.12567001581192017, acc: 95.3125, f1: 94.12250873120438, r: 0.7064266144947465
06/01/2019 10:37:12 step: 1066, epoch: 32, batch: 9, loss: 0.04570379853248596, acc: 100.0, f1: 100.0, r: 0.6787937470627377
06/01/2019 10:37:12 step: 1071, epoch: 32, batch: 14, loss: 0.13664942979812622, acc: 95.3125, f1: 82.5495337995338, r: 0.762755640099277
06/01/2019 10:37:12 step: 1076, epoch: 32, batch: 19, loss: 0.033867478370666504, acc: 100.0, f1: 100.0, r: 0.7558856829701158
06/01/2019 10:37:13 step: 1081, epoch: 32, batch: 24, loss: 0.06364201754331589, acc: 100.0, f1: 100.0, r: 0.7477174580374002
06/01/2019 10:37:13 step: 1086, epoch: 32, batch: 29, loss: 0.06139247491955757, acc: 98.4375, f1: 97.22222222222221, r: 0.7808793523653952
06/01/2019 10:37:13 *** evaluating ***
06/01/2019 10:37:13 step: 33, epoch: 32, acc: 59.401709401709404, f1: 20.648547386876345, r: 0.2691261518344866
06/01/2019 10:37:13 *** epoch: 34 ***
06/01/2019 10:37:13 *** training ***
06/01/2019 10:37:13 step: 1094, epoch: 33, batch: 4, loss: 0.11525717377662659, acc: 93.75, f1: 93.68386243386243, r: 0.7420060647273
06/01/2019 10:37:13 step: 1099, epoch: 33, batch: 9, loss: 0.06748887151479721, acc: 98.4375, f1: 96.19047619047619, r: 0.6731520575679841
06/01/2019 10:37:14 step: 1104, epoch: 33, batch: 14, loss: 0.12031988054513931, acc: 95.3125, f1: 80.93600478468899, r: 0.6715606368859098
06/01/2019 10:37:14 step: 1109, epoch: 33, batch: 19, loss: 0.0817529559135437, acc: 96.875, f1: 92.51700680272108, r: 0.6670706324131976
06/01/2019 10:37:14 step: 1114, epoch: 33, batch: 24, loss: 0.061230484396219254, acc: 96.875, f1: 92.1969696969697, r: 0.72311483090709
06/01/2019 10:37:14 step: 1119, epoch: 33, batch: 29, loss: 0.07293226569890976, acc: 98.4375, f1: 92.06349206349206, r: 0.5973145776252646
06/01/2019 10:37:15 *** evaluating ***
06/01/2019 10:37:15 step: 34, epoch: 33, acc: 59.82905982905983, f1: 21.741461305663638, r: 0.28136945125811297
06/01/2019 10:37:15 *** epoch: 35 ***
06/01/2019 10:37:15 *** training ***
06/01/2019 10:37:15 step: 1127, epoch: 34, batch: 4, loss: 0.3194049894809723, acc: 89.0625, f1: 76.43754471485565, r: 0.6879996679844959
06/01/2019 10:37:15 step: 1132, epoch: 34, batch: 9, loss: 0.06410343945026398, acc: 96.875, f1: 97.64967975494292, r: 0.7170420903410694
06/01/2019 10:37:15 step: 1137, epoch: 34, batch: 14, loss: 0.09389496594667435, acc: 96.875, f1: 84.472049689441, r: 0.6143438152813129
06/01/2019 10:37:16 step: 1142, epoch: 34, batch: 19, loss: 0.08626595884561539, acc: 96.875, f1: 98.05466524216524, r: 0.7719950019006226
06/01/2019 10:37:16 step: 1147, epoch: 34, batch: 24, loss: 0.09749605506658554, acc: 96.875, f1: 94.26415094339622, r: 0.7810117732506103
06/01/2019 10:37:16 step: 1152, epoch: 34, batch: 29, loss: 0.10430864244699478, acc: 96.875, f1: 97.654555005643, r: 0.7096475787892718
06/01/2019 10:37:16 *** evaluating ***
06/01/2019 10:37:16 step: 35, epoch: 34, acc: 59.401709401709404, f1: 20.620801033591736, r: 0.29465963695308556
06/01/2019 10:37:16 *** epoch: 36 ***
06/01/2019 10:37:16 *** training ***
06/01/2019 10:37:16 step: 1160, epoch: 35, batch: 4, loss: 0.08191254734992981, acc: 98.4375, f1: 84.12698412698413, r: 0.5896600043107382
06/01/2019 10:37:17 step: 1165, epoch: 35, batch: 9, loss: 0.11634161323308945, acc: 95.3125, f1: 90.1576858813701, r: 0.7694618309150596
06/01/2019 10:37:17 step: 1170, epoch: 35, batch: 14, loss: 0.071477510035038, acc: 98.4375, f1: 97.26415094339622, r: 0.7480953868978732
06/01/2019 10:37:17 step: 1175, epoch: 35, batch: 19, loss: 0.07737378776073456, acc: 98.4375, f1: 93.65079365079364, r: 0.6714036697587467
06/01/2019 10:37:17 step: 1180, epoch: 35, batch: 24, loss: 0.09084513783454895, acc: 95.3125, f1: 97.18874391862009, r: 0.7952457278021984
06/01/2019 10:37:18 step: 1185, epoch: 35, batch: 29, loss: 0.08047401160001755, acc: 96.875, f1: 85.14814814814815, r: 0.6846410933655014
06/01/2019 10:37:18 *** evaluating ***
06/01/2019 10:37:18 step: 36, epoch: 35, acc: 57.692307692307686, f1: 20.04635885362977, r: 0.2905093464746736
06/01/2019 10:37:18 *** epoch: 37 ***
06/01/2019 10:37:18 *** training ***
06/01/2019 10:37:18 step: 1193, epoch: 36, batch: 4, loss: 0.1268010139465332, acc: 98.4375, f1: 96.36363636363636, r: 0.7901282952510229
06/01/2019 10:37:18 step: 1198, epoch: 36, batch: 9, loss: 0.12551657855510712, acc: 95.3125, f1: 93.89102863367569, r: 0.7107830173555304
06/01/2019 10:37:18 step: 1203, epoch: 36, batch: 14, loss: 0.04929831624031067, acc: 100.0, f1: 100.0, r: 0.7814536177550067
06/01/2019 10:37:19 step: 1208, epoch: 36, batch: 19, loss: 0.062002189457416534, acc: 98.4375, f1: 99.06692406692407, r: 0.8079201376750734
06/01/2019 10:37:19 step: 1213, epoch: 36, batch: 24, loss: 0.15162557363510132, acc: 92.1875, f1: 80.05639097744361, r: 0.7609426209194192
06/01/2019 10:37:19 step: 1218, epoch: 36, batch: 29, loss: 0.10547256469726562, acc: 96.875, f1: 91.93101225016119, r: 0.8180098308279197
06/01/2019 10:37:19 *** evaluating ***
06/01/2019 10:37:19 step: 37, epoch: 36, acc: 58.97435897435898, f1: 20.436325571895424, r: 0.29407924240177513
06/01/2019 10:37:19 *** epoch: 38 ***
06/01/2019 10:37:19 *** training ***
06/01/2019 10:37:20 step: 1226, epoch: 37, batch: 4, loss: 0.12312561273574829, acc: 93.75, f1: 87.42887091943696, r: 0.638797443005344
06/01/2019 10:37:20 step: 1231, epoch: 37, batch: 9, loss: 0.044696904718875885, acc: 98.4375, f1: 95.0, r: 0.7694310408580893
06/01/2019 10:37:20 step: 1236, epoch: 37, batch: 14, loss: 0.07940967381000519, acc: 96.875, f1: 93.11059907834101, r: 0.6430423029592631
06/01/2019 10:37:20 step: 1241, epoch: 37, batch: 19, loss: 0.08415225893259048, acc: 96.875, f1: 92.83333333333333, r: 0.7176885365749314
06/01/2019 10:37:20 step: 1246, epoch: 37, batch: 24, loss: 0.1282123178243637, acc: 95.3125, f1: 89.06442577030812, r: 0.6357213253208952
06/01/2019 10:37:21 step: 1251, epoch: 37, batch: 29, loss: 0.019808873534202576, acc: 100.0, f1: 100.0, r: 0.7042948576992221
06/01/2019 10:37:21 *** evaluating ***
06/01/2019 10:37:21 step: 38, epoch: 37, acc: 59.401709401709404, f1: 20.25100112626705, r: 0.2928013157942657
06/01/2019 10:37:21 *** epoch: 39 ***
06/01/2019 10:37:21 *** training ***
06/01/2019 10:37:21 step: 1259, epoch: 38, batch: 4, loss: 0.09783970564603806, acc: 95.3125, f1: 96.27525252525253, r: 0.6436859461746604
06/01/2019 10:37:21 step: 1264, epoch: 38, batch: 9, loss: 0.05040007084608078, acc: 98.4375, f1: 98.93887945670629, r: 0.7838824717180904
06/01/2019 10:37:22 step: 1269, epoch: 38, batch: 14, loss: 0.028443660587072372, acc: 100.0, f1: 100.0, r: 0.7106621639003265
06/01/2019 10:37:22 step: 1274, epoch: 38, batch: 19, loss: 0.07445695996284485, acc: 96.875, f1: 97.1640749601276, r: 0.7799099152278167
06/01/2019 10:37:22 step: 1279, epoch: 38, batch: 24, loss: 0.053629860281944275, acc: 98.4375, f1: 84.9624060150376, r: 0.5690356075431662
06/01/2019 10:37:22 step: 1284, epoch: 38, batch: 29, loss: 0.07851109653711319, acc: 96.875, f1: 94.29282824240806, r: 0.6664056733322661
06/01/2019 10:37:22 *** evaluating ***
06/01/2019 10:37:22 step: 39, epoch: 38, acc: 58.97435897435898, f1: 21.14151246879584, r: 0.2844113241588646
06/01/2019 10:37:22 *** epoch: 40 ***
06/01/2019 10:37:22 *** training ***
06/01/2019 10:37:23 step: 1292, epoch: 39, batch: 4, loss: 0.16026277840137482, acc: 92.1875, f1: 82.07539428815005, r: 0.6814591611668434
06/01/2019 10:37:23 step: 1297, epoch: 39, batch: 9, loss: 0.12927567958831787, acc: 93.75, f1: 93.35525075414782, r: 0.7657688542101796
06/01/2019 10:37:23 step: 1302, epoch: 39, batch: 14, loss: 0.06479063630104065, acc: 96.875, f1: 94.50216450216449, r: 0.7121103958837602
06/01/2019 10:37:23 step: 1307, epoch: 39, batch: 19, loss: 0.02634505182504654, acc: 100.0, f1: 100.0, r: 0.7041607760508464
06/01/2019 10:37:24 step: 1312, epoch: 39, batch: 24, loss: 0.19861489534378052, acc: 93.75, f1: 89.67490842490842, r: 0.6471413635773385
06/01/2019 10:37:24 step: 1317, epoch: 39, batch: 29, loss: 0.09851778298616409, acc: 96.875, f1: 96.91045796308954, r: 0.7269228085634175
06/01/2019 10:37:24 *** evaluating ***
06/01/2019 10:37:24 step: 40, epoch: 39, acc: 58.97435897435898, f1: 20.788583372280208, r: 0.2921915262447721
06/01/2019 10:37:24 *** epoch: 41 ***
06/01/2019 10:37:24 *** training ***
06/01/2019 10:37:24 step: 1325, epoch: 40, batch: 4, loss: 0.06483346223831177, acc: 96.875, f1: 92.34940352952776, r: 0.6567998248143287
06/01/2019 10:37:24 step: 1330, epoch: 40, batch: 9, loss: 0.08494077622890472, acc: 98.4375, f1: 99.10934020860189, r: 0.6919080781075235
06/01/2019 10:37:25 step: 1335, epoch: 40, batch: 14, loss: 0.03446316346526146, acc: 100.0, f1: 100.0, r: 0.688462702195404
06/01/2019 10:37:25 step: 1340, epoch: 40, batch: 19, loss: 0.051729798316955566, acc: 98.4375, f1: 98.06763285024154, r: 0.7925021811332384
06/01/2019 10:37:25 step: 1345, epoch: 40, batch: 24, loss: 0.13199147582054138, acc: 96.875, f1: 97.86884083658278, r: 0.6697970234311525
06/01/2019 10:37:25 step: 1350, epoch: 40, batch: 29, loss: 0.10838546603918076, acc: 95.3125, f1: 86.39712488769092, r: 0.7398962120724675
06/01/2019 10:37:25 *** evaluating ***
06/01/2019 10:37:26 step: 41, epoch: 40, acc: 58.54700854700855, f1: 20.580312722103763, r: 0.27508780970721036
06/01/2019 10:37:26 *** epoch: 42 ***
06/01/2019 10:37:26 *** training ***
06/01/2019 10:37:26 step: 1358, epoch: 41, batch: 4, loss: 0.03529038280248642, acc: 100.0, f1: 100.0, r: 0.7232181161164409
06/01/2019 10:37:26 step: 1363, epoch: 41, batch: 9, loss: 0.03434610366821289, acc: 100.0, f1: 100.0, r: 0.6750355990560841
06/01/2019 10:37:26 step: 1368, epoch: 41, batch: 14, loss: 0.19822603464126587, acc: 93.75, f1: 87.8145032051282, r: 0.7036632574339757
06/01/2019 10:37:26 step: 1373, epoch: 41, batch: 19, loss: 0.04267898201942444, acc: 100.0, f1: 100.0, r: 0.7074869144059471
06/01/2019 10:37:27 step: 1378, epoch: 41, batch: 24, loss: 0.09053356200456619, acc: 95.3125, f1: 95.62636437636438, r: 0.778693930873346
06/01/2019 10:37:27 step: 1383, epoch: 41, batch: 29, loss: 0.028850562870502472, acc: 100.0, f1: 100.0, r: 0.740229790958704
06/01/2019 10:37:27 *** evaluating ***
06/01/2019 10:37:27 step: 42, epoch: 41, acc: 58.54700854700855, f1: 20.420172665369652, r: 0.2769057518551839
06/01/2019 10:37:27 *** epoch: 43 ***
06/01/2019 10:37:27 *** training ***
06/01/2019 10:37:27 step: 1391, epoch: 42, batch: 4, loss: 0.07618934661149979, acc: 98.4375, f1: 91.66666666666666, r: 0.7075285304115427
06/01/2019 10:37:28 step: 1396, epoch: 42, batch: 9, loss: 0.025375887751579285, acc: 100.0, f1: 100.0, r: 0.673451072479925
06/01/2019 10:37:28 step: 1401, epoch: 42, batch: 14, loss: 0.0487278550863266, acc: 98.4375, f1: 98.20868786386028, r: 0.7077850601485619
06/01/2019 10:37:28 step: 1406, epoch: 42, batch: 19, loss: 0.09442278742790222, acc: 95.3125, f1: 83.39120370370371, r: 0.7960538307548294
06/01/2019 10:37:28 step: 1411, epoch: 42, batch: 24, loss: 0.06144468113780022, acc: 98.4375, f1: 94.55782312925169, r: 0.7615390700112044
06/01/2019 10:37:28 step: 1416, epoch: 42, batch: 29, loss: 0.057411979883909225, acc: 100.0, f1: 100.0, r: 0.7663962592355525
06/01/2019 10:37:28 *** evaluating ***
06/01/2019 10:37:29 step: 43, epoch: 42, acc: 60.256410256410255, f1: 21.907278165503485, r: 0.29252372531295256
06/01/2019 10:37:29 *** epoch: 44 ***
06/01/2019 10:37:29 *** training ***
06/01/2019 10:37:29 step: 1424, epoch: 43, batch: 4, loss: 0.09180416166782379, acc: 95.3125, f1: 97.15286393713814, r: 0.683893693480223
06/01/2019 10:37:29 step: 1429, epoch: 43, batch: 9, loss: 0.04905291646718979, acc: 100.0, f1: 100.0, r: 0.7238842985030575
06/01/2019 10:37:29 step: 1434, epoch: 43, batch: 14, loss: 0.054545558989048004, acc: 98.4375, f1: 97.46657283603096, r: 0.6653831120497318
06/01/2019 10:37:29 step: 1439, epoch: 43, batch: 19, loss: 0.12589119374752045, acc: 93.75, f1: 89.21410669530971, r: 0.642185045013768
06/01/2019 10:37:30 step: 1444, epoch: 43, batch: 24, loss: 0.045072562992572784, acc: 98.4375, f1: 75.0, r: 0.6375813040975068
06/01/2019 10:37:30 step: 1449, epoch: 43, batch: 29, loss: 0.061231888830661774, acc: 98.4375, f1: 85.0, r: 0.8263828034611957
06/01/2019 10:37:30 *** evaluating ***
06/01/2019 10:37:30 step: 44, epoch: 43, acc: 59.82905982905983, f1: 20.88803877622925, r: 0.2872535260023002
06/01/2019 10:37:30 *** epoch: 45 ***
06/01/2019 10:37:30 *** training ***
06/01/2019 10:37:30 step: 1457, epoch: 44, batch: 4, loss: 0.08232483267784119, acc: 95.3125, f1: 90.30969634230503, r: 0.7412517946255809
06/01/2019 10:37:31 step: 1462, epoch: 44, batch: 9, loss: 0.020599298179149628, acc: 100.0, f1: 100.0, r: 0.7862435176941538
06/01/2019 10:37:31 step: 1467, epoch: 44, batch: 14, loss: 0.04504019767045975, acc: 100.0, f1: 100.0, r: 0.680661671320446
06/01/2019 10:37:31 step: 1472, epoch: 44, batch: 19, loss: 0.08183535188436508, acc: 96.875, f1: 96.19886747802569, r: 0.6810396412802426
06/01/2019 10:37:31 step: 1477, epoch: 44, batch: 24, loss: 0.05333443731069565, acc: 96.875, f1: 93.3763376254176, r: 0.5912627580879827
06/01/2019 10:37:31 step: 1482, epoch: 44, batch: 29, loss: 0.1435411423444748, acc: 96.875, f1: 94.24465733235077, r: 0.7506068337699836
06/01/2019 10:37:32 *** evaluating ***
06/01/2019 10:37:32 step: 45, epoch: 44, acc: 60.256410256410255, f1: 21.617433096094754, r: 0.2872123766074957
06/01/2019 10:37:32 *** epoch: 46 ***
06/01/2019 10:37:32 *** training ***
06/01/2019 10:37:32 step: 1490, epoch: 45, batch: 4, loss: 0.030061878263950348, acc: 96.875, f1: 97.19355578488705, r: 0.7763035961888174
06/01/2019 10:37:32 step: 1495, epoch: 45, batch: 9, loss: 0.038721583783626556, acc: 100.0, f1: 100.0, r: 0.7726720946426742
06/01/2019 10:37:32 step: 1500, epoch: 45, batch: 14, loss: 0.09522907435894012, acc: 96.875, f1: 93.937575030012, r: 0.6837464116528327
06/01/2019 10:37:33 step: 1505, epoch: 45, batch: 19, loss: 0.05738312005996704, acc: 96.875, f1: 98.29931972789116, r: 0.7187140198467432
06/01/2019 10:37:33 step: 1510, epoch: 45, batch: 24, loss: 0.10270746797323227, acc: 96.875, f1: 97.11188540456833, r: 0.6968224523821531
06/01/2019 10:37:33 step: 1515, epoch: 45, batch: 29, loss: 0.06526913493871689, acc: 98.4375, f1: 98.3201581027668, r: 0.7835429106397249
06/01/2019 10:37:33 *** evaluating ***
06/01/2019 10:37:33 step: 46, epoch: 45, acc: 59.401709401709404, f1: 22.458521870286575, r: 0.27742671698473154
06/01/2019 10:37:33 *** epoch: 47 ***
06/01/2019 10:37:33 *** training ***
06/01/2019 10:37:33 step: 1523, epoch: 46, batch: 4, loss: 0.10527133196592331, acc: 95.3125, f1: 89.88749345892202, r: 0.7146425265268916
06/01/2019 10:37:34 step: 1528, epoch: 46, batch: 9, loss: 0.11487323045730591, acc: 95.3125, f1: 90.87953053470295, r: 0.627897244477478
06/01/2019 10:37:34 step: 1533, epoch: 46, batch: 14, loss: 0.10582321882247925, acc: 95.3125, f1: 95.23047257180383, r: 0.7070807040194176
06/01/2019 10:37:34 step: 1538, epoch: 46, batch: 19, loss: 0.0825842097401619, acc: 96.875, f1: 96.52589972956106, r: 0.7870405399844798
06/01/2019 10:37:34 step: 1543, epoch: 46, batch: 24, loss: 0.06510169804096222, acc: 98.4375, f1: 99.02818270165209, r: 0.7214989528983714
06/01/2019 10:37:35 step: 1548, epoch: 46, batch: 29, loss: 0.0657029002904892, acc: 98.4375, f1: 98.90070921985816, r: 0.746467791737842
06/01/2019 10:37:35 *** evaluating ***
06/01/2019 10:37:35 step: 47, epoch: 46, acc: 58.97435897435898, f1: 21.103503695102628, r: 0.28136312050821444
06/01/2019 10:37:35 *** epoch: 48 ***
06/01/2019 10:37:35 *** training ***
06/01/2019 10:37:35 step: 1556, epoch: 47, batch: 4, loss: 0.1051424890756607, acc: 93.75, f1: 92.96162171162172, r: 0.7901220811573932
06/01/2019 10:37:35 step: 1561, epoch: 47, batch: 9, loss: 0.05562516674399376, acc: 98.4375, f1: 98.27998088867655, r: 0.7158590419146187
06/01/2019 10:37:35 step: 1566, epoch: 47, batch: 14, loss: 0.09614662826061249, acc: 95.3125, f1: 91.03532884983564, r: 0.592948924110776
06/01/2019 10:37:36 step: 1571, epoch: 47, batch: 19, loss: 0.05151960253715515, acc: 98.4375, f1: 98.44322344322345, r: 0.7575037291113234
06/01/2019 10:37:36 step: 1576, epoch: 47, batch: 24, loss: 0.038616180419921875, acc: 98.4375, f1: 97.667638483965, r: 0.6720288554640379
06/01/2019 10:37:36 step: 1581, epoch: 47, batch: 29, loss: 0.0527133084833622, acc: 98.4375, f1: 87.1951219512195, r: 0.7392674950080533
06/01/2019 10:37:36 *** evaluating ***
06/01/2019 10:37:36 step: 48, epoch: 47, acc: 60.256410256410255, f1: 22.774187351547578, r: 0.28930309824365025
06/01/2019 10:37:36 *** epoch: 49 ***
06/01/2019 10:37:36 *** training ***
06/01/2019 10:37:37 step: 1589, epoch: 48, batch: 4, loss: 0.0676293820142746, acc: 100.0, f1: 100.0, r: 0.7085055130546544
06/01/2019 10:37:37 step: 1594, epoch: 48, batch: 9, loss: 0.073798269033432, acc: 95.3125, f1: 94.68700523687737, r: 0.7022923848728001
06/01/2019 10:37:37 step: 1599, epoch: 48, batch: 14, loss: 0.02871938794851303, acc: 100.0, f1: 100.0, r: 0.7985257926077132
06/01/2019 10:37:37 step: 1604, epoch: 48, batch: 19, loss: 0.10795049369335175, acc: 95.3125, f1: 95.30509173366316, r: 0.6501734653787045
06/01/2019 10:37:37 step: 1609, epoch: 48, batch: 24, loss: 0.0716894119977951, acc: 96.875, f1: 96.69267930137497, r: 0.711065349738335
06/01/2019 10:37:38 step: 1614, epoch: 48, batch: 29, loss: 0.11581220477819443, acc: 98.4375, f1: 95.52845528455285, r: 0.7105798517167814
06/01/2019 10:37:38 *** evaluating ***
06/01/2019 10:37:38 step: 49, epoch: 48, acc: 57.26495726495726, f1: 19.772678111419186, r: 0.2798224842619384
06/01/2019 10:37:38 *** epoch: 50 ***
06/01/2019 10:37:38 *** training ***
06/01/2019 10:37:38 step: 1622, epoch: 49, batch: 4, loss: 0.054349444806575775, acc: 98.4375, f1: 98.35286664554957, r: 0.7129285169361892
06/01/2019 10:37:38 step: 1627, epoch: 49, batch: 9, loss: 0.07898268848657608, acc: 98.4375, f1: 98.55266684534976, r: 0.673199101955137
06/01/2019 10:37:39 step: 1632, epoch: 49, batch: 14, loss: 0.02960464358329773, acc: 98.4375, f1: 98.72122762148338, r: 0.8146111653612707
06/01/2019 10:37:39 step: 1637, epoch: 49, batch: 19, loss: 0.04633677005767822, acc: 98.4375, f1: 99.24759924759925, r: 0.798678419683829
06/01/2019 10:37:39 step: 1642, epoch: 49, batch: 24, loss: 0.029828418046236038, acc: 98.4375, f1: 97.94871794871796, r: 0.7415883228819657
06/01/2019 10:37:39 step: 1647, epoch: 49, batch: 29, loss: 0.07900940626859665, acc: 96.875, f1: 96.73350041771094, r: 0.6928380615419045
06/01/2019 10:37:39 *** evaluating ***
06/01/2019 10:37:39 step: 50, epoch: 49, acc: 59.82905982905983, f1: 22.608893199976787, r: 0.2840287410943723
06/01/2019 10:37:39 *** epoch: 51 ***
06/01/2019 10:37:39 *** training ***
06/01/2019 10:37:40 step: 1655, epoch: 50, batch: 4, loss: 0.04363180696964264, acc: 98.4375, f1: 99.25925925925925, r: 0.7157468668752142
06/01/2019 10:37:40 step: 1660, epoch: 50, batch: 9, loss: 0.04989522323012352, acc: 98.4375, f1: 87.2340425531915, r: 0.7832392207657121
06/01/2019 10:37:40 step: 1665, epoch: 50, batch: 14, loss: 0.03608968108892441, acc: 98.4375, f1: 99.10625620655412, r: 0.743689493707108
06/01/2019 10:37:40 step: 1670, epoch: 50, batch: 19, loss: 0.07957150042057037, acc: 96.875, f1: 96.8634306869601, r: 0.6154822284482118
06/01/2019 10:37:41 step: 1675, epoch: 50, batch: 24, loss: 0.061113081872463226, acc: 95.3125, f1: 83.33994708994709, r: 0.5435912353913009
06/01/2019 10:37:41 step: 1680, epoch: 50, batch: 29, loss: 0.043487533926963806, acc: 98.4375, f1: 99.31386347642506, r: 0.7957371499264401
06/01/2019 10:37:41 *** evaluating ***
06/01/2019 10:37:41 step: 51, epoch: 50, acc: 58.119658119658126, f1: 20.258797096728394, r: 0.2799563444593758
06/01/2019 10:37:41 *** epoch: 52 ***
06/01/2019 10:37:41 *** training ***
06/01/2019 10:37:41 step: 1688, epoch: 51, batch: 4, loss: 0.11635761708021164, acc: 96.875, f1: 95.48661215327883, r: 0.6037961207723587
06/01/2019 10:37:42 step: 1693, epoch: 51, batch: 9, loss: 0.09779081493616104, acc: 95.3125, f1: 94.27952999381571, r: 0.7276962991606205
06/01/2019 10:37:42 step: 1698, epoch: 51, batch: 14, loss: 0.029278669506311417, acc: 100.0, f1: 100.0, r: 0.6975236015753262
06/01/2019 10:37:42 step: 1703, epoch: 51, batch: 19, loss: 0.09730926901102066, acc: 95.3125, f1: 96.2679516250945, r: 0.6802325579868583
06/01/2019 10:37:42 step: 1708, epoch: 51, batch: 24, loss: 0.03243574872612953, acc: 100.0, f1: 100.0, r: 0.7624726100774186
06/01/2019 10:37:42 step: 1713, epoch: 51, batch: 29, loss: 0.1045663133263588, acc: 95.3125, f1: 90.54636748316346, r: 0.7807694884149805
06/01/2019 10:37:43 *** evaluating ***
06/01/2019 10:37:43 step: 52, epoch: 51, acc: 58.54700854700855, f1: 19.98546685772617, r: 0.27320170105866537
06/01/2019 10:37:43 *** epoch: 53 ***
06/01/2019 10:37:43 *** training ***
06/01/2019 10:37:43 step: 1721, epoch: 52, batch: 4, loss: 0.03422015532851219, acc: 100.0, f1: 100.0, r: 0.6737979619094323
06/01/2019 10:37:43 step: 1726, epoch: 52, batch: 9, loss: 0.03394021466374397, acc: 98.4375, f1: 97.57236227824463, r: 0.7383527926572501
06/01/2019 10:37:43 step: 1731, epoch: 52, batch: 14, loss: 0.07546887546777725, acc: 100.0, f1: 100.0, r: 0.8390154681578852
06/01/2019 10:37:44 step: 1736, epoch: 52, batch: 19, loss: 0.025438278913497925, acc: 98.4375, f1: 98.58585858585859, r: 0.8364466612068813
06/01/2019 10:37:44 step: 1741, epoch: 52, batch: 24, loss: 0.009830676019191742, acc: 100.0, f1: 100.0, r: 0.7822684547283165
06/01/2019 10:37:44 step: 1746, epoch: 52, batch: 29, loss: 0.07198911905288696, acc: 98.4375, f1: 95.0, r: 0.7021436503033892
06/01/2019 10:37:44 *** evaluating ***
06/01/2019 10:37:44 step: 53, epoch: 52, acc: 59.401709401709404, f1: 21.595953525641022, r: 0.27739564650622983
06/01/2019 10:37:44 *** epoch: 54 ***
06/01/2019 10:37:44 *** training ***
06/01/2019 10:37:44 step: 1754, epoch: 53, batch: 4, loss: 0.052588410675525665, acc: 98.4375, f1: 95.10204081632654, r: 0.7082652223394879
06/01/2019 10:37:45 step: 1759, epoch: 53, batch: 9, loss: 0.0628170520067215, acc: 98.4375, f1: 98.53846153846153, r: 0.7174396629727148
06/01/2019 10:37:45 step: 1764, epoch: 53, batch: 14, loss: 0.06967610120773315, acc: 98.4375, f1: 96.9047619047619, r: 0.7618095159120032
06/01/2019 10:37:45 step: 1769, epoch: 53, batch: 19, loss: 0.043710604310035706, acc: 98.4375, f1: 98.66666666666667, r: 0.7899472271219691
06/01/2019 10:37:45 step: 1774, epoch: 53, batch: 24, loss: 0.039575062692165375, acc: 98.4375, f1: 97.16216216216216, r: 0.8189130967373395
06/01/2019 10:37:46 step: 1779, epoch: 53, batch: 29, loss: 0.03307093307375908, acc: 98.4375, f1: 97.61904761904762, r: 0.7770431994318551
06/01/2019 10:37:46 *** evaluating ***
06/01/2019 10:37:46 step: 54, epoch: 53, acc: 58.97435897435898, f1: 21.627719480488995, r: 0.2860988043143672
06/01/2019 10:37:46 *** epoch: 55 ***
06/01/2019 10:37:46 *** training ***
06/01/2019 10:37:46 step: 1787, epoch: 54, batch: 4, loss: 0.05902419239282608, acc: 98.4375, f1: 98.47939175670268, r: 0.7013424600112649
06/01/2019 10:37:46 step: 1792, epoch: 54, batch: 9, loss: 0.054838452488183975, acc: 98.4375, f1: 97.64957264957265, r: 0.7444771676531794
06/01/2019 10:37:46 step: 1797, epoch: 54, batch: 14, loss: 0.039576560258865356, acc: 98.4375, f1: 97.73242630385488, r: 0.7003487587783542
06/01/2019 10:37:47 step: 1802, epoch: 54, batch: 19, loss: 0.009965822100639343, acc: 100.0, f1: 100.0, r: 0.7164171498681895
06/01/2019 10:37:47 step: 1807, epoch: 54, batch: 24, loss: 0.012137815356254578, acc: 100.0, f1: 100.0, r: 0.651724981476301
06/01/2019 10:37:47 step: 1812, epoch: 54, batch: 29, loss: 0.061209022998809814, acc: 98.4375, f1: 96.6137566137566, r: 0.6695869696801311
06/01/2019 10:37:47 *** evaluating ***
06/01/2019 10:37:47 step: 55, epoch: 54, acc: 59.401709401709404, f1: 22.855841790268016, r: 0.28255950058394025
06/01/2019 10:37:47 *** epoch: 56 ***
06/01/2019 10:37:47 *** training ***
06/01/2019 10:37:48 step: 1820, epoch: 55, batch: 4, loss: 0.0224643275141716, acc: 100.0, f1: 100.0, r: 0.6705221435573778
06/01/2019 10:37:48 step: 1825, epoch: 55, batch: 9, loss: 0.03408227115869522, acc: 98.4375, f1: 98.10874704491725, r: 0.6881499788327493
06/01/2019 10:37:48 step: 1830, epoch: 55, batch: 14, loss: 0.02478611469268799, acc: 100.0, f1: 100.0, r: 0.7870013654520975
06/01/2019 10:37:48 step: 1835, epoch: 55, batch: 19, loss: 0.012310370802879333, acc: 100.0, f1: 100.0, r: 0.7155399724170803
06/01/2019 10:37:48 step: 1840, epoch: 55, batch: 24, loss: 0.02483931928873062, acc: 100.0, f1: 100.0, r: 0.7725086977279381
06/01/2019 10:37:49 step: 1845, epoch: 55, batch: 29, loss: 0.032579511404037476, acc: 98.4375, f1: 99.18546365914787, r: 0.8604983320398946
06/01/2019 10:37:49 *** evaluating ***
06/01/2019 10:37:49 step: 56, epoch: 55, acc: 58.97435897435898, f1: 19.540689410092394, r: 0.2874826118365928
06/01/2019 10:37:49 *** epoch: 57 ***
06/01/2019 10:37:49 *** training ***
06/01/2019 10:37:49 step: 1853, epoch: 56, batch: 4, loss: 0.0538601353764534, acc: 100.0, f1: 100.0, r: 0.7595056404482289
06/01/2019 10:37:49 step: 1858, epoch: 56, batch: 9, loss: 0.0934290811419487, acc: 95.3125, f1: 94.15374677002585, r: 0.750616351364261
06/01/2019 10:37:50 step: 1863, epoch: 56, batch: 14, loss: 0.0997479259967804, acc: 96.875, f1: 97.09025470653377, r: 0.7640884181707093
06/01/2019 10:37:50 step: 1868, epoch: 56, batch: 19, loss: 0.08382099866867065, acc: 96.875, f1: 96.39961973900269, r: 0.7060601289226337
06/01/2019 10:37:50 step: 1873, epoch: 56, batch: 24, loss: 0.0716327428817749, acc: 100.0, f1: 100.0, r: 0.8008953370454734
06/01/2019 10:37:50 step: 1878, epoch: 56, batch: 29, loss: 0.04982038587331772, acc: 98.4375, f1: 97.65523230568823, r: 0.7036491181738993
06/01/2019 10:37:50 *** evaluating ***
06/01/2019 10:37:50 step: 57, epoch: 56, acc: 60.68376068376068, f1: 22.532539633582136, r: 0.28878109264920765
06/01/2019 10:37:50 *** epoch: 58 ***
06/01/2019 10:37:50 *** training ***
06/01/2019 10:37:51 step: 1886, epoch: 57, batch: 4, loss: 0.04993600398302078, acc: 98.4375, f1: 97.92008757526, r: 0.656830915696609
06/01/2019 10:37:51 step: 1891, epoch: 57, batch: 9, loss: 0.032309502363204956, acc: 100.0, f1: 100.0, r: 0.8105005693714445
06/01/2019 10:37:51 step: 1896, epoch: 57, batch: 14, loss: 0.03637172654271126, acc: 98.4375, f1: 99.08733679807327, r: 0.6525742555219324
06/01/2019 10:37:51 step: 1901, epoch: 57, batch: 19, loss: 0.13716642558574677, acc: 93.75, f1: 90.99038235984048, r: 0.6646404262168851
06/01/2019 10:37:52 step: 1906, epoch: 57, batch: 24, loss: 0.04126739874482155, acc: 98.4375, f1: 94.93414387031407, r: 0.6181080393094814
06/01/2019 10:37:52 step: 1911, epoch: 57, batch: 29, loss: 0.0559476837515831, acc: 98.4375, f1: 95.55555555555556, r: 0.6339992581629422
06/01/2019 10:37:52 *** evaluating ***
06/01/2019 10:37:52 step: 58, epoch: 57, acc: 58.54700854700855, f1: 19.715871638070006, r: 0.28461703324996335
06/01/2019 10:37:52 *** epoch: 59 ***
06/01/2019 10:37:52 *** training ***
06/01/2019 10:37:52 step: 1919, epoch: 58, batch: 4, loss: 0.018021851778030396, acc: 100.0, f1: 100.0, r: 0.6503335465857298
06/01/2019 10:37:52 step: 1924, epoch: 58, batch: 9, loss: 0.030008599162101746, acc: 98.4375, f1: 98.01587301587301, r: 0.7611068714969079
06/01/2019 10:37:53 step: 1929, epoch: 58, batch: 14, loss: 0.06999185681343079, acc: 96.875, f1: 86.84807256235827, r: 0.6169805769352864
06/01/2019 10:37:53 step: 1934, epoch: 58, batch: 19, loss: 0.045765265822410583, acc: 98.4375, f1: 96.53846153846153, r: 0.7562756858555674
06/01/2019 10:37:53 step: 1939, epoch: 58, batch: 24, loss: 0.026898503303527832, acc: 100.0, f1: 100.0, r: 0.7902464400853653
06/01/2019 10:37:53 step: 1944, epoch: 58, batch: 29, loss: 0.02889450639486313, acc: 100.0, f1: 100.0, r: 0.6768820772558058
06/01/2019 10:37:53 *** evaluating ***
06/01/2019 10:37:54 step: 59, epoch: 58, acc: 58.54700854700855, f1: 19.464041814454372, r: 0.2772671172902744
06/01/2019 10:37:54 *** epoch: 60 ***
06/01/2019 10:37:54 *** training ***
06/01/2019 10:37:54 step: 1952, epoch: 59, batch: 4, loss: 0.07311201095581055, acc: 96.875, f1: 96.17610837438424, r: 0.7374440421394693
06/01/2019 10:37:54 step: 1957, epoch: 59, batch: 9, loss: 0.06891581416130066, acc: 96.875, f1: 97.26572447160683, r: 0.6940510371168009
06/01/2019 10:37:54 step: 1962, epoch: 59, batch: 14, loss: 0.06010439991950989, acc: 98.4375, f1: 97.81105990783409, r: 0.7391469146358423
06/01/2019 10:37:54 step: 1967, epoch: 59, batch: 19, loss: 0.07000002264976501, acc: 96.875, f1: 85.28325123152709, r: 0.7590018748801424
06/01/2019 10:37:55 step: 1972, epoch: 59, batch: 24, loss: 0.040836021304130554, acc: 98.4375, f1: 98.75776397515527, r: 0.7374298017966426
06/01/2019 10:37:55 step: 1977, epoch: 59, batch: 29, loss: 0.09606772661209106, acc: 96.875, f1: 96.91764029555485, r: 0.6628321640658246
06/01/2019 10:37:55 *** evaluating ***
06/01/2019 10:37:55 step: 60, epoch: 59, acc: 60.256410256410255, f1: 22.48110995977162, r: 0.28324680861327706
06/01/2019 10:37:55 *** epoch: 61 ***
06/01/2019 10:37:55 *** training ***
06/01/2019 10:37:55 step: 1985, epoch: 60, batch: 4, loss: 0.059578996151685715, acc: 98.4375, f1: 85.71428571428572, r: 0.8424588976676037
06/01/2019 10:37:56 step: 1990, epoch: 60, batch: 9, loss: 0.03624337166547775, acc: 100.0, f1: 100.0, r: 0.8203424527360903
06/01/2019 10:37:56 step: 1995, epoch: 60, batch: 14, loss: 0.04849260672926903, acc: 100.0, f1: 100.0, r: 0.8168667474093863
06/01/2019 10:37:56 step: 2000, epoch: 60, batch: 19, loss: 0.07259926199913025, acc: 96.875, f1: 92.32804232804233, r: 0.6904599269566613
06/01/2019 10:37:56 step: 2005, epoch: 60, batch: 24, loss: 0.04339809715747833, acc: 98.4375, f1: 98.45067213488267, r: 0.7145650005228464
06/01/2019 10:37:57 step: 2010, epoch: 60, batch: 29, loss: 0.08634583652019501, acc: 96.875, f1: 96.58564814814815, r: 0.7756687255205486
06/01/2019 10:37:57 *** evaluating ***
06/01/2019 10:37:57 step: 61, epoch: 60, acc: 59.401709401709404, f1: 21.759507886801625, r: 0.2798666068062418
06/01/2019 10:37:57 *** epoch: 62 ***
06/01/2019 10:37:57 *** training ***
06/01/2019 10:37:57 step: 2018, epoch: 61, batch: 4, loss: 0.027747228741645813, acc: 100.0, f1: 100.0, r: 0.7158137040793452
06/01/2019 10:37:57 step: 2023, epoch: 61, batch: 9, loss: 0.08157353848218918, acc: 96.875, f1: 97.77409372236959, r: 0.7821421391171461
06/01/2019 10:37:58 step: 2028, epoch: 61, batch: 14, loss: 0.050824012607336044, acc: 96.875, f1: 93.91764237949636, r: 0.8133991191460138
06/01/2019 10:37:58 step: 2033, epoch: 61, batch: 19, loss: 0.072401262819767, acc: 98.4375, f1: 95.76719576719577, r: 0.6255817716639117
06/01/2019 10:37:58 step: 2038, epoch: 61, batch: 24, loss: 0.040544405579566956, acc: 100.0, f1: 100.0, r: 0.7035356885262549
06/01/2019 10:37:58 step: 2043, epoch: 61, batch: 29, loss: 0.015432655811309814, acc: 100.0, f1: 100.0, r: 0.832550901215555
06/01/2019 10:37:58 *** evaluating ***
06/01/2019 10:37:58 step: 62, epoch: 61, acc: 59.401709401709404, f1: 22.33633408946054, r: 0.2754646662076194
06/01/2019 10:37:58 *** epoch: 63 ***
06/01/2019 10:37:58 *** training ***
06/01/2019 10:37:59 step: 2051, epoch: 62, batch: 4, loss: 0.014230698347091675, acc: 100.0, f1: 100.0, r: 0.718782310302561
06/01/2019 10:37:59 step: 2056, epoch: 62, batch: 9, loss: 0.063553087413311, acc: 98.4375, f1: 96.36363636363636, r: 0.7618824887001832
06/01/2019 10:37:59 step: 2061, epoch: 62, batch: 14, loss: 0.04051269590854645, acc: 98.4375, f1: 99.11914172783739, r: 0.6731098616167484
06/01/2019 10:37:59 step: 2066, epoch: 62, batch: 19, loss: 0.030528336763381958, acc: 100.0, f1: 100.0, r: 0.6812982725995924
06/01/2019 10:38:00 step: 2071, epoch: 62, batch: 24, loss: 0.04387332871556282, acc: 98.4375, f1: 96.82539682539682, r: 0.7689616810263271
06/01/2019 10:38:00 step: 2076, epoch: 62, batch: 29, loss: 0.0558028444647789, acc: 96.875, f1: 81.29251700680273, r: 0.619774498064689
06/01/2019 10:38:00 *** evaluating ***
06/01/2019 10:38:00 step: 63, epoch: 62, acc: 58.97435897435898, f1: 19.575976457998927, r: 0.2855420441680568
06/01/2019 10:38:00 *** epoch: 64 ***
06/01/2019 10:38:00 *** training ***
06/01/2019 10:38:00 step: 2084, epoch: 63, batch: 4, loss: 0.02266135811805725, acc: 100.0, f1: 100.0, r: 0.7151115972184348
06/01/2019 10:38:00 step: 2089, epoch: 63, batch: 9, loss: 0.01691076159477234, acc: 100.0, f1: 100.0, r: 0.6964815734816842
06/01/2019 10:38:01 step: 2094, epoch: 63, batch: 14, loss: 0.02922505885362625, acc: 100.0, f1: 100.0, r: 0.8388167777573883
06/01/2019 10:38:01 step: 2099, epoch: 63, batch: 19, loss: 0.024059630930423737, acc: 98.4375, f1: 97.22222222222221, r: 0.6863457234519719
06/01/2019 10:38:01 step: 2104, epoch: 63, batch: 24, loss: 0.022534281015396118, acc: 98.4375, f1: 98.01587301587303, r: 0.7416943769435813
06/01/2019 10:38:01 step: 2109, epoch: 63, batch: 29, loss: 0.07118814438581467, acc: 96.875, f1: 95.6800766283525, r: 0.7579392134171377
06/01/2019 10:38:01 *** evaluating ***
06/01/2019 10:38:02 step: 64, epoch: 63, acc: 60.256410256410255, f1: 21.916856589073213, r: 0.28229431082299483
06/01/2019 10:38:02 *** epoch: 65 ***
06/01/2019 10:38:02 *** training ***
06/01/2019 10:38:02 step: 2117, epoch: 64, batch: 4, loss: 0.09143436700105667, acc: 95.3125, f1: 94.12115662115663, r: 0.714159339205073
06/01/2019 10:38:02 step: 2122, epoch: 64, batch: 9, loss: 0.01679098606109619, acc: 100.0, f1: 100.0, r: 0.7395871367322303
06/01/2019 10:38:02 step: 2127, epoch: 64, batch: 14, loss: 0.02591695636510849, acc: 98.4375, f1: 98.7436676798379, r: 0.7101993791519866
06/01/2019 10:38:02 step: 2132, epoch: 64, batch: 19, loss: 0.08410222828388214, acc: 96.875, f1: 91.3694770837628, r: 0.6897850851099738
06/01/2019 10:38:03 step: 2137, epoch: 64, batch: 24, loss: 0.10612384974956512, acc: 95.3125, f1: 88.48290598290598, r: 0.7592240594141864
06/01/2019 10:38:03 step: 2142, epoch: 64, batch: 29, loss: 0.04472474008798599, acc: 96.875, f1: 93.23863636363636, r: 0.7800888483433618
06/01/2019 10:38:03 *** evaluating ***
06/01/2019 10:38:03 step: 65, epoch: 64, acc: 59.401709401709404, f1: 20.325029481132077, r: 0.27981539812169787
06/01/2019 10:38:03 *** epoch: 66 ***
06/01/2019 10:38:03 *** training ***
06/01/2019 10:38:03 step: 2150, epoch: 65, batch: 4, loss: 0.02389059215784073, acc: 100.0, f1: 100.0, r: 0.7012177049055685
06/01/2019 10:38:04 step: 2155, epoch: 65, batch: 9, loss: 0.13142280280590057, acc: 92.1875, f1: 81.23205741626795, r: 0.738982888652378
06/01/2019 10:38:04 step: 2160, epoch: 65, batch: 14, loss: 0.021144695580005646, acc: 100.0, f1: 100.0, r: 0.6953639733321431
06/01/2019 10:38:04 step: 2165, epoch: 65, batch: 19, loss: 0.06258376687765121, acc: 96.875, f1: 95.48701298701299, r: 0.7441905198791194
06/01/2019 10:38:04 step: 2170, epoch: 65, batch: 24, loss: 0.020262904465198517, acc: 100.0, f1: 100.0, r: 0.815033103493833
06/01/2019 10:38:04 step: 2175, epoch: 65, batch: 29, loss: 0.07343252748250961, acc: 95.3125, f1: 83.53469915097821, r: 0.7288853609799764
06/01/2019 10:38:05 *** evaluating ***
06/01/2019 10:38:05 step: 66, epoch: 65, acc: 58.97435897435898, f1: 19.92246801127738, r: 0.2782557695939598
06/01/2019 10:38:05 *** epoch: 67 ***
06/01/2019 10:38:05 *** training ***
06/01/2019 10:38:05 step: 2183, epoch: 66, batch: 4, loss: 0.020323187112808228, acc: 100.0, f1: 100.0, r: 0.7340001144647387
06/01/2019 10:38:05 step: 2188, epoch: 66, batch: 9, loss: 0.06107998639345169, acc: 98.4375, f1: 96.73469387755101, r: 0.6620115538699766
06/01/2019 10:38:05 step: 2193, epoch: 66, batch: 14, loss: 0.007108844816684723, acc: 100.0, f1: 100.0, r: 0.6532403304337246
06/01/2019 10:38:06 step: 2198, epoch: 66, batch: 19, loss: 0.0434129536151886, acc: 98.4375, f1: 96.1111111111111, r: 0.7614994830730243
06/01/2019 10:38:06 step: 2203, epoch: 66, batch: 24, loss: 0.049747664481401443, acc: 98.4375, f1: 98.86811867604185, r: 0.717171682373342
06/01/2019 10:38:06 step: 2208, epoch: 66, batch: 29, loss: 0.029113665223121643, acc: 100.0, f1: 100.0, r: 0.6303012229488802
06/01/2019 10:38:06 *** evaluating ***
06/01/2019 10:38:06 step: 67, epoch: 66, acc: 58.54700854700855, f1: 19.374580109986038, r: 0.2809365516839648
06/01/2019 10:38:06 *** epoch: 68 ***
06/01/2019 10:38:06 *** training ***
06/01/2019 10:38:06 step: 2216, epoch: 67, batch: 4, loss: 0.07008723169565201, acc: 96.875, f1: 92.33403014853695, r: 0.6544436220934515
06/01/2019 10:38:07 step: 2221, epoch: 67, batch: 9, loss: 0.013587981462478638, acc: 100.0, f1: 100.0, r: 0.8155905708560918
06/01/2019 10:38:07 step: 2226, epoch: 67, batch: 14, loss: 0.036681875586509705, acc: 98.4375, f1: 98.26839826839827, r: 0.7591988250356224
06/01/2019 10:38:07 step: 2231, epoch: 67, batch: 19, loss: 0.03396732360124588, acc: 98.4375, f1: 99.13675123697232, r: 0.655143060931066
06/01/2019 10:38:07 step: 2236, epoch: 67, batch: 24, loss: 0.037162065505981445, acc: 98.4375, f1: 99.00960384153662, r: 0.7767102020485779
06/01/2019 10:38:08 step: 2241, epoch: 67, batch: 29, loss: 0.04987343028187752, acc: 98.4375, f1: 86.90476190476191, r: 0.7907481501504753
06/01/2019 10:38:08 *** evaluating ***
06/01/2019 10:38:08 step: 68, epoch: 67, acc: 58.97435897435898, f1: 20.312995088719898, r: 0.2774793172125631
06/01/2019 10:38:08 *** epoch: 69 ***
06/01/2019 10:38:08 *** training ***
06/01/2019 10:38:08 step: 2249, epoch: 68, batch: 4, loss: 0.05316472053527832, acc: 98.4375, f1: 82.85714285714285, r: 0.6845952261168093
06/01/2019 10:38:08 step: 2254, epoch: 68, batch: 9, loss: 0.04077757149934769, acc: 100.0, f1: 100.0, r: 0.6957563030437035
06/01/2019 10:38:08 step: 2259, epoch: 68, batch: 14, loss: 0.055143505334854126, acc: 98.4375, f1: 98.66666666666667, r: 0.8202457693847588
06/01/2019 10:38:09 step: 2264, epoch: 68, batch: 19, loss: 0.06980505585670471, acc: 96.875, f1: 93.26923076923077, r: 0.7402257765801104
06/01/2019 10:38:09 step: 2269, epoch: 68, batch: 24, loss: 0.011270247399806976, acc: 100.0, f1: 100.0, r: 0.7136369473739386
06/01/2019 10:38:09 step: 2274, epoch: 68, batch: 29, loss: 0.07227379083633423, acc: 96.875, f1: 89.29263565891472, r: 0.6999657475424828
06/01/2019 10:38:09 *** evaluating ***
06/01/2019 10:38:09 step: 69, epoch: 68, acc: 58.54700854700855, f1: 19.428406218648682, r: 0.2822984381226846
06/01/2019 10:38:09 *** epoch: 70 ***
06/01/2019 10:38:09 *** training ***
06/01/2019 10:38:10 step: 2282, epoch: 69, batch: 4, loss: 0.0925222858786583, acc: 95.3125, f1: 92.45754951263424, r: 0.8070760146712926
06/01/2019 10:38:10 step: 2287, epoch: 69, batch: 9, loss: 0.010887287557125092, acc: 100.0, f1: 100.0, r: 0.8191091023222907
06/01/2019 10:38:10 step: 2292, epoch: 69, batch: 14, loss: 0.06312499940395355, acc: 98.4375, f1: 98.12987012987013, r: 0.7021019223162659
06/01/2019 10:38:10 step: 2297, epoch: 69, batch: 19, loss: 0.01939849555492401, acc: 100.0, f1: 100.0, r: 0.7542773353590425
06/01/2019 10:38:10 step: 2302, epoch: 69, batch: 24, loss: 0.046798430383205414, acc: 98.4375, f1: 99.14468995010691, r: 0.6944258953464086
06/01/2019 10:38:11 step: 2307, epoch: 69, batch: 29, loss: 0.006194271147251129, acc: 100.0, f1: 100.0, r: 0.6890445181136063
06/01/2019 10:38:11 *** evaluating ***
06/01/2019 10:38:11 step: 70, epoch: 69, acc: 59.82905982905983, f1: 20.763620763620764, r: 0.27361378213762383
06/01/2019 10:38:11 *** epoch: 71 ***
06/01/2019 10:38:11 *** training ***
06/01/2019 10:38:11 step: 2315, epoch: 70, batch: 4, loss: 0.03164432570338249, acc: 100.0, f1: 100.0, r: 0.8234729247538344
06/01/2019 10:38:11 step: 2320, epoch: 70, batch: 9, loss: 0.01162419468164444, acc: 100.0, f1: 100.0, r: 0.697145076306523
06/01/2019 10:38:12 step: 2325, epoch: 70, batch: 14, loss: 0.02254527062177658, acc: 100.0, f1: 100.0, r: 0.6999492309215483
06/01/2019 10:38:12 step: 2330, epoch: 70, batch: 19, loss: 0.11211732774972916, acc: 93.75, f1: 94.00910999608554, r: 0.7591093481662666
06/01/2019 10:38:12 step: 2335, epoch: 70, batch: 24, loss: 0.052633270621299744, acc: 98.4375, f1: 98.86128364389234, r: 0.7933462925652883
06/01/2019 10:38:12 step: 2340, epoch: 70, batch: 29, loss: 0.041023820638656616, acc: 100.0, f1: 100.0, r: 0.6700246997956651
06/01/2019 10:38:12 *** evaluating ***
06/01/2019 10:38:12 step: 71, epoch: 70, acc: 57.26495726495726, f1: 19.04405515538708, r: 0.27636558903041153
06/01/2019 10:38:12 *** epoch: 72 ***
06/01/2019 10:38:12 *** training ***
06/01/2019 10:38:13 step: 2348, epoch: 71, batch: 4, loss: 0.019599273800849915, acc: 100.0, f1: 100.0, r: 0.696280107928817
06/01/2019 10:38:13 step: 2353, epoch: 71, batch: 9, loss: 0.06930284202098846, acc: 98.4375, f1: 93.33333333333333, r: 0.8243311784283225
06/01/2019 10:38:13 step: 2358, epoch: 71, batch: 14, loss: 0.09391550719738007, acc: 96.875, f1: 96.83116883116884, r: 0.6862653511107358
06/01/2019 10:38:13 step: 2363, epoch: 71, batch: 19, loss: 0.008372969925403595, acc: 100.0, f1: 100.0, r: 0.6707375698013236
06/01/2019 10:38:14 step: 2368, epoch: 71, batch: 24, loss: 0.014863178133964539, acc: 100.0, f1: 100.0, r: 0.7728328297162838
06/01/2019 10:38:14 step: 2373, epoch: 71, batch: 29, loss: 0.04770929366350174, acc: 98.4375, f1: 99.27107959022851, r: 0.8103097036282295
06/01/2019 10:38:14 *** evaluating ***
06/01/2019 10:38:14 step: 72, epoch: 71, acc: 59.401709401709404, f1: 20.688987908455715, r: 0.2619060813806291
06/01/2019 10:38:14 *** epoch: 73 ***
06/01/2019 10:38:14 *** training ***
06/01/2019 10:38:14 step: 2381, epoch: 72, batch: 4, loss: 0.03978792205452919, acc: 100.0, f1: 100.0, r: 0.7525370467814685
06/01/2019 10:38:15 step: 2386, epoch: 72, batch: 9, loss: 0.028750233352184296, acc: 100.0, f1: 100.0, r: 0.6975233970918107
06/01/2019 10:38:15 step: 2391, epoch: 72, batch: 14, loss: 0.00454084575176239, acc: 100.0, f1: 100.0, r: 0.7346935906966255
06/01/2019 10:38:15 step: 2396, epoch: 72, batch: 19, loss: 0.06540489196777344, acc: 98.4375, f1: 94.44444444444444, r: 0.7892781611580253
06/01/2019 10:38:15 step: 2401, epoch: 72, batch: 24, loss: 0.06669650971889496, acc: 98.4375, f1: 96.68202764976958, r: 0.7094023026993074
06/01/2019 10:38:15 step: 2406, epoch: 72, batch: 29, loss: 0.04413339123129845, acc: 96.875, f1: 86.13420734862859, r: 0.6778234687918547
06/01/2019 10:38:16 *** evaluating ***
06/01/2019 10:38:16 step: 73, epoch: 72, acc: 58.97435897435898, f1: 20.5873128168848, r: 0.2767097084795699
06/01/2019 10:38:16 *** epoch: 74 ***
06/01/2019 10:38:16 *** training ***
06/01/2019 10:38:16 step: 2414, epoch: 73, batch: 4, loss: 0.020929545164108276, acc: 100.0, f1: 100.0, r: 0.7721191342454572
06/01/2019 10:38:16 step: 2419, epoch: 73, batch: 9, loss: 0.09083721041679382, acc: 96.875, f1: 94.91991991991992, r: 0.5788617865664752
06/01/2019 10:38:16 step: 2424, epoch: 73, batch: 14, loss: 0.040729522705078125, acc: 98.4375, f1: 97.77777777777779, r: 0.8259437893247692
06/01/2019 10:38:17 step: 2429, epoch: 73, batch: 19, loss: 0.03725115954875946, acc: 98.4375, f1: 98.06763285024154, r: 0.7862340159594187
06/01/2019 10:38:17 step: 2434, epoch: 73, batch: 24, loss: 0.03540662303566933, acc: 98.4375, f1: 86.90476190476191, r: 0.8157950682088208
06/01/2019 10:38:17 step: 2439, epoch: 73, batch: 29, loss: 0.008084747940301895, acc: 100.0, f1: 100.0, r: 0.8166921106548586
06/01/2019 10:38:17 *** evaluating ***
06/01/2019 10:38:17 step: 74, epoch: 73, acc: 58.119658119658126, f1: 19.61190327526961, r: 0.2753612030853755
06/01/2019 10:38:17 *** epoch: 75 ***
06/01/2019 10:38:17 *** training ***
06/01/2019 10:38:17 step: 2447, epoch: 74, batch: 4, loss: 0.03533285856246948, acc: 98.4375, f1: 99.22067268252665, r: 0.7523898846588573
06/01/2019 10:38:18 step: 2452, epoch: 74, batch: 9, loss: 0.0512242317199707, acc: 98.4375, f1: 87.17948717948718, r: 0.6912979087248936
06/01/2019 10:38:18 step: 2457, epoch: 74, batch: 14, loss: 0.012795008718967438, acc: 100.0, f1: 100.0, r: 0.66389372694365
06/01/2019 10:38:18 step: 2462, epoch: 74, batch: 19, loss: 0.0387992225587368, acc: 98.4375, f1: 98.3201581027668, r: 0.7779795386965962
06/01/2019 10:38:18 step: 2467, epoch: 74, batch: 24, loss: 0.04640576243400574, acc: 98.4375, f1: 95.71428571428572, r: 0.7797726674214928
06/01/2019 10:38:19 step: 2472, epoch: 74, batch: 29, loss: 0.05639932304620743, acc: 96.875, f1: 91.04761904761904, r: 0.6944991554405168
06/01/2019 10:38:19 *** evaluating ***
06/01/2019 10:38:19 step: 75, epoch: 74, acc: 58.54700854700855, f1: 19.319808363399776, r: 0.27949711379987
06/01/2019 10:38:19 *** epoch: 76 ***
06/01/2019 10:38:19 *** training ***
06/01/2019 10:38:19 step: 2480, epoch: 75, batch: 4, loss: 0.07722865045070648, acc: 95.3125, f1: 94.65417215273206, r: 0.7524226043387451
06/01/2019 10:38:19 step: 2485, epoch: 75, batch: 9, loss: 0.03304041922092438, acc: 100.0, f1: 100.0, r: 0.6672250005067207
06/01/2019 10:38:19 step: 2490, epoch: 75, batch: 14, loss: 0.030419282615184784, acc: 98.4375, f1: 99.1436925647452, r: 0.718893625773842
06/01/2019 10:38:20 step: 2495, epoch: 75, batch: 19, loss: 0.01981852948665619, acc: 98.4375, f1: 97.22222222222221, r: 0.7902023187220777
06/01/2019 10:38:20 step: 2500, epoch: 75, batch: 24, loss: 0.03470926731824875, acc: 98.4375, f1: 99.08733679807327, r: 0.6952463033879446
06/01/2019 10:38:20 step: 2505, epoch: 75, batch: 29, loss: 0.060275740921497345, acc: 96.875, f1: 98.01753801753802, r: 0.6868694018395115
06/01/2019 10:38:20 *** evaluating ***
06/01/2019 10:38:20 step: 76, epoch: 75, acc: 58.97435897435898, f1: 19.553009883198563, r: 0.2696336632887754
06/01/2019 10:38:20 *** epoch: 77 ***
06/01/2019 10:38:20 *** training ***
06/01/2019 10:38:21 step: 2513, epoch: 76, batch: 4, loss: 0.07281241565942764, acc: 96.875, f1: 84.32282003710576, r: 0.7667664334306598
06/01/2019 10:38:21 step: 2518, epoch: 76, batch: 9, loss: 0.0342821404337883, acc: 98.4375, f1: 97.68964189449365, r: 0.6787597952289113
06/01/2019 10:38:21 step: 2523, epoch: 76, batch: 14, loss: 0.017617959529161453, acc: 100.0, f1: 100.0, r: 0.7988179613537104
06/01/2019 10:38:21 step: 2528, epoch: 76, batch: 19, loss: 0.048322658985853195, acc: 98.4375, f1: 98.60742705570291, r: 0.8072971237177402
06/01/2019 10:38:21 step: 2533, epoch: 76, batch: 24, loss: 0.011843524873256683, acc: 100.0, f1: 100.0, r: 0.7962211745607716
06/01/2019 10:38:22 step: 2538, epoch: 76, batch: 29, loss: 0.034034185111522675, acc: 98.4375, f1: 96.66666666666667, r: 0.7271676146016298
06/01/2019 10:38:22 *** evaluating ***
06/01/2019 10:38:22 step: 77, epoch: 76, acc: 58.97435897435898, f1: 21.326522435897434, r: 0.27065658299280637
06/01/2019 10:38:22 *** epoch: 78 ***
06/01/2019 10:38:22 *** training ***
06/01/2019 10:38:22 step: 2546, epoch: 77, batch: 4, loss: 0.06434419751167297, acc: 98.4375, f1: 94.28571428571428, r: 0.6864901515793178
06/01/2019 10:38:22 step: 2551, epoch: 77, batch: 9, loss: 0.008157096803188324, acc: 100.0, f1: 100.0, r: 0.5623337574684195
06/01/2019 10:38:23 step: 2556, epoch: 77, batch: 14, loss: 0.057054005563259125, acc: 98.4375, f1: 98.69883437245866, r: 0.6951670193745518
06/01/2019 10:38:23 step: 2561, epoch: 77, batch: 19, loss: 0.03218948841094971, acc: 98.4375, f1: 94.04761904761905, r: 0.8081927265080496
06/01/2019 10:38:23 step: 2566, epoch: 77, batch: 24, loss: 0.07414666563272476, acc: 96.875, f1: 95.0531914893617, r: 0.6819939727585069
06/01/2019 10:38:23 step: 2571, epoch: 77, batch: 29, loss: 0.05168822780251503, acc: 98.4375, f1: 98.26839826839827, r: 0.6843164976045187
06/01/2019 10:38:23 *** evaluating ***
06/01/2019 10:38:24 step: 78, epoch: 77, acc: 60.256410256410255, f1: 23.146117400848766, r: 0.2687450129261637
06/01/2019 10:38:24 *** epoch: 79 ***
06/01/2019 10:38:24 *** training ***
06/01/2019 10:38:24 step: 2579, epoch: 78, batch: 4, loss: 0.019947342574596405, acc: 100.0, f1: 100.0, r: 0.7958213312238303
06/01/2019 10:38:24 step: 2584, epoch: 78, batch: 9, loss: 0.031595729291439056, acc: 100.0, f1: 100.0, r: 0.7436626124800892
06/01/2019 10:38:24 step: 2589, epoch: 78, batch: 14, loss: 0.05286424234509468, acc: 95.3125, f1: 91.53383458646617, r: 0.708057214343311
06/01/2019 10:38:24 step: 2594, epoch: 78, batch: 19, loss: 0.019505556672811508, acc: 100.0, f1: 100.0, r: 0.6967807996640689
06/01/2019 10:38:25 step: 2599, epoch: 78, batch: 24, loss: 0.0571821928024292, acc: 98.4375, f1: 99.11914172783739, r: 0.7681592734012566
06/01/2019 10:38:25 step: 2604, epoch: 78, batch: 29, loss: 0.01863192766904831, acc: 98.4375, f1: 99.06227106227107, r: 0.6620648340309372
06/01/2019 10:38:25 *** evaluating ***
06/01/2019 10:38:25 step: 79, epoch: 78, acc: 60.68376068376068, f1: 22.98459884011215, r: 0.2822581901833285
06/01/2019 10:38:25 *** epoch: 80 ***
06/01/2019 10:38:25 *** training ***
06/01/2019 10:38:25 step: 2612, epoch: 79, batch: 4, loss: 0.04297881945967674, acc: 98.4375, f1: 98.14315663372267, r: 0.6730126218776805
06/01/2019 10:38:26 step: 2617, epoch: 79, batch: 9, loss: 0.038028784096241, acc: 98.4375, f1: 98.36734693877551, r: 0.6772169664749765
06/01/2019 10:38:26 step: 2622, epoch: 79, batch: 14, loss: 0.0716937780380249, acc: 96.875, f1: 93.81632653061224, r: 0.6370162251530117
06/01/2019 10:38:26 step: 2627, epoch: 79, batch: 19, loss: 0.01075688749551773, acc: 100.0, f1: 100.0, r: 0.6884049525059038
06/01/2019 10:38:26 step: 2632, epoch: 79, batch: 24, loss: 0.0468657910823822, acc: 98.4375, f1: 99.27272727272727, r: 0.6670374850974639
06/01/2019 10:38:26 step: 2637, epoch: 79, batch: 29, loss: 0.07787604629993439, acc: 96.875, f1: 95.56737588652481, r: 0.7582979481388863
06/01/2019 10:38:27 *** evaluating ***
06/01/2019 10:38:27 step: 80, epoch: 79, acc: 59.401709401709404, f1: 22.269642916530074, r: 0.2745724533774854
06/01/2019 10:38:27 *** epoch: 81 ***
06/01/2019 10:38:27 *** training ***
06/01/2019 10:38:27 step: 2645, epoch: 80, batch: 4, loss: 0.03268229216337204, acc: 100.0, f1: 100.0, r: 0.7646561819084047
06/01/2019 10:38:27 step: 2650, epoch: 80, batch: 9, loss: 0.05444851517677307, acc: 98.4375, f1: 96.42857142857143, r: 0.8367745619780533
06/01/2019 10:38:27 step: 2655, epoch: 80, batch: 14, loss: 0.08376839011907578, acc: 96.875, f1: 96.96324143692566, r: 0.7867832184509497
06/01/2019 10:38:28 step: 2660, epoch: 80, batch: 19, loss: 0.05539196729660034, acc: 98.4375, f1: 86.11111111111111, r: 0.714510034646481
06/01/2019 10:38:28 step: 2665, epoch: 80, batch: 24, loss: 0.012176468968391418, acc: 100.0, f1: 100.0, r: 0.6902218834264229
06/01/2019 10:38:28 step: 2670, epoch: 80, batch: 29, loss: 0.0032784566283226013, acc: 100.0, f1: 100.0, r: 0.7182683349642076
06/01/2019 10:38:28 *** evaluating ***
06/01/2019 10:38:28 step: 81, epoch: 80, acc: 59.82905982905983, f1: 21.60651848151848, r: 0.26965346184256855
06/01/2019 10:38:28 *** epoch: 82 ***
06/01/2019 10:38:28 *** training ***
06/01/2019 10:38:28 step: 2678, epoch: 81, batch: 4, loss: 0.02113557606935501, acc: 100.0, f1: 100.0, r: 0.7853902680746201
06/01/2019 10:38:29 step: 2683, epoch: 81, batch: 9, loss: 0.06320833414793015, acc: 96.875, f1: 97.54112554112554, r: 0.7774621193876418
06/01/2019 10:38:29 step: 2688, epoch: 81, batch: 14, loss: 0.004786446690559387, acc: 100.0, f1: 100.0, r: 0.8196328215699128
06/01/2019 10:38:29 step: 2693, epoch: 81, batch: 19, loss: 0.09028696268796921, acc: 95.3125, f1: 72.85719513980384, r: 0.7193729157319562
06/01/2019 10:38:29 step: 2698, epoch: 81, batch: 24, loss: 0.04327952861785889, acc: 96.875, f1: 98.40048725637182, r: 0.7683680725889886
06/01/2019 10:38:30 step: 2703, epoch: 81, batch: 29, loss: 0.010982893407344818, acc: 100.0, f1: 100.0, r: 0.6174270243302967
06/01/2019 10:38:30 *** evaluating ***
06/01/2019 10:38:30 step: 82, epoch: 81, acc: 58.119658119658126, f1: 19.183671698072896, r: 0.27198838466466424
06/01/2019 10:38:30 *** epoch: 83 ***
06/01/2019 10:38:30 *** training ***
06/01/2019 10:38:30 step: 2711, epoch: 82, batch: 4, loss: 0.028941314667463303, acc: 98.4375, f1: 99.38490978676873, r: 0.6921882149371132
06/01/2019 10:38:30 step: 2716, epoch: 82, batch: 9, loss: 0.04866945743560791, acc: 96.875, f1: 94.00487588652481, r: 0.7938073968036939
06/01/2019 10:38:31 step: 2721, epoch: 82, batch: 14, loss: 0.043298158794641495, acc: 98.4375, f1: 95.28985507246377, r: 0.7950572531232791
06/01/2019 10:38:31 step: 2726, epoch: 82, batch: 19, loss: 0.05090809613466263, acc: 98.4375, f1: 98.02102659245516, r: 0.673747574605581
06/01/2019 10:38:31 step: 2731, epoch: 82, batch: 24, loss: 0.06540808826684952, acc: 96.875, f1: 96.93291109547268, r: 0.7670238148130466
06/01/2019 10:38:31 step: 2736, epoch: 82, batch: 29, loss: 0.008713774383068085, acc: 100.0, f1: 100.0, r: 0.7289701667016412
06/01/2019 10:38:31 *** evaluating ***
06/01/2019 10:38:31 step: 83, epoch: 82, acc: 59.401709401709404, f1: 20.0224050294275, r: 0.2790815449012147
06/01/2019 10:38:31 *** epoch: 84 ***
06/01/2019 10:38:31 *** training ***
06/01/2019 10:38:32 step: 2744, epoch: 83, batch: 4, loss: 0.030765868723392487, acc: 98.4375, f1: 93.33333333333333, r: 0.8372831020104899
06/01/2019 10:38:32 step: 2749, epoch: 83, batch: 9, loss: 0.010836027562618256, acc: 100.0, f1: 100.0, r: 0.6347627923158249
06/01/2019 10:38:32 step: 2754, epoch: 83, batch: 14, loss: 0.031016461551189423, acc: 100.0, f1: 100.0, r: 0.6923038465444461
06/01/2019 10:38:32 step: 2759, epoch: 83, batch: 19, loss: 0.021863501518964767, acc: 98.4375, f1: 98.57293868921776, r: 0.8208044430205309
06/01/2019 10:38:33 step: 2764, epoch: 83, batch: 24, loss: 0.05907531827688217, acc: 98.4375, f1: 99.03703703703704, r: 0.7327926838010326
06/01/2019 10:38:33 step: 2769, epoch: 83, batch: 29, loss: 0.03439834713935852, acc: 100.0, f1: 100.0, r: 0.7423241572888201
06/01/2019 10:38:33 *** evaluating ***
06/01/2019 10:38:33 step: 84, epoch: 83, acc: 59.401709401709404, f1: 20.99396213395437, r: 0.2652845880729488
06/01/2019 10:38:33 *** epoch: 85 ***
06/01/2019 10:38:33 *** training ***
06/01/2019 10:38:33 step: 2777, epoch: 84, batch: 4, loss: 0.02979445457458496, acc: 100.0, f1: 100.0, r: 0.759468307777775
06/01/2019 10:38:33 step: 2782, epoch: 84, batch: 9, loss: 0.0445084311068058, acc: 98.4375, f1: 98.02659802659804, r: 0.7317166816697438
06/01/2019 10:38:34 step: 2787, epoch: 84, batch: 14, loss: 0.00621873140335083, acc: 100.0, f1: 100.0, r: 0.7906378507250166
06/01/2019 10:38:34 step: 2792, epoch: 84, batch: 19, loss: 0.02017909288406372, acc: 100.0, f1: 100.0, r: 0.692895443998385
06/01/2019 10:38:34 step: 2797, epoch: 84, batch: 24, loss: 0.008353173732757568, acc: 100.0, f1: 100.0, r: 0.7438163647608373
06/01/2019 10:38:34 step: 2802, epoch: 84, batch: 29, loss: 0.020446844398975372, acc: 100.0, f1: 100.0, r: 0.6012481656274494
06/01/2019 10:38:34 *** evaluating ***
06/01/2019 10:38:35 step: 85, epoch: 84, acc: 58.54700854700855, f1: 19.48621553884712, r: 0.26970738092599095
06/01/2019 10:38:35 *** epoch: 86 ***
06/01/2019 10:38:35 *** training ***
06/01/2019 10:38:35 step: 2810, epoch: 85, batch: 4, loss: 0.04509378969669342, acc: 96.875, f1: 97.67764248556567, r: 0.6640818511636997
06/01/2019 10:38:35 step: 2815, epoch: 85, batch: 9, loss: 0.05008336156606674, acc: 96.875, f1: 95.3102453102453, r: 0.7981877794420665
06/01/2019 10:38:35 step: 2820, epoch: 85, batch: 14, loss: 0.005691744387149811, acc: 100.0, f1: 100.0, r: 0.6513489977792591
06/01/2019 10:38:36 step: 2825, epoch: 85, batch: 19, loss: 0.05202322080731392, acc: 100.0, f1: 100.0, r: 0.6875391556526458
06/01/2019 10:38:36 step: 2830, epoch: 85, batch: 24, loss: 0.012975171208381653, acc: 100.0, f1: 100.0, r: 0.8243810071219567
06/01/2019 10:38:36 step: 2835, epoch: 85, batch: 29, loss: 0.03508511930704117, acc: 98.4375, f1: 99.22171018945212, r: 0.6555266815377051
06/01/2019 10:38:36 *** evaluating ***
06/01/2019 10:38:36 step: 86, epoch: 85, acc: 58.97435897435898, f1: 20.904898218829516, r: 0.2529627140657411
06/01/2019 10:38:36 *** epoch: 87 ***
06/01/2019 10:38:36 *** training ***
06/01/2019 10:38:36 step: 2843, epoch: 86, batch: 4, loss: 0.010916315019130707, acc: 100.0, f1: 100.0, r: 0.6896157655283583
06/01/2019 10:38:37 step: 2848, epoch: 86, batch: 9, loss: 0.008859708905220032, acc: 100.0, f1: 100.0, r: 0.6919259282924329
06/01/2019 10:38:37 step: 2853, epoch: 86, batch: 14, loss: 0.008214429020881653, acc: 100.0, f1: 100.0, r: 0.7660664701647729
06/01/2019 10:38:37 step: 2858, epoch: 86, batch: 19, loss: 0.0030112862586975098, acc: 100.0, f1: 100.0, r: 0.6777174112596379
06/01/2019 10:38:37 step: 2863, epoch: 86, batch: 24, loss: 0.023894943296909332, acc: 98.4375, f1: 97.40259740259741, r: 0.686996939240543
06/01/2019 10:38:38 step: 2868, epoch: 86, batch: 29, loss: 0.057076118886470795, acc: 98.4375, f1: 98.7267634326458, r: 0.6794125324261827
06/01/2019 10:38:38 *** evaluating ***
06/01/2019 10:38:38 step: 87, epoch: 86, acc: 59.401709401709404, f1: 21.60973544805361, r: 0.26938299085927986
06/01/2019 10:38:38 *** epoch: 88 ***
06/01/2019 10:38:38 *** training ***
06/01/2019 10:38:38 step: 2876, epoch: 87, batch: 4, loss: 0.024775691330432892, acc: 98.4375, f1: 98.7468671679198, r: 0.7966972460591195
06/01/2019 10:38:38 step: 2881, epoch: 87, batch: 9, loss: 0.023768804967403412, acc: 100.0, f1: 100.0, r: 0.7168396311226326
06/01/2019 10:38:38 step: 2886, epoch: 87, batch: 14, loss: 0.014316447079181671, acc: 100.0, f1: 100.0, r: 0.703262816257336
06/01/2019 10:38:39 step: 2891, epoch: 87, batch: 19, loss: 0.009691886603832245, acc: 100.0, f1: 100.0, r: 0.7088356808815863
06/01/2019 10:38:39 step: 2896, epoch: 87, batch: 24, loss: 0.014698013663291931, acc: 100.0, f1: 100.0, r: 0.7103143863638199
06/01/2019 10:38:39 step: 2901, epoch: 87, batch: 29, loss: 0.004850953817367554, acc: 100.0, f1: 100.0, r: 0.7030442218477908
06/01/2019 10:38:39 *** evaluating ***
06/01/2019 10:38:39 step: 88, epoch: 87, acc: 59.82905982905983, f1: 23.297244869825512, r: 0.273857321950508
06/01/2019 10:38:39 *** epoch: 89 ***
06/01/2019 10:38:39 *** training ***
06/01/2019 10:38:40 step: 2909, epoch: 88, batch: 4, loss: 0.008372850716114044, acc: 100.0, f1: 100.0, r: 0.7439806329909694
06/01/2019 10:38:40 step: 2914, epoch: 88, batch: 9, loss: 0.05443492531776428, acc: 98.4375, f1: 98.12987012987013, r: 0.6738794808318683
06/01/2019 10:38:40 step: 2919, epoch: 88, batch: 14, loss: 0.013759516179561615, acc: 100.0, f1: 100.0, r: 0.8043417406509463
06/01/2019 10:38:40 step: 2924, epoch: 88, batch: 19, loss: 0.005899831652641296, acc: 100.0, f1: 100.0, r: 0.7066008361099917
06/01/2019 10:38:41 step: 2929, epoch: 88, batch: 24, loss: 0.03729996830224991, acc: 98.4375, f1: 99.25676259614555, r: 0.6935076768917714
06/01/2019 10:38:41 step: 2934, epoch: 88, batch: 29, loss: 0.03458225727081299, acc: 100.0, f1: 100.0, r: 0.7767513135717781
06/01/2019 10:38:41 *** evaluating ***
06/01/2019 10:38:41 step: 89, epoch: 88, acc: 59.401709401709404, f1: 22.49718790560375, r: 0.2702459687356137
06/01/2019 10:38:41 *** epoch: 90 ***
06/01/2019 10:38:41 *** training ***
06/01/2019 10:38:41 step: 2942, epoch: 89, batch: 4, loss: 0.04423138499259949, acc: 98.4375, f1: 98.62318840579711, r: 0.7889219258051385
06/01/2019 10:38:41 step: 2947, epoch: 89, batch: 9, loss: 0.025021381676197052, acc: 100.0, f1: 100.0, r: 0.7433000674861502
06/01/2019 10:38:42 step: 2952, epoch: 89, batch: 14, loss: 0.04842961207032204, acc: 96.875, f1: 95.77664399092971, r: 0.6906341981048952
06/01/2019 10:38:42 step: 2957, epoch: 89, batch: 19, loss: 0.012327738106250763, acc: 100.0, f1: 100.0, r: 0.7445979709401259
06/01/2019 10:38:42 step: 2962, epoch: 89, batch: 24, loss: 0.022555261850357056, acc: 100.0, f1: 100.0, r: 0.799322661436239
06/01/2019 10:38:42 step: 2967, epoch: 89, batch: 29, loss: 0.06369970738887787, acc: 96.875, f1: 94.40359477124183, r: 0.7970158661879576
06/01/2019 10:38:42 *** evaluating ***
06/01/2019 10:38:43 step: 90, epoch: 89, acc: 59.82905982905983, f1: 20.531226199543028, r: 0.2653570020370249
06/01/2019 10:38:43 *** epoch: 91 ***
06/01/2019 10:38:43 *** training ***
06/01/2019 10:38:43 step: 2975, epoch: 90, batch: 4, loss: 0.04416990280151367, acc: 98.4375, f1: 97.98701298701299, r: 0.7710743106332036
06/01/2019 10:38:43 step: 2980, epoch: 90, batch: 9, loss: 0.026265405118465424, acc: 98.4375, f1: 98.59767891682785, r: 0.7804550021443156
06/01/2019 10:38:43 step: 2985, epoch: 90, batch: 14, loss: 0.01641020178794861, acc: 100.0, f1: 100.0, r: 0.7164038082943104
06/01/2019 10:38:43 step: 2990, epoch: 90, batch: 19, loss: 0.01192571222782135, acc: 100.0, f1: 100.0, r: 0.6245651002738141
06/01/2019 10:38:44 step: 2995, epoch: 90, batch: 24, loss: 0.024485543370246887, acc: 98.4375, f1: 98.8795518207283, r: 0.6960913857599388
06/01/2019 10:38:44 step: 3000, epoch: 90, batch: 29, loss: 0.010997354984283447, acc: 100.0, f1: 100.0, r: 0.6381910430887767
06/01/2019 10:38:44 *** evaluating ***
06/01/2019 10:38:44 step: 91, epoch: 90, acc: 58.97435897435898, f1: 20.904290455629308, r: 0.27098888571675434
06/01/2019 10:38:44 *** epoch: 92 ***
06/01/2019 10:38:44 *** training ***
06/01/2019 10:38:44 step: 3008, epoch: 91, batch: 4, loss: 0.00512908399105072, acc: 100.0, f1: 100.0, r: 0.8070633757649226
06/01/2019 10:38:45 step: 3013, epoch: 91, batch: 9, loss: 0.025900375097990036, acc: 100.0, f1: 100.0, r: 0.7353220279400537
06/01/2019 10:38:45 step: 3018, epoch: 91, batch: 14, loss: 0.03179147094488144, acc: 98.4375, f1: 92.38095238095238, r: 0.6311891115562559
06/01/2019 10:38:45 step: 3023, epoch: 91, batch: 19, loss: 0.008367761969566345, acc: 100.0, f1: 100.0, r: 0.8024639175273396
06/01/2019 10:38:45 step: 3028, epoch: 91, batch: 24, loss: 0.022159725427627563, acc: 100.0, f1: 100.0, r: 0.8189324646772929
06/01/2019 10:38:46 step: 3033, epoch: 91, batch: 29, loss: 0.015362229198217392, acc: 100.0, f1: 100.0, r: 0.6412006181990245
06/01/2019 10:38:46 *** evaluating ***
06/01/2019 10:38:46 step: 92, epoch: 91, acc: 58.97435897435898, f1: 20.540008883812426, r: 0.26360946742582053
06/01/2019 10:38:46 *** epoch: 93 ***
06/01/2019 10:38:46 *** training ***
06/01/2019 10:38:46 step: 3041, epoch: 92, batch: 4, loss: 0.004788793623447418, acc: 100.0, f1: 100.0, r: 0.8080779693037183
06/01/2019 10:38:46 step: 3046, epoch: 92, batch: 9, loss: 0.031064748764038086, acc: 98.4375, f1: 95.33333333333334, r: 0.75000474753555
06/01/2019 10:38:46 step: 3051, epoch: 92, batch: 14, loss: 0.006804227828979492, acc: 100.0, f1: 100.0, r: 0.7124639090853339
06/01/2019 10:38:47 step: 3056, epoch: 92, batch: 19, loss: 0.07183780521154404, acc: 96.875, f1: 92.70833333333333, r: 0.8133261163123906
06/01/2019 10:38:47 step: 3061, epoch: 92, batch: 24, loss: 0.03909341245889664, acc: 98.4375, f1: 98.96089691003554, r: 0.5930053968951416
06/01/2019 10:38:47 step: 3066, epoch: 92, batch: 29, loss: 0.02191700041294098, acc: 100.0, f1: 100.0, r: 0.6969259108339185
06/01/2019 10:38:47 *** evaluating ***
06/01/2019 10:38:47 step: 93, epoch: 92, acc: 59.401709401709404, f1: 20.717597637790593, r: 0.26776394637775963
06/01/2019 10:38:47 *** epoch: 94 ***
06/01/2019 10:38:47 *** training ***
06/01/2019 10:38:48 step: 3074, epoch: 93, batch: 4, loss: 0.01419917494058609, acc: 98.4375, f1: 97.06896551724138, r: 0.8226334298694968
06/01/2019 10:38:48 step: 3079, epoch: 93, batch: 9, loss: 0.03931903839111328, acc: 98.4375, f1: 98.81123180979709, r: 0.7111523842493213
06/01/2019 10:38:48 step: 3084, epoch: 93, batch: 14, loss: 0.024671807885169983, acc: 100.0, f1: 100.0, r: 0.8125544173398255
06/01/2019 10:38:48 step: 3089, epoch: 93, batch: 19, loss: 0.13796955347061157, acc: 93.75, f1: 91.68864760970024, r: 0.7284178884592974
06/01/2019 10:38:48 step: 3094, epoch: 93, batch: 24, loss: 0.01733480393886566, acc: 100.0, f1: 100.0, r: 0.7507981812593757
06/01/2019 10:38:49 step: 3099, epoch: 93, batch: 29, loss: 0.09155350923538208, acc: 96.875, f1: 96.22474747474747, r: 0.7912143687315244
06/01/2019 10:38:49 *** evaluating ***
06/01/2019 10:38:49 step: 94, epoch: 93, acc: 59.401709401709404, f1: 21.497027639884784, r: 0.26276005671310365
06/01/2019 10:38:49 *** epoch: 95 ***
06/01/2019 10:38:49 *** training ***
06/01/2019 10:38:49 step: 3107, epoch: 94, batch: 4, loss: 0.04129160940647125, acc: 98.4375, f1: 93.33333333333333, r: 0.6075694124615414
06/01/2019 10:38:49 step: 3112, epoch: 94, batch: 9, loss: 0.06163014471530914, acc: 98.4375, f1: 96.36363636363636, r: 0.7751507518725735
06/01/2019 10:38:50 step: 3117, epoch: 94, batch: 14, loss: 0.01085943728685379, acc: 100.0, f1: 100.0, r: 0.6930447556102945
06/01/2019 10:38:50 step: 3122, epoch: 94, batch: 19, loss: 0.013247653841972351, acc: 100.0, f1: 100.0, r: 0.7683279714438815
06/01/2019 10:38:50 step: 3127, epoch: 94, batch: 24, loss: 0.038253121078014374, acc: 98.4375, f1: 98.94416893297074, r: 0.7116096332428051
06/01/2019 10:38:50 step: 3132, epoch: 94, batch: 29, loss: 0.02419612556695938, acc: 98.4375, f1: 98.97400820793433, r: 0.7913960520953986
06/01/2019 10:38:50 *** evaluating ***
06/01/2019 10:38:51 step: 95, epoch: 94, acc: 58.54700854700855, f1: 20.3571002068539, r: 0.2754357588851496
06/01/2019 10:38:51 *** epoch: 96 ***
06/01/2019 10:38:51 *** training ***
06/01/2019 10:38:51 step: 3140, epoch: 95, batch: 4, loss: 0.01812218502163887, acc: 100.0, f1: 100.0, r: 0.749028275175706
06/01/2019 10:38:51 step: 3145, epoch: 95, batch: 9, loss: 0.045382123440504074, acc: 98.4375, f1: 96.85131195335276, r: 0.7134277331680473
06/01/2019 10:38:51 step: 3150, epoch: 95, batch: 14, loss: 0.02689305692911148, acc: 100.0, f1: 100.0, r: 0.7142821862882622
06/01/2019 10:38:51 step: 3155, epoch: 95, batch: 19, loss: 0.038917895406484604, acc: 100.0, f1: 100.0, r: 0.6945860628902516
06/01/2019 10:38:52 step: 3160, epoch: 95, batch: 24, loss: 0.014470592141151428, acc: 100.0, f1: 100.0, r: 0.7961540035919484
06/01/2019 10:38:52 step: 3165, epoch: 95, batch: 29, loss: 0.0143071748316288, acc: 98.4375, f1: 98.63636363636363, r: 0.7943219649764439
06/01/2019 10:38:52 *** evaluating ***
06/01/2019 10:38:52 step: 96, epoch: 95, acc: 58.97435897435898, f1: 20.86705905771858, r: 0.25940110498603935
06/01/2019 10:38:52 *** epoch: 97 ***
06/01/2019 10:38:52 *** training ***
06/01/2019 10:38:52 step: 3173, epoch: 96, batch: 4, loss: 0.025396648794412613, acc: 98.4375, f1: 98.25396825396825, r: 0.7877617246770192
06/01/2019 10:38:53 step: 3178, epoch: 96, batch: 9, loss: 0.046515073627233505, acc: 96.875, f1: 96.52656046744717, r: 0.6951083945301031
06/01/2019 10:38:53 step: 3183, epoch: 96, batch: 14, loss: 0.012662798166275024, acc: 100.0, f1: 100.0, r: 0.6961710863231051
06/01/2019 10:38:53 step: 3188, epoch: 96, batch: 19, loss: 0.008901894092559814, acc: 100.0, f1: 100.0, r: 0.7766222692101425
06/01/2019 10:38:53 step: 3193, epoch: 96, batch: 24, loss: 0.044029273092746735, acc: 96.875, f1: 85.16395258668439, r: 0.6822885092240227
06/01/2019 10:38:53 step: 3198, epoch: 96, batch: 29, loss: 0.028457455337047577, acc: 98.4375, f1: 96.37188208616782, r: 0.6926638001004296
06/01/2019 10:38:54 *** evaluating ***
06/01/2019 10:38:54 step: 97, epoch: 96, acc: 58.97435897435898, f1: 20.187374015026567, r: 0.2535124715701661
06/01/2019 10:38:54 *** epoch: 98 ***
06/01/2019 10:38:54 *** training ***
06/01/2019 10:38:54 step: 3206, epoch: 97, batch: 4, loss: 0.09489830583333969, acc: 93.75, f1: 81.46657283603096, r: 0.6755464764728225
06/01/2019 10:38:54 step: 3211, epoch: 97, batch: 9, loss: 0.02234196662902832, acc: 98.4375, f1: 99.22924901185772, r: 0.7828628635684242
06/01/2019 10:38:54 step: 3216, epoch: 97, batch: 14, loss: 0.015407733619213104, acc: 100.0, f1: 100.0, r: 0.7254657579899707
06/01/2019 10:38:55 step: 3221, epoch: 97, batch: 19, loss: 0.034252118319272995, acc: 98.4375, f1: 98.15295815295816, r: 0.6598832183594296
06/01/2019 10:38:55 step: 3226, epoch: 97, batch: 24, loss: 0.018916018307209015, acc: 98.4375, f1: 99.10934020860189, r: 0.6973021830568893
06/01/2019 10:38:55 step: 3231, epoch: 97, batch: 29, loss: 0.013929478824138641, acc: 100.0, f1: 100.0, r: 0.7019869634151467
06/01/2019 10:38:55 *** evaluating ***
06/01/2019 10:38:55 step: 98, epoch: 97, acc: 59.82905982905983, f1: 22.956044033327405, r: 0.2665868695949922
06/01/2019 10:38:55 *** epoch: 99 ***
06/01/2019 10:38:55 *** training ***
06/01/2019 10:38:55 step: 3239, epoch: 98, batch: 4, loss: 0.0029976442456245422, acc: 100.0, f1: 100.0, r: 0.7154536469393007
06/01/2019 10:38:56 step: 3244, epoch: 98, batch: 9, loss: 0.019196592271327972, acc: 100.0, f1: 100.0, r: 0.6754159424720977
06/01/2019 10:38:56 step: 3249, epoch: 98, batch: 14, loss: 0.020915493369102478, acc: 100.0, f1: 100.0, r: 0.7336400690847968
06/01/2019 10:38:56 step: 3254, epoch: 98, batch: 19, loss: 0.022397983819246292, acc: 100.0, f1: 100.0, r: 0.8224242032433987
06/01/2019 10:38:56 step: 3259, epoch: 98, batch: 24, loss: 0.05908912420272827, acc: 96.875, f1: 97.35797827903092, r: 0.8223145114212627
06/01/2019 10:38:57 step: 3264, epoch: 98, batch: 29, loss: 0.0014865100383758545, acc: 100.0, f1: 100.0, r: 0.7141123975402766
06/01/2019 10:38:57 *** evaluating ***
06/01/2019 10:38:57 step: 99, epoch: 98, acc: 58.54700854700855, f1: 19.50580115812307, r: 0.25522653513448784
06/01/2019 10:38:57 *** epoch: 100 ***
06/01/2019 10:38:57 *** training ***
06/01/2019 10:38:57 step: 3272, epoch: 99, batch: 4, loss: 0.004895590245723724, acc: 100.0, f1: 100.0, r: 0.6971367787887162
06/01/2019 10:38:57 step: 3277, epoch: 99, batch: 9, loss: 0.04788580164313316, acc: 98.4375, f1: 93.19727891156464, r: 0.7257829736500517
06/01/2019 10:38:57 step: 3282, epoch: 99, batch: 14, loss: 0.010797493159770966, acc: 100.0, f1: 100.0, r: 0.7729028673046723
06/01/2019 10:38:58 step: 3287, epoch: 99, batch: 19, loss: 0.013378873467445374, acc: 100.0, f1: 100.0, r: 0.842694460447417
06/01/2019 10:38:58 step: 3292, epoch: 99, batch: 24, loss: 0.00989842414855957, acc: 100.0, f1: 100.0, r: 0.8416437155001024
06/01/2019 10:38:58 step: 3297, epoch: 99, batch: 29, loss: 0.01744149625301361, acc: 100.0, f1: 100.0, r: 0.717116112207803
06/01/2019 10:38:58 *** evaluating ***
06/01/2019 10:38:58 step: 100, epoch: 99, acc: 58.97435897435898, f1: 20.557416267942582, r: 0.2661300735964358
06/01/2019 10:38:58 *** epoch: 101 ***
06/01/2019 10:38:58 *** training ***
06/01/2019 10:38:59 step: 3305, epoch: 100, batch: 4, loss: 0.00529884546995163, acc: 100.0, f1: 100.0, r: 0.8038421003092425
06/01/2019 10:38:59 step: 3310, epoch: 100, batch: 9, loss: 0.042929843068122864, acc: 98.4375, f1: 98.53854585312386, r: 0.6999740110153679
06/01/2019 10:38:59 step: 3315, epoch: 100, batch: 14, loss: 0.01571100950241089, acc: 100.0, f1: 100.0, r: 0.7311086675762805
06/01/2019 10:38:59 step: 3320, epoch: 100, batch: 19, loss: 0.00914502888917923, acc: 100.0, f1: 100.0, r: 0.6978310573928317
06/01/2019 10:38:59 step: 3325, epoch: 100, batch: 24, loss: 0.007680051028728485, acc: 100.0, f1: 100.0, r: 0.7047427765079377
06/01/2019 10:39:00 step: 3330, epoch: 100, batch: 29, loss: 0.013118721544742584, acc: 100.0, f1: 100.0, r: 0.7123298823707239
06/01/2019 10:39:00 *** evaluating ***
06/01/2019 10:39:00 step: 101, epoch: 100, acc: 58.54700854700855, f1: 19.48234349919743, r: 0.2750190303481607
06/01/2019 10:39:00 *** epoch: 102 ***
06/01/2019 10:39:00 *** training ***
06/01/2019 10:39:00 step: 3338, epoch: 101, batch: 4, loss: 0.01078246533870697, acc: 100.0, f1: 100.0, r: 0.7636465547653507
06/01/2019 10:39:00 step: 3343, epoch: 101, batch: 9, loss: 0.0128091499209404, acc: 100.0, f1: 100.0, r: 0.8046929604426042
06/01/2019 10:39:01 step: 3348, epoch: 101, batch: 14, loss: 0.022489391267299652, acc: 100.0, f1: 100.0, r: 0.64143562915298
06/01/2019 10:39:01 step: 3353, epoch: 101, batch: 19, loss: 0.041175901889801025, acc: 98.4375, f1: 94.13919413919413, r: 0.7915134532401283
06/01/2019 10:39:01 step: 3358, epoch: 101, batch: 24, loss: 0.008943594992160797, acc: 100.0, f1: 100.0, r: 0.7057158391520764
06/01/2019 10:39:01 step: 3363, epoch: 101, batch: 29, loss: 0.008420787751674652, acc: 100.0, f1: 100.0, r: 0.6676719374206723
06/01/2019 10:39:01 *** evaluating ***
06/01/2019 10:39:01 step: 102, epoch: 101, acc: 59.82905982905983, f1: 22.129987020299744, r: 0.26952199745812394
06/01/2019 10:39:01 *** epoch: 103 ***
06/01/2019 10:39:01 *** training ***
06/01/2019 10:39:02 step: 3371, epoch: 102, batch: 4, loss: 0.002558588981628418, acc: 100.0, f1: 100.0, r: 0.7570379722297196
06/01/2019 10:39:02 step: 3376, epoch: 102, batch: 9, loss: 0.024332702159881592, acc: 98.4375, f1: 96.73469387755101, r: 0.7415996737463431
06/01/2019 10:39:02 step: 3381, epoch: 102, batch: 14, loss: 0.003971464931964874, acc: 100.0, f1: 100.0, r: 0.7508451953434317
06/01/2019 10:39:02 step: 3386, epoch: 102, batch: 19, loss: 0.006810232996940613, acc: 100.0, f1: 100.0, r: 0.8054522791050943
06/01/2019 10:39:03 step: 3391, epoch: 102, batch: 24, loss: 0.008670046925544739, acc: 100.0, f1: 100.0, r: 0.7025033783494383
06/01/2019 10:39:03 step: 3396, epoch: 102, batch: 29, loss: 0.042072758078575134, acc: 98.4375, f1: 97.59288330716902, r: 0.7199837453232446
06/01/2019 10:39:03 *** evaluating ***
06/01/2019 10:39:03 step: 103, epoch: 102, acc: 58.54700854700855, f1: 19.770707348359903, r: 0.26972680592376386
06/01/2019 10:39:03 *** epoch: 104 ***
06/01/2019 10:39:03 *** training ***
06/01/2019 10:39:03 step: 3404, epoch: 103, batch: 4, loss: 0.03264494240283966, acc: 98.4375, f1: 99.25160370634354, r: 0.7828019795194244
06/01/2019 10:39:03 step: 3409, epoch: 103, batch: 9, loss: 0.06280839443206787, acc: 96.875, f1: 93.61293859649122, r: 0.7333861138851445
06/01/2019 10:39:04 step: 3414, epoch: 103, batch: 14, loss: 0.02879450097680092, acc: 98.4375, f1: 98.08018068887634, r: 0.7056221664618954
06/01/2019 10:39:04 step: 3419, epoch: 103, batch: 19, loss: 0.012076623737812042, acc: 100.0, f1: 100.0, r: 0.710527691910318
06/01/2019 10:39:04 step: 3424, epoch: 103, batch: 24, loss: 0.02292107790708542, acc: 98.4375, f1: 97.79158040027606, r: 0.6849211945329975
06/01/2019 10:39:04 step: 3429, epoch: 103, batch: 29, loss: 0.02102125994861126, acc: 100.0, f1: 100.0, r: 0.7779004702791794
06/01/2019 10:39:04 *** evaluating ***
06/01/2019 10:39:05 step: 104, epoch: 103, acc: 58.54700854700855, f1: 20.12289071747955, r: 0.2713358547088226
06/01/2019 10:39:05 *** epoch: 105 ***
06/01/2019 10:39:05 *** training ***
06/01/2019 10:39:05 step: 3437, epoch: 104, batch: 4, loss: 0.007138572633266449, acc: 100.0, f1: 100.0, r: 0.8619993997933741
06/01/2019 10:39:05 step: 3442, epoch: 104, batch: 9, loss: 0.012638196349143982, acc: 100.0, f1: 100.0, r: 0.83264862179077
06/01/2019 10:39:05 step: 3447, epoch: 104, batch: 14, loss: 0.027515284717082977, acc: 98.4375, f1: 99.12128877646118, r: 0.7170492304327664
06/01/2019 10:39:05 step: 3452, epoch: 104, batch: 19, loss: 0.00286024808883667, acc: 100.0, f1: 100.0, r: 0.6682644137971421
06/01/2019 10:39:06 step: 3457, epoch: 104, batch: 24, loss: 0.03095245733857155, acc: 100.0, f1: 100.0, r: 0.7800308043285613
06/01/2019 10:39:06 step: 3462, epoch: 104, batch: 29, loss: 0.03109227865934372, acc: 98.4375, f1: 83.33333333333333, r: 0.7318043843041564
06/01/2019 10:39:06 *** evaluating ***
06/01/2019 10:39:06 step: 105, epoch: 104, acc: 58.54700854700855, f1: 19.852464332036316, r: 0.2748014968731072
06/01/2019 10:39:06 *** epoch: 106 ***
06/01/2019 10:39:06 *** training ***
06/01/2019 10:39:06 step: 3470, epoch: 105, batch: 4, loss: 0.029696840792894363, acc: 98.4375, f1: 97.6911976911977, r: 0.6233679034195947
06/01/2019 10:39:07 step: 3475, epoch: 105, batch: 9, loss: 0.06927871704101562, acc: 96.875, f1: 96.30963496509715, r: 0.6951428209296358
06/01/2019 10:39:07 step: 3480, epoch: 105, batch: 14, loss: 0.008343487977981567, acc: 100.0, f1: 100.0, r: 0.7718979441467044
06/01/2019 10:39:07 step: 3485, epoch: 105, batch: 19, loss: 0.058771297335624695, acc: 96.875, f1: 96.0984393757503, r: 0.6763784437445628
06/01/2019 10:39:07 step: 3490, epoch: 105, batch: 24, loss: 0.09271927177906036, acc: 96.875, f1: 95.91649159663866, r: 0.7938634917970945
06/01/2019 10:39:07 step: 3495, epoch: 105, batch: 29, loss: 0.0035345330834388733, acc: 100.0, f1: 100.0, r: 0.607725523823466
06/01/2019 10:39:08 *** evaluating ***
06/01/2019 10:39:08 step: 106, epoch: 105, acc: 57.692307692307686, f1: 19.588241481179804, r: 0.2612044452576758
06/01/2019 10:39:08 *** epoch: 107 ***
06/01/2019 10:39:08 *** training ***
06/01/2019 10:39:08 step: 3503, epoch: 106, batch: 4, loss: 0.020193390548229218, acc: 98.4375, f1: 98.83439943141435, r: 0.7487387142868304
06/01/2019 10:39:08 step: 3508, epoch: 106, batch: 9, loss: 0.020519472658634186, acc: 100.0, f1: 100.0, r: 0.8062492273896791
06/01/2019 10:39:08 step: 3513, epoch: 106, batch: 14, loss: 0.0319538339972496, acc: 98.4375, f1: 99.26406926406926, r: 0.7876261454249452
06/01/2019 10:39:09 step: 3518, epoch: 106, batch: 19, loss: 0.051109813153743744, acc: 98.4375, f1: 97.71705292286406, r: 0.663993308814673
06/01/2019 10:39:09 step: 3523, epoch: 106, batch: 24, loss: 0.015255309641361237, acc: 100.0, f1: 100.0, r: 0.7333804320128904
06/01/2019 10:39:09 step: 3528, epoch: 106, batch: 29, loss: 0.021180443465709686, acc: 100.0, f1: 100.0, r: 0.6834332454688421
06/01/2019 10:39:09 *** evaluating ***
06/01/2019 10:39:09 step: 107, epoch: 106, acc: 58.97435897435898, f1: 19.924633651129945, r: 0.27077904730068086
06/01/2019 10:39:09 *** epoch: 108 ***
06/01/2019 10:39:09 *** training ***
06/01/2019 10:39:09 step: 3536, epoch: 107, batch: 4, loss: 0.002805955708026886, acc: 100.0, f1: 100.0, r: 0.6622809081852462
06/01/2019 10:39:10 step: 3541, epoch: 107, batch: 9, loss: 0.01600247621536255, acc: 100.0, f1: 100.0, r: 0.8122989491220558
06/01/2019 10:39:10 step: 3546, epoch: 107, batch: 14, loss: 0.01195717602968216, acc: 100.0, f1: 100.0, r: 0.6246865962505443
06/01/2019 10:39:10 step: 3551, epoch: 107, batch: 19, loss: 0.0060457512736320496, acc: 100.0, f1: 100.0, r: 0.6765845127661462
06/01/2019 10:39:10 step: 3556, epoch: 107, batch: 24, loss: 0.004646018147468567, acc: 100.0, f1: 100.0, r: 0.7202745775401298
06/01/2019 10:39:11 step: 3561, epoch: 107, batch: 29, loss: 0.007503598928451538, acc: 100.0, f1: 100.0, r: 0.7517186707793916
06/01/2019 10:39:11 *** evaluating ***
06/01/2019 10:39:11 step: 108, epoch: 107, acc: 58.54700854700855, f1: 19.536049192000867, r: 0.28137586024714145
06/01/2019 10:39:11 *** epoch: 109 ***
06/01/2019 10:39:11 *** training ***
06/01/2019 10:39:11 step: 3569, epoch: 108, batch: 4, loss: 0.0515800416469574, acc: 98.4375, f1: 97.03703703703704, r: 0.7464054797856797
06/01/2019 10:39:11 step: 3574, epoch: 108, batch: 9, loss: 0.04000663012266159, acc: 98.4375, f1: 98.66666666666667, r: 0.7586129344200883
06/01/2019 10:39:11 step: 3579, epoch: 108, batch: 14, loss: 0.0037696808576583862, acc: 100.0, f1: 100.0, r: 0.7337594134632434
06/01/2019 10:39:12 step: 3584, epoch: 108, batch: 19, loss: 0.029368512332439423, acc: 100.0, f1: 100.0, r: 0.7171845146025158
06/01/2019 10:39:12 step: 3589, epoch: 108, batch: 24, loss: 0.03387017920613289, acc: 100.0, f1: 100.0, r: 0.6906709048855616
06/01/2019 10:39:12 step: 3594, epoch: 108, batch: 29, loss: 0.006694719195365906, acc: 100.0, f1: 100.0, r: 0.7479691996203094
06/01/2019 10:39:12 *** evaluating ***
06/01/2019 10:39:12 step: 109, epoch: 108, acc: 59.401709401709404, f1: 19.839901477832512, r: 0.27368045638975363
06/01/2019 10:39:12 *** epoch: 110 ***
06/01/2019 10:39:12 *** training ***
06/01/2019 10:39:12 step: 3602, epoch: 109, batch: 4, loss: 0.004548214375972748, acc: 100.0, f1: 100.0, r: 0.6990666049077423
06/01/2019 10:39:13 step: 3607, epoch: 109, batch: 9, loss: 0.009474381804466248, acc: 100.0, f1: 100.0, r: 0.8156670744420428
06/01/2019 10:39:13 step: 3612, epoch: 109, batch: 14, loss: 0.009833283722400665, acc: 100.0, f1: 100.0, r: 0.6678932936918536
06/01/2019 10:39:13 step: 3617, epoch: 109, batch: 19, loss: 0.019606616348028183, acc: 98.4375, f1: 96.89223057644111, r: 0.6727602649890014
06/01/2019 10:39:13 step: 3622, epoch: 109, batch: 24, loss: 0.00930573046207428, acc: 100.0, f1: 100.0, r: 0.8466929740563914
06/01/2019 10:39:14 step: 3627, epoch: 109, batch: 29, loss: 0.022720180451869965, acc: 100.0, f1: 100.0, r: 0.7386076658714469
06/01/2019 10:39:14 *** evaluating ***
06/01/2019 10:39:14 step: 110, epoch: 109, acc: 58.54700854700855, f1: 19.402882205513784, r: 0.2778069030820396
06/01/2019 10:39:14 *** epoch: 111 ***
06/01/2019 10:39:14 *** training ***
06/01/2019 10:39:14 step: 3635, epoch: 110, batch: 4, loss: 0.02537526562809944, acc: 98.4375, f1: 94.94655004859086, r: 0.6816433860390864
06/01/2019 10:39:14 step: 3640, epoch: 110, batch: 9, loss: 0.03133399039506912, acc: 100.0, f1: 100.0, r: 0.6733800953228914
06/01/2019 10:39:14 step: 3645, epoch: 110, batch: 14, loss: 0.15006667375564575, acc: 93.75, f1: 91.05205770251362, r: 0.6858415678566081
06/01/2019 10:39:15 step: 3650, epoch: 110, batch: 19, loss: 0.018226206302642822, acc: 100.0, f1: 100.0, r: 0.6814951852477942
06/01/2019 10:39:15 step: 3655, epoch: 110, batch: 24, loss: 0.002268090844154358, acc: 100.0, f1: 100.0, r: 0.7817679161336439
06/01/2019 10:39:15 step: 3660, epoch: 110, batch: 29, loss: 0.025718316435813904, acc: 100.0, f1: 100.0, r: 0.7579901415414351
06/01/2019 10:39:15 *** evaluating ***
06/01/2019 10:39:15 step: 111, epoch: 110, acc: 59.401709401709404, f1: 21.722469186972234, r: 0.2758678284196183
06/01/2019 10:39:15 *** epoch: 112 ***
06/01/2019 10:39:15 *** training ***
06/01/2019 10:39:16 step: 3668, epoch: 111, batch: 4, loss: 0.011255256831645966, acc: 100.0, f1: 100.0, r: 0.7267332769058639
06/01/2019 10:39:16 step: 3673, epoch: 111, batch: 9, loss: 0.023627005517482758, acc: 100.0, f1: 100.0, r: 0.7713367334050594
06/01/2019 10:39:16 step: 3678, epoch: 111, batch: 14, loss: 0.005753554403781891, acc: 100.0, f1: 100.0, r: 0.7537546229422195
06/01/2019 10:39:16 step: 3683, epoch: 111, batch: 19, loss: 0.01026325672864914, acc: 100.0, f1: 100.0, r: 0.6935804825882654
06/01/2019 10:39:16 step: 3688, epoch: 111, batch: 24, loss: 0.0607367567718029, acc: 96.875, f1: 86.05510752688173, r: 0.7661607591202105
06/01/2019 10:39:17 step: 3693, epoch: 111, batch: 29, loss: 0.01586296409368515, acc: 100.0, f1: 100.0, r: 0.7596584040955386
06/01/2019 10:39:17 *** evaluating ***
06/01/2019 10:39:17 step: 112, epoch: 111, acc: 59.401709401709404, f1: 20.03049047995086, r: 0.26541578400983634
06/01/2019 10:39:17 *** epoch: 113 ***
06/01/2019 10:39:17 *** training ***
06/01/2019 10:39:17 step: 3701, epoch: 112, batch: 4, loss: 0.03956782817840576, acc: 98.4375, f1: 93.19727891156461, r: 0.6564943606578848
06/01/2019 10:39:17 step: 3706, epoch: 112, batch: 9, loss: 0.04123537614941597, acc: 98.4375, f1: 87.28813559322035, r: 0.8178913713589593
06/01/2019 10:39:18 step: 3711, epoch: 112, batch: 14, loss: 0.00282151997089386, acc: 100.0, f1: 100.0, r: 0.7013159794780053
06/01/2019 10:39:18 step: 3716, epoch: 112, batch: 19, loss: 0.0034336447715759277, acc: 100.0, f1: 100.0, r: 0.6194866867917703
06/01/2019 10:39:18 step: 3721, epoch: 112, batch: 24, loss: 0.007273055613040924, acc: 100.0, f1: 100.0, r: 0.7020180290876638
06/01/2019 10:39:18 step: 3726, epoch: 112, batch: 29, loss: 0.026767030358314514, acc: 100.0, f1: 100.0, r: 0.7141830351822247
06/01/2019 10:39:18 *** evaluating ***
06/01/2019 10:39:18 step: 113, epoch: 112, acc: 58.97435897435898, f1: 20.24588461338734, r: 0.27120797977774286
06/01/2019 10:39:18 *** epoch: 114 ***
06/01/2019 10:39:18 *** training ***
06/01/2019 10:39:19 step: 3734, epoch: 113, batch: 4, loss: 0.01326962560415268, acc: 100.0, f1: 100.0, r: 0.7480272782929891
06/01/2019 10:39:19 step: 3739, epoch: 113, batch: 9, loss: 0.027477312833070755, acc: 100.0, f1: 100.0, r: 0.7967809527330447
06/01/2019 10:39:19 step: 3744, epoch: 113, batch: 14, loss: 0.003714248538017273, acc: 100.0, f1: 100.0, r: 0.8262755493957471
06/01/2019 10:39:19 step: 3749, epoch: 113, batch: 19, loss: 0.0055844709277153015, acc: 100.0, f1: 100.0, r: 0.723525847456566
06/01/2019 10:39:20 step: 3754, epoch: 113, batch: 24, loss: 0.009892143309116364, acc: 100.0, f1: 100.0, r: 0.6006011530953946
06/01/2019 10:39:20 step: 3759, epoch: 113, batch: 29, loss: 0.024573206901550293, acc: 100.0, f1: 100.0, r: 0.8070431897235494
06/01/2019 10:39:20 *** evaluating ***
06/01/2019 10:39:20 step: 114, epoch: 113, acc: 58.97435897435898, f1: 20.899425287356323, r: 0.26928742188506627
06/01/2019 10:39:20 *** epoch: 115 ***
06/01/2019 10:39:20 *** training ***
06/01/2019 10:39:20 step: 3767, epoch: 114, batch: 4, loss: 0.0040621161460876465, acc: 100.0, f1: 100.0, r: 0.7910186045730598
06/01/2019 10:39:20 step: 3772, epoch: 114, batch: 9, loss: 0.03336675465106964, acc: 98.4375, f1: 98.57549857549857, r: 0.7652304967444221
06/01/2019 10:39:21 step: 3777, epoch: 114, batch: 14, loss: 0.02807813510298729, acc: 98.4375, f1: 94.87179487179486, r: 0.7955769441320095
06/01/2019 10:39:21 step: 3782, epoch: 114, batch: 19, loss: 0.04687940329313278, acc: 98.4375, f1: 97.1188475390156, r: 0.6940744334943627
06/01/2019 10:39:21 step: 3787, epoch: 114, batch: 24, loss: 0.038359373807907104, acc: 98.4375, f1: 98.77250409165302, r: 0.7492926648602382
06/01/2019 10:39:21 step: 3792, epoch: 114, batch: 29, loss: 0.03635912016034126, acc: 100.0, f1: 100.0, r: 0.7422699450667102
06/01/2019 10:39:21 *** evaluating ***
06/01/2019 10:39:22 step: 115, epoch: 114, acc: 58.97435897435898, f1: 22.02707615767317, r: 0.265727798078179
06/01/2019 10:39:22 *** epoch: 116 ***
06/01/2019 10:39:22 *** training ***
06/01/2019 10:39:22 step: 3800, epoch: 115, batch: 4, loss: 0.05446553975343704, acc: 98.4375, f1: 96.12903225806451, r: 0.5855874215676304
06/01/2019 10:39:22 step: 3805, epoch: 115, batch: 9, loss: 0.03158051520586014, acc: 100.0, f1: 100.0, r: 0.663187328034057
06/01/2019 10:39:22 step: 3810, epoch: 115, batch: 14, loss: 0.014417268335819244, acc: 100.0, f1: 100.0, r: 0.7868570371680234
06/01/2019 10:39:22 step: 3815, epoch: 115, batch: 19, loss: 0.005496129393577576, acc: 100.0, f1: 100.0, r: 0.6240363068537796
06/01/2019 10:39:23 step: 3820, epoch: 115, batch: 24, loss: 0.05412900820374489, acc: 96.875, f1: 95.5637980342937, r: 0.6046422696860035
06/01/2019 10:39:23 step: 3825, epoch: 115, batch: 29, loss: 0.05112404376268387, acc: 98.4375, f1: 98.33333333333334, r: 0.8049495832940683
06/01/2019 10:39:23 *** evaluating ***
06/01/2019 10:39:23 step: 116, epoch: 115, acc: 58.54700854700855, f1: 19.374027773485388, r: 0.2878936587128018
06/01/2019 10:39:23 *** epoch: 117 ***
06/01/2019 10:39:23 *** training ***
06/01/2019 10:39:23 step: 3833, epoch: 116, batch: 4, loss: 0.0063673704862594604, acc: 100.0, f1: 100.0, r: 0.6814458837396777
06/01/2019 10:39:24 step: 3838, epoch: 116, batch: 9, loss: 0.029798239469528198, acc: 98.4375, f1: 98.66946778711485, r: 0.7295077851862636
06/01/2019 10:39:24 step: 3843, epoch: 116, batch: 14, loss: 0.0219801664352417, acc: 100.0, f1: 100.0, r: 0.656134978527444
06/01/2019 10:39:24 step: 3848, epoch: 116, batch: 19, loss: 0.011375121772289276, acc: 100.0, f1: 100.0, r: 0.71309912955055
06/01/2019 10:39:24 step: 3853, epoch: 116, batch: 24, loss: 0.0077080875635147095, acc: 100.0, f1: 100.0, r: 0.6973842923224072
06/01/2019 10:39:24 step: 3858, epoch: 116, batch: 29, loss: 0.005934223532676697, acc: 100.0, f1: 100.0, r: 0.7069926510669976
06/01/2019 10:39:25 *** evaluating ***
06/01/2019 10:39:25 step: 117, epoch: 116, acc: 59.401709401709404, f1: 22.23831459572807, r: 0.2873380217733578
06/01/2019 10:39:25 *** epoch: 118 ***
06/01/2019 10:39:25 *** training ***
06/01/2019 10:39:25 step: 3866, epoch: 117, batch: 4, loss: 0.05825246125459671, acc: 96.875, f1: 97.53720997400596, r: 0.7438382853361397
06/01/2019 10:39:25 step: 3871, epoch: 117, batch: 9, loss: 0.0055022165179252625, acc: 100.0, f1: 100.0, r: 0.6771232127415847
06/01/2019 10:39:25 step: 3876, epoch: 117, batch: 14, loss: 0.005407080054283142, acc: 100.0, f1: 100.0, r: 0.7447518283924709
06/01/2019 10:39:26 step: 3881, epoch: 117, batch: 19, loss: 0.005372874438762665, acc: 100.0, f1: 100.0, r: 0.7912523657974706
06/01/2019 10:39:26 step: 3886, epoch: 117, batch: 24, loss: 0.002605915069580078, acc: 100.0, f1: 100.0, r: 0.7805085715855143
06/01/2019 10:39:26 step: 3891, epoch: 117, batch: 29, loss: 0.007120691239833832, acc: 100.0, f1: 100.0, r: 0.6365731695006989
06/01/2019 10:39:26 *** evaluating ***
06/01/2019 10:39:26 step: 118, epoch: 117, acc: 58.119658119658126, f1: 22.168906810035843, r: 0.27110917653007516
06/01/2019 10:39:26 *** epoch: 119 ***
06/01/2019 10:39:26 *** training ***
06/01/2019 10:39:27 step: 3899, epoch: 118, batch: 4, loss: 0.005078993737697601, acc: 100.0, f1: 100.0, r: 0.7054531977545011
06/01/2019 10:39:27 step: 3904, epoch: 118, batch: 9, loss: 0.048280518501996994, acc: 96.875, f1: 84.25740925740925, r: 0.7861139603287643
06/01/2019 10:39:27 step: 3909, epoch: 118, batch: 14, loss: 0.002404622733592987, acc: 100.0, f1: 100.0, r: 0.5521441451372324
06/01/2019 10:39:27 step: 3914, epoch: 118, batch: 19, loss: 0.029059167951345444, acc: 98.4375, f1: 87.12121212121212, r: 0.6964623805136081
06/01/2019 10:39:27 step: 3919, epoch: 118, batch: 24, loss: 0.027417927980422974, acc: 98.4375, f1: 96.6137566137566, r: 0.6843693776711411
06/01/2019 10:39:28 step: 3924, epoch: 118, batch: 29, loss: 0.021891236305236816, acc: 100.0, f1: 100.0, r: 0.7711534616678741
06/01/2019 10:39:28 *** evaluating ***
06/01/2019 10:39:28 step: 119, epoch: 118, acc: 59.401709401709404, f1: 22.331373073153344, r: 0.2759983499825681
06/01/2019 10:39:28 *** epoch: 120 ***
06/01/2019 10:39:28 *** training ***
06/01/2019 10:39:28 step: 3932, epoch: 119, batch: 4, loss: 0.014835633337497711, acc: 100.0, f1: 100.0, r: 0.7093375054795455
06/01/2019 10:39:28 step: 3937, epoch: 119, batch: 9, loss: 0.012667324393987656, acc: 100.0, f1: 100.0, r: 0.8166624276770462
06/01/2019 10:39:29 step: 3942, epoch: 119, batch: 14, loss: 0.06251421570777893, acc: 96.875, f1: 93.29607174434761, r: 0.766138504622244
06/01/2019 10:39:29 step: 3947, epoch: 119, batch: 19, loss: 0.009103640913963318, acc: 100.0, f1: 100.0, r: 0.7314448683805336
06/01/2019 10:39:29 step: 3952, epoch: 119, batch: 24, loss: 0.013964518904685974, acc: 100.0, f1: 100.0, r: 0.7424683108683422
06/01/2019 10:39:29 step: 3957, epoch: 119, batch: 29, loss: 0.023295730352401733, acc: 100.0, f1: 100.0, r: 0.7817334637465079
06/01/2019 10:39:29 *** evaluating ***
06/01/2019 10:39:29 step: 120, epoch: 119, acc: 59.82905982905983, f1: 22.413793103448278, r: 0.2738666418913026
06/01/2019 10:39:29 *** epoch: 121 ***
06/01/2019 10:39:29 *** training ***
06/01/2019 10:39:30 step: 3965, epoch: 120, batch: 4, loss: 0.0400356650352478, acc: 98.4375, f1: 96.36363636363636, r: 0.8181207296408214
06/01/2019 10:39:30 step: 3970, epoch: 120, batch: 9, loss: 0.0037861689925193787, acc: 100.0, f1: 100.0, r: 0.7262686287461756
06/01/2019 10:39:30 step: 3975, epoch: 120, batch: 14, loss: 0.016060978174209595, acc: 100.0, f1: 100.0, r: 0.7154817105691065
06/01/2019 10:39:30 step: 3980, epoch: 120, batch: 19, loss: 0.009880110621452332, acc: 100.0, f1: 100.0, r: 0.8073286646992845
06/01/2019 10:39:30 step: 3985, epoch: 120, batch: 24, loss: 0.00824001431465149, acc: 100.0, f1: 100.0, r: 0.666441071912644
06/01/2019 10:39:31 step: 3990, epoch: 120, batch: 29, loss: 0.0029803141951560974, acc: 100.0, f1: 100.0, r: 0.6661130970175922
06/01/2019 10:39:31 *** evaluating ***
06/01/2019 10:39:31 step: 121, epoch: 120, acc: 58.54700854700855, f1: 19.455967504969323, r: 0.2719161563363916
06/01/2019 10:39:31 *** epoch: 122 ***
06/01/2019 10:39:31 *** training ***
06/01/2019 10:39:31 step: 3998, epoch: 121, batch: 4, loss: 0.016012810170650482, acc: 100.0, f1: 100.0, r: 0.7050926509906917
06/01/2019 10:39:31 step: 4003, epoch: 121, batch: 9, loss: 0.04150019586086273, acc: 98.4375, f1: 99.26962872793669, r: 0.6739107448091401
06/01/2019 10:39:32 step: 4008, epoch: 121, batch: 14, loss: 0.012923642992973328, acc: 100.0, f1: 100.0, r: 0.7452233108972126
06/01/2019 10:39:32 step: 4013, epoch: 121, batch: 19, loss: 0.01312277466058731, acc: 98.4375, f1: 98.35286664554957, r: 0.6960683903651226
06/01/2019 10:39:32 step: 4018, epoch: 121, batch: 24, loss: 0.06772241741418839, acc: 95.3125, f1: 92.78489475500345, r: 0.7662769368890946
06/01/2019 10:39:32 step: 4023, epoch: 121, batch: 29, loss: 0.04061891511082649, acc: 98.4375, f1: 95.0, r: 0.8498902944480199
06/01/2019 10:39:32 *** evaluating ***
06/01/2019 10:39:32 step: 122, epoch: 121, acc: 59.401709401709404, f1: 22.219711343603993, r: 0.27637553314861746
06/01/2019 10:39:32 *** epoch: 123 ***
06/01/2019 10:39:32 *** training ***
06/01/2019 10:39:33 step: 4031, epoch: 122, batch: 4, loss: 0.00477956235408783, acc: 100.0, f1: 100.0, r: 0.8015940943586793
06/01/2019 10:39:33 step: 4036, epoch: 122, batch: 9, loss: 0.004732251167297363, acc: 100.0, f1: 100.0, r: 0.6678014983897141
06/01/2019 10:39:33 step: 4041, epoch: 122, batch: 14, loss: 0.02887207455933094, acc: 98.4375, f1: 98.62098685628098, r: 0.6819620065011317
06/01/2019 10:39:33 step: 4046, epoch: 122, batch: 19, loss: 0.023439638316631317, acc: 100.0, f1: 100.0, r: 0.7135990765658244
06/01/2019 10:39:34 step: 4051, epoch: 122, batch: 24, loss: 0.09441770613193512, acc: 96.875, f1: 92.49915739804516, r: 0.7228094055753339
06/01/2019 10:39:34 step: 4056, epoch: 122, batch: 29, loss: 0.012366529554128647, acc: 100.0, f1: 100.0, r: 0.5468196945642613
06/01/2019 10:39:34 *** evaluating ***
06/01/2019 10:39:34 step: 123, epoch: 122, acc: 58.54700854700855, f1: 20.369267682091632, r: 0.267183595024711
06/01/2019 10:39:34 *** epoch: 124 ***
06/01/2019 10:39:34 *** training ***
06/01/2019 10:39:34 step: 4064, epoch: 123, batch: 4, loss: 0.009933635592460632, acc: 100.0, f1: 100.0, r: 0.7427349760128894
06/01/2019 10:39:34 step: 4069, epoch: 123, batch: 9, loss: 0.024402335286140442, acc: 98.4375, f1: 98.37199837199837, r: 0.6366783638267022
06/01/2019 10:39:35 step: 4074, epoch: 123, batch: 14, loss: 0.013070069253444672, acc: 100.0, f1: 100.0, r: 0.8099289506678329
06/01/2019 10:39:35 step: 4079, epoch: 123, batch: 19, loss: 0.004550665616989136, acc: 100.0, f1: 100.0, r: 0.6839464033960313
06/01/2019 10:39:35 step: 4084, epoch: 123, batch: 24, loss: 0.009079508483409882, acc: 100.0, f1: 100.0, r: 0.6768033247676399
06/01/2019 10:39:35 step: 4089, epoch: 123, batch: 29, loss: 0.020717855542898178, acc: 100.0, f1: 100.0, r: 0.70418351508643
06/01/2019 10:39:36 *** evaluating ***
06/01/2019 10:39:36 step: 124, epoch: 123, acc: 56.837606837606835, f1: 22.406681700263086, r: 0.2763079995071969
06/01/2019 10:39:36 *** epoch: 125 ***
06/01/2019 10:39:36 *** training ***
06/01/2019 10:39:36 step: 4097, epoch: 124, batch: 4, loss: 0.01889631152153015, acc: 100.0, f1: 100.0, r: 0.7712847129142513
06/01/2019 10:39:36 step: 4102, epoch: 124, batch: 9, loss: 0.009891893714666367, acc: 100.0, f1: 100.0, r: 0.7092701874775482
06/01/2019 10:39:36 step: 4107, epoch: 124, batch: 14, loss: 0.0037465691566467285, acc: 100.0, f1: 100.0, r: 0.8341663706181591
06/01/2019 10:39:37 step: 4112, epoch: 124, batch: 19, loss: 0.0049772709608078, acc: 100.0, f1: 100.0, r: 0.6036030433839271
06/01/2019 10:39:37 step: 4117, epoch: 124, batch: 24, loss: 0.001776270568370819, acc: 100.0, f1: 100.0, r: 0.7264078507160943
06/01/2019 10:39:37 step: 4122, epoch: 124, batch: 29, loss: 0.019483856856822968, acc: 100.0, f1: 100.0, r: 0.7941164326175338
06/01/2019 10:39:37 *** evaluating ***
06/01/2019 10:39:37 step: 125, epoch: 124, acc: 59.401709401709404, f1: 22.16126785092302, r: 0.27856827708738674
06/01/2019 10:39:37 *** epoch: 126 ***
06/01/2019 10:39:37 *** training ***
06/01/2019 10:39:37 step: 4130, epoch: 125, batch: 4, loss: 0.0031624063849449158, acc: 100.0, f1: 100.0, r: 0.7842868647657513
06/01/2019 10:39:38 step: 4135, epoch: 125, batch: 9, loss: 0.04031934216618538, acc: 98.4375, f1: 99.10934020860189, r: 0.6686965875515406
06/01/2019 10:39:38 step: 4140, epoch: 125, batch: 14, loss: 0.003764607012271881, acc: 100.0, f1: 100.0, r: 0.7846506758571102
06/01/2019 10:39:38 step: 4145, epoch: 125, batch: 19, loss: 0.017967551946640015, acc: 100.0, f1: 100.0, r: 0.8046569873686278
06/01/2019 10:39:38 step: 4150, epoch: 125, batch: 24, loss: 0.006940200924873352, acc: 100.0, f1: 100.0, r: 0.7969124111876995
06/01/2019 10:39:39 step: 4155, epoch: 125, batch: 29, loss: 0.01679399609565735, acc: 100.0, f1: 100.0, r: 0.821260790064068
06/01/2019 10:39:39 *** evaluating ***
06/01/2019 10:39:39 step: 126, epoch: 125, acc: 58.97435897435898, f1: 21.39576606958517, r: 0.2634221733102389
06/01/2019 10:39:39 *** epoch: 127 ***
06/01/2019 10:39:39 *** training ***
06/01/2019 10:39:39 step: 4163, epoch: 126, batch: 4, loss: 0.015768639743328094, acc: 100.0, f1: 100.0, r: 0.7588223483276977
06/01/2019 10:39:39 step: 4168, epoch: 126, batch: 9, loss: 0.023700661957263947, acc: 100.0, f1: 100.0, r: 0.7719790816268512
06/01/2019 10:39:39 step: 4173, epoch: 126, batch: 14, loss: 0.09235077351331711, acc: 98.4375, f1: 98.50649350649351, r: 0.7457105074355203
06/01/2019 10:39:40 step: 4178, epoch: 126, batch: 19, loss: 0.03741297125816345, acc: 98.4375, f1: 98.0952380952381, r: 0.6828522386918652
06/01/2019 10:39:40 step: 4183, epoch: 126, batch: 24, loss: 0.008990250527858734, acc: 100.0, f1: 100.0, r: 0.7767191269405979
06/01/2019 10:39:40 step: 4188, epoch: 126, batch: 29, loss: 0.013179481029510498, acc: 98.4375, f1: 98.85714285714286, r: 0.658112862315162
06/01/2019 10:39:40 *** evaluating ***
06/01/2019 10:39:40 step: 127, epoch: 126, acc: 57.26495726495726, f1: 18.88976171772715, r: 0.2720797310524181
06/01/2019 10:39:40 *** epoch: 128 ***
06/01/2019 10:39:40 *** training ***
06/01/2019 10:39:41 step: 4196, epoch: 127, batch: 4, loss: 0.030876267701387405, acc: 98.4375, f1: 98.22082679225537, r: 0.6603028368908205
06/01/2019 10:39:41 step: 4201, epoch: 127, batch: 9, loss: 0.01712384819984436, acc: 100.0, f1: 100.0, r: 0.6345739151380494
06/01/2019 10:39:41 step: 4206, epoch: 127, batch: 14, loss: 0.004582658410072327, acc: 100.0, f1: 100.0, r: 0.7858031069914547
06/01/2019 10:39:41 step: 4211, epoch: 127, batch: 19, loss: 0.022761769592761993, acc: 98.4375, f1: 99.12280701754386, r: 0.8280592907214599
06/01/2019 10:39:41 step: 4216, epoch: 127, batch: 24, loss: 0.060107313096523285, acc: 96.875, f1: 94.59968602825747, r: 0.6870641768735231
06/01/2019 10:39:42 step: 4221, epoch: 127, batch: 29, loss: 0.11420612782239914, acc: 95.3125, f1: 93.33484345112252, r: 0.7655809464823868
06/01/2019 10:39:42 *** evaluating ***
06/01/2019 10:39:42 step: 128, epoch: 127, acc: 58.54700854700855, f1: 20.310649433895794, r: 0.26221133463450225
06/01/2019 10:39:42 *** epoch: 129 ***
06/01/2019 10:39:42 *** training ***
06/01/2019 10:39:42 step: 4229, epoch: 128, batch: 4, loss: 0.015707246959209442, acc: 98.4375, f1: 99.03044993182851, r: 0.6855831231587
06/01/2019 10:39:42 step: 4234, epoch: 128, batch: 9, loss: 0.003379605710506439, acc: 100.0, f1: 100.0, r: 0.6739868022887597
06/01/2019 10:39:43 step: 4239, epoch: 128, batch: 14, loss: 0.016377337276935577, acc: 100.0, f1: 100.0, r: 0.7101266371200704
06/01/2019 10:39:43 step: 4244, epoch: 128, batch: 19, loss: 0.022639572620391846, acc: 100.0, f1: 100.0, r: 0.6912704737656689
06/01/2019 10:39:43 step: 4249, epoch: 128, batch: 24, loss: 0.004348456859588623, acc: 100.0, f1: 100.0, r: 0.785233449067702
06/01/2019 10:39:43 step: 4254, epoch: 128, batch: 29, loss: 0.02600301057100296, acc: 100.0, f1: 100.0, r: 0.6923522299194452
06/01/2019 10:39:43 *** evaluating ***
06/01/2019 10:39:43 step: 129, epoch: 128, acc: 58.97435897435898, f1: 20.515749601275914, r: 0.27090232006511705
06/01/2019 10:39:43 *** epoch: 130 ***
06/01/2019 10:39:43 *** training ***
06/01/2019 10:39:44 step: 4262, epoch: 129, batch: 4, loss: 0.002728760242462158, acc: 100.0, f1: 100.0, r: 0.6931663738768227
06/01/2019 10:39:44 step: 4267, epoch: 129, batch: 9, loss: 0.05283206328749657, acc: 98.4375, f1: 98.97435897435898, r: 0.7532936366031355
06/01/2019 10:39:44 step: 4272, epoch: 129, batch: 14, loss: 0.0036262348294258118, acc: 100.0, f1: 100.0, r: 0.861264094423595
06/01/2019 10:39:44 step: 4277, epoch: 129, batch: 19, loss: 0.003870539367198944, acc: 100.0, f1: 100.0, r: 0.7258925930637655
06/01/2019 10:39:45 step: 4282, epoch: 129, batch: 24, loss: 0.014599286019802094, acc: 100.0, f1: 100.0, r: 0.7973470652983815
06/01/2019 10:39:45 step: 4287, epoch: 129, batch: 29, loss: 0.0012753978371620178, acc: 100.0, f1: 100.0, r: 0.7445606835850785
06/01/2019 10:39:45 *** evaluating ***
06/01/2019 10:39:45 step: 130, epoch: 129, acc: 57.26495726495726, f1: 21.760066195490634, r: 0.27717854939279307
06/01/2019 10:39:45 *** epoch: 131 ***
06/01/2019 10:39:45 *** training ***
06/01/2019 10:39:45 step: 4295, epoch: 130, batch: 4, loss: 0.0116049125790596, acc: 100.0, f1: 100.0, r: 0.7932831278749686
06/01/2019 10:39:45 step: 4300, epoch: 130, batch: 9, loss: 0.005951501429080963, acc: 100.0, f1: 100.0, r: 0.6630360716779663
06/01/2019 10:39:46 step: 4305, epoch: 130, batch: 14, loss: 0.021516263484954834, acc: 100.0, f1: 100.0, r: 0.621333649445071
06/01/2019 10:39:46 step: 4310, epoch: 130, batch: 19, loss: 0.02367079257965088, acc: 100.0, f1: 100.0, r: 0.6841421897324367
06/01/2019 10:39:46 step: 4315, epoch: 130, batch: 24, loss: 0.05030839145183563, acc: 96.875, f1: 96.17673331959047, r: 0.6127986105439169
06/01/2019 10:39:46 step: 4320, epoch: 130, batch: 29, loss: 0.007797494530677795, acc: 100.0, f1: 100.0, r: 0.8016003081664177
06/01/2019 10:39:46 *** evaluating ***
06/01/2019 10:39:47 step: 131, epoch: 130, acc: 58.54700854700855, f1: 19.510796221322536, r: 0.2619635017877083
06/01/2019 10:39:47 *** epoch: 132 ***
06/01/2019 10:39:47 *** training ***
06/01/2019 10:39:47 step: 4328, epoch: 131, batch: 4, loss: 0.0474366657435894, acc: 98.4375, f1: 99.22727711774365, r: 0.7225343579216457
06/01/2019 10:39:47 step: 4333, epoch: 131, batch: 9, loss: 0.01852881908416748, acc: 98.4375, f1: 99.3476165764697, r: 0.6963302986464723
06/01/2019 10:39:47 step: 4338, epoch: 131, batch: 14, loss: 0.00797571986913681, acc: 100.0, f1: 100.0, r: 0.6969886001091335
06/01/2019 10:39:47 step: 4343, epoch: 131, batch: 19, loss: 0.039174117147922516, acc: 96.875, f1: 98.36453201970443, r: 0.7070629801090935
06/01/2019 10:39:48 step: 4348, epoch: 131, batch: 24, loss: 0.0317634716629982, acc: 100.0, f1: 100.0, r: 0.8101785847438976
06/01/2019 10:39:48 step: 4353, epoch: 131, batch: 29, loss: 0.011884383857250214, acc: 100.0, f1: 100.0, r: 0.7567763659661142
06/01/2019 10:39:48 *** evaluating ***
06/01/2019 10:39:48 step: 132, epoch: 131, acc: 58.97435897435898, f1: 22.605807137845975, r: 0.26038146415448393
06/01/2019 10:39:48 *** epoch: 133 ***
06/01/2019 10:39:48 *** training ***
06/01/2019 10:39:48 step: 4361, epoch: 132, batch: 4, loss: 0.03508898988366127, acc: 98.4375, f1: 87.2340425531915, r: 0.7894101254416023
06/01/2019 10:39:49 step: 4366, epoch: 132, batch: 9, loss: 0.06286023557186127, acc: 98.4375, f1: 97.22222222222221, r: 0.7786205710119758
06/01/2019 10:39:49 step: 4371, epoch: 132, batch: 14, loss: 0.016428619623184204, acc: 100.0, f1: 100.0, r: 0.8060789437879238
06/01/2019 10:39:49 step: 4376, epoch: 132, batch: 19, loss: 0.01742561161518097, acc: 100.0, f1: 100.0, r: 0.7411069712512779
06/01/2019 10:39:49 step: 4381, epoch: 132, batch: 24, loss: 0.01582125574350357, acc: 100.0, f1: 100.0, r: 0.7767827436538732
06/01/2019 10:39:49 step: 4386, epoch: 132, batch: 29, loss: 0.03130237013101578, acc: 96.875, f1: 84.02885682574916, r: 0.6592673180220826
06/01/2019 10:39:50 *** evaluating ***
06/01/2019 10:39:50 step: 133, epoch: 132, acc: 59.401709401709404, f1: 21.26923076923077, r: 0.26446500311902854
06/01/2019 10:39:50 *** epoch: 134 ***
06/01/2019 10:39:50 *** training ***
06/01/2019 10:39:50 step: 4394, epoch: 133, batch: 4, loss: 0.002538137137889862, acc: 100.0, f1: 100.0, r: 0.6536830356858663
06/01/2019 10:39:50 step: 4399, epoch: 133, batch: 9, loss: 0.003980293869972229, acc: 100.0, f1: 100.0, r: 0.7622137847539515
06/01/2019 10:39:50 step: 4404, epoch: 133, batch: 14, loss: 0.044811539351940155, acc: 96.875, f1: 98.36148007590133, r: 0.8288965677207117
06/01/2019 10:39:51 step: 4409, epoch: 133, batch: 19, loss: 0.005732990801334381, acc: 100.0, f1: 100.0, r: 0.7733808027891677
06/01/2019 10:39:51 step: 4414, epoch: 133, batch: 24, loss: 0.005267437547445297, acc: 100.0, f1: 100.0, r: 0.7822401215591046
06/01/2019 10:39:51 step: 4419, epoch: 133, batch: 29, loss: 0.014633182436227798, acc: 100.0, f1: 100.0, r: 0.662200129498731
06/01/2019 10:39:51 *** evaluating ***
06/01/2019 10:39:51 step: 134, epoch: 133, acc: 58.119658119658126, f1: 19.350539811066128, r: 0.26240547839455197
06/01/2019 10:39:51 *** epoch: 135 ***
06/01/2019 10:39:51 *** training ***
06/01/2019 10:39:51 step: 4427, epoch: 134, batch: 4, loss: 0.01936686784029007, acc: 100.0, f1: 100.0, r: 0.7325626772135421
06/01/2019 10:39:52 step: 4432, epoch: 134, batch: 9, loss: 0.009548403322696686, acc: 100.0, f1: 100.0, r: 0.7896547221781125
06/01/2019 10:39:52 step: 4437, epoch: 134, batch: 14, loss: 0.043482404202222824, acc: 98.4375, f1: 96.66666666666667, r: 0.8067202020028431
06/01/2019 10:39:52 step: 4442, epoch: 134, batch: 19, loss: 0.004280708730220795, acc: 100.0, f1: 100.0, r: 0.8210235527876097
06/01/2019 10:39:52 step: 4447, epoch: 134, batch: 24, loss: 0.01817462593317032, acc: 100.0, f1: 100.0, r: 0.8067686172929689
06/01/2019 10:39:53 step: 4452, epoch: 134, batch: 29, loss: 0.015074919909238815, acc: 100.0, f1: 100.0, r: 0.668079112940625
06/01/2019 10:39:53 *** evaluating ***
06/01/2019 10:39:53 step: 135, epoch: 134, acc: 58.54700854700855, f1: 19.52168156239656, r: 0.26715131716065377
06/01/2019 10:39:53 *** epoch: 136 ***
06/01/2019 10:39:53 *** training ***
06/01/2019 10:39:53 step: 4460, epoch: 135, batch: 4, loss: 0.019585050642490387, acc: 98.4375, f1: 94.93414387031407, r: 0.7571666888138948
06/01/2019 10:39:53 step: 4465, epoch: 135, batch: 9, loss: 0.009390667080879211, acc: 100.0, f1: 100.0, r: 0.7131980459618456
06/01/2019 10:39:53 step: 4470, epoch: 135, batch: 14, loss: 0.004130363464355469, acc: 100.0, f1: 100.0, r: 0.8206867497782816
06/01/2019 10:39:54 step: 4475, epoch: 135, batch: 19, loss: 0.022305898368358612, acc: 98.4375, f1: 98.68247694334651, r: 0.7875737445287998
06/01/2019 10:39:54 step: 4480, epoch: 135, batch: 24, loss: 0.031230930238962173, acc: 98.4375, f1: 97.78325123152709, r: 0.740768465050601
06/01/2019 10:39:54 step: 4485, epoch: 135, batch: 29, loss: 0.008215833455324173, acc: 100.0, f1: 100.0, r: 0.8002110979576679
06/01/2019 10:39:54 *** evaluating ***
06/01/2019 10:39:54 step: 136, epoch: 135, acc: 58.97435897435898, f1: 20.57047791893527, r: 0.2692846984926393
06/01/2019 10:39:54 *** epoch: 137 ***
06/01/2019 10:39:54 *** training ***
06/01/2019 10:39:55 step: 4493, epoch: 136, batch: 4, loss: 0.02858847752213478, acc: 98.4375, f1: 97.70855710705335, r: 0.6784807106402253
06/01/2019 10:39:55 step: 4498, epoch: 136, batch: 9, loss: 0.002513304352760315, acc: 100.0, f1: 100.0, r: 0.6797752898607403
06/01/2019 10:39:55 step: 4503, epoch: 136, batch: 14, loss: 0.03531736135482788, acc: 98.4375, f1: 97.25274725274726, r: 0.7428208116315685
06/01/2019 10:39:55 step: 4508, epoch: 136, batch: 19, loss: 0.0032285302877426147, acc: 100.0, f1: 100.0, r: 0.7920323411796575
06/01/2019 10:39:55 step: 4513, epoch: 136, batch: 24, loss: 0.010926507413387299, acc: 100.0, f1: 100.0, r: 0.8201697961505084
06/01/2019 10:39:56 step: 4518, epoch: 136, batch: 29, loss: 0.01879250258207321, acc: 100.0, f1: 100.0, r: 0.7856341615820214
06/01/2019 10:39:56 *** evaluating ***
06/01/2019 10:39:56 step: 137, epoch: 136, acc: 58.54700854700855, f1: 19.451015178395426, r: 0.26575281138180484
06/01/2019 10:39:56 *** epoch: 138 ***
06/01/2019 10:39:56 *** training ***
06/01/2019 10:39:56 step: 4526, epoch: 137, batch: 4, loss: 0.0005816668272018433, acc: 100.0, f1: 100.0, r: 0.715332597373798
06/01/2019 10:39:56 step: 4531, epoch: 137, batch: 9, loss: 0.026302359998226166, acc: 98.4375, f1: 95.0, r: 0.7668205347129928
06/01/2019 10:39:56 step: 4536, epoch: 137, batch: 14, loss: 0.01284249871969223, acc: 100.0, f1: 100.0, r: 0.7232204446606973
06/01/2019 10:39:57 step: 4541, epoch: 137, batch: 19, loss: 0.06654024124145508, acc: 96.875, f1: 97.88912362000818, r: 0.7758630759724268
06/01/2019 10:39:57 step: 4546, epoch: 137, batch: 24, loss: 0.002660617232322693, acc: 100.0, f1: 100.0, r: 0.7848155908649314
06/01/2019 10:39:57 step: 4551, epoch: 137, batch: 29, loss: 0.016368448734283447, acc: 100.0, f1: 100.0, r: 0.8187513409889193
06/01/2019 10:39:57 *** evaluating ***
06/01/2019 10:39:57 step: 138, epoch: 137, acc: 58.119658119658126, f1: 19.384784318867183, r: 0.2551615679329401
06/01/2019 10:39:57 *** epoch: 139 ***
06/01/2019 10:39:57 *** training ***
06/01/2019 10:39:58 step: 4559, epoch: 138, batch: 4, loss: 0.0077266693115234375, acc: 100.0, f1: 100.0, r: 0.7578587564043181
06/01/2019 10:39:58 step: 4564, epoch: 138, batch: 9, loss: 0.014175690710544586, acc: 100.0, f1: 100.0, r: 0.7873993449181765
06/01/2019 10:39:58 step: 4569, epoch: 138, batch: 14, loss: 0.09058865904808044, acc: 96.875, f1: 96.58700322234156, r: 0.7855486493119679
06/01/2019 10:39:58 step: 4574, epoch: 138, batch: 19, loss: 0.001073002815246582, acc: 100.0, f1: 100.0, r: 0.755609909346719
06/01/2019 10:39:58 step: 4579, epoch: 138, batch: 24, loss: 0.006734684109687805, acc: 100.0, f1: 100.0, r: 0.7266114858945162
06/01/2019 10:39:59 step: 4584, epoch: 138, batch: 29, loss: 0.011702671647071838, acc: 100.0, f1: 100.0, r: 0.7708122224739794
06/01/2019 10:39:59 *** evaluating ***
06/01/2019 10:39:59 step: 139, epoch: 138, acc: 58.119658119658126, f1: 20.162060810369383, r: 0.26233300977363827
06/01/2019 10:39:59 *** epoch: 140 ***
06/01/2019 10:39:59 *** training ***
06/01/2019 10:39:59 step: 4592, epoch: 139, batch: 4, loss: 0.04406565800309181, acc: 98.4375, f1: 87.29508196721312, r: 0.6283590211702825
06/01/2019 10:39:59 step: 4597, epoch: 139, batch: 9, loss: 0.016586050391197205, acc: 100.0, f1: 100.0, r: 0.7119986855332894
06/01/2019 10:40:00 step: 4602, epoch: 139, batch: 14, loss: 0.008195973932743073, acc: 100.0, f1: 100.0, r: 0.6295196364675453
06/01/2019 10:40:00 step: 4607, epoch: 139, batch: 19, loss: 0.0013473182916641235, acc: 100.0, f1: 100.0, r: 0.8380846776189363
06/01/2019 10:40:00 step: 4612, epoch: 139, batch: 24, loss: 0.01159757375717163, acc: 100.0, f1: 100.0, r: 0.8334084503179915
06/01/2019 10:40:00 step: 4617, epoch: 139, batch: 29, loss: 0.008307322859764099, acc: 100.0, f1: 100.0, r: 0.7157395064806248
06/01/2019 10:40:00 *** evaluating ***
06/01/2019 10:40:00 step: 140, epoch: 139, acc: 58.97435897435898, f1: 20.45945843446602, r: 0.26066146195115125
06/01/2019 10:40:00 *** epoch: 141 ***
06/01/2019 10:40:00 *** training ***
06/01/2019 10:40:01 step: 4625, epoch: 140, batch: 4, loss: 0.0018533393740653992, acc: 100.0, f1: 100.0, r: 0.7508490247996178
06/01/2019 10:40:01 step: 4630, epoch: 140, batch: 9, loss: 0.002097770571708679, acc: 100.0, f1: 100.0, r: 0.7765028625772392
06/01/2019 10:40:01 step: 4635, epoch: 140, batch: 14, loss: 0.0020008906722068787, acc: 100.0, f1: 100.0, r: 0.705507009789467
06/01/2019 10:40:01 step: 4640, epoch: 140, batch: 19, loss: 0.0018470361828804016, acc: 100.0, f1: 100.0, r: 0.6258409397690293
06/01/2019 10:40:02 step: 4645, epoch: 140, batch: 24, loss: 0.01249624788761139, acc: 100.0, f1: 100.0, r: 0.5384912188283617
06/01/2019 10:40:02 step: 4650, epoch: 140, batch: 29, loss: 0.020381256937980652, acc: 98.4375, f1: 98.19245082402978, r: 0.6815443511919873
06/01/2019 10:40:02 *** evaluating ***
06/01/2019 10:40:02 step: 141, epoch: 140, acc: 58.54700854700855, f1: 20.91402631091347, r: 0.2627477728825468
06/01/2019 10:40:02 *** epoch: 142 ***
06/01/2019 10:40:02 *** training ***
06/01/2019 10:40:02 step: 4658, epoch: 141, batch: 4, loss: 0.0028717145323753357, acc: 100.0, f1: 100.0, r: 0.7953153194010432
06/01/2019 10:40:02 step: 4663, epoch: 141, batch: 9, loss: 0.004544995725154877, acc: 100.0, f1: 100.0, r: 0.7960303476055149
06/01/2019 10:40:03 step: 4668, epoch: 141, batch: 14, loss: 0.0029482245445251465, acc: 100.0, f1: 100.0, r: 0.6820354521898392
06/01/2019 10:40:03 step: 4673, epoch: 141, batch: 19, loss: 0.005917072296142578, acc: 100.0, f1: 100.0, r: 0.7826519658973867
06/01/2019 10:40:03 step: 4678, epoch: 141, batch: 24, loss: 0.011269152164459229, acc: 100.0, f1: 100.0, r: 0.6941121849812062
06/01/2019 10:40:03 step: 4683, epoch: 141, batch: 29, loss: 0.058441754430532455, acc: 96.875, f1: 95.4764811907669, r: 0.683367767794036
06/01/2019 10:40:03 *** evaluating ***
06/01/2019 10:40:04 step: 142, epoch: 141, acc: 58.119658119658126, f1: 19.2814267570986, r: 0.2643145041728492
06/01/2019 10:40:04 *** epoch: 143 ***
06/01/2019 10:40:04 *** training ***
06/01/2019 10:40:04 step: 4691, epoch: 142, batch: 4, loss: 0.0008864104747772217, acc: 100.0, f1: 100.0, r: 0.7319819498981134
06/01/2019 10:40:04 step: 4696, epoch: 142, batch: 9, loss: 0.02183983288705349, acc: 100.0, f1: 100.0, r: 0.7919360360490455
06/01/2019 10:40:04 step: 4701, epoch: 142, batch: 14, loss: 0.041641395539045334, acc: 98.4375, f1: 98.04639804639805, r: 0.6460369415870475
06/01/2019 10:40:04 step: 4706, epoch: 142, batch: 19, loss: 0.012940697371959686, acc: 100.0, f1: 100.0, r: 0.6722168571732723
06/01/2019 10:40:05 step: 4711, epoch: 142, batch: 24, loss: 0.046139076352119446, acc: 98.4375, f1: 98.9075630252101, r: 0.7141649374797701
06/01/2019 10:40:05 step: 4716, epoch: 142, batch: 29, loss: 0.013497404754161835, acc: 100.0, f1: 100.0, r: 0.6500161916880541
06/01/2019 10:40:05 *** evaluating ***
06/01/2019 10:40:05 step: 143, epoch: 142, acc: 58.54700854700855, f1: 21.863590968500553, r: 0.26482734288067766
06/01/2019 10:40:05 *** epoch: 144 ***
06/01/2019 10:40:05 *** training ***
06/01/2019 10:40:05 step: 4724, epoch: 143, batch: 4, loss: 0.015237763524055481, acc: 100.0, f1: 100.0, r: 0.5670530157639171
06/01/2019 10:40:06 step: 4729, epoch: 143, batch: 9, loss: 0.00507991760969162, acc: 100.0, f1: 100.0, r: 0.8056627501140748
06/01/2019 10:40:06 step: 4734, epoch: 143, batch: 14, loss: 0.0026016458868980408, acc: 100.0, f1: 100.0, r: 0.7967950066343865
06/01/2019 10:40:06 step: 4739, epoch: 143, batch: 19, loss: 0.009656384587287903, acc: 100.0, f1: 100.0, r: 0.6394831013485303
06/01/2019 10:40:06 step: 4744, epoch: 143, batch: 24, loss: 0.008541300892829895, acc: 100.0, f1: 100.0, r: 0.6866210126230101
06/01/2019 10:40:06 step: 4749, epoch: 143, batch: 29, loss: 0.00074777752161026, acc: 100.0, f1: 100.0, r: 0.7405860804944588
06/01/2019 10:40:06 *** evaluating ***
06/01/2019 10:40:07 step: 144, epoch: 143, acc: 60.256410256410255, f1: 22.859306780718168, r: 0.26947827195739066
06/01/2019 10:40:07 *** epoch: 145 ***
06/01/2019 10:40:07 *** training ***
06/01/2019 10:40:07 step: 4757, epoch: 144, batch: 4, loss: 0.016330987215042114, acc: 98.4375, f1: 97.94832826747721, r: 0.7729048037184594
06/01/2019 10:40:07 step: 4762, epoch: 144, batch: 9, loss: 0.006173528730869293, acc: 100.0, f1: 100.0, r: 0.7829893987491925
06/01/2019 10:40:07 step: 4767, epoch: 144, batch: 14, loss: 0.0025637075304985046, acc: 100.0, f1: 100.0, r: 0.6980365750694127
06/01/2019 10:40:07 step: 4772, epoch: 144, batch: 19, loss: 0.002634979784488678, acc: 100.0, f1: 100.0, r: 0.7581001876482265
06/01/2019 10:40:08 step: 4777, epoch: 144, batch: 24, loss: 0.004959382116794586, acc: 100.0, f1: 100.0, r: 0.7623895187791846
06/01/2019 10:40:08 step: 4782, epoch: 144, batch: 29, loss: 0.01583874225616455, acc: 100.0, f1: 100.0, r: 0.8373672481886354
06/01/2019 10:40:08 *** evaluating ***
06/01/2019 10:40:08 step: 145, epoch: 144, acc: 59.401709401709404, f1: 20.600108225108226, r: 0.2589401523710545
06/01/2019 10:40:08 *** epoch: 146 ***
06/01/2019 10:40:08 *** training ***
06/01/2019 10:40:08 step: 4790, epoch: 145, batch: 4, loss: 0.018521763384342194, acc: 100.0, f1: 100.0, r: 0.8343255643277003
06/01/2019 10:40:09 step: 4795, epoch: 145, batch: 9, loss: 0.011642694473266602, acc: 100.0, f1: 100.0, r: 0.8089373435942105
06/01/2019 10:40:09 step: 4800, epoch: 145, batch: 14, loss: 0.015386544167995453, acc: 100.0, f1: 100.0, r: 0.7186312398462159
06/01/2019 10:40:09 step: 4805, epoch: 145, batch: 19, loss: 0.005885280668735504, acc: 100.0, f1: 100.0, r: 0.7032585380920574
06/01/2019 10:40:09 step: 4810, epoch: 145, batch: 24, loss: 0.0020318180322647095, acc: 100.0, f1: 100.0, r: 0.8161501036607641
06/01/2019 10:40:09 step: 4815, epoch: 145, batch: 29, loss: 0.011710546910762787, acc: 100.0, f1: 100.0, r: 0.612253654619023
06/01/2019 10:40:10 *** evaluating ***
06/01/2019 10:40:10 step: 146, epoch: 145, acc: 57.26495726495726, f1: 19.161906575272912, r: 0.2490879354333849
06/01/2019 10:40:10 *** epoch: 147 ***
06/01/2019 10:40:10 *** training ***
06/01/2019 10:40:10 step: 4823, epoch: 146, batch: 4, loss: 0.018546946346759796, acc: 98.4375, f1: 98.7436676798379, r: 0.6779748494543677
06/01/2019 10:40:10 step: 4828, epoch: 146, batch: 9, loss: 0.04629649594426155, acc: 98.4375, f1: 98.0952380952381, r: 0.634214942004476
06/01/2019 10:40:10 step: 4833, epoch: 146, batch: 14, loss: 0.030213624238967896, acc: 98.4375, f1: 98.06763285024154, r: 0.75151753973247
06/01/2019 10:40:11 step: 4838, epoch: 146, batch: 19, loss: 0.0067218393087387085, acc: 100.0, f1: 100.0, r: 0.705707988510616
06/01/2019 10:40:11 step: 4843, epoch: 146, batch: 24, loss: 0.01615593582391739, acc: 100.0, f1: 100.0, r: 0.6739476241587656
06/01/2019 10:40:11 step: 4848, epoch: 146, batch: 29, loss: 0.00435250997543335, acc: 100.0, f1: 100.0, r: 0.684851057961447
06/01/2019 10:40:11 *** evaluating ***
06/01/2019 10:40:11 step: 147, epoch: 146, acc: 58.119658119658126, f1: 19.12521571326304, r: 0.2662911903132001
06/01/2019 10:40:11 *** epoch: 148 ***
06/01/2019 10:40:11 *** training ***
06/01/2019 10:40:11 step: 4856, epoch: 147, batch: 4, loss: 0.0032261088490486145, acc: 100.0, f1: 100.0, r: 0.6941378818374541
06/01/2019 10:40:12 step: 4861, epoch: 147, batch: 9, loss: 0.0007429569959640503, acc: 100.0, f1: 100.0, r: 0.767555751713639
06/01/2019 10:40:12 step: 4866, epoch: 147, batch: 14, loss: 0.0026953667402267456, acc: 100.0, f1: 100.0, r: 0.7593509973730768
06/01/2019 10:40:12 step: 4871, epoch: 147, batch: 19, loss: 0.007504262030124664, acc: 100.0, f1: 100.0, r: 0.7743850415912525
06/01/2019 10:40:12 step: 4876, epoch: 147, batch: 24, loss: 0.009180948138237, acc: 100.0, f1: 100.0, r: 0.6695557618185454
06/01/2019 10:40:13 step: 4881, epoch: 147, batch: 29, loss: 0.0014305710792541504, acc: 100.0, f1: 100.0, r: 0.7815017641879535
06/01/2019 10:40:13 *** evaluating ***
06/01/2019 10:40:13 step: 148, epoch: 147, acc: 58.97435897435898, f1: 21.58436381521023, r: 0.26114502793804173
06/01/2019 10:40:13 *** epoch: 149 ***
06/01/2019 10:40:13 *** training ***
06/01/2019 10:40:13 step: 4889, epoch: 148, batch: 4, loss: 0.004985518753528595, acc: 100.0, f1: 100.0, r: 0.6812580110474088
06/01/2019 10:40:13 step: 4894, epoch: 148, batch: 9, loss: 0.056358322501182556, acc: 98.4375, f1: 98.2078853046595, r: 0.7163960313897122
06/01/2019 10:40:13 step: 4899, epoch: 148, batch: 14, loss: 0.03210930526256561, acc: 98.4375, f1: 96.66048237476808, r: 0.7320781928090674
06/01/2019 10:40:14 step: 4904, epoch: 148, batch: 19, loss: 0.002385154366493225, acc: 100.0, f1: 100.0, r: 0.704751068764046
06/01/2019 10:40:14 step: 4909, epoch: 148, batch: 24, loss: 0.05169637128710747, acc: 98.4375, f1: 96.65024630541872, r: 0.6847249074604869
06/01/2019 10:40:14 step: 4914, epoch: 148, batch: 29, loss: 0.0064955949783325195, acc: 100.0, f1: 100.0, r: 0.7162389775141611
06/01/2019 10:40:14 *** evaluating ***
06/01/2019 10:40:14 step: 149, epoch: 148, acc: 59.82905982905983, f1: 21.166795528738767, r: 0.26767894739697184
06/01/2019 10:40:14 *** epoch: 150 ***
06/01/2019 10:40:14 *** training ***
06/01/2019 10:40:15 step: 4922, epoch: 149, batch: 4, loss: 0.003725796937942505, acc: 100.0, f1: 100.0, r: 0.679469972509097
06/01/2019 10:40:15 step: 4927, epoch: 149, batch: 9, loss: 0.010499604046344757, acc: 100.0, f1: 100.0, r: 0.8070852440884749
06/01/2019 10:40:15 step: 4932, epoch: 149, batch: 14, loss: 0.01622101664543152, acc: 100.0, f1: 100.0, r: 0.7301332838623096
06/01/2019 10:40:15 step: 4937, epoch: 149, batch: 19, loss: 0.026103802025318146, acc: 100.0, f1: 100.0, r: 0.7346939397565977
06/01/2019 10:40:15 step: 4942, epoch: 149, batch: 24, loss: 0.04825403913855553, acc: 98.4375, f1: 92.38095238095238, r: 0.720529371365228
06/01/2019 10:40:16 step: 4947, epoch: 149, batch: 29, loss: 0.0009512156248092651, acc: 100.0, f1: 100.0, r: 0.7641766543326335
06/01/2019 10:40:16 *** evaluating ***
06/01/2019 10:40:16 step: 150, epoch: 149, acc: 59.401709401709404, f1: 20.3030303030303, r: 0.2752406279878892
06/01/2019 10:40:16 *** epoch: 151 ***
06/01/2019 10:40:16 *** training ***
06/01/2019 10:40:16 step: 4955, epoch: 150, batch: 4, loss: 0.017821654677391052, acc: 100.0, f1: 100.0, r: 0.70302838869573
06/01/2019 10:40:16 step: 4960, epoch: 150, batch: 9, loss: 0.05387846380472183, acc: 98.4375, f1: 99.16891284815813, r: 0.8008748028493042
06/01/2019 10:40:17 step: 4965, epoch: 150, batch: 14, loss: 0.0016462653875350952, acc: 100.0, f1: 100.0, r: 0.6540987604212404
06/01/2019 10:40:17 step: 4970, epoch: 150, batch: 19, loss: 0.012806236743927002, acc: 100.0, f1: 100.0, r: 0.6147541236001053
06/01/2019 10:40:17 step: 4975, epoch: 150, batch: 24, loss: 0.03255545720458031, acc: 98.4375, f1: 95.6140350877193, r: 0.716539948238191
06/01/2019 10:40:17 step: 4980, epoch: 150, batch: 29, loss: 0.006118170917034149, acc: 100.0, f1: 100.0, r: 0.820831274310587
06/01/2019 10:40:17 *** evaluating ***
06/01/2019 10:40:18 step: 151, epoch: 150, acc: 59.82905982905983, f1: 21.412560454745304, r: 0.2764490052073271
06/01/2019 10:40:18 *** epoch: 152 ***
06/01/2019 10:40:18 *** training ***
06/01/2019 10:40:18 step: 4988, epoch: 151, batch: 4, loss: 0.0014918595552444458, acc: 100.0, f1: 100.0, r: 0.6172749637071576
06/01/2019 10:40:18 step: 4993, epoch: 151, batch: 9, loss: 0.004433974623680115, acc: 100.0, f1: 100.0, r: 0.7915920265904371
06/01/2019 10:40:18 step: 4998, epoch: 151, batch: 14, loss: 0.013012118637561798, acc: 100.0, f1: 100.0, r: 0.7507014190631395
06/01/2019 10:40:18 step: 5003, epoch: 151, batch: 19, loss: 0.005515344440937042, acc: 100.0, f1: 100.0, r: 0.6940074202535574
06/01/2019 10:40:19 step: 5008, epoch: 151, batch: 24, loss: 0.005434766411781311, acc: 100.0, f1: 100.0, r: 0.7184762448587072
06/01/2019 10:40:19 step: 5013, epoch: 151, batch: 29, loss: 0.029246468096971512, acc: 98.4375, f1: 94.44444444444444, r: 0.7242001982118327
06/01/2019 10:40:19 *** evaluating ***
06/01/2019 10:40:19 step: 152, epoch: 151, acc: 58.97435897435898, f1: 22.059345843935535, r: 0.2782104209701788
06/01/2019 10:40:19 *** epoch: 153 ***
06/01/2019 10:40:19 *** training ***
06/01/2019 10:40:19 step: 5021, epoch: 152, batch: 4, loss: 0.006411530077457428, acc: 100.0, f1: 100.0, r: 0.6982039999554817
06/01/2019 10:40:20 step: 5026, epoch: 152, batch: 9, loss: 0.0011793822050094604, acc: 100.0, f1: 100.0, r: 0.7659571075164929
06/01/2019 10:40:20 step: 5031, epoch: 152, batch: 14, loss: 0.003822237253189087, acc: 100.0, f1: 100.0, r: 0.7224383620321249
06/01/2019 10:40:20 step: 5036, epoch: 152, batch: 19, loss: 0.04133928567171097, acc: 96.875, f1: 94.28571428571428, r: 0.6883475497122546
06/01/2019 10:40:20 step: 5041, epoch: 152, batch: 24, loss: 0.0021282657980918884, acc: 100.0, f1: 100.0, r: 0.758131197729745
06/01/2019 10:40:21 step: 5046, epoch: 152, batch: 29, loss: 0.007274262607097626, acc: 100.0, f1: 100.0, r: 0.7123290979690178
06/01/2019 10:40:21 *** evaluating ***
06/01/2019 10:40:21 step: 153, epoch: 152, acc: 58.54700854700855, f1: 19.40560861613493, r: 0.265811533613892
06/01/2019 10:40:21 *** epoch: 154 ***
06/01/2019 10:40:21 *** training ***
06/01/2019 10:40:21 step: 5054, epoch: 153, batch: 4, loss: 0.005669906735420227, acc: 100.0, f1: 100.0, r: 0.7832654059231451
06/01/2019 10:40:21 step: 5059, epoch: 153, batch: 9, loss: 0.0006414949893951416, acc: 100.0, f1: 100.0, r: 0.6875323527857989
06/01/2019 10:40:22 step: 5064, epoch: 153, batch: 14, loss: 0.014914758503437042, acc: 100.0, f1: 100.0, r: 0.77097275208339
06/01/2019 10:40:22 step: 5069, epoch: 153, batch: 19, loss: 0.019758345559239388, acc: 100.0, f1: 100.0, r: 0.840951748171402
06/01/2019 10:40:22 step: 5074, epoch: 153, batch: 24, loss: 0.0018799006938934326, acc: 100.0, f1: 100.0, r: 0.7198577268521296
06/01/2019 10:40:22 step: 5079, epoch: 153, batch: 29, loss: 0.021526463329792023, acc: 98.4375, f1: 99.19078742608154, r: 0.684022186398402
06/01/2019 10:40:22 *** evaluating ***
06/01/2019 10:40:22 step: 154, epoch: 153, acc: 59.401709401709404, f1: 22.23831459572807, r: 0.27686454942743666
06/01/2019 10:40:22 *** epoch: 155 ***
06/01/2019 10:40:22 *** training ***
06/01/2019 10:40:23 step: 5087, epoch: 154, batch: 4, loss: 0.0096382275223732, acc: 100.0, f1: 100.0, r: 0.677598814271456
06/01/2019 10:40:23 step: 5092, epoch: 154, batch: 9, loss: 0.0009888559579849243, acc: 100.0, f1: 100.0, r: 0.6699126844814196
06/01/2019 10:40:23 step: 5097, epoch: 154, batch: 14, loss: 0.005521722137928009, acc: 100.0, f1: 100.0, r: 0.8045254045733926
06/01/2019 10:40:23 step: 5102, epoch: 154, batch: 19, loss: 0.023252204060554504, acc: 98.4375, f1: 94.04761904761905, r: 0.8003947345703123
06/01/2019 10:40:24 step: 5107, epoch: 154, batch: 24, loss: 0.0025044679641723633, acc: 100.0, f1: 100.0, r: 0.8220264958631718
06/01/2019 10:40:24 step: 5112, epoch: 154, batch: 29, loss: 0.15442389249801636, acc: 96.875, f1: 81.36363636363636, r: 0.7917379281767877
06/01/2019 10:40:24 *** evaluating ***
06/01/2019 10:40:24 step: 155, epoch: 154, acc: 58.54700854700855, f1: 19.511778841777566, r: 0.2702396440671744
06/01/2019 10:40:24 *** epoch: 156 ***
06/01/2019 10:40:24 *** training ***
06/01/2019 10:40:24 step: 5120, epoch: 155, batch: 4, loss: 0.0069696009159088135, acc: 100.0, f1: 100.0, r: 0.6495057704889466
06/01/2019 10:40:24 step: 5125, epoch: 155, batch: 9, loss: 0.014621816575527191, acc: 100.0, f1: 100.0, r: 0.7111891322703137
06/01/2019 10:40:25 step: 5130, epoch: 155, batch: 14, loss: 0.028003960847854614, acc: 98.4375, f1: 97.20730397422128, r: 0.665771863371785
06/01/2019 10:40:25 step: 5135, epoch: 155, batch: 19, loss: 0.0075474753975868225, acc: 100.0, f1: 100.0, r: 0.7964871895279882
06/01/2019 10:40:25 step: 5140, epoch: 155, batch: 24, loss: 0.03464502468705177, acc: 98.4375, f1: 97.25274725274726, r: 0.7813181312773649
06/01/2019 10:40:25 step: 5145, epoch: 155, batch: 29, loss: 0.004799544811248779, acc: 100.0, f1: 100.0, r: 0.7177809690656454
06/01/2019 10:40:26 *** evaluating ***
06/01/2019 10:40:26 step: 156, epoch: 155, acc: 58.97435897435898, f1: 20.56317468642105, r: 0.26847197058724814
06/01/2019 10:40:26 *** epoch: 157 ***
06/01/2019 10:40:26 *** training ***
06/01/2019 10:40:26 step: 5153, epoch: 156, batch: 4, loss: 0.007647678256034851, acc: 100.0, f1: 100.0, r: 0.8033619152862019
06/01/2019 10:40:26 step: 5158, epoch: 156, batch: 9, loss: 0.005833476781845093, acc: 100.0, f1: 100.0, r: 0.8194093422223874
06/01/2019 10:40:26 step: 5163, epoch: 156, batch: 14, loss: 0.030125970020890236, acc: 98.4375, f1: 97.95186891961085, r: 0.6434037866818838
06/01/2019 10:40:27 step: 5168, epoch: 156, batch: 19, loss: 0.002097085118293762, acc: 100.0, f1: 100.0, r: 0.7382058287089575
06/01/2019 10:40:27 step: 5173, epoch: 156, batch: 24, loss: 0.0013740360736846924, acc: 100.0, f1: 100.0, r: 0.6687793908407036
06/01/2019 10:40:27 step: 5178, epoch: 156, batch: 29, loss: 0.002415098249912262, acc: 100.0, f1: 100.0, r: 0.7943740604828291
06/01/2019 10:40:27 *** evaluating ***
06/01/2019 10:40:27 step: 157, epoch: 156, acc: 59.401709401709404, f1: 21.554286858974358, r: 0.26298478422109356
06/01/2019 10:40:27 *** epoch: 158 ***
06/01/2019 10:40:27 *** training ***
06/01/2019 10:40:27 step: 5186, epoch: 157, batch: 4, loss: 0.0021956264972686768, acc: 100.0, f1: 100.0, r: 0.5082370960638429
06/01/2019 10:40:28 step: 5191, epoch: 157, batch: 9, loss: 0.01458451896905899, acc: 100.0, f1: 100.0, r: 0.8062281759724301
06/01/2019 10:40:28 step: 5196, epoch: 157, batch: 14, loss: 0.002283133566379547, acc: 100.0, f1: 100.0, r: 0.759650297715834
06/01/2019 10:40:28 step: 5201, epoch: 157, batch: 19, loss: 0.011576756834983826, acc: 100.0, f1: 100.0, r: 0.8446877968933566
06/01/2019 10:40:28 step: 5206, epoch: 157, batch: 24, loss: 0.0021003857254981995, acc: 100.0, f1: 100.0, r: 0.7735378562473149
06/01/2019 10:40:29 step: 5211, epoch: 157, batch: 29, loss: 0.007014036178588867, acc: 100.0, f1: 100.0, r: 0.6296769850447211
06/01/2019 10:40:29 *** evaluating ***
06/01/2019 10:40:29 step: 158, epoch: 157, acc: 58.97435897435898, f1: 20.56317468642105, r: 0.27556014944030083
06/01/2019 10:40:29 *** epoch: 159 ***
06/01/2019 10:40:29 *** training ***
06/01/2019 10:40:29 step: 5219, epoch: 158, batch: 4, loss: 0.0017573907971382141, acc: 100.0, f1: 100.0, r: 0.7258846187626325
06/01/2019 10:40:29 step: 5224, epoch: 158, batch: 9, loss: 0.00485026091337204, acc: 100.0, f1: 100.0, r: 0.7106347704623245
06/01/2019 10:40:29 step: 5229, epoch: 158, batch: 14, loss: 0.0018139630556106567, acc: 100.0, f1: 100.0, r: 0.7118566795094058
06/01/2019 10:40:30 step: 5234, epoch: 158, batch: 19, loss: 0.00605841726064682, acc: 100.0, f1: 100.0, r: 0.6794893693785153
06/01/2019 10:40:30 step: 5239, epoch: 158, batch: 24, loss: 0.013950660824775696, acc: 100.0, f1: 100.0, r: 0.7905408054508283
06/01/2019 10:40:30 step: 5244, epoch: 158, batch: 29, loss: 0.010987117886543274, acc: 100.0, f1: 100.0, r: 0.7902693260605839
06/01/2019 10:40:30 *** evaluating ***
06/01/2019 10:40:30 step: 159, epoch: 158, acc: 58.54700854700855, f1: 19.576467542124202, r: 0.2728093272777067
06/01/2019 10:40:30 *** epoch: 160 ***
06/01/2019 10:40:30 *** training ***
06/01/2019 10:40:31 step: 5252, epoch: 159, batch: 4, loss: 0.00047440826892852783, acc: 100.0, f1: 100.0, r: 0.7509917169230856
06/01/2019 10:40:31 step: 5257, epoch: 159, batch: 9, loss: 0.0033339783549308777, acc: 100.0, f1: 100.0, r: 0.7447322229225237
06/01/2019 10:40:31 step: 5262, epoch: 159, batch: 14, loss: 0.04088166356086731, acc: 96.875, f1: 93.82106058797788, r: 0.643846063808974
06/01/2019 10:40:31 step: 5267, epoch: 159, batch: 19, loss: 0.0034452006220817566, acc: 100.0, f1: 100.0, r: 0.7919474836095227
06/01/2019 10:40:31 step: 5272, epoch: 159, batch: 24, loss: 0.034047387540340424, acc: 98.4375, f1: 98.98989898989899, r: 0.5590636787960731
06/01/2019 10:40:32 step: 5277, epoch: 159, batch: 29, loss: 0.005769900977611542, acc: 100.0, f1: 100.0, r: 0.7491317379524891
06/01/2019 10:40:32 *** evaluating ***
06/01/2019 10:40:32 step: 160, epoch: 159, acc: 58.97435897435898, f1: 20.56317468642105, r: 0.27882168033249394
06/01/2019 10:40:32 *** epoch: 161 ***
06/01/2019 10:40:32 *** training ***
06/01/2019 10:40:32 step: 5285, epoch: 160, batch: 4, loss: 0.019657552242279053, acc: 98.4375, f1: 97.71428571428571, r: 0.7679403688327794
06/01/2019 10:40:32 step: 5290, epoch: 160, batch: 9, loss: 0.008490078151226044, acc: 100.0, f1: 100.0, r: 0.6023688171839017
06/01/2019 10:40:33 step: 5295, epoch: 160, batch: 14, loss: 0.02379629760980606, acc: 100.0, f1: 100.0, r: 0.7324386089076693
06/01/2019 10:40:33 step: 5300, epoch: 160, batch: 19, loss: 0.006194964051246643, acc: 100.0, f1: 100.0, r: 0.7116964129877068
06/01/2019 10:40:33 step: 5305, epoch: 160, batch: 24, loss: 0.02101421356201172, acc: 100.0, f1: 100.0, r: 0.6947261990382493
06/01/2019 10:40:33 step: 5310, epoch: 160, batch: 29, loss: 0.003527522087097168, acc: 100.0, f1: 100.0, r: 0.7056089661823809
06/01/2019 10:40:33 *** evaluating ***
06/01/2019 10:40:33 step: 161, epoch: 160, acc: 58.54700854700855, f1: 19.455967504969323, r: 0.2763278932139626
06/01/2019 10:40:33 *** epoch: 162 ***
06/01/2019 10:40:33 *** training ***
06/01/2019 10:40:34 step: 5318, epoch: 161, batch: 4, loss: 0.002307616174221039, acc: 100.0, f1: 100.0, r: 0.6868292414487341
06/01/2019 10:40:34 step: 5323, epoch: 161, batch: 9, loss: 0.009312570095062256, acc: 100.0, f1: 100.0, r: 0.7944024925791646
06/01/2019 10:40:34 step: 5328, epoch: 161, batch: 14, loss: 0.02607567608356476, acc: 100.0, f1: 100.0, r: 0.7906058090212233
06/01/2019 10:40:34 step: 5333, epoch: 161, batch: 19, loss: 0.0037978291511535645, acc: 100.0, f1: 100.0, r: 0.7059583486202274
06/01/2019 10:40:35 step: 5338, epoch: 161, batch: 24, loss: 0.013715796172618866, acc: 100.0, f1: 100.0, r: 0.7168283269607189
06/01/2019 10:40:35 step: 5343, epoch: 161, batch: 29, loss: 0.003965452313423157, acc: 100.0, f1: 100.0, r: 0.7270127406358052
06/01/2019 10:40:35 *** evaluating ***
06/01/2019 10:40:35 step: 162, epoch: 161, acc: 58.97435897435898, f1: 20.899425287356323, r: 0.2657354706580571
06/01/2019 10:40:35 *** epoch: 163 ***
06/01/2019 10:40:35 *** training ***
06/01/2019 10:40:35 step: 5351, epoch: 162, batch: 4, loss: 0.00110694020986557, acc: 100.0, f1: 100.0, r: 0.8317861958199586
06/01/2019 10:40:36 step: 5356, epoch: 162, batch: 9, loss: 0.05053163692355156, acc: 96.875, f1: 94.32386747802569, r: 0.7623441291880295
06/01/2019 10:40:36 step: 5361, epoch: 162, batch: 14, loss: 0.0024262070655822754, acc: 100.0, f1: 100.0, r: 0.6335457288071294
06/01/2019 10:40:36 step: 5366, epoch: 162, batch: 19, loss: 0.004715368151664734, acc: 100.0, f1: 100.0, r: 0.6805826591205519
06/01/2019 10:40:36 step: 5371, epoch: 162, batch: 24, loss: 0.003439798951148987, acc: 100.0, f1: 100.0, r: 0.630911863565871
06/01/2019 10:40:36 step: 5376, epoch: 162, batch: 29, loss: 0.0007447749376296997, acc: 100.0, f1: 100.0, r: 0.7719875483457781
06/01/2019 10:40:37 *** evaluating ***
06/01/2019 10:40:37 step: 163, epoch: 162, acc: 59.82905982905983, f1: 22.413793103448278, r: 0.26289241246016926
06/01/2019 10:40:37 *** epoch: 164 ***
06/01/2019 10:40:37 *** training ***
06/01/2019 10:40:37 step: 5384, epoch: 163, batch: 4, loss: 0.00896398350596428, acc: 100.0, f1: 100.0, r: 0.7743632138816952
06/01/2019 10:40:37 step: 5389, epoch: 163, batch: 9, loss: 0.004768267273902893, acc: 100.0, f1: 100.0, r: 0.7332406184652408
06/01/2019 10:40:37 step: 5394, epoch: 163, batch: 14, loss: 0.007329963147640228, acc: 100.0, f1: 100.0, r: 0.8211420465807766
06/01/2019 10:40:38 step: 5399, epoch: 163, batch: 19, loss: 0.025343269109725952, acc: 98.4375, f1: 97.57236227824464, r: 0.7173788386401715
06/01/2019 10:40:38 step: 5404, epoch: 163, batch: 24, loss: 0.010441094636917114, acc: 100.0, f1: 100.0, r: 0.785711512247136
06/01/2019 10:40:38 step: 5409, epoch: 163, batch: 29, loss: 0.0118846595287323, acc: 100.0, f1: 100.0, r: 0.8405971166827562
06/01/2019 10:40:38 *** evaluating ***
06/01/2019 10:40:38 step: 164, epoch: 163, acc: 59.401709401709404, f1: 21.262378094523633, r: 0.26200808099310113
06/01/2019 10:40:38 *** epoch: 165 ***
06/01/2019 10:40:38 *** training ***
06/01/2019 10:40:38 step: 5417, epoch: 164, batch: 4, loss: 0.008690476417541504, acc: 100.0, f1: 100.0, r: 0.7376264814474309
06/01/2019 10:40:39 step: 5422, epoch: 164, batch: 9, loss: 0.02886452153325081, acc: 98.4375, f1: 98.74829931972789, r: 0.6943449397824359
06/01/2019 10:40:39 step: 5427, epoch: 164, batch: 14, loss: 0.05228248983621597, acc: 96.875, f1: 92.53475303164744, r: 0.8249736222577995
06/01/2019 10:40:39 step: 5432, epoch: 164, batch: 19, loss: 0.030627325177192688, acc: 98.4375, f1: 98.46041055718476, r: 0.8088760322928068
06/01/2019 10:40:39 step: 5437, epoch: 164, batch: 24, loss: 0.008730050176382065, acc: 100.0, f1: 100.0, r: 0.6134316184380504
06/01/2019 10:40:40 step: 5442, epoch: 164, batch: 29, loss: 0.010551340878009796, acc: 100.0, f1: 100.0, r: 0.6426671734004892
06/01/2019 10:40:40 *** evaluating ***
06/01/2019 10:40:40 step: 165, epoch: 164, acc: 58.119658119658126, f1: 19.339019648213835, r: 0.26285510319752897
06/01/2019 10:40:40 *** epoch: 166 ***
06/01/2019 10:40:40 *** training ***
06/01/2019 10:40:40 step: 5450, epoch: 165, batch: 4, loss: 0.0028990358114242554, acc: 100.0, f1: 100.0, r: 0.6828872962065423
06/01/2019 10:40:40 step: 5455, epoch: 165, batch: 9, loss: 0.006279513239860535, acc: 100.0, f1: 100.0, r: 0.6338060391272544
06/01/2019 10:40:41 step: 5460, epoch: 165, batch: 14, loss: 0.1312723159790039, acc: 96.875, f1: 98.4920634920635, r: 0.7016418503515444
06/01/2019 10:40:41 step: 5465, epoch: 165, batch: 19, loss: 0.0038792192935943604, acc: 100.0, f1: 100.0, r: 0.7715564218596622
06/01/2019 10:40:41 step: 5470, epoch: 165, batch: 24, loss: 0.024540670216083527, acc: 98.4375, f1: 94.28571428571428, r: 0.6945764922477708
06/01/2019 10:40:41 step: 5475, epoch: 165, batch: 29, loss: 0.007654011249542236, acc: 100.0, f1: 100.0, r: 0.6819723041698225
06/01/2019 10:40:41 *** evaluating ***
06/01/2019 10:40:41 step: 166, epoch: 165, acc: 58.54700854700855, f1: 20.351613535185674, r: 0.25569339687442905
06/01/2019 10:40:41 *** epoch: 167 ***
06/01/2019 10:40:41 *** training ***
06/01/2019 10:40:42 step: 5483, epoch: 166, batch: 4, loss: 0.03922794759273529, acc: 98.4375, f1: 93.19727891156462, r: 0.6150521926362702
06/01/2019 10:40:42 step: 5488, epoch: 166, batch: 9, loss: 0.001744098961353302, acc: 100.0, f1: 100.0, r: 0.7683334224977686
06/01/2019 10:40:42 step: 5493, epoch: 166, batch: 14, loss: 0.005804166197776794, acc: 100.0, f1: 100.0, r: 0.7106868308555983
06/01/2019 10:40:42 step: 5498, epoch: 166, batch: 19, loss: 0.0029246285557746887, acc: 100.0, f1: 100.0, r: 0.5727813295656311
06/01/2019 10:40:43 step: 5503, epoch: 166, batch: 24, loss: 0.017758622765541077, acc: 100.0, f1: 100.0, r: 0.8353245548950394
06/01/2019 10:40:43 step: 5508, epoch: 166, batch: 29, loss: 0.004309475421905518, acc: 100.0, f1: 100.0, r: 0.7160929577798459
06/01/2019 10:40:43 *** evaluating ***
06/01/2019 10:40:43 step: 167, epoch: 166, acc: 59.82905982905983, f1: 22.413793103448278, r: 0.26374701274629386
06/01/2019 10:40:43 *** epoch: 168 ***
06/01/2019 10:40:43 *** training ***
06/01/2019 10:40:43 step: 5516, epoch: 167, batch: 4, loss: 0.0012976452708244324, acc: 100.0, f1: 100.0, r: 0.6655292091423202
06/01/2019 10:40:43 step: 5521, epoch: 167, batch: 9, loss: 0.001843087375164032, acc: 100.0, f1: 100.0, r: 0.6609467133039831
06/01/2019 10:40:44 step: 5526, epoch: 167, batch: 14, loss: 0.005862198770046234, acc: 100.0, f1: 100.0, r: 0.6867702649133691
06/01/2019 10:40:44 step: 5531, epoch: 167, batch: 19, loss: 0.0028295889496803284, acc: 100.0, f1: 100.0, r: 0.7504924399272864
06/01/2019 10:40:44 step: 5536, epoch: 167, batch: 24, loss: 0.009923584759235382, acc: 100.0, f1: 100.0, r: 0.6333494297435308
06/01/2019 10:40:44 step: 5541, epoch: 167, batch: 29, loss: 0.02512247860431671, acc: 98.4375, f1: 97.25274725274726, r: 0.8153735333713205
06/01/2019 10:40:44 *** evaluating ***
06/01/2019 10:40:45 step: 168, epoch: 167, acc: 59.401709401709404, f1: 20.94871794871795, r: 0.24827912634555788
06/01/2019 10:40:45 *** epoch: 169 ***
06/01/2019 10:40:45 *** training ***
06/01/2019 10:40:45 step: 5549, epoch: 168, batch: 4, loss: 0.005166761577129364, acc: 100.0, f1: 100.0, r: 0.7124995355316638
06/01/2019 10:40:45 step: 5554, epoch: 168, batch: 9, loss: 0.021462418138980865, acc: 98.4375, f1: 99.37747594793436, r: 0.6907820760566229
06/01/2019 10:40:45 step: 5559, epoch: 168, batch: 14, loss: 0.035894617438316345, acc: 98.4375, f1: 98.91589438713062, r: 0.6930141396514228
06/01/2019 10:40:46 step: 5564, epoch: 168, batch: 19, loss: 0.00034646689891815186, acc: 100.0, f1: 100.0, r: 0.6974484069054887
06/01/2019 10:40:46 step: 5569, epoch: 168, batch: 24, loss: 0.011833488941192627, acc: 100.0, f1: 100.0, r: 0.6768585147198749
06/01/2019 10:40:46 step: 5574, epoch: 168, batch: 29, loss: 0.0032442957162857056, acc: 100.0, f1: 100.0, r: 0.6793086883159447
06/01/2019 10:40:46 *** evaluating ***
06/01/2019 10:40:46 step: 169, epoch: 168, acc: 58.97435897435898, f1: 19.611528822055135, r: 0.2570303039133672
06/01/2019 10:40:46 *** epoch: 170 ***
06/01/2019 10:40:46 *** training ***
06/01/2019 10:40:46 step: 5582, epoch: 169, batch: 4, loss: 0.0033562108874320984, acc: 100.0, f1: 100.0, r: 0.7221751610523294
06/01/2019 10:40:47 step: 5587, epoch: 169, batch: 9, loss: 0.02466585487127304, acc: 98.4375, f1: 96.86028257456829, r: 0.6760669267001236
06/01/2019 10:40:47 step: 5592, epoch: 169, batch: 14, loss: 0.034480005502700806, acc: 98.4375, f1: 99.12121212121212, r: 0.8492101768617851
06/01/2019 10:40:47 step: 5597, epoch: 169, batch: 19, loss: 0.016774851828813553, acc: 100.0, f1: 100.0, r: 0.7959445330528292
06/01/2019 10:40:47 step: 5602, epoch: 169, batch: 24, loss: 0.0014237239956855774, acc: 100.0, f1: 100.0, r: 0.7274845494036714
06/01/2019 10:40:48 step: 5607, epoch: 169, batch: 29, loss: 0.04702317714691162, acc: 98.4375, f1: 97.55639097744361, r: 0.8315809046772027
06/01/2019 10:40:48 *** evaluating ***
06/01/2019 10:40:48 step: 170, epoch: 169, acc: 60.256410256410255, f1: 22.845139794292336, r: 0.26325216863042766
06/01/2019 10:40:48 *** epoch: 171 ***
06/01/2019 10:40:48 *** training ***
06/01/2019 10:40:48 step: 5615, epoch: 170, batch: 4, loss: 0.0001437366008758545, acc: 100.0, f1: 100.0, r: 0.7956870002563116
06/01/2019 10:40:48 step: 5620, epoch: 170, batch: 9, loss: 0.021532028913497925, acc: 100.0, f1: 100.0, r: 0.6789408309663921
06/01/2019 10:40:48 step: 5625, epoch: 170, batch: 14, loss: 0.019519135355949402, acc: 100.0, f1: 100.0, r: 0.6776731902735793
06/01/2019 10:40:49 step: 5630, epoch: 170, batch: 19, loss: 0.006942480802536011, acc: 100.0, f1: 100.0, r: 0.6984183342002552
06/01/2019 10:40:49 step: 5635, epoch: 170, batch: 24, loss: 0.03363650292158127, acc: 98.4375, f1: 98.9617486338798, r: 0.7996873207585854
06/01/2019 10:40:49 step: 5640, epoch: 170, batch: 29, loss: 0.010126054286956787, acc: 100.0, f1: 100.0, r: 0.6421971216858917
06/01/2019 10:40:49 *** evaluating ***
06/01/2019 10:40:49 step: 171, epoch: 170, acc: 59.401709401709404, f1: 22.101932948961124, r: 0.2669099698109295
06/01/2019 10:40:49 *** epoch: 172 ***
06/01/2019 10:40:49 *** training ***
06/01/2019 10:40:50 step: 5648, epoch: 171, batch: 4, loss: 0.0022038593888282776, acc: 100.0, f1: 100.0, r: 0.6332747024664475
06/01/2019 10:40:50 step: 5653, epoch: 171, batch: 9, loss: 0.008063048124313354, acc: 100.0, f1: 100.0, r: 0.7878823152440138
06/01/2019 10:40:50 step: 5658, epoch: 171, batch: 14, loss: 0.0011749044060707092, acc: 100.0, f1: 100.0, r: 0.7066791684329298
06/01/2019 10:40:50 step: 5663, epoch: 171, batch: 19, loss: 0.01779048889875412, acc: 98.4375, f1: 93.19727891156461, r: 0.7126396022728044
06/01/2019 10:40:51 step: 5668, epoch: 171, batch: 24, loss: 0.002315014600753784, acc: 100.0, f1: 100.0, r: 0.7108932249777058
06/01/2019 10:40:51 step: 5673, epoch: 171, batch: 29, loss: 0.013516344130039215, acc: 98.4375, f1: 86.76470588235294, r: 0.705463480529788
06/01/2019 10:40:51 *** evaluating ***
06/01/2019 10:40:51 step: 172, epoch: 171, acc: 58.97435897435898, f1: 20.901744430432952, r: 0.25081909624075593
06/01/2019 10:40:51 *** epoch: 173 ***
06/01/2019 10:40:51 *** training ***
06/01/2019 10:40:51 step: 5681, epoch: 172, batch: 4, loss: 0.0007651075720787048, acc: 100.0, f1: 100.0, r: 0.7316647949501759
06/01/2019 10:40:51 step: 5686, epoch: 172, batch: 9, loss: 0.005976967513561249, acc: 100.0, f1: 100.0, r: 0.7214689665634749
06/01/2019 10:40:52 step: 5691, epoch: 172, batch: 14, loss: 0.0026493147015571594, acc: 100.0, f1: 100.0, r: 0.8232543739267006
06/01/2019 10:40:52 step: 5696, epoch: 172, batch: 19, loss: 0.0029457733035087585, acc: 100.0, f1: 100.0, r: 0.6845306074598563
06/01/2019 10:40:52 step: 5701, epoch: 172, batch: 24, loss: 0.00549672544002533, acc: 100.0, f1: 100.0, r: 0.7914041330199358
06/01/2019 10:40:52 step: 5706, epoch: 172, batch: 29, loss: 0.003825075924396515, acc: 100.0, f1: 100.0, r: 0.6405278572277632
06/01/2019 10:40:52 *** evaluating ***
06/01/2019 10:40:53 step: 173, epoch: 172, acc: 58.54700854700855, f1: 19.527168234064785, r: 0.25541649588024307
06/01/2019 10:40:53 *** epoch: 174 ***
06/01/2019 10:40:53 *** training ***
06/01/2019 10:40:53 step: 5714, epoch: 173, batch: 4, loss: 0.00541263073682785, acc: 100.0, f1: 100.0, r: 0.7123462595874712
06/01/2019 10:40:53 step: 5719, epoch: 173, batch: 9, loss: 0.010212160646915436, acc: 100.0, f1: 100.0, r: 0.6239380839757892
06/01/2019 10:40:53 step: 5724, epoch: 173, batch: 14, loss: 0.006642170250415802, acc: 100.0, f1: 100.0, r: 0.7012005476263992
06/01/2019 10:40:54 step: 5729, epoch: 173, batch: 19, loss: 0.02643987536430359, acc: 98.4375, f1: 97.73242630385487, r: 0.669714840466718
06/01/2019 10:40:54 step: 5734, epoch: 173, batch: 24, loss: 0.007096149027347565, acc: 100.0, f1: 100.0, r: 0.6306150022542805
06/01/2019 10:40:54 step: 5739, epoch: 173, batch: 29, loss: 0.024377845227718353, acc: 100.0, f1: 100.0, r: 0.7537241153410502
06/01/2019 10:40:54 *** evaluating ***
06/01/2019 10:40:54 step: 174, epoch: 173, acc: 59.82905982905983, f1: 21.198717948717945, r: 0.24909075049632376
06/01/2019 10:40:54 *** epoch: 175 ***
06/01/2019 10:40:54 *** training ***
06/01/2019 10:40:54 step: 5747, epoch: 174, batch: 4, loss: 0.019383348524570465, acc: 100.0, f1: 100.0, r: 0.8399431891303272
06/01/2019 10:40:55 step: 5752, epoch: 174, batch: 9, loss: 0.019235655665397644, acc: 98.4375, f1: 98.82711705371804, r: 0.7476628117209835
06/01/2019 10:40:55 step: 5757, epoch: 174, batch: 14, loss: 0.006498940289020538, acc: 100.0, f1: 100.0, r: 0.7142515111262187
06/01/2019 10:40:55 step: 5762, epoch: 174, batch: 19, loss: 0.0032103508710861206, acc: 100.0, f1: 100.0, r: 0.6477987784594319
06/01/2019 10:40:55 step: 5767, epoch: 174, batch: 24, loss: 0.0006829872727394104, acc: 100.0, f1: 100.0, r: 0.8031037470337599
06/01/2019 10:40:56 step: 5772, epoch: 174, batch: 29, loss: 0.005159586668014526, acc: 100.0, f1: 100.0, r: 0.6888539863104223
06/01/2019 10:40:56 *** evaluating ***
06/01/2019 10:40:56 step: 175, epoch: 174, acc: 58.54700854700855, f1: 21.40641903146153, r: 0.2641966147534221
06/01/2019 10:40:56 *** epoch: 176 ***
06/01/2019 10:40:56 *** training ***
06/01/2019 10:40:56 step: 5780, epoch: 175, batch: 4, loss: 0.011846959590911865, acc: 100.0, f1: 100.0, r: 0.7787416516348956
06/01/2019 10:40:56 step: 5785, epoch: 175, batch: 9, loss: 0.014243900775909424, acc: 100.0, f1: 100.0, r: 0.6433193049561627
06/01/2019 10:40:57 step: 5790, epoch: 175, batch: 14, loss: 0.00716254860162735, acc: 100.0, f1: 100.0, r: 0.7438928067884225
06/01/2019 10:40:57 step: 5795, epoch: 175, batch: 19, loss: 0.007283642888069153, acc: 100.0, f1: 100.0, r: 0.772176648029525
06/01/2019 10:40:57 step: 5800, epoch: 175, batch: 24, loss: 0.009085021913051605, acc: 100.0, f1: 100.0, r: 0.7917557203974122
06/01/2019 10:40:57 step: 5805, epoch: 175, batch: 29, loss: 0.0005010217428207397, acc: 100.0, f1: 100.0, r: 0.7270209489459735
06/01/2019 10:40:57 *** evaluating ***
06/01/2019 10:40:58 step: 176, epoch: 175, acc: 58.97435897435898, f1: 19.613455820352375, r: 0.25938061830940556
06/01/2019 10:40:58 *** epoch: 177 ***
06/01/2019 10:40:58 *** training ***
06/01/2019 10:40:58 step: 5813, epoch: 176, batch: 4, loss: 0.025142043828964233, acc: 98.4375, f1: 97.47899159663865, r: 0.7505976086391669
06/01/2019 10:40:58 step: 5818, epoch: 176, batch: 9, loss: 0.0019301772117614746, acc: 100.0, f1: 100.0, r: 0.7354354790644391
06/01/2019 10:40:58 step: 5823, epoch: 176, batch: 14, loss: 0.0038142651319503784, acc: 100.0, f1: 100.0, r: 0.6394071012124963
06/01/2019 10:40:58 step: 5828, epoch: 176, batch: 19, loss: 0.0025290697813034058, acc: 100.0, f1: 100.0, r: 0.706339316810187
06/01/2019 10:40:59 step: 5833, epoch: 176, batch: 24, loss: 0.02459186315536499, acc: 98.4375, f1: 99.11111111111111, r: 0.7030096707850753
06/01/2019 10:40:59 step: 5838, epoch: 176, batch: 29, loss: 0.0037605315446853638, acc: 100.0, f1: 100.0, r: 0.758955100055359
06/01/2019 10:40:59 *** evaluating ***
06/01/2019 10:40:59 step: 177, epoch: 176, acc: 58.97435897435898, f1: 22.62225171057432, r: 0.26166042106138
06/01/2019 10:40:59 *** epoch: 178 ***
06/01/2019 10:40:59 *** training ***
06/01/2019 10:40:59 step: 5846, epoch: 177, batch: 4, loss: 0.01171264797449112, acc: 100.0, f1: 100.0, r: 0.6990001211714133
06/01/2019 10:41:00 step: 5851, epoch: 177, batch: 9, loss: 0.004995234310626984, acc: 100.0, f1: 100.0, r: 0.7213104219114872
06/01/2019 10:41:00 step: 5856, epoch: 177, batch: 14, loss: 0.019890770316123962, acc: 98.4375, f1: 99.03703703703704, r: 0.7596782635584164
06/01/2019 10:41:00 step: 5861, epoch: 177, batch: 19, loss: 0.0027364492416381836, acc: 100.0, f1: 100.0, r: 0.7025354403675218
06/01/2019 10:41:00 step: 5866, epoch: 177, batch: 24, loss: 0.05812598764896393, acc: 100.0, f1: 100.0, r: 0.7684669292322956
06/01/2019 10:41:00 step: 5871, epoch: 177, batch: 29, loss: 0.0026000067591667175, acc: 100.0, f1: 100.0, r: 0.7681754792270714
06/01/2019 10:41:01 *** evaluating ***
06/01/2019 10:41:01 step: 178, epoch: 177, acc: 58.54700854700855, f1: 22.10418939807238, r: 0.2717038604662576
06/01/2019 10:41:01 *** epoch: 179 ***
06/01/2019 10:41:01 *** training ***
06/01/2019 10:41:01 step: 5879, epoch: 178, batch: 4, loss: 0.0007114708423614502, acc: 100.0, f1: 100.0, r: 0.7997510243703982
06/01/2019 10:41:01 step: 5884, epoch: 178, batch: 9, loss: 0.003124624490737915, acc: 100.0, f1: 100.0, r: 0.7486606789038452
06/01/2019 10:41:01 step: 5889, epoch: 178, batch: 14, loss: 0.0036906152963638306, acc: 100.0, f1: 100.0, r: 0.7462623203588763
06/01/2019 10:41:02 step: 5894, epoch: 178, batch: 19, loss: 0.00532098114490509, acc: 100.0, f1: 100.0, r: 0.730437078912336
06/01/2019 10:41:02 step: 5899, epoch: 178, batch: 24, loss: 0.030061736702919006, acc: 98.4375, f1: 97.38095238095238, r: 0.7883179750332172
06/01/2019 10:41:02 step: 5904, epoch: 178, batch: 29, loss: 0.00507780909538269, acc: 100.0, f1: 100.0, r: 0.7143720698905678
06/01/2019 10:41:02 *** evaluating ***
06/01/2019 10:41:02 step: 179, epoch: 178, acc: 59.82905982905983, f1: 22.55656108597285, r: 0.26800253731801243
06/01/2019 10:41:02 *** epoch: 180 ***
06/01/2019 10:41:02 *** training ***
06/01/2019 10:41:03 step: 5912, epoch: 179, batch: 4, loss: 0.0012386664748191833, acc: 100.0, f1: 100.0, r: 0.7114110862487818
06/01/2019 10:41:03 step: 5917, epoch: 179, batch: 9, loss: 0.0067134276032447815, acc: 100.0, f1: 100.0, r: 0.7381334272878564
06/01/2019 10:41:03 step: 5922, epoch: 179, batch: 14, loss: 0.004454754292964935, acc: 100.0, f1: 100.0, r: 0.8032464273414097
06/01/2019 10:41:03 step: 5927, epoch: 179, batch: 19, loss: 0.0030257180333137512, acc: 100.0, f1: 100.0, r: 0.8044057585259284
06/01/2019 10:41:03 step: 5932, epoch: 179, batch: 24, loss: 0.03890591114759445, acc: 98.4375, f1: 94.87179487179486, r: 0.7263660560469062
06/01/2019 10:41:04 step: 5937, epoch: 179, batch: 29, loss: 0.003228619694709778, acc: 100.0, f1: 100.0, r: 0.715101297958268
06/01/2019 10:41:04 *** evaluating ***
06/01/2019 10:41:04 step: 180, epoch: 179, acc: 59.401709401709404, f1: 20.71221228189512, r: 0.2619807921990902
06/01/2019 10:41:04 *** epoch: 181 ***
06/01/2019 10:41:04 *** training ***
06/01/2019 10:41:04 step: 5945, epoch: 180, batch: 4, loss: 0.008825473487377167, acc: 100.0, f1: 100.0, r: 0.7161526663933809
06/01/2019 10:41:04 step: 5950, epoch: 180, batch: 9, loss: 0.01029466837644577, acc: 100.0, f1: 100.0, r: 0.7870583736165861
06/01/2019 10:41:05 step: 5955, epoch: 180, batch: 14, loss: 0.0007364600896835327, acc: 100.0, f1: 100.0, r: 0.6670913535801668
06/01/2019 10:41:05 step: 5960, epoch: 180, batch: 19, loss: 0.0027123168110847473, acc: 100.0, f1: 100.0, r: 0.8391947468292347
06/01/2019 10:41:05 step: 5965, epoch: 180, batch: 24, loss: 0.0042013004422187805, acc: 100.0, f1: 100.0, r: 0.6444884240847997
06/01/2019 10:41:05 step: 5970, epoch: 180, batch: 29, loss: 0.008937306702136993, acc: 100.0, f1: 100.0, r: 0.7159718343584694
06/01/2019 10:41:05 *** evaluating ***
06/01/2019 10:41:06 step: 181, epoch: 180, acc: 58.97435897435898, f1: 20.510089386965515, r: 0.264784905736452
06/01/2019 10:41:06 *** epoch: 182 ***
06/01/2019 10:41:06 *** training ***
06/01/2019 10:41:06 step: 5978, epoch: 181, batch: 4, loss: 0.009412407875061035, acc: 100.0, f1: 100.0, r: 0.7601086987897658
06/01/2019 10:41:06 step: 5983, epoch: 181, batch: 9, loss: 0.0032772868871688843, acc: 100.0, f1: 100.0, r: 0.6879446194494752
06/01/2019 10:41:06 step: 5988, epoch: 181, batch: 14, loss: 0.008029162883758545, acc: 100.0, f1: 100.0, r: 0.6827949536504169
06/01/2019 10:41:07 step: 5993, epoch: 181, batch: 19, loss: 0.01709716022014618, acc: 100.0, f1: 100.0, r: 0.7505401877947928
06/01/2019 10:41:07 step: 5998, epoch: 181, batch: 24, loss: 0.005660347640514374, acc: 100.0, f1: 100.0, r: 0.8039253035885524
06/01/2019 10:41:07 step: 6003, epoch: 181, batch: 29, loss: 0.008001871407032013, acc: 100.0, f1: 100.0, r: 0.7909459848336926
06/01/2019 10:41:07 *** evaluating ***
06/01/2019 10:41:07 step: 182, epoch: 181, acc: 58.97435897435898, f1: 19.61441238887, r: 0.26734378972065503
06/01/2019 10:41:07 *** epoch: 183 ***
06/01/2019 10:41:07 *** training ***
06/01/2019 10:41:07 step: 6011, epoch: 182, batch: 4, loss: 0.011845514178276062, acc: 100.0, f1: 100.0, r: 0.8530847052308365
06/01/2019 10:41:08 step: 6016, epoch: 182, batch: 9, loss: 0.006494276225566864, acc: 100.0, f1: 100.0, r: 0.7518244161483574
06/01/2019 10:41:08 step: 6021, epoch: 182, batch: 14, loss: 0.001823924481868744, acc: 100.0, f1: 100.0, r: 0.6656308107400479
06/01/2019 10:41:08 step: 6026, epoch: 182, batch: 19, loss: 0.025434359908103943, acc: 98.4375, f1: 96.8831168831169, r: 0.6351089062037516
06/01/2019 10:41:08 step: 6031, epoch: 182, batch: 24, loss: 0.03103635087609291, acc: 98.4375, f1: 98.71794871794873, r: 0.7882239416610535
06/01/2019 10:41:09 step: 6036, epoch: 182, batch: 29, loss: 0.0031217336654663086, acc: 100.0, f1: 100.0, r: 0.8268837787026352
06/01/2019 10:41:09 *** evaluating ***
06/01/2019 10:41:09 step: 183, epoch: 182, acc: 58.54700854700855, f1: 21.710735445317336, r: 0.2632528900662363
06/01/2019 10:41:09 *** epoch: 184 ***
06/01/2019 10:41:09 *** training ***
06/01/2019 10:41:09 step: 6044, epoch: 183, batch: 4, loss: 0.01363442838191986, acc: 100.0, f1: 100.0, r: 0.6770756735204566
06/01/2019 10:41:09 step: 6049, epoch: 183, batch: 9, loss: 0.0010832920670509338, acc: 100.0, f1: 100.0, r: 0.6692528320526175
06/01/2019 10:41:10 step: 6054, epoch: 183, batch: 14, loss: 0.0032553300261497498, acc: 100.0, f1: 100.0, r: 0.691870209461294
06/01/2019 10:41:10 step: 6059, epoch: 183, batch: 19, loss: 0.005646757781505585, acc: 100.0, f1: 100.0, r: 0.6721879659919542
06/01/2019 10:41:10 step: 6064, epoch: 183, batch: 24, loss: 0.023146770894527435, acc: 98.4375, f1: 94.28571428571428, r: 0.7410198823160099
06/01/2019 10:41:10 step: 6069, epoch: 183, batch: 29, loss: 0.009420275688171387, acc: 100.0, f1: 100.0, r: 0.6582737889459844
06/01/2019 10:41:10 *** evaluating ***
06/01/2019 10:41:11 step: 184, epoch: 183, acc: 59.82905982905983, f1: 22.40748180884782, r: 0.2616991708018147
06/01/2019 10:41:11 *** epoch: 185 ***
06/01/2019 10:41:11 *** training ***
06/01/2019 10:41:11 step: 6077, epoch: 184, batch: 4, loss: 0.010169021785259247, acc: 100.0, f1: 100.0, r: 0.8246165246909701
06/01/2019 10:41:11 step: 6082, epoch: 184, batch: 9, loss: 0.0025969669222831726, acc: 100.0, f1: 100.0, r: 0.8769342446104031
06/01/2019 10:41:11 step: 6087, epoch: 184, batch: 14, loss: 0.0045681968331336975, acc: 100.0, f1: 100.0, r: 0.7957150729552398
06/01/2019 10:41:11 step: 6092, epoch: 184, batch: 19, loss: 0.005176424980163574, acc: 100.0, f1: 100.0, r: 0.7989813679333929
06/01/2019 10:41:12 step: 6097, epoch: 184, batch: 24, loss: 0.006252028048038483, acc: 100.0, f1: 100.0, r: 0.8251131907720455
06/01/2019 10:41:12 step: 6102, epoch: 184, batch: 29, loss: 0.0065521374344825745, acc: 100.0, f1: 100.0, r: 0.7124162292064139
06/01/2019 10:41:12 *** evaluating ***
06/01/2019 10:41:12 step: 185, epoch: 184, acc: 59.82905982905983, f1: 19.941724941724942, r: 0.26361713817392873
06/01/2019 10:41:12 *** epoch: 186 ***
06/01/2019 10:41:12 *** training ***
06/01/2019 10:41:12 step: 6110, epoch: 185, batch: 4, loss: 0.0003871247172355652, acc: 100.0, f1: 100.0, r: 0.6585272487396242
06/01/2019 10:41:13 step: 6115, epoch: 185, batch: 9, loss: 0.0011587515473365784, acc: 100.0, f1: 100.0, r: 0.7172056049402451
06/01/2019 10:41:13 step: 6120, epoch: 185, batch: 14, loss: 0.001451939344406128, acc: 100.0, f1: 100.0, r: 0.7491632006206518
06/01/2019 10:41:13 step: 6125, epoch: 185, batch: 19, loss: 0.003884442150592804, acc: 100.0, f1: 100.0, r: 0.6116578383867318
06/01/2019 10:41:13 step: 6130, epoch: 185, batch: 24, loss: 0.07286615669727325, acc: 96.875, f1: 96.27988338192421, r: 0.7007699856928534
06/01/2019 10:41:13 step: 6135, epoch: 185, batch: 29, loss: 0.006434515118598938, acc: 100.0, f1: 100.0, r: 0.7174953776725561
06/01/2019 10:41:14 *** evaluating ***
06/01/2019 10:41:14 step: 186, epoch: 185, acc: 59.401709401709404, f1: 21.443563066880078, r: 0.26626489459434466
06/01/2019 10:41:14 *** epoch: 187 ***
06/01/2019 10:41:14 *** training ***
06/01/2019 10:41:14 step: 6143, epoch: 186, batch: 4, loss: 0.00172371044754982, acc: 100.0, f1: 100.0, r: 0.5771027826032864
06/01/2019 10:41:14 step: 6148, epoch: 186, batch: 9, loss: 0.046997711062431335, acc: 98.4375, f1: 98.35600907029477, r: 0.7181402213772182
06/01/2019 10:41:14 step: 6153, epoch: 186, batch: 14, loss: 0.0031887292861938477, acc: 100.0, f1: 100.0, r: 0.7321622804126481
06/01/2019 10:41:15 step: 6158, epoch: 186, batch: 19, loss: 0.009400986135005951, acc: 100.0, f1: 100.0, r: 0.780027098569086
06/01/2019 10:41:15 step: 6163, epoch: 186, batch: 24, loss: 0.011392466723918915, acc: 100.0, f1: 100.0, r: 0.7704511931811605
06/01/2019 10:41:15 step: 6168, epoch: 186, batch: 29, loss: 0.05487709864974022, acc: 98.4375, f1: 95.55555555555554, r: 0.7886709791201961
06/01/2019 10:41:15 *** evaluating ***
06/01/2019 10:41:15 step: 187, epoch: 186, acc: 59.401709401709404, f1: 20.30932510885341, r: 0.2677869345152448
06/01/2019 10:41:15 *** epoch: 188 ***
06/01/2019 10:41:15 *** training ***
06/01/2019 10:41:16 step: 6176, epoch: 187, batch: 4, loss: 0.00225067138671875, acc: 100.0, f1: 100.0, r: 0.7381603774738629
06/01/2019 10:41:16 step: 6181, epoch: 187, batch: 9, loss: 0.002585582435131073, acc: 100.0, f1: 100.0, r: 0.5594078228954419
06/01/2019 10:41:16 step: 6186, epoch: 187, batch: 14, loss: 0.013731986284255981, acc: 100.0, f1: 100.0, r: 0.7494018530740257
06/01/2019 10:41:16 step: 6191, epoch: 187, batch: 19, loss: 0.004382364451885223, acc: 100.0, f1: 100.0, r: 0.8320872001195075
06/01/2019 10:41:16 step: 6196, epoch: 187, batch: 24, loss: 0.01563311368227005, acc: 98.4375, f1: 97.93650793650795, r: 0.7628020874079839
06/01/2019 10:41:17 step: 6201, epoch: 187, batch: 29, loss: 0.002065785229206085, acc: 100.0, f1: 100.0, r: 0.6722955169569977
06/01/2019 10:41:17 *** evaluating ***
06/01/2019 10:41:17 step: 188, epoch: 187, acc: 58.54700854700855, f1: 19.736727928782788, r: 0.2512042961123269
06/01/2019 10:41:17 *** epoch: 189 ***
06/01/2019 10:41:17 *** training ***
06/01/2019 10:41:17 step: 6209, epoch: 188, batch: 4, loss: 0.005846858024597168, acc: 100.0, f1: 100.0, r: 0.8361648723751642
06/01/2019 10:41:17 step: 6214, epoch: 188, batch: 9, loss: 0.002701118588447571, acc: 100.0, f1: 100.0, r: 0.8136848841127376
06/01/2019 10:41:18 step: 6219, epoch: 188, batch: 14, loss: 0.016126852482557297, acc: 100.0, f1: 100.0, r: 0.762131585521674
06/01/2019 10:41:18 step: 6224, epoch: 188, batch: 19, loss: 0.0027759000658988953, acc: 100.0, f1: 100.0, r: 0.7127262666481233
06/01/2019 10:41:18 step: 6229, epoch: 188, batch: 24, loss: 0.0014554336667060852, acc: 100.0, f1: 100.0, r: 0.8440170647518976
06/01/2019 10:41:18 step: 6234, epoch: 188, batch: 29, loss: 0.029783058911561966, acc: 98.4375, f1: 97.38095238095238, r: 0.8077751108363205
06/01/2019 10:41:18 *** evaluating ***
06/01/2019 10:41:19 step: 189, epoch: 188, acc: 59.82905982905983, f1: 23.110171435630022, r: 0.2647790802336735
06/01/2019 10:41:19 *** epoch: 190 ***
06/01/2019 10:41:19 *** training ***
06/01/2019 10:41:19 step: 6242, epoch: 189, batch: 4, loss: 0.0034027770161628723, acc: 100.0, f1: 100.0, r: 0.8139773188708064
06/01/2019 10:41:19 step: 6247, epoch: 189, batch: 9, loss: 0.0011161938309669495, acc: 100.0, f1: 100.0, r: 0.835693096574696
06/01/2019 10:41:19 step: 6252, epoch: 189, batch: 14, loss: 0.009721420705318451, acc: 100.0, f1: 100.0, r: 0.643600247495187
06/01/2019 10:41:19 step: 6257, epoch: 189, batch: 19, loss: 0.01866535097360611, acc: 100.0, f1: 100.0, r: 0.8253615230958512
06/01/2019 10:41:20 step: 6262, epoch: 189, batch: 24, loss: 0.0005246549844741821, acc: 100.0, f1: 100.0, r: 0.7835069348189121
06/01/2019 10:41:20 step: 6267, epoch: 189, batch: 29, loss: 0.0030405446887016296, acc: 100.0, f1: 100.0, r: 0.73486616949104
06/01/2019 10:41:20 *** evaluating ***
06/01/2019 10:41:20 step: 190, epoch: 189, acc: 59.82905982905983, f1: 22.89303106134789, r: 0.2680008844304868
06/01/2019 10:41:20 *** epoch: 191 ***
06/01/2019 10:41:20 *** training ***
06/01/2019 10:41:20 step: 6275, epoch: 190, batch: 4, loss: 0.0044679194688797, acc: 100.0, f1: 100.0, r: 0.7046898906604319
06/01/2019 10:41:21 step: 6280, epoch: 190, batch: 9, loss: 0.006887778639793396, acc: 100.0, f1: 100.0, r: 0.7899562047851201
06/01/2019 10:41:21 step: 6285, epoch: 190, batch: 14, loss: 0.0048940107226371765, acc: 100.0, f1: 100.0, r: 0.593164739971758
06/01/2019 10:41:21 step: 6290, epoch: 190, batch: 19, loss: 0.01176142692565918, acc: 100.0, f1: 100.0, r: 0.7752755599067346
06/01/2019 10:41:21 step: 6295, epoch: 190, batch: 24, loss: 0.04937148094177246, acc: 98.4375, f1: 99.24845269672856, r: 0.798580482908182
06/01/2019 10:41:21 step: 6300, epoch: 190, batch: 29, loss: 0.0015176981687545776, acc: 100.0, f1: 100.0, r: 0.8415369945407082
06/01/2019 10:41:22 *** evaluating ***
06/01/2019 10:41:22 step: 191, epoch: 190, acc: 56.837606837606835, f1: 21.213464687273113, r: 0.25928242311651095
06/01/2019 10:41:22 *** epoch: 192 ***
06/01/2019 10:41:22 *** training ***
06/01/2019 10:41:22 step: 6308, epoch: 191, batch: 4, loss: 0.008657678961753845, acc: 100.0, f1: 100.0, r: 0.7555181523222289
06/01/2019 10:41:22 step: 6313, epoch: 191, batch: 9, loss: 0.0065173134207725525, acc: 100.0, f1: 100.0, r: 0.7267399786036511
06/01/2019 10:41:22 step: 6318, epoch: 191, batch: 14, loss: 0.014659915119409561, acc: 100.0, f1: 100.0, r: 0.7796888897341703
06/01/2019 10:41:23 step: 6323, epoch: 191, batch: 19, loss: 0.007418110966682434, acc: 100.0, f1: 100.0, r: 0.7159320982353494
06/01/2019 10:41:23 step: 6328, epoch: 191, batch: 24, loss: 0.015816614031791687, acc: 98.4375, f1: 97.84126984126985, r: 0.6976349608060911
06/01/2019 10:41:23 step: 6333, epoch: 191, batch: 29, loss: 0.0004394054412841797, acc: 100.0, f1: 100.0, r: 0.6267647698511294
06/01/2019 10:41:23 *** evaluating ***
06/01/2019 10:41:23 step: 192, epoch: 191, acc: 60.256410256410255, f1: 23.163150760367685, r: 0.2703611346552627
06/01/2019 10:41:23 *** epoch: 193 ***
06/01/2019 10:41:23 *** training ***
06/01/2019 10:41:24 step: 6341, epoch: 192, batch: 4, loss: 0.0005520880222320557, acc: 100.0, f1: 100.0, r: 0.7810761837178999
06/01/2019 10:41:24 step: 6346, epoch: 192, batch: 9, loss: 0.014345429837703705, acc: 100.0, f1: 100.0, r: 0.7024548525793642
06/01/2019 10:41:24 step: 6351, epoch: 192, batch: 14, loss: 0.01791248470544815, acc: 98.4375, f1: 98.02102659245516, r: 0.8166324483617077
06/01/2019 10:41:24 step: 6356, epoch: 192, batch: 19, loss: 0.02899450808763504, acc: 98.4375, f1: 95.28985507246377, r: 0.730106428089283
06/01/2019 10:41:24 step: 6361, epoch: 192, batch: 24, loss: 0.004816561937332153, acc: 100.0, f1: 100.0, r: 0.6956232199470614
06/01/2019 10:41:25 step: 6366, epoch: 192, batch: 29, loss: 0.0021500736474990845, acc: 100.0, f1: 100.0, r: 0.791463419379926
06/01/2019 10:41:25 *** evaluating ***
06/01/2019 10:41:25 step: 193, epoch: 192, acc: 59.401709401709404, f1: 22.170436567323726, r: 0.25662952013297846
06/01/2019 10:41:25 *** epoch: 194 ***
06/01/2019 10:41:25 *** training ***
06/01/2019 10:41:25 step: 6374, epoch: 193, batch: 4, loss: 0.0043612197041511536, acc: 100.0, f1: 100.0, r: 0.8133923904198711
06/01/2019 10:41:25 step: 6379, epoch: 193, batch: 9, loss: 0.0023291632533073425, acc: 100.0, f1: 100.0, r: 0.7182138690976588
06/01/2019 10:41:26 step: 6384, epoch: 193, batch: 14, loss: 0.014112971723079681, acc: 100.0, f1: 100.0, r: 0.7295708851594698
06/01/2019 10:41:26 step: 6389, epoch: 193, batch: 19, loss: 0.008748043328523636, acc: 100.0, f1: 100.0, r: 0.7037392664529569
06/01/2019 10:41:26 step: 6394, epoch: 193, batch: 24, loss: 0.006756812334060669, acc: 100.0, f1: 100.0, r: 0.7594019375610082
06/01/2019 10:41:26 step: 6399, epoch: 193, batch: 29, loss: 0.015117764472961426, acc: 100.0, f1: 100.0, r: 0.8238668801116675
06/01/2019 10:41:26 *** evaluating ***
06/01/2019 10:41:26 step: 194, epoch: 193, acc: 58.54700854700855, f1: 21.753533882379806, r: 0.2503700418561523
06/01/2019 10:41:26 *** epoch: 195 ***
06/01/2019 10:41:26 *** training ***
06/01/2019 10:41:27 step: 6407, epoch: 194, batch: 4, loss: 0.005292944610118866, acc: 100.0, f1: 100.0, r: 0.7819989614697793
06/01/2019 10:41:27 step: 6412, epoch: 194, batch: 9, loss: 0.00337374210357666, acc: 100.0, f1: 100.0, r: 0.7305476365565521
06/01/2019 10:41:27 step: 6417, epoch: 194, batch: 14, loss: 0.010064482688903809, acc: 100.0, f1: 100.0, r: 0.6278355945932439
06/01/2019 10:41:27 step: 6422, epoch: 194, batch: 19, loss: 0.007127225399017334, acc: 100.0, f1: 100.0, r: 0.7263710061050571
06/01/2019 10:41:28 step: 6427, epoch: 194, batch: 24, loss: 0.0027835965156555176, acc: 100.0, f1: 100.0, r: 0.7329978855021684
06/01/2019 10:41:28 step: 6432, epoch: 194, batch: 29, loss: 0.024959489703178406, acc: 96.875, f1: 95.75997081957104, r: 0.7229434076474373
06/01/2019 10:41:28 *** evaluating ***
06/01/2019 10:41:28 step: 195, epoch: 194, acc: 59.82905982905983, f1: 22.413793103448278, r: 0.26459920339303994
06/01/2019 10:41:28 *** epoch: 196 ***
06/01/2019 10:41:28 *** training ***
06/01/2019 10:41:28 step: 6440, epoch: 195, batch: 4, loss: 0.01086144894361496, acc: 100.0, f1: 100.0, r: 0.8012095531321527
06/01/2019 10:41:28 step: 6445, epoch: 195, batch: 9, loss: 0.02201210707426071, acc: 98.4375, f1: 93.33333333333333, r: 0.7619319652398147
06/01/2019 10:41:29 step: 6450, epoch: 195, batch: 14, loss: 0.009963713586330414, acc: 100.0, f1: 100.0, r: 0.6649538081718553
06/01/2019 10:41:29 step: 6455, epoch: 195, batch: 19, loss: 0.0512949638068676, acc: 96.875, f1: 97.97247585754772, r: 0.7594438300486588
06/01/2019 10:41:29 step: 6460, epoch: 195, batch: 24, loss: 0.0249522402882576, acc: 98.4375, f1: 96.19047619047619, r: 0.6988646201627783
06/01/2019 10:41:29 step: 6465, epoch: 195, batch: 29, loss: 0.0009246468544006348, acc: 100.0, f1: 100.0, r: 0.6717594794114478
06/01/2019 10:41:30 *** evaluating ***
06/01/2019 10:41:30 step: 196, epoch: 195, acc: 59.401709401709404, f1: 21.469447620813632, r: 0.25189258233958234
06/01/2019 10:41:30 *** epoch: 197 ***
06/01/2019 10:41:30 *** training ***
06/01/2019 10:41:30 step: 6473, epoch: 196, batch: 4, loss: 0.003670051693916321, acc: 100.0, f1: 100.0, r: 0.7774429132982505
06/01/2019 10:41:30 step: 6478, epoch: 196, batch: 9, loss: 0.0017823204398155212, acc: 100.0, f1: 100.0, r: 0.7139479925483566
06/01/2019 10:41:30 step: 6483, epoch: 196, batch: 14, loss: 0.0015253946185112, acc: 100.0, f1: 100.0, r: 0.5709386643118807
06/01/2019 10:41:31 step: 6488, epoch: 196, batch: 19, loss: 0.0029898136854171753, acc: 100.0, f1: 100.0, r: 0.768220630875918
06/01/2019 10:41:31 step: 6493, epoch: 196, batch: 24, loss: 0.01168995350599289, acc: 100.0, f1: 100.0, r: 0.7602258804000291
06/01/2019 10:41:31 step: 6498, epoch: 196, batch: 29, loss: 0.004879854619503021, acc: 100.0, f1: 100.0, r: 0.8029567299482786
06/01/2019 10:41:31 *** evaluating ***
06/01/2019 10:41:31 step: 197, epoch: 196, acc: 57.26495726495726, f1: 21.59619864338536, r: 0.2645241175716477
06/01/2019 10:41:31 *** epoch: 198 ***
06/01/2019 10:41:31 *** training ***
06/01/2019 10:41:31 step: 6506, epoch: 197, batch: 4, loss: 0.011375151574611664, acc: 100.0, f1: 100.0, r: 0.7009993412293543
06/01/2019 10:41:32 step: 6511, epoch: 197, batch: 9, loss: 0.004256159067153931, acc: 100.0, f1: 100.0, r: 0.7798802842298496
06/01/2019 10:41:32 step: 6516, epoch: 197, batch: 14, loss: 0.005057379603385925, acc: 100.0, f1: 100.0, r: 0.7312406802165232
06/01/2019 10:41:32 step: 6521, epoch: 197, batch: 19, loss: 0.004641413688659668, acc: 100.0, f1: 100.0, r: 0.8193652326434393
06/01/2019 10:41:32 step: 6526, epoch: 197, batch: 24, loss: 0.04219809174537659, acc: 98.4375, f1: 98.30316742081449, r: 0.8098335604674676
06/01/2019 10:41:33 step: 6531, epoch: 197, batch: 29, loss: 0.003987140953540802, acc: 100.0, f1: 100.0, r: 0.6179198583341801
06/01/2019 10:41:33 *** evaluating ***
06/01/2019 10:41:33 step: 198, epoch: 197, acc: 58.54700854700855, f1: 21.516138067150816, r: 0.2664828708535501
06/01/2019 10:41:33 *** epoch: 199 ***
06/01/2019 10:41:33 *** training ***
06/01/2019 10:41:33 step: 6539, epoch: 198, batch: 4, loss: 0.026761136949062347, acc: 98.4375, f1: 97.31379731379732, r: 0.7076527809868663
06/01/2019 10:41:33 step: 6544, epoch: 198, batch: 9, loss: 0.0031711310148239136, acc: 100.0, f1: 100.0, r: 0.7380105211873484
06/01/2019 10:41:33 step: 6549, epoch: 198, batch: 14, loss: 0.0013986676931381226, acc: 100.0, f1: 100.0, r: 0.6977128059095262
06/01/2019 10:41:34 step: 6554, epoch: 198, batch: 19, loss: 0.01995791494846344, acc: 98.4375, f1: 99.1186839012926, r: 0.8343562200782908
06/01/2019 10:41:34 step: 6559, epoch: 198, batch: 24, loss: 0.0067135244607925415, acc: 100.0, f1: 100.0, r: 0.6373467198741645
06/01/2019 10:41:34 step: 6564, epoch: 198, batch: 29, loss: 0.0009094774723052979, acc: 100.0, f1: 100.0, r: 0.7672510448886942
06/01/2019 10:41:34 *** evaluating ***
06/01/2019 10:41:34 step: 199, epoch: 198, acc: 58.119658119658126, f1: 21.565476190476186, r: 0.2544357938918878
06/01/2019 10:41:34 *** epoch: 200 ***
06/01/2019 10:41:34 *** training ***
06/01/2019 10:41:35 step: 6572, epoch: 199, batch: 4, loss: 0.00651782751083374, acc: 100.0, f1: 100.0, r: 0.8224112252536457
06/01/2019 10:41:35 step: 6577, epoch: 199, batch: 9, loss: 0.0063284337520599365, acc: 100.0, f1: 100.0, r: 0.7337848097056487
06/01/2019 10:41:35 step: 6582, epoch: 199, batch: 14, loss: 0.004662901163101196, acc: 100.0, f1: 100.0, r: 0.7144575951164601
06/01/2019 10:41:35 step: 6587, epoch: 199, batch: 19, loss: 0.017657272517681122, acc: 100.0, f1: 100.0, r: 0.7050877030295383
06/01/2019 10:41:36 step: 6592, epoch: 199, batch: 24, loss: 0.002155289053916931, acc: 100.0, f1: 100.0, r: 0.7522336750848976
06/01/2019 10:41:36 step: 6597, epoch: 199, batch: 29, loss: 0.0018584653735160828, acc: 100.0, f1: 100.0, r: 0.6216167645126356
06/01/2019 10:41:36 *** evaluating ***
06/01/2019 10:41:36 step: 200, epoch: 199, acc: 59.82905982905983, f1: 22.931961073752117, r: 0.2749535178494714
06/01/2019 10:41:36 *** epoch: 201 ***
06/01/2019 10:41:36 *** training ***
06/01/2019 10:41:36 step: 6605, epoch: 200, batch: 4, loss: 0.022309288382530212, acc: 98.4375, f1: 94.6969696969697, r: 0.7948173739636597
06/01/2019 10:41:36 step: 6610, epoch: 200, batch: 9, loss: 0.0006721019744873047, acc: 100.0, f1: 100.0, r: 0.7424106677359962
06/01/2019 10:41:37 step: 6615, epoch: 200, batch: 14, loss: 0.01402294635772705, acc: 100.0, f1: 100.0, r: 0.7374378989647992
06/01/2019 10:41:37 step: 6620, epoch: 200, batch: 19, loss: 0.011807054281234741, acc: 100.0, f1: 100.0, r: 0.7158657089022283
06/01/2019 10:41:37 step: 6625, epoch: 200, batch: 24, loss: 0.00481230765581131, acc: 100.0, f1: 100.0, r: 0.7281275201799139
06/01/2019 10:41:37 step: 6630, epoch: 200, batch: 29, loss: 0.011537060141563416, acc: 100.0, f1: 100.0, r: 0.7771102200021571
06/01/2019 10:41:37 *** evaluating ***
06/01/2019 10:41:38 step: 201, epoch: 200, acc: 59.82905982905983, f1: 22.073985388020528, r: 0.2634123183029716
06/01/2019 10:41:38 *** epoch: 202 ***
06/01/2019 10:41:38 *** training ***
06/01/2019 10:41:38 step: 6638, epoch: 201, batch: 4, loss: 0.0014809370040893555, acc: 100.0, f1: 100.0, r: 0.6817784289398349
06/01/2019 10:41:38 step: 6643, epoch: 201, batch: 9, loss: 0.011874951422214508, acc: 98.4375, f1: 94.44444444444444, r: 0.7588264803911594
06/01/2019 10:41:38 step: 6648, epoch: 201, batch: 14, loss: 0.01656227931380272, acc: 100.0, f1: 100.0, r: 0.7712208286495411
06/01/2019 10:41:39 step: 6653, epoch: 201, batch: 19, loss: 0.006815105676651001, acc: 100.0, f1: 100.0, r: 0.7064792497258755
06/01/2019 10:41:39 step: 6658, epoch: 201, batch: 24, loss: 0.022644944489002228, acc: 100.0, f1: 100.0, r: 0.7107336703371308
06/01/2019 10:41:39 step: 6663, epoch: 201, batch: 29, loss: 0.0013581961393356323, acc: 100.0, f1: 100.0, r: 0.7648645494162762
06/01/2019 10:41:39 *** evaluating ***
06/01/2019 10:41:39 step: 202, epoch: 201, acc: 56.837606837606835, f1: 20.58776595744681, r: 0.26034587461550374
06/01/2019 10:41:39 *** epoch: 203 ***
06/01/2019 10:41:39 *** training ***
06/01/2019 10:41:39 step: 6671, epoch: 202, batch: 4, loss: 0.007739342749118805, acc: 100.0, f1: 100.0, r: 0.7791038852837784
06/01/2019 10:41:40 step: 6676, epoch: 202, batch: 9, loss: 0.0012481361627578735, acc: 100.0, f1: 100.0, r: 0.7109346964142857
06/01/2019 10:41:40 step: 6681, epoch: 202, batch: 14, loss: 0.007860802114009857, acc: 100.0, f1: 100.0, r: 0.6849995039288902
06/01/2019 10:41:40 step: 6686, epoch: 202, batch: 19, loss: 0.005720898509025574, acc: 100.0, f1: 100.0, r: 0.5778740691841321
06/01/2019 10:41:40 step: 6691, epoch: 202, batch: 24, loss: 0.0011100023984909058, acc: 100.0, f1: 100.0, r: 0.7539193401718576
06/01/2019 10:41:41 step: 6696, epoch: 202, batch: 29, loss: 0.003424130380153656, acc: 100.0, f1: 100.0, r: 0.6837606985513861
06/01/2019 10:41:41 *** evaluating ***
06/01/2019 10:41:41 step: 203, epoch: 202, acc: 58.54700854700855, f1: 22.312447919000043, r: 0.27664407651481193
06/01/2019 10:41:41 *** epoch: 204 ***
06/01/2019 10:41:41 *** training ***
06/01/2019 10:41:41 step: 6704, epoch: 203, batch: 4, loss: 0.00566999614238739, acc: 100.0, f1: 100.0, r: 0.7314744514480733
06/01/2019 10:41:41 step: 6709, epoch: 203, batch: 9, loss: 0.0024013370275497437, acc: 100.0, f1: 100.0, r: 0.8101342855872494
06/01/2019 10:41:41 step: 6714, epoch: 203, batch: 14, loss: 0.007748827338218689, acc: 100.0, f1: 100.0, r: 0.7028829135376368
06/01/2019 10:41:42 step: 6719, epoch: 203, batch: 19, loss: 0.0031863078474998474, acc: 100.0, f1: 100.0, r: 0.7095275405657426
06/01/2019 10:41:42 step: 6724, epoch: 203, batch: 24, loss: 0.0003496408462524414, acc: 100.0, f1: 100.0, r: 0.7084124460805362
06/01/2019 10:41:42 step: 6729, epoch: 203, batch: 29, loss: 0.0020273029804229736, acc: 100.0, f1: 100.0, r: 0.7762007053637163
06/01/2019 10:41:42 *** evaluating ***
06/01/2019 10:41:42 step: 204, epoch: 203, acc: 58.97435897435898, f1: 23.0241004153964, r: 0.2645497365749194
06/01/2019 10:41:42 *** epoch: 205 ***
06/01/2019 10:41:42 *** training ***
06/01/2019 10:41:43 step: 6737, epoch: 204, batch: 4, loss: 0.006297409534454346, acc: 100.0, f1: 100.0, r: 0.7773832365060118
06/01/2019 10:41:43 step: 6742, epoch: 204, batch: 9, loss: 0.0005770176649093628, acc: 100.0, f1: 100.0, r: 0.6878485645859352
06/01/2019 10:41:43 step: 6747, epoch: 204, batch: 14, loss: 0.0007290318608283997, acc: 100.0, f1: 100.0, r: 0.6180509097580003
06/01/2019 10:41:43 step: 6752, epoch: 204, batch: 19, loss: 0.007687319070100784, acc: 100.0, f1: 100.0, r: 0.6379361437783522
06/01/2019 10:41:44 step: 6757, epoch: 204, batch: 24, loss: 0.01524382084608078, acc: 100.0, f1: 100.0, r: 0.7120390401944259
06/01/2019 10:41:44 step: 6762, epoch: 204, batch: 29, loss: 0.01634460687637329, acc: 98.4375, f1: 98.7012987012987, r: 0.5372248586454192
06/01/2019 10:41:44 *** evaluating ***
06/01/2019 10:41:44 step: 205, epoch: 204, acc: 58.97435897435898, f1: 19.922370088719898, r: 0.2669228648974914
06/01/2019 10:41:44 *** epoch: 206 ***
06/01/2019 10:41:44 *** training ***
06/01/2019 10:41:44 step: 6770, epoch: 205, batch: 4, loss: 0.007368899881839752, acc: 100.0, f1: 100.0, r: 0.6961648214797976
06/01/2019 10:41:44 step: 6775, epoch: 205, batch: 9, loss: 0.0008151233196258545, acc: 100.0, f1: 100.0, r: 0.8473362133115115
06/01/2019 10:41:45 step: 6780, epoch: 205, batch: 14, loss: 0.0022507235407829285, acc: 100.0, f1: 100.0, r: 0.8286362763215727
06/01/2019 10:41:45 step: 6785, epoch: 205, batch: 19, loss: 0.00651136040687561, acc: 100.0, f1: 100.0, r: 0.7960251984878532
06/01/2019 10:41:45 step: 6790, epoch: 205, batch: 24, loss: 0.004401996731758118, acc: 100.0, f1: 100.0, r: 0.6972501694835593
06/01/2019 10:41:45 step: 6795, epoch: 205, batch: 29, loss: 0.0018295273184776306, acc: 100.0, f1: 100.0, r: 0.7050830765005043
06/01/2019 10:41:45 *** evaluating ***
06/01/2019 10:41:46 step: 206, epoch: 205, acc: 59.82905982905983, f1: 21.804687499999996, r: 0.26325107947063203
06/01/2019 10:41:46 *** epoch: 207 ***
06/01/2019 10:41:46 *** training ***
06/01/2019 10:41:46 step: 6803, epoch: 206, batch: 4, loss: 0.003534860908985138, acc: 100.0, f1: 100.0, r: 0.7391179734842893
06/01/2019 10:41:46 step: 6808, epoch: 206, batch: 9, loss: 0.0034129321575164795, acc: 100.0, f1: 100.0, r: 0.7190146368458504
06/01/2019 10:41:46 step: 6813, epoch: 206, batch: 14, loss: 0.0014194399118423462, acc: 100.0, f1: 100.0, r: 0.6120701602981738
06/01/2019 10:41:46 step: 6818, epoch: 206, batch: 19, loss: 0.005689710378646851, acc: 100.0, f1: 100.0, r: 0.7761361962402504
06/01/2019 10:41:47 step: 6823, epoch: 206, batch: 24, loss: 0.003748588263988495, acc: 100.0, f1: 100.0, r: 0.6471953393227177
06/01/2019 10:41:47 step: 6828, epoch: 206, batch: 29, loss: 0.0068718791007995605, acc: 100.0, f1: 100.0, r: 0.69616892248981
06/01/2019 10:41:47 *** evaluating ***
06/01/2019 10:41:47 step: 207, epoch: 206, acc: 59.82905982905983, f1: 21.90679339817271, r: 0.2693005829736725
06/01/2019 10:41:47 *** epoch: 208 ***
06/01/2019 10:41:47 *** training ***
06/01/2019 10:41:47 step: 6836, epoch: 207, batch: 4, loss: 0.014932587742805481, acc: 100.0, f1: 100.0, r: 0.7001197539110324
06/01/2019 10:41:48 step: 6841, epoch: 207, batch: 9, loss: 0.0025678426027297974, acc: 100.0, f1: 100.0, r: 0.719051130765368
06/01/2019 10:41:48 step: 6846, epoch: 207, batch: 14, loss: 0.008395738899707794, acc: 100.0, f1: 100.0, r: 0.7519625890988597
06/01/2019 10:41:48 step: 6851, epoch: 207, batch: 19, loss: 0.01803039014339447, acc: 100.0, f1: 100.0, r: 0.778941027601372
06/01/2019 10:41:48 step: 6856, epoch: 207, batch: 24, loss: 0.03558187931776047, acc: 98.4375, f1: 98.27998088867655, r: 0.6220167022302328
06/01/2019 10:41:49 step: 6861, epoch: 207, batch: 29, loss: 0.0034656524658203125, acc: 100.0, f1: 100.0, r: 0.7043856595040369
06/01/2019 10:41:49 *** evaluating ***
06/01/2019 10:41:49 step: 208, epoch: 207, acc: 59.401709401709404, f1: 21.48213794061018, r: 0.2706294785392483
06/01/2019 10:41:49 *** epoch: 209 ***
06/01/2019 10:41:49 *** training ***
06/01/2019 10:41:49 step: 6869, epoch: 208, batch: 4, loss: 0.004297345876693726, acc: 100.0, f1: 100.0, r: 0.6809764677211185
06/01/2019 10:41:49 step: 6874, epoch: 208, batch: 9, loss: 0.010049998760223389, acc: 100.0, f1: 100.0, r: 0.6958431736611675
06/01/2019 10:41:49 step: 6879, epoch: 208, batch: 14, loss: 0.0003692507743835449, acc: 100.0, f1: 100.0, r: 0.7551996585738049
06/01/2019 10:41:50 step: 6884, epoch: 208, batch: 19, loss: 0.0019094496965408325, acc: 100.0, f1: 100.0, r: 0.7939849379810657
06/01/2019 10:41:50 step: 6889, epoch: 208, batch: 24, loss: 0.002289608120918274, acc: 100.0, f1: 100.0, r: 0.8053914733643021
06/01/2019 10:41:50 step: 6894, epoch: 208, batch: 29, loss: 0.0011651068925857544, acc: 100.0, f1: 100.0, r: 0.802778877249995
06/01/2019 10:41:50 *** evaluating ***
06/01/2019 10:41:50 step: 209, epoch: 208, acc: 58.54700854700855, f1: 19.408100195185426, r: 0.26252203188565226
06/01/2019 10:41:50 *** epoch: 210 ***
06/01/2019 10:41:50 *** training ***
06/01/2019 10:41:51 step: 6902, epoch: 209, batch: 4, loss: 0.005653463304042816, acc: 100.0, f1: 100.0, r: 0.8021525500777181
06/01/2019 10:41:51 step: 6907, epoch: 209, batch: 9, loss: 0.003879837691783905, acc: 100.0, f1: 100.0, r: 0.8323787173360989
06/01/2019 10:41:51 step: 6912, epoch: 209, batch: 14, loss: 0.003073975443840027, acc: 100.0, f1: 100.0, r: 0.7572464377492283
06/01/2019 10:41:51 step: 6917, epoch: 209, batch: 19, loss: 0.0016623511910438538, acc: 100.0, f1: 100.0, r: 0.6734728632424564
06/01/2019 10:41:51 step: 6922, epoch: 209, batch: 24, loss: 0.002285197377204895, acc: 100.0, f1: 100.0, r: 0.8572339914571012
06/01/2019 10:41:52 step: 6927, epoch: 209, batch: 29, loss: 0.010122023522853851, acc: 100.0, f1: 100.0, r: 0.8311811104091703
06/01/2019 10:41:52 *** evaluating ***
06/01/2019 10:41:52 step: 210, epoch: 209, acc: 58.119658119658126, f1: 19.335635394842345, r: 0.26297527492680384
06/01/2019 10:41:52 *** epoch: 211 ***
06/01/2019 10:41:52 *** training ***
06/01/2019 10:41:52 step: 6935, epoch: 210, batch: 4, loss: 0.0006179213523864746, acc: 100.0, f1: 100.0, r: 0.7882623175412375
06/01/2019 10:41:52 step: 6940, epoch: 210, batch: 9, loss: 0.002274438738822937, acc: 100.0, f1: 100.0, r: 0.7966961734981929
06/01/2019 10:41:53 step: 6945, epoch: 210, batch: 14, loss: 0.0067237019538879395, acc: 100.0, f1: 100.0, r: 0.6805839439979136
06/01/2019 10:41:53 step: 6950, epoch: 210, batch: 19, loss: 0.0042061880230903625, acc: 100.0, f1: 100.0, r: 0.7169141199774782
06/01/2019 10:41:53 step: 6955, epoch: 210, batch: 24, loss: 0.0052961185574531555, acc: 100.0, f1: 100.0, r: 0.7401997155707933
06/01/2019 10:41:53 step: 6960, epoch: 210, batch: 29, loss: 0.0009372904896736145, acc: 100.0, f1: 100.0, r: 0.7240752111607951
06/01/2019 10:41:53 *** evaluating ***
06/01/2019 10:41:53 step: 211, epoch: 210, acc: 59.82905982905983, f1: 21.62618083670715, r: 0.27267448089273866
06/01/2019 10:41:53 *** epoch: 212 ***
06/01/2019 10:41:53 *** training ***
06/01/2019 10:41:54 step: 6968, epoch: 211, batch: 4, loss: 0.0032083317637443542, acc: 100.0, f1: 100.0, r: 0.7988730013533095
06/01/2019 10:41:54 step: 6973, epoch: 211, batch: 9, loss: 0.003302633762359619, acc: 100.0, f1: 100.0, r: 0.7426581664727356
06/01/2019 10:41:54 step: 6978, epoch: 211, batch: 14, loss: 0.03184891864657402, acc: 98.4375, f1: 96.86028257456829, r: 0.7268436568911443
06/01/2019 10:41:54 step: 6983, epoch: 211, batch: 19, loss: 0.0015105605125427246, acc: 100.0, f1: 100.0, r: 0.5985461697486758
06/01/2019 10:41:55 step: 6988, epoch: 211, batch: 24, loss: 0.002176612615585327, acc: 100.0, f1: 100.0, r: 0.7600472398599892
06/01/2019 10:41:55 step: 6993, epoch: 211, batch: 29, loss: 0.001675248146057129, acc: 100.0, f1: 100.0, r: 0.7196608172006416
06/01/2019 10:41:55 *** evaluating ***
06/01/2019 10:41:55 step: 212, epoch: 211, acc: 58.97435897435898, f1: 19.6360153256705, r: 0.27111507474011154
06/01/2019 10:41:55 *** epoch: 213 ***
06/01/2019 10:41:55 *** training ***
06/01/2019 10:41:55 step: 7001, epoch: 212, batch: 4, loss: 0.009394630789756775, acc: 100.0, f1: 100.0, r: 0.7888867375450426
06/01/2019 10:41:56 step: 7006, epoch: 212, batch: 9, loss: 0.0029464438557624817, acc: 100.0, f1: 100.0, r: 0.7986275794687504
06/01/2019 10:41:56 step: 7011, epoch: 212, batch: 14, loss: 0.0009351372718811035, acc: 100.0, f1: 100.0, r: 0.8151906861274525
06/01/2019 10:41:56 step: 7016, epoch: 212, batch: 19, loss: 0.004053719341754913, acc: 100.0, f1: 100.0, r: 0.7634199675674693
06/01/2019 10:41:56 step: 7021, epoch: 212, batch: 24, loss: 0.007158383727073669, acc: 100.0, f1: 100.0, r: 0.7333664418513806
06/01/2019 10:41:56 step: 7026, epoch: 212, batch: 29, loss: 0.0047529637813568115, acc: 100.0, f1: 100.0, r: 0.8008383730541437
06/01/2019 10:41:57 *** evaluating ***
06/01/2019 10:41:57 step: 213, epoch: 212, acc: 59.401709401709404, f1: 20.9978982765868, r: 0.2777176212582469
06/01/2019 10:41:57 *** epoch: 214 ***
06/01/2019 10:41:57 *** training ***
06/01/2019 10:41:57 step: 7034, epoch: 213, batch: 4, loss: 0.016905121505260468, acc: 98.4375, f1: 97.55639097744361, r: 0.8170367807632085
06/01/2019 10:41:57 step: 7039, epoch: 213, batch: 9, loss: 0.006006456911563873, acc: 100.0, f1: 100.0, r: 0.811238047218312
06/01/2019 10:41:57 step: 7044, epoch: 213, batch: 14, loss: 0.005582109093666077, acc: 100.0, f1: 100.0, r: 0.8119867397770568
06/01/2019 10:41:58 step: 7049, epoch: 213, batch: 19, loss: 0.006173770874738693, acc: 100.0, f1: 100.0, r: 0.7903224051925211
06/01/2019 10:41:58 step: 7054, epoch: 213, batch: 24, loss: 0.001041218638420105, acc: 100.0, f1: 100.0, r: 0.5958589412694887
06/01/2019 10:41:58 step: 7059, epoch: 213, batch: 29, loss: 0.00032009929418563843, acc: 100.0, f1: 100.0, r: 0.6973548982619311
06/01/2019 10:41:58 *** evaluating ***
06/01/2019 10:41:58 step: 214, epoch: 213, acc: 58.54700854700855, f1: 19.86527228189184, r: 0.2707939534424054
06/01/2019 10:41:58 *** epoch: 215 ***
06/01/2019 10:41:58 *** training ***
06/01/2019 10:41:58 step: 7067, epoch: 214, batch: 4, loss: 0.0009266585111618042, acc: 100.0, f1: 100.0, r: 0.7245497168673115
06/01/2019 10:41:59 step: 7072, epoch: 214, batch: 9, loss: 0.0018937140703201294, acc: 100.0, f1: 100.0, r: 0.67203678398185
06/01/2019 10:41:59 step: 7077, epoch: 214, batch: 14, loss: 0.02183673530817032, acc: 100.0, f1: 100.0, r: 0.7670923982227914
06/01/2019 10:41:59 step: 7082, epoch: 214, batch: 19, loss: 0.00395037978887558, acc: 100.0, f1: 100.0, r: 0.7457050277089136
06/01/2019 10:41:59 step: 7087, epoch: 214, batch: 24, loss: 0.003818131983280182, acc: 100.0, f1: 100.0, r: 0.7735351110176087
06/01/2019 10:42:00 step: 7092, epoch: 214, batch: 29, loss: 0.0013584345579147339, acc: 100.0, f1: 100.0, r: 0.6901756698998324
06/01/2019 10:42:00 *** evaluating ***
06/01/2019 10:42:00 step: 215, epoch: 214, acc: 58.97435897435898, f1: 19.681964248017323, r: 0.2771607938996839
06/01/2019 10:42:00 *** epoch: 216 ***
06/01/2019 10:42:00 *** training ***
06/01/2019 10:42:00 step: 7100, epoch: 215, batch: 4, loss: 0.0016173496842384338, acc: 100.0, f1: 100.0, r: 0.7859529843510648
06/01/2019 10:42:00 step: 7105, epoch: 215, batch: 9, loss: 0.0034529566764831543, acc: 100.0, f1: 100.0, r: 0.7271479954436575
06/01/2019 10:42:00 step: 7110, epoch: 215, batch: 14, loss: 0.013797715306282043, acc: 100.0, f1: 100.0, r: 0.7156520659253365
06/01/2019 10:42:01 step: 7115, epoch: 215, batch: 19, loss: 0.0019235014915466309, acc: 100.0, f1: 100.0, r: 0.6648814728569701
06/01/2019 10:42:01 step: 7120, epoch: 215, batch: 24, loss: 0.0014370083808898926, acc: 100.0, f1: 100.0, r: 0.814433953077196
06/01/2019 10:42:01 step: 7125, epoch: 215, batch: 29, loss: 0.008577123284339905, acc: 100.0, f1: 100.0, r: 0.800690668337633
06/01/2019 10:42:01 *** evaluating ***
06/01/2019 10:42:01 step: 216, epoch: 215, acc: 56.837606837606835, f1: 20.2741935483871, r: 0.26265111881904685
06/01/2019 10:42:01 *** epoch: 217 ***
06/01/2019 10:42:01 *** training ***
06/01/2019 10:42:02 step: 7133, epoch: 216, batch: 4, loss: 0.0018360167741775513, acc: 100.0, f1: 100.0, r: 0.7112534263889718
06/01/2019 10:42:02 step: 7138, epoch: 216, batch: 9, loss: 0.002472512423992157, acc: 100.0, f1: 100.0, r: 0.7072870474994075
06/01/2019 10:42:02 step: 7143, epoch: 216, batch: 14, loss: 0.003638625144958496, acc: 100.0, f1: 100.0, r: 0.7229818670680448
06/01/2019 10:42:02 step: 7148, epoch: 216, batch: 19, loss: 0.0026375800371170044, acc: 100.0, f1: 100.0, r: 0.697695733133426
06/01/2019 10:42:03 step: 7153, epoch: 216, batch: 24, loss: 0.0003754422068595886, acc: 100.0, f1: 100.0, r: 0.7743950308325364
06/01/2019 10:42:03 step: 7158, epoch: 216, batch: 29, loss: 0.006356276571750641, acc: 100.0, f1: 100.0, r: 0.7928779999108633
06/01/2019 10:42:03 *** evaluating ***
06/01/2019 10:42:03 step: 217, epoch: 216, acc: 58.54700854700855, f1: 19.511778841777566, r: 0.2784958660684356
06/01/2019 10:42:03 *** epoch: 218 ***
06/01/2019 10:42:03 *** training ***
06/01/2019 10:42:03 step: 7166, epoch: 217, batch: 4, loss: 0.0015359744429588318, acc: 100.0, f1: 100.0, r: 0.8544816661701458
06/01/2019 10:42:03 step: 7171, epoch: 217, batch: 9, loss: 0.00029884278774261475, acc: 100.0, f1: 100.0, r: 0.6722718459390288
06/01/2019 10:42:04 step: 7176, epoch: 217, batch: 14, loss: 0.010700501501560211, acc: 100.0, f1: 100.0, r: 0.770418367726575
06/01/2019 10:42:04 step: 7181, epoch: 217, batch: 19, loss: 0.0025363266468048096, acc: 100.0, f1: 100.0, r: 0.8276849388813055
06/01/2019 10:42:04 step: 7186, epoch: 217, batch: 24, loss: 0.019759587943553925, acc: 100.0, f1: 100.0, r: 0.6630462646583056
06/01/2019 10:42:04 step: 7191, epoch: 217, batch: 29, loss: 0.0029153525829315186, acc: 100.0, f1: 100.0, r: 0.6921948805464302
06/01/2019 10:42:04 *** evaluating ***
06/01/2019 10:42:05 step: 218, epoch: 217, acc: 58.119658119658126, f1: 21.088217338217337, r: 0.27783947836552897
06/01/2019 10:42:05 *** epoch: 219 ***
06/01/2019 10:42:05 *** training ***
06/01/2019 10:42:05 step: 7199, epoch: 218, batch: 4, loss: 0.010806962847709656, acc: 100.0, f1: 100.0, r: 0.8056418288931425
06/01/2019 10:42:05 step: 7204, epoch: 218, batch: 9, loss: 0.013959415256977081, acc: 98.4375, f1: 94.44444444444444, r: 0.7448320485441341
06/01/2019 10:42:05 step: 7209, epoch: 218, batch: 14, loss: 0.007500022649765015, acc: 100.0, f1: 100.0, r: 0.6284743464561138
06/01/2019 10:42:05 step: 7214, epoch: 218, batch: 19, loss: 0.0047157928347587585, acc: 100.0, f1: 100.0, r: 0.7534301739607938
06/01/2019 10:42:06 step: 7219, epoch: 218, batch: 24, loss: 0.008479982614517212, acc: 100.0, f1: 100.0, r: 0.7124388901819138
06/01/2019 10:42:06 step: 7224, epoch: 218, batch: 29, loss: 0.005179397761821747, acc: 100.0, f1: 100.0, r: 0.6995918354559885
06/01/2019 10:42:06 *** evaluating ***
06/01/2019 10:42:06 step: 219, epoch: 218, acc: 58.119658119658126, f1: 22.42615206794311, r: 0.2798872314738634
06/01/2019 10:42:06 *** epoch: 220 ***
06/01/2019 10:42:06 *** training ***
06/01/2019 10:42:06 step: 7232, epoch: 219, batch: 4, loss: 0.0183805450797081, acc: 100.0, f1: 100.0, r: 0.6959334130562849
06/01/2019 10:42:07 step: 7237, epoch: 219, batch: 9, loss: 0.005259416997432709, acc: 100.0, f1: 100.0, r: 0.72256756137168
06/01/2019 10:42:07 step: 7242, epoch: 219, batch: 14, loss: 0.0007438436150550842, acc: 100.0, f1: 100.0, r: 0.7014206801044449
06/01/2019 10:42:07 step: 7247, epoch: 219, batch: 19, loss: 0.009292952716350555, acc: 100.0, f1: 100.0, r: 0.7834417552210873
06/01/2019 10:42:07 step: 7252, epoch: 219, batch: 24, loss: 0.003124721348285675, acc: 100.0, f1: 100.0, r: 0.7622854599883438
06/01/2019 10:42:07 step: 7257, epoch: 219, batch: 29, loss: 0.0038335323333740234, acc: 100.0, f1: 100.0, r: 0.6647442514900302
06/01/2019 10:42:07 *** evaluating ***
06/01/2019 10:42:08 step: 220, epoch: 219, acc: 59.82905982905983, f1: 21.580866764005577, r: 0.28824487066155946
06/01/2019 10:42:08 *** epoch: 221 ***
06/01/2019 10:42:08 *** training ***
06/01/2019 10:42:08 step: 7265, epoch: 220, batch: 4, loss: 0.0064217522740364075, acc: 100.0, f1: 100.0, r: 0.698672870201735
06/01/2019 10:42:08 step: 7270, epoch: 220, batch: 9, loss: 0.0008689835667610168, acc: 100.0, f1: 100.0, r: 0.8340142699678019
06/01/2019 10:42:08 step: 7275, epoch: 220, batch: 14, loss: 0.0017694346606731415, acc: 100.0, f1: 100.0, r: 0.676508676480799
06/01/2019 10:42:08 step: 7280, epoch: 220, batch: 19, loss: 0.010321542620658875, acc: 100.0, f1: 100.0, r: 0.8030366921677762
06/01/2019 10:42:09 step: 7285, epoch: 220, batch: 24, loss: 0.0014858171343803406, acc: 100.0, f1: 100.0, r: 0.7365012131480014
06/01/2019 10:42:09 step: 7290, epoch: 220, batch: 29, loss: 0.012714199721813202, acc: 100.0, f1: 100.0, r: 0.7022971527606887
06/01/2019 10:42:09 *** evaluating ***
06/01/2019 10:42:09 step: 221, epoch: 220, acc: 58.97435897435898, f1: 20.530791422470582, r: 0.2877576722808973
06/01/2019 10:42:09 *** epoch: 222 ***
06/01/2019 10:42:09 *** training ***
06/01/2019 10:42:09 step: 7298, epoch: 221, batch: 4, loss: 0.002940461039543152, acc: 100.0, f1: 100.0, r: 0.7225173713181807
06/01/2019 10:42:10 step: 7303, epoch: 221, batch: 9, loss: 0.0013193562626838684, acc: 100.0, f1: 100.0, r: 0.7976827725447366
06/01/2019 10:42:10 step: 7308, epoch: 221, batch: 14, loss: 0.01779186725616455, acc: 98.4375, f1: 97.79158040027606, r: 0.6812234296877365
06/01/2019 10:42:10 step: 7313, epoch: 221, batch: 19, loss: 0.0009728968143463135, acc: 100.0, f1: 100.0, r: 0.6893160074168826
06/01/2019 10:42:10 step: 7318, epoch: 221, batch: 24, loss: 0.0042656660079956055, acc: 100.0, f1: 100.0, r: 0.8148549102659507
06/01/2019 10:42:10 step: 7323, epoch: 221, batch: 29, loss: 0.0766945481300354, acc: 96.875, f1: 89.04143475572047, r: 0.6733200318962289
06/01/2019 10:42:11 *** evaluating ***
06/01/2019 10:42:11 step: 222, epoch: 221, acc: 58.54700854700855, f1: 19.408100195185426, r: 0.27851157414800815
06/01/2019 10:42:11 *** epoch: 223 ***
06/01/2019 10:42:11 *** training ***
06/01/2019 10:42:11 step: 7331, epoch: 222, batch: 4, loss: 0.0003823712468147278, acc: 100.0, f1: 100.0, r: 0.7311223986577137
06/01/2019 10:42:11 step: 7336, epoch: 222, batch: 9, loss: 0.0030429139733314514, acc: 100.0, f1: 100.0, r: 0.727480998853709
06/01/2019 10:42:11 step: 7341, epoch: 222, batch: 14, loss: 0.003849700093269348, acc: 100.0, f1: 100.0, r: 0.8414607426898143
06/01/2019 10:42:12 step: 7346, epoch: 222, batch: 19, loss: 0.0017081499099731445, acc: 100.0, f1: 100.0, r: 0.7250696326923955
06/01/2019 10:42:12 step: 7351, epoch: 222, batch: 24, loss: 0.0071716830134391785, acc: 100.0, f1: 100.0, r: 0.7501422301216062
06/01/2019 10:42:12 step: 7356, epoch: 222, batch: 29, loss: 0.0018548518419265747, acc: 100.0, f1: 100.0, r: 0.6158396456313732
06/01/2019 10:42:12 *** evaluating ***
06/01/2019 10:42:12 step: 223, epoch: 222, acc: 59.82905982905983, f1: 21.62618083670715, r: 0.2848498236067487
06/01/2019 10:42:12 *** epoch: 224 ***
06/01/2019 10:42:12 *** training ***
06/01/2019 10:42:13 step: 7364, epoch: 223, batch: 4, loss: 0.008030533790588379, acc: 100.0, f1: 100.0, r: 0.7778481487810519
06/01/2019 10:42:13 step: 7369, epoch: 223, batch: 9, loss: 0.004687964916229248, acc: 100.0, f1: 100.0, r: 0.6006036543099136
06/01/2019 10:42:13 step: 7374, epoch: 223, batch: 14, loss: 0.002940721809864044, acc: 100.0, f1: 100.0, r: 0.8329922061132667
06/01/2019 10:42:13 step: 7379, epoch: 223, batch: 19, loss: 0.0025686100125312805, acc: 100.0, f1: 100.0, r: 0.7669566448073154
06/01/2019 10:42:13 step: 7384, epoch: 223, batch: 24, loss: 0.0202970951795578, acc: 98.4375, f1: 98.20574162679425, r: 0.8407724116126262
06/01/2019 10:42:14 step: 7389, epoch: 223, batch: 29, loss: 0.0023276060819625854, acc: 100.0, f1: 100.0, r: 0.8045612663201389
06/01/2019 10:42:14 *** evaluating ***
06/01/2019 10:42:14 step: 224, epoch: 223, acc: 60.68376068376068, f1: 23.311529740101168, r: 0.28320355005321246
06/01/2019 10:42:14 *** epoch: 225 ***
06/01/2019 10:42:14 *** training ***
06/01/2019 10:42:14 step: 7397, epoch: 224, batch: 4, loss: 0.0036637336015701294, acc: 100.0, f1: 100.0, r: 0.7208350300439734
06/01/2019 10:42:14 step: 7402, epoch: 224, batch: 9, loss: 0.013411790132522583, acc: 100.0, f1: 100.0, r: 0.690463968827276
06/01/2019 10:42:15 step: 7407, epoch: 224, batch: 14, loss: 0.0004188120365142822, acc: 100.0, f1: 100.0, r: 0.6577689839399342
06/01/2019 10:42:15 step: 7412, epoch: 224, batch: 19, loss: 0.015104129910469055, acc: 98.4375, f1: 98.39924670433145, r: 0.7594628515491021
06/01/2019 10:42:15 step: 7417, epoch: 224, batch: 24, loss: 0.033608827739953995, acc: 98.4375, f1: 95.62841530054644, r: 0.7368942156796027
06/01/2019 10:42:15 step: 7422, epoch: 224, batch: 29, loss: 0.0010937899351119995, acc: 100.0, f1: 100.0, r: 0.6479712722606993
06/01/2019 10:42:15 *** evaluating ***
06/01/2019 10:42:15 step: 225, epoch: 224, acc: 59.401709401709404, f1: 21.25042891861314, r: 0.2705472697009786
06/01/2019 10:42:15 *** epoch: 226 ***
06/01/2019 10:42:15 *** training ***
06/01/2019 10:42:16 step: 7430, epoch: 225, batch: 4, loss: 0.001472271978855133, acc: 100.0, f1: 100.0, r: 0.8358673438445634
06/01/2019 10:42:16 step: 7435, epoch: 225, batch: 9, loss: 0.0022306442260742188, acc: 100.0, f1: 100.0, r: 0.728685527297237
06/01/2019 10:42:16 step: 7440, epoch: 225, batch: 14, loss: 0.005577407777309418, acc: 100.0, f1: 100.0, r: 0.8410304516811888
06/01/2019 10:42:16 step: 7445, epoch: 225, batch: 19, loss: 0.001449868083000183, acc: 100.0, f1: 100.0, r: 0.6995938838159175
06/01/2019 10:42:17 step: 7450, epoch: 225, batch: 24, loss: 0.002638876438140869, acc: 100.0, f1: 100.0, r: 0.6668964146954404
06/01/2019 10:42:17 step: 7455, epoch: 225, batch: 29, loss: 0.038608938455581665, acc: 98.4375, f1: 98.60681114551085, r: 0.8039258923240035
06/01/2019 10:42:17 *** evaluating ***
06/01/2019 10:42:17 step: 226, epoch: 225, acc: 59.401709401709404, f1: 22.499062210587372, r: 0.28106691165012826
06/01/2019 10:42:17 *** epoch: 227 ***
06/01/2019 10:42:17 *** training ***
06/01/2019 10:42:17 step: 7463, epoch: 226, batch: 4, loss: 0.0018338784575462341, acc: 100.0, f1: 100.0, r: 0.7139891551284039
06/01/2019 10:42:17 step: 7468, epoch: 226, batch: 9, loss: 0.005960598587989807, acc: 100.0, f1: 100.0, r: 0.6908848228354756
06/01/2019 10:42:18 step: 7473, epoch: 226, batch: 14, loss: 0.0004847198724746704, acc: 100.0, f1: 100.0, r: 0.740918961631944
06/01/2019 10:42:18 step: 7478, epoch: 226, batch: 19, loss: 0.000643312931060791, acc: 100.0, f1: 100.0, r: 0.8256896765777701
06/01/2019 10:42:18 step: 7483, epoch: 226, batch: 24, loss: 0.0020411014556884766, acc: 100.0, f1: 100.0, r: 0.640502234474431
06/01/2019 10:42:18 step: 7488, epoch: 226, batch: 29, loss: 0.007160983979701996, acc: 100.0, f1: 100.0, r: 0.6643679053448589
06/01/2019 10:42:18 *** evaluating ***
06/01/2019 10:42:19 step: 227, epoch: 226, acc: 59.82905982905983, f1: 21.953598385509547, r: 0.2742314520783398
06/01/2019 10:42:19 *** epoch: 228 ***
06/01/2019 10:42:19 *** training ***
06/01/2019 10:42:19 step: 7496, epoch: 227, batch: 4, loss: 0.0032437294721603394, acc: 100.0, f1: 100.0, r: 0.6871342485010821
06/01/2019 10:42:19 step: 7501, epoch: 227, batch: 9, loss: 0.004681676626205444, acc: 100.0, f1: 100.0, r: 0.8470426620200381
06/01/2019 10:42:19 step: 7506, epoch: 227, batch: 14, loss: 0.021565839648246765, acc: 98.4375, f1: 99.31633407243163, r: 0.7794819151280447
06/01/2019 10:42:19 step: 7511, epoch: 227, batch: 19, loss: 0.0013555511832237244, acc: 100.0, f1: 100.0, r: 0.8293312172080736
06/01/2019 10:42:20 step: 7516, epoch: 227, batch: 24, loss: 0.008083619177341461, acc: 100.0, f1: 100.0, r: 0.78607927071231
06/01/2019 10:42:20 step: 7521, epoch: 227, batch: 29, loss: 0.016012854874134064, acc: 98.4375, f1: 98.26839826839827, r: 0.6682354022631621
06/01/2019 10:42:20 *** evaluating ***
06/01/2019 10:42:20 step: 228, epoch: 227, acc: 59.82905982905983, f1: 23.781616706145005, r: 0.2875344885224798
06/01/2019 10:42:20 *** epoch: 229 ***
06/01/2019 10:42:20 *** training ***
06/01/2019 10:42:20 step: 7529, epoch: 228, batch: 4, loss: 0.00376303493976593, acc: 100.0, f1: 100.0, r: 0.7790347066550944
06/01/2019 10:42:21 step: 7534, epoch: 228, batch: 9, loss: 0.04941315948963165, acc: 96.875, f1: 93.94794268814425, r: 0.7587371199969233
06/01/2019 10:42:21 step: 7539, epoch: 228, batch: 14, loss: 0.006945230066776276, acc: 100.0, f1: 100.0, r: 0.67831958763398
06/01/2019 10:42:21 step: 7544, epoch: 228, batch: 19, loss: 0.0017894729971885681, acc: 100.0, f1: 100.0, r: 0.7194813970439639
06/01/2019 10:42:21 step: 7549, epoch: 228, batch: 24, loss: 0.002621851861476898, acc: 100.0, f1: 100.0, r: 0.7858454084443925
06/01/2019 10:42:21 step: 7554, epoch: 228, batch: 29, loss: 0.008095324039459229, acc: 100.0, f1: 100.0, r: 0.6931445466708424
06/01/2019 10:42:22 *** evaluating ***
06/01/2019 10:42:22 step: 229, epoch: 228, acc: 59.82905982905983, f1: 21.62618083670715, r: 0.2870010315956282
06/01/2019 10:42:22 *** epoch: 230 ***
06/01/2019 10:42:22 *** training ***
06/01/2019 10:42:22 step: 7562, epoch: 229, batch: 4, loss: 0.0016261786222457886, acc: 100.0, f1: 100.0, r: 0.6809626212760131
06/01/2019 10:42:22 step: 7567, epoch: 229, batch: 9, loss: 0.0025179684162139893, acc: 100.0, f1: 100.0, r: 0.6879582236137226
06/01/2019 10:42:22 step: 7572, epoch: 229, batch: 14, loss: 0.009628057479858398, acc: 100.0, f1: 100.0, r: 0.6789035560783889
06/01/2019 10:42:23 step: 7577, epoch: 229, batch: 19, loss: 0.0018437206745147705, acc: 100.0, f1: 100.0, r: 0.7946689763876245
06/01/2019 10:42:23 step: 7582, epoch: 229, batch: 24, loss: 0.0019456073641777039, acc: 100.0, f1: 100.0, r: 0.8263310337529793
06/01/2019 10:42:23 step: 7587, epoch: 229, batch: 29, loss: 0.0013245269656181335, acc: 100.0, f1: 100.0, r: 0.6421337814609233
06/01/2019 10:42:23 *** evaluating ***
06/01/2019 10:42:23 step: 230, epoch: 229, acc: 58.97435897435898, f1: 19.6360153256705, r: 0.27600373192637306
06/01/2019 10:42:23 *** epoch: 231 ***
06/01/2019 10:42:23 *** training ***
06/01/2019 10:42:24 step: 7595, epoch: 230, batch: 4, loss: 0.010240167379379272, acc: 100.0, f1: 100.0, r: 0.8162441483707686
06/01/2019 10:42:24 step: 7600, epoch: 230, batch: 9, loss: 0.00040487945079803467, acc: 100.0, f1: 100.0, r: 0.6377516373175284
06/01/2019 10:42:24 step: 7605, epoch: 230, batch: 14, loss: 0.011522561311721802, acc: 100.0, f1: 100.0, r: 0.7549923087663895
06/01/2019 10:42:24 step: 7610, epoch: 230, batch: 19, loss: 0.035287000238895416, acc: 98.4375, f1: 85.0, r: 0.7385001282401252
06/01/2019 10:42:24 step: 7615, epoch: 230, batch: 24, loss: 0.0007940679788589478, acc: 100.0, f1: 100.0, r: 0.8152688505203936
06/01/2019 10:42:25 step: 7620, epoch: 230, batch: 29, loss: 0.0022172778844833374, acc: 100.0, f1: 100.0, r: 0.7358150979073433
06/01/2019 10:42:25 *** evaluating ***
06/01/2019 10:42:25 step: 231, epoch: 230, acc: 58.54700854700855, f1: 21.143353174603174, r: 0.27778616196034944
06/01/2019 10:42:25 *** epoch: 232 ***
06/01/2019 10:42:25 *** training ***
06/01/2019 10:42:25 step: 7628, epoch: 231, batch: 4, loss: 0.0002959519624710083, acc: 100.0, f1: 100.0, r: 0.7032742107365044
06/01/2019 10:42:25 step: 7633, epoch: 231, batch: 9, loss: 0.0019110441207885742, acc: 100.0, f1: 100.0, r: 0.8089319675639965
06/01/2019 10:42:26 step: 7638, epoch: 231, batch: 14, loss: 0.016806602478027344, acc: 98.4375, f1: 93.65079365079364, r: 0.717656667783002
06/01/2019 10:42:26 step: 7643, epoch: 231, batch: 19, loss: 0.0005001872777938843, acc: 100.0, f1: 100.0, r: 0.6952761739295189
06/01/2019 10:42:26 step: 7648, epoch: 231, batch: 24, loss: 0.0017635077238082886, acc: 100.0, f1: 100.0, r: 0.6570542344719937
06/01/2019 10:42:26 step: 7653, epoch: 231, batch: 29, loss: 0.0009087622165679932, acc: 100.0, f1: 100.0, r: 0.7667341121628295
06/01/2019 10:42:26 *** evaluating ***
06/01/2019 10:42:26 step: 232, epoch: 231, acc: 59.82905982905983, f1: 22.40748180884782, r: 0.27990691364186404
06/01/2019 10:42:26 *** epoch: 233 ***
06/01/2019 10:42:26 *** training ***
06/01/2019 10:42:27 step: 7661, epoch: 232, batch: 4, loss: 0.005043916404247284, acc: 100.0, f1: 100.0, r: 0.756860598383029
06/01/2019 10:42:27 step: 7666, epoch: 232, batch: 9, loss: 0.0012762248516082764, acc: 100.0, f1: 100.0, r: 0.6627634953733912
06/01/2019 10:42:27 step: 7671, epoch: 232, batch: 14, loss: 0.015037156641483307, acc: 100.0, f1: 100.0, r: 0.6839189409502766
06/01/2019 10:42:27 step: 7676, epoch: 232, batch: 19, loss: 0.015364393591880798, acc: 98.4375, f1: 99.19078742608154, r: 0.7097808546532189
06/01/2019 10:42:28 step: 7681, epoch: 232, batch: 24, loss: 0.0008141398429870605, acc: 100.0, f1: 100.0, r: 0.7646637908086085
06/01/2019 10:42:28 step: 7686, epoch: 232, batch: 29, loss: 0.0008302107453346252, acc: 100.0, f1: 100.0, r: 0.679676901358327
06/01/2019 10:42:28 *** evaluating ***
06/01/2019 10:42:28 step: 233, epoch: 232, acc: 58.54700854700855, f1: 19.511778841777566, r: 0.27859189837183834
06/01/2019 10:42:28 *** epoch: 234 ***
06/01/2019 10:42:28 *** training ***
06/01/2019 10:42:28 step: 7694, epoch: 233, batch: 4, loss: 0.02001097798347473, acc: 100.0, f1: 100.0, r: 0.7923160712454219
06/01/2019 10:42:29 step: 7699, epoch: 233, batch: 9, loss: 0.0002071782946586609, acc: 100.0, f1: 100.0, r: 0.7095119310124776
06/01/2019 10:42:29 step: 7704, epoch: 233, batch: 14, loss: 0.0010309219360351562, acc: 100.0, f1: 100.0, r: 0.7617291793439772
06/01/2019 10:42:29 step: 7709, epoch: 233, batch: 19, loss: 0.0013623759150505066, acc: 100.0, f1: 100.0, r: 0.697949752332294
06/01/2019 10:42:29 step: 7714, epoch: 233, batch: 24, loss: 0.004930190742015839, acc: 100.0, f1: 100.0, r: 0.7641126084118124
06/01/2019 10:42:30 step: 7719, epoch: 233, batch: 29, loss: 0.00745798647403717, acc: 100.0, f1: 100.0, r: 0.8134585771913934
06/01/2019 10:42:30 *** evaluating ***
06/01/2019 10:42:30 step: 234, epoch: 233, acc: 59.82905982905983, f1: 22.40748180884782, r: 0.28545155803777233
06/01/2019 10:42:30 *** epoch: 235 ***
06/01/2019 10:42:30 *** training ***
06/01/2019 10:42:30 step: 7727, epoch: 234, batch: 4, loss: 0.0008417069911956787, acc: 100.0, f1: 100.0, r: 0.6696761076777801
06/01/2019 10:42:30 step: 7732, epoch: 234, batch: 9, loss: 0.002360038459300995, acc: 100.0, f1: 100.0, r: 0.8179408351018738
06/01/2019 10:42:30 step: 7737, epoch: 234, batch: 14, loss: 0.0012605339288711548, acc: 100.0, f1: 100.0, r: 0.7012439081452915
06/01/2019 10:42:31 step: 7742, epoch: 234, batch: 19, loss: 0.0016835778951644897, acc: 100.0, f1: 100.0, r: 0.842659363908089
06/01/2019 10:42:31 step: 7747, epoch: 234, batch: 24, loss: 0.0017829611897468567, acc: 100.0, f1: 100.0, r: 0.7395477491913641
06/01/2019 10:42:31 step: 7752, epoch: 234, batch: 29, loss: 0.000544622540473938, acc: 100.0, f1: 100.0, r: 0.6420040318391574
06/01/2019 10:42:31 *** evaluating ***
06/01/2019 10:42:31 step: 235, epoch: 234, acc: 58.97435897435898, f1: 20.56930002093656, r: 0.27584262059889547
06/01/2019 10:42:31 *** epoch: 236 ***
06/01/2019 10:42:31 *** training ***
06/01/2019 10:42:32 step: 7760, epoch: 235, batch: 4, loss: 0.003672003746032715, acc: 100.0, f1: 100.0, r: 0.8271206585242985
06/01/2019 10:42:32 step: 7765, epoch: 235, batch: 9, loss: 0.014037303626537323, acc: 98.4375, f1: 98.17219817219816, r: 0.6713635048813895
06/01/2019 10:42:32 step: 7770, epoch: 235, batch: 14, loss: 0.010207980871200562, acc: 100.0, f1: 100.0, r: 0.6920115370459985
06/01/2019 10:42:32 step: 7775, epoch: 235, batch: 19, loss: 0.0008372515439987183, acc: 100.0, f1: 100.0, r: 0.7974450717188069
06/01/2019 10:42:33 step: 7780, epoch: 235, batch: 24, loss: 0.0024563968181610107, acc: 100.0, f1: 100.0, r: 0.8114438305275352
06/01/2019 10:42:33 step: 7785, epoch: 235, batch: 29, loss: 0.0005601048469543457, acc: 100.0, f1: 100.0, r: 0.625516016603539
06/01/2019 10:42:33 *** evaluating ***
06/01/2019 10:42:33 step: 236, epoch: 235, acc: 58.54700854700855, f1: 21.113386321016925, r: 0.28220083739028534
06/01/2019 10:42:33 *** epoch: 237 ***
06/01/2019 10:42:33 *** training ***
06/01/2019 10:42:33 step: 7793, epoch: 236, batch: 4, loss: 0.0015417933464050293, acc: 100.0, f1: 100.0, r: 0.6113314440006895
06/01/2019 10:42:33 step: 7798, epoch: 236, batch: 9, loss: 0.00027285516262054443, acc: 100.0, f1: 100.0, r: 0.8269727566708635
06/01/2019 10:42:34 step: 7803, epoch: 236, batch: 14, loss: 0.002722412347793579, acc: 100.0, f1: 100.0, r: 0.7202406029625563
06/01/2019 10:42:34 step: 7808, epoch: 236, batch: 19, loss: 0.000511758029460907, acc: 100.0, f1: 100.0, r: 0.7708885599320603
06/01/2019 10:42:34 step: 7813, epoch: 236, batch: 24, loss: 0.003268621861934662, acc: 100.0, f1: 100.0, r: 0.7368557025918828
06/01/2019 10:42:34 step: 7818, epoch: 236, batch: 29, loss: 0.002134852111339569, acc: 100.0, f1: 100.0, r: 0.7174456889658984
06/01/2019 10:42:34 *** evaluating ***
06/01/2019 10:42:35 step: 237, epoch: 236, acc: 58.119658119658126, f1: 20.2092803030303, r: 0.2801284371983945
06/01/2019 10:42:35 *** epoch: 238 ***
06/01/2019 10:42:35 *** training ***
06/01/2019 10:42:35 step: 7826, epoch: 237, batch: 4, loss: 0.009066179394721985, acc: 100.0, f1: 100.0, r: 0.7323490418379895
06/01/2019 10:42:35 step: 7831, epoch: 237, batch: 9, loss: 0.0036469697952270508, acc: 100.0, f1: 100.0, r: 0.7645172418435938
06/01/2019 10:42:35 step: 7836, epoch: 237, batch: 14, loss: 0.01266072690486908, acc: 100.0, f1: 100.0, r: 0.7751279939183483
06/01/2019 10:42:36 step: 7841, epoch: 237, batch: 19, loss: 0.00021156668663024902, acc: 100.0, f1: 100.0, r: 0.8067408435761467
06/01/2019 10:42:36 step: 7846, epoch: 237, batch: 24, loss: 0.0030497536063194275, acc: 100.0, f1: 100.0, r: 0.6462726512240272
06/01/2019 10:42:36 step: 7851, epoch: 237, batch: 29, loss: 0.0008807629346847534, acc: 100.0, f1: 100.0, r: 0.7718144623442686
06/01/2019 10:42:36 *** evaluating ***
06/01/2019 10:42:36 step: 238, epoch: 237, acc: 59.401709401709404, f1: 21.009424533016514, r: 0.28190918132304843
06/01/2019 10:42:36 *** epoch: 239 ***
06/01/2019 10:42:36 *** training ***
06/01/2019 10:42:36 step: 7859, epoch: 238, batch: 4, loss: 0.011910583823919296, acc: 100.0, f1: 100.0, r: 0.8589523359795024
06/01/2019 10:42:37 step: 7864, epoch: 238, batch: 9, loss: 0.028554154559969902, acc: 98.4375, f1: 98.49624060150377, r: 0.6989313467834792
06/01/2019 10:42:37 step: 7869, epoch: 238, batch: 14, loss: 0.0030697882175445557, acc: 100.0, f1: 100.0, r: 0.8222334845015455
06/01/2019 10:42:37 step: 7874, epoch: 238, batch: 19, loss: 0.00043079257011413574, acc: 100.0, f1: 100.0, r: 0.7219705319715808
06/01/2019 10:42:37 step: 7879, epoch: 238, batch: 24, loss: 0.0038842856884002686, acc: 100.0, f1: 100.0, r: 0.8251048049037513
06/01/2019 10:42:38 step: 7884, epoch: 238, batch: 29, loss: 0.00205424427986145, acc: 100.0, f1: 100.0, r: 0.6874582282688857
06/01/2019 10:42:38 *** evaluating ***
06/01/2019 10:42:38 step: 239, epoch: 238, acc: 59.401709401709404, f1: 20.707259291925933, r: 0.27852485288571655
06/01/2019 10:42:38 *** epoch: 240 ***
06/01/2019 10:42:38 *** training ***
06/01/2019 10:42:38 step: 7892, epoch: 239, batch: 4, loss: 0.01694781333208084, acc: 98.4375, f1: 98.37526205450733, r: 0.8218691618015022
06/01/2019 10:42:38 step: 7897, epoch: 239, batch: 9, loss: 0.003674820065498352, acc: 100.0, f1: 100.0, r: 0.664657317520003
06/01/2019 10:42:38 step: 7902, epoch: 239, batch: 14, loss: 0.0016386806964874268, acc: 100.0, f1: 100.0, r: 0.6878735643225199
06/01/2019 10:42:39 step: 7907, epoch: 239, batch: 19, loss: 0.001178443431854248, acc: 100.0, f1: 100.0, r: 0.6936609214170192
06/01/2019 10:42:39 step: 7912, epoch: 239, batch: 24, loss: 0.0037436112761497498, acc: 100.0, f1: 100.0, r: 0.7649426112910039
06/01/2019 10:42:39 step: 7917, epoch: 239, batch: 29, loss: 0.0017568543553352356, acc: 100.0, f1: 100.0, r: 0.7529020098694068
06/01/2019 10:42:39 *** evaluating ***
06/01/2019 10:42:39 step: 240, epoch: 239, acc: 58.54700854700855, f1: 18.29356060606061, r: 0.26999414538438027
06/01/2019 10:42:39 *** epoch: 241 ***
06/01/2019 10:42:39 *** training ***
06/01/2019 10:42:40 step: 7925, epoch: 240, batch: 4, loss: 0.013415329158306122, acc: 100.0, f1: 100.0, r: 0.661049296109296
06/01/2019 10:42:40 step: 7930, epoch: 240, batch: 9, loss: 0.010439500212669373, acc: 100.0, f1: 100.0, r: 0.7331287119088606
06/01/2019 10:42:40 step: 7935, epoch: 240, batch: 14, loss: 0.000873759388923645, acc: 100.0, f1: 100.0, r: 0.8208172046959457
06/01/2019 10:42:40 step: 7940, epoch: 240, batch: 19, loss: 0.015673212707042694, acc: 98.4375, f1: 97.74891774891775, r: 0.6605682569936541
06/01/2019 10:42:40 step: 7945, epoch: 240, batch: 24, loss: 0.004196658730506897, acc: 100.0, f1: 100.0, r: 0.82520119427669
06/01/2019 10:42:41 step: 7950, epoch: 240, batch: 29, loss: 0.0007429122924804688, acc: 100.0, f1: 100.0, r: 0.746001544781252
06/01/2019 10:42:41 *** evaluating ***
06/01/2019 10:42:41 step: 241, epoch: 240, acc: 58.97435897435898, f1: 20.5129015049416, r: 0.2653994946542212
06/01/2019 10:42:41 *** epoch: 242 ***
06/01/2019 10:42:41 *** training ***
06/01/2019 10:42:41 step: 7958, epoch: 241, batch: 4, loss: 0.002902798354625702, acc: 100.0, f1: 100.0, r: 0.6197628686301303
06/01/2019 10:42:41 step: 7963, epoch: 241, batch: 9, loss: 0.0013965144753456116, acc: 100.0, f1: 100.0, r: 0.8278046458133896
06/01/2019 10:42:42 step: 7968, epoch: 241, batch: 14, loss: 0.0016803890466690063, acc: 100.0, f1: 100.0, r: 0.7457243510972413
06/01/2019 10:42:42 step: 7973, epoch: 241, batch: 19, loss: 0.8838927149772644, acc: 89.0625, f1: 91.64715719063545, r: 0.6759236535137179
06/01/2019 10:42:42 step: 7978, epoch: 241, batch: 24, loss: 0.015479251742362976, acc: 98.4375, f1: 94.61697722567288, r: 0.6733961671206683
06/01/2019 10:42:42 step: 7983, epoch: 241, batch: 29, loss: 0.0013102591037750244, acc: 100.0, f1: 100.0, r: 0.6086281560975461
06/01/2019 10:42:42 *** evaluating ***
06/01/2019 10:42:42 step: 242, epoch: 241, acc: 58.54700854700855, f1: 21.104075955491048, r: 0.26641395001597346
06/01/2019 10:42:42 *** epoch: 243 ***
06/01/2019 10:42:42 *** training ***
06/01/2019 10:42:43 step: 7991, epoch: 242, batch: 4, loss: 0.01769794523715973, acc: 100.0, f1: 100.0, r: 0.8221514314096915
06/01/2019 10:42:43 step: 7996, epoch: 242, batch: 9, loss: 0.0007426217198371887, acc: 100.0, f1: 100.0, r: 0.7871671750821712
06/01/2019 10:42:43 step: 8001, epoch: 242, batch: 14, loss: 0.015083052217960358, acc: 98.4375, f1: 99.06896551724138, r: 0.7578800781505559
06/01/2019 10:42:43 step: 8006, epoch: 242, batch: 19, loss: 0.009295366704463959, acc: 100.0, f1: 100.0, r: 0.8143976167957724
06/01/2019 10:42:44 step: 8011, epoch: 242, batch: 24, loss: 0.024897389113903046, acc: 98.4375, f1: 98.98838004101161, r: 0.689461223153693
06/01/2019 10:42:44 step: 8016, epoch: 242, batch: 29, loss: 0.02944789081811905, acc: 98.4375, f1: 98.10874704491727, r: 0.7141730122010869
06/01/2019 10:42:44 *** evaluating ***
06/01/2019 10:42:44 step: 243, epoch: 242, acc: 57.692307692307686, f1: 21.8038323683485, r: 0.2761677478611113
06/01/2019 10:42:44 *** epoch: 244 ***
06/01/2019 10:42:44 *** training ***
06/01/2019 10:42:44 step: 8024, epoch: 243, batch: 4, loss: 0.03053896874189377, acc: 98.4375, f1: 91.66666666666666, r: 0.8176632171142477
06/01/2019 10:42:45 step: 8029, epoch: 243, batch: 9, loss: 0.0008546561002731323, acc: 100.0, f1: 100.0, r: 0.81466130995571
06/01/2019 10:42:45 step: 8034, epoch: 243, batch: 14, loss: 0.01758519560098648, acc: 100.0, f1: 100.0, r: 0.699186008942377
06/01/2019 10:42:45 step: 8039, epoch: 243, batch: 19, loss: 0.0005827546119689941, acc: 100.0, f1: 100.0, r: 0.8210824296778364
06/01/2019 10:42:45 step: 8044, epoch: 243, batch: 24, loss: 0.0017185136675834656, acc: 100.0, f1: 100.0, r: 0.7324109165229657
06/01/2019 10:42:46 step: 8049, epoch: 243, batch: 29, loss: 0.002620615065097809, acc: 100.0, f1: 100.0, r: 0.6729556996688298
06/01/2019 10:42:46 *** evaluating ***
06/01/2019 10:42:46 step: 244, epoch: 243, acc: 57.692307692307686, f1: 21.76581035512251, r: 0.2737908837510919
06/01/2019 10:42:46 *** epoch: 245 ***
06/01/2019 10:42:46 *** training ***
06/01/2019 10:42:46 step: 8057, epoch: 244, batch: 4, loss: 0.0024763718247413635, acc: 100.0, f1: 100.0, r: 0.7695933077006665
06/01/2019 10:42:46 step: 8062, epoch: 244, batch: 9, loss: 0.0038113370537757874, acc: 100.0, f1: 100.0, r: 0.6627353395598453
06/01/2019 10:42:46 step: 8067, epoch: 244, batch: 14, loss: 0.0004956275224685669, acc: 100.0, f1: 100.0, r: 0.7843468579589248
06/01/2019 10:42:47 step: 8072, epoch: 244, batch: 19, loss: 0.0013213008642196655, acc: 100.0, f1: 100.0, r: 0.7854323140993509
06/01/2019 10:42:47 step: 8077, epoch: 244, batch: 24, loss: 0.0036808252334594727, acc: 100.0, f1: 100.0, r: 0.7369080448540182
06/01/2019 10:42:47 step: 8082, epoch: 244, batch: 29, loss: 0.001653619110584259, acc: 100.0, f1: 100.0, r: 0.7790165569243671
06/01/2019 10:42:47 *** evaluating ***
06/01/2019 10:42:47 step: 245, epoch: 244, acc: 60.68376068376068, f1: 23.268868607851655, r: 0.27653598555156517
06/01/2019 10:42:47 *** epoch: 246 ***
06/01/2019 10:42:47 *** training ***
06/01/2019 10:42:48 step: 8090, epoch: 245, batch: 4, loss: 0.0030351728200912476, acc: 100.0, f1: 100.0, r: 0.833407328234789
06/01/2019 10:42:48 step: 8095, epoch: 245, batch: 9, loss: 0.0006881654262542725, acc: 100.0, f1: 100.0, r: 0.7208875224561446
06/01/2019 10:42:48 step: 8100, epoch: 245, batch: 14, loss: 0.00834926962852478, acc: 100.0, f1: 100.0, r: 0.8142585195773134
06/01/2019 10:42:48 step: 8105, epoch: 245, batch: 19, loss: 0.006759166717529297, acc: 100.0, f1: 100.0, r: 0.7973610917738219
06/01/2019 10:42:49 step: 8110, epoch: 245, batch: 24, loss: 0.00540006160736084, acc: 100.0, f1: 100.0, r: 0.8212429587005077
06/01/2019 10:42:49 step: 8115, epoch: 245, batch: 29, loss: 0.022298015654087067, acc: 100.0, f1: 100.0, r: 0.6445339248386213
06/01/2019 10:42:49 *** evaluating ***
06/01/2019 10:42:49 step: 246, epoch: 245, acc: 60.68376068376068, f1: 23.75593466553288, r: 0.27799371482371477
06/01/2019 10:42:49 *** epoch: 247 ***
06/01/2019 10:42:49 *** training ***
06/01/2019 10:42:50 step: 8123, epoch: 246, batch: 4, loss: 0.00209103524684906, acc: 100.0, f1: 100.0, r: 0.7678207065095486
06/01/2019 10:42:50 step: 8128, epoch: 246, batch: 9, loss: 0.01574414223432541, acc: 98.4375, f1: 98.50649350649351, r: 0.81446166927313
06/01/2019 10:42:50 step: 8133, epoch: 246, batch: 14, loss: 0.0018554702401161194, acc: 100.0, f1: 100.0, r: 0.8077837750331702
06/01/2019 10:42:50 step: 8138, epoch: 246, batch: 19, loss: 0.00034405291080474854, acc: 100.0, f1: 100.0, r: 0.7726621694911858
06/01/2019 10:42:51 step: 8143, epoch: 246, batch: 24, loss: 0.0044453442096710205, acc: 100.0, f1: 100.0, r: 0.8323863111519475
06/01/2019 10:42:51 step: 8148, epoch: 246, batch: 29, loss: 0.03645859658718109, acc: 98.4375, f1: 94.66666666666667, r: 0.6541914943472195
06/01/2019 10:42:51 *** evaluating ***
06/01/2019 10:42:51 step: 247, epoch: 246, acc: 60.256410256410255, f1: 21.917953918968117, r: 0.27949699877011014
06/01/2019 10:42:51 *** epoch: 248 ***
06/01/2019 10:42:51 *** training ***
06/01/2019 10:42:51 step: 8156, epoch: 247, batch: 4, loss: 0.006707541644573212, acc: 100.0, f1: 100.0, r: 0.7966410070098603
06/01/2019 10:42:52 step: 8161, epoch: 247, batch: 9, loss: 0.002023652195930481, acc: 100.0, f1: 100.0, r: 0.8343979366702053
06/01/2019 10:42:52 step: 8166, epoch: 247, batch: 14, loss: 0.0002938210964202881, acc: 100.0, f1: 100.0, r: 0.742563740233583
06/01/2019 10:42:52 step: 8171, epoch: 247, batch: 19, loss: 0.0058126673102378845, acc: 100.0, f1: 100.0, r: 0.8177448000023687
06/01/2019 10:42:52 step: 8176, epoch: 247, batch: 24, loss: 0.017482377588748932, acc: 100.0, f1: 100.0, r: 0.73808222397151
06/01/2019 10:42:53 step: 8181, epoch: 247, batch: 29, loss: 0.0006517320871353149, acc: 100.0, f1: 100.0, r: 0.7074691932228038
06/01/2019 10:42:53 *** evaluating ***
06/01/2019 10:42:53 step: 248, epoch: 247, acc: 60.256410256410255, f1: 22.610707803992742, r: 0.28836431704302135
06/01/2019 10:42:53 *** epoch: 249 ***
06/01/2019 10:42:53 *** training ***
06/01/2019 10:42:53 step: 8189, epoch: 248, batch: 4, loss: 0.0009320825338363647, acc: 100.0, f1: 100.0, r: 0.771149598730465
06/01/2019 10:42:53 step: 8194, epoch: 248, batch: 9, loss: 0.0006610006093978882, acc: 100.0, f1: 100.0, r: 0.6866162524613485
06/01/2019 10:42:54 step: 8199, epoch: 248, batch: 14, loss: 0.0010734647512435913, acc: 100.0, f1: 100.0, r: 0.7296304747153358
06/01/2019 10:42:54 step: 8204, epoch: 248, batch: 19, loss: 0.0018388181924819946, acc: 100.0, f1: 100.0, r: 0.6711977255372451
06/01/2019 10:42:54 step: 8209, epoch: 248, batch: 24, loss: 0.004297330975532532, acc: 100.0, f1: 100.0, r: 0.6555551559442051
06/01/2019 10:42:54 step: 8214, epoch: 248, batch: 29, loss: 0.010193221271038055, acc: 100.0, f1: 100.0, r: 0.7926505606909191
06/01/2019 10:42:55 *** evaluating ***
06/01/2019 10:42:55 step: 249, epoch: 248, acc: 59.82905982905983, f1: 23.017445116487455, r: 0.2880284103939485
06/01/2019 10:42:55 *** epoch: 250 ***
06/01/2019 10:42:55 *** training ***
06/01/2019 10:42:55 step: 8222, epoch: 249, batch: 4, loss: 0.06526212394237518, acc: 96.875, f1: 94.13919413919413, r: 0.7145030321044634
06/01/2019 10:42:55 step: 8227, epoch: 249, batch: 9, loss: 0.006982378661632538, acc: 100.0, f1: 100.0, r: 0.6696009023784864
06/01/2019 10:42:55 step: 8232, epoch: 249, batch: 14, loss: 0.0023270845413208008, acc: 100.0, f1: 100.0, r: 0.7322011213348054
06/01/2019 10:42:56 step: 8237, epoch: 249, batch: 19, loss: 0.0004069581627845764, acc: 100.0, f1: 100.0, r: 0.7046085713632554
06/01/2019 10:42:56 step: 8242, epoch: 249, batch: 24, loss: 0.006609000265598297, acc: 100.0, f1: 100.0, r: 0.73918814852891
06/01/2019 10:42:56 step: 8247, epoch: 249, batch: 29, loss: 0.005291447043418884, acc: 100.0, f1: 100.0, r: 0.8289126923079246
06/01/2019 10:42:56 *** evaluating ***
06/01/2019 10:42:56 step: 250, epoch: 249, acc: 59.82905982905983, f1: 22.40748180884782, r: 0.27967309072942553
06/01/2019 10:42:56 *** epoch: 251 ***
06/01/2019 10:42:56 *** training ***
06/01/2019 10:42:57 step: 8255, epoch: 250, batch: 4, loss: 0.0019684284925460815, acc: 100.0, f1: 100.0, r: 0.8169117957895393
06/01/2019 10:42:57 step: 8260, epoch: 250, batch: 9, loss: 0.0009201020002365112, acc: 100.0, f1: 100.0, r: 0.8000662855089107
06/01/2019 10:42:57 step: 8265, epoch: 250, batch: 14, loss: 0.008056119084358215, acc: 100.0, f1: 100.0, r: 0.7565073229514344
06/01/2019 10:42:57 step: 8270, epoch: 250, batch: 19, loss: 0.00902143120765686, acc: 100.0, f1: 100.0, r: 0.7906266245725159
06/01/2019 10:42:58 step: 8275, epoch: 250, batch: 24, loss: 0.010330893099308014, acc: 100.0, f1: 100.0, r: 0.7038342954637669
06/01/2019 10:42:58 step: 8280, epoch: 250, batch: 29, loss: 0.005815476179122925, acc: 100.0, f1: 100.0, r: 0.7785617763982804
06/01/2019 10:42:58 *** evaluating ***
06/01/2019 10:42:58 step: 251, epoch: 250, acc: 59.82905982905983, f1: 22.40748180884782, r: 0.27240316957328653
06/01/2019 10:42:58 *** epoch: 252 ***
06/01/2019 10:42:58 *** training ***
06/01/2019 10:42:58 step: 8288, epoch: 251, batch: 4, loss: 0.017409607768058777, acc: 98.4375, f1: 99.33081674673987, r: 0.786334310674747
06/01/2019 10:42:59 step: 8293, epoch: 251, batch: 9, loss: 0.003313325345516205, acc: 100.0, f1: 100.0, r: 0.7199315130826307
06/01/2019 10:42:59 step: 8298, epoch: 251, batch: 14, loss: 0.0021783262491226196, acc: 100.0, f1: 100.0, r: 0.7933815337242087
06/01/2019 10:42:59 step: 8303, epoch: 251, batch: 19, loss: 0.0018266364932060242, acc: 100.0, f1: 100.0, r: 0.8306718416396559
06/01/2019 10:42:59 step: 8308, epoch: 251, batch: 24, loss: 0.0038567036390304565, acc: 100.0, f1: 100.0, r: 0.784402747767021
06/01/2019 10:42:59 step: 8313, epoch: 251, batch: 29, loss: 0.0004157125949859619, acc: 100.0, f1: 100.0, r: 0.6967751166643261
06/01/2019 10:43:00 *** evaluating ***
06/01/2019 10:43:00 step: 252, epoch: 251, acc: 60.256410256410255, f1: 22.610707803992742, r: 0.272773543102194
06/01/2019 10:43:00 *** epoch: 253 ***
06/01/2019 10:43:00 *** training ***
06/01/2019 10:43:00 step: 8321, epoch: 252, batch: 4, loss: 0.0007945597171783447, acc: 100.0, f1: 100.0, r: 0.7913022077904658
06/01/2019 10:43:00 step: 8326, epoch: 252, batch: 9, loss: 0.004873014986515045, acc: 100.0, f1: 100.0, r: 0.6998176222712896
06/01/2019 10:43:00 step: 8331, epoch: 252, batch: 14, loss: 0.006811276078224182, acc: 100.0, f1: 100.0, r: 0.7660832608344936
06/01/2019 10:43:01 step: 8336, epoch: 252, batch: 19, loss: 0.012637980282306671, acc: 100.0, f1: 100.0, r: 0.8583440762562913
06/01/2019 10:43:01 step: 8341, epoch: 252, batch: 24, loss: 0.004844367504119873, acc: 100.0, f1: 100.0, r: 0.6619675126903832
06/01/2019 10:43:01 step: 8346, epoch: 252, batch: 29, loss: 0.0019523799419403076, acc: 100.0, f1: 100.0, r: 0.6802753493936764
06/01/2019 10:43:01 *** evaluating ***
06/01/2019 10:43:01 step: 253, epoch: 252, acc: 58.97435897435898, f1: 22.004368486488975, r: 0.2725530214360064
06/01/2019 10:43:01 *** epoch: 254 ***
06/01/2019 10:43:01 *** training ***
06/01/2019 10:43:02 step: 8354, epoch: 253, batch: 4, loss: 0.001059025526046753, acc: 100.0, f1: 100.0, r: 0.8000760110693033
06/01/2019 10:43:02 step: 8359, epoch: 253, batch: 9, loss: 0.008861854672431946, acc: 100.0, f1: 100.0, r: 0.7338801016848532
06/01/2019 10:43:02 step: 8364, epoch: 253, batch: 14, loss: 0.0012206286191940308, acc: 100.0, f1: 100.0, r: 0.796528796282185
06/01/2019 10:43:02 step: 8369, epoch: 253, batch: 19, loss: 0.0023492276668548584, acc: 100.0, f1: 100.0, r: 0.6964600131923085
06/01/2019 10:43:02 step: 8374, epoch: 253, batch: 24, loss: 0.0008866190910339355, acc: 100.0, f1: 100.0, r: 0.7956114687524407
06/01/2019 10:43:03 step: 8379, epoch: 253, batch: 29, loss: 0.0008457228541374207, acc: 100.0, f1: 100.0, r: 0.6859874712711939
06/01/2019 10:43:03 *** evaluating ***
06/01/2019 10:43:03 step: 254, epoch: 253, acc: 59.401709401709404, f1: 21.510089386965515, r: 0.2754295711481874
06/01/2019 10:43:03 *** epoch: 255 ***
06/01/2019 10:43:03 *** training ***
06/01/2019 10:43:03 step: 8387, epoch: 254, batch: 4, loss: 0.0011306330561637878, acc: 100.0, f1: 100.0, r: 0.8135854751828152
06/01/2019 10:43:03 step: 8392, epoch: 254, batch: 9, loss: 0.004129223525524139, acc: 100.0, f1: 100.0, r: 0.7783188366241295
06/01/2019 10:43:04 step: 8397, epoch: 254, batch: 14, loss: 0.0017565786838531494, acc: 100.0, f1: 100.0, r: 0.8157957080583386
06/01/2019 10:43:04 step: 8402, epoch: 254, batch: 19, loss: 0.0017520934343338013, acc: 100.0, f1: 100.0, r: 0.700118995036498
06/01/2019 10:43:04 step: 8407, epoch: 254, batch: 24, loss: 0.0018192827701568604, acc: 100.0, f1: 100.0, r: 0.6123860650083426
06/01/2019 10:43:04 step: 8412, epoch: 254, batch: 29, loss: 0.010962717235088348, acc: 100.0, f1: 100.0, r: 0.7813143203296699
06/01/2019 10:43:04 *** evaluating ***
06/01/2019 10:43:05 step: 255, epoch: 254, acc: 59.82905982905983, f1: 22.40748180884782, r: 0.276681362204165
06/01/2019 10:43:05 *** epoch: 256 ***
06/01/2019 10:43:05 *** training ***
06/01/2019 10:43:05 step: 8420, epoch: 255, batch: 4, loss: 0.005141504108905792, acc: 100.0, f1: 100.0, r: 0.6794209637772033
06/01/2019 10:43:05 step: 8425, epoch: 255, batch: 9, loss: 0.004394330084323883, acc: 100.0, f1: 100.0, r: 0.828573622012606
06/01/2019 10:43:05 step: 8430, epoch: 255, batch: 14, loss: 0.0004939734935760498, acc: 100.0, f1: 100.0, r: 0.7645564524915157
06/01/2019 10:43:06 step: 8435, epoch: 255, batch: 19, loss: 0.006218172609806061, acc: 100.0, f1: 100.0, r: 0.6970521502595731
06/01/2019 10:43:06 step: 8440, epoch: 255, batch: 24, loss: 0.0010283738374710083, acc: 100.0, f1: 100.0, r: 0.7303874217907598
06/01/2019 10:43:06 step: 8445, epoch: 255, batch: 29, loss: 0.0004943609237670898, acc: 100.0, f1: 100.0, r: 0.6686531219015639
06/01/2019 10:43:06 *** evaluating ***
06/01/2019 10:43:06 step: 256, epoch: 255, acc: 59.401709401709404, f1: 21.510089386965515, r: 0.28333062169448886
06/01/2019 10:43:06 *** epoch: 257 ***
06/01/2019 10:43:06 *** training ***
06/01/2019 10:43:06 step: 8453, epoch: 256, batch: 4, loss: 0.0037208423018455505, acc: 100.0, f1: 100.0, r: 0.7673675947747657
06/01/2019 10:43:07 step: 8458, epoch: 256, batch: 9, loss: 0.0042225755751132965, acc: 100.0, f1: 100.0, r: 0.6890063647908911
06/01/2019 10:43:07 step: 8463, epoch: 256, batch: 14, loss: 0.00257217139005661, acc: 100.0, f1: 100.0, r: 0.780674688979436
06/01/2019 10:43:07 step: 8468, epoch: 256, batch: 19, loss: 0.001214936375617981, acc: 100.0, f1: 100.0, r: 0.7145570871600065
06/01/2019 10:43:07 step: 8473, epoch: 256, batch: 24, loss: 0.010236233472824097, acc: 100.0, f1: 100.0, r: 0.7158487137518234
06/01/2019 10:43:08 step: 8478, epoch: 256, batch: 29, loss: 0.0009011924266815186, acc: 100.0, f1: 100.0, r: 0.6847024992862973
06/01/2019 10:43:08 *** evaluating ***
06/01/2019 10:43:08 step: 257, epoch: 256, acc: 59.82905982905983, f1: 22.40748180884782, r: 0.2781260365055238
06/01/2019 10:43:08 *** epoch: 258 ***
06/01/2019 10:43:08 *** training ***
06/01/2019 10:43:08 step: 8486, epoch: 257, batch: 4, loss: 0.0005764439702033997, acc: 100.0, f1: 100.0, r: 0.7343013705176413
06/01/2019 10:43:08 step: 8491, epoch: 257, batch: 9, loss: 0.006388142704963684, acc: 100.0, f1: 100.0, r: 0.7931665827572163
06/01/2019 10:43:09 step: 8496, epoch: 257, batch: 14, loss: 0.011440657079219818, acc: 100.0, f1: 100.0, r: 0.6403880814409114
06/01/2019 10:43:09 step: 8501, epoch: 257, batch: 19, loss: 0.002393573522567749, acc: 100.0, f1: 100.0, r: 0.7569677726518291
06/01/2019 10:43:09 step: 8506, epoch: 257, batch: 24, loss: 0.001094236969947815, acc: 100.0, f1: 100.0, r: 0.7395966438974675
06/01/2019 10:43:09 step: 8511, epoch: 257, batch: 29, loss: 0.0008903443813323975, acc: 100.0, f1: 100.0, r: 0.6719237637133465
06/01/2019 10:43:09 *** evaluating ***
06/01/2019 10:43:09 step: 258, epoch: 257, acc: 60.68376068376068, f1: 23.370801033591732, r: 0.2786750474249939
06/01/2019 10:43:09 *** epoch: 259 ***
06/01/2019 10:43:09 *** training ***
06/01/2019 10:43:10 step: 8519, epoch: 258, batch: 4, loss: 0.004868052899837494, acc: 100.0, f1: 100.0, r: 0.8213199174842019
06/01/2019 10:43:10 step: 8524, epoch: 258, batch: 9, loss: 0.0005724877119064331, acc: 100.0, f1: 100.0, r: 0.7627667970874861
06/01/2019 10:43:10 step: 8529, epoch: 258, batch: 14, loss: 0.00032551586627960205, acc: 100.0, f1: 100.0, r: 0.7462662544420557
06/01/2019 10:43:10 step: 8534, epoch: 258, batch: 19, loss: 0.003627397119998932, acc: 100.0, f1: 100.0, r: 0.6122332951699195
06/01/2019 10:43:11 step: 8539, epoch: 258, batch: 24, loss: 0.0010963678359985352, acc: 100.0, f1: 100.0, r: 0.7836519375022308
06/01/2019 10:43:11 step: 8544, epoch: 258, batch: 29, loss: 0.00013637542724609375, acc: 100.0, f1: 100.0, r: 0.6885739041517869
06/01/2019 10:43:11 *** evaluating ***
06/01/2019 10:43:11 step: 259, epoch: 258, acc: 59.82905982905983, f1: 21.6718485334503, r: 0.2805812263275731
06/01/2019 10:43:11 *** epoch: 260 ***
06/01/2019 10:43:11 *** training ***
06/01/2019 10:43:11 step: 8552, epoch: 259, batch: 4, loss: 0.007108796387910843, acc: 100.0, f1: 100.0, r: 0.6983143136922817
06/01/2019 10:43:11 step: 8557, epoch: 259, batch: 9, loss: 0.03097444400191307, acc: 98.4375, f1: 95.59748427672956, r: 0.7350848904252355
06/01/2019 10:43:12 step: 8562, epoch: 259, batch: 14, loss: 0.003371790051460266, acc: 100.0, f1: 100.0, r: 0.7092082526860034
06/01/2019 10:43:12 step: 8567, epoch: 259, batch: 19, loss: 0.0012864023447036743, acc: 100.0, f1: 100.0, r: 0.564347506649398
06/01/2019 10:43:12 step: 8572, epoch: 259, batch: 24, loss: 0.0022524818778038025, acc: 100.0, f1: 100.0, r: 0.8241023278325073
06/01/2019 10:43:12 step: 8577, epoch: 259, batch: 29, loss: 0.004585988819599152, acc: 100.0, f1: 100.0, r: 0.7026276839922981
06/01/2019 10:43:12 *** evaluating ***
06/01/2019 10:43:13 step: 260, epoch: 259, acc: 59.82905982905983, f1: 21.45970181965916, r: 0.2867785959472242
06/01/2019 10:43:13 *** epoch: 261 ***
06/01/2019 10:43:13 *** training ***
06/01/2019 10:43:13 step: 8585, epoch: 260, batch: 4, loss: 0.0007790848612785339, acc: 100.0, f1: 100.0, r: 0.7020268216476614
06/01/2019 10:43:13 step: 8590, epoch: 260, batch: 9, loss: 0.018299326300621033, acc: 98.4375, f1: 99.22222222222223, r: 0.7925704362671772
06/01/2019 10:43:13 step: 8595, epoch: 260, batch: 14, loss: 0.0024743974208831787, acc: 100.0, f1: 100.0, r: 0.8093290092487314
06/01/2019 10:43:13 step: 8600, epoch: 260, batch: 19, loss: 0.002578817307949066, acc: 100.0, f1: 100.0, r: 0.6989706650006942
06/01/2019 10:43:14 step: 8605, epoch: 260, batch: 24, loss: 0.0006509795784950256, acc: 100.0, f1: 100.0, r: 0.8046671187274506
06/01/2019 10:43:14 step: 8610, epoch: 260, batch: 29, loss: 0.0018313229084014893, acc: 100.0, f1: 100.0, r: 0.6561467203252015
06/01/2019 10:43:14 *** evaluating ***
06/01/2019 10:43:14 step: 261, epoch: 260, acc: 58.97435897435898, f1: 21.305774913188387, r: 0.27722409192102637
06/01/2019 10:43:14 *** epoch: 262 ***
06/01/2019 10:43:14 *** training ***
06/01/2019 10:43:14 step: 8618, epoch: 261, batch: 4, loss: 0.004210524260997772, acc: 100.0, f1: 100.0, r: 0.825722582236835
06/01/2019 10:43:15 step: 8623, epoch: 261, batch: 9, loss: 0.0017594918608665466, acc: 100.0, f1: 100.0, r: 0.7519753666321709
06/01/2019 10:43:15 step: 8628, epoch: 261, batch: 14, loss: 0.00040730834007263184, acc: 100.0, f1: 100.0, r: 0.8151163433908576
06/01/2019 10:43:15 step: 8633, epoch: 261, batch: 19, loss: 0.003694579005241394, acc: 100.0, f1: 100.0, r: 0.8037401377001954
06/01/2019 10:43:15 step: 8638, epoch: 261, batch: 24, loss: 0.0007166191935539246, acc: 100.0, f1: 100.0, r: 0.7047365816588101
06/01/2019 10:43:15 step: 8643, epoch: 261, batch: 29, loss: 0.009207397699356079, acc: 100.0, f1: 100.0, r: 0.6925210116726711
06/01/2019 10:43:16 *** evaluating ***
06/01/2019 10:43:16 step: 262, epoch: 261, acc: 59.82905982905983, f1: 22.29871361172885, r: 0.2788452407476197
06/01/2019 10:43:16 *** epoch: 263 ***
06/01/2019 10:43:16 *** training ***
06/01/2019 10:43:16 step: 8651, epoch: 262, batch: 4, loss: 0.0010613277554512024, acc: 100.0, f1: 100.0, r: 0.6862227041273107
06/01/2019 10:43:16 step: 8656, epoch: 262, batch: 9, loss: 0.010598249733448029, acc: 100.0, f1: 100.0, r: 0.7437371012282198
06/01/2019 10:43:16 step: 8661, epoch: 262, batch: 14, loss: 0.008018560707569122, acc: 100.0, f1: 100.0, r: 0.8220690564719307
06/01/2019 10:43:17 step: 8666, epoch: 262, batch: 19, loss: 0.006562352180480957, acc: 100.0, f1: 100.0, r: 0.5943687864492497
06/01/2019 10:43:17 step: 8671, epoch: 262, batch: 24, loss: 0.0010004937648773193, acc: 100.0, f1: 100.0, r: 0.7496697163935907
06/01/2019 10:43:17 step: 8676, epoch: 262, batch: 29, loss: 0.0012124329805374146, acc: 100.0, f1: 100.0, r: 0.7308208983022264
06/01/2019 10:43:17 *** evaluating ***
06/01/2019 10:43:17 step: 263, epoch: 262, acc: 59.401709401709404, f1: 21.228603603603606, r: 0.2733138805533959
06/01/2019 10:43:17 *** epoch: 264 ***
06/01/2019 10:43:17 *** training ***
06/01/2019 10:43:17 step: 8684, epoch: 263, batch: 4, loss: 0.0016372203826904297, acc: 100.0, f1: 100.0, r: 0.6787908798308289
06/01/2019 10:43:18 step: 8689, epoch: 263, batch: 9, loss: 0.03265057131648064, acc: 98.4375, f1: 96.42857142857143, r: 0.7785795196448669
06/01/2019 10:43:18 step: 8694, epoch: 263, batch: 14, loss: 0.0032749921083450317, acc: 100.0, f1: 100.0, r: 0.7804450180661003
06/01/2019 10:43:18 step: 8699, epoch: 263, batch: 19, loss: 0.004650518298149109, acc: 100.0, f1: 100.0, r: 0.7840855715546687
06/01/2019 10:43:18 step: 8704, epoch: 263, batch: 24, loss: 0.004676878452301025, acc: 100.0, f1: 100.0, r: 0.6517734884598202
06/01/2019 10:43:19 step: 8709, epoch: 263, batch: 29, loss: 0.00132046639919281, acc: 100.0, f1: 100.0, r: 0.6703798063096857
06/01/2019 10:43:19 *** evaluating ***
06/01/2019 10:43:19 step: 264, epoch: 263, acc: 58.97435897435898, f1: 20.991424116424117, r: 0.2785413834029503
06/01/2019 10:43:19 *** epoch: 265 ***
06/01/2019 10:43:19 *** training ***
06/01/2019 10:43:19 step: 8717, epoch: 264, batch: 4, loss: 0.002034127712249756, acc: 100.0, f1: 100.0, r: 0.6977003301846587
06/01/2019 10:43:19 step: 8722, epoch: 264, batch: 9, loss: 0.024696193635463715, acc: 100.0, f1: 100.0, r: 0.7534734821693609
06/01/2019 10:43:20 step: 8727, epoch: 264, batch: 14, loss: 0.018837139010429382, acc: 98.4375, f1: 98.99355877616746, r: 0.736361741276718
06/01/2019 10:43:20 step: 8732, epoch: 264, batch: 19, loss: 0.0023248866200447083, acc: 100.0, f1: 100.0, r: 0.6369757425123018
06/01/2019 10:43:20 step: 8737, epoch: 264, batch: 24, loss: 0.014736317098140717, acc: 98.4375, f1: 98.35600907029477, r: 0.7541215915391466
06/01/2019 10:43:20 step: 8742, epoch: 264, batch: 29, loss: 0.001403212547302246, acc: 100.0, f1: 100.0, r: 0.7573426552714148
06/01/2019 10:43:20 *** evaluating ***
06/01/2019 10:43:21 step: 265, epoch: 264, acc: 59.82905982905983, f1: 21.940585644505948, r: 0.27036895971113484
06/01/2019 10:43:21 *** epoch: 266 ***
06/01/2019 10:43:21 *** training ***
06/01/2019 10:43:21 step: 8750, epoch: 265, batch: 4, loss: 0.0007803291082382202, acc: 100.0, f1: 100.0, r: 0.7602668556533426
06/01/2019 10:43:21 step: 8755, epoch: 265, batch: 9, loss: 0.0010675862431526184, acc: 100.0, f1: 100.0, r: 0.7683591321911767
06/01/2019 10:43:21 step: 8760, epoch: 265, batch: 14, loss: 0.007415756583213806, acc: 100.0, f1: 100.0, r: 0.8226741214465172
06/01/2019 10:43:21 step: 8765, epoch: 265, batch: 19, loss: 0.002732202410697937, acc: 100.0, f1: 100.0, r: 0.6133966990921949
06/01/2019 10:43:22 step: 8770, epoch: 265, batch: 24, loss: 0.0016960874199867249, acc: 100.0, f1: 100.0, r: 0.7126193194906778
06/01/2019 10:43:22 step: 8775, epoch: 265, batch: 29, loss: 0.00969512015581131, acc: 100.0, f1: 100.0, r: 0.8345140488663584
06/01/2019 10:43:22 *** evaluating ***
06/01/2019 10:43:22 step: 266, epoch: 265, acc: 60.256410256410255, f1: 22.064092322713012, r: 0.2761999876458309
06/01/2019 10:43:22 *** epoch: 267 ***
06/01/2019 10:43:22 *** training ***
06/01/2019 10:43:22 step: 8783, epoch: 266, batch: 4, loss: 0.0042945146560668945, acc: 100.0, f1: 100.0, r: 0.7938041236971819
06/01/2019 10:43:23 step: 8788, epoch: 266, batch: 9, loss: 0.021700359880924225, acc: 98.4375, f1: 98.9175331149409, r: 0.7084712487982481
06/01/2019 10:43:23 step: 8793, epoch: 266, batch: 14, loss: 0.013932891190052032, acc: 100.0, f1: 100.0, r: 0.6795065705866424
06/01/2019 10:43:23 step: 8798, epoch: 266, batch: 19, loss: 0.00034692883491516113, acc: 100.0, f1: 100.0, r: 0.6291772787100793
06/01/2019 10:43:23 step: 8803, epoch: 266, batch: 24, loss: 0.0007353425025939941, acc: 100.0, f1: 100.0, r: 0.6967804714781782
06/01/2019 10:43:24 step: 8808, epoch: 266, batch: 29, loss: 0.0023989006876945496, acc: 100.0, f1: 100.0, r: 0.7325801862695906
06/01/2019 10:43:24 *** evaluating ***
06/01/2019 10:43:24 step: 267, epoch: 266, acc: 58.54700854700855, f1: 19.550625903025594, r: 0.2626979221455523
06/01/2019 10:43:24 *** epoch: 268 ***
06/01/2019 10:43:24 *** training ***
06/01/2019 10:43:24 step: 8816, epoch: 267, batch: 4, loss: 0.0008241534233093262, acc: 100.0, f1: 100.0, r: 0.6795578224335515
06/01/2019 10:43:24 step: 8821, epoch: 267, batch: 9, loss: 0.009279601275920868, acc: 100.0, f1: 100.0, r: 0.7313214232215919
06/01/2019 10:43:24 step: 8826, epoch: 267, batch: 14, loss: 0.0005922764539718628, acc: 100.0, f1: 100.0, r: 0.7449365962107309
06/01/2019 10:43:25 step: 8831, epoch: 267, batch: 19, loss: 0.001834079623222351, acc: 100.0, f1: 100.0, r: 0.7099277291412244
06/01/2019 10:43:25 step: 8836, epoch: 267, batch: 24, loss: 0.004610806703567505, acc: 100.0, f1: 100.0, r: 0.7131329542402844
06/01/2019 10:43:25 step: 8841, epoch: 267, batch: 29, loss: 0.004370592534542084, acc: 100.0, f1: 100.0, r: 0.7791710574471071
06/01/2019 10:43:25 *** evaluating ***
06/01/2019 10:43:25 step: 268, epoch: 267, acc: 59.401709401709404, f1: 20.231777433733285, r: 0.2684080637366548
06/01/2019 10:43:25 *** epoch: 269 ***
06/01/2019 10:43:25 *** training ***
06/01/2019 10:43:26 step: 8849, epoch: 268, batch: 4, loss: 0.0023729801177978516, acc: 100.0, f1: 100.0, r: 0.7104454071809584
06/01/2019 10:43:26 step: 8854, epoch: 268, batch: 9, loss: 0.005317702889442444, acc: 100.0, f1: 100.0, r: 0.7472735165946823
06/01/2019 10:43:26 step: 8859, epoch: 268, batch: 14, loss: 0.004628196358680725, acc: 100.0, f1: 100.0, r: 0.6729232136940675
06/01/2019 10:43:26 step: 8864, epoch: 268, batch: 19, loss: 0.004565209150314331, acc: 100.0, f1: 100.0, r: 0.6329595561440168
06/01/2019 10:43:26 step: 8869, epoch: 268, batch: 24, loss: 0.0009882450103759766, acc: 100.0, f1: 100.0, r: 0.6842333196922034
06/01/2019 10:43:27 step: 8874, epoch: 268, batch: 29, loss: 0.0034592673182487488, acc: 100.0, f1: 100.0, r: 0.6996092717712332
06/01/2019 10:43:27 *** evaluating ***
06/01/2019 10:43:27 step: 269, epoch: 268, acc: 59.401709401709404, f1: 20.076470384779448, r: 0.2837040237815476
06/01/2019 10:43:27 *** epoch: 270 ***
06/01/2019 10:43:27 *** training ***
06/01/2019 10:43:27 step: 8882, epoch: 269, batch: 4, loss: 0.003912217915058136, acc: 100.0, f1: 100.0, r: 0.7980934753379744
06/01/2019 10:43:27 step: 8887, epoch: 269, batch: 9, loss: 0.00042551755905151367, acc: 100.0, f1: 100.0, r: 0.7995684478579618
06/01/2019 10:43:28 step: 8892, epoch: 269, batch: 14, loss: 0.014677219092845917, acc: 100.0, f1: 100.0, r: 0.7302616133042948
06/01/2019 10:43:28 step: 8897, epoch: 269, batch: 19, loss: 0.004338450729846954, acc: 100.0, f1: 100.0, r: 0.7508389394725453
06/01/2019 10:43:28 step: 8902, epoch: 269, batch: 24, loss: 0.0015213340520858765, acc: 100.0, f1: 100.0, r: 0.7183139885027693
06/01/2019 10:43:28 step: 8907, epoch: 269, batch: 29, loss: 0.00978834182024002, acc: 100.0, f1: 100.0, r: 0.6871673378955115
06/01/2019 10:43:28 *** evaluating ***
06/01/2019 10:43:29 step: 270, epoch: 269, acc: 58.97435897435898, f1: 19.489823609226594, r: 0.28664370132699324
06/01/2019 10:43:29 *** epoch: 271 ***
06/01/2019 10:43:29 *** training ***
06/01/2019 10:43:29 step: 8915, epoch: 270, batch: 4, loss: 0.020846642553806305, acc: 98.4375, f1: 95.10204081632654, r: 0.6978818203115157
06/01/2019 10:43:29 step: 8920, epoch: 270, batch: 9, loss: 0.0010790303349494934, acc: 100.0, f1: 100.0, r: 0.7950085907084699
06/01/2019 10:43:29 step: 8925, epoch: 270, batch: 14, loss: 0.0006867349147796631, acc: 100.0, f1: 100.0, r: 0.6386864068429657
06/01/2019 10:43:30 step: 8930, epoch: 270, batch: 19, loss: 0.000528186559677124, acc: 100.0, f1: 100.0, r: 0.7488444924672633
06/01/2019 10:43:30 step: 8935, epoch: 270, batch: 24, loss: 0.0048797838389873505, acc: 100.0, f1: 100.0, r: 0.717224458919855
06/01/2019 10:43:30 step: 8940, epoch: 270, batch: 29, loss: 0.01760455220937729, acc: 100.0, f1: 100.0, r: 0.6339778914810482
06/01/2019 10:43:30 *** evaluating ***
06/01/2019 10:43:30 step: 271, epoch: 270, acc: 56.837606837606835, f1: 20.47496489154914, r: 0.2851704259267296
06/01/2019 10:43:30 *** epoch: 272 ***
06/01/2019 10:43:30 *** training ***
06/01/2019 10:43:30 step: 8948, epoch: 271, batch: 4, loss: 0.0008132532238960266, acc: 100.0, f1: 100.0, r: 0.6484473379175479
06/01/2019 10:43:31 step: 8953, epoch: 271, batch: 9, loss: 0.0021052733063697815, acc: 100.0, f1: 100.0, r: 0.6671703277698998
06/01/2019 10:43:31 step: 8958, epoch: 271, batch: 14, loss: 0.0005507543683052063, acc: 100.0, f1: 100.0, r: 0.6605975720515163
06/01/2019 10:43:31 step: 8963, epoch: 271, batch: 19, loss: 0.002551548182964325, acc: 100.0, f1: 100.0, r: 0.5235355876142715
06/01/2019 10:43:31 step: 8968, epoch: 271, batch: 24, loss: 0.0019371062517166138, acc: 100.0, f1: 100.0, r: 0.810690748540844
06/01/2019 10:43:32 step: 8973, epoch: 271, batch: 29, loss: 0.0015655755996704102, acc: 100.0, f1: 100.0, r: 0.6882571152625847
06/01/2019 10:43:32 *** evaluating ***
06/01/2019 10:43:32 step: 272, epoch: 271, acc: 60.68376068376068, f1: 22.004995004995003, r: 0.2875708216173301
06/01/2019 10:43:32 *** epoch: 273 ***
06/01/2019 10:43:32 *** training ***
06/01/2019 10:43:32 step: 8981, epoch: 272, batch: 4, loss: 0.0012607946991920471, acc: 100.0, f1: 100.0, r: 0.7993801063181644
06/01/2019 10:43:32 step: 8986, epoch: 272, batch: 9, loss: 0.0001334547996520996, acc: 100.0, f1: 100.0, r: 0.7609348729662205
06/01/2019 10:43:33 step: 8991, epoch: 272, batch: 14, loss: 0.0007540732622146606, acc: 100.0, f1: 100.0, r: 0.7124189486173633
06/01/2019 10:43:33 step: 8996, epoch: 272, batch: 19, loss: 0.0020547807216644287, acc: 100.0, f1: 100.0, r: 0.7327013656963699
06/01/2019 10:43:33 step: 9001, epoch: 272, batch: 24, loss: 0.0022562965750694275, acc: 100.0, f1: 100.0, r: 0.8231552700794234
06/01/2019 10:43:33 step: 9006, epoch: 272, batch: 29, loss: 0.014530394226312637, acc: 98.4375, f1: 85.25345622119815, r: 0.6266927595306998
06/01/2019 10:43:33 *** evaluating ***
06/01/2019 10:43:33 step: 273, epoch: 272, acc: 59.82905982905983, f1: 22.40748180884782, r: 0.2745450754571265
06/01/2019 10:43:33 *** epoch: 274 ***
06/01/2019 10:43:33 *** training ***
06/01/2019 10:43:34 step: 9014, epoch: 273, batch: 4, loss: 0.0014399439096450806, acc: 100.0, f1: 100.0, r: 0.5753316401917353
06/01/2019 10:43:34 step: 9019, epoch: 273, batch: 9, loss: 0.002082809805870056, acc: 100.0, f1: 100.0, r: 0.6902865345111653
06/01/2019 10:43:34 step: 9024, epoch: 273, batch: 14, loss: 0.002798609435558319, acc: 100.0, f1: 100.0, r: 0.7620938854721406
06/01/2019 10:43:34 step: 9029, epoch: 273, batch: 19, loss: 0.0013220906257629395, acc: 100.0, f1: 100.0, r: 0.7034584359790885
06/01/2019 10:43:35 step: 9034, epoch: 273, batch: 24, loss: 0.012078359723091125, acc: 98.4375, f1: 97.6911976911977, r: 0.7003049390678837
06/01/2019 10:43:35 step: 9039, epoch: 273, batch: 29, loss: 0.003191627562046051, acc: 100.0, f1: 100.0, r: 0.7851841560252794
06/01/2019 10:43:35 *** evaluating ***
06/01/2019 10:43:35 step: 274, epoch: 273, acc: 60.256410256410255, f1: 22.610707803992742, r: 0.28688118092583476
06/01/2019 10:43:35 *** epoch: 275 ***
06/01/2019 10:43:35 *** training ***
06/01/2019 10:43:35 step: 9047, epoch: 274, batch: 4, loss: 0.019643008708953857, acc: 98.4375, f1: 97.07792207792207, r: 0.7993853797025602
06/01/2019 10:43:36 step: 9052, epoch: 274, batch: 9, loss: 0.004052549600601196, acc: 100.0, f1: 100.0, r: 0.7064244270530858
06/01/2019 10:43:36 step: 9057, epoch: 274, batch: 14, loss: 0.008479677140712738, acc: 100.0, f1: 100.0, r: 0.807900792489586
06/01/2019 10:43:36 step: 9062, epoch: 274, batch: 19, loss: 0.006087146699428558, acc: 100.0, f1: 100.0, r: 0.7339862581139242
06/01/2019 10:43:36 step: 9067, epoch: 274, batch: 24, loss: 0.0010247379541397095, acc: 100.0, f1: 100.0, r: 0.6983482777548626
06/01/2019 10:43:36 step: 9072, epoch: 274, batch: 29, loss: 0.0012014955282211304, acc: 100.0, f1: 100.0, r: 0.7315571800957363
06/01/2019 10:43:37 *** evaluating ***
06/01/2019 10:43:37 step: 275, epoch: 274, acc: 58.119658119658126, f1: 21.899758686414746, r: 0.2824311177432332
06/01/2019 10:43:37 *** epoch: 276 ***
06/01/2019 10:43:37 *** training ***
06/01/2019 10:43:37 step: 9080, epoch: 275, batch: 4, loss: 0.002933397889137268, acc: 100.0, f1: 100.0, r: 0.7520980425654058
06/01/2019 10:43:37 step: 9085, epoch: 275, batch: 9, loss: 0.00024913251399993896, acc: 100.0, f1: 100.0, r: 0.6996144741053026
06/01/2019 10:43:37 step: 9090, epoch: 275, batch: 14, loss: 0.002831585705280304, acc: 100.0, f1: 100.0, r: 0.8237733919776657
06/01/2019 10:43:38 step: 9095, epoch: 275, batch: 19, loss: 0.0006096810102462769, acc: 100.0, f1: 100.0, r: 0.7632176208671082
06/01/2019 10:43:38 step: 9100, epoch: 275, batch: 24, loss: 0.00024284422397613525, acc: 100.0, f1: 100.0, r: 0.6701472190517093
06/01/2019 10:43:38 step: 9105, epoch: 275, batch: 29, loss: 0.001576744019985199, acc: 100.0, f1: 100.0, r: 0.6811016050655415
06/01/2019 10:43:38 *** evaluating ***
06/01/2019 10:43:38 step: 276, epoch: 275, acc: 60.256410256410255, f1: 21.62617888572978, r: 0.2816533901248486
06/01/2019 10:43:38 *** epoch: 277 ***
06/01/2019 10:43:38 *** training ***
06/01/2019 10:43:39 step: 9113, epoch: 276, batch: 4, loss: 0.00016814470291137695, acc: 100.0, f1: 100.0, r: 0.6119383383965282
06/01/2019 10:43:39 step: 9118, epoch: 276, batch: 9, loss: 0.00038292258977890015, acc: 100.0, f1: 100.0, r: 0.6750284229529877
06/01/2019 10:43:39 step: 9123, epoch: 276, batch: 14, loss: 0.0010700449347496033, acc: 100.0, f1: 100.0, r: 0.7662678229302452
06/01/2019 10:43:39 step: 9128, epoch: 276, batch: 19, loss: 0.007922761142253876, acc: 100.0, f1: 100.0, r: 0.7205751734598518
06/01/2019 10:43:39 step: 9133, epoch: 276, batch: 24, loss: 0.003818891942501068, acc: 100.0, f1: 100.0, r: 0.8233509729624787
06/01/2019 10:43:40 step: 9138, epoch: 276, batch: 29, loss: 0.0004663318395614624, acc: 100.0, f1: 100.0, r: 0.7821531057840225
06/01/2019 10:43:40 *** evaluating ***
06/01/2019 10:43:40 step: 277, epoch: 276, acc: 59.82905982905983, f1: 22.478038104914226, r: 0.28814431153964976
06/01/2019 10:43:40 *** epoch: 278 ***
06/01/2019 10:43:40 *** training ***
06/01/2019 10:43:40 step: 9146, epoch: 277, batch: 4, loss: 0.010973982512950897, acc: 100.0, f1: 100.0, r: 0.7003598550142673
06/01/2019 10:43:40 step: 9151, epoch: 277, batch: 9, loss: 0.0009039640426635742, acc: 100.0, f1: 100.0, r: 0.7710573254133596
06/01/2019 10:43:40 step: 9156, epoch: 277, batch: 14, loss: 0.005955256521701813, acc: 100.0, f1: 100.0, r: 0.718403203960123
06/01/2019 10:43:41 step: 9161, epoch: 277, batch: 19, loss: 0.0017944052815437317, acc: 100.0, f1: 100.0, r: 0.7988241241970363
06/01/2019 10:43:41 step: 9166, epoch: 277, batch: 24, loss: 0.005784258246421814, acc: 100.0, f1: 100.0, r: 0.8150430543532099
06/01/2019 10:43:41 step: 9171, epoch: 277, batch: 29, loss: 0.006448477506637573, acc: 100.0, f1: 100.0, r: 0.6693077425519173
06/01/2019 10:43:41 *** evaluating ***
06/01/2019 10:43:41 step: 278, epoch: 277, acc: 60.256410256410255, f1: 22.118855941678962, r: 0.29190538851791314
06/01/2019 10:43:41 *** epoch: 279 ***
06/01/2019 10:43:41 *** training ***
06/01/2019 10:43:42 step: 9179, epoch: 278, batch: 4, loss: 0.0006477534770965576, acc: 100.0, f1: 100.0, r: 0.5229916848025192
06/01/2019 10:43:42 step: 9184, epoch: 278, batch: 9, loss: 0.02044280618429184, acc: 98.4375, f1: 95.10204081632654, r: 0.7397848110757702
06/01/2019 10:43:42 step: 9189, epoch: 278, batch: 14, loss: 0.004011966288089752, acc: 100.0, f1: 100.0, r: 0.7509028305401684
06/01/2019 10:43:42 step: 9194, epoch: 278, batch: 19, loss: 0.0035793185234069824, acc: 100.0, f1: 100.0, r: 0.831215465081377
06/01/2019 10:43:42 step: 9199, epoch: 278, batch: 24, loss: 0.0057455673813819885, acc: 100.0, f1: 100.0, r: 0.6553435837111595
06/01/2019 10:43:43 step: 9204, epoch: 278, batch: 29, loss: 0.00048501789569854736, acc: 100.0, f1: 100.0, r: 0.7833889906887888
06/01/2019 10:43:43 *** evaluating ***
06/01/2019 10:43:43 step: 279, epoch: 278, acc: 60.256410256410255, f1: 21.35412672239691, r: 0.28405457120828254
06/01/2019 10:43:43 *** epoch: 280 ***
06/01/2019 10:43:43 *** training ***
06/01/2019 10:43:43 step: 9212, epoch: 279, batch: 4, loss: 0.003138601779937744, acc: 100.0, f1: 100.0, r: 0.7258847121110665
06/01/2019 10:43:43 step: 9217, epoch: 279, batch: 9, loss: 0.0010260045528411865, acc: 100.0, f1: 100.0, r: 0.7662517710979504
06/01/2019 10:43:44 step: 9222, epoch: 279, batch: 14, loss: 0.000902228057384491, acc: 100.0, f1: 100.0, r: 0.8016197574434236
06/01/2019 10:43:44 step: 9227, epoch: 279, batch: 19, loss: 0.003423549234867096, acc: 100.0, f1: 100.0, r: 0.7298922270208537
06/01/2019 10:43:44 step: 9232, epoch: 279, batch: 24, loss: 0.0004363134503364563, acc: 100.0, f1: 100.0, r: 0.6113610509373842
06/01/2019 10:43:44 step: 9237, epoch: 279, batch: 29, loss: 0.0011665001511573792, acc: 100.0, f1: 100.0, r: 0.649594213625199
06/01/2019 10:43:44 *** evaluating ***
06/01/2019 10:43:44 step: 280, epoch: 279, acc: 60.68376068376068, f1: 22.34898840172674, r: 0.28935321387895807
06/01/2019 10:43:44 *** epoch: 281 ***
06/01/2019 10:43:44 *** training ***
06/01/2019 10:43:45 step: 9245, epoch: 280, batch: 4, loss: 0.03723342344164848, acc: 96.875, f1: 84.42438513867086, r: 0.6801186242917452
06/01/2019 10:43:45 step: 9250, epoch: 280, batch: 9, loss: 0.001805618405342102, acc: 100.0, f1: 100.0, r: 0.8502811440695254
06/01/2019 10:43:45 step: 9255, epoch: 280, batch: 14, loss: 0.0036795586347579956, acc: 100.0, f1: 100.0, r: 0.6400374655312755
06/01/2019 10:43:45 step: 9260, epoch: 280, batch: 19, loss: 0.00035846978425979614, acc: 100.0, f1: 100.0, r: 0.709550145132473
06/01/2019 10:43:46 step: 9265, epoch: 280, batch: 24, loss: 0.0024232491850852966, acc: 100.0, f1: 100.0, r: 0.734109649214428
06/01/2019 10:43:46 step: 9270, epoch: 280, batch: 29, loss: 0.0013345777988433838, acc: 100.0, f1: 100.0, r: 0.8027486756071972
06/01/2019 10:43:46 *** evaluating ***
06/01/2019 10:43:46 step: 281, epoch: 280, acc: 60.68376068376068, f1: 22.622130774510584, r: 0.2864338754467658
06/01/2019 10:43:46 *** epoch: 282 ***
06/01/2019 10:43:46 *** training ***
06/01/2019 10:43:46 step: 9278, epoch: 281, batch: 4, loss: 0.0036666393280029297, acc: 100.0, f1: 100.0, r: 0.6482619199286628
06/01/2019 10:43:46 step: 9283, epoch: 281, batch: 9, loss: 0.0010427460074424744, acc: 100.0, f1: 100.0, r: 0.6877880971952055
06/01/2019 10:43:47 step: 9288, epoch: 281, batch: 14, loss: 0.0033035054802894592, acc: 100.0, f1: 100.0, r: 0.5932500595715284
06/01/2019 10:43:47 step: 9293, epoch: 281, batch: 19, loss: 0.002254374325275421, acc: 100.0, f1: 100.0, r: 0.7052538212358201
06/01/2019 10:43:47 step: 9298, epoch: 281, batch: 24, loss: 0.0006549134850502014, acc: 100.0, f1: 100.0, r: 0.6426500994049512
06/01/2019 10:43:47 step: 9303, epoch: 281, batch: 29, loss: 0.01966053992509842, acc: 98.4375, f1: 99.06273620559335, r: 0.6697763684792848
06/01/2019 10:43:47 *** evaluating ***
06/01/2019 10:43:47 step: 282, epoch: 281, acc: 57.26495726495726, f1: 23.459160259848616, r: 0.2765148302962743
06/01/2019 10:43:47 *** epoch: 283 ***
06/01/2019 10:43:47 *** training ***
06/01/2019 10:43:48 step: 9311, epoch: 282, batch: 4, loss: 0.03313743323087692, acc: 98.4375, f1: 99.1951219512195, r: 0.7394882927561245
06/01/2019 10:43:48 step: 9316, epoch: 282, batch: 9, loss: 0.03252801299095154, acc: 98.4375, f1: 96.90866510538642, r: 0.7471754933709386
06/01/2019 10:43:48 step: 9321, epoch: 282, batch: 14, loss: 0.03540077060461044, acc: 98.4375, f1: 95.6140350877193, r: 0.7468690347368002
06/01/2019 10:43:48 step: 9326, epoch: 282, batch: 19, loss: 0.0006941854953765869, acc: 100.0, f1: 100.0, r: 0.7507480653357872
06/01/2019 10:43:49 step: 9331, epoch: 282, batch: 24, loss: 0.0021925121545791626, acc: 100.0, f1: 100.0, r: 0.6988357471003221
06/01/2019 10:43:49 step: 9336, epoch: 282, batch: 29, loss: 0.005005568265914917, acc: 100.0, f1: 100.0, r: 0.6856830368042709
06/01/2019 10:43:49 *** evaluating ***
06/01/2019 10:43:49 step: 283, epoch: 282, acc: 60.68376068376068, f1: 22.34898840172674, r: 0.2881904525586282
06/01/2019 10:43:49 *** epoch: 284 ***
06/01/2019 10:43:49 *** training ***
06/01/2019 10:43:49 step: 9344, epoch: 283, batch: 4, loss: 0.005213424563407898, acc: 100.0, f1: 100.0, r: 0.731583040077943
06/01/2019 10:43:50 step: 9349, epoch: 283, batch: 9, loss: 0.0020138397812843323, acc: 100.0, f1: 100.0, r: 0.7265515213917698
06/01/2019 10:43:50 step: 9354, epoch: 283, batch: 14, loss: 0.003693029284477234, acc: 100.0, f1: 100.0, r: 0.7092379324301751
06/01/2019 10:43:50 step: 9359, epoch: 283, batch: 19, loss: 0.0016392767429351807, acc: 100.0, f1: 100.0, r: 0.6183402831033095
06/01/2019 10:43:50 step: 9364, epoch: 283, batch: 24, loss: 0.03161387890577316, acc: 98.4375, f1: 99.07614781634938, r: 0.753546928592004
06/01/2019 10:43:50 step: 9369, epoch: 283, batch: 29, loss: 0.0014088749885559082, acc: 100.0, f1: 100.0, r: 0.8081691350294086
06/01/2019 10:43:51 *** evaluating ***
06/01/2019 10:43:51 step: 284, epoch: 283, acc: 59.401709401709404, f1: 20.723630914683987, r: 0.29438188545340005
06/01/2019 10:43:51 *** epoch: 285 ***
06/01/2019 10:43:51 *** training ***
06/01/2019 10:43:51 step: 9377, epoch: 284, batch: 4, loss: 0.0059797391295433044, acc: 100.0, f1: 100.0, r: 0.8210320966099925
06/01/2019 10:43:51 step: 9382, epoch: 284, batch: 9, loss: 0.0004400685429573059, acc: 100.0, f1: 100.0, r: 0.8191234440468698
06/01/2019 10:43:51 step: 9387, epoch: 284, batch: 14, loss: 0.005116935819387436, acc: 100.0, f1: 100.0, r: 0.8318563950735931
06/01/2019 10:43:52 step: 9392, epoch: 284, batch: 19, loss: 0.005348999053239822, acc: 100.0, f1: 100.0, r: 0.6933856580633203
06/01/2019 10:43:52 step: 9397, epoch: 284, batch: 24, loss: 0.007945992052555084, acc: 100.0, f1: 100.0, r: 0.7754727517767075
06/01/2019 10:43:52 step: 9402, epoch: 284, batch: 29, loss: 0.0038925781846046448, acc: 100.0, f1: 100.0, r: 0.752257815142344
06/01/2019 10:43:52 *** evaluating ***
06/01/2019 10:43:52 step: 285, epoch: 284, acc: 57.692307692307686, f1: 19.11075716956325, r: 0.27647954154121446
06/01/2019 10:43:52 *** epoch: 286 ***
06/01/2019 10:43:52 *** training ***
06/01/2019 10:43:52 step: 9410, epoch: 285, batch: 4, loss: 0.0011328905820846558, acc: 100.0, f1: 100.0, r: 0.6800945201485641
06/01/2019 10:43:53 step: 9415, epoch: 285, batch: 9, loss: 0.00663284957408905, acc: 100.0, f1: 100.0, r: 0.8396756625524952
06/01/2019 10:43:53 step: 9420, epoch: 285, batch: 14, loss: 0.0018104314804077148, acc: 100.0, f1: 100.0, r: 0.6752241058087399
06/01/2019 10:43:53 step: 9425, epoch: 285, batch: 19, loss: 0.0020125359296798706, acc: 100.0, f1: 100.0, r: 0.6077896197306858
06/01/2019 10:43:53 step: 9430, epoch: 285, batch: 24, loss: 0.001659303903579712, acc: 100.0, f1: 100.0, r: 0.7423343298453607
06/01/2019 10:43:54 step: 9435, epoch: 285, batch: 29, loss: 0.0027923882007598877, acc: 100.0, f1: 100.0, r: 0.7179484735880092
06/01/2019 10:43:54 *** evaluating ***
06/01/2019 10:43:54 step: 286, epoch: 285, acc: 59.82905982905983, f1: 21.71221228189512, r: 0.28909011741775764
06/01/2019 10:43:54 *** epoch: 287 ***
06/01/2019 10:43:54 *** training ***
06/01/2019 10:43:54 step: 9443, epoch: 286, batch: 4, loss: 0.017279863357543945, acc: 100.0, f1: 100.0, r: 0.766489911101429
06/01/2019 10:43:54 step: 9448, epoch: 286, batch: 9, loss: 0.0009642839431762695, acc: 100.0, f1: 100.0, r: 0.7883682306051746
06/01/2019 10:43:54 step: 9453, epoch: 286, batch: 14, loss: 0.0004588514566421509, acc: 100.0, f1: 100.0, r: 0.6656198155550537
06/01/2019 10:43:55 step: 9458, epoch: 286, batch: 19, loss: 0.004932329058647156, acc: 100.0, f1: 100.0, r: 0.7312199176150224
06/01/2019 10:43:55 step: 9463, epoch: 286, batch: 24, loss: 0.00171661376953125, acc: 100.0, f1: 100.0, r: 0.811611163610313
06/01/2019 10:43:55 step: 9468, epoch: 286, batch: 29, loss: 0.0018800050020217896, acc: 100.0, f1: 100.0, r: 0.8057819602714291
06/01/2019 10:43:55 *** evaluating ***
06/01/2019 10:43:55 step: 287, epoch: 286, acc: 60.256410256410255, f1: 23.473869126043038, r: 0.2909934059115953
06/01/2019 10:43:55 *** epoch: 288 ***
06/01/2019 10:43:55 *** training ***
06/01/2019 10:43:56 step: 9476, epoch: 287, batch: 4, loss: 0.01267077773809433, acc: 98.4375, f1: 98.31519831519832, r: 0.7048946710485045
06/01/2019 10:43:56 step: 9481, epoch: 287, batch: 9, loss: 0.0020579174160957336, acc: 100.0, f1: 100.0, r: 0.7147796144781406
06/01/2019 10:43:56 step: 9486, epoch: 287, batch: 14, loss: 0.0004654526710510254, acc: 100.0, f1: 100.0, r: 0.7738876911861793
06/01/2019 10:43:56 step: 9491, epoch: 287, batch: 19, loss: 0.004042968153953552, acc: 100.0, f1: 100.0, r: 0.7559673078676588
06/01/2019 10:43:56 step: 9496, epoch: 287, batch: 24, loss: 0.0010242536664009094, acc: 100.0, f1: 100.0, r: 0.8403118475744502
06/01/2019 10:43:57 step: 9501, epoch: 287, batch: 29, loss: 0.0021310895681381226, acc: 100.0, f1: 100.0, r: 0.7057375284742367
06/01/2019 10:43:57 *** evaluating ***
06/01/2019 10:43:57 step: 288, epoch: 287, acc: 61.111111111111114, f1: 23.318538750426768, r: 0.29833841809452444
06/01/2019 10:43:57 *** epoch: 289 ***
06/01/2019 10:43:57 *** training ***
06/01/2019 10:43:57 step: 9509, epoch: 288, batch: 4, loss: 0.0008820965886116028, acc: 100.0, f1: 100.0, r: 0.824999288372744
06/01/2019 10:43:57 step: 9514, epoch: 288, batch: 9, loss: 0.000978626310825348, acc: 100.0, f1: 100.0, r: 0.8234698884835139
06/01/2019 10:43:58 step: 9519, epoch: 288, batch: 14, loss: 0.0019989237189292908, acc: 100.0, f1: 100.0, r: 0.7529232561257969
06/01/2019 10:43:58 step: 9524, epoch: 288, batch: 19, loss: 0.0031284689903259277, acc: 100.0, f1: 100.0, r: 0.6313531482355244
06/01/2019 10:43:58 step: 9529, epoch: 288, batch: 24, loss: 0.0004018247127532959, acc: 100.0, f1: 100.0, r: 0.7782190092308373
06/01/2019 10:43:58 step: 9534, epoch: 288, batch: 29, loss: 0.004946857690811157, acc: 100.0, f1: 100.0, r: 0.7103567116945995
06/01/2019 10:43:58 *** evaluating ***
06/01/2019 10:43:58 step: 289, epoch: 288, acc: 59.82905982905983, f1: 20.876334844153728, r: 0.2728288725900107
06/01/2019 10:43:58 *** epoch: 290 ***
06/01/2019 10:43:58 *** training ***
06/01/2019 10:43:59 step: 9542, epoch: 289, batch: 4, loss: 0.00033676624298095703, acc: 100.0, f1: 100.0, r: 0.7791502292695469
06/01/2019 10:43:59 step: 9547, epoch: 289, batch: 9, loss: 0.0024386420845985413, acc: 100.0, f1: 100.0, r: 0.699989184005637
06/01/2019 10:43:59 step: 9552, epoch: 289, batch: 14, loss: 0.0016542747616767883, acc: 100.0, f1: 100.0, r: 0.7001354329956866
06/01/2019 10:43:59 step: 9557, epoch: 289, batch: 19, loss: 0.017873525619506836, acc: 100.0, f1: 100.0, r: 0.7823499558662314
06/01/2019 10:44:00 step: 9562, epoch: 289, batch: 24, loss: 0.0019647106528282166, acc: 100.0, f1: 100.0, r: 0.7972284149002613
06/01/2019 10:44:00 step: 9567, epoch: 289, batch: 29, loss: 0.0012975335121154785, acc: 100.0, f1: 100.0, r: 0.6702428659363172
06/01/2019 10:44:00 *** evaluating ***
06/01/2019 10:44:00 step: 290, epoch: 289, acc: 60.256410256410255, f1: 21.911361398763983, r: 0.29119761914182224
06/01/2019 10:44:00 *** epoch: 291 ***
06/01/2019 10:44:00 *** training ***
06/01/2019 10:44:00 step: 9575, epoch: 290, batch: 4, loss: 0.0012526661157608032, acc: 100.0, f1: 100.0, r: 0.6712664501838365
06/01/2019 10:44:01 step: 9580, epoch: 290, batch: 9, loss: 0.0019864514470100403, acc: 100.0, f1: 100.0, r: 0.750846658660139
06/01/2019 10:44:01 step: 9585, epoch: 290, batch: 14, loss: 0.012720797210931778, acc: 100.0, f1: 100.0, r: 0.7866744614606785
06/01/2019 10:44:01 step: 9590, epoch: 290, batch: 19, loss: 0.000870727002620697, acc: 100.0, f1: 100.0, r: 0.6987873179754298
06/01/2019 10:44:01 step: 9595, epoch: 290, batch: 24, loss: 0.0034826844930648804, acc: 100.0, f1: 100.0, r: 0.7905238586034002
06/01/2019 10:44:01 step: 9600, epoch: 290, batch: 29, loss: 0.0030487552285194397, acc: 100.0, f1: 100.0, r: 0.7179711447863865
06/01/2019 10:44:02 *** evaluating ***
06/01/2019 10:44:02 step: 291, epoch: 290, acc: 59.82905982905983, f1: 21.680742554059567, r: 0.28904969641254025
06/01/2019 10:44:02 *** epoch: 292 ***
06/01/2019 10:44:02 *** training ***
06/01/2019 10:44:02 step: 9608, epoch: 291, batch: 4, loss: 0.0021478980779647827, acc: 100.0, f1: 100.0, r: 0.8231805703061322
06/01/2019 10:44:02 step: 9613, epoch: 291, batch: 9, loss: 0.0016068369150161743, acc: 100.0, f1: 100.0, r: 0.8107891694388308
06/01/2019 10:44:02 step: 9618, epoch: 291, batch: 14, loss: 0.0005525127053260803, acc: 100.0, f1: 100.0, r: 0.783032316784433
06/01/2019 10:44:03 step: 9623, epoch: 291, batch: 19, loss: 0.000786416232585907, acc: 100.0, f1: 100.0, r: 0.712952240603306
06/01/2019 10:44:03 step: 9628, epoch: 291, batch: 24, loss: 0.0014216974377632141, acc: 100.0, f1: 100.0, r: 0.7787684862778185
06/01/2019 10:44:03 step: 9633, epoch: 291, batch: 29, loss: 0.0022815540432929993, acc: 100.0, f1: 100.0, r: 0.5154896715181753
06/01/2019 10:44:03 *** evaluating ***
06/01/2019 10:44:03 step: 292, epoch: 291, acc: 59.82905982905983, f1: 22.413793103448278, r: 0.2936959691946507
06/01/2019 10:44:03 *** epoch: 293 ***
06/01/2019 10:44:03 *** training ***
06/01/2019 10:44:04 step: 9641, epoch: 292, batch: 4, loss: 0.004232928156852722, acc: 100.0, f1: 100.0, r: 0.7263606373939803
06/01/2019 10:44:04 step: 9646, epoch: 292, batch: 9, loss: 0.0015881285071372986, acc: 100.0, f1: 100.0, r: 0.7299157793134989
06/01/2019 10:44:04 step: 9651, epoch: 292, batch: 14, loss: 0.0033978894352912903, acc: 100.0, f1: 100.0, r: 0.8169690041187809
06/01/2019 10:44:04 step: 9656, epoch: 292, batch: 19, loss: 0.005492836236953735, acc: 100.0, f1: 100.0, r: 0.8168538273683417
06/01/2019 10:44:04 step: 9661, epoch: 292, batch: 24, loss: 0.004766575992107391, acc: 100.0, f1: 100.0, r: 0.7482012795961408
06/01/2019 10:44:05 step: 9666, epoch: 292, batch: 29, loss: 0.01886046677827835, acc: 98.4375, f1: 99.20694459329118, r: 0.731135386206671
06/01/2019 10:44:05 *** evaluating ***
06/01/2019 10:44:05 step: 293, epoch: 292, acc: 58.97435897435898, f1: 20.47154017857143, r: 0.28185498935663267
06/01/2019 10:44:05 *** epoch: 294 ***
06/01/2019 10:44:05 *** training ***
06/01/2019 10:44:05 step: 9674, epoch: 293, batch: 4, loss: 0.0006736665964126587, acc: 100.0, f1: 100.0, r: 0.7456940757624103
06/01/2019 10:44:05 step: 9679, epoch: 293, batch: 9, loss: 0.00158005952835083, acc: 100.0, f1: 100.0, r: 0.7856485946462503
06/01/2019 10:44:06 step: 9684, epoch: 293, batch: 14, loss: 0.0031119585037231445, acc: 100.0, f1: 100.0, r: 0.65696388200622
06/01/2019 10:44:06 step: 9689, epoch: 293, batch: 19, loss: 0.006595052778720856, acc: 100.0, f1: 100.0, r: 0.7844601147229425
06/01/2019 10:44:06 step: 9694, epoch: 293, batch: 24, loss: 0.008408308029174805, acc: 100.0, f1: 100.0, r: 0.6805187099689158
06/01/2019 10:44:06 step: 9699, epoch: 293, batch: 29, loss: 0.0021287575364112854, acc: 100.0, f1: 100.0, r: 0.6912370999092119
06/01/2019 10:44:06 *** evaluating ***
06/01/2019 10:44:07 step: 294, epoch: 293, acc: 59.82905982905983, f1: 21.6718485334503, r: 0.28601508802396036
06/01/2019 10:44:07 *** epoch: 295 ***
06/01/2019 10:44:07 *** training ***
06/01/2019 10:44:07 step: 9707, epoch: 294, batch: 4, loss: 0.0022053346037864685, acc: 100.0, f1: 100.0, r: 0.8176606069379886
06/01/2019 10:44:07 step: 9712, epoch: 294, batch: 9, loss: 0.0003924444317817688, acc: 100.0, f1: 100.0, r: 0.7173015980164107
06/01/2019 10:44:07 step: 9717, epoch: 294, batch: 14, loss: 0.0011440888047218323, acc: 100.0, f1: 100.0, r: 0.8072689395592185
06/01/2019 10:44:07 step: 9722, epoch: 294, batch: 19, loss: 0.002055823802947998, acc: 100.0, f1: 100.0, r: 0.688875830239306
06/01/2019 10:44:08 step: 9727, epoch: 294, batch: 24, loss: 0.002406686544418335, acc: 100.0, f1: 100.0, r: 0.802295781847097
06/01/2019 10:44:08 step: 9732, epoch: 294, batch: 29, loss: 0.005075626075267792, acc: 100.0, f1: 100.0, r: 0.7260615818301597
06/01/2019 10:44:08 *** evaluating ***
06/01/2019 10:44:08 step: 295, epoch: 294, acc: 58.119658119658126, f1: 21.083665999630462, r: 0.28863494299817716
06/01/2019 10:44:08 *** epoch: 296 ***
06/01/2019 10:44:08 *** training ***
06/01/2019 10:44:08 step: 9740, epoch: 295, batch: 4, loss: 0.0003644227981567383, acc: 100.0, f1: 100.0, r: 0.7720592918325208
06/01/2019 10:44:09 step: 9745, epoch: 295, batch: 9, loss: 0.02273261547088623, acc: 98.4375, f1: 98.1111111111111, r: 0.8043264132543263
06/01/2019 10:44:09 step: 9750, epoch: 295, batch: 14, loss: 0.0030435100197792053, acc: 100.0, f1: 100.0, r: 0.7103940891257972
06/01/2019 10:44:09 step: 9755, epoch: 295, batch: 19, loss: 0.00117531418800354, acc: 100.0, f1: 100.0, r: 0.7217373635796115
06/01/2019 10:44:09 step: 9760, epoch: 295, batch: 24, loss: 0.006368644535541534, acc: 100.0, f1: 100.0, r: 0.704875692434523
06/01/2019 10:44:10 step: 9765, epoch: 295, batch: 29, loss: 0.006500907242298126, acc: 100.0, f1: 100.0, r: 0.7429603039454603
06/01/2019 10:44:10 *** evaluating ***
06/01/2019 10:44:10 step: 296, epoch: 295, acc: 54.27350427350427, f1: 19.340991013233257, r: 0.2698858211265822
06/01/2019 10:44:10 *** epoch: 297 ***
06/01/2019 10:44:10 *** training ***
06/01/2019 10:44:10 step: 9773, epoch: 296, batch: 4, loss: 0.0005293190479278564, acc: 100.0, f1: 100.0, r: 0.7967222031426173
06/01/2019 10:44:10 step: 9778, epoch: 296, batch: 9, loss: 0.006732814013957977, acc: 100.0, f1: 100.0, r: 0.739533750632609
06/01/2019 10:44:10 step: 9783, epoch: 296, batch: 14, loss: 0.005780458450317383, acc: 100.0, f1: 100.0, r: 0.6896516487694435
06/01/2019 10:44:11 step: 9788, epoch: 296, batch: 19, loss: 0.0005781650543212891, acc: 100.0, f1: 100.0, r: 0.7283667848966534
06/01/2019 10:44:11 step: 9793, epoch: 296, batch: 24, loss: 0.001476377248764038, acc: 100.0, f1: 100.0, r: 0.7160640884967933
06/01/2019 10:44:11 step: 9798, epoch: 296, batch: 29, loss: 0.0023060068488121033, acc: 100.0, f1: 100.0, r: 0.8516114258049653
06/01/2019 10:44:11 *** evaluating ***
06/01/2019 10:44:11 step: 297, epoch: 296, acc: 56.837606837606835, f1: 19.869888071507674, r: 0.2747884641296824
06/01/2019 10:44:11 *** epoch: 298 ***
06/01/2019 10:44:11 *** training ***
06/01/2019 10:44:12 step: 9806, epoch: 297, batch: 4, loss: 0.0003406703472137451, acc: 100.0, f1: 100.0, r: 0.8039397704474815
06/01/2019 10:44:12 step: 9811, epoch: 297, batch: 9, loss: 0.012203365564346313, acc: 100.0, f1: 100.0, r: 0.7956467695308151
06/01/2019 10:44:12 step: 9816, epoch: 297, batch: 14, loss: 0.0007208213210105896, acc: 100.0, f1: 100.0, r: 0.8124908413319747
06/01/2019 10:44:12 step: 9821, epoch: 297, batch: 19, loss: 0.001763559877872467, acc: 100.0, f1: 100.0, r: 0.7263641235009151
06/01/2019 10:44:13 step: 9826, epoch: 297, batch: 24, loss: 0.0005943179130554199, acc: 100.0, f1: 100.0, r: 0.704699592009423
06/01/2019 10:44:13 step: 9831, epoch: 297, batch: 29, loss: 0.0010340064764022827, acc: 100.0, f1: 100.0, r: 0.7095545909283439
06/01/2019 10:44:13 *** evaluating ***
06/01/2019 10:44:13 step: 298, epoch: 297, acc: 58.54700854700855, f1: 20.762253063265813, r: 0.2722492154226863
06/01/2019 10:44:13 *** epoch: 299 ***
06/01/2019 10:44:13 *** training ***
06/01/2019 10:44:13 step: 9839, epoch: 298, batch: 4, loss: 0.002942316234111786, acc: 100.0, f1: 100.0, r: 0.7841694119577687
06/01/2019 10:44:13 step: 9844, epoch: 298, batch: 9, loss: 0.0006841942667961121, acc: 100.0, f1: 100.0, r: 0.7241944963989705
06/01/2019 10:44:14 step: 9849, epoch: 298, batch: 14, loss: 0.022362053394317627, acc: 98.4375, f1: 97.78325123152709, r: 0.7888763513040664
06/01/2019 10:44:14 step: 9854, epoch: 298, batch: 19, loss: 0.0008143484592437744, acc: 100.0, f1: 100.0, r: 0.6710087538509414
06/01/2019 10:44:14 step: 9859, epoch: 298, batch: 24, loss: 0.001975640654563904, acc: 100.0, f1: 100.0, r: 0.7487675225845063
06/01/2019 10:44:14 step: 9864, epoch: 298, batch: 29, loss: 0.0029683299362659454, acc: 100.0, f1: 100.0, r: 0.8145405984359896
06/01/2019 10:44:14 *** evaluating ***
06/01/2019 10:44:15 step: 299, epoch: 298, acc: 58.97435897435898, f1: 19.681964248017323, r: 0.26113361964771725
06/01/2019 10:44:15 *** epoch: 300 ***
06/01/2019 10:44:15 *** training ***
06/01/2019 10:44:15 step: 9872, epoch: 299, batch: 4, loss: 0.010373100638389587, acc: 100.0, f1: 100.0, r: 0.715187144838564
06/01/2019 10:44:15 step: 9877, epoch: 299, batch: 9, loss: 0.0019231736660003662, acc: 100.0, f1: 100.0, r: 0.7377874670762677
06/01/2019 10:44:15 step: 9882, epoch: 299, batch: 14, loss: 0.0010755211114883423, acc: 100.0, f1: 100.0, r: 0.8077908434932304
06/01/2019 10:44:15 step: 9887, epoch: 299, batch: 19, loss: 0.0025210529565811157, acc: 100.0, f1: 100.0, r: 0.7774157986510786
06/01/2019 10:44:16 step: 9892, epoch: 299, batch: 24, loss: 0.0006627216935157776, acc: 100.0, f1: 100.0, r: 0.7351904965990074
06/01/2019 10:44:16 step: 9897, epoch: 299, batch: 29, loss: 0.0003778785467147827, acc: 100.0, f1: 100.0, r: 0.8216933587735961
06/01/2019 10:44:16 *** evaluating ***
06/01/2019 10:44:16 step: 300, epoch: 299, acc: 59.401709401709404, f1: 21.475758915414087, r: 0.28002675419932893
06/01/2019 10:44:16 
*** Best acc model ***
epoch: 288
acc: 61.111111111111114
f1: 23.318538750426768
corr: 0.29833841809452444
06/01/2019 10:44:16 Loading Test Data
06/01/2019 10:44:16 load data from data/word2vec_temp/test_text.npy, data/word2vec_temp/test_label.npy, training: False
06/01/2019 10:44:40 loaded. total len: 2228
06/01/2019 10:44:40 Test: length: 2228, total batch: 35, batch size: 64
06/01/2019 10:44:40 
*** Test Result ***
acc: 59.401709401709404
f1: 21.475758915414087
corr: 0.28002675419932893
