06/01/2019 11:27:57 {'input_path': 'data/word2vec_temp', 'output_path': 'save/lstm_1', 'gpu': True, 'seed': 20000125, 'display_per_batch': 5, 'optimizer': 'adagrad', 'lr': 0.01, 'lr_decay': 0, 'weight_decay': 0.0001, 'momentum': 0.985, 'num_epochs': 300, 'batch_size': 64, 'num_labels': 8, 'embedding_size': 300, 'type': 'rnn', 'rnn': {'type': 'lstm', 'bidirectional': False, 'rnn_hidden_size': 256, 'mlp_hidden_size': 512, 'dropout': 0.5, 'p_coefficient': 1, 'num_layers': 1, 'param_da': 350, 'param_r': 30, 'loss': 'cross_entropy'}}
06/01/2019 11:27:57 Loading Train Data
06/01/2019 11:27:57 load data from data/word2vec_temp/train_text.npy, data/word2vec_temp/train_label.npy, training: True
06/01/2019 11:28:18 loaded. total len: 2342
06/01/2019 11:28:18 Train: length: 2108, total batch: 33, batch size: 64
06/01/2019 11:28:18 Dev: length: 234, total batch: 4, batch size: 64
06/01/2019 11:28:18 Loading model rnn
06/01/2019 11:28:25 *** epoch: 1 ***
06/01/2019 11:28:25 *** training ***
06/01/2019 11:28:26 step: 5, epoch: 0, batch: 4, loss: 8.198135375976562, acc: 6.25, f1: 3.358638141246837, r: -0.02570393158928488
06/01/2019 11:28:27 step: 10, epoch: 0, batch: 9, loss: 6.89933967590332, acc: 18.75, f1: 6.987201660325147, r: 0.015086464348197
06/01/2019 11:28:28 step: 15, epoch: 0, batch: 14, loss: 5.208093166351318, acc: 42.1875, f1: 9.554597701149426, r: 0.11622712356262724
06/01/2019 11:28:28 step: 20, epoch: 0, batch: 19, loss: 5.397528648376465, acc: 35.9375, f1: 11.468129571577848, r: 0.06810294549104841
06/01/2019 11:28:29 step: 25, epoch: 0, batch: 24, loss: 5.25307035446167, acc: 42.1875, f1: 12.361923326835608, r: 0.07398225572521158
06/01/2019 11:28:30 step: 30, epoch: 0, batch: 29, loss: 5.216521263122559, acc: 35.9375, f1: 12.938172043010754, r: 0.06631382000452717
06/01/2019 11:28:30 *** evaluating ***
06/01/2019 11:28:30 step: 1, epoch: 0, acc: 57.692307692307686, f1: 16.61067444219067, r: 0.30255446630279875
06/01/2019 11:28:30 *** epoch: 2 ***
06/01/2019 11:28:30 *** training ***
06/01/2019 11:28:31 step: 38, epoch: 1, batch: 4, loss: 3.902522563934326, acc: 51.5625, f1: 13.708961845607808, r: 0.1507598264091065
06/01/2019 11:28:31 step: 43, epoch: 1, batch: 9, loss: 5.205309867858887, acc: 43.75, f1: 15.476190476190474, r: 0.012317925941639777
06/01/2019 11:28:32 step: 48, epoch: 1, batch: 14, loss: 4.192645072937012, acc: 31.25, f1: 6.802721088435374, r: 0.1409326586093148
06/01/2019 11:28:33 step: 53, epoch: 1, batch: 19, loss: 4.158708572387695, acc: 40.625, f1: 9.836601307189543, r: 0.20329125651827787
06/01/2019 11:28:33 step: 58, epoch: 1, batch: 24, loss: 3.782261610031128, acc: 43.75, f1: 15.0997150997151, r: 0.2146013530377098
06/01/2019 11:28:34 step: 63, epoch: 1, batch: 29, loss: 3.5562586784362793, acc: 48.4375, f1: 15.104166666666668, r: 0.24241474660012635
06/01/2019 11:28:34 *** evaluating ***
06/01/2019 11:28:34 step: 2, epoch: 1, acc: 57.26495726495726, f1: 16.443850267379677, r: 0.3307840193765201
06/01/2019 11:28:34 *** epoch: 3 ***
06/01/2019 11:28:34 *** training ***
06/01/2019 11:28:35 step: 71, epoch: 2, batch: 4, loss: 3.722060203552246, acc: 34.375, f1: 15.99824510975628, r: 0.19092364319668823
06/01/2019 11:28:36 step: 76, epoch: 2, batch: 9, loss: 3.433547019958496, acc: 51.5625, f1: 19.879658603685858, r: 0.25903446949390346
06/01/2019 11:28:36 step: 81, epoch: 2, batch: 14, loss: 3.718467950820923, acc: 40.625, f1: 12.698412698412698, r: 0.21894694636792628
06/01/2019 11:28:37 step: 86, epoch: 2, batch: 19, loss: 3.315253257751465, acc: 40.625, f1: 14.603174603174601, r: 0.3286214632263878
06/01/2019 11:28:37 step: 91, epoch: 2, batch: 24, loss: 3.360029697418213, acc: 54.6875, f1: 19.85659128516271, r: 0.33059997486471315
06/01/2019 11:28:38 step: 96, epoch: 2, batch: 29, loss: 3.3879823684692383, acc: 48.4375, f1: 13.803088803088803, r: 0.19775889538307764
06/01/2019 11:28:38 *** evaluating ***
06/01/2019 11:28:39 step: 3, epoch: 2, acc: 57.26495726495726, f1: 16.622807017543863, r: 0.3165643574735145
06/01/2019 11:28:39 *** epoch: 4 ***
06/01/2019 11:28:39 *** training ***
06/01/2019 11:28:39 step: 104, epoch: 3, batch: 4, loss: 3.265913963317871, acc: 35.9375, f1: 11.501831501831502, r: 0.1663400748070634
06/01/2019 11:28:40 step: 109, epoch: 3, batch: 9, loss: 3.005807638168335, acc: 48.4375, f1: 16.01700921332388, r: 0.36266408927502997
06/01/2019 11:28:41 step: 114, epoch: 3, batch: 14, loss: 2.878544330596924, acc: 53.125, f1: 17.824513794663048, r: 0.3400754505051727
06/01/2019 11:28:41 step: 119, epoch: 3, batch: 19, loss: 3.28732967376709, acc: 40.625, f1: 16.241496598639454, r: 0.2287820496685286
06/01/2019 11:28:42 step: 124, epoch: 3, batch: 24, loss: 2.979768991470337, acc: 51.5625, f1: 22.105263157894736, r: 0.303910734455268
06/01/2019 11:28:43 step: 129, epoch: 3, batch: 29, loss: 3.174374580383301, acc: 43.75, f1: 15.432098765432102, r: 0.28551913359445613
06/01/2019 11:28:43 *** evaluating ***
06/01/2019 11:28:43 step: 4, epoch: 3, acc: 58.119658119658126, f1: 16.956967213114755, r: 0.34468548513783764
06/01/2019 11:28:43 *** epoch: 5 ***
06/01/2019 11:28:43 *** training ***
06/01/2019 11:28:44 step: 137, epoch: 4, batch: 4, loss: 2.8043999671936035, acc: 50.0, f1: 21.150278293135433, r: 0.29850778440810466
06/01/2019 11:28:44 step: 142, epoch: 4, batch: 9, loss: 2.7270419597625732, acc: 57.8125, f1: 17.36842105263158, r: 0.2853080032159581
06/01/2019 11:28:45 step: 147, epoch: 4, batch: 14, loss: 3.6755638122558594, acc: 59.375, f1: 36.430572228891556, r: 0.3314365251207126
06/01/2019 11:28:46 step: 152, epoch: 4, batch: 19, loss: 2.837221622467041, acc: 56.25, f1: 26.06449106449106, r: 0.24035066498584629
06/01/2019 11:28:46 step: 157, epoch: 4, batch: 24, loss: 2.857074737548828, acc: 46.875, f1: 19.659350307287095, r: 0.3657921505446777
06/01/2019 11:28:47 step: 162, epoch: 4, batch: 29, loss: 2.727174758911133, acc: 50.0, f1: 23.14814814814815, r: 0.29901520640618295
06/01/2019 11:28:47 *** evaluating ***
06/01/2019 11:28:48 step: 5, epoch: 4, acc: 56.41025641025641, f1: 16.49122807017544, r: 0.3651496176521584
06/01/2019 11:28:48 *** epoch: 6 ***
06/01/2019 11:28:48 *** training ***
06/01/2019 11:28:48 step: 170, epoch: 5, batch: 4, loss: 2.656494140625, acc: 42.1875, f1: 19.45642159572508, r: 0.3745435108784724
06/01/2019 11:28:49 step: 175, epoch: 5, batch: 9, loss: 2.3270459175109863, acc: 51.5625, f1: 25.16059318876221, r: 0.3472381667031697
06/01/2019 11:28:50 step: 180, epoch: 5, batch: 14, loss: 2.5582594871520996, acc: 45.3125, f1: 21.958251643409064, r: 0.3828561858789976
06/01/2019 11:28:50 step: 185, epoch: 5, batch: 19, loss: 2.8086538314819336, acc: 42.1875, f1: 15.894736842105262, r: 0.24203514548233912
06/01/2019 11:28:51 step: 190, epoch: 5, batch: 24, loss: 2.5394039154052734, acc: 46.875, f1: 18.49431818181818, r: 0.42602490993038644
06/01/2019 11:28:52 step: 195, epoch: 5, batch: 29, loss: 2.672562599182129, acc: 42.1875, f1: 15.183673469387754, r: 0.3447582653457095
06/01/2019 11:28:52 *** evaluating ***
06/01/2019 11:28:52 step: 6, epoch: 5, acc: 57.692307692307686, f1: 18.483568075117372, r: 0.37599053184496906
06/01/2019 11:28:52 *** epoch: 7 ***
06/01/2019 11:28:52 *** training ***
06/01/2019 11:28:53 step: 203, epoch: 6, batch: 4, loss: 2.239259719848633, acc: 53.125, f1: 19.345238095238095, r: 0.2809780146529216
06/01/2019 11:28:54 step: 208, epoch: 6, batch: 9, loss: 2.3109941482543945, acc: 50.0, f1: 18.54594330400782, r: 0.37148652294235307
06/01/2019 11:28:54 step: 213, epoch: 6, batch: 14, loss: 2.309644937515259, acc: 53.125, f1: 24.53071083505866, r: 0.3948467034567202
06/01/2019 11:28:55 step: 218, epoch: 6, batch: 19, loss: 2.1713149547576904, acc: 51.5625, f1: 19.45862960568843, r: 0.46701752141441455
06/01/2019 11:28:56 step: 223, epoch: 6, batch: 24, loss: 2.6167044639587402, acc: 51.5625, f1: 32.07121676509432, r: 0.36939972881777494
06/01/2019 11:28:56 step: 228, epoch: 6, batch: 29, loss: 2.5270538330078125, acc: 53.125, f1: 19.022556390977442, r: 0.365929527972436
06/01/2019 11:28:57 *** evaluating ***
06/01/2019 11:28:57 step: 7, epoch: 6, acc: 58.119658119658126, f1: 24.10900366077197, r: 0.36276380254466667
06/01/2019 11:28:57 *** epoch: 8 ***
06/01/2019 11:28:57 *** training ***
06/01/2019 11:28:57 step: 236, epoch: 7, batch: 4, loss: 2.281430721282959, acc: 53.125, f1: 24.211876832844574, r: 0.451953712840208
06/01/2019 11:28:58 step: 241, epoch: 7, batch: 9, loss: 2.2477715015411377, acc: 46.875, f1: 18.429565929565932, r: 0.38005277381355873
06/01/2019 11:28:59 step: 246, epoch: 7, batch: 14, loss: 2.148184061050415, acc: 48.4375, f1: 22.808737530099762, r: 0.41500350174530787
06/01/2019 11:28:59 step: 251, epoch: 7, batch: 19, loss: 2.3754806518554688, acc: 48.4375, f1: 20.0, r: 0.3446776505131969
06/01/2019 11:29:00 step: 256, epoch: 7, batch: 24, loss: 2.4219839572906494, acc: 43.75, f1: 21.179985404123332, r: 0.3958088964786471
06/01/2019 11:29:01 step: 261, epoch: 7, batch: 29, loss: 2.545529842376709, acc: 51.5625, f1: 30.830354740766364, r: 0.3851164585687503
06/01/2019 11:29:01 *** evaluating ***
06/01/2019 11:29:01 step: 8, epoch: 7, acc: 58.97435897435898, f1: 20.50869800906761, r: 0.3706723492882889
06/01/2019 11:29:01 *** epoch: 9 ***
06/01/2019 11:29:01 *** training ***
06/01/2019 11:29:02 step: 269, epoch: 8, batch: 4, loss: 1.6712735891342163, acc: 71.875, f1: 37.20605862482292, r: 0.5146455775645152
06/01/2019 11:29:02 step: 274, epoch: 8, batch: 9, loss: 1.7643760442733765, acc: 57.8125, f1: 36.499648999649004, r: 0.5340433199749817
06/01/2019 11:29:03 step: 279, epoch: 8, batch: 14, loss: 2.1310505867004395, acc: 45.3125, f1: 16.52046783625731, r: 0.379534739600251
06/01/2019 11:29:04 step: 284, epoch: 8, batch: 19, loss: 2.3926591873168945, acc: 48.4375, f1: 23.257575757575754, r: 0.42230367590308243
06/01/2019 11:29:04 step: 289, epoch: 8, batch: 24, loss: 2.3409626483917236, acc: 45.3125, f1: 23.56691919191919, r: 0.2935190919181261
06/01/2019 11:29:05 step: 294, epoch: 8, batch: 29, loss: 1.9884934425354004, acc: 57.8125, f1: 29.41513954979582, r: 0.49338052914654185
06/01/2019 11:29:05 *** evaluating ***
06/01/2019 11:29:05 step: 9, epoch: 8, acc: 57.26495726495726, f1: 22.425618050333895, r: 0.3629992202391662
06/01/2019 11:29:05 *** epoch: 10 ***
06/01/2019 11:29:05 *** training ***
06/01/2019 11:29:06 step: 302, epoch: 9, batch: 4, loss: 2.245634078979492, acc: 62.5, f1: 36.38621343201889, r: 0.45601646325297884
06/01/2019 11:29:06 step: 307, epoch: 9, batch: 9, loss: 2.0605530738830566, acc: 59.375, f1: 31.85132214594457, r: 0.46836323360650844
06/01/2019 11:29:07 step: 312, epoch: 9, batch: 14, loss: 2.0687544345855713, acc: 53.125, f1: 26.383406132158804, r: 0.39684654657000407
06/01/2019 11:29:08 step: 317, epoch: 9, batch: 19, loss: 1.8615214824676514, acc: 54.6875, f1: 32.45456636760985, r: 0.3190590303182725
06/01/2019 11:29:08 step: 322, epoch: 9, batch: 24, loss: 1.919681191444397, acc: 54.6875, f1: 28.47723704866562, r: 0.3676276198038489
06/01/2019 11:29:09 step: 327, epoch: 9, batch: 29, loss: 1.7918663024902344, acc: 60.9375, f1: 33.953192275560696, r: 0.47477718696961635
06/01/2019 11:29:09 *** evaluating ***
06/01/2019 11:29:10 step: 10, epoch: 9, acc: 58.119658119658126, f1: 20.449815828533303, r: 0.3700879317778595
06/01/2019 11:29:10 *** epoch: 11 ***
06/01/2019 11:29:10 *** training ***
06/01/2019 11:29:10 step: 335, epoch: 10, batch: 4, loss: 1.482877254486084, acc: 56.25, f1: 33.02678154010552, r: 0.43633093201448736
06/01/2019 11:29:11 step: 340, epoch: 10, batch: 9, loss: 1.7399640083312988, acc: 51.5625, f1: 34.40880450540872, r: 0.519690823140236
06/01/2019 11:29:12 step: 345, epoch: 10, batch: 14, loss: 1.5115585327148438, acc: 59.375, f1: 35.1578947368421, r: 0.5885359036902639
06/01/2019 11:29:12 step: 350, epoch: 10, batch: 19, loss: 1.6725226640701294, acc: 53.125, f1: 36.638888888888886, r: 0.510601867873497
06/01/2019 11:29:13 step: 355, epoch: 10, batch: 24, loss: 1.830773949623108, acc: 40.625, f1: 22.602345415778252, r: 0.3414536092343736
06/01/2019 11:29:13 step: 360, epoch: 10, batch: 29, loss: 1.8740051984786987, acc: 51.5625, f1: 25.78268588137009, r: 0.3831430861593465
06/01/2019 11:29:14 *** evaluating ***
06/01/2019 11:29:14 step: 11, epoch: 10, acc: 57.692307692307686, f1: 26.799448442250295, r: 0.36346664999220724
06/01/2019 11:29:14 *** epoch: 12 ***
06/01/2019 11:29:14 *** training ***
06/01/2019 11:29:14 step: 368, epoch: 11, batch: 4, loss: 1.379690408706665, acc: 64.0625, f1: 37.562417523676935, r: 0.5262116926620636
06/01/2019 11:29:15 step: 373, epoch: 11, batch: 9, loss: 1.439561128616333, acc: 65.625, f1: 48.042396445757795, r: 0.49674779934890007
06/01/2019 11:29:16 step: 378, epoch: 11, batch: 14, loss: 1.4182720184326172, acc: 62.5, f1: 45.482304026943275, r: 0.629653967861128
06/01/2019 11:29:16 step: 383, epoch: 11, batch: 19, loss: 1.7085176706314087, acc: 53.125, f1: 31.008343091334893, r: 0.44805904358076437
06/01/2019 11:29:17 step: 388, epoch: 11, batch: 24, loss: 1.5814745426177979, acc: 56.25, f1: 47.51225490196079, r: 0.4749654387178459
06/01/2019 11:29:18 step: 393, epoch: 11, batch: 29, loss: 1.2880735397338867, acc: 71.875, f1: 57.833333333333336, r: 0.581428050784803
06/01/2019 11:29:18 *** evaluating ***
06/01/2019 11:29:18 step: 12, epoch: 11, acc: 58.54700854700855, f1: 24.03077824049974, r: 0.36880827818899625
06/01/2019 11:29:18 *** epoch: 13 ***
06/01/2019 11:29:18 *** training ***
06/01/2019 11:29:19 step: 401, epoch: 12, batch: 4, loss: 1.453749179840088, acc: 60.9375, f1: 44.10081276929103, r: 0.4874885891755002
06/01/2019 11:29:19 step: 406, epoch: 12, batch: 9, loss: 1.297102451324463, acc: 62.5, f1: 45.34359645497543, r: 0.5358403512688187
06/01/2019 11:29:20 step: 411, epoch: 12, batch: 14, loss: 1.4022094011306763, acc: 60.9375, f1: 45.49603174603175, r: 0.5204145596652365
06/01/2019 11:29:21 step: 416, epoch: 12, batch: 19, loss: 1.3296550512313843, acc: 57.8125, f1: 29.57142857142857, r: 0.40082786330619213
06/01/2019 11:29:21 step: 421, epoch: 12, batch: 24, loss: 1.2584638595581055, acc: 68.75, f1: 42.92415473652147, r: 0.4321186577441175
06/01/2019 11:29:22 step: 426, epoch: 12, batch: 29, loss: 1.35313081741333, acc: 64.0625, f1: 40.717261904761905, r: 0.4789123144720526
06/01/2019 11:29:22 *** evaluating ***
06/01/2019 11:29:22 step: 13, epoch: 12, acc: 56.837606837606835, f1: 20.8800083529837, r: 0.3749441032622835
06/01/2019 11:29:22 *** epoch: 14 ***
06/01/2019 11:29:22 *** training ***
06/01/2019 11:29:23 step: 434, epoch: 13, batch: 4, loss: 1.1762022972106934, acc: 73.4375, f1: 48.47061873959442, r: 0.5502916784076224
06/01/2019 11:29:24 step: 439, epoch: 13, batch: 9, loss: 1.1744639873504639, acc: 65.625, f1: 36.664426523297486, r: 0.4634367491202178
06/01/2019 11:29:24 step: 444, epoch: 13, batch: 14, loss: 1.1153154373168945, acc: 71.875, f1: 43.62228904100825, r: 0.4598355937645062
06/01/2019 11:29:25 step: 449, epoch: 13, batch: 19, loss: 1.2116811275482178, acc: 67.1875, f1: 55.365967365967364, r: 0.5451894958378297
06/01/2019 11:29:26 step: 454, epoch: 13, batch: 24, loss: 1.2129724025726318, acc: 59.375, f1: 34.51282051282051, r: 0.5365698063959108
06/01/2019 11:29:26 step: 459, epoch: 13, batch: 29, loss: 1.3378087282180786, acc: 56.25, f1: 36.520146520146525, r: 0.46934940928650004
06/01/2019 11:29:26 *** evaluating ***
06/01/2019 11:29:27 step: 14, epoch: 13, acc: 57.692307692307686, f1: 23.37571905299067, r: 0.3614928046239755
06/01/2019 11:29:27 *** epoch: 15 ***
06/01/2019 11:29:27 *** training ***
06/01/2019 11:29:27 step: 467, epoch: 14, batch: 4, loss: 1.058500051498413, acc: 67.1875, f1: 55.27028015507278, r: 0.52133509164191
06/01/2019 11:29:28 step: 472, epoch: 14, batch: 9, loss: 1.0166267156600952, acc: 71.875, f1: 46.613069470212324, r: 0.5042293291671122
06/01/2019 11:29:29 step: 477, epoch: 14, batch: 14, loss: 1.0923186540603638, acc: 60.9375, f1: 43.77721853359035, r: 0.4844975010091277
06/01/2019 11:29:29 step: 482, epoch: 14, batch: 19, loss: 1.1557666063308716, acc: 59.375, f1: 36.561468088561675, r: 0.47124572728306313
06/01/2019 11:29:30 step: 487, epoch: 14, batch: 24, loss: 1.063279390335083, acc: 73.4375, f1: 43.81555944055944, r: 0.57098937316282
06/01/2019 11:29:30 step: 492, epoch: 14, batch: 29, loss: 1.076395034790039, acc: 71.875, f1: 56.61595547309833, r: 0.5943403207373461
06/01/2019 11:29:31 *** evaluating ***
06/01/2019 11:29:31 step: 15, epoch: 14, acc: 55.55555555555556, f1: 23.072104124539408, r: 0.3731317708563187
06/01/2019 11:29:31 *** epoch: 16 ***
06/01/2019 11:29:31 *** training ***
06/01/2019 11:29:31 step: 500, epoch: 15, batch: 4, loss: 1.0227323770523071, acc: 67.1875, f1: 46.4935064935065, r: 0.528567940600806
06/01/2019 11:29:32 step: 505, epoch: 15, batch: 9, loss: 1.2627465724945068, acc: 82.8125, f1: 57.18645779151241, r: 0.5805755928613876
06/01/2019 11:29:33 step: 510, epoch: 15, batch: 14, loss: 1.078855276107788, acc: 76.5625, f1: 60.31891693212448, r: 0.652364263850028
06/01/2019 11:29:33 step: 515, epoch: 15, batch: 19, loss: 0.9059127569198608, acc: 68.75, f1: 46.6540404040404, r: 0.6891683254106431
06/01/2019 11:29:34 step: 520, epoch: 15, batch: 24, loss: 1.8381717205047607, acc: 62.5, f1: 46.675096724132366, r: 0.466265498352281
06/01/2019 11:29:35 step: 525, epoch: 15, batch: 29, loss: 1.5219814777374268, acc: 60.9375, f1: 38.34375, r: 0.5105257526742379
06/01/2019 11:29:35 *** evaluating ***
06/01/2019 11:29:35 step: 16, epoch: 15, acc: 57.692307692307686, f1: 22.958138173302107, r: 0.3655494780917077
06/01/2019 11:29:35 *** epoch: 17 ***
06/01/2019 11:29:35 *** training ***
06/01/2019 11:29:36 step: 533, epoch: 16, batch: 4, loss: 1.148258924484253, acc: 59.375, f1: 31.314255919519074, r: 0.5028885967612412
06/01/2019 11:29:36 step: 538, epoch: 16, batch: 9, loss: 0.7708659768104553, acc: 73.4375, f1: 58.53162317558602, r: 0.6024718089058556
06/01/2019 11:29:37 step: 543, epoch: 16, batch: 14, loss: 1.366705060005188, acc: 75.0, f1: 62.49446981589839, r: 0.5252670563787285
06/01/2019 11:29:38 step: 548, epoch: 16, batch: 19, loss: 0.8536213636398315, acc: 75.0, f1: 60.761746596006894, r: 0.6009165930742851
06/01/2019 11:29:38 step: 553, epoch: 16, batch: 24, loss: 1.1025757789611816, acc: 56.25, f1: 28.05219824789098, r: 0.6105824561364713
06/01/2019 11:29:39 step: 558, epoch: 16, batch: 29, loss: 0.999579906463623, acc: 67.1875, f1: 49.331655045940764, r: 0.48616123321714777
06/01/2019 11:29:39 *** evaluating ***
06/01/2019 11:29:39 step: 17, epoch: 16, acc: 58.54700854700855, f1: 25.570230227063874, r: 0.3651673703059582
06/01/2019 11:29:39 *** epoch: 18 ***
06/01/2019 11:29:39 *** training ***
06/01/2019 11:29:40 step: 566, epoch: 17, batch: 4, loss: 0.8434017896652222, acc: 68.75, f1: 45.030525030525034, r: 0.601566407447812
06/01/2019 11:29:40 step: 571, epoch: 17, batch: 9, loss: 0.8714088797569275, acc: 75.0, f1: 63.11547266434484, r: 0.5510031618305548
06/01/2019 11:29:41 step: 576, epoch: 17, batch: 14, loss: 0.8762221932411194, acc: 70.3125, f1: 62.46866339128244, r: 0.558625164007454
06/01/2019 11:29:42 step: 581, epoch: 17, batch: 19, loss: 1.001550555229187, acc: 65.625, f1: 42.11309523809524, r: 0.5910973642911874
06/01/2019 11:29:42 step: 586, epoch: 17, batch: 24, loss: 1.0086270570755005, acc: 67.1875, f1: 49.66057527378281, r: 0.618991213982672
06/01/2019 11:29:43 step: 591, epoch: 17, batch: 29, loss: 0.8489421606063843, acc: 75.0, f1: 61.29251700680272, r: 0.5110312085942564
06/01/2019 11:29:43 *** evaluating ***
06/01/2019 11:29:43 step: 18, epoch: 17, acc: 50.427350427350426, f1: 20.865150646577817, r: 0.34100035562387215
06/01/2019 11:29:43 *** epoch: 19 ***
06/01/2019 11:29:43 *** training ***
06/01/2019 11:29:44 step: 599, epoch: 18, batch: 4, loss: 0.6336894631385803, acc: 87.5, f1: 76.45211930926216, r: 0.6522837360844912
06/01/2019 11:29:45 step: 604, epoch: 18, batch: 9, loss: 1.6390037536621094, acc: 71.875, f1: 56.5761689291101, r: 0.6409946837372763
06/01/2019 11:29:45 step: 609, epoch: 18, batch: 14, loss: 0.6437649726867676, acc: 81.25, f1: 63.398454993282584, r: 0.6817288839788291
06/01/2019 11:29:46 step: 614, epoch: 18, batch: 19, loss: 1.163451910018921, acc: 76.5625, f1: 65.54303539597657, r: 0.5471936383559791
06/01/2019 11:29:47 step: 619, epoch: 18, batch: 24, loss: 0.8627327680587769, acc: 78.125, f1: 70.29165512358787, r: 0.6020003135369353
06/01/2019 11:29:47 step: 624, epoch: 18, batch: 29, loss: 0.8043810129165649, acc: 78.125, f1: 65.07111935683365, r: 0.649220327560441
06/01/2019 11:29:48 *** evaluating ***
06/01/2019 11:29:48 step: 19, epoch: 18, acc: 57.692307692307686, f1: 25.893742913832195, r: 0.3614560506138497
06/01/2019 11:29:48 *** epoch: 20 ***
06/01/2019 11:29:48 *** training ***
06/01/2019 11:29:49 step: 632, epoch: 19, batch: 4, loss: 0.7674811482429504, acc: 75.0, f1: 52.39096307419289, r: 0.5560183126130716
06/01/2019 11:29:49 step: 637, epoch: 19, batch: 9, loss: 0.8502668142318726, acc: 73.4375, f1: 43.11752287121745, r: 0.5564477934626001
06/01/2019 11:29:50 step: 642, epoch: 19, batch: 14, loss: 1.0231082439422607, acc: 60.9375, f1: 53.16241414343421, r: 0.4632577995883615
06/01/2019 11:29:51 step: 647, epoch: 19, batch: 19, loss: 0.6157415509223938, acc: 87.5, f1: 75.36077481840194, r: 0.5696747638313504
06/01/2019 11:29:51 step: 652, epoch: 19, batch: 24, loss: 0.7515086531639099, acc: 75.0, f1: 61.570888128265175, r: 0.5913666020749858
06/01/2019 11:29:52 step: 657, epoch: 19, batch: 29, loss: 0.7247593402862549, acc: 84.375, f1: 61.23955985131101, r: 0.6615163189584348
06/01/2019 11:29:52 *** evaluating ***
06/01/2019 11:29:52 step: 20, epoch: 19, acc: 58.54700854700855, f1: 23.244610152068198, r: 0.35797873793083207
06/01/2019 11:29:52 *** epoch: 21 ***
06/01/2019 11:29:52 *** training ***
06/01/2019 11:29:53 step: 665, epoch: 20, batch: 4, loss: 0.7320846319198608, acc: 79.6875, f1: 63.23275862068966, r: 0.6300724770802522
06/01/2019 11:29:54 step: 670, epoch: 20, batch: 9, loss: 0.8475980162620544, acc: 68.75, f1: 49.00162494147456, r: 0.5887400297089732
06/01/2019 11:29:54 step: 675, epoch: 20, batch: 14, loss: 0.8936707377433777, acc: 60.9375, f1: 47.581215970961885, r: 0.6348647360759793
06/01/2019 11:29:55 step: 680, epoch: 20, batch: 19, loss: 0.6675499081611633, acc: 82.8125, f1: 67.1007206301324, r: 0.6326746640601548
06/01/2019 11:29:55 step: 685, epoch: 20, batch: 24, loss: 0.7891286015510559, acc: 73.4375, f1: 62.47261839098574, r: 0.5899853738255656
06/01/2019 11:29:56 step: 690, epoch: 20, batch: 29, loss: 0.8428933620452881, acc: 68.75, f1: 54.75396800410406, r: 0.7145576821683214
06/01/2019 11:29:56 *** evaluating ***
06/01/2019 11:29:57 step: 21, epoch: 20, acc: 58.119658119658126, f1: 23.100816687896934, r: 0.3550659939089786
06/01/2019 11:29:57 *** epoch: 22 ***
06/01/2019 11:29:57 *** training ***
06/01/2019 11:29:57 step: 698, epoch: 21, batch: 4, loss: 0.7193589210510254, acc: 78.125, f1: 62.48146900269541, r: 0.7124385895445291
06/01/2019 11:29:58 step: 703, epoch: 21, batch: 9, loss: 0.7817704081535339, acc: 73.4375, f1: 60.68850698174006, r: 0.5126594533475994
06/01/2019 11:29:58 step: 708, epoch: 21, batch: 14, loss: 0.6861504912376404, acc: 78.125, f1: 60.49060743178391, r: 0.688549971046152
06/01/2019 11:29:59 step: 713, epoch: 21, batch: 19, loss: 0.7282682657241821, acc: 78.125, f1: 68.86048269231839, r: 0.5408144490595808
06/01/2019 11:30:00 step: 718, epoch: 21, batch: 24, loss: 0.7121753096580505, acc: 82.8125, f1: 63.92775683098264, r: 0.612767882480717
06/01/2019 11:30:00 step: 723, epoch: 21, batch: 29, loss: 0.8283718228340149, acc: 75.0, f1: 54.78151075977162, r: 0.6875815687947414
06/01/2019 11:30:01 *** evaluating ***
06/01/2019 11:30:01 step: 22, epoch: 21, acc: 56.41025641025641, f1: 22.743790613260227, r: 0.35052955109659656
06/01/2019 11:30:01 *** epoch: 23 ***
06/01/2019 11:30:01 *** training ***
06/01/2019 11:30:01 step: 731, epoch: 22, batch: 4, loss: 0.5524130463600159, acc: 85.9375, f1: 65.8881118881119, r: 0.5752174678912232
06/01/2019 11:30:02 step: 736, epoch: 22, batch: 9, loss: 0.6353473663330078, acc: 79.6875, f1: 48.46305457666937, r: 0.6345791284761014
06/01/2019 11:30:03 step: 741, epoch: 22, batch: 14, loss: 1.1093735694885254, acc: 79.6875, f1: 56.30014636972325, r: 0.6591777662979558
06/01/2019 11:30:03 step: 746, epoch: 22, batch: 19, loss: 0.7516799569129944, acc: 70.3125, f1: 52.4056895485467, r: 0.5124890620150049
06/01/2019 11:30:04 step: 751, epoch: 22, batch: 24, loss: 0.6393222212791443, acc: 85.9375, f1: 81.63189467537293, r: 0.5635085489751387
06/01/2019 11:30:04 step: 756, epoch: 22, batch: 29, loss: 0.9752730131149292, acc: 85.9375, f1: 67.00502697785306, r: 0.6479393448836024
06/01/2019 11:30:05 *** evaluating ***
06/01/2019 11:30:05 step: 23, epoch: 22, acc: 50.85470085470085, f1: 26.762906757400025, r: 0.3435557009090586
06/01/2019 11:30:05 *** epoch: 24 ***
06/01/2019 11:30:05 *** training ***
06/01/2019 11:30:06 step: 764, epoch: 23, batch: 4, loss: 0.6238946318626404, acc: 79.6875, f1: 74.88136624202595, r: 0.6298856724180133
06/01/2019 11:30:06 step: 769, epoch: 23, batch: 9, loss: 1.067440390586853, acc: 79.6875, f1: 61.399572649572654, r: 0.6886196925501324
06/01/2019 11:30:07 step: 774, epoch: 23, batch: 14, loss: 0.9480650424957275, acc: 64.0625, f1: 49.05427905427906, r: 0.6754788607987403
06/01/2019 11:30:07 step: 779, epoch: 23, batch: 19, loss: 0.7334054708480835, acc: 81.25, f1: 81.39490139490141, r: 0.6012138060343613
06/01/2019 11:30:08 step: 784, epoch: 23, batch: 24, loss: 0.6831302046775818, acc: 78.125, f1: 64.54274891774891, r: 0.6342127654722912
06/01/2019 11:30:09 step: 789, epoch: 23, batch: 29, loss: 0.6423823833465576, acc: 82.8125, f1: 70.16341256366722, r: 0.6911637873211085
06/01/2019 11:30:09 *** evaluating ***
06/01/2019 11:30:09 step: 24, epoch: 23, acc: 56.837606837606835, f1: 26.206887887979903, r: 0.34191954556047466
06/01/2019 11:30:09 *** epoch: 25 ***
06/01/2019 11:30:09 *** training ***
06/01/2019 11:30:10 step: 797, epoch: 24, batch: 4, loss: 0.5540967583656311, acc: 81.25, f1: 72.68878432599092, r: 0.6032202896963516
06/01/2019 11:30:10 step: 802, epoch: 24, batch: 9, loss: 0.9701923131942749, acc: 84.375, f1: 75.44094794094794, r: 0.6336529122182964
06/01/2019 11:30:11 step: 807, epoch: 24, batch: 14, loss: 0.7347050905227661, acc: 79.6875, f1: 69.42619615413733, r: 0.6152743170959605
06/01/2019 11:30:12 step: 812, epoch: 24, batch: 19, loss: 1.2817749977111816, acc: 84.375, f1: 86.41723356009071, r: 0.639340258809903
06/01/2019 11:30:12 step: 817, epoch: 24, batch: 24, loss: 1.0666495561599731, acc: 75.0, f1: 71.21836550881714, r: 0.639068270392093
06/01/2019 11:30:13 step: 822, epoch: 24, batch: 29, loss: 0.6167900562286377, acc: 84.375, f1: 77.56003587205633, r: 0.6496772822168103
06/01/2019 11:30:13 *** evaluating ***
06/01/2019 11:30:13 step: 25, epoch: 24, acc: 55.55555555555556, f1: 22.653401545941627, r: 0.3357796284264417
06/01/2019 11:30:13 *** epoch: 26 ***
06/01/2019 11:30:13 *** training ***
06/01/2019 11:30:14 step: 830, epoch: 25, batch: 4, loss: 0.963086724281311, acc: 81.25, f1: 73.90342052313883, r: 0.6020642242713358
06/01/2019 11:30:15 step: 835, epoch: 25, batch: 9, loss: 0.596936821937561, acc: 85.9375, f1: 81.63510783648083, r: 0.6415383034532629
06/01/2019 11:30:15 step: 840, epoch: 25, batch: 14, loss: 0.5056046843528748, acc: 87.5, f1: 77.7361853832442, r: 0.6942979416253915
06/01/2019 11:30:16 step: 845, epoch: 25, batch: 19, loss: 0.5181064605712891, acc: 89.0625, f1: 75.852084423513, r: 0.5919528119323243
06/01/2019 11:30:17 step: 850, epoch: 25, batch: 24, loss: 0.6126766204833984, acc: 78.125, f1: 63.77860235003092, r: 0.6126058959000018
06/01/2019 11:30:17 step: 855, epoch: 25, batch: 29, loss: 0.4522400498390198, acc: 90.625, f1: 83.28007518796993, r: 0.6670216340178081
06/01/2019 11:30:18 *** evaluating ***
06/01/2019 11:30:18 step: 26, epoch: 25, acc: 58.54700854700855, f1: 26.06125914315569, r: 0.3338976756534809
06/01/2019 11:30:18 *** epoch: 27 ***
06/01/2019 11:30:18 *** training ***
06/01/2019 11:30:19 step: 863, epoch: 26, batch: 4, loss: 0.5190624594688416, acc: 85.9375, f1: 64.70127698388568, r: 0.7012778557794435
06/01/2019 11:30:19 step: 868, epoch: 26, batch: 9, loss: 0.5745059251785278, acc: 82.8125, f1: 81.21097156811443, r: 0.6332677751128772
06/01/2019 11:30:20 step: 873, epoch: 26, batch: 14, loss: 0.6482999920845032, acc: 76.5625, f1: 64.23074194921266, r: 0.7410737008469226
06/01/2019 11:30:20 step: 878, epoch: 26, batch: 19, loss: 0.4613862633705139, acc: 90.625, f1: 85.8218636291726, r: 0.6694649880932307
06/01/2019 11:30:21 step: 883, epoch: 26, batch: 24, loss: 0.48304814100265503, acc: 87.5, f1: 69.34855993679524, r: 0.5116991558636578
06/01/2019 11:30:22 step: 888, epoch: 26, batch: 29, loss: 0.7515411376953125, acc: 75.0, f1: 63.24013157894737, r: 0.6115888585239738
06/01/2019 11:30:22 *** evaluating ***
06/01/2019 11:30:22 step: 27, epoch: 26, acc: 55.12820512820513, f1: 28.895037364603272, r: 0.3362766770279186
06/01/2019 11:30:22 *** epoch: 28 ***
06/01/2019 11:30:22 *** training ***
06/01/2019 11:30:23 step: 896, epoch: 27, batch: 4, loss: 0.4615316689014435, acc: 85.9375, f1: 49.735542907455745, r: 0.6556423674357573
06/01/2019 11:30:24 step: 901, epoch: 27, batch: 9, loss: 0.5376533269882202, acc: 93.75, f1: 94.95373907138614, r: 0.74503069042969
06/01/2019 11:30:24 step: 906, epoch: 27, batch: 14, loss: 0.5370771288871765, acc: 81.25, f1: 81.53941083712321, r: 0.6636886846844592
06/01/2019 11:30:25 step: 911, epoch: 27, batch: 19, loss: 0.5346813797950745, acc: 87.5, f1: 70.77457264957265, r: 0.6936567777892484
06/01/2019 11:30:26 step: 916, epoch: 27, batch: 24, loss: 0.5031551718711853, acc: 90.625, f1: 85.10247120029729, r: 0.6773545224203648
06/01/2019 11:30:26 step: 921, epoch: 27, batch: 29, loss: 0.4279072880744934, acc: 89.0625, f1: 88.95073180787466, r: 0.6546954608325103
06/01/2019 11:30:27 *** evaluating ***
06/01/2019 11:30:27 step: 28, epoch: 27, acc: 50.427350427350426, f1: 23.12894183664938, r: 0.3218235333769754
06/01/2019 11:30:27 *** epoch: 29 ***
06/01/2019 11:30:27 *** training ***
06/01/2019 11:30:28 step: 929, epoch: 28, batch: 4, loss: 0.5293662548065186, acc: 89.0625, f1: 73.75240500240501, r: 0.6301539062923105
06/01/2019 11:30:28 step: 934, epoch: 28, batch: 9, loss: 0.4091355502605438, acc: 89.0625, f1: 72.11180124223601, r: 0.6826568246455136
06/01/2019 11:30:29 step: 939, epoch: 28, batch: 14, loss: 0.41208264231681824, acc: 89.0625, f1: 81.9319715372347, r: 0.5826355503265906
06/01/2019 11:30:30 step: 944, epoch: 28, batch: 19, loss: 0.5156024098396301, acc: 84.375, f1: 83.1096681096681, r: 0.6202520822911405
06/01/2019 11:30:30 step: 949, epoch: 28, batch: 24, loss: 0.4486124515533447, acc: 82.8125, f1: 83.62012203673463, r: 0.6480553232486813
06/01/2019 11:30:31 step: 954, epoch: 28, batch: 29, loss: 0.8596344590187073, acc: 90.625, f1: 87.5411056716423, r: 0.6633461743988227
06/01/2019 11:30:31 *** evaluating ***
06/01/2019 11:30:31 step: 29, epoch: 28, acc: 53.84615384615385, f1: 29.673399711377012, r: 0.3216577133264976
06/01/2019 11:30:31 *** epoch: 30 ***
06/01/2019 11:30:31 *** training ***
06/01/2019 11:30:32 step: 962, epoch: 29, batch: 4, loss: 0.4505048096179962, acc: 84.375, f1: 81.97399822399822, r: 0.6963525828610345
06/01/2019 11:30:33 step: 967, epoch: 29, batch: 9, loss: 0.3790133595466614, acc: 95.3125, f1: 96.1069590786572, r: 0.676673648281169
06/01/2019 11:30:33 step: 972, epoch: 29, batch: 14, loss: 0.4427123963832855, acc: 82.8125, f1: 60.30894241874278, r: 0.7356788521399087
06/01/2019 11:30:34 step: 977, epoch: 29, batch: 19, loss: 0.40909552574157715, acc: 82.8125, f1: 75.48430735930735, r: 0.7321616516108288
06/01/2019 11:30:35 step: 982, epoch: 29, batch: 24, loss: 0.38860705494880676, acc: 89.0625, f1: 73.1071216365334, r: 0.6578012340581502
06/01/2019 11:30:35 step: 987, epoch: 29, batch: 29, loss: 0.4329947829246521, acc: 85.9375, f1: 77.85319037598673, r: 0.6573859374262674
06/01/2019 11:30:36 *** evaluating ***
06/01/2019 11:30:36 step: 30, epoch: 29, acc: 55.55555555555556, f1: 25.887979230869142, r: 0.3253336545011872
06/01/2019 11:30:36 *** epoch: 31 ***
06/01/2019 11:30:36 *** training ***
06/01/2019 11:30:37 step: 995, epoch: 30, batch: 4, loss: 0.5354541540145874, acc: 87.5, f1: 75.79281716330745, r: 0.7221617928651128
06/01/2019 11:30:37 step: 1000, epoch: 30, batch: 9, loss: 0.3715360462665558, acc: 92.1875, f1: 92.19447719072626, r: 0.6306651699174255
06/01/2019 11:30:38 step: 1005, epoch: 30, batch: 14, loss: 0.3685138523578644, acc: 92.1875, f1: 89.69015801313937, r: 0.6636148801076371
06/01/2019 11:30:39 step: 1010, epoch: 30, batch: 19, loss: 0.4089311957359314, acc: 90.625, f1: 82.77323744142967, r: 0.6267949504369034
06/01/2019 11:30:39 step: 1015, epoch: 30, batch: 24, loss: 0.4247509837150574, acc: 85.9375, f1: 76.59722222222223, r: 0.7309706459553172
06/01/2019 11:30:40 step: 1020, epoch: 30, batch: 29, loss: 0.35996076464653015, acc: 90.625, f1: 81.45370370370371, r: 0.7284476717977738
06/01/2019 11:30:40 *** evaluating ***
06/01/2019 11:30:40 step: 31, epoch: 30, acc: 56.41025641025641, f1: 29.466344502771918, r: 0.3294218049624902
06/01/2019 11:30:40 *** epoch: 32 ***
06/01/2019 11:30:40 *** training ***
06/01/2019 11:30:41 step: 1028, epoch: 31, batch: 4, loss: 0.7105052471160889, acc: 92.1875, f1: 87.29286586429444, r: 0.6648280712876262
06/01/2019 11:30:42 step: 1033, epoch: 31, batch: 9, loss: 0.42449048161506653, acc: 84.375, f1: 67.00174825174825, r: 0.7683393750781907
06/01/2019 11:30:42 step: 1038, epoch: 31, batch: 14, loss: 0.36807486414909363, acc: 90.625, f1: 77.5472747627479, r: 0.7337106429689728
06/01/2019 11:30:43 step: 1043, epoch: 31, batch: 19, loss: 0.3280107378959656, acc: 95.3125, f1: 97.63427109974425, r: 0.7553884059544541
06/01/2019 11:30:44 step: 1048, epoch: 31, batch: 24, loss: 0.597392201423645, acc: 82.8125, f1: 81.89857761286332, r: 0.6742966653994897
06/01/2019 11:30:44 step: 1053, epoch: 31, batch: 29, loss: 0.3811330795288086, acc: 89.0625, f1: 85.34882431233363, r: 0.7541334920109289
06/01/2019 11:30:45 *** evaluating ***
06/01/2019 11:30:45 step: 32, epoch: 31, acc: 56.41025641025641, f1: 22.03818926729726, r: 0.32575282138889605
06/01/2019 11:30:45 *** epoch: 33 ***
06/01/2019 11:30:45 *** training ***
06/01/2019 11:30:45 step: 1061, epoch: 32, batch: 4, loss: 0.40908282995224, acc: 84.375, f1: 76.54547404547404, r: 0.5761409476539212
06/01/2019 11:30:46 step: 1066, epoch: 32, batch: 9, loss: 0.3956986665725708, acc: 90.625, f1: 90.84012113617376, r: 0.7533247950545229
06/01/2019 11:30:47 step: 1071, epoch: 32, batch: 14, loss: 0.3341839015483856, acc: 95.3125, f1: 91.92268305171531, r: 0.6110778831147105
06/01/2019 11:30:47 step: 1076, epoch: 32, batch: 19, loss: 0.30283787846565247, acc: 92.1875, f1: 79.20547309833023, r: 0.7104796489745606
06/01/2019 11:30:48 step: 1081, epoch: 32, batch: 24, loss: 0.35823166370391846, acc: 92.1875, f1: 91.39409609997846, r: 0.6932263756521448
06/01/2019 11:30:49 step: 1086, epoch: 32, batch: 29, loss: 0.3665076196193695, acc: 93.75, f1: 88.2909023600513, r: 0.7425698526483764
06/01/2019 11:30:49 *** evaluating ***
06/01/2019 11:30:49 step: 33, epoch: 32, acc: 55.12820512820513, f1: 25.156929220371204, r: 0.31934918693490044
06/01/2019 11:30:49 *** epoch: 34 ***
06/01/2019 11:30:49 *** training ***
06/01/2019 11:30:50 step: 1094, epoch: 33, batch: 4, loss: 0.42972174286842346, acc: 89.0625, f1: 89.88605733888753, r: 0.6656631062869771
06/01/2019 11:30:51 step: 1099, epoch: 33, batch: 9, loss: 0.3452073335647583, acc: 90.625, f1: 83.73207970190728, r: 0.7544751667459494
06/01/2019 11:30:51 step: 1104, epoch: 33, batch: 14, loss: 0.2541065812110901, acc: 95.3125, f1: 96.46043611560853, r: 0.6710814598409243
06/01/2019 11:30:52 step: 1109, epoch: 33, batch: 19, loss: 0.425113320350647, acc: 87.5, f1: 64.30991332661591, r: 0.697152147908589
06/01/2019 11:30:53 step: 1114, epoch: 33, batch: 24, loss: 0.25338709354400635, acc: 100.0, f1: 100.0, r: 0.7669950736351794
06/01/2019 11:30:53 step: 1119, epoch: 33, batch: 29, loss: 0.3243899345397949, acc: 90.625, f1: 89.08304391497668, r: 0.7189664297015119
06/01/2019 11:30:54 *** evaluating ***
06/01/2019 11:30:54 step: 34, epoch: 33, acc: 53.84615384615385, f1: 24.6151418026418, r: 0.31436337114341856
06/01/2019 11:30:54 *** epoch: 35 ***
06/01/2019 11:30:54 *** training ***
06/01/2019 11:30:54 step: 1127, epoch: 34, batch: 4, loss: 0.26203447580337524, acc: 96.875, f1: 96.50252525252525, r: 0.751895249926158
06/01/2019 11:30:55 step: 1132, epoch: 34, batch: 9, loss: 0.3556557595729828, acc: 100.0, f1: 100.0, r: 0.7066449950927302
06/01/2019 11:30:56 step: 1137, epoch: 34, batch: 14, loss: 0.2674165964126587, acc: 89.0625, f1: 71.57988309479045, r: 0.5855966698279991
06/01/2019 11:30:56 step: 1142, epoch: 34, batch: 19, loss: 0.3768349587917328, acc: 87.5, f1: 78.63095238095238, r: 0.6844500261240483
06/01/2019 11:30:57 step: 1147, epoch: 34, batch: 24, loss: 0.2976057231426239, acc: 93.75, f1: 89.84577922077924, r: 0.7285262660337922
06/01/2019 11:30:58 step: 1152, epoch: 34, batch: 29, loss: 0.372890830039978, acc: 85.9375, f1: 77.83698327815975, r: 0.7364435213370946
06/01/2019 11:30:58 *** evaluating ***
06/01/2019 11:30:58 step: 35, epoch: 34, acc: 55.55555555555556, f1: 26.237905681182315, r: 0.3170359117446255
06/01/2019 11:30:58 *** epoch: 36 ***
06/01/2019 11:30:58 *** training ***
06/01/2019 11:30:59 step: 1160, epoch: 35, batch: 4, loss: 0.3114372193813324, acc: 95.3125, f1: 94.86021331609568, r: 0.6389563744553262
06/01/2019 11:31:00 step: 1165, epoch: 35, batch: 9, loss: 0.25689226388931274, acc: 95.3125, f1: 96.5836649125059, r: 0.6181255492637717
06/01/2019 11:31:00 step: 1170, epoch: 35, batch: 14, loss: 0.2426043301820755, acc: 95.3125, f1: 95.32679451090782, r: 0.7033654133522875
06/01/2019 11:31:01 step: 1175, epoch: 35, batch: 19, loss: 0.26286986470222473, acc: 98.4375, f1: 98.76750700280112, r: 0.6782233641505351
06/01/2019 11:31:01 step: 1180, epoch: 35, batch: 24, loss: 0.21151289343833923, acc: 96.875, f1: 95.91478696741855, r: 0.6978832010309699
06/01/2019 11:31:02 step: 1185, epoch: 35, batch: 29, loss: 0.26507192850112915, acc: 96.875, f1: 94.25054112554112, r: 0.7997820768958184
06/01/2019 11:31:02 *** evaluating ***
06/01/2019 11:31:03 step: 36, epoch: 35, acc: 47.863247863247864, f1: 24.576919748278375, r: 0.29623737816898454
06/01/2019 11:31:03 *** epoch: 37 ***
06/01/2019 11:31:03 *** training ***
06/01/2019 11:31:03 step: 1193, epoch: 36, batch: 4, loss: 0.3181927800178528, acc: 93.75, f1: 93.73420109413318, r: 0.774743930499681
06/01/2019 11:31:04 step: 1198, epoch: 36, batch: 9, loss: 0.22157034277915955, acc: 95.3125, f1: 94.86853832442068, r: 0.7552130229448393
06/01/2019 11:31:04 step: 1203, epoch: 36, batch: 14, loss: 0.6596035957336426, acc: 92.1875, f1: 68.29107400535972, r: 0.7690379014687848
06/01/2019 11:31:05 step: 1208, epoch: 36, batch: 19, loss: 0.22449582815170288, acc: 96.875, f1: 86.60600255427842, r: 0.7576630466387401
06/01/2019 11:31:06 step: 1213, epoch: 36, batch: 24, loss: 0.33824026584625244, acc: 90.625, f1: 87.66086144602768, r: 0.6506611656332345
06/01/2019 11:31:06 step: 1218, epoch: 36, batch: 29, loss: 0.2892584502696991, acc: 96.875, f1: 85.64433811802232, r: 0.7097911894160989
06/01/2019 11:31:07 *** evaluating ***
06/01/2019 11:31:07 step: 37, epoch: 36, acc: 54.27350427350427, f1: 25.052810234546683, r: 0.31325028896856355
06/01/2019 11:31:07 *** epoch: 38 ***
06/01/2019 11:31:07 *** training ***
06/01/2019 11:31:07 step: 1226, epoch: 37, batch: 4, loss: 0.27334800362586975, acc: 95.3125, f1: 95.68104318542608, r: 0.808674373768109
06/01/2019 11:31:08 step: 1231, epoch: 37, batch: 9, loss: 0.24083273112773895, acc: 100.0, f1: 100.0, r: 0.7868683364724894
06/01/2019 11:31:09 step: 1236, epoch: 37, batch: 14, loss: 0.38089826703071594, acc: 85.9375, f1: 72.24132077073253, r: 0.7047468196698179
06/01/2019 11:31:09 step: 1241, epoch: 37, batch: 19, loss: 0.31371554732322693, acc: 93.75, f1: 93.31472620946305, r: 0.5420195363490662
06/01/2019 11:31:10 step: 1246, epoch: 37, batch: 24, loss: 0.87250816822052, acc: 100.0, f1: 100.0, r: 0.6698958142949663
06/01/2019 11:31:11 step: 1251, epoch: 37, batch: 29, loss: 0.3221646249294281, acc: 90.625, f1: 81.48051522484371, r: 0.7632687624361507
06/01/2019 11:31:11 *** evaluating ***
06/01/2019 11:31:11 step: 38, epoch: 37, acc: 55.55555555555556, f1: 26.979953608058278, r: 0.311864187993482
06/01/2019 11:31:11 *** epoch: 39 ***
06/01/2019 11:31:11 *** training ***
06/01/2019 11:31:12 step: 1259, epoch: 38, batch: 4, loss: 0.6510152816772461, acc: 96.875, f1: 97.90265639322243, r: 0.7123167472477838
06/01/2019 11:31:13 step: 1264, epoch: 38, batch: 9, loss: 0.3171880841255188, acc: 96.875, f1: 96.44847489675077, r: 0.7403711992794132
06/01/2019 11:31:13 step: 1269, epoch: 38, batch: 14, loss: 0.2803055942058563, acc: 93.75, f1: 84.78369569243304, r: 0.7461235293608185
06/01/2019 11:31:14 step: 1274, epoch: 38, batch: 19, loss: 0.279314249753952, acc: 90.625, f1: 74.59789078674947, r: 0.7413048864354654
06/01/2019 11:31:15 step: 1279, epoch: 38, batch: 24, loss: 0.16458120942115784, acc: 100.0, f1: 100.0, r: 0.7657622207064712
06/01/2019 11:31:15 step: 1284, epoch: 38, batch: 29, loss: 0.17596453428268433, acc: 98.4375, f1: 96.66048237476808, r: 0.7326720007468293
06/01/2019 11:31:16 *** evaluating ***
06/01/2019 11:31:16 step: 39, epoch: 38, acc: 57.26495726495726, f1: 25.30529440515209, r: 0.31048602432923933
06/01/2019 11:31:16 *** epoch: 40 ***
06/01/2019 11:31:16 *** training ***
06/01/2019 11:31:16 step: 1292, epoch: 39, batch: 4, loss: 0.2717427909374237, acc: 93.75, f1: 83.89674051438757, r: 0.7287037054943133
06/01/2019 11:31:17 step: 1297, epoch: 39, batch: 9, loss: 0.19158263504505157, acc: 98.4375, f1: 99.03961584633853, r: 0.6924788906137015
06/01/2019 11:31:18 step: 1302, epoch: 39, batch: 14, loss: 0.20554371178150177, acc: 96.875, f1: 95.81323438466295, r: 0.6682712317168218
06/01/2019 11:31:18 step: 1307, epoch: 39, batch: 19, loss: 0.2565755248069763, acc: 98.4375, f1: 99.24489795918367, r: 0.7814306522164668
06/01/2019 11:31:19 step: 1312, epoch: 39, batch: 24, loss: 0.34062859416007996, acc: 89.0625, f1: 88.493265993266, r: 0.723044292579145
06/01/2019 11:31:20 step: 1317, epoch: 39, batch: 29, loss: 0.18042823672294617, acc: 100.0, f1: 100.0, r: 0.6874529424266179
06/01/2019 11:31:20 *** evaluating ***
06/01/2019 11:31:20 step: 40, epoch: 39, acc: 57.26495726495726, f1: 23.12313925603092, r: 0.3024367090773637
06/01/2019 11:31:20 *** epoch: 41 ***
06/01/2019 11:31:20 *** training ***
06/01/2019 11:31:21 step: 1325, epoch: 40, batch: 4, loss: 0.18577128648757935, acc: 98.4375, f1: 97.72727272727273, r: 0.7392010478287218
06/01/2019 11:31:21 step: 1330, epoch: 40, batch: 9, loss: 0.2014089822769165, acc: 95.3125, f1: 81.57792207792207, r: 0.7579006143783669
06/01/2019 11:31:22 step: 1335, epoch: 40, batch: 14, loss: 0.2708244025707245, acc: 95.3125, f1: 90.67660910518055, r: 0.7117578893632623
06/01/2019 11:31:23 step: 1340, epoch: 40, batch: 19, loss: 0.1956721395254135, acc: 96.875, f1: 93.16526610644257, r: 0.6494590461010297
06/01/2019 11:31:23 step: 1345, epoch: 40, batch: 24, loss: 0.1990111619234085, acc: 92.1875, f1: 91.97419495926958, r: 0.5963925838489877
06/01/2019 11:31:24 step: 1350, epoch: 40, batch: 29, loss: 0.3129502534866333, acc: 96.875, f1: 94.5392832185285, r: 0.7492481154983666
06/01/2019 11:31:24 *** evaluating ***
06/01/2019 11:31:24 step: 41, epoch: 40, acc: 50.0, f1: 23.50216988537648, r: 0.29200114639134606
06/01/2019 11:31:24 *** epoch: 42 ***
06/01/2019 11:31:24 *** training ***
06/01/2019 11:31:25 step: 1358, epoch: 41, batch: 4, loss: 0.17760784924030304, acc: 98.4375, f1: 97.61075161772025, r: 0.6898261783268325
06/01/2019 11:31:26 step: 1363, epoch: 41, batch: 9, loss: 0.17922146618366241, acc: 95.3125, f1: 80.187324929972, r: 0.7334600597121731
06/01/2019 11:31:26 step: 1368, epoch: 41, batch: 14, loss: 0.2123199999332428, acc: 98.4375, f1: 99.24489795918367, r: 0.7939288928702215
06/01/2019 11:31:27 step: 1373, epoch: 41, batch: 19, loss: 0.21968574821949005, acc: 96.875, f1: 84.16495784916836, r: 0.7365825659567742
06/01/2019 11:31:27 step: 1378, epoch: 41, batch: 24, loss: 0.3625284433364868, acc: 90.625, f1: 87.84201305940437, r: 0.661994396931638
06/01/2019 11:31:28 step: 1383, epoch: 41, batch: 29, loss: 0.1946064978837967, acc: 93.75, f1: 81.92881192881192, r: 0.6574456416138356
06/01/2019 11:31:28 *** evaluating ***
06/01/2019 11:31:29 step: 42, epoch: 41, acc: 54.700854700854705, f1: 23.136422291825227, r: 0.2959206373553821
06/01/2019 11:31:29 *** epoch: 43 ***
06/01/2019 11:31:29 *** training ***
06/01/2019 11:31:29 step: 1391, epoch: 42, batch: 4, loss: 0.2177904099225998, acc: 96.875, f1: 97.6622633659247, r: 0.7618181260908908
06/01/2019 11:31:30 step: 1396, epoch: 42, batch: 9, loss: 0.15665671229362488, acc: 98.4375, f1: 83.06878306878306, r: 0.5405402207902624
06/01/2019 11:31:31 step: 1401, epoch: 42, batch: 14, loss: 0.20618994534015656, acc: 100.0, f1: 100.0, r: 0.6492277303411697
06/01/2019 11:31:31 step: 1406, epoch: 42, batch: 19, loss: 0.6342310905456543, acc: 95.3125, f1: 91.36819321029847, r: 0.8507985501090212
06/01/2019 11:31:32 step: 1411, epoch: 42, batch: 24, loss: 0.21416692435741425, acc: 98.4375, f1: 95.56737588652481, r: 0.8238850328973487
06/01/2019 11:31:33 step: 1416, epoch: 42, batch: 29, loss: 0.21622295677661896, acc: 95.3125, f1: 97.420795076178, r: 0.7677065830755578
06/01/2019 11:31:33 *** evaluating ***
06/01/2019 11:31:33 step: 43, epoch: 42, acc: 54.700854700854705, f1: 23.05118946474087, r: 0.30676764885890223
06/01/2019 11:31:33 *** epoch: 44 ***
06/01/2019 11:31:33 *** training ***
06/01/2019 11:31:34 step: 1424, epoch: 43, batch: 4, loss: 0.2401185780763626, acc: 96.875, f1: 95.49886621315193, r: 0.7440566468602835
06/01/2019 11:31:34 step: 1429, epoch: 43, batch: 9, loss: 0.8960945010185242, acc: 96.875, f1: 95.8717610891524, r: 0.6903714650496765
06/01/2019 11:31:35 step: 1434, epoch: 43, batch: 14, loss: 0.22119173407554626, acc: 95.3125, f1: 86.81222159483029, r: 0.6849631872458435
06/01/2019 11:31:35 step: 1439, epoch: 43, batch: 19, loss: 0.1393425166606903, acc: 96.875, f1: 92.65504610332196, r: 0.7868735362977779
06/01/2019 11:31:36 step: 1444, epoch: 43, batch: 24, loss: 0.16283823549747467, acc: 98.4375, f1: 98.66666666666667, r: 0.8177057496537823
06/01/2019 11:31:37 step: 1449, epoch: 43, batch: 29, loss: 0.16992312669754028, acc: 96.875, f1: 96.39423076923077, r: 0.7907538874106368
06/01/2019 11:31:37 *** evaluating ***
06/01/2019 11:31:37 step: 44, epoch: 43, acc: 55.98290598290598, f1: 24.528219099739946, r: 0.3028826858794219
06/01/2019 11:31:37 *** epoch: 45 ***
06/01/2019 11:31:37 *** training ***
06/01/2019 11:31:38 step: 1457, epoch: 44, batch: 4, loss: 0.19452577829360962, acc: 96.875, f1: 85.41760383865648, r: 0.7647144900158845
06/01/2019 11:31:38 step: 1462, epoch: 44, batch: 9, loss: 0.15038475394248962, acc: 96.875, f1: 85.05772005772005, r: 0.7496614588668141
06/01/2019 11:31:39 step: 1467, epoch: 44, batch: 14, loss: 0.18400481343269348, acc: 96.875, f1: 92.9466475553432, r: 0.666858245502531
06/01/2019 11:31:40 step: 1472, epoch: 44, batch: 19, loss: 0.14154480397701263, acc: 100.0, f1: 100.0, r: 0.7084740316233062
06/01/2019 11:31:41 step: 1477, epoch: 44, batch: 24, loss: 0.16658741235733032, acc: 96.875, f1: 97.63725929243171, r: 0.6873986389238856
06/01/2019 11:31:41 step: 1482, epoch: 44, batch: 29, loss: 0.6804078817367554, acc: 98.4375, f1: 97.71428571428571, r: 0.8126536539479272
06/01/2019 11:31:41 *** evaluating ***
06/01/2019 11:31:42 step: 45, epoch: 44, acc: 55.98290598290598, f1: 25.930226214856194, r: 0.29380181914995446
06/01/2019 11:31:42 *** epoch: 46 ***
06/01/2019 11:31:42 *** training ***
06/01/2019 11:31:42 step: 1490, epoch: 45, batch: 4, loss: 0.21514646708965302, acc: 93.75, f1: 75.91216216216216, r: 0.7144703359637203
06/01/2019 11:31:43 step: 1495, epoch: 45, batch: 9, loss: 0.11332935839891434, acc: 98.4375, f1: 99.3552546744036, r: 0.8016142003748316
06/01/2019 11:31:44 step: 1500, epoch: 45, batch: 14, loss: 0.6371225118637085, acc: 93.75, f1: 85.48546691403834, r: 0.6687346420039061
06/01/2019 11:31:44 step: 1505, epoch: 45, batch: 19, loss: 0.21611396968364716, acc: 96.875, f1: 94.67948717948718, r: 0.786567316360098
06/01/2019 11:31:45 step: 1510, epoch: 45, batch: 24, loss: 0.17403441667556763, acc: 96.875, f1: 95.71327239932715, r: 0.6765423320673165
06/01/2019 11:31:45 step: 1515, epoch: 45, batch: 29, loss: 0.11365357041358948, acc: 100.0, f1: 100.0, r: 0.6999673803355947
06/01/2019 11:31:46 *** evaluating ***
06/01/2019 11:31:46 step: 46, epoch: 45, acc: 55.98290598290598, f1: 27.252930706878075, r: 0.3033077732907115
06/01/2019 11:31:46 *** epoch: 47 ***
06/01/2019 11:31:46 *** training ***
06/01/2019 11:31:47 step: 1523, epoch: 46, batch: 4, loss: 0.1774255335330963, acc: 100.0, f1: 100.0, r: 0.7071367943225202
06/01/2019 11:31:47 step: 1528, epoch: 46, batch: 9, loss: 0.13467422127723694, acc: 98.4375, f1: 94.44444444444444, r: 0.8035513444333342
06/01/2019 11:31:48 step: 1533, epoch: 46, batch: 14, loss: 0.1901920735836029, acc: 96.875, f1: 96.17382617382619, r: 0.6396526863955198
06/01/2019 11:31:49 step: 1538, epoch: 46, batch: 19, loss: 0.12880587577819824, acc: 98.4375, f1: 95.33333333333334, r: 0.6934652237381725
06/01/2019 11:31:49 step: 1543, epoch: 46, batch: 24, loss: 1.248250961303711, acc: 96.875, f1: 97.81915139558899, r: 0.7202159821121727
06/01/2019 11:31:50 step: 1548, epoch: 46, batch: 29, loss: 0.16918347775936127, acc: 96.875, f1: 97.5550766283525, r: 0.7656492636605485
06/01/2019 11:31:50 *** evaluating ***
06/01/2019 11:31:50 step: 47, epoch: 46, acc: 53.41880341880342, f1: 26.484795950900686, r: 0.29195036575822914
06/01/2019 11:31:50 *** epoch: 48 ***
06/01/2019 11:31:50 *** training ***
06/01/2019 11:31:51 step: 1556, epoch: 47, batch: 4, loss: 0.21273669600486755, acc: 96.875, f1: 97.98026048026047, r: 0.7901332674392857
06/01/2019 11:31:52 step: 1561, epoch: 47, batch: 9, loss: 0.15587688982486725, acc: 98.4375, f1: 97.67907162865146, r: 0.7293647506216714
06/01/2019 11:31:52 step: 1566, epoch: 47, batch: 14, loss: 0.13396362960338593, acc: 98.4375, f1: 94.13919413919415, r: 0.6097976310106079
06/01/2019 11:31:53 step: 1571, epoch: 47, batch: 19, loss: 0.11002916842699051, acc: 98.4375, f1: 97.61075161772025, r: 0.7210140483580754
06/01/2019 11:31:53 step: 1576, epoch: 47, batch: 24, loss: 0.13393324613571167, acc: 98.4375, f1: 96.42857142857143, r: 0.8160494506644561
06/01/2019 11:31:54 step: 1581, epoch: 47, batch: 29, loss: 0.14682874083518982, acc: 98.4375, f1: 99.13702623906705, r: 0.7705425601257188
06/01/2019 11:31:54 *** evaluating ***
06/01/2019 11:31:55 step: 48, epoch: 47, acc: 57.26495726495726, f1: 24.285230352303525, r: 0.28712401962176476
06/01/2019 11:31:55 *** epoch: 49 ***
06/01/2019 11:31:55 *** training ***
06/01/2019 11:31:55 step: 1589, epoch: 48, batch: 4, loss: 0.1979863941669464, acc: 98.4375, f1: 98.61853832442068, r: 0.7560521483755916
06/01/2019 11:31:56 step: 1594, epoch: 48, batch: 9, loss: 0.16281065344810486, acc: 96.875, f1: 94.26929392446634, r: 0.6785816261936465
06/01/2019 11:31:57 step: 1599, epoch: 48, batch: 14, loss: 0.15497295558452606, acc: 100.0, f1: 100.0, r: 0.8292258937681888
06/01/2019 11:31:57 step: 1604, epoch: 48, batch: 19, loss: 0.5724063515663147, acc: 95.3125, f1: 94.15432520695678, r: 0.7187538318852557
06/01/2019 11:31:58 step: 1609, epoch: 48, batch: 24, loss: 0.12965887784957886, acc: 98.4375, f1: 99.04665607395637, r: 0.7503651514883485
06/01/2019 11:31:58 step: 1614, epoch: 48, batch: 29, loss: 0.1386217474937439, acc: 100.0, f1: 100.0, r: 0.715087139043751
06/01/2019 11:31:59 *** evaluating ***
06/01/2019 11:31:59 step: 49, epoch: 48, acc: 57.26495726495726, f1: 27.469754477374497, r: 0.29869879419129863
06/01/2019 11:31:59 *** epoch: 50 ***
06/01/2019 11:31:59 *** training ***
06/01/2019 11:32:00 step: 1622, epoch: 49, batch: 4, loss: 0.13020533323287964, acc: 100.0, f1: 100.0, r: 0.7677820985280428
06/01/2019 11:32:00 step: 1627, epoch: 49, batch: 9, loss: 0.847165584564209, acc: 98.4375, f1: 98.57142857142858, r: 0.8257926771265657
06/01/2019 11:32:01 step: 1632, epoch: 49, batch: 14, loss: 0.12992142140865326, acc: 100.0, f1: 100.0, r: 0.759056745007104
06/01/2019 11:32:02 step: 1637, epoch: 49, batch: 19, loss: 0.4664369225502014, acc: 100.0, f1: 100.0, r: 0.6762925936293666
06/01/2019 11:32:02 step: 1642, epoch: 49, batch: 24, loss: 0.17260009050369263, acc: 98.4375, f1: 99.30976430976432, r: 0.8011176104420548
06/01/2019 11:32:03 step: 1647, epoch: 49, batch: 29, loss: 0.1192924901843071, acc: 100.0, f1: 100.0, r: 0.7183792887061266
06/01/2019 11:32:03 *** evaluating ***
06/01/2019 11:32:03 step: 50, epoch: 49, acc: 55.55555555555556, f1: 23.80811302075726, r: 0.29626109686785235
06/01/2019 11:32:03 *** epoch: 51 ***
06/01/2019 11:32:03 *** training ***
06/01/2019 11:32:04 step: 1655, epoch: 50, batch: 4, loss: 0.14227426052093506, acc: 100.0, f1: 100.0, r: 0.7050112256387454
06/01/2019 11:32:04 step: 1660, epoch: 50, batch: 9, loss: 0.15410803258419037, acc: 98.4375, f1: 99.20135769192372, r: 0.766045013453323
06/01/2019 11:32:05 step: 1665, epoch: 50, batch: 14, loss: 0.48855042457580566, acc: 98.4375, f1: 99.25111925111925, r: 0.6602069728351768
06/01/2019 11:32:06 step: 1670, epoch: 50, batch: 19, loss: 0.14295819401741028, acc: 96.875, f1: 97.26572447160684, r: 0.6534368108257881
06/01/2019 11:32:07 step: 1675, epoch: 50, batch: 24, loss: 0.12986452877521515, acc: 98.4375, f1: 98.27998088867655, r: 0.7254889968449663
06/01/2019 11:32:07 step: 1680, epoch: 50, batch: 29, loss: 0.7839873433113098, acc: 98.4375, f1: 93.33333333333333, r: 0.734479578825071
06/01/2019 11:32:07 *** evaluating ***
06/01/2019 11:32:08 step: 51, epoch: 50, acc: 55.98290598290598, f1: 24.31585775233841, r: 0.2887778551938629
06/01/2019 11:32:08 *** epoch: 52 ***
06/01/2019 11:32:08 *** training ***
06/01/2019 11:32:08 step: 1688, epoch: 51, batch: 4, loss: 0.8719421029090881, acc: 98.4375, f1: 99.33051444679352, r: 0.7371085761345202
06/01/2019 11:32:09 step: 1693, epoch: 51, batch: 9, loss: 0.10741238296031952, acc: 98.4375, f1: 97.27272727272727, r: 0.795376353730645
06/01/2019 11:32:09 step: 1698, epoch: 51, batch: 14, loss: 0.1190863847732544, acc: 98.4375, f1: 98.24046920821115, r: 0.663975902735906
06/01/2019 11:32:10 step: 1703, epoch: 51, batch: 19, loss: 0.1292232871055603, acc: 98.4375, f1: 99.20135769192372, r: 0.6944219444769022
06/01/2019 11:32:11 step: 1708, epoch: 51, batch: 24, loss: 0.5357670187950134, acc: 100.0, f1: 100.0, r: 0.7171631786761338
06/01/2019 11:32:11 step: 1713, epoch: 51, batch: 29, loss: 0.13093474507331848, acc: 96.875, f1: 95.53571428571429, r: 0.7207996953017755
06/01/2019 11:32:12 *** evaluating ***
06/01/2019 11:32:12 step: 52, epoch: 51, acc: 50.0, f1: 22.338470381853163, r: 0.274432792205118
06/01/2019 11:32:12 *** epoch: 53 ***
06/01/2019 11:32:12 *** training ***
06/01/2019 11:32:13 step: 1721, epoch: 52, batch: 4, loss: 0.12069627642631531, acc: 98.4375, f1: 98.85730211817167, r: 0.7999327635623401
06/01/2019 11:32:13 step: 1726, epoch: 52, batch: 9, loss: 0.09648795425891876, acc: 100.0, f1: 100.0, r: 0.6706370083283034
06/01/2019 11:32:14 step: 1731, epoch: 52, batch: 14, loss: 0.09371428936719894, acc: 100.0, f1: 100.0, r: 0.7996546601637573
06/01/2019 11:32:15 step: 1736, epoch: 52, batch: 19, loss: 0.15723641216754913, acc: 96.875, f1: 95.38724580741389, r: 0.663792395255184
06/01/2019 11:32:15 step: 1741, epoch: 52, batch: 24, loss: 0.1374550312757492, acc: 98.4375, f1: 98.66522366522366, r: 0.7769686457690765
06/01/2019 11:32:16 step: 1746, epoch: 52, batch: 29, loss: 0.09004516899585724, acc: 100.0, f1: 100.0, r: 0.6281590898465291
06/01/2019 11:32:16 *** evaluating ***
06/01/2019 11:32:16 step: 53, epoch: 52, acc: 52.991452991452995, f1: 24.974951979814406, r: 0.29336662723413187
06/01/2019 11:32:16 *** epoch: 54 ***
06/01/2019 11:32:16 *** training ***
06/01/2019 11:32:17 step: 1754, epoch: 53, batch: 4, loss: 0.08982037752866745, acc: 98.4375, f1: 99.20135769192372, r: 0.7427135996854334
06/01/2019 11:32:17 step: 1759, epoch: 53, batch: 9, loss: 0.1271311640739441, acc: 100.0, f1: 100.0, r: 0.7051861925793185
06/01/2019 11:32:18 step: 1764, epoch: 53, batch: 14, loss: 0.1038576066493988, acc: 98.4375, f1: 99.15893630179345, r: 0.746639724442832
06/01/2019 11:32:19 step: 1769, epoch: 53, batch: 19, loss: 0.08809472620487213, acc: 100.0, f1: 100.0, r: 0.6997890295071311
06/01/2019 11:32:20 step: 1774, epoch: 53, batch: 24, loss: 0.1708054095506668, acc: 98.4375, f1: 98.2078853046595, r: 0.7316591943598327
06/01/2019 11:32:20 step: 1779, epoch: 53, batch: 29, loss: 0.11843879520893097, acc: 98.4375, f1: 97.26415094339622, r: 0.7740537524234639
06/01/2019 11:32:20 *** evaluating ***
06/01/2019 11:32:21 step: 54, epoch: 53, acc: 56.41025641025641, f1: 23.663386300710243, r: 0.2920423142960495
06/01/2019 11:32:21 *** epoch: 55 ***
06/01/2019 11:32:21 *** training ***
06/01/2019 11:32:21 step: 1787, epoch: 54, batch: 4, loss: 0.06559716165065765, acc: 100.0, f1: 100.0, r: 0.6900409338728752
06/01/2019 11:32:22 step: 1792, epoch: 54, batch: 9, loss: 0.08593643456697464, acc: 98.4375, f1: 99.05329593267882, r: 0.8311539855475655
06/01/2019 11:32:23 step: 1797, epoch: 54, batch: 14, loss: 0.13657307624816895, acc: 98.4375, f1: 99.02818270165209, r: 0.7057937755987808
06/01/2019 11:32:23 step: 1802, epoch: 54, batch: 19, loss: 0.13953529298305511, acc: 98.4375, f1: 94.90586932447397, r: 0.6678315562414381
06/01/2019 11:32:24 step: 1807, epoch: 54, batch: 24, loss: 0.10179038345813751, acc: 100.0, f1: 100.0, r: 0.8320741412036933
06/01/2019 11:32:25 step: 1812, epoch: 54, batch: 29, loss: 0.13251353800296783, acc: 96.875, f1: 94.4368858654573, r: 0.7195162914620263
06/01/2019 11:32:25 *** evaluating ***
06/01/2019 11:32:25 step: 55, epoch: 54, acc: 57.26495726495726, f1: 27.253056668845954, r: 0.3006444850699003
06/01/2019 11:32:25 *** epoch: 56 ***
06/01/2019 11:32:25 *** training ***
06/01/2019 11:32:26 step: 1820, epoch: 55, batch: 4, loss: 0.10596325993537903, acc: 100.0, f1: 100.0, r: 0.7143853902479199
06/01/2019 11:32:26 step: 1825, epoch: 55, batch: 9, loss: 0.13266319036483765, acc: 100.0, f1: 100.0, r: 0.7528021642022042
06/01/2019 11:32:27 step: 1830, epoch: 55, batch: 14, loss: 0.5290829539299011, acc: 98.4375, f1: 94.87179487179486, r: 0.6373923836580653
06/01/2019 11:32:27 step: 1835, epoch: 55, batch: 19, loss: 0.0882732942700386, acc: 98.4375, f1: 98.40619307832424, r: 0.7698856098503358
06/01/2019 11:32:28 step: 1840, epoch: 55, batch: 24, loss: 0.11155127733945847, acc: 98.4375, f1: 99.26525841195625, r: 0.7285732957306251
06/01/2019 11:32:29 step: 1845, epoch: 55, batch: 29, loss: 0.12134721130132675, acc: 96.875, f1: 97.38775510204081, r: 0.6542476116928975
06/01/2019 11:32:29 *** evaluating ***
06/01/2019 11:32:29 step: 56, epoch: 55, acc: 56.41025641025641, f1: 23.744085417647863, r: 0.29051622482698675
06/01/2019 11:32:29 *** epoch: 57 ***
06/01/2019 11:32:29 *** training ***
06/01/2019 11:32:30 step: 1853, epoch: 56, batch: 4, loss: 0.08585365861654282, acc: 98.4375, f1: 98.98838004101161, r: 0.6968682384124
06/01/2019 11:32:31 step: 1858, epoch: 56, batch: 9, loss: 0.09244758635759354, acc: 96.875, f1: 95.48257960682464, r: 0.6074683200888783
06/01/2019 11:32:31 step: 1863, epoch: 56, batch: 14, loss: 0.10267901420593262, acc: 100.0, f1: 100.0, r: 0.7273701344726685
06/01/2019 11:32:32 step: 1868, epoch: 56, batch: 19, loss: 0.10964950919151306, acc: 100.0, f1: 100.0, r: 0.7175349302912619
06/01/2019 11:32:33 step: 1873, epoch: 56, batch: 24, loss: 0.0666450709104538, acc: 100.0, f1: 100.0, r: 0.699782673221621
06/01/2019 11:32:33 step: 1878, epoch: 56, batch: 29, loss: 0.0887211486697197, acc: 100.0, f1: 100.0, r: 0.7113644147083314
06/01/2019 11:32:34 *** evaluating ***
06/01/2019 11:32:34 step: 57, epoch: 56, acc: 54.27350427350427, f1: 26.268836487664228, r: 0.29479294828308183
06/01/2019 11:32:34 *** epoch: 58 ***
06/01/2019 11:32:34 *** training ***
06/01/2019 11:32:34 step: 1886, epoch: 57, batch: 4, loss: 0.16845230758190155, acc: 96.875, f1: 97.42755581764871, r: 0.7094748640618301
06/01/2019 11:32:35 step: 1891, epoch: 57, batch: 9, loss: 0.11337356269359589, acc: 96.875, f1: 95.45239911372636, r: 0.7066632861185617
06/01/2019 11:32:36 step: 1896, epoch: 57, batch: 14, loss: 0.09615802019834518, acc: 98.4375, f1: 93.33333333333333, r: 0.7261299766376745
06/01/2019 11:32:36 step: 1901, epoch: 57, batch: 19, loss: 0.08742833137512207, acc: 100.0, f1: 100.0, r: 0.7105519195369756
06/01/2019 11:32:37 step: 1906, epoch: 57, batch: 24, loss: 0.05115237087011337, acc: 100.0, f1: 100.0, r: 0.5734318505345123
06/01/2019 11:32:37 step: 1911, epoch: 57, batch: 29, loss: 0.08838338404893875, acc: 100.0, f1: 100.0, r: 0.7576319830919466
06/01/2019 11:32:38 *** evaluating ***
06/01/2019 11:32:38 step: 58, epoch: 57, acc: 55.55555555555556, f1: 26.486780889004123, r: 0.2894094650725391
06/01/2019 11:32:38 *** epoch: 59 ***
06/01/2019 11:32:38 *** training ***
06/01/2019 11:32:39 step: 1919, epoch: 58, batch: 4, loss: 0.11387372761964798, acc: 96.875, f1: 82.953216374269, r: 0.791784338931016
06/01/2019 11:32:39 step: 1924, epoch: 58, batch: 9, loss: 0.08468835800886154, acc: 100.0, f1: 100.0, r: 0.6358647772050278
06/01/2019 11:32:40 step: 1929, epoch: 58, batch: 14, loss: 0.422261118888855, acc: 100.0, f1: 100.0, r: 0.7103753162650807
06/01/2019 11:32:40 step: 1934, epoch: 58, batch: 19, loss: 0.11162001639604568, acc: 98.4375, f1: 97.97843665768194, r: 0.8102232024671292
06/01/2019 11:32:41 step: 1939, epoch: 58, batch: 24, loss: 0.1296355277299881, acc: 96.875, f1: 91.25, r: 0.7563780092179329
06/01/2019 11:32:42 step: 1944, epoch: 58, batch: 29, loss: 0.07604369521141052, acc: 98.4375, f1: 99.28193499622071, r: 0.7512346868478673
06/01/2019 11:32:42 *** evaluating ***
06/01/2019 11:32:42 step: 59, epoch: 58, acc: 54.700854700854705, f1: 25.304454317612212, r: 0.29063326213235047
06/01/2019 11:32:42 *** epoch: 60 ***
06/01/2019 11:32:42 *** training ***
06/01/2019 11:32:43 step: 1952, epoch: 59, batch: 4, loss: 0.11478088051080704, acc: 100.0, f1: 100.0, r: 0.7060239661548905
06/01/2019 11:32:44 step: 1957, epoch: 59, batch: 9, loss: 0.05857895687222481, acc: 100.0, f1: 100.0, r: 0.735601993925508
06/01/2019 11:32:44 step: 1962, epoch: 59, batch: 14, loss: 0.07263398170471191, acc: 100.0, f1: 100.0, r: 0.7107574521214926
06/01/2019 11:32:45 step: 1967, epoch: 59, batch: 19, loss: 0.10116016864776611, acc: 100.0, f1: 100.0, r: 0.7838609518520974
06/01/2019 11:32:45 step: 1972, epoch: 59, batch: 24, loss: 0.1313261240720749, acc: 98.4375, f1: 98.2905982905983, r: 0.7583732242268743
06/01/2019 11:32:46 step: 1977, epoch: 59, batch: 29, loss: 0.06207854673266411, acc: 98.4375, f1: 99.2158439730572, r: 0.6795865350621682
06/01/2019 11:32:47 *** evaluating ***
06/01/2019 11:32:47 step: 60, epoch: 59, acc: 55.55555555555556, f1: 24.079598050085355, r: 0.2848110971568206
06/01/2019 11:32:47 *** epoch: 61 ***
06/01/2019 11:32:47 *** training ***
06/01/2019 11:32:47 step: 1985, epoch: 60, batch: 4, loss: 0.07016149163246155, acc: 100.0, f1: 100.0, r: 0.7077972012808268
06/01/2019 11:32:48 step: 1990, epoch: 60, batch: 9, loss: 0.08368442952632904, acc: 100.0, f1: 100.0, r: 0.7213097938893277
06/01/2019 11:32:49 step: 1995, epoch: 60, batch: 14, loss: 0.0663800835609436, acc: 98.4375, f1: 99.24759924759925, r: 0.7322506358720698
06/01/2019 11:32:49 step: 2000, epoch: 60, batch: 19, loss: 0.09549891203641891, acc: 100.0, f1: 100.0, r: 0.8095897886411765
06/01/2019 11:32:50 step: 2005, epoch: 60, batch: 24, loss: 0.1088985949754715, acc: 100.0, f1: 100.0, r: 0.7677559948270687
06/01/2019 11:32:51 step: 2010, epoch: 60, batch: 29, loss: 0.5038488507270813, acc: 98.4375, f1: 96.9047619047619, r: 0.8331127443766827
06/01/2019 11:32:51 *** evaluating ***
06/01/2019 11:32:51 step: 61, epoch: 60, acc: 54.27350427350427, f1: 24.01047657716675, r: 0.2872574671402833
06/01/2019 11:32:51 *** epoch: 62 ***
06/01/2019 11:32:51 *** training ***
06/01/2019 11:32:52 step: 2018, epoch: 61, batch: 4, loss: 0.07153518497943878, acc: 100.0, f1: 100.0, r: 0.81123170527022
06/01/2019 11:32:52 step: 2023, epoch: 61, batch: 9, loss: 0.08434394001960754, acc: 98.4375, f1: 98.43175692232296, r: 0.6917476388915826
06/01/2019 11:32:53 step: 2028, epoch: 61, batch: 14, loss: 0.1363222301006317, acc: 98.4375, f1: 97.57236227824464, r: 0.6829224895319326
06/01/2019 11:32:54 step: 2033, epoch: 61, batch: 19, loss: 0.100826196372509, acc: 100.0, f1: 100.0, r: 0.7648257106501735
06/01/2019 11:32:54 step: 2038, epoch: 61, batch: 24, loss: 0.09793436527252197, acc: 98.4375, f1: 98.4077841662981, r: 0.7185858781459312
06/01/2019 11:32:55 step: 2043, epoch: 61, batch: 29, loss: 0.07350722700357437, acc: 100.0, f1: 100.0, r: 0.8279516074146156
06/01/2019 11:32:55 *** evaluating ***
06/01/2019 11:32:55 step: 62, epoch: 61, acc: 55.98290598290598, f1: 24.88975515488531, r: 0.2862232758164707
06/01/2019 11:32:55 *** epoch: 63 ***
06/01/2019 11:32:55 *** training ***
06/01/2019 11:32:56 step: 2051, epoch: 62, batch: 4, loss: 0.10208576172590256, acc: 96.875, f1: 96.84188827045969, r: 0.6456730291223854
06/01/2019 11:32:57 step: 2056, epoch: 62, batch: 9, loss: 0.09417739510536194, acc: 100.0, f1: 100.0, r: 0.7177587398869045
06/01/2019 11:32:57 step: 2061, epoch: 62, batch: 14, loss: 0.07559213787317276, acc: 98.4375, f1: 97.75132275132276, r: 0.7499542924230752
06/01/2019 11:32:58 step: 2066, epoch: 62, batch: 19, loss: 0.04598351940512657, acc: 100.0, f1: 100.0, r: 0.6218405279762385
06/01/2019 11:32:58 step: 2071, epoch: 62, batch: 24, loss: 0.06828779727220535, acc: 100.0, f1: 100.0, r: 0.7736106582064923
06/01/2019 11:32:59 step: 2076, epoch: 62, batch: 29, loss: 0.07865703105926514, acc: 98.4375, f1: 96.84210526315789, r: 0.7534589720680028
06/01/2019 11:32:59 *** evaluating ***
06/01/2019 11:33:00 step: 63, epoch: 62, acc: 54.700854700854705, f1: 23.74451901191592, r: 0.27359753611873616
06/01/2019 11:33:00 *** epoch: 64 ***
06/01/2019 11:33:00 *** training ***
06/01/2019 11:33:00 step: 2084, epoch: 63, batch: 4, loss: 0.07428732514381409, acc: 100.0, f1: 100.0, r: 0.8114989051602016
06/01/2019 11:33:01 step: 2089, epoch: 63, batch: 9, loss: 0.0755167007446289, acc: 98.4375, f1: 98.26839826839827, r: 0.7254875667244115
06/01/2019 11:33:02 step: 2094, epoch: 63, batch: 14, loss: 0.07379847019910812, acc: 100.0, f1: 100.0, r: 0.770672771033668
06/01/2019 11:33:02 step: 2099, epoch: 63, batch: 19, loss: 0.06427177041769028, acc: 100.0, f1: 100.0, r: 0.6994867675048462
06/01/2019 11:33:03 step: 2104, epoch: 63, batch: 24, loss: 0.07450734823942184, acc: 100.0, f1: 100.0, r: 0.7517612350077572
06/01/2019 11:33:03 step: 2109, epoch: 63, batch: 29, loss: 0.0885612741112709, acc: 100.0, f1: 100.0, r: 0.7221122254144424
06/01/2019 11:33:04 *** evaluating ***
06/01/2019 11:33:04 step: 64, epoch: 63, acc: 55.12820512820513, f1: 24.172669824265565, r: 0.2816976896255408
06/01/2019 11:33:04 *** epoch: 65 ***
06/01/2019 11:33:04 *** training ***
06/01/2019 11:33:05 step: 2117, epoch: 64, batch: 4, loss: 0.0628325343132019, acc: 100.0, f1: 100.0, r: 0.7396705843419911
06/01/2019 11:33:05 step: 2122, epoch: 64, batch: 9, loss: 0.06800348311662674, acc: 100.0, f1: 100.0, r: 0.7855362592643587
06/01/2019 11:33:06 step: 2127, epoch: 64, batch: 14, loss: 0.07104326784610748, acc: 100.0, f1: 100.0, r: 0.7501768695143007
06/01/2019 11:33:06 step: 2132, epoch: 64, batch: 19, loss: 0.07220173627138138, acc: 100.0, f1: 100.0, r: 0.7911653122035512
06/01/2019 11:33:07 step: 2137, epoch: 64, batch: 24, loss: 0.7719838619232178, acc: 96.875, f1: 95.00622471210707, r: 0.7493545376315759
06/01/2019 11:33:08 step: 2142, epoch: 64, batch: 29, loss: 0.04897603020071983, acc: 100.0, f1: 100.0, r: 0.727638965189833
06/01/2019 11:33:08 *** evaluating ***
06/01/2019 11:33:08 step: 65, epoch: 64, acc: 55.55555555555556, f1: 23.30937636593551, r: 0.2641880337462656
06/01/2019 11:33:08 *** epoch: 66 ***
06/01/2019 11:33:08 *** training ***
06/01/2019 11:33:09 step: 2150, epoch: 65, batch: 4, loss: 0.06971339136362076, acc: 100.0, f1: 100.0, r: 0.7497803796068597
06/01/2019 11:33:10 step: 2155, epoch: 65, batch: 9, loss: 0.4606836438179016, acc: 100.0, f1: 100.0, r: 0.8314934383350518
06/01/2019 11:33:10 step: 2160, epoch: 65, batch: 14, loss: 0.10507449507713318, acc: 96.875, f1: 95.69991789819376, r: 0.8286494058871704
06/01/2019 11:33:11 step: 2165, epoch: 65, batch: 19, loss: 0.10067711770534515, acc: 100.0, f1: 100.0, r: 0.6751573202760821
06/01/2019 11:33:12 step: 2170, epoch: 65, batch: 24, loss: 0.06342950463294983, acc: 100.0, f1: 100.0, r: 0.8067853176989223
06/01/2019 11:33:12 step: 2175, epoch: 65, batch: 29, loss: 0.07553321123123169, acc: 100.0, f1: 100.0, r: 0.8439179012385873
06/01/2019 11:33:13 *** evaluating ***
06/01/2019 11:33:13 step: 66, epoch: 65, acc: 55.12820512820513, f1: 24.31092744667983, r: 0.27639681678452965
06/01/2019 11:33:13 *** epoch: 67 ***
06/01/2019 11:33:13 *** training ***
06/01/2019 11:33:13 step: 2183, epoch: 66, batch: 4, loss: 0.07357367873191833, acc: 98.4375, f1: 98.51851851851852, r: 0.7542829673956146
06/01/2019 11:33:14 step: 2188, epoch: 66, batch: 9, loss: 0.05621366202831268, acc: 100.0, f1: 100.0, r: 0.7835072100447864
06/01/2019 11:33:15 step: 2193, epoch: 66, batch: 14, loss: 0.10457754135131836, acc: 100.0, f1: 100.0, r: 0.827709497103962
06/01/2019 11:33:15 step: 2198, epoch: 66, batch: 19, loss: 0.05437389388680458, acc: 100.0, f1: 100.0, r: 0.7320798767196077
06/01/2019 11:33:16 step: 2203, epoch: 66, batch: 24, loss: 0.05829349905252457, acc: 100.0, f1: 100.0, r: 0.6041649430150906
06/01/2019 11:33:17 step: 2208, epoch: 66, batch: 29, loss: 0.06495019793510437, acc: 100.0, f1: 100.0, r: 0.6226385266007053
06/01/2019 11:33:17 *** evaluating ***
06/01/2019 11:33:17 step: 67, epoch: 66, acc: 53.41880341880342, f1: 23.317912602452076, r: 0.28423108876608216
06/01/2019 11:33:17 *** epoch: 68 ***
06/01/2019 11:33:17 *** training ***
06/01/2019 11:33:18 step: 2216, epoch: 67, batch: 4, loss: 0.066867895424366, acc: 100.0, f1: 100.0, r: 0.6996092075278142
06/01/2019 11:33:18 step: 2221, epoch: 67, batch: 9, loss: 0.7188785672187805, acc: 100.0, f1: 100.0, r: 0.8301953569713354
06/01/2019 11:33:19 step: 2226, epoch: 67, batch: 14, loss: 0.08615384995937347, acc: 98.4375, f1: 98.23232323232322, r: 0.8203308083262788
06/01/2019 11:33:20 step: 2231, epoch: 67, batch: 19, loss: 0.04252098128199577, acc: 100.0, f1: 100.0, r: 0.6670402805369002
06/01/2019 11:33:20 step: 2236, epoch: 67, batch: 24, loss: 0.09976823627948761, acc: 98.4375, f1: 96.36363636363636, r: 0.782789997738211
06/01/2019 11:33:21 step: 2241, epoch: 67, batch: 29, loss: 0.07099060714244843, acc: 100.0, f1: 100.0, r: 0.7898508128862317
06/01/2019 11:33:21 *** evaluating ***
06/01/2019 11:33:21 step: 68, epoch: 67, acc: 55.55555555555556, f1: 24.10817380308625, r: 0.2727743886527195
06/01/2019 11:33:21 *** epoch: 69 ***
06/01/2019 11:33:21 *** training ***
06/01/2019 11:33:22 step: 2249, epoch: 68, batch: 4, loss: 0.05776004120707512, acc: 100.0, f1: 100.0, r: 0.6618439490277183
06/01/2019 11:33:23 step: 2254, epoch: 68, batch: 9, loss: 0.15928694605827332, acc: 100.0, f1: 100.0, r: 0.765406612852473
06/01/2019 11:33:23 step: 2259, epoch: 68, batch: 14, loss: 0.05616912990808487, acc: 100.0, f1: 100.0, r: 0.7455610173660303
06/01/2019 11:33:24 step: 2264, epoch: 68, batch: 19, loss: 0.12577541172504425, acc: 96.875, f1: 92.49299719887955, r: 0.7020194684297231
06/01/2019 11:33:25 step: 2269, epoch: 68, batch: 24, loss: 0.07956142723560333, acc: 100.0, f1: 100.0, r: 0.7011992885250077
06/01/2019 11:33:25 step: 2274, epoch: 68, batch: 29, loss: 0.045435767620801926, acc: 100.0, f1: 100.0, r: 0.7197328248949091
06/01/2019 11:33:26 *** evaluating ***
06/01/2019 11:33:26 step: 69, epoch: 68, acc: 55.12820512820513, f1: 25.26340172478325, r: 0.28726027182303226
06/01/2019 11:33:26 *** epoch: 70 ***
06/01/2019 11:33:26 *** training ***
06/01/2019 11:33:26 step: 2282, epoch: 69, batch: 4, loss: 0.0577392503619194, acc: 100.0, f1: 100.0, r: 0.7563931334087709
06/01/2019 11:33:27 step: 2287, epoch: 69, batch: 9, loss: 0.07588356733322144, acc: 98.4375, f1: 99.24762531740808, r: 0.6911892473968018
06/01/2019 11:33:28 step: 2292, epoch: 69, batch: 14, loss: 0.05849579721689224, acc: 100.0, f1: 100.0, r: 0.6923803241937652
06/01/2019 11:33:28 step: 2297, epoch: 69, batch: 19, loss: 0.3972185552120209, acc: 100.0, f1: 100.0, r: 0.5931095189280305
06/01/2019 11:33:29 step: 2302, epoch: 69, batch: 24, loss: 0.07012468576431274, acc: 98.4375, f1: 99.19078742608154, r: 0.7761016392102824
06/01/2019 11:33:29 step: 2307, epoch: 69, batch: 29, loss: 0.050389207899570465, acc: 100.0, f1: 100.0, r: 0.7253761582982196
06/01/2019 11:33:30 *** evaluating ***
06/01/2019 11:33:30 step: 70, epoch: 69, acc: 53.41880341880342, f1: 24.800805235649506, r: 0.28120024476787975
06/01/2019 11:33:30 *** epoch: 71 ***
06/01/2019 11:33:30 *** training ***
06/01/2019 11:33:31 step: 2315, epoch: 70, batch: 4, loss: 0.03688740357756615, acc: 100.0, f1: 100.0, r: 0.7752934919310304
06/01/2019 11:33:31 step: 2320, epoch: 70, batch: 9, loss: 0.05625348538160324, acc: 98.4375, f1: 98.18007662835248, r: 0.7492610530780833
06/01/2019 11:33:32 step: 2325, epoch: 70, batch: 14, loss: 0.10674145817756653, acc: 98.4375, f1: 96.8733153638814, r: 0.7250508916225302
06/01/2019 11:33:32 step: 2330, epoch: 70, batch: 19, loss: 0.08624067157506943, acc: 98.4375, f1: 97.87581699346406, r: 0.7769379044031255
06/01/2019 11:33:33 step: 2335, epoch: 70, batch: 24, loss: 0.04817834496498108, acc: 100.0, f1: 100.0, r: 0.7837088480046541
06/01/2019 11:33:34 step: 2340, epoch: 70, batch: 29, loss: 0.060387250036001205, acc: 100.0, f1: 100.0, r: 0.809818004286068
06/01/2019 11:33:34 *** evaluating ***
06/01/2019 11:33:34 step: 71, epoch: 70, acc: 55.12820512820513, f1: 25.50009270995197, r: 0.28216263114310836
06/01/2019 11:33:34 *** epoch: 72 ***
06/01/2019 11:33:34 *** training ***
06/01/2019 11:33:35 step: 2348, epoch: 71, batch: 4, loss: 0.05448140203952789, acc: 100.0, f1: 100.0, r: 0.7786718887566262
06/01/2019 11:33:35 step: 2353, epoch: 71, batch: 9, loss: 0.08430856466293335, acc: 100.0, f1: 100.0, r: 0.685266986723928
06/01/2019 11:33:36 step: 2358, epoch: 71, batch: 14, loss: 0.0667887032032013, acc: 100.0, f1: 100.0, r: 0.6317021955450216
06/01/2019 11:33:37 step: 2363, epoch: 71, batch: 19, loss: 0.049335721880197525, acc: 100.0, f1: 100.0, r: 0.8401522227828933
06/01/2019 11:33:37 step: 2368, epoch: 71, batch: 24, loss: 0.10153870284557343, acc: 100.0, f1: 100.0, r: 0.7867918323938545
06/01/2019 11:33:38 step: 2373, epoch: 71, batch: 29, loss: 0.46547290682792664, acc: 100.0, f1: 100.0, r: 0.7266292837726213
06/01/2019 11:33:38 *** evaluating ***
06/01/2019 11:33:38 step: 72, epoch: 71, acc: 55.12820512820513, f1: 23.793229664727317, r: 0.2757985352376539
06/01/2019 11:33:38 *** epoch: 73 ***
06/01/2019 11:33:38 *** training ***
06/01/2019 11:33:39 step: 2381, epoch: 72, batch: 4, loss: 0.052478961646556854, acc: 100.0, f1: 100.0, r: 0.7847462063955842
06/01/2019 11:33:40 step: 2386, epoch: 72, batch: 9, loss: 0.07456750422716141, acc: 100.0, f1: 100.0, r: 0.7128649508592659
06/01/2019 11:33:40 step: 2391, epoch: 72, batch: 14, loss: 0.05245644226670265, acc: 100.0, f1: 100.0, r: 0.7020179048829754
06/01/2019 11:33:41 step: 2396, epoch: 72, batch: 19, loss: 0.04945622384548187, acc: 100.0, f1: 100.0, r: 0.6964130607006366
06/01/2019 11:33:42 step: 2401, epoch: 72, batch: 24, loss: 0.0778331607580185, acc: 96.875, f1: 96.88552188552188, r: 0.571095813135841
06/01/2019 11:33:42 step: 2406, epoch: 72, batch: 29, loss: 0.07064007222652435, acc: 100.0, f1: 100.0, r: 0.8383846024231021
06/01/2019 11:33:43 *** evaluating ***
06/01/2019 11:33:43 step: 73, epoch: 72, acc: 55.55555555555556, f1: 25.500822977314975, r: 0.2837084771416868
06/01/2019 11:33:43 *** epoch: 74 ***
06/01/2019 11:33:43 *** training ***
06/01/2019 11:33:44 step: 2414, epoch: 73, batch: 4, loss: 0.05172385647892952, acc: 100.0, f1: 100.0, r: 0.7193442164951924
06/01/2019 11:33:44 step: 2419, epoch: 73, batch: 9, loss: 0.08788178116083145, acc: 96.875, f1: 96.51923076923077, r: 0.7983154527621467
06/01/2019 11:33:45 step: 2424, epoch: 73, batch: 14, loss: 0.4020932912826538, acc: 98.4375, f1: 98.4006734006734, r: 0.8097031742087051
06/01/2019 11:33:45 step: 2429, epoch: 73, batch: 19, loss: 0.04735106974840164, acc: 100.0, f1: 100.0, r: 0.7112369640865758
06/01/2019 11:33:46 step: 2434, epoch: 73, batch: 24, loss: 0.04600520431995392, acc: 100.0, f1: 100.0, r: 0.7842073484991811
06/01/2019 11:33:47 step: 2439, epoch: 73, batch: 29, loss: 0.06631768494844437, acc: 100.0, f1: 100.0, r: 0.7645271310750648
06/01/2019 11:33:47 *** evaluating ***
06/01/2019 11:33:47 step: 74, epoch: 73, acc: 55.98290598290598, f1: 26.70517439431913, r: 0.2873316039813282
06/01/2019 11:33:47 *** epoch: 75 ***
06/01/2019 11:33:47 *** training ***
06/01/2019 11:33:48 step: 2447, epoch: 74, batch: 4, loss: 0.057408906519412994, acc: 100.0, f1: 100.0, r: 0.6605949355976857
06/01/2019 11:33:48 step: 2452, epoch: 74, batch: 9, loss: 0.10235070437192917, acc: 98.4375, f1: 96.85131195335276, r: 0.6563228014114456
06/01/2019 11:33:49 step: 2457, epoch: 74, batch: 14, loss: 0.06866741925477982, acc: 98.4375, f1: 98.96800825593395, r: 0.729912690061469
06/01/2019 11:33:50 step: 2462, epoch: 74, batch: 19, loss: 0.056753166019916534, acc: 98.4375, f1: 96.57142857142857, r: 0.743452626332795
06/01/2019 11:33:50 step: 2467, epoch: 74, batch: 24, loss: 0.08833980560302734, acc: 98.4375, f1: 96.52173913043478, r: 0.7326061952920817
06/01/2019 11:33:51 step: 2472, epoch: 74, batch: 29, loss: 0.06204218417406082, acc: 100.0, f1: 100.0, r: 0.6973410883597249
06/01/2019 11:33:51 *** evaluating ***
06/01/2019 11:33:51 step: 75, epoch: 74, acc: 55.98290598290598, f1: 26.84425946394032, r: 0.2822765342658235
06/01/2019 11:33:51 *** epoch: 76 ***
06/01/2019 11:33:51 *** training ***
06/01/2019 11:33:52 step: 2480, epoch: 75, batch: 4, loss: 0.055866241455078125, acc: 98.4375, f1: 97.75132275132275, r: 0.8062326258776824
06/01/2019 11:33:53 step: 2485, epoch: 75, batch: 9, loss: 0.7199088931083679, acc: 100.0, f1: 100.0, r: 0.6837994186079915
06/01/2019 11:33:53 step: 2490, epoch: 75, batch: 14, loss: 0.12110252678394318, acc: 100.0, f1: 100.0, r: 0.8271922343183284
06/01/2019 11:33:54 step: 2495, epoch: 75, batch: 19, loss: 0.08588725328445435, acc: 100.0, f1: 100.0, r: 0.7096145168658873
06/01/2019 11:33:55 step: 2500, epoch: 75, batch: 24, loss: 0.04694739356637001, acc: 100.0, f1: 100.0, r: 0.7733006936965418
06/01/2019 11:33:55 step: 2505, epoch: 75, batch: 29, loss: 0.0533624142408371, acc: 100.0, f1: 100.0, r: 0.7999898036378443
06/01/2019 11:33:56 *** evaluating ***
06/01/2019 11:33:56 step: 76, epoch: 75, acc: 53.84615384615385, f1: 23.511351909184725, r: 0.2719583895747601
06/01/2019 11:33:56 *** epoch: 77 ***
06/01/2019 11:33:56 *** training ***
06/01/2019 11:33:56 step: 2513, epoch: 76, batch: 4, loss: 0.5144674777984619, acc: 98.4375, f1: 95.23809523809523, r: 0.7306234636441291
06/01/2019 11:33:57 step: 2518, epoch: 76, batch: 9, loss: 0.06557679921388626, acc: 100.0, f1: 100.0, r: 0.7773669127968725
06/01/2019 11:33:58 step: 2523, epoch: 76, batch: 14, loss: 0.03790157288312912, acc: 100.0, f1: 100.0, r: 0.6372548039444501
06/01/2019 11:33:58 step: 2528, epoch: 76, batch: 19, loss: 0.06302614510059357, acc: 98.4375, f1: 86.76470588235294, r: 0.7222957263483073
06/01/2019 11:33:59 step: 2533, epoch: 76, batch: 24, loss: 0.049674343317747116, acc: 100.0, f1: 100.0, r: 0.6851314544162217
06/01/2019 11:34:00 step: 2538, epoch: 76, batch: 29, loss: 0.07533891499042511, acc: 100.0, f1: 100.0, r: 0.787358737192703
06/01/2019 11:34:00 *** evaluating ***
06/01/2019 11:34:00 step: 77, epoch: 76, acc: 54.700854700854705, f1: 26.153224766761884, r: 0.2793998584906986
06/01/2019 11:34:00 *** epoch: 78 ***
06/01/2019 11:34:00 *** training ***
06/01/2019 11:34:01 step: 2546, epoch: 77, batch: 4, loss: 0.043691396713256836, acc: 100.0, f1: 100.0, r: 0.8185906431400904
06/01/2019 11:34:01 step: 2551, epoch: 77, batch: 9, loss: 0.048658303916454315, acc: 100.0, f1: 100.0, r: 0.763951707494154
06/01/2019 11:34:02 step: 2556, epoch: 77, batch: 14, loss: 0.06849314272403717, acc: 98.4375, f1: 99.1282554211616, r: 0.6492709332374861
06/01/2019 11:34:03 step: 2561, epoch: 77, batch: 19, loss: 0.07460718601942062, acc: 96.875, f1: 96.78030303030303, r: 0.82249199735674
06/01/2019 11:34:03 step: 2566, epoch: 77, batch: 24, loss: 0.4600785970687866, acc: 100.0, f1: 100.0, r: 0.6818006379263264
06/01/2019 11:34:04 step: 2571, epoch: 77, batch: 29, loss: 0.03892967477440834, acc: 100.0, f1: 100.0, r: 0.7904503775291404
06/01/2019 11:34:04 *** evaluating ***
06/01/2019 11:34:04 step: 78, epoch: 77, acc: 55.55555555555556, f1: 25.472144087920523, r: 0.28526682718773655
06/01/2019 11:34:04 *** epoch: 79 ***
06/01/2019 11:34:04 *** training ***
06/01/2019 11:34:05 step: 2579, epoch: 78, batch: 4, loss: 0.0597313828766346, acc: 100.0, f1: 100.0, r: 0.798326487692831
06/01/2019 11:34:06 step: 2584, epoch: 78, batch: 9, loss: 0.07398006319999695, acc: 100.0, f1: 100.0, r: 0.7474988915152327
06/01/2019 11:34:06 step: 2589, epoch: 78, batch: 14, loss: 0.07272887974977493, acc: 98.4375, f1: 97.83549783549783, r: 0.7585287521651237
06/01/2019 11:34:07 step: 2594, epoch: 78, batch: 19, loss: 0.033793315291404724, acc: 100.0, f1: 100.0, r: 0.8252230028965374
06/01/2019 11:34:07 step: 2599, epoch: 78, batch: 24, loss: 0.05302663892507553, acc: 100.0, f1: 100.0, r: 0.8205532770628798
06/01/2019 11:34:08 step: 2604, epoch: 78, batch: 29, loss: 0.04715137928724289, acc: 100.0, f1: 100.0, r: 0.7481159426355672
06/01/2019 11:34:08 *** evaluating ***
06/01/2019 11:34:09 step: 79, epoch: 78, acc: 55.12820512820513, f1: 25.408235275541696, r: 0.27867949656209307
06/01/2019 11:34:09 *** epoch: 80 ***
06/01/2019 11:34:09 *** training ***
06/01/2019 11:34:09 step: 2612, epoch: 79, batch: 4, loss: 0.03400185704231262, acc: 100.0, f1: 100.0, r: 0.645145314440631
06/01/2019 11:34:10 step: 2617, epoch: 79, batch: 9, loss: 0.045607324689626694, acc: 100.0, f1: 100.0, r: 0.6887704002112508
06/01/2019 11:34:10 step: 2622, epoch: 79, batch: 14, loss: 0.06206616386771202, acc: 98.4375, f1: 94.74548440065682, r: 0.7582178053887081
06/01/2019 11:34:11 step: 2627, epoch: 79, batch: 19, loss: 0.06292777508497238, acc: 98.4375, f1: 95.71428571428571, r: 0.7708145630304347
06/01/2019 11:34:12 step: 2632, epoch: 79, batch: 24, loss: 0.046080946922302246, acc: 100.0, f1: 100.0, r: 0.6996648918519927
06/01/2019 11:34:12 step: 2637, epoch: 79, batch: 29, loss: 0.03554420918226242, acc: 100.0, f1: 100.0, r: 0.7287044453360902
06/01/2019 11:34:13 *** evaluating ***
06/01/2019 11:34:13 step: 80, epoch: 79, acc: 55.55555555555556, f1: 25.521657938537736, r: 0.286149507521443
06/01/2019 11:34:13 *** epoch: 81 ***
06/01/2019 11:34:13 *** training ***
06/01/2019 11:34:13 step: 2645, epoch: 80, batch: 4, loss: 0.07839265465736389, acc: 98.4375, f1: 96.8602825745683, r: 0.6822456800696275
06/01/2019 11:34:14 step: 2650, epoch: 80, batch: 9, loss: 0.04122835770249367, acc: 100.0, f1: 100.0, r: 0.7464907859644108
06/01/2019 11:34:15 step: 2655, epoch: 80, batch: 14, loss: 0.06131364032626152, acc: 98.4375, f1: 98.47619047619048, r: 0.7164450326550251
06/01/2019 11:34:15 step: 2660, epoch: 80, batch: 19, loss: 0.0607643760740757, acc: 100.0, f1: 100.0, r: 0.7022986800410893
06/01/2019 11:34:16 step: 2665, epoch: 80, batch: 24, loss: 0.04201872646808624, acc: 100.0, f1: 100.0, r: 0.6395892212536631
06/01/2019 11:34:17 step: 2670, epoch: 80, batch: 29, loss: 0.03625704348087311, acc: 100.0, f1: 100.0, r: 0.6506237552387186
06/01/2019 11:34:17 *** evaluating ***
06/01/2019 11:34:17 step: 81, epoch: 80, acc: 56.837606837606835, f1: 26.01384082262399, r: 0.2846935065080549
06/01/2019 11:34:17 *** epoch: 82 ***
06/01/2019 11:34:17 *** training ***
06/01/2019 11:34:18 step: 2678, epoch: 81, batch: 4, loss: 0.07527820765972137, acc: 95.3125, f1: 88.75, r: 0.8247508134178747
06/01/2019 11:34:19 step: 2683, epoch: 81, batch: 9, loss: 0.3971945643424988, acc: 98.4375, f1: 96.6137566137566, r: 0.6197932291732761
06/01/2019 11:34:19 step: 2688, epoch: 81, batch: 14, loss: 0.03690900653600693, acc: 100.0, f1: 100.0, r: 0.7051742339878317
06/01/2019 11:34:20 step: 2693, epoch: 81, batch: 19, loss: 0.06888249516487122, acc: 100.0, f1: 100.0, r: 0.6809744798863151
06/01/2019 11:34:21 step: 2698, epoch: 81, batch: 24, loss: 0.05911736562848091, acc: 100.0, f1: 100.0, r: 0.7338419607661163
06/01/2019 11:34:21 step: 2703, epoch: 81, batch: 29, loss: 0.03872738033533096, acc: 100.0, f1: 100.0, r: 0.8330877687790652
06/01/2019 11:34:22 *** evaluating ***
06/01/2019 11:34:22 step: 82, epoch: 81, acc: 54.700854700854705, f1: 23.60479797979798, r: 0.27704541811591826
06/01/2019 11:34:22 *** epoch: 83 ***
06/01/2019 11:34:22 *** training ***
06/01/2019 11:34:22 step: 2711, epoch: 82, batch: 4, loss: 0.031095296144485474, acc: 100.0, f1: 100.0, r: 0.6246997695496129
06/01/2019 11:34:23 step: 2716, epoch: 82, batch: 9, loss: 0.06305146962404251, acc: 96.875, f1: 96.74634684312103, r: 0.8261068027740368
06/01/2019 11:34:23 step: 2721, epoch: 82, batch: 14, loss: 0.03369150310754776, acc: 100.0, f1: 100.0, r: 0.7717357648127467
06/01/2019 11:34:24 step: 2726, epoch: 82, batch: 19, loss: 0.035068392753601074, acc: 100.0, f1: 100.0, r: 0.7460102541911772
06/01/2019 11:34:25 step: 2731, epoch: 82, batch: 24, loss: 0.05583364516496658, acc: 100.0, f1: 100.0, r: 0.7276606251610187
06/01/2019 11:34:25 step: 2736, epoch: 82, batch: 29, loss: 0.08240164816379547, acc: 100.0, f1: 100.0, r: 0.5987841795894344
06/01/2019 11:34:26 *** evaluating ***
06/01/2019 11:34:26 step: 83, epoch: 82, acc: 55.55555555555556, f1: 25.27348715992881, r: 0.280018466045883
06/01/2019 11:34:26 *** epoch: 84 ***
06/01/2019 11:34:26 *** training ***
06/01/2019 11:34:27 step: 2744, epoch: 83, batch: 4, loss: 0.04655662178993225, acc: 98.4375, f1: 97.49835418038182, r: 0.7120750150398586
06/01/2019 11:34:27 step: 2749, epoch: 83, batch: 9, loss: 0.029793038964271545, acc: 100.0, f1: 100.0, r: 0.8196576598235333
06/01/2019 11:34:28 step: 2754, epoch: 83, batch: 14, loss: 0.03308972716331482, acc: 100.0, f1: 100.0, r: 0.7196733772588663
06/01/2019 11:34:29 step: 2759, epoch: 83, batch: 19, loss: 0.03435043618083, acc: 100.0, f1: 100.0, r: 0.7717621229341564
06/01/2019 11:34:29 step: 2764, epoch: 83, batch: 24, loss: 0.05813833326101303, acc: 98.4375, f1: 94.28571428571428, r: 0.6949989588459355
06/01/2019 11:34:30 step: 2769, epoch: 83, batch: 29, loss: 0.04824773594737053, acc: 100.0, f1: 100.0, r: 0.7031104179233734
06/01/2019 11:34:30 *** evaluating ***
06/01/2019 11:34:31 step: 84, epoch: 83, acc: 54.700854700854705, f1: 24.465415744181435, r: 0.2767396252432928
06/01/2019 11:34:31 *** epoch: 85 ***
06/01/2019 11:34:31 *** training ***
06/01/2019 11:34:31 step: 2777, epoch: 84, batch: 4, loss: 0.027729414403438568, acc: 100.0, f1: 100.0, r: 0.671900064417247
06/01/2019 11:34:32 step: 2782, epoch: 84, batch: 9, loss: 0.05535252392292023, acc: 100.0, f1: 100.0, r: 0.7380430582236311
06/01/2019 11:34:32 step: 2787, epoch: 84, batch: 14, loss: 0.04174869880080223, acc: 100.0, f1: 100.0, r: 0.6999744360427871
06/01/2019 11:34:33 step: 2792, epoch: 84, batch: 19, loss: 0.061056431382894516, acc: 100.0, f1: 100.0, r: 0.7767443493475525
06/01/2019 11:34:34 step: 2797, epoch: 84, batch: 24, loss: 0.027456829324364662, acc: 100.0, f1: 100.0, r: 0.6666480124325662
06/01/2019 11:34:34 step: 2802, epoch: 84, batch: 29, loss: 0.06023469194769859, acc: 100.0, f1: 100.0, r: 0.6996765749533675
06/01/2019 11:34:35 *** evaluating ***
06/01/2019 11:34:35 step: 85, epoch: 84, acc: 51.70940170940172, f1: 22.76849962010041, r: 0.2707823235805349
06/01/2019 11:34:35 *** epoch: 86 ***
06/01/2019 11:34:35 *** training ***
06/01/2019 11:34:35 step: 2810, epoch: 85, batch: 4, loss: 0.10250627994537354, acc: 100.0, f1: 100.0, r: 0.8079883304485324
06/01/2019 11:34:36 step: 2815, epoch: 85, batch: 9, loss: 0.05341911315917969, acc: 98.4375, f1: 97.1188475390156, r: 0.71731539332244
06/01/2019 11:34:37 step: 2820, epoch: 85, batch: 14, loss: 0.4368177056312561, acc: 100.0, f1: 100.0, r: 0.6875195379164398
06/01/2019 11:34:37 step: 2825, epoch: 85, batch: 19, loss: 0.062160514295101166, acc: 98.4375, f1: 87.20930232558139, r: 0.7329330303964926
06/01/2019 11:34:38 step: 2830, epoch: 85, batch: 24, loss: 0.6971851587295532, acc: 100.0, f1: 100.0, r: 0.753033874090081
06/01/2019 11:34:39 step: 2835, epoch: 85, batch: 29, loss: 0.07291018962860107, acc: 100.0, f1: 100.0, r: 0.8206103534824919
06/01/2019 11:34:39 *** evaluating ***
06/01/2019 11:34:39 step: 86, epoch: 85, acc: 55.12820512820513, f1: 23.94733129253253, r: 0.26987024651213587
06/01/2019 11:34:39 *** epoch: 87 ***
06/01/2019 11:34:39 *** training ***
06/01/2019 11:34:40 step: 2843, epoch: 86, batch: 4, loss: 0.0308125801384449, acc: 100.0, f1: 100.0, r: 0.793894208468516
06/01/2019 11:34:40 step: 2848, epoch: 86, batch: 9, loss: 0.032111428678035736, acc: 100.0, f1: 100.0, r: 0.8253811351669796
06/01/2019 11:34:41 step: 2853, epoch: 86, batch: 14, loss: 0.05098947882652283, acc: 100.0, f1: 100.0, r: 0.764099046049157
06/01/2019 11:34:42 step: 2858, epoch: 86, batch: 19, loss: 0.028898734599351883, acc: 100.0, f1: 100.0, r: 0.8118949738992524
06/01/2019 11:34:42 step: 2863, epoch: 86, batch: 24, loss: 0.056038904935121536, acc: 100.0, f1: 100.0, r: 0.799327219155915
06/01/2019 11:34:43 step: 2868, epoch: 86, batch: 29, loss: 0.03869694471359253, acc: 100.0, f1: 100.0, r: 0.6891665259186641
06/01/2019 11:34:43 *** evaluating ***
06/01/2019 11:34:43 step: 87, epoch: 86, acc: 52.13675213675214, f1: 23.064202745318312, r: 0.2694863200829032
06/01/2019 11:34:43 *** epoch: 88 ***
06/01/2019 11:34:43 *** training ***
06/01/2019 11:34:44 step: 2876, epoch: 87, batch: 4, loss: 0.051462914794683456, acc: 100.0, f1: 100.0, r: 0.7529795128428608
06/01/2019 11:34:45 step: 2881, epoch: 87, batch: 9, loss: 0.03497535362839699, acc: 100.0, f1: 100.0, r: 0.7567801977834845
06/01/2019 11:34:45 step: 2886, epoch: 87, batch: 14, loss: 0.7208268642425537, acc: 100.0, f1: 100.0, r: 0.6817856990027851
06/01/2019 11:34:46 step: 2891, epoch: 87, batch: 19, loss: 0.029977945610880852, acc: 100.0, f1: 100.0, r: 0.7342201312194584
06/01/2019 11:34:46 step: 2896, epoch: 87, batch: 24, loss: 0.034911222755908966, acc: 100.0, f1: 100.0, r: 0.6679074685049978
06/01/2019 11:34:47 step: 2901, epoch: 87, batch: 29, loss: 0.09037564694881439, acc: 100.0, f1: 100.0, r: 0.808571828621917
06/01/2019 11:34:47 *** evaluating ***
06/01/2019 11:34:48 step: 88, epoch: 87, acc: 55.98290598290598, f1: 25.346440961603584, r: 0.28188671056966785
06/01/2019 11:34:48 *** epoch: 89 ***
06/01/2019 11:34:48 *** training ***
06/01/2019 11:34:48 step: 2909, epoch: 88, batch: 4, loss: 0.02627604827284813, acc: 100.0, f1: 100.0, r: 0.7378343380102855
06/01/2019 11:34:49 step: 2914, epoch: 88, batch: 9, loss: 0.05995407700538635, acc: 98.4375, f1: 98.3201581027668, r: 0.7551832341316559
06/01/2019 11:34:50 step: 2919, epoch: 88, batch: 14, loss: 0.03152592107653618, acc: 100.0, f1: 100.0, r: 0.7016700266027068
06/01/2019 11:34:50 step: 2924, epoch: 88, batch: 19, loss: 0.059303294867277145, acc: 100.0, f1: 100.0, r: 0.7068614658688233
06/01/2019 11:34:51 step: 2929, epoch: 88, batch: 24, loss: 0.04058489948511124, acc: 100.0, f1: 100.0, r: 0.6772849777923975
06/01/2019 11:34:52 step: 2934, epoch: 88, batch: 29, loss: 0.4580836594104767, acc: 100.0, f1: 100.0, r: 0.7546106986615676
06/01/2019 11:34:52 *** evaluating ***
06/01/2019 11:34:52 step: 89, epoch: 88, acc: 55.98290598290598, f1: 25.685868231490605, r: 0.2820203754703132
06/01/2019 11:34:52 *** epoch: 90 ***
06/01/2019 11:34:52 *** training ***
06/01/2019 11:34:53 step: 2942, epoch: 89, batch: 4, loss: 0.033070147037506104, acc: 100.0, f1: 100.0, r: 0.7905791732920464
06/01/2019 11:34:53 step: 2947, epoch: 89, batch: 9, loss: 0.03947579860687256, acc: 100.0, f1: 100.0, r: 0.6796498685129778
06/01/2019 11:34:54 step: 2952, epoch: 89, batch: 14, loss: 0.05203654617071152, acc: 100.0, f1: 100.0, r: 0.711517208325602
06/01/2019 11:34:55 step: 2957, epoch: 89, batch: 19, loss: 0.05542971193790436, acc: 100.0, f1: 100.0, r: 0.7808681831885401
06/01/2019 11:34:55 step: 2962, epoch: 89, batch: 24, loss: 0.038863200694322586, acc: 100.0, f1: 100.0, r: 0.8266894015774539
06/01/2019 11:34:56 step: 2967, epoch: 89, batch: 29, loss: 0.022264990955591202, acc: 100.0, f1: 100.0, r: 0.8117187375683946
06/01/2019 11:34:56 *** evaluating ***
06/01/2019 11:34:57 step: 90, epoch: 89, acc: 55.98290598290598, f1: 25.20603377310592, r: 0.2827595464754006
06/01/2019 11:34:57 *** epoch: 91 ***
06/01/2019 11:34:57 *** training ***
06/01/2019 11:34:57 step: 2975, epoch: 90, batch: 4, loss: 0.04980938136577606, acc: 100.0, f1: 100.0, r: 0.7367678539675446
06/01/2019 11:34:58 step: 2980, epoch: 90, batch: 9, loss: 0.023797374218702316, acc: 100.0, f1: 100.0, r: 0.7468506433475532
06/01/2019 11:34:59 step: 2985, epoch: 90, batch: 14, loss: 0.01988709159195423, acc: 100.0, f1: 100.0, r: 0.8243623554815486
06/01/2019 11:34:59 step: 2990, epoch: 90, batch: 19, loss: 0.037558846175670624, acc: 98.4375, f1: 98.57142857142858, r: 0.7586733857898599
06/01/2019 11:35:00 step: 2995, epoch: 90, batch: 24, loss: 0.0577908530831337, acc: 100.0, f1: 100.0, r: 0.7215132399914159
06/01/2019 11:35:00 step: 3000, epoch: 90, batch: 29, loss: 0.04242894798517227, acc: 100.0, f1: 100.0, r: 0.73688720122804
06/01/2019 11:35:01 *** evaluating ***
06/01/2019 11:35:01 step: 91, epoch: 90, acc: 54.700854700854705, f1: 25.24822717611965, r: 0.28003181187254567
06/01/2019 11:35:01 *** epoch: 92 ***
06/01/2019 11:35:01 *** training ***
06/01/2019 11:35:02 step: 3008, epoch: 91, batch: 4, loss: 0.02567652426660061, acc: 100.0, f1: 100.0, r: 0.746266277436542
06/01/2019 11:35:02 step: 3013, epoch: 91, batch: 9, loss: 0.1084165871143341, acc: 98.4375, f1: 96.9047619047619, r: 0.7517779733527743
06/01/2019 11:35:03 step: 3018, epoch: 91, batch: 14, loss: 0.03824075683951378, acc: 100.0, f1: 100.0, r: 0.755830715378597
06/01/2019 11:35:04 step: 3023, epoch: 91, batch: 19, loss: 0.03913220763206482, acc: 100.0, f1: 100.0, r: 0.7157648593754457
06/01/2019 11:35:04 step: 3028, epoch: 91, batch: 24, loss: 0.03633652999997139, acc: 100.0, f1: 100.0, r: 0.7267268470770962
06/01/2019 11:35:05 step: 3033, epoch: 91, batch: 29, loss: 0.05293109267950058, acc: 100.0, f1: 100.0, r: 0.7598689154768328
06/01/2019 11:35:05 *** evaluating ***
06/01/2019 11:35:06 step: 92, epoch: 91, acc: 55.55555555555556, f1: 25.249984350946995, r: 0.2744427615787854
06/01/2019 11:35:06 *** epoch: 93 ***
06/01/2019 11:35:06 *** training ***
06/01/2019 11:35:06 step: 3041, epoch: 92, batch: 4, loss: 0.053815655410289764, acc: 100.0, f1: 100.0, r: 0.7382257430198957
06/01/2019 11:35:07 step: 3046, epoch: 92, batch: 9, loss: 0.46504706144332886, acc: 100.0, f1: 100.0, r: 0.7674003272396913
06/01/2019 11:35:07 step: 3051, epoch: 92, batch: 14, loss: 0.062400929629802704, acc: 98.4375, f1: 98.2051282051282, r: 0.8311256673130399
06/01/2019 11:35:08 step: 3056, epoch: 92, batch: 19, loss: 0.04112544283270836, acc: 100.0, f1: 100.0, r: 0.797519998869822
06/01/2019 11:35:09 step: 3061, epoch: 92, batch: 24, loss: 0.06261562556028366, acc: 98.4375, f1: 97.47899159663865, r: 0.7875786161475289
06/01/2019 11:35:09 step: 3066, epoch: 92, batch: 29, loss: 0.03753945603966713, acc: 100.0, f1: 100.0, r: 0.7919561306111212
06/01/2019 11:35:10 *** evaluating ***
06/01/2019 11:35:10 step: 93, epoch: 92, acc: 55.98290598290598, f1: 25.39003723905321, r: 0.2745160849132058
06/01/2019 11:35:10 *** epoch: 94 ***
06/01/2019 11:35:10 *** training ***
06/01/2019 11:35:11 step: 3074, epoch: 93, batch: 4, loss: 0.03281690552830696, acc: 100.0, f1: 100.0, r: 0.7714627441884959
06/01/2019 11:35:11 step: 3079, epoch: 93, batch: 9, loss: 0.048461005091667175, acc: 100.0, f1: 100.0, r: 0.5838181743177264
06/01/2019 11:35:12 step: 3084, epoch: 93, batch: 14, loss: 0.041498392820358276, acc: 100.0, f1: 100.0, r: 0.725549580986009
06/01/2019 11:35:13 step: 3089, epoch: 93, batch: 19, loss: 0.02947617508471012, acc: 100.0, f1: 100.0, r: 0.7639461977356914
06/01/2019 11:35:13 step: 3094, epoch: 93, batch: 24, loss: 0.03257777914404869, acc: 100.0, f1: 100.0, r: 0.7023762771723151
06/01/2019 11:35:14 step: 3099, epoch: 93, batch: 29, loss: 0.04744208604097366, acc: 100.0, f1: 100.0, r: 0.7888377678865096
06/01/2019 11:35:14 *** evaluating ***
06/01/2019 11:35:15 step: 94, epoch: 93, acc: 55.98290598290598, f1: 25.292493483465496, r: 0.26945229353763295
06/01/2019 11:35:15 *** epoch: 95 ***
06/01/2019 11:35:15 *** training ***
06/01/2019 11:35:15 step: 3107, epoch: 94, batch: 4, loss: 0.020144086331129074, acc: 100.0, f1: 100.0, r: 0.5975777223778082
06/01/2019 11:35:16 step: 3112, epoch: 94, batch: 9, loss: 0.10273236781358719, acc: 100.0, f1: 100.0, r: 0.6971812389534309
06/01/2019 11:35:16 step: 3117, epoch: 94, batch: 14, loss: 0.02708003669977188, acc: 100.0, f1: 100.0, r: 0.720413337525837
06/01/2019 11:35:17 step: 3122, epoch: 94, batch: 19, loss: 0.027570709586143494, acc: 100.0, f1: 100.0, r: 0.730224439372546
06/01/2019 11:35:18 step: 3127, epoch: 94, batch: 24, loss: 0.7007747888565063, acc: 100.0, f1: 100.0, r: 0.7278429658545773
06/01/2019 11:35:19 step: 3132, epoch: 94, batch: 29, loss: 0.052740465849637985, acc: 100.0, f1: 100.0, r: 0.815459654317446
06/01/2019 11:35:19 *** evaluating ***
06/01/2019 11:35:19 step: 95, epoch: 94, acc: 55.55555555555556, f1: 25.19324129883078, r: 0.2674890700123608
06/01/2019 11:35:19 *** epoch: 96 ***
06/01/2019 11:35:19 *** training ***
06/01/2019 11:35:20 step: 3140, epoch: 95, batch: 4, loss: 0.060220807790756226, acc: 100.0, f1: 100.0, r: 0.6722405245090204
06/01/2019 11:35:20 step: 3145, epoch: 95, batch: 9, loss: 0.03407259285449982, acc: 100.0, f1: 100.0, r: 0.7217500776087267
06/01/2019 11:35:21 step: 3150, epoch: 95, batch: 14, loss: 0.03548121824860573, acc: 100.0, f1: 100.0, r: 0.7759216430084884
06/01/2019 11:35:22 step: 3155, epoch: 95, batch: 19, loss: 0.041945211589336395, acc: 100.0, f1: 100.0, r: 0.7226770817120011
06/01/2019 11:35:22 step: 3160, epoch: 95, batch: 24, loss: 0.03373539820313454, acc: 100.0, f1: 100.0, r: 0.8013768027667212
06/01/2019 11:35:23 step: 3165, epoch: 95, batch: 29, loss: 0.03840292990207672, acc: 100.0, f1: 100.0, r: 0.6545779138341751
06/01/2019 11:35:23 *** evaluating ***
06/01/2019 11:35:24 step: 96, epoch: 95, acc: 54.700854700854705, f1: 25.071640219964635, r: 0.2688587244980203
06/01/2019 11:35:24 *** epoch: 97 ***
06/01/2019 11:35:24 *** training ***
06/01/2019 11:35:24 step: 3173, epoch: 96, batch: 4, loss: 0.04098723828792572, acc: 100.0, f1: 100.0, r: 0.8582545172207632
06/01/2019 11:35:25 step: 3178, epoch: 96, batch: 9, loss: 0.06622131168842316, acc: 100.0, f1: 100.0, r: 0.5410396348207621
06/01/2019 11:35:25 step: 3183, epoch: 96, batch: 14, loss: 0.031656377017498016, acc: 100.0, f1: 100.0, r: 0.7299839925076902
06/01/2019 11:35:26 step: 3188, epoch: 96, batch: 19, loss: 0.04156341776251793, acc: 100.0, f1: 100.0, r: 0.7267460668568324
06/01/2019 11:35:27 step: 3193, epoch: 96, batch: 24, loss: 0.02916557341814041, acc: 100.0, f1: 100.0, r: 0.7413775388801
06/01/2019 11:35:27 step: 3198, epoch: 96, batch: 29, loss: 0.05247505009174347, acc: 100.0, f1: 100.0, r: 0.7743374754600574
06/01/2019 11:35:28 *** evaluating ***
06/01/2019 11:35:28 step: 97, epoch: 96, acc: 52.991452991452995, f1: 23.09066112681309, r: 0.2742580720373244
06/01/2019 11:35:28 *** epoch: 98 ***
06/01/2019 11:35:28 *** training ***
06/01/2019 11:35:28 step: 3206, epoch: 97, batch: 4, loss: 0.03870014473795891, acc: 100.0, f1: 100.0, r: 0.7926201226429953
06/01/2019 11:35:29 step: 3211, epoch: 97, batch: 9, loss: 0.034456733614206314, acc: 100.0, f1: 100.0, r: 0.7588606080309684
06/01/2019 11:35:30 step: 3216, epoch: 97, batch: 14, loss: 0.3910452425479889, acc: 100.0, f1: 100.0, r: 0.7726309524303653
06/01/2019 11:35:30 step: 3221, epoch: 97, batch: 19, loss: 0.024646375328302383, acc: 100.0, f1: 100.0, r: 0.6516798098810557
06/01/2019 11:35:31 step: 3226, epoch: 97, batch: 24, loss: 0.03726687282323837, acc: 100.0, f1: 100.0, r: 0.7080034231935614
06/01/2019 11:35:32 step: 3231, epoch: 97, batch: 29, loss: 0.04461238905787468, acc: 98.4375, f1: 97.64957264957265, r: 0.8118898655417004
06/01/2019 11:35:32 *** evaluating ***
06/01/2019 11:35:32 step: 98, epoch: 97, acc: 55.98290598290598, f1: 25.576555676855893, r: 0.26991923113248234
06/01/2019 11:35:32 *** epoch: 99 ***
06/01/2019 11:35:32 *** training ***
06/01/2019 11:35:33 step: 3239, epoch: 98, batch: 4, loss: 0.026074351742863655, acc: 100.0, f1: 100.0, r: 0.7990173011102364
06/01/2019 11:35:33 step: 3244, epoch: 98, batch: 9, loss: 0.07521730661392212, acc: 96.875, f1: 96.36633362381696, r: 0.72750667221199
06/01/2019 11:35:34 step: 3249, epoch: 98, batch: 14, loss: 0.07534217834472656, acc: 100.0, f1: 100.0, r: 0.6560690757878076
06/01/2019 11:35:35 step: 3254, epoch: 98, batch: 19, loss: 0.02606784552335739, acc: 100.0, f1: 100.0, r: 0.7233064006866005
06/01/2019 11:35:35 step: 3259, epoch: 98, batch: 24, loss: 0.03722704201936722, acc: 100.0, f1: 100.0, r: 0.7299592007514895
06/01/2019 11:35:36 step: 3264, epoch: 98, batch: 29, loss: 0.03900150582194328, acc: 100.0, f1: 100.0, r: 0.7443669499782892
06/01/2019 11:35:36 *** evaluating ***
06/01/2019 11:35:37 step: 99, epoch: 98, acc: 54.27350427350427, f1: 24.966636685386685, r: 0.27040362815063235
06/01/2019 11:35:37 *** epoch: 100 ***
06/01/2019 11:35:37 *** training ***
06/01/2019 11:35:37 step: 3272, epoch: 99, batch: 4, loss: 0.0419662743806839, acc: 98.4375, f1: 99.18992884510126, r: 0.7060177791874025
06/01/2019 11:35:38 step: 3277, epoch: 99, batch: 9, loss: 0.032854724675416946, acc: 100.0, f1: 100.0, r: 0.6896913720425939
06/01/2019 11:35:39 step: 3282, epoch: 99, batch: 14, loss: 0.025949502363801003, acc: 100.0, f1: 100.0, r: 0.7437484694163654
06/01/2019 11:35:39 step: 3287, epoch: 99, batch: 19, loss: 0.0938856229186058, acc: 100.0, f1: 100.0, r: 0.746318096838868
06/01/2019 11:35:40 step: 3292, epoch: 99, batch: 24, loss: 0.05672106519341469, acc: 100.0, f1: 100.0, r: 0.7587867684281203
06/01/2019 11:35:41 step: 3297, epoch: 99, batch: 29, loss: 0.0533023402094841, acc: 100.0, f1: 100.0, r: 0.6878174185967302
06/01/2019 11:35:41 *** evaluating ***
06/01/2019 11:35:41 step: 100, epoch: 99, acc: 56.41025641025641, f1: 25.686847950980628, r: 0.2743992718826541
06/01/2019 11:35:41 *** epoch: 101 ***
06/01/2019 11:35:41 *** training ***
06/01/2019 11:35:42 step: 3305, epoch: 100, batch: 4, loss: 0.02875990979373455, acc: 100.0, f1: 100.0, r: 0.7490087438329948
06/01/2019 11:35:42 step: 3310, epoch: 100, batch: 9, loss: 0.07343333959579468, acc: 100.0, f1: 100.0, r: 0.7751673744542382
06/01/2019 11:35:43 step: 3315, epoch: 100, batch: 14, loss: 0.034147948026657104, acc: 100.0, f1: 100.0, r: 0.7160370876226549
06/01/2019 11:35:44 step: 3320, epoch: 100, batch: 19, loss: 0.027117619290947914, acc: 100.0, f1: 100.0, r: 0.7510745956079123
06/01/2019 11:35:44 step: 3325, epoch: 100, batch: 24, loss: 0.03623364120721817, acc: 100.0, f1: 100.0, r: 0.6123266549809673
06/01/2019 11:35:45 step: 3330, epoch: 100, batch: 29, loss: 0.026351742446422577, acc: 100.0, f1: 100.0, r: 0.8097931423705728
06/01/2019 11:35:45 *** evaluating ***
06/01/2019 11:35:45 step: 101, epoch: 100, acc: 55.98290598290598, f1: 25.546396613417155, r: 0.2745288508037343
06/01/2019 11:35:45 *** epoch: 102 ***
06/01/2019 11:35:45 *** training ***
06/01/2019 11:35:46 step: 3338, epoch: 101, batch: 4, loss: 0.030314315110445023, acc: 100.0, f1: 100.0, r: 0.6898998715054577
06/01/2019 11:35:47 step: 3343, epoch: 101, batch: 9, loss: 0.053666651248931885, acc: 100.0, f1: 100.0, r: 0.7403170013465622
06/01/2019 11:35:48 step: 3348, epoch: 101, batch: 14, loss: 0.05574486777186394, acc: 98.4375, f1: 95.47619047619047, r: 0.790337708344839
06/01/2019 11:35:48 step: 3353, epoch: 101, batch: 19, loss: 0.111776202917099, acc: 100.0, f1: 100.0, r: 0.7333378132440802
06/01/2019 11:35:49 step: 3358, epoch: 101, batch: 24, loss: 0.035296034067869186, acc: 100.0, f1: 100.0, r: 0.6904506298754698
06/01/2019 11:35:50 step: 3363, epoch: 101, batch: 29, loss: 0.10080428421497345, acc: 98.4375, f1: 97.88359788359789, r: 0.6756166314382422
06/01/2019 11:35:50 *** evaluating ***
06/01/2019 11:35:50 step: 102, epoch: 101, acc: 55.12820512820513, f1: 23.7843969243375, r: 0.26781857068746273
06/01/2019 11:35:50 *** epoch: 103 ***
06/01/2019 11:35:50 *** training ***
06/01/2019 11:35:51 step: 3371, epoch: 102, batch: 4, loss: 0.03382131829857826, acc: 100.0, f1: 100.0, r: 0.5971595758821828
06/01/2019 11:35:51 step: 3376, epoch: 102, batch: 9, loss: 0.022630665451288223, acc: 100.0, f1: 100.0, r: 0.7436082890147594
06/01/2019 11:35:52 step: 3381, epoch: 102, batch: 14, loss: 0.03688126429915428, acc: 100.0, f1: 100.0, r: 0.705939446121468
06/01/2019 11:35:53 step: 3386, epoch: 102, batch: 19, loss: 0.02361961640417576, acc: 100.0, f1: 100.0, r: 0.7648905371283481
06/01/2019 11:35:53 step: 3391, epoch: 102, batch: 24, loss: 0.3985973000526428, acc: 100.0, f1: 100.0, r: 0.7805685670222773
06/01/2019 11:35:54 step: 3396, epoch: 102, batch: 29, loss: 0.025508109480142593, acc: 100.0, f1: 100.0, r: 0.8011558083326875
06/01/2019 11:35:54 *** evaluating ***
06/01/2019 11:35:55 step: 103, epoch: 102, acc: 51.70940170940172, f1: 22.684837202621058, r: 0.2628206158218833
06/01/2019 11:35:55 *** epoch: 104 ***
06/01/2019 11:35:55 *** training ***
06/01/2019 11:35:55 step: 3404, epoch: 103, batch: 4, loss: 0.04038859158754349, acc: 98.4375, f1: 85.14285714285714, r: 0.7120646669281852
06/01/2019 11:35:56 step: 3409, epoch: 103, batch: 9, loss: 0.03710456192493439, acc: 100.0, f1: 100.0, r: 0.7636858700144228
06/01/2019 11:35:56 step: 3414, epoch: 103, batch: 14, loss: 0.038589958101511, acc: 100.0, f1: 100.0, r: 0.6979684877195378
06/01/2019 11:35:57 step: 3419, epoch: 103, batch: 19, loss: 0.031599681824445724, acc: 100.0, f1: 100.0, r: 0.7562900763019834
06/01/2019 11:35:58 step: 3424, epoch: 103, batch: 24, loss: 0.023875029757618904, acc: 100.0, f1: 100.0, r: 0.642813983258848
06/01/2019 11:35:58 step: 3429, epoch: 103, batch: 29, loss: 0.6924006938934326, acc: 100.0, f1: 100.0, r: 0.6851660326368421
06/01/2019 11:35:59 *** evaluating ***
06/01/2019 11:35:59 step: 104, epoch: 103, acc: 55.98290598290598, f1: 25.487461822703043, r: 0.26905964693732254
06/01/2019 11:35:59 *** epoch: 105 ***
06/01/2019 11:35:59 *** training ***
06/01/2019 11:35:59 step: 3437, epoch: 104, batch: 4, loss: 0.03691147267818451, acc: 100.0, f1: 100.0, r: 0.7278324640988179
06/01/2019 11:36:00 step: 3442, epoch: 104, batch: 9, loss: 0.026139987632632256, acc: 100.0, f1: 100.0, r: 0.7740165780478472
06/01/2019 11:36:01 step: 3447, epoch: 104, batch: 14, loss: 0.05363927781581879, acc: 100.0, f1: 100.0, r: 0.6963356045046284
06/01/2019 11:36:01 step: 3452, epoch: 104, batch: 19, loss: 0.03932730853557587, acc: 100.0, f1: 100.0, r: 0.7275575007825898
06/01/2019 11:36:02 step: 3457, epoch: 104, batch: 24, loss: 0.06620796769857407, acc: 98.4375, f1: 99.37146448774355, r: 0.816000243026123
06/01/2019 11:36:02 step: 3462, epoch: 104, batch: 29, loss: 0.022722363471984863, acc: 100.0, f1: 100.0, r: 0.7904048214400932
06/01/2019 11:36:03 *** evaluating ***
06/01/2019 11:36:03 step: 105, epoch: 104, acc: 53.84615384615385, f1: 23.39527364177079, r: 0.25629564179562786
06/01/2019 11:36:03 *** epoch: 106 ***
06/01/2019 11:36:03 *** training ***
06/01/2019 11:36:04 step: 3470, epoch: 105, batch: 4, loss: 0.040516555309295654, acc: 98.4375, f1: 98.82882882882883, r: 0.7636951767122732
06/01/2019 11:36:04 step: 3475, epoch: 105, batch: 9, loss: 0.029739413410425186, acc: 100.0, f1: 100.0, r: 0.7635991313751079
06/01/2019 11:36:05 step: 3480, epoch: 105, batch: 14, loss: 0.020907551050186157, acc: 100.0, f1: 100.0, r: 0.7645871718309121
06/01/2019 11:36:06 step: 3485, epoch: 105, batch: 19, loss: 0.03439829498529434, acc: 98.4375, f1: 99.21142369991475, r: 0.8249888401610563
06/01/2019 11:36:06 step: 3490, epoch: 105, batch: 24, loss: 0.02409413456916809, acc: 100.0, f1: 100.0, r: 0.6992478994507453
06/01/2019 11:36:07 step: 3495, epoch: 105, batch: 29, loss: 0.03901626914739609, acc: 98.4375, f1: 95.87301587301587, r: 0.6311534339278836
06/01/2019 11:36:07 *** evaluating ***
06/01/2019 11:36:08 step: 106, epoch: 105, acc: 55.55555555555556, f1: 25.639473952233555, r: 0.26453683444657944
06/01/2019 11:36:08 *** epoch: 107 ***
06/01/2019 11:36:08 *** training ***
06/01/2019 11:36:08 step: 3503, epoch: 106, batch: 4, loss: 0.08112110942602158, acc: 100.0, f1: 100.0, r: 0.6942288531250627
06/01/2019 11:36:09 step: 3508, epoch: 106, batch: 9, loss: 0.020034968852996826, acc: 100.0, f1: 100.0, r: 0.681775713207719
06/01/2019 11:36:10 step: 3513, epoch: 106, batch: 14, loss: 0.02868526615202427, acc: 100.0, f1: 100.0, r: 0.6944045831859397
06/01/2019 11:36:10 step: 3518, epoch: 106, batch: 19, loss: 0.04367914795875549, acc: 100.0, f1: 100.0, r: 0.7408271710727545
06/01/2019 11:36:11 step: 3523, epoch: 106, batch: 24, loss: 0.03817310556769371, acc: 98.4375, f1: 97.81105990783409, r: 0.7359884217321648
06/01/2019 11:36:11 step: 3528, epoch: 106, batch: 29, loss: 0.045528020709753036, acc: 100.0, f1: 100.0, r: 0.7976598771477201
06/01/2019 11:36:12 *** evaluating ***
06/01/2019 11:36:12 step: 107, epoch: 106, acc: 55.12820512820513, f1: 25.119256685431495, r: 0.2641011576050517
06/01/2019 11:36:12 *** epoch: 108 ***
06/01/2019 11:36:12 *** training ***
06/01/2019 11:36:13 step: 3536, epoch: 107, batch: 4, loss: 0.059649743139743805, acc: 98.4375, f1: 98.40848806366047, r: 0.7310326331879261
06/01/2019 11:36:13 step: 3541, epoch: 107, batch: 9, loss: 0.01694999635219574, acc: 100.0, f1: 100.0, r: 0.8359399171300591
06/01/2019 11:36:14 step: 3546, epoch: 107, batch: 14, loss: 0.43453630805015564, acc: 100.0, f1: 100.0, r: 0.7459363515798466
06/01/2019 11:36:15 step: 3551, epoch: 107, batch: 19, loss: 0.029900535941123962, acc: 100.0, f1: 100.0, r: 0.6518862592053973
06/01/2019 11:36:15 step: 3556, epoch: 107, batch: 24, loss: 0.031623128801584244, acc: 100.0, f1: 100.0, r: 0.7446232289811964
06/01/2019 11:36:16 step: 3561, epoch: 107, batch: 29, loss: 0.6908263564109802, acc: 100.0, f1: 100.0, r: 0.7020335652190348
06/01/2019 11:36:16 *** evaluating ***
06/01/2019 11:36:16 step: 108, epoch: 107, acc: 55.98290598290598, f1: 25.48434773220046, r: 0.2638986156142105
06/01/2019 11:36:16 *** epoch: 109 ***
06/01/2019 11:36:16 *** training ***
06/01/2019 11:36:17 step: 3569, epoch: 108, batch: 4, loss: 0.02559943124651909, acc: 100.0, f1: 100.0, r: 0.697574569871436
06/01/2019 11:36:18 step: 3574, epoch: 108, batch: 9, loss: 0.04588568955659866, acc: 100.0, f1: 100.0, r: 0.7317695089041849
06/01/2019 11:36:18 step: 3579, epoch: 108, batch: 14, loss: 0.053063251078128815, acc: 98.4375, f1: 93.65079365079364, r: 0.6772527022867623
06/01/2019 11:36:19 step: 3584, epoch: 108, batch: 19, loss: 0.05362170189619064, acc: 98.4375, f1: 98.05128205128204, r: 0.6718943599442239
06/01/2019 11:36:20 step: 3589, epoch: 108, batch: 24, loss: 0.02678496390581131, acc: 100.0, f1: 100.0, r: 0.8170989905024579
06/01/2019 11:36:20 step: 3594, epoch: 108, batch: 29, loss: 0.026473987847566605, acc: 100.0, f1: 100.0, r: 0.7251305334931337
06/01/2019 11:36:20 *** evaluating ***
06/01/2019 11:36:21 step: 109, epoch: 108, acc: 55.98290598290598, f1: 25.60040657536593, r: 0.2666102086160688
06/01/2019 11:36:21 *** epoch: 110 ***
06/01/2019 11:36:21 *** training ***
06/01/2019 11:36:21 step: 3602, epoch: 109, batch: 4, loss: 0.08455710858106613, acc: 100.0, f1: 100.0, r: 0.7288353321409834
06/01/2019 11:36:22 step: 3607, epoch: 109, batch: 9, loss: 0.024177201092243195, acc: 100.0, f1: 100.0, r: 0.7938733002112628
06/01/2019 11:36:23 step: 3612, epoch: 109, batch: 14, loss: 0.036241162568330765, acc: 100.0, f1: 100.0, r: 0.7854348366467303
06/01/2019 11:36:23 step: 3617, epoch: 109, batch: 19, loss: 0.09607692062854767, acc: 98.4375, f1: 97.61904761904762, r: 0.7165204533767583
06/01/2019 11:36:24 step: 3622, epoch: 109, batch: 24, loss: 0.4638981521129608, acc: 100.0, f1: 100.0, r: 0.7052409614747738
06/01/2019 11:36:25 step: 3627, epoch: 109, batch: 29, loss: 0.028483517467975616, acc: 100.0, f1: 100.0, r: 0.7140005425919436
06/01/2019 11:36:25 *** evaluating ***
06/01/2019 11:36:25 step: 110, epoch: 109, acc: 55.55555555555556, f1: 25.615215601799115, r: 0.26478959934774615
06/01/2019 11:36:25 *** epoch: 111 ***
06/01/2019 11:36:25 *** training ***
06/01/2019 11:36:26 step: 3635, epoch: 110, batch: 4, loss: 0.0506925955414772, acc: 100.0, f1: 100.0, r: 0.8492365056937815
06/01/2019 11:36:26 step: 3640, epoch: 110, batch: 9, loss: 0.03520814701914787, acc: 100.0, f1: 100.0, r: 0.7332550152880554
06/01/2019 11:36:27 step: 3645, epoch: 110, batch: 14, loss: 0.03119141235947609, acc: 100.0, f1: 100.0, r: 0.7502205895442131
06/01/2019 11:36:28 step: 3650, epoch: 110, batch: 19, loss: 0.03500789403915405, acc: 100.0, f1: 100.0, r: 0.771211232705031
06/01/2019 11:36:28 step: 3655, epoch: 110, batch: 24, loss: 0.06180654093623161, acc: 100.0, f1: 100.0, r: 0.8079584406547681
06/01/2019 11:36:29 step: 3660, epoch: 110, batch: 29, loss: 0.04420151934027672, acc: 100.0, f1: 100.0, r: 0.6660730775293866
06/01/2019 11:36:29 *** evaluating ***
06/01/2019 11:36:30 step: 111, epoch: 110, acc: 56.41025641025641, f1: 25.734352453102456, r: 0.26760898179230863
06/01/2019 11:36:30 *** epoch: 112 ***
06/01/2019 11:36:30 *** training ***
06/01/2019 11:36:30 step: 3668, epoch: 111, batch: 4, loss: 0.025143355131149292, acc: 100.0, f1: 100.0, r: 0.8132116240856632
06/01/2019 11:36:31 step: 3673, epoch: 111, batch: 9, loss: 0.06339149177074432, acc: 100.0, f1: 100.0, r: 0.7312295115857425
06/01/2019 11:36:32 step: 3678, epoch: 111, batch: 14, loss: 0.022968435660004616, acc: 100.0, f1: 100.0, r: 0.6676862257167381
06/01/2019 11:36:32 step: 3683, epoch: 111, batch: 19, loss: 0.04849160462617874, acc: 100.0, f1: 100.0, r: 0.7172942669068363
06/01/2019 11:36:33 step: 3688, epoch: 111, batch: 24, loss: 0.03480622544884682, acc: 100.0, f1: 100.0, r: 0.6782898105635572
06/01/2019 11:36:34 step: 3693, epoch: 111, batch: 29, loss: 0.39712151885032654, acc: 98.4375, f1: 98.1111111111111, r: 0.748003197474874
06/01/2019 11:36:34 *** evaluating ***
06/01/2019 11:36:34 step: 112, epoch: 111, acc: 55.98290598290598, f1: 25.186373293941912, r: 0.2673521232620424
06/01/2019 11:36:34 *** epoch: 113 ***
06/01/2019 11:36:34 *** training ***
06/01/2019 11:36:35 step: 3701, epoch: 112, batch: 4, loss: 0.07035885751247406, acc: 100.0, f1: 100.0, r: 0.8101209530669428
06/01/2019 11:36:35 step: 3706, epoch: 112, batch: 9, loss: 0.03514253720641136, acc: 98.4375, f1: 95.10204081632652, r: 0.7188054446482
06/01/2019 11:36:36 step: 3711, epoch: 112, batch: 14, loss: 0.01840217038989067, acc: 100.0, f1: 100.0, r: 0.8307166617332344
06/01/2019 11:36:37 step: 3716, epoch: 112, batch: 19, loss: 0.6837549209594727, acc: 100.0, f1: 100.0, r: 0.6845209520712513
06/01/2019 11:36:37 step: 3721, epoch: 112, batch: 24, loss: 0.010004203766584396, acc: 100.0, f1: 100.0, r: 0.7777623937682528
06/01/2019 11:36:38 step: 3726, epoch: 112, batch: 29, loss: 0.37981462478637695, acc: 100.0, f1: 100.0, r: 0.721374571069977
06/01/2019 11:36:38 *** evaluating ***
06/01/2019 11:36:39 step: 113, epoch: 112, acc: 54.27350427350427, f1: 24.85700006110583, r: 0.265980339608418
06/01/2019 11:36:39 *** epoch: 114 ***
06/01/2019 11:36:39 *** training ***
06/01/2019 11:36:39 step: 3734, epoch: 113, batch: 4, loss: 0.02349642664194107, acc: 100.0, f1: 100.0, r: 0.6957843266087606
06/01/2019 11:36:40 step: 3739, epoch: 113, batch: 9, loss: 0.017568308860063553, acc: 100.0, f1: 100.0, r: 0.7723200787822635
06/01/2019 11:36:41 step: 3744, epoch: 113, batch: 14, loss: 0.7108460664749146, acc: 98.4375, f1: 96.04395604395604, r: 0.6626052780076703
06/01/2019 11:36:41 step: 3749, epoch: 113, batch: 19, loss: 0.030094318091869354, acc: 100.0, f1: 100.0, r: 0.7184158914498362
06/01/2019 11:36:42 step: 3754, epoch: 113, batch: 24, loss: 0.09213323891162872, acc: 96.875, f1: 95.58690351793801, r: 0.5746399936825798
06/01/2019 11:36:43 step: 3759, epoch: 113, batch: 29, loss: 0.030128110200166702, acc: 100.0, f1: 100.0, r: 0.8126936124876808
06/01/2019 11:36:43 *** evaluating ***
06/01/2019 11:36:43 step: 114, epoch: 113, acc: 55.12820512820513, f1: 24.88388347763348, r: 0.2662915415022578
06/01/2019 11:36:43 *** epoch: 115 ***
06/01/2019 11:36:43 *** training ***
06/01/2019 11:36:44 step: 3767, epoch: 114, batch: 4, loss: 0.02839973196387291, acc: 100.0, f1: 100.0, r: 0.8111695906852306
06/01/2019 11:36:45 step: 3772, epoch: 114, batch: 9, loss: 0.018571458756923676, acc: 100.0, f1: 100.0, r: 0.6684529644835392
06/01/2019 11:36:45 step: 3777, epoch: 114, batch: 14, loss: 0.4263472855091095, acc: 100.0, f1: 100.0, r: 0.7070468371946618
06/01/2019 11:36:46 step: 3782, epoch: 114, batch: 19, loss: 0.03317701816558838, acc: 100.0, f1: 100.0, r: 0.8423038063355124
06/01/2019 11:36:46 step: 3787, epoch: 114, batch: 24, loss: 0.031798332929611206, acc: 100.0, f1: 100.0, r: 0.7887113346666763
06/01/2019 11:36:47 step: 3792, epoch: 114, batch: 29, loss: 0.13040503859519958, acc: 100.0, f1: 100.0, r: 0.6873578565785288
06/01/2019 11:36:47 *** evaluating ***
06/01/2019 11:36:47 step: 115, epoch: 114, acc: 56.837606837606835, f1: 26.42118411291457, r: 0.2699911466105769
06/01/2019 11:36:47 *** epoch: 116 ***
06/01/2019 11:36:47 *** training ***
06/01/2019 11:36:48 step: 3800, epoch: 115, batch: 4, loss: 0.05614830181002617, acc: 100.0, f1: 100.0, r: 0.7897031823884573
06/01/2019 11:36:49 step: 3805, epoch: 115, batch: 9, loss: 0.05080986022949219, acc: 98.4375, f1: 98.29313543599258, r: 0.6296583003028533
06/01/2019 11:36:49 step: 3810, epoch: 115, batch: 14, loss: 0.04139350354671478, acc: 100.0, f1: 100.0, r: 0.7553853073077924
06/01/2019 11:36:50 step: 3815, epoch: 115, batch: 19, loss: 0.029516447335481644, acc: 100.0, f1: 100.0, r: 0.7801521020367186
06/01/2019 11:36:50 step: 3820, epoch: 115, batch: 24, loss: 0.061915431171655655, acc: 96.875, f1: 95.58747412008282, r: 0.8161290655453772
06/01/2019 11:36:51 step: 3825, epoch: 115, batch: 29, loss: 0.03745370730757713, acc: 100.0, f1: 100.0, r: 0.7272036535740791
06/01/2019 11:36:52 *** evaluating ***
06/01/2019 11:36:52 step: 116, epoch: 115, acc: 55.55555555555556, f1: 25.16046925245584, r: 0.26931089610061876
06/01/2019 11:36:52 *** epoch: 117 ***
06/01/2019 11:36:52 *** training ***
06/01/2019 11:36:52 step: 3833, epoch: 116, batch: 4, loss: 0.03007410280406475, acc: 100.0, f1: 100.0, r: 0.8261737071197568
06/01/2019 11:36:53 step: 3838, epoch: 116, batch: 9, loss: 0.38461631536483765, acc: 100.0, f1: 100.0, r: 0.7735689887084345
06/01/2019 11:36:54 step: 3843, epoch: 116, batch: 14, loss: 0.020051587373018265, acc: 100.0, f1: 100.0, r: 0.6902066211639423
06/01/2019 11:36:55 step: 3848, epoch: 116, batch: 19, loss: 0.04487771913409233, acc: 100.0, f1: 100.0, r: 0.582799051295038
06/01/2019 11:36:55 step: 3853, epoch: 116, batch: 24, loss: 0.07714296877384186, acc: 98.4375, f1: 97.6608187134503, r: 0.724822927062053
06/01/2019 11:36:56 step: 3858, epoch: 116, batch: 29, loss: 0.013131075538694859, acc: 100.0, f1: 100.0, r: 0.7476901518082849
06/01/2019 11:36:56 *** evaluating ***
06/01/2019 11:36:56 step: 117, epoch: 116, acc: 55.55555555555556, f1: 25.52501572946661, r: 0.2634975977828038
06/01/2019 11:36:56 *** epoch: 118 ***
06/01/2019 11:36:56 *** training ***
06/01/2019 11:36:57 step: 3866, epoch: 117, batch: 4, loss: 0.0392792746424675, acc: 100.0, f1: 100.0, r: 0.5347897317623648
06/01/2019 11:36:58 step: 3871, epoch: 117, batch: 9, loss: 0.03229402378201485, acc: 100.0, f1: 100.0, r: 0.8390135166010922
06/01/2019 11:36:58 step: 3876, epoch: 117, batch: 14, loss: 0.02895207516849041, acc: 100.0, f1: 100.0, r: 0.6938340642085289
06/01/2019 11:36:59 step: 3881, epoch: 117, batch: 19, loss: 0.023552628234028816, acc: 100.0, f1: 100.0, r: 0.6849435720381796
06/01/2019 11:37:00 step: 3886, epoch: 117, batch: 24, loss: 0.027271557599306107, acc: 100.0, f1: 100.0, r: 0.7638416854212216
06/01/2019 11:37:00 step: 3891, epoch: 117, batch: 29, loss: 0.7251636385917664, acc: 100.0, f1: 100.0, r: 0.6933420553747691
06/01/2019 11:37:01 *** evaluating ***
06/01/2019 11:37:01 step: 118, epoch: 117, acc: 55.12820512820513, f1: 25.29739638082732, r: 0.26691519100710126
06/01/2019 11:37:01 *** epoch: 119 ***
06/01/2019 11:37:01 *** training ***
06/01/2019 11:37:01 step: 3899, epoch: 118, batch: 4, loss: 0.03013615496456623, acc: 100.0, f1: 100.0, r: 0.7778839276243779
06/01/2019 11:37:02 step: 3904, epoch: 118, batch: 9, loss: 0.0197441466152668, acc: 100.0, f1: 100.0, r: 0.6998438396976846
06/01/2019 11:37:03 step: 3909, epoch: 118, batch: 14, loss: 0.4657563865184784, acc: 98.4375, f1: 95.51282051282051, r: 0.8014848179301064
06/01/2019 11:37:04 step: 3914, epoch: 118, batch: 19, loss: 0.6885910034179688, acc: 100.0, f1: 100.0, r: 0.6724099695018957
06/01/2019 11:37:04 step: 3919, epoch: 118, batch: 24, loss: 0.009709101170301437, acc: 100.0, f1: 100.0, r: 0.8152298257275621
06/01/2019 11:37:05 step: 3924, epoch: 118, batch: 29, loss: 0.378348171710968, acc: 100.0, f1: 100.0, r: 0.7835762238039662
06/01/2019 11:37:05 *** evaluating ***
06/01/2019 11:37:06 step: 119, epoch: 118, acc: 55.98290598290598, f1: 25.47916557072341, r: 0.27083928443386635
06/01/2019 11:37:06 *** epoch: 120 ***
06/01/2019 11:37:06 *** training ***
06/01/2019 11:37:06 step: 3932, epoch: 119, batch: 4, loss: 0.048313356935977936, acc: 100.0, f1: 100.0, r: 0.8065240855521955
06/01/2019 11:37:07 step: 3937, epoch: 119, batch: 9, loss: 0.04252392426133156, acc: 98.4375, f1: 99.16694810311832, r: 0.7664769848952354
06/01/2019 11:37:07 step: 3942, epoch: 119, batch: 14, loss: 0.014153610914945602, acc: 100.0, f1: 100.0, r: 0.7852683137225782
06/01/2019 11:37:08 step: 3947, epoch: 119, batch: 19, loss: 0.028436949476599693, acc: 100.0, f1: 100.0, r: 0.7237216251674387
06/01/2019 11:37:09 step: 3952, epoch: 119, batch: 24, loss: 0.0477738156914711, acc: 100.0, f1: 100.0, r: 0.8192100512023269
06/01/2019 11:37:09 step: 3957, epoch: 119, batch: 29, loss: 0.04389678314328194, acc: 100.0, f1: 100.0, r: 0.702060725337932
06/01/2019 11:37:10 *** evaluating ***
06/01/2019 11:37:10 step: 120, epoch: 119, acc: 55.98290598290598, f1: 25.49866764106455, r: 0.26815613311259406
06/01/2019 11:37:10 *** epoch: 121 ***
06/01/2019 11:37:10 *** training ***
06/01/2019 11:37:11 step: 3965, epoch: 120, batch: 4, loss: 0.07777903228998184, acc: 100.0, f1: 100.0, r: 0.7028375717405283
06/01/2019 11:37:11 step: 3970, epoch: 120, batch: 9, loss: 0.4327767491340637, acc: 100.0, f1: 100.0, r: 0.8330134810577646
06/01/2019 11:37:12 step: 3975, epoch: 120, batch: 14, loss: 0.04631635919213295, acc: 100.0, f1: 100.0, r: 0.7219793860493717
06/01/2019 11:37:13 step: 3980, epoch: 120, batch: 19, loss: 0.014222173020243645, acc: 100.0, f1: 100.0, r: 0.8270145335442276
06/01/2019 11:37:13 step: 3985, epoch: 120, batch: 24, loss: 0.045390255749225616, acc: 96.875, f1: 93.85865457294028, r: 0.7030825535159859
06/01/2019 11:37:14 step: 3990, epoch: 120, batch: 29, loss: 0.025079626590013504, acc: 100.0, f1: 100.0, r: 0.7062413406773734
06/01/2019 11:37:14 *** evaluating ***
06/01/2019 11:37:14 step: 121, epoch: 120, acc: 56.41025641025641, f1: 25.668960098977706, r: 0.269681196585139
06/01/2019 11:37:14 *** epoch: 122 ***
06/01/2019 11:37:14 *** training ***
06/01/2019 11:37:15 step: 3998, epoch: 121, batch: 4, loss: 0.01568729616701603, acc: 100.0, f1: 100.0, r: 0.7837074890539568
06/01/2019 11:37:16 step: 4003, epoch: 121, batch: 9, loss: 0.021153824403882027, acc: 100.0, f1: 100.0, r: 0.812101834091371
06/01/2019 11:37:16 step: 4008, epoch: 121, batch: 14, loss: 0.019354676827788353, acc: 100.0, f1: 100.0, r: 0.688019304338248
06/01/2019 11:37:17 step: 4013, epoch: 121, batch: 19, loss: 0.04873108118772507, acc: 100.0, f1: 100.0, r: 0.6873046585404448
06/01/2019 11:37:17 step: 4018, epoch: 121, batch: 24, loss: 0.037897445261478424, acc: 100.0, f1: 100.0, r: 0.7722675916030646
06/01/2019 11:37:18 step: 4023, epoch: 121, batch: 29, loss: 0.04654260352253914, acc: 100.0, f1: 100.0, r: 0.8533729584641161
06/01/2019 11:37:18 *** evaluating ***
06/01/2019 11:37:19 step: 122, epoch: 121, acc: 55.98290598290598, f1: 25.5366847826087, r: 0.2717231161002197
06/01/2019 11:37:19 *** epoch: 123 ***
06/01/2019 11:37:19 *** training ***
06/01/2019 11:37:19 step: 4031, epoch: 122, batch: 4, loss: 0.02006721682846546, acc: 100.0, f1: 100.0, r: 0.8062828305499391
06/01/2019 11:37:20 step: 4036, epoch: 122, batch: 9, loss: 0.03889106959104538, acc: 100.0, f1: 100.0, r: 0.6721127850603054
06/01/2019 11:37:21 step: 4041, epoch: 122, batch: 14, loss: 0.02082168310880661, acc: 100.0, f1: 100.0, r: 0.7439892720465922
06/01/2019 11:37:21 step: 4046, epoch: 122, batch: 19, loss: 0.044363878667354584, acc: 98.4375, f1: 98.44026940801133, r: 0.7031402001082233
06/01/2019 11:37:22 step: 4051, epoch: 122, batch: 24, loss: 0.02920900098979473, acc: 100.0, f1: 100.0, r: 0.8381571057382952
06/01/2019 11:37:22 step: 4056, epoch: 122, batch: 29, loss: 0.049029357731342316, acc: 98.4375, f1: 99.32517263025737, r: 0.8054852879458044
06/01/2019 11:37:23 *** evaluating ***
06/01/2019 11:37:23 step: 123, epoch: 122, acc: 55.55555555555556, f1: 24.991908477081857, r: 0.27069340134705877
06/01/2019 11:37:23 *** epoch: 124 ***
06/01/2019 11:37:23 *** training ***
06/01/2019 11:37:23 step: 4064, epoch: 123, batch: 4, loss: 0.025334343314170837, acc: 100.0, f1: 100.0, r: 0.7678278689957214
06/01/2019 11:37:24 step: 4069, epoch: 123, batch: 9, loss: 0.034869760274887085, acc: 100.0, f1: 100.0, r: 0.6936751990157006
06/01/2019 11:37:25 step: 4074, epoch: 123, batch: 14, loss: 0.030950376763939857, acc: 100.0, f1: 100.0, r: 0.8180527856659334
06/01/2019 11:37:25 step: 4079, epoch: 123, batch: 19, loss: 0.02098815143108368, acc: 100.0, f1: 100.0, r: 0.7461264459552002
06/01/2019 11:37:26 step: 4084, epoch: 123, batch: 24, loss: 0.031131476163864136, acc: 100.0, f1: 100.0, r: 0.7195066720877021
06/01/2019 11:37:27 step: 4089, epoch: 123, batch: 29, loss: 0.02014184184372425, acc: 100.0, f1: 100.0, r: 0.6980108349703198
06/01/2019 11:37:27 *** evaluating ***
06/01/2019 11:37:27 step: 124, epoch: 123, acc: 55.98290598290598, f1: 26.19097933045802, r: 0.2814386243229985
06/01/2019 11:37:27 *** epoch: 125 ***
06/01/2019 11:37:27 *** training ***
06/01/2019 11:37:28 step: 4097, epoch: 124, batch: 4, loss: 0.014485636726021767, acc: 100.0, f1: 100.0, r: 0.6661545141555206
06/01/2019 11:37:29 step: 4102, epoch: 124, batch: 9, loss: 0.026044027879834175, acc: 98.4375, f1: 95.10204081632652, r: 0.7531511891231981
06/01/2019 11:37:29 step: 4107, epoch: 124, batch: 14, loss: 0.01661444827914238, acc: 100.0, f1: 100.0, r: 0.7068785252348034
06/01/2019 11:37:30 step: 4112, epoch: 124, batch: 19, loss: 0.023747485131025314, acc: 100.0, f1: 100.0, r: 0.6064590681748157
06/01/2019 11:37:31 step: 4117, epoch: 124, batch: 24, loss: 0.048254117369651794, acc: 100.0, f1: 100.0, r: 0.799112705520548
06/01/2019 11:37:31 step: 4122, epoch: 124, batch: 29, loss: 0.04480423405766487, acc: 98.4375, f1: 94.66666666666667, r: 0.6768248712889222
06/01/2019 11:37:31 *** evaluating ***
06/01/2019 11:37:32 step: 125, epoch: 124, acc: 54.700854700854705, f1: 25.14639929025942, r: 0.26679848656389765
06/01/2019 11:37:32 *** epoch: 126 ***
06/01/2019 11:37:32 *** training ***
06/01/2019 11:37:32 step: 4130, epoch: 125, batch: 4, loss: 0.019601816311478615, acc: 100.0, f1: 100.0, r: 0.7062337734459726
06/01/2019 11:37:33 step: 4135, epoch: 125, batch: 9, loss: 0.4270473122596741, acc: 100.0, f1: 100.0, r: 0.7063539711582021
06/01/2019 11:37:33 step: 4140, epoch: 125, batch: 14, loss: 0.4119126796722412, acc: 100.0, f1: 100.0, r: 0.7662434672916522
06/01/2019 11:37:34 step: 4145, epoch: 125, batch: 19, loss: 0.02238021418452263, acc: 100.0, f1: 100.0, r: 0.8098651218500252
06/01/2019 11:37:35 step: 4150, epoch: 125, batch: 24, loss: 0.031678345054388046, acc: 100.0, f1: 100.0, r: 0.7236041624057652
06/01/2019 11:37:35 step: 4155, epoch: 125, batch: 29, loss: 0.01675092801451683, acc: 100.0, f1: 100.0, r: 0.7177776898152959
06/01/2019 11:37:36 *** evaluating ***
06/01/2019 11:37:36 step: 126, epoch: 125, acc: 54.700854700854705, f1: 25.254642295002476, r: 0.2628600335433312
06/01/2019 11:37:36 *** epoch: 127 ***
06/01/2019 11:37:36 *** training ***
06/01/2019 11:37:37 step: 4163, epoch: 126, batch: 4, loss: 0.04572141170501709, acc: 98.4375, f1: 93.19727891156464, r: 0.6854211439903156
06/01/2019 11:37:37 step: 4168, epoch: 126, batch: 9, loss: 0.020107630640268326, acc: 100.0, f1: 100.0, r: 0.6477297053624822
06/01/2019 11:37:38 step: 4173, epoch: 126, batch: 14, loss: 0.012373578734695911, acc: 100.0, f1: 100.0, r: 0.7697433000914194
06/01/2019 11:37:38 step: 4178, epoch: 126, batch: 19, loss: 0.03449719026684761, acc: 98.4375, f1: 94.28571428571429, r: 0.701797785070264
06/01/2019 11:37:39 step: 4183, epoch: 126, batch: 24, loss: 0.03781573474407196, acc: 100.0, f1: 100.0, r: 0.8227114149020124
06/01/2019 11:37:40 step: 4188, epoch: 126, batch: 29, loss: 0.03746386989951134, acc: 100.0, f1: 100.0, r: 0.6095250008338254
06/01/2019 11:37:40 *** evaluating ***
06/01/2019 11:37:40 step: 127, epoch: 126, acc: 55.12820512820513, f1: 25.38435400727786, r: 0.2640001545499614
06/01/2019 11:37:40 *** epoch: 128 ***
06/01/2019 11:37:40 *** training ***
06/01/2019 11:37:41 step: 4196, epoch: 127, batch: 4, loss: 0.02813158929347992, acc: 100.0, f1: 100.0, r: 0.6985672249937964
06/01/2019 11:37:41 step: 4201, epoch: 127, batch: 9, loss: 0.017002632841467857, acc: 100.0, f1: 100.0, r: 0.7028148764660375
06/01/2019 11:37:42 step: 4206, epoch: 127, batch: 14, loss: 0.0629611536860466, acc: 96.875, f1: 94.95373907138614, r: 0.7719927744895454
06/01/2019 11:37:43 step: 4211, epoch: 127, batch: 19, loss: 0.018757473677396774, acc: 100.0, f1: 100.0, r: 0.7462234563398205
06/01/2019 11:37:43 step: 4216, epoch: 127, batch: 24, loss: 0.04760073125362396, acc: 100.0, f1: 100.0, r: 0.8256822329849729
06/01/2019 11:37:44 step: 4221, epoch: 127, batch: 29, loss: 0.03414111211895943, acc: 100.0, f1: 100.0, r: 0.8123842186973691
06/01/2019 11:37:45 *** evaluating ***
06/01/2019 11:37:45 step: 128, epoch: 127, acc: 55.55555555555556, f1: 25.305557293223323, r: 0.26299338386763943
06/01/2019 11:37:45 *** epoch: 129 ***
06/01/2019 11:37:45 *** training ***
06/01/2019 11:37:45 step: 4229, epoch: 128, batch: 4, loss: 0.04374965280294418, acc: 100.0, f1: 100.0, r: 0.6751603961482163
06/01/2019 11:37:46 step: 4234, epoch: 128, batch: 9, loss: 0.018699631094932556, acc: 100.0, f1: 100.0, r: 0.7090689349186805
06/01/2019 11:37:47 step: 4239, epoch: 128, batch: 14, loss: 0.06192668527364731, acc: 98.4375, f1: 98.06076276664513, r: 0.7318137962851222
06/01/2019 11:37:47 step: 4244, epoch: 128, batch: 19, loss: 0.03185395896434784, acc: 100.0, f1: 100.0, r: 0.6474546454081481
06/01/2019 11:37:48 step: 4249, epoch: 128, batch: 24, loss: 0.046012528240680695, acc: 100.0, f1: 100.0, r: 0.6918031512116886
06/01/2019 11:37:49 step: 4254, epoch: 128, batch: 29, loss: 0.05609015375375748, acc: 98.4375, f1: 93.33333333333333, r: 0.7913108270861275
06/01/2019 11:37:49 *** evaluating ***
06/01/2019 11:37:49 step: 129, epoch: 128, acc: 52.56410256410257, f1: 23.831555761870213, r: 0.2588187445889787
06/01/2019 11:37:49 *** epoch: 130 ***
06/01/2019 11:37:49 *** training ***
06/01/2019 11:37:50 step: 4262, epoch: 129, batch: 4, loss: 0.018575355410575867, acc: 100.0, f1: 100.0, r: 0.7527536360423462
06/01/2019 11:37:50 step: 4267, epoch: 129, batch: 9, loss: 0.031591854989528656, acc: 100.0, f1: 100.0, r: 0.689010034700927
06/01/2019 11:37:51 step: 4272, epoch: 129, batch: 14, loss: 0.03131582960486412, acc: 100.0, f1: 100.0, r: 0.7193467916403247
06/01/2019 11:37:51 step: 4277, epoch: 129, batch: 19, loss: 0.02121851034462452, acc: 100.0, f1: 100.0, r: 0.7251773128506311
06/01/2019 11:37:52 step: 4282, epoch: 129, batch: 24, loss: 0.02494768612086773, acc: 100.0, f1: 100.0, r: 0.8301514399867201
06/01/2019 11:37:53 step: 4287, epoch: 129, batch: 29, loss: 0.07454172521829605, acc: 98.4375, f1: 98.14814814814814, r: 0.7883283471370186
06/01/2019 11:37:53 *** evaluating ***
06/01/2019 11:37:53 step: 130, epoch: 129, acc: 53.84615384615385, f1: 24.83080328905756, r: 0.2651819455892289
06/01/2019 11:37:53 *** epoch: 131 ***
06/01/2019 11:37:53 *** training ***
06/01/2019 11:37:54 step: 4295, epoch: 130, batch: 4, loss: 0.013316700235009193, acc: 100.0, f1: 100.0, r: 0.6735972334486996
06/01/2019 11:37:55 step: 4300, epoch: 130, batch: 9, loss: 0.01866021938621998, acc: 100.0, f1: 100.0, r: 0.7826108493540297
06/01/2019 11:37:55 step: 4305, epoch: 130, batch: 14, loss: 0.030441101640462875, acc: 100.0, f1: 100.0, r: 0.6328858787781931
06/01/2019 11:37:56 step: 4310, epoch: 130, batch: 19, loss: 0.01880030706524849, acc: 100.0, f1: 100.0, r: 0.6712376006869304
06/01/2019 11:37:56 step: 4315, epoch: 130, batch: 24, loss: 0.032156288623809814, acc: 100.0, f1: 100.0, r: 0.7341803258001646
06/01/2019 11:37:57 step: 4320, epoch: 130, batch: 29, loss: 0.03225350379943848, acc: 98.4375, f1: 98.12987012987013, r: 0.7667562438537778
06/01/2019 11:37:57 *** evaluating ***
06/01/2019 11:37:58 step: 131, epoch: 130, acc: 54.27350427350427, f1: 24.804348773002022, r: 0.24246200785874641
06/01/2019 11:37:58 *** epoch: 132 ***
06/01/2019 11:37:58 *** training ***
06/01/2019 11:37:58 step: 4328, epoch: 131, batch: 4, loss: 0.017159033566713333, acc: 100.0, f1: 100.0, r: 0.7689133680570661
06/01/2019 11:37:59 step: 4333, epoch: 131, batch: 9, loss: 0.07529652118682861, acc: 100.0, f1: 100.0, r: 0.8001100440126293
06/01/2019 11:38:00 step: 4338, epoch: 131, batch: 14, loss: 0.021089110523462296, acc: 100.0, f1: 100.0, r: 0.7152650636288534
06/01/2019 11:38:00 step: 4343, epoch: 131, batch: 19, loss: 0.024158833548426628, acc: 100.0, f1: 100.0, r: 0.7914919435841621
06/01/2019 11:38:01 step: 4348, epoch: 131, batch: 24, loss: 0.7369745969772339, acc: 96.875, f1: 94.5, r: 0.8539669277041917
06/01/2019 11:38:01 step: 4353, epoch: 131, batch: 29, loss: 0.024882696568965912, acc: 100.0, f1: 100.0, r: 0.7345000491740293
06/01/2019 11:38:02 *** evaluating ***
06/01/2019 11:38:02 step: 132, epoch: 131, acc: 54.27350427350427, f1: 24.512883657089937, r: 0.25747824365743754
06/01/2019 11:38:02 *** epoch: 133 ***
06/01/2019 11:38:02 *** training ***
06/01/2019 11:38:02 step: 4361, epoch: 132, batch: 4, loss: 0.022396959364414215, acc: 100.0, f1: 100.0, r: 0.7547408640512474
06/01/2019 11:38:03 step: 4366, epoch: 132, batch: 9, loss: 0.014875215478241444, acc: 100.0, f1: 100.0, r: 0.598123181491632
06/01/2019 11:38:04 step: 4371, epoch: 132, batch: 14, loss: 0.03689677268266678, acc: 100.0, f1: 100.0, r: 0.6667418281266225
06/01/2019 11:38:04 step: 4376, epoch: 132, batch: 19, loss: 0.4142017364501953, acc: 100.0, f1: 100.0, r: 0.6735217355331069
06/01/2019 11:38:05 step: 4381, epoch: 132, batch: 24, loss: 0.012870339676737785, acc: 100.0, f1: 100.0, r: 0.7178933034998899
06/01/2019 11:38:06 step: 4386, epoch: 132, batch: 29, loss: 0.38505735993385315, acc: 100.0, f1: 100.0, r: 0.7161950328207903
06/01/2019 11:38:06 *** evaluating ***
06/01/2019 11:38:06 step: 133, epoch: 132, acc: 54.700854700854705, f1: 25.215170149667017, r: 0.26195670335733096
06/01/2019 11:38:06 *** epoch: 134 ***
06/01/2019 11:38:06 *** training ***
06/01/2019 11:38:07 step: 4394, epoch: 133, batch: 4, loss: 0.04655103385448456, acc: 100.0, f1: 100.0, r: 0.7798150487241258
06/01/2019 11:38:07 step: 4399, epoch: 133, batch: 9, loss: 0.06271108984947205, acc: 100.0, f1: 100.0, r: 0.7452192641664368
06/01/2019 11:38:08 step: 4404, epoch: 133, batch: 14, loss: 0.01861526072025299, acc: 100.0, f1: 100.0, r: 0.809844489446594
06/01/2019 11:38:09 step: 4409, epoch: 133, batch: 19, loss: 0.682996392250061, acc: 100.0, f1: 100.0, r: 0.7740325094651489
06/01/2019 11:38:09 step: 4414, epoch: 133, batch: 24, loss: 0.014361906796693802, acc: 100.0, f1: 100.0, r: 0.7277811245303111
06/01/2019 11:38:10 step: 4419, epoch: 133, batch: 29, loss: 0.022755565121769905, acc: 100.0, f1: 100.0, r: 0.8196281136803611
06/01/2019 11:38:10 *** evaluating ***
06/01/2019 11:38:10 step: 134, epoch: 133, acc: 55.98290598290598, f1: 25.621762082759524, r: 0.26585967833915525
06/01/2019 11:38:10 *** epoch: 135 ***
06/01/2019 11:38:10 *** training ***
06/01/2019 11:38:11 step: 4427, epoch: 134, batch: 4, loss: 0.024159379303455353, acc: 100.0, f1: 100.0, r: 0.8031510251968401
06/01/2019 11:38:12 step: 4432, epoch: 134, batch: 9, loss: 0.4225776791572571, acc: 100.0, f1: 100.0, r: 0.6915832381208789
06/01/2019 11:38:12 step: 4437, epoch: 134, batch: 14, loss: 0.010509900748729706, acc: 100.0, f1: 100.0, r: 0.7854214289745234
06/01/2019 11:38:13 step: 4442, epoch: 134, batch: 19, loss: 0.025109071284532547, acc: 100.0, f1: 100.0, r: 0.7268196589167127
06/01/2019 11:38:14 step: 4447, epoch: 134, batch: 24, loss: 0.04166890308260918, acc: 98.4375, f1: 98.49498327759197, r: 0.7655625580364347
06/01/2019 11:38:14 step: 4452, epoch: 134, batch: 29, loss: 0.027085669338703156, acc: 100.0, f1: 100.0, r: 0.7576639303534591
06/01/2019 11:38:15 *** evaluating ***
06/01/2019 11:38:15 step: 135, epoch: 134, acc: 55.98290598290598, f1: 25.49931968150053, r: 0.2666202075002061
06/01/2019 11:38:15 *** epoch: 136 ***
06/01/2019 11:38:15 *** training ***
06/01/2019 11:38:16 step: 4460, epoch: 135, batch: 4, loss: 0.012981733307242393, acc: 100.0, f1: 100.0, r: 0.781091026970482
06/01/2019 11:38:16 step: 4465, epoch: 135, batch: 9, loss: 0.039098478853702545, acc: 100.0, f1: 100.0, r: 0.7248520461793845
06/01/2019 11:38:17 step: 4470, epoch: 135, batch: 14, loss: 0.021488267928361893, acc: 100.0, f1: 100.0, r: 0.7565528778508891
06/01/2019 11:38:17 step: 4475, epoch: 135, batch: 19, loss: 0.024581875652074814, acc: 100.0, f1: 100.0, r: 0.7203432928839736
06/01/2019 11:38:18 step: 4480, epoch: 135, batch: 24, loss: 0.4404239356517792, acc: 100.0, f1: 100.0, r: 0.8401570648799016
06/01/2019 11:38:19 step: 4485, epoch: 135, batch: 29, loss: 0.03418172895908356, acc: 100.0, f1: 100.0, r: 0.7931108130734794
06/01/2019 11:38:19 *** evaluating ***
06/01/2019 11:38:19 step: 136, epoch: 135, acc: 52.13675213675214, f1: 25.877401983140473, r: 0.25641945966522917
06/01/2019 11:38:19 *** epoch: 137 ***
06/01/2019 11:38:19 *** training ***
06/01/2019 11:38:20 step: 4493, epoch: 136, batch: 4, loss: 0.02147737145423889, acc: 100.0, f1: 100.0, r: 0.6497068475844372
06/01/2019 11:38:20 step: 4498, epoch: 136, batch: 9, loss: 0.022677361965179443, acc: 100.0, f1: 100.0, r: 0.804937288481438
06/01/2019 11:38:21 step: 4503, epoch: 136, batch: 14, loss: 0.033939190208911896, acc: 100.0, f1: 100.0, r: 0.7084823454264125
06/01/2019 11:38:22 step: 4508, epoch: 136, batch: 19, loss: 0.03870762884616852, acc: 100.0, f1: 100.0, r: 0.7482201050553092
06/01/2019 11:38:22 step: 4513, epoch: 136, batch: 24, loss: 0.012828200124204159, acc: 100.0, f1: 100.0, r: 0.746543971873485
06/01/2019 11:38:23 step: 4518, epoch: 136, batch: 29, loss: 0.01662876456975937, acc: 100.0, f1: 100.0, r: 0.7168540844268065
06/01/2019 11:38:23 *** evaluating ***
06/01/2019 11:38:24 step: 137, epoch: 136, acc: 55.55555555555556, f1: 25.4055512768107, r: 0.26868672382351455
06/01/2019 11:38:24 *** epoch: 138 ***
06/01/2019 11:38:24 *** training ***
06/01/2019 11:38:24 step: 4526, epoch: 137, batch: 4, loss: 0.03739013895392418, acc: 98.4375, f1: 97.6608187134503, r: 0.666062758076047
06/01/2019 11:38:25 step: 4531, epoch: 137, batch: 9, loss: 0.02421410381793976, acc: 100.0, f1: 100.0, r: 0.7528229545251133
06/01/2019 11:38:25 step: 4536, epoch: 137, batch: 14, loss: 0.021155286580324173, acc: 100.0, f1: 100.0, r: 0.7716696528888718
06/01/2019 11:38:26 step: 4541, epoch: 137, batch: 19, loss: 0.024414513260126114, acc: 100.0, f1: 100.0, r: 0.7127828464686693
06/01/2019 11:38:27 step: 4546, epoch: 137, batch: 24, loss: 0.01691542938351631, acc: 100.0, f1: 100.0, r: 0.6753254408640582
06/01/2019 11:38:27 step: 4551, epoch: 137, batch: 29, loss: 0.0202907994389534, acc: 100.0, f1: 100.0, r: 0.7552567539252248
06/01/2019 11:38:28 *** evaluating ***
06/01/2019 11:38:28 step: 138, epoch: 137, acc: 56.41025641025641, f1: 25.788169351864365, r: 0.2626774399612505
06/01/2019 11:38:28 *** epoch: 139 ***
06/01/2019 11:38:28 *** training ***
06/01/2019 11:38:29 step: 4559, epoch: 138, batch: 4, loss: 0.016616620123386383, acc: 100.0, f1: 100.0, r: 0.8093631457386757
06/01/2019 11:38:29 step: 4564, epoch: 138, batch: 9, loss: 0.022156890481710434, acc: 100.0, f1: 100.0, r: 0.8214050667694038
06/01/2019 11:38:30 step: 4569, epoch: 138, batch: 14, loss: 0.014727449044585228, acc: 100.0, f1: 100.0, r: 0.7999259142437068
06/01/2019 11:38:31 step: 4574, epoch: 138, batch: 19, loss: 0.016798783093690872, acc: 100.0, f1: 100.0, r: 0.7254047373299158
06/01/2019 11:38:31 step: 4579, epoch: 138, batch: 24, loss: 0.013554745353758335, acc: 100.0, f1: 100.0, r: 0.8297603605256786
06/01/2019 11:38:32 step: 4584, epoch: 138, batch: 29, loss: 0.047695331275463104, acc: 98.4375, f1: 98.64433811802233, r: 0.8368565391253832
06/01/2019 11:38:32 *** evaluating ***
06/01/2019 11:38:32 step: 139, epoch: 138, acc: 56.41025641025641, f1: 25.803123312510046, r: 0.2683942401391726
06/01/2019 11:38:32 *** epoch: 140 ***
06/01/2019 11:38:32 *** training ***
06/01/2019 11:38:33 step: 4592, epoch: 139, batch: 4, loss: 0.019268356263637543, acc: 100.0, f1: 100.0, r: 0.6559082367984372
06/01/2019 11:38:34 step: 4597, epoch: 139, batch: 9, loss: 0.02532685361802578, acc: 100.0, f1: 100.0, r: 0.6877534200364664
06/01/2019 11:38:34 step: 4602, epoch: 139, batch: 14, loss: 0.015873916447162628, acc: 100.0, f1: 100.0, r: 0.6053574768609403
06/01/2019 11:38:35 step: 4607, epoch: 139, batch: 19, loss: 0.0237311739474535, acc: 100.0, f1: 100.0, r: 0.7363741535573758
06/01/2019 11:38:36 step: 4612, epoch: 139, batch: 24, loss: 0.025755684822797775, acc: 100.0, f1: 100.0, r: 0.8333702371489978
06/01/2019 11:38:36 step: 4617, epoch: 139, batch: 29, loss: 0.019924236461520195, acc: 100.0, f1: 100.0, r: 0.7704778277068566
06/01/2019 11:38:37 *** evaluating ***
06/01/2019 11:38:37 step: 140, epoch: 139, acc: 56.837606837606835, f1: 27.98245574580293, r: 0.2712032461083362
06/01/2019 11:38:37 *** epoch: 141 ***
06/01/2019 11:38:37 *** training ***
06/01/2019 11:38:37 step: 4625, epoch: 140, batch: 4, loss: 0.01699456013739109, acc: 100.0, f1: 100.0, r: 0.7549937611616734
06/01/2019 11:38:38 step: 4630, epoch: 140, batch: 9, loss: 0.024971721693873405, acc: 100.0, f1: 100.0, r: 0.6627265652449777
06/01/2019 11:38:39 step: 4635, epoch: 140, batch: 14, loss: 0.012008179910480976, acc: 100.0, f1: 100.0, r: 0.693631822237462
06/01/2019 11:38:39 step: 4640, epoch: 140, batch: 19, loss: 0.030875619500875473, acc: 100.0, f1: 100.0, r: 0.7630219098262927
06/01/2019 11:38:40 step: 4645, epoch: 140, batch: 24, loss: 0.01966285891830921, acc: 98.4375, f1: 96.9047619047619, r: 0.8072512334339609
06/01/2019 11:38:40 step: 4650, epoch: 140, batch: 29, loss: 0.033070940524339676, acc: 98.4375, f1: 85.18518518518519, r: 0.6981487331535987
06/01/2019 11:38:41 *** evaluating ***
06/01/2019 11:38:41 step: 141, epoch: 140, acc: 55.98290598290598, f1: 25.452798140532813, r: 0.2646163805935183
06/01/2019 11:38:41 *** epoch: 142 ***
06/01/2019 11:38:41 *** training ***
06/01/2019 11:38:42 step: 4658, epoch: 141, batch: 4, loss: 0.03201061487197876, acc: 100.0, f1: 100.0, r: 0.73637748567055
06/01/2019 11:38:42 step: 4663, epoch: 141, batch: 9, loss: 0.6762649416923523, acc: 100.0, f1: 100.0, r: 0.823205039274157
06/01/2019 11:38:43 step: 4668, epoch: 141, batch: 14, loss: 0.05231265351176262, acc: 100.0, f1: 100.0, r: 0.7336244528917815
06/01/2019 11:38:44 step: 4673, epoch: 141, batch: 19, loss: 0.02115495316684246, acc: 100.0, f1: 100.0, r: 0.7151359605833137
06/01/2019 11:38:44 step: 4678, epoch: 141, batch: 24, loss: 0.010468054562807083, acc: 100.0, f1: 100.0, r: 0.7872084366029248
06/01/2019 11:38:45 step: 4683, epoch: 141, batch: 29, loss: 0.021814167499542236, acc: 100.0, f1: 100.0, r: 0.7905348251403961
06/01/2019 11:38:45 *** evaluating ***
06/01/2019 11:38:46 step: 142, epoch: 141, acc: 56.41025641025641, f1: 25.734352453102456, r: 0.2659982251088535
06/01/2019 11:38:46 *** epoch: 143 ***
06/01/2019 11:38:46 *** training ***
06/01/2019 11:38:46 step: 4691, epoch: 142, batch: 4, loss: 0.016220491379499435, acc: 100.0, f1: 100.0, r: 0.7012607709150354
06/01/2019 11:38:47 step: 4696, epoch: 142, batch: 9, loss: 0.04474560171365738, acc: 98.4375, f1: 99.24963924963926, r: 0.7329379266799955
06/01/2019 11:38:47 step: 4701, epoch: 142, batch: 14, loss: 0.03848904371261597, acc: 98.4375, f1: 96.82539682539682, r: 0.7722404429498089
06/01/2019 11:38:48 step: 4706, epoch: 142, batch: 19, loss: 0.016147052869200706, acc: 100.0, f1: 100.0, r: 0.7127841901485497
06/01/2019 11:38:49 step: 4711, epoch: 142, batch: 24, loss: 0.01848267763853073, acc: 100.0, f1: 100.0, r: 0.7146156109682305
06/01/2019 11:38:49 step: 4716, epoch: 142, batch: 29, loss: 0.009073063731193542, acc: 100.0, f1: 100.0, r: 0.7030281621401395
06/01/2019 11:38:50 *** evaluating ***
06/01/2019 11:38:50 step: 143, epoch: 142, acc: 55.55555555555556, f1: 25.411175516834163, r: 0.26738039748716064
06/01/2019 11:38:50 *** epoch: 144 ***
06/01/2019 11:38:50 *** training ***
06/01/2019 11:38:51 step: 4724, epoch: 143, batch: 4, loss: 0.011414884589612484, acc: 100.0, f1: 100.0, r: 0.6646516529778558
06/01/2019 11:38:51 step: 4729, epoch: 143, batch: 9, loss: 0.026419946923851967, acc: 100.0, f1: 100.0, r: 0.6684927765069696
06/01/2019 11:38:52 step: 4734, epoch: 143, batch: 14, loss: 0.01428249105811119, acc: 100.0, f1: 100.0, r: 0.8127443423327684
06/01/2019 11:38:53 step: 4739, epoch: 143, batch: 19, loss: 0.06744485348463058, acc: 100.0, f1: 100.0, r: 0.7816881391908421
06/01/2019 11:38:53 step: 4744, epoch: 143, batch: 24, loss: 0.047251150012016296, acc: 100.0, f1: 100.0, r: 0.8196629807474486
06/01/2019 11:38:54 step: 4749, epoch: 143, batch: 29, loss: 0.023708175867795944, acc: 100.0, f1: 100.0, r: 0.7560813109621812
06/01/2019 11:38:54 *** evaluating ***
06/01/2019 11:38:55 step: 144, epoch: 143, acc: 55.12820512820513, f1: 25.303166997199472, r: 0.2616421551353879
06/01/2019 11:38:55 *** epoch: 145 ***
06/01/2019 11:38:55 *** training ***
06/01/2019 11:38:55 step: 4757, epoch: 144, batch: 4, loss: 0.04582797735929489, acc: 98.4375, f1: 98.92156862745098, r: 0.7828391617631547
06/01/2019 11:38:56 step: 4762, epoch: 144, batch: 9, loss: 0.42842182517051697, acc: 100.0, f1: 100.0, r: 0.6807563865518194
06/01/2019 11:38:56 step: 4767, epoch: 144, batch: 14, loss: 0.41050928831100464, acc: 100.0, f1: 100.0, r: 0.7707330444404783
06/01/2019 11:38:57 step: 4772, epoch: 144, batch: 19, loss: 0.007929306477308273, acc: 100.0, f1: 100.0, r: 0.6831014944229207
06/01/2019 11:38:58 step: 4777, epoch: 144, batch: 24, loss: 0.02201078087091446, acc: 100.0, f1: 100.0, r: 0.6333296151676848
06/01/2019 11:38:59 step: 4782, epoch: 144, batch: 29, loss: 0.031345054507255554, acc: 100.0, f1: 100.0, r: 0.7818745044765283
06/01/2019 11:38:59 *** evaluating ***
06/01/2019 11:38:59 step: 145, epoch: 144, acc: 55.12820512820513, f1: 25.783532758764956, r: 0.2703170674116997
06/01/2019 11:38:59 *** epoch: 146 ***
06/01/2019 11:38:59 *** training ***
06/01/2019 11:39:00 step: 4790, epoch: 145, batch: 4, loss: 0.6813881993293762, acc: 100.0, f1: 100.0, r: 0.6412144971604179
06/01/2019 11:39:00 step: 4795, epoch: 145, batch: 9, loss: 0.018444379791617393, acc: 100.0, f1: 100.0, r: 0.665827534925871
06/01/2019 11:39:01 step: 4800, epoch: 145, batch: 14, loss: 0.02400391176342964, acc: 100.0, f1: 100.0, r: 0.7314888233666976
06/01/2019 11:39:02 step: 4805, epoch: 145, batch: 19, loss: 0.027504311874508858, acc: 100.0, f1: 100.0, r: 0.7952154893171278
06/01/2019 11:39:02 step: 4810, epoch: 145, batch: 24, loss: 0.03240523487329483, acc: 100.0, f1: 100.0, r: 0.7651965842364957
06/01/2019 11:39:03 step: 4815, epoch: 145, batch: 29, loss: 0.4238120913505554, acc: 100.0, f1: 100.0, r: 0.8011795982467286
06/01/2019 11:39:03 *** evaluating ***
06/01/2019 11:39:03 step: 146, epoch: 145, acc: 56.41025641025641, f1: 27.736057882912647, r: 0.2715580956335859
06/01/2019 11:39:03 *** epoch: 147 ***
06/01/2019 11:39:03 *** training ***
06/01/2019 11:39:04 step: 4823, epoch: 146, batch: 4, loss: 0.04287097603082657, acc: 100.0, f1: 100.0, r: 0.6923686133954582
06/01/2019 11:39:05 step: 4828, epoch: 146, batch: 9, loss: 0.020083481445908546, acc: 100.0, f1: 100.0, r: 0.7250309947045835
06/01/2019 11:39:05 step: 4833, epoch: 146, batch: 14, loss: 0.027443356812000275, acc: 100.0, f1: 100.0, r: 0.6517094415662188
06/01/2019 11:39:06 step: 4838, epoch: 146, batch: 19, loss: 0.017618155106902122, acc: 100.0, f1: 100.0, r: 0.7744205155818702
06/01/2019 11:39:07 step: 4843, epoch: 146, batch: 24, loss: 0.41702550649642944, acc: 100.0, f1: 100.0, r: 0.7115918891252094
06/01/2019 11:39:07 step: 4848, epoch: 146, batch: 29, loss: 0.016038749366998672, acc: 100.0, f1: 100.0, r: 0.7428110688056431
06/01/2019 11:39:08 *** evaluating ***
06/01/2019 11:39:08 step: 147, epoch: 146, acc: 54.27350427350427, f1: 24.989100733781587, r: 0.2664010811992816
06/01/2019 11:39:08 *** epoch: 148 ***
06/01/2019 11:39:08 *** training ***
06/01/2019 11:39:09 step: 4856, epoch: 147, batch: 4, loss: 0.015046532265841961, acc: 100.0, f1: 100.0, r: 0.7995804058725134
06/01/2019 11:39:09 step: 4861, epoch: 147, batch: 9, loss: 0.044932492077350616, acc: 100.0, f1: 100.0, r: 0.7629039459035158
06/01/2019 11:39:10 step: 4866, epoch: 147, batch: 14, loss: 0.0090109808370471, acc: 100.0, f1: 100.0, r: 0.7725552482033593
06/01/2019 11:39:10 step: 4871, epoch: 147, batch: 19, loss: 0.013412433676421642, acc: 100.0, f1: 100.0, r: 0.7918029637874078
06/01/2019 11:39:11 step: 4876, epoch: 147, batch: 24, loss: 0.04244164749979973, acc: 100.0, f1: 100.0, r: 0.7076891311206353
06/01/2019 11:39:12 step: 4881, epoch: 147, batch: 29, loss: 0.014084482565522194, acc: 100.0, f1: 100.0, r: 0.744522885212524
06/01/2019 11:39:12 *** evaluating ***
06/01/2019 11:39:12 step: 148, epoch: 147, acc: 56.837606837606835, f1: 27.96446918514996, r: 0.26927973976808184
06/01/2019 11:39:12 *** epoch: 149 ***
06/01/2019 11:39:12 *** training ***
06/01/2019 11:39:13 step: 4889, epoch: 148, batch: 4, loss: 0.37471097707748413, acc: 100.0, f1: 100.0, r: 0.8159714065988432
06/01/2019 11:39:14 step: 4894, epoch: 148, batch: 9, loss: 0.021574238315224648, acc: 100.0, f1: 100.0, r: 0.7967938543003972
06/01/2019 11:39:14 step: 4899, epoch: 148, batch: 14, loss: 0.00754095334559679, acc: 100.0, f1: 100.0, r: 0.7203472435692555
06/01/2019 11:39:15 step: 4904, epoch: 148, batch: 19, loss: 0.040607765316963196, acc: 100.0, f1: 100.0, r: 0.7142441034769532
06/01/2019 11:39:15 step: 4909, epoch: 148, batch: 24, loss: 0.05221671983599663, acc: 100.0, f1: 100.0, r: 0.7111307602100992
06/01/2019 11:39:16 step: 4914, epoch: 148, batch: 29, loss: 0.02477262355387211, acc: 100.0, f1: 100.0, r: 0.8018450089827529
06/01/2019 11:39:16 *** evaluating ***
06/01/2019 11:39:17 step: 149, epoch: 148, acc: 55.55555555555556, f1: 25.04859211126311, r: 0.2684932589801694
06/01/2019 11:39:17 *** epoch: 150 ***
06/01/2019 11:39:17 *** training ***
06/01/2019 11:39:17 step: 4922, epoch: 149, batch: 4, loss: 0.4127594828605652, acc: 100.0, f1: 100.0, r: 0.7230097144841031
06/01/2019 11:39:18 step: 4927, epoch: 149, batch: 9, loss: 0.06417690217494965, acc: 100.0, f1: 100.0, r: 0.674609428463895
06/01/2019 11:39:19 step: 4932, epoch: 149, batch: 14, loss: 0.020746340975165367, acc: 100.0, f1: 100.0, r: 0.7853363214839648
06/01/2019 11:39:19 step: 4937, epoch: 149, batch: 19, loss: 0.03355294466018677, acc: 98.4375, f1: 95.33333333333334, r: 0.7471277507564944
06/01/2019 11:39:20 step: 4942, epoch: 149, batch: 24, loss: 0.018309399485588074, acc: 100.0, f1: 100.0, r: 0.7507563477569108
06/01/2019 11:39:21 step: 4947, epoch: 149, batch: 29, loss: 0.6932439804077148, acc: 100.0, f1: 100.0, r: 0.8218997041291144
06/01/2019 11:39:21 *** evaluating ***
06/01/2019 11:39:21 step: 150, epoch: 149, acc: 55.98290598290598, f1: 25.437120205780012, r: 0.2642137947218287
06/01/2019 11:39:21 *** epoch: 151 ***
06/01/2019 11:39:21 *** training ***
06/01/2019 11:39:22 step: 4955, epoch: 150, batch: 4, loss: 0.025431938469409943, acc: 100.0, f1: 100.0, r: 0.8187448019751276
06/01/2019 11:39:22 step: 4960, epoch: 150, batch: 9, loss: 0.018762104213237762, acc: 100.0, f1: 100.0, r: 0.826196008256018
06/01/2019 11:39:23 step: 4965, epoch: 150, batch: 14, loss: 0.04642040282487869, acc: 100.0, f1: 100.0, r: 0.7935165220807039
06/01/2019 11:39:24 step: 4970, epoch: 150, batch: 19, loss: 0.08225781470537186, acc: 98.4375, f1: 97.75132275132276, r: 0.7401838441245795
06/01/2019 11:39:24 step: 4975, epoch: 150, batch: 24, loss: 0.023425638675689697, acc: 100.0, f1: 100.0, r: 0.6954282555298263
06/01/2019 11:39:25 step: 4980, epoch: 150, batch: 29, loss: 0.012617011554539204, acc: 100.0, f1: 100.0, r: 0.7118516147133515
06/01/2019 11:39:25 *** evaluating ***
06/01/2019 11:39:25 step: 151, epoch: 150, acc: 53.41880341880342, f1: 23.99854390648039, r: 0.25553962286499776
06/01/2019 11:39:25 *** epoch: 152 ***
06/01/2019 11:39:25 *** training ***
06/01/2019 11:39:26 step: 4988, epoch: 151, batch: 4, loss: 0.01734020560979843, acc: 100.0, f1: 100.0, r: 0.6928829583055487
06/01/2019 11:39:27 step: 4993, epoch: 151, batch: 9, loss: 0.6844873428344727, acc: 100.0, f1: 100.0, r: 0.7187232595175764
06/01/2019 11:39:28 step: 4998, epoch: 151, batch: 14, loss: 0.020826762542128563, acc: 100.0, f1: 100.0, r: 0.6634770588267761
06/01/2019 11:39:28 step: 5003, epoch: 151, batch: 19, loss: 0.012480845674872398, acc: 100.0, f1: 100.0, r: 0.7010483735396646
06/01/2019 11:39:29 step: 5008, epoch: 151, batch: 24, loss: 0.08198081701993942, acc: 100.0, f1: 100.0, r: 0.7084192851482414
06/01/2019 11:39:30 step: 5013, epoch: 151, batch: 29, loss: 0.02692263573408127, acc: 100.0, f1: 100.0, r: 0.7268237905779233
06/01/2019 11:39:30 *** evaluating ***
06/01/2019 11:39:30 step: 152, epoch: 151, acc: 56.837606837606835, f1: 28.096139357581006, r: 0.2657551072746397
06/01/2019 11:39:30 *** epoch: 153 ***
06/01/2019 11:39:30 *** training ***
06/01/2019 11:39:31 step: 5021, epoch: 152, batch: 4, loss: 0.018614212051033974, acc: 100.0, f1: 100.0, r: 0.702497422574943
06/01/2019 11:39:31 step: 5026, epoch: 152, batch: 9, loss: 0.020176168531179428, acc: 100.0, f1: 100.0, r: 0.8460221854209522
06/01/2019 11:39:32 step: 5031, epoch: 152, batch: 14, loss: 0.01989833451807499, acc: 100.0, f1: 100.0, r: 0.6982505164299558
06/01/2019 11:39:33 step: 5036, epoch: 152, batch: 19, loss: 0.02845141291618347, acc: 100.0, f1: 100.0, r: 0.7687318610870933
06/01/2019 11:39:33 step: 5041, epoch: 152, batch: 24, loss: 0.019088663160800934, acc: 100.0, f1: 100.0, r: 0.7777634440361263
06/01/2019 11:39:34 step: 5046, epoch: 152, batch: 29, loss: 0.4228038191795349, acc: 100.0, f1: 100.0, r: 0.7212693691416968
06/01/2019 11:39:34 *** evaluating ***
06/01/2019 11:39:34 step: 153, epoch: 152, acc: 53.84615384615385, f1: 24.75386291134595, r: 0.2657906233591187
06/01/2019 11:39:34 *** epoch: 154 ***
06/01/2019 11:39:34 *** training ***
06/01/2019 11:39:35 step: 5054, epoch: 153, batch: 4, loss: 0.3757760226726532, acc: 100.0, f1: 100.0, r: 0.7317233176265157
06/01/2019 11:39:36 step: 5059, epoch: 153, batch: 9, loss: 0.015700023621320724, acc: 100.0, f1: 100.0, r: 0.7726361152812238
06/01/2019 11:39:36 step: 5064, epoch: 153, batch: 14, loss: 0.053329527378082275, acc: 98.4375, f1: 96.68202764976958, r: 0.6848074065480874
06/01/2019 11:39:37 step: 5069, epoch: 153, batch: 19, loss: 0.02525871805846691, acc: 100.0, f1: 100.0, r: 0.7148138403837337
06/01/2019 11:39:37 step: 5074, epoch: 153, batch: 24, loss: 0.06324657797813416, acc: 100.0, f1: 100.0, r: 0.6474861934225482
06/01/2019 11:39:38 step: 5079, epoch: 153, batch: 29, loss: 0.023263517767190933, acc: 100.0, f1: 100.0, r: 0.6476102590903342
06/01/2019 11:39:38 *** evaluating ***
06/01/2019 11:39:39 step: 154, epoch: 153, acc: 55.12820512820513, f1: 25.323227825136296, r: 0.26488212613970613
06/01/2019 11:39:39 *** epoch: 155 ***
06/01/2019 11:39:39 *** training ***
06/01/2019 11:39:39 step: 5087, epoch: 154, batch: 4, loss: 0.01791171357035637, acc: 100.0, f1: 100.0, r: 0.7545066702107412
06/01/2019 11:39:40 step: 5092, epoch: 154, batch: 9, loss: 0.010051186196506023, acc: 100.0, f1: 100.0, r: 0.690041465558964
06/01/2019 11:39:41 step: 5097, epoch: 154, batch: 14, loss: 0.02151770144701004, acc: 100.0, f1: 100.0, r: 0.7024991324376574
06/01/2019 11:39:41 step: 5102, epoch: 154, batch: 19, loss: 0.00788884423673153, acc: 100.0, f1: 100.0, r: 0.6882926986446194
06/01/2019 11:39:42 step: 5107, epoch: 154, batch: 24, loss: 0.007468170020729303, acc: 100.0, f1: 100.0, r: 0.8050332118290967
06/01/2019 11:39:42 step: 5112, epoch: 154, batch: 29, loss: 0.011786461807787418, acc: 100.0, f1: 100.0, r: 0.8209989821350011
06/01/2019 11:39:43 *** evaluating ***
06/01/2019 11:39:43 step: 155, epoch: 154, acc: 55.12820512820513, f1: 23.688890206620282, r: 0.25367385838741546
06/01/2019 11:39:43 *** epoch: 156 ***
06/01/2019 11:39:43 *** training ***
06/01/2019 11:39:43 step: 5120, epoch: 155, batch: 4, loss: 0.010625278577208519, acc: 100.0, f1: 100.0, r: 0.7089759476633166
06/01/2019 11:39:44 step: 5125, epoch: 155, batch: 9, loss: 0.022553104907274246, acc: 100.0, f1: 100.0, r: 0.6732038203140376
06/01/2019 11:39:45 step: 5130, epoch: 155, batch: 14, loss: 0.021676423028111458, acc: 100.0, f1: 100.0, r: 0.7862749683830255
06/01/2019 11:39:45 step: 5135, epoch: 155, batch: 19, loss: 0.0246228389441967, acc: 100.0, f1: 100.0, r: 0.8383156150898387
06/01/2019 11:39:46 step: 5140, epoch: 155, batch: 24, loss: 0.01907839998602867, acc: 100.0, f1: 100.0, r: 0.7181436780770886
06/01/2019 11:39:47 step: 5145, epoch: 155, batch: 29, loss: 0.02200247161090374, acc: 100.0, f1: 100.0, r: 0.7691136723092054
06/01/2019 11:39:47 *** evaluating ***
06/01/2019 11:39:47 step: 156, epoch: 155, acc: 56.41025641025641, f1: 27.725181885254656, r: 0.26462375942210226
06/01/2019 11:39:47 *** epoch: 157 ***
06/01/2019 11:39:47 *** training ***
06/01/2019 11:39:48 step: 5153, epoch: 156, batch: 4, loss: 0.020371954888105392, acc: 100.0, f1: 100.0, r: 0.7206467427355076
06/01/2019 11:39:49 step: 5158, epoch: 156, batch: 9, loss: 0.006857294589281082, acc: 100.0, f1: 100.0, r: 0.6203429971958556
06/01/2019 11:39:49 step: 5163, epoch: 156, batch: 14, loss: 0.42079049348831177, acc: 100.0, f1: 100.0, r: 0.8148043080816514
06/01/2019 11:39:50 step: 5168, epoch: 156, batch: 19, loss: 0.014069274999201298, acc: 100.0, f1: 100.0, r: 0.7945959191931843
06/01/2019 11:39:51 step: 5173, epoch: 156, batch: 24, loss: 0.01739034429192543, acc: 100.0, f1: 100.0, r: 0.7903579489256296
06/01/2019 11:39:51 step: 5178, epoch: 156, batch: 29, loss: 0.00846344605088234, acc: 100.0, f1: 100.0, r: 0.7130731268144372
06/01/2019 11:39:52 *** evaluating ***
06/01/2019 11:39:52 step: 157, epoch: 156, acc: 53.84615384615385, f1: 24.69347727340055, r: 0.2584873076914632
06/01/2019 11:39:52 *** epoch: 158 ***
06/01/2019 11:39:52 *** training ***
06/01/2019 11:39:52 step: 5186, epoch: 157, batch: 4, loss: 0.036157071590423584, acc: 100.0, f1: 100.0, r: 0.7216293143251854
06/01/2019 11:39:53 step: 5191, epoch: 157, batch: 9, loss: 0.008999023586511612, acc: 100.0, f1: 100.0, r: 0.7904646063853331
06/01/2019 11:39:54 step: 5196, epoch: 157, batch: 14, loss: 0.00958432536572218, acc: 100.0, f1: 100.0, r: 0.7605970935011142
06/01/2019 11:39:54 step: 5201, epoch: 157, batch: 19, loss: 0.022065408527851105, acc: 100.0, f1: 100.0, r: 0.8009071929505335
06/01/2019 11:39:55 step: 5206, epoch: 157, batch: 24, loss: 0.023726388812065125, acc: 100.0, f1: 100.0, r: 0.7261236244441858
06/01/2019 11:39:56 step: 5211, epoch: 157, batch: 29, loss: 0.012536173686385155, acc: 100.0, f1: 100.0, r: 0.6194463332157327
06/01/2019 11:39:56 *** evaluating ***
06/01/2019 11:39:56 step: 158, epoch: 157, acc: 55.98290598290598, f1: 25.38725380601753, r: 0.2627630896651377
06/01/2019 11:39:56 *** epoch: 159 ***
06/01/2019 11:39:56 *** training ***
06/01/2019 11:39:57 step: 5219, epoch: 158, batch: 4, loss: 0.014133255928754807, acc: 100.0, f1: 100.0, r: 0.7434242209473352
06/01/2019 11:39:57 step: 5224, epoch: 158, batch: 9, loss: 0.01567499339580536, acc: 100.0, f1: 100.0, r: 0.7267902203521432
06/01/2019 11:39:58 step: 5229, epoch: 158, batch: 14, loss: 0.012131943367421627, acc: 100.0, f1: 100.0, r: 0.7393688400976454
06/01/2019 11:39:58 step: 5234, epoch: 158, batch: 19, loss: 0.023648463189601898, acc: 100.0, f1: 100.0, r: 0.7445728588431152
06/01/2019 11:39:59 step: 5239, epoch: 158, batch: 24, loss: 0.015390975400805473, acc: 100.0, f1: 100.0, r: 0.7813022291566388
06/01/2019 11:40:00 step: 5244, epoch: 158, batch: 29, loss: 0.03558918088674545, acc: 98.4375, f1: 98.3201581027668, r: 0.733293123416475
06/01/2019 11:40:00 *** evaluating ***
06/01/2019 11:40:00 step: 159, epoch: 158, acc: 56.837606837606835, f1: 27.98245574580293, r: 0.2623214568052777
06/01/2019 11:40:00 *** epoch: 160 ***
06/01/2019 11:40:00 *** training ***
06/01/2019 11:40:01 step: 5252, epoch: 159, batch: 4, loss: 0.01722831279039383, acc: 100.0, f1: 100.0, r: 0.7630589366052789
06/01/2019 11:40:02 step: 5257, epoch: 159, batch: 9, loss: 0.007779644336551428, acc: 100.0, f1: 100.0, r: 0.6505969107625839
06/01/2019 11:40:02 step: 5262, epoch: 159, batch: 14, loss: 0.038290832191705704, acc: 100.0, f1: 100.0, r: 0.7060451658429412
06/01/2019 11:40:03 step: 5267, epoch: 159, batch: 19, loss: 0.010329707525670528, acc: 100.0, f1: 100.0, r: 0.6664194222751278
06/01/2019 11:40:04 step: 5272, epoch: 159, batch: 24, loss: 0.008998136967420578, acc: 100.0, f1: 100.0, r: 0.819293002356563
06/01/2019 11:40:04 step: 5277, epoch: 159, batch: 29, loss: 0.017793191596865654, acc: 100.0, f1: 100.0, r: 0.7173878608610718
06/01/2019 11:40:05 *** evaluating ***
06/01/2019 11:40:05 step: 160, epoch: 159, acc: 54.700854700854705, f1: 25.01610314167938, r: 0.25931046569525606
06/01/2019 11:40:05 *** epoch: 161 ***
06/01/2019 11:40:05 *** training ***
06/01/2019 11:40:06 step: 5285, epoch: 160, batch: 4, loss: 0.028616346418857574, acc: 100.0, f1: 100.0, r: 0.6779783181281199
06/01/2019 11:40:06 step: 5290, epoch: 160, batch: 9, loss: 0.011454731225967407, acc: 100.0, f1: 100.0, r: 0.6903535414850529
06/01/2019 11:40:07 step: 5295, epoch: 160, batch: 14, loss: 0.016188258305191994, acc: 100.0, f1: 100.0, r: 0.7996661635738245
06/01/2019 11:40:08 step: 5300, epoch: 160, batch: 19, loss: 0.4052914083003998, acc: 100.0, f1: 100.0, r: 0.6625318082970428
06/01/2019 11:40:08 step: 5305, epoch: 160, batch: 24, loss: 0.028339968994259834, acc: 100.0, f1: 100.0, r: 0.8094997531063347
06/01/2019 11:40:09 step: 5310, epoch: 160, batch: 29, loss: 0.010168179869651794, acc: 100.0, f1: 100.0, r: 0.6938849626254299
06/01/2019 11:40:09 *** evaluating ***
06/01/2019 11:40:09 step: 161, epoch: 160, acc: 55.98290598290598, f1: 25.458828253399073, r: 0.2591109814217012
06/01/2019 11:40:09 *** epoch: 162 ***
06/01/2019 11:40:09 *** training ***
06/01/2019 11:40:10 step: 5318, epoch: 161, batch: 4, loss: 0.026280388236045837, acc: 100.0, f1: 100.0, r: 0.6953729151864554
06/01/2019 11:40:11 step: 5323, epoch: 161, batch: 9, loss: 0.017799420282244682, acc: 100.0, f1: 100.0, r: 0.712624203144127
06/01/2019 11:40:11 step: 5328, epoch: 161, batch: 14, loss: 0.012113407254219055, acc: 100.0, f1: 100.0, r: 0.7547757063534657
06/01/2019 11:40:12 step: 5333, epoch: 161, batch: 19, loss: 0.3984140455722809, acc: 100.0, f1: 100.0, r: 0.5720313183655933
06/01/2019 11:40:12 step: 5338, epoch: 161, batch: 24, loss: 0.013955829665064812, acc: 100.0, f1: 100.0, r: 0.8177887578952059
06/01/2019 11:40:13 step: 5343, epoch: 161, batch: 29, loss: 0.014044149778783321, acc: 100.0, f1: 100.0, r: 0.7161768471342991
06/01/2019 11:40:13 *** evaluating ***
06/01/2019 11:40:14 step: 162, epoch: 161, acc: 56.41025641025641, f1: 25.830759656675628, r: 0.2617128424070579
06/01/2019 11:40:14 *** epoch: 163 ***
06/01/2019 11:40:14 *** training ***
06/01/2019 11:40:14 step: 5351, epoch: 162, batch: 4, loss: 0.014944917522370815, acc: 100.0, f1: 100.0, r: 0.8158445220425306
06/01/2019 11:40:15 step: 5356, epoch: 162, batch: 9, loss: 0.43785056471824646, acc: 100.0, f1: 100.0, r: 0.7726202880989337
06/01/2019 11:40:15 step: 5361, epoch: 162, batch: 14, loss: 0.022443953901529312, acc: 100.0, f1: 100.0, r: 0.6960768639429806
06/01/2019 11:40:16 step: 5366, epoch: 162, batch: 19, loss: 0.009726638905704021, acc: 100.0, f1: 100.0, r: 0.7036231860594294
06/01/2019 11:40:17 step: 5371, epoch: 162, batch: 24, loss: 0.015131935477256775, acc: 100.0, f1: 100.0, r: 0.6244301922832906
06/01/2019 11:40:17 step: 5376, epoch: 162, batch: 29, loss: 0.011889506131410599, acc: 100.0, f1: 100.0, r: 0.8397474869525842
06/01/2019 11:40:18 *** evaluating ***
06/01/2019 11:40:18 step: 163, epoch: 162, acc: 54.700854700854705, f1: 25.122901554724102, r: 0.2646971122187359
06/01/2019 11:40:18 *** epoch: 164 ***
06/01/2019 11:40:18 *** training ***
06/01/2019 11:40:18 step: 5384, epoch: 163, batch: 4, loss: 0.016589587554335594, acc: 100.0, f1: 100.0, r: 0.828881926737839
06/01/2019 11:40:19 step: 5389, epoch: 163, batch: 9, loss: 0.017466165125370026, acc: 100.0, f1: 100.0, r: 0.6851676605807148
06/01/2019 11:40:20 step: 5394, epoch: 163, batch: 14, loss: 0.041267987340688705, acc: 100.0, f1: 100.0, r: 0.7170440801405014
06/01/2019 11:40:20 step: 5399, epoch: 163, batch: 19, loss: 0.021511157974600792, acc: 100.0, f1: 100.0, r: 0.7509506342608349
06/01/2019 11:40:21 step: 5404, epoch: 163, batch: 24, loss: 0.017416559159755707, acc: 100.0, f1: 100.0, r: 0.7186553694047357
06/01/2019 11:40:22 step: 5409, epoch: 163, batch: 29, loss: 0.012839391827583313, acc: 100.0, f1: 100.0, r: 0.8073305493160902
06/01/2019 11:40:22 *** evaluating ***
06/01/2019 11:40:22 step: 164, epoch: 163, acc: 54.700854700854705, f1: 25.059838001910794, r: 0.2553659286651957
06/01/2019 11:40:22 *** epoch: 165 ***
06/01/2019 11:40:22 *** training ***
06/01/2019 11:40:23 step: 5417, epoch: 164, batch: 4, loss: 0.05642864480614662, acc: 100.0, f1: 100.0, r: 0.82621844054402
06/01/2019 11:40:24 step: 5422, epoch: 164, batch: 9, loss: 0.013629132881760597, acc: 100.0, f1: 100.0, r: 0.867825191114928
06/01/2019 11:40:24 step: 5427, epoch: 164, batch: 14, loss: 0.42448434233665466, acc: 100.0, f1: 100.0, r: 0.6853434905836931
06/01/2019 11:40:25 step: 5432, epoch: 164, batch: 19, loss: 0.022354785352945328, acc: 100.0, f1: 100.0, r: 0.6436666703042663
06/01/2019 11:40:25 step: 5437, epoch: 164, batch: 24, loss: 0.036945827305316925, acc: 100.0, f1: 100.0, r: 0.7236737982080945
06/01/2019 11:40:26 step: 5442, epoch: 164, batch: 29, loss: 0.024313010275363922, acc: 100.0, f1: 100.0, r: 0.6618389597823151
06/01/2019 11:40:26 *** evaluating ***
06/01/2019 11:40:26 step: 165, epoch: 164, acc: 54.27350427350427, f1: 24.89336742180328, r: 0.2600357083129337
06/01/2019 11:40:26 *** epoch: 166 ***
06/01/2019 11:40:26 *** training ***
06/01/2019 11:40:27 step: 5450, epoch: 165, batch: 4, loss: 0.009540259838104248, acc: 100.0, f1: 100.0, r: 0.7123175828547114
06/01/2019 11:40:28 step: 5455, epoch: 165, batch: 9, loss: 0.016172127798199654, acc: 100.0, f1: 100.0, r: 0.7186433991895416
06/01/2019 11:40:28 step: 5460, epoch: 165, batch: 14, loss: 0.014484493061900139, acc: 100.0, f1: 100.0, r: 0.675726226709628
06/01/2019 11:40:29 step: 5465, epoch: 165, batch: 19, loss: 0.030136721208691597, acc: 100.0, f1: 100.0, r: 0.7541736900591778
06/01/2019 11:40:29 step: 5470, epoch: 165, batch: 24, loss: 0.01576349325478077, acc: 100.0, f1: 100.0, r: 0.7881089703720292
06/01/2019 11:40:30 step: 5475, epoch: 165, batch: 29, loss: 0.01488281786441803, acc: 100.0, f1: 100.0, r: 0.6864219330722381
06/01/2019 11:40:31 *** evaluating ***
06/01/2019 11:40:31 step: 166, epoch: 165, acc: 55.98290598290598, f1: 27.741835879897025, r: 0.2642422441319582
06/01/2019 11:40:31 *** epoch: 167 ***
06/01/2019 11:40:31 *** training ***
06/01/2019 11:40:31 step: 5483, epoch: 166, batch: 4, loss: 0.026728220283985138, acc: 100.0, f1: 100.0, r: 0.7652813224127993
06/01/2019 11:40:32 step: 5488, epoch: 166, batch: 9, loss: 0.04873574897646904, acc: 100.0, f1: 100.0, r: 0.5814870133514716
06/01/2019 11:40:33 step: 5493, epoch: 166, batch: 14, loss: 0.009490023367106915, acc: 100.0, f1: 100.0, r: 0.8351082218208248
06/01/2019 11:40:34 step: 5498, epoch: 166, batch: 19, loss: 0.008613213896751404, acc: 100.0, f1: 100.0, r: 0.7069455447567478
06/01/2019 11:40:34 step: 5503, epoch: 166, batch: 24, loss: 0.40742599964141846, acc: 100.0, f1: 100.0, r: 0.7166990067424217
06/01/2019 11:40:35 step: 5508, epoch: 166, batch: 29, loss: 0.020070277154445648, acc: 100.0, f1: 100.0, r: 0.7064427775520079
06/01/2019 11:40:35 *** evaluating ***
06/01/2019 11:40:35 step: 167, epoch: 166, acc: 55.12820512820513, f1: 25.047154220704375, r: 0.2611458382207066
06/01/2019 11:40:35 *** epoch: 168 ***
06/01/2019 11:40:35 *** training ***
06/01/2019 11:40:36 step: 5516, epoch: 167, batch: 4, loss: 0.014385693706572056, acc: 100.0, f1: 100.0, r: 0.69793417913939
06/01/2019 11:40:37 step: 5521, epoch: 167, batch: 9, loss: 0.6845165491104126, acc: 100.0, f1: 100.0, r: 0.6593853105207815
06/01/2019 11:40:37 step: 5526, epoch: 167, batch: 14, loss: 0.022849861532449722, acc: 100.0, f1: 100.0, r: 0.8350716050655723
06/01/2019 11:40:38 step: 5531, epoch: 167, batch: 19, loss: 0.010372985154390335, acc: 100.0, f1: 100.0, r: 0.7631135451551074
06/01/2019 11:40:39 step: 5536, epoch: 167, batch: 24, loss: 0.015257117338478565, acc: 100.0, f1: 100.0, r: 0.6848429141503172
06/01/2019 11:40:39 step: 5541, epoch: 167, batch: 29, loss: 0.030102072283625603, acc: 100.0, f1: 100.0, r: 0.7999861540351775
06/01/2019 11:40:39 *** evaluating ***
06/01/2019 11:40:40 step: 168, epoch: 167, acc: 55.12820512820513, f1: 27.281309963132514, r: 0.2649985187735518
06/01/2019 11:40:40 *** epoch: 169 ***
06/01/2019 11:40:40 *** training ***
06/01/2019 11:40:40 step: 5549, epoch: 168, batch: 4, loss: 0.01743505336344242, acc: 100.0, f1: 100.0, r: 0.7440605039183967
06/01/2019 11:40:41 step: 5554, epoch: 168, batch: 9, loss: 0.04151660203933716, acc: 100.0, f1: 100.0, r: 0.7737944271577246
06/01/2019 11:40:41 step: 5559, epoch: 168, batch: 14, loss: 0.027535438537597656, acc: 98.4375, f1: 93.71980676328504, r: 0.6192624757853885
06/01/2019 11:40:42 step: 5564, epoch: 168, batch: 19, loss: 0.06955630332231522, acc: 100.0, f1: 100.0, r: 0.7059426120555009
06/01/2019 11:40:43 step: 5569, epoch: 168, batch: 24, loss: 0.018014855682849884, acc: 100.0, f1: 100.0, r: 0.7243356096006572
06/01/2019 11:40:43 step: 5574, epoch: 168, batch: 29, loss: 0.013036789372563362, acc: 100.0, f1: 100.0, r: 0.6732614180970434
06/01/2019 11:40:44 *** evaluating ***
06/01/2019 11:40:44 step: 169, epoch: 168, acc: 55.55555555555556, f1: 25.29674846345071, r: 0.2632344843707901
06/01/2019 11:40:44 *** epoch: 170 ***
06/01/2019 11:40:44 *** training ***
06/01/2019 11:40:45 step: 5582, epoch: 169, batch: 4, loss: 0.008852837607264519, acc: 100.0, f1: 100.0, r: 0.7254013812287953
06/01/2019 11:40:45 step: 5587, epoch: 169, batch: 9, loss: 0.011439507827162743, acc: 100.0, f1: 100.0, r: 0.6804892091834283
06/01/2019 11:40:46 step: 5592, epoch: 169, batch: 14, loss: 0.02278951182961464, acc: 100.0, f1: 100.0, r: 0.6900473025765318
06/01/2019 11:40:47 step: 5597, epoch: 169, batch: 19, loss: 0.009015149436891079, acc: 100.0, f1: 100.0, r: 0.7071260127625761
06/01/2019 11:40:47 step: 5602, epoch: 169, batch: 24, loss: 0.06654071807861328, acc: 100.0, f1: 100.0, r: 0.7213769964756019
06/01/2019 11:40:48 step: 5607, epoch: 169, batch: 29, loss: 0.010887356474995613, acc: 100.0, f1: 100.0, r: 0.6890354090305884
06/01/2019 11:40:48 *** evaluating ***
06/01/2019 11:40:49 step: 170, epoch: 169, acc: 55.98290598290598, f1: 25.587844531781407, r: 0.26105479682382826
06/01/2019 11:40:49 *** epoch: 171 ***
06/01/2019 11:40:49 *** training ***
06/01/2019 11:40:49 step: 5615, epoch: 170, batch: 4, loss: 0.020732099190354347, acc: 100.0, f1: 100.0, r: 0.7454528455820052
06/01/2019 11:40:50 step: 5620, epoch: 170, batch: 9, loss: 0.695512592792511, acc: 100.0, f1: 100.0, r: 0.7320524436189265
06/01/2019 11:40:51 step: 5625, epoch: 170, batch: 14, loss: 0.01121879555284977, acc: 100.0, f1: 100.0, r: 0.816317915601629
06/01/2019 11:40:51 step: 5630, epoch: 170, batch: 19, loss: 0.017615346238017082, acc: 100.0, f1: 100.0, r: 0.7043336737243932
06/01/2019 11:40:52 step: 5635, epoch: 170, batch: 24, loss: 0.38290029764175415, acc: 100.0, f1: 100.0, r: 0.7788258029726192
06/01/2019 11:40:52 step: 5640, epoch: 170, batch: 29, loss: 0.02095232903957367, acc: 100.0, f1: 100.0, r: 0.6410901027834806
06/01/2019 11:40:53 *** evaluating ***
06/01/2019 11:40:53 step: 171, epoch: 170, acc: 55.98290598290598, f1: 25.669031541859795, r: 0.2590897747641861
06/01/2019 11:40:53 *** epoch: 172 ***
06/01/2019 11:40:53 *** training ***
06/01/2019 11:40:54 step: 5648, epoch: 171, batch: 4, loss: 0.015026903711259365, acc: 100.0, f1: 100.0, r: 0.6871864839678345
06/01/2019 11:40:54 step: 5653, epoch: 171, batch: 9, loss: 0.018158486112952232, acc: 100.0, f1: 100.0, r: 0.8019763248080708
06/01/2019 11:40:55 step: 5658, epoch: 171, batch: 14, loss: 0.024222373962402344, acc: 100.0, f1: 100.0, r: 0.8244227483246962
06/01/2019 11:40:55 step: 5663, epoch: 171, batch: 19, loss: 0.04200638085603714, acc: 98.4375, f1: 98.63056333644569, r: 0.741338992946282
06/01/2019 11:40:56 step: 5668, epoch: 171, batch: 24, loss: 0.016155652701854706, acc: 100.0, f1: 100.0, r: 0.7833400640538571
06/01/2019 11:40:57 step: 5673, epoch: 171, batch: 29, loss: 0.02893831953406334, acc: 100.0, f1: 100.0, r: 0.7321337335908484
06/01/2019 11:40:57 *** evaluating ***
06/01/2019 11:40:57 step: 172, epoch: 171, acc: 55.98290598290598, f1: 25.245646390740205, r: 0.26539908098700604
06/01/2019 11:40:57 *** epoch: 173 ***
06/01/2019 11:40:57 *** training ***
06/01/2019 11:40:58 step: 5681, epoch: 172, batch: 4, loss: 0.03006747178733349, acc: 98.4375, f1: 97.03703703703704, r: 0.7799303667320331
06/01/2019 11:40:59 step: 5686, epoch: 172, batch: 9, loss: 0.0241110697388649, acc: 100.0, f1: 100.0, r: 0.683778462795306
06/01/2019 11:40:59 step: 5691, epoch: 172, batch: 14, loss: 0.02082105353474617, acc: 100.0, f1: 100.0, r: 0.8483758750956578
06/01/2019 11:41:00 step: 5696, epoch: 172, batch: 19, loss: 0.37911948561668396, acc: 100.0, f1: 100.0, r: 0.694714675045408
06/01/2019 11:41:01 step: 5701, epoch: 172, batch: 24, loss: 0.010251437313854694, acc: 100.0, f1: 100.0, r: 0.7885947762098627
06/01/2019 11:41:02 step: 5706, epoch: 172, batch: 29, loss: 0.06371121108531952, acc: 100.0, f1: 100.0, r: 0.6485996164561673
06/01/2019 11:41:02 *** evaluating ***
06/01/2019 11:41:02 step: 173, epoch: 172, acc: 53.84615384615385, f1: 24.159982174688057, r: 0.25474420656059743
06/01/2019 11:41:02 *** epoch: 174 ***
06/01/2019 11:41:02 *** training ***
06/01/2019 11:41:03 step: 5714, epoch: 173, batch: 4, loss: 0.007427713368088007, acc: 100.0, f1: 100.0, r: 0.8219159228664836
06/01/2019 11:41:03 step: 5719, epoch: 173, batch: 9, loss: 0.017883116379380226, acc: 100.0, f1: 100.0, r: 0.8130616576552907
06/01/2019 11:41:04 step: 5724, epoch: 173, batch: 14, loss: 0.05246537923812866, acc: 98.4375, f1: 97.09677419354838, r: 0.733496865355435
06/01/2019 11:41:05 step: 5729, epoch: 173, batch: 19, loss: 0.012065714225172997, acc: 100.0, f1: 100.0, r: 0.6840540863967578
06/01/2019 11:41:05 step: 5734, epoch: 173, batch: 24, loss: 0.023387251421809196, acc: 100.0, f1: 100.0, r: 0.72720521184575
06/01/2019 11:41:06 step: 5739, epoch: 173, batch: 29, loss: 0.014734908007085323, acc: 100.0, f1: 100.0, r: 0.7083523370725869
06/01/2019 11:41:06 *** evaluating ***
06/01/2019 11:41:06 step: 174, epoch: 173, acc: 54.700854700854705, f1: 24.94850185460903, r: 0.25747970938285747
06/01/2019 11:41:06 *** epoch: 175 ***
06/01/2019 11:41:06 *** training ***
06/01/2019 11:41:07 step: 5747, epoch: 174, batch: 4, loss: 0.009808676317334175, acc: 100.0, f1: 100.0, r: 0.7951151829960783
06/01/2019 11:41:08 step: 5752, epoch: 174, batch: 9, loss: 0.05921635031700134, acc: 100.0, f1: 100.0, r: 0.7840121614422759
06/01/2019 11:41:08 step: 5757, epoch: 174, batch: 14, loss: 0.010357923805713654, acc: 100.0, f1: 100.0, r: 0.7292337638621728
06/01/2019 11:41:09 step: 5762, epoch: 174, batch: 19, loss: 0.019839394837617874, acc: 100.0, f1: 100.0, r: 0.794423135906249
06/01/2019 11:41:10 step: 5767, epoch: 174, batch: 24, loss: 0.01832689344882965, acc: 100.0, f1: 100.0, r: 0.7777222378826336
06/01/2019 11:41:10 step: 5772, epoch: 174, batch: 29, loss: 0.01655225269496441, acc: 100.0, f1: 100.0, r: 0.708381805462025
06/01/2019 11:41:11 *** evaluating ***
06/01/2019 11:41:11 step: 175, epoch: 174, acc: 55.55555555555556, f1: 27.30887398303892, r: 0.2654821348194
06/01/2019 11:41:11 *** epoch: 176 ***
06/01/2019 11:41:11 *** training ***
06/01/2019 11:41:12 step: 5780, epoch: 175, batch: 4, loss: 0.06431987136602402, acc: 100.0, f1: 100.0, r: 0.7227116576133198
06/01/2019 11:41:12 step: 5785, epoch: 175, batch: 9, loss: 0.058921389281749725, acc: 100.0, f1: 100.0, r: 0.7425525923592854
06/01/2019 11:41:13 step: 5790, epoch: 175, batch: 14, loss: 0.017039205878973007, acc: 100.0, f1: 100.0, r: 0.7053432850244262
06/01/2019 11:41:13 step: 5795, epoch: 175, batch: 19, loss: 0.012605792842805386, acc: 100.0, f1: 100.0, r: 0.7320247373097667
06/01/2019 11:41:14 step: 5800, epoch: 175, batch: 24, loss: 0.02210714854300022, acc: 100.0, f1: 100.0, r: 0.6149021155965814
06/01/2019 11:41:15 step: 5805, epoch: 175, batch: 29, loss: 0.008248353376984596, acc: 100.0, f1: 100.0, r: 0.7190270360798432
06/01/2019 11:41:15 *** evaluating ***
06/01/2019 11:41:15 step: 176, epoch: 175, acc: 55.55555555555556, f1: 25.23918452427084, r: 0.2623554091673226
06/01/2019 11:41:15 *** epoch: 177 ***
06/01/2019 11:41:15 *** training ***
06/01/2019 11:41:16 step: 5813, epoch: 176, batch: 4, loss: 0.008749226108193398, acc: 100.0, f1: 100.0, r: 0.7745475757402417
06/01/2019 11:41:16 step: 5818, epoch: 176, batch: 9, loss: 0.00932623352855444, acc: 100.0, f1: 100.0, r: 0.8352150197880602
06/01/2019 11:41:17 step: 5823, epoch: 176, batch: 14, loss: 0.02156870812177658, acc: 100.0, f1: 100.0, r: 0.7910996039604864
06/01/2019 11:41:18 step: 5828, epoch: 176, batch: 19, loss: 0.017130672931671143, acc: 100.0, f1: 100.0, r: 0.6816408202170978
06/01/2019 11:41:19 step: 5833, epoch: 176, batch: 24, loss: 0.02679331973195076, acc: 100.0, f1: 100.0, r: 0.6830466812313218
06/01/2019 11:41:19 step: 5838, epoch: 176, batch: 29, loss: 0.009472083300352097, acc: 100.0, f1: 100.0, r: 0.7630383153765603
06/01/2019 11:41:20 *** evaluating ***
06/01/2019 11:41:20 step: 177, epoch: 176, acc: 53.84615384615385, f1: 24.225402999037453, r: 0.26092774153535975
06/01/2019 11:41:20 *** epoch: 178 ***
06/01/2019 11:41:20 *** training ***
06/01/2019 11:41:20 step: 5846, epoch: 177, batch: 4, loss: 0.022116871550679207, acc: 98.4375, f1: 98.57549857549857, r: 0.7757153988414639
06/01/2019 11:41:21 step: 5851, epoch: 177, batch: 9, loss: 0.016960978507995605, acc: 100.0, f1: 100.0, r: 0.646343840956944
06/01/2019 11:41:22 step: 5856, epoch: 177, batch: 14, loss: 0.007231035735458136, acc: 100.0, f1: 100.0, r: 0.8057679437420713
06/01/2019 11:41:22 step: 5861, epoch: 177, batch: 19, loss: 0.05660706385970116, acc: 100.0, f1: 100.0, r: 0.7357098123224692
06/01/2019 11:41:23 step: 5866, epoch: 177, batch: 24, loss: 0.009864641353487968, acc: 100.0, f1: 100.0, r: 0.7827910641691966
06/01/2019 11:41:24 step: 5871, epoch: 177, batch: 29, loss: 0.00916314497590065, acc: 100.0, f1: 100.0, r: 0.7339708060982911
06/01/2019 11:41:24 *** evaluating ***
06/01/2019 11:41:24 step: 178, epoch: 177, acc: 54.27350427350427, f1: 24.50755526042581, r: 0.26183166079118236
06/01/2019 11:41:24 *** epoch: 179 ***
06/01/2019 11:41:24 *** training ***
06/01/2019 11:41:25 step: 5879, epoch: 178, batch: 4, loss: 0.013766251504421234, acc: 100.0, f1: 100.0, r: 0.748342650743443
06/01/2019 11:41:26 step: 5884, epoch: 178, batch: 9, loss: 0.021555539220571518, acc: 100.0, f1: 100.0, r: 0.6369076465572888
06/01/2019 11:41:26 step: 5889, epoch: 178, batch: 14, loss: 0.014311248436570168, acc: 100.0, f1: 100.0, r: 0.8162999292055529
06/01/2019 11:41:27 step: 5894, epoch: 178, batch: 19, loss: 0.01159060187637806, acc: 100.0, f1: 100.0, r: 0.7772398148860609
06/01/2019 11:41:28 step: 5899, epoch: 178, batch: 24, loss: 0.005499836057424545, acc: 100.0, f1: 100.0, r: 0.7202018067209885
06/01/2019 11:41:28 step: 5904, epoch: 178, batch: 29, loss: 0.018140684813261032, acc: 100.0, f1: 100.0, r: 0.6700390679499606
06/01/2019 11:41:29 *** evaluating ***
06/01/2019 11:41:29 step: 179, epoch: 178, acc: 56.41025641025641, f1: 27.710271318051703, r: 0.26210137937645384
06/01/2019 11:41:29 *** epoch: 180 ***
06/01/2019 11:41:29 *** training ***
06/01/2019 11:41:30 step: 5912, epoch: 179, batch: 4, loss: 0.41176795959472656, acc: 100.0, f1: 100.0, r: 0.6997968030697561
06/01/2019 11:41:30 step: 5917, epoch: 179, batch: 9, loss: 0.02409057691693306, acc: 100.0, f1: 100.0, r: 0.765174868461532
06/01/2019 11:41:31 step: 5922, epoch: 179, batch: 14, loss: 0.012444580905139446, acc: 100.0, f1: 100.0, r: 0.6653365227231677
06/01/2019 11:41:32 step: 5927, epoch: 179, batch: 19, loss: 0.09418946504592896, acc: 98.4375, f1: 98.60742705570291, r: 0.7979024009381893
06/01/2019 11:41:32 step: 5932, epoch: 179, batch: 24, loss: 0.01904071681201458, acc: 100.0, f1: 100.0, r: 0.6754794421916434
06/01/2019 11:41:33 step: 5937, epoch: 179, batch: 29, loss: 0.02843373641371727, acc: 100.0, f1: 100.0, r: 0.8038185732164772
06/01/2019 11:41:33 *** evaluating ***
06/01/2019 11:41:33 step: 180, epoch: 179, acc: 56.837606837606835, f1: 28.051722582972584, r: 0.2660263982231535
06/01/2019 11:41:33 *** epoch: 181 ***
06/01/2019 11:41:33 *** training ***
06/01/2019 11:41:34 step: 5945, epoch: 180, batch: 4, loss: 0.01637394167482853, acc: 100.0, f1: 100.0, r: 0.7968173417557678
06/01/2019 11:41:35 step: 5950, epoch: 180, batch: 9, loss: 0.018586061894893646, acc: 100.0, f1: 100.0, r: 0.7446903054310138
06/01/2019 11:41:35 step: 5955, epoch: 180, batch: 14, loss: 0.01670825108885765, acc: 100.0, f1: 100.0, r: 0.8195742375273943
06/01/2019 11:41:36 step: 5960, epoch: 180, batch: 19, loss: 0.04391730576753616, acc: 100.0, f1: 100.0, r: 0.7246149316379309
06/01/2019 11:41:36 step: 5965, epoch: 180, batch: 24, loss: 0.36959391832351685, acc: 100.0, f1: 100.0, r: 0.8020998227085611
06/01/2019 11:41:37 step: 5970, epoch: 180, batch: 29, loss: 0.01545349508523941, acc: 100.0, f1: 100.0, r: 0.8014340341853694
06/01/2019 11:41:37 *** evaluating ***
06/01/2019 11:41:38 step: 181, epoch: 180, acc: 56.41025641025641, f1: 29.01785978731154, r: 0.2647193209553612
06/01/2019 11:41:38 *** epoch: 182 ***
06/01/2019 11:41:38 *** training ***
06/01/2019 11:41:38 step: 5978, epoch: 181, batch: 4, loss: 0.018988491967320442, acc: 100.0, f1: 100.0, r: 0.6572674880946338
06/01/2019 11:41:39 step: 5983, epoch: 181, batch: 9, loss: 0.016033172607421875, acc: 100.0, f1: 100.0, r: 0.843118014025175
06/01/2019 11:41:40 step: 5988, epoch: 181, batch: 14, loss: 0.015783263370394707, acc: 100.0, f1: 100.0, r: 0.6553281774779893
06/01/2019 11:41:40 step: 5993, epoch: 181, batch: 19, loss: 0.012387283146381378, acc: 100.0, f1: 100.0, r: 0.7137867437995228
06/01/2019 11:41:41 step: 5998, epoch: 181, batch: 24, loss: 0.00910800788551569, acc: 100.0, f1: 100.0, r: 0.680923272023698
06/01/2019 11:41:41 step: 6003, epoch: 181, batch: 29, loss: 0.012768161483108997, acc: 100.0, f1: 100.0, r: 0.758503685744589
06/01/2019 11:41:42 *** evaluating ***
06/01/2019 11:41:42 step: 182, epoch: 181, acc: 55.98290598290598, f1: 25.51619331943304, r: 0.2619553489515252
06/01/2019 11:41:42 *** epoch: 183 ***
06/01/2019 11:41:42 *** training ***
06/01/2019 11:41:43 step: 6011, epoch: 182, batch: 4, loss: 0.012110445648431778, acc: 100.0, f1: 100.0, r: 0.7849163696842719
06/01/2019 11:41:43 step: 6016, epoch: 182, batch: 9, loss: 0.014546293765306473, acc: 100.0, f1: 100.0, r: 0.7208607467305135
06/01/2019 11:41:44 step: 6021, epoch: 182, batch: 14, loss: 0.018190661445260048, acc: 100.0, f1: 100.0, r: 0.7031695950873694
06/01/2019 11:41:45 step: 6026, epoch: 182, batch: 19, loss: 0.01907275803387165, acc: 100.0, f1: 100.0, r: 0.6515456926679527
06/01/2019 11:41:45 step: 6031, epoch: 182, batch: 24, loss: 0.05673186853528023, acc: 100.0, f1: 100.0, r: 0.739083714300808
06/01/2019 11:41:46 step: 6036, epoch: 182, batch: 29, loss: 0.008147993125021458, acc: 100.0, f1: 100.0, r: 0.7312433588162657
06/01/2019 11:41:46 *** evaluating ***
06/01/2019 11:41:47 step: 183, epoch: 182, acc: 55.55555555555556, f1: 27.46377836276681, r: 0.26085085050635454
06/01/2019 11:41:47 *** epoch: 184 ***
06/01/2019 11:41:47 *** training ***
06/01/2019 11:41:47 step: 6044, epoch: 183, batch: 4, loss: 0.6928778886795044, acc: 100.0, f1: 100.0, r: 0.5498420549531817
06/01/2019 11:41:48 step: 6049, epoch: 183, batch: 9, loss: 0.017941514030098915, acc: 100.0, f1: 100.0, r: 0.6890772319129687
06/01/2019 11:41:48 step: 6054, epoch: 183, batch: 14, loss: 0.008782723918557167, acc: 100.0, f1: 100.0, r: 0.7168230433490551
06/01/2019 11:41:49 step: 6059, epoch: 183, batch: 19, loss: 0.03552713617682457, acc: 100.0, f1: 100.0, r: 0.8227164549342716
06/01/2019 11:41:50 step: 6064, epoch: 183, batch: 24, loss: 0.014394807629287243, acc: 100.0, f1: 100.0, r: 0.7997691764707552
06/01/2019 11:41:51 step: 6069, epoch: 183, batch: 29, loss: 0.013005726039409637, acc: 100.0, f1: 100.0, r: 0.831518308365857
06/01/2019 11:41:51 *** evaluating ***
06/01/2019 11:41:51 step: 184, epoch: 183, acc: 55.55555555555556, f1: 25.19925718424837, r: 0.2638324319499008
06/01/2019 11:41:51 *** epoch: 185 ***
06/01/2019 11:41:51 *** training ***
06/01/2019 11:41:52 step: 6077, epoch: 184, batch: 4, loss: 0.0561048686504364, acc: 100.0, f1: 100.0, r: 0.8187189963196362
06/01/2019 11:41:52 step: 6082, epoch: 184, batch: 9, loss: 0.00970799196511507, acc: 100.0, f1: 100.0, r: 0.7986374033149534
06/01/2019 11:41:53 step: 6087, epoch: 184, batch: 14, loss: 0.023206407204270363, acc: 100.0, f1: 100.0, r: 0.8227281284985177
06/01/2019 11:41:53 step: 6092, epoch: 184, batch: 19, loss: 0.01134536787867546, acc: 100.0, f1: 100.0, r: 0.6609196491723511
06/01/2019 11:41:54 step: 6097, epoch: 184, batch: 24, loss: 0.00707941735163331, acc: 100.0, f1: 100.0, r: 0.7963585563359853
06/01/2019 11:41:55 step: 6102, epoch: 184, batch: 29, loss: 0.3970090448856354, acc: 98.4375, f1: 97.74891774891773, r: 0.698351176307945
06/01/2019 11:41:55 *** evaluating ***
06/01/2019 11:41:55 step: 185, epoch: 184, acc: 55.98290598290598, f1: 25.414521869353802, r: 0.2585899947139334
06/01/2019 11:41:55 *** epoch: 186 ***
06/01/2019 11:41:55 *** training ***
06/01/2019 11:41:56 step: 6110, epoch: 185, batch: 4, loss: 0.6921089887619019, acc: 100.0, f1: 100.0, r: 0.7669704117294616
06/01/2019 11:41:56 step: 6115, epoch: 185, batch: 9, loss: 0.012619545683264732, acc: 100.0, f1: 100.0, r: 0.7200271014319392
06/01/2019 11:41:57 step: 6120, epoch: 185, batch: 14, loss: 0.01525693479925394, acc: 100.0, f1: 100.0, r: 0.723041583406624
06/01/2019 11:41:58 step: 6125, epoch: 185, batch: 19, loss: 0.009904855862259865, acc: 100.0, f1: 100.0, r: 0.6925937543451068
06/01/2019 11:41:58 step: 6130, epoch: 185, batch: 24, loss: 0.04077252000570297, acc: 100.0, f1: 100.0, r: 0.629091026412431
06/01/2019 11:41:59 step: 6135, epoch: 185, batch: 29, loss: 0.01136003714054823, acc: 100.0, f1: 100.0, r: 0.7325733189657282
06/01/2019 11:41:59 *** evaluating ***
06/01/2019 11:42:00 step: 186, epoch: 185, acc: 56.41025641025641, f1: 25.964516293051965, r: 0.26697993027253
06/01/2019 11:42:00 *** epoch: 187 ***
06/01/2019 11:42:00 *** training ***
06/01/2019 11:42:00 step: 6143, epoch: 186, batch: 4, loss: 0.03022109344601631, acc: 98.4375, f1: 97.67080745341616, r: 0.8208722794354074
06/01/2019 11:42:01 step: 6148, epoch: 186, batch: 9, loss: 0.6804765462875366, acc: 100.0, f1: 100.0, r: 0.7018480675448212
06/01/2019 11:42:01 step: 6153, epoch: 186, batch: 14, loss: 0.008979599922895432, acc: 100.0, f1: 100.0, r: 0.7465662041621557
06/01/2019 11:42:02 step: 6158, epoch: 186, batch: 19, loss: 0.07008397579193115, acc: 98.4375, f1: 97.55639097744361, r: 0.7789613183582134
06/01/2019 11:42:03 step: 6163, epoch: 186, batch: 24, loss: 0.00727689266204834, acc: 100.0, f1: 100.0, r: 0.7783402089136158
06/01/2019 11:42:03 step: 6168, epoch: 186, batch: 29, loss: 0.06775066256523132, acc: 100.0, f1: 100.0, r: 0.6737422533646734
06/01/2019 11:42:04 *** evaluating ***
06/01/2019 11:42:04 step: 187, epoch: 186, acc: 55.55555555555556, f1: 25.59532873343542, r: 0.26458958825014
06/01/2019 11:42:04 *** epoch: 188 ***
06/01/2019 11:42:04 *** training ***
06/01/2019 11:42:05 step: 6176, epoch: 187, batch: 4, loss: 0.013193336315453053, acc: 100.0, f1: 100.0, r: 0.6287949389524669
06/01/2019 11:42:05 step: 6181, epoch: 187, batch: 9, loss: 0.019764449447393417, acc: 100.0, f1: 100.0, r: 0.6412007263774068
06/01/2019 11:42:06 step: 6186, epoch: 187, batch: 14, loss: 0.01646633818745613, acc: 100.0, f1: 100.0, r: 0.7767547259527962
06/01/2019 11:42:06 step: 6191, epoch: 187, batch: 19, loss: 0.36531001329421997, acc: 100.0, f1: 100.0, r: 0.7699361407699645
06/01/2019 11:42:07 step: 6196, epoch: 187, batch: 24, loss: 0.020501302555203438, acc: 100.0, f1: 100.0, r: 0.7831259528315944
06/01/2019 11:42:08 step: 6201, epoch: 187, batch: 29, loss: 0.006709457375109196, acc: 100.0, f1: 100.0, r: 0.7050574060002832
06/01/2019 11:42:08 *** evaluating ***
06/01/2019 11:42:08 step: 188, epoch: 187, acc: 56.41025641025641, f1: 25.70972847307566, r: 0.2609632241285154
06/01/2019 11:42:08 *** epoch: 189 ***
06/01/2019 11:42:08 *** training ***
06/01/2019 11:42:09 step: 6209, epoch: 188, batch: 4, loss: 0.022716298699378967, acc: 100.0, f1: 100.0, r: 0.8195218277839363
06/01/2019 11:42:10 step: 6214, epoch: 188, batch: 9, loss: 0.02002178505063057, acc: 100.0, f1: 100.0, r: 0.8157578971693914
06/01/2019 11:42:10 step: 6219, epoch: 188, batch: 14, loss: 0.021529454737901688, acc: 100.0, f1: 100.0, r: 0.6629783680665771
06/01/2019 11:42:11 step: 6224, epoch: 188, batch: 19, loss: 0.021514790132641792, acc: 100.0, f1: 100.0, r: 0.7382276482755205
06/01/2019 11:42:11 step: 6229, epoch: 188, batch: 24, loss: 0.016112955287098885, acc: 100.0, f1: 100.0, r: 0.7340911934166389
06/01/2019 11:42:12 step: 6234, epoch: 188, batch: 29, loss: 0.00782906636595726, acc: 100.0, f1: 100.0, r: 0.7477864934071715
06/01/2019 11:42:12 *** evaluating ***
06/01/2019 11:42:13 step: 189, epoch: 188, acc: 56.41025641025641, f1: 27.892902906206263, r: 0.26560602986043125
06/01/2019 11:42:13 *** epoch: 190 ***
06/01/2019 11:42:13 *** training ***
06/01/2019 11:42:13 step: 6242, epoch: 189, batch: 4, loss: 0.019865205511450768, acc: 100.0, f1: 100.0, r: 0.6470731819632025
06/01/2019 11:42:14 step: 6247, epoch: 189, batch: 9, loss: 0.006625359877943993, acc: 100.0, f1: 100.0, r: 0.7334770027043919
06/01/2019 11:42:14 step: 6252, epoch: 189, batch: 14, loss: 0.01141672395169735, acc: 100.0, f1: 100.0, r: 0.8128737210343525
06/01/2019 11:42:15 step: 6257, epoch: 189, batch: 19, loss: 0.004784844815731049, acc: 100.0, f1: 100.0, r: 0.8043925816999379
06/01/2019 11:42:16 step: 6262, epoch: 189, batch: 24, loss: 0.04365759342908859, acc: 100.0, f1: 100.0, r: 0.6627810126894569
06/01/2019 11:42:16 step: 6267, epoch: 189, batch: 29, loss: 0.007849571295082569, acc: 100.0, f1: 100.0, r: 0.7785547557574433
06/01/2019 11:42:17 *** evaluating ***
06/01/2019 11:42:17 step: 190, epoch: 189, acc: 56.837606837606835, f1: 28.030869159901417, r: 0.2662463145079592
06/01/2019 11:42:17 *** epoch: 191 ***
06/01/2019 11:42:17 *** training ***
06/01/2019 11:42:18 step: 6275, epoch: 190, batch: 4, loss: 0.006068528164178133, acc: 100.0, f1: 100.0, r: 0.8469532763920422
06/01/2019 11:42:18 step: 6280, epoch: 190, batch: 9, loss: 0.011601291596889496, acc: 100.0, f1: 100.0, r: 0.7704140628933565
06/01/2019 11:42:19 step: 6285, epoch: 190, batch: 14, loss: 0.015965549275279045, acc: 100.0, f1: 100.0, r: 0.8444232633611167
06/01/2019 11:42:20 step: 6290, epoch: 190, batch: 19, loss: 0.012080428190529346, acc: 100.0, f1: 100.0, r: 0.762671318777297
06/01/2019 11:42:20 step: 6295, epoch: 190, batch: 24, loss: 0.02252225950360298, acc: 100.0, f1: 100.0, r: 0.8001149419338923
06/01/2019 11:42:21 step: 6300, epoch: 190, batch: 29, loss: 0.01772492378950119, acc: 100.0, f1: 100.0, r: 0.6869917092549335
06/01/2019 11:42:21 *** evaluating ***
06/01/2019 11:42:21 step: 191, epoch: 190, acc: 55.55555555555556, f1: 27.506309334539658, r: 0.2596590733834953
06/01/2019 11:42:21 *** epoch: 192 ***
06/01/2019 11:42:21 *** training ***
06/01/2019 11:42:22 step: 6308, epoch: 191, batch: 4, loss: 0.04126305878162384, acc: 98.4375, f1: 97.03703703703704, r: 0.6888557307937592
06/01/2019 11:42:23 step: 6313, epoch: 191, batch: 9, loss: 0.019382109865546227, acc: 100.0, f1: 100.0, r: 0.6777433774889501
06/01/2019 11:42:23 step: 6318, epoch: 191, batch: 14, loss: 0.007408970966935158, acc: 100.0, f1: 100.0, r: 0.7567168894870018
06/01/2019 11:42:24 step: 6323, epoch: 191, batch: 19, loss: 0.01699347048997879, acc: 100.0, f1: 100.0, r: 0.780641187319201
06/01/2019 11:42:25 step: 6328, epoch: 191, batch: 24, loss: 0.012888669967651367, acc: 100.0, f1: 100.0, r: 0.600260929435856
06/01/2019 11:42:25 step: 6333, epoch: 191, batch: 29, loss: 0.007071441970765591, acc: 100.0, f1: 100.0, r: 0.7402912441659673
06/01/2019 11:42:26 *** evaluating ***
06/01/2019 11:42:26 step: 192, epoch: 191, acc: 56.41025641025641, f1: 27.91375632927743, r: 0.26302742327886297
06/01/2019 11:42:26 *** epoch: 193 ***
06/01/2019 11:42:26 *** training ***
06/01/2019 11:42:26 step: 6341, epoch: 192, batch: 4, loss: 0.6793158054351807, acc: 100.0, f1: 100.0, r: 0.7501840433774662
06/01/2019 11:42:27 step: 6346, epoch: 192, batch: 9, loss: 0.007166755385696888, acc: 100.0, f1: 100.0, r: 0.6073423996886858
06/01/2019 11:42:28 step: 6351, epoch: 192, batch: 14, loss: 0.015675300732254982, acc: 100.0, f1: 100.0, r: 0.8447734260784765
06/01/2019 11:42:28 step: 6356, epoch: 192, batch: 19, loss: 0.04179038479924202, acc: 100.0, f1: 100.0, r: 0.7227253215064289
06/01/2019 11:42:29 step: 6361, epoch: 192, batch: 24, loss: 0.03445964679121971, acc: 100.0, f1: 100.0, r: 0.7470983956143133
06/01/2019 11:42:30 step: 6366, epoch: 192, batch: 29, loss: 0.011577263474464417, acc: 100.0, f1: 100.0, r: 0.6100915643502933
06/01/2019 11:42:30 *** evaluating ***
06/01/2019 11:42:30 step: 193, epoch: 192, acc: 54.700854700854705, f1: 24.992309751477762, r: 0.25771929326033877
06/01/2019 11:42:30 *** epoch: 194 ***
06/01/2019 11:42:30 *** training ***
06/01/2019 11:42:31 step: 6374, epoch: 193, batch: 4, loss: 0.03754792734980583, acc: 100.0, f1: 100.0, r: 0.6775078610200512
06/01/2019 11:42:31 step: 6379, epoch: 193, batch: 9, loss: 0.04546554014086723, acc: 98.4375, f1: 97.67080745341615, r: 0.833793914121619
06/01/2019 11:42:32 step: 6384, epoch: 193, batch: 14, loss: 0.01896907575428486, acc: 100.0, f1: 100.0, r: 0.694256893360116
06/01/2019 11:42:33 step: 6389, epoch: 193, batch: 19, loss: 0.6778035163879395, acc: 100.0, f1: 100.0, r: 0.6548985093270338
06/01/2019 11:42:33 step: 6394, epoch: 193, batch: 24, loss: 0.009289814159274101, acc: 100.0, f1: 100.0, r: 0.7653475116377966
06/01/2019 11:42:34 step: 6399, epoch: 193, batch: 29, loss: 0.02309323474764824, acc: 100.0, f1: 100.0, r: 0.7094050781015646
06/01/2019 11:42:34 *** evaluating ***
06/01/2019 11:42:35 step: 194, epoch: 193, acc: 56.41025641025641, f1: 28.305959173420558, r: 0.26228027966650785
06/01/2019 11:42:35 *** epoch: 195 ***
06/01/2019 11:42:35 *** training ***
06/01/2019 11:42:35 step: 6407, epoch: 194, batch: 4, loss: 0.009802905842661858, acc: 100.0, f1: 100.0, r: 0.7077559711877327
06/01/2019 11:42:36 step: 6412, epoch: 194, batch: 9, loss: 0.010344956070184708, acc: 100.0, f1: 100.0, r: 0.6981212030492
06/01/2019 11:42:37 step: 6417, epoch: 194, batch: 14, loss: 0.034828055649995804, acc: 100.0, f1: 100.0, r: 0.7967979582314896
06/01/2019 11:42:37 step: 6422, epoch: 194, batch: 19, loss: 0.006811557337641716, acc: 100.0, f1: 100.0, r: 0.6749477347792224
06/01/2019 11:42:38 step: 6427, epoch: 194, batch: 24, loss: 0.01762929931282997, acc: 100.0, f1: 100.0, r: 0.7403503286101109
06/01/2019 11:42:38 step: 6432, epoch: 194, batch: 29, loss: 0.012631449848413467, acc: 100.0, f1: 100.0, r: 0.6879313407320403
06/01/2019 11:42:39 *** evaluating ***
06/01/2019 11:42:39 step: 195, epoch: 194, acc: 55.55555555555556, f1: 24.54085843906379, r: 0.26454533595902574
06/01/2019 11:42:39 *** epoch: 196 ***
06/01/2019 11:42:39 *** training ***
06/01/2019 11:42:40 step: 6440, epoch: 195, batch: 4, loss: 0.6720409393310547, acc: 100.0, f1: 100.0, r: 0.7464816090631274
06/01/2019 11:42:40 step: 6445, epoch: 195, batch: 9, loss: 0.022478681057691574, acc: 100.0, f1: 100.0, r: 0.7899350178536988
06/01/2019 11:42:41 step: 6450, epoch: 195, batch: 14, loss: 0.012590124271810055, acc: 100.0, f1: 100.0, r: 0.8057629559466474
06/01/2019 11:42:41 step: 6455, epoch: 195, batch: 19, loss: 0.01846594735980034, acc: 100.0, f1: 100.0, r: 0.6708103227590089
06/01/2019 11:42:42 step: 6460, epoch: 195, batch: 24, loss: 0.02152378484606743, acc: 100.0, f1: 100.0, r: 0.7986820258899728
06/01/2019 11:42:43 step: 6465, epoch: 195, batch: 29, loss: 0.00947551429271698, acc: 100.0, f1: 100.0, r: 0.7490382830883917
06/01/2019 11:42:43 *** evaluating ***
06/01/2019 11:42:43 step: 196, epoch: 195, acc: 55.55555555555556, f1: 25.178581273184065, r: 0.25869204959771086
06/01/2019 11:42:43 *** epoch: 197 ***
06/01/2019 11:42:43 *** training ***
06/01/2019 11:42:44 step: 6473, epoch: 196, batch: 4, loss: 0.0208741407841444, acc: 100.0, f1: 100.0, r: 0.8038888047151168
06/01/2019 11:42:45 step: 6478, epoch: 196, batch: 9, loss: 0.011907732114195824, acc: 100.0, f1: 100.0, r: 0.8268214315519032
06/01/2019 11:42:45 step: 6483, epoch: 196, batch: 14, loss: 0.01090640015900135, acc: 100.0, f1: 100.0, r: 0.7068999494403381
06/01/2019 11:42:46 step: 6488, epoch: 196, batch: 19, loss: 0.012797728180885315, acc: 100.0, f1: 100.0, r: 0.8368684598836064
06/01/2019 11:42:46 step: 6493, epoch: 196, batch: 24, loss: 0.008776229806244373, acc: 100.0, f1: 100.0, r: 0.8116544445167112
06/01/2019 11:42:47 step: 6498, epoch: 196, batch: 29, loss: 0.014021012932062149, acc: 100.0, f1: 100.0, r: 0.8051070928128548
06/01/2019 11:42:47 *** evaluating ***
06/01/2019 11:42:47 step: 197, epoch: 196, acc: 56.41025641025641, f1: 27.91375632927743, r: 0.26380918224584554
06/01/2019 11:42:47 *** epoch: 198 ***
06/01/2019 11:42:47 *** training ***
06/01/2019 11:42:48 step: 6506, epoch: 197, batch: 4, loss: 0.03847797214984894, acc: 100.0, f1: 100.0, r: 0.788905312129941
06/01/2019 11:42:49 step: 6511, epoch: 197, batch: 9, loss: 0.019687356427311897, acc: 100.0, f1: 100.0, r: 0.6908937866570574
06/01/2019 11:42:49 step: 6516, epoch: 197, batch: 14, loss: 0.38791748881340027, acc: 100.0, f1: 100.0, r: 0.7268346431881286
06/01/2019 11:42:50 step: 6521, epoch: 197, batch: 19, loss: 0.012315673753619194, acc: 100.0, f1: 100.0, r: 0.8298057001998363
06/01/2019 11:42:51 step: 6526, epoch: 197, batch: 24, loss: 0.02248622477054596, acc: 100.0, f1: 100.0, r: 0.7145734558110449
06/01/2019 11:42:51 step: 6531, epoch: 197, batch: 29, loss: 0.015290779992938042, acc: 100.0, f1: 100.0, r: 0.74469109514386
06/01/2019 11:42:52 *** evaluating ***
06/01/2019 11:42:52 step: 198, epoch: 197, acc: 55.55555555555556, f1: 27.169018404122358, r: 0.26197722398822065
06/01/2019 11:42:52 *** epoch: 199 ***
06/01/2019 11:42:52 *** training ***
06/01/2019 11:42:52 step: 6539, epoch: 198, batch: 4, loss: 0.01589054986834526, acc: 100.0, f1: 100.0, r: 0.8239809979479258
06/01/2019 11:42:53 step: 6544, epoch: 198, batch: 9, loss: 0.01845860853791237, acc: 100.0, f1: 100.0, r: 0.8060927210363344
06/01/2019 11:42:54 step: 6549, epoch: 198, batch: 14, loss: 0.011357772164046764, acc: 100.0, f1: 100.0, r: 0.7803171738544061
06/01/2019 11:42:54 step: 6554, epoch: 198, batch: 19, loss: 0.012078637257218361, acc: 100.0, f1: 100.0, r: 0.7888820936184002
06/01/2019 11:42:55 step: 6559, epoch: 198, batch: 24, loss: 0.007286150008440018, acc: 100.0, f1: 100.0, r: 0.6756702043406224
06/01/2019 11:42:55 step: 6564, epoch: 198, batch: 29, loss: 0.02108966000378132, acc: 100.0, f1: 100.0, r: 0.7098155564452295
06/01/2019 11:42:56 *** evaluating ***
06/01/2019 11:42:56 step: 199, epoch: 198, acc: 53.41880341880342, f1: 23.828925338428935, r: 0.25633752329180004
06/01/2019 11:42:56 *** epoch: 200 ***
06/01/2019 11:42:56 *** training ***
06/01/2019 11:42:56 step: 6572, epoch: 199, batch: 4, loss: 0.00562022440135479, acc: 100.0, f1: 100.0, r: 0.5099817251953087
06/01/2019 11:42:57 step: 6577, epoch: 199, batch: 9, loss: 0.012110843323171139, acc: 100.0, f1: 100.0, r: 0.7291755344975691
06/01/2019 11:42:58 step: 6582, epoch: 199, batch: 14, loss: 0.007147895637899637, acc: 100.0, f1: 100.0, r: 0.7593055638412465
06/01/2019 11:42:58 step: 6587, epoch: 199, batch: 19, loss: 0.42079150676727295, acc: 100.0, f1: 100.0, r: 0.7833018744582654
06/01/2019 11:42:59 step: 6592, epoch: 199, batch: 24, loss: 0.028162728995084763, acc: 98.4375, f1: 97.6023976023976, r: 0.681747346021367
06/01/2019 11:43:00 step: 6597, epoch: 199, batch: 29, loss: 0.018017735332250595, acc: 100.0, f1: 100.0, r: 0.7985514778738996
06/01/2019 11:43:00 *** evaluating ***
06/01/2019 11:43:00 step: 200, epoch: 199, acc: 55.55555555555556, f1: 24.9465182533437, r: 0.2618233408739838
06/01/2019 11:43:00 *** epoch: 201 ***
06/01/2019 11:43:00 *** training ***
06/01/2019 11:43:01 step: 6605, epoch: 200, batch: 4, loss: 0.016391970217227936, acc: 100.0, f1: 100.0, r: 0.8013547465483506
06/01/2019 11:43:02 step: 6610, epoch: 200, batch: 9, loss: 0.023960275575518608, acc: 100.0, f1: 100.0, r: 0.6759605166771363
06/01/2019 11:43:02 step: 6615, epoch: 200, batch: 14, loss: 0.013503526337444782, acc: 100.0, f1: 100.0, r: 0.7813084880071444
06/01/2019 11:43:03 step: 6620, epoch: 200, batch: 19, loss: 0.015073657035827637, acc: 100.0, f1: 100.0, r: 0.7896539000439221
06/01/2019 11:43:03 step: 6625, epoch: 200, batch: 24, loss: 0.016598114743828773, acc: 100.0, f1: 100.0, r: 0.7708432987121887
06/01/2019 11:43:04 step: 6630, epoch: 200, batch: 29, loss: 0.016510428860783577, acc: 100.0, f1: 100.0, r: 0.7746333924764146
06/01/2019 11:43:04 *** evaluating ***
06/01/2019 11:43:05 step: 201, epoch: 200, acc: 55.12820512820513, f1: 25.108805546036873, r: 0.2621052201640795
06/01/2019 11:43:05 *** epoch: 202 ***
06/01/2019 11:43:05 *** training ***
06/01/2019 11:43:05 step: 6638, epoch: 201, batch: 4, loss: 0.014197414740920067, acc: 100.0, f1: 100.0, r: 0.7120810168503874
06/01/2019 11:43:06 step: 6643, epoch: 201, batch: 9, loss: 0.017261192202568054, acc: 100.0, f1: 100.0, r: 0.7960485364811465
06/01/2019 11:43:07 step: 6648, epoch: 201, batch: 14, loss: 0.014470134861767292, acc: 100.0, f1: 100.0, r: 0.738287682032989
06/01/2019 11:43:07 step: 6653, epoch: 201, batch: 19, loss: 0.01715840958058834, acc: 100.0, f1: 100.0, r: 0.6829015575093892
06/01/2019 11:43:08 step: 6658, epoch: 201, batch: 24, loss: 0.011479897424578667, acc: 100.0, f1: 100.0, r: 0.706229560597703
06/01/2019 11:43:08 step: 6663, epoch: 201, batch: 29, loss: 0.0065391650423407555, acc: 100.0, f1: 100.0, r: 0.8202747087251259
06/01/2019 11:43:09 *** evaluating ***
06/01/2019 11:43:09 step: 202, epoch: 201, acc: 55.98290598290598, f1: 27.360014959618905, r: 0.2606169641763457
06/01/2019 11:43:09 *** epoch: 203 ***
06/01/2019 11:43:09 *** training ***
06/01/2019 11:43:10 step: 6671, epoch: 202, batch: 4, loss: 0.011290188878774643, acc: 100.0, f1: 100.0, r: 0.8191828991649313
06/01/2019 11:43:10 step: 6676, epoch: 202, batch: 9, loss: 0.04279158264398575, acc: 100.0, f1: 100.0, r: 0.7386708613130819
06/01/2019 11:43:11 step: 6681, epoch: 202, batch: 14, loss: 0.02891046181321144, acc: 98.4375, f1: 94.66666666666667, r: 0.6978506684702708
06/01/2019 11:43:12 step: 6686, epoch: 202, batch: 19, loss: 0.01217376347631216, acc: 100.0, f1: 100.0, r: 0.7187069674480069
06/01/2019 11:43:12 step: 6691, epoch: 202, batch: 24, loss: 0.013113582506775856, acc: 100.0, f1: 100.0, r: 0.7011771191240196
06/01/2019 11:43:13 step: 6696, epoch: 202, batch: 29, loss: 0.012936778366565704, acc: 100.0, f1: 100.0, r: 0.8005839599134509
06/01/2019 11:43:13 *** evaluating ***
06/01/2019 11:43:13 step: 203, epoch: 202, acc: 55.12820512820513, f1: 25.267318710489782, r: 0.26235388478034166
06/01/2019 11:43:13 *** epoch: 204 ***
06/01/2019 11:43:13 *** training ***
06/01/2019 11:43:14 step: 6704, epoch: 203, batch: 4, loss: 0.034589871764183044, acc: 100.0, f1: 100.0, r: 0.8004400268658025
06/01/2019 11:43:15 step: 6709, epoch: 203, batch: 9, loss: 0.4106364846229553, acc: 100.0, f1: 100.0, r: 0.7640789256618521
06/01/2019 11:43:15 step: 6714, epoch: 203, batch: 14, loss: 0.09036437422037125, acc: 100.0, f1: 100.0, r: 0.8176385106611469
06/01/2019 11:43:16 step: 6719, epoch: 203, batch: 19, loss: 0.024145014584064484, acc: 100.0, f1: 100.0, r: 0.7073517711653211
06/01/2019 11:43:16 step: 6724, epoch: 203, batch: 24, loss: 0.010108470916748047, acc: 100.0, f1: 100.0, r: 0.7262998940627936
06/01/2019 11:43:17 step: 6729, epoch: 203, batch: 29, loss: 0.018171843141317368, acc: 100.0, f1: 100.0, r: 0.6763886834548423
06/01/2019 11:43:17 *** evaluating ***
06/01/2019 11:43:18 step: 204, epoch: 203, acc: 55.98290598290598, f1: 27.525098632165268, r: 0.2643369983607979
06/01/2019 11:43:18 *** epoch: 205 ***
06/01/2019 11:43:18 *** training ***
06/01/2019 11:43:18 step: 6737, epoch: 204, batch: 4, loss: 0.009627558290958405, acc: 100.0, f1: 100.0, r: 0.773616439689111
06/01/2019 11:43:19 step: 6742, epoch: 204, batch: 9, loss: 0.0162351131439209, acc: 100.0, f1: 100.0, r: 0.5741452051660045
06/01/2019 11:43:19 step: 6747, epoch: 204, batch: 14, loss: 0.01866026595234871, acc: 100.0, f1: 100.0, r: 0.6897054019481895
06/01/2019 11:43:20 step: 6752, epoch: 204, batch: 19, loss: 0.02123461291193962, acc: 100.0, f1: 100.0, r: 0.7135376901272324
06/01/2019 11:43:21 step: 6757, epoch: 204, batch: 24, loss: 0.37139463424682617, acc: 100.0, f1: 100.0, r: 0.7413836890476748
06/01/2019 11:43:21 step: 6762, epoch: 204, batch: 29, loss: 0.011987346224486828, acc: 100.0, f1: 100.0, r: 0.7617199815545782
06/01/2019 11:43:22 *** evaluating ***
06/01/2019 11:43:22 step: 205, epoch: 204, acc: 55.12820512820513, f1: 25.12997723140306, r: 0.26231776257882355
06/01/2019 11:43:22 *** epoch: 206 ***
06/01/2019 11:43:22 *** training ***
06/01/2019 11:43:22 step: 6770, epoch: 205, batch: 4, loss: 0.02027086168527603, acc: 100.0, f1: 100.0, r: 0.8627606160118455
06/01/2019 11:43:23 step: 6775, epoch: 205, batch: 9, loss: 0.03772106021642685, acc: 98.4375, f1: 97.953216374269, r: 0.8341641969618664
06/01/2019 11:43:24 step: 6780, epoch: 205, batch: 14, loss: 0.016837993636727333, acc: 100.0, f1: 100.0, r: 0.6709459779364815
06/01/2019 11:43:24 step: 6785, epoch: 205, batch: 19, loss: 0.022017601877450943, acc: 100.0, f1: 100.0, r: 0.6929673125432403
06/01/2019 11:43:25 step: 6790, epoch: 205, batch: 24, loss: 0.00806199386715889, acc: 100.0, f1: 100.0, r: 0.7591827654123218
06/01/2019 11:43:26 step: 6795, epoch: 205, batch: 29, loss: 0.009435158222913742, acc: 100.0, f1: 100.0, r: 0.7472123769150054
06/01/2019 11:43:26 *** evaluating ***
06/01/2019 11:43:26 step: 206, epoch: 205, acc: 55.98290598290598, f1: 27.297825904892537, r: 0.2607501926303522
06/01/2019 11:43:26 *** epoch: 207 ***
06/01/2019 11:43:26 *** training ***
06/01/2019 11:43:27 step: 6803, epoch: 206, batch: 4, loss: 0.017307747155427933, acc: 100.0, f1: 100.0, r: 0.7366396499751579
06/01/2019 11:43:27 step: 6808, epoch: 206, batch: 9, loss: 0.015737401321530342, acc: 100.0, f1: 100.0, r: 0.741173753763284
06/01/2019 11:43:28 step: 6813, epoch: 206, batch: 14, loss: 0.007960250601172447, acc: 100.0, f1: 100.0, r: 0.7737636290260343
06/01/2019 11:43:29 step: 6818, epoch: 206, batch: 19, loss: 0.013038731180131435, acc: 100.0, f1: 100.0, r: 0.686527513317386
06/01/2019 11:43:29 step: 6823, epoch: 206, batch: 24, loss: 0.011993980035185814, acc: 100.0, f1: 100.0, r: 0.7220847259732709
06/01/2019 11:43:30 step: 6828, epoch: 206, batch: 29, loss: 0.40837040543556213, acc: 100.0, f1: 100.0, r: 0.7829183185425135
06/01/2019 11:43:30 *** evaluating ***
06/01/2019 11:43:30 step: 207, epoch: 206, acc: 55.55555555555556, f1: 25.275524804698613, r: 0.2640821274201935
06/01/2019 11:43:30 *** epoch: 208 ***
06/01/2019 11:43:30 *** training ***
06/01/2019 11:43:31 step: 6836, epoch: 207, batch: 4, loss: 0.009326926432549953, acc: 100.0, f1: 100.0, r: 0.7891555969995772
06/01/2019 11:43:32 step: 6841, epoch: 207, batch: 9, loss: 0.03638545423746109, acc: 100.0, f1: 100.0, r: 0.8206105725402474
06/01/2019 11:43:32 step: 6846, epoch: 207, batch: 14, loss: 0.007088086102157831, acc: 100.0, f1: 100.0, r: 0.8385783117388107
06/01/2019 11:43:33 step: 6851, epoch: 207, batch: 19, loss: 0.04295646771788597, acc: 100.0, f1: 100.0, r: 0.7158409197455486
06/01/2019 11:43:34 step: 6856, epoch: 207, batch: 24, loss: 0.01917879655957222, acc: 100.0, f1: 100.0, r: 0.750774156594884
06/01/2019 11:43:34 step: 6861, epoch: 207, batch: 29, loss: 0.02207641303539276, acc: 100.0, f1: 100.0, r: 0.8374244821172214
06/01/2019 11:43:35 *** evaluating ***
06/01/2019 11:43:35 step: 208, epoch: 207, acc: 54.700854700854705, f1: 24.522007557130863, r: 0.25901515310113704
06/01/2019 11:43:35 *** epoch: 209 ***
06/01/2019 11:43:35 *** training ***
06/01/2019 11:43:35 step: 6869, epoch: 208, batch: 4, loss: 0.3706929087638855, acc: 100.0, f1: 100.0, r: 0.787514857514168
06/01/2019 11:43:36 step: 6874, epoch: 208, batch: 9, loss: 0.007710210047662258, acc: 100.0, f1: 100.0, r: 0.7812349426233847
06/01/2019 11:43:37 step: 6879, epoch: 208, batch: 14, loss: 0.010532493703067303, acc: 100.0, f1: 100.0, r: 0.7207248358687564
06/01/2019 11:43:37 step: 6884, epoch: 208, batch: 19, loss: 0.012575197964906693, acc: 100.0, f1: 100.0, r: 0.7360707486481137
06/01/2019 11:43:38 step: 6889, epoch: 208, batch: 24, loss: 0.011294889263808727, acc: 100.0, f1: 100.0, r: 0.7722898473719242
06/01/2019 11:43:38 step: 6894, epoch: 208, batch: 29, loss: 0.032176364213228226, acc: 100.0, f1: 100.0, r: 0.7430565993943667
06/01/2019 11:43:39 *** evaluating ***
06/01/2019 11:43:39 step: 209, epoch: 208, acc: 55.55555555555556, f1: 27.230886919159296, r: 0.25736465879171383
06/01/2019 11:43:39 *** epoch: 210 ***
06/01/2019 11:43:39 *** training ***
06/01/2019 11:43:39 step: 6902, epoch: 209, batch: 4, loss: 0.014346965588629246, acc: 100.0, f1: 100.0, r: 0.6344334263412414
06/01/2019 11:43:40 step: 6907, epoch: 209, batch: 9, loss: 0.016715675592422485, acc: 100.0, f1: 100.0, r: 0.7050786173828003
06/01/2019 11:43:41 step: 6912, epoch: 209, batch: 14, loss: 0.008830145001411438, acc: 100.0, f1: 100.0, r: 0.851899600053682
06/01/2019 11:43:42 step: 6917, epoch: 209, batch: 19, loss: 0.014246195554733276, acc: 100.0, f1: 100.0, r: 0.7275999547322165
06/01/2019 11:43:42 step: 6922, epoch: 209, batch: 24, loss: 0.00862155482172966, acc: 100.0, f1: 100.0, r: 0.8288823769500923
06/01/2019 11:43:43 step: 6927, epoch: 209, batch: 29, loss: 0.008095061406493187, acc: 100.0, f1: 100.0, r: 0.7232916711390094
06/01/2019 11:43:43 *** evaluating ***
06/01/2019 11:43:43 step: 210, epoch: 209, acc: 54.700854700854705, f1: 26.951962124562744, r: 0.25789488671566657
06/01/2019 11:43:43 *** epoch: 211 ***
06/01/2019 11:43:43 *** training ***
06/01/2019 11:43:44 step: 6935, epoch: 210, batch: 4, loss: 0.013858121819794178, acc: 100.0, f1: 100.0, r: 0.7306859982068821
06/01/2019 11:43:44 step: 6940, epoch: 210, batch: 9, loss: 0.3618317246437073, acc: 100.0, f1: 100.0, r: 0.8001447798494853
06/01/2019 11:43:45 step: 6945, epoch: 210, batch: 14, loss: 0.012590092606842518, acc: 100.0, f1: 100.0, r: 0.6753869571067461
06/01/2019 11:43:46 step: 6950, epoch: 210, batch: 19, loss: 0.019637584686279297, acc: 100.0, f1: 100.0, r: 0.736046169588139
06/01/2019 11:43:46 step: 6955, epoch: 210, batch: 24, loss: 0.03644460439682007, acc: 100.0, f1: 100.0, r: 0.7536937258675663
06/01/2019 11:43:47 step: 6960, epoch: 210, batch: 29, loss: 0.017933541908860207, acc: 100.0, f1: 100.0, r: 0.8202256780811156
06/01/2019 11:43:47 *** evaluating ***
06/01/2019 11:43:47 step: 211, epoch: 210, acc: 54.700854700854705, f1: 24.37243333534078, r: 0.2534518936994453
06/01/2019 11:43:47 *** epoch: 212 ***
06/01/2019 11:43:47 *** training ***
06/01/2019 11:43:48 step: 6968, epoch: 211, batch: 4, loss: 0.011355134658515453, acc: 100.0, f1: 100.0, r: 0.7543908500305035
06/01/2019 11:43:49 step: 6973, epoch: 211, batch: 9, loss: 0.031027864664793015, acc: 98.4375, f1: 98.07692307692307, r: 0.7353929459905462
06/01/2019 11:43:49 step: 6978, epoch: 211, batch: 14, loss: 0.3643653988838196, acc: 100.0, f1: 100.0, r: 0.8285974829317325
06/01/2019 11:43:50 step: 6983, epoch: 211, batch: 19, loss: 0.06411127746105194, acc: 100.0, f1: 100.0, r: 0.7275756561175223
06/01/2019 11:43:51 step: 6988, epoch: 211, batch: 24, loss: 0.017641322687268257, acc: 100.0, f1: 100.0, r: 0.745086356352542
06/01/2019 11:43:51 step: 6993, epoch: 211, batch: 29, loss: 0.013455690816044807, acc: 100.0, f1: 100.0, r: 0.6652683133354328
06/01/2019 11:43:52 *** evaluating ***
06/01/2019 11:43:52 step: 212, epoch: 211, acc: 56.41025641025641, f1: 27.807758191982444, r: 0.2581207439364994
06/01/2019 11:43:52 *** epoch: 213 ***
06/01/2019 11:43:52 *** training ***
06/01/2019 11:43:52 step: 7001, epoch: 212, batch: 4, loss: 0.3995840847492218, acc: 100.0, f1: 100.0, r: 0.7605633995429484
06/01/2019 11:43:53 step: 7006, epoch: 212, batch: 9, loss: 0.02146277017891407, acc: 100.0, f1: 100.0, r: 0.6977062842882733
06/01/2019 11:43:54 step: 7011, epoch: 212, batch: 14, loss: 0.011638117022812366, acc: 100.0, f1: 100.0, r: 0.8188572866187561
06/01/2019 11:43:54 step: 7016, epoch: 212, batch: 19, loss: 0.017707590013742447, acc: 100.0, f1: 100.0, r: 0.8426103640494318
06/01/2019 11:43:55 step: 7021, epoch: 212, batch: 24, loss: 0.01771559752523899, acc: 100.0, f1: 100.0, r: 0.7750818543902593
06/01/2019 11:43:56 step: 7026, epoch: 212, batch: 29, loss: 0.008802445605397224, acc: 100.0, f1: 100.0, r: 0.7689586158418416
06/01/2019 11:43:56 *** evaluating ***
06/01/2019 11:43:56 step: 213, epoch: 212, acc: 55.55555555555556, f1: 24.78410517484784, r: 0.2586125859550877
06/01/2019 11:43:56 *** epoch: 214 ***
06/01/2019 11:43:56 *** training ***
06/01/2019 11:43:57 step: 7034, epoch: 213, batch: 4, loss: 0.0057929884642362595, acc: 100.0, f1: 100.0, r: 0.7142071183890428
06/01/2019 11:43:57 step: 7039, epoch: 213, batch: 9, loss: 0.00712253712117672, acc: 100.0, f1: 100.0, r: 0.7514650668264069
06/01/2019 11:43:58 step: 7044, epoch: 213, batch: 14, loss: 0.010238037444651127, acc: 100.0, f1: 100.0, r: 0.7964107503661609
06/01/2019 11:43:59 step: 7049, epoch: 213, batch: 19, loss: 0.030255436897277832, acc: 100.0, f1: 100.0, r: 0.7351056583953391
06/01/2019 11:43:59 step: 7054, epoch: 213, batch: 24, loss: 0.012356223538517952, acc: 100.0, f1: 100.0, r: 0.7062670598311936
06/01/2019 11:44:00 step: 7059, epoch: 213, batch: 29, loss: 0.04988407343626022, acc: 98.4375, f1: 96.30252100840336, r: 0.7140264480335365
06/01/2019 11:44:00 *** evaluating ***
06/01/2019 11:44:01 step: 214, epoch: 213, acc: 56.41025641025641, f1: 25.527752603017497, r: 0.2647982274191241
06/01/2019 11:44:01 *** epoch: 215 ***
06/01/2019 11:44:01 *** training ***
06/01/2019 11:44:01 step: 7067, epoch: 214, batch: 4, loss: 0.006101977080106735, acc: 100.0, f1: 100.0, r: 0.8142449973038047
06/01/2019 11:44:02 step: 7072, epoch: 214, batch: 9, loss: 0.01454496942460537, acc: 100.0, f1: 100.0, r: 0.8088321852165434
06/01/2019 11:44:03 step: 7077, epoch: 214, batch: 14, loss: 0.023144230246543884, acc: 100.0, f1: 100.0, r: 0.7485480749828257
06/01/2019 11:44:03 step: 7082, epoch: 214, batch: 19, loss: 0.6882555484771729, acc: 98.4375, f1: 96.82539682539682, r: 0.5868130775485537
06/01/2019 11:44:04 step: 7087, epoch: 214, batch: 24, loss: 0.012601863592863083, acc: 100.0, f1: 100.0, r: 0.8197027332895903
06/01/2019 11:44:05 step: 7092, epoch: 214, batch: 29, loss: 0.009974387474358082, acc: 100.0, f1: 100.0, r: 0.8101915328161561
06/01/2019 11:44:05 *** evaluating ***
06/01/2019 11:44:05 step: 215, epoch: 214, acc: 54.27350427350427, f1: 23.683727771892794, r: 0.25130826666705064
06/01/2019 11:44:05 *** epoch: 216 ***
06/01/2019 11:44:05 *** training ***
06/01/2019 11:44:06 step: 7100, epoch: 215, batch: 4, loss: 0.04027961939573288, acc: 100.0, f1: 100.0, r: 0.6558554876046232
06/01/2019 11:44:06 step: 7105, epoch: 215, batch: 9, loss: 0.013347053900361061, acc: 100.0, f1: 100.0, r: 0.6730883327378323
06/01/2019 11:44:07 step: 7110, epoch: 215, batch: 14, loss: 0.016305886209011078, acc: 100.0, f1: 100.0, r: 0.7931977684869068
06/01/2019 11:44:08 step: 7115, epoch: 215, batch: 19, loss: 0.009582639671862125, acc: 100.0, f1: 100.0, r: 0.6932139366430853
06/01/2019 11:44:08 step: 7120, epoch: 215, batch: 24, loss: 0.012147873640060425, acc: 100.0, f1: 100.0, r: 0.6802683595096435
06/01/2019 11:44:09 step: 7125, epoch: 215, batch: 29, loss: 0.019076449796557426, acc: 100.0, f1: 100.0, r: 0.6438160999717604
06/01/2019 11:44:09 *** evaluating ***
06/01/2019 11:44:10 step: 216, epoch: 215, acc: 56.41025641025641, f1: 25.577048334271822, r: 0.2633164938172492
06/01/2019 11:44:10 *** epoch: 217 ***
06/01/2019 11:44:10 *** training ***
06/01/2019 11:44:10 step: 7133, epoch: 216, batch: 4, loss: 0.011312056332826614, acc: 100.0, f1: 100.0, r: 0.7926185071238305
06/01/2019 11:44:11 step: 7138, epoch: 216, batch: 9, loss: 0.008365929126739502, acc: 100.0, f1: 100.0, r: 0.7231677627368003
06/01/2019 11:44:12 step: 7143, epoch: 216, batch: 14, loss: 0.023790355771780014, acc: 98.4375, f1: 96.42857142857143, r: 0.7758652199876456
06/01/2019 11:44:12 step: 7148, epoch: 216, batch: 19, loss: 0.021879158914089203, acc: 100.0, f1: 100.0, r: 0.647881712049774
06/01/2019 11:44:13 step: 7153, epoch: 216, batch: 24, loss: 0.016007404774427414, acc: 100.0, f1: 100.0, r: 0.7112391448636824
06/01/2019 11:44:13 step: 7158, epoch: 216, batch: 29, loss: 0.009921644814312458, acc: 100.0, f1: 100.0, r: 0.6903481577069436
06/01/2019 11:44:13 *** evaluating ***
06/01/2019 11:44:14 step: 217, epoch: 216, acc: 56.41025641025641, f1: 27.91375632927743, r: 0.2638057960409823
06/01/2019 11:44:14 *** epoch: 218 ***
06/01/2019 11:44:14 *** training ***
06/01/2019 11:44:14 step: 7166, epoch: 217, batch: 4, loss: 0.03321614861488342, acc: 100.0, f1: 100.0, r: 0.6552840789729809
06/01/2019 11:44:15 step: 7171, epoch: 217, batch: 9, loss: 0.38234591484069824, acc: 100.0, f1: 100.0, r: 0.7697148729809635
06/01/2019 11:44:15 step: 7176, epoch: 217, batch: 14, loss: 0.01109365001320839, acc: 100.0, f1: 100.0, r: 0.8152563762804581
06/01/2019 11:44:16 step: 7181, epoch: 217, batch: 19, loss: 0.006227528676390648, acc: 100.0, f1: 100.0, r: 0.7774960574021467
06/01/2019 11:44:17 step: 7186, epoch: 217, batch: 24, loss: 0.010442416183650494, acc: 100.0, f1: 100.0, r: 0.747341543232553
06/01/2019 11:44:17 step: 7191, epoch: 217, batch: 29, loss: 0.010729089379310608, acc: 100.0, f1: 100.0, r: 0.7565590088021268
06/01/2019 11:44:18 *** evaluating ***
06/01/2019 11:44:18 step: 218, epoch: 217, acc: 55.98290598290598, f1: 27.57585041592394, r: 0.26515815410016214
06/01/2019 11:44:18 *** epoch: 219 ***
06/01/2019 11:44:18 *** training ***
06/01/2019 11:44:19 step: 7199, epoch: 218, batch: 4, loss: 0.677505373954773, acc: 100.0, f1: 100.0, r: 0.7215817345039601
06/01/2019 11:44:19 step: 7204, epoch: 218, batch: 9, loss: 0.017492394894361496, acc: 100.0, f1: 100.0, r: 0.6856650702036122
06/01/2019 11:44:20 step: 7209, epoch: 218, batch: 14, loss: 0.03304252400994301, acc: 98.4375, f1: 96.84210526315789, r: 0.8040829258221592
06/01/2019 11:44:21 step: 7214, epoch: 218, batch: 19, loss: 0.013473842293024063, acc: 100.0, f1: 100.0, r: 0.6654456375650609
06/01/2019 11:44:21 step: 7219, epoch: 218, batch: 24, loss: 0.0076944176107645035, acc: 100.0, f1: 100.0, r: 0.6110713264936066
06/01/2019 11:44:22 step: 7224, epoch: 218, batch: 29, loss: 0.008371779695153236, acc: 100.0, f1: 100.0, r: 0.7935586043678886
06/01/2019 11:44:22 *** evaluating ***
06/01/2019 11:44:22 step: 219, epoch: 218, acc: 56.41025641025641, f1: 27.80385800391858, r: 0.2649374486996011
06/01/2019 11:44:22 *** epoch: 220 ***
06/01/2019 11:44:22 *** training ***
06/01/2019 11:44:23 step: 7232, epoch: 219, batch: 4, loss: 0.00892854668200016, acc: 100.0, f1: 100.0, r: 0.7882460325643312
06/01/2019 11:44:24 step: 7237, epoch: 219, batch: 9, loss: 0.014275997877120972, acc: 100.0, f1: 100.0, r: 0.7141379503912181
06/01/2019 11:44:24 step: 7242, epoch: 219, batch: 14, loss: 0.019664479419589043, acc: 100.0, f1: 100.0, r: 0.7421428716984589
06/01/2019 11:44:25 step: 7247, epoch: 219, batch: 19, loss: 0.005463624373078346, acc: 100.0, f1: 100.0, r: 0.834711135029293
06/01/2019 11:44:26 step: 7252, epoch: 219, batch: 24, loss: 0.028349123895168304, acc: 98.4375, f1: 96.66666666666667, r: 0.7568782288336287
06/01/2019 11:44:26 step: 7257, epoch: 219, batch: 29, loss: 0.005964142270386219, acc: 100.0, f1: 100.0, r: 0.5935710004216652
06/01/2019 11:44:27 *** evaluating ***
06/01/2019 11:44:27 step: 220, epoch: 219, acc: 56.41025641025641, f1: 28.00834891774632, r: 0.26605405410971883
06/01/2019 11:44:27 *** epoch: 221 ***
06/01/2019 11:44:27 *** training ***
06/01/2019 11:44:27 step: 7265, epoch: 220, batch: 4, loss: 0.016854168847203255, acc: 100.0, f1: 100.0, r: 0.7488317024060092
06/01/2019 11:44:28 step: 7270, epoch: 220, batch: 9, loss: 0.016365179792046547, acc: 100.0, f1: 100.0, r: 0.7645828065823556
06/01/2019 11:44:29 step: 7275, epoch: 220, batch: 14, loss: 0.6732781529426575, acc: 100.0, f1: 100.0, r: 0.7111180679549762
06/01/2019 11:44:29 step: 7280, epoch: 220, batch: 19, loss: 0.017606809735298157, acc: 100.0, f1: 100.0, r: 0.7256925736303247
06/01/2019 11:44:30 step: 7285, epoch: 220, batch: 24, loss: 0.025921251624822617, acc: 98.4375, f1: 97.79158040027606, r: 0.6955535506777566
06/01/2019 11:44:31 step: 7290, epoch: 220, batch: 29, loss: 0.013414994813501835, acc: 100.0, f1: 100.0, r: 0.7371861826579765
06/01/2019 11:44:31 *** evaluating ***
06/01/2019 11:44:31 step: 221, epoch: 220, acc: 55.55555555555556, f1: 25.177072723028605, r: 0.2667257611049638
06/01/2019 11:44:31 *** epoch: 222 ***
06/01/2019 11:44:31 *** training ***
06/01/2019 11:44:32 step: 7298, epoch: 221, batch: 4, loss: 0.017831072211265564, acc: 100.0, f1: 100.0, r: 0.776770986607917
06/01/2019 11:44:32 step: 7303, epoch: 221, batch: 9, loss: 0.008955823257565498, acc: 100.0, f1: 100.0, r: 0.8011437435170605
06/01/2019 11:44:33 step: 7308, epoch: 221, batch: 14, loss: 0.010937589220702648, acc: 100.0, f1: 100.0, r: 0.8367223879242833
06/01/2019 11:44:34 step: 7313, epoch: 221, batch: 19, loss: 0.01069130189716816, acc: 100.0, f1: 100.0, r: 0.7741395765034051
06/01/2019 11:44:34 step: 7318, epoch: 221, batch: 24, loss: 0.005210413131862879, acc: 100.0, f1: 100.0, r: 0.85276018711019
06/01/2019 11:44:35 step: 7323, epoch: 221, batch: 29, loss: 0.010662602260708809, acc: 100.0, f1: 100.0, r: 0.6442672939002679
06/01/2019 11:44:35 *** evaluating ***
06/01/2019 11:44:35 step: 222, epoch: 221, acc: 55.98290598290598, f1: 27.778120424376525, r: 0.26970127284328677
06/01/2019 11:44:35 *** epoch: 223 ***
06/01/2019 11:44:35 *** training ***
06/01/2019 11:44:36 step: 7331, epoch: 222, batch: 4, loss: 0.006155459675937891, acc: 100.0, f1: 100.0, r: 0.719074468150922
06/01/2019 11:44:37 step: 7336, epoch: 222, batch: 9, loss: 0.012535156682133675, acc: 100.0, f1: 100.0, r: 0.666870178270647
06/01/2019 11:44:37 step: 7341, epoch: 222, batch: 14, loss: 0.012432403862476349, acc: 100.0, f1: 100.0, r: 0.7228289789626653
06/01/2019 11:44:38 step: 7346, epoch: 222, batch: 19, loss: 0.034768618643283844, acc: 100.0, f1: 100.0, r: 0.7814219478582055
06/01/2019 11:44:38 step: 7351, epoch: 222, batch: 24, loss: 0.031244123354554176, acc: 100.0, f1: 100.0, r: 0.7158581676596103
06/01/2019 11:44:39 step: 7356, epoch: 222, batch: 29, loss: 0.37204331159591675, acc: 100.0, f1: 100.0, r: 0.5823997475443782
06/01/2019 11:44:39 *** evaluating ***
06/01/2019 11:44:40 step: 223, epoch: 222, acc: 55.98290598290598, f1: 25.50834891774632, r: 0.26445282056829844
06/01/2019 11:44:40 *** epoch: 224 ***
06/01/2019 11:44:40 *** training ***
06/01/2019 11:44:40 step: 7364, epoch: 223, batch: 4, loss: 0.011197192594408989, acc: 100.0, f1: 100.0, r: 0.7789511342214169
06/01/2019 11:44:41 step: 7369, epoch: 223, batch: 9, loss: 0.013871317729353905, acc: 100.0, f1: 100.0, r: 0.7570137953396002
06/01/2019 11:44:41 step: 7374, epoch: 223, batch: 14, loss: 0.04307007044553757, acc: 100.0, f1: 100.0, r: 0.8004202406286456
06/01/2019 11:44:42 step: 7379, epoch: 223, batch: 19, loss: 0.05639661103487015, acc: 100.0, f1: 100.0, r: 0.6986270024697045
06/01/2019 11:44:43 step: 7384, epoch: 223, batch: 24, loss: 0.010837562382221222, acc: 100.0, f1: 100.0, r: 0.8100438860741329
06/01/2019 11:44:43 step: 7389, epoch: 223, batch: 29, loss: 0.03211240470409393, acc: 100.0, f1: 100.0, r: 0.7175267121862091
06/01/2019 11:44:44 *** evaluating ***
06/01/2019 11:44:44 step: 224, epoch: 223, acc: 55.12820512820513, f1: 24.639655064577667, r: 0.259410549462628
06/01/2019 11:44:44 *** epoch: 225 ***
06/01/2019 11:44:44 *** training ***
06/01/2019 11:44:45 step: 7397, epoch: 224, batch: 4, loss: 0.008368906565010548, acc: 100.0, f1: 100.0, r: 0.8386495303075253
06/01/2019 11:44:45 step: 7402, epoch: 224, batch: 9, loss: 0.011959068477153778, acc: 100.0, f1: 100.0, r: 0.8282254722625007
06/01/2019 11:44:46 step: 7407, epoch: 224, batch: 14, loss: 0.03934289142489433, acc: 100.0, f1: 100.0, r: 0.8401602705457237
06/01/2019 11:44:47 step: 7412, epoch: 224, batch: 19, loss: 0.021390730515122414, acc: 100.0, f1: 100.0, r: 0.8000866722464792
06/01/2019 11:44:47 step: 7417, epoch: 224, batch: 24, loss: 0.012781478464603424, acc: 100.0, f1: 100.0, r: 0.6798408283757128
06/01/2019 11:44:48 step: 7422, epoch: 224, batch: 29, loss: 0.008542206138372421, acc: 100.0, f1: 100.0, r: 0.8162273938703677
06/01/2019 11:44:48 *** evaluating ***
06/01/2019 11:44:48 step: 225, epoch: 224, acc: 56.837606837606835, f1: 28.09482575361467, r: 0.2661445851279117
06/01/2019 11:44:48 *** epoch: 226 ***
06/01/2019 11:44:48 *** training ***
06/01/2019 11:44:49 step: 7430, epoch: 225, batch: 4, loss: 0.4062163233757019, acc: 100.0, f1: 100.0, r: 0.6826126271823678
06/01/2019 11:44:50 step: 7435, epoch: 225, batch: 9, loss: 0.004792485851794481, acc: 100.0, f1: 100.0, r: 0.8222558687263005
06/01/2019 11:44:51 step: 7440, epoch: 225, batch: 14, loss: 0.012974106706678867, acc: 100.0, f1: 100.0, r: 0.7628487337683245
06/01/2019 11:44:51 step: 7445, epoch: 225, batch: 19, loss: 0.006227698177099228, acc: 100.0, f1: 100.0, r: 0.7244363057735316
06/01/2019 11:44:52 step: 7450, epoch: 225, batch: 24, loss: 0.01900964044034481, acc: 100.0, f1: 100.0, r: 0.677820245330055
06/01/2019 11:44:53 step: 7455, epoch: 225, batch: 29, loss: 0.024625688791275024, acc: 100.0, f1: 100.0, r: 0.7889019990275473
06/01/2019 11:44:53 *** evaluating ***
06/01/2019 11:44:53 step: 226, epoch: 225, acc: 55.12820512820513, f1: 25.126985571456306, r: 0.2639589831113066
06/01/2019 11:44:53 *** epoch: 227 ***
06/01/2019 11:44:53 *** training ***
06/01/2019 11:44:54 step: 7463, epoch: 226, batch: 4, loss: 0.012567738071084023, acc: 100.0, f1: 100.0, r: 0.7412559278412505
06/01/2019 11:44:54 step: 7468, epoch: 226, batch: 9, loss: 0.021427474915981293, acc: 100.0, f1: 100.0, r: 0.7184741706970601
06/01/2019 11:44:55 step: 7473, epoch: 226, batch: 14, loss: 0.014942958950996399, acc: 100.0, f1: 100.0, r: 0.7156638494440157
06/01/2019 11:44:56 step: 7478, epoch: 226, batch: 19, loss: 0.005661608185619116, acc: 100.0, f1: 100.0, r: 0.664364897543642
06/01/2019 11:44:56 step: 7483, epoch: 226, batch: 24, loss: 0.013510061427950859, acc: 100.0, f1: 100.0, r: 0.7110466791286627
06/01/2019 11:44:57 step: 7488, epoch: 226, batch: 29, loss: 0.012517988681793213, acc: 100.0, f1: 100.0, r: 0.8201263294159225
06/01/2019 11:44:57 *** evaluating ***
06/01/2019 11:44:58 step: 227, epoch: 226, acc: 54.700854700854705, f1: 24.492964949137548, r: 0.2581039353407914
06/01/2019 11:44:58 *** epoch: 228 ***
06/01/2019 11:44:58 *** training ***
06/01/2019 11:44:58 step: 7496, epoch: 227, batch: 4, loss: 0.006087565794587135, acc: 100.0, f1: 100.0, r: 0.7031442884765831
06/01/2019 11:44:59 step: 7501, epoch: 227, batch: 9, loss: 0.0045896125957369804, acc: 100.0, f1: 100.0, r: 0.7791462300679166
06/01/2019 11:44:59 step: 7506, epoch: 227, batch: 14, loss: 0.014927271753549576, acc: 100.0, f1: 100.0, r: 0.7655958306545385
06/01/2019 11:45:00 step: 7511, epoch: 227, batch: 19, loss: 0.008845547214150429, acc: 100.0, f1: 100.0, r: 0.7429224088842818
06/01/2019 11:45:01 step: 7516, epoch: 227, batch: 24, loss: 0.036980561912059784, acc: 100.0, f1: 100.0, r: 0.7868457828748053
06/01/2019 11:45:02 step: 7521, epoch: 227, batch: 29, loss: 0.007701513823121786, acc: 100.0, f1: 100.0, r: 0.8161059162891405
06/01/2019 11:45:02 *** evaluating ***
06/01/2019 11:45:02 step: 228, epoch: 227, acc: 56.41025641025641, f1: 26.707201746752318, r: 0.2676918821648839
06/01/2019 11:45:02 *** epoch: 229 ***
06/01/2019 11:45:02 *** training ***
06/01/2019 11:45:03 step: 7529, epoch: 228, batch: 4, loss: 0.016640491783618927, acc: 100.0, f1: 100.0, r: 0.717074176749394
06/01/2019 11:45:03 step: 7534, epoch: 228, batch: 9, loss: 0.009907914325594902, acc: 100.0, f1: 100.0, r: 0.7377317873663619
06/01/2019 11:45:04 step: 7539, epoch: 228, batch: 14, loss: 0.01323956623673439, acc: 100.0, f1: 100.0, r: 0.7103830200536676
06/01/2019 11:45:05 step: 7544, epoch: 228, batch: 19, loss: 0.009591701440513134, acc: 100.0, f1: 100.0, r: 0.709544606097402
06/01/2019 11:45:05 step: 7549, epoch: 228, batch: 24, loss: 0.02392321452498436, acc: 100.0, f1: 100.0, r: 0.7921054708085459
06/01/2019 11:45:06 step: 7554, epoch: 228, batch: 29, loss: 0.010158359073102474, acc: 100.0, f1: 100.0, r: 0.7406423762513126
06/01/2019 11:45:06 *** evaluating ***
06/01/2019 11:45:06 step: 229, epoch: 228, acc: 56.837606837606835, f1: 27.98245574580293, r: 0.2645947046505531
06/01/2019 11:45:06 *** epoch: 230 ***
06/01/2019 11:45:06 *** training ***
06/01/2019 11:45:07 step: 7562, epoch: 229, batch: 4, loss: 0.014526477083563805, acc: 100.0, f1: 100.0, r: 0.5919519293630421
06/01/2019 11:45:08 step: 7567, epoch: 229, batch: 9, loss: 0.00998383667320013, acc: 100.0, f1: 100.0, r: 0.74000356543722
06/01/2019 11:45:08 step: 7572, epoch: 229, batch: 14, loss: 0.040236249566078186, acc: 98.4375, f1: 98.18007662835248, r: 0.7884823086913721
06/01/2019 11:45:09 step: 7577, epoch: 229, batch: 19, loss: 0.03348160535097122, acc: 100.0, f1: 100.0, r: 0.7853140936748254
06/01/2019 11:45:10 step: 7582, epoch: 229, batch: 24, loss: 0.00661873584613204, acc: 100.0, f1: 100.0, r: 0.7810080795169884
06/01/2019 11:45:10 step: 7587, epoch: 229, batch: 29, loss: 0.014634563587605953, acc: 100.0, f1: 100.0, r: 0.6910600141631078
06/01/2019 11:45:11 *** evaluating ***
06/01/2019 11:45:11 step: 230, epoch: 229, acc: 54.700854700854705, f1: 25.814655639864405, r: 0.261468032449327
06/01/2019 11:45:11 *** epoch: 231 ***
06/01/2019 11:45:11 *** training ***
06/01/2019 11:45:11 step: 7595, epoch: 230, batch: 4, loss: 0.00742516852915287, acc: 100.0, f1: 100.0, r: 0.6415300428691652
06/01/2019 11:45:12 step: 7600, epoch: 230, batch: 9, loss: 0.006628588307648897, acc: 100.0, f1: 100.0, r: 0.7846574029501734
06/01/2019 11:45:13 step: 7605, epoch: 230, batch: 14, loss: 0.01778077334165573, acc: 100.0, f1: 100.0, r: 0.8016857772233589
06/01/2019 11:45:13 step: 7610, epoch: 230, batch: 19, loss: 0.013246314600110054, acc: 100.0, f1: 100.0, r: 0.6644458205514336
06/01/2019 11:45:14 step: 7615, epoch: 230, batch: 24, loss: 0.01816486567258835, acc: 100.0, f1: 100.0, r: 0.8187704461701363
06/01/2019 11:45:15 step: 7620, epoch: 230, batch: 29, loss: 0.052547622472047806, acc: 100.0, f1: 100.0, r: 0.7673477587974612
06/01/2019 11:45:15 *** evaluating ***
06/01/2019 11:45:15 step: 231, epoch: 230, acc: 56.837606837606835, f1: 27.939681863446374, r: 0.2660051845703798
06/01/2019 11:45:15 *** epoch: 232 ***
06/01/2019 11:45:15 *** training ***
06/01/2019 11:45:16 step: 7628, epoch: 231, batch: 4, loss: 0.010431493632495403, acc: 100.0, f1: 100.0, r: 0.688773088057925
06/01/2019 11:45:16 step: 7633, epoch: 231, batch: 9, loss: 0.005014245864003897, acc: 100.0, f1: 100.0, r: 0.7387425091465838
06/01/2019 11:45:17 step: 7638, epoch: 231, batch: 14, loss: 0.005657354835420847, acc: 100.0, f1: 100.0, r: 0.8000577675700955
06/01/2019 11:45:18 step: 7643, epoch: 231, batch: 19, loss: 0.02176107093691826, acc: 100.0, f1: 100.0, r: 0.7297787024110065
06/01/2019 11:45:18 step: 7648, epoch: 231, batch: 24, loss: 0.01120750792324543, acc: 100.0, f1: 100.0, r: 0.5931312482948196
06/01/2019 11:45:19 step: 7653, epoch: 231, batch: 29, loss: 0.00850193202495575, acc: 100.0, f1: 100.0, r: 0.7799329463171198
06/01/2019 11:45:19 *** evaluating ***
06/01/2019 11:45:19 step: 232, epoch: 231, acc: 55.12820512820513, f1: 27.251445088518583, r: 0.26868282992531234
06/01/2019 11:45:19 *** epoch: 233 ***
06/01/2019 11:45:19 *** training ***
06/01/2019 11:45:20 step: 7661, epoch: 232, batch: 4, loss: 0.013725153170526028, acc: 100.0, f1: 100.0, r: 0.7183844351393599
06/01/2019 11:45:21 step: 7666, epoch: 232, batch: 9, loss: 0.01246457640081644, acc: 100.0, f1: 100.0, r: 0.6596548953587071
06/01/2019 11:45:21 step: 7671, epoch: 232, batch: 14, loss: 0.34251850843429565, acc: 100.0, f1: 100.0, r: 0.7544713083742638
06/01/2019 11:45:22 step: 7676, epoch: 232, batch: 19, loss: 0.0063720159232616425, acc: 100.0, f1: 100.0, r: 0.7731378543744958
06/01/2019 11:45:23 step: 7681, epoch: 232, batch: 24, loss: 0.42121970653533936, acc: 100.0, f1: 100.0, r: 0.8352570295392772
06/01/2019 11:45:23 step: 7686, epoch: 232, batch: 29, loss: 0.01394282653927803, acc: 100.0, f1: 100.0, r: 0.7353593070506678
06/01/2019 11:45:24 *** evaluating ***
06/01/2019 11:45:24 step: 233, epoch: 232, acc: 55.12820512820513, f1: 27.23068135067457, r: 0.26197315333947385
06/01/2019 11:45:24 *** epoch: 234 ***
06/01/2019 11:45:24 *** training ***
06/01/2019 11:45:24 step: 7694, epoch: 233, batch: 4, loss: 0.05263833329081535, acc: 100.0, f1: 100.0, r: 0.7199507485410291
06/01/2019 11:45:25 step: 7699, epoch: 233, batch: 9, loss: 0.43109753727912903, acc: 100.0, f1: 100.0, r: 0.6912291887261083
06/01/2019 11:45:26 step: 7704, epoch: 233, batch: 14, loss: 0.00655639823526144, acc: 100.0, f1: 100.0, r: 0.7466883339285219
06/01/2019 11:45:26 step: 7709, epoch: 233, batch: 19, loss: 0.011564990505576134, acc: 100.0, f1: 100.0, r: 0.7589626109391406
06/01/2019 11:45:27 step: 7714, epoch: 233, batch: 24, loss: 0.00842349138110876, acc: 100.0, f1: 100.0, r: 0.702905011451242
06/01/2019 11:45:27 step: 7719, epoch: 233, batch: 29, loss: 0.02167796529829502, acc: 100.0, f1: 100.0, r: 0.8228511019631806
06/01/2019 11:45:28 *** evaluating ***
06/01/2019 11:45:28 step: 234, epoch: 233, acc: 55.12820512820513, f1: 24.99473925369933, r: 0.2659756707077048
06/01/2019 11:45:28 *** epoch: 235 ***
06/01/2019 11:45:28 *** training ***
06/01/2019 11:45:29 step: 7727, epoch: 234, batch: 4, loss: 0.006790373474359512, acc: 100.0, f1: 100.0, r: 0.8098705528398318
06/01/2019 11:45:29 step: 7732, epoch: 234, batch: 9, loss: 0.008639318868517876, acc: 100.0, f1: 100.0, r: 0.7273040765809702
06/01/2019 11:45:30 step: 7737, epoch: 234, batch: 14, loss: 0.03332335501909256, acc: 100.0, f1: 100.0, r: 0.6490220275831154
06/01/2019 11:45:31 step: 7742, epoch: 234, batch: 19, loss: 0.0064322687685489655, acc: 100.0, f1: 100.0, r: 0.6916148624353617
06/01/2019 11:45:31 step: 7747, epoch: 234, batch: 24, loss: 0.009745370596647263, acc: 100.0, f1: 100.0, r: 0.6905686656099234
06/01/2019 11:45:32 step: 7752, epoch: 234, batch: 29, loss: 0.014811919070780277, acc: 100.0, f1: 100.0, r: 0.7529632314841529
06/01/2019 11:45:32 *** evaluating ***
06/01/2019 11:45:32 step: 235, epoch: 234, acc: 55.12820512820513, f1: 25.06216585900043, r: 0.26359035997730806
06/01/2019 11:45:32 *** epoch: 236 ***
06/01/2019 11:45:32 *** training ***
06/01/2019 11:45:33 step: 7760, epoch: 235, batch: 4, loss: 0.03663189336657524, acc: 98.4375, f1: 95.23809523809523, r: 0.7502785025417914
06/01/2019 11:45:34 step: 7765, epoch: 235, batch: 9, loss: 0.014503153041005135, acc: 100.0, f1: 100.0, r: 0.7189226445763943
06/01/2019 11:45:34 step: 7770, epoch: 235, batch: 14, loss: 0.007513450458645821, acc: 100.0, f1: 100.0, r: 0.8205243528586187
06/01/2019 11:45:35 step: 7775, epoch: 235, batch: 19, loss: 0.013315021060407162, acc: 100.0, f1: 100.0, r: 0.6560855223918008
06/01/2019 11:45:36 step: 7780, epoch: 235, batch: 24, loss: 0.04098920524120331, acc: 100.0, f1: 100.0, r: 0.8036553656154335
06/01/2019 11:45:36 step: 7785, epoch: 235, batch: 29, loss: 0.029988721013069153, acc: 98.4375, f1: 94.77726574500768, r: 0.7086602803009858
06/01/2019 11:45:36 *** evaluating ***
06/01/2019 11:45:37 step: 236, epoch: 235, acc: 55.98290598290598, f1: 27.409085021397132, r: 0.2649654089434868
06/01/2019 11:45:37 *** epoch: 237 ***
06/01/2019 11:45:37 *** training ***
06/01/2019 11:45:37 step: 7793, epoch: 236, batch: 4, loss: 0.020046930760145187, acc: 100.0, f1: 100.0, r: 0.6883729340751771
06/01/2019 11:45:38 step: 7798, epoch: 236, batch: 9, loss: 0.007378187030553818, acc: 100.0, f1: 100.0, r: 0.6740452390923263
06/01/2019 11:45:39 step: 7803, epoch: 236, batch: 14, loss: 0.04029218107461929, acc: 100.0, f1: 100.0, r: 0.7445852892749654
06/01/2019 11:45:39 step: 7808, epoch: 236, batch: 19, loss: 0.004824167583137751, acc: 100.0, f1: 100.0, r: 0.7988025478671951
06/01/2019 11:45:40 step: 7813, epoch: 236, batch: 24, loss: 0.007033533416688442, acc: 100.0, f1: 100.0, r: 0.7759430536800734
06/01/2019 11:45:40 step: 7818, epoch: 236, batch: 29, loss: 0.055274803191423416, acc: 98.4375, f1: 98.12987012987013, r: 0.66073526646976
06/01/2019 11:45:41 *** evaluating ***
06/01/2019 11:45:41 step: 237, epoch: 236, acc: 55.55555555555556, f1: 27.538499973636267, r: 0.2655736069669844
06/01/2019 11:45:41 *** epoch: 238 ***
06/01/2019 11:45:41 *** training ***
06/01/2019 11:45:42 step: 7826, epoch: 237, batch: 4, loss: 0.05575527250766754, acc: 100.0, f1: 100.0, r: 0.8372889695174266
06/01/2019 11:45:42 step: 7831, epoch: 237, batch: 9, loss: 0.006594875827431679, acc: 100.0, f1: 100.0, r: 0.6205617774364963
06/01/2019 11:45:43 step: 7836, epoch: 237, batch: 14, loss: 0.01089450716972351, acc: 100.0, f1: 100.0, r: 0.7569734065029922
06/01/2019 11:45:44 step: 7841, epoch: 237, batch: 19, loss: 0.017098605632781982, acc: 100.0, f1: 100.0, r: 0.6294915950989024
06/01/2019 11:45:44 step: 7846, epoch: 237, batch: 24, loss: 0.008883384987711906, acc: 100.0, f1: 100.0, r: 0.7224832619673284
06/01/2019 11:45:45 step: 7851, epoch: 237, batch: 29, loss: 0.008730174973607063, acc: 100.0, f1: 100.0, r: 0.8359419372002234
06/01/2019 11:45:45 *** evaluating ***
06/01/2019 11:45:45 step: 238, epoch: 237, acc: 56.41025641025641, f1: 27.447954392687446, r: 0.2620714626313725
06/01/2019 11:45:45 *** epoch: 239 ***
06/01/2019 11:45:45 *** training ***
06/01/2019 11:45:46 step: 7859, epoch: 238, batch: 4, loss: 0.012556948699057102, acc: 100.0, f1: 100.0, r: 0.7549222351328336
06/01/2019 11:45:47 step: 7864, epoch: 238, batch: 9, loss: 0.013004460372030735, acc: 100.0, f1: 100.0, r: 0.5759100933886185
06/01/2019 11:45:47 step: 7869, epoch: 238, batch: 14, loss: 0.00734855979681015, acc: 100.0, f1: 100.0, r: 0.7026207162056467
06/01/2019 11:45:48 step: 7874, epoch: 238, batch: 19, loss: 0.010414950549602509, acc: 100.0, f1: 100.0, r: 0.6076902838060942
06/01/2019 11:45:48 step: 7879, epoch: 238, batch: 24, loss: 0.6754427552223206, acc: 100.0, f1: 100.0, r: 0.761229802922051
06/01/2019 11:45:49 step: 7884, epoch: 238, batch: 29, loss: 0.011992008425295353, acc: 100.0, f1: 100.0, r: 0.796489255261063
06/01/2019 11:45:49 *** evaluating ***
06/01/2019 11:45:49 step: 239, epoch: 238, acc: 55.55555555555556, f1: 27.83833222239173, r: 0.26237708215960376
06/01/2019 11:45:49 *** epoch: 240 ***
06/01/2019 11:45:49 *** training ***
06/01/2019 11:45:50 step: 7892, epoch: 239, batch: 4, loss: 0.006781785748898983, acc: 100.0, f1: 100.0, r: 0.7431952044276904
06/01/2019 11:45:51 step: 7897, epoch: 239, batch: 9, loss: 0.010492486879229546, acc: 100.0, f1: 100.0, r: 0.7397380946034724
06/01/2019 11:45:51 step: 7902, epoch: 239, batch: 14, loss: 0.01325950026512146, acc: 100.0, f1: 100.0, r: 0.7655395573359722
06/01/2019 11:45:52 step: 7907, epoch: 239, batch: 19, loss: 0.02034663036465645, acc: 100.0, f1: 100.0, r: 0.726290815225363
06/01/2019 11:45:53 step: 7912, epoch: 239, batch: 24, loss: 0.0192129984498024, acc: 100.0, f1: 100.0, r: 0.5444633241159473
06/01/2019 11:45:53 step: 7917, epoch: 239, batch: 29, loss: 0.014453032054007053, acc: 100.0, f1: 100.0, r: 0.7357896749820735
06/01/2019 11:45:54 *** evaluating ***
06/01/2019 11:45:54 step: 240, epoch: 239, acc: 56.41025641025641, f1: 28.288488253292737, r: 0.2618753363185276
06/01/2019 11:45:54 *** epoch: 241 ***
06/01/2019 11:45:54 *** training ***
06/01/2019 11:45:54 step: 7925, epoch: 240, batch: 4, loss: 0.008659245446324348, acc: 100.0, f1: 100.0, r: 0.6847724796485258
06/01/2019 11:45:55 step: 7930, epoch: 240, batch: 9, loss: 0.00802876427769661, acc: 100.0, f1: 100.0, r: 0.8501063150795356
06/01/2019 11:45:56 step: 7935, epoch: 240, batch: 14, loss: 0.01887795701622963, acc: 100.0, f1: 100.0, r: 0.7536593134367949
06/01/2019 11:45:56 step: 7940, epoch: 240, batch: 19, loss: 0.019010765478014946, acc: 100.0, f1: 100.0, r: 0.7279849876506329
06/01/2019 11:45:57 step: 7945, epoch: 240, batch: 24, loss: 0.036733657121658325, acc: 98.4375, f1: 97.953216374269, r: 0.7278307104460756
06/01/2019 11:45:58 step: 7950, epoch: 240, batch: 29, loss: 0.007185426540672779, acc: 100.0, f1: 100.0, r: 0.79956413221867
06/01/2019 11:45:58 *** evaluating ***
06/01/2019 11:45:58 step: 241, epoch: 240, acc: 56.41025641025641, f1: 28.63531313028217, r: 0.2633933327598694
06/01/2019 11:45:58 *** epoch: 242 ***
06/01/2019 11:45:58 *** training ***
06/01/2019 11:45:59 step: 7958, epoch: 241, batch: 4, loss: 0.036397989839315414, acc: 100.0, f1: 100.0, r: 0.6951920879339063
06/01/2019 11:45:59 step: 7963, epoch: 241, batch: 9, loss: 0.012737570330500603, acc: 100.0, f1: 100.0, r: 0.7249293748740449
06/01/2019 11:46:00 step: 7968, epoch: 241, batch: 14, loss: 0.012266851030290127, acc: 100.0, f1: 100.0, r: 0.6020480524280731
06/01/2019 11:46:01 step: 7973, epoch: 241, batch: 19, loss: 0.022415390238165855, acc: 98.4375, f1: 97.6608187134503, r: 0.7045520576169848
06/01/2019 11:46:01 step: 7978, epoch: 241, batch: 24, loss: 0.015669191256165504, acc: 100.0, f1: 100.0, r: 0.6490339882653076
06/01/2019 11:46:02 step: 7983, epoch: 241, batch: 29, loss: 0.013918139040470123, acc: 100.0, f1: 100.0, r: 0.8330743008506079
06/01/2019 11:46:02 *** evaluating ***
06/01/2019 11:46:03 step: 242, epoch: 241, acc: 56.41025641025641, f1: 28.07374127187107, r: 0.2615191247811671
06/01/2019 11:46:03 *** epoch: 243 ***
06/01/2019 11:46:03 *** training ***
06/01/2019 11:46:03 step: 7991, epoch: 242, batch: 4, loss: 0.008576427586376667, acc: 100.0, f1: 100.0, r: 0.6891007919659052
06/01/2019 11:46:04 step: 7996, epoch: 242, batch: 9, loss: 0.013044588267803192, acc: 100.0, f1: 100.0, r: 0.7133589679632498
06/01/2019 11:46:04 step: 8001, epoch: 242, batch: 14, loss: 0.006362797226756811, acc: 100.0, f1: 100.0, r: 0.6745960323777616
06/01/2019 11:46:05 step: 8006, epoch: 242, batch: 19, loss: 0.011673718690872192, acc: 100.0, f1: 100.0, r: 0.7029936307911134
06/01/2019 11:46:06 step: 8011, epoch: 242, batch: 24, loss: 0.0110179353505373, acc: 100.0, f1: 100.0, r: 0.8177978897123147
06/01/2019 11:46:07 step: 8016, epoch: 242, batch: 29, loss: 0.028709083795547485, acc: 100.0, f1: 100.0, r: 0.8122251247092684
06/01/2019 11:46:07 *** evaluating ***
06/01/2019 11:46:07 step: 243, epoch: 242, acc: 55.98290598290598, f1: 27.60967574716101, r: 0.2606437957295812
06/01/2019 11:46:07 *** epoch: 244 ***
06/01/2019 11:46:07 *** training ***
06/01/2019 11:46:08 step: 8024, epoch: 243, batch: 4, loss: 0.007278194651007652, acc: 100.0, f1: 100.0, r: 0.7423678154181241
06/01/2019 11:46:08 step: 8029, epoch: 243, batch: 9, loss: 0.33405059576034546, acc: 100.0, f1: 100.0, r: 0.7505319630549819
06/01/2019 11:46:09 step: 8034, epoch: 243, batch: 14, loss: 0.051528170704841614, acc: 100.0, f1: 100.0, r: 0.8164887696447893
06/01/2019 11:46:10 step: 8039, epoch: 243, batch: 19, loss: 0.006698722951114178, acc: 100.0, f1: 100.0, r: 0.6913889794345669
06/01/2019 11:46:10 step: 8044, epoch: 243, batch: 24, loss: 0.018020521849393845, acc: 100.0, f1: 100.0, r: 0.6839362836658648
06/01/2019 11:46:11 step: 8049, epoch: 243, batch: 29, loss: 0.04738115891814232, acc: 98.4375, f1: 94.28571428571429, r: 0.7057961559284669
06/01/2019 11:46:11 *** evaluating ***
06/01/2019 11:46:11 step: 244, epoch: 243, acc: 55.98290598290598, f1: 27.195283042968505, r: 0.26118990713479806
06/01/2019 11:46:11 *** epoch: 245 ***
06/01/2019 11:46:11 *** training ***
06/01/2019 11:46:12 step: 8057, epoch: 244, batch: 4, loss: 0.00636295136064291, acc: 100.0, f1: 100.0, r: 0.814605116430204
06/01/2019 11:46:12 step: 8062, epoch: 244, batch: 9, loss: 0.010673650540411472, acc: 100.0, f1: 100.0, r: 0.6756458883763856
06/01/2019 11:46:13 step: 8067, epoch: 244, batch: 14, loss: 0.010673045180737972, acc: 100.0, f1: 100.0, r: 0.7448086810068687
06/01/2019 11:46:14 step: 8072, epoch: 244, batch: 19, loss: 0.019524842500686646, acc: 100.0, f1: 100.0, r: 0.6743295610739929
06/01/2019 11:46:14 step: 8077, epoch: 244, batch: 24, loss: 0.4219169616699219, acc: 100.0, f1: 100.0, r: 0.7068818038271327
06/01/2019 11:46:15 step: 8082, epoch: 244, batch: 29, loss: 0.014238388277590275, acc: 100.0, f1: 100.0, r: 0.7105128675571474
06/01/2019 11:46:15 *** evaluating ***
06/01/2019 11:46:15 step: 245, epoch: 244, acc: 55.12820512820513, f1: 26.437444899296185, r: 0.2661585864844478
06/01/2019 11:46:15 *** epoch: 246 ***
06/01/2019 11:46:15 *** training ***
06/01/2019 11:46:16 step: 8090, epoch: 245, batch: 4, loss: 0.03895924985408783, acc: 100.0, f1: 100.0, r: 0.8673866414660528
06/01/2019 11:46:17 step: 8095, epoch: 245, batch: 9, loss: 0.012443612329661846, acc: 100.0, f1: 100.0, r: 0.7227813572077176
06/01/2019 11:46:17 step: 8100, epoch: 245, batch: 14, loss: 0.013896578922867775, acc: 100.0, f1: 100.0, r: 0.8254053197551098
06/01/2019 11:46:18 step: 8105, epoch: 245, batch: 19, loss: 0.02030956745147705, acc: 100.0, f1: 100.0, r: 0.7953661388482995
06/01/2019 11:46:19 step: 8110, epoch: 245, batch: 24, loss: 0.026730099692940712, acc: 100.0, f1: 100.0, r: 0.785845132106189
06/01/2019 11:46:19 step: 8115, epoch: 245, batch: 29, loss: 0.005903527140617371, acc: 100.0, f1: 100.0, r: 0.7869031421210816
06/01/2019 11:46:20 *** evaluating ***
06/01/2019 11:46:20 step: 246, epoch: 245, acc: 55.12820512820513, f1: 24.98208758983131, r: 0.2596326572492421
06/01/2019 11:46:20 *** epoch: 247 ***
06/01/2019 11:46:20 *** training ***
06/01/2019 11:46:20 step: 8123, epoch: 246, batch: 4, loss: 0.00798837374895811, acc: 100.0, f1: 100.0, r: 0.7572584531180074
06/01/2019 11:46:21 step: 8128, epoch: 246, batch: 9, loss: 0.41488564014434814, acc: 100.0, f1: 100.0, r: 0.7415923874371152
06/01/2019 11:46:22 step: 8133, epoch: 246, batch: 14, loss: 0.012468474917113781, acc: 100.0, f1: 100.0, r: 0.8040125511707956
06/01/2019 11:46:22 step: 8138, epoch: 246, batch: 19, loss: 0.6672264337539673, acc: 100.0, f1: 100.0, r: 0.6786626860647639
06/01/2019 11:46:23 step: 8143, epoch: 246, batch: 24, loss: 0.3361831307411194, acc: 100.0, f1: 100.0, r: 0.8325960283636701
06/01/2019 11:46:24 step: 8148, epoch: 246, batch: 29, loss: 0.005020482465624809, acc: 100.0, f1: 100.0, r: 0.8548322307102757
06/01/2019 11:46:24 *** evaluating ***
06/01/2019 11:46:24 step: 247, epoch: 246, acc: 56.41025641025641, f1: 27.871696810908276, r: 0.259612880449982
06/01/2019 11:46:24 *** epoch: 248 ***
06/01/2019 11:46:24 *** training ***
06/01/2019 11:46:25 step: 8156, epoch: 247, batch: 4, loss: 0.006669612601399422, acc: 100.0, f1: 100.0, r: 0.6689571931198284
06/01/2019 11:46:25 step: 8161, epoch: 247, batch: 9, loss: 0.01134913507848978, acc: 100.0, f1: 100.0, r: 0.8056469524396446
06/01/2019 11:46:26 step: 8166, epoch: 247, batch: 14, loss: 0.014315451495349407, acc: 100.0, f1: 100.0, r: 0.8205989883486424
06/01/2019 11:46:27 step: 8171, epoch: 247, batch: 19, loss: 0.0059336512349545956, acc: 100.0, f1: 100.0, r: 0.8075327367129821
06/01/2019 11:46:27 step: 8176, epoch: 247, batch: 24, loss: 0.012068708427250385, acc: 100.0, f1: 100.0, r: 0.6797518361673635
06/01/2019 11:46:28 step: 8181, epoch: 247, batch: 29, loss: 0.01045162882655859, acc: 100.0, f1: 100.0, r: 0.7430465016950587
06/01/2019 11:46:28 *** evaluating ***
06/01/2019 11:46:28 step: 248, epoch: 247, acc: 55.98290598290598, f1: 27.822267618766684, r: 0.26326246868715236
06/01/2019 11:46:28 *** epoch: 249 ***
06/01/2019 11:46:28 *** training ***
06/01/2019 11:46:29 step: 8189, epoch: 248, batch: 4, loss: 0.012424006126821041, acc: 100.0, f1: 100.0, r: 0.6871835455911072
06/01/2019 11:46:30 step: 8194, epoch: 248, batch: 9, loss: 0.41279059648513794, acc: 100.0, f1: 100.0, r: 0.7903400033107264
06/01/2019 11:46:30 step: 8199, epoch: 248, batch: 14, loss: 0.010370585136115551, acc: 100.0, f1: 100.0, r: 0.7563189838546265
06/01/2019 11:46:31 step: 8204, epoch: 248, batch: 19, loss: 0.011568322777748108, acc: 100.0, f1: 100.0, r: 0.6900908353459728
06/01/2019 11:46:31 step: 8209, epoch: 248, batch: 24, loss: 0.0274213757365942, acc: 100.0, f1: 100.0, r: 0.7828419756157001
06/01/2019 11:46:32 step: 8214, epoch: 248, batch: 29, loss: 0.02540489472448826, acc: 98.4375, f1: 96.57142857142857, r: 0.6619929207790738
06/01/2019 11:46:32 *** evaluating ***
06/01/2019 11:46:33 step: 249, epoch: 248, acc: 55.55555555555556, f1: 27.27541583290022, r: 0.2612188062886925
06/01/2019 11:46:33 *** epoch: 250 ***
06/01/2019 11:46:33 *** training ***
06/01/2019 11:46:33 step: 8222, epoch: 249, batch: 4, loss: 0.005345791112631559, acc: 100.0, f1: 100.0, r: 0.5813579435177293
06/01/2019 11:46:34 step: 8227, epoch: 249, batch: 9, loss: 0.02548968605697155, acc: 100.0, f1: 100.0, r: 0.8376022864080339
06/01/2019 11:46:34 step: 8232, epoch: 249, batch: 14, loss: 0.007980569265782833, acc: 100.0, f1: 100.0, r: 0.7075551117482443
06/01/2019 11:46:35 step: 8237, epoch: 249, batch: 19, loss: 0.008090922608971596, acc: 100.0, f1: 100.0, r: 0.7006024340032155
06/01/2019 11:46:36 step: 8242, epoch: 249, batch: 24, loss: 0.003884324338287115, acc: 100.0, f1: 100.0, r: 0.7998188839595377
06/01/2019 11:46:36 step: 8247, epoch: 249, batch: 29, loss: 0.014735633507370949, acc: 100.0, f1: 100.0, r: 0.7810817587066619
06/01/2019 11:46:37 *** evaluating ***
06/01/2019 11:46:37 step: 250, epoch: 249, acc: 55.12820512820513, f1: 27.53186749222667, r: 0.2633795000958103
06/01/2019 11:46:37 *** epoch: 251 ***
06/01/2019 11:46:37 *** training ***
06/01/2019 11:46:38 step: 8255, epoch: 250, batch: 4, loss: 0.013920772820711136, acc: 100.0, f1: 100.0, r: 0.6994629621817463
06/01/2019 11:46:38 step: 8260, epoch: 250, batch: 9, loss: 0.6743761897087097, acc: 100.0, f1: 100.0, r: 0.6900068382338013
06/01/2019 11:46:39 step: 8265, epoch: 250, batch: 14, loss: 0.01101019885390997, acc: 100.0, f1: 100.0, r: 0.6716517615986519
06/01/2019 11:46:40 step: 8270, epoch: 250, batch: 19, loss: 0.005804098676890135, acc: 100.0, f1: 100.0, r: 0.7264359839467976
06/01/2019 11:46:40 step: 8275, epoch: 250, batch: 24, loss: 0.009993714280426502, acc: 100.0, f1: 100.0, r: 0.7624628474107397
06/01/2019 11:46:41 step: 8280, epoch: 250, batch: 29, loss: 0.005199391394853592, acc: 100.0, f1: 100.0, r: 0.6747259657953502
06/01/2019 11:46:41 *** evaluating ***
06/01/2019 11:46:41 step: 251, epoch: 250, acc: 56.41025641025641, f1: 28.07374127187107, r: 0.2590461408415797
06/01/2019 11:46:41 *** epoch: 252 ***
06/01/2019 11:46:41 *** training ***
06/01/2019 11:46:42 step: 8288, epoch: 251, batch: 4, loss: 0.021907903254032135, acc: 100.0, f1: 100.0, r: 0.6835075449634958
06/01/2019 11:46:43 step: 8293, epoch: 251, batch: 9, loss: 0.006834007333964109, acc: 100.0, f1: 100.0, r: 0.721798176491547
06/01/2019 11:46:43 step: 8298, epoch: 251, batch: 14, loss: 0.010968565940856934, acc: 100.0, f1: 100.0, r: 0.7996646302822124
06/01/2019 11:46:44 step: 8303, epoch: 251, batch: 19, loss: 0.0076502179726958275, acc: 100.0, f1: 100.0, r: 0.824793560552859
06/01/2019 11:46:44 step: 8308, epoch: 251, batch: 24, loss: 0.01968262903392315, acc: 100.0, f1: 100.0, r: 0.8186486690939965
06/01/2019 11:46:45 step: 8313, epoch: 251, batch: 29, loss: 0.010014688596129417, acc: 100.0, f1: 100.0, r: 0.7128006685616288
06/01/2019 11:46:45 *** evaluating ***
06/01/2019 11:46:46 step: 252, epoch: 251, acc: 55.12820512820513, f1: 26.811748494932637, r: 0.2609801933136883
06/01/2019 11:46:46 *** epoch: 253 ***
06/01/2019 11:46:46 *** training ***
06/01/2019 11:46:46 step: 8321, epoch: 252, batch: 4, loss: 0.00989610143005848, acc: 100.0, f1: 100.0, r: 0.8260308938290867
06/01/2019 11:46:47 step: 8326, epoch: 252, batch: 9, loss: 0.01503870077431202, acc: 100.0, f1: 100.0, r: 0.7549280111873486
06/01/2019 11:46:47 step: 8331, epoch: 252, batch: 14, loss: 0.01559954509139061, acc: 100.0, f1: 100.0, r: 0.7245538598436658
06/01/2019 11:46:48 step: 8336, epoch: 252, batch: 19, loss: 0.011833975091576576, acc: 100.0, f1: 100.0, r: 0.6001600050263367
06/01/2019 11:46:49 step: 8341, epoch: 252, batch: 24, loss: 0.01424576248973608, acc: 100.0, f1: 100.0, r: 0.7595761666944939
06/01/2019 11:46:49 step: 8346, epoch: 252, batch: 29, loss: 0.006042955443263054, acc: 100.0, f1: 100.0, r: 0.7442941362705816
06/01/2019 11:46:50 *** evaluating ***
06/01/2019 11:46:50 step: 253, epoch: 252, acc: 55.98290598290598, f1: 26.118722936703676, r: 0.26350424885836615
06/01/2019 11:46:50 *** epoch: 254 ***
06/01/2019 11:46:50 *** training ***
06/01/2019 11:46:51 step: 8354, epoch: 253, batch: 4, loss: 0.05725815147161484, acc: 98.4375, f1: 96.70995670995671, r: 0.7386026976061855
06/01/2019 11:46:51 step: 8359, epoch: 253, batch: 9, loss: 0.6692124605178833, acc: 100.0, f1: 100.0, r: 0.8177876940786485
06/01/2019 11:46:52 step: 8364, epoch: 253, batch: 14, loss: 0.016474220901727676, acc: 100.0, f1: 100.0, r: 0.7872823660483657
06/01/2019 11:46:53 step: 8369, epoch: 253, batch: 19, loss: 0.023943178355693817, acc: 100.0, f1: 100.0, r: 0.7341447687981306
06/01/2019 11:46:53 step: 8374, epoch: 253, batch: 24, loss: 0.3775162696838379, acc: 100.0, f1: 100.0, r: 0.819664732470003
06/01/2019 11:46:54 step: 8379, epoch: 253, batch: 29, loss: 0.4058659076690674, acc: 100.0, f1: 100.0, r: 0.695019960856112
06/01/2019 11:46:54 *** evaluating ***
06/01/2019 11:46:54 step: 254, epoch: 253, acc: 55.98290598290598, f1: 25.604306996384285, r: 0.2624573044741295
06/01/2019 11:46:54 *** epoch: 255 ***
06/01/2019 11:46:54 *** training ***
06/01/2019 11:46:55 step: 8387, epoch: 254, batch: 4, loss: 0.008353278040885925, acc: 100.0, f1: 100.0, r: 0.7865848559872859
06/01/2019 11:46:56 step: 8392, epoch: 254, batch: 9, loss: 0.008327905088663101, acc: 100.0, f1: 100.0, r: 0.723805322736787
06/01/2019 11:46:56 step: 8397, epoch: 254, batch: 14, loss: 0.012344339862465858, acc: 100.0, f1: 100.0, r: 0.6536877881681175
06/01/2019 11:46:57 step: 8402, epoch: 254, batch: 19, loss: 0.02739052288234234, acc: 98.4375, f1: 97.03703703703704, r: 0.8318985072953968
06/01/2019 11:46:58 step: 8407, epoch: 254, batch: 24, loss: 0.01444936916232109, acc: 100.0, f1: 100.0, r: 0.765677035309761
06/01/2019 11:46:58 step: 8412, epoch: 254, batch: 29, loss: 0.011717171408236027, acc: 100.0, f1: 100.0, r: 0.7884413212273618
06/01/2019 11:46:59 *** evaluating ***
06/01/2019 11:46:59 step: 255, epoch: 254, acc: 55.98290598290598, f1: 25.53372430442978, r: 0.25779444621900943
06/01/2019 11:46:59 *** epoch: 256 ***
06/01/2019 11:46:59 *** training ***
06/01/2019 11:46:59 step: 8420, epoch: 255, batch: 4, loss: 0.01476319134235382, acc: 100.0, f1: 100.0, r: 0.7805719338174315
06/01/2019 11:47:00 step: 8425, epoch: 255, batch: 9, loss: 0.010417049750685692, acc: 100.0, f1: 100.0, r: 0.8364935238150306
06/01/2019 11:47:01 step: 8430, epoch: 255, batch: 14, loss: 0.08763362467288971, acc: 100.0, f1: 100.0, r: 0.696547593334715
06/01/2019 11:47:01 step: 8435, epoch: 255, batch: 19, loss: 0.019876902922987938, acc: 100.0, f1: 100.0, r: 0.6782416785802453
06/01/2019 11:47:02 step: 8440, epoch: 255, batch: 24, loss: 0.006178004667162895, acc: 100.0, f1: 100.0, r: 0.6902709928700745
06/01/2019 11:47:02 step: 8445, epoch: 255, batch: 29, loss: 0.007354896515607834, acc: 100.0, f1: 100.0, r: 0.6781487443453942
06/01/2019 11:47:03 *** evaluating ***
06/01/2019 11:47:03 step: 256, epoch: 255, acc: 55.98290598290598, f1: 27.13129909778641, r: 0.25830639414662715
06/01/2019 11:47:03 *** epoch: 257 ***
06/01/2019 11:47:03 *** training ***
06/01/2019 11:47:03 step: 8453, epoch: 256, batch: 4, loss: 0.01971418969333172, acc: 100.0, f1: 100.0, r: 0.774928235960819
06/01/2019 11:47:04 step: 8458, epoch: 256, batch: 9, loss: 0.016980938613414764, acc: 100.0, f1: 100.0, r: 0.8035074027354175
06/01/2019 11:47:05 step: 8463, epoch: 256, batch: 14, loss: 0.013666464015841484, acc: 100.0, f1: 100.0, r: 0.7118847470678022
06/01/2019 11:47:05 step: 8468, epoch: 256, batch: 19, loss: 0.016894005239009857, acc: 100.0, f1: 100.0, r: 0.6898220432923559
06/01/2019 11:47:06 step: 8473, epoch: 256, batch: 24, loss: 0.0166399534791708, acc: 100.0, f1: 100.0, r: 0.76716672400029
06/01/2019 11:47:07 step: 8478, epoch: 256, batch: 29, loss: 0.03465700522065163, acc: 98.4375, f1: 98.14814814814815, r: 0.8018533026582015
06/01/2019 11:47:07 *** evaluating ***
06/01/2019 11:47:07 step: 257, epoch: 256, acc: 55.98290598290598, f1: 25.509857467901774, r: 0.25825951178611917
06/01/2019 11:47:07 *** epoch: 258 ***
06/01/2019 11:47:07 *** training ***
06/01/2019 11:47:08 step: 8486, epoch: 257, batch: 4, loss: 0.014549905434250832, acc: 100.0, f1: 100.0, r: 0.6822685252321514
06/01/2019 11:47:08 step: 8491, epoch: 257, batch: 9, loss: 0.013294070959091187, acc: 100.0, f1: 100.0, r: 0.6244411825742082
06/01/2019 11:47:09 step: 8496, epoch: 257, batch: 14, loss: 0.019906949251890182, acc: 100.0, f1: 100.0, r: 0.7108217340074567
06/01/2019 11:47:10 step: 8501, epoch: 257, batch: 19, loss: 0.006112515926361084, acc: 100.0, f1: 100.0, r: 0.6781601881446939
06/01/2019 11:47:10 step: 8506, epoch: 257, batch: 24, loss: 0.033433303236961365, acc: 100.0, f1: 100.0, r: 0.7762684464476837
06/01/2019 11:47:11 step: 8511, epoch: 257, batch: 29, loss: 0.02272733673453331, acc: 100.0, f1: 100.0, r: 0.628881236688122
06/01/2019 11:47:11 *** evaluating ***
06/01/2019 11:47:11 step: 258, epoch: 257, acc: 56.41025641025641, f1: 28.121922619922547, r: 0.2615087775094191
06/01/2019 11:47:11 *** epoch: 259 ***
06/01/2019 11:47:11 *** training ***
06/01/2019 11:47:12 step: 8519, epoch: 258, batch: 4, loss: 0.012892371974885464, acc: 100.0, f1: 100.0, r: 0.7547327790602355
06/01/2019 11:47:13 step: 8524, epoch: 258, batch: 9, loss: 0.008228426799178123, acc: 100.0, f1: 100.0, r: 0.8377660988372012
06/01/2019 11:47:13 step: 8529, epoch: 258, batch: 14, loss: 0.01470082439482212, acc: 100.0, f1: 100.0, r: 0.7393793966781563
06/01/2019 11:47:14 step: 8534, epoch: 258, batch: 19, loss: 0.009524086490273476, acc: 100.0, f1: 100.0, r: 0.6928639939131361
06/01/2019 11:47:15 step: 8539, epoch: 258, batch: 24, loss: 0.010206609964370728, acc: 100.0, f1: 100.0, r: 0.7399872148047051
06/01/2019 11:47:15 step: 8544, epoch: 258, batch: 29, loss: 0.005743010900914669, acc: 100.0, f1: 100.0, r: 0.7356690017986346
06/01/2019 11:47:16 *** evaluating ***
06/01/2019 11:47:16 step: 259, epoch: 258, acc: 54.700854700854705, f1: 26.294511606295412, r: 0.2571840703980618
06/01/2019 11:47:16 *** epoch: 260 ***
06/01/2019 11:47:16 *** training ***
06/01/2019 11:47:16 step: 8552, epoch: 259, batch: 4, loss: 0.3371928036212921, acc: 100.0, f1: 100.0, r: 0.8258451726389031
06/01/2019 11:47:17 step: 8557, epoch: 259, batch: 9, loss: 0.06359031796455383, acc: 100.0, f1: 100.0, r: 0.6549741264414158
06/01/2019 11:47:18 step: 8562, epoch: 259, batch: 14, loss: 0.011378977447748184, acc: 100.0, f1: 100.0, r: 0.7832608503984292
06/01/2019 11:47:18 step: 8567, epoch: 259, batch: 19, loss: 0.016741784289479256, acc: 100.0, f1: 100.0, r: 0.703247637381783
06/01/2019 11:47:19 step: 8572, epoch: 259, batch: 24, loss: 0.015033372677862644, acc: 100.0, f1: 100.0, r: 0.8438560986407107
06/01/2019 11:47:20 step: 8577, epoch: 259, batch: 29, loss: 0.013709669932723045, acc: 100.0, f1: 100.0, r: 0.8304700239814039
06/01/2019 11:47:20 *** evaluating ***
06/01/2019 11:47:20 step: 260, epoch: 259, acc: 55.98290598290598, f1: 26.013459053883746, r: 0.2603018773145867
06/01/2019 11:47:20 *** epoch: 261 ***
06/01/2019 11:47:20 *** training ***
06/01/2019 11:47:21 step: 8585, epoch: 260, batch: 4, loss: 0.011888169683516026, acc: 100.0, f1: 100.0, r: 0.8090681038478834
06/01/2019 11:47:21 step: 8590, epoch: 260, batch: 9, loss: 0.01290050707757473, acc: 100.0, f1: 100.0, r: 0.57726007725457
06/01/2019 11:47:22 step: 8595, epoch: 260, batch: 14, loss: 0.007203679997473955, acc: 100.0, f1: 100.0, r: 0.6943793395375389
06/01/2019 11:47:23 step: 8600, epoch: 260, batch: 19, loss: 0.006940939929336309, acc: 100.0, f1: 100.0, r: 0.7919524631477621
06/01/2019 11:47:23 step: 8605, epoch: 260, batch: 24, loss: 0.016915826126933098, acc: 100.0, f1: 100.0, r: 0.6878566271693606
06/01/2019 11:47:24 step: 8610, epoch: 260, batch: 29, loss: 0.009424563497304916, acc: 100.0, f1: 100.0, r: 0.7605595559161717
06/01/2019 11:47:24 *** evaluating ***
06/01/2019 11:47:25 step: 261, epoch: 260, acc: 56.41025641025641, f1: 25.639256302767034, r: 0.2548213117953524
06/01/2019 11:47:25 *** epoch: 262 ***
06/01/2019 11:47:25 *** training ***
06/01/2019 11:47:25 step: 8618, epoch: 261, batch: 4, loss: 0.019025417044758797, acc: 100.0, f1: 100.0, r: 0.7495358467941019
06/01/2019 11:47:26 step: 8623, epoch: 261, batch: 9, loss: 0.008213439956307411, acc: 100.0, f1: 100.0, r: 0.7552266420553885
06/01/2019 11:47:26 step: 8628, epoch: 261, batch: 14, loss: 0.023188387975096703, acc: 100.0, f1: 100.0, r: 0.7176191374429912
06/01/2019 11:47:27 step: 8633, epoch: 261, batch: 19, loss: 0.016371378675103188, acc: 100.0, f1: 100.0, r: 0.6775096122059143
06/01/2019 11:47:28 step: 8638, epoch: 261, batch: 24, loss: 0.0035653486847877502, acc: 100.0, f1: 100.0, r: 0.6893189936630358
06/01/2019 11:47:28 step: 8643, epoch: 261, batch: 29, loss: 0.009342918172478676, acc: 100.0, f1: 100.0, r: 0.693593094799367
06/01/2019 11:47:29 *** evaluating ***
06/01/2019 11:47:29 step: 262, epoch: 261, acc: 56.837606837606835, f1: 28.24840233576268, r: 0.25900470887932486
06/01/2019 11:47:29 *** epoch: 263 ***
06/01/2019 11:47:29 *** training ***
06/01/2019 11:47:29 step: 8651, epoch: 262, batch: 4, loss: 0.006534833926707506, acc: 100.0, f1: 100.0, r: 0.7086800328496152
06/01/2019 11:47:30 step: 8656, epoch: 262, batch: 9, loss: 0.005263173952698708, acc: 100.0, f1: 100.0, r: 0.7526790070554916
06/01/2019 11:47:31 step: 8661, epoch: 262, batch: 14, loss: 0.4040367603302002, acc: 100.0, f1: 100.0, r: 0.7242745556726371
06/01/2019 11:47:31 step: 8666, epoch: 262, batch: 19, loss: 0.011034851893782616, acc: 100.0, f1: 100.0, r: 0.6845146695870344
06/01/2019 11:47:32 step: 8671, epoch: 262, batch: 24, loss: 0.027046991512179375, acc: 100.0, f1: 100.0, r: 0.6590596771167095
06/01/2019 11:47:33 step: 8676, epoch: 262, batch: 29, loss: 0.005387429613620043, acc: 100.0, f1: 100.0, r: 0.6749936285851531
06/01/2019 11:47:33 *** evaluating ***
06/01/2019 11:47:33 step: 263, epoch: 262, acc: 55.12820512820513, f1: 25.969178947501952, r: 0.25845365996577296
06/01/2019 11:47:33 *** epoch: 264 ***
06/01/2019 11:47:33 *** training ***
06/01/2019 11:47:34 step: 8684, epoch: 263, batch: 4, loss: 0.0365024097263813, acc: 100.0, f1: 100.0, r: 0.6494561421381971
06/01/2019 11:47:34 step: 8689, epoch: 263, batch: 9, loss: 0.0037954291328787804, acc: 100.0, f1: 100.0, r: 0.7557781794923792
06/01/2019 11:47:35 step: 8694, epoch: 263, batch: 14, loss: 0.0064535001292824745, acc: 100.0, f1: 100.0, r: 0.6994795932060529
06/01/2019 11:47:36 step: 8699, epoch: 263, batch: 19, loss: 0.016439933329820633, acc: 100.0, f1: 100.0, r: 0.7783359580589676
06/01/2019 11:47:36 step: 8704, epoch: 263, batch: 24, loss: 0.013169759884476662, acc: 100.0, f1: 100.0, r: 0.7188495803068241
06/01/2019 11:47:37 step: 8709, epoch: 263, batch: 29, loss: 0.009867201559245586, acc: 100.0, f1: 100.0, r: 0.7887862514897979
06/01/2019 11:47:37 *** evaluating ***
06/01/2019 11:47:38 step: 264, epoch: 263, acc: 55.98290598290598, f1: 28.058286942155164, r: 0.2657662444393676
06/01/2019 11:47:38 *** epoch: 265 ***
06/01/2019 11:47:38 *** training ***
06/01/2019 11:47:38 step: 8717, epoch: 264, batch: 4, loss: 0.011944524012506008, acc: 100.0, f1: 100.0, r: 0.7498973534162681
06/01/2019 11:47:39 step: 8722, epoch: 264, batch: 9, loss: 0.019545990973711014, acc: 100.0, f1: 100.0, r: 0.6963708660945483
06/01/2019 11:47:40 step: 8727, epoch: 264, batch: 14, loss: 0.42984846234321594, acc: 100.0, f1: 100.0, r: 0.8562520615372676
06/01/2019 11:47:40 step: 8732, epoch: 264, batch: 19, loss: 0.00854770839214325, acc: 100.0, f1: 100.0, r: 0.573379775844173
06/01/2019 11:47:41 step: 8737, epoch: 264, batch: 24, loss: 0.0037369830533862114, acc: 100.0, f1: 100.0, r: 0.7961539222464145
06/01/2019 11:47:41 step: 8742, epoch: 264, batch: 29, loss: 0.039911456406116486, acc: 100.0, f1: 100.0, r: 0.7158395425267683
06/01/2019 11:47:42 *** evaluating ***
06/01/2019 11:47:42 step: 265, epoch: 264, acc: 56.837606837606835, f1: 28.665528711484594, r: 0.26870951323232717
06/01/2019 11:47:42 *** epoch: 266 ***
06/01/2019 11:47:42 *** training ***
06/01/2019 11:47:42 step: 8750, epoch: 265, batch: 4, loss: 0.028526674956083298, acc: 100.0, f1: 100.0, r: 0.7826964641574146
06/01/2019 11:47:43 step: 8755, epoch: 265, batch: 9, loss: 0.01591852307319641, acc: 100.0, f1: 100.0, r: 0.683872499182175
06/01/2019 11:47:44 step: 8760, epoch: 265, batch: 14, loss: 0.009461648762226105, acc: 100.0, f1: 100.0, r: 0.7274905283245755
06/01/2019 11:47:44 step: 8765, epoch: 265, batch: 19, loss: 0.00916281621903181, acc: 100.0, f1: 100.0, r: 0.7690903619576734
06/01/2019 11:47:45 step: 8770, epoch: 265, batch: 24, loss: 0.008862001821398735, acc: 100.0, f1: 100.0, r: 0.7197157600731949
06/01/2019 11:47:45 step: 8775, epoch: 265, batch: 29, loss: 0.676546037197113, acc: 100.0, f1: 100.0, r: 0.8152138329729287
06/01/2019 11:47:46 *** evaluating ***
06/01/2019 11:47:46 step: 266, epoch: 265, acc: 55.98290598290598, f1: 25.546817206143402, r: 0.2567238271897878
06/01/2019 11:47:46 *** epoch: 267 ***
06/01/2019 11:47:46 *** training ***
06/01/2019 11:47:47 step: 8783, epoch: 266, batch: 4, loss: 0.024142000824213028, acc: 100.0, f1: 100.0, r: 0.8594916872337439
06/01/2019 11:47:47 step: 8788, epoch: 266, batch: 9, loss: 0.6678327322006226, acc: 100.0, f1: 100.0, r: 0.7937167431527985
06/01/2019 11:47:48 step: 8793, epoch: 266, batch: 14, loss: 0.014811653643846512, acc: 100.0, f1: 100.0, r: 0.6874016264225489
06/01/2019 11:47:48 step: 8798, epoch: 266, batch: 19, loss: 0.007176574319601059, acc: 100.0, f1: 100.0, r: 0.7915612878793303
06/01/2019 11:47:49 step: 8803, epoch: 266, batch: 24, loss: 0.00935694295912981, acc: 100.0, f1: 100.0, r: 0.7210361579216876
06/01/2019 11:47:50 step: 8808, epoch: 266, batch: 29, loss: 0.009022188372910023, acc: 100.0, f1: 100.0, r: 0.7982454895406643
06/01/2019 11:47:50 *** evaluating ***
06/01/2019 11:47:50 step: 267, epoch: 266, acc: 54.27350427350427, f1: 24.652139351627845, r: 0.2649094336026289
06/01/2019 11:47:50 *** epoch: 268 ***
06/01/2019 11:47:50 *** training ***
06/01/2019 11:47:51 step: 8816, epoch: 267, batch: 4, loss: 0.01217877771705389, acc: 100.0, f1: 100.0, r: 0.7294093721026624
06/01/2019 11:47:51 step: 8821, epoch: 267, batch: 9, loss: 0.0247100368142128, acc: 100.0, f1: 100.0, r: 0.7157263690553504
06/01/2019 11:47:52 step: 8826, epoch: 267, batch: 14, loss: 0.007512770127505064, acc: 100.0, f1: 100.0, r: 0.7325660826925522
06/01/2019 11:47:53 step: 8831, epoch: 267, batch: 19, loss: 0.009674077853560448, acc: 100.0, f1: 100.0, r: 0.8133617531578057
06/01/2019 11:47:53 step: 8836, epoch: 267, batch: 24, loss: 0.009246138855814934, acc: 100.0, f1: 100.0, r: 0.7871022130180964
06/01/2019 11:47:54 step: 8841, epoch: 267, batch: 29, loss: 0.014774540439248085, acc: 100.0, f1: 100.0, r: 0.7633506417947429
06/01/2019 11:47:54 *** evaluating ***
06/01/2019 11:47:55 step: 268, epoch: 267, acc: 55.98290598290598, f1: 27.822267618766684, r: 0.26661963038331327
06/01/2019 11:47:55 *** epoch: 269 ***
06/01/2019 11:47:55 *** training ***
06/01/2019 11:47:55 step: 8849, epoch: 268, batch: 4, loss: 0.005371563136577606, acc: 100.0, f1: 100.0, r: 0.6875826385443037
06/01/2019 11:47:56 step: 8854, epoch: 268, batch: 9, loss: 0.008512483909726143, acc: 100.0, f1: 100.0, r: 0.7577440471289559
06/01/2019 11:47:57 step: 8859, epoch: 268, batch: 14, loss: 0.009517861530184746, acc: 100.0, f1: 100.0, r: 0.8113986663422259
06/01/2019 11:47:57 step: 8864, epoch: 268, batch: 19, loss: 0.010033725760877132, acc: 100.0, f1: 100.0, r: 0.8584169745186118
06/01/2019 11:47:58 step: 8869, epoch: 268, batch: 24, loss: 0.007213271223008633, acc: 100.0, f1: 100.0, r: 0.7769355701191749
06/01/2019 11:47:58 step: 8874, epoch: 268, batch: 29, loss: 0.015304317697882652, acc: 100.0, f1: 100.0, r: 0.6895615893125967
06/01/2019 11:47:59 *** evaluating ***
06/01/2019 11:47:59 step: 269, epoch: 268, acc: 55.12820512820513, f1: 27.35601989322153, r: 0.26269112041955534
06/01/2019 11:47:59 *** epoch: 270 ***
06/01/2019 11:47:59 *** training ***
06/01/2019 11:47:59 step: 8882, epoch: 269, batch: 4, loss: 0.005059855058789253, acc: 100.0, f1: 100.0, r: 0.6707949366678962
06/01/2019 11:48:00 step: 8887, epoch: 269, batch: 9, loss: 0.34332647919654846, acc: 100.0, f1: 100.0, r: 0.793973222952152
06/01/2019 11:48:01 step: 8892, epoch: 269, batch: 14, loss: 0.004779085982590914, acc: 100.0, f1: 100.0, r: 0.8328350621873785
06/01/2019 11:48:01 step: 8897, epoch: 269, batch: 19, loss: 0.007279334124177694, acc: 100.0, f1: 100.0, r: 0.760916232881856
06/01/2019 11:48:02 step: 8902, epoch: 269, batch: 24, loss: 0.011941421777009964, acc: 100.0, f1: 100.0, r: 0.6702215507255519
06/01/2019 11:48:03 step: 8907, epoch: 269, batch: 29, loss: 0.011379068717360497, acc: 100.0, f1: 100.0, r: 0.6175531629796499
06/01/2019 11:48:03 *** evaluating ***
06/01/2019 11:48:03 step: 270, epoch: 269, acc: 56.837606837606835, f1: 28.171868179514448, r: 0.26677322891792676
06/01/2019 11:48:03 *** epoch: 271 ***
06/01/2019 11:48:03 *** training ***
06/01/2019 11:48:04 step: 8915, epoch: 270, batch: 4, loss: 0.026924364268779755, acc: 100.0, f1: 100.0, r: 0.7406250529361029
06/01/2019 11:48:04 step: 8920, epoch: 270, batch: 9, loss: 0.011356234550476074, acc: 100.0, f1: 100.0, r: 0.7435738701057983
06/01/2019 11:48:05 step: 8925, epoch: 270, batch: 14, loss: 0.010321981273591518, acc: 100.0, f1: 100.0, r: 0.7893514267695972
06/01/2019 11:48:06 step: 8930, epoch: 270, batch: 19, loss: 0.004686133004724979, acc: 100.0, f1: 100.0, r: 0.7992424027031184
06/01/2019 11:48:06 step: 8935, epoch: 270, batch: 24, loss: 0.010760822333395481, acc: 100.0, f1: 100.0, r: 0.6883039622330779
06/01/2019 11:48:07 step: 8940, epoch: 270, batch: 29, loss: 0.017618490383028984, acc: 98.4375, f1: 96.81063122923588, r: 0.7626685279196967
06/01/2019 11:48:07 *** evaluating ***
06/01/2019 11:48:07 step: 271, epoch: 270, acc: 56.837606837606835, f1: 27.985790403922607, r: 0.2569725961669204
06/01/2019 11:48:07 *** epoch: 272 ***
06/01/2019 11:48:07 *** training ***
06/01/2019 11:48:08 step: 8948, epoch: 271, batch: 4, loss: 0.023880768567323685, acc: 98.4375, f1: 97.87581699346404, r: 0.8021576703805304
06/01/2019 11:48:09 step: 8953, epoch: 271, batch: 9, loss: 0.3685835897922516, acc: 100.0, f1: 100.0, r: 0.7648726826899661
06/01/2019 11:48:09 step: 8958, epoch: 271, batch: 14, loss: 0.0057251229882240295, acc: 100.0, f1: 100.0, r: 0.6877061000931608
06/01/2019 11:48:10 step: 8963, epoch: 271, batch: 19, loss: 0.027033580467104912, acc: 100.0, f1: 100.0, r: 0.6331252622179661
06/01/2019 11:48:11 step: 8968, epoch: 271, batch: 24, loss: 0.00867505557835102, acc: 100.0, f1: 100.0, r: 0.78075360287655
06/01/2019 11:48:11 step: 8973, epoch: 271, batch: 29, loss: 0.0052106985822319984, acc: 100.0, f1: 100.0, r: 0.7765758231742391
06/01/2019 11:48:12 *** evaluating ***
06/01/2019 11:48:12 step: 272, epoch: 271, acc: 55.98290598290598, f1: 25.444773017902815, r: 0.2611087174458281
06/01/2019 11:48:12 *** epoch: 273 ***
06/01/2019 11:48:12 *** training ***
06/01/2019 11:48:12 step: 8981, epoch: 272, batch: 4, loss: 0.01464899256825447, acc: 100.0, f1: 100.0, r: 0.7028918664022333
06/01/2019 11:48:13 step: 8986, epoch: 272, batch: 9, loss: 0.01320650801062584, acc: 100.0, f1: 100.0, r: 0.6829164218596634
06/01/2019 11:48:13 step: 8991, epoch: 272, batch: 14, loss: 0.012760575860738754, acc: 100.0, f1: 100.0, r: 0.7101271438072515
06/01/2019 11:48:14 step: 8996, epoch: 272, batch: 19, loss: 0.008946776390075684, acc: 100.0, f1: 100.0, r: 0.7288509883956815
06/01/2019 11:48:15 step: 9001, epoch: 272, batch: 24, loss: 0.012607725337147713, acc: 100.0, f1: 100.0, r: 0.8372727965054337
06/01/2019 11:48:15 step: 9006, epoch: 272, batch: 29, loss: 0.011404024437069893, acc: 100.0, f1: 100.0, r: 0.7545556480847225
06/01/2019 11:48:16 *** evaluating ***
06/01/2019 11:48:16 step: 273, epoch: 272, acc: 56.41025641025641, f1: 27.862371739146585, r: 0.2614327851298334
06/01/2019 11:48:16 *** epoch: 274 ***
06/01/2019 11:48:16 *** training ***
06/01/2019 11:48:17 step: 9014, epoch: 273, batch: 4, loss: 0.021190984174609184, acc: 100.0, f1: 100.0, r: 0.731698677961932
06/01/2019 11:48:17 step: 9019, epoch: 273, batch: 9, loss: 0.009379997849464417, acc: 100.0, f1: 100.0, r: 0.7584812348449967
06/01/2019 11:48:18 step: 9024, epoch: 273, batch: 14, loss: 0.008031127043068409, acc: 100.0, f1: 100.0, r: 0.6907320285998346
06/01/2019 11:48:19 step: 9029, epoch: 273, batch: 19, loss: 0.01368067879229784, acc: 100.0, f1: 100.0, r: 0.7375963370556036
06/01/2019 11:48:19 step: 9034, epoch: 273, batch: 24, loss: 0.004384245723485947, acc: 100.0, f1: 100.0, r: 0.7195009435030365
06/01/2019 11:48:20 step: 9039, epoch: 273, batch: 29, loss: 0.3372373580932617, acc: 100.0, f1: 100.0, r: 0.6696266507308541
06/01/2019 11:48:20 *** evaluating ***
06/01/2019 11:48:20 step: 274, epoch: 273, acc: 55.55555555555556, f1: 25.830146642488284, r: 0.26394072300824367
06/01/2019 11:48:20 *** epoch: 275 ***
06/01/2019 11:48:20 *** training ***
06/01/2019 11:48:21 step: 9047, epoch: 274, batch: 4, loss: 0.011490962468087673, acc: 100.0, f1: 100.0, r: 0.8270048749569787
06/01/2019 11:48:21 step: 9052, epoch: 274, batch: 9, loss: 0.006060976069420576, acc: 100.0, f1: 100.0, r: 0.8761330746740054
06/01/2019 11:48:22 step: 9057, epoch: 274, batch: 14, loss: 0.00565838348120451, acc: 100.0, f1: 100.0, r: 0.6983528347727939
06/01/2019 11:48:23 step: 9062, epoch: 274, batch: 19, loss: 0.015453444793820381, acc: 100.0, f1: 100.0, r: 0.7025869254789404
06/01/2019 11:48:23 step: 9067, epoch: 274, batch: 24, loss: 0.020953210070729256, acc: 100.0, f1: 100.0, r: 0.8260546418495976
06/01/2019 11:48:24 step: 9072, epoch: 274, batch: 29, loss: 0.013728047721087933, acc: 100.0, f1: 100.0, r: 0.7815867069574041
06/01/2019 11:48:24 *** evaluating ***
06/01/2019 11:48:24 step: 275, epoch: 274, acc: 56.41025641025641, f1: 28.07374127187107, r: 0.2625694513442525
06/01/2019 11:48:24 *** epoch: 276 ***
06/01/2019 11:48:24 *** training ***
06/01/2019 11:48:25 step: 9080, epoch: 275, batch: 4, loss: 0.013462217524647713, acc: 100.0, f1: 100.0, r: 0.7196657116826841
06/01/2019 11:48:26 step: 9085, epoch: 275, batch: 9, loss: 0.02808292955160141, acc: 100.0, f1: 100.0, r: 0.7621168588529397
06/01/2019 11:48:26 step: 9090, epoch: 275, batch: 14, loss: 0.010830695740878582, acc: 100.0, f1: 100.0, r: 0.6551399702361745
06/01/2019 11:48:27 step: 9095, epoch: 275, batch: 19, loss: 0.012585144490003586, acc: 100.0, f1: 100.0, r: 0.6898478230226779
06/01/2019 11:48:28 step: 9100, epoch: 275, batch: 24, loss: 0.01079319417476654, acc: 100.0, f1: 100.0, r: 0.6581951587323848
06/01/2019 11:48:28 step: 9105, epoch: 275, batch: 29, loss: 0.004323380067944527, acc: 100.0, f1: 100.0, r: 0.7239921736432425
06/01/2019 11:48:29 *** evaluating ***
06/01/2019 11:48:29 step: 276, epoch: 275, acc: 55.55555555555556, f1: 24.926193222875124, r: 0.26019173923117295
06/01/2019 11:48:29 *** epoch: 277 ***
06/01/2019 11:48:29 *** training ***
06/01/2019 11:48:30 step: 9113, epoch: 276, batch: 4, loss: 0.0377412773668766, acc: 100.0, f1: 100.0, r: 0.7470374933222583
06/01/2019 11:48:30 step: 9118, epoch: 276, batch: 9, loss: 0.003540255129337311, acc: 100.0, f1: 100.0, r: 0.7772257555612732
06/01/2019 11:48:31 step: 9123, epoch: 276, batch: 14, loss: 0.00879613310098648, acc: 100.0, f1: 100.0, r: 0.6898365559592348
06/01/2019 11:48:31 step: 9128, epoch: 276, batch: 19, loss: 0.006866699084639549, acc: 100.0, f1: 100.0, r: 0.7970515383041179
06/01/2019 11:48:32 step: 9133, epoch: 276, batch: 24, loss: 0.025065952911973, acc: 100.0, f1: 100.0, r: 0.7133249892338634
06/01/2019 11:48:33 step: 9138, epoch: 276, batch: 29, loss: 0.3392244577407837, acc: 100.0, f1: 100.0, r: 0.7638817164492614
06/01/2019 11:48:33 *** evaluating ***
06/01/2019 11:48:33 step: 277, epoch: 276, acc: 56.837606837606835, f1: 28.29348759567153, r: 0.2619809306973505
06/01/2019 11:48:33 *** epoch: 278 ***
06/01/2019 11:48:33 *** training ***
06/01/2019 11:48:34 step: 9146, epoch: 277, batch: 4, loss: 0.03373151645064354, acc: 100.0, f1: 100.0, r: 0.8382449539680479
06/01/2019 11:48:35 step: 9151, epoch: 277, batch: 9, loss: 0.009019017219543457, acc: 100.0, f1: 100.0, r: 0.7760342275007891
06/01/2019 11:48:36 step: 9156, epoch: 277, batch: 14, loss: 0.007337224204093218, acc: 100.0, f1: 100.0, r: 0.7265308977479641
06/01/2019 11:48:36 step: 9161, epoch: 277, batch: 19, loss: 0.007392391096800566, acc: 100.0, f1: 100.0, r: 0.8525575418734285
06/01/2019 11:48:37 step: 9166, epoch: 277, batch: 24, loss: 0.019598063081502914, acc: 100.0, f1: 100.0, r: 0.7747971918720167
06/01/2019 11:48:37 step: 9171, epoch: 277, batch: 29, loss: 0.02021552436053753, acc: 100.0, f1: 100.0, r: 0.6841610957900768
06/01/2019 11:48:38 *** evaluating ***
06/01/2019 11:48:38 step: 278, epoch: 277, acc: 55.98290598290598, f1: 25.243600601117542, r: 0.25461085869794253
06/01/2019 11:48:38 *** epoch: 279 ***
06/01/2019 11:48:38 *** training ***
06/01/2019 11:48:38 step: 9179, epoch: 278, batch: 4, loss: 0.023032447323203087, acc: 98.4375, f1: 98.06763285024155, r: 0.8210525580785077
06/01/2019 11:48:39 step: 9184, epoch: 278, batch: 9, loss: 0.012179844081401825, acc: 100.0, f1: 100.0, r: 0.7128623528491257
06/01/2019 11:48:40 step: 9189, epoch: 278, batch: 14, loss: 0.006804203614592552, acc: 100.0, f1: 100.0, r: 0.6994204918648402
06/01/2019 11:48:40 step: 9194, epoch: 278, batch: 19, loss: 0.026610957458615303, acc: 98.4375, f1: 98.6842105263158, r: 0.8073552923924057
06/01/2019 11:48:41 step: 9199, epoch: 278, batch: 24, loss: 0.00829659216105938, acc: 100.0, f1: 100.0, r: 0.6520703080232936
06/01/2019 11:48:41 step: 9204, epoch: 278, batch: 29, loss: 0.00983728002756834, acc: 100.0, f1: 100.0, r: 0.7535008969662614
06/01/2019 11:48:42 *** evaluating ***
06/01/2019 11:48:42 step: 279, epoch: 278, acc: 55.98290598290598, f1: 26.383622286698287, r: 0.2549438472129797
06/01/2019 11:48:42 *** epoch: 280 ***
06/01/2019 11:48:42 *** training ***
06/01/2019 11:48:43 step: 9212, epoch: 279, batch: 4, loss: 0.02091880328953266, acc: 100.0, f1: 100.0, r: 0.7536338183834392
06/01/2019 11:48:43 step: 9217, epoch: 279, batch: 9, loss: 0.006806533318012953, acc: 100.0, f1: 100.0, r: 0.727085444492803
06/01/2019 11:48:44 step: 9222, epoch: 279, batch: 14, loss: 0.01736501231789589, acc: 100.0, f1: 100.0, r: 0.8046732773987118
06/01/2019 11:48:45 step: 9227, epoch: 279, batch: 19, loss: 0.013001343235373497, acc: 100.0, f1: 100.0, r: 0.6946674128025561
06/01/2019 11:48:45 step: 9232, epoch: 279, batch: 24, loss: 0.0045779491774737835, acc: 100.0, f1: 100.0, r: 0.7640337851824461
06/01/2019 11:48:46 step: 9237, epoch: 279, batch: 29, loss: 1.0267258882522583, acc: 100.0, f1: 100.0, r: 0.7891391839573902
06/01/2019 11:48:46 *** evaluating ***
06/01/2019 11:48:46 step: 280, epoch: 279, acc: 55.98290598290598, f1: 27.637203440027253, r: 0.25808131217091435
06/01/2019 11:48:46 *** epoch: 281 ***
06/01/2019 11:48:46 *** training ***
06/01/2019 11:48:47 step: 9245, epoch: 280, batch: 4, loss: 0.012101947329938412, acc: 100.0, f1: 100.0, r: 0.7908014293633382
06/01/2019 11:48:48 step: 9250, epoch: 280, batch: 9, loss: 0.6744794249534607, acc: 100.0, f1: 100.0, r: 0.7082777752300805
06/01/2019 11:48:48 step: 9255, epoch: 280, batch: 14, loss: 0.011895567178726196, acc: 100.0, f1: 100.0, r: 0.8219576275053954
06/01/2019 11:48:49 step: 9260, epoch: 280, batch: 19, loss: 0.011706862598657608, acc: 100.0, f1: 100.0, r: 0.7247160447903016
06/01/2019 11:48:50 step: 9265, epoch: 280, batch: 24, loss: 0.03548523783683777, acc: 100.0, f1: 100.0, r: 0.7547671404262325
06/01/2019 11:48:50 step: 9270, epoch: 280, batch: 29, loss: 0.05910013988614082, acc: 100.0, f1: 100.0, r: 0.8198932725949553
06/01/2019 11:48:51 *** evaluating ***
06/01/2019 11:48:51 step: 281, epoch: 280, acc: 55.55555555555556, f1: 24.77136602221881, r: 0.2545595162204067
06/01/2019 11:48:51 *** epoch: 282 ***
06/01/2019 11:48:51 *** training ***
06/01/2019 11:48:51 step: 9278, epoch: 281, batch: 4, loss: 0.004473681561648846, acc: 100.0, f1: 100.0, r: 0.6805261094735245
06/01/2019 11:48:52 step: 9283, epoch: 281, batch: 9, loss: 0.007871240377426147, acc: 100.0, f1: 100.0, r: 0.8078088116832054
06/01/2019 11:48:53 step: 9288, epoch: 281, batch: 14, loss: 0.013455288484692574, acc: 100.0, f1: 100.0, r: 0.76307678296487
06/01/2019 11:48:54 step: 9293, epoch: 281, batch: 19, loss: 0.01969766989350319, acc: 100.0, f1: 100.0, r: 0.8095225254861793
06/01/2019 11:48:54 step: 9298, epoch: 281, batch: 24, loss: 0.010994880460202694, acc: 100.0, f1: 100.0, r: 0.6962718991561849
06/01/2019 11:48:55 step: 9303, epoch: 281, batch: 29, loss: 0.024403803050518036, acc: 100.0, f1: 100.0, r: 0.7759048808505445
06/01/2019 11:48:55 *** evaluating ***
06/01/2019 11:48:56 step: 282, epoch: 281, acc: 55.55555555555556, f1: 25.06198411003845, r: 0.260735054952423
06/01/2019 11:48:56 *** epoch: 283 ***
06/01/2019 11:48:56 *** training ***
06/01/2019 11:48:56 step: 9311, epoch: 282, batch: 4, loss: 0.010137675330042839, acc: 100.0, f1: 100.0, r: 0.8109407626378021
06/01/2019 11:48:57 step: 9316, epoch: 282, batch: 9, loss: 0.012771367095410824, acc: 100.0, f1: 100.0, r: 0.6895655689072798
06/01/2019 11:48:58 step: 9321, epoch: 282, batch: 14, loss: 0.009620173834264278, acc: 100.0, f1: 100.0, r: 0.6728473376821952
06/01/2019 11:48:58 step: 9326, epoch: 282, batch: 19, loss: 0.008220051415264606, acc: 100.0, f1: 100.0, r: 0.7616556801414107
06/01/2019 11:48:59 step: 9331, epoch: 282, batch: 24, loss: 0.01172645017504692, acc: 100.0, f1: 100.0, r: 0.6494833900357126
06/01/2019 11:48:59 step: 9336, epoch: 282, batch: 29, loss: 0.016726970672607422, acc: 100.0, f1: 100.0, r: 0.8390102042059931
06/01/2019 11:49:00 *** evaluating ***
06/01/2019 11:49:00 step: 283, epoch: 282, acc: 55.12820512820513, f1: 25.22759878166242, r: 0.259676870724941
06/01/2019 11:49:00 *** epoch: 284 ***
06/01/2019 11:49:00 *** training ***
06/01/2019 11:49:00 step: 9344, epoch: 283, batch: 4, loss: 0.008400341495871544, acc: 100.0, f1: 100.0, r: 0.7267843462142428
06/01/2019 11:49:01 step: 9349, epoch: 283, batch: 9, loss: 0.016388505697250366, acc: 100.0, f1: 100.0, r: 0.740452686811475
06/01/2019 11:49:02 step: 9354, epoch: 283, batch: 14, loss: 0.012491239234805107, acc: 100.0, f1: 100.0, r: 0.714977680041717
06/01/2019 11:49:02 step: 9359, epoch: 283, batch: 19, loss: 0.012841761112213135, acc: 100.0, f1: 100.0, r: 0.5984752241222824
06/01/2019 11:49:03 step: 9364, epoch: 283, batch: 24, loss: 0.008461838588118553, acc: 100.0, f1: 100.0, r: 0.7160208724068211
06/01/2019 11:49:04 step: 9369, epoch: 283, batch: 29, loss: 0.007026154547929764, acc: 100.0, f1: 100.0, r: 0.8013564778408833
06/01/2019 11:49:04 *** evaluating ***
06/01/2019 11:49:04 step: 284, epoch: 283, acc: 56.837606837606835, f1: 28.335529559290656, r: 0.26541038834396036
06/01/2019 11:49:04 *** epoch: 285 ***
06/01/2019 11:49:04 *** training ***
06/01/2019 11:49:05 step: 9377, epoch: 284, batch: 4, loss: 0.019429877400398254, acc: 100.0, f1: 100.0, r: 0.7319676759458049
06/01/2019 11:49:05 step: 9382, epoch: 284, batch: 9, loss: 0.005982446484267712, acc: 100.0, f1: 100.0, r: 0.8068727249281925
06/01/2019 11:49:06 step: 9387, epoch: 284, batch: 14, loss: 0.018261738121509552, acc: 100.0, f1: 100.0, r: 0.7057155634185337
06/01/2019 11:49:07 step: 9392, epoch: 284, batch: 19, loss: 0.009778574109077454, acc: 100.0, f1: 100.0, r: 0.7133765531114604
06/01/2019 11:49:07 step: 9397, epoch: 284, batch: 24, loss: 0.011150604113936424, acc: 100.0, f1: 100.0, r: 0.6791841875616011
06/01/2019 11:49:08 step: 9402, epoch: 284, batch: 29, loss: 0.014935748651623726, acc: 100.0, f1: 100.0, r: 0.8170440678322181
06/01/2019 11:49:08 *** evaluating ***
06/01/2019 11:49:09 step: 285, epoch: 284, acc: 54.700854700854705, f1: 24.41433892904481, r: 0.25504145039432136
06/01/2019 11:49:09 *** epoch: 286 ***
06/01/2019 11:49:09 *** training ***
06/01/2019 11:49:09 step: 9410, epoch: 285, batch: 4, loss: 0.009652704000473022, acc: 100.0, f1: 100.0, r: 0.7228144117218178
06/01/2019 11:49:10 step: 9415, epoch: 285, batch: 9, loss: 0.014555949717760086, acc: 100.0, f1: 100.0, r: 0.7300341342035978
06/01/2019 11:49:11 step: 9420, epoch: 285, batch: 14, loss: 0.010442011058330536, acc: 100.0, f1: 100.0, r: 0.7417330544104803
06/01/2019 11:49:11 step: 9425, epoch: 285, batch: 19, loss: 0.008846521377563477, acc: 100.0, f1: 100.0, r: 0.7907370766260408
06/01/2019 11:49:12 step: 9430, epoch: 285, batch: 24, loss: 0.011428858153522015, acc: 100.0, f1: 100.0, r: 0.5605733762994035
06/01/2019 11:49:13 step: 9435, epoch: 285, batch: 29, loss: 0.006823026109486818, acc: 100.0, f1: 100.0, r: 0.6643400315739278
06/01/2019 11:49:13 *** evaluating ***
06/01/2019 11:49:13 step: 286, epoch: 285, acc: 55.55555555555556, f1: 25.09704666386891, r: 0.2645747720619804
06/01/2019 11:49:13 *** epoch: 287 ***
06/01/2019 11:49:13 *** training ***
06/01/2019 11:49:14 step: 9443, epoch: 286, batch: 4, loss: 0.003930134233087301, acc: 100.0, f1: 100.0, r: 0.8100000843993139
06/01/2019 11:49:14 step: 9448, epoch: 286, batch: 9, loss: 0.011253499425947666, acc: 100.0, f1: 100.0, r: 0.8293677611704322
06/01/2019 11:49:15 step: 9453, epoch: 286, batch: 14, loss: 0.010905329138040543, acc: 100.0, f1: 100.0, r: 0.761079410619886
06/01/2019 11:49:16 step: 9458, epoch: 286, batch: 19, loss: 0.008920012041926384, acc: 100.0, f1: 100.0, r: 0.6488018217541813
06/01/2019 11:49:16 step: 9463, epoch: 286, batch: 24, loss: 0.008738944306969643, acc: 100.0, f1: 100.0, r: 0.7982326938990646
06/01/2019 11:49:17 step: 9468, epoch: 286, batch: 29, loss: 0.011507325805723667, acc: 100.0, f1: 100.0, r: 0.8114609277340185
06/01/2019 11:49:17 *** evaluating ***
06/01/2019 11:49:17 step: 287, epoch: 286, acc: 55.98290598290598, f1: 25.364155465308468, r: 0.25811929564848546
06/01/2019 11:49:17 *** epoch: 288 ***
06/01/2019 11:49:17 *** training ***
06/01/2019 11:49:18 step: 9476, epoch: 287, batch: 4, loss: 0.009088377468287945, acc: 100.0, f1: 100.0, r: 0.6744594634728346
06/01/2019 11:49:19 step: 9481, epoch: 287, batch: 9, loss: 0.013343166559934616, acc: 100.0, f1: 100.0, r: 0.830968351686543
06/01/2019 11:49:19 step: 9486, epoch: 287, batch: 14, loss: 0.00972115807235241, acc: 100.0, f1: 100.0, r: 0.7173222594561979
06/01/2019 11:49:20 step: 9491, epoch: 287, batch: 19, loss: 0.004656195640563965, acc: 100.0, f1: 100.0, r: 0.8216440869490446
06/01/2019 11:49:21 step: 9496, epoch: 287, batch: 24, loss: 0.01047278568148613, acc: 100.0, f1: 100.0, r: 0.6902595281600145
06/01/2019 11:49:21 step: 9501, epoch: 287, batch: 29, loss: 0.03157907724380493, acc: 98.4375, f1: 97.47899159663865, r: 0.7962818683229876
06/01/2019 11:49:21 *** evaluating ***
06/01/2019 11:49:22 step: 288, epoch: 287, acc: 54.700854700854705, f1: 24.835134120926146, r: 0.2568437406099719
06/01/2019 11:49:22 *** epoch: 289 ***
06/01/2019 11:49:22 *** training ***
06/01/2019 11:49:22 step: 9509, epoch: 288, batch: 4, loss: 0.006841777823865414, acc: 100.0, f1: 100.0, r: 0.6792049797186231
06/01/2019 11:49:23 step: 9514, epoch: 288, batch: 9, loss: 0.025006622076034546, acc: 100.0, f1: 100.0, r: 0.7462028640410185
06/01/2019 11:49:23 step: 9519, epoch: 288, batch: 14, loss: 0.010135505348443985, acc: 100.0, f1: 100.0, r: 0.7522831801016281
06/01/2019 11:49:24 step: 9524, epoch: 288, batch: 19, loss: 0.058033477514982224, acc: 100.0, f1: 100.0, r: 0.6632239301346833
06/01/2019 11:49:25 step: 9529, epoch: 288, batch: 24, loss: 0.014311087317764759, acc: 100.0, f1: 100.0, r: 0.6663133887008931
06/01/2019 11:49:25 step: 9534, epoch: 288, batch: 29, loss: 0.6697617769241333, acc: 100.0, f1: 100.0, r: 0.831197041287833
06/01/2019 11:49:26 *** evaluating ***
06/01/2019 11:49:26 step: 289, epoch: 288, acc: 55.12820512820513, f1: 25.216148945580475, r: 0.25895417543321336
06/01/2019 11:49:26 *** epoch: 290 ***
06/01/2019 11:49:26 *** training ***
06/01/2019 11:49:26 step: 9542, epoch: 289, batch: 4, loss: 0.055434104055166245, acc: 100.0, f1: 100.0, r: 0.7057839562497937
06/01/2019 11:49:27 step: 9547, epoch: 289, batch: 9, loss: 0.01766286790370941, acc: 100.0, f1: 100.0, r: 0.6499534121369746
06/01/2019 11:49:28 step: 9552, epoch: 289, batch: 14, loss: 0.011125193908810616, acc: 100.0, f1: 100.0, r: 0.7903222071667451
06/01/2019 11:49:28 step: 9557, epoch: 289, batch: 19, loss: 0.005787801928818226, acc: 100.0, f1: 100.0, r: 0.7115487776881111
06/01/2019 11:49:29 step: 9562, epoch: 289, batch: 24, loss: 0.018071230500936508, acc: 100.0, f1: 100.0, r: 0.7981384749991328
06/01/2019 11:49:30 step: 9567, epoch: 289, batch: 29, loss: 0.3703101575374603, acc: 100.0, f1: 100.0, r: 0.8084991698427036
06/01/2019 11:49:30 *** evaluating ***
06/01/2019 11:49:30 step: 290, epoch: 289, acc: 56.41025641025641, f1: 27.91792788970816, r: 0.26616778473621144
06/01/2019 11:49:30 *** epoch: 291 ***
06/01/2019 11:49:30 *** training ***
06/01/2019 11:49:31 step: 9575, epoch: 290, batch: 4, loss: 0.03212951123714447, acc: 100.0, f1: 100.0, r: 0.7254151024746426
06/01/2019 11:49:31 step: 9580, epoch: 290, batch: 9, loss: 0.011205730959773064, acc: 100.0, f1: 100.0, r: 0.7317116605739549
06/01/2019 11:49:32 step: 9585, epoch: 290, batch: 14, loss: 0.008128469809889793, acc: 100.0, f1: 100.0, r: 0.6850725783313248
06/01/2019 11:49:33 step: 9590, epoch: 290, batch: 19, loss: 0.011874672956764698, acc: 100.0, f1: 100.0, r: 0.6986252491438929
06/01/2019 11:49:33 step: 9595, epoch: 290, batch: 24, loss: 0.014065833762288094, acc: 100.0, f1: 100.0, r: 0.7930288339111553
06/01/2019 11:49:34 step: 9600, epoch: 290, batch: 29, loss: 0.011486539617180824, acc: 100.0, f1: 100.0, r: 0.7054957814923326
06/01/2019 11:49:34 *** evaluating ***
06/01/2019 11:49:35 step: 291, epoch: 290, acc: 56.41025641025641, f1: 25.69147977271642, r: 0.26577240594232504
06/01/2019 11:49:35 *** epoch: 292 ***
06/01/2019 11:49:35 *** training ***
06/01/2019 11:49:35 step: 9608, epoch: 291, batch: 4, loss: 0.34660759568214417, acc: 100.0, f1: 100.0, r: 0.6370622585722792
06/01/2019 11:49:36 step: 9613, epoch: 291, batch: 9, loss: 0.004196742083877325, acc: 100.0, f1: 100.0, r: 0.696546276145741
06/01/2019 11:49:37 step: 9618, epoch: 291, batch: 14, loss: 0.010752804577350616, acc: 100.0, f1: 100.0, r: 0.6383856277875893
06/01/2019 11:49:37 step: 9623, epoch: 291, batch: 19, loss: 0.006970331072807312, acc: 100.0, f1: 100.0, r: 0.731375149627703
06/01/2019 11:49:38 step: 9628, epoch: 291, batch: 24, loss: 0.009906770661473274, acc: 100.0, f1: 100.0, r: 0.7545847870922753
06/01/2019 11:49:38 step: 9633, epoch: 291, batch: 29, loss: 0.3740772604942322, acc: 100.0, f1: 100.0, r: 0.6150952787154428
06/01/2019 11:49:39 *** evaluating ***
06/01/2019 11:49:39 step: 292, epoch: 291, acc: 55.55555555555556, f1: 27.45824853460579, r: 0.26094189445461785
06/01/2019 11:49:39 *** epoch: 293 ***
06/01/2019 11:49:39 *** training ***
06/01/2019 11:49:40 step: 9641, epoch: 292, batch: 4, loss: 0.007199874613434076, acc: 100.0, f1: 100.0, r: 0.7272203345802346
06/01/2019 11:49:40 step: 9646, epoch: 292, batch: 9, loss: 0.04842366278171539, acc: 100.0, f1: 100.0, r: 0.790582628290147
06/01/2019 11:49:41 step: 9651, epoch: 292, batch: 14, loss: 0.010448075830936432, acc: 100.0, f1: 100.0, r: 0.6760896622845878
06/01/2019 11:49:42 step: 9656, epoch: 292, batch: 19, loss: 0.009078917093575, acc: 100.0, f1: 100.0, r: 0.7118622078672728
06/01/2019 11:49:42 step: 9661, epoch: 292, batch: 24, loss: 0.018822766840457916, acc: 100.0, f1: 100.0, r: 0.8163713137145687
06/01/2019 11:49:43 step: 9666, epoch: 292, batch: 29, loss: 0.017508450895547867, acc: 100.0, f1: 100.0, r: 0.605144257900803
06/01/2019 11:49:43 *** evaluating ***
06/01/2019 11:49:44 step: 293, epoch: 292, acc: 55.55555555555556, f1: 25.225758047049606, r: 0.26126189782506776
06/01/2019 11:49:44 *** epoch: 294 ***
06/01/2019 11:49:44 *** training ***
06/01/2019 11:49:44 step: 9674, epoch: 293, batch: 4, loss: 0.012324760667979717, acc: 100.0, f1: 100.0, r: 0.6538592950879388
06/01/2019 11:49:45 step: 9679, epoch: 293, batch: 9, loss: 0.36300933361053467, acc: 100.0, f1: 100.0, r: 0.802183578052083
06/01/2019 11:49:46 step: 9684, epoch: 293, batch: 14, loss: 0.05494023114442825, acc: 100.0, f1: 100.0, r: 0.8154178824500085
06/01/2019 11:49:46 step: 9689, epoch: 293, batch: 19, loss: 0.013972058892250061, acc: 100.0, f1: 100.0, r: 0.7893401567703597
06/01/2019 11:49:47 step: 9694, epoch: 293, batch: 24, loss: 0.008082251995801926, acc: 100.0, f1: 100.0, r: 0.801023057414215
06/01/2019 11:49:47 step: 9699, epoch: 293, batch: 29, loss: 0.023545758798718452, acc: 100.0, f1: 100.0, r: 0.6487431849951042
06/01/2019 11:49:48 *** evaluating ***
06/01/2019 11:49:48 step: 294, epoch: 293, acc: 54.27350427350427, f1: 24.76500515857702, r: 0.2598859812205369
06/01/2019 11:49:48 *** epoch: 295 ***
06/01/2019 11:49:48 *** training ***
06/01/2019 11:49:49 step: 9707, epoch: 294, batch: 4, loss: 0.007597908843308687, acc: 100.0, f1: 100.0, r: 0.7553301910162198
06/01/2019 11:49:49 step: 9712, epoch: 294, batch: 9, loss: 0.02021477185189724, acc: 98.4375, f1: 96.57142857142857, r: 0.6713967965901926
06/01/2019 11:49:50 step: 9717, epoch: 294, batch: 14, loss: 0.021676981821656227, acc: 100.0, f1: 100.0, r: 0.6694088370173215
06/01/2019 11:49:51 step: 9722, epoch: 294, batch: 19, loss: 0.008852905593812466, acc: 100.0, f1: 100.0, r: 0.7253775780858955
06/01/2019 11:49:51 step: 9727, epoch: 294, batch: 24, loss: 0.011551303789019585, acc: 100.0, f1: 100.0, r: 0.8747503044189299
06/01/2019 11:49:52 step: 9732, epoch: 294, batch: 29, loss: 0.008933842182159424, acc: 100.0, f1: 100.0, r: 0.8133913134085656
06/01/2019 11:49:52 *** evaluating ***
06/01/2019 11:49:52 step: 295, epoch: 294, acc: 56.41025641025641, f1: 25.67171684738471, r: 0.25963857221723136
06/01/2019 11:49:52 *** epoch: 296 ***
06/01/2019 11:49:52 *** training ***
06/01/2019 11:49:53 step: 9740, epoch: 295, batch: 4, loss: 0.017176833003759384, acc: 100.0, f1: 100.0, r: 0.6325720087951917
06/01/2019 11:49:54 step: 9745, epoch: 295, batch: 9, loss: 0.026297712698578835, acc: 100.0, f1: 100.0, r: 0.7699301849823451
06/01/2019 11:49:54 step: 9750, epoch: 295, batch: 14, loss: 0.011384221725165844, acc: 100.0, f1: 100.0, r: 0.6919120506639641
06/01/2019 11:49:55 step: 9755, epoch: 295, batch: 19, loss: 0.012573720887303352, acc: 100.0, f1: 100.0, r: 0.8279354602882354
06/01/2019 11:49:56 step: 9760, epoch: 295, batch: 24, loss: 0.012277227826416492, acc: 100.0, f1: 100.0, r: 0.7851012626707978
06/01/2019 11:49:56 step: 9765, epoch: 295, batch: 29, loss: 0.005933993496000767, acc: 100.0, f1: 100.0, r: 0.8459539860168046
06/01/2019 11:49:57 *** evaluating ***
06/01/2019 11:49:57 step: 296, epoch: 295, acc: 57.26495726495726, f1: 28.47168955688002, r: 0.26089960429783265
06/01/2019 11:49:57 *** epoch: 297 ***
06/01/2019 11:49:57 *** training ***
06/01/2019 11:49:57 step: 9773, epoch: 296, batch: 4, loss: 0.02634701132774353, acc: 100.0, f1: 100.0, r: 0.7125495401787801
06/01/2019 11:49:58 step: 9778, epoch: 296, batch: 9, loss: 0.01646662876009941, acc: 98.4375, f1: 98.57142857142858, r: 0.801906686146677
06/01/2019 11:49:59 step: 9783, epoch: 296, batch: 14, loss: 0.030824892222881317, acc: 100.0, f1: 100.0, r: 0.6151068359790193
06/01/2019 11:49:59 step: 9788, epoch: 296, batch: 19, loss: 0.04031708091497421, acc: 100.0, f1: 100.0, r: 0.6868419103413749
06/01/2019 11:50:00 step: 9793, epoch: 296, batch: 24, loss: 0.009436127729713917, acc: 100.0, f1: 100.0, r: 0.7930858009038274
06/01/2019 11:50:01 step: 9798, epoch: 296, batch: 29, loss: 0.016839750111103058, acc: 100.0, f1: 100.0, r: 0.7690724708206669
06/01/2019 11:50:01 *** evaluating ***
06/01/2019 11:50:01 step: 297, epoch: 296, acc: 56.41025641025641, f1: 27.784339141083613, r: 0.26285915136433957
06/01/2019 11:50:01 *** epoch: 298 ***
06/01/2019 11:50:01 *** training ***
06/01/2019 11:50:02 step: 9806, epoch: 297, batch: 4, loss: 0.01014808937907219, acc: 100.0, f1: 100.0, r: 0.780974932653099
06/01/2019 11:50:03 step: 9811, epoch: 297, batch: 9, loss: 0.05233047530055046, acc: 100.0, f1: 100.0, r: 0.6806164279451091
06/01/2019 11:50:03 step: 9816, epoch: 297, batch: 14, loss: 0.014801042154431343, acc: 100.0, f1: 100.0, r: 0.6864856380575398
06/01/2019 11:50:04 step: 9821, epoch: 297, batch: 19, loss: 0.006112918257713318, acc: 100.0, f1: 100.0, r: 0.8291587044491593
06/01/2019 11:50:05 step: 9826, epoch: 297, batch: 24, loss: 0.013111967593431473, acc: 100.0, f1: 100.0, r: 0.701164871425051
06/01/2019 11:50:05 step: 9831, epoch: 297, batch: 29, loss: 0.008185775950551033, acc: 100.0, f1: 100.0, r: 0.651641270904707
06/01/2019 11:50:06 *** evaluating ***
06/01/2019 11:50:06 step: 298, epoch: 297, acc: 55.55555555555556, f1: 25.258551330997932, r: 0.26050235334974686
06/01/2019 11:50:06 *** epoch: 299 ***
06/01/2019 11:50:06 *** training ***
06/01/2019 11:50:07 step: 9839, epoch: 298, batch: 4, loss: 0.04610861465334892, acc: 98.4375, f1: 98.19245082402976, r: 0.6513992460598608
06/01/2019 11:50:07 step: 9844, epoch: 298, batch: 9, loss: 0.015354412607848644, acc: 100.0, f1: 100.0, r: 0.7060887869518786
06/01/2019 11:50:08 step: 9849, epoch: 298, batch: 14, loss: 0.054206520318984985, acc: 100.0, f1: 100.0, r: 0.7652617926330627
06/01/2019 11:50:08 step: 9854, epoch: 298, batch: 19, loss: 0.014449799433350563, acc: 100.0, f1: 100.0, r: 0.8047838831533891
06/01/2019 11:50:09 step: 9859, epoch: 298, batch: 24, loss: 0.006501548923552036, acc: 100.0, f1: 100.0, r: 0.6123600785234438
06/01/2019 11:50:10 step: 9864, epoch: 298, batch: 29, loss: 0.006338045001029968, acc: 100.0, f1: 100.0, r: 0.826036750989019
06/01/2019 11:50:10 *** evaluating ***
06/01/2019 11:50:10 step: 299, epoch: 298, acc: 57.26495726495726, f1: 25.91905361935788, r: 0.26636159940269416
06/01/2019 11:50:10 *** epoch: 300 ***
06/01/2019 11:50:10 *** training ***
06/01/2019 11:50:11 step: 9872, epoch: 299, batch: 4, loss: 0.011516417376697063, acc: 100.0, f1: 100.0, r: 0.711707361558576
06/01/2019 11:50:12 step: 9877, epoch: 299, batch: 9, loss: 0.03928632289171219, acc: 100.0, f1: 100.0, r: 0.6851800593950559
06/01/2019 11:50:12 step: 9882, epoch: 299, batch: 14, loss: 0.0065850550308823586, acc: 100.0, f1: 100.0, r: 0.7982281031796742
06/01/2019 11:50:13 step: 9887, epoch: 299, batch: 19, loss: 0.34004268050193787, acc: 100.0, f1: 100.0, r: 0.7219340695768539
06/01/2019 11:50:13 step: 9892, epoch: 299, batch: 24, loss: 0.006023291498422623, acc: 100.0, f1: 100.0, r: 0.7991805603706459
06/01/2019 11:50:14 step: 9897, epoch: 299, batch: 29, loss: 0.005579117685556412, acc: 100.0, f1: 100.0, r: 0.8229472871980501
06/01/2019 11:50:14 *** evaluating ***
06/01/2019 11:50:15 step: 300, epoch: 299, acc: 55.55555555555556, f1: 25.2753589150648, r: 0.26143424268140736
06/01/2019 11:50:15 
*** Best acc model ***
epoch: 8
acc: 58.97435897435898
f1: 20.50869800906761
corr: 0.3706723492882889
06/01/2019 11:50:15 Loading Test Data
06/01/2019 11:50:15 load data from data/word2vec_temp/test_text.npy, data/word2vec_temp/test_label.npy, training: False
06/01/2019 11:50:34 loaded. total len: 2228
06/01/2019 11:50:34 Test: length: 2228, total batch: 35, batch size: 64
06/01/2019 11:50:34 
*** Test Result ***
acc: 55.55555555555556
f1: 25.2753589150648
corr: 0.26143424268140736
